<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 18 Jan 2026 22:41:01 +0000</lastBuildDate><item><title>Overlapping Markup</title><link>https://en.wikipedia.org/wiki/Overlapping_markup</link><description>&lt;doc fingerprint="7eff01cb9371a2d0"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Overlapping markup&lt;/head&gt;&lt;p&gt;In markup languages and the digital humanities, overlap occurs when a document has two or more structures that interact in a non-hierarchical manner. A document with overlapping markup cannot be represented as a tree. This is also known as concurrent markup. Overlap happens, for instance, in poetry, where there may be a metrical structure of feet and lines; a linguistic structure of sentences and quotations; and a physical structure of volumes and pages and editorial annotations.[1][2]&lt;/p&gt;&lt;head rend="h2"&gt;History&lt;/head&gt;[edit]&lt;p&gt;The problem of non-hierarchical structures in documents has been recognised since 1988; resolving it against the dominant paradigm of text as a single hierarchy (an ordered hierarchy of content objects or OHCO) was initially thought to be merely a technical issue, but has, in fact, proven much more difficult.[4] In 2008, Jeni Tennison identified markup overlap as "the main remaining problem area for markup technologists".[5] Markup overlap continues to be a primary issue in the digital study of theological texts in 2019, and is a major reason for the field retaining specialised markup formats—the Open Scripture Information Standard and the Theological Markup Language—rather than the inter-operable Text Encoding Initiative-based formats common to the rest of the digital humanities.[6]&lt;/p&gt;&lt;head rend="h2"&gt;Properties and types&lt;/head&gt;[edit]&lt;p&gt;A distinction exists between schemes that allow non-contiguous overlap, and those that allow only contiguous overlap. Often, 'markup overlap' strictly means the latter. Contiguous overlap can always be represented as a linear document with milestones (typically co-indexed start- and end-markers), without the need for fragmenting a (logical) component into multiple physical ones. Non-contiguous overlap may require document fragmentation. Another distinction in overlapping markup schemes is whether elements can overlap with other elements of the same kind (self-overlap).[2]&lt;/p&gt;&lt;p&gt;A scheme may have a privileged hierarchy. Some XML-based schemes, for example, represent one hierarchy directly in the XML document tree, and represent other, overlapping, structures by another means; these are said to be non-privileged.&lt;/p&gt;&lt;p&gt;Schmidt (2012) identifies a tripartite classification of instances of overlap: 1. "Variation of content and structure", 2. "Overlay of multiple perspectives or markup sets", and 3. "Overlap of individual start and end tags within a single markup perspective"; additionally, some apparent instances of overlap are in fact schema definition problems, which can be resolved hierarchically. He contends that type 1 is best resolved by a system of multiple documents external to the markup, but types 2 and 3 require dealing with internally.&lt;/p&gt;&lt;head rend="h2"&gt;Approaches and implementations&lt;/head&gt;[edit]&lt;p&gt;DeRose (2004, Evaluation criteria) identifies several criteria for judging solutions to the overlap problem:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;readability and maintainability,&lt;/item&gt;&lt;item&gt;tool support and compatibility with XML,&lt;/item&gt;&lt;item&gt;possible validation schemes, and&lt;/item&gt;&lt;item&gt;ease of processing.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Tag soup is, strictly speaking, not overlapping markup—it is malformed HTML, which is a non-overlapping language, and may be ill-defined. Some web browsers attempted to represent overlapping start and end tags with non-hierarchical Document Object Models (DOM), but this was not standardised across all browsers and was incompatible with the innately hierarchical nature of the DOM.[7][8] HTML5 defines how processors should deal with such mis-nested markup in the HTML syntax and turn it into a single hierarchy.[9] With XHTML and SGML-based HTML, however, mis-nested markup is a strict error and makes processing by standards-compliant systems impossible.[10] The HTML standard defines a paragraph concept which can cause overlap with other elements and can be non-contiguous.[11]&lt;/p&gt;&lt;p&gt;SGML, which early versions of HTML were based on, has a feature called CONCUR that allows multiple independent hierarchies to co-exist without privileging any. DTD validation is only defined for each individual hierarchy with CONCUR. Validation across hierarchies is not defined by the standard. CONCUR cannot support self-overlap, and it interacts poorly with some of SGML's abbreviatory features. This feature has been poorly supported by tools and has seen very little actual use; using CONCUR to represent document overlap was not a recommended use case, according to a commentary by the standard's editor.[12][13]&lt;/p&gt;&lt;head rend="h3"&gt;Within hierarchical languages&lt;/head&gt;[edit]&lt;p&gt;There are several approaches to representing overlap in a non-overlapping language.[14] The Text Encoding Initiative, as an XML-based markup scheme, cannot directly represent overlapping markup. All four of the below approaches are suggested.[15] The Open Scripture Information Standard is another XML-based scheme, designed to mark up the Bible. It uses empty milestone elements to encode non-privileged components.[16]&lt;/p&gt;&lt;p&gt;To illustrate these approaches, marking up the sentences and lines of a fragment of Richard III by William Shakespeare will be used as a running example. Where there is a privileged hierarchy, the lines will be used.&lt;/p&gt;&lt;head rend="h4"&gt;Multiple documents&lt;/head&gt;[edit]&lt;p&gt;Multiple documents can each provide different internally consistent hierarchies. The advantage of this approach is that each document is simple and can be processed with existing tools, but requires maintenance of redundant content and it can be difficult to cross-reference between different views.[17] With multiple documents, the overlap can be analysed with data comparison and delta encoding techniques, and, in an XML context, specific XML tree differencing algorithms are available.[18][19]&lt;/p&gt;&lt;p&gt;Schmidt (2012, 3.5 Variation) recommends this approach for encoding multiple variants of a single text and to accept the duplication of the parts which do not vary, rather than attempting to create a structure that represents all of the variation present; further, he suggests that this alignment be performed automatically, and that misalignment is rare in practice.[20]&lt;/p&gt;&lt;p&gt;Example, with lines marked up:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;Who prays continually for Richmond's good.&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;So much for that.—The silent hours steal on,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;And flaky darkness breaks within the east.&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;p&gt;With sentences marked up:&lt;/p&gt;&lt;code&gt;  &amp;lt;sentence&amp;gt;I, by attorney, bless thee from thy mother,
  Who prays continually for Richmond's good.&amp;lt;/sentence&amp;gt;
  &amp;lt;sentence&amp;gt;So much for that.&amp;lt;/sentence&amp;gt;&amp;lt;sentence&amp;gt;—The silent hours steal on,
  And flaky darkness breaks within the east.&amp;lt;/sentence&amp;gt;
&lt;/code&gt;&lt;head rend="h4"&gt;Milestones&lt;/head&gt;[edit]&lt;p&gt;Milestones are empty elements that mark the beginning and end of a component, typically using the XML ID mechanism to indicate which "begin" element goes with which "end" element. Milestones can be used to embed a non-privileged structure within a hierarchical language, In their basic form they can only represent contiguous overlap. Generic XML can of course parse the milestone elements, but do not understand their special meaning and so cannot easily process or validate the non-privileged structure.[21][22]&lt;/p&gt;&lt;p&gt;Milestone have the advantage that the markup for overlapping elements is located right at the relevant boundaries, like other markup. This is an advantage for maintainability and readability.[23] CLIX (DeRose 2004) is an example of such an approach.&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;&amp;lt;sentence-start /&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;Who prays continually for Richmond's good.&amp;lt;sentence-end /&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence-start /&amp;gt;So much for that.&amp;lt;sentence-end /&amp;gt;&amp;lt;sentence-start /&amp;gt;—The silent hours steal on,&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;And flaky darkness breaks within the east.&amp;lt;sentence-end /&amp;gt;&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;p&gt;Punctuation and spaces have been identified as a type of milestone-style 'crypto-overlap' or 'pseudo-markup', as the boundaries of words, clauses, sentences and the like do not necessarily align with the formal markup boundaries hierarchically.[24][25]&lt;/p&gt;&lt;p&gt;It is also possible to use more complex milestones to represent non-contiguous structures. For example, TAGML's "suspend" and "resume" semantic[26] can be expressed using milestones, for example by adding an attribute to indicate whether each milestone represents a start, suspend, resume, or end point. Re-ordering and even self-overlap can be achieved similarly, by annotating each milestone with a "next chunk" reference.&lt;/p&gt;&lt;head rend="h4"&gt;Joins&lt;/head&gt;[edit]&lt;p&gt;Joins are pointers within a privileged hierarchy to other components of the privileged hierarchy, which may be used to reconstruct a non-privileged component akin to following a linked list. A single non-privileged element is segmented into several partial elements within the privileged hierarchy; the partial elements themselves do not represent a single unit in the non-privileged hierarchy, which can be misleading and make processing difficult.[27][28] While this approach can support some discontiguous structures, it is not able to re-order elements.[29] A slightly different approach can, however, express re-ordering by expressing the join away from the content, at the cost of directness and maintainability.[30]&lt;/p&gt;&lt;p&gt;Join-based representations can introduce the possibility of cycles between elements; detecting and rejecting these adds complexity to implementations.[31]&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;line&amp;gt;&amp;lt;sentence id="a"&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence continues="a"&amp;gt;Who prays continually for Richmond's good.&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence id="b"&amp;gt;So much for that.&amp;lt;/sentence&amp;gt;&amp;lt;sentence id="c"&amp;gt;—The silent hours steal on,&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
  &amp;lt;line&amp;gt;&amp;lt;sentence continues="c"&amp;gt;And flaky darkness breaks within the east.&amp;lt;/sentence&amp;gt;&amp;lt;/line&amp;gt;
&lt;/code&gt;&lt;head rend="h4"&gt;Stand-off markup&lt;/head&gt;[edit]&lt;p&gt;Stand-off markup is similar to using joins, except that there may be no privileged hierarchy: each part of the document is given a label (or might be referred to by an offset), and the document structure is expressed by pointing to the content from markup that 'stands off' from the content (possibly in an entirely different file), and might contain no content itself. The TEI guidelines identify the unity of the elements as a primary advantage of stand-off markup over joins, in addition to the ability to produce and distribute annotations separately from the text, possibly even by different authors applying markup to a read-only document,[32] allowing collaborative approaches to markup by a divide and conquer strategy.[33]&lt;/p&gt;&lt;p&gt;Example:&lt;/p&gt;&lt;code&gt;  &amp;lt;span id="a"&amp;gt;I, by attorney, bless thee from thy mother,&amp;lt;/span&amp;gt;
  &amp;lt;span id="b"&amp;gt;Who prays continually for Richmond's good.&amp;lt;/span&amp;gt;
  &amp;lt;span id="c"&amp;gt;So much for that.&amp;lt;/span&amp;gt;&amp;lt;span id="d"&amp;gt;—The silent hours steal on,&amp;lt;/span&amp;gt;
  &amp;lt;span id="e"&amp;gt;And flaky darkness breaks within the east.&amp;lt;/span&amp;gt;
  ...
  &amp;lt;line contents="a" /&amp;gt;
  &amp;lt;line contents="b" /&amp;gt;
  &amp;lt;line contents="c d" /&amp;gt;
  &amp;lt;line contents="e" /&amp;gt;
  &amp;lt;sentence contents="a b" /&amp;gt;
  &amp;lt;sentence contents="c" /&amp;gt;
  &amp;lt;sentence contents="d e" /&amp;gt;
&lt;/code&gt;&lt;p&gt;It has been claimed that separating markup and text can result in overall simplification and increased maintainability,[34] and by 2017, "[t]he current state of the art to [represent] (...) linguistically annotated data is to use a graph-based representation serialized as standoff XML as a pivot format",[35] i.e., that standoff was the most widely accepted approach to address the overlapping markup challenge.&lt;/p&gt;&lt;p&gt;Standoff formalisms have been the basis for an ISO standard for linguistic annotation,[36] they have been successfully applied for developing corpus management systems,[37] and (as of April 2020) they are actively being developed in the TEI.[38] One published example of a successful stand-off annotation scheme was developed as part of a bitext natural language documentation project focused on the preservation of low-resource or endangered languages.[39]&lt;/p&gt;&lt;head rend="h4"&gt;Challenges&lt;/head&gt;[edit]&lt;p&gt;Representing overlapping markup within hierarchical languages is challenging, for reasons of redundancy and/or complexity. In the 2000s to 2010s, standoff formalisms were generally accepted as the most promising approach here,[35] but a disadvantage of standoff is that validation is very challenging.[40] Standoff formalisms are not natively supported by database management systems, so that (by 2017) it was suggested to "use ... standoff XML as a pivot format (...) and relational data bases for querying."[35] In practical applications, this requires complicated architectures and/or labor-intense transformation between pivot format and internal representation. As a result, maintenance is problematic.[41] This has been a motivation to develop corpus management systems on the basis of graph data bases and for using established graph-based formalisms as pivot formats.&lt;/p&gt;&lt;head rend="h3"&gt;Special-purpose languages&lt;/head&gt;[edit]&lt;p&gt;For implementing the above-mentioned strategies, either existing markup languages (such as the TEI) can be extended or special-purpose languages can be designed.&lt;/p&gt;&lt;head rend="h4"&gt;Historical formalisms&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;LMNL is a non-hierarchical markup language first described in 2002 by Jeni Tennison and Wendell Piez, annotating ranges of a document with properties and allowing self-overlap. CLIX, which originally stood for 'Canonical LMNL In XML', provides a method for representing any LMNL document in a milestone-style XML document.[42] It also has another XML serialisation, xLMNL.[43]&lt;/item&gt;&lt;item&gt;MECS was developed by the University of Bergen's Wittgenstein Archive. However, it had several problems: it allowed some non-sensical documents of overlapping elements, it could not support self-overlap, and it did not have the capacity to define a DTD-like grammar.[44] The theory of General Ordered-Descendant Directed Acyclic Graphs (GODDAGs), while not strictly a markup language itself, is a general data model for non-hierarchical markup. Restricted GODDAGs were designed specifically to match the semantics of MECS; general GODDAGs may be non-contiguous and need a more powerful language.[45] TexMECS is a successor to MECS, which has a formal grammar and is designed to represent every GODDAG and nothing that is not a GODDAG.[46]&lt;/item&gt;&lt;item&gt;XCONCUR (previously MuLaX) is a melding-together of XML and SGML's CONCUR, and also contains a validation language, XCONCUR-CL, and a SAX-like API.[47][48][49]&lt;/item&gt;&lt;item&gt;Marinelli, Vitali and Zacchiroli provide algorithms to convert between restricted GODDAGs, ECLIX, LMNL, parallel documents in XML, contiguous stand-off markup and TexMECS.[50]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;None of these formalisms seem to be maintained anymore. Consensus community seems to be to employ standoff XML or graph-based formalisms.&lt;/p&gt;&lt;head rend="h4"&gt;Actively maintained standoff XML languages&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;GrAF-XML,[51] standoff-XML serialization of the Linguistic Annotation Framework (LAF),[36] used, e.g., for the American National Corpus[52]&lt;/item&gt;&lt;item&gt;PAULA-XML,[53] standoff-XML serialization of the data model underlying the corpus management system ANNIS and the converter suite SALT[54]&lt;/item&gt;&lt;item&gt;NAF (NLP Annotation Format / Newsreader Annotation Format),[55] standoff XML format originally developed in the NewsReader project (FP7, 2013-2015[56]), currently used by NLP tools such as FreeLing[57] (with support for English, Spanish, Portuguese, Italian, French, German, Russian, Catalan, Galician, Croatian, Slovene, etc.), and EusTagger[58] (with support for Basque, English, Spanish).&lt;/item&gt;&lt;item&gt;The Charles Harpur Critical Archive is encoded using 'multi-version documents' (MVD) to represent the variant versions of documents and as a means of indicating additions, deletions and revisions using a tactical combination of multiple documents and stand-off ranges within an underlying graph-based model. MVD is presented as an application file format, requiring specialised tools to view or edit.[59]&lt;/item&gt;&lt;item&gt;A standoff XML scheme was developed by the Odin, Intent, and XigtEdit collaboration, which is focused on a large dataset of Interlinear Glossed Text (IGT) for supporting natural language resource and documentation projects.[39]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Standoff approaches have two parts, commonly called the "content" and the "annotations." These can be expressed in unrelated representations. Simple standoff annotations per se, involve no more than a list of (location, type) pairs. Thus, in a few applications[example needed] standoff annotations are expressed in CSV, JSON(-LD, or other representations. (e.g., Web Annotation[60]) or graph formalisms grounded in string URIs (see below). However, representing and validating content in such representations is much more difficult and much less common.&lt;/p&gt;&lt;head rend="h3"&gt;Graph-based formalisms&lt;/head&gt;[edit]&lt;p&gt;Standoff markup employs a data model based on directed graphs,[61] thus complicating its representation when grounding markup information in a tree. Representing overlapping hierarchies in a graph eliminates this challenge. Standoff annotations can thus be more adequately represented as generalised directed multigraphs and use formalisms and technologies developed for this purpose, most notably those based on the Resource Description Framework (RDF).[62][63] EARMARK is an early RDF/OWL representation that encompasses General Ordered-Descendant Directed Acyclic Graphs (GODDAGs).[14] The theory of GODDAGs, while not strictly a markup language itself, is a general data model for non-hierarchical markup.&lt;/p&gt;&lt;p&gt;RDF is a semantic data model that is linearization-independent, and it provides different linearisations, including an XML format (RDF/XML) that can be modeled to mirror standoff XML, a linearisation that lets RDF be expressed in XML attributes (RDFa), a JSON format (JSON-LD), and binary formats designed to facilitate querying or processing (RDF-HDT,[64] RDF-Thrift[65]). RDF is semantically equivalent to graph-based data models underlying standoff markup; it does not require special-purpose technology for storing, parsing and querying. Multiple interlinked RDF files representing a document or a corpus constitute an example of Linguistic Linked Open Data.&lt;/p&gt;&lt;p&gt;An established technique to link arbitrary graphs with an annotated document is to use URI fragment identifiers to refer to parts of a text and/or document, see overview under Web annotation. The Web Annotation standard provides format-specific 'selectors' as an additional means, e.g., offset-, string-match- or XPath-based selectors.[66]&lt;/p&gt;&lt;p&gt;Native RDF vocabularies capable to represent linguistic annotations include:[67]&lt;/p&gt;&lt;p&gt;Related vocabularies include&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;POWLA, an OWL2/DL serialization of PAULA-XML[71]&lt;/item&gt;&lt;item&gt;RDF-NAF, an RDF serialization of the NLP Annotation Format[72]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In early 2020, W3C Community Group LD4LT has launched an initiative to harmonize these vocabularies and to develop a consolidated RDF vocabulary for linguistic annotations on the web.[73]&lt;/p&gt;&lt;head rend="h2"&gt;Notes&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ Text Encoding Initiative.&lt;/item&gt;&lt;item&gt;^ a b DeRose 2004, The problem types.&lt;/item&gt;&lt;item&gt;^ Piez 2014.&lt;/item&gt;&lt;item&gt;^ Renear, Mylonas &amp;amp; Durand 1993.&lt;/item&gt;&lt;item&gt;^ Tennison 2008.&lt;/item&gt;&lt;item&gt;^ MoChridhe 2019.&lt;/item&gt;&lt;item&gt;^ Hickson 2002.&lt;/item&gt;&lt;item&gt;^ Sivonen 2003.&lt;/item&gt;&lt;item&gt;^ HTML, § 8.2.8 An introduction to error handling and strange cases in the parser.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.1. Non-SGML Notations.&lt;/item&gt;&lt;item&gt;^ HTML, § 3.2.5.4 Paragraphs.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.2. CONCUR.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, SGML CONCUR.&lt;/item&gt;&lt;item&gt;^ a b Di Iorio, Peroni &amp;amp; Vitali 2009.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, § 20 Non-hierarchical Structures.&lt;/item&gt;&lt;item&gt;^ Durusau 2006.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, § 20.1 Multiple Encodings of the Same Information.&lt;/item&gt;&lt;item&gt;^ Schmidt 2009.&lt;/item&gt;&lt;item&gt;^ La Fontaine 2016.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 4.1 Automating Variation.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, § 20.2 Boundary Marking with Empty Elements.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.4. Milestones.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, TEI-style milestones.&lt;/item&gt;&lt;item&gt;^ Birnbaum &amp;amp; Thorsen 2015.&lt;/item&gt;&lt;item&gt;^ Haentjens Dekker &amp;amp; Birnbaum 2017.&lt;/item&gt;&lt;item&gt;^ Dekker 2018.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, § 20.3 Fragmentation and Reconstitution of Virtual Elements.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Segmentation.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.5. Fragmentation.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Joins.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 3.4 Interlinking.&lt;/item&gt;&lt;item&gt;^ Text Encoding Initiative, § 20.4 Stand-off Markup.&lt;/item&gt;&lt;item&gt;^ Schmidt 2012, 4.2 Markup Outside the Text.&lt;/item&gt;&lt;item&gt;^ Eggert &amp;amp; Schmidt 2019, Conclusion.&lt;/item&gt;&lt;item&gt;^ a b c Ide et al. 2017, p.99.&lt;/item&gt;&lt;item&gt;^ a b "ISO 24612:2012". ISO.&lt;/item&gt;&lt;item&gt;^ Chiarcos et al. 2008.&lt;/item&gt;&lt;item&gt;^ "Standoff: Annotation microstructure · Issue #1745 · TEIC/TEI". GitHub.&lt;/item&gt;&lt;item&gt;^ a b Xia, F., Lewis, W.D., Goodman, M.W. et al. Enriching a massively multilingual database of interlinear glossed text. Lang Resources &amp;amp; Evaluation 50, 321–349 (2016). https://doi.org/10.1007/s10579-015-9325-4&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.6. Standoff Markup.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, Standoff markup.&lt;/item&gt;&lt;item&gt;^ DeRose 2004, CLIX and LMNL.&lt;/item&gt;&lt;item&gt;^ Piez 2012.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000, 2.7. MECS.&lt;/item&gt;&lt;item&gt;^ Sperberg-McQueen &amp;amp; Huitfeldt 2000.&lt;/item&gt;&lt;item&gt;^ Huitfeldt &amp;amp; Sperberg-McQueen 2003.&lt;/item&gt;&lt;item&gt;^ Hilbert, Schonefeld &amp;amp; Witt 2005.&lt;/item&gt;&lt;item&gt;^ Witt et al. 2007.&lt;/item&gt;&lt;item&gt;^ Schonefeld 2008.&lt;/item&gt;&lt;item&gt;^ Marinelli, Vitali &amp;amp; Zacchiroli 2008.&lt;/item&gt;&lt;item&gt;^ "ISO GrAF". 7 March 2015.&lt;/item&gt;&lt;item&gt;^ "Home". anc.org.&lt;/item&gt;&lt;item&gt;^ "PAULA XML: Interchange Format for Linguistic Annotations". Archived from the original on 2020-08-17.&lt;/item&gt;&lt;item&gt;^ Zipser, Florian (2016-11-18). "Salt". corpus-tools.org. doi:10.5281/zenodo.17557. Retrieved 2022-09-11.&lt;/item&gt;&lt;item&gt;^ "NAF". GitHub. 30 June 2021.&lt;/item&gt;&lt;item&gt;^ "Building structured event indexes of large volumes of financial and economic data for decision making". Community Research and Development Information Service (CORDIS).&lt;/item&gt;&lt;item&gt;^ "Home - FreeLing Home Page". Archived from the original on 2012-04-29. Retrieved 2020-04-06.&lt;/item&gt;&lt;item&gt;^ "Text Analysis | HiTZ Zentroa".&lt;/item&gt;&lt;item&gt;^ Eggert &amp;amp; Schmidt 2019.&lt;/item&gt;&lt;item&gt;^ "Web Annotation Data Model". 23 February 2017.&lt;/item&gt;&lt;item&gt;^ Ide &amp;amp; Suderman 2007.&lt;/item&gt;&lt;item&gt;^ Cassidy 2010, cassidy.&lt;/item&gt;&lt;item&gt;^ Chiarcos 2012, POWLA.&lt;/item&gt;&lt;item&gt;^ "Home". rdfhdt.org.&lt;/item&gt;&lt;item&gt;^ "RDF Binary using Apache Thrift". afs.github.io.&lt;/item&gt;&lt;item&gt;^ "Selectors and States". 23 February 2017.&lt;/item&gt;&lt;item&gt;^ Cimiano, Philipp; Chiarcos, Christian; McCrae, John P.; Gracia, Jorge (2020). Linguistic Linked Data. Representation, Generation and Applications. Cham: Springer.&lt;/item&gt;&lt;item&gt;^ Verspoor, Karin; Livingston, Kevin (2012). "Towards Adaptation of Linguistic Annotations to Scholarly Annotation Formalisms on the Semantic Web". Proceedings of the Sixth Linguistic Annotation Workshop, Jeju, Republic of Korea: 75–84. Retrieved 6 April 2020.&lt;/item&gt;&lt;item&gt;^ "NLP Interchange Format (NIF) 2.0 - Overview and Documentation".&lt;/item&gt;&lt;item&gt;^ "LIF Overview".&lt;/item&gt;&lt;item&gt;^ "POWLA". January 2022.&lt;/item&gt;&lt;item&gt;^ "NLP Annotation Format | Background information on NAF".&lt;/item&gt;&lt;item&gt;^ "Towards a consolidated LOD vocabulary for linguistic annotations". GitHub. 7 September 2021.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Birnbaum, David J; Thorsen, Elise (2015). "Markup and meter: Using XML tools to teach a computer to think about versification". Proceedings of Balisage: The Markup Conference 2015. Balisage: The Markup Conference 2015. Vol. 15. Montréal. doi:10.4242/BalisageVol15.Birnbaum01. ISBN 978-1-935958-11-6.&lt;/item&gt;&lt;item&gt;Cassidy, Steve (2010). An RDF realisation of LAF in the DADA annotation server (PDF). Proceedings of ISA-5. Hong Kong. CiteSeerX 10.1.1.454.9146. Archived from the original (PDF) on 2016-03-12. Retrieved 2016-05-24.&lt;/item&gt;&lt;item&gt;Chiarcos, Christian (2012). "POWLA: Modeling linguistic corpora in OWL/DL" (PDF). The Semantic Web: Research and Applications. Proceedings of the 9th Extended Semantic Web Conference (ESWC 2012, Heraklion, Crete; LNCS 7295). Lecture Notes in Computer Science. Vol. 7295. pp. 225–239. doi:10.1007/978-3-642-30284-8_22. ISBN 978-3-642-30283-1. Retrieved 2016-05-24.[dead link]&lt;/item&gt;&lt;item&gt;Chiarcos, Christian; Dipper, Stefanie; Götze, Michael; Leser, Ulf; Lüdeling, Anke; Ritz, Julia; Stede, Manfred (2008). "A flexible framework for integrating annotations from different tools and tagsets". Traitement Automatique des Langues. 49 (2): 271–293. Archived from the original on 2020-07-18. Retrieved 2020-04-06.&lt;/item&gt;&lt;item&gt;Dekker, Ronald Haentjens; Bleeker, Elli; Buitendijk, Bram; Kulsdom, Astrid; Birnbaum, David J (2018). "TAGML: A markup language of many dimensions". Proceedings of Balisage: The Markup Conference 2018. Balisage: The Markup Conference 2018. Vol. 21. Rockville, MD. doi:10.4242/BalisageVol21.HaentjensDekker01. ISBN 978-1-935958-18-5.&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;DeRose, Steven (2004). Markup Overlap: A Review and a Horse. Extreme Markup Languages 2004. Montréal. CiteSeerX 10.1.1.108.9959. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Di Iorio, Angelo; Peroni, Silvio; Vitali, Fabio (August 2009). "Towards markup support for full GODDAGs and beyond: the EARMARK approach". Proceedings of Balisage: The Markup Conference 2009. Balisage: The Markup Conference 2009. Vol. 3. Montréal. doi:10.4242/BalisageVol3.Peroni01. ISBN 978-0-9824344-2-0.&lt;/item&gt;&lt;item&gt;Eggert, Paul; Schmidt, Desmond A (2019). "The Charles Harpur Critical Archive: A History and Technical Report". International Journal of Digital Humanities. 1 (1). Retrieved 2019-03-25.&lt;/item&gt;&lt;item&gt;Haentjens Dekker, Ronald; Birnbaum, David J (2017). "It's more than just overlap: Text As Graph". Proceedings of Balisage: The Markup Conference 2017. Balisage: The Markup Conference 2017. Vol. 19. Montréal. doi:10.4242/BalisageVol19.Dekker01. ISBN 978-1-935958-15-4.&lt;/item&gt;&lt;item&gt;Durusau, Patrick (2006). OSIS Users Manual (OSIS Schema 2.1.1) (PDF). Archived (PDF) from the original on 2014-10-23. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Ian Hickson (2002-11-21). "Tag Soup: How UAs handle &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;/x&amp;gt; &amp;lt;/y&amp;gt;". Retrieved 2017-11-05.&lt;/item&gt;&lt;item&gt;Hilbert, Mirco; Schonefeld, Oliver; Witt, Andreas (2005). Making CONCUR work. Extreme Markup Languages 2005. Montréal. CiteSeerX 10.1.1.104.634. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Huitfeldt, Claus; Sperberg-McQueen, C M (2003). "TexMECS: An experimental markup meta-language for complex documents". Archived from the original on 2017-02-27. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Ide, Nancy; Chiarcos, Christian; Stede, Manfred; Cassidy, Steve (2017). "Designing Annotation Schemes: From Model to Representation". In Ide, Nancy; Pustejovsky, James (eds.). Handbook of Linguistic Annotation. Dordrecht: Springer. p. 99. doi:10.1007/978-94-024-0881-2_3. ISBN 978-94-024-0879-9.&lt;/item&gt;&lt;item&gt;La Fontaine, Robin (2016). "Representing Overlapping Hierarchy as Change in XML". Proceedings of Balisage: The Markup Conference 2016. Balisage: The Markup Conference 2016. Vol. 17. Montréal. doi:10.4242/BalisageVol17.LaFontaine01. ISBN 978-1-935958-13-0.&lt;/item&gt;&lt;item&gt;Marinelli, Paolo; Vitali, Fabio; Zacchiroli, Stefano (January 2008). "Towards the unification of formats for overlapping markup" (PDF). New Review of Hypermedia and Multimedia. 14 (1): 57–94. CiteSeerX 10.1.1.383.1636. doi:10.1080/13614560802316145. ISSN 1361-4568. S2CID 16909224. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;MoChridhe, Race J (2019-04-24). "Twenty Years of Theological Markup Languages: A Retro- and Prospective". Theological Librarianship. 12 (1). doi:10.31046/tl.v12i1.523. ISSN 1937-8904. S2CID 171582852. Archived from the original on 2019-07-15. Retrieved 2019-07-15.&lt;/item&gt;&lt;item&gt;Piez, Wendell (August 2012). "Luminescent: parsing LMNL by XSLT upconversion". Proceedings of Balisage: The Markup Conference 2012. Balisage: The Markup Conference 2012. Vol. 8. Montréal. doi:10.4242/BalisageVol8.Piez01. ISBN 978-1-935958-04-8. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Piez, Wendell (2014). Hierarchies within range space: From LMNL to OHCO. Balisage: The Markup Conference 2014. Montréal. doi:10.4242/BalisageVol13.Piez01.&lt;/item&gt;&lt;item&gt;Renear, Allen; Mylonas, Elli; Durand, David (1993-01-06). "Refining our Notion of What Text Really Is: The Problem of Overlapping Hierarchies". CiteSeerX 10.1.1.172.9017. hdl:2142/9407. Archived from the original on 2021-03-23. Retrieved 2016-10-02.&lt;/item&gt;&lt;item&gt;Schonefeld, Oliver (August 2008). A Simple API for XCONCUR: Processing concurrent markup using an event-centric API. Balisage: The Markup Conference 2008. Montréal. doi:10.4242/BalisageVol1.Schonefeld01. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Sperberg-McQueen, C M; Huitfeldt, Claus (2004). "GODDAG: A Data Structure for Overlapping Hierarchies". Digital Documents: Systems and Principles. Lecture Notes in Computer Science. Vol. 2023. pp. 139–160. doi:10.1007/978-3-540-39916-2_12. ISBN 978-3-540-21070-2. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Schmidt, Desmond (2009). "Merging Multi-Version Texts: A Generic Solution to the Overlap Problem". Merging Multi-Version Texts: a General Solution to the Overlap Problem. Balisage: The Markup Conference 2009. Proceedings of Balisage: The Markup Conference 2009. Vol. 3. Montréal. doi:10.4242/BalisageVol3.Schmidt01. ISBN 978-0-9824344-2-0.&lt;/item&gt;&lt;item&gt;Schmidt, Desmond (2012). "The role of markup in the digital humanities". Historical Social Research. 27 (3): 125–146. doi:10.12759/hsr.37.2012.3.125-146.&lt;/item&gt;&lt;item&gt;Henri Sivonen (2003-08-16). "Tag Soup: How Mac IE 5 and Safari handle &amp;lt;x&amp;gt; &amp;lt;y&amp;gt; &amp;lt;/x&amp;gt; &amp;lt;/y&amp;gt;". Retrieved 2017-11-05.&lt;/item&gt;&lt;item&gt;Ide, Nancy; Suderman, Keith (2007). GrAF: A graph-based format for linguistic annotations (PDF). Proceedings of the First Linguistic Annotation Workshop (LAW-2007, Prague, Czech Republic). pp. 1–8. CiteSeerX 10.1.1.146.4543.&lt;/item&gt;&lt;item&gt;Tennison, Jenni (2008-12-06). "Overlap, Containment and Dominance". Retrieved 2016-10-02.&lt;/item&gt;&lt;item&gt;Witt, Andreas; Schonefeld, Oliver; Rehm, Georg; Khoo, Jonathan; Evang, Kilian (2007). On the Lossless Transformation of Single-File, Multi-Layer Annotations into Multi-Rooted Trees. Extreme Markup Languages 2007. Montréal. Archived from the original on 2014-10-17. Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;Text Encoding Initiative Consortium (16 September 2014). "Guidelines for Electronic Text Encoding and Interchange" (5 ed.). Retrieved 2014-10-14.&lt;/item&gt;&lt;item&gt;WHATWG. "HTML Living Standard". Retrieved 2019-03-25.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46666650</guid><pubDate>Sun, 18 Jan 2026 10:37:59 +0000</pubDate></item><item><title>Show HN: Xenia – A monospaced font built with a custom Python engine</title><link>https://github.com/Loretta1982/xenia</link><description>&lt;doc fingerprint="3e089154df2e396c"&gt;
  &lt;main&gt;
    &lt;p&gt;I made this because monofont shouldn't have to be fugly. I use it myself for coding and I always forget its mono.&lt;/p&gt;
    &lt;p&gt;I will release more weights if I get enough interest...&lt;/p&gt;
    &lt;p&gt;If you want to support an indie coder, buy me a coffee or just download and use for free... enjoy :)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;download &lt;code&gt;xenia_regular.ttf&lt;/code&gt;from this repository&lt;/item&gt;
      &lt;item&gt;open the file and click Install&lt;/item&gt;
      &lt;item&gt;in your editor (Sublime Text, VS Code, etc.) or Terminal, set your font face to &lt;code&gt;xenia&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;700+ glyphs: deep support for symbols and math.&lt;/item&gt;
      &lt;item&gt;non_ambiguous: distinctive &lt;code&gt;1&lt;/code&gt;,&lt;code&gt;l&lt;/code&gt;,&lt;code&gt;I&lt;/code&gt;,&lt;code&gt;0&lt;/code&gt;,&lt;code&gt;O&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;clean geometry: no "yucko" lowercase &lt;code&gt;a&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;python generated: built with a custom procedural engine&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46666661</guid><pubDate>Sun, 18 Jan 2026 10:39:52 +0000</pubDate></item><item><title>Starting from scratch: Training a 30M Topological Transformer</title><link>https://www.tuned.org.uk/posts/013_the_topological_transformer_training_tauformer</link><description>&lt;doc fingerprint="141d03d4f13c1c9b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Training a 30M parameters Topological Transformer&lt;/head&gt;
    &lt;p&gt;Tauformer is a topological transformer (see paper) that replaces dot‑product attention with a Laplacian-derived scalar (taumode) per token/head, then attends using distances in that scalar space. Below is a post-style overview of the idea and the first training signals from a 30M-parameter run.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tauformer in one idea&lt;/head&gt;
    &lt;p&gt;Tauformer’s goal is to inject domain structure directly into attention by using a Graph Laplacian built from a domain embedding space (a “domain memory”) as a persistent reference. Instead of ranking keys by \(Q\cdot K\), Tauformer ranks them by how similar their Laplacian-derived taumode scalars are, which is intended to bias attention toward domain-relevant relations rather than generic geometric similarity.&lt;/p&gt;
    &lt;p&gt;At the implementation level, Tauformer keeps the familiar Q/K/V projections, RoPE, causal masking, and stable softmax/value aggregation pipeline, but changes how attention logits are computed. Each head vector is compressed into a scalar \(\lambda\) using a bounded Rayleigh-quotient energy computed with a feature-space Laplacian \(L\), then logits are computed as a negative distance \(-|\lambda_q-\lambda_k|/\text{temperature}\).&lt;/p&gt;
    &lt;p&gt;Key building blocks (as implemented):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Taumode scalar: compute \(E_{\text{raw}}=(x^\top L x)/(x^\top x+\varepsilon)\), then bound it as \(E_{\text{raw}}/(E_{\text{raw}}+\tau)\) to produce \(\lambda\in[0,1)\).&lt;/item&gt;
      &lt;item&gt;Logits: \(\text{att}_{ij} = -\|\lambda^Q_i - \lambda^K_j\|/\text{temperature}\), then reuse causal mask \(→\) subtract row max \(→\) softmax \(→\) multiply by \(V\).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why it can be cheaper&lt;/head&gt;
    &lt;p&gt;Because scoring no longer needs full key vectors, Tauformer’s KV-cache can store values plus a compact key-side scalar stream rather than both K and V tensors. Concretely, the cache payload is \((V,\lambda_k)\) (not \((K,V)\)), which yields an approximate ~50% per-layer cache reduction for typical head dimensions (small overhead for storing the extra scalar).&lt;/p&gt;
    &lt;p&gt;The design also anticipates using a sparse Laplacian from a precomputed domain manifold so computing \(\lambda\) can depend on Laplacian sparsity (nnz) rather than dense \(D^2\) multiplication. It exchanges the long preliminary adjustment of weights with a pre-training shorter phase in which a Laplacian is built using &lt;code&gt;arrowspace&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Run setup (what was trained)&lt;/head&gt;
    &lt;p&gt;This run trains a 30M-class TauGPT. Training uses AdamW with base LR \(5\times10^{-4}\) and a warmup of 100 steps, then keeps the base LR constant unless the plateau logic scales it down. Data comes from a local JSONL file (&lt;code&gt;train.jsonl&lt;/code&gt;) streamed through an IterableDataset, with a routed split where every 20th batch is used for validation (\(≈5%\)).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Setting&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Class / size&lt;/cell&gt;
        &lt;cell&gt;TauGPT ~30M parameters (GPT2-inspired)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Layers (&lt;code&gt;n_layer&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Heads (&lt;code&gt;n_head&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Embedding size (&lt;code&gt;n_embd&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;384&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Sequence length (&lt;code&gt;seq_len&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;1024&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Model&lt;/cell&gt;
        &lt;cell&gt;Vocabulary size (&lt;code&gt;vocab_size&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;30522&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Optimizer&lt;/cell&gt;
        &lt;cell&gt;Optimizer&lt;/cell&gt;
        &lt;cell&gt;AdamW&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Optimizer&lt;/cell&gt;
        &lt;cell&gt;Base learning rate&lt;/cell&gt;
        &lt;cell&gt;5e-4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LR schedule&lt;/cell&gt;
        &lt;cell&gt;Warmup&lt;/cell&gt;
        &lt;cell&gt;100 steps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LR schedule&lt;/cell&gt;
        &lt;cell&gt;Post-warmup behavior&lt;/cell&gt;
        &lt;cell&gt;Constant LR (no decay unless manually/externally adjusted)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;Source file&lt;/cell&gt;
        &lt;cell&gt;Local JSONL file &lt;code&gt;train.jsonl&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Data&lt;/cell&gt;
        &lt;cell&gt;Loading mode&lt;/cell&gt;
        &lt;cell&gt;Streamed via an IterableDataset-style pipeline (no shuffle in DataLoader)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Validation&lt;/cell&gt;
        &lt;cell&gt;Split rule&lt;/cell&gt;
        &lt;cell&gt;Routed split where every 20th batch is used for validation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Validation&lt;/cell&gt;
        &lt;cell&gt;Approx. validation fraction&lt;/cell&gt;
        &lt;cell&gt;About 5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Results at a glance&lt;/head&gt;
    &lt;p&gt;At step 100 the run reports train loss 4.6772 and val loss 4.9255 (PPL 107.47), and by step 2000 it reaches val loss 2.3585 (Perplexity 6.59). The best validation point in the log is step 4500 with &lt;code&gt;val_loss=1.9146&lt;/code&gt;, after which validation regresses to &lt;code&gt;2.3746&lt;/code&gt; by step 5000.
The final run summary records &lt;code&gt;step=5000&lt;/code&gt;, &lt;code&gt;best_val_loss=1.914555&lt;/code&gt;, &lt;code&gt;current_lr_scale=0.03125&lt;/code&gt;, and &lt;code&gt;total_tokens=655360000&lt;/code&gt;. That is a good result for \(~2\) hours of training on this smallest model (at an average of ~60K Tokens Per Second).&lt;/p&gt;
    &lt;p&gt;The early phase is strong: validation drops from 4.93 at step 100 to ~2.36 by step 2000, showing that the model and pipeline learn effectively at this scale. After that, validation becomes noisy (e.g., rising back to 2.92 at step 2100 and peaking near 2.95 at step 4200) before the late “lucky break” to 1.91 at step 4500. Throughout, the run holds a fixed taumode value which means the attention geometry is not being updated as weights evolve as this will be take place in the next iterations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Baseline: Closing note&lt;/head&gt;
    &lt;p&gt;All the model’s files, data, training settings and logs will be published with a permissive license once the results are consolidated and tests will move to a larger scale model.&lt;/p&gt;
    &lt;p&gt;This baseline run kept taumode fixed throughout, while using a simple validation loop and plateau-triggered LR scaling, and it still converged quickly in the early-to-mid training window.&lt;/p&gt;
    &lt;p&gt;Because the later part of the run shows volatility and regression after the best checkpoint, the next experiments focus on “adaptive” taumode strategies where taumode is recalibrated at intervals (including the “gradient” strategy that detects energy drift and gates recalibration by performance of the gradient in the previous steps) plus more sophisticated validation behaviors already implemented in the training loop.&lt;/p&gt;
    &lt;p&gt;Considering the small model size and the short training horizon (5,000 steps total, lowest loss at 4600), these results support the architecture as promising, with broader evaluation and scaled tests planned next—especially at 100M parameters.&lt;/p&gt;
    &lt;p&gt;A very interesting question has been raised by this test: what is the correlation between cross-entropy and taumode? Model convergence brings the loss down but at the same time recalibrating the taumode used on the learned weights brings down the taumode.&lt;/p&gt;
    &lt;head rend="h2"&gt;What may be correlated (and why)&lt;/head&gt;
    &lt;p&gt;Cross-entropy and taumode are likely correlated because Tauformer’s attention kernel is built from Laplacian-derived scalar energies (λ/taumode) rather than dot-product similarity, so changes in the λ distribution change attention behavior and therefore training dynamics. In the current training loop, the observed “taumode convergence” is also mechanically explained by how taumode is recalibrated: on (re)start, the code can compute a median energy from block0 key (K) vectors produced by the current weights and then set that median as the global taumode.&lt;/p&gt;
    &lt;head rend="h2"&gt;What “converging taumode” means here&lt;/head&gt;
    &lt;p&gt;The calibration is effectively computing a Rayleigh-style energy statistic on K vectors under a Laplacian (numerator/denominator), and then taking a median over the batch to set a single scalar taumode. In the reference implementation, taumode/λ is based on a bounded Rayleigh quotient: \(E_{\text{raw}}(x) = \frac{x^\top L x}{x^\top x + \varepsilon}\) and then \(\lambda_\tau(x)=\frac{E_{\text{raw}}}{E_{\text{raw}}+\tau}\), which maps energies into \([0,1)\).&lt;/p&gt;
    &lt;head rend="h2"&gt;Why taumode can drift downward as loss improves&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Healthy interpretation: as training progresses, the model may learn K representations that are “smoother” (lower-energy, so closer) with respect to the domain/manifold Laplacian, pushing the median energy down while also improving next-token prediction (lower cross-entropy).&lt;/item&gt;
      &lt;item&gt;Unhealthy interpretation (collapse risk): median energy can also drop if K vectors collapse toward low-variance or less-discriminative configurations, which can reduce contrast in λ-distance logits even if loss continues improving short-term.&lt;/item&gt;
      &lt;item&gt;Key confound: if taumode is recalibrated on resume, then taumode changes are not purely a passive “measurement of convergence”; they can act like a mid-training hyperparameter change, so correlation with loss does not automatically imply causality in the direction “lower taumode \(⇒\) lower loss”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A strong explanation for “converging taumode” (as a property of learned representations, not an artifact) is: as weights converge, the distribution of per-token energies \(x^\top L x\) stabilizes, so repeated measurements (median, p50) across batches and checkpoints become consistent and typically shift toward lower-energy manifold-aligned directions. To validate that, it helps to separate (1) the fixed constant used by attention from (2) a purely diagnostic “current batch median energy”, and track not just the median but also the spread (p05/p95), because collapse would show shrinking spread even when the median looks lower.&lt;/p&gt;
    &lt;p&gt;“lower loss \(⇒\) lower taumode” is a plausible causal direction in Tauformer, because the cross-entropy gradient flows through the Tauformer attention path that depends on Laplacian-energy-derived scalars computed from Q/K (and in your calibration code, specifically from block0 K vectors). As the model improves next-token prediction, it can simultaneously learn representations whose Laplacian Rayleigh energy is lower, so any “recalibrate taumode from learned weights” procedure will tend to output a smaller median. If this it true, where is the optimal stopping state?&lt;/p&gt;
    &lt;head rend="h2"&gt;Further readings&lt;/head&gt;
    &lt;p&gt;Some shift is happening in understanding information thanks to large scale learning machines!&lt;/p&gt;
    &lt;p&gt;In this recent paper, MDL refers to the “minimum description length principle”, which says the best explanation/model is the one that minimizes the total code length needed to describe (1) the model and (2) the data given the model. Epiplexity \(ST(X)\) is defined as the program length of the compute-feasible model \(P\) that minimizes time-bounded MDL, while time-bounded entropy HT(X) is the expected code length of the data under that model. Operationally, the paper proposes practical estimators based on neural-network training dynamics (e.g., prequential “area under the loss curve above final loss”) to approximate how much structure a bounded learner actually absorbs from data&lt;/p&gt;
    &lt;p&gt;Qualitatively, &lt;code&gt;arrowspace&lt;/code&gt;, &lt;code&gt;taumode&lt;/code&gt; and &lt;code&gt;tau-attention&lt;/code&gt; are exactly the kind of deterministic computations that can increase usable/learnable structure for bounded learners, which is one of the central motivations for epiplexity.
Through the epiplexity lens, the operations carried on by &lt;code&gt;arrowspace&lt;/code&gt; and Tauformer (converts each head vector into a bounded scalar λτ using a Rayleigh-quotient-style energy followed by a bounding map) is a deterministic compression that can re-factor information into a form that is cheaper for downstream computation to exploit, potentially increasing the amount of structure a bounded observer can learn from the same underlying signal.&lt;/p&gt;
    &lt;p&gt;I am happy I have somehow anticipated this switch in point of view in &lt;code&gt;arrowspace&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknoledgements&lt;/head&gt;
    &lt;p&gt;I gratefully acknowledge Enverge Labs for kindly providing the computation time used to run these experiments on their H100 GPU cluster powered by clean and cheap energy, this aligns perfectly with the topological tranformer objective to provide cheaper computation for Transformers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46666963</guid><pubDate>Sun, 18 Jan 2026 11:39:14 +0000</pubDate></item><item><title>Keystone (YC S25) Is Hiring</title><link>https://news.ycombinator.com/item?id=46667101</link><description>&lt;doc fingerprint="8d21d67548473058"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Keystone builds infrastructure for autonomous coding agents. We give agents sandboxed environments that mirror production, event-based triggers (Sentry, Linear, GitHub), and verification workflows so they can ship code end-to-end— not just write it. We're hiring a founding engineer to work directly with me (solo founder) on core product. Stack is TypeScript, React (Next.js), Python, Postgres, Redis, AWS.&lt;/p&gt;
      &lt;p&gt;In-person in SoMa. $150K-$350K + 0.5-3% equity.&lt;/p&gt;
      &lt;p&gt;https://www.workatastartup.com/jobs/88801&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46667101</guid><pubDate>Sun, 18 Jan 2026 12:00:10 +0000</pubDate></item><item><title>Software engineers can no longer neglect their soft skills</title><link>https://www.qu8n.com/posts/most-important-software-engineering-skill-2026</link><description>&lt;doc fingerprint="bcc5561f2e328e5d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Software engineers can no longer neglect their soft skills&lt;/head&gt;
    &lt;p&gt;January 6, 2026&lt;/p&gt;
    &lt;p&gt;Starting in 2026, communication has become the most important skill for software engineers.&lt;/p&gt;
    &lt;p&gt;It's not writing code, system designs, or having estoric knowledge of a programming language (i.e., Rust).&lt;/p&gt;
    &lt;p&gt;AI coding agents have gotten very, very good. A year ago, I'd reach out to Cursor hesitantly for MVPs or quick fixes. Today, I use Claude Code for almost all non-trivial programming tasks and have spent $500+ on it just last December.&lt;/p&gt;
    &lt;p&gt;AI talks online revolve much around the hard skils. Initially it was prompt tricks to accomplish X, then the best MCPs for Y, and so on. But with Opus 4.5, using vanilla Claude Code gets you 80% there. Even in the age of AI, the 80/20 rule still applies. So, what should engineers focus on?&lt;/p&gt;
    &lt;p&gt;One thing with coding agents is that the better the spec, the more in line they will be with the technical and business requirements. But getting a good spec is hard.&lt;/p&gt;
    &lt;p&gt;In real life, tickets rarely contain all the requirements. To do so, you might need to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ask questions that reveal assumptions people didn't know they had&lt;/item&gt;
      &lt;item&gt;Facilite trade-off discussions&lt;/item&gt;
      &lt;item&gt;Push back on scope without burning bridges&lt;/item&gt;
      &lt;item&gt;Make calls on things nobody thought to specify&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Doing these things well used to be optional for individual contributors. Certain teams would enable engineers to thrive being an average communicator but excellent coder. Now, the non-coding parts are becoming a non-negotiable.&lt;/p&gt;
    &lt;p&gt;Software engineers are problem solvers. We believe that every problem has a solution, a "best practice". But working with people is messy.&lt;/p&gt;
    &lt;p&gt;&lt;del&gt;Un&lt;/del&gt;fortunately, we won't be able to AI our way into better communication skills. Good communication requires empathy, and we can all use a little more of that in today's landscape.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46667572</guid><pubDate>Sun, 18 Jan 2026 13:14:20 +0000</pubDate></item><item><title>Predicting OpenAI's ad strategy</title><link>https://ossa-ma.github.io/blog/openads</link><description>&lt;doc fingerprint="281eb85881276e3e"&gt;
  &lt;main&gt;
    &lt;p&gt;The World is Ads&lt;/p&gt;
    &lt;p&gt;Here we go again, the tech press is having another AI doom cycle.&lt;/p&gt;
    &lt;p&gt;I've primarily written this as a response to an NYT analyst painting a completely unsubstantiated, baseless, speculative, outrageous, EGREGIOUS, preposterous "grim picture" on OpenAI going bust.&lt;/p&gt;
    &lt;p&gt;Mate come on. OpenAI is not dying, they're not running out of money. Yes, they're creating possibly the craziest circular economy and defying every economics law since Adam Smith published 'The Wealth of Nations'. $1T in commitments is genuinely insane. But I doubt they're looking to be acquired; honestly by who? you don't raise $40 BILLION at $260 BILLION VALUATION to get acquired. It's all for the $1T IPO.&lt;/p&gt;
    &lt;p&gt;But it seems that the pinnacle of human intelligence: the greatest, smartest, brightest minds have all come together to... build us another ad engine. What happened to superintelligence and AGI?&lt;/p&gt;
    &lt;p&gt;See if OpenAI was not a direct threat to the current ad giants would Google be advertising Gemini every chance they get? Don't forget they're also capitalising on their brand new high-intent ad funnel by launching ads on Gemini and AI overview.&lt;/p&gt;
    &lt;p&gt;Let's crunch the numbers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick Recap of OpenAI's 2025&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;March: Closed $40B funding round at $260B valuation, the largest raise by a private tech company on record.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;June: Hit $10B ARR.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;July: First $1B revenue month, doubled from $500M monthly in January.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;November: Sam Altman says OpenAI expects $20B ARR for 2025.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Reached 800M WAU, ~190M DAU, 35M paying subscribers, 1M business customers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;January 2026: "Both our Weekly Active User (WAU) and Daily Active User (DAU) figures continue to produce all-time-highs (Jan 14 was the highest, Jan 13 was the second highest, etc.)"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;January 16, 2026: Announced ads in ChatGPT free and Go tiers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, OpenAI is burning $8-12B in 2025. Compute infrastructure is obviously not cheap when serving 190M people daily.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting OpenAI's Ad Strategy&lt;/head&gt;
    &lt;p&gt;So let's try to model their expected ARPU (annual revenue per user) by understanding what OpenAI is actually building and how it compares to existing ad platforms.&lt;/p&gt;
    &lt;p&gt;The ad products they've confirmed thus far:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ads at bottom of answers when there's a relevant sponsored product or service based on your current conversation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rollout:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Q1 2026: Limited beta with select advertisers&lt;/item&gt;
      &lt;item&gt;Q2-Q3 2026: Expanded to ChatGPT Search for free-tier users&lt;/item&gt;
      &lt;item&gt;Q4 2026: Sidebar sponsored content + affiliate features&lt;/item&gt;
      &lt;item&gt;2027: Full international expansion, self-serve platform&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Testing starts "in the coming weeks" for logged-in adults in the U.S. on free and Go tiers. Ads will be "clearly labeled and separated from the organic answer." Users can learn why they're seeing an ad or dismiss it.&lt;/p&gt;
    &lt;p&gt;Their principles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Answer independence: Ads don't influence ChatGPT's answers&lt;/item&gt;
      &lt;item&gt;Conversation privacy: Conversations stay private from advertisers, data never sold&lt;/item&gt;
      &lt;item&gt;Choice and control: Users can turn off personalization and clear ad data&lt;/item&gt;
      &lt;item&gt;Plus, Pro, Business, and Enterprise tiers won't have ads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They also mentioned a possibility of conversational ads where you can ask follow-up questions about products directly.&lt;/p&gt;
    &lt;p&gt;Revenue targets: Reports suggest OpenAI is targeting $1B in ad revenue for 2026, scaling to $25B by 2029, though OpenAI hasn't confirmed these numbers publicly. We can use these as the conservative benchmark, but knowing the sheer product talent at OpenAI, the funding and hunger. I think they're blow past this.&lt;/p&gt;
    &lt;p&gt;Personal speculations on integration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Self-serve platform: Advertisers bid for placements, super super super likely, exactly what Google does, probably their biggest revenue stream.&lt;/item&gt;
      &lt;item&gt;Affiliate commissions: Built-in checkouts so users can buy products inside ChatGPT, OpenAI takes commission, similar to their Shopify collab.&lt;/item&gt;
      &lt;item&gt;Sidebar sponsored content: When users ask about topics with market potential, sponsored info appears in a sidebar marked "Sponsored"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now let's compare this to existing ad platforms:&lt;/p&gt;
    &lt;head rend="h3"&gt;Google: Intent + Vertical Integration = Highest Revenue&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Auction-based system where advertisers bid on keywords. Ads appear in search results based on bid + quality score.&lt;/item&gt;
      &lt;item&gt;Why it works: High intent (search queries) + owns the entire vertical stack (ad tech, auction system, targeting, decades of optimization)&lt;/item&gt;
      &lt;item&gt;Ad revenue: [$212.4B in ad revenue in the first 3 quarters of 2025]https://www.demandsage.com/google-ads-statistics/ (8.4% growth from 2024's $273.4B)&lt;/item&gt;
      &lt;item&gt;Google doesn't report ARPU so we need to calculate it: ARPU = $296.2B (projected) ÷ 5.01B = $59.12 per user annually.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Meta: No Intent + Vertical Integration = High ARPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Newsfeed ads delivered via auction. Meta's Andromeda AI evaluates bid + predicted action rate + ad quality to determine placement.&lt;/item&gt;
      &lt;item&gt;Why it works: Passive scrolling = low purchase intent, but on a massive scale + owns targeting infrastructure + Andromeda AI&lt;/item&gt;
      &lt;item&gt;ARPU: $68.44 in North America, $49.63 globally (Q1 2025)&lt;/item&gt;
      &lt;item&gt;Revenue: $160B in 2024 (97.3% of total revenue)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Twitter/X: Engagement + No Vertical Stack = Low ARPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Auction-based promoted tweets in timeline. Advertisers only pay when users complete actions (click, follow, engage).&lt;/item&gt;
      &lt;item&gt;Why it works: Timeline engagement, CPC ~$0.18, but doesn't own vertical stack and does it on a smaller scale&lt;/item&gt;
      &lt;item&gt;ARPU: ~$5.54 ($2.3B revenue ÷ 415M MAU)&lt;/item&gt;
      &lt;item&gt;Revenue: ~$2.3B in 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;ChatGPT: High Intent + No Vertical Stack = Where Does It Sit?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intent level: High. 2.5B prompts daily includes product research, recommendations, comparisons. More intent than Meta's passive scrolling, comparable to Google search.&lt;/item&gt;
      &lt;item&gt;Vertical integration: None. Yet.&lt;/item&gt;
      &lt;item&gt;Scale: 1B WAU by Feb 2026, but free users only (~950M at 95% free tier).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So where should ChatGPT's ARPU sit?&lt;/p&gt;
    &lt;p&gt;It sits with Search, not Social.&lt;/p&gt;
    &lt;p&gt;Which puts it between X ($5.54) and Meta ($49.63). OpenAI has better intent than Meta but worse infrastructure. They have more scale than X but no vertical integration. When a user asks ChatGPT "Help me plan a 5-day trip to Kyoto" or "Best CRM for small business," that is High Intent. That is a Google-level query, not a Facebook-level scroll.&lt;/p&gt;
    &lt;p&gt;We already have a benchmark for this: Perplexity.&lt;/p&gt;
    &lt;p&gt;In late 2024/2025, reports confirmed Perplexity was charging CPMs exceeding $50. This is comparable to premium video or high-end search, and miles above the ~$2-6 CPMs seen on social feeds.&lt;/p&gt;
    &lt;p&gt;If Perplexity can command $50+ CPMs with a smaller user base, OpenAI’s "High Agency" product team will likely floor their pricing there.&lt;/p&gt;
    &lt;p&gt;Super Bullish Target ARPU Trajectory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026: $5.50 (The "Perplexity Floor") - Even with a clumsy beta and low fill rate, high-intent queries command premium pricing. If they serve just one ad every 20 queries at a Perplexity-level CPM, they hit this number effortlessly.&lt;/item&gt;
      &lt;item&gt;2027: $18.00 - The launch of a self-serve ad manager (like Meta/Google) allows millions of SMBs to bid. Competition drives price.&lt;/item&gt;
      &lt;item&gt;2028: $30.00 - This is where "Ads" become "Actions." OpenAI won't just show an ad for a flight; they will book it. Taking a cut of the transaction (CPA model) yields 10x the revenue of showing a banner.&lt;/item&gt;
      &lt;item&gt;2029: $50.00 (Suuuuuuuper bullish case) - Approaching Google’s ~$60 ARPU. By now, the infrastructure is mature, and "Conversational Commerce" is the standard. This is what Softbank is praying will happen.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And we're forgetting that OpenAI have a serious serious product team, I don't doubt for once they'll be fully capable of building out the stack and integrating ads til they occupy your entire subconscious.&lt;/p&gt;
    &lt;p&gt;In fact they hired Fidji Simo as their "CEO of Applications", a newly created role that puts her in charge of their entire revenue engine. Fidji is a Meta powerhouse who spent a decade at Facebook working on the Facebook App and... ads:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Leading Monetization of the Facebook App, with a focus on mobile advertising that represents the vast majority of Facebook's revenue. Launched new ad products such as Video Ads, Lead Ads, Instant Experiences, Carousel ads, etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Launched and grew video advertising to be a large portion of Facebook's revenue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Being Realistic About Competition&lt;/head&gt;
    &lt;p&gt;ChatGPT will hit 1B WAU by February 2026.&lt;/p&gt;
    &lt;p&gt;But 1.5-1.8B free users by 2028? That assumes zero competition impact from anyone, certainly not the looming giant Gemini. Unrealistic.&lt;/p&gt;
    &lt;p&gt;Let's estimate growth super conservatively accounting for competition:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026: 950M free users (1B WAU × 95% free tier)&lt;/item&gt;
      &lt;item&gt;2027: 1.1B free users (slower growth as market saturates)&lt;/item&gt;
      &lt;item&gt;2028: 1.2-1.3B free users (competition from Google, Claude)&lt;/item&gt;
      &lt;item&gt;2029: 1.4B free users (mature market, multi-player landscape)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The main revenue growth comes from ARPU scaling not just user growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting 2026&lt;/head&gt;
    &lt;p&gt;Crunching all the numbers from "High Intent" model, 2026 looks different.&lt;/p&gt;
    &lt;p&gt;Base revenue from subscriptions + enterprise + API: $25-30B&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;35M paying subscribers: $8.4B minimum (conservatively assuming all at $20/mo Plus tier)&lt;/item&gt;
      &lt;item&gt;Definitely higher with Pro ($200/mo) and Enterprise (custom pricing)&lt;/item&gt;
      &lt;item&gt;Enterprise/API: $2.3B in 2025 → $17.4B by mid-2027&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ad revenue (year 1): ~$5.2B&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;950M free users x $5.50 ARPU&lt;/item&gt;
      &lt;item&gt;ChatGPT does 2.5B prompts daily this is what advertisers would class as both higher engagement and higher intent than passive scrolling (although you can fit more ads in a scroll than a chat)&lt;/item&gt;
      &lt;item&gt;Reality Check: This assumes they monetise typical search queries at rates Perplexity has already proven possible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total 2026 Revenue: ~$30-35B.&lt;/p&gt;
    &lt;head rend="h2"&gt;Projecting 2027-2029&lt;/head&gt;
    &lt;p&gt;These projections use futuresearch.ai's base forecast ($39B median for mid-2027, no ads) + advertising overlay from internal OpenAI docs + conservative user growth.&lt;/p&gt;
    &lt;p&gt;2027:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $39B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $19.8B (1.1B free users × $18 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $58.8B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2028:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $55-60B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $36-39B (1.2-1.3B free users × $30 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $91-99B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2029:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $70-80B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $70B (1.4B free users × $50 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $140-150B&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The World is Ads&lt;/head&gt;
    &lt;p&gt;Ads were the key to unlocking profitability, you must've seen it coming, thanks to you not skipping that 3 minute health insurance ad - you, yes you helped us achieve AGI!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Mission alignment: Our mission is to ensure AGI benefits all of humanity; our pursuit of advertising is always in support of that mission and making AI more accessible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The A in AGI stands for Ads! It's all ads!! Ads that you can't even block because they are BAKED into the streamed probabilistic word selector purposefully skewed to output the highest bidder's marketing copy.&lt;/p&gt;
    &lt;p&gt;Look on the bright side, if they're turning to ads it likely means AGI is not on the horizon. Your job is safe!&lt;/p&gt;
    &lt;p&gt;It's 4:41AM in London, I'm knackered. Idek if I'm gonna post this because I love AI and do agree that some things are a necessary evil to achieve a greater goal (AGI). Nevertheless, if you have any questions or comments, shout me -&amp;gt; ossamachaib.cs@gmail.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46668021</guid><pubDate>Sun, 18 Jan 2026 14:25:49 +0000</pubDate></item><item><title>Sins of the Children (Adrian Tchaikovsky)</title><link>https://asteriskmag.com/issues/07/sins-of-the-children</link><description>&lt;doc fingerprint="e67013d6963124f4"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The thing that came down right beside us was three meters high with a massive articulated body. A bug, really, Chelicer style. Eight crooked legs out from a central hub like all the mobile life here had, but most of what we’d seen was gracile, delicate, and came up to your waist. Even the Farmers — which we’d pegged as the most advanced species around — were only a meter and a half tall, and most of that was stilting limbs. This thing was not gracile. Every segment and joint of it was ridgy, armored, and spiky. It was dun and khaki like the planet’s dust, but too big to have hidden anywhere nearby, towering over the scrub. There were spread vanes like sails projecting from its back, but it couldn’t have flown under organic power. It must have weighed five tons.&lt;/p&gt;
      &lt;p&gt;We just stared. In that moment, when we could have run or called for help, we goggled at it. The stalked globes of its eyes looked back, devoid of living connection. A vast armored monster, airdropped from nowhere.&lt;/p&gt;
      &lt;p&gt;I saw the motion, off on a neighboring hillside. There was a second monster out there, surprisingly hard to spot. It hunkered down, drawing its limbs in.&lt;/p&gt;
      &lt;p&gt;Chunk! That same sound. The thing on the hillside was gone.&lt;/p&gt;
      &lt;p&gt;A second later it was on us, coming down right in front of Merrit. I thought of mechanical advantage, the tricks you could do with a rigid exoskeleton. I thought of fleas, but on an absurd macro scale. It jumped and came down on eight legs that must have been shock absorbers par excellence.&lt;/p&gt;
      &lt;p&gt;Chelicer life doesn’t quite have a front or a back, built around that hub of legs. The mouth is on the underside and that’s what this thing tilted at Merrit. &lt;/p&gt;
      &lt;p&gt;I’d dissected some of the Farmers and they had an arrangement like eight knuckly stumps to mumble over their food with. These new arrivals had a setup like a sphincter made of scissor blades and nutcrackers, more an industrial process than biology. We’d seen what those tools had done to the weather station already. Right then we were more concerned with what it did to Merrit. &lt;/p&gt;
      &lt;p&gt;He was just crouched there, midway through sifting the wreckage. The monster took instant offense. Its mouthparts extended out and just … macerated him. Chopped and crushed so that in a heartbeat there was nothing left that looked remotely human, just a wadded bloody ball of flesh and splintered bone and rags of suit. &lt;/p&gt;
      &lt;p&gt;Greffin and I started shooting. Our guns were so badly printed you could see the mold lines. They chewed up their own mass for ammo in a spray of flechettes. True to our miserly resource budget, most of that barrage just slanted off the things’ carapaces, and I knew we were both going to follow Merrit into extinction, carved up and spat out with alien contempt. Except then Greffin hit a joint, and one monster was suddenly down a leg. That, apparently, was enough. We watched them ratchet down for takeoff, still shrugging off our fire, then ping upward. I recorded the flight of one, desperately trying to keep it in my field of vision. Without that, who’d believe us? Alien mega-fleas utilizing sheer mechanical tension to jump a half kilometer at a time.&lt;/p&gt;
      &lt;p&gt;We bagged what was left of Merrit. And I grabbed the leg when the skimmer came for evac. Because it was proof that here be monsters.&lt;/p&gt;
      &lt;p&gt;The Farmers had been the tipping point, the reason to establish a human presence planetside. Yes, 14d was a unique world, unknown alien ecosphere, all that. But if it hadn’t held anything useful then the Garveneer would have focused elsewhere in the Chelicer system. And if the world had only offered mineral wealth, we’d have a robot mining operation stripping the place instead. All that unique ecosphere would have been flensed from the planet’s surface as an incidental side effect of our efforts. But on this world, the valuable thing was the biology, which needed more finesse. A human presence on the ground. Meaning a whole team of us thawed off the shelves and given this chance to justify our existence on the payroll.&lt;/p&gt;
      &lt;p&gt;Which was now under threat, as were we. We evacuated back to the farms with our grisly souvenirs.&lt;/p&gt;
      &lt;p&gt;The Concerns that have spearheaded humanity’s expansion from star to star have refined an efficient system for exploiting exoplants. When a Concern builds farms, that means a continent’s span of identical fields, robot tended. Everything growing and being harvested at an accelerated rate, processed and dried for minimal weight in transit. Turned into the Ship’s Reconstitute we’re all thoroughly sick of eating. The stuff from the Chelicer farms can look mighty good in comparison, which is a shame because a bite would kill you stone dead. But then they’re not our farms. They’re a thing the locals were doing long before we arrived.&lt;/p&gt;
      &lt;p&gt;The locals — Species 11 — are like spiders only ganglier. Four stilty legs interspersed with four spindly arms, and a hub of a body in the middle, high enough to come up to your waist. We called them Farmers from the start because it’s what they do: tend great stretches of this one crop. Not even a very exciting-looking crop, sort of a warty purple potato-looking thing, except it turns out to be superefficient at concentrating the elements in the crappy soil they’ve got here. Many of which elements are useful to us, for our superconductors and our computational substructures and all that good stuff. When we discovered that, you can be damn sure we moved in and took possession double time. Built our processing plant and started making off with a big chunk of the crop. &lt;/p&gt;
      &lt;p&gt;What did the locals think of this? My professional xenobiologist’s opinion was they didn’t think a damn thing. They didn’t react at all. The whole farming schtick they had going was just instinct, like ants, only they didn’t even defend anything. When they got in the way of the machines, they got chewed up. We thought at the time they’d evolved with no natural predators. &lt;/p&gt;
      &lt;p&gt;We sure as hell were wrong about that.&lt;/p&gt;
      &lt;p&gt;Greffin and I made our reports. The dozen on-planet crew came to commiserate, meaning get the gory details. We told everyone to carry a gun and know the emergency drill. Chelicer had an apex predator we hadn’t known about. After which cautionary tales, I was left facing up to the mission’s biggest pain in my ass, namely FenJuan.&lt;/p&gt;
      &lt;p&gt;FenJuan had screwed up royally on some past previous assignments, was my guess. They’d been something senior, and something had gone south in expensive ways. Meaning FenJuan slumming it on our team was an invisible mark against every one of us, because their personnel file came with baggage. Worse, they were my immediate colleague in biosciences, the two of us responsible for figuring out the local biochemistry.&lt;/p&gt;
      &lt;p&gt;“Stort,” they addressed me, frosty as always.&lt;/p&gt;
      &lt;p&gt;“Fen,” I replied with just as much love.&lt;/p&gt;
      &lt;p&gt;“My samples?” they said. Because they didn’t do fieldwork, just like they didn’t do basic human interaction, just sat at base camp and bitched.&lt;/p&gt;
      &lt;p&gt;And I’d given them samples previously. I’d cut a chunk out of a dozen critters on four other excursions and brought them back. And I’d just seen a work colleague turned to paste by some local monster-bug neither my nor FenJuan’s science had accounted for. But in the Concerns you don’t get time off for inefficient foibles like grief or trauma, so I made do with snarling at FenJuan that they’d had all the damn samples they were getting from me and if that wasn’t good enough then maybe they were the problem.&lt;/p&gt;
      &lt;p&gt;“When I say, ‘Get me a selection so I can run comparative studies,’” they snapped, “I do not mean just go snip bits off the Farmers and call the job done. A man is dead because we don’t understand the world here.”&lt;/p&gt;
      &lt;p&gt;Which was turning it back on me, making it my fault. And which wasn’t true to boot. I told them that if they were having difficulty distinguishing between samples maybe they didn’t have the basic analytical skills required for the task. The structures that they’d pegged as the local equivalent of a genome were probably just some essential organelle that every damn beastie possessed, and the real genome-equivalent had gone completely under FenJuan’s radar. &lt;/p&gt;
      &lt;p&gt;“You want a sample?” I asked FenJuan. “For real? Cut your own out of this. You can be absolutely sure it doesn’t come from a Farmer.” And I pointed them at the leg, the one we’d shot off the big bouncing bastard.&lt;/p&gt;
      &lt;p&gt;Shouting at people works, when you’re not allowed time off to process death. Works remarkably well, if it’s the only outlet you’ve got. Just as well. There would be plenty of both shouting and death in everyone’s future.&lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;p&gt;I liked to sit outside to complete my reports. Chelicer has good sun, if you’ve had the treatments to ward off skin damage. I wrote up my thoughts on our giant killer flea problem, watching the Farmers pick their way across the vast fields of “Species 13 Resource” as per official Concern designation, or the Chelicetato as our vulgar parlance had it. They groped over each tuber in turn, then pissed out the right chemicals to help the things grow. The Farmers were a remarkable find. We’d have gone way over budget making robot gardeners even half as efficient. Worth fighting off a few giant bugs for.&lt;/p&gt;
      &lt;p&gt;Past the processing plants, the elevator cable stretched into forever. Up there was the Garveneer, our home away from home, taking every processed tuber we could hoik out of the ground. And our little outpost here was just the beginning. There were tens of millions of Farmers all over the planet, wherever the conditions suited their crop, all ready to become part of the industrial agriculture of the Concerns. We’d struck the jackpot when we surveyed Chelicer 14d.&lt;/p&gt;
      &lt;p&gt;Greffin had been going through recent survey images, looking for monsters. She sent me what she found. Holes like burrows I could have driven a ground-car into, written off as geological because nothing we’d seen could have made them. Now we knew better. Maybe the monster fleas had emerged only recently. Maybe there was a cicada thing going on, killer flea season. We made some recommendations for the next security meeting, and I did a tour of the turret guns that had been put in with the processing plant and never needed since.&lt;/p&gt;
      &lt;p&gt;Doing that put me in FenJuan’s orbit and I braced myself for the sandpaper of their company. They were deep in analyzing the giant leg, though, or thin-sliced samples thereof. They had a few dead Farmers too — there were plenty of aimless ones not working, now we’d harvested their plots. &lt;/p&gt;
      &lt;p&gt;“Stort,” they said, not the usual bark, but thoughtful. On the screens was a variety of different views of microscopic-scale Chelicer cell structure. The spiral-walled cones that FenJuan reckoned were hereditary information, and that they’d been unspooling and trying to decode. Sections had been flagged up on each, identical one to another.&lt;/p&gt;
      &lt;p&gt;“Junk DNA,” I said, and waited for their usual invective. It didn’t come, though. FenJuan actually nodded a little, a tiny iota of acknowledgment I’d said something that wasn’t stupid. And Earth life accumulates a certain amount of genetic junk, right? Stuff in the genome that’s been switched off, acquired from bacteria, or from benign transcription errors carried on down through the generations. But FenJuan reckoned something like 90 percent of any given beastie’s hereditary was this unused junk. &lt;/p&gt;
      &lt;p&gt;I wanted to say they were imagining things. I wanted to say it was a crap planet with crap aliens who had crap hereditary code, and us coming along to exploit them was the best thing that could have happened. That was how my encounters with FenJuan generally went. It was basically entertainment for the rest of the team.&lt;/p&gt;
      &lt;p&gt;I didn’t say any of that. FenJuan and I looked at each other, not quite ready to bury the hatchet, but maybe agreeing there was a bigger problem out there to save that mutual hatchet for.&lt;/p&gt;
      &lt;p&gt;The attack came the next day, and we weren’t prepared.&lt;/p&gt;
      &lt;p&gt;I heard the sound, distant, echoing across flat farmland from the dry hills. Chunk. For two whole seconds I was thinking some piece of machinery had gone wrong and how that was someone else’s problem. And then the first of them came down, just like before. Crashing onto the roof of the processing plant hard enough to buckle the plastic composite. Leering over the edge like a gargoyle. I swear it was twice the size of the one that killed Merrit. &lt;/p&gt;
      &lt;p&gt;I was shouting. Most of us were shouting, but I still caught a rapid heavy drumroll underneath the human noise. Chunkchunkchunkchunkchunkchunkchunk…&lt;/p&gt;
      &lt;p&gt;They started dropping down all round us. We were running for the plant, because it was the most reinforced building and that was the emergency drill. Someone got word to the guns that their services were needed, and they started running friend-or-foe algorithms as a dozen human beings fled frantically into their arcs of fire. &lt;/p&gt;
      &lt;p&gt;One of the death-fleas crashed down in front of me, outspread sails barely slowing it. The articulation of its legs popped and twisted, absorbing the force of impact. A gun hammered chips out of its carapace. It lunged forward and snipped someone — one of the resources team I think — right in half with its scissor-blade face. I screamed and just about ducked through the shadow of its wings, not knowing if I’d get killed by its jaws or our own turrets.&lt;/p&gt;
      &lt;p&gt;Most of us got inside. They didn’t break in after us, but only because they didn’t try. Maybe object permanence isn’t a big thing on Chelicer: Once we were out of sight they seemed to forget us, through they chewed up all the guns.&lt;/p&gt;
      &lt;p&gt;Through our cameras, we got to see all the rest of what they did.&lt;/p&gt;
      &lt;p&gt;The Farmers, it turned out, had natural predators. Or they did in death-flea season. The monsters went to town, mostly on the Farmers that didn’t have anything left to farm, because they were just milling about. It was a massacre. And though they were weird alien spider guys, and you can’t really anthropomorphize that, we were all surprisingly cut up. It wasn’t that they were getting slaughtered out there. It was that they were ours. Our livelihood, our profit, the injection of resources that was earning us our wage-worth.&lt;/p&gt;
      &lt;p&gt;The massacre was monopolizing our attention, so the real damage went almost unnoticed until the earthquakelike convulsion that cracked every wall and trashed the processor floor. For a moment the problem was so big I couldn’t work out what had happened.&lt;/p&gt;
      &lt;p&gt;The elevator cable. Something about it — maybe just that it was the biggest thing around — had drawn their ire. A half dozen of the bastards had jumped to it, and those mouthparts had sawn through the supertensile material like it was string.&lt;/p&gt;
      &lt;p&gt;That took a long while to clear up. The actual cable was, after all, a long weighted strand that stretched a good way out of atmosphere and into space, and our actual ship was tethered at the halfway point. The Garveneer decoupled sharpish, you can be sure, and the vast length of the cable, cut free at its anchor, just vanished upward and sideways like the blade of God’s own scythe, on its way toward the outer reaches of the system. &lt;/p&gt;
      &lt;p&gt;We were stuck on-planet for some time, and we’d just had it demonstrated to us that the death-fleas were more than capable of carving their way into our compromised fortress if they wanted. Yes, our lords and masters in the Concern could shuttle us back to orbit, but that would require circumstances to fall into a very narrow gap indeed. That (1) it wasn’t worth continuing work on Chelicer 14d, and (2) it was actually worth retrieving us, rather than writing us off. &lt;/p&gt;
      &lt;p&gt;You can imagine the mood on the ground as we waited for their decision. We all gathered in the surviving common space and tried to convince ourselves we weren’t screwed. All except FenJuan, who didn’t do social graces, but just kept on studying the samples, which our remaining instruments couldn’t tell apart.&lt;/p&gt;
      &lt;p&gt;In the end, after they’d left us hanging for five days, there was a meeting. A handful of us on a staticky link to the chief director safe aboard the Garveneer. We were ready to be bawled out for a colossal loss of resources. That was the very best we thought we’d get. Instead, though, the Great Man was onside. The harvest from Chelicer had been very good indeed, solving a variety of rare elements shortages none of us knew the Concern had. This world we had worked on was the new hope of further human expansion. If only we could solve our little pest problem.&lt;/p&gt;
      &lt;p&gt;“We need to keep you folks safe,” said the director heartily. I looked over the recommendations. What they actually wanted to keep safe was the harvest, of course, which meant the Farmers. By then we had images from all over the planet of sporadic attacks on Farmer colonies. Death-fleas picking off the weak. Nothing as sustained as we’d seen at our base camp, but plainly a part of the circle of life in these parts.&lt;/p&gt;
      &lt;p&gt;“Our engineers up here are working on a new cable,” the Great Man told us. “But drones, too. Hunter drones. A whole fleet of them. We can justify the cost, given the potential resource revenue you’ve demonstrated. We’re proposing a global initiative to wipe out these things.”&lt;/p&gt;
      &lt;p&gt;“Wipe out the species, Director?” FenJuan clarified. &lt;/p&gt;
      &lt;p&gt;“Given the losses we’ve sustained and the clear threat to productivity, it’s the leading proposal. But I’m here for your thoughts.” That cheery smile of his. “Stort?”&lt;/p&gt;
      &lt;p&gt;“We’re obviously still adjusting our picture of the ecosphere to incorporate these things,” I said. “Given the low species count on-world, having an apex predator that only emerges sporadically makes some sense. What happens if we remove it? We can’t know. If this was a matter of wanting to preserve a working natural ecosystem I’d say there would be too many potential imbalances generated by cropping the top of the food chain. But.”&lt;/p&gt;
      &lt;p&gt;“But,” the director agreed. Because we were not, after all, interested in preserving the ecosystem. Just that part of it that worked for us.&lt;/p&gt;
      &lt;p&gt;FenJuan’s eyes were boring into me; I didn’t meet them. “Historically,” I said, “in a managed agricultural paradigm, removal of the top predators has been accomplished very profitably. Wolves, sheep, so on. It’s not as though we’re going to have a problem with some Farmer population explosion. If some other species booms, we can manage the consequences. I say do it.”&lt;/p&gt;
      &lt;p&gt;“Director,” FenJuan put in, unasked. “I have yet to come to any understanding of the biology or relationships involved here. There’s a commonality between species I can’t account for. This world plainly went through some severe ecological crisis that left a depauperate web of interdependence. We don’t know—”&lt;/p&gt;
      &lt;p&gt;On our screens, the director settled back in his big chair. “We know all we need to. What this world could be worth to us. How much damage those beasts are capable of doing. An elevator cable! We’ll conduct a localized culling in your region first. Barring any obvious consequence, we can roll it out to the rest of the world and follow up with plant and personnel wherever these Farmer creatures are to be found.” His smile was genuinely pleased, a man who’s going to see a nice bonus. “Well done, all. I know it’s been tough, but you’re heroes.”&lt;/p&gt;
      &lt;p&gt;The local cull, when it first happened, was something to watch. Drone footage wheeling and spinning as our machines found and chased the fleas. Killed them as they leapt through the air, as they landed thunderously on the ground, as they emerged from their burrows. Wiping them out within 200 klicks of the processing plant. &lt;/p&gt;
      &lt;p&gt;And nothing broke. The Farmers kept on farming. The crops grew. The crops that, at a cellular level, seemed weirdly indistinguishable from the things that tended them. FenJuan was raising issues every day, by then. Desperate to communicate how weird their results were. Not doing their job, because their job was solely and specifically to identify aspects of the local biochemistry that could be profitably exploited. Instead of which, they were going nuts about how every critter just seemed to have this enormous bolus of unused genetic-equivalent information, with a huge overlap between species. And I think they’d just about worked it out, except by then they’d made such a nuisance of themselves that FenJuan was the very last person our bosses up in orbit wanted to hear from. Did this discovery open up new vistas of planetary exploitation for our already profitable operation? No? Then pipe down and stop using up comms resources.&lt;/p&gt;
      &lt;p&gt;The people the director did want to hear from were designing and deploying the hunter-killers. Our expanded drone fleet was greenlit: hundreds of machines shipped downwell and let loose across the globe. Wherever they found the fleas, they destroyed them. We felt we were liberators. Whole populations of Farmers could live without those monstrous shadows falling on them. Yes, we were making a species extinct, but it wasn’t a nice species. We were already on the next phase of occupation, a 10-year building plan where we’d fill the planet with farms and processing plants, replicating our first outpost over and over until there wasn’t an inch of the world that wasn’t working for us.&lt;/p&gt;
      &lt;p&gt;A couple of years into our agricultural expansion, the cacti disappeared. Not cacti, obviously. Species 43 in the Concern bestiary, but cactus enough that the name had stuck. We had a look one morning and there just wasn’t any more of it left. I suggested maybe it had been living off some sort of death-flea by-products, though the timing seemed unusually lethargic for that kind of interaction. I ended up working alongside FenJuan, and we found drone footage of the cacti stuff getting up and running around, so that Species 43 turned out to be the larval-or-something form of Species 22, and we had to recalibrate the records. &lt;/p&gt;
      &lt;p&gt;At around the same time, the little hairy critters that were Species 38 rooted down and grew long spires with puffballs on them, making them actually Species 17. Half a year later our existing Species 11s lost their poles and became another sort of thing we’d already seen, and so on and so on. To most of us it was a curiosity. To FenJuan it was a crawling horror that I was starting to share. All their snapping, bitching at me for not seeing, and I’d just written it all off as someone pissed their Concern work record was full of demerits. Except they’d been right and I’d been wrong.&lt;/p&gt;
      &lt;p&gt;There were no more cacti. That was what scared FenJuan. We watched a wave of transformations. Each form turned into something else, but none of it turned into the cactuslike Species 43. Then it was something else, where our current batch just metamorphosed and there were no new ones. None at all, anywhere on Chelicer. The dry country became less and less inhabited as species after species vanished away.&lt;/p&gt;
      &lt;p&gt;Or not species. That was what FenJuan had been trying to understand. Developmental stages. Not a circle of life, but a life cycle.&lt;/p&gt;
      &lt;p&gt;Our prized cheliceratos, which had been putting out runners and new tubers happily for over a decade, were suddenly ambulatory one morning, sprouting a thicket of spindly legs and just giving up their life of being agricultural produce. That got people’s attention. Around the same time one weird round critter rooted down where the Farmers were and became the new Chelicetato crop, and the dumbest of our colleagues reckoned that was all OK then. FenJuan and I had stopped trying to raise the alarm, by then, because it obviously wasn’t going to help. Soon after, some buried fungal-looking thing we’d found no use for sprouted legs and became new Farmers. And the old farmers … died off. Wore out, natural causes. Leaving only the least dregs we’d left of their crop. From which a handful of stunted things crawled, devouring their own left-behind husks and the last corpses of their tenders. They were tiny, but we recognized them even as they began to wearily dig down into the parched, lifeless soil. Nascent fleas, entering that dormant part of their cycle from which they would emerge, at some future date, into a world devoid of anything that could sustain them. Behind them, the whole ecosystem of life stages had been rolled up. There was nothing left of it. They were the last.&lt;/p&gt;
      &lt;p&gt; As we had harvested and plundered, we had been watching a decade-long series of transformations. One that had definitively ended. Life on Chelicer vanished. Plant forms, bug forms, just about every macrobiological creature dying off one at a time and not being replaced by a new generation. As though death had asked them to form an orderly queue. &lt;/p&gt;
      &lt;p&gt;There had been a mass extinction in Chelicer’s past, FenJuan and I reckoned. Something that had killed off everything except a hardy species that inherited an utterly impoverished planetary biome. Colder at the poles, warmer at the equator, but barren, desperate. So, over the ages, that species had developed to exploit every last opportunity that the world had left to it, not through speciation but through adaption of its life cycle. Gathering the meager resources of the world, concentrating them in living forms that could be harvested in turn. Sedentary stages, mobile stages, squeezing every possible niche of everything that could be gained and then transforming into the next phase of its long and complex chain of shapes. A desperate ecosystem of one, harvesting and gathering and recycling, each stage into the next, surviving everything thrown at it. Except us, who came and severed a single link utterly and irrevocably. Cut one thread and watched the whole unravel over a mere decade.&lt;/p&gt;
      &lt;p&gt;FenJuan and I were last off the planet, on the final elevator car along with the last salvage from our farming operations. It was on us, we had been told. We were the biologists, and we should have seen it coming. And they were right; we should. But all that would have done was salve our professional pride. I don’t believe for a moment they’d have listened to us if we’d said Stop. Stop isn’t the way of the Concerns. Stop doesn’t meet quotas or hit targets.&lt;/p&gt;
      &lt;p&gt;We stepped into the elevator car, FenJuan and I. We looked back over a world unrelieved by messy, complicated stuff, such as life. A failed commercial opportunity, as the report would say. &lt;/p&gt;
      &lt;p&gt;I wanted to say something. Possibly You were right. But what good would it do? We were both going to be back on ice when we reached the ship, with personnel files so dire they’ll probably never thaw us out again. But, like the life of Chelicer, we’re not important, compared to the bigger picture of the Concerns and their expansion. We humans go on, world to world, star to star, making the universe our own. But on Chelicer there will only ever be dust.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46669663</guid><pubDate>Sun, 18 Jan 2026 17:08:58 +0000</pubDate></item><item><title>Gaussian Splatting – A$AP Rocky "Helicopter" music video</title><link>https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting</link><description>&lt;doc fingerprint="2ff1eb95032d70b0"&gt;
  &lt;main&gt;
    &lt;p&gt;Michael Rubloff&lt;/p&gt;
    &lt;p&gt;Jan 13, 2026&lt;/p&gt;
    &lt;p&gt;Believe it or not, A$AP Rocky is a huge fan of radiance fields.&lt;/p&gt;
    &lt;p&gt;Yesterday, when A$AP Rocky released the music video for Helicopter, many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. Whatâs easier to miss, unless you know what youâre looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats.&lt;/p&gt;
    &lt;p&gt;I spoke with Evercoast, the team responsible for capturing the performances, as well as Chris Rutledge, the projectâs CG Supervisor at Grin Machine, and Wilfred Driscoll of WildCapture and FitsÅ«.ai, to understand how Helicopter came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date.&lt;/p&gt;
    &lt;p&gt;The decision to shoot Helicopter volumetrically wasnât driven by technology for technologyâs sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines.&lt;/p&gt;
    &lt;p&gt;Chris told me heâd been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply werenât possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a âsomedayâ workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on.&lt;/p&gt;
    &lt;p&gt;The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D.&lt;/p&gt;
    &lt;p&gt;Almost every human figure in the video, including Rocky himself, was recorded volumetrically using Evercoastâs system. Itâs all real performance, preserved spatially.&lt;/p&gt;
    &lt;p&gt;This is not the first time that A$AP Rocky has featured a radiance field in one of his music videos. The 2023 music video for Shittinâ Me featured several NeRFs and even the GUI for Instant-NGP, which you can spot throughout the piece.&lt;/p&gt;
    &lt;p&gt;The primary shoot for Helicopter took place in August in Los Angeles. Evercoast deployed a 56 camera RGB-D array, synchronized across two Dell workstations. Performers were suspended from wires, hanging upside down, doing pull-ups on ceiling-mounted bars, swinging props, and performing stunts, all inside the capture volume.&lt;/p&gt;
    &lt;p&gt;Scenes that appear surreal in the final video were, in reality, grounded in very physical setups, such as wooden planks standing in for helicopter blades, real wire rigs, and real props. The volumetric data allowed those elements to be removed, recomposed, or entirely recontextualized later without losing the authenticity of the human motion.&lt;/p&gt;
    &lt;p&gt;Over the course of the shoot, Evercoast recorded more than 10 terabytes of raw data, ultimately rendering roughly 30 minutes of final splatted footage, exported as PLY sequences totaling around one terabyte.&lt;/p&gt;
    &lt;p&gt;That data was then brought into Houdini, where the post production team used CG Nomads GSOPs for manipulation and sequencing, and OTOYâs OctaneRender for final rendering. Thanks to this combination, the production team was also able to relight the splats.&lt;/p&gt;
    &lt;p&gt;One of the more powerful aspects of the workflow was Evercoastâs ability to preview volumetric captures at multiple stages. The director could see live spatial feedback on set, generate quick mesh based previews seconds after a take, and later review fully rendered splats through Evercoastâs web player before downloading massive PLY sequences for Houdini.&lt;/p&gt;
    &lt;p&gt;In practice, this meant creative decisions could be made rapidly and cheaply, without committing to heavy downstream processing until the team knew exactly what they wanted. Itâs a workflow that more closely resembles simulation than traditional filming.&lt;/p&gt;
    &lt;p&gt;Chris also discovered that Octaneâs Houdini integration had matured, and that Octaneâs early splat support was far enough along to enable relighting. According to the team, the ability to relight splats, introduce shadowing, and achieve a more dimensional â3D videoâ look was a major reason the final aesthetic lands the way it does.&lt;/p&gt;
    &lt;p&gt;The team also used Blender heavily for layout and previs, converting splat sequences into lightweight proxy caches for scene planning. Wilfred described how WildCaptureâs internal tooling was used selectively to introduce temporal consistency. In his words, the team derived primitive pose estimation skeletons that could be used to transfer motion, support collision setups, and allow Houdiniâs simulation toolset to handle rigid body, soft body, and more physically grounded interactions.&lt;/p&gt;
    &lt;p&gt;One recurring reaction to the video has been confusion. Viewers assume the imagery is AI-generated. According to Evercoast, that couldnât be further from the truth. Every stunt, every swing, every fall was physically performed and captured in real space. What makes it feel synthetic is the freedom volumetric capture affords. You arenât limited by the cameraâs composition. You have free rein to explore, reposition cameras after the fact, break spatial continuity, and recombine performances in ways that 2D simply canât.&lt;/p&gt;
    &lt;p&gt;In other words, radiance field technology isnât replacing reality. Itâs preserving everything.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46670024</guid><pubDate>Sun, 18 Jan 2026 17:40:55 +0000</pubDate></item><item><title>Show HN: Lume 0.2 – Build and Run macOS VMs with unattended setup</title><link>https://cua.ai/docs/lume/guide/getting-started/introduction</link><description>&lt;doc fingerprint="25b849d06d0791ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What is Lume?&lt;/head&gt;
    &lt;p&gt;Introduction to Lume - the macOS VM CLI and framework&lt;/p&gt;
    &lt;p&gt;Lume is a VM runtime for building AI agents, running CI/CD pipelines, and automating macOS. It uses Apple's native Virtualization Framework to run macOS and Linux VMs at near-native speed on Apple Silicon.&lt;/p&gt;
    &lt;p&gt;MIT License&lt;/p&gt;
    &lt;p&gt;Lume is open-source and MIT licensed. If you find it useful, we'd appreciate a star on GitHub!&lt;/p&gt;
    &lt;p&gt;Cloud macOS Sandboxes&lt;/p&gt;
    &lt;p&gt;We're piloting a managed service for customers who want to run cloud macOS sandboxes for CI/CD and agent workloads. Book a demo if you're interested.&lt;/p&gt;
    &lt;p&gt;A single binary with an HTTP API. Create a VM, run it headlessly, control it programmatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;You can use Lume directly via CLI, or run &lt;code&gt;lume serve&lt;/code&gt; to expose an HTTP API for programmatic access. The Computer SDK uses this API to automate macOS interactions.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Lume is a thin layer over Apple's Virtualization Framework, which provides hardware-accelerated virtualization on Apple Silicon. This gives you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native speed — CPU instructions execute directly via hardware virtualization&lt;/item&gt;
      &lt;item&gt;Paravirtualized graphics — Basic GPU support via Apple's virtualization layer (limited to GPU Family 5)&lt;/item&gt;
      &lt;item&gt;Efficient storage — Sparse disk files only consume actual usage, not allocated size&lt;/item&gt;
      &lt;item&gt;Rosetta 2 support — Run x86 Linux binaries in ARM Linux VMs&lt;/item&gt;
      &lt;item&gt;Automated golden images — Go from IPSW to fully configured macOS VM without manual intervention&lt;/item&gt;
      &lt;item&gt;Registry support — Pull and push VM images from GHCR or GCS registries&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;When to use Lume&lt;/head&gt;
    &lt;p&gt;Testing across macOS versions — Spin up a VM with a specific macOS version, test your software, tear it down. No need to maintain multiple physical machines.&lt;/p&gt;
    &lt;p&gt;Automating macOS tasks — Combine Lume with Unattended Setup to create pre-configured VMs. The setup automation uses VNC and OCR to click through the Setup Assistant without manual intervention.&lt;/p&gt;
    &lt;p&gt;Running CI/CD locally — Test your macOS builds in isolated VMs before pushing to remote CI. The &lt;code&gt;--no-display&lt;/code&gt; flag runs VMs headlessly.&lt;/p&gt;
    &lt;p&gt;Sandboxing risky operations — Need to test untrusted software or destructive scripts? Run them in a VM, then delete it. Clone a known-good VM to reset to a clean state instantly.&lt;/p&gt;
    &lt;p&gt;Building AI agents — Lume powers the Cua Computer SDK, providing VMs that AI models can interact with through screenshots and input simulation.&lt;/p&gt;
    &lt;p&gt;Used by Anthropic&lt;/p&gt;
    &lt;p&gt;Apple's Virtualization Framework—the same technology Lume is built on—powers Claude Cowork, Anthropic's sandboxed environment for Claude Code. It downloads a Linux root filesystem and boots it in an isolated VM where Claude can safely execute commands without access to your broader system.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Lume doesn't do&lt;/head&gt;
    &lt;p&gt;Lume requires Apple Silicon—it won't work on Intel Macs or other platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started&lt;/head&gt;
    &lt;p&gt;Ready to try it? Install Lume and create your first VM in the Quickstart.&lt;/p&gt;
    &lt;p&gt;Was this page helpful?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46670181</guid><pubDate>Sun, 18 Jan 2026 17:53:21 +0000</pubDate></item><item><title>Flux 2 Klein pure C inference</title><link>https://github.com/antirez/flux2.c</link><description>&lt;doc fingerprint="2e5c8b38193e188c"&gt;
  &lt;main&gt;
    &lt;p&gt;This program generates images from text prompts (and optionally from other images) using the FLUX.2-klein-4B model from Black Forest Labs. It can be used as a library as well, and is implemented entirely in C, with zero external dependencies beyond the C standard library. MPS and BLAS acceleration are optional but recommended.&lt;/p&gt;
    &lt;p&gt;I (the human here, Salvatore) wanted to test code generation with a more ambitious task, over the weekend. This is the result. It is my first open source project where I wrote zero lines of code. I believe that inference systems not using the Python stack (which I do not appreciate) are a way to free open models usage and make AI more accessible. There is already a project doing the inference of diffusion models in C / C++ that supports multiple models, and is based on GGML. I wanted to see if, with the assistance of modern AI, I could reproduce this work in a more concise way, from scratch, in a weekend. Looks like it is possible.&lt;/p&gt;
    &lt;p&gt;This code base was written with Claude Code, using the Claude Max plan, the small one of ~80 euros per month. I almost reached the limits but this plan was definitely sufficient for such a large task, which was surprising. In order to simplify the usage of this software, no quantization is used, nor do you need to convert the model. It runs directly with the safetensors model as input, using floats.&lt;/p&gt;
    &lt;p&gt;Even if the code was generated using AI, my help in steering towards the right design, implementation choices, and correctness has been vital during the development. I learned quite a few things about working with non trivial projects and AI.&lt;/p&gt;
    &lt;code&gt;# Build (choose your backend)
make mps       # Apple Silicon (fastest)
# or: make blas    # Intel Mac / Linux with OpenBLAS
# or: make generic # Pure C, no dependencies

# Download the model (~16GB)
pip install huggingface_hub
python download_model.py

# Generate an image
./flux -d flux-klein-model -p "A woman wearing sunglasses" -o output.png&lt;/code&gt;
    &lt;p&gt;That's it. No Python runtime, no PyTorch, no CUDA toolkit required at inference time.&lt;/p&gt;
    &lt;p&gt;Generated with: &lt;code&gt;./flux -d flux-klein-model -p "A picture of a woman in 1960 America. Sunglasses. ASA 400 film. Black and White." -W 250 -H 250 -o /tmp/woman.png&lt;/code&gt;, and later processed with image to image generation via &lt;code&gt;./flux -d flux-klein-model -i /tmp/woman.png -o /tmp/woman2.png -p "oil painting of woman with sunglasses" -v -H 256 -W 256&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero dependencies: Pure C implementation, works standalone. BLAS optional for ~30x speedup (Apple Accelerate on macOS, OpenBLAS on Linux)&lt;/item&gt;
      &lt;item&gt;Metal GPU acceleration: Automatic on Apple Silicon Macs&lt;/item&gt;
      &lt;item&gt;Text-to-image: Generate images from text prompts&lt;/item&gt;
      &lt;item&gt;Image-to-image: Transform existing images guided by prompts&lt;/item&gt;
      &lt;item&gt;Integrated text encoder: Qwen3-4B encoder built-in, no external embedding computation needed&lt;/item&gt;
      &lt;item&gt;Memory efficient: Automatic encoder release after encoding (~8GB freed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./flux -d flux-klein-model -p "A fluffy orange cat sitting on a windowsill" -o cat.png&lt;/code&gt;
    &lt;p&gt;Transform an existing image based on a prompt:&lt;/p&gt;
    &lt;code&gt;./flux -d flux-klein-model -p "oil painting style" -i photo.png -o painting.png -t 0.7&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;-t&lt;/code&gt; (strength) parameter controls how much the image changes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.0&lt;/code&gt;= no change (output equals input)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1.0&lt;/code&gt;= full generation (input only provides composition hint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.7&lt;/code&gt;= good balance for style transfer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required:&lt;/p&gt;
    &lt;code&gt;-d, --dir PATH        Path to model directory
-p, --prompt TEXT     Text prompt for generation
-o, --output PATH     Output image path (.png or .ppm)
&lt;/code&gt;
    &lt;p&gt;Generation options:&lt;/p&gt;
    &lt;code&gt;-W, --width N         Output width in pixels (default: 256)
-H, --height N        Output height in pixels (default: 256)
-s, --steps N         Sampling steps (default: 4)
-S, --seed N          Random seed for reproducibility
&lt;/code&gt;
    &lt;p&gt;Image-to-image options:&lt;/p&gt;
    &lt;code&gt;-i, --input PATH      Input image for img2img
-t, --strength N      How much to change the image, 0.0-1.0 (default: 0.75)
&lt;/code&gt;
    &lt;p&gt;Output options:&lt;/p&gt;
    &lt;code&gt;-q, --quiet           Silent mode, no output
-v, --verbose         Show detailed config and timing info
&lt;/code&gt;
    &lt;p&gt;Other options:&lt;/p&gt;
    &lt;code&gt;-e, --embeddings PATH Load pre-computed text embeddings (advanced)
-h, --help            Show help
&lt;/code&gt;
    &lt;p&gt;The seed is always printed to stderr, even when random:&lt;/p&gt;
    &lt;code&gt;$ ./flux -d flux-klein-model -p "a landscape" -o out.png
Seed: 1705612345
out.png
&lt;/code&gt;
    &lt;p&gt;To reproduce the same image, use the printed seed:&lt;/p&gt;
    &lt;code&gt;$ ./flux -d flux-klein-model -p "a landscape" -o out.png -S 1705612345
&lt;/code&gt;
    &lt;p&gt;Choose a backend when building:&lt;/p&gt;
    &lt;code&gt;make            # Show available backends
make generic    # Pure C, no dependencies (slow)
make blas       # BLAS acceleration (~30x faster)
make mps        # Apple Silicon Metal GPU (fastest, macOS only)&lt;/code&gt;
    &lt;p&gt;Recommended:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS Apple Silicon: &lt;code&gt;make mps&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;macOS Intel: &lt;code&gt;make blas&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux with OpenBLAS: &lt;code&gt;make blas&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux without OpenBLAS: &lt;code&gt;make generic&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For &lt;code&gt;make blas&lt;/code&gt; on Linux, install OpenBLAS first:&lt;/p&gt;
    &lt;code&gt;# Ubuntu/Debian
sudo apt install libopenblas-dev

# Fedora
sudo dnf install openblas-devel&lt;/code&gt;
    &lt;p&gt;Other targets:&lt;/p&gt;
    &lt;code&gt;make clean      # Clean build artifacts
make info       # Show available backends for this platform
make test       # Run reference image test&lt;/code&gt;
    &lt;p&gt;The model weights are downloaded from HuggingFace:&lt;/p&gt;
    &lt;code&gt;pip install huggingface_hub
python download_model.py&lt;/code&gt;
    &lt;p&gt;This downloads approximately 16GB to &lt;code&gt;./flux-klein-model&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VAE (~300MB)&lt;/item&gt;
      &lt;item&gt;Transformer (~4GB)&lt;/item&gt;
      &lt;item&gt;Qwen3-4B Text Encoder (~8GB)&lt;/item&gt;
      &lt;item&gt;Tokenizer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FLUX.2-klein-4B is a rectified flow transformer optimized for fast inference:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Architecture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Transformer&lt;/cell&gt;
        &lt;cell&gt;5 double blocks + 20 single blocks, 3072 hidden dim, 24 attention heads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;VAE&lt;/cell&gt;
        &lt;cell&gt;AutoencoderKL, 128 latent channels, 8x spatial compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Text Encoder&lt;/cell&gt;
        &lt;cell&gt;Qwen3-4B, 36 layers, 2560 hidden dim&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Inference steps: This is a distilled model that produces good results with exactly 4 sampling steps.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Phase&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text encoding&lt;/cell&gt;
        &lt;cell&gt;~8GB (encoder weights)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Diffusion&lt;/cell&gt;
        &lt;cell&gt;~8GB (transformer ~4GB + VAE ~300MB + activations)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Peak&lt;/cell&gt;
        &lt;cell&gt;~16GB (if encoder not released)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The text encoder is automatically released after encoding, reducing peak memory during diffusion. If you generate multiple images with different prompts, the encoder reloads automatically.&lt;/p&gt;
    &lt;p&gt;Benchmarks on Apple M3 Max (128GB RAM), generating a 4-step image:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;C (MPS)&lt;/cell&gt;
        &lt;cell role="head"&gt;C (BLAS)&lt;/cell&gt;
        &lt;cell role="head"&gt;C (Generic)&lt;/cell&gt;
        &lt;cell role="head"&gt;PyTorch (MPS)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;512x512&lt;/cell&gt;
        &lt;cell&gt;49.6s&lt;/cell&gt;
        &lt;cell&gt;51.9s&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;5.4s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;256x256&lt;/cell&gt;
        &lt;cell&gt;32.4s&lt;/cell&gt;
        &lt;cell&gt;29.7s&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;3.0s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;64x64&lt;/cell&gt;
        &lt;cell&gt;25.0s&lt;/cell&gt;
        &lt;cell&gt;23.5s&lt;/cell&gt;
        &lt;cell&gt;605.6s&lt;/cell&gt;
        &lt;cell&gt;2.2s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The C implementation uses float32 throughout, while PyTorch uses bfloat16 with highly optimized MPS kernels. The next step of this project is likely to implement such an optimization, in order to reach similar speed, or at least try to approach it.&lt;/item&gt;
      &lt;item&gt;The generic (pure C) backend is extremely slow and only practical for testing at small sizes.&lt;/item&gt;
      &lt;item&gt;Times include text encoding, denoising (4 steps), and VAE decode.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Maximum resolution: 1024x1024 pixels. Higher resolutions require prohibitive memory for the attention mechanisms.&lt;/p&gt;
    &lt;p&gt;Minimum resolution: 64x64 pixels.&lt;/p&gt;
    &lt;p&gt;Dimensions should be multiples of 16 (the VAE downsampling factor).&lt;/p&gt;
    &lt;p&gt;The library can be integrated into your own C/C++ projects. Link against &lt;code&gt;libflux.a&lt;/code&gt; and include &lt;code&gt;flux.h&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here's a complete program that generates an image from a text prompt:&lt;/p&gt;
    &lt;code&gt;#include "flux.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    /* Load the model. This loads VAE, transformer, and text encoder. */
    flux_ctx *ctx = flux_load_dir("flux-klein-model");
    if (!ctx) {
        fprintf(stderr, "Failed to load model: %s\n", flux_get_error());
        return 1;
    }

    /* Configure generation parameters. Start with defaults and customize. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.width = 512;
    params.height = 512;
    params.seed = 42;  /* Use -1 for random seed */

    /* Generate the image. This handles text encoding, diffusion, and VAE decode. */
    flux_image *img = flux_generate(ctx, "A fluffy orange cat in a sunbeam", &amp;amp;params);
    if (!img) {
        fprintf(stderr, "Generation failed: %s\n", flux_get_error());
        flux_free(ctx);
        return 1;
    }

    /* Save to file. Format is determined by extension (.png or .ppm). */
    flux_image_save(img, "cat.png");
    printf("Saved cat.png (%dx%d)\n", img-&amp;gt;width, img-&amp;gt;height);

    /* Clean up */
    flux_image_free(img);
    flux_free(ctx);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Compile with:&lt;/p&gt;
    &lt;code&gt;gcc -o myapp myapp.c -L. -lflux -lm -framework Accelerate  # macOS
gcc -o myapp myapp.c -L. -lflux -lm -lopenblas              # Linux&lt;/code&gt;
    &lt;p&gt;Transform an existing image guided by a text prompt. The &lt;code&gt;strength&lt;/code&gt; parameter controls how much the image changes:&lt;/p&gt;
    &lt;code&gt;#include "flux.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    flux_ctx *ctx = flux_load_dir("flux-klein-model");
    if (!ctx) return 1;

    /* Load the input image */
    flux_image *photo = flux_image_load("photo.png");
    if (!photo) {
        fprintf(stderr, "Failed to load image\n");
        flux_free(ctx);
        return 1;
    }

    /* Set up parameters. Output size defaults to input size. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.strength = 0.7;  /* 0.0 = no change, 1.0 = full regeneration */
    params.seed = 123;

    /* Transform the image */
    flux_image *painting = flux_img2img(ctx, "oil painting, impressionist style",
                                         photo, &amp;amp;params);
    flux_image_free(photo);  /* Done with input */

    if (!painting) {
        fprintf(stderr, "Transformation failed: %s\n", flux_get_error());
        flux_free(ctx);
        return 1;
    }

    flux_image_save(painting, "painting.png");
    printf("Saved painting.png\n");

    flux_image_free(painting);
    flux_free(ctx);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Strength values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.3&lt;/code&gt;- Subtle style transfer, preserves most details&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.5&lt;/code&gt;- Moderate transformation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.7&lt;/code&gt;- Strong transformation, good for style transfer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.9&lt;/code&gt;- Almost complete regeneration, keeps only composition&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When generating multiple images with different seeds but the same prompt, you can avoid reloading the text encoder:&lt;/p&gt;
    &lt;code&gt;flux_ctx *ctx = flux_load_dir("flux-klein-model");
flux_params params = FLUX_PARAMS_DEFAULT;
params.width = 256;
params.height = 256;

/* Generate 5 variations with different seeds */
for (int i = 0; i &amp;lt; 5; i++) {
    flux_set_seed(1000 + i);

    flux_image *img = flux_generate(ctx, "A mountain landscape at sunset", &amp;amp;params);

    char filename[64];
    snprintf(filename, sizeof(filename), "landscape_%d.png", i);
    flux_image_save(img, filename);
    flux_image_free(img);
}

flux_free(ctx);&lt;/code&gt;
    &lt;p&gt;Note: The text encoder (~8GB) is automatically released after the first generation to save memory. It reloads automatically if you use a different prompt.&lt;/p&gt;
    &lt;p&gt;All functions that can fail return NULL on error. Use &lt;code&gt;flux_get_error()&lt;/code&gt; to get a description:&lt;/p&gt;
    &lt;code&gt;flux_ctx *ctx = flux_load_dir("nonexistent-model");
if (!ctx) {
    fprintf(stderr, "Error: %s\n", flux_get_error());
    /* Prints something like: "Failed to load VAE - cannot generate images" */
    return 1;
}&lt;/code&gt;
    &lt;p&gt;Core functions:&lt;/p&gt;
    &lt;code&gt;flux_ctx *flux_load_dir(const char *model_dir);   /* Load model, returns NULL on error */
void flux_free(flux_ctx *ctx);                     /* Free all resources */

flux_image *flux_generate(flux_ctx *ctx, const char *prompt, const flux_params *params);
flux_image *flux_img2img(flux_ctx *ctx, const char *prompt, const flux_image *input,
                          const flux_params *params);&lt;/code&gt;
    &lt;p&gt;Image handling:&lt;/p&gt;
    &lt;code&gt;flux_image *flux_image_load(const char *path);     /* Load PNG or PPM */
int flux_image_save(const flux_image *img, const char *path);  /* 0=success, -1=error */
flux_image *flux_image_resize(const flux_image *img, int new_w, int new_h);
void flux_image_free(flux_image *img);&lt;/code&gt;
    &lt;p&gt;Utilities:&lt;/p&gt;
    &lt;code&gt;void flux_set_seed(int64_t seed);                  /* Set RNG seed for reproducibility */
const char *flux_get_error(void);                  /* Get last error message */
void flux_release_text_encoder(flux_ctx *ctx);     /* Manually free ~8GB (optional) */&lt;/code&gt;
    &lt;code&gt;typedef struct {
    int width;              /* Output width in pixels (default: 256) */
    int height;             /* Output height in pixels (default: 256) */
    int num_steps;          /* Denoising steps, use 4 for klein (default: 4) */
    float guidance_scale;   /* CFG scale, use 1.0 for klein (default: 1.0) */
    int64_t seed;           /* Random seed, -1 for random (default: -1) */
    float strength;         /* img2img only: 0.0-1.0 (default: 0.75) */
} flux_params;

/* Initialize with sensible defaults */
#define FLUX_PARAMS_DEFAULT { 256, 256, 4, 1.0f, -1, 0.75f }&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46670279</guid><pubDate>Sun, 18 Jan 2026 18:01:58 +0000</pubDate></item><item><title>Show HN: HTTP:COLON – A quick HTTP header/directive inspector and reference</title><link>https://httpcolon.dev/</link><description>&lt;doc fingerprint="b5099b1f4f445c15"&gt;
  &lt;main&gt;
    &lt;p&gt;HTTP:DOCS&lt;/p&gt;
    &lt;head rend="h1"&gt;What are HTTP Headers?&lt;/head&gt;
    &lt;p&gt;HTTP headers are a fundamental component of the HTTP protocol, which is the backbone of the internet. These headers contain important information about the request and response, such as content type, caching instructions, authentication tokens, and more. By understanding how to read and manipulate HTTP headers, developers can optimize their web applications for performance, security, and functionality. Moreover, HTTP headers play a critical role in API integrations, allowing developers to communicate with external services and systems. In short, HTTP headers are an essential tool in the web developer's arsenal, and any developer serious about building high-quality web applications should invest the time to learn and master them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46670290</guid><pubDate>Sun, 18 Jan 2026 18:03:03 +0000</pubDate></item><item><title>Prediction markets are ushering in a world in which news becomes about gambling</title><link>https://www.msn.com/en-us/money/markets/america-is-slow-walking-into-a-polymarket-disaster/ar-AA1Upfdb</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46670524</guid><pubDate>Sun, 18 Jan 2026 18:20:25 +0000</pubDate></item><item><title>Breaking the Zimmermann Telegram (2018)</title><link>https://medium.com/lapsed-historian/breaking-the-zimmermann-telegram-b34ed1d73614</link><description>&lt;doc fingerprint="f7b4d05db0a2feeb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Breaking the Zimmermann Telegram&lt;/head&gt;
    &lt;p&gt;Just over one hundred years ago, the British carried out one of the most audacious acts in the history of codebreaking. So audacious, in fact, that they had to convince the Americans they hadn’t done it at all…&lt;/p&gt;
    &lt;head rend="h3"&gt;The Admiralty&lt;/head&gt;
    &lt;p&gt;Running, Lieutenant Nigel De Grey decided as he narrowly avoided colliding with another paper-laden trolley, was not something that the corridors of the Admiralty Old Building had been designed for.&lt;/p&gt;
    &lt;p&gt;Nor was it something that the Royal Navy approved of from its junior officers, apparently. This was clear from the angry shouts of the people he dodged as he raced down the building’s narrow back corridors.&lt;/p&gt;
    &lt;p&gt;Right now though De Grey didn’t care. It was 17th January 1917 and Europe had been locked in a bloody stalemate for almost three years, but the scrap of paper he held in his hand might well change the outcome of the Great War.&lt;/p&gt;
    &lt;head rend="h3"&gt;The dormouse&lt;/head&gt;
    &lt;p&gt;Although he now spent his days in London, was more than familiar with the horrors happening on the Western Front. The son of a reverend, De Grey had worked at a publishing company before the war where he’d been nicknamed “dormouse” by his colleagues due to his shyness. At the same time he had been a member of the Royal Naval Reserve. He was called up early and, as a result, had been in combat in Belgium during the early days of the war.&lt;/p&gt;
    &lt;p&gt;In 1915, however, De Grey’s fluency in both German and French, his quick mind and his love of a good puzzle had been noticed by the powers that be. Without warning, he was ordered back to London to join a mysterious Naval department known as ‘Room 40.’&lt;/p&gt;
    &lt;head rend="h3"&gt;The Room&lt;/head&gt;
    &lt;p&gt;Room 40 had only existed for a few short months when De Grey joined, although plans had existed for such an organisation should war break out since 1911. That it existed at all was because the world of warfare — or more importantly the way that people communicated in war — was changing. Radio, telegraph and telephony were now viable forms of communication, and so were also potentially vital sources of intelligence too. The arrival of war brought with it a myriad of opportunities for such intelligence gathering. In August 1914, for example, a Russian attaché gave the Admiralty a copy of a German codebook taken from the beached German cruiser SS Magdeburg. In a spare room (you can guess the number) at the back of the old Admiralty building, a small group of officers and civilians were given a new job — break and read German communications. De Grey joined soon after. It was here that he discovered what he would later describe as his ‘higher calling’ — he became a codebreaker.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Research Group&lt;/head&gt;
    &lt;p&gt;In fact, De Grey was soon assigned to an even smaller, more mysterious team within Room 40 — the ‘Research Group’. A secret department within a secret department, its innocuous name was cover for work which was anything but. For whilst trying to read your enemy’s message traffic was considered acceptable (if unsporting) behaviour during wartime, doing the same thing to neutral powers was seriously frowned upon. Yet this was exactly what the Research Group had been created to do.&lt;/p&gt;
    &lt;p&gt;That such an opportunity existed was due to the way transatlantic communication worked at the time. Radio was getting more advanced and powerful, but it was not yet good enough to provide worldwide coverage. This meant that most diplomatic traffic still circulated in telegraph form, sent across vast distances by cable.&lt;/p&gt;
    &lt;p&gt;For the Entente powers in the First World War this wasn’t really a problem. Britain and France were both at the height of their imperial power and their telegraph networks spanned the globe. Germany, however, did not have that luxury. Its cables — particularly those stretching across the Atlantic — lay well outside its zone of military control.&lt;/p&gt;
    &lt;p&gt;This situation was not lost on the Entente. Almost as soon as war was declared, much of Germany’s overseas cable network went dark. It didn’t take an expert to know why — the Royal Navy had cut most of the cables, and Germany realised those that which remained suspiciously uncut should probably be considered compromised.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Neutral&lt;/head&gt;
    &lt;p&gt;Robbed of the ability to communicate with their embassies throughout the world, the Germans protested. They complained that this was as an outrageous violation of diplomatic protocol — even during war.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, their complaints fell on deaf ears within the Entente itself. Luckily for the Germans, however, there was one major power who agreed with them — the United States of America. America was staunchly neutral at the time, the only ‘great power’ not involved in the war and its President, Woodrow Wilson, believed that if the the US were to have any hope of mediating an end to the war in Europe, then German diplomats in the US and beyond needed to be able to talk freely to their government.&lt;/p&gt;
    &lt;p&gt;It was a noble goal, and so to further it the US State Department granted Germany permission to use the American transatlantic cable, via Copenhagen, for diplomatic telegraph traffic.&lt;/p&gt;
    &lt;p&gt;Both Germany and the US believed these messages to be entirely secure. German intelligence had sufficiently penetrated the State Department to know that the Americans weren’t interested in breaking Germany’s codes. More importantly though, both powers believed that the British would not tap into US traffic — to do so would cause an enormous diplomatic incident. Not that it mattered anyway — even if they were tempted, the Germans thought they were safe. They understood that the US cable was entirely submarine, and thus safe from tampering.&lt;/p&gt;
    &lt;p&gt;The Germans were right on the first account, Unfortunately were wrong on both the latter.&lt;/p&gt;
    &lt;head rend="h3"&gt;The interception&lt;/head&gt;
    &lt;p&gt;Whatever the thoughts of the British Foreign Office might be, the Admiralty had its own opinions on what was, and wasn’t, fair game when it came to intelligence gathering. If the Americans were going to transmit coded German messages for them, then as far as Captain Reginald “Blinker” Hall, the Director of Naval Intelligence and ultimate head of Room 40 was concerned, American diplomatic traffic was absolutely fair game.&lt;/p&gt;
    &lt;p&gt;Again, had the Germans been correct about the American submarine cable then this still wouldn’t have been a problem, but they weren’t. In fact, US telegraph traffic came ashore on Britain via a relay station just north of Newcastle and then travelled across the country to Cornwall. From there it was then transmitted onward to Washington. This presented multiple opportunities for the messages to be intercepted by the British, and the Research Group was born. Every day they would receive copies of the traffic sent across the line. Their job was to crack the codes and read every diplomatic message the Americans and the Germans sent.&lt;/p&gt;
    &lt;p&gt;It was a decrypt of one of those diplomatic messages that De Grey now clutched in his hand as he raced down the Admiralty’s narrow oak halls. Sent the night before, it was pure luck that it had been decrypted so quickly. It was only a short message, which had been sent by Arthur Zimmermann, the German Foreign Minister, to the German ambassador in Mexico. As such, it was considered low-level diplomatic traffic and had been marked as low-priority for breaking and decryption. By chance, however, when it had arrived at Room 40 the pneumatic Tube system had dumped it on the desk of one of the department’s other rising stars, Alfred Dillwyn Knox.&lt;/p&gt;
    &lt;head rend="h3"&gt;The genius&lt;/head&gt;
    &lt;p&gt;A Classics scholar and papyrologist at Cambridge before the war, “Dilly” had joined Room 40 in 1914. There he swiftly demonstrated an unquestionable genius for codebreaking. Indeed Dilly Knox remains one of the greatest codebreakers Britain has ever produced. After the end of the First World War, he would become one of the founding fathers of the Government Code and Cypher School — GCHQ, which remains Britain’s primary cryptographic line of defence to this day. Nor does his influence end there. In 1925 in Vienna, he became the first British Intelligence officer to acquire an Enigma machine. Then in Warsaw, in 1938, it was to Dilly that the Poles were prepared to turnover their own Enigma codebreaking efforts. It was also Dilly who oversaw the transfer of that information — and a number of Polish codebreakers who managed to escape the Nazi invasion of Poland — to a new codebreaking institution he had helped set up back in Britain — Bletchley Park.&lt;/p&gt;
    &lt;p&gt;What many people don’t realise is that ‘Enigma’ wasn’t one code — it was many. The most complex of these (thanks to an extra rotor on the machine) was the German Naval code. The honour for breaking that rightly belongs to Alan Turing, but he was not the only man working on Enigmas. Dilly himself broke not one, but three of the other key codes — those of Spanish Intelligence, the German Army and the Italian Navy. To take full advantage of these, he then fought for the right to form a unique codebreaking outfit at Bletchley — “Intelligence Service Knox” (ISK). Under Knox, ISK became the only codebreaking department at Bletchley entirely staffed by women.&lt;/p&gt;
    &lt;head rend="h3"&gt;The ‘Dilly Girls’&lt;/head&gt;
    &lt;p&gt;Dilly had spotted that whilst women were considered a vital cog in the Bletchley codebreaking machine, they were almost exclusively confined to ‘support’ roles — Bombe operators, transcribers, translators and beyond. Dilly saw this as a waste of good minds, based solely on flawed preconceptions about gender, at a time when Britain needed good minds the most.&lt;/p&gt;
    &lt;p&gt;The formation of ISK was not without controversy. Rumours soon circulated that Dilly had wandered round the huts pointing at the prettiest girls for his ‘eastern harem’, and they were soon being referred to by the derogatory nickname ‘Dilly’s Girls’.&lt;/p&gt;
    &lt;p&gt;Nothing could have been further from the truth. Once permission had been given to form ISK, Dilly had immediately approached the head of the Women’s section, who interviewed all of the female staff sent to Bletchley and managed them once they’d arrived. He asked her to reassign those she considered most wasted in their current roles to ISK and the results soon spoke for themselves. ISK became one of the most successful codebreaking teams at Bletchley, contributing critical decryptions that would help win the naval war in the Mediterranean and ensure the success of the D-Day landings. Indeed ISK’s contributions outlived Dilly himself (who died suddenly of cancer in 1943), with the department proudly adopting and subverting the ‘Dilly’s Girls’ moniker until the end of the war.&lt;/p&gt;
    &lt;head rend="h3"&gt;The revelation&lt;/head&gt;
    &lt;p&gt;In 1917, of course, all this was in the future. Right now Dilly’s efforts were focused firmly on finding new ways into German naval codes. Unusually, Dilly was not particularly mathematical. What he was good at, however, was spotting patterns and looking at things from unusual angles, in part the result of his experience rebuilding and translating Greek manuscripts from mere fragments before the war. He also had a near-uncanny ability to put himself in the mind of the people at the other end of the line. In 1915 he had broken the German Admiralty’s flag code by spotting — and exploiting — one particular German telegraph operator’s love of romantic poetry. These efforts had put Dilly on the Research Group’s radar, and though he was not officially a member of the team he had been quietly called in to help with their work from time to time.&lt;/p&gt;
    &lt;p&gt;Indeed this was perhaps why this particular intercept had dropped from the Admiralty’s pneumatic tube system onto Dilly’s desk on the night of the 16th January. With the rest of the Research Group busy that night, it might have been that Dilly was seen as an overflow for the low-level traffic. Whatever the reason, something about this particular message caught Dilly’s eye. Rather than leaving it at the bottom of his pile, he worked on trying to break it right through the night.&lt;/p&gt;
    &lt;p&gt;By morning, he had begun to make inroads into the telegram. Dilly didn’t speak German, but he recognised words such as “Submarine”, “Mexico” and “Arizona”. He became increasingly convinced that the telegram was important and so, when De Grey arrived at work the next morning, Dilly roped him in to help. The two men had worked as a decryption team before with considerable success — De Grey’s fluent German and experience as an editor meshing well with Dilly’s own skills. Together they worked on the telegram right through the morning. The more they decrypted, the more both men became astonished at what they were reading — indeed they could barely believe it. By lunchtime, however, they had decrypted enough to know that they weren’t wrong. They agreed the Captain needed to see this immediately.&lt;/p&gt;
    &lt;p&gt;Normally athletics wouldn’t have been necessary. Officially, everyone in Room 40 reported to Sir Alfred Ewing, who himself then reported to “Blinker” Hall. Sometime before, however, the Captain himself had quietly pulled De Grey and the other men of the Research Group aside. Ewing, Hall told them, was a bit of a chatterbox in the corridors of power and Hall didn’t trust him to keep a really big secret. If the Research Group’s work ever yielded something particularly sensitive or explosive, then they were ordered to bypass Ewing completely and only reveal what they had found to Hall himself. So this was where De Grey was headed.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Captain&lt;/head&gt;
    &lt;p&gt;De Grey entered the Captain’s outer office at a sprint, bursting into the Hall’s office before his personal secretary could object. Luckily, the Captain was in.&lt;/p&gt;
    &lt;p&gt;“Do you want to bring America into the war sir?” De Grey burst out breathlessly.&lt;/p&gt;
    &lt;p&gt;“Yes, why?” Replied the slightly bemused Hall. He had long since stopped expecting any semblance of military decorum or normality from his codebreakers.&lt;/p&gt;
    &lt;p&gt;“I’ve got a telegram that will bring them in if you give it to them.” De Grey blurted out, thrusting the results of his own and Dilly’s efforts towards the Captain.&lt;/p&gt;
    &lt;p&gt;Hall took the decrypt and read it, silently, as De Grey explained who it was from, for and how they had broken it. For the very first time, a senior member of British Intelligence held in his hands a copy of what would become known to history as the ‘Zimmermann Telegram.’&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We intend to begin on the first of February unrestricted submarine warfare. We shall endeavour in spite of this to keep the United States of America neutral. In the event of this not succeeding, we make Mexico a proposal of alliance on the following basis: make war together, make peace together, generous financial support and an understanding on our part that Mexico is to reconquer the lost territory in Texas, New Mexico, and Arizona. The settlement in detail is left to you. You will inform the President of the above most secretly as soon as the outbreak of war with the United States of America is certain and add the suggestion that he should, on his own initiative, invite Japan to immediate adherence and at the same time mediate between Japan and ourselves. Please call the President’s attention to the fact that the ruthless employment of our submarines now offers the prospect of compelling England in a few months to make peace.&lt;/p&gt;
      &lt;p&gt;Signed, ZIMMERMANN&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;The telegram&lt;/head&gt;
    &lt;p&gt;Hall listened patiently as De Grey outlined both what they new for certain and what were guesses at length. By the time De Grey had finished, Hall was happy to accept what he was saying was true. At this stage, they had not fully decrypted the message (the above is the full, final text), but it was more than enough for Captain Hall to grasp that De Grey wasn’t exaggerating. This wasn’t just confirmation that Germany were preparing to conduct unrestricted submarine warfare — it was incitement to Mexico to declare war on the United States.&lt;/p&gt;
    &lt;p&gt;Whilst Zimmermann has been cast in history as something of a naive operator, the truth is anything but. Zimmermann was one of the architects of Germany’s successful policy of funnelling money and support to rebellions and rivals of the Entente powers. This had caused enormous problems for them, forcing them to spread their forces thinner across the world. Indeed at that very moment this approach was yielding enormous results in Russia, who would be forced out of the war entirely before the year was out.&lt;/p&gt;
    &lt;p&gt;Zimmermann’s telegram was intended to lay the groundwork for the same approach to be taken across the Atlantic, in the event that the declaration of unrestricted submarine warfare be enough to tip the balance of US government into intervening.&lt;/p&gt;
    &lt;p&gt;Not only were the Germans suggesting Mexico declare war on the United States (with German backing) but, even more incredibly, they were using the using US State Department’s own telegraph network to do it.&lt;/p&gt;
    &lt;head rend="h3"&gt;The problem&lt;/head&gt;
    &lt;p&gt;If unrestricted submarine warfare itself didn’t drag the US into the war, then Hall realised that De Grey and Dilly were right — this telegram (and the outrageous way it had been sent) could well be enough to do so.&lt;/p&gt;
    &lt;p&gt;Hall, however, was fully aware that he had a problem. Indeed the mother of all intelligence problems. One of the regular problems with good intelligence was working out how to use it without ‘burning’ the source — because revealing it might inadvertently reveal to the enemy how you got it, cutting you off from all future intelligence by the same method.&lt;/p&gt;
    &lt;p&gt;Hall’s problem here was even worse. Not only would revealing the existence of the telegram burn the source, as the Germans would know the US cable was compromised, but that source was, effectively, the US State Department itself.&lt;/p&gt;
    &lt;p&gt;“Hello chaps, we’ve been reading your mail, and there’s some things in here you really should see…” Was a line that was hardly likely to go over well with the Americans. Indeed they may be more than outraged enough about that to eclipse any horror at the telegram itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hall’s solution&lt;/head&gt;
    &lt;p&gt;Recognising the explosiveness of the situation, Hall and De Grey briefly discussed their options. Realising that whatever he did, he should probably lock things down until they had a plan.&lt;/p&gt;
    &lt;p&gt;Claude Serocold, Hall’s personal assistant was inducted into the secret and the men then pitched around more ideas as to how they could get the telegram into the hands of the Americans without blowing the source. In the end, it was Hall himself who had the brainwave that led to the solution.&lt;/p&gt;
    &lt;p&gt;Looking at the intercept, he realised that although the final destination of the telegram was the German Ambassador in Mexico, it hadn’t been sent to him directly. It was routed via Johann Heinrich von Bernstorff, the German Ambassador to the US. Although the British didn’t know it at the time, this was because the arrangement between the US State Department and the German Foreign Office was that they could send diplomatic communications down the main US cable to Washington but no further. At that point, the Germans would have to make their own arrangements for onward transmission.&lt;/p&gt;
    &lt;p&gt;Whatever the reasons, Hall realised that this presented an opportunity. Von Bernstorff would have to retransmit the message at the American end. The German Embassy, Room 40 knew, had a commercial relationship with Western Union in the United States, so this was likely how von Bernstorff would do it.&lt;/p&gt;
    &lt;p&gt;Room 40 also knew that he would also have to decrypt and then re-encrypt the message before doing so, as the Germans never used their own, high-level codes on commercial networks. Doing so risked opening them up too much to codebreaking efforts. Based on previous experience, the men posited that the whole process of receipt in New York, handover from the State Department to the Germans, decryption, re-encryption and transmission over Western Union would take about five days.&lt;/p&gt;
    &lt;p&gt;Hall realised this whole process offered an opportunity they could exploit. The Western Union message would be in a lower code, transcribed by the Germans themselves. If they could get hold of that, at the Mexican end, then they could claim this was the source instead.&lt;/p&gt;
    &lt;p&gt;The Mexican connection&lt;/p&gt;
    &lt;p&gt;Until now, Room 40 had generally ignored the Western Union traffic as a potential source of high-value intelligence. Any kind of operation across the Atlantic would have involved not just stepping on American toes but smashing a large boot down on them repeatedly. Given the perceived low value of the traffic, it simply wasn’t worth the risk.&lt;/p&gt;
    &lt;p&gt;Hall pointed out though that right now they didn’t need everything that Germany was sending over Western Union. They didn’t even need a tap on the line. They just needed a copy of this specific telegram. They knew who it was going to, who it was from and — roughly — when it was likely to be sent. They just needed someone who could get hold of a copy from the Western Union office in Mexico City, no questions asked.&lt;/p&gt;
    &lt;p&gt;Hall made discreet inquiries with the British Embassy in Mexico. They confirmed that they had a source in the Western Union office in Mexico City — a clerk who, for the right price, would occasionally lift telegrams for them from Western Union’s files. Hall told them what to watch out for and when, although he refused to tell them why. Nonetheless, they agreed that they would try.&lt;/p&gt;
    &lt;p&gt;It was an inspired idea. A few days later, courtesy of the British Embassy in Mexico, a copy of the telegram, lifted directly from the files of the Mexico City office of Western Union, was delivered to Captain Hall’s desk by the Foreign Office.&lt;/p&gt;
    &lt;head rend="h3"&gt;The ambassador&lt;/head&gt;
    &lt;p&gt;On 19th February 1917, Captain Hall found himself standing in the offices of the US Ambassador to Britain in the heart of London.&lt;/p&gt;
    &lt;p&gt;19 days before, on the exact day indicated in the Zimmermann Telegram had indicated, Germany had begun waging unrestricted submarine warfare in the Atlantic. It had caused outrage and the breaking of diplomatic relations between Germany and the United States. Yet the US had remained neutral.&lt;/p&gt;
    &lt;p&gt;On 5th February — two weeks after De Grey and Dilly had first decrypted it — ‘Blinker’ Hall finally informed the British Foreign Office that the Zimmermann Telegram existed.&lt;/p&gt;
    &lt;p&gt;As Hall expected, the Foreign Office demanded to know the source. Hall was able to present them with the Western Union telegram, describing — with a straight face — how the message had been a ‘lucky intercept’ in Mexico, that had fallen into the hands of the British Embassy. They’d suspected it was significant, so had passed it on to Room 40, where it had been decrypted.&lt;/p&gt;
    &lt;p&gt;This was actually a lie on both accounts. Whilst it was clearly the same telegram as the one in the original “high” code that they had broken, somewhat ironically the “lesser” code that von Bernstorff had used (Diplomatic Code 13040) was one which the British hadn’t previously bothered trying to break. Luckily, another of the Room 40 codebreakers had spotted that it was similar to another naval code that they had broken elsewhere, and this had led to a partial decryption. Enough, at least, to fill in the gaps left in Dilly’s work on the original interception and confirm beyond a doubt that they were the same message.&lt;/p&gt;
    &lt;p&gt;On 18th February 1917, the Foreign Office had discreetly informed the US Ambassador, Walter Hines Page of the telegram’s existence, but Page was naturally suspicious. Whatever the state of US / German relations, he found it hard to believe that such an incredible telegram existed, let alone that the British would somehow have managed to obtain a copy. He told his personal secretary, Edward Bell, that he wanted more proof. Only then would he present this information to President Wilson.&lt;/p&gt;
    &lt;p&gt;This was why Captain Hall was standing in front of Edward Bell in the US Embassy now. He had been dispatched by the Foreign Office to meet with Bell and satisfy the Ambassador’s demands. The two men chatted cordially and the Captain told Bell the Mexico story and offered up his copy of the Western Union telegram as evidence. Bell agreed that it was compelling, but he still wanted more.&lt;/p&gt;
    &lt;p&gt;“I want to see it decrypted. In person.” Bell told the Captain.&lt;/p&gt;
    &lt;p&gt;Captain Hall smiled and sent for Nigel De Grey.&lt;/p&gt;
    &lt;head rend="h3"&gt;The final bluff&lt;/head&gt;
    &lt;p&gt;De Grey arrived soon after, clutching his notes on Diplomatic Code 13040. Captain Hall introduced him to Edward Bell and, with a relaxed smile, told De Grey what he was to do — decrypt the telegram while Bell watched.&lt;/p&gt;
    &lt;p&gt;On his part, De Grey couldn’t understand why the Captain was so relaxed, because internally De Grey himself was screaming. Hall had made an uncharacteristic mistake — he seemed to have forgotten that they hadn’t solved the Mexican version of the telegram. They only had a partial decrypt, largely based off the naval code it had been a close match for. Worse, De Grey hadn’t even bothered to write down all of the keys they had discovered in his own notes. There hadn’t seemed to be much point once they’d done enough to fill in the gaps on the original.&lt;/p&gt;
    &lt;p&gt;As he began to decode the telegram, under Bell’s watchful eye, De Grey realised he was going to have to improvise.&lt;/p&gt;
    &lt;p&gt;“If I stopped and fetched another book,” De Grey said later, “he would suspect at once that we’d faked it up for his benefit. If I let him see that I was writing it down out of my head, he would not believe me. If he did not believe me, we should fail and lose the greatest opportunity ever presented to us. Several seconds of bloody sweat. Then I bluffed. I showed him all the groups when they had been written in my book and passed quickly over those that were not, writing the words into the copy of the telegram by heart.”&lt;/p&gt;
    &lt;p&gt;“Edward Bell, the most charming man, was thoroughly convinced — the more easily I think in that he wanted to be convinced anyhow and regarded the whole thing as black magic.”&lt;/p&gt;
    &lt;p&gt;On the 20th February 1917, Bell handed over Hall’s copy of the Zimmermann telegram to Ambassador Page, telling him he agreed it was genuine, and suggesting they get Western Union to confirm that it was genuine. By the end of the month, the company had done so and a copy of Room 40’s decrypted version was in the hands of the President. On the 28th February 1917, Wilson handed it over to the American press.&lt;/p&gt;
    &lt;head rend="h3"&gt;The result&lt;/head&gt;
    &lt;p&gt;The United States of America declared war on Germany on the 5th April 1917, just over a month after the Zimmermann telegram had been handed over to the US Government. It is possible that unrestricted submarine warfare would have been enough to tip the US into intervention, eventually. The Zimmermann telegram, however, almost certainly made that inevitable. Few documents, in the entire history of information warfare, can be said to have had such an impact world history.&lt;/p&gt;
    &lt;p&gt;For the men of Room 40, it was a spectacular triumph, albeit one that none of the key players could talk about for considerable time to come. Indeed so good was Captain Hall’s cover story that it remained, for a long time, the official version of events. This suited ‘Blinker’ very well indeed. The Admiralty continued to read US Diplomatic traffic right up to — and indeed beyond — the end of the First World War.&lt;/p&gt;
    &lt;p&gt;“He was a perfectly marvellous person” Edward Bell later said of Captain Hall, “but the coldest-hearted proposition that ever was — he’d eat a man’s heart and hand it back to him.”&lt;/p&gt;
    &lt;p&gt;Both Dilly and De Grey were happy to keep the secret. They were codebreakers, and accepted that public acknowledgement rarely came with the job.&lt;/p&gt;
    &lt;p&gt;One of ‘Dilly’s Girls’ would later recall that, having been told the real story from the man himself at Bletchley, she asked him if either he, or De Grey, had received any kind of recognition.&lt;/p&gt;
    &lt;p&gt;“Gosh no!” Dilly replied, with a laugh. “But I believe Nigel did get an official telling off for running in the corridor!”&lt;/p&gt;
    &lt;p&gt;Like what I write? Then help me do more of it. Back London Reconnections, my transport site on Patreon. Every little helps tell a story.&lt;/p&gt;
    &lt;p&gt;Want a thorough and detailed account of Room 40 and its impact? Then buy Inside Room 40 by Paul Gannon.&lt;/p&gt;
    &lt;p&gt;Update!!!!&lt;/p&gt;
    &lt;p&gt;To everyone who said they wanted to know more about Dilly and the ‘Dilly Girls’ in WW2 — if we reach our Patreon target, then I will write up the remarkable tale of how Dilly, the women of the ISK, Prince Philip, a golfing British Admiral and an amorous Italian Ambassador all played a part in the last, great naval battle in the history of warfare.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46671174</guid><pubDate>Sun, 18 Jan 2026 19:19:48 +0000</pubDate></item><item><title>Evolution Unleashed (2018)</title><link>https://aeon.co/essays/science-in-flux-is-a-revolution-brewing-in-evolutionary-theory</link><description>&lt;doc fingerprint="a710924c9c358151"&gt;
  &lt;main&gt;
    &lt;p&gt;When researchers at Emory University in Atlanta trained mice to fear the smell of almonds (by pairing it with electric shocks), they found, to their consternation, that both the children and grandchildren of these mice were spontaneously afraid of the same smell. That is not supposed to happen. Generations of schoolchildren have been taught that the inheritance of acquired characteristics is impossible. A mouse should not be born with something its parents have learned during their lifetimes, any more than a mouse that loses its tail in an accident should give birth to tailless mice.&lt;/p&gt;
    &lt;p&gt;If you are not a biologist, you’d be forgiven for being confused about the state of evolutionary science. Modern evolutionary biology dates back to a synthesis that emerged around the 1940s-60s, which married Charles Darwin’s mechanism of natural selection with Gregor Mendel’s discoveries of how genes are inherited. The traditional, and still dominant, view is that adaptations – from the human brain to the peacock’s tail – are fully and satisfactorily explained by natural selection (and subsequent inheritance). Yet as novel ideas flood in from genomics, epigenetics and developmental biology, most evolutionists agree that their field is in flux. Much of the data implies that evolution is more complex than we once assumed.&lt;/p&gt;
    &lt;p&gt;Some evolutionary biologists, myself included, are calling for a broader characterisation of evolutionary theory, known as the extended evolutionary synthesis (EES). A central issue is whether what happens to organisms during their lifetime – their development – can play important and previously unanticipated roles in evolution. The orthodox view has been that developmental processes are largely irrelevant to evolution, but the EES views them as pivotal. Protagonists with authoritative credentials square up on both sides of this debate, with big-shot professors at Ivy League universities and members of national academies going head-to-head over the mechanisms of evolution. Some people are even starting to wonder if a revolution is on the cards.&lt;/p&gt;
    &lt;p&gt;In his book On Human Nature (1978), the evolutionary biologist Edward O Wilson claimed that human culture is held on a genetic leash. The metaphor was contentious for two reasons. First, as we’ll see, it’s no less true that culture holds genes on a leash. Second, while there must be a genetic propensity for cultural learning, few cultural differences can be explained by underlying genetic differences.&lt;/p&gt;
    &lt;p&gt;Nonetheless, the phrase has explanatory potential. Imagine a dog-walker (the genes) struggling to retain control of a brawny mastiff (human culture). The pair’s trajectory (the pathway of evolution) reflects the outcome of the struggle. Now imagine the same dog-walker struggling with multiple dogs, on leashes of varied lengths, with each dog tugging in different directions. All these tugs represent the influence of developmental factors, including epigenetics, antibodies and hormones passed on by parents, as well as the ecological legacies and culture they bequeath.&lt;/p&gt;
    &lt;p&gt;The struggling dog-walker is a good metaphor for how EES views the adaptive process. Does this require a revolution in evolution? Before we can answer this question, we need to examine how science works. The best authorities here are not biologists but philosophers and historians of science. Thomas Kuhn’s book The Structure of Scientific Revolutions (1962) popularised the idea that sciences change through revolutions in understanding. These ‘paradigm shifts’ were thought to follow a crisis of confidence in the old theory that arose through the accumulation of conflicting data.&lt;/p&gt;
    &lt;p&gt;Then there’s Karl Popper, and his conjecture that scientific theories can’t be proven but can be falsified. Consider the hypothesis: ‘All sheep are white.’ Popper maintained that no amount of positive findings consistent with this hypothesis could prove it to be correct, since one could never rule out the chance that a conflicting data-point might arise in the future; conversely, the observation of a single black sheep would decisively prove the hypothesis to be false. He maintained that scientists should strive to carry out critical experiments that could potentially falsify their theories.&lt;/p&gt;
    &lt;p&gt;Everything from diet to air pollution to parental behaviour can influence gene expression&lt;/p&gt;
    &lt;p&gt;While Kuhn and Popper’s ideas are well-known, they remain disputed and contentious in the eyes of philosophers and historians. Contemporary thinking in these fields is better captured by the Hungarian philosopher Imre Lakatos in The Methodology of Scientific Research Programmes (1978):&lt;/p&gt;
    &lt;quote&gt;The history of science refutes both Popper and Kuhn: on close inspection both Popperian crucial experiments and Kuhnian revolutions turn out to be myths.&lt;/quote&gt;
    &lt;p&gt;Popper’s arguments might make logical sense, but they don’t quite map on to how science works in the real world. Scientific observations are susceptible to errors of measurement; scientists are human beings and get attached to their theories; and scientific ideas can be fiendishly complex – all of which makes evaluating scientific hypotheses a messy business. Rather than accepting that our hypotheses might be wrong, we challenge the methodology (‘That sheep’s not black – your instruments are faulty’), dispute the interpretation (‘The sheep’s just dirty’), or come up with tweaks to our hypotheses (‘I meant domesticated breeds, not wild mouflon’). Lakatos called such fixes and fudges ‘auxiliary hypotheses’; scientists propose them to ‘protect’ their core ideas, so that they need not be rejected.&lt;/p&gt;
    &lt;p&gt;This sort of behaviour is clearly manifest in scientific debates over evolution. Take the idea that new features acquired by an organism during its life can be passed on to the next generation. This hypothesis was brought to prominence in the early 1800s by the French biologist Jean-Baptiste Lamarck, who used it to explain how species evolved. However, it has long been regarded as discredited by experiment – to the point that the term ‘Lamarckian’ has a derogatory connotation in evolutionary circles, and any researchers expressing sympathy for the idea effectively brand themselves ‘eccentric’. The received wisdom is that parental experiences can’t affect the characters of their offspring.&lt;/p&gt;
    &lt;p&gt;Except they do. The way that genes are expressed to produce an organism’s phenotype – the actual characteristics it ends up with – is affected by chemicals that attach to them. Everything from diet to air pollution to parental behaviour can influence the addition or removal of these chemical marks, which switches genes on or off. Usually these so-called ‘epigenetic’ attachments are removed during the production of sperm and eggs cells, but it turns out that some escape the resetting process and are passed on to the next generation, along with the genes. This is known as ‘epigenetic inheritance’, and more and more studies are confirming that it really happens.&lt;/p&gt;
    &lt;p&gt;Let’s return to the almond-fearing mice. The inheritance of an epigenetic mark transmitted in the sperm is what led the mice’s offspring to acquire an inherited fear. In 2011, another extraordinary study reported that worms responded to exposure to a nasty virus by producing virus-silencing factors – chemicals that shut down the virus – but, remarkably, subsequent generations epigenetically inherited these chemicals via regulatory molecules (known as ‘small RNAs’). There are now hundreds of such studies, many published in the most prominent and prestigious journals. Biologists dispute whether epigenetic inheritance is truly Lamarckian or only superficially resembles it, but there is no getting away from the fact that the inheritance of acquired characteristics really does happen.&lt;/p&gt;
    &lt;p&gt;By Popper’s reasoning, a single experimental demonstration of epigenetic inheritance – like a single black sheep – should suffice to convince evolutionary biologists that it’s possible. Yet, by and large, evolutionary biologists have not rushed to change their theories. Rather, as Lakatos anticipated, we have come up with auxiliary hypotheses that allow us to retain our long-held beliefs (ie, that inheritance is pretty much explained by the transmission of genes across generations). These include the ideas that epigenetic inheritance is rare, that it does not affect functionally important traits, that it is under genetic control, and that it is too unstable to underpin the spread of traits through selection.&lt;/p&gt;
    &lt;p&gt;Unfortunately for the traditionalists, none of these attempts to bracket epigenetic inheritance look credible. It is now known to be widespread in nature, with more and more examples appearing every day. It affects functionally important features such as fruit size, flowering time and root growth in plants – and while only a fraction of epigenetic variants are adaptive, that’s no less true of genetic variation, so it’s hardly grounds for dismissal. In some systems where rates of epigenetic change have been measured carefully, such as the plant Arabidopsis thaliana, the pace has been found to be low enough to be selected and lead to cumulative evolution. Mathematical models have shown that systems with epigenetic inheritance evolve differently from those solely reliant on genetic inheritance – for instance, selection on epigenetic marks can cause changes in gene frequencies. There’s no longer any doubt that epigenetic inheritance pushes us to think about evolution in a different way.&lt;/p&gt;
    &lt;p&gt;Epigenetics is only part of the story. Through culture and society, all of us inherit knowledge and skills acquired by our parents. Evolutionary biologists have accepted this for at least a century, but until recently it was considered to be restricted to humans. That’s no longer tenable: creatures across the animal kingdom learn socially about diet, feeding techniques, predator avoidance, communication, migration, and mate and breeding-site choices. Hundreds of experimental studies have demonstrated social learning in mammals, birds, fish and insects.&lt;/p&gt;
    &lt;p&gt;In a single mating season, ‘fads’ can develop in the qualities that individuals find attractive in their partners&lt;/p&gt;
    &lt;p&gt;Among the most compelling data are studies that cross-fostered great tits and blue tits. When raised by the other species, these birds shifted numerous aspects of their behaviour towards the behaviour of their foster parent (including the height in trees at which they foraged, their choice of prey, foraging method, calls and songs, and even their choice of mate). Everyone had assumed that the behavioural differences between these two species were genetic, but it turns out that many are cultural traditions.&lt;/p&gt;
    &lt;p&gt;Animal cultures can be sustained for surprisingly long periods. Archaeological remains show that chimpanzees have used stone tools to crack open nuts for at least 4,300 years. However, as for epigenetic inheritance, it would be a mistake to assume that animal culture must exhibit gene-like stability to be evolutionarily important. In the course of a single mating season, ‘fads’ can develop in the qualities that individuals find attractive in their partners; the process has been experimentally demonstrated in fruit flies, fish, birds and mammals, and mathematical models show that such ‘mate-choice copying’ can strongly affect sexual selection.&lt;/p&gt;
    &lt;p&gt;Another illustration comes from studies of birdsong. When young male birds learn their songs (usually from nearby adult males), they modify the natural-selection pressures of genes that affect how songs are acquired (in males) and which songs are preferred (in females). The cultural transmission of song is known to promote the evolution of brood parasitism – where birds, such as cuckoos, don’t make nests but lay eggs in other birds’ nests – as some brood parasites rely on cultural learning to figure out whom to mate with. It also facilitates speciation, since preferences for particular birdsong ‘dialects’ help to maintain genetic differences between populations.&lt;/p&gt;
    &lt;p&gt;Likewise, the diverse, culturally learned foraging traditions of orcas – where different groups specialise in particular types of fish, seals or dolphins – is thought to be driving them to split into several species. Of course, culture reaches its zenith in our own species, where it is now well-established that our cultural habits have been a major source of natural selection on our genes. Dairy farming and milk consumption generated selection for a genetic variant that increased lactase (the enzyme that metabolises dairy products), while starchy agricultural diets favoured increased amylase (the corresponding enzyme that breaks down starch).&lt;/p&gt;
    &lt;p&gt;All this complexity can’t be reconciled with a strictly genetic currency for adaptive evolution, as many biologists now acknowledge. Rather, it points to an evolutionary process in which genomes (over hundreds to thousands of generations), epigenetic modifications and inherited cultural factors (over several, perhaps tens or hundreds of generations), and parental effects (over single-generation timespans) collectively inform how organisms adapt. These extra-genetic kinds of inheritance give organisms the flexibility to make rapid adjustments to environmental challenges, dragging genetic change in their wake – much like a rowdy pack of dogs.&lt;/p&gt;
    &lt;p&gt;Despite the excitement of all the new data, it’s unlikely to trigger an evolution revolution for the simple reason that science doesn’t work that way – at least, not evolutionary science. Kuhnian paradigm shifts, like Popper’s critical experiments, are closer to myths than reality. Look back at the history of evolutionary biology, and you will see nothing that resembles a revolution. Even Charles Darwin’s theory of evolution through natural selection took approximately 70 years to become widely accepted by the scientific community, and at the turn of the 20th century was viewed with considerable skepticism. Over the following decades, new ideas appeared, they were critically evaluated by the scientific community, and gradually became integrated with pre-existing knowledge. By and large, evolutionary biology was updated without experiencing great periods of ‘crisis’.&lt;/p&gt;
    &lt;p&gt;The same holds for the present. Epigenetic inheritance does not disprove genetic inheritance, but shows it to be just one of several mechanisms through which traits are inherited. I know of no biologist who wants to rip up the textbooks, or throw out natural selection. The debate in evolutionary biology concerns whether we want to extend our understanding of the causes of evolution, and whether that changes how we think about the process as a whole. In this respect, what is going on is ‘normal science’.&lt;/p&gt;
    &lt;p&gt;Why, then, are traditionally minded evolutionary biologists complaining about the misguided evolutionary radicals that lobby for paradigm shift? Why are journalists writing articles about scientists calling for a ‘revolution’ in evolutionary biology? If nobody actually wants a revolution, and scientific revolutions rarely happen anyway, what’s all the fuss about? The answer to these questions provides a fascinating insight into the sociology of evolutionary biology.&lt;/p&gt;
    &lt;p&gt;Revolution in evolution is a misattribution – a myth propagated by an unlikely alliance of conservative-minded evolutionists, creationists and the press. I don’t doubt that there are a small number of genuine, revolutionarily minded evolutionary radicals out there, but the vast majority of researchers working towards an extended evolutionary synthesis are simply ordinary, hardworking evolutionary biologists.&lt;/p&gt;
    &lt;p&gt;We all know that sensationalism sells newspapers, and articles that portend a major upheaval make for better copy. Creationists and advocates of ‘intelligent design’ also feed this impression, with propaganda that exaggerates differences of opinion among evolutionists and gives a false impression that the field of evolutionary biology is in turmoil. What’s more surprising is how commonly conservative-minded biologists play the ‘We’re under attack!’ card against their fellow evolutionists. Portraying intellectual opponents as extremist, and telling people that they are being attacked, are age-old rhetorical tricks to win debate or allegiance.&lt;/p&gt;
    &lt;p&gt;I had always associated such games with politics, not science, but now realise I was naive. Some of the behind-the-scenes shenanigans I have witnessed, seemingly designed to prevent new ideas from spreading by fair means or foul, have truly shocked me, and are out of kilter with practice in other fields that I know. Scientists, too, have careers and legacies at stake, as well as struggles for funding, power and influence. I worry that the traditionalists’ rhetoric is backfiring, creating confusion and inadvertently fuelling creationism by exaggerating division. Too many reputable scientists feel the need for change in evolutionary biology for all to be credibly dismissed as fringe elements.&lt;/p&gt;
    &lt;p&gt;If the extended evolutionary synthesis is not a call for revolution in evolution, then what is it, and why do we need it? To answer these questions, we need to recognise what Kuhn got right – namely, that every scientific field possesses shared ways of thinking, or ‘conceptual frameworks’. Evolutionary biology is no different, and our shared values and assumptions influence what data is collected, how that data is interpreted, and what factors are built into explanations for how evolution works.&lt;/p&gt;
    &lt;p&gt;That is why pluralism in science is healthy. Lakatos stressed that alternative conceptual frameworks – what he called different ‘research programmes’ – can be valuable to the extent that they encourage new hypotheses to be generated and tested, or lead to novel insights. That is the primary function of the EES: to nurture, or even open up, new lines of enquiry, and new productive ways of thinking.&lt;/p&gt;
    &lt;p&gt;What if some ways of building a fish are just more probable than others?&lt;/p&gt;
    &lt;p&gt;A good example concerns what’s known as ‘developmental bias’. Consider the intriguing cichlid fishes of East Africa. For tens, perhaps hundreds, of the cichlid species in Lake Malawi, there exists an independently evolved, ‘duplicate’ species in Lake Tanganyika, with a strikingly similar body shape and way of feeding. Such likenesses are usually explained through convergent evolution: random genetic variation has bubbled up as usual, but similar environmental conditions have selected the genes to produce equivalent results. The way that organisms grow and develop might limit which traits arise, but the variation itself is assumed to be essentially random.&lt;/p&gt;
    &lt;p&gt;However, the extraordinary level of parallel evolution seen in these two lakes suggests that something else might be going on. What if some ways of building a fish are just more probable than others? What if trait variation skews towards certain solutions? Selection would still be part of the explanation, but parallel evolution would be much more likely.&lt;/p&gt;
    &lt;p&gt;Cheek teeth (molars) in mammals provide some of the most convincing data for bias. Studies show that it’s possible to use a mathematical model, based on laboratory mice, to predict the size and number of teeth in a sample of 29 other rodent species. Rather than being free to make any shape or number of teeth, it appears that natural selection is pushing species along a highly specific pathway created by the mechanisms of development. The existence of exceptions – rodents such as voles with different ratios of teeth – demonstrates that the old way of thinking (that developmental ‘constraints’ restrict selection) isn’t quite right. The effect of development is both more subtle and more interesting: developmental mechanisms bias the landscape for selection, and help to determine which features evolve.&lt;/p&gt;
    &lt;p&gt;Such studies are exciting as they help to make evolutionary biology a more predictive science. Why, then, have these ideas received comparatively little attention until recently? We come back to conceptual frameworks. Historically, evolutionary biologists have treated bias in phenotypic variation as a ‘constraint’ – an explanation for why evolution or adaptation has not occurred. The way that organisms grow restricts what sorts of features it is possible or adaptive to possess. Traditionally minded evolutionists have been far more reticent to embrace a positive role for development as a cause of evolutionary direction and change.&lt;/p&gt;
    &lt;p&gt;It took a different perspective (in this instance, that of evolutionary developmental biology, so-called ‘evo devo’), to motivate this kind of experimentation. From the evo-devo perspective, bias partly explains what evolution and adaptation has occurred. Rodents’ teeth and fishes’ bodies look the way they do because the way that creatures grow makes those characteristics more likely to arise. Bias thus becomes a much more significant concept in evolutionary explanation. By bringing the phenomenon to the fore, the EES hopes it will be investigated.&lt;/p&gt;
    &lt;p&gt;The EES, at least as my collaborators and I frame it, is best viewed as an alternative research programme for evolutionary biology. Inspired by recent findings emerging within evolutionary biology and adjacent fields, the EES starts from the assumption that developmental processes play important roles as causes of novel (and potentially beneficial) phenotypic variation, causes of differences in fitness of those variants, and causes of inheritance. In contrast to how evolution has traditionally been conceived, in the EES the burden of creativity in evolution does not rest on natural selection alone. This alternative way of thinking is being used to generate fresh hypotheses and establish new research agendas. It’s early days, but there are already signs that this research is starting to yield dividends.&lt;/p&gt;
    &lt;p&gt;If evolution is not to be explained solely in terms of changes in gene frequencies; if previously rejected mechanisms such as the inheritance of acquired characteristics turn out to be important after all; and if organisms are acknowledged to bias evolution through development, learning and other forms of plasticity – does all this mean a radically different and profoundly richer account of evolution is emerging? No one knows: but from the perspective of our adapting dog-walker, evolution is looking less like a gentle genetic stroll, and more like a frantic struggle by genes to keep up with strident developmental processes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46671191</guid><pubDate>Sun, 18 Jan 2026 19:21:14 +0000</pubDate></item><item><title>Dead Internet Theory</title><link>https://kudmitry.com/articles/dead-internet-theory/</link><description>&lt;doc fingerprint="32e0135d28e5b5bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Dead Internet Theory&lt;/head&gt;
    &lt;p&gt;The other day I was browsing my one-and-only social network — which is not a social network, but I’m tired of arguing with people online about it — HackerNews. It’s like this dark corner of the internet, where anonymous tech-enthusiasts, scientists, entrepreneurs, and internet-trolls, like to lurk. I like HackerNews. It helps me stay up-to-date about recent tech news (like Cloudflare acquiring Astro which makes me happy for the Astro team, but also sad and worried since I really like Astro, and big-tech has a tendency to ruin things); it mostly avoids politics; and it’s not a social network.&lt;/p&gt;
    &lt;p&gt;And, in the fashion of HackerNews, I stumbled upon someone sharing their open-source project. It’s great to see people work on their projects and decide to show them to the world. I think people underestimate the fear of actually shipping stuff, which involves sharing it with the world.&lt;/p&gt;
    &lt;p&gt;Upon glancing at the comment section, I started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was AI-generated. I grabbed my popcorn, and started to follow this thread. More accusations started to appear: the commit timeline does not make sense; the code has AI-generated comments; etc. And at the same time, the author tried to reply to every comment claiming that they wrote this 100% without using AI.&lt;/p&gt;
    &lt;p&gt;I don’t mind people using AI to write code, even though I tried to resist it myself, until eventually succumbing to it. But I think it’s fair to disclose the use of AI, especially in open-source software. People on the internet are, mostly, anonymous, and it’s not always possible to verify the claims or expertise of particular individuals. But as the amount of code is growing, considering that everyone is using AI to generate whatever-app they want, it’s impossible to verify every piece of code we are going to use. So it’s fair to know, I think, if some project is AI generated and to what extent. In the end, LLMs are just probabilistic next-token generators. And while they are getting extremely good at most simple tasks, they have the potential to wreak havoc with harder problems or edge-cases (especially if there are no experienced engineers, with domain knowledge, to review the generated code).&lt;/p&gt;
    &lt;p&gt;As I was following this thread, I stared to see a pattern: the comments of the author looked AI generated too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The use of em-dashes, which on most keyboard require a special key-combination that most people don’t know, and while in markdown two dashes will render as em-dash, this is not true of HackerNews (hence, you often see &lt;code&gt;--&lt;/code&gt;in HackerNews comments, where the author is probably used to Markdown renderer turning it into em-dash)&lt;/item&gt;
      &lt;item&gt;The notorious “you are absolutely right”, which no living human ever used before, at least not that I know of&lt;/item&gt;
      &lt;item&gt;The other notorious “let me know if you want to [do that thing] or [explore this other thing]” at the end of the sentence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was sitting there, refreshing the page, seeing the author being confronted with use of AI in both their code and their comments, while the author claiming to have not used AI at all. Honestly, I was thinking I was going insane. Am I wrong to suspect them? What if people DO USE em-dashes in real life? What if English is not their native language and in their native language it’s fine to use phrases like “you are absolutely right”? Is this even a real person? Are the people who are commenting real?&lt;/p&gt;
    &lt;p&gt;And then it hit me. We have reached the Dead Internet. The Dead Internet Theory claims that since around 2016 (a whooping 10 years already), the internet is mainly dead, i.e. most interactions are between bots, and most content is machine generated to either sell you stuff, or game the SEO game (in order to sell you stuff).&lt;/p&gt;
    &lt;p&gt;I’m &lt;del&gt;ashamed&lt;/del&gt; proud to say that I spent a good portion of my teenage years on the internet, chatting and learning from real people who knew more than me. Back in the early 2000s, there were barely bots on the internet. The average non-tech human didn’t know anything about phpBB forums, and the weird people with pseudonyms who hanged-out in there. I spent countless hours inside IRC channels, and on phpBB forums, learning things like network programming, OS-development, game-development, and of course web-development (which became my profession for almost two decades now). I’m basically a graduate of the Internet University. Back then, nobody had doubts that they were talking to a human-being. Sure, you could think that you spoke to a hot girl, who in reality was a fat guy, but hey, at least they were real!&lt;/p&gt;
    &lt;p&gt;But today, I no longer know what is real. I saw a picture on LinkedIn, from a real tech company, posting about their “office vibes” and their happy employees. And then I went to the comment section, and sure enough this picture is AI generated (mangled text that does not make sense, weird hand artifacts). It was posted by an employee of the company, it showed other employees of said company, and it was altered with AI to showcase a different reality. Hell, maybe the people on the picture do not even exist!&lt;/p&gt;
    &lt;p&gt;And these are mild examples. I don’t use social networks (and no, HackerNews is not a social network), but I hear horror stories about AI generated content on Facebook, Xitter, TikTok, ranging from photos of giants that built the pyramids in Egypt, all the way to short videos of pretty girls saying that the EU is bad for Poland.&lt;/p&gt;
    &lt;p&gt;I honestly got sad that day. Hopeless, if I could say. AI is easily available to the masses, which allow them to generate shitload of AI-slop. People no longer need to write comments or code, they can just feed this to AI agents who will generate the next “you are absolutely right” masterpiece.&lt;/p&gt;
    &lt;p&gt;I like technology. I like software engineering, and the concept of the internet where people could share knowledge and create communities. Were there malicious actors back then on the internet? For sure. But what I am seeing today, makes me question whether the future we are headed to is a future where technology is useful anymore. Or, rather, it’s a future where bots talk with bots, and human knowledge just gets recycled and repackaged into “10 step to fix your [daily problem] you are having” for the sake of selling you more stuff.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46671731</guid><pubDate>Sun, 18 Jan 2026 20:19:07 +0000</pubDate></item><item><title>Show HN: Dock – Slack minus the bloat, tax, and 90-day memory loss</title><link>https://getdock.io/</link><description>&lt;doc fingerprint="2c22189aa3f9fafa"&gt;
  &lt;main&gt;&lt;p&gt; Your time. Your decisions. Your sanity.&lt;lb/&gt; Team chat that just works. &lt;/p&gt;&lt;code&gt;main&lt;/code&gt;?&lt;p&gt;Async messages for deep work. Real-time chat when it matters. Work across timezones without the noise.&lt;/p&gt;&lt;p&gt;Decisions get lost in chat. Not here. One click to mark, instant recall months later from your Decisions inbox.&lt;/p&gt;&lt;p&gt;SOC 2 compliant infrastructure. Your data encrypted in transit and at rest. One-click data import/export. Your data stays yours.&lt;lb/&gt;Learn more →&lt;/p&gt;&lt;p&gt;The feature Slack charges $8.75/user for? Free on Dock.&lt;/p&gt;&lt;p&gt;Find any message, from any time. No 90-day cutoff. No upgrade required. Your entire team history, always at your fingertips — completely free.&lt;/p&gt;&lt;p&gt;No learning curve. No feature overwhelm. Create a channel, invite your team, start talking. It's chat — we didn't reinvent it.&lt;/p&gt;&lt;p&gt;90-day message limit. Hidden AI costs. Features you never asked for.&lt;/p&gt;&lt;p&gt;Just to search your old messages. The "Pro" tax for basic functionality.&lt;/p&gt;&lt;p&gt;Search everything. Forever. No 90-day limit. No upgrade wall. Just free.&lt;/p&gt;&lt;p&gt;You're paying for Slack AI whether you use it or not. It's in the price.&lt;/p&gt;&lt;p&gt;We don't bundle AI you didn't ask for. Chat is chat. Pay for what you use.&lt;/p&gt;&lt;p&gt;Workflows, canvases, clips, huddles, lists... When did chat get this complicated?&lt;/p&gt;&lt;p&gt;Channels. DMs. Threads. Files. That's it. Fast, focused, done.&lt;/p&gt;&lt;p&gt;No surprises when your team grows. One price for your whole team.&lt;/p&gt;&lt;p&gt;100+ members? Contact us&lt;/p&gt;&lt;p&gt;Have questions? Read our FAQ →&lt;/p&gt;&lt;p&gt;Join the waitlist. Team chat that respects your time and budget.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46671952</guid><pubDate>Sun, 18 Jan 2026 20:42:49 +0000</pubDate></item><item><title>Stirling Cycle Machine Analysis</title><link>https://ohioopen.library.ohio.edu/opentextbooks/9/</link><description>&lt;doc fingerprint="13d6d2d304ba8708"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;College&lt;/head&gt;
    &lt;p&gt;Russ College of Engineering and Technology&lt;/p&gt;
    &lt;head rend="h2"&gt;Files&lt;/head&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Dedicated to William T. Beale (1928 - 2016), inventor of the Free Piston Stirling Engine, Mentor and Frien.&lt;/p&gt;
    &lt;p&gt;This web resource is intended to be totally self contained learning resource for the analysis and development of computer simulation of single phase, piston/cylinder Stirling cycle machines. It includes thermodynamic, heat transfer and fluid flow friction analysis, and until 2012 it was used as resource material for an advanced course for Mechanical Engineering majors. The course structure was based on the book by I.Urieli &amp;amp; D.M.Berchowitz 'Stirling Cycle Engine Analysis' (Adam Hilger, 1984). The computer simulation program modules (originally written in FORTRAN) have all been updated and rewritten in MATLAB, a convenient interactive language which allows direct graphical output - essential for Stirling cycle analysis. A complete set of all the m-files are developed and provided, and they can be augmented and adapted as needed for specific engine/refrigerator configurations.&lt;/p&gt;
    &lt;p&gt;It is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International license and as such is freely available. Comments and constructive criticism are welcomed by the author.&lt;/p&gt;
    &lt;p&gt;Chapter 1: Background and Introduction&lt;/p&gt;
    &lt;p&gt;Chapter 2: Basic Engine Configurations&lt;/p&gt;
    &lt;p&gt;Chapter 3: Ideal Isothermal Analysis&lt;/p&gt;
    &lt;p&gt;We define and analyze the Ideal Isothermal model of a Stirling engine, including the Schmidt Analysis, and discuss its limitations. One obviously incorrect conclusion of this analysis is that all three heat exchangers are redundant, and only contribute dead space, since all required heat transfer processes occur in the isothermal compression and expansion spaces. Nevertheless we can obtain a better understanding of a specific design, particularly when we augment the solution with Allan Organ's particle mass flow analysis.&lt;/p&gt;
    &lt;p&gt;Chapter 4: Ideal Adiabatic Analysis&lt;/p&gt;
    &lt;p&gt;We find that the Ideal Isothermal analysis predicts that the heat exchangers of a Stirling engine are redundant, thus we cannot seriously use this model to predict the ideal performance of an actual machine. We thus turn to an alternative model in which the compression and expansion spaces are adiabatic. We find that there is no closed form solution to this model and we have to resort to computer simulation. We gain various insights from using this model in particular with regards to the importance of the regenerator, which was not understood for a significant period.&lt;/p&gt;
    &lt;p&gt;Chapter 5: Simple Analysis&lt;/p&gt;
    &lt;p&gt;This analysis approach uses the Ideal Adiabatic model as a basis to predict the real performance of the three heat exchanger sections, particularly with regards to heat transfer and pressure drop. The name Simple Analysis is to indicate that this is a simplification of the actual non-steady flow heat exchange, however it enables a parametric analysis of a specific machine.&lt;/p&gt;
    &lt;p&gt;This learning resource includes a set of tutorial MATLAB computer program modules for simulating specific Stirling engine configurations. The complete set of m-files can be downloaded in compressed format sea.zip (sea = stirling engine analysis). These modules can be augmented and adapted as required to simulate a specific engine design. Currently the engine modules are for Alpha machines, including a Sinusoidal drive, a Ross Yoke-drive and a Ross Rocker-V engine. The heat exchanger types include tubular, annular gap, and slot heat exchangers, and the regenerator matrix types include screen mesh and rolled foil matrices. Working gas types include air, helium, and hydrogen.&lt;/p&gt;
    &lt;p&gt;Note that the purpose of this learning resource is to develop an appreciation and understanding of the complexity of practical Stirling cycle machine performance simulation, mainly due to the heat transfer processes. It is not intended as an alternative to the Sage Software for engineering modeling and optimization of Stirling cycle machines.&lt;/p&gt;
    &lt;head rend="h2"&gt;Publication Date&lt;/head&gt;
    &lt;p&gt;12-12-2020&lt;/p&gt;
    &lt;head rend="h2"&gt;Publisher&lt;/head&gt;
    &lt;p&gt;Israel Urieli&lt;/p&gt;
    &lt;head rend="h2"&gt;City&lt;/head&gt;
    &lt;p&gt;Athens&lt;/p&gt;
    &lt;head rend="h2"&gt;Keywords&lt;/head&gt;
    &lt;p&gt;Stirling engines, thermal analysis&lt;/p&gt;
    &lt;head rend="h2"&gt;Disciplines&lt;/head&gt;
    &lt;p&gt;Engineering&lt;/p&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;This repository version of Stirling Cycle Machine Analysis by Israel Urieli is a PDF version of the original website (https://people.ohio.edu/trembly/mechanical/stirling/) which may not be accessible. Please visit the Internet Archive's Wayback Machine for archived versions of the website.&lt;/p&gt;
    &lt;head rend="h2"&gt;Creative Commons License&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt;This work is licensed under a Creative Commons Attribution-NonCommercial-Share Alike 4.0 International License.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recommended Citation&lt;/head&gt;
    &lt;p&gt; Urieli, Israel, "Stirling Cycle Machine Analysis" (2020). OHIO Open Faculty Textbooks. 9. &lt;lb/&gt; https://ohioopen.library.ohio.edu/opentextbooks/9 &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46671982</guid><pubDate>Sun, 18 Jan 2026 20:45:54 +0000</pubDate></item><item><title>Police Invested Millions in Shadowy Phone-Tracking Software Won't Say How Used</title><link>https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46672150</guid><pubDate>Sun, 18 Jan 2026 21:05:14 +0000</pubDate></item><item><title>Show HN: Beats, a web-based drum machine</title><link>https://beats.lasagna.pizza</link><description>&lt;doc fingerprint="32e55a1cb8f697b3"&gt;
  &lt;main&gt;
    &lt;p&gt;Share your beat with this URL:&lt;/p&gt;
    &lt;p&gt;BEATS&lt;/p&gt;
    &lt;p&gt;A web-based drum machine inspired by the Teenage Engineering Pocket Operators.&lt;/p&gt;
    &lt;p&gt;CREDITS:&lt;/p&gt;
    &lt;p&gt;• Wrote by @kinduff&lt;/p&gt;
    &lt;p&gt;• Built with Tone.js and Stimulus.js&lt;/p&gt;
    &lt;p&gt;• With the awesome VT323 font&lt;/p&gt;
    &lt;p&gt;THANKS TO:&lt;/p&gt;
    &lt;p&gt;• andiam03 for transposing patterns and inspiring!&lt;/p&gt;
    &lt;p&gt;• ethanhein for the original idea!&lt;/p&gt;
    &lt;p&gt;• all beta reviewers&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46672181</guid><pubDate>Sun, 18 Jan 2026 21:10:08 +0000</pubDate></item><item><title>EU to Retaliate on Tariffs</title><link>https://www.spiegel.de/politik/groenland-plaene-der-usa-eu-plant-gegenzoelle-im-wert-von-93-milliarden-euro-a-2262fda5-0071-4352-bad5-289e8ca37cf9</link><description>&lt;doc fingerprint="f2b9e48e8b084ffe"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Reaktion auf Grönland-Pläne der USA EU plant Gegenzölle im Wert von 93 Milliarden Euro&lt;/head&gt;
    &lt;p&gt;Die Europäische Union prüft Vergeltungsmaßnahmen gegen die USA. Geplant sind Strafzölle in Höhe von 93 Milliarden Euro oder Einschränkungen für amerikanische Unternehmen auf dem europäischen Markt. Die Krise gilt als die schwerste in den transatlantischen Beziehungen seit Jahrzehnten. Zuerst berichtete die »Financial Times«.&lt;/p&gt;
    &lt;p&gt;Hintergrund ist die Drohung von US-Präsident Donald Trump, zusätzliche Zölle gegen Dänemark, Deutschland, Norwegen, Schweden, Frankreich, Großbritannien, die Niederlande und Finnland zu verhängen, falls die USA Grönland nicht kaufen können.&lt;/p&gt;
    &lt;head rend="h3"&gt;EU-Sondergipfel geplant&lt;/head&gt;
    &lt;p&gt;Wie ein EU-Diplomat dem SPIEGEL sagt, seien sich die Mitgliedsstaaten bei einem Treffen der ständigen Vertreter in Brüssel einig gewesen, dass eine Einführung solcher Zölle durch die USA zu weit gehen würde und die EU reagieren müsse. Am Sonntagabend hatten sich die EU-Botschafter noch nicht auf die Verhängung neuer Gegenzölle gegen die USA geeinigt. Das Treffen der Botschafter sei jedoch »gut verlaufen«.&lt;/p&gt;
    &lt;p&gt;Der EU-Ratspräsident António Costa will in den kommenden Tagen einen Sonder-Gipfel der EU-Staats- und Regierungschefs einberufen. Dabei solle die Reaktion der EU Trumps Zoll-Androhung erörtert werden, schreibt Costa in einem Eintrag auf der Plattform X. Seine Konsultationen mit den EU-Mitgliedern hätten gezeigt, dass diese sich stark für Dänemark und Grönland einsetzten und bereit seien, sich gegen jede Form von Zwang zu wehren. Dabei seien sie weiterhin bereit, konstruktiv mit den USA zusammenzuarbeiten. Ein EU-Insider sagte gegenüber der Nachrichtenagentur Reuters, der Gipfel werde wahrscheinlich am Donnerstag stattfinden. Es sei ein persönliches Treffen geplant.&lt;/p&gt;
    &lt;head rend="h3"&gt;Handelsabkommen auf unbestimmte Zeit verschoben&lt;/head&gt;
    &lt;p&gt;Wenn es keine Einigung mit den USA in der neuesten Runde des Zollstreits wegen Grönland geben sollte, würden die bereits im Juli designierten Gegen-Strafzölle im Volumen von 93 Milliarden Euro etwa für US-Bourbon, Flugzeugteile, Sojabohnen und Geflügel automatisch am 6. Februar in Kraft treten, sagte ein EU-Diplomat der Nachrichtenagentur Reuters.&lt;/p&gt;
    &lt;p&gt;Die EU hatte die Verhängung dieser Zölle ausgesetzt, nachdem sie sich mit den USA auf ein Handelsabkommen geeinigt hatte. Dieses sah eigentlich Zollfreiheit für US-Waren und einen 15-Prozent-Zoll auf EU-Waren in den USA vor. Das Europäische Parlament hat nach Angaben von EVP-Chef Manfred Weber aber jetzt die nötige und für Mittwoch vorgesehene Verabschiedung dieses Abkommens auf unbestimmte Zeit verschoben.&lt;/p&gt;
    &lt;p&gt;Auslöser war die Ankündigung von US-Präsident Donald Trump, ab dem 1. Februar einen zusätzlichen 10-prozentigen Zoll auf Waren aus bestimmten EU-Ländern zu erheben, wenn sich die EU gegen einen US-Kauf des zu Dänemark gehörenden Grönlands wehrt. Betroffen von der Zoll-Androhung sind acht europäische Nato-Länder, die jüngst Soldaten zu einer Erkundungsmission auf die Insel geschickt haben, darunter Deutschland.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46672714</guid><pubDate>Sun, 18 Jan 2026 22:16:46 +0000</pubDate></item></channel></rss>