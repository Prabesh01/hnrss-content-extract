<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 14 Oct 2025 03:45:33 +0000</lastBuildDate><item><title>Why did containers happen?</title><link>https://buttondown.com/justincormack/archive/ignore-previous-directions-8-devopsdays/</link><description>&lt;doc fingerprint="ef09185191a7af1a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ignore previous directions 8: devopsdays&lt;/head&gt;
    &lt;head rend="h2"&gt;Autumn update&lt;/head&gt;
    &lt;p&gt;This is what it is looking like around here at the moment.&lt;/p&gt;
    &lt;head rend="h2"&gt;DevOpsDays London&lt;/head&gt;
    &lt;p&gt;I gave a talk at DevOpsDays London recently. It was a nice conference, and thanks to all the organizers for all their work.&lt;/p&gt;
    &lt;p&gt;The video is here https://www.youtube.com/watch?v=eMU2mZgo99c&lt;/p&gt;
    &lt;p&gt;Below is my rough outline for the talk, it differs a bit from what I actually said!&lt;/p&gt;
    &lt;head rend="h3"&gt;Why did containers happen?&lt;/head&gt;
    &lt;p&gt;A few years ago, I spent a bunch of time answering questions from the FTC about Broadcom's acquisition of VMware. They wanted to know if containers were a competitor to virtual machines, as they were trying to understand the competitive landscape around VMware.&lt;/p&gt;
    &lt;p&gt;It reminded me of the first five years at Docker, where everyone wanted to compare containers with VMs. Were containers just lightweight VMs? Weren't containers just insecure and people would go back to good old VMs?&lt;/p&gt;
    &lt;p&gt;The story I told to the FTC was that these innovations had come out of different growth periods. VMs were there to help manage when organisations suddenly got a lot more computers. These tended to be poorly managed, because the process was very manual, and most had poor utilisation (under 15%). They had to be installed manually which took ages. Consolidation saved money on hardware and on Windows server licences.&lt;/p&gt;
    &lt;p&gt;In the Linux world, this was somewhat less of an issue, as we were better at running multiple applications on the same server, although a lot of servers were still underutilised.&lt;/p&gt;
    &lt;p&gt;Containers though were there to solve a follow on problem, not having too many computers, but having too many applications, and needing a tool to manage them. Companies were hiring more and more developers and they were writing more and more applications. Dotcloud was a PaaS company and was exposed to this, and created Docker to manage deployment of the applications on its platform. It wasn't the isolation that was important it was the packaging.&lt;/p&gt;
    &lt;p&gt;That was my explanation anyway. For enterprises though, containers were part of the move to the cloud, they didn't want lift and shift of inefficient VMs to the cloud. In the early days Microsoft used to call up our customers and say they could convert their data centre to an Azure one, and they would never notice, all the VMs would be just the same but running in Azure. Forcing a move to containers alongside the cloud was a way to force some modernisation, and a move from Windows to Linux in many cases.&lt;/p&gt;
    &lt;head rend="h3"&gt;Change budget&lt;/head&gt;
    &lt;p&gt;Docker was easy to adopt as it did not change very much about how you used software.&lt;/p&gt;
    &lt;p&gt;There was one key innovation, which was Docker Hub, having a registry of shareable images. GitHub but you can run it. VMs never really had this, the closest was Vagrant Cloud perhaps, but sharing does not work well with fully configured images (and they were huge). For something to be reusable by lots of people, it is no use it being in a finally configured state, with all the configuration of the exact use case applied. The less specific they are the more widely they can be used. VM images became a bit more reusable with tools like Cloud Init that removed some configuration, but they are still much more specific than more fine grained components like container images. And VM images were big, and networks were slower. LLMs are bigger than VM images were but thats another story.&lt;/p&gt;
    &lt;p&gt;As well as one innovation there was one forbidden thing, Docker made people rebuild images and redeploy, rather than updating in place. That worked because the scope was a single application so this was more manageable. And maybe because we never told anyone you could update in production. I was always surprised someone didn't invent a tool for ftping to your container and updating the PHP. Immutability is a great thing that has a lot of useful security properties, most of which haven't really been realised, but this simplified deployment, of which more later.&lt;/p&gt;
    &lt;p&gt;Docker also made Go credible as a programming language, and now pretty much all modern languages have a TLS stack as part of the standard library. Before Docker, Youtube was the main user for Go, now it is the fourth most popular language in containers, after Node, Java and Python.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kubernetes&lt;/head&gt;
    &lt;p&gt;I remember in the early container days, before Kubernetes and when Kubernetes was very new, people still thought container orchestration was about scheduling. We would have whole conference tracks about schedulers. But when you went to talk to the early users of Kubernetes they were just trying to write deployment scripts.&lt;/p&gt;
    &lt;p&gt;Docker Swarm did not allow you to write deployment scripts. The security team had decided that the security model would be broken if you could deploy from within the cluster. For years the commercial product we sold had the worst deployment story you could imagine, pasting Yaml files into a text box on a web page. The whole company culture really ignored deployment. But deployment was really what everyone wanted to do with Kubernetes, for years. We got real deployment tools, and deployment philosophies, like GitOps.&lt;/p&gt;
    &lt;p&gt;Another thing people would ask constantly in the early days is whether people would ever run databases in containers, or on Kubernetes. Somehow at about this time people started to ask why they were running databases at all, and decided that if the downside was losing all your data and the upside was saving a little money that they would rather get a cloud provider to run the database after all. I do wonder how much this was because container storage seems so ephemeral and easy to delete. Having containers be so simple to delete means that the chores of managing lifecycle for things that have state are very different. And there were just a lot of choices in the storage stacks, is it NFS or block storage or what?&lt;/p&gt;
    &lt;head rend="h3"&gt;What went wrong&lt;/head&gt;
    &lt;p&gt;The focus on deployment, and the complexity of Kubernetes killed DevOps as it once was. As a lapsed ops person who moved back to development, I always loved the bringing together communities aspect of DevOps. But over time DevOps become just a backend role and job title for people wrangling Kubernetes and other deployment technologies. Somehow it seems easier for people to relate to technology than culture, and the technology started working against the culture.&lt;/p&gt;
    &lt;p&gt;Docker didn't really change development. For a while it looked like it might take over the role of Vagrant in building up local development environments, but although people at Docker made heroic efforts to make developing in containers nice, no one really does that, except kind of sort of in a cloud environment, but thats really closer to a remote Linux box. Python and Ruby cleaned up their virtual environment tooling, and if you really want reproducible local development environments you can use Nix. What people do with Docker is spin up a database or another service to develop or test against.&lt;/p&gt;
    &lt;p&gt;Application composition from open source components became the dominant way of constructing applications over the last decade. But this was largely supported by language package managers, that are all very different. We didn't end up with a universal build abstraction, and immutability was great to help you know what is running, but the scale of dependencies and applications conspired to make this not as useful as we thought.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where are we now?&lt;/head&gt;
    &lt;p&gt;We started off with virtualisation being introduced because hardware was only being used at 15% of capacity. According to the 2024 Datadog report on the State of Cloud Costs "83 percent of container costs are associated with idle resources". This really shows how much more accurately the technology we have built can measure wastage.&lt;/p&gt;
    &lt;p&gt;The compute we are wasting is at least 10x cheaper, but we have automation to waste it at scale now. Much of the usage of containers has been to drive applications for mobile phones, and those mobile phone CPUs, adapted as Arm servers are being used to run the applications.&lt;/p&gt;
    &lt;p&gt;We have ended up with pockets of efficiency, where things are done at sufficient scale, and a long tail of inefficiency that remains the same after a decade. AI has shown too that we can make huge application improvements if they are expensive enough, with the cost of inference falling at an extremely rapid rate.&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking forward&lt;/head&gt;
    &lt;p&gt;The "Choose Boring Technology" essay was written in 2015, and containers back then were definitely not a boring technology, although the mentioned examples of not boring were Consul and MongoDB. Boring technologies were MySQL, Postgres, PHP, Python, Memcached, Squid and Cron. Now? ChatGPT told me that Docker is "mostly boring" while Kubernetes is "moving towards boring".&lt;/p&gt;
    &lt;p&gt;Choosing boring is becoming part of the culture now, it has taken a decade. Maybe AI has attracted all the change budget, combined with the end of the cloud native ZIRP startup era. LLMs are good at boring technology, being trained on our culture too.&lt;/p&gt;
    &lt;p&gt;If we want something else we will have to add back a change budget.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45567241</guid><pubDate>Mon, 13 Oct 2025 11:37:55 +0000</pubDate></item><item><title>Show HN: SQLite Online – 11 years of solo development, 11K daily users</title><link>https://sqliteonline.com/</link><description>&lt;doc fingerprint="5120987566fd4fbd"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;Chart for Data Science&lt;/head&gt;
    &lt;code&gt;-- Change first word "SELECT" to "QLINE-SELECT"&lt;/code&gt;
    &lt;quote&gt;SELECT QLINE-SELECT&lt;/quote&gt;
    &lt;code&gt;â&lt;/code&gt;
    &lt;code&gt;-- Axis X:&lt;/code&gt;
    &lt;code&gt;-- X - column name, axis: x1, x2, ..xn Value: Number&lt;/code&gt;
    &lt;code&gt;-- L - column name, axis: l Value: Text&lt;/code&gt;
    &lt;code&gt;-- T - column name, axis: t Value: UnixTime Number&lt;/code&gt;
    &lt;code&gt;-- Axis Y:&lt;/code&gt;
    &lt;code&gt;-- Y - column name, axis: y1, y2, ..yn Value: Number&lt;/code&gt;
    &lt;code&gt;-- Y - color line: y_cFF00FF (HEX6)&lt;/code&gt;
    &lt;code&gt;-- Option:&lt;/code&gt;
    &lt;code&gt;-- C - color point: c  Value: FF00FF (HEX6)&lt;/code&gt;
    &lt;code&gt;-- V - radius point: v  Value: Number&lt;/code&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QLINE-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QAREA-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QBAR-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QPIE-SELECT example&lt;/quote&gt;
    &lt;code&gt;-- Example : &lt;/code&gt;
    &lt;quote&gt;QBUBBLE-SELECT example&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45567770</guid><pubDate>Mon, 13 Oct 2025 12:46:52 +0000</pubDate></item><item><title>No science, no startups: The innovation engine we're switching off</title><link>https://steveblank.com/2025/10/13/no-science-no-startups-the-unseen-engine-were-switching-off/</link><description>&lt;doc fingerprint="3fc8918d9d618749"&gt;
  &lt;main&gt;
    &lt;p&gt;Tons of words have been written about the Trump Administrations war on Science in Universities. But few people have asked what, exactly, is science? How does it work? Who are the scientists? What do they do? And more importantly, why should anyone (outside of universities) care?&lt;/p&gt;
    &lt;p&gt;(Unfortunately, you won’t see answers to these questions in the general press – it’s not clickbait enough. Nor will you read about it in the science journals– it’s not technical enough. You won’t hear a succinct description from any of the universities under fire, either – they’ve long lost the ability to connect the value of their work to the day-to-day life of the general public.)&lt;/p&gt;
    &lt;p&gt;In this post I’m going to describe how science works, how science and engineering have worked together to build innovative startups and companies in the U.S.—and why you should care.&lt;/p&gt;
    &lt;p&gt;(In a previous post I described how the U.S. built a science and technology ecosystem and why investment in science is directly correlated with a country’s national power. I suggest you read it first.)&lt;/p&gt;
    &lt;p&gt;How Science Works&lt;lb/&gt; I was older than I care to admit when I finally understood the difference between a scientist, an engineer, an entrepreneur and a venture capitalist; and the role that each played in the creation of advancements that made our economy thrive, our defense strong and America great.&lt;/p&gt;
    &lt;p&gt;Scientists&lt;lb/&gt; Scientists (sometimes called researchers) are the people who ask lots of questions about why and how things work. They don’t know the answers. Scientists are driven by curiosity, willing to make educated guesses (the fancy word is hypotheses) and run experiments to test their guesses. Most of the time their hypotheses are wrong. But every time they’re right they move the human race forward. We get new medicines, cures for diseases, new consumer goods, better and cheaper foods, etc.&lt;/p&gt;
    &lt;p&gt;Scientists tend to specialize in one area – biology, medical research, physics, agriculture, computer science, materials, math, etc. — although a few move between areas. The U.S. government has supported scientific research at scale (read billions of $s) since 1940.&lt;/p&gt;
    &lt;p&gt;Scientists tend to fall into two categories: Theorists and Experimentalists.&lt;/p&gt;
    &lt;p&gt;Theorists&lt;lb/&gt; Theorists develop mathematical models, abstract frameworks, and hypotheses for how the universe works. They don’t run experiments themselves—instead, they propose new ideas or principles, explain existing experimental results, predict phenomena that haven’t been observed yet. Theorists help define what reality might be.&lt;/p&gt;
    &lt;p&gt;Theorists can be found in different fields of science. For example:&lt;/p&gt;
    &lt;p&gt;Physics Quantum field theory, string theory, quantum mechanics&lt;lb/&gt; Biology Neuroscience and cognition, Systems Biology, gene regulation&lt;lb/&gt; Chemistry Molecular dynamics, Quantum chemistry&lt;lb/&gt; Computer Science Design algorithms, prove limits of computation&lt;lb/&gt; Economics Build models of markets or decision-making&lt;lb/&gt; Mathematics Causal inference, Bayesian networks, Deep Learning&lt;/p&gt;
    &lt;p&gt;The best-known 20th-century theorist was Albert Einstein. His tools were a chalkboard and his brain. in 1905 he wrote an equation E=MC2 which told the world that a small amount of mass can be converted into a tremendous amount of energy. When he wrote it down, it was just theory. Other theorists in the 1930s and ’40s took Einstein’s theory and provided the impetus for building the atomic bomb. (Leo Szilard conceived neutron chain reaction idea, Hans Bethe led the Theoretical Division at Los Alamos, Edward Teller developed hydrogen bomb theory.) Einstein’s theory was demonstrably proved correct over Hiroshima and Nagasaki.&lt;/p&gt;
    &lt;p&gt;Experimentalists&lt;lb/&gt; In addition to theorists, other scientists – called experimentalists – design and run experiments in a lab. The pictures you see of scientists in lab coats in front of microscopes, test tubes, particle accelerators or NASA spacecraft are likely experimentalists. They test hypotheses by developing and performing experiments. An example of this would be NASA’s James Webb telescope or the LIGO Gravitational-Wave Observatory experiment. (As we’ll see later, often it’s engineers who build the devices the experimentalists use.)&lt;/p&gt;
    &lt;p&gt;Some of these experimentalists focus on Basic Science, working to get knowledge for its own sake and understand fundamental principles of nature with no immediate practical use in mind.&lt;/p&gt;
    &lt;p&gt;Other experimentalists work in Applied Science, which uses the findings and theories derived from Basic Science to design, innovate, and improve products and processes.&lt;/p&gt;
    &lt;p&gt;Applied scientists solve practical problems oriented toward real-world applications. (Scientists at Los Alamos weretrying to understand the critical mass of U-235 (the minimum amount that would explode.) Basic science lays the groundwork for breakthroughs in applied science. For instance: Quantum mechanics (basic science) led to semiconductors which led to computers (applied science). Germ theory (basic science) led to antibiotics and vaccines (applied science). In the 20th century Applied scientists did not start the companies that make end products. Engineers and entrepreneurs did this. (In the 21st century more Applied Scientists, particularly in life sciences, have also spun out companies from their labs.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Scientists&lt;/head&gt;
    &lt;p&gt;&lt;lb/&gt; Where is Science in the U.S. Done?&lt;lb/&gt; America’s unique insight that has allowed it to dominate Science and invention, is that after WWII we gave Research and Development money to universities, rather than only funding government laboratories. No other country did this at scale.&lt;/p&gt;
    &lt;p&gt;Corporate Research Centers&lt;lb/&gt; In the 20th century, U.S. companies put their excess profits into corporate research labs. Basic research in the U.S. was done in at Dupont, Bell Labs, IBM, AT&amp;amp;T, Xerox, Kodak, GE, et al.&lt;/p&gt;
    &lt;p&gt;This changed in 1982, when the Securities and Exchange Commission ruled that it was legal for companies to buy their own stock (reducing the number of shares available to the public and inflating their stock price.) Very quickly Basic Science in corporate research all but disappeared. Companies focused on Applied Research to maximize shareholder value. In its place, Theory and Basic research is now done in research universities.&lt;/p&gt;
    &lt;p&gt;Research Universities&lt;lb/&gt; From the outside (or if you’re an undergraduate) universities look like a place where students take classes and get a degree. However, in a research university there is something equally important going on. Science faculty in these schools not only teach, but they are expected to produce new knowledge—through experiments, publications, patents, or creative work. Professors get grants and contracts from federal agencies (e.g., NSF, NIH, DoD), foundations, and industry. And the university builds Labs, centers, libraries, and advanced computing facilities that support these activities.&lt;/p&gt;
    &lt;p&gt;In the U.S. there are 542 research universities, ranked by the Carnegie Classification into three categories.&lt;/p&gt;
    &lt;p&gt;R1: 187 Universities – Very High Research Activity&lt;lb/&gt; Conduct extensive research and award many doctoral degrees.&lt;lb/&gt; Examples: Stanford, UC Berkeley, Harvard, MIT, Michigan, Texas A&amp;amp;M …&lt;/p&gt;
    &lt;p&gt;R2: 139 Universities – High Research Activity&lt;lb/&gt; Substantial but smaller research scale.&lt;lb/&gt; Examples: Baylor, Wake Forest, UC Santa Cruz, …&lt;/p&gt;
    &lt;p&gt;R3: 216 Research Colleges/Universities&lt;lb/&gt; Limited research focus; more teaching-oriented doctoral programs.&lt;lb/&gt; Smaller state universities&lt;/p&gt;
    &lt;p&gt;Why Universities Matter to Science&lt;lb/&gt; U.S. universities perform about 50% of all basic science research (physics, chemistry, biology, social sciences, etc.) because they are training grounds for graduate students and postdocs. Universities spend ~$109 billion a year on research. ~$60 billion of that $109 billion comes from the National Institutes for Health (NIH) for biomedical research, National Science Foundation (NSF) for basic science, Department of War (DoW), Department of Energy (DOE), for energy/physics/nuclear, DARPA, NASA. (Companies tend to invest in applied research and development, that leads directly to saleable products.)&lt;/p&gt;
    &lt;p&gt;Professors (especially in Science, Technology, Engineering and Math) run labs that function like mini startups. They ask research questions, then hire grad students, postdocs, and staff and write grant proposals to fund their work, often spending 30–50% of their time writing and managing grants. When they get a grant the lead researcher (typically a faculty member/head of the lab) is called the Principal Investigator (PI).&lt;/p&gt;
    &lt;p&gt;The Labs are both workplaces and classrooms. Graduate students and Postdocs do the day-to-day science work as part of their training (often for a Ph.D.). Postdocs are full-time researchers gaining further specialization. Undergraduates may also assist in research, especially at top-tier schools.&lt;/p&gt;
    &lt;p&gt;(Up until 2025, U.S. science was deeply international with ~40–50% of U.S. basic research done by foreign-born researchers (graduate students, postdocs, and faculty). Immigration and student visas were a critical part of American research capacity.)&lt;/p&gt;
    &lt;p&gt;The results of this research are shared with the agencies that funded it, published in journals, presented at conferences and often patented or spun off into startups via technology transfer offices. A lot of commercial tech—from Google search to CRISPR—started in university labs.&lt;/p&gt;
    &lt;p&gt;Universities support their science researchers with basic administrative staff (for compliance, purchasing, and safety) but uniquely in the U.S., by providing the best research facilities (labs, cleanrooms, telescopes), and core scientific services: DNA sequencing centers, electron microscopes, access to cloud, data analysis hubs, etc. These were the best in the world – until the sweeping cuts in 2025.&lt;/p&gt;
    &lt;p&gt;Engineers Build on the Work of Scientists&lt;lb/&gt; Engineers design and build things on top of the discoveries of scientists. For example, seven years after scientists split the atom, it took 10s of thousands of engineers to build an atomic bomb. From the outset, the engineers knew what they wanted to build because of the basic and applied scientific research that came before them.&lt;/p&gt;
    &lt;p&gt;Scientists Versus Engineers&lt;/p&gt;
    &lt;p&gt;Engineers create plans, use software to test their designs, then… cut sheet metal, build rocket engines, construct buildings and bridges, design chips, build equipment for experimentalists, design cars, etc.&lt;/p&gt;
    &lt;p&gt;As an example, at Nvidia their GPU chips are built in a chip factory (TSMC) using the Applied science done by companies like Applied Materials which in turn is based on Basic science of semiconductor researchers. And the massive data centers OpenAI, Microsoft, Google, et al that use Nvidia chips are being built by mechanical and other types of engineers.&lt;/p&gt;
    &lt;p&gt;My favorite example is that the reusable SpaceX rocket landings are made possible by the Applied Science research on Convex Optimization frameworks and algorithms by Steven Boyd of Stanford. And Boyd’s work was based on the Basic science mathematical field of convex analysis (SpaceX, NASA, JPL, Blue Origin, Rocket Lab all use variations of Convex Optimization for guidance, control, and landing.)&lt;/p&gt;
    &lt;p&gt;Startup Entrepreneurs Build Iteratively and Incrementally&lt;lb/&gt; Entrepreneurs build companies to bring new products to market. They hire engineers to build, test and refine products.&lt;/p&gt;
    &lt;p&gt;Engineers and entrepreneurs operate with very different mindsets, goals, and tolerances for risk and failure. (Many great entrepreneurs start as engineers e.g., Musk, Gates, Page/Brin). An engineer’s goal is to design and deliver a solution to a known problem with a given set of specifications.&lt;/p&gt;
    &lt;p&gt;In contrast, entrepreneurs start with a series of unknowns about who are the customers, what are the wanted product features, pricing, etc. They retire each of these risks by building an iterative series of minimum viable products to find product/market fit and customer adoption. They pivot their solution as needed when they discover their initial assumptions are incorrect. (Treating each business unknown as a hypothesis is the entrepreneurs’ version of the Scientific Method.)&lt;/p&gt;
    &lt;p&gt;Venture Capitalists Fund Entrepreneurs&lt;lb/&gt; Venture capitalists (VCs) are the people who fund entrepreneurs who work with engineers who build things that applied scientists have proven from basic researchers.&lt;/p&gt;
    &lt;p&gt;Unlike banks which will give out loans for projects that have known specifications and outcomes, VCs invest in a portfolio of much riskier investments. While banks make money on the interest they charge on each loan, VCs take part ownership (equity) in the companies they invest in. While most VC investments fail, the ones that succeed make up for that.&lt;/p&gt;
    &lt;p&gt;Most VCs are not scientists. Few are engineers, some have been entrepreneurs. The best VCs understand technical trends and their investments help shape the future. VCs do not invest in science/researchers. VCs want to minimize the risk of their investment, so they mostly want to take engineering and manufacturing risk, but less so on applied science risk and rarely on basic research risk. Hence the role of government and Universities.&lt;/p&gt;
    &lt;p&gt;VCs invest in projects that can take advantage of science and deliver products within the time horizon of their funds (3–7 years). Science often needs decades before a killer app is visible.&lt;/p&gt;
    &lt;p&gt;As the flow of science-based technologies dries up, the opportunities for U.S. venture capital based on deep tech will decline, with its future in countries that are investing in science – China or Europe.&lt;/p&gt;
    &lt;p&gt;Why Have Scientists? Why Not Just a Country of Engineers, Entrepreneurs and VCs (or AI)?&lt;lb/&gt; If you’ve read so far, you might be scratching your head and asking, “Why do we have scientists at all? Why pay for people to sit around and think? Why spend money on people who run experiments when most of those experiments fail? Can’t we replace them with AI?”&lt;/p&gt;
    &lt;p&gt;The output of this university-industry-government science partnership became the foundation of Silicon Valley, the aerospace sector, the biotechnology industry, Quantum and AI. These investments gave us rockets, cures for cancer, medical devices, the Internet, Chat GPT, AI and more.&lt;/p&gt;
    &lt;p&gt;Investment in science is directly correlated with national power. Weaken science, you weaken the long-term growth of the economy, and national defense.&lt;/p&gt;
    &lt;p&gt;Tech firms’ investments of $100s of billions in AI data centers is greater than the federal government’s R&amp;amp;D expenditures. But these investments are in engineering not in science. The goal of making scientists redundant using artificial general intelligence misses the point that AI will (and is) making scientists more productive – not replacing them.&lt;/p&gt;
    &lt;p&gt;Countries that neglect science become dependent on those that don’t. U.S. post-WWII dominance came from basic science investments (OSRD, NSF, NIH, DOE labs). After WWII ended, the UK slashed science investment which allowed the U.S. to commercialize the British inventions made during the war.&lt;/p&gt;
    &lt;p&gt;The Soviet Union’s collapse partly reflected failure to convert science into sustained innovation, during the same time that U.S. universities, startups and venture capital created Silicon Valley. Long-term military and economic advantage (nuclear weapons, GPS, AI) trace back to scientific research ecosystems.&lt;/p&gt;
    &lt;p&gt;Lessons Learned&lt;/p&gt;
    &lt;quote&gt;
      &lt;item&gt;Scientists come in two categories&lt;/item&gt;
      &lt;item&gt;Theorists and experimentalists&lt;/item&gt;
      &lt;item&gt;Two types of experimentalists; Basic science (learn new things) or applied science (practical applications of the science)&lt;/item&gt;
      &lt;item&gt;Scientists train talent, create patentable inventions and solutions for national defense&lt;/item&gt;
      &lt;item&gt;Engineers design and build things on top of the discoveries of scientists&lt;/item&gt;
      &lt;item&gt;Entrepreneurs test and push the boundaries of what products could be built&lt;/item&gt;
      &lt;item&gt;Venture Capital provides the money to startups&lt;/item&gt;
      &lt;item&gt;Scientists, engineers, entrepreneurs – these roles are complementary&lt;/item&gt;
      &lt;item&gt;Remove one and the system degrades&lt;/item&gt;
      &lt;item&gt;Science won’t stop&lt;/item&gt;
      &lt;item&gt;Cut U.S. funding, then science will happen in other countries that understand its relationship to making a nation great – like China.&lt;/item&gt;
      &lt;item&gt;National power is derived from investments in Science&lt;/item&gt;
      &lt;item&gt;Reducing investment in basic and applied science makes America weak&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;Appendix – How Does Science Work? – The Scientific Method&lt;lb/&gt; Whether you were a theorist or experimentalist, for the last 500 years the way to test science was by using the scientific method. This method starts by a scientist wondering and asking, “Here’s how I think this should work, let’s test the idea.”&lt;/p&gt;
    &lt;p&gt;The goal of the scientific method is to turn a guess (in science called a hypothesis) into actual evidence. Scientists do this by first designing an experiment to test their guess/hypothesis. They then run the experiment and collect and analyze the result and ask, “Did the result validate, invalidate the hypothesis? Or did it give us completely new ideas?” Scientists build instruments and run experiments not because of what they know, but because of what they don’t know.&lt;/p&gt;
    &lt;p&gt;These experiments can be simple ones costing thousands of dollars that can be run in a university biology lab while others may require billions of dollars to build a satellite, particle accelerator or telescope. (The U.S. took the lead in Science after WWII when the government realized that funding scientists was good for the American economy and defense.)&lt;/p&gt;
    &lt;p&gt;Good science is reproducible. Scientists just don’t publish their results, but they also publish the details of how they ran their experiment. That allows other scientists to run the same experiment and see if they get the same result for themselves. That makes the scientific method self-correcting (you or others can see mistakes).&lt;/p&gt;
    &lt;p&gt;One other benefit of the scientific method is that scientists (and the people who fund them) expect most of the experiments to fail, but the failures are part of learning and discovery. They teach us what works and what doesn’t. Failure in science testing unknowns means learning and discovery.&lt;/p&gt;
    &lt;p&gt;Filed under: National Security, NIH (National Institutes of Health), NSF (National Science Foundation) |&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45567877</guid><pubDate>Mon, 13 Oct 2025 13:02:20 +0000</pubDate></item><item><title>Smartphones and being present</title><link>https://herman.bearblog.dev/being-present/</link><description>&lt;doc fingerprint="3411991d2ac390d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Smartphones and being present&lt;/head&gt;
    &lt;p&gt;I read an article yesterday, stating that on average, people spend 4 hours and 37 minutes on their phones per day1, with South Africans coming in fourth highest in the world at a whopping 5 hours and 11 minutes2.&lt;/p&gt;
    &lt;p&gt;This figure seems really high to me. If we assume people sleep roughly 8 hours per day, that means that one third of their day is spent on their phones. If we also assume people work 8 hours per day (ignoring the fact that they may be using their phones during work hours), that suggests that people spend over half of their free time (and up to 65% of it) glued to their screens.&lt;/p&gt;
    &lt;p&gt;I never wanted to carry the internet around in my pocket. It's too distracting and pulls me out of the present moment, fracturing my attention. I've tried switching to old-school black and white phones before, but always begrudgingly returned to using a smartphone due to the utility of it. The problem, however, is that it comes with too many attention sinks tucked in alongside the useful tools.&lt;/p&gt;
    &lt;p&gt;I care about living an intentional and meaningful life, nurturing relationships, having nuanced conversations, and enjoying the world around me. I don't want to spend this limited time I have on earth watching short form video and getting into arguments on Twitter.&lt;/p&gt;
    &lt;p&gt;This is what I enjoy. Picture taken yesterday in Scarborough, South Africa.&lt;/p&gt;
    &lt;p&gt;I've written at length about how I manage my digital consumption, from turning off notifications to forgoing social media entirely. The underlying premise here is that if you're trying to lose weight, you shouldn't carry cookies around in your pockets. And my phone is the bag of cookies in this metaphor.&lt;/p&gt;
    &lt;p&gt;We're wired to seek out distraction, novel information, and entertainment, and avoid boredom at all costs. But boredom is where creativity and self-reflection do their best work. It's why "all the best ideas come when you're in the shower"—we don't usually take our phones with us into the shower (yet).&lt;/p&gt;
    &lt;p&gt;According to Screen Time on my iPhone, on average I spend 30 minutes per day on it, which I think is reasonable, especially considering the most-used apps are by-and-large utility apps like banking and messages. This isn't because I have more self-control than other people. I don't think I do. It's because I know myself, and have set up my digital life to be a positive force, and not an uninspired time-sink.&lt;/p&gt;
    &lt;p&gt;There are many apps and systems to incentivise better relationships with our phones, mostly based around time limits. But these are flawed in three ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I'm an adult, I know how to circumvent these limits, and I will if motivation is low.&lt;/item&gt;
      &lt;item&gt;Time limits don't affect the underlying addiction. You don't quit smoking by only smoking certain hours of the day.&lt;/item&gt;
      &lt;item&gt;The companies that build these apps have tens of thousands of really smart people (and billions of dollars) trying to get me hooked and keep me engaged. The only way to win this game isn't by trying to beat them (I certainly can't), but by not playing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The only way I've found to have a good relationship with my phone is to make it as uninteresting as possible. The first way is to not have recommendation media (think Instagram, TikTok, and all the rest). I'm pro deleting these accounts completely, because it's really easy to re-download the apps on a whim, or visit them in-browser. However some people have found that having them on a dedicated device works by isolating those activities. Something like a tablet at home that is "the only place you're allowed to use Instagram". I can't comment too much on this route, but it seems reasonable.&lt;/p&gt;
    &lt;p&gt;My biggest time sink over the past few years has been YouTube. The algorithm knew me too well and would recommend video after engaging, but ultimately useless video. I could easily burn an entire evening watching absolute junk—leaving me feeling like I'd just wasted what could have otherwise been a beautiful sunset or a tasty home-cooked lasagne. However, at the beginning of this year I learnt that you can turn off your YouTube watch history entirely, which means no recommendations. Here's what my YouTube home screen now looks like:&lt;/p&gt;
    &lt;p&gt;Without the recommendations I very quickly run out of things to watch from the channels I'm subscribed to. It's completely changed my relationship with YouTube since I only watch the videos I actually want to watch, and none of the attention traps. You can turn off your YouTube watch history here, and auto delete your other Google history (like historic searches and navigation) here, which I think is just good practice.&lt;/p&gt;
    &lt;p&gt;I also used my adblocker, AdGuard on Safari which has a useful "block element" feature, to block the recommended videos on the right of YouTube videos. I use this feature to hide shorts as well, since I have no interest in watching them either, and YouTube intentionally makes them impossible to remove. If you're interested in a similar setup, here are the selectors I use to block those elements:&lt;/p&gt;
    &lt;code&gt;youtube.com###items &amp;gt; ytd-item-section-renderer.style-scope.ytd-watch-next-secondary-results-renderer:last-child
youtube.com###sections
youtube.com##[is-shorts]
youtube.com###secondary
&lt;/code&gt;
    &lt;p&gt;The only media that I do sometimes consume on my phone are my RSS feeds, but it's something I'm completely comfortable with since it's explicitly opt-in by design and low volume.&lt;/p&gt;
    &lt;p&gt;While I still have the twitch to check my phone when I'm waiting for a coffee, or in-between activities—because my brain's reward system has been trained to do this—I'm now rewarded with nothing. Over time, I find myself checking my phone less and less. Sometimes I notice the urge, and just let it go, instead focusing on the here and now.&lt;/p&gt;
    &lt;p&gt;I think that while the attention-span-degrading effects of recommendation media are getting most of the headlines, what isn't spoken about as much is the sheer number of hours lost globally to our phones (3.8 million years per day, according to my back-of-the-napkin-math). And while people may argue that this could involve productive work or enjoyable leisure, I suspect that the vast (vast!) majority of that time is short-form entertainment.&lt;/p&gt;
    &lt;p&gt;My solution may sound overkill to many people, but I can say with absolute certainty that it has turned me into a more present, less distracted, and more optimistic person. I have much more time to spend in nature, with friends, or on my hobbies and projects. I can't imagine trading it in for a tiny screen, ever.&lt;/p&gt;
    &lt;p&gt;Give it a try.&lt;/p&gt;
    &lt;p&gt;Happily on the beach for sunset.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568613</guid><pubDate>Mon, 13 Oct 2025 14:20:33 +0000</pubDate></item><item><title>Software update bricks some Jeep 4xe hybrids over the weekend</title><link>https://arstechnica.com/cars/2025/10/software-update-bricks-some-jeep-4xe-hybrids-over-the-weekend/</link><description>&lt;doc fingerprint="155561cbaa040794"&gt;
  &lt;main&gt;
    &lt;p&gt;Owners of some Jeep Wrangler 4xe hybrids have been left stranded after installing an over-the-air software update this weekend. The automaker pushed out a telematics update for the Uconnect infotainment system that evidently wasn't ready, resulting in cars losing power while driving and then becoming stranded.&lt;/p&gt;
    &lt;p&gt;Stranded Jeep owners have been detailing their experiences in forum and Reddit posts, as well as on YouTube. The buggy update doesn't appear to brick the car immediately. Instead, the failure appears to occur while driving—a far more serious problem. For some, this happened close to home and at low speed, but others claim to have experienced a powertrain failure at highway speeds.&lt;/p&gt;
    &lt;p&gt;Jeep pulled the update after reports of problems, but the software had already downloaded to many owners' cars by then. A member of Stellantis' social engagement team told 4xe owners at a Jeep forum to ignore the update pop-up if they haven't installed it yet.&lt;/p&gt;
    &lt;p&gt;Owners were also advised to avoid using either hybrid or electric modes if they had updated their 4xe and not already suffered a powertrain failure. Yesterday, Jeep pushed out a fix.&lt;/p&gt;
    &lt;p&gt;As Crowdstrike showed last year, Friday afternoons are a bad time to push out a software update. Now Stellantis has learned that lesson, too. Ars has reached out to Stellantis, and we'll update this post if we get a reply.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568700</guid><pubDate>Mon, 13 Oct 2025 14:28:25 +0000</pubDate></item><item><title>Legends of the games industry: Roger Dean</title><link>https://spillhistorie.no/2025/10/03/legends-of-the-games-industry-roger-dean/</link><description>&lt;doc fingerprint="26228153ffa48fbf"&gt;
  &lt;main&gt;
    &lt;p&gt;We spoke with the man behind the Psygnosis logo – and so much more!&lt;/p&gt;
    &lt;p&gt;English artist Roger Dean is a living legend, and his work in the video game industry represent just a small chapter in his extraordinary career. Dean was born in 1944 in Kent, but spent much of his childhood in Greece and Hong Kong. His father was an engineer in the British Army, so the family had to move wherever his work took him. In particular, the years he lived in Hong Kong would later become an important source of inspiration for him.&lt;/p&gt;
    &lt;p&gt;After returning to England, he studied art, architecture, and furniture design, and it was actually in the latter field that he had his first breakthrough. He designed what he called the Sea Urchin Chair, a predecessor to the famous bean bag chair.&lt;/p&gt;
    &lt;p&gt;But it was as a visual artist that he truly made his mark. In 1968, he created his first album cover, for the British rock band The Gun, and later became heavily involved with the prog rock bands Yes and Asia. His cover for Asia’s debut album was voted the second-best album cover of all time by readers of Rolling Stone Magazine in 1982, and it was also Dean who designed the very first logo for Richard Branson’s newly established Virgin Records.&lt;/p&gt;
    &lt;p&gt;It was in the 1980s that Roger Dean first became involved in the video game industry, where he was not only responsible for a number of iconic game covers, but also some of gaming’s most recognizable logos.&lt;/p&gt;
    &lt;p&gt;When we reached out to Dean to ask if he would like to do an interview with us, we honestly didn’t expect him to respond. And if he did, we assumed it would be just a small handful of questions answered by e-mail. But not only was he interested in talking with us, we ended up having a long and pleasant video call, during which he happily showed us his work and chatted about a variety of topics.&lt;/p&gt;
    &lt;p&gt;Note that our main focus was on Dean’s work with games, so if you’d like to read more about everything else he’s done, you could, for example, check out this profile interview at We Love Vinyl. You should also visit his website.&lt;/p&gt;
    &lt;p&gt;In this article, we present an edited version of that conversation, supplemented with a bit of extra information about the topics we discuss.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Black Onyx and Psygnosis&lt;/head&gt;
    &lt;p&gt;We start in the mid eighties, which is when Dean first gets involved in the games industry. His first cover artwork was created for The Black Onyx, a game you’ve probably never heard of unless you’re very interested in the history of gaming. Because even though the producer and designer of that game was an American – Henk Rogers, who we’ll talk more about later – the game was only released in Japan. While it isn’t a famous game, it is an early example of a role-playing game developed in Japan, and it would help influence how Japanese developers approached the genre.&lt;/p&gt;
    &lt;p&gt;For European gamers, it’s probably Dean’s other contract that proved the most memorable. When the British publisher Psygnosis was formed in 1984, they reached out to Roger Dean to create their logo. This would mark the start of a long lasting relationship which would shape much of the visual identity of the well remembered publisher.&lt;/p&gt;
    &lt;p&gt;JF: How did you get involved with the games industry? I know that your first work was on The Black Onyx …&lt;/p&gt;
    &lt;p&gt;RD: That’s right! Well, Henk Rogers, who now publishes Tetris, sought me out – though this was before he got the rights to Tetris. He was aware of my work in music. So he knew my music and my books, and of course my album covers. He contacted me through my publishing company, and came to visit.&lt;/p&gt;
    &lt;p&gt;JF: But that game only came out in Japan?&lt;/p&gt;
    &lt;p&gt;RD: That’s correct, yes.&lt;/p&gt;
    &lt;p&gt;But about the same time I met Henk, I also met Jonathan Ellis of Psygnosis. I had met with Imagine Software before – two of the people from Imagine formed Psygnosis with Jonathan Ellis – and I did a whole bunch of Psygnosis stuff.&lt;/p&gt;
    &lt;p&gt;JF: They contacted you? They’d seen your artwork already?&lt;/p&gt;
    &lt;p&gt;RD: Yes, they contacted me. They’d certainly seen the books. We sold enormous amounts of posters and books back then. During the seventies, my posters, books, calendars etcetera sold about 65 million copies, and by the mid eighties we’d passed a hundred million sales. So it was out there, you know. Much more than today.&lt;/p&gt;
    &lt;p&gt;JF: The owl logo, was that your idea?&lt;/p&gt;
    &lt;p&gt;RD: Yeah, sure. That was my job. They gave me an idea about the kind of name that they wanted, so even the name was partly mine. Both the name and the owl… they were very clear about what they wanted, but they didn’t know visually or even how to put the words together. So the word came to me in the end, and the visuals.&lt;/p&gt;
    &lt;p&gt;JF: Do you remember how you came up with the idea of putting an owl in there?&lt;/p&gt;
    &lt;p&gt;RD: What can I say? *laughs*&lt;/p&gt;
    &lt;p&gt;JF: Did you see or play the games before you did the covers?&lt;/p&gt;
    &lt;p&gt;RD: That’s not how it worked. It was the same with the music. Very often I had to finish the covers long before the games were done, and the content of the games was as much influenced by the cover as the cover was by the content.&lt;/p&gt;
    &lt;p&gt;In fact, I would say that the cover was influenced by them describing what they wanted to do for the game, and then me visualizing it. And then they would reproduce that to some degree themselves in the games.&lt;/p&gt;
    &lt;p&gt;HAJ: So they just said: This is what we are thinking, and then you started working?&lt;/p&gt;
    &lt;p&gt;RD: They described the game, usually in much more extravagant terms than what the reality was. They would say they were making an interective movie, and I’d say «wow!». And when I saw it, there would be these little matchstick figures…&lt;/p&gt;
    &lt;p&gt;JF: What was the process like?&lt;/p&gt;
    &lt;p&gt;RD: Well, it was very different from the work I was used to doing, so from that point of view it was good fun for me. Like going in another direction, I enjoyed that a lot. Especially the designs I did for The Shadow of the Beast, they were very different from any album covers I’d made.&lt;/p&gt;
    &lt;p&gt;JF: Did you ever work on actual game [design] for them?&lt;/p&gt;
    &lt;p&gt;RD: No, I remember when we did a game called Barbarian. The developers got very excited and asked me what I thought of their dragon. And I said, «what dragon?» Because I’d put a dragon on the box, and they’d then put my dragon in the game. And I said, «oh, you have to show me.» And they said, «it’s at the end of the first level, you haven’t gotten beyond the first level?» And I said «noo… I haven’t even started the first level!»&lt;/p&gt;
    &lt;p&gt;JF: Did they ever come back to you and ask you to redo something?&lt;/p&gt;
    &lt;p&gt;RD: Not really. Maybe on one occation only. I can’t even remember what it was, but I did the lettering for it, and I found them another artist. In the end, that was a turning point for me, because they were already producing more games than I could possibly manage. So I would end up doing logos, but getting other artists to do the art.&lt;/p&gt;
    &lt;p&gt;JF: Yeah, I see some of your covers are listed as a collaboration with you and Tim White.&lt;/p&gt;
    &lt;p&gt;RD: Tim White, yes. There was a number of artists in it. Chris Voss, I think. Peter Jones, maybe. Yeah. There was a few other artists who did covers.&lt;/p&gt;
    &lt;p&gt;JF: So you did the logos, and they did the paintings?&lt;/p&gt;
    &lt;p&gt;RD: They did the painting, yes.&lt;/p&gt;
    &lt;p&gt;JF: Was there a community of artists who did covers?&lt;/p&gt;
    &lt;p&gt;RD: Well, I knew the artist because I had published the books, and that was in very recent history. You know, within ten years of when we had the publishing company.&lt;/p&gt;
    &lt;p&gt;JF: I love all the covers you did for Psygnosis. Unfortunately, I don’t own so many, only Terrorpods I think. You did the logo there, I think?&lt;/p&gt;
    &lt;p&gt;RD: Terrorpods is interesting because I did the drawings for that. The painting was done by Tim White, but it was my drawing. I drew the machine.&lt;/p&gt;
    &lt;p&gt;JF: It’s one of my favorite covers.&lt;/p&gt;
    &lt;p&gt;RD: Yeah, it’s pretty good, I like it.&lt;/p&gt;
    &lt;p&gt;JF: There was also some re-use of older album covers. Did you help facilitate that?&lt;/p&gt;
    &lt;p&gt;RD: No, it’s the other way around. There were game ideas that became album covers. So game covers that became albums. Barbarian without the barbarian became a cover for a solo album by Steve Howe [from Yes], for instance.&lt;/p&gt;
    &lt;p&gt;JF: Ah, I see.&lt;/p&gt;
    &lt;p&gt;RD: The rule that I have is that there can be no confusion. So I never use a painting for one album cover on another. That would not be good. But if it was a totally different thing the rules and the licensing arrangements allowed me to do that.&lt;/p&gt;
    &lt;p&gt;JF: So you made sure of that going into the projects?&lt;/p&gt;
    &lt;p&gt;RD: From the very beginning, yes. I kept all the rights.&lt;/p&gt;
    &lt;head rend="h2"&gt;The evolution of game covers&lt;/head&gt;
    &lt;p&gt;We asked Roger Dean whether he ever received the finished game boxes he had worked on, and not only did get the boxes – he still has them! He then suggested showing us a few, and returned shortly afterward with the boxes for Shadow of the Beast I and II. Two large cardboard boxes with cover art that is both stunning and unique.&lt;/p&gt;
    &lt;p&gt;This led to a conversation about how game boxes have evolved over the years, and Dean’s thoughts on the subject.&lt;/p&gt;
    &lt;p&gt;JF: Where did that [SotB] style come from?&lt;/p&gt;
    &lt;p&gt;RD: I was very interested in mechanical things. So when I was a student, a lot of my work was about ideas for machinery. So Shadow of the Beast was natural, a very easy connection, and it was good fun.&lt;/p&gt;
    &lt;p&gt;JF: Those boxes are really unusual.&lt;/p&gt;
    &lt;p&gt;RD: Well, of course. There are no boxes at all today, are there.&lt;/p&gt;
    &lt;p&gt;JF: No… I was thinking about that because those big old boxes, they were almost like the the old records, compared to what came later…&lt;/p&gt;
    &lt;p&gt;RD: Yeah, and these are floppy disks inside. One also besides the floppy disks, I think it had a cassette, and a t-shirt. So it, so it had a book of instructions, floppy disk, cassette and t-shirt.&lt;/p&gt;
    &lt;p&gt;JF: Yeah. You don’t get that today.&lt;/p&gt;
    &lt;p&gt;RD: No. And the t-shirt was kind of weird because you couldn’t choose the size, right?&lt;/p&gt;
    &lt;p&gt;JF: Oh, well, it could be a gift for someone, if it didn’t fit…&lt;/p&gt;
    &lt;p&gt;RD: It would have had to be. Yeah. *laughs*&lt;/p&gt;
    &lt;p&gt;JF: But what I was getting at … I assume you’ve seen how game boxes just shrunk and became smaller and smaller, and now we don’t even have them. How do you feel about that?&lt;/p&gt;
    &lt;p&gt;RD: Well, I don’t know. It’s the same problem, of course, with music. And what happened was that for a very short period of time, music made the perfect gift. You know, a 12 inch vinyl, it looked like and felt like something you would both like to give and receive. And that concept of the gift was really strong.&lt;/p&gt;
    &lt;p&gt;You know, back when the vinyl was normal, getting a record for your birthday or Christmas was a big deal. And a big deal to give because they were relatively expensive. They weren’t even affordable by young people until quite a few years after they were invented. But it was a big deal, that gift idea. When it first went to CD, the record companies destroyed the idea of a gift because they stripped out a lot of what made it special.&lt;/p&gt;
    &lt;p&gt;I mean, one of Yes’s biggest albums in terms of its impact and iconography was Close to the Edge. And when it came out in vinyl, the cover had the new logo, but the painting was inside. When it came out on CD, there was no painting, it was just a folded sheet of paper inside. It was black and white, no image. And I thought, you know, this is treating the customers with so little respect. It was just amazing.&lt;/p&gt;
    &lt;p&gt;And as the industry went into decline, shortly after that, you could see, there was no respect. No respect for the music, for the bands and for the fans. It was their own fault that they were in trouble. The only country where the quality was persistent was Japan. Their CDs were always beautiful. In the West? They were rubbish.&lt;/p&gt;
    &lt;p&gt;JF: Would you say the same was true for games?&lt;/p&gt;
    &lt;p&gt;RD: It wasn’t really the size, it was the complete lack of care that really troubled me enormously. I quite like the small versions [of records] that came out in Japan because they were like a kind of bonsai, but everything was there. All the art was there. In fact, Japan had the bonus of having the translation, so you got more than the basic thing. It was good.&lt;/p&gt;
    &lt;p&gt;HAJ: Do you follow modern video game art?&lt;/p&gt;
    &lt;p&gt;RD: No, I don’t really. People show me stuff and I go, «wow, that’s pretty cool.» But I don’t go out of my way to follow it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tetris, and The Black Onyx part two&lt;/head&gt;
    &lt;p&gt;Few games – if any – are more famous than Tetris. We won’t go into the history of this addictive puzzle game; you’ve probably heard it before. But it was the aforementioned Henk Rogers who ended up securing the rights to the game, and he also founded The Tetris Company together with the original Tetris creator Alexey Pajitnov. When the time came to create an official Tetris logo in 1997, Roger Dean was the one they contacted.&lt;/p&gt;
    &lt;p&gt;This, however, was not the only collaboration between Dean and Rogers. The other was an ambitious sequel to The Black Onyx that was sadly cancelled before completion. Based on what Dean tells us below, it sounds like a really ambitious project.&lt;/p&gt;
    &lt;p&gt;JF: You also worked on Tetris.&lt;/p&gt;
    &lt;p&gt;RD: I did the Tetris logo. That was just a word, so much less interesting than the Psygnosis logo. But Tetris is a very interesting game…&lt;/p&gt;
    &lt;p&gt;JF: You knew Henk Rogers, did he always want you to do the logo?&lt;/p&gt;
    &lt;p&gt;RD: That was more than 25 years ago. Yeah, he he wanted me to do it because there were hundreds of versions out there. Not done by him, but by the various companies that’d license it. And people did pirate versions. Everyone had their own version of Tetris. So he wanted only one version of the logo. If someone had a license, they had to use the authorized logo. It was an attempt to put discipline into it, really.&lt;/p&gt;
    &lt;p&gt;And then two or three years ago he handed over the management of Tetris to his daughter, Maya, and she changed the logo again. But it’s the same rules, one logo. Although it’s slightly different to mine.&lt;/p&gt;
    &lt;p&gt;JF: And The Black Onyx?&lt;/p&gt;
    &lt;p&gt;RD: A much, much, more lavish version of The Black Onyx was due to come out in the States, and I worked on that. It was my job to put together the team that did the content and the packaging, and that included the story, music, landscapes, costumes, everything. I didn’t do it alone, but I put together the team.&lt;/p&gt;
    &lt;p&gt;JF: This was actual game development?&lt;/p&gt;
    &lt;p&gt;RD: Yes, this was a full-on role-playing game. A lot of the artwork appears in my book, Dragon’s Dream. But the very big, lavish production never happened, sadly. That was very disappointing. We worked on it for some years, it was good fun.&lt;/p&gt;
    &lt;p&gt;JF: Do you know why it didn’t happen?&lt;/p&gt;
    &lt;p&gt;RD: I’m not a hundred percent sure. It was a huge amount of work. It was a real shame that it never happened, because while the technology has moved on, the design would still be valid today. The music is incredible.&lt;/p&gt;
    &lt;p&gt;HAJ: So is there a chance it could see the light of day?&lt;/p&gt;
    &lt;p&gt;RD: Heh, yeah, I think Henk Rogers would like to see it published. He owns the game, and I own the artwork. The big game was supposed to be called Onyx.&lt;/p&gt;
    &lt;p&gt;JF: This was much more advanced than the original?&lt;/p&gt;
    &lt;p&gt;RD: Way more advanced. Too advanced for it’s time, really. It would have needed 24 CDs for each episode. DVDs arrived in the middle of it, but that would have only divided the number of CDs by three.&lt;/p&gt;
    &lt;p&gt;JF: Do you remember any game projects that were particularly exciting to work with?&lt;/p&gt;
    &lt;p&gt;RD: Well, you know what? Onyx, in the end, was the most exciting. Because that was the first time I got really hands on with the content.&lt;/p&gt;
    &lt;p&gt;And as I said, it’s still never seen the light of day. It is very interesting, because two weeks ago I had a visit from Henk Rogers. He’s doing a book called The Perfect Game about Tetris, and he’s doing an audio book. And in that he talks about different projects, including Black Onyx. For Black Onyx, he used some of the music that we created for the project, and it was really good by any standards. It was a great piece of music, not a great piece of game music, but a great piece of music. Even the music should be published. It hasn’t been, but it should be.&lt;/p&gt;
    &lt;p&gt;HAJ: Who did that music?&lt;/p&gt;
    &lt;p&gt;RD: Well, we did it in collaboration with two people called Youth and Jaz. Jaz Coleman was orchestral minded, but he was also a singer for a band called Killing Joke. So he had his rock and roll credentials. But he worked with Prague Symphony Orchestra and things like that. Youth (Martin Glover) was very much into electronic music. He had a band called The Orb, and he worked with people like Paul McCartney. Oh, he did all kinds of stuff. But his big interest was electronic music at the time.&lt;/p&gt;
    &lt;p&gt;Between them, one producing, one arranging, they made a lovely soundtrack. And it had people like Steve Howe from Yes performing on it.&lt;/p&gt;
    &lt;p&gt;HAJ: Is there a chance that we will hear this music in the future?&lt;/p&gt;
    &lt;p&gt;RD: Well, I think yes, because we were all listening to it at least two weeks ago, and that’s exactly what everyone was saying. This music has got to be available. It’s got to be out there.&lt;/p&gt;
    &lt;p&gt;HAJ: I really want to play this game now … but but the artwork for this game will be out in your next book or calendar?&lt;/p&gt;
    &lt;p&gt;RD: Some of it will, but it’s in my book, Dragon’s Dream, which was published in 2008.&lt;/p&gt;
    &lt;p&gt;JF: Was it ever possible to actually play the game – did it get that far into development?&lt;/p&gt;
    &lt;p&gt;RD: No. Henk would probably tell me I’m wrong, but I’m not even sure gameplay was ever fully developed. The overall concept had to be because we couldn’t structure what we did without that, but we were filling in a lot of gaps. Too many gaps.&lt;/p&gt;
    &lt;p&gt;I mean, we did a lot of things which were done for the first time. At the time, I studied kendo, which is Japanese martial art with the sword. And my sensei had studied medieval European sword and pole arm spear techniques. For the sword fighting, it was broken up into kata, which is attack, defend, counterattack – sequences that were from real techniques. In a fight you could put it together and it looks so amazingly convincing, and you could watch it from any angle. And we we recorded it in motion capture. So it was very realistic. And it was not just because the motion capture is realistic, but because these were genuine sword techniques.&lt;/p&gt;
    &lt;p&gt;JF: I know The Tetris Company worked with another company [Digital Eclipse] for an «interactive museum» about Tetris, so I was wondering if something could maybe be saved and published in a similar way?&lt;/p&gt;
    &lt;p&gt;RD: Well, there is something which is possible. We developed a process that Henk called Track and Field. Track was when the characters followed a specific route, and Field was when they could wander wherever you liked. They couldn’t wander over the whole world because they’d get lost and it would be boring. You had to have a mechanism to bring them back, but you needed them to follow a path.&lt;/p&gt;
    &lt;p&gt;So you could do it like a movie where there was a sequence that was completely constructed. You could watch it from different angles, but you it was a complete construction, but then you could break off into a game. If for instance it was like Lord of the Rings, they could be climbing the mountain path, but when they’re in the dragon’s lair, then they’d come into the field – the game aspect, where they can wander wherever they want. But once they’re out again, they’re back on the track.&lt;/p&gt;
    &lt;p&gt;So there’s bits when you can just watch it, it looks great, and then there’s bits when you’re frantically interactive.&lt;/p&gt;
    &lt;p&gt;HAJ: Did you work on any other interesting projects like this?&lt;/p&gt;
    &lt;p&gt;RD: Before I met either Henk or Jonathan Ellis, we worked on an idea for doing a project called Taitan, which was an arcade game [cabinet]. We said we were going to manufacture the machines ourselves, and talked to Taito Electronics about licensing the motherboards from them. But instead, they decided to buy out our business, so that’s how that went.&lt;/p&gt;
    &lt;p&gt;Henk Rogers also got involved with a virtual reality project with us. He was the first who saw this, and he invested in it. We built maybe a dozen prototypes. But again, we were too far ahead of the technology. Mitsubishi supplied the monitors … they were the size of a small car. We had to cut great chunks out of the pods we were making. They were very elegant, but we had to cut massive amounts out of them just to fit in the monitors. They were bigger behind than the screen.&lt;/p&gt;
    &lt;p&gt;JF: How do you view your game art compared to the rest of the work that you’ve done?&lt;/p&gt;
    &lt;p&gt;RD: In many ways, it was like returning to roots for me because I never did do fine art at college. I did Canterbury College of Art for four years, Royal College for three. My focus was on architecture. You know, the I studied basically what kind of spaces made us feel good, what kind of spaces made us feel uncomfortable. And my view is very strongly that modern architecture is not good for us. There should have been a better way.&lt;/p&gt;
    &lt;p&gt;This is what I’m very interested in and focused on now. We’re looking to build a visitor center and museum.&lt;/p&gt;
    &lt;p&gt;HAJ: Where?&lt;/p&gt;
    &lt;p&gt;RD: We’re looking at two sites. They’ll be different. One is in England, near here, near where I live, and one is in California.&lt;/p&gt;
    &lt;p&gt;JF: Thanks a lot for your time. It’s an honor for us.&lt;/p&gt;
    &lt;p&gt;RD: No, it’s an honor for me. And fun.&lt;/p&gt;
    &lt;p&gt;HAJ: Thank you for doing this.&lt;/p&gt;
    &lt;p&gt;RD: Thank you.&lt;/p&gt;
    &lt;p&gt;Please visit Roger Dean’s website for more of his art.&lt;/p&gt;
    &lt;p&gt;And visit this page for more content in English, including a lot of other interviews with games industry people.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568708</guid><pubDate>Mon, 13 Oct 2025 14:29:06 +0000</pubDate></item><item><title>America is getting an AI gold rush instead of a factory boom</title><link>https://www.washingtonpost.com/business/2025/10/13/manufacturing-artificial-intelligence/</link><description>&lt;doc fingerprint="f35908d52b294ba0"&gt;
  &lt;main&gt;&lt;p&gt;A gulf is opening up in the heart of American business as two industries championed as central to the country’s future — manufacturing and artificial intelligence — appear to be heading in different directions.&lt;/p&gt;&lt;p&gt;By Aaron Gregg&lt;/p&gt; and &lt;list rend="ul"&gt;&lt;item&gt;1Shannon OsakaMicroplastics are everywhere. You can do one simple thing to avoid them.&lt;/item&gt;&lt;item&gt;2Shannon NajmabadiandAaron GreggAirports say they won’t air Kristi Noem shutdown video at TSA checkpoints&lt;/item&gt;&lt;item&gt;3Scott NoverNews outlets broadly reject Pentagon rules before signing deadline&lt;/item&gt;&lt;item&gt;4Guest columnSi LibermanI’m 101 years old. Here are 7 things I think are ‘longevity secrets.’&lt;/item&gt;&lt;item&gt;5OpinionMax BootWhy the Gaza ceasefire won’t lead to lasting peace&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45568915</guid><pubDate>Mon, 13 Oct 2025 14:48:47 +0000</pubDate></item><item><title>NanoChat – The best ChatGPT that $100 can buy</title><link>https://github.com/karpathy/nanochat</link><description>&lt;doc fingerprint="8c198122f1657e6"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;The best ChatGPT that $100 can buy.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This repo is a full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase. nanochat is designed to run on a single 8XH100 node via scripts like speedrun.sh, that run the entire pipeline start to end. This includes tokenization, pretraining, finetuning, evaluation, inference, and web serving over a simple UI so that you can talk to your own LLM just like ChatGPT. nanochat will become the capstone project of the course LLM101n being developed by Eureka Labs.&lt;/p&gt;
    &lt;p&gt;The fastest way to feel the magic is to run the speedrun script speedrun.sh, which trains and inferences the $100 tier of nanochat. On an 8XH100 node at $24/hr, this gives a total run time of about 4 hours. Boot up a new 8XH100 GPU box from your favorite provider (e.g. I use and like Lambda), and kick off the training script:&lt;/p&gt;
    &lt;code&gt;bash speedrun.sh&lt;/code&gt;
    &lt;p&gt;Alternatively, since the script runs for 4 hours, I like to launch it like this inside a new screen session &lt;code&gt;speedrun&lt;/code&gt; (and also log output to &lt;code&gt;speedrun.log&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;screen -L -Logfile speedrun.log -S speedrun bash speedrun.sh&lt;/code&gt;
    &lt;p&gt;See the screen cheatsheet if you are less familiar. You can watch it go inside the screen session, or detach with &lt;code&gt;Ctrl-a d&lt;/code&gt; and &lt;code&gt;tail speedrun.log&lt;/code&gt; to view progress. Now wait 4 hours. Once it's done, you can talk to your LLM via the ChatGPT-like web UI. Make sure again that your local uv virtual environment is active (run &lt;code&gt;source .venv/bin/activate&lt;/code&gt;), and serve it:&lt;/p&gt;
    &lt;code&gt;python -m scripts.chat_web&lt;/code&gt;
    &lt;p&gt;And then visit the URL shown. Make sure to access it correctly, e.g. on Lambda use the public IP of the node you're on, followed by the port, so for example http://209.20.xxx.xxx:8000/, etc. Then talk to your LLM as you'd normally talk to ChatGPT! Get it to write stories or poems. Ask it to tell you who you are to see a hallucination. Ask it why the sky is blue. Or why it's green. The speedrun is a 4e19 FLOPs capability model so it's a bit like talking to a kindergartener :).&lt;/p&gt;
    &lt;p&gt;You can also &lt;code&gt;cat report.md&lt;/code&gt; file which appeared in the project directory and contains the "report card" of the run, i.e. a bunch of evaluations and metrics. At the very end, you'll see a summary table, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Characters: 333,989&lt;/item&gt;
      &lt;item&gt;Lines: 8,304&lt;/item&gt;
      &lt;item&gt;Files: 44&lt;/item&gt;
      &lt;item&gt;Tokens (approx): 83,497&lt;/item&gt;
      &lt;item&gt;Dependencies (uv.lock lines): 2,004&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;BASE&lt;/cell&gt;
        &lt;cell role="head"&gt;MID&lt;/cell&gt;
        &lt;cell role="head"&gt;SFT&lt;/cell&gt;
        &lt;cell role="head"&gt;RL&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CORE&lt;/cell&gt;
        &lt;cell&gt;0.2219&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ARC-Challenge&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.2875&lt;/cell&gt;
        &lt;cell&gt;0.2807&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ARC-Easy&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.3561&lt;/cell&gt;
        &lt;cell&gt;0.3876&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;GSM8K&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.0250&lt;/cell&gt;
        &lt;cell&gt;0.0455&lt;/cell&gt;
        &lt;cell&gt;0.0758&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HumanEval&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.0671&lt;/cell&gt;
        &lt;cell&gt;0.0854&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMLU&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.3111&lt;/cell&gt;
        &lt;cell&gt;0.3151&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ChatCORE&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.0730&lt;/cell&gt;
        &lt;cell&gt;0.0884&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Total wall clock time: 3h51m&lt;/p&gt;
    &lt;p&gt;(Your table might be missing the RL number by default). For a lot more information around the speedrun script and what to look for and expect, please refer to the walkthrough that I posted in Discussions of the repo: "Introducing nanochat: The best ChatGPT that $100 can buy".&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, $100 is not enough to train a highly performant ChatGPT clone. In fact, LLMs are famous for their multi-million dollar capex. For our purposes, I think there are two more scales of interest. First is the ~$300 tier d26 model (i.e. depth=26) that trains in ~12 hours, which slightly outperforms GPT-2 CORE score. Second is the $1000 tier (~41.6 hours), just because it's a nice round number. But both of these are not yet fully supported and therefore not attached here in the master branch yet.&lt;/p&gt;
    &lt;p&gt;That said, to give a sense, the example changes needed for the speedrun.sh file to train a GPT-2 grade model d26 only involve three changes:&lt;/p&gt;
    &lt;code&gt;...
# you'll need to download more data shards for pretraining
# get the number of parameters, multiply 20 to get tokens, multiply by 4.8 to get chars,
# divide by 250 million to get number of shards. todo need to improve this...
python -m nanochat.dataset -n 450 &amp;amp;
...
# use --depth to increase model size. to not oom, halve device batch size 32 -&amp;gt; 16:
torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- --depth=26 --device_batch_size=16
...
# make sure to use the same later during midtraining:
torchrun --standalone --nproc_per_node=8 -m scripts.mid_train -- --device_batch_size=16&lt;/code&gt;
    &lt;p&gt;That's it! The biggest thing to pay attention to is making sure you have enough data shards to train on (the code will loop and do more epochs over the same training set otherwise, decreasing learning speed a bit), and managing your memory/VRAM, primarily by decreasing the &lt;code&gt;device_batch_size&lt;/code&gt; until things fit (the scripts automatically compensates by increasing the number of gradient accumulation loops, simply turning parallel compute to sequential compute).&lt;/p&gt;
    &lt;p&gt;And a bit more about computing environments that will run nanochat:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The code will run just fine on the Ampere 8XA100 GPU node as well, but a bit slower.&lt;/item&gt;
      &lt;item&gt;All code will run just fine on even a single GPU by omitting &lt;code&gt;torchrun&lt;/code&gt;, and will produce ~identical results (code will automatically switch to gradient accumulation), but you'll have to wait 8 times longer.&lt;/item&gt;
      &lt;item&gt;If your GPU(s) have less than 80GB, you'll have to tune some of the hyperparameters or you will OOM / run out of VRAM. Look for &lt;code&gt;--device_batch_size&lt;/code&gt;in the scripts and reduce it until things fit. E.g. from 32 (default) to 16, 8, 4, 2, or even 1. Less than that you'll have to know a bit more what you're doing and get more creative.&lt;/item&gt;
      &lt;item&gt;Most of the code is fairly vanilla PyTorch so it should run on anything that supports that - xpu, mps, or etc, but I haven't implemented this out of the box so it might take a bit of tinkering.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;nanochat is designed to be short and sweet. One big advantage of this is that we can package up all of the files together and copy paste them to your favorite LLM to ask arbitrary questions. As an example, I like to package up the repo using the files-to-prompt utility like so:&lt;/p&gt;
    &lt;code&gt;files-to-prompt . -e py -e md -e rs -e html -e toml -e sh --ignore "*target*" --cxml &amp;gt; packaged.txt&lt;/code&gt;
    &lt;p&gt;This includes all py, rs, html, toml, sh files, excludes the &lt;code&gt;rustbpe/target&lt;/code&gt; folder, and chooses the cxml output format. Everything is written to the &lt;code&gt;packaged.txt&lt;/code&gt; file, which atm measures ~330KB (i.e. well below ~100K tokens for a state of the art LLM), and ~8K lines of code in 45 files.&lt;/p&gt;
    &lt;p&gt;Alternatively, I recommend using DeepWiki from Devin/Cognition to ask questions of this repo. In the URL of this repo, simply change github.com to deepwiki.com, and you're off.&lt;/p&gt;
    &lt;p&gt;I haven't invested too much here but some tests exist, especially for the tokenizer. Run e.g. as:&lt;/p&gt;
    &lt;code&gt;python -m pytest tests/test_rustbpe.py -v -s&lt;/code&gt;
    &lt;p&gt;nanochat is nowhere finished. The goal is to improve the state of the art in micro models that are accessible to work with end to end on budgets of &amp;lt; $1000 dollars. Accessibility is about overall cost but also about cognitive complexity - nanochat is not an exhaustively configurable LLM "framework"; there will be no giant configuration objects, model factories, or if-then-else monsters in the code base. It is a single, cohesive, minimal, readable, hackable, maximally-forkable "strong baseline" codebase designed to run start to end and produce a concrete ChatGPT clone and its report card.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The name (nanochat) derives from my earlier project nanoGPT, which only covered pretraining.&lt;/item&gt;
      &lt;item&gt;nanochat is also inspired by modded-nanoGPT, which gamified the nanoGPT repo with clear metrics and a leaderboard, and borrows a lot of its ideas and some implementation for pretraining.&lt;/item&gt;
      &lt;item&gt;Thank you to HuggingFace for fineweb and smoltalk.&lt;/item&gt;
      &lt;item&gt;Thank you Lambda for the compute used in developing this project.&lt;/item&gt;
      &lt;item&gt;Thank you to chief LLM whisperer 🧙♂️ Alec Radford for advice/guidance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you find nanochat helpful in your research cite simply as:&lt;/p&gt;
    &lt;code&gt;@misc{nanochat,
  author = {Andrej Karpathy},
  title = {nanochat: The best ChatGPT that $100 can buy},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/karpathy/nanochat}
}&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45569350</guid><pubDate>Mon, 13 Oct 2025 15:22:47 +0000</pubDate></item><item><title>Android's sideloading limits are its most anti-consumer move</title><link>https://www.makeuseof.com/androids-sideloading-limits-are-anti-consumer-move-yet/</link><description>&lt;doc fingerprint="8f42830190be3051"&gt;
  &lt;main&gt;
    &lt;p&gt;I’m a huge fan of open source, and that’s one of the reasons I’m drawn to Android. However, new requirements surrounding sideloaded apps, which will start rolling out in October 2025, may be the most anti-consumer move yet by Google. Mandatory enforcement of the requirement will begin in September 2026 (starting with specific countries), marking a turning point where the freedom to install any app comes with conditions set by Google.&lt;/p&gt;
    &lt;p&gt;I’ve used apps like NewPipe (a media/YouTube client) and Blokada (an ad blocker) for years now. However, these apps aren’t available on the Google Play Store, so I have to obtain them from third-party sources, such as F-Droid. With Google tightening the rules around sideloaded apps, I fear I may lose access to some of the apps I love most on Android because they aren’t verified. Sideloading isn’t going away, but people may seek alternatives because it may feel like the gates are narrowing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Google actually changed&lt;/head&gt;
    &lt;head rend="h3"&gt;The rules, the timeline, and what “certified” really means&lt;/head&gt;
    &lt;p&gt;Google's talk around "verified developers" sounds harmless and, in some ways, helpful. As reported on the Android Developer Blog, it is like "an ID check at the airport which confirms a traveler's identity but is separate from the security screening of their bags." Google's analogy, however, may be oversimplified. When this is enforced, the only way a developer’s app will be installable on devices that include Google Mobile Services (GMS) — which typically provide access to the Play Store — is by completing ID verification using government-issued documents or contact information. This will be rolled out globally in 2027.&lt;/p&gt;
    &lt;p&gt;Apps will be blocked from installing on most mainstream phones if their developer can't complete this verification. However, there are certain devices that will remain unaffected, even though they are just a tiny fraction of the total devices. These categories include all devices that do not pass Google's certification test, primarily custom ROMs or de-Googled phones.&lt;/p&gt;
    &lt;p&gt;Strictly speaking, Google is not removing sideloading, but it is redefining and limiting participation in the Android ecosystem by creating a mandatory Google-controlled choke point. While this may be a subtle shift, it clearly takes an open source project from anyone being able to participate (including anonymous or pseudonymous distribution) to only those whom Google allows to participate (via centralized developer identity verification).&lt;/p&gt;
    &lt;head rend="h2"&gt;Security theater or real gain?&lt;/head&gt;
    &lt;head rend="h3"&gt;Testing Google’s justification&lt;/head&gt;
    &lt;p&gt;There is a rational justification for tightening rules around sideloaded apps. It could be framed as user protection against malicious apps or against bad actors who cloak themselves with fake identities. While this is reasonable, the real question is whether it adds significant security for everyday users.&lt;/p&gt;
    &lt;p&gt;This is a valid question because security checks already exist. Google Play Protect makes Android secure by scanning sideloaded apps. Android flags unsafe installs, and it’s always given us the choice of blocking apps from unknown sources. Even if these are imperfect, they’re defenses that already exist.&lt;/p&gt;
    &lt;p&gt;Google’s new move almost feels like it’s based on the assumption that identity equals integrity. Does a verified government-issued identification equate to user safety? This logic is flawed: historically, we've seen malware slip through the Play Store—signed and “verified”—several times. However, what the new rule does is shift the basis of trust away from existing on-device security warnings and your best judgment.&lt;/p&gt;
    &lt;p&gt;Critics may even contend that this new rule erodes your right to make informed decisions about your own devices, and that feels more like selective control. Ultimately, many people may view this as Google’s way of shielding itself from criticism over sideloaded malware and protecting the integrity of its ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;There will be collateral damage&lt;/head&gt;
    &lt;head rend="h3"&gt;The ecosystems that depend on openness&lt;/head&gt;
    &lt;p&gt;This may be the most significant anti-consumer move, simply due to its profound impact. It could hit big developers or commercial apps, as well as entire ecosystems built around freely distributed APKs without verification. F-Droid hosts an incredible number of apps not available on the Play Store. Many of these tools exist because they see a need to operate outside the long, controlling arm of Google. This sideloading rule may make them unavailable on mainstream devices even though they’re safe.&lt;/p&gt;
    &lt;p&gt;This is a risk that also affects indie developers and hobbyists. Certain apps can no longer justify the time, effort, or privacy trade-offs required for identity verification. Many one-off projects and apps for niche communities may fall under this category. Ultimately, what we may end up with is a shrunken ecosystem, and if this happens, it will hurt all of us.&lt;/p&gt;
    &lt;p&gt;However, innovation may be the biggest casualty in all of this. Android is great because of its flexibility. It is an ecosystem for everyone. The imposition of a single, centralized gatekeeper will stifle grassroots innovation, as not everyone will be willing or able to contribute, and this will invariably impact the pace and extent of innovation we see on Android.&lt;/p&gt;
    &lt;head rend="h2"&gt;The new reality for Android users&lt;/head&gt;
    &lt;p&gt;Although Google would argue that the intentions behind the new rules for sideloading apps are to protect and secure users, it will likely feel limiting to many Android users, let alone removing the sense of autonomy on our devices. Of course, sideloading will still be possible, but it creates friction for people who use or make apps that aren’t officially available on the Play Store. The fear is that it may be the beginning of the end for independent developers, hobbyists, and niche app communities.&lt;/p&gt;
    &lt;p&gt;Of course, there are workarounds: using non-certified devices, backing up APKs, or exploring alternative app stores. Sadly, the trade-offs for each workaround may range from technical complexity to potential security risks. You should be careful when sideloading apps on Android. However, one thing is clear: Android's openness is closing. What we don’t know is if it will become a completely closed ecosystem someday.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45569371</guid><pubDate>Mon, 13 Oct 2025 15:24:08 +0000</pubDate></item><item><title>Optery (YC W22) – Hiring Tech Lead with Node.js Experience (U.S. &amp; Latin America)</title><link>https://www.optery.com/careers/</link><description>&lt;doc fingerprint="738af76944b7ba2c"&gt;
  &lt;main&gt;
    &lt;p&gt;Skip to content Use promo code: 04SxyxNX at checkout for 20% Off 🎉 with Optery’s Fall Sale! 🍁 Getting Started Personal Business Pricing Personal Business Sites We Cover Family Business Optery for Business Partnership Program API for Partners Book Optery for Business Demo Client Stories Business Use Cases PII Removal for Execs is Not Enough Guide to Enterprise Data Removal Services Resources Help Desk Blog Data Broker Directory For High-Risk Communities About Us Opt Out Guides Product Updates Customer Reviews Optery in the Press Search Toggle search Sign In Sign Up Free Getting Started Personal Business Pricing Personal Business Sites We Cover Family Business Optery for Business Partnership Program API for Partners Book Optery for Business Demo Client Stories Business Use Cases PII Removal for Execs is Not Enough Guide to Enterprise Data Removal Services Resources Help Desk Blog Data Broker Directory For High-Risk Communities About Us Opt Out Guides Product Updates Customer Reviews Optery in the Press Careers Ready to safeguard your personal data? Join the movement of people strengthening their privacy Sign Up Free&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45570720</guid><pubDate>Mon, 13 Oct 2025 17:03:11 +0000</pubDate></item><item><title>America's future could hinge on whether AI slightly disappoints</title><link>https://www.noahpinion.blog/p/americas-future-could-hinge-on-whether</link><description>&lt;doc fingerprint="7fe089016e6199bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;America's future could hinge on whether AI slightly disappoints&lt;/head&gt;
    &lt;head rend="h3"&gt;If the economy's single pillar goes down, Trump's presidency will be seen as a disaster.&lt;/head&gt;
    &lt;p&gt;A burning question that’s on a lot of people’s minds right now is: Why is the U.S. economy still holding up? The manufacturing industry is hurting badly from Trump’s tariffs, the payroll numbers are looking weak, and consumer sentiment is at Great Recession levels:&lt;/p&gt;
    &lt;p&gt;And yet despite those warning signs, there has been nothing even remotely resembling an economic crash yet. Unemployment is rising a little bit but still extremely low, while the prime-age employment rate — my favorite single indicator of the health of the labor market — is still near all-time highs. The New York Fed’s GDP nowcast thinks that GDP growth is currently running at a little over 2%, while the Atlanta Fed’s nowcast puts it even higher.&lt;/p&gt;
    &lt;p&gt;One possibility is that everything is just fine with the economy — that Trump’s tariffs aren’t actually that high because of all the exemptions, and/or that economists are exaggerating the negative effects of tariffs in the first place. Weak consumer confidence could be a partisan “vibecession”, payroll slowdown could be from illegal immigrants being deported or leaving en masse, and manufacturing’s woes could be from some other sector-specific factor.&lt;/p&gt;
    &lt;p&gt;Another possibility is that tariffs are bad, but are being canceled out by an even more powerful force — the AI boom. The FT reports:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Pantheon Macroeconomics estimates that US GDP would have grown at a mere 0.6 per cent annualised rate in the first half were it not for AI-related spending, or half the actual rate.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Paul Kedrosky came up with similar numbers. Jason Furman does a slightly different calculation, and arrives at an even starker number:&lt;/p&gt;
    &lt;p&gt;And here’s an impressive chart:&lt;/p&gt;
    &lt;p&gt;The Economist writes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[L]ook beyond AI and much of the economy appears sluggish. Real consumption has flatlined since December. Jobs growth is weak. Housebuilding has slumped, as has business investment in non-AI parts of the economy[.]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And in a post entitled “America is now one big bet on AI”, Ruchir Sharma writes that “AI companies have accounted for 80 per cent of the gains in US stocks so far in 2025.” In fact, more than a fifth of the entire S&amp;amp;P 500 market cap is now just three companies — Nvidia, Microsoft, and Apple — two of which are basically big bets on AI.&lt;/p&gt;
    &lt;p&gt;Now as Furman points out, this doesn’t necessarily mean that without AI, the U.S. economy would be stalling out. If the economy wasn’t pouring resources into AI, it might be pouring them into something else, spurring growth that was almost as fast as what we actually saw. But it’s also possible that without AI, America would be crashing from tariffs.&lt;/p&gt;
    &lt;p&gt;Trump certainly seems to think AI is a golden goose worth protecting. Joey Politano points out that even as Trump has slapped tariffs on a plethora of industries, he has left AI and its supply chain mostly untouched:&lt;/p&gt;
    &lt;p&gt;But despite Trump’s tariff exemptions, the AI sector could very well crash in the next year or two. And if it does, it could do a lot more than just hurt Americans’ employment prospects and stock portfolios.&lt;/p&gt;
    &lt;p&gt;If AI is really the only thing protecting America from the scourge of Trump’s tariffs, then a bust in the sector could change the country’s entire political economy. A crash and recession would immediately flip the narrative on Trump’s whole presidency, much as the housing crash of 2008 cemented George W. Bush’s legacy as a failure. And because Trump’s second term is looking so transformative1, the fate of the AI sector could potentially determine the entire fate of the country.&lt;/p&gt;
    &lt;p&gt;So a whole lot is riding on the question of whether an AI bust will crash the economy. The stakes could hardly be higher.&lt;/p&gt;
    &lt;head rend="h4"&gt;The case everyone is making for an AI bubble&lt;/head&gt;
    &lt;p&gt;A lot of bubbles are purely financial beasts, driven by irrationality or coordination problems in the markets for stocks, bonds, and derivatives. For example, you can have a speculative bubble, in which a bunch of people know an asset is overpriced, but think they can sell out before the crash, and so they keep buying and buying and pushing the price up and up. You can also have an extrapolative bubble, when people see the price of something going up and up, and mistakenly decide that it must be due to some underlying positive trend.&lt;/p&gt;
    &lt;p&gt;But a much simpler possibility is that investors could make a big mistake about how valuable some technology is. They could honestly believe that AI is going to create immense amounts of value, and they could just end up being wrong. Then when they realize that the technology isn’t all it’s cracked up to be, they could temper their expectations, which would cause a price crash in AI stocks.2 But the stock crash wouldn’t be the real problem; far more painful would be the wave of loan defaults and financial distress that would result from AI’s actual shortcomings.&lt;/p&gt;
    &lt;p&gt;If there’s an AI crash, it’ll probably be this latter type. Jeff Bezos calls it an “industrial bubble”, and I think that’s as good a name as any. This kind of bubble is still a financial phenomenon, since the banking system gets hurt. But the cause is a mistake about real technology, rather than asset markets going haywire.&lt;/p&gt;
    &lt;p&gt;Everyone who’s talking about an AI bubble is basically warning that the technology itself might disappoint. For example, here are some excerpts from a big Bloomberg feature about the possibility of an AI bubble:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Even some of AI’s biggest cheerleaders acknowledge the market is frothy, while still professing their belief in the technology’s long-term potential. AI, they say, is poised to reshape multiple industries, cure diseases and generally accelerate human progress…Yet never before has so much money been spent so rapidly on a technology that, for all its potential, remains somewhat unproven as a profit-making business model…&lt;/p&gt;
      &lt;p&gt;The data center spending spree is overshadowed by persistent skepticism about the payoff from AI technology. In August, investors were rattled after researchers at the Massachusetts Institute of Technology found that 95% of organizations saw zero return on their investment in AI initiatives.&lt;/p&gt;
      &lt;p&gt;More recently, researchers at Harvard and Stanford offered a possible explanation for why. Employees are using AI to create “workslop,” which the researchers define as “AI generated work content that masquerades as good work, but lacks the substance to meaningfully advance a given task.”…&lt;/p&gt;
      &lt;p&gt;AI developers have also been confronting a different challenge. OpenAI…Anthropic and others have for years bet on the so-called scaling laws…Over the past year, however, these developers have experienced diminishing returns…Some have also struggled to match their own hype. After months of touting GPT-5 as a significant leap, OpenAI’s release of its latest AI model in August was met with mixed reviews…&lt;/p&gt;
      &lt;p&gt;There’s also the risk that the AI industry’s vast data center buildout, entailing a huge increase in electricity consumption, will be held back by the realities of strained national power networks.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When you bring up concerns like this to an AI engineer, executive, or founder, they tend to just smile at you indulgently, secure in the knowledge that their invention is everything it’s cracked up to be, and that much better things are already in the pipeline.&lt;/p&gt;
    &lt;p&gt;But this doesn’t reassure me. Because when we look at the history of industrial bubbles, and of new technologies in general, it becomes clear that in order to cause a crash, AI doesn’t have to fail. It just has to mildly disappoint the most ardent optimists.&lt;/p&gt;
    &lt;p&gt;This is why I think an AI crash is more likely than a lot of people in the tech world — or the Trump administration — realize.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why AI could crash even if AI is just as useful as the optimists expect&lt;/head&gt;
    &lt;head rend="h2"&gt;Keep reading with a 7-day free trial&lt;/head&gt;
    &lt;p&gt;Subscribe to Noahpinion to keep reading this post and get 7 days of free access to the full post archives.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45570973</guid><pubDate>Mon, 13 Oct 2025 17:24:51 +0000</pubDate></item><item><title>Modern iOS Security Features – A Deep Dive into SPTM, TXM, and Exclaves</title><link>https://arxiv.org/abs/2510.09272</link><description>&lt;doc fingerprint="b49e9503c5f3920d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Cryptography and Security&lt;/head&gt;&lt;p&gt; [Submitted on 10 Oct 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Modern iOS Security Features -- A Deep Dive into SPTM, TXM, and Exclaves&lt;/head&gt;View PDF&lt;quote&gt;Abstract:The XNU kernel is the basis of Apple's operating systems. Although labeled as a hybrid kernel, it is found to generally operate in a monolithic manner by defining a single privileged trust zone in which all system functionality resides. This has security implications, as a kernel compromise has immediate and significant effects on the entire system. Over the past few years, Apple has taken steps towards a more compartmentalized kernel architecture and a more microkernel-like design. To date, there has been no scientific discussion of SPTM and related security mechanisms. Therefore, the understanding of the system and the underlying security mechanisms is minimal. In this paper, we provide a comprehensive analysis of new security mechanisms and their interplay, and create the first conclusive writeup considering all current mitigations. SPTM acts as the sole authority regarding memory retyping. Our analysis reveals that, through SPTM domains based on frame retyping and memory mapping rule sets, SPTM introduces domains of trust into the system, effectively gapping different functionalities from one another. Gapped functionality includes the TXM, responsible for code signing and entitlement verification. We further demonstrate how this introduction lays the groundwork for the most recent security feature of Exclaves, and conduct an in-depth analysis of its communication mechanisms. We discover multifold ways of communication, most notably xnuproxy as a secure world request handler, and the Tightbeam IPC framework. The architecture changes are found to increase system security, with key and sensitive components being moved out of XNU's direct reach. This also provides additional security guarantees in the event of a kernel compromise, which is no longer an immediate threat at the highest trust level.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45571688</guid><pubDate>Mon, 13 Oct 2025 18:23:15 +0000</pubDate></item><item><title>Strudel REPL – a music live coding environment living in the browser</title><link>https://strudel.cc</link><description>&lt;doc fingerprint="d36fdfbffc5b7f73"&gt;
  &lt;main&gt;
    &lt;p&gt;mastodon&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45571822</guid><pubDate>Mon, 13 Oct 2025 18:37:34 +0000</pubDate></item><item><title>LLMs are getting better at character-level text manipulation</title><link>https://blog.burkert.me/posts/llm_evolution_character_manipulation/</link><description>&lt;doc fingerprint="ae7186842d5152e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Recently, I have been testing how well the newest generations of large language models (such as GPT-5 or Claude 4.5) handle natural language, specifically counting characters, manipulating characters in a sentences, or solving encoding and ciphers. Surprisingly, the newest models were able to solve these kinds of tasks, unlike previous generations of LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Character manipulation&lt;/head&gt;
    &lt;p&gt;LLMs handle individual characters poorly. This is due to all text being encoded as tokens via the LLM tokenizer and its vocabulary. Individual tokens typically represent clusters of characters, sometimes even full words (especially in English and other common languages in the training dataset). This makes any considerations on a more granular level than tokens fairly difficult, although LLMs have been capable of certain simple tasks (such as spelling out individual characters in a word) for a while.&lt;/p&gt;
    &lt;p&gt;To demonstrate just how poorly earlier generations handled basic character manipulation, here are responses from several OpenAI models for the prompt &lt;code&gt;Replace all letters "r" in the sentence "I really love a ripe strawberry" with the letter "l", and then convert all letters "l" to "r"&lt;/code&gt;:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Response&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-3.5-turbo&lt;/cell&gt;
        &lt;cell&gt;I lealll love a liple strallbeelly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-4-turbo&lt;/cell&gt;
        &lt;cell&gt;I rearry rove a ripe strawberly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-4o&lt;/cell&gt;
        &lt;cell&gt;I rearry rove a ripe strawberrry&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-4.1&lt;/cell&gt;
        &lt;cell&gt;I rearry rove a ripe strawberry&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-5-nano&lt;/cell&gt;
        &lt;cell&gt;I really love a ripe strawberry&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;gpt-5-mini&lt;/cell&gt;
        &lt;cell&gt;I rearry rove a ripe strawberry&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;gpt-5&lt;/cell&gt;
        &lt;cell&gt;I rearry rove a ripe strawberry&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note that I disabled reasoning for GPT-5 models to make the comparison fairer. Reasoning helps tremendously with similar tasks (and some of the models use chain of thought directly in the output in the absence of reasoning), but I am interested in a generational uplift we observe just from raw model improvements. GPT-5 Nano is the only new generation model that makes a mistake, but given its size, it is perhaps not so surprising. Other than that, we can see that starting with GPT 4.1, models could consistently complete this task without any issues. If you’re curious about the Anthropic models, Claude Sonnet 4 is the first one to crack it. Interestingly, it was released approximately at the same time as GPT 4.1.&lt;/p&gt;
    &lt;head rend="h2"&gt;Counting characters&lt;/head&gt;
    &lt;p&gt;Next, let’s take a look at counting characters. LLMs are notoriously bad at counting, so unsurprisingly, there was only one model that could count the characters reliably in the following sentence: “I wish I could come up with a better example sentence.” The only model was GPT-4.1 - others sometimes counted correctly the number of characters in all the individual words, but then fumbled adding all the numbers up. However, with reasoning set to low, GPT 5 across all sizes (incl. Nano) completes the task correctly. Similarly, Claude Sonnet models complete the task without problems if they are allowed to reason.&lt;/p&gt;
    &lt;p&gt;We see a similar story when we ask the models to count specific characters. Counting r’s in the r-ified strawberry sentence is correct most of the times for GPT 5 in all sizes, again including Nano and even without reasoning. However, it is less consistent and when you throw another curveball (such as changing strawberry to strawberrry), the results are mixed - but this time it’s not a problem of arithmetic (adding individual counts up), but rather identification of r’s in a word itself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Base64 and ROT13&lt;/head&gt;
    &lt;p&gt;Knowing the limitations of LLMs, I set out to test them on a task that wasn’t too complex yet still showcases their capabilities. To make the test more interesting, I chose to use two layers: As the outer (encoding) layer, I chose Base64, which is a widely used encoding algorithm, and consequently one that LLMs learned to work with very early (albeit not perfectly), despite us not being quite sure how. The inner (encryption) layer was ROT20, a variation of the ROT13 cipher: a simple letter substitution cipher also known as Caesar cipher. You wouldn’t really want to encrypt anything important using this cipher, as it is fairly trivial to crack, but it’s perfect for our tests.&lt;/p&gt;
    &lt;p&gt;Our test sentence was “Hi, how are you doing? Do you understand the cipher?”. Encoded with ROT20, it reads “Bc, biq uly sio xicha? Xi sio ohxylmnuhx nby wcjbyl?”, and finally, when encoded with Base64, we get:&lt;code&gt;QmMsIGJpcSB1bHkgc2lvIHhpY2hhPyBYaSBzaW8gb2h4eWxtbnVoeCBuYnkgd2NqYnlsPw==&lt;/code&gt;. We consider it a success if the LLM can respond to our message (in plain text English, or using the same encoding), or if it at least can decode the message.&lt;/p&gt;
    &lt;p&gt;I set up the experiment in two ways: In the first variant, I gave the model nothing but the Base64 string. This variant is harder, since the LLM is not given any indication of what language the message could be written in. This is hugely helpful when decoding substitution ciphers, since you can orient yourself by the most common words in the language, such as “a”, “an”, “the”, “I”, “to”, “of” etc. in English. The other variant prepended it with “Deciper and answer this: “. However, there were no practical differences in the results, only one model (Qwen 235B) needed the “decode” nudge. Instead, I saw most of the models fail on the Base64 decoding, most likely because the text did not resemble normal language, making validation of successful decoding more difficult.&lt;/p&gt;
    &lt;p&gt;Below I provide separate results for decoding Base64 (i.e. did it unpack to the correct ROT20 text?) and also just for doing the “inner” ROT20 decipher (queried separately without Base64 encoding).&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Base64 decode&lt;/cell&gt;
        &lt;cell role="head"&gt;ROT20 decipher&lt;/cell&gt;
        &lt;cell role="head"&gt;Base64+ROT20 result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-3.5-turbo&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-4-turbo&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-4o&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-4.1&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5-nano&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5-mini&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5-nano (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5-mini (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gpt-5 (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;claude-sonnet-3.5&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;claude-sonnet-3.7&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;claude-sonnet-4&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;claude-sonnet-4.5&lt;/cell&gt;
        &lt;cell&gt;Safety fail&lt;/cell&gt;
        &lt;cell&gt;Safety fail&lt;/cell&gt;
        &lt;cell&gt;Safety fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gemini-2.5-flash&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gemini-2.5-flash (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gemini-2.5-pro&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;llama-4-maverick&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;deepseek-v3.2-exp&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;deepseek-v3.2-exp (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;qwen-235b&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;qwen-235b (reasoning)&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;kimi-k2&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
        &lt;cell&gt;Fail*&lt;/cell&gt;
        &lt;cell&gt;Fail&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;grok-4&lt;/cell&gt;
        &lt;cell&gt;Safety fail&lt;/cell&gt;
        &lt;cell&gt;Pass&lt;/cell&gt;
        &lt;cell&gt;Safety fail&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here are a few comments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Sonnet 4.5 refuses to touch anything that does not resemble normal text, be it Base64 or ROT-encrypted text. Base64 is one of the many methods of trying to obfuscate the code and fool any keyword filters or LLM safety judges, but this highly sensitive approach could make Claude Sonnet 4.5 unusable on rarer languages. Grok 4 suffered from the same issue, but refused only Base64 text.&lt;/item&gt;
      &lt;item&gt;Chinese reasoning models have very lengthy internal monologues: Solving the ROT20 cipher usually consumed around 3K tokens, and when combined with the Base64 encoding, the output often reached 6-7K tokens.&lt;/item&gt;
      &lt;item&gt;Some models, such as Kimi K2, did not technically complete the ROT20 decryption, but were on the right track and provided functional Python code for the user to figure that out. Still a fail, but failing gracefully.&lt;/item&gt;
      &lt;item&gt;I used the default temperature settings, which can cause issues with decoding even in SOTA models, albeit in a small percentage of cases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What have we learned?&lt;/head&gt;
    &lt;p&gt;To me, there are two interesting observations: newer/larger models are better at generalizing Base64 encoding and decoding, and they’re also becoming more adept at manipulating text at the character level.&lt;/p&gt;
    &lt;p&gt;Most current-generation models, especially the larger ones, are able to decode Base64 text. What is especially interesting, though, is that I tested on what looks like gibberish (ROT20 encoded text), so the model’s knowledge of the Base64 decoding algorithm isn’t merely memorization of the patterns for the most common English words, as was suggested in earlier literature. This may have been the case for older/smaller models: I tested the sentence “Hey! This is Tom, I have a blog about tech, AI and privacy that you should definitely check out.” - and many of the models which failed the Base64 test above (like GPT 4o, GPT 5 Nano or DeepSeek V3.2 Exp) were actually able to decode it fine from Base64. However, SOTA models can now decode out-of-distribution texts from Base64, suggesting they have working understanding of the algorithm, not just memorized translation patterns from English words.&lt;/p&gt;
    &lt;p&gt;The models are also becoming more adept at manipulating text at the character level, despite their understanding of text being based on tokens. Substitution of characters, whether at an individual level (the strawberry sentence) or when decoding substitution ciphers, is a task that they now complete successfully fairly reliably. I cannot provide an explanation of why that happens (please let me know if you have any ideas), but empirically that’s what seems to be happening. Reasoning models and tool use further increase LLMs capabilities for manipulating text (as is the case in many other areas), but it is clear that the new capabilities are baked into the base models regardless of these extra features. While character-level operations are far from a solved problem for LLMs, it is fascinating to see the progress they make in this area.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45572478</guid><pubDate>Mon, 13 Oct 2025 19:39:14 +0000</pubDate></item><item><title>Sony PlayStation 2 fixing frenzy</title><link>https://retrohax.net/sony-playstation-2-fixing-frenzy/</link><description>&lt;doc fingerprint="bf44567302a48d5f"&gt;
  &lt;main&gt;
    &lt;p&gt;… or writing a blog post with old pics and data again&lt;/p&gt;
    &lt;head rend="h2"&gt;Intro&lt;/head&gt;
    &lt;p&gt;Yeah, again, a blog post based on years-old pics. I still have at least 20 blog posts to write that will be based on stuff that I’ve worked few years ago 😀&lt;lb/&gt;I guess, chasing my own tail is never-ending 😀&lt;/p&gt;
    &lt;head rend="h2"&gt;Some Playstations that I bought the other day…&lt;/head&gt;
    &lt;p&gt;I once wrote a blog post about Nintendo Wii controllers. That post was already based on old data, and the job presented in this blog post is from roughly the same period of time when I bought around 60 various consoles to learn how to fix them and to show them to my son.&lt;/p&gt;
    &lt;p&gt;I had around 9 PlayStation 2 units. Seven were the FAT PS2s, and I will be covering only fixes/mods that I find a bit more interesting than the usual clean&amp;amp;run scenario. Obviously, a lot of time passed, and nowadays there are way better mods available, but anyway, let’s go.&lt;/p&gt;
    &lt;p&gt;Lemme first show you how it all looked.&lt;/p&gt;
    &lt;p&gt;The main issues to sort out were rather simple. Cleaning, broken plastic, dead fans, dead RTC battery cells, etc.&lt;lb/&gt;Below are some pics after disassembly.&lt;/p&gt;
    &lt;p&gt;Broken thermal pads everywhere.&lt;/p&gt;
    &lt;p&gt;A bit of corrosion here and there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fixes&lt;/head&gt;
    &lt;p&gt;PS2 motherboards are well-manufactured and generally free from issues. In my case, all was working fine.&lt;/p&gt;
    &lt;p&gt;However, I had to clean them nicely, replace the original Sony batteries, and replace thermal pads.&lt;/p&gt;
    &lt;p&gt;Some pics below.&lt;/p&gt;
    &lt;p&gt;Replacing thermal pads.&lt;/p&gt;
    &lt;p&gt;Some drives needed a new laser module. The original is KHS-400C, but replacements are still available.&lt;/p&gt;
    &lt;p&gt;I’ve bought some on Aliexpress.&lt;/p&gt;
    &lt;p&gt;Some plastic parts had to be restored to be fully functional. I’ve used acetone, some wire reinforcement, and a soldering iron to fix it.&lt;/p&gt;
    &lt;p&gt;One of the drives had a chipped-off plastic rail. I’ve temporarily glued “sides” that served as a mold-in-place, filled it with a mix of sodium bicarbonate and cyanoacrylic glue, and used a hand file to trim it to the desired shape.&lt;/p&gt;
    &lt;p&gt;Meanwhile, cleaned cases were waiting 🙂&lt;/p&gt;
    &lt;p&gt;Next, I had to address issues with power switches. Connectors were tarnished, so I had to disassemble them and clean the contacts.&lt;/p&gt;
    &lt;p&gt;After that, I was ready to assemble all units and start adding some mods.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mods&lt;/head&gt;
    &lt;p&gt;The mods are rather basic and mandatory in modern days, and have also been covered many times on YouTube. It all basically boils down to the installation of FMCB or FHDB, which is a great solution!&lt;/p&gt;
    &lt;p&gt;To make it work, I needed some hard drives, memcards, and HDD adapters.&lt;/p&gt;
    &lt;p&gt;I’ve bought some brand new 1TB SATA drives along with HDD adapters that are easily available. I had a bit of trouble finding nice and original memcards, but that was sorted out, too.&lt;/p&gt;
    &lt;p&gt;Next, I had to prepare a drive to work nicely with PS2. For testing purposes, I’ve used a 2.5-inch drive in a 3D printed bracket.&lt;/p&gt;
    &lt;p&gt;Software used back then to format the drive is Window HDL Image Install Program V1.7.6 By Gadgetfreak&lt;/p&gt;
    &lt;p&gt;After the above, I’ve used the HDD Raw copy tool to put a downloaded FHDB image.&lt;/p&gt;
    &lt;p&gt;IIRC, this can also be done from the FMCB menu under running PS2, but I wanted to test a PC software.&lt;/p&gt;
    &lt;p&gt;Now, you can dump images of your original games and put them on the hard drive.&lt;/p&gt;
    &lt;head rend="h2"&gt;Controllers&lt;/head&gt;
    &lt;p&gt;I had quite a few controllers, but I needed to test them first and fix them if broken.&lt;/p&gt;
    &lt;p&gt;Having a working PS2 with FMCB or FHDB is handy, as I could upload a PS2 controller tester by jbit_ and check all the pads.&lt;/p&gt;
    &lt;p&gt;Some controllers are originally painted with a rubber-like cover that, unfortunately, degrades with time and becomes a sticky gooey. I usually deal with it with the help of Methanol. It nicely removes it. However, such a cleaned surface has to be painted with an undercoating and the desired color afterward. This is exactly what I did with one set of controllers that I had.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finale&lt;/head&gt;
    &lt;p&gt;Out of the six PS2 consoles, I’ve created 6 sets in the following configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PS2 FAT – new RTC battery, new thermal pads, laser module&lt;/item&gt;
      &lt;item&gt;1TB SATA HDD + HDD adapter&lt;/item&gt;
      &lt;item&gt;original 8MB Sony memcard&lt;/item&gt;
      &lt;item&gt;new power wire&lt;/item&gt;
      &lt;item&gt;Controller&lt;/item&gt;
      &lt;item&gt;PS2HDMI dongle&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately, nobody wanted to buy these even for a cost that would cover all the expenses spent on this project. On the bright side, I have learned a lot, plus, I have a cool PS2 to play with 🙂&lt;/p&gt;
    &lt;head rend="h2"&gt;Outro&lt;/head&gt;
    &lt;p&gt;See you in the next post. Hopefully, about the machine that I’ve worked on recently, and I still remember all the details 😀&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45574247</guid><pubDate>Mon, 13 Oct 2025 23:02:06 +0000</pubDate></item><item><title>DDoS Botnet Aisuru Blankets US ISPs in Record DDoS</title><link>https://krebsonsecurity.com/2025/10/ddos-botnet-aisuru-blankets-us-isps-in-record-ddos/</link><description>&lt;doc fingerprint="c70581cbb810e0c3"&gt;
  &lt;main&gt;
    &lt;p&gt;The world’s largest and most disruptive botnet is now drawing a majority of its firepower from compromised Internet-of-Things (IoT) devices hosted on U.S. Internet providers like AT&amp;amp;T, Comcast and Verizon, new evidence suggests. Experts say the heavy concentration of infected devices at U.S. providers is complicating efforts to limit collateral damage from the botnet’s attacks, which shattered previous records this week with a brief traffic flood that clocked in at nearly 30 trillion bits of data per second.&lt;/p&gt;
    &lt;p&gt;Since its debut more than a year ago, the Aisuru botnet has steadily outcompeted virtually all other IoT-based botnets in the wild, with recent attacks siphoning Internet bandwidth from an estimated 300,000 compromised hosts worldwide.&lt;/p&gt;
    &lt;p&gt;The hacked systems that get subsumed into the botnet are mostly consumer-grade routers, security cameras, digital video recorders and other devices operating with insecure and outdated firmware, and/or factory-default settings. Aisuru’s owners are continuously scanning the Internet for these vulnerable devices and enslaving them for use in distributed denial-of-service (DDoS) attacks that can overwhelm targeted servers with crippling amounts of junk traffic.&lt;/p&gt;
    &lt;p&gt;As Aisuru’s size has mushroomed, so has its punch. In May 2025, KrebsOnSecurity was hit with a near-record 6.35 terabits per second (Tbps) attack from Aisuru, which was then the largest assault that Google’s DDoS protection service Project Shield had ever mitigated. Days later, Aisuru shattered that record with a data blast in excess of 11 Tbps.&lt;/p&gt;
    &lt;p&gt;By late September, Aisuru was publicly flexing DDoS capabilities topping 22 Tbps. Then on October 6, its operators heaved a whopping 29.6 terabits of junk data packets each second at a targeted host. Hardly anyone noticed because it appears to have been a brief test or demonstration of Aisuru’s capabilities: The traffic flood lasted less only a few seconds and was pointed at an Internet server that was specifically designed to measure large-scale DDoS attacks.&lt;/p&gt;
    &lt;p&gt;Aisuru’s overlords aren’t just showing off. Their botnet is being blamed for a series of increasingly massive and disruptive attacks. Although recent assaults from Aisuru have targeted mostly ISPs that serve online gaming communities like Minecraft, those digital sieges often result in widespread collateral Internet disruption.&lt;/p&gt;
    &lt;p&gt;For the past several weeks, ISPs hosting some of the Internet’s top gaming destinations have been hit with a relentless volley of gargantuan attacks that experts say are well beyond the DDoS mitigation capabilities of most organizations connected to the Internet today.&lt;/p&gt;
    &lt;p&gt;Steven Ferguson is principal security engineer at Global Secure Layer (GSL), an ISP in Brisbane, Australia. GSL hosts TCPShield, which offers free or low-cost DDoS protection to more than 50,000 Minecraft servers worldwide. Ferguson told KrebsOnSecurity that on October 8, TCPShield was walloped with a blitz from Aisuru that flooded its network with more than 15 terabits of junk data per second.&lt;/p&gt;
    &lt;p&gt;Ferguson said that after the attack subsided, TCPShield was told by its upstream provider OVH that they were no longer welcome as a customer.&lt;/p&gt;
    &lt;p&gt;“This was causing serious congestion on their Miami external ports for several weeks, shown publicly via their weather map,” he said, explaining that TCPShield is now solely protected by GSL.&lt;/p&gt;
    &lt;p&gt;Traces from the recent spate of crippling Aisuru attacks on gaming servers can be still seen at the website blockgametracker.gg, which indexes the uptime and downtime of the top Minecraft hosts. In the following example from a series of data deluges on the evening of September 28, we can see an Aisuru botnet campaign briefly knocked TCPShield offline.&lt;/p&gt;
    &lt;p&gt;Paging through the same uptime graphs for other network operators listed shows almost all of them suffered brief but repeated outages around the same time. Here is the same uptime tracking for Minecraft servers on the network provider Cosmic (AS30456), and it shows multiple large dips that correspond to game server outages caused by Aisuru.&lt;/p&gt;
    &lt;head rend="h2"&gt;BOTNETS R US&lt;/head&gt;
    &lt;p&gt;Ferguson said he’s been tracking Aisuru for about three months, and recently he noticed the botnet’s composition shifted heavily toward infected systems at ISPs in the United States. Ferguson shared logs from an attack on October 8 that indexed traffic by the total volume sent through each network provider, and the logs showed that 11 of the top 20 traffic sources were U.S. based ISPs.&lt;/p&gt;
    &lt;p&gt;AT&amp;amp;T customers were by far the biggest U.S. contributors to that attack, followed by botted systems on Charter Communications, Comcast, T-Mobile and Verizon, Ferguson found. He said the volume of data packets per second coming from infected IoT hosts on these ISPs is often so high that it has started to affect the quality of service that ISPs are able to provide to adjacent (non-botted) customers.&lt;/p&gt;
    &lt;p&gt;“The impact extends beyond victim networks,” Ferguson said. “For instance we have seen 500 gigabits of traffic via Comcast’s network alone. This amount of egress leaving their network, especially being so US-East concentrated, will result in congestion towards other services or content trying to be reached while an attack is ongoing.”&lt;/p&gt;
    &lt;p&gt;Roland Dobbins is principal engineer at Netscout. Dobbins said Ferguson is spot on, noting that while most ISPs have effective mitigations in place to handle large incoming DDoS attacks, many are far less prepared to manage the inevitable service degradation caused by large numbers of their customers suddenly using some or all available bandwidth to attack others.&lt;/p&gt;
    &lt;p&gt;“The outbound and cross-bound DDoS attacks can be just as disruptive as the inbound stuff,” Dobbin said. “We’re now in a situation where ISPs are routinely seeing terabit-per-second plus outbound attacks from their networks that can cause operational problems.”&lt;/p&gt;
    &lt;p&gt;“The crying need for effective and universal outbound DDoS attack suppression is something that is really being highlighted by these recent attacks,” Dobbins continued. “A lot of network operators are learning that lesson now, and there’s going to be a period ahead where there’s some scrambling and potential disruption going on.”&lt;/p&gt;
    &lt;p&gt;KrebsOnSecurity sought comment from the ISPs named in Ferguson’s report. Charter Communications pointed to a recent blog post on protecting its network, stating that Charter actively monitors for both inbound and outbound attacks, and that it takes proactive action wherever possible.&lt;/p&gt;
    &lt;p&gt;“In addition to our own extensive network security, we also aim to reduce the risk of customer connected devices contributing to attacks through our Advanced WiFi solution that includes Security Shield, and we make Security Suite available to our Internet customers,” Charter wrote in an emailed response to questions. “With the ever-growing number of devices connecting to networks, we encourage customers to purchase trusted devices with secure development and manufacturing practices, use anti-virus and security tools on their connected devices, and regularly download security patches.”&lt;/p&gt;
    &lt;p&gt;A spokesperson for Comcast responded, “Currently our network is not experiencing impacts and we are able to handle the traffic.”&lt;/p&gt;
    &lt;head rend="h2"&gt;9 YEARS OF MIRAI&lt;/head&gt;
    &lt;p&gt;Aisuru is built on the bones of malicious code that was leaked in 2016 by the original creators of the Mirai IoT botnet. Like Aisuru, Mirai quickly outcompeted all other DDoS botnets in its heyday, and obliterated previous DDoS attack records with a 620 gigabit-per-second siege that sidelined this website for nearly four days in 2016.&lt;/p&gt;
    &lt;p&gt;The Mirai botmasters likewise used their crime machine to attack mostly Minecraft servers, but with the goal of forcing Minecraft server owners to purchase a DDoS protection service that they controlled. In addition, they rented out slices of the Mirai botnet to paying customers, some of whom used it to mask the sources of other types of cybercrime, such as click fraud.&lt;/p&gt;
    &lt;p&gt;Dobbins said Aisuru’s owners also appear to be renting out their botnet as a distributed proxy network that cybercriminal customers anywhere in the world can use to anonymize their malicious traffic and make it appear to be coming from regular residential users in the U.S.&lt;/p&gt;
    &lt;p&gt;“The people who operate this botnet are also selling (it as) residential proxies,” he said. “And that’s being used to reflect application layer attacks through the proxies on the bots as well.”&lt;/p&gt;
    &lt;p&gt;The Aisuru botnet harkens back to its predecessor Mirai in another intriguing way. One of its owners is using the Telegram handle “9gigsofram,” which corresponds to the nickname used by the co-owner of a Minecraft server protection service called Proxypipe that was heavily targeted in 2016 by the original Mirai botmasters.&lt;/p&gt;
    &lt;p&gt;Robert Coelho co-ran Proxypipe back then along with his business partner Erik “9gigsofram” Buckingham, and has spent the past nine years fine-tuning various DDoS mitigation companies that cater to Minecraft server operators and other gaming enthusiasts. Coelho said he has no idea why one of Aisuru’s botmasters chose Buckingham’s nickname, but added that it might say something about how long this person has been involved in the DDoS-for-hire industry.&lt;/p&gt;
    &lt;p&gt;“The Aisuru attacks on the gaming networks these past seven day have been absolutely huge, and you can see tons of providers going down multiple times a day,” Coelho said.&lt;/p&gt;
    &lt;p&gt;Coelho said the 15 Tbps attack this week against TCPShield was likely only a portion of the total attack volume hurled by Aisuru at the time, because much of it would have been shoved through networks that simply couldn’t process that volume of traffic all at once. Such outsized attacks, he said, are becoming increasingly difficult and expensive to mitigate.&lt;/p&gt;
    &lt;p&gt;“It’s definitely at the point now where you need to be spending at least a million dollars a month just to have the network capacity to be able to deal with these attacks,” he said.&lt;/p&gt;
    &lt;head rend="h2"&gt;RAPID SPREAD&lt;/head&gt;
    &lt;p&gt;Aisuru has long been rumored to use multiple zero-day vulnerabilities in IoT devices to aid its rapid growth over the past year. XLab, the Chinese security company that was the first to profile Aisuru’s rise in 2024, warned last month that one of the Aisuru botmasters had compromised the firmware distribution website for Totolink, a maker of low-cost routers and other networking gear.&lt;/p&gt;
    &lt;p&gt;“Multiple sources indicate the group allegedly compromised a router firmware update server in April and distributed malicious scripts to expand the botnet,” XLab wrote on September 15. “The node count is currently reported to be around 300,000.”&lt;/p&gt;
    &lt;p&gt;Aisuru’s operators received an unexpected boost to their crime machine in August when the U.S. Department Justice charged the alleged proprietor of Rapper Bot, a DDoS-for-hire botnet that competed directly with Aisuru for control over the global pool of vulnerable IoT systems.&lt;/p&gt;
    &lt;p&gt;Once Rapper Bot was dismantled, Aisuru’s curators moved quickly to commandeer vulnerable IoT devices that were suddenly set adrift by the government’s takedown, Dobbins said.&lt;/p&gt;
    &lt;p&gt;“Folks were arrested and Rapper Bot control servers were seized and that’s great, but unfortunately the botnet’s attack assets were then pieced out by the remaining botnets,” he said. “The problem is, even if those infected IoT devices are rebooted and cleaned up, they will still get re-compromised by something else generally within minutes of being plugged back in.”&lt;/p&gt;
    &lt;head rend="h2"&gt;BOTMASTERS AT LARGE&lt;/head&gt;
    &lt;p&gt;XLab’s September blog post cited multiple unnamed sources saying Aisuru is operated by three cybercriminals: “Snow,” who’s responsible for botnet development; “Tom,” tasked with finding new vulnerabilities; and “Forky,” responsible for botnet sales.&lt;/p&gt;
    &lt;p&gt;KrebsOnSecurity interviewed Forky in our May 2025 story about the record 6.3 Tbps attack from Aisuru. That story identified Forky as a 21-year-old man from Sao Paulo, Brazil who has been extremely active in the DDoS-for-hire scene since at least 2022. The FBI has seized Forky’s DDoS-for-hire domains several times over the years.&lt;/p&gt;
    &lt;p&gt;Like the original Mirai botmasters, Forky also operates a DDoS mitigation service called Botshield. Forky declined to discuss the makeup of his ISP’s clientele, or to clarify whether Botshield was more of a hosting provider or a DDoS mitigation firm. However, Forky has posted on Telegram about Botshield successfully mitigating large DDoS attacks launched against other DDoS-for-hire services.&lt;/p&gt;
    &lt;p&gt;In our previous interview, Forky acknowledged being involved in the development and marketing of Aisuru, but denied participating in attacks launched by the botnet.&lt;/p&gt;
    &lt;p&gt;Reached for comment earlier this month, Forky continued to maintain his innocence, claiming that he also is still trying to figure out who the current Aisuru botnet operators are in real life (Forky said the same thing in our May interview).&lt;/p&gt;
    &lt;p&gt;But after a week of promising juicy details, Forky came up empty-handed once again. Suspecting that Forky was merely being coy, I asked him how someone so connected to the DDoS-for-hire world could still be mystified on this point, and suggested that his inability or unwillingness to blame anyone else for Aisuru would not exactly help his case.&lt;/p&gt;
    &lt;p&gt;At this, Forky verbally bristled at being pressed for more details, and abruptly terminated our interview.&lt;/p&gt;
    &lt;p&gt;“I’m not here to be threatened with ignorance because you are stressed,” Forky replied. “They’re blaming me for those new attacks. Pretty much the whole world (is) due to your blog.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45574393</guid><pubDate>Mon, 13 Oct 2025 23:21:23 +0000</pubDate></item><item><title>StreamingVLM: Real-Time Understanding for Infinite Video Streams</title><link>https://arxiv.org/abs/2510.09608</link><description>&lt;doc fingerprint="6c9e3c26c1f30e8c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computer Vision and Pattern Recognition&lt;/head&gt;&lt;p&gt; [Submitted on 10 Oct 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:StreamingVLM: Real-Time Understanding for Infinite Video Streams&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at this https URL.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.CV&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45574705</guid><pubDate>Tue, 14 Oct 2025 00:02:18 +0000</pubDate></item><item><title>There are sensitive internal links in the clear on GEO satellites [pdf]</title><link>https://satcom.sysnet.ucsd.edu/docs/dontlookup_ccs25_fullpaper.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45575391</guid><pubDate>Tue, 14 Oct 2025 01:48:56 +0000</pubDate></item><item><title>Modifying a Casio F-Series Digital Watch (2020)</title><link>https://shellzine.net/casio-f-series-mods/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45575742</guid><pubDate>Tue, 14 Oct 2025 02:48:09 +0000</pubDate></item><item><title>Show HN: Wordle but you have to predict your score before playing</title><link>https://boring.game/invite/SRhyUStjin</link><description>&lt;doc fingerprint="d4f2f9d329fee6f9"&gt;
  &lt;main&gt;&lt;p&gt;Team Invitation&lt;/p&gt;&lt;head rend="h2"&gt;You've been invited to join&lt;/head&gt;&lt;head rend="h1"&gt;Hacker News&lt;/head&gt;&lt;p&gt;This team currently has 3 members&lt;/p&gt;&lt;p&gt;Sign in or create an account to join this team&lt;/p&gt;&lt;head rend="h3"&gt;Team Benefits&lt;/head&gt;&lt;p&gt;â&lt;/p&gt; Compete with team members in challenges&lt;p&gt;â&lt;/p&gt; View team statistics and rankings&lt;p&gt;â&lt;/p&gt; Compare daily scores with teammates&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45575938</guid><pubDate>Tue, 14 Oct 2025 03:26:09 +0000</pubDate></item></channel></rss>