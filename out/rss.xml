<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 07 Jan 2026 16:53:18 +0000</lastBuildDate><item><title>Electronic nose for indoor mold detection and identification</title><link>https://advanced.onlinelibrary.wiley.com/doi/10.1002/adsr.202500124</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46520935</guid><pubDate>Wed, 07 Jan 2026 00:31:01 +0000</pubDate></item><item><title>Optery (YC W22) Hiring a CISO and Web Scraping Engineers (Node) (US and Latam)</title><link>https://www.optery.com/careers/</link><description>&lt;doc fingerprint="ca6bb85f43b74372"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Careers&lt;/head&gt;
    &lt;p&gt;üí°Page not loading? Optery‚Äôs Career page uses Cookies to display the full page content. If you‚Äôre not seeing anything, try opening the cookie banner (cookie icon in the bottom left corner) and Accept Personalization cookies.&lt;/p&gt;
    &lt;p&gt;üí°Page not loading? Optery‚Äôs Career page uses Cookies to display the full page content. If you‚Äôre not seeing anything, try opening the cookie banner (cookie icon in the bottom left corner) and Accept Personalization cookies.&lt;/p&gt;
    &lt;p&gt;Ready to safeguard your personal data?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46525394</guid><pubDate>Wed, 07 Jan 2026 12:00:20 +0000</pubDate></item><item><title>The Eric and Wendy Schmidt Observatory System</title><link>https://www.schmidtsciences.org/schmidt-observatory-system/</link><description>&lt;doc fingerprint="f611b05667864f3b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Eric and Wendy Schmidt Observatory System&lt;/head&gt;
    &lt;p&gt;Four novel observatories expanding access and enabling new ways to explore the cosmos&lt;/p&gt;
    &lt;p&gt;The Eric and Wendy Schmidt Observatory System is designed to pioneer a new paradigm for astronomical observatories, fundamentally rethinking how they are conceived, developed, and utilized. This initiative compresses development timelines from decades to years, dramatically lowering barriers to global participation and accelerating the pace of discovery. By uniting rapid development cycles with open data and shared scientific tools, the system empowers researchers everywhere to engage in frontier astrophysics.&lt;/p&gt;
    &lt;head rend="h2"&gt; Strategic Pillars &lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Rapid observatory development leveraging risk-tolerant technical innovation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Modular designs that leverage economies of scale&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open data and software for global access&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Global, cross-disciplinary scientific collaboration&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt; Major Projects &lt;/head&gt;
    &lt;p&gt;Through more accessible and responsive scientific infrastructure, the Eric and Wendy Schmidt Observatory System seeks to support discovery for the benefit of all. Explore our projects by clicking the tiles below.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Argus Array&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Deep Synoptic Array (DSA)&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Large Fiber Array Spectroscopic Telescope (LFAST)&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Lazuli Space Observatory&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;FirstLight Awards&lt;/head&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46525542</guid><pubDate>Wed, 07 Jan 2026 12:19:01 +0000</pubDate></item><item><title>‚ÄúStop Designing Languages. Write Libraries Instead‚Äù (2016)</title><link>https://lbstanza.org/purpose_of_programming_languages.html</link><description>&lt;doc fingerprint="2fc219af0a3a92d0"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;HomePhilosophyDownloadsDocumentationPeopleCommunityNewsReference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;NAVIGATION&lt;/head&gt;
          &lt;head&gt;"Stop Designing Languages. Write Libraries Instead."&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;"Stop Designing Languages. Write Libraries Instead."&lt;/head&gt;
          &lt;p&gt;Patrick S. Li - May 29, 2016&lt;/p&gt;
          &lt;p&gt;I had a friend tell me recently that all programming languages seem very similar to each other. They all have variables, and arrays, a few loop constructs, functions, and some arithmetic constructs. Sure, some languages have fancier features like first-class functions or coroutines, but he doesn't consider himself an expert programmer anyway and doesn't use those features.&lt;/p&gt;
          &lt;p&gt;What really makes a programming language productive for him, he says, are the libraries it comes with. For example, he got into programming by using the popular Ruby on Rails web framework. There is no way that he could have written a full database-driven web stack by himself, nor is he interested in doing so. But thanks to Ruby on Rails, he doesn't have to! So he said that he has no particular opinion about the Ruby programming language, but he absolutely loves Rails. The vast majority of programmers are non-experts, like himself, and the largest gains in productivity for non-experts come from having a wide spectrum of easy-to-use libraries. Subtle language features like first-class functions, and object systems, are lost on them because they don't really use them anyway. Computer scientists should really be spending their time developing new libraries rather than inventing new programming languages.&lt;/p&gt;
          &lt;p&gt;My friend's opinion about programming languages is a common one, and I have heard it repeatedly from experts and non-experts alike. Being a language designer myself, I, of course, don't share this opinion. Here is what I consider to be the purpose of a general-purpose programming language.&lt;/p&gt;
          &lt;p&gt;To start off, I would say that my friend's opinion is completely correct, just incomplete. The greatest productivity gains are indeed the result of having a wide spectrum of libraries. Ruby on Rails is a fantastic framework, and it has enabled thousands (if not millions) of non-experts to build sophisticated websites quickly. So the natural question then is, why isn't there now a Rails framework for every programming language?&lt;/p&gt;
          &lt;p&gt;Some languages that are semantically similar to Ruby do have their own web frameworks. Python, for example, has Django. But as of now, there is still no decent web framework for Java that is as easy to use as Ruby on Rails. Why is that? Are Java developers just not as competent as Ruby programmers? If David Hansson could design and develop Rails by himself, why can't a group of programmers just copy the design to Java? What makes this even more embarrassing is the fact that Java initially marketed itself as the web programming language, because of its applet technology. To emphasize this point, let me add that there is no good web framework for C either, and it is unlikely that there ever will be. Let me assure you that it's not because C programmers are worse than Ruby programmers.&lt;/p&gt;
          &lt;p&gt;Economics is not the reason either. The Tiobe index lists Java and C as the most widely used programming languages today, with Ruby coming in eighth place. There are many times more Java and C programmers than there are Ruby programmers. If someone would just write Java on Rails their framework would have many times more users than Ruby on Rails, and it would instantly propel him to internet fame and fortune.&lt;/p&gt;
          &lt;p&gt;So it's not because of incompetency. Nor is it because of economics. So why else wouldn't someone port Ruby on Rails to Java? Well, simply, because they can't.&lt;/p&gt;
          &lt;p&gt;If you're a knowledgeable Ruby programmer and you take a deep look through an introductory Rails tutorial, you'll notice that pretty much all of the Ruby language features come into play in some way. Rail's ActiveRecords library makes pervasive use of Ruby's meta-programming features. Rail's template system heavily relies upon Ruby's runtime evaluation features. To make your website respond to a user click, you subclass &lt;/p&gt;
          &lt;p&gt;So, completely unbeknownst to my friend, he is actually making heavy use of all those subtle language features that he claimed he never cared about. And this is intentional! Ruby on Rails was designed to make it possible to build websites without understanding type theory, or memory management, or object-oriented design patterns. Rails allow website designers to focus on designing websites, not managing their software infrastructure. My friend is enjoying all the benefits of Ruby without even knowing it, and that's the whole point.&lt;/p&gt;
          &lt;p&gt;Taking a step back, the concept of packaging code into easy-to-use libraries is not new. It's been around even in the days when programs were stored on punched paper tape. There are still vast libraries of assembly code containing useful subroutines. And every programming language ever designed provided some way for common functionality to be reused. To me, this is the primary purpose of a general-purpose programming language, to enable the creation of a wide spectrum of easy-to-use libraries.&lt;/p&gt;
          &lt;p&gt;The design of the programming language directly determines what sort of libraries you can write and how easy they are to use in the end. In the C language, the only major feature provided for enabling reuse is the ability to declare and call functions. So guess what? The majority of C libraries are basically large collections of functions. Ruby on Rails provides a concise way for expressing: do this when the button is clicked. The "do this" part is implemented in Ruby as a first-class function. How would it be implemented in languages like Java which don't support them? Well, the behaviour of first-class functions can be mocked by defining a new event handler class with a single &lt;/p&gt;
          &lt;p&gt;In the early days of software, collections of functions were sufficient in allowing us to code reusable components. A lot of early software was numerical in nature, and there was a library function for every numerical algorithm you would want to run. Numbers go in. Numbers come out. Functions were perfectly adequate for this. Unix and C were also designed in a time when the majority of computing happens in batch mode. You prepare some input data, call a function or run a program, and you get some output data back. But computing has changed radically since the 70's. Nowadays, most interesting programs are interactive. When a user clicks a button, it should do something. It was rare to want to extend the functionality of a library of the 70's. The library provides a collection of useful functions. If one of them does what you want, then use it. If not, then write your own. But with the advent of interactive software, the need for extensible libraries became apparent. Programmers wanted GUI libraries that allowed them to say: when a user clicks a button, please run my code. Java (and C++) provides a limited method for extending an existing library's functionality through its subclassing mechanism. So using a Java library often consists of subclassing a number of magical classes and then overriding a number of magical methods. This style of library became so pervasive at one point that we even gave them a new name. They're called frameworks.&lt;/p&gt;
          &lt;p&gt;I surmise that probably many general purpose programming languages were originally designed because of the author's inability to write a good library for the language that he was using at the time. The initial impetus that got me thinking about designing Stanza, for example, came out of my frustrations with trying to write an easy-to-use game programming library in Java. To handle concurrency, traditional game programming frameworks required sprite behaviours to be programmed using a state machine model. But that's not how we intuitively think about sprites in our heads. Intuitively, we think about a character's behaviour as consisting of a sequence of steps. For example, first the character jumps, and then after he lands he looks to his left and then his right for the nearest enemy. If he sees one then he goes to attack it, otherwise he jumps again. He does this three times, and if he doesn't see an enemy after three jumps, then he takes a short nap. Transforming this sequence of steps into a state machine is an incredibly tedious and error-prone process, and most importantly, feels repetitive. It felt like I was doing the same thing again and again. So the natural question is, can I just make this state machine transformation a library and re-use it? It turns out I couldn't, not in Java at least. The language feature that I needed was some sort of coroutine or continuation mechanism. After some research I found that the Scheme language supports continuations, so the Scheme version of my game programming library was much easier to use than the Java version.&lt;/p&gt;
          &lt;p&gt;Because of its support for continuations, the Scheme version of my game library does not require users to write their sprite behaviour as state machines. But it wasn't better than the Java version in every way. Most importantly, the Java version was statically typed and so the compiler automatically caught many of your mistakes for you. The Scheme version didn't have this ability and thus debugging my games took a bit longer. At this point, the right question to ask would be, well can you write a static-typing library for Scheme that then automatically checks your code for type errors? And the current answer, for now and for the foreseeable future, is no. No mainstream language today allows you to write a library to extend its type system. Stanza doesn't either. It just attempts to provide one that is useful for a wider audience.&lt;/p&gt;
          &lt;p&gt;Since the purpose of general-purpose programming languages are to enable the creation of powerful libraries, this means that different languages can also be characterized by what features they provide that cannot be written as libraries. Stanza provides an optional type system, garbage collection, and a multimethod based object system. But if you don't like Stanza's object system, there is no way to write your own. This is one of the main directions of programming language research. Can we design a language so expressive that library writers can easily write the most appropriate object system, or most appropriate type system, to fit their application? Perhaps one day we'll have such a language. Racket and Shen provide mechanisms for extending their type systems and research on meta-object protocols were attempts at designing extensible object systems. So languages are differentiated by what types of libraries you can write in them and what types of libraries you can't.&lt;/p&gt;
          &lt;p&gt;In summary, the purpose of a general-purpose programming language is to enable the creation of powerful and easy-to-use libraries. The more powerful the language, the easier the libraries are to use. Code that makes use of a perfectly tuned library should read almost like a set of instructions for a coworker. So the next time you come across a particularly elegant library, know that many decades of language research has gone into making that possible. If you're curious about specifically which language features a library makes use of, then you can dig deeper, explore, and appreciate the thought that went into its implementation. If you're not curious about all this subtle language stuff, you can safely ignore it all and get on with your work. That's the whole point.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Site design by Luca Li. Copyright 2015.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46525640</guid><pubDate>Wed, 07 Jan 2026 12:29:11 +0000</pubDate></item><item><title>A4 Paper Stories</title><link>https://susam.net/a4-paper-stories.html</link><description>&lt;doc fingerprint="7299db7cc73604b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A4 Paper Stories&lt;/head&gt;
    &lt;p&gt;I sometimes resort to a rather common measuring technique that is neither fast, nor accurate, nor recommended by any standards body and yet it hasn't failed me whenever I have had to use it. I will describe it here, though calling it a technique might be overselling it. Please do not use it for installing kitchen cabinets or anything that will stare back at you every day for the next ten years. It involves one tool: a sheet of A4 paper.&lt;/p&gt;
    &lt;p&gt;Like most sensible people with a reasonable sense of priorities, I do not carry a ruler with me wherever I go. Nevertheless, I often find myself needing to measure something at short notice, usually in situations where a certain amount of inaccuracy is entirely forgivable. When I cannot easily fetch a ruler, I end up doing what many people do and reach for the next best thing, which for me is a sheet of A4 paper, available in abundant supply where I live.&lt;/p&gt;
    &lt;p&gt;From photocopying night-sky charts to serving as a scratch pad for working through mathematical proofs, A4 paper has been a trusted companion since my childhood days. I use it often. If I am carrying a bag, there is almost always some A4 paper inside: perhaps a printed research paper or a mathematical problem I have worked on recently and need to chew on a bit more during my next train ride.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dimensions&lt;/head&gt;
    &lt;p&gt;The dimensions of A4 paper are the solution to a simple, elegant problem. Imagine designing a sheet of paper such that, when you cut it in half parallel to its shorter side, both halves have exactly the same aspect ratio as the original. In other words, if the shorter side has length \( x \) and the longer side has length \( y , \) then \[ \frac{y}{x} = \frac{x}{y / 2} \] which gives us \[ \frac{y}{x} = \sqrt{2}. \] Test it out. Suppose we have \( y/x = \sqrt{2}. \) We cut the paper in half parallel to the shorter side to get two halves, each with shorter side \( x' = y / 2 = x \sqrt{2} / 2 = x / \sqrt{2} \) and longer side \( y' = x. \) Then indeed \[ \frac{y'}{x'} = \frac{x}{x / \sqrt{2}} = \sqrt{2}. \] In fact, we can keep cutting the halves like this and we'll keep getting even smaller sheets with the aspect ratio \( \sqrt{2} \) intact. To summarise, when a sheet of paper has the aspect ratio \( \sqrt{2}, \) bisecting it parallel to the shorter side leaves us with two halves that preserve the aspect ratio. A4 paper has this property.&lt;/p&gt;
    &lt;p&gt;But what are the exact dimensions of A4 and why is it called A4? What does 4 mean here? Like most good answers, this one too begins by considering the numbers \( 0 \) and \( 1. \) Let me elaborate.&lt;/p&gt;
    &lt;p&gt;Let us say we want to make a sheet of paper that is \( 1 \, \mathrm{m}^2 \) in area and has the aspect-ratio-preserving property that we just discussed. What should its dimensions be? We want \[ xy = 1 \, \mathrm{m}^2 \] subject to the condition \[ \frac{y}{x} = \sqrt{2}. \] Solving these two equations gives us \[ x^2 = \frac{1}{\sqrt{2}} \, \mathrm{m}^2 \] from which we obtain \[ x = \frac{1}{\sqrt[4]{2}} \, \mathrm{m}, \quad y = \sqrt[4]{2} \, \mathrm{m}. \] Up to three decimal places, this amounts to \[ x = 0.841 \, \mathrm{m}, \quad y = 1.189 \, \mathrm{m}. \] These are the dimensions of A0 paper. They are precisely the dimensions specified by the ISO standard for it. It is quite large to scribble mathematical solutions on, unless your goal is to make a spectacle of yourself and cause your friends and family to reassess your sanity. So we need something smaller that allows us to work in peace, without inviting commentary or concerns from passersby. We take the A0 paper of size \[ 84.1 \, \mathrm{cm} \times 118.9 \, \mathrm{cm} \] and bisect it to get A1 paper of size \[ 59.4 \, \mathrm{cm} \times 84.1 \, \mathrm{cm}. \] Then we bisect it again to get A2 paper with dimensions \[ 42.0 \, \mathrm{cm} \times 59.4 \, \mathrm{cm}. \] And once again to get A3 paper with dimensions \[ 29.7 \, \mathrm{cm} \times 42.0 \, \mathrm{cm}. \] And then once again to get A4 paper with dimensions \[ 21.0 \, \mathrm{cm} \times 29.7 \, \mathrm{cm}. \] There we have it. The dimensions of A4 paper. These numbers are etched in my memory like the multiplication table of \( 1. \) We can keep going further to get A5, A6, etc. We could, in theory, go all the way up to A\( \infty. \) Hold on, I think I hear someone heckle. What's that? Oh, we can't go all the way to A\( \infty? \) Something about atoms, was it? Hmm. Security! Where's security? Ah yes, thank you, sir. Please show this gentleman out, would you?&lt;/p&gt;
    &lt;p&gt;Sorry for the interruption, ladies and gentlemen. Phew! That fellow! Atoms? Honestly. We, the mathematically inclined, are not particularly concerned with such trivial limitations. We drink our tea from doughnuts. We are not going to let the size of atoms dictate matters, now are we?&lt;/p&gt;
    &lt;p&gt;So I was saying that we can bisect our paper like this and go all the way to A\( \infty. \) That reminds me. Last night I was at a bar in Hoxton and I saw an infinite number of mathematicians walk in. The first one asked, "Sorry to bother you, but would it be possible to have a sheet of A0 paper? I just need something to scribble a few equations on." The second one asked, "If you happen to have one spare, could I please have an A1 sheet?" The third one said, "An A2 would be perfectly fine for me, thank you." Before the fourth one could ask, the bartender disappeared into the back for a moment and emerged with two sheets of A0 paper and said, "Right. That should do it. Do know your limits and split these between yourselves."&lt;/p&gt;
    &lt;p&gt;In general, a sheet of A\( n \) paper has the dimensions \[ 2^{-(2n + 1)/4} \, \mathrm{m} \times 2^{-(2n - 1)/4} \, \mathrm{m}. \] If we plug in \( n = 4, \) we indeed get the dimensions of A4 paper: \[ 0.210 \, \mathrm{m} \times 0.297 \, \mathrm{m}. \]&lt;/p&gt;
    &lt;head rend="h2"&gt;Measuring Stuff&lt;/head&gt;
    &lt;p&gt;Let us now return to the business of measuring things. As I mentioned earlier, the dimensions of A4 are lodged firmly into my memory. Getting hold of a sheet of A4 paper is rarely a challenge where I live. I have accumulated a number of A4 paper stories over the years. Let me share a recent one. I was hanging out with a few folks of the nerd variety one afternoon when the conversation drifted, as it sometimes does, to a nearby computer monitor that happened to be turned off. At some point, someone confidently declared that the screen in front of us was 27 inches. That sounded plausible but we wanted to confirm it. So I reached for my trusted measuring instrument: an A4 sheet of paper. What followed was neither fast, nor especially precise, but it was more than adequate for settling the matter at hand.&lt;/p&gt;
    &lt;p&gt;I lined up the longer edge of the A4 sheet with the width of the monitor. One length. Then I repositioned it and measured a second length. The screen was still sticking out slightly at the end. By eye, drawing on an entirely unjustified confidence built from years of measuring things that never needed measuring, I estimated the remaining bit at about \( 1 \, \mathrm{cm}. \) That gives us a width of \[ 29.7 \, \mathrm{cm} + 29.7 \, \mathrm{cm} + 1.0 \, \mathrm{cm} = 60.4 \, \mathrm{cm}. \] Let us round that down to \( 60 \, \mathrm{cm}. \) For the height, I switched to the shorter edge. One full \( 21 \, \mathrm{cm} \) fit easily. For the remainder, I folded the paper parallel to the shorter side, producing an A5-sized rectangle with dimensions \( 14.8 \, \mathrm{cm} \times 21.0 \, \mathrm{cm}. \) Using the \( 14.8 \, \mathrm{cm} \) edge, I discovered that it overshot the top of the screen slightly. Again, by eye, I estimated the excess at around \( 2 \, \mathrm{cm}. \) That gives us \[ 21.0 \, \mathrm{cm} + 14.8 \, \mathrm{cm} -2.0 \, \mathrm{cm} = 33.8 \, \mathrm{cm}. \] Let us round this up to \( 34 \, \mathrm{cm}. \) The ratio \( 60 / 34 \approx 1.76 \) is quite close to \( 16/9, \) a popular aspect ratio of modern displays. At this point the measurements were looking good. So far, the paper had not embarrassed itself. Invoking the wisdom of the Pythagoreans, we can now estimate the diagonal as \[ \sqrt{(60 \, \mathrm{cm})^2 + (34 \, \mathrm{cm})^2} \approx 68.9 \,\mathrm{cm}. \] Finally, there is the small matter of units. One inch is \( 2.54 \, \mathrm{cm}, \) another figure that has embedded itself in my head. Dividing \( 68.9 \) by \( 2.54 \) gives us roughly \( 27.2 \, \mathrm{in}. \) So yes. It was indeed a \( 27 \)-inch display. My elaborate exercise in showing off my A4 paper skills was now complete. Nobody said anything. A few people looked away in silence. I assumed they were reflecting. I am sure they were impressed deep down. Or perhaps... no, no. They were definitely impressed. I am sure.&lt;/p&gt;
    &lt;p&gt;Hold on. I think I hear another heckle. What is that? There are mobile phone apps that can measure things now? Really? Right. Security. Where's security?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46525888</guid><pubDate>Wed, 07 Jan 2026 12:54:43 +0000</pubDate></item><item><title>Show HN: KeelTest ‚Äì AI-driven VS Code unit test generator with bug discovery</title><link>https://keelcode.dev/keeltest</link><description>&lt;doc fingerprint="14373b5026f706a0"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;AI Tests That Find &lt;lb/&gt;Bugs Before Production&lt;/head&gt;&lt;p&gt;Generate pytest suites that actually run - and expose issues in your code. 90% average pass rate. Source bugs flagged with fix suggestions.&lt;/p&gt;&lt;p&gt;Free forever ¬∑ 7 credits/month ¬∑ No credit card required&lt;/p&gt;Join waitlist for paid plans&lt;p&gt;* Stats updated weekly. Based on active Alpha usage.&lt;/p&gt;&lt;head rend="h2"&gt;Get Started in 3 Simple Steps&lt;/head&gt;&lt;p&gt;KeelTest is a VS Code extension that installs in seconds. No complex setup, no external services.&lt;/p&gt;&lt;head rend="h3"&gt;Open VS Code Extensions&lt;/head&gt;&lt;p&gt;Press Ctrl+Shift+X (Windows/Linux) or Cmd+Shift+X (Mac) to open the Extensions view in VS Code.&lt;/p&gt;&lt;head rend="h3"&gt;Search for KeelTest&lt;/head&gt;&lt;p&gt;Type "KeelTest" in the search bar and click Install on the official KeelTest extension.&lt;/p&gt;&lt;head rend="h3"&gt;Right-Click and Generate&lt;/head&gt;&lt;p&gt;Right-click any Python file in your workspace and select "KeelTest: Generate Tests" to start.&lt;/p&gt;&lt;p&gt;Free to install ‚Ä¢ Available on VS Code Marketplace ‚Ä¢ No credit card required&lt;/p&gt;&lt;head rend="h2"&gt;Why developers switch &lt;lb/&gt;to KeelTest&lt;/head&gt;&lt;p&gt;Moving beyond simple prompts. We combined static analysis with a multi-step verification pipeline to deliver production-grade tests.&lt;/p&gt;&lt;head rend="h3"&gt;Deep Static Analysis&lt;/head&gt;&lt;p&gt;Our engine builds a full AST (Abstract Syntax Tree) representation of your code, identifying exactly which branches need coverage and which edge cases are most likely to cause regressions.&lt;/p&gt;&lt;head rend="h2"&gt;From Code to Tests in 3 Clicks&lt;/head&gt;&lt;head rend="h2"&gt;Start Free, Scale When Ready&lt;/head&gt;&lt;p&gt;Join ... developers already on the waitlist for our upcoming premium tiers.&lt;/p&gt;&lt;head rend="h3"&gt;Individual Plans&lt;/head&gt;&lt;head rend="h4"&gt;Starter&lt;/head&gt;&lt;p&gt;For regular development&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Priority queue&lt;/item&gt;&lt;item&gt;Usage analytics&lt;/item&gt;&lt;item&gt;Bug detection&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Pro&lt;/head&gt;&lt;p&gt;For power users&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Priority queue&lt;/item&gt;&lt;item&gt;Usage analytics&lt;/item&gt;&lt;item&gt;Bug detection&lt;/item&gt;&lt;item&gt;Early access to features&lt;/item&gt;&lt;/list&gt;&lt;head rend="h4"&gt;Free&lt;/head&gt;&lt;p&gt;Perfect for trying it out&lt;/p&gt;&lt;head rend="h4"&gt;Detailed Comparison&lt;/head&gt;&lt;p&gt;Everything you get with each tier&lt;/p&gt;&lt;head rend="h2"&gt;Real Pass Rates, Not marketing&lt;/head&gt;&lt;p&gt;Every test is executed in a sandbox before it reaches your editor. We don't just generate code; we deliver verified functionality.&lt;/p&gt;&lt;p&gt;Pass Rate&lt;/p&gt;&lt;p&gt;Self-Healing GenerationFailures are automatically fixed by our AI validator before delivery.&lt;/p&gt;&lt;p&gt;Source Bug DetectionReal issues in your source code are triaged and clearly flagged.&lt;/p&gt;&lt;head rend="h3"&gt;How far your credits go&lt;/head&gt;&lt;p&gt;Estimated file generation per month based on complexity&lt;/p&gt;&lt;table&gt;&lt;row span="4"&gt;&lt;cell role="head"&gt;Complexity&lt;/cell&gt;&lt;cell role="head"&gt;Free&lt;/cell&gt;&lt;cell role="head"&gt;Starter&lt;/cell&gt;&lt;cell role="head"&gt;Pro&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Small files (‚â§15 fn)Approx. 15 functions&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~7&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~30&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~70&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row span="4"&gt;&lt;cell&gt;Medium files (~30 fn)Approx. 30 functions&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~3&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~15&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~35&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;Large files (~50 fn)Approx. 50 functions&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~1&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~7&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;cell&gt;&lt;p&gt;~17&lt;/p&gt;&lt;p&gt;files&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;1 credit = up to 15 functions. Larger files use proportionally more credits.&lt;/p&gt;&lt;head rend="h2"&gt;Tests That Actually Test&lt;/head&gt;&lt;p&gt;We tested leading AI models on generating unit tests for complex e-commerce logic. KeelTest's agentic approach-combining AI with static code analysis, test validation, and actual execution-achieved a staff-level score of 8.5/10, outperforming pure zero-shot prompts by 54%.&lt;/p&gt;&lt;head rend="h3"&gt;Overall Quality ScoreStaff Engineer = 10&lt;/head&gt;&lt;head rend="h3"&gt;Detailed Evaluation Criteria&lt;/head&gt;&lt;table&gt;&lt;row span="5"&gt;&lt;cell role="head"&gt;Criteria&lt;/cell&gt;&lt;cell role="head"&gt;KeelTest&lt;/cell&gt;&lt;cell role="head"&gt;Model B&lt;/cell&gt;&lt;cell role="head"&gt;Model C&lt;/cell&gt;&lt;cell role="head"&gt;Model A&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Unit Test Isolation&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Mocking &amp;amp; Dependency Injection&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Edge Case Coverage&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Following Instructions&lt;/cell&gt;&lt;/row&gt;&lt;row span="5"&gt;&lt;cell&gt;Technical Correctness&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;DateTime/Float Precision&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;"KeelTest demonstrates the deepest understanding of unit testing principles with proper isolation, comprehensive mocking, and dependency injection. It's what a staff engineer would produce."&lt;/p&gt;&lt;p&gt;* KeelTest leverages advanced AI models enhanced with static code analysis, automated test validation, and real-time execution feedback-not just raw prompts.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46526088</guid><pubDate>Wed, 07 Jan 2026 13:22:35 +0000</pubDate></item><item><title>Sugar industry influenced researchers and blamed fat for CVD</title><link>https://www.ucsf.edu/news/2016/09/404081/sugar-papers-reveal-industry-role-shifting-national-heart-disease-focus</link><description>&lt;doc fingerprint="e398a9b742a7c320"&gt;
  &lt;main&gt;&lt;p&gt;This article is archived and only made available for historical reference. If you‚Äôd like to discover UCSF‚Äôs most recent advances in research, education and patient care, please visit the UCSF News Center.&lt;/p&gt;&lt;head rend="h1"&gt;Archive: Sugar Papers Reveal Industry Role in Shifting National Heart Disease Focus to Saturated Fat&lt;/head&gt;&lt;p&gt;A newly discovered cache of industry documents revealed that the sugar industry began working closely with nutrition scientists in the mid-1960s to single out fat and cholesterol as the dietary causes of coronary heart disease and to downplay evidence that sucrose consumption was also a risk factor.&lt;/p&gt;&lt;p&gt;An analysis of those papers by researchers at UC San Francisco appears Sept. 12, 2016, in JAMA Internal Medicine.&lt;/p&gt;&lt;p&gt;The internal industry documents, which were found in public archives, showed that a sugar industry trade organization recognized as early as 1954 that if Americans adopted low-fat diets, then per-capita consumption of sucrose would increase by more than one-third. The trade organization represented 30 international members.&lt;/p&gt;&lt;p&gt;Meanwhile, evidence linking sugar consumption to high blood cholesterol and triglyceride levels ‚Äì both thought to be risk factors for coronary heart disease ‚Äì began to emerge in the scientific literature and popular press.&lt;/p&gt;&lt;head rend="h2"&gt;Literature Shaped Public Opinion&lt;/head&gt;&lt;p&gt;After a 1965 spike in media attention to the heart disease risks of sucrose, the sugar industry commissioned Project 226, a literature review written by researchers at the Harvard University School of Public Health Nutrition Department, which was published in the highly respected New England Journal of Medicine (NEJM) in 1967. It concluded there was ‚Äúno doubt‚Äù that the only dietary intervention required to prevent coronary heart disease was to reduce dietary cholesterol and substitute polyunsaturated fat for saturated fat in the American diet.&lt;/p&gt;Cristin Kearns, DDS, MBA&lt;p&gt;‚ÄúThe literature review helped shape not only public opinion on what causes heart problems but also the scientific community‚Äôs view of how to evaluate dietary risk factors for heart disease,‚Äù said lead author Cristin Kearns, DDS, MBA, who discovered the industry documents.&lt;/p&gt;&lt;p&gt;The UCSF researchers analyzed more than 340 documents, totaling 1,582 pages of text, between the sugar industry and two individuals: Roger Adams, then a professor of organic chemistry who served on scientific advisory boards for the sugar industry; and D. Mark Hegsted, one of the Harvard researchers who produced the literature review.&lt;/p&gt;&lt;p&gt;To conduct the literature review, the sugar industry paid the Harvard scientists the equivalent of $50,000 in 2016 dollars, then set the review‚Äôs objective, contributed articles to be included, and received drafts. Yet the industry‚Äôs funding and role were not disclosed in the final NEJM publication.&lt;/p&gt;&lt;p&gt;The literature review heavily criticized studies linking sucrose to heart disease, while ignoring limitations of studies investigating dietary fats. The review argued that blood cholesterol levels were the only significant risk factor for coronary heart disease, which made the high sucrose content of the American diet seem less hazardous than if blood triglycerides were also considered to be a risk factor.&lt;/p&gt;&lt;head rend="h2"&gt;Need for More Transparent Scientific Reviews&lt;/head&gt;Stanton A. Glantz, PhD&lt;p&gt;The authors emphasized that this analysis demonstrates the importance of having scientific reviews written by people without conflicts of interest, as well as the need for financial disclosure in nutrition science.&lt;/p&gt;&lt;p&gt;‚ÄúAs the saying goes, he who pays the piper calls the tune,‚Äù said senior author Stanton A. Glantz, PhD, UCSF professor of medicine and director of the UCSF Center for Tobacco Control Research and Education. ‚ÄúThere are all kinds of ways that you can subtly manipulate the outcome of a study, which industry is very well practiced at.‚Äù&lt;/p&gt;&lt;p&gt;Co-author Laura Schmidt, PhD, who is also principal investigator on the UCSF-led SugarScience initiative, noted that after decades of focusing on saturated fat as the dietary culprit in heart disease, the science is building around sugar‚Äôs role, but health policy has only just begun to catch up.&lt;/p&gt;Laura Schmidt, PhD&lt;p&gt;‚ÄúThere is now a considerable body of evidence linking added sugars to hypertension and cardiovascular disease, which is the No. 1 cause of premature death in the developed world,‚Äù Schmidt said. ‚ÄúYet, health policy documents are still inconsistent in citing heart disease risk as a health consequence of added sugars consumption.‚Äù&lt;/p&gt;&lt;p&gt;The study was funded by the UCSF Philip R. Lee Institute for Health Policy Studies; a donation by the Hellmann Family Fund to the UCSF Center for Tobacco Control Research and Education; the UCSF School of Dentistry Department of Orofacial Sciences and Global Oral Health Program; and grants from the National Institute of Dental and Craniofacial Research and the National Cancer Institute.&lt;/p&gt;&lt;p&gt;UCSF is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It includes top-ranked graduate schools of dentistry, medicine, nursing and pharmacy; a graduate division with nationally renowned programs in basic, biomedical, translational and population sciences; and a preeminent biomedical research enterprise. It also includes UCSF Health, which comprises two top-ranked hospitals, UCSF Medical Center and UCSF Benioff Children‚Äôs Hospital San Francisco, and other partner and affiliated hospitals and healthcare providers throughout the Bay Area.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46526740</guid><pubDate>Wed, 07 Jan 2026 14:29:25 +0000</pubDate></item><item><title>LaTeX Coffee Stains [pdf] (2021)</title><link>https://ctan.math.illinois.edu/graphics/pgf/contrib/coffeestains/coffeestains-en.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46526933</guid><pubDate>Wed, 07 Jan 2026 14:46:31 +0000</pubDate></item><item><title>Meditation as Wakeful Relaxation: Unclenching Smooth Muscle</title><link>https://psychotechnology.substack.com/p/meditation-as-wakeful-relaxation</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527157</guid><pubDate>Wed, 07 Jan 2026 15:03:34 +0000</pubDate></item><item><title>Shipmap.org</title><link>https://www.shipmap.org/</link><description>&lt;doc fingerprint="14b4e15227c5a82f"&gt;
  &lt;main&gt;
    &lt;p&gt;Data: exactEarth &amp;amp; Clarksons&lt;/p&gt;
    &lt;p&gt;Due to popular demand the designers of this map, Kiln, are now selling stunning high-resolution versions of the world √¢routes√¢ view. There are two versions available: coloured by ship type over the inky-blue base map; or just the ship in a single colour a transparent background so you can overlay or print onto whatever background colour you like. Contact [email protected] for pricing and further information.&lt;/p&gt;
    &lt;p&gt;Yes. You are welcome to embed this map. Please include a link back to Kiln somewhere in the text of your article. Use the following embed code for a fully responsive embed that will adjust to the width of your website. Feel free to change the height and/or give it a fixed width if you prefer.&lt;/p&gt;
    &lt;p&gt;You can see movements of the global merchant fleet over the course of 2012, overlaid on a bathymetric map. You can also see a few statistics such as a counter for emitted CO2 (in thousand tonnes) and maximum freight carried by represented vessels (varying units).&lt;/p&gt;
    &lt;p&gt;You can pan and zoom in the usual ways, and skip back and forward in time using the timeline at the bottom of the screen. The controls at the top right let you show and hide different map layers: port names, the background map, routes (a plot of all recorded vessel positions), and the animated ships view. There are also controls for filtering and colouring by vessel type.&lt;/p&gt;
    &lt;p&gt;The merchant fleet is divided into five categories, each of which has a filter and a CO2 and freight counter for the hour shown on the clock. The ship types and units are as follows:&lt;/p&gt;
    &lt;p&gt;In some cases this is because there are ships navigating via canals or rivers that aren√¢t visible on the map. Generally, though, this effect is an artefact of animating a ship between two recorded positions with missing data between, especially when the positions are separated by a narrow strip of land. We may develop the map to remove this effect in the future.&lt;/p&gt;
    &lt;p&gt;Unfortunately the data we are using for the map is incomplete for the first few months of the year: roughly January to April.&lt;/p&gt;
    &lt;p&gt;The map was created by Kiln based on data from the UCL Energy Institute (UCL EI)&lt;/p&gt;
    &lt;p&gt;Website: Duncan Clark &amp;amp; Robin Houston from Kiln&lt;/p&gt;
    &lt;p&gt;Data: Julia Schaumeier &amp;amp; Tristan Smith from the UCL EI&lt;/p&gt;
    &lt;p&gt;Music: Bach Goldberg Variations played by Kimiko Ishizaka&lt;/p&gt;
    &lt;p&gt;UCL EI took data showing location and speed of ships and cross-checked it with another database to get the vessel characteristics, such as engine type and hull measurements. With this information they were able to compute the CO2 emissions for each observed hour, following the approach laid out in the Third IMO Greenhouse Gas Study 2014. Kiln took the resulting dataset and visualized it with WebGL on top of a specially created base map, which shows bathymetry (ocean depth), based on the GEBCO_2014 Grid (version 20150318), as well as continents and major rivers from Natural Earth.&lt;/p&gt;
    &lt;p&gt;Our data sources for shipping positions are exactEarth for AIS data (location/speed) and Clarksons Research UK World Fleet Register (static vessel information). We are very grateful to our funders, the European Climate Foundation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527161</guid><pubDate>Wed, 07 Jan 2026 15:03:41 +0000</pubDate></item><item><title>US Job Openings Decline to Lowest Level in More Than a Year</title><link>https://www.bloomberg.com/news/articles/2026-01-07/us-job-openings-decline-to-lowest-level-in-more-than-a-year</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527533</guid><pubDate>Wed, 07 Jan 2026 15:32:44 +0000</pubDate></item><item><title>LLM Problems Observed in Humans</title><link>https://embd.cc/llm-problems-observed-in-humans</link><description>&lt;doc fingerprint="b44f0a9d2b235e5a"&gt;
  &lt;main&gt;
    &lt;p&gt;While some are still discussing why computers will never be able to pass the Turing test, I find myself repeatedly facing the idea that as the models improve and humans don√¢t, the bar for the test gets raised and eventually humans won√¢t pass the test themselves. Here√¢s a list of what used to be LLM failure modes but that are now more commonly observed when talking to people.&lt;/p&gt;
    &lt;p&gt;This has always been an issue in conversations: you ask a seemingly small and limited question, and in return have to listen to what seems like hours of incoherent rambling. Despite exhausting their knowledge of the topic, people will keep on talking about stuff you have no interest in. I find myself searching for the √¢stop generating√¢ button, only to remember that all I can do is drop hints, or rudely walk away.&lt;/p&gt;
    &lt;p&gt;The best thing about a good deep conversation is when the other person gets you: you explain a complicated situation you find yourself in, and find some resonance in their replies. That, at least, is what happens when chatting with the recent large models. But when subjecting the limited human mind to the same prompt√¢a rather long one√¢again and again the information in the prompt somehow gets lost, their focus drifts away, and you have to repeat crucial facts. In such a case, my gut reaction is to see if there√¢s a way to pay to upgrade to a bigger model, only to remember that there√¢s no upgrading of the human brain. At most what you can do is give them a good night√¢s sleep and then they may possibly switch from the √¢Fast√¢ to the √¢Thinking√¢ mode, but that√¢s not guaranteed with all people.&lt;/p&gt;
    &lt;p&gt;I√¢ve got a lot of interests and on any given day, I may be excited to discuss various topics, from kernels to music to cultures and religions. I know I can put together a prompt to give any of today√¢s leading models and am essentially guaranteed a fresh perspective on the topic of interest. But let me pose the same prompt to people and more often then not the reply will be a polite nod accompanied by clear signs of their thinking something else entirely, or maybe just a summary of the prompt itself, or vague general statements about how things should be. In fact, so rare it is to find someone who knows what I mean that it feels like a magic moment. With the proliferation of genuinely good models√¢well educated, as it were√¢finding a conversational partner with a good foundation of shared knowledge has become trivial with AI. This does not bode well for my interest in meeting new people.&lt;/p&gt;
    &lt;p&gt;Models with a small context window, or a small number of parameters, seem to have a hard time learning from their mistakes. This should not be a problem for humans: we have a long term memory span measured in decades, with emotional reinforcement of the most crucial memories. And yet, it happens all too often that I must point out the same logical fallacy again and again in the same conversation! Surely, I think, if I point out the mistake in the reasoning, this will count as an important correction that the brain should immediately make use of? As it turns out, there seems to be some kind of a fundamental limitation on how quickly the neural connections can get rewired. Chatting with recent models, who can make use the extra information immediately, has deteriorated my patience regarding having to repeat myself.&lt;/p&gt;
    &lt;p&gt;By this point, it√¢s possible to explain what happens in a given situation, and watch the model apply the lessons learned to a similar situation. Not so with humans. When I point out that the same principles would apply elsewhere, their response will be somewhere along the spectrum of total bafflement on the one end and on the other, a face-saving explanation that the comparison doesn√¢t apply √¢because it√¢s different√¢. Indeed the whole point of comparisons is to apply same principles in different situations, so why the excuse? I√¢ve learned to take up such discussions with AI and not trouble people with them.&lt;/p&gt;
    &lt;p&gt;This is the opposite issue: given a principle stated in general terms, the person will not be able to apply it in a specific situation. Indeed, I√¢ve had a lifetime of observing this very failure mode in myself: given the laws of physics, which are typically √¢obvious√¢ and easy to understand, I find it very difficult to calculate how long before the next eclipse. More and more, rather than think these things through myself, I√¢d just send a quick prompt to the most recent big model, and receive a good answer in seconds. In other words, models threaten to sever me not only from other flawed humans, but from my own √¢slow√¢ thinking as well!&lt;/p&gt;
    &lt;p&gt;Understood in the medical sense, hallucination refers to when something appears to be real even as you know very well it isn√¢t. Having no direct insight into the √¢inner mental life√¢ of models, we claim that every false fact they spit out is a form of hallucination. The meaning of the word is shifting from the medical sense towards the direction of √¢just being wrong, and persistently so√¢. This has plagued human speech for centuries. As a convenient example, look up some heated debate between proponents of science and those of religion. (As if the two need be in conflict!) When a model exhibits hallucination, often providing more context and evidence will dispel it, but the same trick does not appear to work so well on humans.&lt;/p&gt;
    &lt;p&gt;Where to go from here? One conclusion is that LLMs are damaging the connection people feel with each other, much like a decade before social networks threatened to destroy it by replacing it with a shallower, simulated versions. Another interpretation would be to conclude cynically that it√¢s time humans get either enhanced or replaced by a more powerful form of intelligence. I√¢d say we√¢re not there yet entirely, but that some of the replacement has been effected already: I√¢ll never again ask a human to write a computer program shorter than about a thousand lines, since an LLM will do it better.&lt;/p&gt;
    &lt;p&gt;Indeed, why am I even writing this? I asked GPT-5 for additional failure modes and found more additional examples than I could hope to get from a human:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Beyond the failure modes already discussed, humans also exhibit analogues of several newer LLM pathologies: conversations often suffer from instruction drift, where the original goal quietly decays as social momentum takes over; mode collapse, in which people fall back on a small set of safe clich√É¬©s and conversational templates; and reward hacking, where social approval or harmony is optimized at the expense of truth or usefulness. Humans frequently overfit the prompt, responding to the literal wording rather than the underlying intent, and display safety overrefusal, declining to engage with reasonable questions to avoid social or reputational risk. Reasoning is also marked by inconsistency across turns, with contradictions going unnoticed, and by temperature instability, where fatigue, emotion, or audience dramatically alters the quality and style of thought from one moment to the next.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527581</guid><pubDate>Wed, 07 Jan 2026 15:36:25 +0000</pubDate></item><item><title>Commodore 64 floppy drive has the power to be a computer and runs BASIC</title><link>https://www.tomshardware.com/pc-components/cpus/commodore-64-floppy-drive-has-the-power-to-be-a-computer-bulky-1982-commodore-1541-5-25-inch-drive-packs-a-1-mhz-mos-6502-cpu</link><description>&lt;doc fingerprint="f7845cde6324b9ed"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Commodore 64 floppy drive has the power to be a computer ‚Äî bulky 1982 Commodore 1541 5.25 inch drive packs a 1 MHz MOS 6502 CPU&lt;/head&gt;
    &lt;p&gt;10 PRINT "IT WORKS"&lt;/p&gt;
    &lt;p&gt;The Commodore History channel on YouTube has confirmed that the Commodore 1541 floppy disk drives electronics are powerful and capable enough to work as a standalone computer. This 1982 vintage peripheral, designed to add a 5.25-inch floppy disc to the equally ancient Commodore 64, actually has its own processor, RAM, ROM and I/O.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a 1 MHz MOS 6502 in the floppy drive electronics, which is closely related to the C64‚Äôs MOS 6510, and exactly the same processor as in the VIC-20. However, Dave from the Commodore History channel did his work with minimal hardware modding, so the resulting ‚Äò1541 computer‚Äô ended up being rather limited.&lt;/p&gt;
    &lt;p&gt;The video starts with Dave explaining that a channel subscriber had asked about whether the Commodore 1541 floppy disk could work as a general purpose computer ‚Äì as it was known to pack a MOS 6502 chip, its own RAM, its own I/O chips, alongside the ROMs which help it carry out its day job as a storage device. The CPU is very similar to the C64‚Äôs MOS 6510, which is just ‚Äúa customized upgrade for the Commodore 64‚Äù based on the 6502. But the VIC-20 is actually a much closer match, and you can see a comparison in the infographic, below.&lt;/p&gt;
    &lt;p&gt;Turning the 1541 into a VIC-20-a-like was still too much of a stretch for this investigation, as Dave wanted to keep hardware modding off the menu. The VIC-20 owes a lot of its general purpose computing ability to its additional 6560 VIC chip ‚Äì a custom IC for graphics and sound. It also offers lot more I/O for general purpose computing appeal.&lt;/p&gt;
    &lt;p&gt;Thus, Dave had to wind-back the Commodore clock even further for inspiration. And he decided the first way to demonstrate that the Commodore 1541 floppy disk could work as a general purpose computer was to look at the Commodore KIM-1, the firm‚Äôs first, and most simple computer, which would be described as a Single Board Computer (SBC) today.&lt;/p&gt;
    &lt;p&gt;The KIM-1 was programmed using an onboard keypad, punching in values in 6502 machine language, byte-by-byte. Its only display was a set of 6 segmented LCDs. This computer could also be used via Teletype (TTY) over serial connection, and this method was adopted as the way to interface and work with the Commodore 1541.&lt;/p&gt;
    &lt;p&gt;So, the KIM-1 became the new target of the Commodore 1541 as a computer project. Dave found the KIM-1 kernel had already been published, so set about modifying it with code to initialize the 1541, and tweak I/O routines so serial teletype would work. This code was burned onto an EEPROM, and is now available on GitHub.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Commodore 64's original price was $599 in 1982 (~$1,950 in today‚Äôs money)&lt;/item&gt;
      &lt;item&gt;The Commodore 1541 disk drive was originally priced at $399 in 1982 (~$1,300 in today‚Äôs money)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To teletype interface with the pair of 1541 serial connectors, an adaptor / dongle was required. Dave brewed up a USB to RS232 to TTL dongle. The finished MacBook USB to 1541 serial adaptor looks a bit messy, but did the job.&lt;/p&gt;
    &lt;p&gt;Next up, Dave communicated with the 1541, with its freshly baked KIM-1 ROMs, and dongle, via a Minicom terminal on his Mac. His hand typed assembly Hello World code worked first time (as far as we saw in the video).&lt;/p&gt;
    &lt;p&gt;Before signing off, Dave wanted to get a bit nearer to making the 1541 into a VIC-20 by adding a BASIC interpreter. He ported Tiny Basic to the KIM-1 and burned it to a ROM to insert on the 1541's PCB. Again, this worked, making it much quicker to code a Hello World program.&lt;/p&gt;
    &lt;p&gt;The TechTuber made it clear that this 1541 ‚Äògeneral purpose computer‚Äô remained very limited without major hardware mods due to its lack of I/O ‚Äì limiting it to serial terminal use. But we don‚Äôt blame him for not wanting to mess with this precious retro hardware too much.&lt;/p&gt;
    &lt;p&gt;This project makes us wonder about the general purpose computing abilities of modern drive controller electronics.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;snemarch&lt;/header&gt;Iirc there was software (can't remember if any commercial games did it, or only demos) back in the day that would offload computation to the 1541 drive, but can't remember any specific titles right now.Reply&lt;lb/&gt;But I did stumble upon Freespin from 2021, which is kinda crazy ‚Äì it bit-bangs video and sound output ü§Ø&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Jason Ka&lt;/header&gt;I recall that that there was a cartridge for the Commodore 64 that At Least doubled the load speed from the disk drive called ‚ÄúMach 5‚Äù. I am wondering how it worked. Would that have had an effect on using the hard drive as it own computer?Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Fruitmaniac&lt;/header&gt;Yup. The C64 didn't have room for a disk controller so they put everything in the drive.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;blppt&lt;/header&gt;Reply&lt;quote/&gt;The 1541 was a buggy mess to the point where it defaulted to something ridiculous like 300 BYTES/sec transfer rates, though the mechanism was capable of much higher.Jason Ka said:I recall that that there was a cartridge for the Commodore 64 that At Least doubled the load speed from the disk drive called ‚ÄúMach 5‚Äù. I am wondering how it worked. Would that have had an effect on using the hard drive as it own computer?&lt;lb/&gt;Epyx's Fast Load Cartridge was the popular fix for it---basically all it did was enable the 1541 to transfer at the rate it was originally supposed to.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;BFG-9000&lt;/header&gt;The $400 1541 was initially so unreliable that a host of 3rd party clone external floppy drives from many manufacturers popped up, usually selling for around half that price, and I had one (still have it, and it did not turn out to be as completely compatible as they claimed). All of those also used the $25 6502 because the closest equivalent from Intel in their 8080 was $370 for the chip alone. The low price was why it or a variant was used in many 8-bit computers through the 1970s and 80s from Acorn, Apple, Atari, and CommodoreReply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Tanakoi&lt;/header&gt;Reply&lt;quote/&gt;The original C64 didn't come with a hard drive, and even the floppy drive was an aftermarket purchase.Jason Ka said:I recall that that there was a cartridge for the Commodore 64 that At Least doubled the load speed from the disk drive called ‚ÄúMach 5‚Äù. I am wondering how it worked. Would that have had an effect on using the hard drive as it own computer?&lt;lb/&gt;I can't recall its name, but there was one program that made the rounds back then which purported to raise the floppy drive's speed, but would literally make the drive catch fire if run.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527600</guid><pubDate>Wed, 07 Jan 2026 15:37:41 +0000</pubDate></item><item><title>Target has their own forensic lab to investigate shoplifters</title><link>https://thehorizonsun.com/features/2024/04/11/the-target-forensics-lab/</link><description>&lt;doc fingerprint="7e0e1880e3b5a198"&gt;
  &lt;main&gt;
    &lt;p&gt;Target, just like many other retailers, has fallen victim to shoplifters, with almost a billion dollars in goods stolen from their stores in 2023. However, the numbers could have been much worse if it weren‚Äôt for their unique anti-shoplifting tactics. Target‚Äôs way of combating shoplifting was to establish a forensics lab in Minneapolis, Minnesota, that is more advanced and high-tech than many police departments‚Äô forensics labs.&lt;/p&gt;
    &lt;p&gt;The lab was developed in 2003 to give the company expertise when it came to analyzing surveillance footage from in and around the store. Forbes states that Target has had cameras in all their stores since the 1980s, but it hadn‚Äôt been enough to stop serial shoplifting from occurring. The lab hires specialists in analyzing video evidence from cameras and smartphone recordings to help identify shoplifters, frauds, and injuries inside Target stores. However, due to the skill and technology the lab possesses, it has been of help in many cases outside of Target stores, solving some of the most gruesome crimes including murders, arsons, abductions, rapes, and mass robberies.&lt;/p&gt;
    &lt;p&gt;In many cases, the Target lab has been able to solve cases that even the Federal Bureau of Investigation (FBI) can‚Äôt solve. Forbes goes on to say that in one specific case, the experts at the Target lab were contacted by the Huston police department to help solve an arson case. A convenience store camera had caught two boys buying gasoline a short time before the fire, but the tape was damaged, making it impossible to make out the boys‚Äô faces. After the FBI was unable to solve the case, the tapes were passed over to Target, where they were repaired, and the faces of the boys were able to be seen.&lt;/p&gt;
    &lt;p&gt;Aside from stopping shoplifting and helping law enforcement, the Target lab also teaches and supports government agencies. According to the Washington Post, experts at the lab have taken a leading role in teaching government protection agencies about how to use technology to help solve crimes. In the past, Target has also helped organize undercover investigations, as well as helping United States customs verify overseas imports are coming from reputable sources.&lt;/p&gt;
    &lt;p&gt;While a retailer may seem like an odd group of people to help solve crimes, Target has proven to be a helpful resource to police forces and government agencies alike.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527645</guid><pubDate>Wed, 07 Jan 2026 15:41:01 +0000</pubDate></item><item><title>Dell's CES 2026 chat was the most pleasingly un-AI briefing I've had in 5 years</title><link>https://www.pcgamer.com/hardware/dells-ces-2026-chat-was-the-most-pleasingly-un-ai-briefing-ive-had-in-maybe-5-years/</link><description>&lt;doc fingerprint="6e03155f29ac24c7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Dell's CES 2026 chat was the most pleasingly un-AI briefing I've had in maybe 5 years&lt;/head&gt;
    &lt;p&gt;"A bit of a shift from a year ago where we were all about the AI PC."&lt;/p&gt;
    &lt;p&gt;The unshakable presence of AI has been an unwelcome companion of my job for the past few years, but it sure feels like longer. It's not even like it's some excitingly malevolent artificial mind with tendrils of influence weaving its way throughout my world. That would at least be satisfying from a sci-fi perspective. No, what I've had to deal with can barely write, definitely cannot count, and has only just figured out what fingers are.&lt;/p&gt;
    &lt;p&gt;Yet it's been something that has pervading every product announcement, presentation, or pre-briefing I've been a part of in recent times from any company even tangentially related to tech. To the point where I now have a bullshit AI bingo card I fill out just to distract myself from the barely resistible desire to stab a pen through my own hand just to feel something real.&lt;/p&gt;
    &lt;p&gt;Every new piece of technology, whether that's a laptop, graphics card, mouse, keyboard, BBQ, whatever, is now presented as being powered by AI or comes with an AI assistant, or just has an 'AI' sticker on the box.&lt;/p&gt;
    &lt;p&gt;Catch up with CES 2026: We're on the ground in sunny Las Vegas covering all the latest announcements from some of the biggest names in tech, including Nvidia, AMD, Intel, Asus, Razer, MSI and more.&lt;/p&gt;
    &lt;p&gt;So thank you, Dell, for making your CES 2026 pre-briefing so blessedly free of effusive AI chat that I just had to mention it.&lt;/p&gt;
    &lt;p&gt;It started off with Dell vice chairman and COO, Jeff Clarke, taking to a small stage to talk about the state of the industry and where Dell and its Alienware sub-brand is going this year. He talks tariffs, the slow transitioning of the industry (he says CPU, but I'm presuming he meant OS and Windows 10 ‚Üí 11), and then "we have this un-met promise of AI, and the expectation of AI driving end user demand," as well as the fact that "we're about ready to enter 2026 with a memory shortage that is pretty significant."&lt;/p&gt;
    &lt;p&gt;Clarke and his co-presenters then go on to introduce the return of the XPS laptop lineup, some new high-end ultraslim Alienware laptops, as well as some entry-level Alienware laptops (cheap Alienwares? Really?), new spins of its Area-51 desktops, and a handful of new monitors.&lt;/p&gt;
    &lt;p&gt;All of this is very "consumer-first" and aimed at dialling in to both expand the numbers of people using Dell/Alienware tech and the areas in which it operates. And the only mention of AI in the entire thing is Jeff's little line at the beginning. It's clear, concise, focused on the tech and, in the Q&amp;amp;A that followed, refreshingly honest.&lt;/p&gt;
    &lt;p&gt;Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.&lt;/p&gt;
    &lt;p&gt;"One thing you'll notice is the message we delivered around our products was not AI-first," Dell head of product, Kevin Terwilliger says with a smile. "So, a bit of a shift from a year ago where we were all about the AI PC."&lt;/p&gt;
    &lt;p&gt;It's not that Dell doesn't care about AI or AI PCs anymore, it's just that over the past year or so it's come to realise that the consumer doesn't.&lt;/p&gt;
    &lt;p&gt;"We're very focused on delivering upon the AI capabilities of a device‚Äîin fact everything that we're announcing has an NPU in it‚Äîbut what we've learned over the course of this year, especially from a consumer perspective, is they're not buying based on AI," Terwilliger says bluntly. "In fact I think AI probably confuses them more than it helps them understand a specific outcome."&lt;/p&gt;
    &lt;p&gt;In a way, you could argue that's tantamount to dumbing down the technology for the end user. But this isn't like withholding information about the core counts of the chips inside your machine, or the TGP of the mobile GPU at its heart for fear of confusing some fictitious customer. There are people who care about the hardware inside these devices, but it's becoming clear there are precious few who care about the AI components or theoretical capabilities of those machines.&lt;/p&gt;
    &lt;p&gt;The fact that a huge PC brand such as Dell/Alienware has decided to ditch the AI-first marketing that seems to otherwise permeate everything‚Äîand honestly still permeates‚Äîis entirely welcome, very refreshing, and hopefully the mark of things to come.&lt;/p&gt;
    &lt;p&gt;Because, until AI becomes a valid, useful technology for the end user of these devices, and not just some marketing check box or buzzword for investors, every company ought to take a leaf out of Dell's book and just keep schtum. And that's honestly not something I've said many times about the big PC box shifter in the past.&lt;/p&gt;
    &lt;p&gt;1. Best CPU: AMD Ryzen 7 9800X3D&lt;/p&gt;
    &lt;p&gt;2. Best motherboard: MSI MAG X870 Tomahawk WiFi&lt;/p&gt;
    &lt;p&gt;3. Best RAM: G.Skill Trident Z5 RGB 32 GB DDR5-7200&lt;/p&gt;
    &lt;p&gt;4. Best SSD: WD_Black SN7100&lt;/p&gt;
    &lt;p&gt;5. Best graphics card: AMD Radeon RX 9070&lt;/p&gt;
    &lt;p&gt;Dave has been gaming since the days of Zaxxon and Lady Bug on the Colecovision, and code books for the Commodore Vic 20 (Death Race 2000!). He built his first gaming PC at the tender age of 16, and finally finished bug-fixing the Cyrix-based system around a year later. When he dropped it out of the window. He first started writing for Official PlayStation Magazine and Xbox World many decades ago, then moved onto PC Format full-time, then PC Gamer, TechRadar, and T3 among others. Now he's back, writing about the nightmarish graphics card market, CPUs with more cores than sense, gaming laptops hotter than the sun, and SSDs more capacious than a Cybertruck.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527706</guid><pubDate>Wed, 07 Jan 2026 15:46:07 +0000</pubDate></item><item><title>Many Hells of WebDAV: Writing a Client/Server in Go</title><link>https://candid.dev/blog/many-hells-of-webdav</link><description>&lt;doc fingerprint="376b9954f126582c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Many Hells of WebDAV&lt;/head&gt;
    &lt;p&gt;Implementing a WebDAV/CalDAV client and server should be easy! It‚Äôs a well documented spec, standardized in the early 00s, and somewhat widely supported. At least, that‚Äôs the naive assumption we started from when creating one for Homechart.&lt;/p&gt;
    &lt;head rend="h2"&gt;Existing Go Implementations&lt;/head&gt;
    &lt;p&gt;Now before you mention NIH syndrome, yes, we looked at the existing Go implementation, go-webdav. This library was lacking some key features we needed, like server-side collection synchronization, and the interfaces didn‚Äôt really align with our data model. This is also going to be a key feature of our product, so we should have some level of ownership for what gets implemented.&lt;/p&gt;
    &lt;head rend="h2"&gt;RFC Breadcrumbs&lt;/head&gt;
    &lt;p&gt;To start creating our client and server, we should read the RFCs, right? Well, where do you start?&lt;/p&gt;
    &lt;p&gt;How about the original, RFC 2518? Ah, looks like it was somewhat superseded by RFC 4918, but we‚Äôre not going to tell you which parts! How about those extension RFCs? There‚Äôs only 7 of them‚Ä¶&lt;/p&gt;
    &lt;p&gt;Reading through the RFCs, all that our implementation cares about is CRUD for Calendar events. After spending almost a month trying to implement the full RFC spec, we threw in the towel, there‚Äôs just to much legacy cruft that we didn‚Äôt need.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reverse Engineering&lt;/head&gt;
    &lt;p&gt;With a decent understanding of the RFC in hand, we instead looked into reverse engineering existing clients and servers by inspecting their requests and responses. This process was MUCH faster, and we quickly had the API mapped out and what kind of requests/responses we needed to support.&lt;/p&gt;
    &lt;p&gt;We started by identifying the clients/servers we wanted to support:&lt;/p&gt;
    &lt;p&gt;Clients:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple Calendar&lt;/item&gt;
      &lt;item&gt;DavX&lt;/item&gt;
      &lt;item&gt;Thunderbird&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Servers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apple iCloud&lt;/item&gt;
      &lt;item&gt;Google Calendar&lt;/item&gt;
      &lt;item&gt;Radicale&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then ran HTTP proxies or Wireshark to capture the HTTP requests. Because WebDAV is so obtuse, you not only need to inspect the HTTP body, but also the headers!&lt;/p&gt;
    &lt;head rend="h2"&gt;XML in Go&lt;/head&gt;
    &lt;p&gt;As an aside, we spent quite a bit of time trying to make XML work well in Go. The default Go XML library is truly terrible, and we decided to create a wrapper around it for managing XML nodes similar to how JavaScript manages HTML nodes:&lt;/p&gt;
    &lt;code&gt;var davDisplayName = xmel.Element{
  Name:  "displayname",
  Space: davNS,
}

davDisplayName.SetValue("name")
n, err := davResponse.Find(davCollectionType)
davOwner = davOwner.AddChild(davHref.SetValue("http://example.com"))
&lt;/code&gt;
    &lt;p&gt;With WebDAV having such an‚Ä¶‚Äúunstructured‚Äù schema to a lot of the requests/responses, this library was key in helping us marshal/unmarshal things without writing a bunch of ‚Äúbest case‚Äù structs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Standards are Just Suggestions&lt;/head&gt;
    &lt;p&gt;When we finally had our MVP built out, we put it to the test: validating our client and server against the existing implementations! For the most part, it worked as expected, but as always, things drift from the RFC.&lt;/p&gt;
    &lt;p&gt;Apple and Google, for instance, don‚Äôt implement half of the RFCs, and basically provide a MVP for other clients to use. They don‚Äôt really document what they support/don‚Äôt support, as WebDAV is supposed to do it via HTTP responses advertising capabilities, but both seem to provide generic responses advertising capabilities they don‚Äôt have a lot of the time.&lt;/p&gt;
    &lt;p&gt;The clients were another story. CalDAV clients are all over the place with what they support and how they will request it. Most clients should prefer to support &lt;code&gt;sync-collection&lt;/code&gt; as it‚Äôs very efficient, but Apple Calendar doesn‚Äôt, and uses ctags and etags instead.&lt;/p&gt;
    &lt;p&gt;As a little fish in a big pond, it‚Äôs frustrating dealing with situations where big providers can skirt around some standards or add quirks for their implementations, but I‚Äôm required to follow them to the T because I don‚Äôt have their inertia. I can‚Äôt file a bug, or a lawsuit, against them claiming nonconformance, they‚Äôll tell me to get bent. And you see this in other open source libraries too, where they‚Äôre littered with comments about workarounds for Google‚Äôs specific implementation or whatever.&lt;/p&gt;
    &lt;p&gt;I wouldn‚Äôt recommend anyone who values their sanity to pursue creating a WebDAV/CalDAV library.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527775</guid><pubDate>Wed, 07 Jan 2026 15:50:50 +0000</pubDate></item><item><title>Creators of Tailwind laid off 75% of their engineering team</title><link>https://github.com/tailwindlabs/tailwindcss.com/pull/2388</link><description>&lt;doc fingerprint="e68715d48379ef9"&gt;
  &lt;main&gt;&lt;list rend="ul"&gt;&lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;&lt;item&gt;Fork 0&lt;/item&gt;&lt;/list&gt;&lt;head rend="h1"&gt;feat: add llms.txt endpoint for LLM-optimized documentation #2388&lt;/head&gt;&lt;head id="button-67a95cccbe5eb25f" class="btn btn-sm btn-primary m-0 ml-0 ml-md-2"&gt;New issue&lt;/head&gt;&lt;p&gt;Have a question about this project? Sign up for a free GitHub account to open an issue and contact its maintainers and the community.&lt;/p&gt;&lt;p&gt;By clicking ‚ÄúSign up for GitHub‚Äù, you agree to our terms of service and privacy statement. We‚Äôll occasionally send you account related emails.&lt;/p&gt;&lt;p&gt;Already on GitHub? Sign in to your account&lt;/p&gt;&lt;head rend="h2"&gt;Conversation&lt;/head&gt;&lt;p&gt;Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Extract text from MDX files, removing JSX components and preserving code blocks&lt;/item&gt;&lt;item&gt;Remove standalone HTML blocks (not in code blocks)&lt;/item&gt;&lt;item&gt;Extract meaningful content from custom components (ApiTable, ResponsiveDesign, etc.)&lt;/item&gt;&lt;item&gt;Statically generate the output at build time&lt;/item&gt;&lt;item&gt;Include all 185 documentation files in proper order with sections&lt;/item&gt;&lt;/list&gt;&lt;p&gt;:)&lt;/p&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@quantizor is attempting to deploy a commit to the Tailwind Labs Team on Vercel.&lt;/p&gt;&lt;p&gt;A member of the Team first needs to authorize it.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head class="color-bg-subtle border-bottom-0 py-0 px-2"&gt; This comment was marked as outdated. &lt;/head&gt;&lt;head rend="h3"&gt;This comment was marked as outdated.&lt;/head&gt;&lt;head class="color-bg-subtle border-bottom-0 py-0 px-2"&gt; This comment was marked as outdated. &lt;/head&gt;&lt;head rend="h3"&gt;This comment was marked as outdated.&lt;/head&gt;&lt;code&gt;5dc6fde&lt;/code&gt;    to
    &lt;code&gt;326c151&lt;/code&gt;      
    Compare
  



    &lt;quote&gt;Add /llms.txt endpoint that serves a concatenated, text-only version of all Tailwind CSS documentation pages optimized for Large Language Model consumption. - Extract text from MDX files, removing JSX components and preserving code blocks - Remove standalone HTML blocks (not in code blocks) - Extract meaningful content from custom components (ApiTable, ResponsiveDesign, etc.) - Statically generate the output at build time - Include all 185 documentation files in proper order with sections&lt;/quote&gt;&lt;code&gt;326c151&lt;/code&gt;    to
    &lt;code&gt;5c005a9&lt;/code&gt;      
    Compare
  



    &lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@reinink this is ready to be reviewed&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Why is this one not moving?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Yeah I've been wondering that myself.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@petersuhm maybe you missed this before?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Have more important things to do like figure out how to make enough money for the business to be sustainable right now. And making it easier for LLMs to read our docs just means less traffic to our docs which means less people learning about our paid products and the business being even less sustainable.&lt;/p&gt;&lt;p&gt;Just don't have time to work on things that don't help us pay the bills right now, sorry. We may add this one day but closing for now.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Wow, what a disappointing response. This is complementary not replacement.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@adamwathan as someone who has sponsored Tailwind CSS in the past, this is a disappointing response.&lt;/p&gt;&lt;p&gt;Would you like to disclose the fact that sponsoring gives one access to an official collection of LLM rules for Tailwind? Does that have anything to do with the rejection of this PR?&lt;/p&gt;&lt;p&gt;If yes, fine. You're running a business, and that's cool. But you should disclose the fact that you are monetizing this (making Tailwind docs LLM-friendly).&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;It is mentioned on the sponsorship page. Seems strange to not mention that when closing this PR, though.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;In general I object to the spirit of closing this. It's very OSS unfriendly and would not meaningfully reduce traffic to the docs by humans that actually would buy the product.&lt;/p&gt;&lt;p&gt;Just bad vibes.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;Here's a friendly tip for the Tailwind team that you should already know, but I will repeat anyways:&lt;/p&gt;&lt;p&gt;If your goal is monetizing your software, then making your software as easy to use for people's workflows, is paramount.&lt;/p&gt;&lt;p&gt;The more people that find which your software fits into their workflow seamlessly, and solves pain in their daily interactions, the more people you have as potential monetization candidates.&lt;/p&gt;&lt;p&gt;By scrapping features under the guise of 'monetization' you are sending the opposite of the message you likely intend.&lt;/p&gt;&lt;p&gt;You are telling your customers that getting money from them, is more important than providing a service to help them.&lt;/p&gt;&lt;p&gt;Tell me, would you enjoy doing business with a company who had a stance like that?&lt;/p&gt;&lt;p&gt;This feature is so that people can build MORE things with Tailwind in a FASTER and more EFFICIENT capacity.&lt;/p&gt;&lt;p&gt;From a business management perspective, if you remove the stigmatic 'AI' and 'LLM' from the conversation, and you simply are evaluating a feature XYZ which allows your customers to work in a more automated and efficient capacity with your software, with minimal engineering effort (all it takes is a simple build-time script)...&lt;/p&gt;&lt;p&gt;Why would you not want that for your customers?&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;I totally see the value in the feature and I would like to find a way to add it.&lt;/p&gt;&lt;p&gt;But the reality is that 75% of the people on our engineering team lost their jobs here yesterday because of the brutal impact AI has had on our business. And every second I spend trying to do fun free things for the community like this is a second I'm not spending trying to turn the business around and make sure the people who are still here are getting their paychecks every month.&lt;/p&gt;&lt;p&gt;Traffic to our docs is down about 40% from early 2023 despite Tailwind being more popular than ever. The docs are the only way people find out about our commercial products, and without customers we can't afford to maintain the framework. I really want to figure out a way to offer LLM-optimized docs that don't make that situation even worse (again we literally had to lay off 75% of the team yesterday), but I can't prioritize it right now unfortunately, and I'm nervous to offer them without solving that problem first.&lt;/p&gt;&lt;p&gt;@PaulRBerg I don't see the AGENTS.md stuff we offer as part of the sponsorship program as anything similar to this at all ‚Äî that's just a short markdown file with a bunch of my own personal opinions and what I consider best practices to nudge LLMs into writing their Tailwind stuff in a specific way. It's not the docs at all, and I resent the accusation that I am not disclosing my "true intentions" here or something.&lt;/p&gt;&lt;p&gt;@mtsears4 Tailwind is growing faster than it ever has and is bigger than it ever has been, and our revenue is down close to 80%. Right now there's just no correlation between making Tailwind easier to use and making development of the framework more sustainable. I need to fix that before making Tailwind easier to use benefits anyone, because if I can't fix that this project is going to become unmaintained abandonware when there is no one left employed to work on it. I appreciate the sentiment and agree in spirit, it's just more complicated than that in reality right now.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;@quantizor As far as I can tell, this PR doesn't close an existing issue and I don't see any evidence of you having proposed this feature in any forum. You just opened a PR. That entitles you to neither a merge nor other people's time to review it.&lt;/p&gt;&lt;p&gt;(I'm not a Tailwind employee, just some guy)&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;&lt;p&gt;There is an associated discussion. tailwindlabs/tailwindcss#14677 (comment)&lt;/p&gt;&lt;p&gt;You're entirely right that I am not entitled to anyone's time. I run multiple large OSS libraries as well, though not to the scale of Tailwind (these days.)&lt;/p&gt;&lt;p&gt;My objection is the way this was handled.&lt;/p&gt;&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527950</guid><pubDate>Wed, 07 Jan 2026 16:02:19 +0000</pubDate></item><item><title>The Case for Nushell (2023)</title><link>https://www.sophiajt.com/case-for-nushell/</link><description>&lt;doc fingerprint="1708481539a47a9d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The case for Nushell&lt;/head&gt;August 30, 2023 -&lt;p&gt;Recently, I had a chat with some of my friends about Nushell and why they stuck with traditional shells like bash/zsh or the "new" hotness like fish rather than using Nushell. After chatting with them, my brain kept bubbling away at the state of how folks were using their terminals and the end result is this blog post.&lt;/p&gt;&lt;p&gt;In this post, I make the case for really taking a hard look at Nushell and also for generally asking the question: "can the state of shells be improved enough to overcome the inertia of sticking to what you know?"&lt;/p&gt;&lt;head rend="h2"&gt;The lay of the land&lt;/head&gt;&lt;p&gt;Let's take a look at some of the offerings out there that people are using everyday.&lt;/p&gt;&lt;head rend="h3"&gt;Bash/zsh&lt;/head&gt;&lt;p&gt;Bash, originally a set of improvements on the Bourne shell, has grown to be the default shell for almost all Linux distros. That's generally people's first experience when they hit the terminal. It's what they see when they log into a remote machine. It's reached the definition of ubiquitous.&lt;/p&gt;&lt;p&gt;I also throw 'zsh' in here as well. Apple's macOS switched from bash to zsh, an operationally similar shell but created a bit more recently.&lt;/p&gt;&lt;p&gt;Bash at this point has become so well known that people often confuse support for bash-isms as part of the POSIX standard, but we'll talk about that later.&lt;/p&gt;&lt;p&gt;Pros: it's everywhere. Learn once, run anywhere.&lt;/p&gt;&lt;p&gt;Cons: as a language, bash/zsh feels a bit too retro. It doesn't offer any of the modern programming language style, tool support, etc folks would be used to from other languages. In truth, bash was never really meant for writing the kind of large scripts that people are maintaining today.&lt;/p&gt;&lt;p&gt;Example for loop in bash:&lt;/p&gt;&lt;code&gt;#!/bin/bash
for i in `seq 1 10`;
do
        echo $i
done
&lt;/code&gt;
&lt;head rend="h3"&gt;Fish&lt;/head&gt;&lt;p&gt;As fish's website says: "Finally, a command line shell for the 90s"&lt;/p&gt;&lt;p&gt;It's enough to elicit a smirk, because you know it's a bit true. The bash/zsh style shells are getting left behind by something that feels a bit nicer, has nicer completions, looks nicer (you can get similar improvements out of bash if you work at it, but fish ships with them out of the box)&lt;/p&gt;&lt;p&gt;Fish also bravely steps away from the shell scripting form of bash to something a bit more readable.&lt;/p&gt;&lt;p&gt;Example for loop in fish:&lt;/p&gt;&lt;code&gt;for i in (seq 1 10);
    echo $i;
end
&lt;/code&gt;
&lt;p&gt;Pros: the interactive experience of fish does feel quite a bit nicer that bash/zsh out of the box. Scripting is a bit nicer.&lt;/p&gt;&lt;p&gt;Cons: As it says on the tin, it's a shell for the 90s. It ain't the 90s anymore.&lt;/p&gt;&lt;head rend="h3"&gt;PowerShell&lt;/head&gt;&lt;p&gt;Coming into 1.0 at 2006, PowerShell is one of the first shells to really draw a line in the sand to say "enough, we're going to do things differently". The unix style of pipelines, where commands communicate via text to each other was replaced by a .NET engine that passed objects between commands.&lt;/p&gt;&lt;p&gt;The impact wasn't immediately obvious but as devops folks (and others) discovered what was possible when you have ways to work with data directly a fanbase grew.&lt;/p&gt;&lt;p&gt;Example of a for(each) loop in PowerShell:&lt;/p&gt;&lt;code&gt;foreach ($i in 1..10) {
    echo $i
}
&lt;/code&gt;
&lt;p&gt;PowerShell came with an opinionated design that focused on verb-noun naming, improvements to shell syntax, and a vast set of functionality drawn from the .NET ecosystem.&lt;/p&gt;&lt;p&gt;Pros: it's a structured shell - you can actually work with objects rather than text. Powerful set of tools and capabilities taken from .NET.&lt;/p&gt;&lt;p&gt;Cons: I'll go ahead and say it: PowerShell was never really designed to be a language first. The verb-noun convention forces a style that feels very awkward coming from other languages. Worth a mention: earlier versions of PowerShell were Windows-only and modern crossplatform support lacks some of the features of the earlier versions.&lt;/p&gt;&lt;head rend="h3"&gt;Other shells&lt;/head&gt;&lt;p&gt;When I was coming up, there were a lot of other shells, including the csh/tcsh family. Having said that, I don't know anyone who is using any of the other family of shells. Bash/zsh and to some extent fish really dominate the developer mindshare.&lt;/p&gt;&lt;head rend="h2"&gt;Hold up, we really need to talk about POSIX&lt;/head&gt;&lt;p&gt;We really need to take a minute and talk about POSIX before we continue. A lot of folks have leveled "but it's not POSIX" as an argument against using Nushell, but I'd like to turn that around and ask the question:&lt;/p&gt;&lt;p&gt;"What's so good about POSIX?"&lt;/p&gt;&lt;p&gt;Most folks when asked would likely point to it as a common ground that code can be ported to. In reply, I'd like to quote a few bits of the POSIX standard.&lt;/p&gt;&lt;p&gt;The following are reserved words in the POSIX standard for shell scripting:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;case&lt;/item&gt;&lt;item&gt;do&lt;/item&gt;&lt;item&gt;done&lt;/item&gt;&lt;item&gt;elif&lt;/item&gt;&lt;item&gt;else&lt;/item&gt;&lt;item&gt;esac&lt;/item&gt;&lt;item&gt;fi&lt;/item&gt;&lt;item&gt;for&lt;/item&gt;&lt;item&gt;if&lt;/item&gt;&lt;item&gt;in&lt;/item&gt;&lt;item&gt;then&lt;/item&gt;&lt;item&gt;until&lt;/item&gt;&lt;item&gt;while&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Yes, really. &lt;code&gt;fi&lt;/code&gt; and &lt;code&gt;esac&lt;/code&gt; are a joke that never found their end. No one would design a language that did that with a straight face these days.&lt;/p&gt;&lt;p&gt;Let's take a quick look at the number of flags common Unix commands ship with. These are on my macOS system, so ymmv.&lt;/p&gt;&lt;table&gt;&lt;row span="2"&gt;&lt;cell role="head"&gt;command&lt;/cell&gt;&lt;cell role="head"&gt;number of flags&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;ls&lt;/cell&gt;&lt;cell&gt;45&lt;/cell&gt;&lt;/row&gt;&lt;row span="2"&gt;&lt;cell&gt;man&lt;/cell&gt;&lt;cell&gt;15&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;ps&lt;/cell&gt;&lt;cell&gt;29&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;p&gt;If you look through the flags of &lt;code&gt;ls&lt;/code&gt; to see why it has so many, notice how many are configuring what &lt;code&gt;ls&lt;/code&gt; is displaying. In a real sense, this is going against the underlying philosophy of unix pipelines. Rather than composing a pipeline to get the display you want, you're learning a language of flags for each command to configure the display.&lt;/p&gt;&lt;p&gt;Let's talk about exit codes. Actually, wait, I already did that. As I point out in the post, the standard says:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;"The value of status may be 0, EXIT_SUCCESS, EXIT_FAILURE, [CX] [Option Start] or any other value, though only the least significant 8 bits (that is, status &amp;amp; 0377) shall be available from wait() and waitpid(); the full value shall be available from waitid() and in the siginfo_t passed to a signal handler for SIGCHLD. [Option End]"&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Sure - we all still use 8-bit machines while flipping through a manual we printed trying to find the exit code description to understand why a command failed.&lt;/p&gt;&lt;p&gt;I hear you: "but, look, it doesn't matter how archaic this stuff is, if we all agree to use it things keep working."&lt;/p&gt;&lt;p&gt;I dunno - these arguments just don't hold up. We'd still be using C as our main systems language because it's the most documented, most portable, etc. But, by and large, we don't. We're increasingly choosing other languages.&lt;/p&gt;&lt;p&gt;The truth is, in 2023 if someone asked us to design a system, we wouldn't design POSIX. If, in 2023, someone asked us to design a shell language, we wouldn't design bash/zsh. This matters.&lt;/p&gt;&lt;head rend="h2"&gt;Show us the money&lt;/head&gt;&lt;p&gt;I make some pretty bold statements in the above. There is a heritage of technology that got us to this point. While that heritage is important, it's not without its drawbacks. Are there better ways of doing it?&lt;/p&gt;&lt;head rend="h3"&gt;Why structure matters&lt;/head&gt;&lt;p&gt;Before we get into talking about Nushell, let's talk about why structured data matters.&lt;/p&gt;&lt;p&gt;In the Unix pipeline way of thinking, text passes between commands. This is very flexible, but has a major problem: both the outputting command and the inputting command have to agree what shape that text will take so the info can be passed. This locks representation to presentation, disallowing commands from evolving their output over time. As we showed earlier, it also encourages a proliferation of flags.&lt;/p&gt;&lt;p&gt;That's annoying. Does separating the structure from its presentation help us?&lt;/p&gt;&lt;code&gt;&amp;gt; ls | where size &amp;gt; 10kb
&lt;/code&gt;
&lt;p&gt;I always start with this example when showing off Nushell, because not only is it immediately readable, we didn't have to dig through any flags to figure out what we needed to pass to &lt;code&gt;ls&lt;/code&gt; to get that. We also aren't parsing anything from &lt;code&gt;ls&lt;/code&gt;. Instead, the data is passed directly to &lt;code&gt;where&lt;/code&gt;, which handles it directly.&lt;/p&gt;&lt;p&gt;Commands already know this structure, why not make use of it?&lt;/p&gt;&lt;p&gt;The same &lt;code&gt;where&lt;/code&gt; command works on other things. For example, we can process the output of the &lt;code&gt;ps&lt;/code&gt; command:&lt;/p&gt;&lt;code&gt;&amp;gt; ps | where cpu &amp;gt; 40
&lt;/code&gt;
&lt;p&gt;Or open a &lt;code&gt;csv&lt;/code&gt; file and processing its rows:&lt;/p&gt;&lt;code&gt;&amp;gt; open fields.csv | where area &amp;gt; 5
&lt;/code&gt;
&lt;p&gt;And so on. It's the same &lt;code&gt;where&lt;/code&gt; regardless of where the data is coming from. It also gives us the freedom to present the data however we want.&lt;/p&gt;&lt;head rend="h2"&gt;Why Nushell matters&lt;/head&gt;&lt;head rend="h3"&gt;Nushell is designed to be a language&lt;/head&gt;&lt;p&gt;I had the good fortune of being a part of some prominent programming language teams, including helping to create TypeScript and helping create Rust's error messages as part of the Rust team in Mozilla. Designing languages to be easy to use, easy to read, easy to scale up, and easy to debug is something I care about and have worked on for many years.&lt;/p&gt;&lt;p&gt;To that end, Nushell is designed with an eye towards being readable at a glance.&lt;/p&gt;&lt;p&gt;Let's do a &lt;code&gt;for&lt;/code&gt; loop in Nushell:&lt;/p&gt;&lt;code&gt;for i in 1..10 {
    print $i
}
&lt;/code&gt;
&lt;p&gt;(aside: "but why do variables have dollar signs?". Turns out the flexibility of shell programming allows paths to not use quotes, so it's nice to tell a difference between &lt;code&gt;cd foo&lt;/code&gt; and &lt;code&gt;cd $foo&lt;/code&gt;)&lt;/p&gt;&lt;p&gt;This eye towards usable design shows up in many ways. Working with data is improved by not only having structure, but also being able to pattern match against it. Here's an example of pattern matching a list in Nushell:&lt;/p&gt;&lt;code&gt;match $list {
  [$one] =&amp;gt; { print "one element list" }
  [$one, $two] =&amp;gt; { print "two element list" }
  [$head, ..$tail] =&amp;gt; { print $"the tail of the list is ($tail)" }
}
&lt;/code&gt;
&lt;p&gt;In a way, working in Nushell should feel at home both interactively as a shell and as a full scripting language. We've had folks write COVID reporting software in Nushell, research experiments, even entire shells for well-known database services.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell is typechecked&lt;/head&gt;&lt;p&gt;Since Nushell doesn't treat all data as text, you can represent tables, records, numbers, booleans, etc directly in the language.&lt;/p&gt;&lt;p&gt;As a result of this, Nushell is fully typechecked. Common errors can be caught early and shown to you before the script even runs.&lt;/p&gt;&lt;p&gt;Taking what we learned from TypeScript - the types also feed into another important tool.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell has IDE support&lt;/head&gt;&lt;p&gt;The types, autocompletion, and early error reporting feed into an engine in Nushell that knows a lot more about your code. As a result, you can write scripts and then work with them using the IDE support Nushell provides. Seeing errors, jumping to definitions, getting documentation on hovers, etc are all part of the Nushell experience.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell has nice errors&lt;/head&gt;&lt;p&gt;In Nushell, we make extensive use of remembering where data comes from, as well as what caused an error. Simple errors, like division by zero, are shown clearly:&lt;/p&gt;&lt;p&gt;A more complex error may need to show more to help track down where a mistake came from. Let's say you've accidentally put a string in your list of numbers, and then tried to process it:&lt;/p&gt;&lt;head rend="h3"&gt;Nushell has a SQL-like style&lt;/head&gt;&lt;p&gt;When you start using Nushell to compose pipelines, you'll notice that it has a distinct SQL-like style. Data flows through each stage, and you build up what you want to do to it as you add more commands.&lt;/p&gt;&lt;p&gt;This gives Nushell a distinctive design that encourages experimentation and exploration.&lt;/p&gt;&lt;head rend="h3"&gt;Nushell is, and has always been, crossplatform&lt;/head&gt;&lt;p&gt;An important decision we made from day 1 was to be crossplatform. You can run Nushell on Windows, Linux, and macOS (and BSD, and Android) and get the same experience. You can easily write scripts in a way that they can be run across different platforms. Everything that you learn transfers between OSes without friction.&lt;/p&gt;&lt;head rend="h2"&gt;Is Nushell good enough to overcome the inertia?&lt;/head&gt;&lt;p&gt;I distinctly remember going to a SIAM conference many years back and giving a talk on the Chapel programming language. Even back then, it was a clever language. In a couple lines, you could write code that could distribute and process a matrix across a network of computers. Coming from a lineage of array languages, it ate up data parallel processing. The equivalent code in other languages looked verbose in comparison.&lt;/p&gt;&lt;p&gt;I went through my talk, hoping I'd done a decent job of conveying the main points, and at the end, someone in the audience stood up and said "but I can do all this in C++".&lt;/p&gt;&lt;p&gt;He proceeded to explain that if he could recreate many of the techniques we showed all as part of a C++ library that people could use. At this time, I wasn't sure how to respond other than "but you don't have to, we already built this language" but he couldn't be swayed. If it wasn't C++, he didn't want it.&lt;/p&gt;&lt;p&gt;Fast forward a couple years, and I'm standing in front of a JavaScript audience giving a similar talk, this time promoting TypeScript. I remember the kind of politely confused looks on people's faces as I showed off the features TypeScript offered. There was a similar sense of "why do we need to leave JavaScript?".&lt;/p&gt;&lt;p&gt;To answer whether Nushell can overcome this kind of inertia, I'll pose two questions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Is Nushell compelling enough for a single person to adopt it?&lt;/item&gt;&lt;item&gt;Would adopting Nushell broadly as a community move the needle?&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Let's tackle the first question. Time and again, as people try Nushell, they come back with quotes like "this is the most excited I've been about tech in 15 years". It has a fanbase that loves it, and that fanbase is growing. It reminds me of the early days of Rust, just after hitting 1.0.&lt;/p&gt;&lt;p&gt;To the second question: would adopting Nushell broadly actually improve things noticeably? Without a doubt. I say this without any reservation. Thinking of our shells as structured, interactive processing engines opens up the doors to a much wider array of things you can do with them. The commands would be far simpler than their POSIX equivalents and would compose far better. They'd benefit from the full knowledge of the data being shared between them. Adaptors could be made to connect to all parts of the system, allowing you full, structured interaction with everything you have access to.&lt;/p&gt;&lt;p&gt;In essence, as the saying goes, we'd be building a skyscraper starting from the 15th floor instead of the 1st.&lt;/p&gt;&lt;head rend="h2"&gt;It's time to be honest about what Nushell is&lt;/head&gt;&lt;p&gt;It's time I come clean about what Nushell is. Nushell isn't exactly a shell, at least not in the traditional Unix sense of the word. Nushell is trying to answer the question: "what if we asked more of our shells?"&lt;/p&gt;&lt;p&gt;Nushell is really an interactive, data-focused scripting language with shell capabilities. It merges these three things into one:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A fully-typed scripting language&lt;/item&gt;&lt;item&gt;An interactive shell&lt;/item&gt;&lt;item&gt;A data processing system (that can also handle large data loads via dataframes)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Rather than these being three separate ideas glued together, in practice it feels like Nushell is treating everything you interact with as data. This allows you to pull together different kinds of data, and know that the same commands will work over them.&lt;/p&gt;&lt;p&gt;You might look at that list and think you don't need all that, but the way I might frame it is this: it's nice to have it when you need it.&lt;/p&gt;&lt;p&gt;I don't need to do heavy data processing everyday, but it's nice to not have to shift what I'm doing at all when I need to do it. I don't have to download new utilities or switch languages. It's all right there. Need to write a script to load some files and handle some directory processing? Still right there. Need to throw together some web query that outputs the top download results for a github repo? You guessed it, all still right there.&lt;/p&gt;&lt;p&gt;This is just scratching the surface, really. Nushell has a plugin system that allows more capabilities to be added based on your needs. We already have plugins that add a variety of additional file formats, querying capabilities, and more.&lt;/p&gt;&lt;head rend="h2"&gt;It's okay to have nice things&lt;/head&gt;&lt;p&gt;Nushell was built with a simple idea: working in the shell, writing code, and processing data should be fun. To that end, we work hard to make Nushell feel nice.&lt;/p&gt;&lt;p&gt;You can write readable scripts that come with their own documentation, and then come back to them 6 months later and still understand what they're doing.&lt;/p&gt;&lt;p&gt;You can sit in the shell and play with pipeline ideas until one grows into a scripting project and then effortlessly transition your experiment into a full script.&lt;/p&gt;&lt;head rend="h2"&gt;That's it&lt;/head&gt;&lt;p&gt;That's my case. It's okay to have fun. It's okay to write attractive, well-documented code. It's okay to leave the designs of the past behind when they no longer fit the present day.&lt;/p&gt;&lt;p&gt;It's okay to move on to better ways of doing things.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46528142</guid><pubDate>Wed, 07 Jan 2026 16:15:01 +0000</pubDate></item><item><title>BillG the Manager</title><link>https://hardcoresoftware.learningbyshipping.com/p/019-billg-the-manager</link><description>&lt;doc fingerprint="27401b374f6f26de"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;019. BillG the Manager&lt;/head&gt;
    &lt;head rend="h3"&gt;‚ÄúIntelli‚Ä¶what?‚Äù‚ÄìBill Gates&lt;/head&gt;
    &lt;p&gt;The breadth of the Microsoft product line and the rapid turnover of core technologies all but precluded BillG from micro-managing the company in spite of the perceptions and lore around that topic. In less than 10 years the technology base of the business changed from the 8-bit BASIC era to the 16-bit MS-DOS era and to now the tail end of the 16-bit Windows era, on the verge of the Win32 decade. How did Bill manage this ‚Äî where and how did he engage? This post introduces the topic and along with the next several posts we will explore some specific projects.&lt;/p&gt;
    &lt;p&gt;Please feel free to share this and subscribers, please join in the discussion.&lt;/p&gt;
    &lt;p&gt;Back to 018. Microsoft‚Äôs Two Bountiful Gardens&lt;/p&gt;
    &lt;p&gt;At 38, having grown Microsoft as CEO from the start, Bill was leading Microsoft at a global scale that in 1993 was comparable to an industrial-era CEO. Even the legendary Thomas Watson Jr., son of the IBM founder, did not lead IBM until his 40s. Microsoft could never have scaled the way it did had BillG managed via a centralized hub-and-spoke system, with everything bottlenecked through him. In many ways, this was BillG‚Äôs product leadership gift to Microsoft‚Äîa deeply empowered organization that also had deep product conversations at the top and across the whole organization.&lt;/p&gt;
    &lt;p&gt;This video from the early 1980‚Äôs is a great introduction to the breadth of Microsoft‚Äôs product offerings, even at a very early stage of the company. It also features some vintage BillG voiceover and early sales executive Vern Raburn. (Source: Microsoft videotape)&lt;/p&gt;
    &lt;p&gt;Bill honed a set of undocumented principles that defined interactions with product groups. The times of legendary BillG reviews characterized by hardcore challenges and even insults had become, mostly, a thing of the past excepting the occasional sentimental outburst. More generally, they were a collective memory of hyper-growth moments any start-up experiences, only before the modern era when such stories were more commonly understood.&lt;/p&gt;
    &lt;p&gt;Much later in 2006, when BillG announced his intent to transition from full time Microsoft and part time philanthropy to full time philanthropy, many reporters surprised him by asking how Microsoft would continue without his coordination of technical strategy and oversight. But even in the early ‚Äô90s, at the height of the deepest and most challenging technology strategy questions, he never devoted the bulk of his time to micromanaging product development. He spent a good deal of time engaged with products, but there were far too many at too many stages of development to micro-manage them. In many ways this was the opposite of the approach Steve Jobs took, even if both were known for their own forms of challenging interactions. The most obvious contrast between the two was the breadth of the product line and the different market touchpoints.&lt;/p&gt;
    &lt;p&gt;Having grown up through Development Tools and Languages, I was familiar with Microsoft‚Äôs product line, but only as TA did it become clear how comparatively broad Microsoft had so quickly become. The software world was thought of through a lens of major categories: operating systems, tools and languages, networking, and applications, roughly mirroring Microsoft‚Äôs org chart. The latter was thought of as word processing, spreadsheets, graphics, databases, as well as assorted smaller categories. It was easy to identify leaders in each of those areas‚Äînames that were tip of the tongue at the time and most of which are no longer in the PC software space (IBM, Borland, Novell, WordPerfect, Lotus, Aldus, Ashton Tate, and many more). The ah-ha moment in the early 1990s was the realization that no company on that list was competing in more than one category. Microsoft was hardly winning in every category. In fact, in most categories it was new entry, a distant second, or even third place, but the company was in every space. Bill was committed and patient. Microsoft was relentless. And Microsoft was focused on Windows.&lt;/p&gt;
    &lt;p&gt;BillG had fostered Microsoft with a grand vision to compete in every category of PC software, from some of the earliest days. With rare exceptions, no other company set out to do that. BillG led a deep technology strategy. It started with the operating system, supported by tools and languages, and then using those to build applications. This seemed simple enough. In fact, it is what IBM built for mainframes and DEC built for minicomputers.&lt;/p&gt;
    &lt;p&gt;There was a crucial difference. Microsoft did not build hardware and was not vertically integrated to reduce competition. Microsoft built an operating system on an openly architected PC (the same Intel-based architecture that came to power both Macintosh and Linux years later) and published APIs so that anyone could build tools and applications for the operating system‚Äîan open hardware platform and open operating system APIs. This approach simply addressed all the early challenges Microsoft itself faced trying to figure out how to build winning applications‚Äîit was so busy dealing with dozens of proprietary computing platforms, each with their own tools and APIs just different enough to make things difficult, but not so different as to be valuable. Bill saw the value in software and in openness at key points in the overall architecture. At the formation of the company, he and PaulA saw the immense and expansive value of software and, essentially, the liability that being in the hardware business carried. Building Microsoft‚Äôs software-only business on an open hardware platform where many players competed to drive prices down while maintaining compatibility with the operating system was one of the all-time great strategy choices. The idea of building hardware seemed like a sucker‚Äôs bet, with low margins, manufacturing, and inventory‚Äîthe baggage of the physical world. While Microsoft would dabble in peripherals or hardware that could bootstrap new PC scenarios, building whole computers was a headache better left to others.&lt;/p&gt;
    &lt;p&gt;Expanding the impact of that breadth software strategy was BillG‚Äôs day-to-day operating model, not micromanaging the specifics of any given project. I am painting this with a broad brush, intentionally so. Part of the difference between the then dominant cultures of Systems and Apps was that during the MikeMap era (and arguably during the earlier JeffH era), Apps weaned itself from Bill‚Äôs intense and constant scrutiny whereas the Systems culture more clearly embraced that dynamic. That was largely true until PaulMa took a more hands-off (or walled-off) approach to the nurturing of the NT project.&lt;/p&gt;
    &lt;p&gt;In his May 1991 email, ‚ÄúChallenges and Strategy,‚Äù BillG set the company on the Windows strategy, clarifying the foundations for every product and group, solidifying what had been complex platform choices every team faced. Regardless of whether Bill was a savant when it came to the technical details of projects or he simply remembered everything each group sent or told him, he operated the company at a higher level of abstraction than reporters believed to be the case in 2008 when he ultimately reduced his full-time commitment to Microsoft.&lt;/p&gt;
    &lt;p&gt;I had a glimpse of this when our AFX team had our pivotal review. Later as TA I was there to connect the dots and amplify the Windows strategy. By and large the company was still wrapping itself around the details of what it really meant to embrace Windows, exclusively. That, and coping with the myriad of choices and decisions that come from the tension between aligning with a Windows strategy and having some control over your own destiny as a product. Which version of Windows? When is that shipping? Will the APIs our product needs be in Windows? Will those APIs work on older versions of Windows? What about Windows NT? On, which microprocessors? What about the other parts of Microsoft? The questions were endless. This was truly big company stuff‚Äîthe strategy at a high level is one thing, but execution across a $600M (1994) research and development budget was another. The fascinating thing was how products so quickly scaled beyond what Bill personally experienced as a programmer, both in size and technology specifics. This was to be expected‚Äîby any measure the company was huge‚Äîbut people and Bill himself still expected to interact on product details as though he was a member of the product team. I often found myself looking for ways to help Bill engage at that level, even if just for show.&lt;/p&gt;
    &lt;p&gt;In addition to the Windows strategy, with the late 1993 launch of Office 4, Microsoft also declared 1994 ‚ÄúYear of Office‚Äù. It was the biggest launch for Apps and represented a major pivot of the organization to the opportunity of selling a suite of products. This too was in the earliest days of a strategy, one that I would end up spending significant time on as TA and then later as a member of the team.&lt;/p&gt;
    &lt;p&gt;Just because Bill operated at a level of abstraction across products groups did not preclude product groups from engaging on what might seem like relatively small, non-technical matters. One of the more entertaining meetings I attended was preparing for the launch of Office 4, which was a worldwide event complete with a reporter given permission to shadow the team. A key differentiator would be how the user would experience ‚Äúintelligence‚Äù in the product, so that it understood what was intended and how to achieve it in the new Office software. The development team built a series of features along the lines of what was termed ‚Äúbasic use‚Äù such as AutoCorrect in Word, AutoFilter in Excel tables, and a host of Wizards (guided step-by-step flows such as for creating charts), and more. To bring them together and actually communicate with the market and on retail packaging, the marketing team came up with an umbrella term. Pete Higgins (PeteH) came over to brief BillG on that choice in a small meeting in Bill‚Äôs office.&lt;/p&gt;
    &lt;p&gt;PeteH was by then the spiritual leader of the business side of Apps. He rose through the ranks of Excel and was clearly MikeMap‚Äôs lead executive. Pete was the kind of calm and in control leader that everyone enjoyed working for‚Äîhe was at once clearly the boss, but also a member of the team. Pete was a native of the Seattle area, high school football star, and Stanford graduate. He was a new generation of Microsoft product executive, coming from the business and not the coding side. For me in my TA role, Pete was one of my biggest supporters and mentors and made connecting with Apps super easy.&lt;/p&gt;
    &lt;p&gt;Sitting at the little couch under the Intel chip poster, after going through the details of the launch, Pete said the proverbial ‚Äúthere‚Äôs one more thing.‚Äù Bill rocking in his chair shook his head, given that the meeting was mostly an uneventful recap of the upcoming press tour. Pete went on to explain the problem of communicating all the features and how Microsoft needed a term to market and describe them. Pete was dancing around this because he knew well enough that Bill was not a fan of ‚Äúmarketing‚Äù. Ever so delicately Pete said, ‚Äúthis is your chance‚Ä¶we want to go with this term but if you don‚Äôt like it‚Ä¶‚Äù&lt;/p&gt;
    &lt;p&gt;Pete then said, ‚ÄúIntelliSense. Microsoft Office introduces IntelliSense.‚Äù&lt;/p&gt;
    &lt;p&gt;Bill‚Äôs reply, ‚ÄúIntelli‚Ä¶what?‚Äù&lt;/p&gt;
    &lt;p&gt;Pete again tried to position the positioning, his instinct about resistance proving correct. ‚ÄúIt is IntelliSense‚Ä¶it means that Office has built-in intelligence, and it understands what you need and how to do it.‚Äù&lt;/p&gt;
    &lt;p&gt;Bill still not warming up, went full pedantic, ‚Äúwhat intelligence‚Ä¶is there a Prolog rules engine, a neural network, ‚Ä¶.‚Äù He was also making the scrunched up surprised look that he does, which turns out (once you realize it) to also be a bit sarcastic. It meant he was warming up.&lt;/p&gt;
    &lt;p&gt;A few more times back and forth, and Pete just made Bill say IntelliSense in a sentence one more time, which he did with kind of a devilish smirk.&lt;/p&gt;
    &lt;p&gt;Done.&lt;/p&gt;
    &lt;p&gt;Looking back this all seems absurd. Consternation over a single phrase. Literally seeking approval to use it from the CEO of a billion-dollar company. All on the heels of what was no doubt months of preparation, including getting SteveB‚Äôs approval which was actually critical. Finally, the theater that Pete would pull the plug a few weeks before the tour. In some ways this was the Apps way of bringing decisions to Bill‚Äîit wasn‚Äôt really a choice and it had been broadly vetted and was buttoned-up. Any debate would probably be theater more than anything.&lt;/p&gt;
    &lt;p&gt;On average, there was one product-focused meeting on most days. Most teams saw Bill once or twice a year. NathanM saw Bill most every day or at least in most every technology context, present day or far out there. Most executives, like PaulMa, PeteH (leading Apps), and Susan Boeschen (SusanB leading consumer), saw Bill in product review contexts several times a month because each had many ongoing projects or, in the case of the big projects (like operating systems), many large components. Everyone was in constant contact over email. Bill was always forwarding emails across the company, adding relevant people from all levels of the organization to the CC line, and never backed off a good reply-all opportunity. Phone or in-person 1:1s were not the typical way of interacting across the product executive team. For the most part, work happened in groups or at least with an audience, with outcomes and flare-ups quickly disseminated by email. I found myself constantly on the move walking around campus from one building to the next to meet people in person, rarely was I in my office (a pattern that continued my entire career).&lt;/p&gt;
    &lt;p&gt;I was often asked to meet with teams before they met with Bill. They hoped for insight into how BillG might think about choices and decisions or even the presentation overall. I often disappointed teams in these pre-meetings since I was hardly a stand-in for Bill, and I was hardcore about leaving any such impression. Pre-meetings gave me a chance to better understand the issues the team was struggling with and to make sure those were brought forward in an objective and transparent manner. The fastest path to failure was to structure a conversation so Bill discovered an issue rather than having it revealed to him. To be fair, an equally fast path to failure was a first slide listing a slew of problems and issues in the hopes of inoculating the remainder of the meeting. In that case, I would caution teams that they were exposing themselves to the inevitable ‚ÄúHow can this be so difficult?‚Äù comments. Getting this balance right was the essence of leading an effective meeting.&lt;/p&gt;
    &lt;p&gt;For most meetings, I wrote a summary meeting preview. Even though Bill said he did not want this, I could not help myself. While he was always effective, I felt that a little bit of specifics could go a long way in making the meeting more effective and less random. I could tell he had read my mail if he raised a point verbatim from my note, and frequently he would kick off the meeting doing so, never crediting me of course. In these, and all mails talking about other teams, I always tried to separate the facts of the meeting, the team‚Äôs analysis, and my own opinion. Bill was transparent with email and thought little of forwarding an entire thread. I learned the ramifications of that the hard way.&lt;/p&gt;
    &lt;p&gt;As an example of where I failed to follow my own rules about fact versus opinion, I totally offended Jim Allchin (JimAll), leading the Cairo project, on the role of a specific technology in distributed programming. Not only did Jim inform me that my opinions were wrong, but also that I stepped all over his own PhD dissertation as a leading expert. In hindsight, this was terrifying‚ÄîJim‚Äôs reply was brutal‚Äîbut it proved a good early learning experience, so to speak.&lt;/p&gt;
    &lt;p&gt;While the product line was already broad, the expansion to entirely new areas was unstoppable. On most any product area, we were forming an opinion, beginning work, or already in the market. There was not a booth at a tradeshow, a focused conference, or a major company looking to partner that Microsoft was not already connected to or connecting with in some way. While Microsoft was in the earliest days of achieving a PC in every home (about 25 percent of US households in 1993) and on every desktop (about half of US workers in 1993), every day in this job was either furthering that or expanding beyond homes and desktops from data centers to handhelds to airplanes (the first in-flight PC-based system was an early partnership between Microsoft and an airline, including certification for Windows Server).1&lt;/p&gt;
    &lt;p&gt;Product meetings had no set format or structure and usually reflected the culture of the organization. This might be a surprise to some as many CEOs (or perhaps their staff!) might have imposed some more rigor on meetings. Microsoft had two bountiful gardens, but there were micro-cultures throughout out the company. While one group did slick and well-rehearsed presentations, another might present research-heavy deep dives. Bill often pushed a team outside its comfort zone, deliberately pushing the team to discuss places they were less prepared, or even less interested. It was a technique he employed. He once said to me, ‚ÄúWhy spend all the time with the Windows team talking about architecture, if that was their predisposition anyway?‚Äù This was also a strategy to level the playing field‚Äîtalking about architecture to Windows or ease of use to Excel was too lopsided and Bill was disadvantaged.&lt;/p&gt;
    &lt;p&gt;The reality of BillG Reviews never lived up to lore.&lt;/p&gt;
    &lt;p&gt;Most meetings progressed without incident‚Äîmeaning without yelling. Sometimes, though, there were comments such as ‚ÄúThat was the stupidest thing I ever heard‚Äù or ‚ÄúThat is brain-dead.‚Äù The worst was ‚ÄúThat‚Äôs trivial . . . let me show you.‚Äù Those were all the clich√©s that teams anticipated but then wore as a badge of honor. They happened with far less frequency compared to how much they were talked about. Even over the short period of time I worked as TA, Bill became more intentional in his use of meeting dynamics. Still, the first seconds of a meeting remained a bit of a mood thermometer, pity those for whom it was clearly a bad day.&lt;/p&gt;
    &lt;p&gt;When meetings ended up ‚Äúbad‚Äù it was always because the team was poorly prepared, or they came to talk about the project in a way that diverged from expectations. There were typical capital offenses in the meeting, such as failing to understand a product strategy of competitors or downplaying a competitor‚Äôs potential. Worst was coming across as though a product was making mostly tactical decisions driven by schedule or failing to understand the architecture of the product relative to the evolving platform and related teams across Microsoft. PivotTables were just making their way across most teams, so many were still making the common errors of using static charts and graphs that always seemed to have the data oriented or filtered in the least useful way. Those moments always held potential for a lively discussion.&lt;/p&gt;
    &lt;p&gt;Part of my role was to reduce the potential for such liveliness ahead of time. I tried to alert teams about potential issues without acting as a surrogate for Bill, and to make sure meetings did not save the difficult or bad news for the end. I was also there to throw myself on the grenade, so to speak, and get meetings back on track by helping the team through a tough moment‚Äîusually by restating or interpreting what they were saying or by redirecting the topic at hand to a follow-up discussion.&lt;/p&gt;
    &lt;p&gt;By far the biggest strategic error one could make was knowingly duplicating code outside core expertise, and then compounding that by attempting to explain why in this particular case it is justified. Microsoft Publisher was a new product in the desktop publishing category. It was being built by the Consumer Division under the leadership of Melinda French (MelindaF). The product aimed for the small business and non-professional market, compared to the incumbent Aldus PageMaker. It differentiated itself with ease-of-use features, pioneering Wizards and other user interface innovations. But it also produced printed pages that looked a lot like what one should be able to create with Microsoft Word. This overlap was the source of endless consternation‚Äîwhy can‚Äôt they share code, why can‚Äôt Word do all these features, and then ultimately why does Publisher even exist. Yet, customers loved it. At one point, a meeting went down a rabbit hole over bullets and numbering and how Publisher was basically writing all the same code Word was and wasting everyone‚Äôs resources. There was little actionable in this kind of rant, but it did establish the norm of being called out for redundancy and the need to be prepared to cope with the feedback.&lt;/p&gt;
    &lt;p&gt;Bill maintained a deep commitment to evaluating a portfolio of efforts, and even within a single product he believed in the portfolio approach of features‚Äînot every product nor every feature was a winner or a breakthrough, but on the whole something needed to be working. As much as Bill might give a group a difficult time (as happened with Visual C++), he knew there was always more to the product and more products to the company. It was not just that Bill was building a product portfolio for Microsoft, he was managing the teams as a portfolio of efforts. This portfolio approach created a resiliency in the company‚Äîresilient to the unpredictable nature of technology bets and to the ability of the people on the team to execute. Not everything went as planned nor did every planned bet ultimately make sense.&lt;/p&gt;
    &lt;p&gt;Whether deliberate or not, BillG had three axes that created a constant state of balance, of push and pull, across the hundred teams creating software. Bill‚Äôs approach of constantly balancing the tension between innovation and shipping, expanding the portfolio while maintaining coherency, and the injection of new ideas while also executing on existing work proved to be the most interesting ‚Äúmanagement‚Äù lesson. The next three sections are examples of each of these dimensions.&lt;/p&gt;
    &lt;p&gt;On to 020. Innovation versus Shipping: The Cairo Project&lt;/p&gt;
    &lt;p&gt;https://nces.ed.gov/programs/digest/d08/tables/dt08_432.asp&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46528192</guid><pubDate>Wed, 07 Jan 2026 16:18:23 +0000</pubDate></item><item><title>Health care data breach affects over 600k patients, Illinois agency says</title><link>https://www.nprillinois.org/illinois/2026-01-06/health-care-data-breach-affects-600-000-patients-illinois-agency-says</link><description>&lt;doc fingerprint="e6188ade5513a61a"&gt;
  &lt;main&gt;
    &lt;p&gt;The names and addresses of thousands of patients of the Illinois Department of Human Services were incorrectly made publicly viewable for the last several years, the agency said Friday.&lt;/p&gt;
    &lt;p&gt;Several maps created to assist the agency with decisions ‚Äî like where to open new offices and allocate certain resources ‚Äî were made public through incorrect privacy settings between 2021 and 2025, the Department of Human Services said in a statement.&lt;/p&gt;
    &lt;p&gt;More than 32,000 customers with the IDHS division of rehabilitation services had information publicly viewable between April 2021 and September 2025. The information included names, addresses, case numbers, case status, referral source information, region and office information and status as Division of Rehabilitation Services recipients, the agency said.&lt;/p&gt;
    &lt;p&gt;Around 670,000 Medicaid and Medicare Savings Program recipients had their addresses, case numbers, demographic information and the name of medical assistance plans publicly viewable between January 2022 and September 2025, IDHS said.&lt;/p&gt;
    &lt;p&gt;The state agency said the mapping website was unable to identify who viewed the maps, and IDHS is unaware of any misuse of personal information resulting from the data leak.&lt;/p&gt;
    &lt;p&gt;IDHS discovered the issue Sept. 22 and immediately changed the privacy settings for all maps, restricting access to authorized IDHS employees, the agency said. It also implemented a secure map policy that prohibits uploading customer data to public mapping websites.&lt;/p&gt;
    &lt;p&gt;Individuals whose information was made public will receive a notice about the leak from IDHS. The notices will include a phone number that people can call for more information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46528353</guid><pubDate>Wed, 07 Jan 2026 16:28:14 +0000</pubDate></item></channel></rss>