<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 02 Oct 2025 20:10:43 +0000</lastBuildDate><item><title>NL Judge: Meta must respect user's choice of recommendation system</title><link>https://www.bitsoffreedom.nl/2025/10/02/judge-in-the-bits-of-freedom-vs-meta-lawsuit-meta-must-respect-users-choice/</link><description>&lt;doc fingerprint="bc501a786403c13c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Judge in the Bits of Freedom vs. Meta lawsuit: Meta must respect users’ choice&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;02 oktober 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Today the judge issued a ruling in the summary proceedings brought by digital human rights organisation Bits of Freedom against Meta. The organisation demanded that Meta gives its users on apps such as Instagram and Facebook the option to select a feed that is not based on profiling.&lt;/p&gt;
    &lt;p&gt;Bits of Freedom sued Meta for a breach of the Digital Services Act (DSA). This European legislation is intended to give users more autonomy and control over the major online platforms. One of the core elements of the DSA is that users must have greater influence over the information they see.&lt;/p&gt;
    &lt;p&gt;For many people, and especially for young people, social media platforms are a major source of news and information. Therefore it is crucial that users themselves can decide which content appears on their feed. Without that freedom of choice, participation in the public debate is seriously hampered. That is problematic at any time, but especially so during election periods. In the Netherlands, national elections will be held at the end of this month.&lt;/p&gt;
    &lt;p&gt;The judge states that Meta is indeed acting in violation of the law. He says that “a non‑persistent choice option for a recommendation system runs counter to the purpose of the DSA, which is to give users genuine autonomy, freedom of choice, and control over how information is presented to them.” The judge also concludes that the way Meta has designed its platforms constitutes “a significant disruption of the autonomy of Facebook and Instagram users.” The judge orders Meta to adjust its apps so that the user’s choice is preserved, even when the user navigates to another section or restarts the app.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“We are pleased that the judge now makes clear that Meta must respect the user’s choice,” says Maartje Knaap, spokesperson for Bits of Freedom. “It is absolutely unacceptable that a handful of American tech billionaires determine how we see the world. That concentration of power poses a risk to our democracy. At the same time, it is regrettable that we need to go to court to ensure Meta complies with the law.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Meta has an interest in steering users toward a feed where it can show as many interest‑ and behavior‑based ads as possible. That is the core of Meta’s revenue model. Subtle design techniques push users toward that feed, while the non‑profiled feed is hidden behind a logo, making it hard to find. Users who do choose the alternative timeline also lose direct access to features such as Direct Messages. Moreover, when you open the app, it always starts with Meta’s feed, even if the user selected a different one before. Because of the judge’s ruling, Meta must change its behavior.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“This ruling shows that Meta is not untouchable,” continues Maartje Knaap. “But we are also realistic, this is just a drop in the ocean. There’s still a long way to go. We hope the decision will inspire individuals, civil society organisations, regulators and lawmakers worldwide around the world who are working to rein in Meta’s power. Together we can stand up to a company that has become overwhelmingly powerful.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can find the ruling here (in Dutch).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45448326</guid><pubDate>Thu, 02 Oct 2025 11:32:19 +0000</pubDate></item><item><title>Red Hat confirms security incident after hackers breach GitLab instance</title><link>https://www.bleepingcomputer.com/news/security/red-hat-confirms-security-incident-after-hackers-claim-github-breach/</link><description>&lt;doc fingerprint="f046ce5b18c3474a"&gt;
  &lt;main&gt;
    &lt;p&gt;Correction: After publishing, Red Hat confirmed that it was a breach of one of its GitLab instances, and not GitHub. Title and story updated.&lt;/p&gt;
    &lt;p&gt;An extortion group calling itself the Crimson Collective claims to have stolen nearly 570GB of compressed data across 28,000 internal development respositories, with the company confirming it was a breach of one of its GitLab instances.&lt;/p&gt;
    &lt;p&gt;This data allegedly includes approximately 800 Customer Engagement Reports (CERs), which can contain sensitive information about a customer's network and platforms.&lt;/p&gt;
    &lt;p&gt;A CER is a consulting document prepared for clients that often contains infrastructure details, configuration data, authentication tokens, and other information that could be abused to breach customer networks.&lt;/p&gt;
    &lt;p&gt;Red Hat confirmed that it suffered a security incident related to its consulting business, but would not verify any of the attacker's claims regarding the stolen GitLab repositories and customer CERs.&lt;/p&gt;
    &lt;p&gt;"Red Hat is aware of reports regarding a security incident related to our consulting business and we have initiated necessary remediation steps," Red Hat told BleepingComputer.&lt;/p&gt;
    &lt;p&gt;"The security and integrity of our systems and the data entrusted to us are our highest priority. At this time, we have no reason to believe the security issue impacts any of our other Red Hat services or products and are highly confident in the integrity of our software supply chain."&lt;/p&gt;
    &lt;p&gt;After publishing our story, Red Hat confirmed that the security incident was a breach of its GitLab instance used solely for Red Hat Consulting on consulting engagements, and not GitHub.&lt;/p&gt;
    &lt;p&gt;While Red Hat did not respond to any further questions about the breach, the hackers told BleepingComputer that the intrusion occurred approximately two weeks ago.&lt;/p&gt;
    &lt;p&gt;They allegedly found authentication tokens, full database URIs, and other private information in Red Hat code and CERs, which they claimed to use to gain access to downstream customer infrastructure.&lt;/p&gt;
    &lt;p&gt;The hacking group also published a complete directory listing of the allegedly stolen GitLab repositories and a list of CERs from 2020 through 2025 on Telegram.&lt;/p&gt;
    &lt;p&gt;The directory listing of CERs include a wide range of sectors and well known organizations such as Bank of America, T-Mobile, AT&amp;amp;T, Fidelity, Kaiser, Mayo Clinic, Walmart, Costco, the U.S. Navy’s Naval Surface Warfare Center, Federal Aviation Administration, the House of Representatives, and many others.&lt;/p&gt;
    &lt;p&gt;If you have any information regarding this incident or any other undisclosed attacks, you can contact us confidentially via Signal at 646-961-3731 or at tips@bleepingcomputer.com.&lt;/p&gt;
    &lt;p&gt;The hackers stated that they attempted to contact Red Hat with an extortion demand but received no response other than a templated reply instructing them to submit a vulnerability report to their security team.&lt;/p&gt;
    &lt;p&gt;According to them, the created ticket was repeatedly assigned to additional people, including Red Hat's legal and security staff members.&lt;/p&gt;
    &lt;p&gt;BleepingComputer sent Red Hat additional questions, and we will update this story if we receive more information.&lt;/p&gt;
    &lt;p&gt;The same group also claimed responsibility for briefly defacing Nintendo’s topic page last week to include contact information and links to their Telegram channel&lt;/p&gt;
    &lt;p&gt;Update 10/2/25: Story updated with correction from Red Hat that it was a GitLab instance that was breached and not a GitHub account.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Security Validation Event of the Year: The Picus BAS Summit&lt;/head&gt;
    &lt;p&gt;Join the Breach and Attack Simulation Summit and experience the future of security validation. Hear from top experts and see how AI-powered BAS is transforming breach and attack simulation.&lt;/p&gt;
    &lt;p&gt;Don't miss the event that will shape the future of your security strategy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45448772</guid><pubDate>Thu, 02 Oct 2025 12:28:27 +0000</pubDate></item><item><title>EU funds are flowing into spyware companies and politicians demanding answers</title><link>https://www.theregister.com/2025/10/02/eu_spyware_funding/</link><description>&lt;doc fingerprint="894fb13d86a73093"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;EU funds are flowing into spyware companies, and politicians are demanding answers&lt;/head&gt;
    &lt;head rend="h2"&gt;Experts say Commission is ‘fanning the flames’ of the continent’s own Watergate&lt;/head&gt;
    &lt;p&gt;An arsenal of angry European Parliament members (MEPs) is demanding answers from senior commissioners about why EU subsidies are ending up in the pockets of spyware companies.&lt;/p&gt;
    &lt;p&gt;The group of 39 politicians referred to recent investigations that revealed countries such as Italy, Greece, Hungary, and Spain have funnelled millions of taxpayer euros at a time to help support commercial spyware-makers' finances.&lt;/p&gt;
    &lt;p&gt;They wrote: "According to these findings, entities such as Intellexa, Cy4Gate, Verint and Cognyte – whose technologies have been linked to unlawful surveillance of journalists, human rights defenders and political actors in the EU, as well as in third countries with dreadful human rights records – have benefitted from public financing, including EU programmes.&lt;/p&gt;
    &lt;p&gt;"This raises serious questions about the governance, transparency, and accountability of the Union's funding mechanisms. In the light of the scandals uncovered in Italy, Greece, Poland, Hungary, and Spain, among others, and of the recommendations of the PEGA inquiry, it is deeply troubling that the Union is directly or indirectly enabling tools that erode democracy, fundamental rights, and the rule of law."&lt;/p&gt;
    &lt;p&gt;MEPs cited investigative journalism from Follow The Money, which revealed in September that institutions such as Spain's public-funded Centre for the Development Of Industrial Technology (CDTI) handed over €1.3 million (c $1.5 million) to now-shuttered spyware peddler Mollitiam Industries.&lt;/p&gt;
    &lt;p&gt;Likewise, Italy's state-owned bank, Mediocredito Centrale, was found to have acted as a guarantor to a €2.5 million (c $2.9 million) loan to Dataflow Security, an Italy-based commercial spyware developer.&lt;/p&gt;
    &lt;p&gt;FTM said that it did not prove that any of the money, in any of the cases it found, was used to directly fund spyware development, although funding was provided in several instances.&lt;/p&gt;
    &lt;p&gt;The letter addressed to senior commissioners Henna Virkkunen (Finland), Michael McGrath (Ireland), and Piotr Serafin (Poland) – who oversee tech, justice, and anti-fraud respectively – requested greater transparency over how EU funds were distributed, among other matters.&lt;/p&gt;
    &lt;p&gt;Various questions were raised by the MEPs, such as how the European Commission verifies the integrity of the entities that receive EU funds, whether any risk assessments are carried out before investments are made to spyware companies, and how much money in total has been awarded to these organizations.&lt;/p&gt;
    &lt;p&gt;They also asked for answers on how the Commission plans to ensure its funding mechanisms align with its stances on matters such as human rights and digital resilience, and why it has not implemented the recommendations made by the PEGA inquiry.&lt;/p&gt;
    &lt;p&gt;The Register approached the European Commission for a response to the letter.&lt;/p&gt;
    &lt;p&gt;For the uninitiated, the PEGA inquiry was launched in 2022 following reports of several EU governments using NSO Group's Pegasus spyware a year earlier, and three years after Saudi journalist Jamal Khashoggi's murder.&lt;/p&gt;
    &lt;p&gt;The results were published in 2023, branding the pervasive spyware use "Europe's Watergate" and a "severe violation of all the values of the European Union."&lt;/p&gt;
    &lt;p&gt;The report stated: "The spyware scandal is not a series of isolated national cases of abuse, but a full-blown European affair.&lt;/p&gt;
    &lt;p&gt;"EU Member State governments have been using spyware on their citizens for political purposes and to cover up corruption and criminal activity. Some went even further and embedded spyware in a system deliberately designed for authoritarian rule."&lt;/p&gt;
    &lt;p&gt;Among the main recommendations made by the PEGA committee were to restrict law enforcement's use of spyware only to exceptional cases, protect sensitive targets like politicians, lawyers, and doctors, and to set the conditions for legal use.&lt;/p&gt;
    &lt;p&gt;The 39 MEPs asked the European Commission to additionally commit to launching an immediate public review of EU subsidies flowing into spyware companies.&lt;/p&gt;
    &lt;p&gt;In that review, the politicians specifically requested details on all funds issued and awarded to spyware companies since 2015, a commitment to excluding all spyware vendors from future EU funding instruments, and a follow-up on the PEGA recommendations.&lt;/p&gt;
    &lt;p&gt;"Citizens of the Union have the right to know whether their taxes are being used to finance technologies that endanger their fundamental rights," they wrote.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;North Korea's Lazarus Group shares its malware with IT work scammers&lt;/item&gt;
      &lt;item&gt;Google pushes emergency patch for Chrome 0-day – check your browser version now&lt;/item&gt;
      &lt;item&gt;We're number 1! America now leads the world in surveillanceware investment&lt;/item&gt;
      &lt;item&gt;Who watches the watchmen? Surveillanceware firms make bank, avoid oversight&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;"As Members of the European Parliament, we expect your full cooperation in ensuring accountability and restoring public trust."&lt;/p&gt;
    &lt;p&gt;Rebecca White, researcher and advisor on targeted surveillance at Amnesty Tech's Security Lab, said she and Amnesty support the letter, and highlighted the Commission's lack of communication on the matter.&lt;/p&gt;
    &lt;p&gt;She told The Register: "At Amnesty, we've documented for many years how spyware has enabled human rights abuses in Europe and beyond, and how the surveillance industry is under-regulated and thriving.&lt;/p&gt;
    &lt;p&gt;"The European Commission has remained silent. These latest allegations should alarm all of us.&lt;/p&gt;
    &lt;p&gt;"They suggest that not only is the EU failing to put out the fire, they're fanning the flames. We welcome this collective call for transparency and explanations – the Commission can no longer wash its hands of Europe's complicity in the spyware crisis, which is fuelling human rights abuses across the world."&lt;/p&gt;
    &lt;p&gt;Aljosa Ajanovic Andelic, policy advisor at European Digital Rights (EDRi), echoed the MEPs' request for a ban on spyware.&lt;/p&gt;
    &lt;p&gt;He said: "The lack of action by the Commission when it comes to spyware is appalling and dangerous. In fact, not only have they not done anything to stop the proliferation of shady spyware vendors in the EU, they actually used EU taxpayers' money to directly fund the industry. This has to stop, and we are calling for a full ban on commercial spyware in the EU."&lt;/p&gt;
    &lt;p&gt;"As the largest digital rights network in Europe, our position is firm: the use of spyware is inherently incompatible with fundamental rights, and therefore should be banned, as well as the market of private companies that are profiting from human rights violations." ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45448825</guid><pubDate>Thu, 02 Oct 2025 12:34:43 +0000</pubDate></item><item><title>Daniel Stenberg on 22 curl bugs found by AI and fixed</title><link>https://mastodon.social/@bagder/115241241075258997</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45449348</guid><pubDate>Thu, 02 Oct 2025 13:29:55 +0000</pubDate></item><item><title>N8n added native persistent storage with DataTables</title><link>https://community.n8n.io/t/data-tables-are-here/192256</link><description>&lt;doc fingerprint="bfbc187f418403ca"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Hey everyone &lt;/head&gt;
        &lt;head rend="h2"&gt;We’re super excited to share that starting with v1.113 we’re rolling out data tables (beta) to all plans. &lt;/head&gt;
        &lt;p&gt;Since the very beginning of n8n we’ve heard many of you mention the need for a proper table inside n8n to store data between workflow executions without needing to switch platforms or setting up credentials and now it’s finally here.&lt;/p&gt;
        &lt;p&gt;With data tables you can:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Save specific data from your workflow runs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Keep data around between multiple executions&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Avoid duplicate runs by tracking execution status&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Store reusable prompts for different workflows&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Collect evaluation data for your AI workflows&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Do lookups, merges, enhancements…&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;…and honestly, probably 100 other creative things we haven’t even thought of yet &lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
        &lt;p&gt; To make sure your instance stays performant, we’ve set a 50MB limit for everyone. If you’re self-hosting (and know what you’re doing), you can change that via the ENV variable &lt;code&gt;N8N_DATA_TABLES_MAX_SIZE_BYTES&lt;/code&gt;&lt;/p&gt;
        &lt;p&gt; Upgrade to 1.113, give data tables a spin, and let us know what you think! What’s missing? What would make it even more useful for you? We’re really curious to hear your ideas and thoughts! &lt;/p&gt;
        &lt;p&gt; Read more about the data tables in the docs here.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 38 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;liam
2&lt;/div&gt;
      &lt;p&gt; 7 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;dszp
3&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is absolutely awesome to see, I can’t wait to use these! It’s probably been my number one frustration that saving even a small amount of data between executions for all sorts of purposes requires either integrating PostgreSQL and dealing with schemas, using a third party database or API like Supabase (as handy as they are), or using variables that are powerful but are somewhat clumsy to instantiate and track since they only work in Code nodes and only save data for production executions, making testing hard. Hoping data-tables makes a ton of these things easier! Probably won’t run the new version until it’s in final release rather than pre-release, but this is awesome to see!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 4 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;bartv
5&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is really great - when I migrated from “the other platform” almost 4 years ago, I really felt the pain of not having a simple in-app data storage. I played around with Data tables this weekend and it’s just SUCH a good and fast experience! Kudos to our Product and Engineering teams &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Hey all,&lt;/p&gt;
        &lt;p&gt;IMPORTANT NOTE: There is an issue with very large SQLite databases that is causing instances to slow down. Out of an abundance of caution, we are unfortunately removing version 1.113.0 until we fix this issue. We hope to have this released again with a fix within the next couple of days.&lt;/p&gt;
        &lt;p&gt;Very sorry about this!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 10 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;This is great!&lt;/p&gt;
        &lt;p&gt;It´d be cool for self hosting to be able to add a second DB, where n8n pulls the data from. So one could have performance without having to set up each time a postgres connection.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;bartv
10&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Data tables is back on!&lt;/p&gt;
        &lt;p&gt;A patch was released earlier today. It has now been tested and we have high confidence. Please update to 1.113.1 (which is still in beta) to try this feature.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 4 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;TH1
11&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;is that Data Tables only available for the Cloud version? local host will not have Data Tables?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;liam
12&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;It’s on all plans (cloud and self hosted) starting on version 1.113.1 &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;Sujit
13&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;I am unable to see the data tables in my local self hosted n8n. I’ve also updated the docker image to pick the latest one. What am I missing?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Does this mean we can share data between multiple workflows now? This would make splitting up complex workflows across multiple workflows so much easier.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;I believe this is still only available in the beta version?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jabbson
16&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;The fact that the “latest” is not “1.113.1”. The latest is “the latest stable”, where 1.113.1 is not that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Very happy to see this feature. I’ve been testing it out, and was commenting feedback on Reddit but someone in the Discord server said the forums is the best place to post this instead. Here’s my list(so far)&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;When going to the Data Tables tab, “Create Table” is not default on the upper right button, it’s defaulted to “Create Workflow” instead.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Cannot change the data type after a column is created.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Cannot set any of the column’s as primary or unique such as the ID column (To prevent duplicates)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;For some odd reason, setting a column data type to “number” then pushing data from JSON array into the table, physically opening the table and looking at the rows, the numbers in the “number” data type column are not all together. For example “29683389” shows in the table as “29 683 389”. This isn’t a one off either, ALL rows exhibit the same behavior and ALL columns set as “numbers” too.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Table page can only show 50 rows per page. Which I understand is probably for performance reasons. However, there really needs to be a “search” function for the table to search for data.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Are there any limitations for creating tables?&lt;lb/&gt; or we can create multiples/unlimited (in 50Mb limit)?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Love this list @compaholic, thanks so much for sharing it.&lt;lb/&gt; 1, 2 and 5 are all planned. For (4), I think that is just a highlighting to make it easier to read that it’s actually 29M. So the number should still be correct.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;You can created unlimited ones within the storage limit &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450044</guid><pubDate>Thu, 02 Oct 2025 14:26:23 +0000</pubDate></item><item><title>Two Amazon delivery drones crash into crane in commercial area of Tolleson, AZ</title><link>https://www.abc15.com/news/region-west-valley/tolleson/two-amazon-delivery-drones-crash-into-crane-in-commercial-area-of-tolleson</link><description>&lt;doc fingerprint="554baf5711d3a7d2"&gt;
  &lt;main&gt;
    &lt;p&gt;TOLLESON, AZ — The Tolleson Police Department is investigating after two Amazon delivery drones crashed on Wednesday morning.&lt;/p&gt;
    &lt;p&gt;Officials say they are working an active investigation after the two drones crashed into a crane that was in a commercial area near 96th Avenue and Roosevelt Street.&lt;/p&gt;
    &lt;p&gt;It's unclear if anyone was injured during the incident.&lt;/p&gt;
    &lt;p&gt;ABC15 reached out to Amazon which provided the following statement: “We’re aware of an incident involving two Prime Air drones in Tolleson, Arizona. We’re currently working with the relevant authorities to investigate.”&lt;/p&gt;
    &lt;p&gt;This is a developing story and will be updated once new information becomes available.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450449</guid><pubDate>Thu, 02 Oct 2025 14:52:49 +0000</pubDate></item><item><title>Work is not school: Surviving institutional stupidity</title><link>https://www.leadingsapiens.com/surviving-institutional-stupidity/</link><description>&lt;doc fingerprint="37111b1e29438f9c"&gt;
  &lt;main&gt;
    &lt;p&gt;For 16+ years, we master the rules of school. Study hard, get good grades, follow the formula and ultimately merit wins. Then we enter the workforce and none of it works quite like we thought. This becomes painfully obvious as you rise higher in the org.&lt;/p&gt;
    &lt;p&gt;Even seasoned veterans forget this. Recently, a director-level client hit a minor career bump and spiraled into crisis mode, their expectations still anchored in what I call "school rules".&lt;/p&gt;
    &lt;p&gt;Organizations don't run purely on merit or even clear criteria. Although they claim otherwise using buzzwords like “merit” and “data”. That’s only one part of the story, and also what’s visible.&lt;/p&gt;
    &lt;p&gt;The other part, often more consequential, runs on flawed psychology, imperfect decisions, and competing interests. You can call it organizational absurdities. Or more bluntly, institutional stupidity.&lt;/p&gt;
    &lt;p&gt;What follows is a reality check. It’s a “letter to frustrated high-performers” who keep bumping up against these unwritten rules of work. Consider it your guide to staying sane while playing the long game.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have to, blame stupidity not malice&lt;/item&gt;
      &lt;item&gt;Organizations are anything but meritocracies&lt;/item&gt;
      &lt;item&gt;Perception matters as much as performance&lt;/item&gt;
      &lt;item&gt;Don’t waste time fighting for “objective fairness.”&lt;/item&gt;
      &lt;item&gt;Positioning what you offer&lt;/item&gt;
      &lt;item&gt;Mind the gap: your standards vs their’s&lt;/item&gt;
      &lt;item&gt;Higher you go, more it’s an inverted funnel&lt;/item&gt;
      &lt;item&gt;Know which game you’re choosing to play&lt;/item&gt;
      &lt;item&gt;Watching your circle of control&lt;/item&gt;
      &lt;item&gt;Keep a balanced portfolio&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;If you have to, blame stupidity not malice&lt;/head&gt;
    &lt;p&gt;Most of what we chalk up to “politics” or “backstabbing”, aka bad intent, is often better explained by stupidity, inertia, bad incentives, fragmented attention, and misaligned maps of reality.&lt;/p&gt;
    &lt;p&gt;People are juggling too much, thinking too little, and rarely stepping back to ask, “What actually makes sense here?”&lt;/p&gt;
    &lt;p&gt;When you assume stupidity instead of malice, you stay above the fray, stop taking slights personally, or turning misjudgments into betrayals. This way we retain agency and choice.&lt;/p&gt;
    &lt;p&gt;Assuming malice turns you into a cynic. In contrast, assuming stupidity keeps you curious. Instead of fighting ghosts, you study the system and ask better questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What pressures is that person responding to?&lt;/item&gt;
      &lt;item&gt;What game are they trying to win?&lt;/item&gt;
      &lt;item&gt;What am I assuming as rational that’s not?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No one is out to get you; they’re just out to get through the week. The shift from malice to stupidity gives you just enough distance to be curious instead of reactive.&lt;/p&gt;
    &lt;head rend="h2"&gt;Organizations are anything but meritocracies&lt;/head&gt;
    &lt;p&gt;Managers will claim they are, and we want to believe them. But the reality is that the best don’t always rise. At least not as easily or automatically as we think they should.&lt;/p&gt;
    &lt;p&gt;Sometimes they do. But often, what gets rewarded isn’t performance but proximity to power, timing, perception, and political usefulness.&lt;/p&gt;
    &lt;p&gt;This doesn’t mean performance doesn’t matter. It means performance is necessary but not sufficient. It is the entry ticket that gets you through the door, but does not guarantee a seat at the table.&lt;/p&gt;
    &lt;p&gt;Assuming that excellence is obvious is the fatal error of the conscientious expert. Although it creates value, performance doesn’t automatically generate visibility, influence, or narrative. And those are the currencies that get traded when decisions are made by humans.&lt;/p&gt;
    &lt;p&gt;Merit matters, but it needs a stage and a spotlight. It doesn’t mean becoming a shameless self-promoter. Rather, your work needs a distribution strategy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perception matters as much as performance&lt;/head&gt;
    &lt;p&gt;In school, everyone was evaluated against an objective criteria by someone paid to assess fairly.&lt;/p&gt;
    &lt;p&gt;In organizations, no such thing exists. Instead, perception is the “data”. And this data is constructed often haphazardly by busy people working off limited inputs. You have to manage the story by shaping impressions intentionally.&lt;/p&gt;
    &lt;p&gt;Not only does perception matter as much as performance but who’s doing the perceiving matters even more.&lt;/p&gt;
    &lt;p&gt;Not all perceivers are created equal. A peer may love your work but they might not be a critical node in the web of influence. Who gets consulted? Do they know what you’ve built, and have they heard your name in relevant contexts?&lt;/p&gt;
    &lt;p&gt;It’s not just “do great work.”; it’s also “do the work that’s perceived as valuable.” This means translating your work’s significance up the chain and shape its interpretation. If not, others will do it for you and they may not be generous, or even accurate.&lt;/p&gt;
    &lt;p&gt;For more, see my last two articles: Schrodinger’s Cat at Work Part I and Schrodinger’s Cat at Work Part II.&lt;/p&gt;
    &lt;head rend="h2"&gt;Don’t waste time fighting for “objective fairness.”&lt;/head&gt;
    &lt;p&gt;On paper, organizations love metrics: KPIs, OKRs, dashboards. They create the appearance of detached objectivity.&lt;/p&gt;
    &lt;p&gt;Meanwhile, subjective decisions are constantly happening behind the scenes. The decisions about who to trust, or who gets a shot are made through informal reputations and shared stories about your value. Then the “data” is used to justify them in retrospect.&lt;/p&gt;
    &lt;p&gt;Rather than rant against the system, get good at reading the underlying subjective logic:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Who does this person trust, and why?&lt;/item&gt;
      &lt;item&gt;What do they consider strategic vs tactical?&lt;/item&gt;
      &lt;item&gt;What would make them feel safe betting on me?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Subjectivity isn’t the enemy. It’s the underlying physics of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Positioning what you offer&lt;/head&gt;
    &lt;p&gt;You may have had the right message but at the wrong moment, or in the wrong wrapper.&lt;/p&gt;
    &lt;p&gt;Positioning is the context around your contributions: Why now? Why you? Why this way? A good idea, or stellar performance, poorly positioned can seem irrelevant. In contrast, a mediocre one nicely positioned is deemed visionary.&lt;/p&gt;
    &lt;p&gt;It’s not just what you say but also when, how, and through whom you say it.&lt;/p&gt;
    &lt;p&gt;Persistence matters as well. Think in campaigns, not just one-time efforts. In my piece on effective conversations I wrote:&lt;/p&gt;
    &lt;quote&gt;It is the series of messages in different forms that over time makes the difference. More akin to waves shaping the shoreline rather than the occasional once in a lifetime tsunami.&lt;/quote&gt;
    &lt;p&gt;What messages are you sending, are they varied, and are you doing it consistently? This is as true for marketing products as it is for positioning yourself inside organizations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mind the gap: your standards vs their’s&lt;/head&gt;
    &lt;p&gt;Another obvious but forgotten reality of organizational life: Not everyone operates by the same playbook.&lt;/p&gt;
    &lt;p&gt;You prioritize substance and direct contribution, while others focus on visibility and relationship-building in ways that are uncomfortable to you. It’s that colleague who excels at positioning routine work as “strategic”, or the peer who builds influence through cultivating key relationships.&lt;/p&gt;
    &lt;p&gt;And yes, these approaches do yield results.&lt;/p&gt;
    &lt;p&gt;But this doesn’t mean dismissing it as pure politics or simply abandoning your principles. It means understanding the landscape you're operating in.&lt;/p&gt;
    &lt;p&gt;You can’t effectively participate in a game you refuse to see clearly. You're not at a disadvantage because you choose to act with integrity. It’s because you fail to recognize that influence flows through multiple channels and others are willing to play differently.&lt;/p&gt;
    &lt;p&gt;The key is not to match their behavior but to factor it in. Instead of expecting fairness, anticipate asymmetry. And then get creative about how you play.&lt;/p&gt;
    &lt;p&gt;Being ethical doesn’t mean being passive but tactically awake.&lt;/p&gt;
    &lt;head rend="h2"&gt;Higher you go, more it’s an inverted funnel&lt;/head&gt;
    &lt;p&gt;Perhaps the most obvious point but also easy to forget especially if your career has been on autopilot so far.&lt;/p&gt;
    &lt;p&gt;There’s a bottleneck up top: fewer seats, more ambiguity, less structure and high subjectivity. It’s not just hard to get in but even harder to stay clear on what “good” even looks like.&lt;/p&gt;
    &lt;p&gt;This means you can do everything right and still get passed over. That’s not a verdict on your worth or ability, just geometry.&lt;/p&gt;
    &lt;p&gt;It also means that staying the course when things don’t go your way isn’t just a virtue but a practice. To play the long game, you have to keep showing up even after crushing disappointment without getting cynical of the process. Put differently, you need high levels of frustration tolerance.&lt;/p&gt;
    &lt;p&gt;Cliched? Yes, very much so, but also uncommon. It means if you can pull it off, it’s a source of power.&lt;/p&gt;
    &lt;head rend="h2"&gt;Know which game you’re choosing to play&lt;/head&gt;
    &lt;p&gt;There is no one game being played. There are multiple, overlapping games with different scoring systems. Some are playing to build long-term credibility; others for short-term visibility.&lt;/p&gt;
    &lt;p&gt;You can’t play them all and neither should you try.&lt;/p&gt;
    &lt;p&gt;The real problem is we slide into playing someone else's game without realizing it. We adopt the norms and metrics of others without checking if that’s the game we actually want to play, let alone win. So we end up optimizing for a role we don’t respect, or chasing promotions that hollow us out.&lt;/p&gt;
    &lt;p&gt;Whatever you’re doing, own it outright. Not just the upside but also the downside. If you're focused on building something lasting like developing others, or robust systems, you need to accept that visible status markers (titles, promotions, recognition) might not happen right away.&lt;/p&gt;
    &lt;p&gt;The danger isn't which path you pick, whether it's chasing promotions or maintaining your autonomy. The real disaster is to sleepwalk down a path while pretending you had no choice in the matter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Watching your circle of control&lt;/head&gt;
    &lt;p&gt;An easy way to burn out is to focus relentlessly on things you care about but cannot actually influence. Over time, especially in large organizations, it's tempting to attribute everything to forces outside yourself. This induces organizational helplessness. A sense that nothing you do matters unless someone above says so.&lt;/p&gt;
    &lt;p&gt;Fight that, not with bluster, but with deliberate ownership of the space you control and influence. While experienced professionals often have more influence than they think, it's distributed differently than they expect.&lt;/p&gt;
    &lt;p&gt;The key is maintaining an internal locus of control which includes your positioning, relationships, and what you are building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Keep a balanced portfolio&lt;/head&gt;
    &lt;p&gt;This is a well-understood concept in investing but often missing in the context of long careers. If all your identity is wrapped up in organizational validation, you're fragile. This means setbacks don't just rattle your job, it rattles your sense of self.&lt;/p&gt;
    &lt;p&gt;Many mid-career professionals learn this the hard way. To state the obvious: you are not your title, or your most recent performance review.&lt;/p&gt;
    &lt;p&gt;The anti-dote is diversification not of money, but meaning.&lt;/p&gt;
    &lt;p&gt;Rebalancing here means investing in other sources of connection and community. This includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Developing a craft that exists beyond a given employer.&lt;/item&gt;
      &lt;item&gt;Investing in communities that outlast org charts.&lt;/item&gt;
      &lt;item&gt;Projects, relationships, and sources of learning that replenish.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You are building adaptive capacity.&lt;/p&gt;
    &lt;p&gt;A balanced portfolio also helps to play the long game with more psychological courage because your whole life isn't riding on the next promotion cycle or external validation.&lt;/p&gt;
    &lt;head rend="h2"&gt;In closing&lt;/head&gt;
    &lt;p&gt;By recognizing the subjective currents that shape work environments, we can operate within them more skillfully.&lt;/p&gt;
    &lt;p&gt;This isn't cynicism or gaming the system. Rather, it's developing a nuanced understanding of how organizations actually function. This stance equips us to make more intentional choices about how to engage, contribute, and create meaning.&lt;/p&gt;
    &lt;p&gt;As ideal as it sounds, the goal isn't to eliminate organizational absurdities, but to work effectively within and around them. By staying in the game, you find ways to gradually improve the system from within.&lt;/p&gt;
    &lt;p&gt;Organizations are ultimately human constructs. Imperfect, but not immutable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450525</guid><pubDate>Thu, 02 Oct 2025 14:58:04 +0000</pubDate></item><item><title>Pharma is a small component of US health care spending</title><link>https://www.economist.com/business/2025/10/02/does-big-pharma-gouge-americans</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450694</guid><pubDate>Thu, 02 Oct 2025 15:07:48 +0000</pubDate></item><item><title>Signal Protocol and Post-Quantum Ratchets</title><link>https://signal.org/blog/spqr/</link><description>&lt;doc fingerprint="ac708592c921c484"&gt;
  &lt;main&gt;
    &lt;p&gt;We are excited to announce a significant advancement in the security of the Signal Protocol: the introduction of the Sparse Post Quantum Ratchet (SPQR). This new ratchet enhances the Signal Protocol’s resilience against future quantum computing threats while maintaining our existing security guarantees of forward secrecy and post-compromise security.&lt;/p&gt;
    &lt;p&gt;The Signal Protocol is a set of cryptographic specifications that provides end-to-end encryption for private communications exchanged daily by billions of people around the world. After its publication in 2013, the open source Signal Protocol was adopted not only by the Signal application but also by other major messaging products. Technical information on the Signal Protocol can be found in the specifications section of our docs site.&lt;/p&gt;
    &lt;p&gt;In a previous blog post, we announced the first step towards advancing quantum resistance for the Signal Protocol: an upgrade called PQXDH that incorporates quantum-resistent cryptographic secrets when chat sessions are established in order to protect against harvest-now-decrypt-later attacks that could allow current chat sessions to become compromised if a sufficiently powerful quantum computer is developed in the future. However, the Signal Protocol isn’t just about protecting cryptographic material and keys at the beginning of a new chat or phone call; it’s also designed to minimize damage and heal from compromise as that conversation continues.&lt;/p&gt;
    &lt;p&gt;We refer to these security goals as Forward Secrecy (FS) and Post-Compromise Security (PCS). FS and PCS can be considered mirrors of each other: FS protects past messages against future compromise, while PCS protects future messages from past compromise. Today, we are happy to announce the next step in advancing quantum resistance for the Signal Protocol: an additional regularly advancing post-quantum ratchet called the Sparse Post Quantum Ratchet, or SPQR. On its own, SPQR provides secure messaging that provably achieves these FS and PCS guarantees in a quantum safe manner. We mix the output of this new ratcheting protocol with Signal’s existing Double Ratchet, in a combination we refer to as the Triple Ratchet.&lt;/p&gt;
    &lt;p&gt;What does this mean for you as a Signal user? First, when it comes to your experience using the app, nothing changes. Second, because of how we’re rolling this out and mixing it in with our existing encryption, eventually all of your conversations will move to this new protocol without you needing to take any action. Third, and most importantly, this protects your communications both now and in the event that cryptographically relevant quantum computers eventually become a reality, and it allows us to maintain our existing security guarantees of forward secrecy and post-compromise security as we proactively prepare for that new world.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Current State of the Signal Protocol&lt;/head&gt;
    &lt;p&gt;The original Signal ratchet uses hash functions for FS and a set of elliptic-curve Diffie Hellman (ECDH) secret exchanges for PCS. The hash functions are quantum safe, but elliptic-curve cryptography is not. An example is in order: our favorite users, Alice and Bob, establish a long-term connection and chat over it regularly. During that session’s lifetime, Alice and Bob regularly agree on new ECDH secrets and use them to “ratchet” their session. Mean ol’ Mallory records the entire (encrypted) communication, and really wants to know what Alice and Bob are talking about.&lt;/p&gt;
    &lt;p&gt;The concept of a “ratchet” is crucial to our current non-quantum FS/PCS protection. In the physical world, a ratchet is a mechanism that allows a gear to rotate forward, but disallows rotation backwards. In the Signal Protocol, it takes on a similar role. When Alice and Bob “ratchet” their session, they replace the set of keys they were using prior with a new set based on both the older secrets and a new one they agree upon. Given access to those new secrets, though, there’s no (non-quantum) way to compute the older secrets. By being “one-way”, this ratcheting mechanism provides FS.&lt;/p&gt;
    &lt;p&gt;The ECDH mechanism allows Alice and Bob to generate new, small (32 bytes) data blobs and attach them to every message. Whenever each party receives a message from the other, they can locally (and relatively cheaply) use this data blob to agree on a new shared secret, then use that secret to ratchet their side of the protocol. Crucially, ECDH also allows Alice and Bob to both agree on the new secret without sending that secret itself over their session, and in fact without sending anything over the session that Mallory could use to determine it. This description of Diffie-Hellman key exchange provides more details on the concepts of such a key exchange, and this description of ECDH provides specific details on the variant used by the current Signal protocol.&lt;/p&gt;
    &lt;p&gt;Sometime midway through the lifetime of Alice and Bob’s session, Mallory successfully breaches the defences of both Alice and Bob, gaining access to all of the (current) secrets used for their session at the time of request. Alice and Bob should have the benefits of Forward Secrecy - they’ve ratcheted sometime recently before the compromise, so no messages earlier than their last ratchet are accessible to Mallory, since ratcheting isn’t reversible. They also retain the benefits of Post-Compromise Security. Their ratcheting after Mallory’s secret access agrees upon new keys that can’t be gleaned just from the captured data they pass between each other, re-securing the session.&lt;/p&gt;
    &lt;p&gt;Should Mallory have access to a quantum computer, though, things aren’t so simple. Because elliptic curve cryptography is not quantum resistant, it’s possible that Mallory could glean access to the secret that Alice and Bob agreed upon, just by looking at the communication between them. Given this, Alice and Bob’s session will never “heal”; Mallory’s access to their network traffic from this point forward will allow her to decrypt all future communications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mixing In Quantum Security&lt;/head&gt;
    &lt;p&gt;In order to make our security guarantees stand up to quantum attacks, we need to mix in secrets generated from quantum secure algorithms. In PQXDH, we did this by performing an additional round of key agreement during the session-initiating handshake, then mixing the resulting shared secret into the initial secret material used to create Signal sessions. To handle FS and PCS, we need to do continuous key agreement, where over the lifetime of a session we keep generating new shared secrets and mixing those keys into our encryption keys.&lt;/p&gt;
    &lt;p&gt;Luckily there is a tool designed exactly for this purpose: the quantum-secure Key-Encapsulation Mechanism (KEM). KEMs share similar behavior to the Diffie-Hellman mechanisms we described earlier, where two clients provide each other with information, eventually deciding on a shared secret, without anyone who intercepts their communications being able to access that secret. However, there is one important distinction for KEMs - they require ordered, asymmetric messages to be passed between their clients. In ECDH, both clients send the other some public parameters, and both combine these parameters with their locally held secrets and come up with an identical shared secret. In the standardized ML-KEM key-encapsulation mechanism, though, the initiating client generates a pair of encapsulation key (EK) and decapsulation key (DK) (analogous to a public and private key respectively) and sends the EK. The receiving client receives it, generates a secret, and wraps it into a ciphertext (CT) with that key. The initiating client receives that CT and decapsulates with its previously generated DK. In the end, both clients have access to the new, shared secret, just through slightly different means.&lt;/p&gt;
    &lt;p&gt;Wanting to integrate this quantum-secure key sharing into Signal, we could take a simple, naive approach for each session. When Alice initiates a session with Bob,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alice, with every message she sends, sends an EK&lt;/item&gt;
      &lt;item&gt;Bob, with every message he receives, generates a secret and a CT, and sends the CT back&lt;/item&gt;
      &lt;item&gt;Alice, on receiving a CT, extracts the secret with her DK and mixes it in&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This initially simple-looking approach, though, quickly runs into a number of issues we’ll need to address to make our protocol actually robust. First, encapsulation keys and CTs are large - over 1000 bytes each for ML-KEM 768, compared to the 32 bytes required for ECDH. Second, while this protocol works well when both clients are online, what happens when a client is offline? Or a message is dropped or reordered? Or Alice wants to send 10 messages before Bob wants to send one?&lt;/p&gt;
    &lt;p&gt;Some of these problems have well-understood solutions, but others have trade-offs that may shine in certain circumstances but fall short in others. Let’s dive in and come to some conclusions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who Wants What When&lt;/head&gt;
    &lt;p&gt;How does Alice decide what to send based on what Bob needs next, and vice versa? If Bob hasn’t received an EK yet, she shouldn’t send the next one. What does Bob send when he hasn’t yet received an EK from Alice, or when he has, but he’s already responded to it? This is a common problem when remote parties send messages to communicate, so there’s a good, well-understood solution: a state machine. Alice and Bob both keep track of “what state am I in”, and base their decisions on that. When sending or receiving a message, they might also change their state. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alice wants to send a message, but she’s in a StartingA state, so she doesn’t have an EK. So, she generates an EK/DK pair, stores them locally, and transitions to the SendEK state&lt;/item&gt;
      &lt;item&gt;Alice wants to send a message and is in the SendEK state. She sends the EK along with the message&lt;/item&gt;
      &lt;item&gt;Alice wants to send another message, but she’s still in the SendEK state. So, she sends the EK with the new message as well&lt;/item&gt;
      &lt;item&gt;Bob receives the message with the EK. He generates a secret and uses the EK to create a CT. He transitions to the SendingCT state.&lt;/item&gt;
      &lt;item&gt;Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the message&lt;/item&gt;
      &lt;item&gt;Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the message&lt;/item&gt;
      &lt;item&gt;… etc …&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By crafting a set of states and transitions, both sides can coordinate what’s sent. Note, though, that even in this simple case, we see problems. For example, we’re sending our (large) EK and (large) CT multiple times.&lt;/p&gt;
    &lt;head rend="h2"&gt;Say (or Send) Less&lt;/head&gt;
    &lt;p&gt;We’ve already mentioned that the size of the data we’re sending has increased pretty drastically, from 32 bytes to over 1000 per message. But bandwidth is expensive, especially on consumer devices like client phones, that may be anywhere in the world and have extremely varied costs for sending bytes over the wire. So let’s discuss strategies for conserving that bandwidth.&lt;/p&gt;
    &lt;p&gt;First, the simplest approach - don’t send a new key with every message. Just, for example, send with every 50 messages, or once a week, or every 50 messages unless you haven’t sent a key in a week, or any other combination of options. All of these approaches tend to work pretty well in online cases, where both sides of a session are communicating in real-time with no message loss. But in cases where one side is offline or loss can occur, they can be problematic. Consider the case of “send a key if you haven’t sent one in a week”. If Bob has been offline for 2 weeks, what does Alice do when she wants to send a message? What happens if we can lose messages, and we lose the one in fifty that contains a new key? Or, what happens if there’s an attacker in the middle that wants to stop us from generating new secrets, and can look for messages that are 1000 bytes larger than the others and drop them, only allowing keyless messages through?&lt;/p&gt;
    &lt;p&gt;Another method is to chunk up a message. Want to send 1000 bytes? Send 10 chunks of 100 bytes each. Already sent 10 chunks? Resend the first chunk, then the second, etc. This smooths out the total number of bytes sent, keeping individual message sizes small and uniform. And often, loss of messages is handled. If chunk 1 was lost, just wait for it to be resent. But it runs into an issue with message loss - if chunk 99 was lost, the receiver has to wait for all of chunks 1-98 to be resent before it receives the chunk it missed. More importantly, if a malicious middleman wants to stop keys from being decided upon, they could always drop chunk 3, never allowing the full key to pass between the two parties.&lt;/p&gt;
    &lt;p&gt;We can get around all of these issues using a concept called erasure codes. Erasure codes work by breaking up a larger message into smaller chunks, then sending those along. Let’s consider our 1000 byte message being sent as 100 byte chunks again. After chunk #10 has been sent, the entirety of the original 1000 byte message has been sent along in cleartext. But rather than just send the first chunk over again, erasure codes build up a new chunk #11, and #12, etc. And they build them in such a way that, once the recipient receives any 10 chunks in any order, they’ll be able to reconstruct the original 1000 byte message.&lt;/p&gt;
    &lt;p&gt;When we put this concept of erasure code chunks together with our previous state machine, it gives us a way to send large blocks of data in small chunks, while handling messages that are dropped. Crucially, this includes messages dropped by a malicious middleman: since any N chunks can be used to recreate the original message, a bad actor would need to drop all messages after #N-1 to disallow the data to go through, forcing them into a complete (and highly noticeable) denial of service. Now, if Alice wants to send an EK to Bob, Alice will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transition from the StartingA state to the SendingEK state, by generating a new EK and chunking it&lt;/item&gt;
      &lt;item&gt;While in the SendingEK state, send a new chunk of the EK along with any messages she sends&lt;/item&gt;
      &lt;item&gt;When she receives confirmation that the recipient has received the EK (when she receives a chunk of CT), transition to the ReceivingCT state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On Bob’s side, he will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transition from the StartingB state to the ReceivingEK state when he receives its first EK chunk&lt;/item&gt;
      &lt;item&gt;Keep receiving EK chunks until he has enough to reconstruct the EK&lt;/item&gt;
      &lt;item&gt;At that point, reconstruct the EK, generate the CT, chunk the CT, and transition to the SendingCT state&lt;/item&gt;
      &lt;item&gt;From this point on, he will send a chunk of the CT with every message&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One interesting way of looking at this protocol so far is to consider the messages flowing from Alice to Bob as potential capacity for sending data associated with post-quantum ratcheting: each message that we send, we could also send additional data like a chunk of EK or of the CT. If we look at Bob’s side, above, we notice that sometimes he’s using that capacity (IE: in step 4 when he’s sending CT chunks) and sometimes he’s not (if he sends a message to Alice during step 2, he has no additional data to send). This capacity is pretty limited, so using more of it gives us the potential to speed up our protocol and agree on new secrets more frequently.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Meditation On How Faster Isn’t Always Better&lt;/head&gt;
    &lt;p&gt;We want to generate shared secrets, then use them to secure messages. So, does that mean that we want to generate shared secrets as fast as possible? Let’s introduce a new term: an epoch. Alice and Bob start their sessions in epoch 0, sending the EKs for epoch 1 (EK#1) and associated ciphertext (CT#1) to each other. Once that process completes, they have a new shared secret they use to enter epoch 1, after which all newly sent messages are protected by the new secret. Each time they generate a new shared secret, they use it to enter a new epoch. Surely, every time we enter a new epoch with a new shared secret, we protect messages before that secret (FS) and after that secret (PCS), so faster generation is better? It seems simple, but there’s an interesting complexity here that deserves attention.&lt;/p&gt;
    &lt;p&gt;First, let’s discuss how to do things faster. Right now, there’s a lot of capacity we’re not using: Bob sends nothing while Alice sends an EK, and Alice sends nothing while Bob sends a CT. Speeding this up isn’t actually that hard. Let’s change things so that Alice sends EK#1, and once Bob acknowledges its receipt, Alice immediately generates and sends EK#2. And once she notices Bob has received that, she generates and sends EK#3, etc. Whenever Alice sends a new message, she always has data to send along with it (new EK chunks), so she’s using its full capacity. Bob doesn’t always have a new CT to send, but he is receiving EKs as fast as Alice can send them, so he often has a new CT to send along.&lt;/p&gt;
    &lt;p&gt;But now let’s consider what happens when an attacker gains access to Alice. Let’s say that Alice has sent EK#1 and EK#2 to Bob, and she’s in the process of sending EK#3. Bob has acknowledged receipt of EK#1 and EK#2, but he’s still in the process of sending CT#1, since in this case Bob sends fewer messages to Alice than vice versa. Because Alice has already generated 3 EKs she hasn’t used, Alice needs to keep the associated DK#1, DK#2, and DK#3 around. So, if at this point someone maliciously gains control of Alice’s device, they gain access to both the secrets associated with the current epoch (here, epoch 0) and to the DKs necessary to reconstruct the secrets to other epochs (here, epochs 1, 2, and 3) using only the over-the-wire CT that Bob has yet to send. This is a big problem: by generating secrets early, we’ve actually made the in-progress epochs and any messages that will be sent within them less secure against this single-point-in-time breach.&lt;/p&gt;
    &lt;p&gt;To test this out, we at Signal built a number of different state machines, each sending different sets of data either in parallel or serially. We then ran these state machines in numerous simulations, varying things like the ratio of messages sent by Alice vs Bob, the amount of data loss or reordering, etc. And while running these simulations, we tracked what epochs’ secrets were exposed at any point in time, assuming an attacker were to breach either Alice’s or Bob’s secret store. The results showed that, in general, while simulations that handled multiple epochs’ secrets in parallel (IE: by sending EK#2 before receipt of CT#1) did generate new epochs more quickly, they actually made more messages vulnerable to a single breach.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Let’s Still Be Efficient&lt;/head&gt;
    &lt;p&gt;This still leaves us with a problem, though: the capacity present in messages we send in either direction is still a precious resource, and we want to use it as efficiently as possible. And our simple approach of Alice’s “send EK, receive CT, repeat” and Bob’s “receive EK, send CT, repeat” leaves lots of time where Alice and Bob have nothing to send, should that capacity be available.&lt;/p&gt;
    &lt;p&gt;To improve our use of our sending capacity, we decided to take a harder look into the ML-KEM algorithm we’re using to share secrets, to see if there was room to improve. Let’s break things down more and share some actual specifics on the ML-KEM algorithm.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates an EK of 1184 bytes to send to Bob, and an associated DK&lt;/item&gt;
      &lt;item&gt;Bob receives the EK&lt;/item&gt;
      &lt;item&gt;Bob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice&lt;/item&gt;
      &lt;item&gt;Alice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secret&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Diving in further, we can break out step #3 into some sub-steps&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates an EK of 1184 bytes to send to Bob, and an associated DK&lt;/item&gt;
      &lt;item&gt;Bob receives the EK&lt;/item&gt;
      &lt;item&gt;Bob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice1&lt;list rend="ol"&gt;&lt;item&gt;Bob creates a new shared secret S and sampled randomness R by sampling entropy and combining it with a hash of EK&lt;/item&gt;&lt;item&gt;Bob hashes the EK into a Hash&lt;/item&gt;&lt;item&gt;Bob pulls 32 bytes of the EK, a Seed&lt;/item&gt;&lt;item&gt;Bob uses the Seed and R to generate the majority of the CT&lt;/item&gt;&lt;item&gt;Bob then uses S and EK to generate the last portion of the CT&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Alice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secret&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Step 3.d, which generates 960 bytes of the 1088-byte CT, only needs 64 bytes of input: a Seed that’s 32 of EK’s bytes, and the hash of EK, which is an additional 32. If we combine these values and send them first, then most of EK and most of the CT can be sent in parallel from Alice to Bob and Bob to Alice respectively. Our more complicated but more efficient secret sharing now looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates EK and DK. Alice extracts the 32-byte Seed from EK&lt;/item&gt;
      &lt;item&gt;Alice sends 64 bytes EK1 (Seed + Hash(EK)) to Bob. Bob sends nothing during this time.&lt;/item&gt;
      &lt;item&gt;Bob receives the Seed and Hash, and generates the first, largest part of the CT from them (CT1)&lt;/item&gt;
      &lt;item&gt;After this point, Alice sends EK2 (the rest of the EK minus the Seed), while Bob sends CT1&lt;/item&gt;
      &lt;item&gt;Bob eventually receives EK2, and uses it to generate the final portion of the CT (CT2)&lt;/item&gt;
      &lt;item&gt;Once Alice tells Bob that she has received all of CT1, Bob sends Alice CT2. Alice sends nothing during this time.&lt;/item&gt;
      &lt;item&gt;With both sides having all of the pieces of EK and the CT that they need, they extract their shared secret and increment their epoch&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are still places in this algorithm (specifically steps 2 and 6) where one side has nothing to send. But during those times, the other side has only a very small amount of information to send, so the duration of those steps is minimal compared to the rest of the process. Specifically, while the full EK is 37 chunks and the full CT is 34, the two pieces of the new protocol which must be sent without data being received (EK1 and CT2) are 2 and 4 chunks respectively, while the pieces that can be sent while also receiving (EK2 and CT1) are the bulk of the data, at 36 and 30 chunks respectively. Far more of our sending capacity is actually used with this approach.&lt;/p&gt;
    &lt;p&gt;Remember that all of this is just to perform a quantum-safe key exchange that gives us a secret we can mix into the bigger protocol. To help us organize our code, our security proofs, and our understanding better we treat this process as a standalone protocol that we call the ML-KEM Braid.&lt;/p&gt;
    &lt;p&gt;This work was greatly aided by the authors of the libcrux-ml-kem Rust library, who graciously exposed the APIs necessary to work with this incremental version of ML-KEM 768. With this approach completed, we’ve been able to really efficiently use the sending capacity of messages sent between two parties to share secrets as quickly as possible without exposing secrets from multiple epochs to potential attackers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mixing Things Up - The Triple Ratchet&lt;/head&gt;
    &lt;p&gt;There are plenty of details to add to make sure that we reached every corner - check those out in our online protocol documentation - but this basic idea lets us build secure messaging that has post-quantum FS and PCS without using up anyone’s data. We’re not done, though! Remember, at the beginning of this post we said we wanted post-quantum security without taking away our existing guarantees.&lt;/p&gt;
    &lt;p&gt;While today’s Double Ratchet may not be quantum safe, it provides a high level of security today and we believe it will continue to be strong well into the future. We aren’t going to take that away from our users. So what can we do?&lt;/p&gt;
    &lt;p&gt;Our answer ends up being really simple: we run both the Double Ratchet and the Sparse Post Quantum Ratchet alongside each other and mix their keys together, into what we’re calling the Triple Ratchet protocol. When you want to send a message you ask both the Double Ratchet and SPQR “What encryption key should I use for the next message?” and they will both give you a key (along with some other data you need to put in a message header). Instead of either key being used directly, both are passed into a Key Derivation Function - a special function that takes random-enough inputs and produces a secure cryptographic key that’s as long as you need. This gives you a new “mixed” key that has hybrid security. An attacker has to break both our elliptic curve and ML-KEM to even be able to distinguish this key from random bits. We use that mixed key to encrypt our message.&lt;/p&gt;
    &lt;p&gt;Receiving messages is just as easy. We take the message header - remember it has some extra data in it - and send it to the Double Ratchet and SPQR and ask them “What key should I use to decrypt a message with this header?” They both return their keys and you feed them both into that Key Derivation Function to get your decryption key. After that, everything proceeds just like it always has.&lt;/p&gt;
    &lt;head rend="h2"&gt;Heterogeneous Rollout&lt;/head&gt;
    &lt;p&gt;So we’ve got this new, snazzy protocol, and we want to roll it out to all of our users across all of their devices… but none of the devices currently support that protocol. We roll it out to Alice, and Alice tries to talk to Bob, but Alice speaks SPQR and Bob doesn’t. Or we roll it out to Bob, but Alice wants to talk to Bob and Alice doesn’t know the new protocol Bob wants to use. How do we make this work?&lt;/p&gt;
    &lt;p&gt;Let’s talk about the simplest option: allowing downgrades. Alice tries to establish a session with Bob using SPQR and sends a message over it. Bob fails to read the message and establish the session, because Bob hasn’t been upgraded yet. Bob sends Alice an error, so Alice has to try again. This sounds fine, but in practice it’s not tenable. Consider what happens if Alice and Bob aren’t online at the same time… Alice sends a message at 1am, then shuts down. Bob starts up at 3am, sends an error, then shuts down. Alice gets that error when she restarts at 5am, then resends. Bob starts up at 7am and finally gets the message he should have received at 3am, 4 hours behind schedule.&lt;/p&gt;
    &lt;p&gt;To handle this, we designed the SPQR protocol to allow itself to downgrade to not being used. When Alice sends her first message, she attaches the SPQR data she would need to start up negotiation of the protocol. Noticing that downgrades are allowed for this session, Alice doesn’t mix any SPQR key material into the message yet. Bob ignores that data, because it’s in a location he glosses over, but since there’s no mixed in keys yet, he can still decrypt the message. He sends a response that lacks SPQR data (since he doesn’t yet know how to fill it in), which Alice receives. Alice sees a message without SPQR data, and understands that Bob doesn’t speak SPQR yet. So, she downgrades to not using it for that session, and they happily talk without SPQR protection.&lt;/p&gt;
    &lt;p&gt;There’s some scary potential problems here… let’s work through them. First off, can a malicious middleman force a downgrade and disallow Alice and Bob from using SPQR, even if both of them are able to? We protect against that by having the SPQR data attached to the message be MAC’d by the message-wide authentication code - a middleman can’t remove it without altering the whole message in such a way that the other party sees it, even if that other party doesn’t speak SPQR. Second, could some error cause messages to accidentally downgrade sometime later in their lifecycle, due either to bugs in the code or malicious activity? Crucially, SPQR only allows a downgrade when it first receives a message from a remote party. So, Bob can only downgrade if he receives his first message from Alice and notices that she doesn’t support SPQR, and Alice will only downgrade if she receives her first reply from Bob and notices that he doesn’t. After that first back-and-forth, SPQR is locked in and used for the remainder of the session.&lt;/p&gt;
    &lt;p&gt;Finally, those familiar with Signal’s internal workings might note that Signal sessions last a really long time, potentially years. Can we ever say “every session is protected by SPQR”, given that SPQR is only added to new sessions as they’re being initiated? To accomplish this, Signal will eventually (once all clients support the new protocol) roll out a code change that enforces SPQR for all sessions, and that archives all sessions which don’t yet have that protection. After the full rollout of that future update, we’ll be able to confidently assert complete coverage of SPQR.&lt;/p&gt;
    &lt;p&gt;One nice benefit to setting up this “maybe downgrade if the other side doesn’t support things” approach is that it also sets us up for the future: the same mechanisms that allow us to choose between SPQR or no-SPQR are designed to also allow us to upgrade from SPQR to some far-future (as yet unimagined) SPQRv2.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Sure We Get It Right&lt;/head&gt;
    &lt;p&gt;Complex protocols require extraordinary care. We have to ensure that the new protocol doesn’t lose any of the security guarantees the Double Ratchet gives us. We have to ensure that we actually get the post-quantum protection we’re aiming for. And even then, after we have full confidence in the protocol, we have to make sure that our implementation is correct and robust and stays that way as we maintain it. This is a tall order.&lt;/p&gt;
    &lt;p&gt;To make sure we got this right, we started by building the protocol on a firm foundation of fundamental research. We built on the years of research the academic community has put into secure messaging and we collaborated with researchers from PQShield, AIST, and NYU to explore what was possible with post-quantum secure messaging. In a paper at Eurocrypt 25 we introduced erasure code based chunking and proposed the high-level Triple Ratchet protocol, proving that it gave us the post-quantum security we wanted without taking away any of the security of the classic Double Ratchet. In a follow up paper at USENIX 25, we observed that there are many different ways to design a post-quantum ratchet protocol and we need to pick the one that protects user messages the best. We introduced and analyzed six different protocols and two stood out: one is essentially SPQR, the other is a protocol using a new KEM, called Katana, that we designed just for ratcheting. That second one is exciting, but we want to stick to standards to start!&lt;/p&gt;
    &lt;head rend="h2"&gt;Formal Verification From the Start&lt;/head&gt;
    &lt;p&gt;This research gave us the framework to think about protocol design and prove protocols are secure, but there is a big leap from an academic paper to code. Already when designing PQXDH - a much simpler protocol! - we found that formal verification was an important tool for getting the details right. With the Triple Ratchet we partnered with Cryspen and made formal verification part of the process from the beginning.&lt;/p&gt;
    &lt;p&gt;As we kept finding better protocol candidates - and we implemented around a dozen of them - we modeled them in ProVerif to prove that they had the security properties we needed. Rather than wrapping up a protocol design and performing formal verification as a last step we made it a core part of the design process. Now that the design is settled, this gives us machine verified proof that our protocol has the security properties we demand from it. We wrote our Rust code to closely match the ProVerif models, so it is easy to check that we’re modeling what we implement. In particular, ProVerif is very good at reasoning about state machines, which we’re already using, making the mapping from code to model much simpler.&lt;/p&gt;
    &lt;p&gt;We are taking formal verification further than that, though. We are using hax to translate our Rust implementation into F* on every CI run. Once the F* models are extracted, we prove that core parts of our highly optimized implementation are correct, that function pre-conditions and post-conditions cannot be violated, and that the entire crate is panic free. That last one is a big deal. It is great for usability, of course, because nobody wants their app to crash. But it also matters for correctness. We aggressively add assertions about things we believe must be true when the protocol is running correctly - and we crash the app if they are false. With hax and F*, we prove that those assertions will never fail.&lt;/p&gt;
    &lt;head rend="h2"&gt;Formal Verification Doesn’t Freeze Our Progress&lt;/head&gt;
    &lt;p&gt;Often when people think about formally verified protocol implementations, they imagine a one-time huge investment in verification that leaves you with a codebase frozen in time. This is not the case here. We re-run formal verification in our CI pipeline every time a developer pushes a change to GitHub. If the proofs fail then the build fails, and the developer needs to fix it. In our experience so far, this is usually as simple as adding a pre- or postcondition or returning an error when a value is out of bounds. For us, formal verification is a dynamic part of the development process and ensures that the quality is high on every merge.&lt;/p&gt;
    &lt;head rend="h2"&gt;TLDR&lt;/head&gt;
    &lt;p&gt;Signal is rolling out a new version of the Signal Protocol with the Triple Ratchet. It adds the Sparse Post-Quantum Ratchet, or SPQR, to the existing Double Ratchet to create a new Triple Ratchet which gives our users quantum-safe messaging without taking away any of our existing security promises. It’s being added in such a way that it can be rolled out without disruption. It’s relatively lightweight, not using much additional bandwidth for each message, to keep network costs low for our users. It’s resistant to meddling by malicious middlemen - to disrupt it, all messages after a certain time must be dropped, causing a noticeable denial of service for users. We’re rolling it out slowly and carefully now, but in such a way that we’ll eventually be able to say with confidence “every message sent by Signal is protected by this.” Its code has been formally verified, and will continue to be so even as future updates affect the protocol. It’s the combined effort of Signal employees and external researchers and contributors, and it’s only possible due to the continued work and diligence of the larger crypto community. And as a user of Signal, our biggest hope is that you never even notice or care. Except one day, when headlines scream “OMG, quantum computers are here”, you can look back on this blog post and say “oh, I guess I don’t have to care about that, because it’s already been handled”, as you sip your Nutri-Algae while your self-driving flying car wends its way through the floating tenements of Megapolis Prime.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Those that are interested can look at https://nvlpubs.nist.gov/nistpubs/fips/nist.fips.203.pdf and note that Algorithm 17 uses randomness plus the hash of EK to generate a shared secret and random value, then that random value is used in Algorithm 14 to create c1. The rest of ekPKE is only used by Algorithm 14 to generate c2. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451527</guid><pubDate>Thu, 02 Oct 2025 16:06:10 +0000</pubDate></item><item><title>Launch HN: Simplex (YC S24) – Browser automation platform for developers</title><link>https://www.simplex.sh/</link><description>&lt;doc fingerprint="f4968ef0cf13274b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Browser automation&lt;lb/&gt;for developers&lt;/head&gt;&lt;p&gt;Simplex provides all the infrastructure needed for modern browser automation. &lt;lb/&gt; Remote browsers, steerable web agents, and more.&lt;/p&gt;&lt;head rend="h3"&gt;A team from world class institutions&lt;/head&gt;&lt;head rend="h2"&gt;Watch a live demo of Simplex automating&lt;lb/&gt; a real billing portal.&lt;/head&gt;&lt;head rend="h3"&gt;Live Session Stream&lt;/head&gt;&lt;p&gt;Demo Preview&lt;/p&gt;&lt;head rend="h3"&gt;Live Demo Logs&lt;/head&gt;&lt;p&gt;Agent logs will appear here when demo is running&lt;/p&gt;&lt;head rend="h2"&gt;Engineered from the ground up to work with legacy systems.&lt;/head&gt;&lt;p&gt;Reliably automate every legacy portal your customers use.&lt;/p&gt;&lt;head rend="h3"&gt;Billing Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to log into a billing portal and download the list of invoices for a specified customer.&lt;/p&gt;&lt;head rend="h3"&gt;Prior Authorization Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to fill out complex, branching-logic prior authorization forms on medical provider portals.&lt;/p&gt;&lt;head rend="h3"&gt;ERPs&lt;/head&gt;&lt;p&gt;Simplex has been used to automate data entry and download report PDFs across different ERPs.&lt;/p&gt;&lt;head rend="h3"&gt;Government Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to search and extract structured information across public government portals.&lt;/p&gt;&lt;head rend="h3"&gt;TMS/WMS Software&lt;/head&gt;&lt;p&gt;Simplex has been used to log into a TMS portal, create and edit the information for a shipment, then dispatch the shipment.&lt;/p&gt;&lt;head rend="h3"&gt;... and more&lt;/head&gt;&lt;p&gt;with us to discuss your specific use case.&lt;/p&gt;&lt;table&gt;&lt;row span="6"&gt;&lt;cell role="head"&gt;Order ID&lt;/cell&gt;&lt;cell role="head"&gt;Customer&lt;/cell&gt;&lt;cell role="head"&gt;Status&lt;/cell&gt;&lt;cell role="head"&gt;Amount&lt;/cell&gt;&lt;cell role="head"&gt;Last Updated&lt;/cell&gt;&lt;cell role="head"&gt;Actions&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-001&lt;/cell&gt;&lt;cell&gt;John Smith&lt;/cell&gt;&lt;cell&gt;PENDING&lt;/cell&gt;&lt;cell&gt;$1,234.56&lt;/cell&gt;&lt;cell&gt;01/15/2024&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-002&lt;/cell&gt;&lt;cell&gt;Jane Doe&lt;/cell&gt;&lt;cell&gt;PROCESSING&lt;/cell&gt;&lt;cell&gt;$987.65&lt;/cell&gt;&lt;cell&gt;01/14/2024&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-003&lt;/cell&gt;&lt;cell&gt;Bob Johnson&lt;/cell&gt;&lt;cell&gt;COMPLETED&lt;/cell&gt;&lt;cell&gt;$2,345.67&lt;/cell&gt;&lt;cell&gt;01/13/2024&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;PO-2024-004&lt;/cell&gt;&lt;cell&gt;Alice Brown&lt;/cell&gt;&lt;cell&gt;ERROR&lt;/cell&gt;&lt;cell&gt;$876.54&lt;/cell&gt;&lt;cell&gt;01/12/2024&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Deploy reliably, scale easily.&lt;/head&gt;&lt;head rend="h3"&gt;Run consistent workflows&lt;/head&gt;&lt;p&gt;Simplex automatically caches agent actions. This increases reliability of runs and makes developing flows lightning fast.&lt;/p&gt;&lt;head rend="h3"&gt;Create realtime flows&lt;/head&gt;We've achieved realtime latency to handle complex workflows during phone calls.&lt;p&gt;if low latency flows are a priority for you.&lt;/p&gt;&lt;quote&gt;simplex.click(“New Order”)simplex.click(“Shipment Address”)simplex.type(“”)&lt;/quote&gt;&lt;head rend="h2"&gt;Simplex just works.&lt;/head&gt;&lt;head rend="h3"&gt;Production-ready&lt;/head&gt;&lt;p&gt;Eval harnesses to stress-test your workflows at scale and emulate production conditions.&lt;/p&gt;&lt;head rend="h3"&gt;Authentication Handling&lt;/head&gt;&lt;p&gt;Authentication SDK functions to handle 2FA, login data, and more on your customers' sites. See more here.&lt;/p&gt;&lt;head rend="h3"&gt;Scalable Headless Browsers&lt;/head&gt;&lt;p&gt;Headless browsers that can scale to 100s of concurrent sessions in seconds.&lt;/p&gt;&lt;head rend="h3"&gt;Stealth Mode&lt;/head&gt;&lt;p&gt;Automatic CAPTCHA solving, proxies, and anti-bot protections.&lt;/p&gt;&lt;head rend="h3"&gt;Controllable Workflows&lt;/head&gt;&lt;p&gt;Our web agents are constrained to only take the actions you tell it to.&lt;/p&gt;&lt;head rend="h3"&gt;Optimized Workflows&lt;/head&gt;&lt;p&gt;Automatically cache your workflows for fast and reliable execution in production.&lt;/p&gt;&lt;head rend="h3"&gt;Robust SDKs&lt;/head&gt;&lt;p&gt;Our SDKs are designed to be robust and easy to use. Available in both Python and TypeScript.&lt;/p&gt;&lt;head rend="h3"&gt;Detailed Logging and Replays&lt;/head&gt;&lt;p&gt;View a livestream and live logs of sessions as they happen. Share session replays and detailed agent logs with your team and customers.&lt;/p&gt;&lt;head rend="h2"&gt;Ready to get started?&lt;/head&gt;&lt;p&gt;Book a call with our team to discuss your use case and get started.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451547</guid><pubDate>Thu, 02 Oct 2025 16:07:23 +0000</pubDate></item><item><title>Watch MLB games from the comfort of your own terminal</title><link>https://github.com/paaatrick/playball</link><description>&lt;doc fingerprint="607031164bc1d92f"&gt;
  &lt;main&gt;
    &lt;p&gt;Watch MLB games from the comfort of your own terminal&lt;/p&gt;
    &lt;p&gt;MLB Gameday and MLB.tv are great, but sometimes you want to keep an eye on a game a bit more discreetly. &lt;code&gt;playball&lt;/code&gt; puts the game in a terminal window.&lt;/p&gt;
    &lt;p&gt;Just want to try it out?&lt;/p&gt;
    &lt;code&gt;$ npx playball
&lt;/code&gt;
    &lt;p&gt;Ready for the big leagues? Install the package globally&lt;/p&gt;
    &lt;code&gt;$ npm install -g playball
&lt;/code&gt;
    &lt;p&gt;Then run it&lt;/p&gt;
    &lt;code&gt;$ playball
&lt;/code&gt;
    &lt;code&gt;$ docker build -t playball .
$ docker run -it --rm --name playball playball:latest
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;q&lt;/cell&gt;
        &lt;cell&gt;quit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;c&lt;/cell&gt;
        &lt;cell&gt;go to schedule view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;s&lt;/cell&gt;
        &lt;cell&gt;go to standings view&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;↓/j, ↑/k, ←/h, →/l&lt;/cell&gt;
        &lt;cell&gt;change highlighted game&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;enter&lt;/cell&gt;
        &lt;cell&gt;view highlighted game&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;show previous day's schedule/results&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;n&lt;/cell&gt;
        &lt;cell&gt;show next day's schedule&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;t&lt;/cell&gt;
        &lt;cell&gt;return to today's schedule&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;↓/j, ↑/k&lt;/cell&gt;
        &lt;cell&gt;scroll list of all plays&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Playball can be configured using the &lt;code&gt;config&lt;/code&gt; subcommand. To list the current configuration values run the subcommand with no additional arguments:&lt;/p&gt;
    &lt;code&gt;playball config&lt;/code&gt;
    &lt;p&gt;You should see output similar to:&lt;/p&gt;
    &lt;code&gt;color.ball = green
color.favorite-star = yellow
color.in-play-no-out = blue
color.in-play-out = white
color.in-play-runs-bg = white
color.in-play-runs-fg = black
color.on-base = yellow
color.other-event = white
color.out = red
color.strike = red
color.strike-out = red
color.walk = green
favorites = 
&lt;/code&gt;
    &lt;p&gt;To get the value of a single setting pass the key as an additional argument:&lt;/p&gt;
    &lt;code&gt;playball config color.strike&lt;/code&gt;
    &lt;p&gt;To change a setting pass the key and value as arguments:&lt;/p&gt;
    &lt;code&gt;playball config color.strike blue&lt;/code&gt;
    &lt;p&gt;To revert a setting to its default value provide the key and the &lt;code&gt;--unset&lt;/code&gt; flag:&lt;/p&gt;
    &lt;code&gt;playball config color.strike --unset&lt;/code&gt;
    &lt;p&gt;This table summarizes the available settings:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
        &lt;cell role="head"&gt;default&lt;/cell&gt;
        &lt;cell role="head"&gt;allowed values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.ball&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing balls in top row of game view&lt;/cell&gt;
        &lt;cell&gt;green&lt;/cell&gt;
        &lt;cell&gt;One of the following: &lt;code&gt;black&lt;/code&gt;, &lt;code&gt;red&lt;/code&gt;, &lt;code&gt;green&lt;/code&gt;, &lt;code&gt;yellow&lt;/code&gt;, &lt;code&gt;blue&lt;/code&gt;, &lt;code&gt;magenta&lt;/code&gt;, &lt;code&gt;cyan&lt;/code&gt;, &lt;code&gt;white&lt;/code&gt;, &lt;code&gt;grey&lt;/code&gt;. Any of those colors may be prefixed by &lt;code&gt;bright-&lt;/code&gt; or &lt;code&gt;light-&lt;/code&gt; (for example &lt;code&gt;bright-green&lt;/code&gt;). The exact color used will depend on your terminal settings. The value &lt;code&gt;default&lt;/code&gt; may be used to specify the default text color for your terminal. Finally hex colors (e.g &lt;code&gt;#FFA500&lt;/code&gt;) can be specified. If your terminal does not support true color, the closest supported color may be used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.favorite-star&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of star indiciating favorite team in schedule and standing views&lt;/cell&gt;
        &lt;cell&gt;yellow&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-no-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where ball was put in play and no out was made (single, double, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;blue&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where ball was put in play and an out was made (flyout, fielder's choice, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-runs-bg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Background color for score update in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-runs-fg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Foreground color for score update in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;black&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.on-base&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of diamonds representing runners on base in top row of game view&lt;/cell&gt;
        &lt;cell&gt;yellow&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.other-event&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of other events (mound visit, injury delay, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing outs in top row of game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.strike&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing strikes in top row of game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.strike-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where play ends on a strike (strike out) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.walk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where play ends on a ball (walk, hit by pitch) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;green&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;favorites&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Teams to highlight in schedule and standings views&lt;/cell&gt;
        &lt;cell&gt;Any one of the following: &lt;code&gt;ATL&lt;/code&gt;, &lt;code&gt;AZ&lt;/code&gt;, &lt;code&gt;BAL&lt;/code&gt;, &lt;code&gt;BOS&lt;/code&gt;, &lt;code&gt;CHC&lt;/code&gt;, &lt;code&gt;CIN&lt;/code&gt;, &lt;code&gt;CLE&lt;/code&gt;, &lt;code&gt;COL&lt;/code&gt;, &lt;code&gt;CWS&lt;/code&gt;, &lt;code&gt;DET&lt;/code&gt;, &lt;code&gt;HOU&lt;/code&gt;, &lt;code&gt;KC&lt;/code&gt;, &lt;code&gt;LAA&lt;/code&gt;, &lt;code&gt;LAD&lt;/code&gt;, &lt;code&gt;MIA&lt;/code&gt;, &lt;code&gt;MIL&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;NYM&lt;/code&gt;, &lt;code&gt;NYY&lt;/code&gt;, &lt;code&gt;OAK&lt;/code&gt;, &lt;code&gt;PHI&lt;/code&gt;, &lt;code&gt;PIT&lt;/code&gt;, &lt;code&gt;SD&lt;/code&gt;, &lt;code&gt;SEA&lt;/code&gt;, &lt;code&gt;SF&lt;/code&gt;, &lt;code&gt;STL&lt;/code&gt;, &lt;code&gt;TB&lt;/code&gt;, &lt;code&gt;TEX&lt;/code&gt;, &lt;code&gt;TOR&lt;/code&gt;, &lt;code&gt;WSH&lt;/code&gt;. Or a comma-separated list of multiple (e.g. &lt;code&gt;SEA,MIL&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;git clone https://github.com/paaatrick/playball.git
cd playball
npm install
npm start
&lt;/code&gt;
    &lt;p&gt;Contributions are welcome!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451577</guid><pubDate>Thu, 02 Oct 2025 16:09:15 +0000</pubDate></item><item><title>The Atlantic Quantum team is joining Google</title><link>https://blog.google/technology/research/scaling-quantum-computing-even-faster-with-atlantic-quantum/</link><description>&lt;doc fingerprint="e10f117325f6a4e6"&gt;
  &lt;main&gt;
    &lt;p&gt;Google Quantum AI was founded in 2012 and our mission today remains the same — build quantum computing for otherwise unsolvable problems. We’re making steady progress on our roadmap, including with our latest Willow chip.&lt;/p&gt;
    &lt;p&gt;Today, we’re excited to announce that the Atlantic Quantum team is joining Google. Atlantic Quantum is an MIT-founded startup that develops highly integrated quantum computing hardware. Its modular chip stack, which combines qubits and superconducting control electronics within the cold stage, will help Google Quantum AI more effectively scale our superconducting qubit hardware, and accelerate progress on our roadmap to a large error-corrected quantum computer and real-world applications.&lt;/p&gt;
    &lt;p&gt;We’re delighted for Atlantic Quantum to join us as Google continues to invest in the future of quantum computing and deliver its benefits to society. Learn more about our mission and follow our progress at quantumai.google.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451961</guid><pubDate>Thu, 02 Oct 2025 16:36:53 +0000</pubDate></item><item><title>Why I chose Lua for this blog</title><link>https://andregarzia.com/2025/03/why-i-choose-lua-for-this-blog.html</link><description>&lt;doc fingerprint="7f443511072312dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I chose Lua for this blog&lt;/head&gt;
    &lt;p&gt;This blog used to run using with a stack based on Racket using Pollen and lots of hacks on top of it. At some point I realised that my setup was working against me. The moving parts and workflow I created added too much friction to keep my blog active. That happened mostly because it was a static generator trying to behave as if it was dynamic website with an editing interface. That can be done really well â cue Grav CMS â but that was not the case for me.&lt;/p&gt;
    &lt;p&gt;Once I decided to rewrite this blog as a simpler system, I faced the dilema of what stack to choose. The obvious choice for me would be Javascript, it is the language I use more often and one that I am quite confortable with. Still, I don't think it is a wise choice for the kind of blog I want to maintain.&lt;/p&gt;
    &lt;p&gt;Talking to some friends recently, I noticed that many people I know that have implemented their own blogging systems face many challenges keeping them running over many years. Not because it is hard to keep software running, but because their stack of choice is moving faster than their codebase.&lt;/p&gt;
    &lt;p&gt;This problem is specially prevalent in the Javascript world. It is almost a crime that JS as understood by the browser is this beautiful language with extreme retrocompatibility, while JS as understood and used by the current tooling and workflows is this mess moving at lightspeed. Let me unpack that for a bit.&lt;/p&gt;
    &lt;p&gt;You can open a web page from 1995 on your browser of choice and it will just work because browser vendors try really hard to make sure they don't break the web.&lt;/p&gt;
    &lt;p&gt;Developers who built the whole ecosystem of NodeJS, NPM, and all those libraries and frameworks don't share the same ethos. They all make a big case of semantic versioning and thus being able to handle breaking changes, but they have breaking changes all the time. You'd be hardpressed to actually run some JS code from ten years ago based on NodeJS and NPM. There is a big chance that dependencies might be gone, broken, or it might be incompatible with the current NodeJS.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I know this sounds like FUD, and that for many many projects, maybe even most projects, that will not be the case. But I heard from many people that keeping their blogging systems up to date requires a lot more work than they would like to do and if they don't, then they're screwed.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That is also true about other languages even though many of them move at a slower speed. A friend recently complained about a blogging system he implemented that requires Ruby 2.0 and that keeping that running sucks.&lt;/p&gt;
    &lt;p&gt;I want a simpler blogging system; one that requires minimal changes over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Now we talk about Lua&lt;/head&gt;
    &lt;p&gt;Lua is a wonderful and nimble language that is often misunderstood.&lt;/p&gt;
    &lt;p&gt;One characteristic that I love about it, is that is evolves very slowly. Lua 5.1 was introduced in 2006, Lua 5.4 which is the current version initial release was in 2020. Yes, there are point released in between, but you can see how much slower it moves when compared to JS.&lt;/p&gt;
    &lt;p&gt;The differences between Lua 5.1 and Lua 5.4 are minimal when compared with how much other languages changed in the same time period.&lt;/p&gt;
    &lt;p&gt;Lua only requires a C89 compiler to bootstrap itself. It is very easy to make Lua work and even easier to make it interface with something.&lt;/p&gt;
    &lt;p&gt;JS is a lot larger than Lua, there is more to understand and more to remember. My blog needs are very simple and Lua can handle them with ease.&lt;/p&gt;
    &lt;head rend="h2"&gt;How this blog works&lt;/head&gt;
    &lt;p&gt;This is an old-school blog. I uses cgi-bin â aka Comon Gateway Interface â scripts to run it. It is a dynamic website with a SQLite database holding its data. When you open a page, it fetches the data from a database and assembles a HTML to send to the browser using Mustache templates.&lt;/p&gt;
    &lt;p&gt;One process per request. Like the old days.&lt;/p&gt;
    &lt;p&gt;You might argue that if I went with NodeJS, I'd be able to serve more requests using fewer resources. That is true. I don't need to serve that many requests though. My peak access was a couple years ago with 50k visitors on a week, even my old Racket blog could handle that fine. The Lua one should handle it too; and if it breaks it breaks. I'm a flawed human being, my code can be flawed too, we're in this together, holding hands.&lt;/p&gt;
    &lt;p&gt;Your blog is your place to experiment and program how you want it. You can drop the JS fatigue, you can drop your fancy Haskell types, you can just do whatever you find fun and keep going (and that includes JS and Haskell if that's your thing. You do you).&lt;/p&gt;
    &lt;p&gt;Cause I'm using Lua, I don't have as many libraries and frameworks available to me as JS people have, but I still have quite a large collection via Luarocks. I try not to add many dependencies to my blog. At the moment there are about ten and that is mostly because Lua is a batteries-not-included language so you start from a minimal core and build things up to suit your needs.&lt;/p&gt;
    &lt;p&gt;For a lot of things I went with the questionable choice of implementing things myself. I got my own little CGI library. It is 200 lines long and does the bare minimum to make this blog work. I got my own little libraries for many things. Micropub and IndieAuth were all implemented by hand.&lt;/p&gt;
    &lt;p&gt;At the moment I'm &lt;del&gt;despairing&lt;/del&gt;&lt;del&gt;frustrated&lt;/del&gt; having a lot of fun implementing WebMentions. Doing the Microformats2 &lt;del&gt;exorcism&lt;/del&gt; extraction on my own is teaching me a lot of things.&lt;/p&gt;
    &lt;p&gt;What I want to say is that by choosing a small language that moves very slowly and very few dependencies, I can keep all of my blogging system in my head. I can make sure it will run without too much change for the next ten or twenty years.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Lua is a lego set, a toolkit, it adapts to you and your needs. I don't need to keep chasing the new shiny or the latest framework du jour. I can focus on making the features I want and actually understanding how they work.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Instead of installing a single dependency in another language and it pulling a hundred of other small dependencies all of which were transpiled into something the engine understands to the point that understanding how all the pieces work and fit together takes more time than to learn a new language, I decided to keep things simple.&lt;/p&gt;
    &lt;p&gt;I got 29 Luarocks installed here and that is for all my Lua projects in this machine. That is my blog, my game development, my own work scripts for my day job. Not even half of those are for my blog.&lt;/p&gt;
    &lt;p&gt;I often see wisdom in websites such as Hacker News and Lobsters around the idea of "choosing boring" because it is proven, safe, easier to maintain. I think that boring is not necessarily applicable to my case. I don't find Lua boring at all, but all that those blog posts talk about that kind of mindset are all applicable to my own choices here.&lt;/p&gt;
    &lt;p&gt;Next time you're building your own blogging software, consider for a bit for how long do you want to maintain it. I first started blogging on macOS 8 in 2001. I choose badly many times and in the end couldn't keep my content moving forward in time with me as softwares I used or created became impossible to run. The last two changes: from JS to Racket and from Racket to Lua have been a lot safer and I managed to carry all my content forward into increasingly simpler setups and workflows.&lt;/p&gt;
    &lt;p&gt;My blogging system is not becoming more complex over the years, it is becoming smaller, because with each change I select a stack that is more nimble and smaller than the one I had before. I don't think I can go smaller than Lua though.&lt;/p&gt;
    &lt;p&gt;By small I mean:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A language I can fully understand and keep on my head.&lt;/item&gt;
      &lt;item&gt;A language that I know how to build the engine and can do it if needed.&lt;/item&gt;
      &lt;item&gt;An engine that requires very few resources and is easy to interface with third-party libraries in native code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I chose Lua because of all that, and I'm happy with it and hope this engine will see me through the next ten or so years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45452261</guid><pubDate>Thu, 02 Oct 2025 16:58:55 +0000</pubDate></item><item><title>Liva AI (YC S25) Is Hiring</title><link>https://www.ycombinator.com/companies/liva-ai/jobs/6xM8JYU-founding-operations-lead</link><description>&lt;doc fingerprint="a3de624f74d97bb3"&gt;
  &lt;main&gt;
    &lt;p&gt;Scale AI for video and voice data.&lt;/p&gt;
    &lt;p&gt;The mission at Liva AI (YC S25) is to make AI feel truly human. AI voices and faces today still feel flat and generic, missing emotion, nuance, and identity. We’re fixing that by building the world’s richest library of human voice and video data, fueling the next generation of realistic AI.&lt;/p&gt;
    &lt;p&gt;We’re hiring an extremely organized and committed operator to take on a full-time role. You’ll manage complex projects and people with efficiency, solve problems in uncertain situations, and help us scale fast. Over time, you’ll also play a key role in building the most automated operations system of any data company, translating the workflows you run today into scalable processes and overseeing the internal systems we’re developing.&lt;/p&gt;
    &lt;p&gt;This is a founding role: your work will directly fuel the next generation of AI in a tangible way, while shaping the foundation of how Liva runs at scale.&lt;/p&gt;
    &lt;p&gt;ABOUT THE ROLE&lt;/p&gt;
    &lt;p&gt;What you’ll do:&lt;/p&gt;
    &lt;p&gt;WHAT WE’RE LOOKING FOR&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;p&gt;Nice to have:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; BENEFITS:&lt;/p&gt;
    &lt;p&gt;Liva's mission is to make AI look and sound truly human. The AI voices and faces today feel off, and lack the capability to reflect diverse people across different ethnicities, races, accents, and career professions. We’re fixing that by building the world’s richest library of human voice and video data, fueling the next generation of realistic AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45452299</guid><pubDate>Thu, 02 Oct 2025 17:01:16 +0000</pubDate></item><item><title>Email immutability matters more in a world with AI</title><link>https://www.fastmail.com/blog/not-written-with-ai/</link><description>&lt;doc fingerprint="da5219db73f0ae99"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This blog post was not written with AI&lt;/head&gt;
    &lt;head rend="h2"&gt;Post categories&lt;/head&gt;
    &lt;p&gt;CEO&lt;/p&gt;
    &lt;p&gt;It’s all the rage right now. Everyone is scrambling to put AI into their products. The uncanny valley is shrinking enough that it’s hard to see how much AI was used to write something.&lt;/p&gt;
    &lt;p&gt;This isn’t entirely new, auto-complete on my phone already suggests the most likely word when I’m typing. AI writing tools are an extension of this, but they’re also much more capable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your electronic memory&lt;/head&gt;
    &lt;p&gt;I stand by one of the most important truths about email. It’s not only the largest and most diverse social network, email is your electronic memory.&lt;/p&gt;
    &lt;p&gt;In the novel 1984, the “Ministry of Truth” has a whole massive department which rewrites history. In a world where there’s enough AI capability to process the entire web and rewrite every page to remove something, the cost of “changing history” is much reduced, so we can expect more of it.&lt;/p&gt;
    &lt;p&gt;This is where the immutability of email really shines. An email is your copy, and the sender can’t revise it later. This is frustrating when you’ve sent the wrong thing and have to send a separate correction later, but in the long term it’s insanely valuable.&lt;/p&gt;
    &lt;p&gt;It makes a huge difference to be able to go back and double-check your memory against an email you saw years ago and know that if they disagree, the email is correct. This is already not the case with web pages — they change, and it’s only becoming worse.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adapting to a changing world&lt;/head&gt;
    &lt;p&gt;My son is studying at University now, and he’s one of a few students in his class who refuses to use AI to write his assignments. As he said “what’s the point of paying to be here if I’m not going to build the knowledge and skills for myself, and come out knowing how to do the thing” (near enough… I didn’t write the exact words down in an email, so I’m going off my own fallible memory!) I am so proud of him for having that attitude.&lt;/p&gt;
    &lt;p&gt;I’m also pleased to see that Fastmail’s staff, and many of our customers, are wary of AI tools.&lt;/p&gt;
    &lt;p&gt;But they are that, tools. The world is changing, and we need to adapt and understand it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our service, your data&lt;/head&gt;
    &lt;p&gt;For our service, we want you to be able to do what you desire with your own email, calendars, and contacts. We will continue to build tools and integrations to make that easier.&lt;/p&gt;
    &lt;p&gt;You are welcome to operate your Fastmail account with AI tools, so long as that usage doesn’t otherwise breach our Terms of Service, or degrade the performance of our systems for other customers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our staff, your privacy&lt;/head&gt;
    &lt;p&gt;For our staff, we encourage understanding the tools that exist in the world, and how to use them safely. Our policy makes it clear that any use of tools, including tools with AI in them, must follow clear privacy-preserving principles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data Protection: All data protection, confidentiality, and privacy policies must be followed (our vendors for things like anti-abuse and support are moving towards using AI for translation, categorization, abuse detection – and we are ensuring that their policies continue to provide protection for our customers)&lt;/item&gt;
      &lt;item&gt;Accountability for work: Any AI generated writing or code must be reviewed and understood by a human being, and go through our regular second-set-of-eyes processes before being used&lt;/item&gt;
      &lt;item&gt;Bias awareness: Actively look for biases or hallucinations in AI output&lt;/item&gt;
      &lt;item&gt;Human authority: Always have a path for appeal to a human from any decision that is made by automated tools&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The future&lt;/head&gt;
    &lt;p&gt;Who knows what the future will bring, but we continue to be guided by the principles that we first publicly articulated in 2016 and have held even longer. The data is yours, and we will be good stewards and good internet citizens, helping enable you to use your data in the ways you choose.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453135</guid><pubDate>Thu, 02 Oct 2025 18:00:27 +0000</pubDate></item><item><title>I Keep Blogging with Emacs</title><link>https://entropicthoughts.com/why-stick-to-emacs-blog</link><description>&lt;doc fingerprint="26d8bd04cf2550b8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I Keep Blogging With Emacs&lt;/head&gt;
    &lt;p&gt;Every time I look at someone’s simple static site generation setup for their blog, I feel a pang of envy. I’m sure I could make a decent blogging engine in 2,000 lines of code, and it would be something I’d understand, be proud over, able to extend, and willing to share with others.&lt;/p&gt;
    &lt;p&gt; Instead, I write these articles in Org mode, and use mostly the standard Org publishing functions to export them to html. This is sometimes brittle, but most annoyingly, I don’t understand it. I have been asked for details on how my publishing flow works, but the truth is I have no idea what happens when I run the &lt;code&gt;org-publish-current-file&lt;/code&gt; command.
&lt;/p&gt;
    &lt;p&gt; I could find out by tracing the evaluation of the Lisp code that runs on export, but I won’t, because just the html exporting code (&lt;code&gt;ox-html.el&lt;/code&gt;) is 5,000
lines of complexity. The general exporting framework (&lt;code&gt;ox-publish.el&lt;/code&gt; and
&lt;code&gt;ox.el&lt;/code&gt;) is 8,000 lines. The framework depends on Org parsing code
(&lt;code&gt;org-element.el&lt;/code&gt;) which is at least another 9,000 lines. This is over 20,000
lines of complexity I’d need to contend with.
&lt;/p&gt;
    &lt;p&gt;It might seem like a no-brainer to just write that 2,000 line custom static generator and use that instead.&lt;/p&gt;
    &lt;p&gt;Except one thing: Babel.&lt;/p&gt;
    &lt;p&gt;Any lightweight markup format (like Markdown or ReStructuredText or whatever) allows for embedding code blocks, but Org, through Babel, can run that code on export, and then display the output in the published document, even when the output is a table or an image. It supports sessions that lets code reuse definitions from earlier code blocks. It allows for injecting variables from the markup into the code, and vice versa. As a bonus, Org doesn’t require a JavaScript syntax highlighter, because it generates inline styles in the source code.&lt;/p&gt;
    &lt;p&gt;It does this for a large number of languages, although I mainly use it with R for drawing plots. Being able to do this is incredibly convenient, because it makes it trivial to draft data, illustrations, and text at the same time, adjusting both until the article coheres. Having tried it, I cannot see myself living without it.&lt;/p&gt;
    &lt;p&gt;A simple 2,000 line blogging engine would be a fun weekend project. Mirroring the features of Babel I use would turn it into a multi-month endeavour for someone with limited time such as myself. Not going to happen, and I will continue to beat myself up for overcomplicating my publishing workflow.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453222</guid><pubDate>Thu, 02 Oct 2025 18:06:41 +0000</pubDate></item><item><title>The Answer (1954)</title><link>https://sfshortstories.com/?p=5983</link><description>&lt;doc fingerprint="73199b7271208fff"&gt;
  &lt;main&gt;
    &lt;p&gt;The Answer by Fredric Brown (Angels and Spaceships, 1954) opens with a scientist called Dwar Ev completing a connection and then moving towards a switch:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The switch that would connect, all at once, all of the monster computing machines of all the populated planets in the universe—ninety-six billion planets—into the supercircuit that would connect them all into one super-calculator, one cybernetics machine that would combine all the knowledge of all the galaxies. p. 36&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ev then asks the super-computer if there is a God, and it replies (spoiler), “Yes, now there is a God”. Then, when Ev rushes towards the switch to turn the computer off, it zaps him with a lightning bolt.&lt;lb/&gt;This is one of these squibs (it is less than a page long) that you find (a) pretty neat when you are twelve, but (b) a not very good gimmick story when older. The real sense of wonder here lies in the idea of ninety-six billion inhabited and interconnected planets.&lt;lb/&gt;* (Mediocre). 250 words. Story link.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453299</guid><pubDate>Thu, 02 Oct 2025 18:13:24 +0000</pubDate></item><item><title>Gemini 3.0 Pro – early tests</title><link>https://twitter.com/chetaslua/status/1973694615518880236</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453448</guid><pubDate>Thu, 02 Oct 2025 18:26:57 +0000</pubDate></item><item><title>OpenAI's H1 2025: $4.3B in income, $13.5B in loss</title><link>https://www.techinasia.com/news/openais-revenue-rises-16-to-4-3b-in-h1-2025</link><description>&lt;doc fingerprint="d267d1111c03bee1"&gt;
  &lt;main&gt;
    &lt;p&gt;If you're seeing this message, that means JavaScript has been disabled on your browser.&lt;/p&gt;
    &lt;p&gt;Please enable JavaScript to make this website work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453586</guid><pubDate>Thu, 02 Oct 2025 18:37:28 +0000</pubDate></item><item><title>Why most product planning is bad and what to do about it</title><link>https://blog.railway.com/p/product-planning-improvement</link><description>&lt;doc fingerprint="ad29913538a777e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why most product planning is bad and what to do about it&lt;/head&gt;
    &lt;p&gt;TL;DR: We tried OKRs, they created more ceremony than clarity. Our solution: Problem Driven Development, a 4-day quarterly process focused on identifying problems (not solutions), prioritizing as a team, and committing publicly. It's kept us shipping at velocity even as we've scaled to 1.7M+ users.&lt;/p&gt;
    &lt;p&gt;For most of my friends and colleagues at mature software companies, there are usually three ways for an item of work to get put on the board to eventually be done.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;There is a giant ceremony that determines what gets done by a grab bag of metrics.&lt;/item&gt;
      &lt;item&gt;A deal gets blocked by a missing feature, and the engineering team scrambles jets to eliminate the blocker.&lt;/item&gt;
      &lt;item&gt;Founder feels like we have to build something.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thats not to say that every company is a disorganized mess or a bureaucratic hell scape but, I have never met any engineer who said: “Wow, I just love my company planning process.”&lt;/p&gt;
    &lt;p&gt;These words are seldom spoken in the english language.&lt;/p&gt;
    &lt;p&gt;Railway, was fast approaching 1. and 2. at the same time.&lt;/p&gt;
    &lt;p&gt;Despite us using excellent tools (shoutout Linear) planning is much as a cultural phenomena as well as an interesting engineering problem. From our perspective, we would finalize the features and the requirements what we would want to build once every 3 months and then at times get blindsided every now and then from a new business priority or an incident. Especially now serving 1.7M+ customers, we have to aggregate our taste, feedback, and opportunities and make a roadmap that will get us everything we ever wanted.&lt;/p&gt;
    &lt;p&gt;This was an issue.&lt;/p&gt;
    &lt;p&gt;Why write about something that you would read in Rand’s? It’s Q4 for those who celebrate, and we felt that reigniting the agile vs. waterfall armistice needed to be torn up. (Besides, we’re blogging like it’s 2005.)&lt;/p&gt;
    &lt;p&gt;…and we have spent the better part of 18 months to improve our planning so that we finally get to a process that is not bad, and if you’d like, you can steal it, so that you can deliver excellent products.&lt;/p&gt;
    &lt;p&gt;Mike Tyson asked about his fight plan against Evander Holyfield responded with; “Everyone has a plan until they get punched in the mouth.”&lt;/p&gt;
    &lt;p&gt;And that quote ever since would be used out of context to fight the implementation of Objectives, Key Results across the world. At Railway, we aren’t Anti-Planning, we are Anti-Bad-Planning and as such, we used to avoid a lot of it because we always felt that a bad process is massively negative to the status quo. However, we did get to the point where we needed to finally harness the collective attention spans of the company and work towards a goal.&lt;/p&gt;
    &lt;p&gt;Railway at the time was (and is) a vision led company. We fundamentally believe that most infrastructure boilerplate shouldn’t exist so that you can work on what matters. Back in ‘21, we went from a world where we could do our work in weeks, to work that required us to think in months. I implemented a somewhat lighter form of SAFE which splits work between “sprints”, which were short pieces of work, then initiatives which helped grouped “epics” which are long running work-streams to deliver a feature.&lt;/p&gt;
    &lt;p&gt;Despite us having a strong founder, we still wanted the ability to have employees bring projects that they would be excited to work on.&lt;/p&gt;
    &lt;p&gt;As with most young companies, we found quickly that new work would come in and unseat the old work and we would begin anew.&lt;/p&gt;
    &lt;p&gt;So then we turned to OKRs.&lt;/p&gt;
    &lt;p&gt;We thought that having some long term objectives will help us focus the company.&lt;/p&gt;
    &lt;p&gt;We implemented them faithfully, we all read the John Doer book. …and it worked… somewhat.&lt;/p&gt;
    &lt;p&gt;We can make this a whole blog post on OKRs. But, trying to hold back here from just ranting and to give you the relevant information.&lt;/p&gt;
    &lt;p&gt;OKRs work REALLY well when you have concrete goals and really simple ways to measure them. Its why they belong at their primary birthplace, the factory. Objectives work when you have something binary or “limited”, like a product existing… or not. Or a product meeting a benchmark… or not. It’s easy to rally a team around them and conquer the world.&lt;/p&gt;
    &lt;p&gt;Where OKRs start to falter, are when you need to use them to prioritize work to meet a “unlimited” objective. Like: “Increase conversion rate of landing page”&lt;/p&gt;
    &lt;p&gt;OKRheads will notice that’s a particularly weak objective, however, teaching an entire organization to all of a sudden be great goal setters, which is what OKRs require you to do made it difficult for engineers to 1) bring them and then 2) plan work around them. Which leads to the first big issue of OKRs- which is that it really depends on the human psychology of the team. Engineers straddle the line between concrete numbers such as uptime, and more creative endeavors such as figuring out how to get a feature implemented right. Whenever the work enters the creative realm, the wheels come off.&lt;/p&gt;
    &lt;p&gt;Which is why outside of the factory, OKRs are great for Sales. You set a number, you get alignment on hitting that number, and then you pick up the phone, email, or LinkedIn until you hit it. The psychology of an account executive matches the planning process.&lt;/p&gt;
    &lt;p&gt;Where OKRs work great, is for alignment. If you are able to set great goals and if you wanted to prescribe a bunch of work toward said goal. …and most startups reach for the High Output Management book (which I love) because there is the lack of alignment between different orgs at a startup. Which is to say, I don’t shame any company reaching for them, the same way newborns have the palmar grasp reflex. We yearn for the alignment.&lt;/p&gt;
    &lt;p&gt;The second big issue for OKRs is that they are, by design, inflexible, once you commit to them. If you spend a week or two planning for them for the year or quarter, and you are midway through realizing that the “KR” part is incorrect, thats a one way door.&lt;/p&gt;
    &lt;p&gt;So we felt pressure into making sure that the OKRs were indeed correct. Which is where you enter the issues of the performative aspects of planning. It felt “mature” for Railway to be having these discussions, even though… it wasn’t productive.&lt;/p&gt;
    &lt;p&gt;At Railway, we would spend a significant amount of time discussing what is a valid OKR, only for us to realize we were two days into the planning process and we’ve yet to decide what we should build for the OKR.&lt;/p&gt;
    &lt;p&gt;This is when Christian, our Head of Operations had an answer for us.&lt;/p&gt;
    &lt;p&gt;Instead of crowd sourcing the OKRs from the company and bubbling them up per function.&lt;/p&gt;
    &lt;p&gt;Jake and Christian would work on some top level guidance that would provide a macro view of our finances, priorities, and strategy that we have to keep in mind. We then have a Hex dashboard with topline metrics from different sides of the product and business. From a GTM perspective, it’s engagement, signups, and revenue. From an Engineering perspective, it’s support tickets, uptime, and feature performance.&lt;/p&gt;
    &lt;p&gt;This data would give us a “theme”. (You may have seen them in our launch weeks.)&lt;/p&gt;
    &lt;p&gt;Whenever someone goes on Central Station and requests a feature, depending on how large the work is, that usually determines if that request becomes a “Project”.&lt;/p&gt;
    &lt;p&gt;Then around the start of 2024, Christian spun up a Notion DB, and punched in a simple template for each new entry.&lt;/p&gt;
    &lt;p&gt;Railway at this point and time was organized into three sections of the business. Product Engineering which delivers features and value to our customers via the UI and terminal. Platform which supports the product with the Infrastructure and the APIs to control that Infrastructure. Then Logistics, which is a synthetic team which includes the Support, Marketing, and Sales function working to be the voice of the customer.&lt;/p&gt;
    &lt;p&gt;We would then fill out a “Project Candidate” and then go to bat on trying to figure out if was a priority that tied to the “theme” that was presented.&lt;/p&gt;
    &lt;p&gt;If something is a P0: it’s an existential company risk, we MUST deliver it.&lt;/p&gt;
    &lt;p&gt;Then, a P1: something we need to deliver for this quarter&lt;/p&gt;
    &lt;p&gt;Lastly, P2: a nice to have.&lt;/p&gt;
    &lt;p&gt;This worked well until some cracks formed.&lt;/p&gt;
    &lt;p&gt;At the start of this new process, our team was small enough to list maybe 50 project candidates, discuss as a group and then be on with it but with 200. Spending all day on a call with your co-workers to discuss if something was worth it isn’t the best use of time.&lt;/p&gt;
    &lt;p&gt;Second, Project Candidates would sometimes have full on fleshed out RFC style sections where the author would have a solution ready to go. When a candidate got deprioritized, it’s understandable why someone would feel not great about their hard effort not being reciprocated.&lt;/p&gt;
    &lt;p&gt;Third, to deal with the larger amount of project candidates and additional sources of project candidates, we would set up additional meetings before the planning week that would eat into our engineering cycles. Not that we want to squeeze every engineer for what they are worth, but these negotiation calls would take longer than the main call sometimes. Worse even, was that we would completely reset the board quarter after quarter.&lt;/p&gt;
    &lt;p&gt;Lastly, which was starting to be a bigger issue, was that having Project Candidates being a bottom up process was great when the people who knew that work they needed to do can fit the whole work-stream in their head. But… we were about to embark on a multi-quarter effort that would span all parts of the business to deliver Railway Metal. This system was not great for uncovering planning gaps that required attention from a different team.&lt;/p&gt;
    &lt;p&gt;(With that said, we did ship some really good software thanks to this planning process.)&lt;/p&gt;
    &lt;p&gt;However, Jake, the team, aren’t ones to sit on their laurels so we refined it until we have our current process.&lt;/p&gt;
    &lt;p&gt;So we flipped the project system on it’s head.&lt;/p&gt;
    &lt;p&gt;Instead, we practice what I call: “Problem Driven Development”&lt;/p&gt;
    &lt;p&gt;Rather than spending a loaded amount of time picking feedback items from our feedback systems, and spending a loaded amount of time on trying to flesh out requirements. We collect problems on a continuous process that begins in earnest on the Friday before our planning week.&lt;/p&gt;
    &lt;p&gt;We stopped asking people to propose solutions and started asking them to articulate problems. No more half-baked RFCs that people felt attached to. No more solution-first thinking that locked us into approaches before we understood what we were solving. Just clear problem statements.&lt;/p&gt;
    &lt;p&gt;Here's what a problem looks like in our system:&lt;/p&gt;
    &lt;p&gt;Problem Title: "Users can't debug failed deployments without SSHing into containers"&lt;/p&gt;
    &lt;p&gt;Notice what's missing? Any mention of how we'd solve it. That comes later, after we've committed to solving the problem. Then we turned the two week ceremony into a lightning 4 day sprint.&lt;/p&gt;
    &lt;p&gt;After the problems are listed and filled out fully, each team on Day 1 enters problems on their own. If a problem isn't fleshed out, it's put in the Parking Lot and we kindly nudge the idea person to fill the template.&lt;/p&gt;
    &lt;p&gt;By the time we get into a room together, everyone's had time to process what's on the board.&lt;/p&gt;
    &lt;p&gt;Then on Day 2, the planning captain will have a closed session with the team (with spectators from other teams) to put a best guess priority from P0, P1, and P2. If we find that a problem is contentious or requires an external dependency, we mark it so the day after, we can discuss it.&lt;/p&gt;
    &lt;p&gt;By having each team prioritize independently first, we avoid the tragedy of the commons where everything becomes P0 because someone shouted loudest on the call. Platform can look at their problems and say "yes, API reliability is more important than API versioning right now" without having to negotiate with Product in real-time.&lt;/p&gt;
    &lt;p&gt;(We have spent countless hours talking about difficult questions on a call when we have been able to diffuse rough conversations ahead of time.)&lt;/p&gt;
    &lt;p&gt;On Day 3, we get to a 95 percent certainty on the priorities listed and tie break the dependencies. We also confirm that we have the capacity and staffing to deal with the problems listed on the board. If not, we add a problem for hiring for a specific role.&lt;/p&gt;
    &lt;p&gt;This is where cross-team dependencies surface naturally. When Platform marks "Multi-mount volumes" as P1 and Product marks "HA DBs" as P1, we can see that one blocks the other. We're not discovering this mid-quarter when someone's already three weeks into building the wrong thing.&lt;/p&gt;
    &lt;p&gt;The capacity check is crucial too. We look at people's current commitments, oncall rotations, and whether anyone's about to take parental leave. If we have eight P1 problems and capacity for five, we have an honest conversation about what drops to P2 or what we need to hire for.&lt;/p&gt;
    &lt;p&gt;Then lastly, Day 4, commitment. Everyone looks at the whole list, mention final objections if we should be working on something that we aren't— or vice versa. After we confirm priorities, we assign problems to people by giving them a DRI (Directly Responsible Individual).&lt;/p&gt;
    &lt;p&gt;Then, we look into each other's eyes, as much as anyone can on a video call, and commit.&lt;/p&gt;
    &lt;p&gt;After the starting gun is off, we then write RFCs on problems to see how we can best solve them which eventually become Linear tickets.&lt;/p&gt;
    &lt;p&gt;We're working on open-sourcing our Notion templates because honestly, this isn't rocket science. It's just:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cataloguing your problems, not solutions&lt;/item&gt;
      &lt;item&gt;Let teams prioritize independently before negotiating&lt;/item&gt;
      &lt;item&gt;Front-load the hard conversations&lt;/item&gt;
      &lt;item&gt;Commit publicly&lt;/item&gt;
      &lt;item&gt;Then, and only then, figure out how to solve it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the real hard part was building a culture where people feel safe bringing up problems without having to defend their pet solutions.&lt;/p&gt;
    &lt;p&gt;Keep process to a minimum, focus on performance of shipping, not performative work.&lt;/p&gt;
    &lt;p&gt;And like any good product, our planning process will keep evolving. Maybe in six months we'll realize Problem Driven Development has its own cracks. Theres always plenty to do on planning and I think we generally know how it’ll need to evolve, but we try to take small steps from cycle to cycle. If that happens, we'll write another blog post about what we learned and what we changed.&lt;/p&gt;
    &lt;p&gt;Until then, we're shipping.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45454374</guid><pubDate>Thu, 02 Oct 2025 19:34:08 +0000</pubDate></item></channel></rss>