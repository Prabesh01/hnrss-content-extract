<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 08 Oct 2025 13:02:48 +0000</lastBuildDate><item><title>An illustrated introduction to linear algebra</title><link>https://www.ducktyped.org/p/an-illustrated-introduction-to-linear</link><description>&lt;doc fingerprint="2f3c03fd2e3517a9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;An Illustrated Introduction to Linear Algebra&lt;/head&gt;
    &lt;head rend="h3"&gt;Chapter 1: Row vs column picture&lt;/head&gt;
    &lt;p&gt;This post assumes you know algebra, but no linear algebra. Lets dive in.&lt;/p&gt;
    &lt;p&gt;There are two big ideas I want to introduce in the first chapter: Gaussian elimination, (which is not strictly a linear algebra thing, and has been around for years before linear algebra came along), and row picture versus column picture, which is a linear algebra thing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Money example&lt;/head&gt;
    &lt;p&gt;Let’s say you have a bunch of nickels and pennies, and you want to know how many of each do you need to have 23 cents.&lt;/p&gt;
    &lt;p&gt;You could write that as an equation that looks like this:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;x&lt;/code&gt; is the number of nickels you need, &lt;code&gt;y&lt;/code&gt; is the number of pennies you need. And you need to figure out the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values that would make the left-hand side work out to 23. And this one is pretty easy, you can just work it out yourself. You’d need four nickels and three pennies.&lt;/p&gt;
    &lt;p&gt;So &lt;code&gt;x&lt;/code&gt; is four, &lt;code&gt;y&lt;/code&gt; is three.&lt;/p&gt;
    &lt;p&gt;This kind of equation is called a linear equation. And that’s because when you plot this equation, everything is flat and smooth. There are no curves or holes. There isn’t a &lt;code&gt;^2&lt;/code&gt; in the equation for example to make it curved. Linear equations are great because they’re much easier to work with than curved equations.&lt;/p&gt;
    &lt;p&gt;Aside: Another solution for the above is 23 pennies. Or -4 nickels + 43 pennies.&lt;/p&gt;
    &lt;p&gt;The point is you have two variables (x and y for nickels and pennies), and you are trying to combine them in different ways to hit one number. The trouble starts when you have two variables, and you need to combine them in different ways to hit two different numbers. That’s when Gaussian elimination comes in. In what world would you have to hit two different numbers? Does that seem outlandish? It’s actually very common! Read on for an example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Food example&lt;/head&gt;
    &lt;p&gt;Now let’s look at a different example. In the last one we were trying to make 23 cents with nickels and pennies. Here we have two foods. One is milk, the other is bread. They both have some macros in terms of carbs and protein:&lt;/p&gt;
    &lt;p&gt;and now we want to figure out how many of each we need to eat to hit this target of 5 carbs and 7 protein.&lt;/p&gt;
    &lt;p&gt;This is a very similar question to the one we just asked with nickels and pennies, except instead of one equation, we have two equations:&lt;/p&gt;
    &lt;p&gt;Again we have an &lt;code&gt;x&lt;/code&gt; and a &lt;code&gt;y&lt;/code&gt;. Lets find their values. To solve these kinds of questions, we usually use Gaussian elimination. If you’ve never used Gaussian elimination, strap in.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gaussian elimination&lt;/head&gt;
    &lt;p&gt;Step one is to rewrite this as a set of two equations:&lt;/p&gt;
    &lt;p&gt;Now you subtract multiples of one equation from another to try to narrow down the value of one variable. Lets double that second equation:&lt;/p&gt;
    &lt;p&gt;See how we have a &lt;code&gt;+2y&lt;/code&gt; and a &lt;code&gt;-2y&lt;/code&gt; now? Now we can add the two equations together to eliminate &lt;code&gt;y&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;We’re left with one equation and one variable. We can solve for &lt;code&gt;x&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Aha, we know &lt;code&gt;x = 3&lt;/code&gt;. Now we can plug that into one of the equations to find &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We plug that in to one of the equations and find out that &lt;code&gt;y&lt;/code&gt; equals 1, and there we have answer: three milks, one bread, is what we need.&lt;/p&gt;
    &lt;p&gt;This method is called Gaussian elimination, even though it was not discovered by Gauss. If you haven’t seen Gaussian elimination, congratulations, you learned a big idea! Gaussian elimination is something we will talk about more. It’s part of what makes linear algebra useful.&lt;/p&gt;
    &lt;p&gt;We can also find the solution by drawing pictures. Let’s see how that works.&lt;/p&gt;
    &lt;head rend="h3"&gt;Picture version&lt;/head&gt;
    &lt;p&gt;Let’s plot one of these lines. First, we need to rewrite the equations in terms of &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Reminder: first equation is for carbs, second for protein. x is number of milks, y is number of breads.&lt;/p&gt;
    &lt;p&gt;Now let’s plot the graph for the first equation.&lt;/p&gt;
    &lt;p&gt;Now, what does this line represent?&lt;/p&gt;
    &lt;p&gt;It’s all the combinations of bread and milk that you can have to get exactly five carbs:&lt;/p&gt;
    &lt;p&gt;So you can eat no milk and two-and-a-half breads, or two milks and one-and-a-half breads, or five milks and no bread, to get to exactly five carbs. All of those combinations would mean you have eaten exactly five carbs. You can pick any point that sits on this line to get to your goal of eating five carbs.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: You can see the line goes into the negative as well. Technically, 5 breads and -5 milks will give you 5 carbs as well, but you can’t drink negative milks. For these examples, let’s assume only positive numbers for the variables.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Now, let’s plot the other one. This is the same thing, but for protein.&lt;/p&gt;
    &lt;p&gt;If you eat any of these combinations, you’ll have met the protein goal:&lt;/p&gt;
    &lt;p&gt;You can pick a point that sits on the first line to meet the carb goal. You can pick a point that sits on the second line to meet the protein goal. But you need a point that sits on both lines to hit both goals.&lt;/p&gt;
    &lt;p&gt;How would a point sit on both lines? Well, it would be where the lines cross. Since these are straight lines, the lines cross only once, which makes sense because there’s only a single milk and bread combo that would get you to exactly five grams of carbs and seven grams of protein.&lt;/p&gt;
    &lt;p&gt;Now we plot the lines together, see where they intersect, and that’s our answer:&lt;/p&gt;
    &lt;p&gt;Bam! We just found the solution using pictures.&lt;/p&gt;
    &lt;p&gt;So that’s a quick intro to Gaussian elimination. But you don’t need linear algebra to do Gaussian elimination. This is a technique that has been around for 2,000 years. It was discovered in Asia, it was rediscovered in Europe, I think in the 1600s or something, and no one was really talking about “linear algebra”. This trick is just very useful.&lt;/p&gt;
    &lt;p&gt;That’s the first big idea you learned. You can stop there if you want. You can practice doing this sort of elimination. It’s a very common and useful thing.&lt;/p&gt;
    &lt;head rend="h2"&gt;The column picture&lt;/head&gt;
    &lt;p&gt;What we just saw is called the “row picture”. Now I want to show you the column picture. I’m going to introduce a new idea, which is: instead of writing this series of equations, what if we write just one equation? Remember how we had one equation for the nickels and pennies question?&lt;/p&gt;
    &lt;p&gt;What if we write one like that for food? Not a system of equations, just a single equation? What do you think that would look like? Something like this:&lt;/p&gt;
    &lt;p&gt;It’s an equation where the coefficients aren’t numbers, they’re an “array” of numbers. The big idea here is: what if we have a linear equation, but instead of numbers, we have arrays of numbers? What if we treat &lt;code&gt;[1 2]&lt;/code&gt;, the way we treat a number?&lt;/p&gt;
    &lt;p&gt;Can that actually work? If so, it is pretty revolutionary. Our whole lives we have been looking at just numbers, and now we’re saying, what if we look at arrays of numbers instead?&lt;/p&gt;
    &lt;p&gt;Let’s see how it could work in our food example. What if the coefficients are an array of numbers? Well, this way of thinking is actually kind of intuitive. You might find it even more intuitive than the system of equations version.&lt;/p&gt;
    &lt;p&gt;Each of these coefficients are called vectors. If you’re coming from computer science, you can kind of think of a vector as an array of numbers (i.e. the order matters).&lt;/p&gt;
    &lt;p&gt;Lets see how we can use vectors to find a solution to the bread and milk question.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step one: graph the vectors.&lt;/head&gt;
    &lt;p&gt;Yeah, we can graph vectors. We can graph them either as a point, like I’ve done for the target vector here, or as an arrow, which is what I’ve done with the vector for bread and the vector for milk:&lt;/p&gt;
    &lt;p&gt;Use the two numbers in the vector as the x and y coordinates.&lt;/p&gt;
    &lt;p&gt;That is another big idea here: We always think of a set of coordinates giving a point, but you can think of vectors as an arrow instead of just a point.&lt;/p&gt;
    &lt;p&gt;Now what we’re asking is how much milk and how much bread do we need, to get to that point?&lt;/p&gt;
    &lt;p&gt;This is a pretty simple question. It’s simple enough that we can actually see it. Let me add some milks:&lt;/p&gt;
    &lt;p&gt;And let me add a bread. Bingo bango, we’re at the point:&lt;/p&gt;
    &lt;p&gt;Yeah, we literally add them on, visually. I personally find this more intuitive. I think the system of equations picture can confuse me sometimes, because the initial question was, “how much bread and how much milk should I eat?” The vector way, you see it in terms of breads and milks. The row way, you see it as one of the lines is the carbs, the other line is the protein, and the x and y axes are the amount of bread, which results in the same thing, but it’s a little more roundabout, a little more abstract. This one is very direct.&lt;/p&gt;
    &lt;head rend="h3"&gt;The algebra way&lt;/head&gt;
    &lt;p&gt;We just saw that we can graph vectors too. Graphing it works differently from graphing the rows, but there is a graph we can make, and it works, which is pretty cool. What about the algebra way?&lt;/p&gt;
    &lt;p&gt;Here is the equation again:&lt;/p&gt;
    &lt;p&gt;Since we already know the answer, I’ll just plug that in:&lt;/p&gt;
    &lt;p&gt;Now, the question is how does the left side equal the right side? The first question is how do you define this multiplication? Well, in linear algebra, it’s defined as, if you multiply a scalar by a vector, you just multiply it by each number in that vector:&lt;/p&gt;
    &lt;p&gt;Now you are left with two vectors. How do you add two vectors? Well, in the linear algebra you just add the individual elements of each vector:&lt;/p&gt;
    &lt;p&gt;And you end up with the answer.&lt;/p&gt;
    &lt;p&gt;Congratulations, you’ve just had your first taste of linear algebra. It’s a pretty big step, right? Instead of numbers, we’re working with arrays of numbers. In future chapters, we will see why this is so powerful.&lt;/p&gt;
    &lt;p&gt;That’s the first big concept of linear algebra: row picture vs column picture.&lt;/p&gt;
    &lt;p&gt;Finally, I’ll just leave you with this last teaser, which is: how would you write these two equations in matrix notation? Like this:&lt;/p&gt;
    &lt;p&gt;This is the exact same thing as before. You can write it as scalars times columns, as we had done before:&lt;/p&gt;
    &lt;p&gt;or you can write it as a matrix times a vector, as above. Either one works.&lt;/p&gt;
    &lt;p&gt;Matrices are a big part of linear algebra. But before we talk about matrices, we will talk about the dot product, which is coming up next.&lt;/p&gt;
    &lt;head rend="h3"&gt;Additional reading&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Check out Gilbert Strang’s lectures on linear algebra on YouTube.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Summary&lt;/head&gt;
    &lt;p&gt;P.S. Want more art? Check out my Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45490713</guid><pubDate>Mon, 06 Oct 2025 12:38:40 +0000</pubDate></item><item><title>The case for an iceberg-native database</title><link>https://www.warpstream.com/blog/the-case-for-an-iceberg-native-database-why-spark-jobs-and-zero-copy-kafka-wont-cut-it</link><description>&lt;doc fingerprint="63f8997654ebcf8c"&gt;
  &lt;main&gt;
    &lt;p&gt;We launched a new product called WarpStream Tableflow that is the easiest, cheapest, and most flexible way to convert Kafka topic data into Iceberg tables with low latency, and keep them compacted. If youâre already familiar with the challenges of converting Kafka topics into Iceberg tables, feel free to skip ahead to our solution in the âWhat if we had a magic box?â section.&lt;/p&gt;
    &lt;p&gt;Apache Iceberg and Delta Lake are table formats that provide the illusion of a traditional database table on top of object storage, including schema evolution, concurrency control, and partitioning that is transparent to the user. These table formats allow many open-source and proprietary query engines and data warehouse systems to operate on the same underlying data, which prevents vendor lock-in and allows using best-of-breed tools for different workloads without making additional copies of that data that are expensive and hard to govern.&lt;/p&gt;
    &lt;p&gt;Table formats are really cool, but they're just that, formats. Something or someone has to actually build and maintain them. As a result, one of the most debated topics in the data infrastructure space right now is the best way to build Iceberg and Delta Lake tables from real-time data stored in Kafka.&lt;/p&gt;
    &lt;p&gt;The canonical solution to this problem is to use Spark batch jobs.&lt;/p&gt;
    &lt;p&gt;This is how things have been done historically, and itâs not a terrible solution, but there are a few problems with it:&lt;/p&gt;
    &lt;p&gt;Problems 1 and 3 canât be solved with Spark, but we might be able to solve problem 2 (table update delay) by using Spark Streaming and micro-batching processing:&lt;/p&gt;
    &lt;p&gt;Well not quite. Itâs true that if you use Spark Streaming to run smaller micro-batch jobs, your Iceberg table will be updated much more frequently. However, now you have two new problems in addition to the ones you already had:&lt;/p&gt;
    &lt;p&gt;Anyone who has ever built a data lake is familiar with the small files problem: the more often you write to the data lake, the faster it will accumulate files, and the longer your queries will take until eventually they become so expensive and slow that they stop working altogether.&lt;/p&gt;
    &lt;p&gt;Thatâs ok though, because there is a well known solution: more Spark!&lt;/p&gt;
    &lt;p&gt;We can create a new Spark batch job that periodically runs compactions that take all of the small files that were created by the Spark Streaming job and merges them together into bigger files:&lt;/p&gt;
    &lt;p&gt;The compaction job solves the small file problem, but it introduces a new one. Iceberg tables suffer from an issue known as the âsingle writer problemâ which is that only one process can mutate the table concurrently. If two processes try to mutate the table at the same time, one of them will fail and have to redo a bunch of work [1].&lt;/p&gt;
    &lt;p&gt;This means that your ingestion process and compaction processes are racing with each other, and if either of them runs too frequently relative to the other, the conflict rate will spike and the overall throughput of the system will come crashing down.&lt;/p&gt;
    &lt;p&gt;Of course, there is a solution to this problem: run compaction infrequently (say once a day), and with coarse granularity. That works, but it introduces two new problems:Â&lt;/p&gt;
    &lt;p&gt;Exhausted yet? Well, weâre still not done. Every Iceberg table modification results in a new snapshot being created. Over time, these snapshots will accumulate (costing you money) and eventually the metadata JSON file will get so large that the table becomes un-queriable. So in addition to compaction, you need another periodic background job to prune old snapshots.&lt;/p&gt;
    &lt;p&gt;Also, sometimes your ingestion or compaction jobs will fail, and youâll have orphan parquet files stuck in your object storage bucket that donât belong to any snapshot. So youâll need yet another periodic background job to scan the bucket for orphan files and delete them.&lt;/p&gt;
    &lt;p&gt;It feels like weâre playing a never-ending game of whack-a-mole where every time we try to solve one problem, we end up introducing two more. Well, thereâs a reason for that: the Iceberg and Delta Lake specifications are just that, specifications. They are not implementations.Â&lt;/p&gt;
    &lt;p&gt;Imagine I gave you the specification for how PostgreSQL lays out its B-trees on disk and some libraries that could manipulate those B-trees. Would you feel confident building and deploying a PostgreSQL-compatible database to power your companyâs most critical applications? Probably not, because youâd still have to figure out: concurrency control, connection pool management, transactions, isolation levels, locking, MVCC, schema modifications, and the million other things that a modern transactional database does besides just arranging bits on disk.&lt;/p&gt;
    &lt;p&gt;The same analogy applies to data lakes. Spark provides a small toolkit for manipulating parquet and Iceberg manifest files, but what users actually want is 50% of the functionality of a modern data warehouse. The gap between what Spark actually provides out of the box, and what users need to be successful, is a chasm.&lt;/p&gt;
    &lt;p&gt;When we look at things through this lens, itâs no longer surprising that all of this is so hard. Saying: âIâm going to use Spark to create a modern data lake for my companyâ is practically equivalent to announcing: âIâm going to create a bespoke database for every single one of my companyâs data pipelinesâ. No one would ever expect that to be easy. Databases are hard.&lt;/p&gt;
    &lt;p&gt;Most people want nothing to do with managing any of this infrastructure. They just want to be able to emit events from one application and have those events show up in their Iceberg tables within a reasonable amount of time. Thatâs it.&lt;/p&gt;
    &lt;p&gt;Itâs a simple enough problem statement, but the unfortunate reality is that solving it to a satisfactory degree requires building and running half of the functionality of a modern database.&lt;/p&gt;
    &lt;p&gt;Itâs no small undertaking! I would know. My co-founder and I (along with some other folks at WarpStream) have done all of this before.Â&lt;/p&gt;
    &lt;p&gt;Hopefully by now you can see why people have been looking for a better solution to this problem. Many different approaches have been tried, but one that has been gaining traction recently is to have Kafka itself (and its various different protocol-compatible implementations) build the Iceberg tables for you.&lt;/p&gt;
    &lt;p&gt;The thought process goes like this: Kafka (and many other Kafka-compatible implementations) already have tiered storage for historical topic data. Once records / log segments are old enough, Kafka can tier them off to object storage to reduce disk usage and costs for data that is infrequently consumed.&lt;/p&gt;
    &lt;p&gt;Why not âjustâ have the tiered log segments be parquet files instead, then add a little metadata magic on-top and voila, we now have a âzero-copyâ streaming data lake where we only have to maintain one copy of the data to serve both Kafka consumers and Iceberg queries, and we didnât even have to learn anything about Spark!&lt;/p&gt;
    &lt;p&gt;Problem solved, we can all just switch to a Kafka implementation that supports this feature, modify a few topic configs, and rest easy that our colleagues will be able to derive insights from our real time Iceberg tables using the query engine of their choice.&lt;/p&gt;
    &lt;p&gt;Of course, thatâs not actually true in practice. This is the WarpStream blog after all, so dedicated readers will know that the last 4 paragraphs were just an elaborate axe sharpening exercise for my real point which is this: none of this works, and it will never work.&lt;/p&gt;
    &lt;p&gt;I know what youâre thinking: âRichie, you say everything doesnât work. Didnât you write like a 10 page rant about how tiered storage in Kafka doesnât work?â. Yes, I did.&lt;/p&gt;
    &lt;p&gt;I will admit, I am extremely biased against tiered storage in Kafka. Itâs an idea that sounds great in practice, but falls flat on its face in most practical implementations. Maybe I am a little jaded because a non-trivial percentage of all migrations to WarpStream get (temporarily) stalled at some point when the customer tries to actually copy the historical data out of their Kafka cluster into WarpStream and loading the historical from tiered storage degrades their Kafka cluster.&lt;/p&gt;
    &lt;p&gt;But thatâs exactly my point: I have seen tiered storage fail at serving historical reads in the real world, time and time again.&lt;/p&gt;
    &lt;p&gt;I wonât repeat the (numerous) problems associated with tiered storage in Apache Kafka and most vendor implementations in this blog post, but I will (predictably) point out that changing the tiered storage format fixes none of those problems, makes some of them worse, and results in a sub-par Iceberg experience to boot.&lt;/p&gt;
    &lt;p&gt;Letâs start with how the Iceberg format makes existing tiered storage implementations that already perform poorly, perform even worse. First off, generating parquet files is expensive. Like really expensive. Compared to copying a log segment from the local disk to object storage, it uses at least an order of magnitude more CPU cycles and significant amounts of memory.&lt;/p&gt;
    &lt;p&gt;That would be fine if this operation were running on a random stateless compute node, but itâs not, itâs running on one of the incredibly important Kafka brokers that is the leader for some of the topic-partitions in your cluster. This is the worst possible place to perform computationally expensive operations like generating parquet files.&lt;/p&gt;
    &lt;p&gt;To make matters worse, loading the tiered data from object storage to serve historical Kafka consumers (the primary performance issue with tiered storage) becomes even more operationally difficult and expensive because now the Parquet files have to be decoded and converted back into the Kafka record batch format, once again, in the worst possible place to perform computationally expensive operations: the Kafka broker responsible for serving the producers and consumers that power your real-time workloads.&lt;/p&gt;
    &lt;p&gt;This approach works in prototypes and technical demos, but it will become an operational and performance nightmare for anyone who tries to take this approach into production at any kind of meaningful scale. Or youâll just have to massively overprovision your Kafka cluster, which essentially amounts to throwing an incredible amount of money at the problem and hoping for the best.&lt;/p&gt;
    &lt;p&gt;Letâs say you donât believe me about the performance issues with tiered storage. Thatâs fine, because it doesnât really matter anyways. The point of using Iceberg as the tiered storage format for Apache Kafka would be to generate a real-time Iceberg table that can be used for something. Unfortunately, tiered storage doesn't give you Iceberg tables that are actually useful.&lt;/p&gt;
    &lt;p&gt;If the Iceberg table is generated by Kafkaâs tiered storage system then the partitioning of the Iceberg table has to match the partitioning of the Kafka topic. This is extremely annoying for all of the obvious reasons. Your Kafka partitioning strategy is selected for operational use-cases, but your Iceberg partitioning strategy should be selected for analytical use-cases.&lt;/p&gt;
    &lt;p&gt;There is a natural impedance mismatch here that will constantly get in your way. Optimal query performance is always going to come from partitioning and sorting your data to get the best pruning of files on the Iceberg side, but this is impossible if the same set of files must also be capable of serving as tiered storage for Kafka consumers as well.&lt;/p&gt;
    &lt;p&gt;There is an obvious way to solve this problem: store two copies of the tiered data, one for serving Kafka consumers, and the other optimized for Iceberg queries. This is a great idea, and itâs how every modern data system that is capable of serving both operational and analytic workloads at scale is designed.&lt;/p&gt;
    &lt;p&gt;But if youâre going to store two different copies of the data, thereâs no point in conflating the two use-cases at all. The only benefit you get is perceived convenience, but you will pay for it dearly down the line in unending operational and performance problems.&lt;/p&gt;
    &lt;p&gt;In summary, the idea of a âzero-copyâ Iceberg implementation running inside of production Kafka clusters is a pipe dream. It would be much better to just let Kafka be Kafka and Iceberg be Iceberg.&lt;/p&gt;
    &lt;p&gt;Remember the small file problem from the Spark section? Unfortunately, the small file problem doesnât just magically disappear if we shove parquet file generation into our Kafka brokers. We still need to perform table maintenance and file compaction to keep the tables queryable.&lt;/p&gt;
    &lt;p&gt;This is a hard problem to solve in Spark, but itâs an even harder problem to solve when the maintenance and compaction work has to be performed in the same nodes powering your Kafka cluster. The reason for that is simple: Spark is a stateless compute layer that can be spun up and down at will.&lt;/p&gt;
    &lt;p&gt;When you need to run your daily major compaction session on your Iceberg table with Spark, you can literally cobble together a Spark cluster on-demand from whatever mixed-bag, spare-part virtual machines happen to be lying around your multi-tenant Kubernetes cluster at the moment. You can even use spot instances, itâs all stateless, it just doesnât matter!&lt;/p&gt;
    &lt;p&gt;No matter how much compaction you need to run, or how compute intensive it is, or how long it takes, it will never in a million years impair the performance or availability of your real-time Kafka workloads.&lt;/p&gt;
    &lt;p&gt;Contrast that with your pristine Kafka cluster that has been carefully provisioned to run on high end VMs with tons of spare RAM and expensive SSDs/EBS volumes. Resizing the cluster takes hours, maybe even days. If the cluster goes down, you immediately start incurring data loss in your business. THATâS where you want to spend precious CPU cycles and RAM smashing Parquet files together!?&lt;/p&gt;
    &lt;p&gt;It just doesnât make any sense.&lt;/p&gt;
    &lt;p&gt;âDisklessâ Kafka implementations like WarpStream are in a slightly better position to just build the Iceberg functionality directly into the Kafka brokers because they separate storage from compute which makes the compute itself more fungible.&lt;/p&gt;
    &lt;p&gt;However, I still think this is a bad idea, primarily because building and compacting Iceberg files is an incredibly expensive operation compared to just shuffling bytes around like Kafka normally does. In addition, the cost and memory required to build and maintain Iceberg tables is highly variable with the schema itself. A small schema change to add a few extra columns to the Iceberg table could easily result in the load on your Kafka cluster increasing by more than 10x. That would be disastrous if that Kafka cluster, diskless or not, is being used to serve live production traffic for critical applications.&lt;/p&gt;
    &lt;p&gt;Finally, all of the existing Kafka implementations that do support this functionality inevitably end up tying the partitioning of the Iceberg tables to the partitioning of the Kafka topics themselves, which results in sad Iceberg tables as we described earlier. Either that, or they leave out the issue of table maintenance and compaction altogether.&lt;/p&gt;
    &lt;p&gt;Look, I get it. Creating Iceberg tables with any kind of reasonable latency guarantees is really hard and annoying. Tiered storage and diskless architectures like WarpStream and Freight are all the rage in the Kafka ecosystem right now. If Kafka is already moving towards storing its data in object storage anyways, canât we all just play nice, massage the log segments into parquet files somehow (waves hands), and just live happily ever after?&lt;/p&gt;
    &lt;p&gt;I get it, I really do. The idea is obvious, irresistible even. We all crave simplicity in our systems. Thatâs why this idea has taken root so quickly in the community, and why so many vendors have rushed poorly conceived implementations out the door. But as I explained in the previous section, itâs a bad idea, and there is a much better way.&lt;/p&gt;
    &lt;p&gt;What if instead of all of this tiered storage insanity, we had, and please bear with me for a moment, a magic box.&lt;/p&gt;
    &lt;p&gt;Instead of looking inside the magic box, let's first talk about what the magic box does. The magic box knows how to do only one thing: it reads from Kafka, builds Iceberg tables, and keeps them compacted. Ok thatâs three things, but I fit them into a short sentence so it still counts.&lt;/p&gt;
    &lt;p&gt;Thatâs all this box does and ever strives to do. If we had a magic box like this, then all of our Kafka and Iceberg problems would be solved because we could just do this:&lt;/p&gt;
    &lt;p&gt;And life would be beautiful.&lt;/p&gt;
    &lt;p&gt;Again, I know what youâre thinking: âItâs Spark isnât it? You put Spark in the box!?â&lt;/p&gt;
    &lt;p&gt;That would be one way to do it. You could write an elaborate set of Spark programs that all interacted with each other to integrate with schema registries, carefully handle schema migrations, DLQ invalid records, handle upserts, solve the concurrent writer problem, gracefully schedule incremental compactions, and even auto-scale to boot.&lt;/p&gt;
    &lt;p&gt;And it would work.&lt;/p&gt;
    &lt;p&gt;But it would not be a magic box.&lt;/p&gt;
    &lt;p&gt;It would be Spark in a box, and Sparkâs sharp edges would always find a way to poke holes in our beautiful box.&lt;/p&gt;
    &lt;p&gt;That wouldnât be a problem if you were building this box to run as a SaaS service in a pristine environment operated by the experts who built the box. But thatâs not a box that you would ever want to deploy and run yourself.&lt;/p&gt;
    &lt;p&gt;Spark is a garage full of tools. You can carefully arrange the tools in a garage into an elaborate rube goldberg machine that with sufficient and frequent human intervention periodically spits out widgets of varying quality.&lt;/p&gt;
    &lt;p&gt;But thatâs not what we need. What we need is an Iceberg assembly line. A coherent, custom-built, well-oiled machine that does nothing but make Iceberg, day in and day out, with ruthless efficiency and without human supervision or intervention. Kafka goes in, Iceberg comes out.&lt;/p&gt;
    &lt;p&gt;THAT would be a magic box that you could deploy into your own environment and run yourself.&lt;/p&gt;
    &lt;p&gt;Itâs a matter of packaging.&lt;/p&gt;
    &lt;p&gt;Youâre on the WarpStream blog, so this is the part where I tell you that we built the magic box. Itâs called Tableflow, and itâs not a new idea. In fact, Confluent Cloud users have been able to enjoy Tableflow as a fully managed service for over 6 months now, and they love it. Itâs cost effective, efficient, and tightly integrated with Confluent Cloudâs entire ecosystem, including Flink.&lt;/p&gt;
    &lt;p&gt;However, thereâs one problem with Confluent Cloud Tableflow: itâs a fully managed service that runs in Confluent Cloud, and therefore it doesnât work with WarpStreamâs BYOC deployment model. We realized that we needed a BYOC version of Tableflow, so that all of Confluentâs WarpStream users could get the same benefits of Tableflow, but in their own cloud account with a BYOC deployment model.&lt;/p&gt;
    &lt;p&gt;So thatâs what we built!&lt;/p&gt;
    &lt;p&gt;WarpStream Tableflow (henceforth referred to as just Tableflow in this blog post) is to Iceberg generating Spark pipelines what WarpStream is to Apache Kafka.&lt;/p&gt;
    &lt;p&gt;Itâs a magic, auto-scaling, completely stateless, single-binary database that runs in your environment, connects to your Kafka cluster (whether itâs Apache Kafka, WarpStream, AWS MSK, Confluent Platform, or any other Kafka-compatible implementation) and manufactures Iceberg tables to your exacting specification using a declarative YAML configuration.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;p&gt;Tableflow automates all of the annoying parts about generating and maintaining Iceberg tables:&lt;/p&gt;
    &lt;p&gt;Unfortunately, Tableflow canât actually do all of these things yet. But it can do a lot of them, and the missing gaps will all be filled in shortly.Â&lt;/p&gt;
    &lt;p&gt;How does it work? Well, thatâs the subject of our next blog post. But to summarize: we built a custom, BYOC-native and cloud-native database whose only function is the efficient creation and maintenance of streaming data lakes.&lt;/p&gt;
    &lt;p&gt;More on the technical details in our next post, but if this interests you, please check out our documentation, and contact us to get admitted to our early access program. You can also subscribe to our newsletter to make sure youâre notified when we publish our next post in this series with all the gory technical details.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45491128</guid><pubDate>Mon, 06 Oct 2025 13:21:51 +0000</pubDate></item><item><title>Show HN: I'm building a browser for reverse engineers</title><link>https://nullpt.rs/reverse-engineering-browser</link><description>&lt;doc fingerprint="aa009620a90d26e8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Preamble&lt;/head&gt;
    &lt;p&gt;In the expanding world of AI my heart still lies in AST transforms, browser fingerprinting, and anti-bot circumvention. In fact, that's the majority of this blog's content. But my workflow always felt... primitive. I was still manually sifting through page scripts, pasting suspicious snippets into an editor, and writing bespoke deobfuscators by hand. Tools like Webcrack and deobfuscate.io help, but the end-to-end loop still felt slow and manual. I wanted to build a tool that would be my web reverse-engineering Swiss Army knife&lt;/p&gt;
    &lt;p&gt;If you're just curious about what it looks like and don't care about how it works then here's a quick showcase:&lt;/p&gt;
    &lt;head rend="h2"&gt;Humble Beginnings&lt;/head&gt;
    &lt;p&gt;My first idea was simple: make a browser extension. For an MVP I wanted to hook an arbitrary function like &lt;code&gt;Array.prototype.push&lt;/code&gt; as early as possible and log every call to it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hooking functions in JavaScript&lt;/head&gt;
    &lt;p&gt;In JavaScript, it's trivial to hook into and override existing functions because you can reassign references at runtime. A common pattern is to stash the original function, replace it with a wrapper that does whatever instrumentation you want, and then call the original so the page keeps behaving normally:&lt;/p&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};
&lt;/code&gt;
    &lt;p&gt;Here's what that looks like in Chrome's devtools:&lt;/p&gt;
    &lt;p&gt;This technique should make it pretty straightforward to build a Chrome extension that hooks arbitrary global functions on page load and surfaces calls in a small UI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Content Scripts&lt;/head&gt;
    &lt;p&gt;Chrome's content scripts are files that run in the context of web pages, which we can use to install our hooks early.&lt;/p&gt;
    &lt;p&gt;The idea is simple, we create a content script that runs at document_start that injects a tiny bit of code that replaces &lt;code&gt;Array.prototype.push&lt;/code&gt; with a wrapper that logs and then calls the original.&lt;/p&gt;
    &lt;code&gt;{
 "name": "My extension",
 "content_scripts": [
   {
     "run_at": "document_start", // Script is injected after any files from css, but before any other DOM is constructed or any other script is run.
     "matches": ["&amp;lt;all_urls&amp;gt;"],
     "js": ["content-script.js"]
   }
 ]
}
&lt;/code&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};
&lt;/code&gt;
    &lt;p&gt;Running this on a page that clearly used Array.push gave me... absolutely nothing. At first, I thought it had to be an execution order issue. Maybe my hook was loading too late? But after another read through the docs, I found this painfully obvious note staring me right in the face:&lt;/p&gt;
    &lt;p&gt;âContent scripts live in an isolated world, allowing a content script to make changes to its JavaScript environment without conflicting with the page or other extensionsâ content scripts.â&lt;/p&gt;
    &lt;p&gt;In hindsight, of course that makes sense. Still, it sucked. I wasnât ready to give up yet, though. I had a potentially clever workaround: injecting a &lt;code&gt;&amp;lt;script&amp;gt;&lt;/code&gt; tag directly into the page with my hook inside. But, naturally, it could never be that easy.&lt;/p&gt;
    &lt;p&gt;I knew if I wanted to get this done, I would have to go down a layer.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Editor's Note: Some readers have kindly pointed out to me that this is actually possible to accomplish from an extension. However, the custom browser approach still has benefits explained later in the post :-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Chrome Devtools Protocol&lt;/head&gt;
    &lt;p&gt;The Chrome DevTools Protocol (CDP) is the low-level bridge for instrumenting, inspecting, and debugging Chromium-based browsers. Itâs what automation tools like Selenium and Playwright use under the hood. CDP exposes a large set of methods and events split across domains. The docs publish a convenient, comprehensive list of them.&lt;/p&gt;
    &lt;p&gt;While reading the domains, one method jumped out: &lt;code&gt;Page.addScriptToEvaluateOnNewDocument&lt;/code&gt;. Its description "Evaluates given script in every frame upon creation (before loading frame's scripts)" sounded like exactly the hook we needed: run code before the pageâs own scripts so we can win the prototype race.&lt;/p&gt;
    &lt;p&gt;To prove the idea I built a tiny test: a page with a script that pushes a secret value into an array, and a CDP-injected hook that tries to observe that push. If the hook sees the secret, the technique works. I chose to prototype this using Electron. I could have spoken directly to the browser over raw CDP, but Electron made wiring up a UI, IPC, and a quick demo app way faster for a weekend PoC.&lt;/p&gt;
    &lt;code&gt;const { app, BrowserWindow } = require("electron/main");

function createWindow() {
  const win = new BrowserWindow({
    width: 800,
    height: 600,
  });

  const dbg = win.webContents.debugger;
  dbg.attach("1.3");
  // Enables the Page domain so we can run the script on new document command after
  dbg.sendCommand("Page.enable");
  dbg.sendCommand("Page.addScriptToEvaluateOnNewDocument", {
    source: `(() =&amp;gt; {
        const _origPush = Array.prototype.push;
        Array.prototype.push = function (...args) {
            console.log('Array.push called on', this, 'with', args);
            return _origPush.apply(this, args);
        };
})();`,
  });
  win.webContents.openDevTools();
  win.loadURL("file:///Users/veritas/demo/index.html");
}

app.whenReady().then(() =&amp;gt; {
  createWindow();
});
&lt;/code&gt;
    &lt;p&gt;The result?:&lt;/p&gt;
    &lt;p&gt;It worked! I knew this PoC could take me far. I could hook any arbitrary global function or property and log (or spoof!) arguments and return values. The next step was building a user interface around it.&lt;/p&gt;
    &lt;p&gt;Since this started as a fun weekend project, I wanted the fastest path to a working demo. In true open-source fashion I searched for âelectron web browserâ and stumbled across electron-browser-shell by Samuel Maddock.&lt;/p&gt;
    &lt;p&gt;That project gave me an address bar, tabs, and a basic IPC-ready shell bridging the webview environment and my browser UI.&lt;/p&gt;
    &lt;p&gt;From there I added a sidebar that would display hooked function events as they fired.&lt;/p&gt;
    &lt;p&gt;To make things more interesting I needed to hook more than Array.push. A favorite target of fingerprinting scripts is the Canvas API. Sites can draw a static image to a &lt;code&gt;&amp;lt;canvas&amp;gt;&lt;/code&gt;, call &lt;code&gt;toDataURL()&lt;/code&gt; (or read pixel data), and use the resulting hash to fingerprint your GPU using subtle rendering differences. By correlating canvas hashes with other signals (user agent, installed fonts, etc.), trackers can build a surprisingly robust fingerprint. Watching and optionally spoofing these kind of calls is extremely useful for this kind of RE work.&lt;/p&gt;
    &lt;p&gt;The result looked as follows:&lt;/p&gt;
    &lt;p&gt;I was pretty happy with the direction the project was taking. The PoC actually felt useful. Remembering my previous work reverse-engineering TikTokâs web collector and how aggressively those collectors scrape client-side signals, I couldnât resist testing the hook there. I fired up the demo, pointed it at TikTok, and watched the UI for activity.&lt;/p&gt;
    &lt;p&gt;The site was pulling a decent amount of telemetry. Canvas calls (like &lt;code&gt;toDataURL&lt;/code&gt;), WebGL stats, font and plugin probes, and other subtle signals that, when combined, paint a detailed fingerprint. Seeing those calls appear in my sidebar made this project feel immediately worthwhile.&lt;/p&gt;
    &lt;p&gt;I even made sure to include all canvas operations in a secrion of the detail pane to be able to recreate a canvas if necessary.&lt;/p&gt;
    &lt;p&gt;I wanted to run this against more anti-bots. Out of curiosity I pointed the demo at a site using Cloudflareâs Turnstile. I knew Turnstile was collecting various browser signals, but to my surprise, my sidebar showed nothing. Why was I seeing zero logs?&lt;/p&gt;
    &lt;head rend="h2"&gt;OOPif(S) I did it again&lt;/head&gt;
    &lt;p&gt;Cloudflare renders the Turnstile widget inside a sandboxed iframe tucked into a closed shadow root. This iframe is an OOPIF (out-of-process iframe). It lives in a different renderer process so page-level scripts (and our injected hooks) simply wonât run there, thus, no logs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hopping the Turnstile&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;In 2024, the M.T.A. reports to have lost a combined $568 million in unpaid bus fares and $350 million in unpaid subway fares, wait, wrong turnstile.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We needed a way to run our hooks inside those out-of-process frames. While scanning CDP I noticed the &lt;code&gt;Target.attachedToTarget&lt;/code&gt; event. It fires when the debugger auto-attaches to a new target or when you explicitly call &lt;code&gt;attachToTarget&lt;/code&gt;. This was the key: if we tell CDP to auto-attach to targets, it will notify us (and give us a &lt;code&gt;sessionId&lt;/code&gt;) for every new frame/process as it appears. With that target/session info we can evaluate code in the correct context so our hook actually runs inside OOPIFs as they spawn.&lt;/p&gt;
    &lt;code&gt;const dbg = view.webContents.debugger
dbg.on('message', async (_, method, params) =&amp;gt; {
    if (method === 'Target.attachedToTarget') {
        const { sessionId, targetInfo } = params
        // Prepare child session
        dbg.sendCommand('Runtime.enable', {}, sessionId).catch(() =&amp;gt; {})
        dbg.sendCommand('Page.enable', {}, sessionId).catch(() =&amp;gt; {})
        // Inject hook script into child frames (iframes)
        dbg.sendCommand('Page.addScriptToEvaluateOnNewDocument', { source: hook }, sessionId)
    }
})
&lt;/code&gt;
    &lt;p&gt;Tada, we have events!&lt;/p&gt;
    &lt;p&gt;Iâm not the first to hook common globals and dynamically analyze page scripts. Anti-bots are well aware of this trick and will use a variety of techniques to detect runtime JS patches, so you canât assume your wrappers will stay hidden.&lt;/p&gt;
    &lt;p&gt;How is this possible?&lt;/p&gt;
    &lt;head rend="h2"&gt;toString theory&lt;/head&gt;
    &lt;p&gt;In JavaScript, functions contain a &lt;code&gt;toString&lt;/code&gt; instance method. Let's try calling this on a native function:&lt;/p&gt;
    &lt;code&gt;const mapToString = Array.prototype.map.toString()
// returns 'function map() { [native code] }'
&lt;/code&gt;
    &lt;p&gt;This means that the implementation of this function is provided by the browser's native code. How does this look like with our hook applied?:&lt;/p&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};
const pushToString = Array.prototype.push.toString(); // Returns "function (...args) {\n  console.log('Array.push called on', this, 'with', args);\n  return _origPush.apply(this, args);\n}"
&lt;/code&gt;
    &lt;p&gt;Oh no, our hook has been discovered! Luckily for us, this is easily patched&lt;/p&gt;
    &lt;code&gt;Array.prototype.push.toString = () =&amp;gt; "function push() { [native code] }";
&lt;/code&gt;
    &lt;p&gt;Phew, that was close.&lt;/p&gt;
    &lt;code&gt;const haha = Array.prototype.push.toString.toString(); // '() =&amp;gt; "function push() { [native code] }"'
&lt;/code&gt;
    &lt;p&gt;Oh no, another leak!&lt;/p&gt;
    &lt;code&gt;Array.prototype.push.toString.toString = () =&amp;gt; "function toString() { [native code] }"; // Yay it's fixed
&lt;/code&gt;
    &lt;p&gt;Ahhh! Another one!&lt;/p&gt;
    &lt;code&gt;const haha = Array.prototype.push.toString.toString.toString.toString.toString.toString();
&lt;/code&gt;
    &lt;p&gt;Wait, you can do what now!?&lt;/p&gt;
    &lt;code&gt;const youCantEscape = Function.toString.call(Array.prototype.push); // Returns "function (...args) {\n  console.log('Array.push called on', this, 'with', args);\n  return _origPush.apply(this, args);\n}"
&lt;/code&gt;
    &lt;p&gt;These JS runtime patches turned out to be frustratingly leaky. Patch one hole and another opens. Fixes were possible, but every patch felt like a bandaid that introduced new detection vectors. see:&lt;/p&gt;
    &lt;code&gt;const _origPush = Array.prototype.push;
Array.prototype.push = function (...args) {
  console.log('Array.push called on', this, 'with', args);
  return _origPush.apply(this, args);
};

// Yay, we patched this
Array.prototype.push.toString = () =&amp;gt; "function push() { [native code] }";
Array.prototype.push.toString.toString = () =&amp;gt; "function toString() { [native code] }";

// *facepalm*
const anotherLeak = Array.prototype.push.name; // returns "" instead of "push"
&lt;/code&gt;
    &lt;p&gt;Those runtime patches were very brittle. Targets we were analyzing could detect the instrumentation and change behavior or even self destruct if they noticed they were being watched. Fixing each leak felt like an endless game of whack-a-mole, so I decided to go a layer deeper.&lt;/p&gt;
    &lt;head rend="h2"&gt;Forking Chromium&lt;/head&gt;
    &lt;p&gt;At this point I knew I wanted to fork Chromium. I still planned to use Electron, at least for now, since Iâd already built a decent UI and didnât feel like rewriting it all in native C++. The idea was simple: fork Electron (and by extension Chromium), patch into the Blink layer where these API calls happen, and expose them somehow.&lt;/p&gt;
    &lt;p&gt;I didnât spend too long figuring out how Iâd surface those events. I was already using CDP, so why not create my own custom CDP domain and emit events from there? That way my existing Electron app could just subscribe to them like any other CDP event.&lt;/p&gt;
    &lt;p&gt;Luckily, Electron has a well-documented guide for building from source. Unluckily, building it took more than three hours on my M2 Pro Mac Mini. To make things worse, macOS 26 had broken parts of the build chain. The Metal toolchain wasnât being detected no matter what I had tried. Eventually, I hardcoded the path into the build script just to move forward. After several hours of C++ compilation errors and boredom, I finally had a locally built Electron binary running from source.&lt;/p&gt;
    &lt;p&gt;Now came the hard part: creating a custom CDP domain. The &lt;code&gt;devtools-frontend&lt;/code&gt; repository actually provides documentation on defining new protocol domains.&lt;/p&gt;
    &lt;p&gt;The gist is that protocols are defined in a &lt;code&gt;.pdl&lt;/code&gt; (Protocol Definition Language) file, specifically &lt;code&gt;browser_protocol.pdl&lt;/code&gt;. To add your own domain, you simply declare it there alongside the existing ones.&lt;/p&gt;
    &lt;p&gt;I decided to name my new domain &lt;code&gt;Snitch&lt;/code&gt;, and defined it like this:&lt;/p&gt;
    &lt;code&gt;experimental domain Snitch
  command disable
  command enable
  event toDataURLCalled
    parameters
      string dataURL
      optional string frameId
      optional string contextId
&lt;/code&gt;
    &lt;p&gt;Next, you include your new protocol files in Blinkâs build configuration, found in &lt;code&gt;third_party/blink/renderer/core/inspector/BUILD.gn&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;From there, you define an agent, the bridge that connects Blinkâs internals to the DevTools Protocol so your new CDP domain can send and receive events.&lt;/p&gt;
    &lt;p&gt;Iâll be honest, the documentation for this part was pretty lacking. The only promising link in the documentation pointed to a locked Google Doc presumably restricted to Chrome team members. They say there's no better documentation than the source code itself! By dissecting existing domains like &lt;code&gt;DOMStorage&lt;/code&gt;, &lt;code&gt;Network&lt;/code&gt;, and others, I reverse-engineered how they registered and dispatched events, then adapted that pattern for my own &lt;code&gt;Snitch&lt;/code&gt; domain.&lt;/p&gt;
    &lt;p&gt;I eventually landed on this:&lt;/p&gt;
    &lt;p&gt;snitch_agent.h&lt;/p&gt;
    &lt;code&gt;#ifndef THIRD_PARTY_BLINK_RENDERER_CORE_INSPECTOR_SNITCH_AGENT_H_
#define THIRD_PARTY_BLINK_RENDERER_CORE_INSPECTOR_SNITCH_AGENT_H_
#include &amp;lt;optional&amp;gt;

#include "third_party/blink/renderer/core/core_export.h"
#include "third_party/blink/renderer/core/inspector/inspector_base_agent.h"
#include "third_party/blink/renderer/core/inspector/protocol/snitch.h"

namespace blink {

class InspectedFrames;

class CORE_EXPORT SnitchAgent final
    : public InspectorBaseAgent&amp;lt;protocol::Snitch::Metainfo&amp;gt; {
 public:
  SnitchAgent(InspectedFrames*);
  SnitchAgent(const SnitchAgent&amp;amp;) = delete;
  SnitchAgent&amp;amp; operator=(const SnitchAgent&amp;amp;) = delete;
  ~SnitchAgent() override;

  void Trace(Visitor*) const override;
  protocol::Response enable() override;
  protocol::Response disable() override;

  void DidCanvasToDataURL(ExecutionContext*, const String&amp;amp; data_url,
                          const String&amp;amp; frame_id,
                          const String&amp;amp; context_id);

 private:
  Member&amp;lt;InspectedFrames&amp;gt; inspected_frames_;
  InspectorAgentState::Boolean enabled_;
};

}  // namespace blink

#endif  // THIRD_PARTY_BLINK_RENDERER_CORE_INSPECTOR_SNITCH_AGENT_H_

&lt;/code&gt;
    &lt;p&gt;snitch_agent.cpp&lt;/p&gt;
    &lt;code&gt;#include "third_party/blink/renderer/core/inspector/snitch_agent.h"
#include "third_party/blink/renderer/core/inspector/inspected_frames.h"

namespace blink {

SnitchAgent::SnitchAgent(
  InspectedFrames* inspected_frames)
  : inspected_frames_(inspected_frames),
    enabled_(&amp;amp;agent_state_, /*default_value=*/false) {}


SnitchAgent::~SnitchAgent() = default;

void SnitchAgent::Trace(Visitor* visitor) const {
  visitor-&amp;gt;Trace(inspected_frames_);
  InspectorBaseAgent::Trace(visitor);
}

protocol::Response SnitchAgent::enable() {
  enabled_.Set(true);
  instrumenting_agents_-&amp;gt;AddSnitchAgent(this);
  return protocol::Response::Success();
}

protocol::Response SnitchAgent::disable() {
  enabled_.Clear();
  instrumenting_agents_-&amp;gt;RemoveSnitchAgent(this);
  return protocol::Response::Success();
}

void SnitchAgent::DidCanvasToDataURL(ExecutionContext* context, const String&amp;amp; data_url,
                                     const String&amp;amp; frame_id,
                                     const String&amp;amp; context_id) {

  if (!enabled_.Get()) {
    return;
  }

  std::optional&amp;lt;String&amp;gt; maybe_frame;
  if (!frame_id.empty()) {
    maybe_frame = frame_id;
  }

  std::optional&amp;lt;String&amp;gt; maybe_ctx;
  if (!context_id.empty()) {
    maybe_ctx = context_id;
  }

  GetFrontend()-&amp;gt;toDataURLCalled(data_url, maybe_frame, maybe_ctx);
}

}  // namespace blink

&lt;/code&gt;
    &lt;p&gt;Now I needed a way to trigger my new event from the native C++ implementation of &lt;code&gt;toDataURL&lt;/code&gt;. The implementation for that function lives in &lt;code&gt;src/third_party/blink/renderer/core/html/canvas/html_canvas_element.cc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;While digging through how other events were dispatched, I noticed something interesting. The agents werenât called directly. Instead, events were emitted through probes. These probes act as intermediary hooks that Blink uses to fire instrumentation events into the DevTools pipeline.&lt;/p&gt;
    &lt;p&gt;Hereâs a comment from that same class showing how a probe fires when a canvas element is created:&lt;/p&gt;
    &lt;code&gt;CanvasRenderingContext* HTMLCanvasElement::GetCanvasRenderingContextInternal(
    ExecutionContext* execution_context,
    const String&amp;amp; type,
    const CanvasContextCreationAttributesCore&amp;amp; attributes) {
  CanvasRenderingContext::CanvasRenderingAPI rendering_api =
      CanvasRenderingContext::RenderingAPIFromId(type);

  // ...

  CanvasRenderingContextFactory* factory =
      GetRenderingContextFactory(static_cast&amp;lt;int&amp;gt;(rendering_api));

  // Tell the debugger about the attempt to create a canvas context
  // even if it will fail, to ease debugging.
  probe::DidCreateCanvasContext(&amp;amp;GetDocument());
  // ...
}  
&lt;/code&gt;
    &lt;p&gt;These probes are defined in a file called &lt;code&gt;core_probes.pidl&lt;/code&gt;. The comment at the top of this file states:&lt;/p&gt;
    &lt;code&gt;/*
 * make_instrumenting_probes.py uses this file as a source to generate
 * core_probes_inl.h, core_probes_impl.cc and core_probe_sink.h.
 *
 * The code below is not a correct IDL but a mix of IDL and C++.
 *
 * The syntax for an instrumentation method is as follows:
 *
 *    returnValue methodName([paramAttr1] param1, [paramAttr2] param2, ...)
&lt;/code&gt;
    &lt;p&gt;Following this syntax, I added my custom probe:&lt;/p&gt;
    &lt;code&gt;void DidCanvasToDataURL([Keep] ExecutionContext*, String&amp;amp; data_url, String&amp;amp; frame_id, String&amp;amp; context_id);
&lt;/code&gt;
    &lt;p&gt;A similarly named &lt;code&gt;core_probes.json5&lt;/code&gt; holds the mappings of which agents are responsible for which probes. We can add our entry as such:&lt;/p&gt;
    &lt;code&gt;{
    observers: {
        // ...,
        SnitchAgent: {
            probes: ["DidCanvasToDataURL"]
        }
        // ...
    }
}
&lt;/code&gt;
    &lt;p&gt;The final step in adding our custom domain is to register the agent in &lt;code&gt;WebDevToolsAgentImpl::AttachSession&lt;/code&gt; like so:&lt;/p&gt;
    &lt;code&gt;session-&amp;gt;CreateAndAppend&amp;lt;SnitchAgent&amp;gt;(inspected_frames);
&lt;/code&gt;
    &lt;p&gt;and actually calling it in the implementation of &lt;code&gt;toDataURL&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;String HTMLCanvasElement::toDataURL(const String&amp;amp; mime_type,
                                    const ScriptValue&amp;amp; quality_argument,
                                    ExceptionState&amp;amp; exception_state) const {
  // ...
  String data_url = ToDataURLInternal(mime_type, quality, kBackBuffer,
                                      ReadbackType::kWebExposed);

  // // Hook up call to our new CDP event (Snitch.toDataURLCalled)
  probe::DidCanvasToDataURL(GetExecutionContext(), data_url, frame_id, context_id);

  return data_url;
}
&lt;/code&gt;
    &lt;p&gt;After accidentally nuking my local repository and having to restart the entire process, including sitting through another 3 hour compilation ð¥², it was ready to test!&lt;/p&gt;
    &lt;p&gt;We can create a simple Electron test application:&lt;/p&gt;
    &lt;code&gt;const { app, BrowserWindow } = require("electron/main");

function createWindow() {
  const win = new BrowserWindow({
    width: 800,
    height: 600,
  });

  const dbg = win.webContents.debugger;
  dbg.attach("1.3");
  dbg.sendCommand("Snitch.enable");
  dbg.on("message", (_, method, { dataURL }) =&amp;gt; {
    if (method === "Snitch.toDataURLCalled") {
      console.log("toDataURL called", dataURL);
    }
  });
  win.loadURL("https://demo.fingerprint.com/playground");
}

app.whenReady().then(() =&amp;gt; {
  createWindow();
});
&lt;/code&gt;
    &lt;p&gt;and run it pointing to our custom Electron build:&lt;/p&gt;
    &lt;code&gt;$ /Users/veritas/electron/src/out/Testing/Electron.app/Contents/MacOS/Electron demo.js
&lt;/code&gt;
    &lt;p&gt;Drumroll, please!&lt;/p&gt;
    &lt;p&gt;It worked! We can see our custom CDP event firing and returning to us the result of a toDataURL call on FingerprintJS' playground. We can now use these stealthy CDP events and not leak the fact that we're instrumenting these functions.&lt;/p&gt;
    &lt;p&gt;Note: Depending on what we do in these hooks, it may still be possible to detect us through any side-effects we introduce or potentially through timing checks (Is the function slower than it would usually be?).&lt;/p&gt;
    &lt;head rend="h2"&gt;Extras&lt;/head&gt;
    &lt;p&gt;This was powerful, but I wanted more. I needed a few extra tools to make this thing a real web reverse-engineering Swiss Army knife.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deobfuscation&lt;/head&gt;
    &lt;p&gt;One of the biggest time sinks in this kind of work is dealing with obfuscated scripts. I wanted a built-in tool that could automatically detect and attempt to deobfuscate scripts as they load. Using CDPâs &lt;code&gt;Network&lt;/code&gt; domain, I intercept incoming JavaScript files and run a few lightweight heuristics to score their likelihood of being obfuscated. Suspicious ones are displayed in a separate tab, where I integrate tools like bensbâs deobfuscate.io to automatically try recovering a more readable version. The plan is to add more tools such as Webcrack and even custom deobfuscators of my own.&lt;/p&gt;
    &lt;p&gt;I also added a section that extracts and displays recovered string literals from the processed script for added speed.&lt;/p&gt;
    &lt;head rend="h3"&gt;Overwriting properties and functions&lt;/head&gt;
    &lt;p&gt;Hooking and reading is fun, but sometimes you want to change behavior. You now know that doing so in a browser environment, across OOPIFs and with anti-tamper checks is non-trivial. I built an Overrides tab where you can define custom JavaScript snippets that overwrite functions or properties across all frames. These execute without triggering common integrity checks, giving a clean way to spoof or alter these values.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fingerprint payload decryption&lt;/head&gt;
    &lt;p&gt;My bread and butter is dissecting anti-bot and fingerprinting scripts. These scripts often encrypt or encode their payloads before sending them to backend validators, which makes analysis painful. To make life easier, I added a feature that detects known collectors and automatically intercepts their outbound requests. It decrypts (or decodes) the payloads and displays both the plaintext and structured data in a neat table view.&lt;/p&gt;
    &lt;p&gt;Of course, each collector still needs to be reverse-engineered beforehand. Maybe this is where AI-assisted payload analysis could step in someday? Maybe, but for now I will continue to hand-roll my own parsers :-)&lt;/p&gt;
    &lt;head rend="h2"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;Iâm really happy with how this project has evolved. Itâs gone from my quick weekend curiosity to a genuinely useful research tool. Still, I have a few major goals remain before I can say I'm truly proud:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Abandon Electron&lt;/p&gt;&lt;lb/&gt;Electron was great for rapid prototyping, but it is heavy and adds its own leaks. Theyâre fixable, sure, but the cleaner path is to embed the UI directly inside Chromium. Iâm looking into how other Chromium forks (like Brave) integrate their native UIs and exploring whether I can do the same.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Hook all the things&lt;/p&gt;&lt;lb/&gt;Iâve already implemented a broad set of hooks. Canvas, WebGL, audio fingerprinting,&lt;code&gt;navigator&lt;/code&gt;accessors, document and window properties, and more. But can we hook everything?&lt;lb/&gt;Iâve experimented with injecting hooks deeper in V8 where function calls are dispatched, however, V8âs optimizations quickly complicate things. Disabling those optimizations would work but at the cost of performance (and thus introducing timing leaks). Another idea is to modify the IDL code generator to automatically insert hooks during buildtime. This is likely the approach I will take.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Release?&lt;/p&gt;&lt;lb/&gt;I havenât decided what to do once itâs ready. Maybe open source it? Would others find it useful? Was this all built into Chromium this entire time under some obscure setting that I missed? Who knows.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And with that, I present to you a gallery of canvas fingerprint images that I've collected during this project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fingerprint Gallery&lt;/head&gt;
    &lt;head rend="h3"&gt;Tiktok&lt;/head&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
const gradient0 = context.createLinearGradient(10, 0, 180, 1)
gradient0.addColorStop(0, "red")
gradient0.addColorStop(0.1, "white")
gradient0.addColorStop(0.2, "blue")
gradient0.addColorStop(0.3, "yellow")
gradient0.addColorStop(0.4, "purple")
gradient0.addColorStop(0.7, "orange")
gradient0.addColorStop(1, "magenta")
context.fillStyle = gradient0
context.fillRect(0, 10, 100, 6)
const gradient1 = context.createLinearGradient(0, 0, 100, 100)
gradient1.addColorStop(0, "green")
gradient1.addColorStop(0.5, "yellow")
gradient1.addColorStop(0.7, "orange")
gradient1.addColorStop(1, "magenta")
context.beginPath()
context.fillStyle = gradient1
context.arc(50, 10, 25, 0, 6.283185307179586)
context.stroke()
context.fillStyle = "rgba(150, 32, 170, .97)"
context.font = "12px Sans"
context.textBaseline = "top"
context.fillText("*+(}#?ð¼ ð", 18, 18)
context.shadowBlur = 1
context.fillStyle = "rgba(47, 211, 69, .99)"
context.font = "14px Sans"
context.textBaseline = "top"
context.fillText("ð¼OynG@%tp$", 3, 3)
context.beginPath()
context.arc(30, 10, 20, 0, 6.283185307179586)
context.strokeStyle = "rgba(255, 12, 220, 1)"
context.stroke()
&lt;/code&gt;
    &lt;head rend="h3"&gt;FingerprintJS&lt;/head&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
context.rect(0, 0, 10, 10)
context.rect(2, 2, 6, 6)
context.isPointInPath(5, 5, "evenodd")
context.textBaseline = "alphabetic"
context.fillStyle = "#f60"
context.fillRect(100, 1, 62, 20)
context.fillStyle = "#069"
context.font = "11pt \"Times New Roman\""
context.fillText("Cwm fjordbank gly ð", 2, 15)
context.fillStyle = "rgba(102, 204, 0, 0.2)"
context.font = "18pt Arial"
context.fillText("Cwm fjordbank gly ð", 4, 45)
&lt;/code&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
context.globalCompositeOperation = "multiply"
context.fillStyle = "#f2f"
context.beginPath()
context.arc(40, 40, 40, 0, 6.283185307179586, true)
context.closePath()
context.fill()
context.fillStyle = "#2ff"
context.beginPath()
context.arc(80, 40, 40, 0, 6.283185307179586, true)
context.closePath()
context.fill()
context.fillStyle = "#ff2"
context.beginPath()
context.arc(60, 80, 40, 0, 6.283185307179586, true)
context.closePath()
context.fill()
context.fillStyle = "#f9c"
context.arc(60, 60, 60, 0, 6.283185307179586, true)
context.arc(60, 60, 20, 0, 6.283185307179586, true)
context.fill("evenodd")
&lt;/code&gt;
    &lt;head rend="h3"&gt;Cloudflare&lt;/head&gt;
    &lt;head&gt;Canvas operations:&lt;/head&gt;
    &lt;code&gt;const canvas = document.createElement('canvas')
const context = canvas.getContext("2d")
const gradient0 = context.createRadialGradient(33, 18, 8, 42, 10, 226)
gradient0.addColorStop(0, "#809900")
gradient0.addColorStop(1, "#404041")
context.fillStyle = gradient0
context.shadowBlur = 11
context.shadowColor = "#F38020"
context.beginPath()
context.moveTo(9, 14)
context.quadraticCurveTo(93, 48, 116, 111)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient1 = context.createRadialGradient(77, 98, 2, 27, 30, 206)
gradient1.addColorStop(0, "#809900")
gradient1.addColorStop(1, "#404041")
context.fillStyle = gradient1
context.beginPath()
context.ellipse(58, 55, 31, 28, 1.4441705959829747, 0.5401125108618993, 4.052233984744969)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient2 = context.createRadialGradient(108, 12, 10, 65, 118, 169)
gradient2.addColorStop(0, "#1AB399")
gradient2.addColorStop(1, "#E666B3")
context.fillStyle = gradient2
context.shadowBlur = 16
context.shadowColor = "#809980"
context.font = "27.77777777777778px aanotafontaa"
context.fillText("Ry", 13, 67)
context.shadowBlur = 0
const gradient3 = context.createRadialGradient(46, 47, 0, 101, 108, 207)
gradient3.addColorStop(0, "#4DB380")
gradient3.addColorStop(1, "#FF4D4D")
context.fillStyle = gradient3
context.shadowBlur = 3
context.shadowColor = "#FF6633"
context.beginPath()
context.moveTo(54, 5)
context.bezierCurveTo(54, 90, 32, 74, 71, 120)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient4 = context.createRadialGradient(119, 123, 3, 109, 90, 137)
gradient4.addColorStop(0, "#E6B333")
gradient4.addColorStop(1, "#3366E6")
context.fillStyle = gradient4
context.shadowBlur = 4
context.shadowColor = "#B3B31A"
context.beginPath()
context.moveTo(76, 0)
context.bezierCurveTo(1, 49, 103, 67, 49, 125)
context.stroke()
context.fill()
context.shadowBlur = 0
const gradient5 = context.createRadialGradient(34, 47, 1, 37, 59, 245)
gradient5.addColorStop(0, "#809900")
gradient5.addColorStop(1, "#404041")
context.fillStyle = gradient5
context.beginPath()
context.ellipse(56, 57, 14, 8, 1.2273132071162383, 4.1926143018618225, 2.8853539230051624)
context.stroke()
context.fill()
context.shadowBlur = 0
context.shadowBlur = 14
context.shadowColor = "#809900"
context.font = "11.904761904761905px aanotafontaa"
context.strokeText("@H1", 30, 73)
context.shadowBlur = 0;
&lt;/code&gt;
    &lt;head rend="h2"&gt;Until next time&lt;/head&gt;
    &lt;p&gt;I'd love to know if you found this even remotely interesting or think it's just a giant waste of time :-) I certainly had fun building it and had even more fun using it&lt;/p&gt;
    &lt;head rend="h2"&gt;Credits&lt;/head&gt;
    &lt;p&gt;pimothyxd: Helped with the design of the UI! Always someone I can depend on.&lt;/p&gt;
    &lt;p&gt;bensb: Used his deobfuscator for the scripts tab. Also very knowledgable and a great person to chat ideas with.&lt;/p&gt;
    &lt;p&gt;samuelmaddock: Your electron-browser-shell project made it very easy to get this spun up.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45492489</guid><pubDate>Mon, 06 Oct 2025 15:32:32 +0000</pubDate></item><item><title>Nobel Prize in Physics 2025</title><link>https://www.nobelprize.org/prizes/physics/2025/popular-information/</link><description>&lt;doc fingerprint="715db15f2195d5ea"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Popular information&lt;/head&gt;
    &lt;p&gt;Popular science background: Quantum properties on a human scale (pdf)&lt;lb/&gt;Populärvetenskaplig information: Kvantegenskaper på mänsklig skala (pdf)&lt;/p&gt;
    &lt;head rend="h2"&gt;Quantum properties on a human scale&lt;/head&gt;
    &lt;p&gt;The Nobel Prize laureates in physics for 2025, John Clarke, Michel H. Devoret and John M. Martinis, used a series of experiments to demonstrate that the bizarre properties of the quantum world can be made concrete in a system big enough to be held in the hand. Their superconducting electrical system could tunnel from one state to another, as if it were passing straight through a wall. They also showed that the system absorbed and emitted energy in doses of specific sizes, just as predicted by quantum mechanics.&lt;/p&gt;
    &lt;head rend="h3"&gt;A series of groundbreaking experiments&lt;/head&gt;
    &lt;p&gt;Quantum mechanics describes properties that are significant on a scale that involves single particles. In quantum physics, these phenomena are called microscopic, even when they are much smaller than can be seen using an optical microscope. This contrasts with macroscopic phenomena, which consist of a large number of particles. For example, an everyday ball is built up of an astronomical amount of molecules and displays no quantum mechanical effects. We know that the ball will bounce back every time it is thrown at a wall. A single particle, however, will sometimes pass straight through an equivalent barrier in its microscopic world and appear on the other side. This quantum mechanical phenomenon is called tunnelling.&lt;/p&gt;
    &lt;p&gt;This year’s Nobel Prize in Physics recognises experiments that demonstrated how quantum tunnelling can be observed on a macroscopic scale, involving many particles. In 1984 and 1985, John Clarke, Michel Devoret and John Martinis conducted a series of experiments at the University of California, Berkeley. They built an electrical circuit with two superconductors, components that can conduct a current without any electrical resistance. They separated these with a thin layer of material that did not conduct any current at all. In this experiment, they showed that they could control and investigate a phenomenon in which all the charged particles in the superconductor behave in unison, as if they are a single particle that fills the entire circuit.&lt;/p&gt;
    &lt;p&gt;This particle-like system is trapped in a state in which current flows without any voltage – a state from which it does not have enough energy to escape. In the experiment, the system shows its quantum character by using tunnelling to escape the zero-voltage state, generating an electrical voltage. The laureates were also able to show that the system is quantised, which means it only absorbs or emits energy in specific amounts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tunnels and crossings&lt;/head&gt;
    &lt;p&gt;To help them, the laureates had concepts and experimental tools that had been developed over decades. Together with the theory of relativity, quantum physics is the foundation of what has come to be called modern physics, and researchers have spent the last century exploring what it entails.&lt;/p&gt;
    &lt;p&gt;Individual particles’ ability to tunnel is well known. In 1928, the physicist George Gamow realised that tunnelling is the reason why some heavy atomic nuclei tend to decay in a particular manner. The interaction between the forces in the nucleus creates a barrier around it, holding in the particles it contains. However, despite this, a small piece of the atomic nucleus can sometimes split off, move outside the barrier and escape – leaving behind a nucleus that has been transformed into another element. Without tunnelling, this type of nuclear decay could not occur.&lt;/p&gt;
    &lt;p&gt;Tunnelling is a quantum mechanical process, which entails that chance plays a role. Some types of atomic nuclei have a tall, wide barrier, so it can take a long while for a piece of the nucleus to appear outside it, while other types decay more easily. If we only look at a single atom, we cannot predict when this will happen, but by watching the decay of a large number of nuclei of the same type, we can measure an expected time before tunnelling occurs. The most common way of describing this is through the concept of half-life, which is how long it takes for half the nuclei in a sample to decay.&lt;/p&gt;
    &lt;p&gt;Physicists were quick to wonder whether it would be possible to investigate a type of tunnelling that involves more than one particle at a time. One approach to new types of experiments originated in a phenomenon that arises when some materials get extremely cold.&lt;/p&gt;
    &lt;p&gt;In an ordinary conductive material, current flows because there are electrons that are free to move through the entire material. In some materials, the individual electrons that push their way through the conductor may become organised, forming a synchronised dance that flows without any resistance. The material has become a superconductor and the electrons are joined together as pairs. These are called Cooper pairs, after Leon Cooper who, along with John Bardeen and Robert Schrieffer, provided a detailed description of how superconductors work (Nobel Prize in Physics 1972).&lt;/p&gt;
    &lt;p&gt;Cooper pairs behave completely differently to ordinary electrons. Electrons have a great deal of integrity and like to stay at a distance from each other – two electrons cannot be in the same place if they have the same properties. We can see this in an atom, for example, where the electrons divide themselves into different energy levels, called shells. However, when the electrons in a superconductor join up as pairs, they lose a bit of their individuality; while two separate electrons are always distinct, two Cooper pairs can be exactly the same. This means the Cooper pairs in a superconductor can be described as a single unit, one quantum mechanical system. In the language of quantum mechanics, they are then described as a single wave function. This wave function describes the probability of observing the system in a given state and with given properties.&lt;/p&gt;
    &lt;p&gt;If two superconductors are joined together with a thin insulating barrier between them, it creates a Josephson junction. This component is named after Brian Josephson, who performed quantum mechanical calculations for the junction. He discovered that interesting phenomena arise when the wave functions on each side of the junction are considered (Nobel Prize in Physics 1973). The Josephson junction rapidly found areas of application, including in precise measurements of fundamental physical constants and magnetic fields.&lt;/p&gt;
    &lt;p&gt;The construction also provided tools for exploring the fundamentals of quantum physics in a new way. One person who did so was Anthony Leggett (Nobel Prize in Physics 2003), whose theoretical work on macroscopic quantum tunnelling at a Josephson junction inspired new types of experiments.&lt;/p&gt;
    &lt;head rend="h3"&gt;The research group starts its work&lt;/head&gt;
    &lt;p&gt;These subjects were a perfect match for John Clarke’s research interests. He was a professor at the University of California, Berkeley, in the US, where he had moved after completing his doctoral degree at the University of Cambridge, UK, in 1968. At UC Berkeley he built up his research group and specialised in exploring a range of phenomena using superconductors and the Josephson junction.&lt;/p&gt;
    &lt;p&gt;By the mid-1980s, Michel Devoret had joined John Clarke’s research group as a postdoc, after receiving his doctorate in Paris. This group also included the doctoral student John Martinis. Together, they took on the challenge of demonstrating macroscopic quantum tunnelling. Vast amounts of care and precision were necessary to screen the experimental setup from all the interference that could affect it. They succeeded in refining and measuring all the properties of their electrical circuit, allowing them to understand it in detail.&lt;/p&gt;
    &lt;p&gt;To measure the quantum phenomena, they fed a weak current into the Josephson junction and measured the voltage, which is related to the electrical resistance in the circuit. The voltage over the Josephson junction was initially zero, as expected. This is because the wave function for the system is enclosed in a state that does not allow a voltage to arise. Then they studied how long it took for the system to tunnel out of this state, causing a voltage. Because quantum mechanics entails an element of chance, they took numerous measurements and plotted their results as graphs, from which they could read the duration of the zero-voltage state. This is similar to how measurements of the half-lives of atomic nuclei are based on statistics of numerous instances of decay.&lt;/p&gt;
    &lt;p&gt;The tunnelling demonstrates how the experimental setup’s Cooper pairs, in their synchronised dance, behave like a single giant particle. The researchers obtained further confirmation of this when they saw that the system had quantised energy levels. Quantum mechanics was named after the observation that the energy in microscopic processes is divided into separate packages, quanta. The laureates introduced microwaves of varying wavelengths into the zero-voltage state. Some of these were absorbed, and the system then moved to a higher energy level. This showed that the zero-voltage state had a shorter duration when the system contained more energy – which is exactly what quantum mechanics predicts. A microscopic particle shut behind a barrier functions in the same way.&lt;/p&gt;
    &lt;head rend="h3"&gt;Practical and theoretical benefit&lt;/head&gt;
    &lt;p&gt;This experiment has consequences for the understanding of quantum mechanics. Other types of quantum mechanical effects that are demonstrated on the macroscopic scale are composed of many tiny individual pieces and their separate quantum properties. The microscopic components are then combined to cause macroscopic phenomena such as lasers, superconductors and superfluid liquids. However, this experiment instead created a macroscopic effect – a measurable voltage – from a state that is in itself macroscopic, in the form of a common wave function for vast numbers of particles.&lt;/p&gt;
    &lt;p&gt;Theorists like Anthony Leggett have compared the laureates’ macroscopic quantum system with Erwin Schrödinger’s famous thought experiment featuring a cat in a box, where the cat would be both alive and dead if we did not look inside. (Erwin Schrödinger received the Nobel Prize in Physics 1933.) The intention of his thought experiment was to show the absurdity of this situation, because the special properties of quantum mechanics are often erased at a macroscopic scale. The quantum properties of an entire cat cannot be demonstrated in a laboratory experiment.&lt;/p&gt;
    &lt;p&gt;However, Legget has argued that the series of experiments conducted by John Clarke, Michel Devoret and John Martinis showed that there are phenomena that involve vast numbers of particles which together behave just as quantum mechanics predicts. The macroscopic system that consists of many Cooper pairs is still many orders of magnitude smaller than a kitten – but because the experiment measures the quantum mechanical properties that apply to the system as a whole, for a quantum physicist it is fairly similar to Schrödinger’s imaginary cat.&lt;/p&gt;
    &lt;p&gt;This type of macroscopic quantum state offers new potential for experiments using the phenomena that govern the microscopic world of particles. It can be regarded as a form of artificial atom on a large scale – an atom with cables and sockets that can be connected into new test set-ups or utilised in new quantum technology. For example, artificial atoms are used to simulate other quantum systems and aid in understanding them.&lt;/p&gt;
    &lt;p&gt;Another example is the quantum computer experiment subsequently performed by Martinis, in which he utilised exactly the energy quantisation that he and the other two laureates had demonstrated. He used a circuit with quantised states as information-bearing units – a quantum bit. The lowest energy state and the first step upward functioned as zero and one, respectively. Superconducting circuits are one of the techniques being explored in attempts to construct a future quantum computer.&lt;/p&gt;
    &lt;p&gt;This year’s laureates have thus contributed to both practical benefit in physics laboratories and to providing new information for the theoretical understanding of our physical world.&lt;/p&gt;
    &lt;head rend="h3"&gt;Further reading&lt;/head&gt;
    &lt;p&gt;Additional information on this year’s prizes, including a scientific background in English, is available on the website of the Royal Swedish Academy of Sciences, www.kva.se, and at www.nobelprize.org, where you can watch video from the press conferences, the Nobel Prize lectures and more. Information on exhibitions and activities related to the Nobel Prizes and the prize in economic sciences is available at www.nobelprizemuseum.se.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Physics 2025 to&lt;/head&gt;
    &lt;p&gt;JOHN CLARKE&lt;lb/&gt;Born 1942 in Cambridge, UK. PhD 1968 from University of Cambridge, UK. Professor at University of California, Berkeley, USA.&lt;/p&gt;
    &lt;p&gt;MICHEL H. DEVORET&lt;lb/&gt;Born 1953 in Paris, France. PhD 1982 from Paris-Sud University, France. Professor at Yale University, New Haven, CT and University of California, Santa Barbara, USA.&lt;/p&gt;
    &lt;p&gt;JOHN M. MARTINIS&lt;lb/&gt;Born 1958. PhD 1987 from University of Californa, Berkeley, USA. Professor at University of California, Santa Barbara, USA.&lt;/p&gt;
    &lt;p&gt;“for the discovery of macroscopic quantum mechanical tunnelling and energy quantisation in an electric circuit”&lt;/p&gt;
    &lt;p&gt;Science Editors: Ulf Danielsson, Göran Johansson and Eva Lindroth, the Nobel Committee for Physics&lt;lb/&gt;Text: Anna Davour&lt;lb/&gt;Translation: Clare Barnes&lt;lb/&gt;Illustrations: Johan Jarnestad&lt;lb/&gt;Editor: Sara Gustavsson&lt;lb/&gt;© The Royal Swedish Academy of Sciences&lt;/p&gt;
    &lt;head rend="h3"&gt;Nobel Prize announcements 2025&lt;/head&gt;
    &lt;p&gt;Don't miss the Nobel Prize announcements 6–13 October. All announcements are streamed live here on nobelprize.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45501189</guid><pubDate>Tue, 07 Oct 2025 09:50:49 +0000</pubDate></item><item><title>Canadian bill would strip internet access from 'specified persons', no warrant</title><link>https://nationalpost.com/opinion/canadian-bill-would-strip-internet-access-from-specified-persons</link><description>&lt;doc fingerprint="7fe2a1f61ea2d7f5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FIRST READING: Canadian bill would strip internet access from 'specified persons'&lt;/head&gt;
    &lt;p&gt;Not too long ago, Liberals were defending internet access as akin to a human right&lt;/p&gt;
    &lt;p&gt;First Reading is a Canadian politics newsletter curated by the National Post’s own Tristin Hopper. To get an early version sent directly to your inbox, sign up here.&lt;/p&gt;
    &lt;head rend="h3"&gt;TOP STORY&lt;/head&gt;
    &lt;p&gt;In spite of multiple international statements framing internet access as a human right, the Liberal government is pursuing legislation that would allow them to unilaterally quarantine Canadian citizens from the online world.&lt;/p&gt;
    &lt;p&gt;Bill C-8, which is now undergoing second reading in the House of Commons, includes a provision under which Ottawa can pull internet services from “any specified person.”&lt;/p&gt;
    &lt;p&gt;The denial of service would requires only the personal order of the minister of industry, a position currently filled by Mélanie Joly, in consultation with the public safety minister, a position currently filled by Gary Anandasangaree.&lt;/p&gt;
    &lt;p&gt;Bill C-8 would do this by amending the Telecommunications Act with a clause requiring telecom providers such as Rogers or Telus to pull the services of any individual customer singled out by Ottawa.&lt;/p&gt;
    &lt;p&gt;As the text states, the industry minister would be allowed to “prohibit a telecommunications service provider from providing any service to any specified person.”&lt;/p&gt;
    &lt;p&gt;None of this would require a warrant, and oversight only kicks in after the order is made.&lt;/p&gt;
    &lt;p&gt;Once a minister has ordered the internet of a “specified person” to be cut off, only then can a Federal Court subject the decision to judicial review.&lt;/p&gt;
    &lt;p&gt;Bill C-8 has been pitched by the Carney government as a way to combat “unprecedented cyber-threats.”&lt;/p&gt;
    &lt;p&gt;In the House of Commons last week, Anandasangaree, the public safety minister defended Bill C-8 as a means to crack down on hackers and ransomware fraudsters.&lt;/p&gt;
    &lt;p&gt;“Malicious cyber-actors are breaching our country’s IT systems, accessing sensitive information and putting lives in danger,” he said.&lt;/p&gt;
    &lt;p&gt;Anandasangaree added that “hostile state actors are stealing information and gaining access to systems that are critical to our national security and public safety.”&lt;/p&gt;
    &lt;p&gt;As to why a federal minister would need the power to deny internet service to specific individuals, Bill C-8 states simply that it might be “necessary to do so to secure the Canadian telecommunications system against any threat, including that of interference, manipulation, disruption or degradation.”&lt;/p&gt;
    &lt;p&gt;Critics of Bill C-8 have mostly pointed to how it gives the federal government massive new powers to collect internet data without a warrant, and without the target even being aware that they’re under surveillance.&lt;/p&gt;
    &lt;p&gt;“Bill C-8 would empower the federal government to secretly order telecom providers “to do anything or refrain from doing anything,” with no limits that would prevent such orders from being used to impose surveillance obligations on private companies, and to weaken encryption standards,” reads a critique published last week by the Canadian Civil Liberties Association.&lt;/p&gt;
    &lt;p&gt;Bill C-8 comes despite a long track record of Liberal government statements slamming government controls on internet access, deeming them an active threat to human rights.&lt;/p&gt;
    &lt;p&gt;Canada was a founding member of the Freedom Online Coalition, an international body dedicated to maintaining free and open access to the online sphere. As such, Canada has frequently signed onto joint statements decrying state control over the internet.&lt;/p&gt;
    &lt;p&gt;In 2019, for instance, Canada signed onto an FOC joint statement decrying “shrinking civic and democratic spaces online as a result of State-sponsored obstruction of free expression, peaceful assembly, and free association.”&lt;/p&gt;
    &lt;p&gt;It was just three years ago that the Liberal government released an info sheet declaring “the rights and freedoms that individuals have offline must also be protected online.”&lt;/p&gt;
    &lt;p&gt;“Canada is committed to working with international partners to protect Internet freedom, including the rights to online freedom of expression, association and peaceful assembly,” it read.&lt;/p&gt;
    &lt;p&gt;Bill C-8 is only the latest in a slew of Liberal-championed bills introducing new controls on the Canadian internet.&lt;/p&gt;
    &lt;p&gt;The Online News Act, made law in 2023, requires social media companies to compensate news outlets for any links shared on their platforms. One of the most visible impacts of the act being that Facebook simply banned the sharing of news links altogether.&lt;/p&gt;
    &lt;p&gt;The Online Streaming Act, made law around the same time, extended Canadian content controls to much of the Canadian internet. Everything from YouTube to Netflix to Canadian podcasts is now subject to federal regulations on what content qualifies as Canadian, and how it should be artificially promoted above non-Canadian offerings.&lt;/p&gt;
    &lt;p&gt;Currently, there is no mechanism by which a private citizen could have their internet services pulled by government order.&lt;/p&gt;
    &lt;p&gt;The closest analogue would probably be bail conditions under which accused criminals can be ordered not to posses a “device capable of accessing the internet.”&lt;/p&gt;
    &lt;head rend="h3"&gt;IN OTHER NEWS&lt;/head&gt;
    &lt;p&gt;It may be hard to remember, but the standards for a political scandal in Canada were once so low that it could become a term-defying disgrace for a member of the prime minister’s office to personally cover the spending overruns of a wayward senator. That’s what Nigel Wright, chief of staff to then prime minister Stephen Harper, did in 2013 when he personally paid off $90,000 worth of expenses charged by then Conservative-appointed senator Mike Duffy. Wright, who lost his job over the matter, just died at age 62 of unspecified causes.&lt;/p&gt;
    &lt;p&gt;First Reading is a Canadian politics newsletter curated by the National Post’s own Tristin Hopper. To get an early version sent directly to your inbox, sign up here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45502216</guid><pubDate>Tue, 07 Oct 2025 12:20:14 +0000</pubDate></item><item><title>The evolution of Lua, continued [pdf]</title><link>https://www.lua.org/doc/cola.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45502502</guid><pubDate>Tue, 07 Oct 2025 12:54:14 +0000</pubDate></item><item><title>Qualcomm to acquire Arduino</title><link>https://www.qualcomm.com/news/releases/2025/10/qualcomm-to-acquire-arduino-accelerating-developers--access-to-i</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45502541</guid><pubDate>Tue, 07 Oct 2025 13:00:08 +0000</pubDate></item><item><title>Vibe engineering</title><link>https://simonwillison.net/2025/Oct/7/vibe-engineering/</link><description>&lt;doc fingerprint="a3d0c07761f5138d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Vibe engineering&lt;/head&gt;
    &lt;p&gt;7th October 2025&lt;/p&gt;
    &lt;p&gt;I feel like vibe coding is pretty well established now as covering the fast, loose and irresponsible way of building software with AI—entirely prompt-driven, and with no attention paid to how the code actually works. This leaves us with a terminology gap: what should we call the other end of the spectrum, where seasoned professionals accelerate their work with LLMs while staying proudly and confidently accountable for the software they produce?&lt;/p&gt;
    &lt;p&gt;I propose we call this vibe engineering, with my tongue only partially in my cheek.&lt;/p&gt;
    &lt;p&gt;One of the lesser spoken truths of working productively with LLMs as a software engineer on non-toy-projects is that it’s difficult. There’s a lot of depth to understanding how to use the tools, there are plenty of traps to avoid, and the pace at which they can churn out working code raises the bar for what the human participant can and should be contributing.&lt;/p&gt;
    &lt;p&gt;The rise of coding agents—tools like Claude Code (released February 2025), OpenAI’s Codex CLI (April) and Gemini CLI (June) that can iterate on code, actively testing and modifying it until it achieves a specified goal, has dramatically increased the usefulness of LLMs for real-world coding problems.&lt;/p&gt;
    &lt;p&gt;I’m increasingly hearing from experienced, credible software engineers who are running multiple copies of agents at once, tackling several problems in parallel and expanding the scope of what they can take on. I was skeptical of this at first but I’ve started running multiple agents myself now and it’s surprisingly effective, if mentally exhausting!&lt;/p&gt;
    &lt;p&gt;This feels very different from classic vibe coding, where I outsource a simple, low-stakes task to an LLM and accept the result if it appears to work. Most of my tools.simonwillison.net collection (previously) were built like that. Iterating with coding agents to produce production-quality code that I’m confident I can maintain in the future feels like a different process entirely.&lt;/p&gt;
    &lt;p&gt;It’s also become clear to me that LLMs actively reward existing top tier software engineering practices:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Automated testing. If your project has a robust, comprehensive and stable test suite agentic coding tools can fly with it. Without tests? Your agent might claim something works without having actually tested it at all, plus any new change could break an unrelated feature without you realizing it. Test-first development is particularly effective with agents that can iterate in a loop.&lt;/item&gt;
      &lt;item&gt;Planning in advance. Sitting down to hack something together goes much better if you start with a high level plan. Working with an agent makes this even more important—you can iterate on the plan first, then hand it off to the agent to write the code.&lt;/item&gt;
      &lt;item&gt;Comprehensive documentation. Just like human programmers, an LLM can only keep a subset of the codebase in its context at once. Being able to feed in relevant documentation lets it use APIs from other areas without reading the code first. Write good documentation first and the model may be able to build the matching implementation from that input alone.&lt;/item&gt;
      &lt;item&gt;Good version control habits. Being able to undo mistakes and understand when and how something was changed is even more important when a coding agent might have made the changes. LLMs are also fiercely competent at Git—they can navigate the history themselves to track down the origin of bugs, and they’re better than most developers at using git bisect. Use that to your advantage.&lt;/item&gt;
      &lt;item&gt;Having effective automation in place. Continuous integration, automated formatting and linting, continuous deployment to a preview environment—all things that agentic coding tools can benefit from too. LLMs make writing quick automation scripts easier as well, which can help them then repeat tasks accurately and consistently next time.&lt;/item&gt;
      &lt;item&gt;A culture of code review. This one explains itself. If you’re fast and productive at code review you’re going to have a much better time working with LLMs than if you’d rather write code yourself than review the same thing written by someone (or something) else.&lt;/item&gt;
      &lt;item&gt;A very weird form of management. Getting good results out of a coding agent feels uncomfortably close to getting good results out of a human collaborator. You need to provide clear instructions, ensure they have the necessary context and provide actionable feedback on what they produce. It’s a lot easier than working with actual people because you don’t have to worry about offending or discouraging them—but any existing management experience you have will prove surprisingly useful.&lt;/item&gt;
      &lt;item&gt;Really good manual QA (quality assurance). Beyond automated tests, you need to be really good at manually testing software, including predicting and digging into edge-cases.&lt;/item&gt;
      &lt;item&gt;Strong research skills. There are dozens of ways to solve any given coding problem. Figuring out the best options and proving an approach has always been important, and remains a blocker on unleashing an agent to write the actual code.&lt;/item&gt;
      &lt;item&gt;The ability to ship to a preview environment. If an agent builds a feature, having a way to safely preview that feature (without deploying it straight to production) makes reviews much more productive and greatly reduces the risk of shipping something broken.&lt;/item&gt;
      &lt;item&gt;An instinct for what can be outsourced to AI and what you need to manually handle yourself. This is constantly evolving as the models and tools become more effective. A big part of working effectively with LLMs is maintaining a strong intuition for when they can best be applied.&lt;/item&gt;
      &lt;item&gt;An updated sense of estimation. Estimating how long a project will take has always been one of the hardest but most important parts of being a senior engineer, especially in organizations where budget and strategy decisions are made based on those estimates. AI-assisted coding makes this even harder—things that used to take a long time are much faster, but estimations now depend on new factors which we’re all still trying to figure out.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re going to really exploit the capabilities of these new tools, you need to be operating at the top of your game. You’re not just responsible for writing the code—you’re researching approaches, deciding on high-level architecture, writing specifications, defining success criteria, designing agentic loops, planning QA, managing a growing army of weird digital interns who will absolutely cheat if you give them a chance, and spending so much time on code review.&lt;/p&gt;
    &lt;p&gt;Almost all of these are characteristics of senior software engineers already!&lt;/p&gt;
    &lt;p&gt;AI tools amplify existing expertise. The more skills and experience you have as a software engineer the faster and better the results you can get from working with LLMs and coding agents.&lt;/p&gt;
    &lt;head rend="h4"&gt;“Vibe engineering”, really?&lt;/head&gt;
    &lt;p&gt;Is this a stupid name? Yeah, probably. “Vibes” as a concept in AI feels a little tired at this point. “Vibe coding” itself is used by a lot of developers in a dismissive way. I’m ready to reclaim vibes for something more constructive.&lt;/p&gt;
    &lt;p&gt;I’ve never really liked the artificial distinction between “coders” and “engineers”—that’s always smelled to me a bit like gatekeeping. But in this case a bit of gatekeeping is exactly what we need!&lt;/p&gt;
    &lt;p&gt;Vibe engineering establishes a clear distinction from vibe coding. It signals that this is a different, harder and more sophisticated way of working with AI tools to build production software.&lt;/p&gt;
    &lt;p&gt;I like that this is cheeky and likely to be controversial. This whole space is still absurd in all sorts of different ways. We shouldn’t take ourselves too seriously while we figure out the most productive ways to apply these new tools.&lt;/p&gt;
    &lt;p&gt;I’ve tried in the past to get terms like AI-assisted programming to stick, with approximately zero success. May as well try rubbing some vibes on it and see what happens.&lt;/p&gt;
    &lt;p&gt;I also really like the clear mismatch between “vibes” and “engineering”. It makes the combined term self-contradictory in a way that I find mischievous and (hopefully) sticky.&lt;/p&gt;
    &lt;head rend="h2"&gt;More recent articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenAI DevDay 2025 live blog - 6th October 2025&lt;/item&gt;
      &lt;item&gt;Embracing the parallel coding agent lifestyle - 5th October 2025&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45503867</guid><pubDate>Tue, 07 Oct 2025 14:55:14 +0000</pubDate></item><item><title>Launch HN: LlamaFarm (YC W22) – Open-source framework for distributed AI</title><link>https://github.com/llama-farm/llamafarm</link><description>&lt;doc fingerprint="ed1fa4511bbd11f2"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;Build powerful AI locally, extend anywhere.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;LlamaFarm is an open-source framework for building retrieval-augmented and agentic AI applications. It ships with opinionated defaults (Ollama for local models, Chroma for vector storage) while staying 100% extendable—swap in vLLM, remote OpenAI-compatible hosts, new parsers, or custom stores without rewriting your app.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local-first developer experience with a single CLI (&lt;code&gt;lf&lt;/code&gt;) that manages projects, datasets, and chat sessions.&lt;/item&gt;
      &lt;item&gt;Production-ready architecture that mirrors server endpoints and enforces schema-based configuration.&lt;/item&gt;
      &lt;item&gt;Composable RAG pipelines you can tailor through YAML, not bespoke code.&lt;/item&gt;
      &lt;item&gt;Extendable everything: runtimes, embedders, databases, extractors, and CLI tooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;📺 Video demo (90 seconds): https://youtu.be/W7MHGyN0MdQ&lt;/p&gt;
    &lt;p&gt;Prerequisites:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install the CLI&lt;/p&gt;
        &lt;p&gt;macOS / Linux&lt;/p&gt;
        &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/llama-farm/llamafarm/main/install.sh | bash&lt;/code&gt;
        &lt;p&gt;Windows (via winget)&lt;/p&gt;
        &lt;code&gt;winget install LlamaFarm.CLI&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Adjust Ollama context window&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Open the Ollama app, go to Settings → Advanced, and set the context window to match production (e.g., 100K tokens).&lt;/item&gt;
          &lt;item&gt;Larger context windows improve RAG answers when long documents are ingested.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create and run a project&lt;/p&gt;
        &lt;quote&gt;lf init my-project # Generates llamafarm.yaml using the server template lf start # Spins up Docker services &amp;amp; opens the dev chat UI&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Start an interactive project chat or send a one-off message&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Interactive project chat (auto-detects namespace/project from llamafarm.yaml)
lf chat

# One-off message
lf chat "Hello, LlamaFarm!"&lt;/code&gt;
    &lt;p&gt;Need the full walkthrough with dataset ingestion and troubleshooting tips? Jump to the Quickstart guide.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Prefer building from source? Clone the repo and follow the steps in Development &amp;amp; Testing.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Run services manually (without Docker auto-start):&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/llama-farm/llamafarm.git
cd llamafarm

# Install Nx globally and bootstrap the workspace
npm install -g nx
nx init --useDotNxInstallation --interactive=false

# Option 1: start both server and RAG worker with one command
nx dev

# Option 2: start services in separate terminals
# Terminal 1
nx start rag
# Terminal 2
nx start server&lt;/code&gt;
    &lt;p&gt;Open another terminal to run &lt;code&gt;lf&lt;/code&gt; commands (installed or built from source). This is equivalent to what &lt;code&gt;lf start&lt;/code&gt; orchestrates automatically.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Own your stack – Run small local models today and swap to hosted vLLM, Together, or custom APIs tomorrow by changing &lt;code&gt;llamafarm.yaml&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Battle-tested RAG – Configure parsers, extractors, embedding strategies, and databases without touching orchestration code.&lt;/item&gt;
      &lt;item&gt;Config over code – Every project is defined by YAML schemas that are validated at runtime and easy to version control.&lt;/item&gt;
      &lt;item&gt;Friendly CLI – &lt;code&gt;lf&lt;/code&gt;handles project bootstrapping, dataset lifecycle, RAG queries, and non-interactive chats.&lt;/item&gt;
      &lt;item&gt;Built to extend – Add a new provider or vector store by registering a backend and regenerating schema types.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Task&lt;/cell&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Initialize a project&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf init my-project&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Creates &lt;code&gt;llamafarm.yaml&lt;/code&gt; from server template.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Start dev stack + chat TUI&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf start&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Spins up server, rag worker, monitors Ollama/vLLM.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Interactive project chat&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf chat&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Opens TUI using project from &lt;code&gt;llamafarm.yaml&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Send single prompt&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf chat "Explain retrieval augmented generation"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Uses RAG by default; add &lt;code&gt;--no-rag&lt;/code&gt; for pure LLM.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Preview REST call&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf chat --curl "What models are configured?"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Prints sanitized &lt;code&gt;curl&lt;/code&gt; command.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Create dataset&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf datasets create -s pdf_ingest -b main_db research-notes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Validates strategy/database against project config.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Upload files&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf datasets upload research-notes ./docs/*.pdf&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Supports globs and directories.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Process dataset&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf datasets process research-notes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Streams heartbeat dots during long processing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Semantic query&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lf rag query --database main_db "What did the 2024 FDA letters require?"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Use &lt;code&gt;--filter&lt;/code&gt;, &lt;code&gt;--include-metadata&lt;/code&gt;, etc.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;See the CLI reference for full command details and troubleshooting advice.&lt;/p&gt;
    &lt;p&gt;LlamaFarm provides a comprehensive REST API (compatible with OpenAI's format) for integrating with your applications. The API runs at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Chat Completions (OpenAI-compatible)&lt;/p&gt;
    &lt;code&gt;curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "What are the FDA requirements?"}
    ],
    "stream": false,
    "rag_enabled": true,
    "database": "main_db"
  }'&lt;/code&gt;
    &lt;p&gt;RAG Query&lt;/p&gt;
    &lt;code&gt;curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/rag/query \
  -H "Content-Type: application/json" \
  -d '{
    "query": "clinical trial requirements",
    "database": "main_db",
    "top_k": 5
  }'&lt;/code&gt;
    &lt;p&gt;Dataset Management&lt;/p&gt;
    &lt;code&gt;# Upload file
curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/datasets/{dataset}/data \
  -F "file=@document.pdf"

# Process dataset
curl -X POST http://localhost:8000/v1/projects/{namespace}/{project}/datasets/{dataset}/process&lt;/code&gt;
    &lt;p&gt;Check your &lt;code&gt;llamafarm.yaml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;name: my-project        # Your project name
namespace: my-org       # Your namespace&lt;/code&gt;
    &lt;p&gt;Or inspect the file system: &lt;code&gt;~/.llamafarm/projects/{namespace}/{project}/&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;See the complete API Reference for all endpoints, request/response formats, Python/TypeScript clients, and examples.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;llamafarm.yaml&lt;/code&gt; is the source of truth for each project. The schema enforces required fields and documents every extension point.&lt;/p&gt;
    &lt;code&gt;version: v1
name: fda-assistant
namespace: default

runtime:
  provider: openai                   # "openai" for any OpenAI-compatible host, "ollama" for local Ollama
  model: qwen2.5:7b
  base_url: http://localhost:8000/v1 # Point to vLLM, Together, etc.
  api_key: sk-local-placeholder
  instructor_mode: tools             # Optional: json, md_json, tools, etc.

prompts:
  - role: system
    content: &amp;gt;-
      You are an FDA specialist. Answer using short paragraphs and cite document titles when available.

rag:
  databases:
    - name: main_db
      type: ChromaStore
      default_embedding_strategy: default_embeddings
      default_retrieval_strategy: filtered_search
      embedding_strategies:
        - name: default_embeddings
          type: OllamaEmbedder
          config:
            model: nomic-embed-text:latest
      retrieval_strategies:
        - name: filtered_search
          type: MetadataFilteredStrategy
          config:
            top_k: 5
  data_processing_strategies:
    - name: pdf_ingest
      parsers:
        - type: PDFParser_LlamaIndex
          config:
            chunk_size: 1500
            chunk_overlap: 200
      extractors:
        - type: HeadingExtractor
        - type: ContentStatisticsExtractor

datasets:
  - name: research-notes
    data_processing_strategy: pdf_ingest
    database: main_db&lt;/code&gt;
    &lt;p&gt;Configuration reference: Configuration Guide • Extending LlamaFarm&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swap runtimes by pointing to any OpenAI-compatible endpoint (vLLM, Mistral, Anyscale). Update &lt;code&gt;runtime.provider&lt;/code&gt;,&lt;code&gt;base_url&lt;/code&gt;, and&lt;code&gt;api_key&lt;/code&gt;; regenerate schema types if you add a new provider enum.&lt;/item&gt;
      &lt;item&gt;Bring your own vector store by implementing a store backend, adding it to &lt;code&gt;rag/schema.yaml&lt;/code&gt;, and updating the server service registry.&lt;/item&gt;
      &lt;item&gt;Add parsers/extractors to support new file formats or metadata pipelines. Register implementations and extend the schema definitions.&lt;/item&gt;
      &lt;item&gt;Extend the CLI with new Cobra commands under &lt;code&gt;cli/cmd&lt;/code&gt;; the docs include guidance on adding dataset utilities or project tooling.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Check the Extending guide for step-by-step instructions.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
        &lt;cell role="head"&gt;What it Shows&lt;/cell&gt;
        &lt;cell role="head"&gt;Location&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FDA Letters Assistant&lt;/cell&gt;
        &lt;cell&gt;Multi-document PDF ingestion, RAG queries, reference-style prompts&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;examples/fda_rag/&lt;/code&gt; &amp;amp; Docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Raleigh UDO Planning Helper&lt;/cell&gt;
        &lt;cell&gt;Large ordinance ingestion, long-running processing tips, geospatial queries&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;examples/gov_rag/&lt;/code&gt; &amp;amp; Docs&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run &lt;code&gt;lf datasets&lt;/code&gt; and &lt;code&gt;lf rag query&lt;/code&gt; commands from each example folder to reproduce the flows demonstrated in the docs.&lt;/p&gt;
    &lt;code&gt;# Python server + RAG tests
cd server
uv sync
uv run --group test python -m pytest

# CLI tests
cd ../cli
go test ./...

# RAG tooling smoke tests
cd ../rag
uv sync
uv run python cli.py test

# Docs build (ensures navigation/link integrity)
cd ..
nx build docs&lt;/code&gt;
    &lt;p&gt;Linting: &lt;code&gt;uv run ruff check --fix .&lt;/code&gt; (Python), &lt;code&gt;go fmt ./...&lt;/code&gt; and &lt;code&gt;go vet ./...&lt;/code&gt; (Go).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Discord – chat with the team, share feedback, find collaborators.&lt;/item&gt;
      &lt;item&gt;GitHub Issues – bug reports and feature requests.&lt;/item&gt;
      &lt;item&gt;Discussions – ideas, RFCs, roadmap proposals.&lt;/item&gt;
      &lt;item&gt;Contributing Guide – code style, testing expectations, doc updates, schema regeneration steps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Want to add a new provider, parser, or example? Start a discussion or open a draft PR—we love extensions!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Licensed under the Apache 2.0 License.&lt;/item&gt;
      &lt;item&gt;Built by the LlamaFarm community and inspired by the broader open-source AI ecosystem. See CREDITS for detailed acknowledgments.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Build locally. Deploy anywhere. Own your AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45504388</guid><pubDate>Tue, 07 Oct 2025 15:30:20 +0000</pubDate></item><item><title>IKEA Catalogs 1951-2021</title><link>https://ikeamuseum.com/en/explore/ikea-catalogue/</link><description>&lt;doc fingerprint="71c3125f48ed4455"&gt;
  &lt;main&gt;
    &lt;p&gt;Good question! We know that a lot of people are curious about what the IKEA catalogue has looked like through the ages. The catalogue has always reflected the age and its views on interior design and everyday living, especially in Sweden, but in recent decades also internationally. The catalogue was in print for 70 years, and by digitising all the catalogues we could make them available to everybody. Making the story of IKEA available to as many people as possible is our main task at IKEA Museum. So we hope that the catalogues will bring some joy and nostalgia, and maybe even a few surprises.&lt;/p&gt;
    &lt;head rend="h1"&gt;IKEA catalogue&lt;/head&gt;
    &lt;p&gt;For over 70 years, the IKEA catalogue was produced in Ãlmhult, constantly growing in number, scope and distribution. From the 1950s when Ingvar Kamprad wrote most of the texts himself, via the poppy, somewhat radical 1970s and all the way into the scaled-down 2000s â the IKEA catalogue always captured the spirit of the time. The 2021 IKEA catalogue was the very last one printed on paper.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1950s&lt;/item&gt;
      &lt;item&gt;1960s&lt;/item&gt;
      &lt;item&gt;1970s&lt;/item&gt;
      &lt;item&gt;1980s&lt;/item&gt;
      &lt;item&gt;1990s&lt;/item&gt;
      &lt;item&gt;2000s&lt;/item&gt;
      &lt;item&gt;2010s&lt;/item&gt;
      &lt;item&gt;2020s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Just like the perception of the home, the catalogue has changed dramatically since 1951, when it was first published. Look in the older catalogues and youâll be amazed at what you find. In fact, youâll probably even have a giggle or two. In the 1950s and 1960s, there are rarely any people in the pictures, and never any children. But in the 1970s there are children playing all over the home, you can see adults smoking and even the occasional political poster on the wall. Browse on to the 1980s IKEA catalogues and the trends have changed again, with shiny fabrics and other fancy materials. In the 1990s homes become more scaled-down and clearly inspired by a Scandinavian tradition. In this way, the IKEA catalogues are a kind of time capsule for you to travel in. And who knows? When we look back at the most recent catalogues in 10 or 20 yearsâ time, weâll probably shake our heads and give a sigh.&lt;/p&gt;
    &lt;p&gt;IKEA Museum decided to start with the Swedish catalogue as it has been around the longest. In the future, we hope to be able to digitise catalogues from more countries in more languages.&lt;/p&gt;
    &lt;p&gt;No. The IKEA catalogue has always only shown a selection of whatâs available in the stores. The catalogues from the 1970s and onwards show around 30â50 per cent of the entire range. The products that are not featured are generally smaller ones in textiles, decorations and lighting. Temporary collections are rarely included either. But the farther back you go, the higher a percentage of the range can be found in the catalogue.&lt;/p&gt;
    &lt;p&gt;Yes, but the older a product is, the harder it may be to find information about it. If you have a specific question about a product, weâre happy to help you out if we can. But 70 years is a long time, so we canât promise anything. While youâre waiting for our response you can always browse through the catalogues â the product texts that are there are quite detailed. You can search in the catalogues by product name and product type. There are also various stories about different products on our site, and more are constantly being added.&lt;lb/&gt; Browse through stories about IKEA products from 7 decades. &lt;/p&gt;
    &lt;p&gt;IKEA was founded in the 1940s, so why are you showing no catalogues from before 1951?&lt;lb/&gt; The first catalogue did not come out until 1951. Before that, IKEA was a mail order company that didnât sell furniture, but pens, clocks, electric razors, wallets and bags. At that time, the range was only presented in a small mail order brochure called ikÃ©a-nytt (literally ikÃ©a news). Sometimes it was distributed as a supplement in farming paper Jordbrukarnas FÃ¶reningsblad, which reached hundreds of thousands of people in the Swedish countryside. From autumn 1948 Ingvar Kamprad started including furniture in the range, and things quickly grew from there. In the 1950 ikÃ©a-nytt, as many as six of the 18 pages featured furniture. And when you look at the 1951 catalogue, youâll see that there are no more pens and wallets. Ingvar Kamprad was now truly focusing on home furnishing, and shelving the rest.&lt;lb/&gt; Browse through all issues of ikÃ©a-nytt.&lt;/p&gt;
    &lt;p&gt;Not really. We do have a few copies of each yearâs IKEA catalogue in our archives, which weâre saving for posterity. They should be handled as little as possible to keep them in good condition, so weâve made the catalogues available digitally, both online and on monitors at IKEA Museum. You can browse through those as much as you like!&lt;/p&gt;
    &lt;p&gt;Yes you can. The easiest way to share the catalogues is to click on the arrow at the bottom left corner for each catalogue, or in the left-hand menu once youâve started browsing through. This will copy a link which you can share on a website or social media. If you would like to download and publish on your own digital platform, you can share a maximum of three complete digital catalogues. Donât forget to state the copyright details, “Â© Inter IKEA Systems B.V.”, the catalogue year, and the link /en/explore/ikea-catalogue/ so that anyone interested can find out more. You may not publish the digital catalogues for commercial purposes.&lt;/p&gt;
    &lt;p&gt;Absolutely! You can share up to 30 images from the catalogues on your own digital platform, such as a blog, on Instagram or similar (as long as itâs not for commercial purposes). Donât forget to state the copyright details, “Â© Inter IKEA Systems B.V.”, the catalogue year, and the link /en/explore/ikea-catalogue/ so that anyone interested can find out more.&lt;/p&gt;
    &lt;p&gt;Yes! You can find all press material, including images, information about current exhibitions and much more, in the IKEA Museum press room.&lt;/p&gt;
    &lt;p&gt;At the moment we have a good amount of catalogues in all languages at the museum, and do not need any more. Having said that, please contact us anyway if youâve been collecting catalogues for several decades, or if you have any other material you think might be of interest to IKEA Museum.&lt;/p&gt;
    &lt;p&gt;Unfortunately not. We sometimes wish we did, as we handle quite a lot of old products that may need putting together and taking apart.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45504470</guid><pubDate>Tue, 07 Oct 2025 15:35:48 +0000</pubDate></item><item><title>Show HN: Timelinize – Privately organize your own data from everywhere, locally</title><link>https://timelinize.com</link><description>&lt;doc fingerprint="e64842e6eda1dcc1"&gt;
  &lt;main&gt;
    &lt;p&gt;Timelinize ("time-lynn-eyes") is an open source personal archival suite, designed for modern family history. It organizes all your data onto a single, unified timeline on your own computer.&lt;/p&gt;
    &lt;p&gt;Photos, videos, text messages, locations, chats, social media, and more. Timelinize unifies it all.&lt;/p&gt;
    &lt;p&gt;By adding all your data, Timelinize documents your family's life with more detail and privacy, and gives you a more complete view of your story, than standard photo library and journaling apps.&lt;/p&gt;
    &lt;p&gt;Most apps store your data "in the cloud" and out of your control. What if you lost access to your Google/Apple/Facebook accounts, or your phone? By bringing that data home to your own computer, Timelinize preserves a richer story than any one app or service can do alone.&lt;/p&gt;
    &lt;p&gt;Timelinize isn't a replacement for the apps and services you already use, so you don't need to disrupt your way of life. Instead, it "sits behind" what you already use to become the permanent private archive of your working copy from:&lt;/p&gt;
    &lt;p&gt;With several projections for your data, it's easy to keep moments alive that would otherwise be forgotten, rotting on a hard drive in your closet... or in a bigcorp's cloud.&lt;/p&gt;
    &lt;p&gt;The timeline view semantically groups all your data into a single linear layout. Easily see what occurred on a specific day in the order it happened.&lt;/p&gt;
    &lt;p&gt;Visualize your data on a huge, beautiful map of the world that plots points when and where they happened, even for data that doesn't have coordinates (like text messages and emails).&lt;/p&gt;
    &lt;p&gt;Follow connections with people across all kinds of chats and messages. Combine conversations with people across platforms into one view.&lt;/p&gt;
    &lt;p&gt;Browse through a rich display of photos and videos from photo libraries, messages sent and received, and other sources.&lt;/p&gt;
    &lt;p&gt;Add millions of data points to your timeline in a matter of minutes. You get full control over background jobs like imports, thumbnails, and embeddings.&lt;/p&gt;
    &lt;p&gt;Timelinize supports playing "live photos" (or "motion photos") for photos taken on Apple, Google, and Samsung devices.&lt;/p&gt;
    &lt;p&gt;Timelinize specializes in combining data from multiple sets and sources. It can identify people and other entities across data sources by their attributes. If a person or contact appears in multiple data sets, it will automatically merge them if possible. If not, you can easily merge entities with the click of a button.&lt;/p&gt;
    &lt;p&gt;Because Timelinize is entity-aware, it can project data points onto a map even without coordinate data. If a geolocated point is known for an entity around the same time of others of that entity's data points, it will appear on the map.&lt;/p&gt;
    &lt;p&gt;Customize the map to change its theme, layers, and even make it 3D.&lt;/p&gt;
    &lt;p&gt;The heatmap shows where your data is concentrated. It smoothly blends as you zoom in and out.&lt;/p&gt;
    &lt;p&gt;Customize what defines a duplicate item, and how to handle that, with a fine degree of control—perfect for merging separate, disparate data sets.&lt;/p&gt;
    &lt;p&gt;An implicit conversation is discovered when a data source links items and entities with a "sent to" relation. You can easily view conversations between entities across modalities in a single scroll: chats, emails, messages, texts, and more.&lt;/p&gt;
    &lt;p&gt;Since I need this to function well for my own family, I have tried to give special attention to less-visible aspects of this application, such as:&lt;/p&gt;
    &lt;p&gt;Timelinize deduplicates, denoises, clusters, and simplifies location data for optimal preservation, with an algorithm that subjectively performs better than Google Maps Timeline.&lt;/p&gt;
    &lt;p&gt;For nerds like me: you can use Timelinize through its CLI, which mirrors all the functions of the HTTP API used by the frontend.&lt;/p&gt;
    &lt;p&gt;Search for pictures and messages by describing them, or find similar items to what you're viewing.&lt;/p&gt;
    &lt;p&gt;All items are stored verbatim, then thumbnails are generated for all images and video media, which are stored separately. Your original data is not modified.&lt;/p&gt;
    &lt;p&gt;The database schema has been meticulously designed and refined to be as adaptable as possible.&lt;/p&gt;
    &lt;p&gt;Timelinize will continue to develop and evolve. In the future, I anticipate the following capabilities:&lt;/p&gt;
    &lt;p&gt;Annotate your timeline, write rich stories with live embeddings from your timeline data, or make physical media like photo books (but with more than just photos!).&lt;/p&gt;
    &lt;p&gt;Add context to your timeline with additional public timelines which have weather, local/regional news, and global events.&lt;/p&gt;
    &lt;p&gt;Securely and privately share parts of your timeline with trusted friends and family members, directly from your computer to theirs.&lt;/p&gt;
    &lt;p&gt;Right now, Timelinize sits "behind" the apps and platforms you already use. But in the future, you could sync data directly to your timeline as it is originated.&lt;/p&gt;
    &lt;p&gt;Collect your data from various sources. Import it with a few clicks. Within minutes, explore millions of your data points in several intuitive ways.&lt;/p&gt;
    &lt;p&gt;Imported data is copied into your timeline folder, ensuring long-term stability and integrity. Timelines are portable—you can copy them or move them to other devices and computers.&lt;/p&gt;
    &lt;p&gt;Your timeline is simply a folder on disk containing a SQLite database alongside your data files. You can freely explore it with other tooling, so you're not locked into Timelinize.&lt;/p&gt;
    &lt;p&gt;Unlike writing a journal, you don't have to take extra time to create content. You're already making the data your timeline can display! And it doesn't replace your current workflow or apps.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45504973</guid><pubDate>Tue, 07 Oct 2025 16:10:22 +0000</pubDate></item><item><title>Seeing like a software company</title><link>https://www.seangoedecke.com/seeing-like-a-software-company/</link><description>&lt;doc fingerprint="2615d9142c2d05e0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Seeing like a software company&lt;/head&gt;
    &lt;p&gt;The big idea of James C. Scott’s Seeing Like A State can be expressed in three points:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Modern organizations exert control by maximising “legibility”: by altering the system so that all parts of it can be measured, reported on, and so on.&lt;/item&gt;
      &lt;item&gt;However, these organizations are dependent on a huge amount of “illegible” work: work that cannot be tracked or planned for, but is nonetheless essential.&lt;/item&gt;
      &lt;item&gt;Increasing legibility thus often actually lowers efficiency - but the other benefits are high enough that organizations are typically willing to do so regardless.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By “legible”, I mean work that is predictable, well-estimated, has a paper trail, and doesn’t depend on any contingent factors (like the availability of specific people). Quarterly planning, OKRs, and Jira all exist to make work legible. Illegible work is everything else: asking for and giving favors, using tacit knowledge that isn’t or can’t be written down, fitting in unscheduled changes, and drawing on interpersonal relationships. As I’ll argue, tech companies need to support both of these kinds of work.&lt;/p&gt;
    &lt;p&gt;Thinking in terms of legibility and illegibility explains so many of the things that are confusing about large software companies. It explains why companies do many things that seem obviously counter-productive, why the rules in practice are so often out of sync with the rules as written, and why companies are surprisingly willing to tolerate rule-breaking in some contexts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Seeing like a state&lt;/head&gt;
    &lt;p&gt;James C. Scott was writing about the “high modernist” movement in governance that produced (among other things) the tidy German forests of the 19th century1. In order to produce wood at scale, the German state demanded legibility: forests that an inspector could visit to tally up the amount of healthy trees. That means that you must be able to walk through the forest - i.e. the underbrush must be controlled - and the trees ought to be ideally laid out in neat rows of a single type.&lt;/p&gt;
    &lt;p&gt;Proponents of legibility often describe their processes as “efficiency measures” or ways to “avoid waste”. But overall, the new “efficient” forests were in fact far less efficient than the old, illegible forests. They produced less wood per year and required more effort to fight disease, because the underbrush proved surprisingly load-bearing to the health of the soil, and the variety of species turned out to have been an asset. The new homogeneous forests could be wiped out by a single parasite or disease in a way that the older, more varied forests could not.&lt;/p&gt;
    &lt;p&gt;However, the advantages of legibility are enormous. Once you know exactly how many trees you have, you can plan ahead, make large trade deals, avoid graft, and so on. To me, this is the most interesting point Scott makes. Large organizations did genuinely think that more legibility would necessarily increase efficiency2. But even when it became clear that that was false, those organizations continued pushing for legibility anyway, because the other advantages were too powerful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Seeing like a software company&lt;/head&gt;
    &lt;p&gt;It’s the same way in software companies. It’s almost a truism among software engineers that a single engineer can be more efficient alone than they can by working as part of a team. That’s why there are so many anecdotes about engineers taking leave to finally get some work done, or about productive work being done on nights and weekends.&lt;/p&gt;
    &lt;p&gt;Likewise, it should be obvious to any practicing engineer that engineer-driven work goes far more swiftly than work that is mandated from above. Engineer-driven work doesn’t need to be translated into something that makes sense, doesn’t need to be actively communicated in all directions, and can in general just be done in the most straightforward and efficient way.&lt;/p&gt;
    &lt;p&gt;This is why tiny software companies are often much better than large software companies at delivering software: it doesn’t matter that the large company is throwing ten times the number of engineers at the problem if the small company is twenty times more efficient3.&lt;/p&gt;
    &lt;p&gt;Why don’t large companies react to this by doing away with all of their processes? Are they stupid? No. The processes that slow engineers down are the same processes that make their work legible to the rest of the company. And that legibility (in dollar terms) is more valuable than being able to produce software more efficiently.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why legibility is valuable to tech companies&lt;/head&gt;
    &lt;p&gt;What does legibility mean to a tech company, in practice? It means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The head of a department knows, to the engineer, all the projects the department is currently working on&lt;/item&gt;
      &lt;item&gt;That head also knows (or can request) a comprehensive list of all the projects the department has shipped in the last quarter&lt;/item&gt;
      &lt;item&gt;That head has the ability to plan work at least one quarter ahead (ideally longer)&lt;/item&gt;
      &lt;item&gt;That head can, in an emergency, direct the entire resources of the department at immediate work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that “shipping high quality software” or “making customers happy” or even “making money” is not on this list. Those are all things tech companies want to do, but they’re not legibility.&lt;/p&gt;
    &lt;p&gt;Our small-but-efficient software company meets only one of these criteria: the ability to pivot to some immediate problem that needs solving. The other information is all locked up in various engineers’ heads, who may or may not remember what they did two months ago (and who certainly won’t be willing to commit to work two months from now). That’s not necessarily a problem, so long as everyone’s on the same page about what needs doing and the product is continuing to improve.&lt;/p&gt;
    &lt;p&gt;A typical large software company meets almost all of these criteria - I say almost, because in some companies or departments the ability to direct immediate work has atrophied (more on that later). But aside from that, large companies are usually very good at cataloguing what is being worked on, remembering what’s been shipped in the past, and planning work in the medium-to-long-term.&lt;/p&gt;
    &lt;p&gt;Why are these capabilities so valuable to a large software company, when small software companies can do without them? This is leaving my area of expertise somewhat, but I’m pretty sure the main answer is large enterprise deals. Making deals with large enterprise customers is fantastically profitable. Any sufficiently large SaaS will thus pivot from small customers to enterprise customers, if it can4. But enterprise deals (a) can take many, many months to set up, and (b) require making long-term feature commitments. An illegible company is not configured to be able to stick with a boring enterprise deal for many months, constantly answering questions and delivering features. Large enterprise customers simply won’t trust a small software company to deliver the things they need over the next year or two.&lt;/p&gt;
    &lt;p&gt;Customers like this typically value legibility very highly, and so demand that their vendors also be legible. In fact, highly legible organizations struggle to communicate at all with organizations that are less legible (and vice versa). They don’t have access to the right bona fides, they don’t talk the same language, and so on. This puts real pressure on growing tech companies to become more legible, even if it hurts their ability to deliver software.&lt;/p&gt;
    &lt;head rend="h3"&gt;Legible assumptions&lt;/head&gt;
    &lt;p&gt;In the pursuit of legibility, large tech companies make simplifying assumptions about the nature of tech work. For instance, they assume:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Any engineers with the same job title perform roughly the same.&lt;/item&gt;
      &lt;item&gt;Engineers can be shuffled and reorganized without substantial loss of productivity.&lt;/item&gt;
      &lt;item&gt;A team will maintain the same level of productivity over time, if it has the same number of engineers.&lt;/item&gt;
      &lt;item&gt;Projects can be estimated ahead of time, albeit with some margin for error. The more time spent estimating a project, the more accurate the estimate will become.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, all of these are false. Within the same job title, there is significant variance in engineering ability. Engineers have different skillsets and interests, and will work much more productively on projects that are a good fit for them. Because of this, the productivity of a team has a weak relationship to the number of engineers on the team.&lt;/p&gt;
    &lt;p&gt;Project estimates are largely fantasy. More accurately, they’re performative: the initial estimate determines the kind of engineering work that gets done to deliver by that estimate, not the other way around. For this reason, breaking down a project into parts and estimating each part often delivers a less accurate estimate, because it makes it harder for engineers to align with the overall ship date.&lt;/p&gt;
    &lt;p&gt;However, these assumptions are true enough for their purpose, which is to provide legibility to the executives in charge of the company. Whether the project estimate is accurate or not, it can be used to plan and to communicate with other large organizations (who are themselves typically aware that these estimates ought not to be taken completely seriously).&lt;/p&gt;
    &lt;head rend="h3"&gt;Temporary sanctioned zones of illegibility&lt;/head&gt;
    &lt;p&gt;I mentioned above that large companies sometimes lose the ability to prioritize immediate work. This is because the processes that make work legible also impose a serious delay. Consider the steps that a hypothetical large company might take before beginning to write code on a problem:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Somebody has a product idea.&lt;/item&gt;
      &lt;item&gt;They take that idea to the Product org, where it goes into the “planning” stage. Meetings are had about the idea.&lt;/item&gt;
      &lt;item&gt;Once the Product org formally decide they want to do it, the idea then passes to the Engineering org: into the hands of some council of engineering architects, who are tasked with the initial technical review. They figure out how it fits into the general engineering priorities and give it a very rough time estimate.&lt;/item&gt;
      &lt;item&gt;The VPs and senior managers in the engineering org then negotiate which team will own the work. Often this is a semi-technical, semi-organizational decision (because which service the work should fall into is at least partly a technical question).&lt;/item&gt;
      &lt;item&gt;Finally the work lands on the team. It enters the team planning backlog, where the team technical lead breaks it out into smaller pieces of work.&lt;/item&gt;
      &lt;item&gt;Those smaller pieces of work enter the team ticket backlog, and are estimated in the team’s weekly planning meeting.&lt;/item&gt;
      &lt;item&gt;Finally some of those pieces of work make it into the next sprint, and are picked up by an engineer who can actually do it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m leaving out many crucial parts of this process: the updates on each ticket, which then roll up to higher levels of management, legal and design review, which can themselves take weeks, and then the final steps involved in shipping the change to customers. All of this makes the work very legible, but none of this is compatible with work that has to be done right now. What do you do when there’s a sudden, urgent technical problem - maybe you’re about to overflow your &lt;code&gt;int&lt;/code&gt; ID column on the users table, or some very large customer is experiencing a show-stopping bug?5&lt;/p&gt;
    &lt;p&gt;To solve this kind of problem, tech companies often reserve the right to create temporary zones where illegible work is allowed. Sometimes these are called “virtual teams”, or “strike teams” (or even the colourful name “tiger teams”). They are composed of hand-picked engineers who are trusted by the organization. Often there is no manager assigned at all, but instead some very senior engineer who’s tasked with running the project. These teams are given a loose mandate - like “stop the database from falling over every few days” - and allowed to do basically whatever it takes to get it done.&lt;/p&gt;
    &lt;p&gt;This is a smart compromise between complete illegibility, which as I discussed above would make the company unable to make deals with its richest customers, and complete legibility, which would force even urgent company-killing issues to go through the entire laborious process of scoping, planning and estimating.&lt;/p&gt;
    &lt;p&gt;Even when siloed to a temporary team, sanctioned illegibility still coexists awkwardly with the rest of the organization. Engineers outside the team don’t like seeing other engineers given the freedom to work without the burden of process: either because they’re jealous, or because they’re believers in process and think that such work is unacceptably dangerous. Managers also don’t like extending that level of trust. That’s why sanctioned efforts like this are almost always temporary. The majority of the illegible work that occurs in large organizations is still unsanctioned.&lt;/p&gt;
    &lt;head rend="h3"&gt;Permanent zones of unsanctioned illegibility&lt;/head&gt;
    &lt;p&gt;If you’re an engineer on team A, and you need team B to do some kind of work for you, the formal way to do this is to create an issue in their “planning” backlog and wait for it to go through the entire twelve-step process before it finally makes its way into one of their sprints, where hopefully somebody will pick it up and do it. This can take weeks to months. When what you want is a one-line change, it’s incredibly frustrating to watch your requested work item go through all these steps - each one of which takes many times longer than it would take to simply do the work.&lt;/p&gt;
    &lt;p&gt;The official way around this problem is that team A should anticipate in their planning process that team B will need to do this work, so that piece for team B can enter their backlog at the same time as it enters team A’s backlog. That way (in theory) they should be complete at around the same time6. Any practicing software engineer knows how ridiculous this idea is. You can never anticipate every change that has to be made months before you start writing code.&lt;/p&gt;
    &lt;p&gt;The actual way around this problem is illegible backchannels. An engineer on team A reaches out to an engineer on team B asking “hey, can you make this one-line change for me”. That engineer on team B then does it immediately, maybe creating a ticket, maybe not. Then it’s done! This works great, but it’s illegible because the company can’t expect it or plan for it - it relies on the interpersonal relationships between engineers on different teams, which are very difficult to quantify. If you’re a well-liked engineer, your ability to pull on these backchannels is significantly greater than if you’re brand-new or have a bad reputation. But how well-liked you are is not something companies can officially use when they’re planning projects.&lt;/p&gt;
    &lt;p&gt;Backchannels are a constant presence at all levels of the company. As well as engineer-engineer cross-team backchannels, there are backchannels inside teams, between managers, product managers, and so on. Often when a question is asked formally in a public space, it’s already been rehearsed and workshopped privately with the person who’s answering the question. None of this is or can be documented as part of the formal processes of the company, but it’s load-bearing nonetheless. Many formal processes simply cannot function without the consensus mechanisms or safety valves offered by backchannels.&lt;/p&gt;
    &lt;p&gt;Sometimes backchannels can go badly. Earlier this year I wrote Protecting your time from predators in large tech companies about how some people use backchannels to benefit themselves at the expense of the naive engineers they’re requesting work from. And it never feels good when you get the sense that everyone in a meeting has privately discussed the topic ahead of time except for you. For these reasons, some people think that backchannels themselves are a bad thing, and that all communication should go via formal, legible channels.&lt;/p&gt;
    &lt;head rend="h3"&gt;Sociopaths, clueless, and losers&lt;/head&gt;
    &lt;p&gt;There’s another text which has been as influential to many as Seeing Like A State. This one isn’t a book, but a blog post: The Gervais Principle by Venkatesh Rao. Rao divides organizations into three groups. At the top are the “sociopaths”, who cynically use organizational rules for their own benefit. In middle management are the “clueless”, who are bought into the formal rules of the organization and don’t realise that there’s a deeper game being played above their heads. Below them are the “losers”, who realise there’s a game being played but don’t want to play it. The name “losers” is not a value judgement - I think it’s meant to affectionately pick out people like the leads in Clerks, who are too authentic to get involved in the corporate game.&lt;/p&gt;
    &lt;p&gt;I don’t agree with everything in The Gervais Principle, though I think it’s worth a read (if you’re interested in this stuff, you should also read the excellent Moral Mazes). But the categories here can be very naturally read in terms of legibility. Both sociopaths and losers are engaged with the illegible world of the organization. Sociopaths use this world to climb the ladder, while losers use it to carve out a cosy low-effort niche for themselves.&lt;/p&gt;
    &lt;p&gt;The “clueless” are only engaged with legible processes. They’re the people who, when they want to get promoted, go and look up the formal job ladder and make a plan for how they can exemplify each of the values at the next level. They’re concerned with doing everything by the book. When they’re forced into an encounter with the illegible world, their reaction is to shake their heads and start drafting updates to the legible process that can accommodate some pale approximation of the more-efficient illegible process.&lt;/p&gt;
    &lt;p&gt;I think it’s far too cynical to call these people clueless. Legible process is still very important - after all, it’s the large part of what the organization does. Improving formal processes is still very high-leverage work, even if formal processes can’t ever describe the entirety of how an organization operates. People who are invested in legibility have real value to any tech company.&lt;/p&gt;
    &lt;p&gt;However, thinking about people in Rao’s categories - people who exploit illegibility, people who find it distasteful, and people who use it casually - can be illuminating. Many frequent areas of conflict in software companies stem from the friction between these groups of people.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;I write a lot about recognizing and using illegibility in tech companies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Breaking the (formal, legible) rules is sometimes the right thing to do&lt;/item&gt;
      &lt;item&gt;Beware of savvy product managers (and others) exploiting illegible channels to chisel work out of naive engineers&lt;/item&gt;
      &lt;item&gt;Competent engineers should work on “side bets” that are outside the normal planning process&lt;/item&gt;
      &lt;item&gt;Getting promoted to Staff and above has very little to do with the formal job ladder&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In general, advice about illegible processes is what I call “dangerous advice”. It’s dangerous because if you make it legible - for instance, if you announce publicly that you’re getting a piece of work done through backchannels instead of the formal process - you will be punished by the organization even if your management chain wanted you to do it. You can’t speak too loudly about it. It has to stay illegible.&lt;/p&gt;
    &lt;p&gt;I get a lot of negative feedback on these posts from people who say that you should never sidestep the formal process. According to them, if it needs changing, you should change the process instead of going around it. In other words, everything that goes on in a tech company should be legible, and illegible processes should be stamped out and converted to legible ones.&lt;/p&gt;
    &lt;p&gt;I think this view is naive. All organizations - tech companies, social clubs, governments - have both a legible and an illegible side. The legible side is important, past a certain size. It lets the organization do things that would otherwise be impossible: long-term planning, coordination with other very large organizations, and so on. But the illegible side is just as important. It allows for high-efficiency work, offers a release valve for processes that don’t fit the current circumstances, and fills the natural human desire for gossip and soft consensus.&lt;/p&gt;
    &lt;p&gt;edit: this got some comments on Hacker News. Some commenters agree with everything except the idea that large enterprise deals are the primary driver for legibility - they think it’s communication at scale, or being able to target market share, or internal control.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;This is the first example Scott gives, but I promise I did read the whole book. Other examples: the construction of Brasília, Operation Vijiji in Tanzania, and the Soviet attempt to replace individual peasant farms with state-run collectives.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This is a very common false belief today among software engineers.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I don’t think small companies just work harder; plenty of people at large companies work very hard. I also don’t think that small companies just have better engineers - what advantage they have in enthusiasm is often outweighed by the fact that they can’t afford to pay as well.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I was at Zendesk during the height of its pivot.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ironically, the most urgent types of problem typically can be solved via a normal “incident” process - but this itself is usually a zone where the rules are relaxed a bit in order to resolve the incident as quickly as possible. Anyway, here I’m not talking about incidents but about projects that will take a couple of weeks to resolve.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The other, healthier official way is to allow teams to make small changes to other teams’ services themselves. But this only goes so far - the other team will always be the gatekeepers for changes like this, and are always in a position to slow down the change by days or weeks.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;September 3, 2025 │ Tags: tech companies&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45505539</guid><pubDate>Tue, 07 Oct 2025 16:49:09 +0000</pubDate></item><item><title>German government comes out against Chat Control</title><link>https://xcancel.com/paddi_hansen/status/1975595307800142205</link><description>&lt;doc fingerprint="77d803d92c0426bd"&gt;
  &lt;main&gt;
    &lt;p&gt;Great news and big win for privacy in the EU! 🇪🇺🇩🇪 Germany’s ruling CDU/CSU party made it clear today: there will be no chat control - as pushed for by other EU countries - with this German government.&lt;/p&gt;
    &lt;p&gt;40 Sekunden kurz und präzise: Mit der CDU/CSU wird es keine anlasslose Chatkontrolle geben, wie sie von einigen Staaten in der EU gefordert wird.&lt;/p&gt;
    &lt;p&gt;Oct 7, 2025 · 4:13 PM UTC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45506143</guid><pubDate>Tue, 07 Oct 2025 17:31:49 +0000</pubDate></item><item><title>Less is more: Recursive reasoning with tiny networks</title><link>https://alexiajm.github.io/2025/09/29/tiny_recursive_models.html</link><description>&lt;doc fingerprint="e51fd9a9ac595e1b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Less is More: Recursive Reasoning with Tiny Networks&lt;/head&gt;
    &lt;p&gt;In this new paper, I propose Tiny Recursion Model (TRM), a recursive reasoning model that achieves amazing scores of 45% on ARC-AGI-1 and 8% on ARC-AGI-2 with a tiny 7M parameters neural network. The idea that one must rely on massive foundational models trained for millions of dollars by some big corporation in order to achieve success on hard tasks is a trap. Currently, there is too much focus on exploiting LLMs rather than devising and expanding new lines of direction. With recursive reasoning, it turns out that “less is more”: you don’t always need to crank up model size in order for a model to reason and solve hard problems. A tiny model pretrained from scratch, recursing on itself and updating its answers over time, can achieve a lot without breaking the bank.&lt;/p&gt;
    &lt;p&gt;This work came to be after I learned about the recent innovative Hierarchical Reasoning Model (HRM). I was amazed that an approach using small models could do so well on hard tasks like the ARC-AGI competition (reaching 40% accuracy when normally only Large Language Models could compete). But I kept thinking that it is too complicated, relying too much on biological arguments about the human brain, and that this recursive reasoning process could be greatly simplified and improved. Tiny Recursion Model (TRM) simplifies recursive reasoning to its core essence, which ultimately has nothing to do with the human brain, does not require any mathematical (fixed-point) theorem, nor any hierarchy.&lt;/p&gt;
    &lt;p&gt;See the paper for more details.&lt;/p&gt;
    &lt;head rend="h3"&gt;TLDR&lt;/head&gt;
    &lt;p&gt;Tiny Recursion Model (TRM) recursively improves its predicted answer y with a tiny network. It starts with the embedded input question x and initial embedded answer y and latent z. For up to K improvements steps, it tries to improve its answer y. It does so by i) recursively updating n times its latent z given the question x, current answer y, and current latent z (recursive reasoning), and then ii) updating its answer y given the current answer y and current latent z. This recursive process allows the model to progressively improve its answer (potentially addressing any errors from its previous answer) in an extremely parameter-efficient manner while minimizing overfitting.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45506268</guid><pubDate>Tue, 07 Oct 2025 17:42:06 +0000</pubDate></item><item><title>Gemini 2.5 Computer Use model</title><link>https://blog.google/technology/google-deepmind/gemini-computer-use-model/</link><description>&lt;doc fingerprint="b97269db1c538405"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing the Gemini 2.5 Computer Use model&lt;/head&gt;
    &lt;p&gt;Earlier this year, we mentioned that we're bringing computer use capabilities to developers via the Gemini API. Today, we are releasing the Gemini 2.5 Computer Use model, our new specialized model built on Gemini 2.5 Pro’s visual understanding and reasoning capabilities that powers agents capable of interacting with user interfaces (UIs). It outperforms leading alternatives on multiple web and mobile control benchmarks, all with lower latency. Developers can access these capabilities via the Gemini API in Google AI Studio and Vertex AI.&lt;/p&gt;
    &lt;p&gt;While AI models can interface with software through structured APIs, many digital tasks still require direct interaction with graphical user interfaces, for example, filling and submitting forms. To complete these tasks, agents must navigate web pages and applications just as humans do: by clicking, typing and scrolling. The ability to natively fill out forms, manipulate interactive elements like dropdowns and filters, and operate behind logins is a crucial next step in building powerful, general-purpose agents.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;The model’s core capabilities are exposed through the new `computer_use` tool in the Gemini API and should be operated within a loop. Inputs to the tool are the user request, screenshot of the environment, and a history of recent actions. The input can also specify whether to exclude functions from the full list of supported UI actions or specify additional custom functions to include.&lt;/p&gt;
    &lt;p&gt;Gemini 2.5 Computer Use Model flow&lt;/p&gt;
    &lt;p&gt;The model then analyzes these inputs and generates a response, typically a function call representing one of the UI actions such as clicking or typing. This response may also contain a request for an end user confirmation, which is required for certain actions such as making a purchase. The client-side code then executes the received action.&lt;/p&gt;
    &lt;p&gt;After the action is executed, a new screenshot of the GUI and the current URL are sent back to the Computer Use model as a function response restarting the loop. This iterative process continues until the task is complete, an error occurs or the interaction is terminated by a safety response or user decision.&lt;/p&gt;
    &lt;p&gt;The Gemini 2.5 Computer Use model is primarily optimized for web browsers, but also demonstrates strong promise for mobile UI control tasks. It is not yet optimized for desktop OS-level control.&lt;/p&gt;
    &lt;p&gt;Check out a few demos below to see the model in action (shown here at 3X speed).&lt;/p&gt;
    &lt;p&gt;Prompt: “From https://tinyurl.com/pet-care-signup, get all details for any pet with a California residency and add them as a guest in my spa CRM at https://pet-luxe-spa.web.app/. Then, set up a follow up visit appointment with the specialist Anima Lavar for October 10th anytime after 8am. The reason for the visit is the same as their requested treatment.”&lt;/p&gt;
    &lt;p&gt;Prompt: “My art club brainstormed tasks ahead of our fair. The board is chaotic and I need your help organizing the tasks into some categories I created. Go to sticky-note-jam.web.app and ensure notes are clearly in the right sections. Drag them there if not.”&lt;/p&gt;
    &lt;head rend="h2"&gt;How it performs&lt;/head&gt;
    &lt;p&gt;The Gemini 2.5 Computer Use model demonstrates strong performance on multiple web and mobile control benchmarks. The table below includes results from self-reported numbers, evaluations run by Browserbase and evaluations we ran ourselves. Evaluation details are available in the Gemini 2.5 Computer Use evaluation info and in Browserbase’s blog post. Unless otherwise indicated, scores shown are for computer use tools exposed via API.&lt;/p&gt;
    &lt;p&gt;Gemini 2.5 Computer Use outperforms leading alternatives on multiple benchmarks&lt;/p&gt;
    &lt;p&gt;The model offers leading quality for browser control at the lowest latency, as measured by performance on the Browserbase harness for Online-Mind2Web.&lt;/p&gt;
    &lt;p&gt;Gemini 2.5 Computer Use delivers high accuracy while maintaining low latency&lt;/p&gt;
    &lt;head rend="h2"&gt;How we approached safety&lt;/head&gt;
    &lt;p&gt;We believe that the only way to build agents that will benefit everyone is to be responsible from the start. AI agents that control computers introduce unique risks, including intentional misuse by users, unexpected model behavior, and prompt injections and scams in the web environment. Thus, it is critical to implement safety guardrails with care.&lt;/p&gt;
    &lt;p&gt;We have trained safety features directly into the model to address these three key risks (described in the Gemini 2.5 Computer Use System Card).&lt;/p&gt;
    &lt;p&gt;Further, we also provide developers with safety controls, which empower developers to prevent the model from auto-completing potentially high-risk or harmful actions. Examples of these actions include harming a system's integrity, compromising security, bypassing CAPTCHAs, or controlling medical devices. The controls:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Per-step safety service: An out-of-model, inference-time safety service that assesses each action the model proposes before it’s executed.&lt;/item&gt;
      &lt;item&gt;System instructions: Developers can further specify that the agent either refuses or asks for user confirmation before it takes specific kinds of high-stakes actions. (Example in documentation).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additional recommendations for developers on safety measures and best practices can be found in our documentation. While these safeguards are designed to reduce risk, we urge all developers to thoroughly test their systems before launch.&lt;/p&gt;
    &lt;head rend="h2"&gt;How early testers have used it&lt;/head&gt;
    &lt;p&gt;Google teams have already deployed the model to production for use cases including UI testing, which can make software development signficantly faster. Versions of this model have also been powering Project Mariner, the Firebase Testing Agent, and some agentic capabilities in AI Mode in Search.&lt;/p&gt;
    &lt;p&gt;Users from our early access program have also been testing the model to power personal assistants, workflow automation, and UI testing, and have seen strong results. In their own words:&lt;/p&gt;
    &lt;head rend="h2"&gt;How to get started&lt;/head&gt;
    &lt;p&gt;Starting today, the model is available in public preview, accessible via the Gemini API on Google AI Studio and Vertex AI.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Try it now: In a demo environment hosted by Browserbase.&lt;/item&gt;
      &lt;item&gt;Start building: Dive into our reference and documentation (see Vertex AI docs for enterprise use) to learn how to build your own agent loop locally with Playwright or in a cloud VM with Browserbase.&lt;/item&gt;
      &lt;item&gt;Join the community: We’re excited to see what you build. Share feedback and help guide our roadmap in our Developer Forum.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45507936</guid><pubDate>Tue, 07 Oct 2025 19:49:11 +0000</pubDate></item><item><title>Study of 1M-year-old skull points to earlier origins of modern humans</title><link>https://www.theguardian.com/science/2025/sep/25/study-of-1m-year-old-skull-points-to-earlier-origins-of-modern-humans</link><description>&lt;doc fingerprint="f65511fdb8b11bbe"&gt;
  &lt;main&gt;
    &lt;p&gt;A million-year-old human skull suggests that the origins of modern humans may reach back far deeper in time than previously thought and raises the possibility that Homo sapiens first emerged outside of Africa.&lt;/p&gt;
    &lt;p&gt;Leading scientists reached this conclusion after reanalysis of a skull known as Yunxian 2 discovered in China and previously classified as belonging to a member of the primitive human species Homo erectus.&lt;/p&gt;
    &lt;p&gt;After applying sophisticated reconstruction techniques to the skull, scientists believe that it may instead belong to a group called Homo longi (dragon man), closely linked to the elusive Denisovans who lived alongside our own ancestors.&lt;/p&gt;
    &lt;p&gt;This repositioning would make the fossil the closest on record to the split between modern humans and our closest relatives, the Neanderthals and Denisovans, and would radically revise understanding of the last 1m years of human evolution.&lt;/p&gt;
    &lt;p&gt;Prof Chris Stringer, an anthropologist and research leader in human evolution at the Natural History Museum in London, said: “This changes a lot of thinking because it suggests that by 1m years ago our ancestors had already split into distinct groups, pointing to a much earlier and more complex human evolutionary split than previously believed. It more or less doubles the time of origin of Homo sapiens.”&lt;/p&gt;
    &lt;p&gt;The skull was first unearthed in Hubei province in 1990, badly crushed and difficult to interpret. Based on its age and some broad-brush traits, it was assigned as Homo erectus, a group that is thought to have contained direct ancestors of modern humans.&lt;/p&gt;
    &lt;p&gt;The latest work used advanced CT imaging, high-resolution surface scanning and sophisticated digital techniques to produce a virtual reconstruction of the skull. The skull’s large, squat brain case and jutting lower jaw are reminiscent of Homo erectus. But the overall shape and size of the brain case and teeth appear to place it much closer to Homo longi, a species that scientists have recently argued should incorporate the Denisovans.&lt;/p&gt;
    &lt;p&gt;This would push the split between our own ancestors, Neanderthals and Homo longi back by at least 400,000 years and, according to Springer, raises the possibility that our common ancestor – and potentially the first Homo sapiens – lived in western Asia rather than Africa.&lt;/p&gt;
    &lt;p&gt;“This fossil is the closest we’ve got to the ancestor of all those groups,” Stringer said.&lt;/p&gt;
    &lt;p&gt;A computational analysis of a wider selection of fossils suggests that in the last 800,000 years, large-brained humans evolved along just five major branches: Asian erectus, heidelbergensis, sapiens, Neanderthals and Homo longi (including the Denisovans).&lt;/p&gt;
    &lt;p&gt;“We feel that this study is a landmark step towards resolving the ‘muddle in the middle’ [the confusing array of human fossils from between 1m and 300,000 years ago] that has preoccupied palaeoanthropologists for decades,” Stringer said.&lt;/p&gt;
    &lt;p&gt;The findings run counter to some recent analyses based on genetic comparisons of living humans and ancient DNA, meaning the conclusions are likely to be contentious.&lt;/p&gt;
    &lt;p&gt;Dr Frido Welker, an associate professor in human evolution at the University of Copenhagen, who was not involved in the research, said: “It’s exciting to have a digital reconstruction of this important cranium available. If confirmed by additional fossils and genetic evidence, the divergence dating would be surprising indeed. Alternatively, molecular data from the specimen itself could provide insights confirming or disproving the authors’ morphological hypothesis.”&lt;/p&gt;
    &lt;p&gt;The findings are published in the journal Science.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45510582</guid><pubDate>Wed, 08 Oct 2025 00:17:21 +0000</pubDate></item><item><title>Show HN: Oh Yah – Routine management app I built for my sons</title><link>https://ohyahapp.com</link><description>&lt;doc fingerprint="3c11f5fd244164e3"&gt;
  &lt;main&gt;
    &lt;p&gt;Each task has a dedicated timer in a distraction-free environment. Navigation is disabled during focus time to maintain attention and encourage task completion.&lt;/p&gt;
    &lt;p&gt;Kids can submit photo evidence of completed tasks, building accountability whilst giving parents peace of mind about task completion.&lt;/p&gt;
    &lt;p&gt;Ordered tasks provide structure and sequence, whilst unordered tasks offer flexibility. Clear priorities help kids understand what needs to be done when.&lt;/p&gt;
    &lt;p&gt;Parents review individual tasks but award stars for the complete schedule. Auto-approval with full stars occurs after 24 hours if all tasks are submitted and no manual award is given.&lt;/p&gt;
    &lt;p&gt;Manage up to 8 child profiles with individual schedules and tasks. Perfect for families with multiple children.&lt;/p&gt;
    &lt;p&gt;Clean interface with simple navigation. Kids tap their profile and start immediately - no complex menus or overwhelming choices.&lt;/p&gt;
    &lt;p&gt;Oh Yah! is for families who need routine management that works with their child's unique needs — not generic productivity apps that overwhelm.&lt;/p&gt;
    &lt;p&gt;Child taps their profile and sees today's tasks in a consistent order set by parents.&lt;/p&gt;
    &lt;p&gt;Each task opens in a focused timer modal with no distractions or navigation.&lt;/p&gt;
    &lt;p&gt;Take a photo as proof and submit the completed task for parent review.&lt;/p&gt;
    &lt;p&gt;Parents review tasks and award stars based on quality. Unreviewed tasks auto-approve after 24 hours, keeping parents accountable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45513459</guid><pubDate>Wed, 08 Oct 2025 08:15:49 +0000</pubDate></item><item><title>Synology reverses policy banning third-party HDDs after sales allegedly plummet</title><link>https://www.guru3d.com/story/synology-reverses-policy-banning-thirdparty-hdds-after-nas-sales-plummet/</link><description>&lt;doc fingerprint="7da303bb4dfe4cfc"&gt;
  &lt;main&gt;
    &lt;p&gt;Now, with the release of DSM 7.3, Synology has quietly walked the policy back. Third-party hard drives and 2.5-inch SATA SSDs can once again be used without triggering warning messages or reduced functionality. Drives from Seagate, WD, and others will work exactly as they did before—complete with full monitoring, alerts, and storage features.&lt;/p&gt;
    &lt;p&gt;For users, this means more choice and lower costs when building or upgrading a NAS. For Synology, it’s a much-needed course correction after months of backlash. While the company hasn’t publicly admitted fault, it’s clear that sales pressure and community outrage played a major role in reversing the decision.&lt;/p&gt;
    &lt;p&gt;Critics say the entire episode has damaged Synology’s reputation. The company seemed to believe that after QNAP’s well-known ransomware troubles, it could tighten control of the market without losing customers. Instead, the plan backfired—hard. Many loyal users have since turned to alternative brands or expressed hesitation about buying another Synology product.&lt;/p&gt;
    &lt;p&gt;Still, the return of open drive support is good news for anyone running a Synology NAS. It restores the flexibility that made the brand so popular in the first place. Whether this move is enough to win back frustrated users remains to be seen, but for now, DSM 7.3 brings a welcome dose of freedom back to the platform.&lt;/p&gt;
    &lt;p&gt;Source: Synology / nascompares&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45513485</guid><pubDate>Wed, 08 Oct 2025 08:19:36 +0000</pubDate></item><item><title>Nobel Prize in Chemistry 2025</title><link>https://www.nobelprize.org/prizes/chemistry/2025/popular-information/</link><description>&lt;doc fingerprint="37d0923c8f250160"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Popular information&lt;/head&gt;
    &lt;p&gt;Popular science background: They have created new rooms for chemistry (pdf)&lt;lb/&gt;Populärvetenskaplig information: De har skapat nya rum för kemi (pdf)&lt;/p&gt;
    &lt;head rend="h2"&gt;They have created new rooms for chemistry&lt;/head&gt;
    &lt;p&gt;Susumu Kitagawa, Richard Robson and Omar M. Yaghi are awarded the Nobel Prize in Chemistry 2025 for the development of a new type of molecular architecture. The constructions they created – metal–organic frameworks – contain large cavities in which molecules can flow in and out. Researchers have used them to harvest water from desert air, extract pollutants from water, capture carbon dioxide and store hydrogen.&lt;/p&gt;
    &lt;p&gt;An attractive and very spacious studio apartment, specifically designed for your life as a water molecule – this is how an estate agent might describe one of all the metal–organic frameworks that laboratories around the world have developed in recent decades. Other constructions of this type are tailormade for capturing carbon dioxide, separating PFAS from water, delivering pharmaceuticals in the body or managing extremely toxic gases. Some can trap the ethylene gas from fruit – so they ripen more slowly – or encapsulate enzymes that break down traces of antibiotics in the environment.&lt;/p&gt;
    &lt;p&gt;Simply stated, metal–organic frameworks are exceptionally useful. Susumu Kitagawa, Richard Robson and Omar Yaghi are awarded the Nobel Prize in Chemistry 2025 because they created the first metal–organic frameworks (MOF) and demonstrated their potential. Thanks to the laureates’ work, chemists have been able to design tens of thousands of different MOFs, facilitating new chemical wonders.&lt;/p&gt;
    &lt;p&gt;As so often in the sciences, the story of the Nobel Prize in Chemistry 2025 begins with someone who thought outside the box. This time, inspiration came during preparations for a classic chemistry lesson, in which the students were to build molecules from rods and balls.&lt;/p&gt;
    &lt;head rend="h3"&gt;A simple wooden model of a molecule generates an idea&lt;/head&gt;
    &lt;p&gt;It was 1974. Richard Robson, who was teaching at the University of Melbourne, Australia, had been tasked with turning wooden balls into models of atoms, so students could create molecular structures. For this to work, he needed the university’s workshop to drill holes in them, so that wooden rods – the chemical bonds – could be attached to the atoms. However, the holes could not be randomly placed. Each atom – such as carbon, nitrogen or chlorine – forms chemical bonds in a specific way. Robson needed to mark out where the holes should be drilled.&lt;/p&gt;
    &lt;p&gt;When the workshop returned the wooden balls, he tested building some molecules. This was when he had a moment of insight: there was a vast amount of information baked into the holes’ positioning. The model molecules automatically had the correct form and structure, because of where the holes were situated. This insight led to his next idea: what would happen if he utilised the atoms’ inherent properties to link together different types of molecules, rather than individual atoms? Could he design new types of molecular constructions?&lt;/p&gt;
    &lt;head rend="h3"&gt;Robson builds innovative chemical creations&lt;/head&gt;
    &lt;p&gt;Every year, when Robson brought out the wooden models to teach new students, the same idea occurred to him. However, more than a decade passed before he decided to test it out. He started with a very simple model, inspired by the structure of a diamond, in which each carbon atom bonds to four others, forming a tiny pyramid (figure 2). Robson’s aim was to build a similar structure, but his would be based on positively charged copper ions, Cu+. Like carbon, they prefer to have four other atoms around them.&lt;/p&gt;
    &lt;p&gt;He combined the copper ions with a molecule that has four arms: 4′,4″,4”’,4””-tetracyanotetraphenylmethane. There’s no need to remember its complicated name, but it is important that the molecule at the end of each arm had a chemical group, nitrile, that was attracted to the positively charged copper ions (figure 2).&lt;/p&gt;
    &lt;p&gt;At that time, most chemists would have assumed that combining copper ions with the four-armed molecules would result in a bird’s nest of ions and molecules. But things went Robson’s way. As he had predicted, the ions and molecules inherent attraction to each other mattered, so they organised themselves into a large molecular construction. Just like carbon atoms in a diamond, they formed a regular crystalline structure. However, unlike diamond – which is a compact material – this crystal contained a vast number of large cavities (figure 2).&lt;/p&gt;
    &lt;p&gt;In 1989, Robson presented his innovative chemical creation in the Journal of the American Chemical Society. In his article, he speculates about the future and suggests that this could offer a new way to construct materials. These, he writes, could be given never previously seen properties, potentially beneficial ones.&lt;/p&gt;
    &lt;p&gt;As it turned out, he had foreseen the future.&lt;/p&gt;
    &lt;head rend="h3"&gt;Robson brings about a pioneering spirit in chemistry&lt;/head&gt;
    &lt;p&gt;As soon as the year after his pioneering work was published, Robson presented several new types of molecular constructions with cavities that were filled with various substances. He used one of them to exchange ions. He submerged the ion-filled construction in a fluid that contained a different type of ion. The result was that the ions changed places, demonstrating that substances could flow in and out of the construction.&lt;/p&gt;
    &lt;p&gt;In his experiments, Robson showed that rational design can be utilised for building crystals with spacious interiors that are optimised for specific chemicals. He suggested that this new form of molecular construction – when correctly designed – could be used to catalyse chemical reactions, for example.&lt;/p&gt;
    &lt;p&gt;However, Robson’s constructions were quite rickety and tended to fall apart. Many chemists thought they were useless, but some could see that he was onto something and, for them, his ideas about the future awakened a pioneering spirit. Those who would come to lay a stable foundation for his visions were Susumu Kitagawa and Omar Yaghi. Between 1992 and 2003 they made – separately – a series of groundbreaking discoveries. We will begin in the 1990s, with Kitagawa, who was working at Kindai University, Japan.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kitagawa’s motto: even useless things can become useful&lt;/head&gt;
    &lt;p&gt;Throughout his research career, Susumu Kitagawa has followed an important principle: to try to see “the usefulness of useless.” As a young student, he read a book by the Nobel Prize laureate Hideki Yukawa. In it, Yukawa refers to an ancient Chinese philosopher, Zhuangzi, who says that we must question what we believe to be useful. Even if something does not bring immediate benefit, it may still turn out to be valuable.&lt;/p&gt;
    &lt;p&gt;Accordingly, when Kitagawa began to investigate the potential for creating porous molecular structures, he did not believe they had to have a specific purpose. When he presented his first molecular construction in 1992, it was indeed not particularly useful: a two-dimensional material with cavities in which acetone molecules could hide. However, it had resulted from a new way of thinking about the art of building with molecules. Like Robson, he used copper ions as cornerstones that were linked together by larger molecules.&lt;/p&gt;
    &lt;p&gt;Kitagawa wanted to continue experimenting with this new construction technology, but when he applied for grants, research funders did not think there was any particular point to his ambitions. The materials he created were unstable and had no purpose, so many of his proposals were rejected.&lt;/p&gt;
    &lt;p&gt;However, he did not give up and in 1997 he had his first major breakthrough. Using cobalt, nickel or zinc ions and a molecule called 4,4′-bipyridine, his research group created three-dimensional metal–organic frameworks that were intersected by open channels (figure 3). When they dried one of these materials – emptying it of water – it was stable and the spaces could even be filled with gases. The material could absorb and release methane, nitrogen and oxygen, without changing shape.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kitagawa sees the uniqueness of his creations&lt;/head&gt;
    &lt;p&gt;Kitagawa’s constructions were both stable and had a function, but research funders were still unable to see their charm. One reason was that chemists already had zeolites, stable and porous materials, which they could build from silicon dioxide. These can absorb gases, so why would anyone develop a similar material that did not work as well?&lt;/p&gt;
    &lt;p&gt;Susumu Kitagawa understood that if he were to receive any major grants, he had to define what made metal–organic frameworks unique. So, in 1998, he described his vision in the Bulletin of the Chemical Society of Japan. He presented several advantages with MOFs. For example, they can be created from many types of molecules, so there is enormous potential for integrating different functions. Also – and this is important – he realised that MOFs can form soft materials. Unlike zeolites, which are usually hard materials, MOFs contain flexible molecular building blocks (figure 4) that can create a pliant material.&lt;/p&gt;
    &lt;p&gt;After this, all he had to do was to put his ideas into practice. Kitagawa, along with other researchers, started developing flexible MOFs. While they work on this, we will move our focus to the US, where Omar Yaghi was also occupied with taking molecular architecture to new heights.&lt;/p&gt;
    &lt;head rend="h3"&gt;A secret library visit opens Yaghi’s eyes to chemistry&lt;/head&gt;
    &lt;p&gt;Studying chemistry was not an obvious choice for Omar Yaghi. He and his many siblings were raised in a single room in Amman, Jordan, with no electricity or running water. School was a refuge from his otherwise challenging life. One day, when he was ten years old, he sneaked into the school library, which was usually locked, and picked a book at random from the shelf. On opening it, his eyes were drawn to unintelligible but captivating pictures – his first encounter with molecular structures.&lt;/p&gt;
    &lt;p&gt;At the age of 15 – and on his father’s stern instruction – Yaghi moved to the US to study. He was attracted by chemistry and eventually by the art of designing new materials, but found the traditional way of building new molecules too unpredictable. Normally, chemists combine substances that are to react with each other in a container. Then, to start the chemical reaction, they heat the container. The desired molecule forms, but is also often accompanied by a range of contaminating side products.&lt;/p&gt;
    &lt;p&gt;In 1992, when Yaghi started his first position as research group leader, at Arizona State University, he wanted to find more controlled ways in which to create materials. His aim was to use rational design to connect different chemical constituents, like pieces of Lego, to make large crystals. This turned out to be challenging, but they finally succeeded when the research group started combining metal ions with organic molecules. In 1995, Yaghi published the structure of two different two-dimensional materials; these were like nets and were held together by copper or cobalt. The latter could host guest molecules in its spaces and, when these were fully occupied, it was so stable that it could be heated to 350°C without collapsing. Yaghi describes this material in an article in Nature where he coins the name “metal–organic framework;” this term is now used to describe extended and ordered molecular structures that potentially contain cavities, and are built from metals and organic (carbon-based) molecules.&lt;/p&gt;
    &lt;head rend="h3"&gt;Just a few grams of Yaghi’s framework can contain a football pitch&lt;/head&gt;
    &lt;p&gt;Yaghi established the next milestone in the development of metal–organic frameworks in 1999, when he presented MOF-5 to the world. This material has become a classic in the field. It is an exceptionally spacious and stable molecular construction. Even when empty, it can be heated to 300°C without collapsing.&lt;/p&gt;
    &lt;p&gt;However, what caused many researchers to raise their eyebrows was the enormous area hiding inside the material’s cubic spaces. A couple of grams of MOF-5 holds an area as big as a football pitch, which means it can absorb much more gas than a zeolite could (figure 5).&lt;/p&gt;
    &lt;p&gt;Speaking of the differences between zeolites and MOFs, it took just a few years for researchers to succeed in developing soft MOFs. One of those who was able to present a flexible material was Susumu Kitagawa himself. When his material was filled with water or methane, it changed shape, and when it was emptied, it returned to its original form. The material behaved somewhat like a lung that can breathe gas in and out, changeable but stable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Yaghi’s research group conjures drinking water from desert air&lt;/head&gt;
    &lt;p&gt;Omar Yaghi laid the final bricks in the foundation of metal–organic frameworks in 2002 and 2003. In two articles, in Science and Nature, he shows that it is possible to modify and change MOFs in a rational manner, giving them different properties. One thing he did was to produce 16 variants of MOF-5, with cavities that were both larger and smaller than those in the original material (figure 6). One variant could store huge volumes of methane gas, which Yaghi suggested could be used in RNG-fuelled vehicles.&lt;/p&gt;
    &lt;p&gt;Subsequently, metal–organic frameworks have taken the world by storm. Researchers have developed a molecular kit with a wide range of different pieces that can be used to create new MOFs. These have different shapes and characters, providing incredible potential for the rational – or AI-based – design of MOFs for different purposes. Figure 7 provides examples of how MOFs can be utilised. For instance, Yaghi’s research group has harvested water from the desert air of Arizona. During the night, their MOF material captured water vapour from the air. When dawn came and the sun heated the material, they were able to collect the water.&lt;/p&gt;
    &lt;head rend="h3"&gt;MOF materials that capture carbon dioxide and toxic gases&lt;/head&gt;
    &lt;p&gt;Researchers have created numerous different and functional MOFs. So far, in most cases, the materials have only been used on a small scale. To harness the benefits of MOF materials for humanity, many companies are now investing in their mass production and commercialisation. Some have succeeded. For example, the electronics industry can now use MOF materials to contain some of the toxic gases required to produce semiconductors. Another MOF can instead break down harmful gases, including some that can be used as chemical weapons. Numerous companies are also testing materials that can capture carbon dioxide from factories and power stations, to reduce greenhouse gas emissions.&lt;/p&gt;
    &lt;p&gt;Some researchers believe that metal–organic frameworks have such huge potential that they will be the material of the twenty-first century. Time will tell, but through the development of metal–organic frameworks, Susumu Kitagawa, Richard Robson and Omar Yaghi have provided chemists with new opportunities for solving some of the challenges we face. They have thus – as Alfred Nobel’s will states – brought the greatest benefit to humankind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further reading&lt;/head&gt;
    &lt;p&gt;Additional information on this year’s prizes, including a scientific background in English, is available on the website of the Royal Swedish Academy of Sciences, www.kva.se, and at www.nobelprize.org, where you can watch video from the press conferences, the Nobel Prize lectures and more. Information on exhibitions and activities related to the Nobel Prizes and the prize in economic sciences is available at www.nobelprizemuseum.se.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Royal Swedish Academy of Sciences has decided to award the Nobel Prize in Chemistry 2025 to&lt;/head&gt;
    &lt;p&gt;SUSUMU KITAGAWA&lt;lb/&gt;Born 1951 in Kyoto, Japan. PhD 1979 from Kyoto University, Japan. Professor at Kyoto University, Japan.&lt;/p&gt;
    &lt;p&gt;RICHARD ROBSON&lt;lb/&gt;Born 1937 in Glusburn, UK. PhD 1962 from University of Oxford, UK. Professor at University of Melbourne, Australia.&lt;/p&gt;
    &lt;p&gt;OMAR M. YAGHI&lt;lb/&gt;Born 1965 in Amman, Jordan. PhD 1990 from University of Illinois Urbana-Champaign, USA. Professor at University of California, Berkeley, USA.&lt;/p&gt;
    &lt;p&gt;“for the development of metal–organic frameworks”&lt;/p&gt;
    &lt;p&gt;Science Editors: Peter Brzezinski, Heiner Linke, Olof Ramström and Xiaodong Zou, the Nobel Committee for Chemistry&lt;lb/&gt;Text: Ann Fernholm&lt;lb/&gt;Translation: Clare Barnes&lt;lb/&gt;Illustrations: Johan Jarnestad&lt;lb/&gt;Editor: Alicia Hegner&lt;lb/&gt;© The Royal Swedish Academy of Sciences&lt;/p&gt;
    &lt;head rend="h3"&gt;Nobel Prize announcements 2025&lt;/head&gt;
    &lt;p&gt;Don't miss the Nobel Prize announcements 6–13 October. All announcements are streamed live here on nobelprize.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45514164</guid><pubDate>Wed, 08 Oct 2025 09:49:36 +0000</pubDate></item><item><title>A Clausewitzian Lens on Modern Urban Warfare</title><link>https://mwi.westpoint.edu/a-clausewitzian-lens-on-modern-urban-warfare/</link><description>&lt;doc fingerprint="faa359510591dbb3"&gt;
  &lt;main&gt;
    &lt;p&gt;Among Carl von Clausewitz’s many poignant dictums, the most commonly cited is undoubtedly that “war is not merely an act of policy but a true political instrument, a continuation of political intercourse, carried on by other means.” While Clausewitz never fought in a city like Fallujah, Kyiv, or Gaza if the Prussian general and philosopher of war could visit the battlefields of the twenty-first century, he would recognize modern urban warfare’s core challenges—and would find that his theories about war’s objective and the considerations needed for victory remain strikingly relevant.&lt;/p&gt;
    &lt;p&gt;Clausewitz wrote that “war is more than a true chameleon that slightly adapts its characteristics to the given case.” Its essential elements—violence, chance and probability, and subordination to policy—form what he famously described as a wunderliche Dreifaltigkeit, or “remarkable trinity.” Rather than being in conflict, these elements interact dynamically and, in successful systems like that of Napoleonic France, can operate in harmony. Clausewitz and his fellow Prussian reformers admired how the French system aligned popular will, military force, and political direction. Nowhere is the need for such harmony more acute than in modern urban warfare, where civilians, combatants, and national objectives share the same congested terrain. This environment tests the limits of military doctrine, challenges the notion of strategic clarity, and often leaves combatants with ambiguous definitions of victory.&lt;/p&gt;
    &lt;p&gt;In my work on urban warfare, from my book Understanding Urban Warfare to the numerous case studies I have authored and the field research I have conducted, I’ve seen the truths Clausewitz described play out on concrete streets and in bombed-out buildings. Urban warfare has become the norm, not the exception, and Clausewitz’s insights are not relics of Napoleonic Europe—they are essential tools for understanding the future of conflict.&lt;/p&gt;
    &lt;p&gt;Historical Context: Urban Warfare in Clausewitz’s Era&lt;/p&gt;
    &lt;p&gt;While Clausewitz never commanded modern urban battles, his military career immersed him in conflicts where cities played central strategic and symbolic roles. As a young officer in the Prussian Army, he fought in the Rhine campaigns (1793–1794), including the siege of Mainz, where revolutionary France defended the city against a Prussian-Austrian coalition. This early exposure to urban siege warfare—marked by fortified positions, complex logistics, and the suffering of civilians—gave Clausewitz firsthand insight into the unique challenges of fighting in and around cities.&lt;/p&gt;
    &lt;p&gt;Clausewitz’s later experiences reinforced the political and psychological weight of urban centers. As aide-de-camp to Prussian Prince Augustus Ferdinand, he was present during Napoleon’s 1806 victory in the battles of Jena and Auerstedt, which led to the French occupation of Berlin. He later served with the Russian Army during France’s 1812 invasion of Russia, taking part in the Battle of Borodino—a prelude to the burning of Moscow that serves as a powerful example of a capital’s strategic and symbolic significance. In 1815, having reentered Prussian service, he participated in the Battles of Ligny and Wavre, fighting on terrain where towns, roads, and rivers constrained operations and shaped outcomes.&lt;/p&gt;
    &lt;p&gt;Clausewitz drew clear conclusions from these experiences. In Principles of War, he argued that “public opinion is won through great victories and the occupation of the enemy’s capital.” He understood cities not only as symbolic centers of national will but also as logistical and operational hubs, writing of the importance of targeting “principal cities, storehouses, and large fortresses.” Though he did not witness the dense, protracted urban warfare of the modern era, Clausewitz’s strategic emphasis on cities foreshadowed many of the dynamics seen in today’s urban battles.&lt;/p&gt;
    &lt;p&gt;The Urban Trinity, Fog, and Friction: Clausewitz’s Theories in Concrete and Steel&lt;/p&gt;
    &lt;p&gt;Clausewitz’s “remarkable trinity”—violence and hatred (the people), chance and probability (in military action), and reason and policy (the government)—finds its most visceral expression in urban warfare. Cities collapse these elements into a single, compact battlespace. Unlike operations in open terrain, urban warfare places civilians, combatants, and political objectives in constant, physical contact. The Clausewitzian trinity becomes spatially literal: civilians live among the fight, military action is hyperlocalized and constrained, and every movement carries political weight.&lt;/p&gt;
    &lt;p&gt;Clausewitz also famously wrote, “No one starts a war—or rather, no one in his senses ought to do so—without first being clear in his mind what he intends to achieve by that war and how he intends to conduct it.” He noted, therefore, that “the first, the supreme, the most far-reaching act of judgment that the statesman and commander have to make is to establish . . . the kind of war on which they are embarking.”&lt;/p&gt;
    &lt;p&gt;This act of judgment is especially difficult in cities, where the kind of war one is fighting can shift from block to block. Is the objective to destroy an entrenched enemy force? To hold key, vital, or symbolic terrain? To safeguard a civilian population? In urban warfare the answer is often all of the above.&lt;/p&gt;
    &lt;p&gt;Clausewitz’s dictum that war is a continuation of politics by other means is vividly realized in urban combat. Tactical decisions in cities reverberate at strategic levels. Urban warfare does not allow separation between military action and political consequence—they are fused.&lt;/p&gt;
    &lt;p&gt;But Clausewitz reminds us that strategy is not made of battlefield maneuvers alone—it is also made of will. He defined war as “an act of force to compel our enemy to do our will.”&lt;/p&gt;
    &lt;p&gt;Victory, then, is not always the annihilation of enemy forces—it is the collapse of the enemy’s will to resist. And in modern urban warfare, maintaining the will of one’s own people (or of your ally’s population)—to support the fight or to accept the moral and political costs—is just as critical. Clausewitz considered these moral forces among the most decisive in war, writing that they “constitute the spirit that permeates war as a whole.” In cities under siege or attack, public opinion, national resolve, and leadership cohesion become as important as any tactical maneuver.&lt;/p&gt;
    &lt;p&gt;Urban warfare places enormous pressure on the internal willpower of a combatant nation and that of its involved and invested allies. The proximity of civilians, the visibility of destruction, and the speed at which information spreads can erode public support even as military objectives are met. A video, a collapsed building, or a failed operation can shift the strategic balance—not through force, but by weakening the political object that gives war its purpose. Clausewitz warned, “The political object is the goal, war is the means of reaching it, and means can never be considered in isolation from their purpose.”&lt;/p&gt;
    &lt;p&gt;This relationship between political objectives, military action, and national will is especially fragile in urban combat. When will breaks—on either side—the war may be lost regardless of battlefield gains.&lt;/p&gt;
    &lt;p&gt;Yet even the clearest strategy must contend with the inherent chaos of war. “War is the realm of uncertainty,” Clausewitz cautioned. “Three quarters of the factors on which action in war is based are wrapped in a fog of greater or lesser uncertainty.” This fog of war—its confusion, unpredictability, and lack of reliable information—is magnified in dense urban environments where lines between civilian and combatant blur and information spreads instantly and globally. Commanders must make high-stakes decisions in environments where clarity is fleeting.&lt;/p&gt;
    &lt;p&gt;Compounding this is what Clausewitz called friction—the accumulation of countless small obstacles that derail even the best-laid plans. As he wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Everything in war is very simple, but the simplest thing is difficult. The difficulties accumulate and end by producing a kind of friction that is inconceivable unless one has experienced war. . . . Friction is the only concept that more or less corresponds to the factors that distinguish real war from war on paper.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Friction in cities is not theoretical—it’s tactical and visceral. Streets canalize movement. Buildings obscure lines of sight. Civilians become obstacles or allies. The environment itself resists clean execution. Urban warfare reveals Clausewitz’s insights not just as philosophical musings, but as hard realities in concrete and steel.&lt;/p&gt;
    &lt;p&gt;Urban Warfare in Iraq: Lessons from Baghdad and ISIS&lt;/p&gt;
    &lt;p&gt;The US campaigns in Iraq and the broader fight against the Islamic State of Iraq and Syria (ISIS) offer a laboratory of Clausewitzian warfare in cities—where tactical success frequently collided with political complexity, and where the will of the population, not battlefield metrics, often defined the limits of victory.&lt;/p&gt;
    &lt;p&gt;The 2003 “Thunder Run” into Baghdad was more than a demonstration of military power—it was a calculated strike at the enemy’s political center of gravity. Recall Clausewitz’s observation that “public opinion is won through great victories and the occupation of the enemy’s capital.” Taking Baghdad had immediate strategic effects: It overthrew Saddam Hussein and dismantled the Ba’athist regime. But it did not yield strategic clarity. Once the political objective shifted from regime removal to establishing a new political order—that is, nation-building—the occupation of the capital no longer compelled the enemy to do our will. Instead, it ushered in a new phase of resistance—fought not by conventional armies, but by insurgents embedded in the population. The Clausewitzian trinity fractured, and the fog of war deepened.&lt;/p&gt;
    &lt;p&gt;The 2004 First and Second Battles of Fallujah posed a different Clausewitzian challenge: how to reestablish control over a city that had become both a symbol and a stronghold of insurgent defiance. The battles exposed the full weight of friction. Every block was contested. Civilian presence, urban density, and improvised defenses neutralized many of the coalition’s technological advantages. Clausewitz’s observation rang true: The simplest thing became difficult. But beyond the tactical grind, Fallujah also heightened the strategic burden of fighting in cities under global media scrutiny. Images of destruction and civilian displacement reverberated internationally, influencing Iraqi public opinion, straining allied cohesion, and testing the will of the Iraqi government itself. Tactical brilliance could not guarantee strategic clarity—and each gain came at political and moral cost.&lt;/p&gt;
    &lt;p&gt;Clausewitz’s theories are no less relevant in the fight against ISIS. The battles for Mosul, Raqqa, and Aleppo offer vivid examples of Clausewitzian dynamics playing out in dense urban terrain.&lt;/p&gt;
    &lt;p&gt;The Battle of Mosul (2016–2017), the largest urban combat operation since World War II, marked both the height and unraveling of ISIS’s territorial control. The city—where ISIS declared its caliphate—was a living example of Clausewitz’s trinity: ideological violence among the people, unpredictable chance in military operations, and the overarching influence of policy and statecraft. ISIS weaponized the city’s geography to negate coalition advantages. Each alley and rooftop became a node of Clausewitzian friction, where the fog of war was compounded by hidden explosives, civilian shields, and the media theater of terror.&lt;/p&gt;
    &lt;p&gt;The struggle to recapture Raqqa (2017) similarly underscored Clausewitz’s emphasis on the strategic value of cities—but also on the cost of capturing them. Coalition forces had to balance the immediate tactical need for firepower with the long-term strategic imperative of minimizing civilian casualties and preserving infrastructure. Raqqa’s fall signaled not just military defeat for ISIS, but the collapse of its political narrative of governance and legitimacy.&lt;/p&gt;
    &lt;p&gt;Aleppo (2012–2016) offered a final case study in how urban warfare reshapes Clausewitzian dynamics. The regime of Bashar al-Assad, with Russian support, waged a prolonged campaign of attrition to reclaim the city. Aleppo’s recapture was not just a battlefield event—it was a strategic and psychological victory that reshaped the regional balance. Clausewitz’s insight that war is always shaped by the interaction of violence, politics, and chance was on full display. In Aleppo, military power served political ends—but at enormous humanitarian and reputational cost.&lt;/p&gt;
    &lt;p&gt;The Battle of Kyiv: A Clausewitzian Struggle&lt;/p&gt;
    &lt;p&gt;The 2022 Battle of Kyiv illustrates many of Clausewitz’s core principles. Russian forces launched a lightning assault on the capital, aiming to swiftly decapitate Ukraine’s political leadership and seize its strategic center of gravity. But what they encountered was not just a military defense, but a national resistance. The people, military, and government acted as one cohesive trinity. And President Volodymyr Zelenskyy’s decision to remain in Kyiv was not just political theater—it was a deliberate act of strategic will.&lt;/p&gt;
    &lt;p&gt;The defenders of Kyiv skillfully leveraged the urban environment to neutralize Russia’s advantages and impose costs at every level of engagement. What followed was a textbook display of Clausewitzian friction: stalled armored columns, logistical failures, intelligence breakdowns, and a general underestimation of local resistance. Citizen volunteers, guided by their knowledge of terrain and empowered by social networks, became a force multiplier against a numerically and technologically superior invader. Even simple tactical objectives—securing key intersections or resupplying units—became unexpectedly complex under the weight of terrain, resistance, and human error. This was the very essence of what Clausewitz warned distinguishes real war from war on paper.&lt;/p&gt;
    &lt;p&gt;Russia’s inability to capture the capital—the symbolic heart of the Ukrainian state—had cascading effects. It allowed Ukraine to garner international support, secure military resupply, and build momentum on the strategic level. In urban warfare, just holding out can be a victory. The defense of a city like Kyiv can serve not only to blunt an assault but to buy time for political conditions to shift, for alliances to strengthen, and for strategic clarity to emerge. In such contexts, endurance becomes its own form of offense.&lt;/p&gt;
    &lt;p&gt;This was not just a tactical defeat for Russia—it was a strategic failure born of a fundamental mismatch between political ambition and military means. The objective—seizing Kyiv—was politically clear, but Russia failed to align its resources, capabilities, and assumptions with that goal. Clausewitz’s admonition echoed loudly as Russian columns stalled short of the capital: “The political object is the goal, war is the means of reaching it, and means can never be considered in isolation from their purpose.” The Battle of Kyiv proved that even with overwhelming force, war conducted without coherence between ends and means is destined to fail.&lt;/p&gt;
    &lt;p&gt;Gaza and the Israel Defense Forces: Tactical Success, Strategic Strain&lt;/p&gt;
    &lt;p&gt;If Kyiv is a case study of Clausewitzian alignment of war and policy, Israel’s ongoing operations in Gaza provide another example of the dangers when the alignment falters.&lt;/p&gt;
    &lt;p&gt;The Israel Defense Forces are one of the most experienced militaries in urban warfare in the world. Israeli military operations are precise, intelligence-driven, and supported by technological superiority. Yet even these capabilities cannot eliminate the strategic dilemma of fighting in cities densely packed with civilians, under intense global scrutiny, and against nonstate actors that use the urban fabric—cities’ terrain and their people—as both shield and weapon.&lt;/p&gt;
    &lt;p&gt;Clausewitz emphasized that war is not an isolated act but part of a “continuous interaction”—including, notably, interaction with political objectives. In Gaza, the Israel Defense Forces face a situation where the tactical destruction of enemy infrastructure—tunnels, command nodes, rocket sites—does not necessarily translate to strategic success of all the war’s political goals. Every collapsed apartment building and every civilian casualty reverberates globally. The moral forces Clausewitz emphasized—public opinion and will—are not abstract; they are measurable in diplomatic isolation or support, domestic cohesion, and battlefield morale.&lt;/p&gt;
    &lt;p&gt;This is not to say the Israeli military lacks clarity in its objectives, but rather that the urban environment imposes costs and constraints that can undermine strategic coherence. As I argued in Understanding Urban Warfare, a city can be the greatest ally or the worst foe, depending on how it is approached. Clausewitz would remind any military leader that the means employed must remain proportionate and consistent with the political purpose.&lt;/p&gt;
    &lt;p&gt;Clausewitz also cautioned against rigid formulas. “Every age,” he wrote, “[has] had its own kind of war, its own limiting conditions, and its own peculiar preconceptions”. Urban warfare in the twenty-first century demands adaptation, and nowhere is this more evident than in the lessons derived from Gaza.&lt;/p&gt;
    &lt;p&gt;For the Israel Defense Forces operating in Gaza, every strike, pause, or maneuver is interpreted through political, humanitarian, and informational lenses. This is enhanced by the magnified friction of fighting in dense urban terrain. Streets can canalize movement, buildings and tunnels can conceal threats, and civilians can either support or sabotage operations.&lt;/p&gt;
    &lt;p&gt;Clausewitz, with his emphasis on uncertainty, chance, and moral forces, would have found urban warfare like that seen in Gaza to be the ultimate test of the statesman’s clarity and the commander’s judgment. In today’s information environment, that friction is amplified—a single video or narrative about the use (or misuse) of force, whether true or fabricated, can influence entire populations and political bodies. This aligns with Clausewitz’s trinity of wills—the people, the military, and the government, all three of which must be in balance for coherent strategy. In cities, that balance is constantly tested in real time and often in front of a global audience.&lt;/p&gt;
    &lt;p&gt;The Strategic Center of Gravity is Urban&lt;/p&gt;
    &lt;p&gt;Clausewitz’s concept of the “center of gravity”—the source of power that holds everything in war together—was one of his most important strategic insights. He described it as the “hub of all power and movement, on which everything depends.” In his time, contending military with the center of gravity often meant the destruction of the enemy’s main army or the occupation of its capital. But in modern warfare—especially in urban environments—the center of gravity is rarely a fixed physical point. It is dynamic, psychological, and deeply political.&lt;/p&gt;
    &lt;p&gt;Today, the center of gravity often resides in urban areas, not just as terrain to be seized but as spaces where power is concentrated: political authority, public opinion, information control, and the will of the people. Cities like Kyiv, Gaza City, Mosul, or Aleppo are not merely battlefields—they are arenas where military action collides with political meaning. Clausewitz would recognize these dynamics, because for him, the essence of war was not tactical victory but the pursuit of a political object shaped by what he called moral forces.&lt;/p&gt;
    &lt;p&gt;Again, Clausewitz wrote, “The moral elements are among the most important in war. They constitute the spirit that permeates war as a whole.” He was referring to intangible but decisive factors—public support, national will, leadership cohesion, and belief in the cause. These forces are especially visible in cities, where every strike and every image can either strengthen or fracture the political foundations of the war effort. What we might now call legitimacy in modern strategy—credibility in the eyes of a population or the international community—can be understood as the sum of these moral forces. Clausewitz didn’t use the term, but he clearly grasped its meaning and importance.&lt;/p&gt;
    &lt;p&gt;In Kyiv, the city itself became the center of gravity—not only for its political and logistical importance, but for what it symbolized. Its defense became an act of national will. In Gaza, the battle shifts between tactical objectives and a struggle over public opinion, both local and global.&lt;/p&gt;
    &lt;p&gt;In today’s urban conflicts, the political object—the goal, in Clausewitz’s terms, which must not be separated from war as the means of reaching it—is constantly under pressure. This pressure comes not just from the enemy, but also from how one’s own population, allies, and adversaries perceive the use of force. A commander may win the battle for terrain and still lose the war if public opinion collapses or the political object becomes unsustainable.&lt;/p&gt;
    &lt;p&gt;This is why the center of gravity in modern warfare often runs through the city—not because of what is physically located there, but because of what is at stake symbolically, psychologically, and politically. In cities, Clausewitz’s theory finds its sharpest edge: Moral forces meet material realities, and the balance of war can shift not through firepower alone, but through the will of those watching, enduring, or resisting.&lt;/p&gt;
    &lt;p&gt;Clausewitz in the Urban Century&lt;/p&gt;
    &lt;p&gt;Cities have become the default terrain of modern war. From Kyiv to Gaza, the battles fought today are not anomalies—they are signals. Urban warfare is not an exception to Clausewitz’s theory; it is its most vivid and volatile expression.&lt;/p&gt;
    &lt;p&gt;Cities compress all the elements Clausewitz identified as fundamental to war: violence, chance, political purpose, friction, and uncertainty. They bring the political object, the will of the people, and military action into immediate proximity—requiring a level of harmony among these forces that is difficult to achieve but critical to sustaining strategic coherence. In this space, tactical actions instantly reverberate across strategic and political spheres. Every strike is a message, every misstep a liability.&lt;/p&gt;
    &lt;p&gt;Clausewitz would demand that today’s commanders and policymakers understand that war in cities is not just about maneuver and firepower—it is about narrative, perception, endurance, and will. Modern urban warfare is fought in full view of the world, under moral scrutiny, and amid civilian populations whose support or suffering can shape the outcome as much as any weapon system.&lt;/p&gt;
    &lt;p&gt;Victory in this environment requires more than technological superiority. It demands clarity of purpose, coherence between means and ends, disciplined execution, and moral restraint—the very fundamentals Clausewitz insisted upon. These are not optional in the urban century. They are decisive.&lt;/p&gt;
    &lt;p&gt;Clausewitz offers no checklist for success in cities, but rather something more valuable. What he offers is a way to think clearly, to adapt amid chaos, and to confront the true nature of war—a contest of wills, shaped by politics, distorted by chance, and fought in the dense, contested, and morally fraught terrain of the modern city.&lt;/p&gt;
    &lt;p&gt;John Spencer is chair of urban warfare studies at the Modern War Institute, codirector of MWI’s Urban Warfare Project, and host of the Urban Warfare Project Podcast. He served twenty-five years as an infantry soldier, which included two combat tours in Iraq. He is the author of the book Connected Soldiers: Life, Leadership, and Social Connections in Modern War and coauthor of Understanding Urban Warfare.&lt;/p&gt;
    &lt;p&gt;The views expressed are those of the author and do not reflect the official position of the United States Military Academy, Department of the Army, or Department of Defense.&lt;/p&gt;
    &lt;p&gt;Image credit: Staff Sgt. Jason Hull, US Army&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45515146</guid><pubDate>Wed, 08 Oct 2025 11:56:06 +0000</pubDate></item></channel></rss>