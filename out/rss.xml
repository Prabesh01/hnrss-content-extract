<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 28 Nov 2025 13:04:23 +0000</lastBuildDate><item><title>Same-day upstream Linux support for Snapdragon 8 Elite Gen 5</title><link>https://www.qualcomm.com/developer/blog/2025/10/same-day-snapdragon-8-elite-gen-5-upstream-linux-support</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46070668</guid><pubDate>Thu, 27 Nov 2025 16:19:03 +0000</pubDate></item><item><title>A programmer-friendly I/O abstraction over io_uring and kqueue (2022)</title><link>https://tigerbeetle.com/blog/2022-11-23-a-friendly-abstraction-over-iouring-and-kqueue/</link><description>&lt;doc fingerprint="7a1e315b29e0178f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Programmer-Friendly I/O Abstraction Over io_uring and kqueue&lt;/head&gt;
    &lt;p&gt;Consider this tale of I/O and performance. We√¢ll start with blocking I/O, explore io_uring and kqueue, and take home an event loop very similar to some software you may find familiar.&lt;/p&gt;
    &lt;p&gt;This is a twist on King√¢s talk at Software You Can Love Milan √¢22.&lt;/p&gt;
    &lt;p&gt;When you want to read from a file you might &lt;code&gt;open()&lt;/code&gt; and
then call &lt;code&gt;read()&lt;/code&gt; as many times as necessary to fill a
buffer of bytes from the file. And in the opposite direction, you call
&lt;code&gt;write()&lt;/code&gt; as many times as needed until everything is
written. It√¢s similar for a TCP client with sockets, but instead of
&lt;code&gt;open()&lt;/code&gt; you first call &lt;code&gt;socket()&lt;/code&gt; and then
&lt;code&gt;connect()&lt;/code&gt; to your server. Fun stuff.&lt;/p&gt;
    &lt;p&gt;In the real world though you can√¢t always read everything you want immediately from a file descriptor. Nor can you always write everything you want immediately to a file descriptor.&lt;/p&gt;
    &lt;p&gt;You can switch a file descriptor into non-blocking mode so the call won√¢t block while data you requested is not available. But system calls are still expensive, incurring context switches and cache misses. In fact, networks and disks have become so fast that these costs can start to approach the cost of doing the I/O itself. For the duration of time a file descriptor is unable to read or write, you don√¢t want to waste time continuously retrying read or write system calls.&lt;/p&gt;
    &lt;p&gt;So you switch to io_uring on Linux or kqueue on FreeBSD/macOS. (I√¢m skipping the generation of epoll/select users.) These APIs let you submit requests to the kernel to learn about readiness: when a file descriptor is ready to read or write. You can send readiness requests in batches (also referred to as queues). Completion events, one for each submitted request, are available in a separate queue.&lt;/p&gt;
    &lt;p&gt;Being able to batch I/O like this is especially important for TCP servers that want to multiplex reads and writes for multiple connected clients.&lt;/p&gt;
    &lt;p&gt;However in io_uring, you can even go one step further. Instead of having to call &lt;code&gt;read()&lt;/code&gt; or &lt;code&gt;write()&lt;/code&gt; in userland
after a readiness event, you can request that the kernel do the
&lt;code&gt;read()&lt;/code&gt; or &lt;code&gt;write()&lt;/code&gt; itself with a buffer you
provide. Thus almost all of your I/O is done in the kernel, amortizing
the overhead of system calls.&lt;/p&gt;
    &lt;p&gt;If you haven√¢t seen io_uring or kqueue before, you√¢d probably like an example! Consider this code: a simple, minimal, not-production-ready TCP echo server.&lt;/p&gt;
    &lt;code&gt;const std = @import("std");
const os = std.os;
const linux = os.linux;
const allocator = std.heap.page_allocator;

const State = enum{ accept, recv, send };
const Socket = struct {
: os.socket_t,
     handle: [1024]u8,
     buffer: State,
     state
 };
pub fn main() !void {
const entries = 32;
     const flags = 0;
     var ring = try linux.IO_Uring.init(entries, flags);
     defer ring.deinit();
     
var server: Socket = undefined;
     .handle = try os.socket(os.AF.INET, os.SOCK.STREAM, os.IPPROTO.TCP);
     serverdefer os.closeSocket(server.handle);
     
const port = 12345;
     var addr = std.net.Address.initIp4(.{127, 0, 0, 1}, port);
     var addr_len: os.socklen_t = addr.getOsSockLen();
     
try os.setsockopt(server.handle, os.SOL.SOCKET, os.SO.REUSEADDR, &amp;amp;std.mem.toBytes(@as(c_int, 1)));
     try os.bind(server.handle, &amp;amp;addr.any, addr_len);
     const backlog = 128;
     try os.listen(server.handle, backlog);
     
.state = .accept;
     server= try ring.accept(@ptrToInt(&amp;amp;server), server.handle, &amp;amp;addr.any, &amp;amp;addr_len, 0);
     _ 
while (true) {
     = try ring.submit_and_wait(1);
         _ 
while (ring.cq_ready() &amp;gt; 0) {
         const cqe = try ring.copy_cqe();
             var client = @intToPtr(*Socket, @intCast(usize, cqe.user_data));
             
if (cqe.res &amp;lt; 0) std.debug.panic("{}({}): {}", .{
             .state,
                 client.handle,
                 client@intToEnum(os.E, -cqe.res),
                 
             });
switch (client.state) {
             .accept =&amp;gt; {
                 = try allocator.create(Socket);
                     client .handle = @intCast(os.socket_t, cqe.res);
                     client.state = .recv;
                     client= try ring.recv(@ptrToInt(client), client.handle, .{.buffer = &amp;amp;client.buffer}, 0);
                     _ = try ring.accept(@ptrToInt(&amp;amp;server), server.handle, &amp;amp;addr.any, &amp;amp;addr_len, 0);
                     _ ,
                 }.recv =&amp;gt; {
                 const read = @intCast(usize, cqe.res);
                     .state = .send;
                     client= try ring.send(@ptrToInt(client), client.handle, client.buffer[0..read], 0);
                     _ ,
                 }.send =&amp;gt; {
                 .closeSocket(client.handle);
                     os.destroy(client);
                     allocator,
                 }
             }
         }
     } }&lt;/code&gt;
    &lt;p&gt;This is a great, minimal example. But notice that this code ties io_uring behavior directly to business logic (in this case, handling echoing data between request and response). It is fine for a small example like this. But in a large application you might want to do I/O throughout the code base, not just in one place. You might not want to keep adding business logic to this single loop.&lt;/p&gt;
    &lt;p&gt;Instead, you might want to be able to schedule I/O and pass a callback (and sometimes with some application context) to be called when the event is complete.&lt;/p&gt;
    &lt;p&gt;The interface might look like:&lt;/p&gt;
    &lt;code&gt;.dispatch({
 io_dispatch// some big struct/union with relevant fields for all event types
     , my_callback); }&lt;/code&gt;
    &lt;p&gt;This is great! Now your business logic can schedule and handle I/O no matter where in the code base it is.&lt;/p&gt;
    &lt;p&gt;Under the hood it can decide whether to use io_uring or kqueue depending on what kernel it√¢s running on. The dispatch can also batch these individual calls through io_uring or kqueue to amortize system calls. The application no longer needs to know the details.&lt;/p&gt;
    &lt;p&gt;Additionally, we can use this wrapper to stop thinking about readiness events, just I/O completion. That is, if we dispatch a read event, the io_uring implementation would actually ask the kernel to read data into a buffer. Whereas the kqueue implementation would send a √¢read√¢ readiness event, do the read back in userland, and then call our callback.&lt;/p&gt;
    &lt;p&gt;And finally, now that we√¢ve got this central dispatcher, we don√¢t need spaghetti code in a loop switching on every possible submission and completion event.&lt;/p&gt;
    &lt;p&gt;Every time we call io_uring or kqueue we both submit event requests and poll for completion events. The io_uring and kqueue APIs tie these two actions together in the same system call.&lt;/p&gt;
    &lt;p&gt;To sync our requests to io_uring or kqueue we√¢ll build a &lt;code&gt;flush&lt;/code&gt; function that submits requests and polls for
completion events. (In the next section we√¢ll talk about how the user of
the central dispatch learns about completion events.)&lt;/p&gt;
    &lt;p&gt;To make &lt;code&gt;flush&lt;/code&gt; more convenient, we√¢ll build a nice
wrapper around it so that we can submit as many requests (and process as
many completion events) as possible. To avoid accidentally blocking
indefinitely we√¢ll also introduce a time limit. We√¢ll call the wrapper
&lt;code&gt;run_for_ns&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Finally we√¢ll put the user in charge of setting up a loop to call this &lt;code&gt;run_for_ns&lt;/code&gt; function, independent of normal program
execution.&lt;/p&gt;
    &lt;p&gt;This is now your traditional event loop.&lt;/p&gt;
    &lt;p&gt;You may have noticed that in the API above we passed a callback. The idea is that after the requested I/O has completed, our callback should be invoked. But the question remains: how to track this callback between the submission and completion queue?&lt;/p&gt;
    &lt;p&gt;Thankfully, io_uring and kqueue events have user data fields. The user data field is opaque to the kernel. When a submitted event completes, the kernel sends a completion event back to userland containing the user data value from the submission event.&lt;/p&gt;
    &lt;p&gt;We can store the callback in the user data field by setting it to the callback√¢s pointer casted to an integer. When the completion for a requested event comes up, we cast from the integer in the user data field back to the callback pointer. Then, we invoke the callback.&lt;/p&gt;
    &lt;p&gt;As described above, the struct for &lt;code&gt;io_dispatch.dispatch&lt;/code&gt;
could get quite large handling all the different kinds of I/O events and
their arguments. We could make our API a little more expressive by
creating wrapper functions for each event type.&lt;/p&gt;
    &lt;p&gt;So if we wanted to schedule a read function we could call:&lt;/p&gt;
    &lt;code&gt;.read(fd, &amp;amp;buf, nBytesToRead, callback); io_dispatch&lt;/code&gt;
    &lt;p&gt;Or to write, similarly:&lt;/p&gt;
    &lt;code&gt;.write(fd, buf, nBytesToWrite, callback); io_dispatch&lt;/code&gt;
    &lt;p&gt;One more thing we need to worry about is that the batch we pass to io_uring or kqueue has a fixed size (technically, kqueue allows any batch size but using that might introduce unnecessary allocations). So we√¢ll build our own queue on top of our I/O abstraction to keep track of requests that we could not immediately submit to io_uring or kqueue.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;To keep this API simple we could allocate for each entry in the queue. Or we could modify the&lt;/p&gt;&lt;code&gt;io_dispatch.X&lt;/code&gt;calls slightly to accept a struct that can be used in an intrusive linked list to contain all request context, including the callback. The latter is what we do in TigerBeetle.&lt;/quote&gt;
    &lt;p&gt;Put another way: every time code calls &lt;code&gt;io_dispatch&lt;/code&gt;,
we√¢ll try to immediately submit the requested event to io_uring or
kqueue. But if there√¢s no room, we store the event in an overflow
queue.&lt;/p&gt;
    &lt;p&gt;The overflow queue needs to be processed eventually, so we update our &lt;code&gt;flush&lt;/code&gt; function (described in Callbacks and context above) to pull
as many events from our overflow queue before submitting a batch to
io_uring or kqueue.&lt;/p&gt;
    &lt;p&gt;We√¢ve now built something similar to libuv, the I/O library that Node.js uses. And if you squint, it is basically TigerBeetle√¢s I/O library! (And interestingly enough, TigerBeetle√¢s I/O code was adopted into Bun! Open-source for the win!)&lt;/p&gt;
    &lt;p&gt;Let√¢s check out how the Darwin version of TigerBeetle√¢s I/O library (with kqueue) differs from the Linux version. As mentioned, the complete &lt;code&gt;send&lt;/code&gt; call in the
Darwin implementation waits for file descriptor readiness (through
kqueue). Once ready, the actual &lt;code&gt;send&lt;/code&gt; call is made back in
userland:&lt;/p&gt;
    &lt;code&gt;pub fn send(
self: *IO,
     comptime Context: type,
     : Context,
     contextcomptime callback: fn (
     : Context,
         context: *Completion,
         completion: SendError!usize,
         resultvoid,
     ) : *Completion,
     completion: os.socket_t,
     socket: []const u8,
     buffervoid {
 ) self.submit(
     ,
         context,
         callback,
         completion.send,
         .{
         .socket = socket,
             .buf = buffer.ptr,
             .len = @intCast(u32, buffer_limit(buffer.len)),
             ,
         }struct {
         fn do_operation(op: anytype) SendError!usize {
             return os.send(op.socket, op.buf[0..op.len], 0);
                 
             },
         }
     ); }&lt;/code&gt;
    &lt;p&gt;Compare this to the Linux version (with io_uring) where the kernel handles everything and there is no send system call in userland:&lt;/p&gt;
    &lt;code&gt;pub fn send(
self: *IO,
     comptime Context: type,
     : Context,
     contextcomptime callback: fn (
     : Context,
         context: *Completion,
         completion: SendError!usize,
         resultvoid,
     ) : *Completion,
     completion: os.socket_t,
     socket: []const u8,
     buffervoid {
 ) .* = .{
     completion.io = self,
         .context = context,
         .callback = struct {
         fn wrapper(ctx: ?*anyopaque, comp: *Completion, res: *const anyopaque) void {
             
                 callback(@intToPtr(Context, @ptrToInt(ctx)),
                     ,
                     comp@intToPtr(*const SendError!usize, @ptrToInt(res)).*,
                     
                 );
             }.wrapper,
         }.operation = .{
         .send = .{
             .socket = socket,
                 .buffer = buffer,
                 ,
             },
         }
     };// Fill out a submission immediately if possible, otherwise adds to overflow buffer
     self.enqueue(completion);
      }&lt;/code&gt;
    &lt;p&gt;Similarly, take a look at &lt;code&gt;flush&lt;/code&gt; on Linux
and macOS
for event processing. Look at &lt;code&gt;run_for_ns&lt;/code&gt; on Linux
and macOS
for the public API users must call. And finally, look at what puts this
all into practice, the loop calling &lt;code&gt;run_for_ns&lt;/code&gt; in
src/main.zig.&lt;/p&gt;
    &lt;p&gt;We√¢ve come this far and you might be wondering √¢ what about cross-platform support for Windows? The good news is that Windows also has a completion based system similar to io_uring but without batching, called IOCP. And for bonus points, TigerBeetle provides the same I/O abstraction over it! But it√¢s enough to cover just Linux and macOS in this post. :)&lt;/p&gt;
    &lt;p&gt;In both this blog post and in TigerBeetle, we implemented a single-threaded event loop. Keeping I/O code single-threaded in userspace is beneficial (whether or not I/O processing is single-threaded in the kernel is not our concern). It√¢s the simplest code and best for workloads that are not embarrassingly parallel. It is also best for determinism, which is integral to the design of TigerBeetle because it enables us to do Deterministic Simulation Testing&lt;/p&gt;
    &lt;p&gt;But there are other valid architectures for other workloads.&lt;/p&gt;
    &lt;p&gt;For workloads that are embarrassingly parallel, like many web servers, you could instead use multiple threads where each thread has its own queue. In optimal conditions, this architecture has the highest I/O throughput possible.&lt;/p&gt;
    &lt;p&gt;But if each thread has its own queue, individual threads can become starved if an uneven amount of work is scheduled on one thread. In the case of dynamic amounts of work, the better architecture would be to have a single queue but multiple worker threads doing the work made available on the queue.&lt;/p&gt;
    &lt;p&gt;Hey, maybe we√¢ll split this out so you can use it too. It√¢s written in Zig so we can easily expose a C API. Any language with a C foreign function interface (i.e. every language) should work well with it. Keep an eye on our GitHub. :)&lt;/p&gt;
    &lt;p&gt;Additional resources:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46073817</guid><pubDate>Thu, 27 Nov 2025 22:41:06 +0000</pubDate></item><item><title>250MWh 'Sand Battery' to start construction in Finland</title><link>https://www.energy-storage.news/250mwh-sand-battery-to-start-construction-in-finland-for-both-heating-and-ancillary-services/</link><description>&lt;doc fingerprint="b76f0758cbd3b7b"&gt;
  &lt;main&gt;
    &lt;p&gt;Technology provider Polar Night Energy and utility Lahti Energia have partnered for a large-scale project using Polar‚Äôs ‚ÄòSand Battery‚Äô technology for the latter‚Äôs district heating network in V√§√§ksy, Finland.&lt;/p&gt;
    &lt;p&gt;The project will have a heating power of 2MW and a thermal energy storage (TES) capacity of 250MW, making it a 125-hour system and the largest sand-based TES project once complete.&lt;/p&gt;
    &lt;p&gt;It will supply heat to Lahti Energia‚Äôs V√§√§ksy district heating network but is also large enough to participate in Fingrid‚Äôs reserve and grid balancing markets.&lt;/p&gt;
    &lt;p&gt;Polar Night Energy‚Äôs technology works by heating a sand or a similar solid material using electricity, retaining that heat and then discharging that for industrial or heating use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try Premium for just $1&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full premium access for the first month at only $1&lt;/item&gt;
      &lt;item&gt;Converts to an annual rate after 30 days unless cancelled&lt;/item&gt;
      &lt;item&gt;Cancel anytime during the trial period&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Premium Benefits&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Expert industry analysis and interviews&lt;/item&gt;
      &lt;item&gt;Digital access to PV Tech Power journal&lt;/item&gt;
      &lt;item&gt;Exclusive event discounts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Or get the full Premium subscription right away&lt;/head&gt;
    &lt;head rend="h3"&gt;Or continue reading this article for free&lt;/head&gt;
    &lt;p&gt;The project will cut fossil-based emissions in the V√§√§ksy district heating network by around 60% each year, by reducing natural gas use bu 80% and also decreasing wood chip consumption.&lt;/p&gt;
    &lt;p&gt;It follows Polar Night Energy completing and putting a 1MW/100MWh Sand Battery TES project into commercial operations this summer, for another utility Loviisan L√§mp√∂. That project uses soapstone as its storage medium, a byproduct of ceramics production.&lt;/p&gt;
    &lt;p&gt;This latest project will use locally available natural sand, held in a container 14m high and 15m wide. Lahti Energia received a grant for the project from state body Business Finland.&lt;/p&gt;
    &lt;p&gt;Polar Night Energy will act as the main contractor for the construction project, with on-site work beginning in early 2026, and the Sand Battery will be completed in summer 2027.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe want to offer our customers affordable district heating and make use of renewable energy in our heat production. The scale of this Sand Battery also enables us to participate in Fingrid‚Äôs reserve and grid balancing markets. As the share of weather-dependent energy grows in the grid, the Sand Battery will contribute to balancing electricity supply and demand‚Äù, says Jouni Haikarainen, CEO of Lahti Energia.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46073855</guid><pubDate>Thu, 27 Nov 2025 22:48:44 +0000</pubDate></item><item><title>Vsora Jotunn-8 5nm European inference chip</title><link>https://vsora.com/products/jotunn-8/</link><description>&lt;doc fingerprint="a495c47b155c4c8f"&gt;
  &lt;main&gt;
    &lt;p&gt;In modern data centers, success means deploying trained models with blistering speed, minimal cost, and effortless scalability. Designing and operating inference systems requires balancing key factors such as high throughput, low latency, optimized power consumption, and sustainable infrastructure. Achieving optimal performance while maintaining cost and energy efficiency is critical to meeting the growing demand for large-scale, real-time AI services across a variety of applications.&lt;/p&gt;
    &lt;p&gt;Unlock the full potential of your AI investments with our high-performance inference solutions. Engineered for speed, efficiency, and scalability, our platform ensures your AI models deliver maximum impact‚Äîat lower operational costs and with a commitment to sustainability. Whether you‚Äôre scaling up deployments or optimizing existing infrastructure, we provide the technology and expertise to help you stay competitive and drive business growth.&lt;/p&gt;
    &lt;p&gt;This is not just faster inference. It‚Äôs a new foundation for AI at scale.&lt;/p&gt;
    &lt;p&gt;In the world of AI data centers, speed, efficiency, and scale aren‚Äôt optional‚Äîthey‚Äôre everything. Jotunn8, our ultra-high-performance inference chip is built to deploy trained models with lightning-fast throughput, minimal cost, and maximum scalability. Designed around what matters most‚Äîperformance, cost-efficiency, and sustainability‚Äîthey deliver the power to run AI at scale, without compromise!&lt;/p&gt;
    &lt;p&gt;Why it matters: Critical for real-time applications like chatbots, fraud detection, and search.&lt;/p&gt;
    &lt;p&gt;Reasoning models, Generative AI and Agentic AI are increasingly being combined to build more capable and reliable systems. Generative AI provide flexibility and language fluency. Reasoning models provide rigor and correctness. Agentic frameworks provide autonomy and decision-making. The VSORA architecture enables smooth and easy integration of these algorithms, providing near-theory performance.&lt;/p&gt;
    &lt;p&gt;Why it matters: AI inference is often run at massive scale ‚Äì reducing cost per inference is essential for business viability.&lt;/p&gt;
    &lt;p&gt;Unmatched Performance at the Edge with Edge AI.&lt;/p&gt;
    &lt;p&gt;Fully programmable&lt;/p&gt;
    &lt;p&gt;Algorithm agnostic&lt;/p&gt;
    &lt;p&gt;Host processor agnostic&lt;/p&gt;
    &lt;p&gt;RISC-V core to offload &amp;amp; run AI completely on-chip&lt;/p&gt;
    &lt;p&gt;Tyr 4&lt;lb/&gt;fp8: 1600 Tflops&lt;lb/&gt;fp16: 400 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 2&lt;lb/&gt;fp8: 800 Tflops&lt;lb/&gt;fp16: 200 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 4&lt;lb/&gt;fp8/int8: 50 Tflops&lt;lb/&gt;fp16/int16: 25 Tflops&lt;lb/&gt;fp32/int32: 12 Tflops&lt;/p&gt;
    &lt;p&gt;Tyr 2&lt;lb/&gt;fp8/int8: 25 Tflops&lt;lb/&gt;fp16/int16: 12 Tflops&lt;lb/&gt;fp32/int32: 6 Tflops&lt;/p&gt;
    &lt;p&gt;Close to theory efficiency&lt;/p&gt;
    &lt;p&gt;Fully programmable&lt;/p&gt;
    &lt;p&gt;Algorithm agnostic&lt;/p&gt;
    &lt;p&gt;Host processor agnostic&lt;/p&gt;
    &lt;p&gt;RISC-V cores to offload host &lt;lb/&gt;&amp;amp; run AI completely on-chip.&lt;/p&gt;
    &lt;p&gt;fp8: 3200 Tflops&lt;lb/&gt;fp16: 800 Tflops &lt;/p&gt;
    &lt;p&gt;fp8/int8: 100 Tflops&lt;lb/&gt;fp16/int16: 50 Tflops&lt;lb/&gt;fp32/int32: 25 Tflops&lt;lb/&gt;Close to theory efficiency&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46074111</guid><pubDate>Thu, 27 Nov 2025 23:30:11 +0000</pubDate></item><item><title>How Charles M Schulz created Charlie Brown and Snoopy (2024)</title><link>https://www.bbc.com/culture/article/20241205-how-charles-m-schulz-created-charlie-brown-and-snoopy</link><description>&lt;doc fingerprint="2e364174bde00afd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'You have to just draw something that you hope is funny': How Charles M Schulz created Charlie Brown and Snoopy&lt;/head&gt;
    &lt;p&gt;Charles M Schulz drew his beloved Peanuts strip for 50 years until his announcement on 14 December 1999 that ill health was forcing him to retire. In History looks at how an unassuming cartoonist built a billion-dollar empire out of the lives of a group of children, a dog and a bird.&lt;/p&gt;
    &lt;p&gt;Charles M Schulz's timeless creation Charlie Brown may have been as popular as any character in all of literature, but the cartoonist was modest about the scope of his miniature parables. In a 1977 BBC interview, he said: "I'm talking only about the minor everyday problems in life. Leo Tolstoy dealt with the major problems of the world. I'm only dealing with why we all have the feeling that people don't like us."&lt;/p&gt;
    &lt;p&gt;This did not mean that he felt as if he was dealing with trivial matters. He said: "I'm always very much offended when someone asks me, 'Do I ever do satire on the social condition?' Well, I do it almost every day. And they say, 'Well, do you ever do political things?' I say, 'I do things which are more important than politics. I'm dealing with love and hate and mistrust and fear and insecurity.'"&lt;/p&gt;
    &lt;p&gt;While Charlie Brown may have been the eternal failure, the universal feelings that Schulz channelled helped make Peanuts a global success. Born in 1922, Schulz drew every single Peanuts strip himself from 1950 until his death in February 2000. It was so popular that Nasa named two of the modules in its May 1969 Apollo 10 lunar mission after Charlie Brown and Snoopy. The strip was syndicated in more than 2,600 newspapers worldwide, and inspired films, music and countless items of merchandise.&lt;/p&gt;
    &lt;p&gt;Part of its success, according to the writer Umberto Eco, was that it worked on different levels. He wrote: "Peanuts charms both sophisticated adults and children with equal intensity, as if each reader found there something for himself, and it is always the same thing, to be enjoyed in two different keys. Peanuts is thus a little human comedy for the innocent reader and for the sophisticated."&lt;/p&gt;
    &lt;p&gt;Schulz's initial reason for focusing on children in the strip was strictly commercial. In 1990, he told the BBC: "I always hate to say it, but I drew little kids because this is what sold. I wanted to draw something, I didn't know what it was, but it just seemed as if whenever I drew children, these were the cartoons that editors seemed to like the best. And so, back in 1950, I mailed a batch of cartoons to New York City, to United Features Syndicate, and they said they liked them, and so ever since I've been drawing little kids."&lt;/p&gt;
    &lt;p&gt;IN HISTORY&lt;/p&gt;
    &lt;p&gt;In History is a series which uses the BBC's unique audio and video archive to explore historical events that still resonate today. Subscribe to the accompanying weekly newsletter.&lt;/p&gt;
    &lt;p&gt;Of Snoopy and Charlie Brown, he said: "I've always been a little bit intrigued by the fact that dogs apparently tolerate the actions of the children with whom they are playing. It's almost as if the dogs are smarter than the kids. I think also that the characters I have serve as a good outlet for any idea that I may come up with. I never think of an idea and then find that I have no way of using it. I can use any idea that I think of because I've got the right repertory company."&lt;/p&gt;
    &lt;p&gt;Schulz called upon some of his earliest experiences as a shy child to create the strip. As a teenager, he studied drawing by correspondence course because he was too reticent to attend art school in person. Speaking in 1977, he said: "I couldn't see myself sitting in a room where everyone else in the room could draw much better than I, and this way I was protected by drawing at home and simply mailing my drawings in and having them criticised. I wish I had a better education, but I think that my entire background made me well suited for what I do.&lt;/p&gt;
    &lt;p&gt;"If I could write better than I can, perhaps I would have tried to become a novelist, and I might have become a failure. If I could draw better than I can, I might have tried to become an illustrator or an artist and would have failed there, but my entire being seems to be just right for being a cartoonist."&lt;/p&gt;
    &lt;head rend="h2"&gt;Never give up&lt;/head&gt;
    &lt;p&gt;Peanuts remained remarkably consistent despite the relentless publishing schedule, and Schulz would not let the expectations of his millions of fans become a distraction. He said: "You have to kind of bend over the drawing board, shut the world out and just draw something that you hope is funny. Cartooning is still drawing funny pictures, whether they're just silly little things or rather meaningful political cartoons, but it's still drawing something funny, and that's all you should think about at that time ‚Äì keep kind of a light feeling.&lt;/p&gt;
    &lt;p&gt;"I suppose when a composer is composing well, the music is coming faster than he can think of it, and when I have a good idea I can hardly get the words down fast enough. I'm afraid that they will leave me before I get them down on the paper. Sometimes my hand will literally shake with excitement as I'm drawing it because I'm having a good time. Unfortunately, this does not happen every day."&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;‚Ä¢ Julie Andrews on being 'teased' for Mary Poppins&lt;/p&gt;
    &lt;p&gt;Despite his modesty, Schulz insisted he was always confident that Peanuts would be a hit. He said: "I mean, when you sign up to play at Wimbledon, you expect to win. Obviously, there are a lot of things that I didn't anticipate, like Snoopy's going to the Moon and things like that, but I always had hopes it would become big."&lt;/p&gt;
    &lt;p&gt;Schulz generally worked five weeks in advance. On 14 December 1999, fans were dismayed to learn that he would be hanging up his pen because he had cancer. He said that his cartoon for 3 January 2000 would be the final daily release. It would be followed on 13 February with the final strip for a Sunday newspaper. He died one day before that last strip ran.&lt;/p&gt;
    &lt;p&gt;In it, Schulz wrote: "I have been grateful over the years for the loyalty of our editors and the wonderful support and love expressed to me by fans of the comic strip. Charlie Brown, Snoopy, Linus, Lucy... how can I ever forget them..."&lt;/p&gt;
    &lt;p&gt;Back in 1977, Schulz insisted that the cartoonist's role was mostly to point out problems rather than trying to solve them, but there was one lesson that people could take from his work. He said: "I suppose one of the solutions is, as Charlie Brown, just to keep on trying. He never gives up. And if anybody should give up, he should."&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;For more stories and never-before-published radio scripts to your inbox, sign up to the In History newsletter, while The Essential List delivers a handpicked selection of features and insights twice a week.&lt;/p&gt;
    &lt;p&gt;For more Culture stories from the BBC, follow us on Facebook, X and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46074362</guid><pubDate>Fri, 28 Nov 2025 00:10:38 +0000</pubDate></item><item><title>Shor's algorithm: the one quantum algo that ends RSA/ECC tomorrow</title><link>https://blog.ellipticc.com/posts/what-is-shors-algorithm-and-why-its-the-single-biggest-threat-to-classical-cryptography/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075219</guid><pubDate>Fri, 28 Nov 2025 03:19:51 +0000</pubDate></item><item><title>Pocketbase ‚Äì open-source realtime back end in 1 file</title><link>https://pocketbase.io/</link><description>&lt;doc fingerprint="91362922a55dae74"&gt;
  &lt;main&gt;
    &lt;code&gt;// JavaScript SDK
import PocketBase from 'pocketbase';

const pb = new PocketBase('http://127.0.0.1:8090');

...

// list and search for 'example' collection records
const list = await pb.collection('example').getList(1, 100, {
    filter: 'title != "" &amp;amp;&amp;amp; created &amp;gt; "2022-08-01"',
    sort: '-created,title',
});

// or fetch a single 'example' collection record
const record = await pb.collection('example').getOne('RECORD_ID');

// delete a single 'example' collection record
await pb.collection('example').delete('RECORD_ID');

// create a new 'example' collection record
const newRecord = await pb.collection('example').create({
    title: 'Lorem ipsum dolor sit amet',
});

// subscribe to changes in any record from the 'example' collection
pb.collection('example').subscribe('*', function (e) {
    console.log(e.record);
});

// stop listening for changes in the 'example' collection
pb.collection('example').unsubscribe();&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075320</guid><pubDate>Fri, 28 Nov 2025 03:45:04 +0000</pubDate></item><item><title>Migrating to Positron, a next-generation data science IDE for Python and R</title><link>https://posit.co/blog/positron-migration-guides</link><description>&lt;doc fingerprint="8daf6973e9b8e2c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Guides for migrating to Positron&lt;/head&gt;
    &lt;p&gt;Since Positron was released from beta, we‚Äôve been working hard to create documentation that could help you, whether you are curious about the IDE or interested in switching. We‚Äôve released two migration guides to help you on your journey, which you can find linked below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migrating to Positron from VS Code&lt;/head&gt;
    &lt;p&gt;Positron is a next-generation IDE for data science, built by Posit PBC. It‚Äôs built on Code OSS, the open-source core of Visual Studio Code, which means that many of the features and keyboard shortcuts you‚Äôre familiar with are already in place.&lt;/p&gt;
    &lt;p&gt;However, Positron is specifically designed for data work and includes integrated tools that aren‚Äôt available in VS Code by default. These include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A built-in data explorer: This feature gives you a spreadsheet-style view of your dataframes, making it easy to inspect, sort, and filter data.&lt;/item&gt;
      &lt;item&gt;An interactive console and variables pane: Positron lets you execute code interactively and view the variables and objects in your session, similar to a traditional data science IDE.&lt;/item&gt;
      &lt;item&gt;AI assistance: Positron Assistant is a powerful AI tool for data science that can generate and refine code, debug issues, and guide you through exploratory data analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the VS Code migration guide here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migrating to Positron from RStudio&lt;/head&gt;
    &lt;p&gt;We anticipate many RStudio users will be curious about Positron. When building Positron, we strived to create a familiar interface while adding extensibility and new features, as well as native support for multiple languages. Positron is designed for data scientists and analysts who work with both R and Python and want a flexible, modern, and powerful IDE.&lt;/p&gt;
    &lt;p&gt;Key features for RStudio users include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native multi-language support: Positron is a polyglot IDE, designed from the ground up to support both R and Python seamlessly.&lt;/item&gt;
      &lt;item&gt;Familiar interface: We designed Positron with a layout similar to RStudio, so you‚Äôll feel right at home with the editor, console, and file panes. We also offer an option to use your familiar RStudio keyboard shortcuts.&lt;/item&gt;
      &lt;item&gt;Extensibility: Because Positron is built on Code OSS, you can use thousands of extensions from the Open VSX marketplace to customize your IDE and workflow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the RStudio migration guide here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Migration walkthroughs in Positron&lt;/head&gt;
    &lt;p&gt;Also, check out our migration walkthroughs in Positron itself; find them by searching ‚ÄúWelcome: Open Walkthrough‚Äù in the Command Palette (hit the shortcut Cmd + Shift + P to open the Command Palette), or on the Welcome page when you open Positron:&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs next&lt;/head&gt;
    &lt;p&gt;We‚Äôre committed to making your transition as smooth as possible, and we‚Äôll be continuing to add to these migration guides. Look out for guides for Jupyter users and more!&lt;/p&gt;
    &lt;p&gt;We‚Äôd love to hear from you. What other guides would you like to see? What features would make your transition easier? Join the conversation in our GitHub Discussions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075462</guid><pubDate>Fri, 28 Nov 2025 04:15:34 +0000</pubDate></item><item><title>Beads ‚Äì A memory upgrade for your coding agent</title><link>https://github.com/steveyegge/beads</link><description>&lt;doc fingerprint="2410fd5a88a3a19b"&gt;
  &lt;main&gt;
    &lt;p&gt;Give your coding agent a memory upgrade&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Hash-based IDs eliminate merge conflicts and collision issues!&lt;/p&gt;&lt;p&gt;Previous versions used sequential IDs (bd-1, bd-2, bd-3...) which caused frequent collisions when multiple agents or branches created issues concurrently. Version 0.20.1 switches to hash-based IDs (bd-a1b2, bd-f14c, bd-3e7a...) that are collision-resistant and merge-friendly.&lt;/p&gt;&lt;p&gt;What's new: ‚úÖ Multi-clone, multi-branch, multi-agent workflows now work reliably&lt;/p&gt;&lt;lb/&gt;What changed: Issue IDs are now short hashes instead of sequential numbers&lt;lb/&gt;Migration: Run&lt;code&gt;bd migrate&lt;/code&gt;to upgrade existing databases (optional - old DBs still work)&lt;p&gt;Hash IDs use progressive length scaling (4/5/6 characters) with birthday paradox math to keep collisions extremely rare while maintaining human readability. See "Hash-Based Issue IDs" section below for details.&lt;/p&gt;&lt;/quote&gt;
    &lt;quote&gt;&lt;g-emoji&gt;‚ö†Ô∏è&lt;/g-emoji&gt;Alpha Status: This project is in active development. The core features work well, but expect API changes before 1.0. Use for development/internal projects first.&lt;/quote&gt;
    &lt;p&gt;Beads is a lightweight memory system for coding agents, using a graph-based issue tracker. Four kinds of dependencies work to chain your issues together like beads, making them easy for agents to follow for long distances, and reliably perform complex task streams in the right order.&lt;/p&gt;
    &lt;p&gt;Drop Beads into any project where you're using a coding agent, and you'll enjoy an instant upgrade in organization, focus, and your agent's ability to handle long-horizon tasks over multiple compaction sessions. Your agents will use issue tracking with proper epics, rather than creating a swamp of rotten half-implemented markdown plans.&lt;/p&gt;
    &lt;p&gt;Instant start:&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/steveyegge/beads/main/scripts/install.sh | bash&lt;/code&gt;
    &lt;p&gt;Then tell your coding agent to start using the &lt;code&gt;bd&lt;/code&gt; tool instead of markdown for all new work, somewhere in your &lt;code&gt;AGENTS.md&lt;/code&gt; or &lt;code&gt;CLAUDE.md&lt;/code&gt;. That's all there is to it!&lt;/p&gt;
    &lt;p&gt;You don't use Beads directly as a human. Your coding agent will file and manage issues on your behalf. They'll file things they notice automatically, and you can ask them at any time to add or update issues for you.&lt;/p&gt;
    &lt;p&gt;Beads gives agents unprecedented long-term planning capability, solving their amnesia when dealing with complex nested plans. They can trivially query the ready work, orient themselves, and land on their feet as soon as they boot up.&lt;/p&gt;
    &lt;p&gt;Agents using Beads will no longer silently pass over problems they notice due to lack of context space -- instead, they will automatically file issues for newly-discovered work as they go. No more lost work, ever.&lt;/p&gt;
    &lt;p&gt;Beads issues are backed by git, but through a clever design it manages to act like a managed, centrally hosted SQL database shared by all of the agents working on a project (repo), even across machines.&lt;/p&gt;
    &lt;p&gt;Beads even improves work auditability. The issue tracker has a sophisticated audit trail, which agents can use to reconstruct complex operations that may have spanned multiple sessions.&lt;/p&gt;
    &lt;p&gt;Agents report that they enjoy working with Beads, and they will use it spontaneously for both recording new work and reasoning about your project in novel ways. Whether you are a human or an AI, Beads lets you have more fun and less stress with agentic coding.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚ú® Zero setup - &lt;code&gt;bd init&lt;/code&gt;creates project-local database (and your agent will do it)&lt;/item&gt;
      &lt;item&gt;üîó Dependency tracking - Four dependency types (blocks, related, parent-child, discovered-from)&lt;/item&gt;
      &lt;item&gt;üìã Ready work detection - Automatically finds issues with no open blockers&lt;/item&gt;
      &lt;item&gt;ü§ñ Agent-friendly - &lt;code&gt;--json&lt;/code&gt;flags for programmatic integration&lt;/item&gt;
      &lt;item&gt;üì¶ Git-versioned - JSONL records stored in git, synced across machines&lt;/item&gt;
      &lt;item&gt;üåç Distributed by design - Agents on multiple machines share one logical database via git&lt;/item&gt;
      &lt;item&gt;üöÄ Optional Agent Mail - Real-time multi-agent coordination (&amp;lt;100ms vs 2-5s git sync, 98.5% reduction in git traffic)&lt;/item&gt;
      &lt;item&gt;üîê Protected branch support - Works with GitHub/GitLab protected branches via separate sync branch&lt;/item&gt;
      &lt;item&gt;üèóÔ∏è Extensible - Add your own tables to the SQLite database&lt;/item&gt;
      &lt;item&gt;üîç Multi-project isolation - Each project gets its own database, auto-discovered by directory&lt;/item&gt;
      &lt;item&gt;üå≤ Dependency trees - Visualize full dependency graphs&lt;/item&gt;
      &lt;item&gt;üé® Beautiful CLI - Colored output for humans, JSON for bots&lt;/item&gt;
      &lt;item&gt;üíæ Full audit trail - Every change is logged&lt;/item&gt;
      &lt;item&gt;‚ö° High performance - Batch operations for bulk imports (1000 issues in ~950ms)&lt;/item&gt;
      &lt;item&gt;üóúÔ∏è Memory decay - Semantic compaction gracefully reduces old closed issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Linux users: Beads requires glibc 2.32+ (Ubuntu 22.04+, Debian 11+, RHEL 9+, or equivalent).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Supported: Ubuntu 22.04+ (Jammy), Debian 11+ (Bullseye), Fedora 34+, RHEL 9+&lt;/item&gt;
      &lt;item&gt;‚ùå Not supported: Ubuntu 20.04 (glibc 2.31), Debian 10 (glibc 2.28), CentOS 7, RHEL 8&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ubuntu 20.04 users: Standard support ended April 2025. Please upgrade to Ubuntu 22.04+ or build from source:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/steveyegge/beads.git
cd beads
go build -o bd ./cmd/bd&lt;/code&gt;
    &lt;p&gt;macOS/Windows: No special requirements.&lt;/p&gt;
    &lt;p&gt;npm (Node.js environments, Claude Code for Web):&lt;/p&gt;
    &lt;code&gt;npm install -g @beads/bd&lt;/code&gt;
    &lt;p&gt;Quick install (macOS / Linux):&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/steveyegge/beads/main/scripts/install.sh | bash&lt;/code&gt;
    &lt;p&gt;Quick install (Windows - PowerShell):&lt;/p&gt;
    &lt;code&gt;irm https://raw.githubusercontent.com/steveyegge/beads/main/install.ps1 | iex&lt;/code&gt;
    &lt;p&gt;Homebrew (macOS/Linux):&lt;/p&gt;
    &lt;code&gt;brew tap steveyegge/beads
brew install bd&lt;/code&gt;
    &lt;p&gt;For full, platform-specific instructions (Windows, Arch Linux, manual builds, IDE integrations, etc.) see the canonical guide in docs/INSTALLING.md.&lt;/p&gt;
    &lt;p&gt;Claude Code for Web: See npm-package/CLAUDE_CODE_WEB.md for SessionStart hook setup.&lt;/p&gt;
    &lt;p&gt;Beads is designed for AI coding agents to use on your behalf. Setup takes 30 seconds:&lt;/p&gt;
    &lt;p&gt;You run this once (humans only):&lt;/p&gt;
    &lt;code&gt;# In your project root:
bd init

# For OSS contributors (fork workflow):
bd init --contributor

# For team members (branch workflow):
bd init --team

# For protected branches (GitHub/GitLab):
bd init --branch beads-metadata

# bd will:
# - Create .beads/ directory with database
# - Import existing issues from git (if any)
# - Prompt to install git hooks (recommended: say yes)
# - Prompt to configure git merge driver (recommended: say yes)
# - Auto-start daemon for sync

# Then tell your agent about bd:
echo "\nBEFORE ANYTHING ELSE: run 'bd onboard' and follow the instructions" &amp;gt;&amp;gt; AGENTS.md&lt;/code&gt;
    &lt;p&gt;Protected branches? If your &lt;code&gt;main&lt;/code&gt; branch is protected, use &lt;code&gt;bd init --branch beads-metadata&lt;/code&gt; to commit issue updates to a separate branch. See docs/PROTECTED_BRANCHES.md for details.&lt;/p&gt;
    &lt;p&gt;Your agent does the rest: Next time your agent starts, it will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run &lt;code&gt;bd onboard&lt;/code&gt;and receive integration instructions&lt;/item&gt;
      &lt;item&gt;Add bd workflow documentation to AGENTS.md&lt;/item&gt;
      &lt;item&gt;Update CLAUDE.md with a note (if present)&lt;/item&gt;
      &lt;item&gt;Remove the bootstrap instruction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For agents setting up repos: Use &lt;code&gt;bd init --quiet&lt;/code&gt; for non-interactive setup (auto-installs git hooks and merge driver, no prompts).&lt;/p&gt;
    &lt;p&gt;For new repo clones: Run &lt;code&gt;bd init&lt;/code&gt; (or &lt;code&gt;bd init --quiet&lt;/code&gt; for agents) to import existing issues from &lt;code&gt;.beads/issues.jsonl&lt;/code&gt; automatically.&lt;/p&gt;
    &lt;p&gt;Git merge driver: During &lt;code&gt;bd init&lt;/code&gt;, beads configures git to use &lt;code&gt;bd merge&lt;/code&gt; for intelligent JSONL merging. This prevents conflicts when multiple branches modify issues. Skip with &lt;code&gt;--skip-merge-driver&lt;/code&gt; if needed. To configure manually later:&lt;/p&gt;
    &lt;code&gt;git config merge.beads.driver "bd merge %A %O %A %B"
git config merge.beads.name "bd JSONL merge driver"
echo ".beads/beads.jsonl merge=beads" &amp;gt;&amp;gt; .gitattributes&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;bd init&lt;/code&gt; creates these files in your repository:&lt;/p&gt;
    &lt;p&gt;Should be committed to git:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;.gitattributes&lt;/code&gt;- Configures git merge driver for intelligent JSONL merging (critical for team collaboration)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/beads.jsonl&lt;/code&gt;- Issue data in JSONL format (source of truth, synced via git)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/deletions.jsonl&lt;/code&gt;- Deletion manifest for cross-clone propagation (tracks deleted issues)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/config.yaml&lt;/code&gt;- Repository configuration template&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/README.md&lt;/code&gt;- Documentation about beads for repository visitors&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/metadata.json&lt;/code&gt;- Database metadata&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Should be in &lt;code&gt;.gitignore&lt;/code&gt; (local-only):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;.beads/beads.db&lt;/code&gt;- SQLite cache (auto-synced with JSONL)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/beads.db-*&lt;/code&gt;- SQLite journal files&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/bd.sock&lt;/code&gt;/&lt;code&gt;.beads/bd.pipe&lt;/code&gt;- Daemon communication socket&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.beads/.exclusive-lock&lt;/code&gt;- Daemon lock file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;.git/beads-worktrees/&lt;/code&gt;- Git worktrees (only created when using protected branch workflows)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;.gitignore&lt;/code&gt; entries are automatically created inside &lt;code&gt;.beads/.gitignore&lt;/code&gt; by &lt;code&gt;bd init&lt;/code&gt;, but your project's root &lt;code&gt;.gitignore&lt;/code&gt; should also exclude the database and daemon files if you want to keep your git status clean.&lt;/p&gt;
    &lt;p&gt;Using devcontainers? Open the repository in a devcontainer (GitHub Codespaces or VS Code Remote Containers) and bd will be automatically installed with git hooks configured. See .devcontainer/README.md for details.&lt;/p&gt;
    &lt;p&gt;Want to use beads in your local clone without other collaborators seeing any beads-related files? Use stealth mode:&lt;/p&gt;
    &lt;code&gt;bd init --stealth&lt;/code&gt;
    &lt;p&gt;Stealth mode configures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Global gitignore (&lt;code&gt;~/.config/git/ignore&lt;/code&gt;) - Ignores&lt;code&gt;**/.beads/&lt;/code&gt;and&lt;code&gt;**/.claude/settings.local.json&lt;/code&gt;globally&lt;/item&gt;
      &lt;item&gt;Claude Code settings (&lt;code&gt;.claude/settings.local.json&lt;/code&gt;) - Adds&lt;code&gt;bd onboard&lt;/code&gt;instruction for AI agents&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Perfect for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Personal experimentation with beads&lt;/item&gt;
      &lt;item&gt;Working on repos where not everyone uses beads yet&lt;/item&gt;
      &lt;item&gt;Keeping your issue tracking private while contributing to open source projects&lt;/item&gt;
      &lt;item&gt;AI agents that should use beads without affecting the main repo&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What stays invisible to others:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No &lt;code&gt;.beads/&lt;/code&gt;directory tracked in git&lt;/item&gt;
      &lt;item&gt;No AGENTS.md or README.md mentions of beads&lt;/item&gt;
      &lt;item&gt;No local &lt;code&gt;.gitattributes&lt;/code&gt;or&lt;code&gt;.gitignore&lt;/code&gt;modifications&lt;/item&gt;
      &lt;item&gt;Your beads database and issues remain local-only&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How it works: The global git configuration handles beads merging automatically, while the global gitignore ensures beads files never get committed to shared repos. Your AI agents get the onboard instruction automatically without exposing beads to other repo collaborators.&lt;/p&gt;
    &lt;p&gt;Most tasks will be created and managed by agents during conversations. You can check on things with:&lt;/p&gt;
    &lt;code&gt;bd list                  # See what's being tracked
bd show &amp;lt;issue-id&amp;gt;       # Review a specific issue
bd ready                 # See what's ready to work on
bd dep tree &amp;lt;issue-id&amp;gt;   # Visualize dependencies&lt;/code&gt;
    &lt;p&gt;Run the interactive guide to learn the full workflow:&lt;/p&gt;
    &lt;code&gt;bd quickstart&lt;/code&gt;
    &lt;p&gt;Quick reference for agent workflows:&lt;/p&gt;
    &lt;code&gt;# Find ready work
bd ready --json | jq '.[0]'

# Create issues during work
bd create "Discovered bug" -t bug -p 0 --json

# Link discovered work back to parent
bd dep add &amp;lt;new-id&amp;gt; &amp;lt;parent-id&amp;gt; --type discovered-from

# Update status
bd update &amp;lt;issue-id&amp;gt; --status in_progress --json

# Complete work
bd close &amp;lt;issue-id&amp;gt; --reason "Implemented" --json&lt;/code&gt;
    &lt;p&gt;Recommendation for project maintainers: Add a session-ending protocol to your project's &lt;code&gt;AGENTS.md&lt;/code&gt; file to ensure agents properly manage issue tracking and sync the database before finishing work.&lt;/p&gt;
    &lt;p&gt;This pattern has proven invaluable for maintaining database hygiene and preventing lost work. Here's what to include (adapt for your workflow):&lt;/p&gt;
    &lt;p&gt;1. File/update issues for remaining work&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Agents should proactively create issues for discovered bugs, TODOs, and follow-up tasks&lt;/item&gt;
      &lt;item&gt;Close completed issues and update status for in-progress work&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2. Run quality gates (if applicable)&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tests, linters, builds - only if code changes were made&lt;/item&gt;
      &lt;item&gt;File P0 issues if builds are broken&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;3. Sync the issue tracker carefully&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Work methodically to ensure local and remote issues merge safely&lt;/item&gt;
      &lt;item&gt;Handle git conflicts thoughtfully (sometimes accepting remote and re-importing)&lt;/item&gt;
      &lt;item&gt;Goal: clean reconciliation where no issues are lost&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;4. Verify clean state&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All changes committed and pushed&lt;/item&gt;
      &lt;item&gt;No untracked files remain&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;5. Choose next work&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Provide a formatted prompt for the next session with context&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See the "Landing the Plane" section in this project's documentation for a complete example you can adapt. The key insight: explicitly reminding agents to maintain issue tracker hygiene prevents the common problem of agents creating issues during work but forgetting to sync them at session end.&lt;/p&gt;
    &lt;p&gt;Here's the crazy part: bd acts like a centralized database, but it's actually distributed via git.&lt;/p&gt;
    &lt;p&gt;When you install bd on any machine with your project repo, you get:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Full query capabilities (dependencies, ready work, etc.)&lt;/item&gt;
      &lt;item&gt;‚úÖ Fast local operations (&amp;lt;100ms via SQLite)&lt;/item&gt;
      &lt;item&gt;‚úÖ Shared state across all machines (via git)&lt;/item&gt;
      &lt;item&gt;‚úÖ No server, no daemon required, no configuration&lt;/item&gt;
      &lt;item&gt;‚úÖ AI-assisted merge conflict resolution&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How it works: Each machine has a local SQLite cache (&lt;code&gt;.beads/*.db&lt;/code&gt;, gitignored). Source of truth is JSONL (&lt;code&gt;.beads/issues.jsonl&lt;/code&gt;, committed to git). Auto-sync keeps them in sync: SQLite ‚Üí JSONL after CRUD operations (5-second debounce), JSONL ‚Üí SQLite when JSONL is newer (e.g., after &lt;code&gt;git pull&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The result: Agents on your laptop, your desktop, and your coworker's machine all query and update what feels like a single shared database, but it's really just git doing what git does best - syncing text files across machines.&lt;/p&gt;
    &lt;p&gt;No PostgreSQL instance. No MySQL server. No hosted service. Just install bd, clone the repo, and you're connected to the "database."&lt;/p&gt;
    &lt;p&gt;bd automatically syncs your local database with git:&lt;/p&gt;
    &lt;p&gt;Making changes (auto-export):&lt;/p&gt;
    &lt;code&gt;bd create "Fix bug" -p 1
bd update bd-a1b2 --status in_progress
# bd automatically exports to .beads/issues.jsonl after 5 seconds

git add .beads/issues.jsonl
git commit -m "Working on bd-a1b2"
git push&lt;/code&gt;
    &lt;p&gt;Pulling changes (auto-import):&lt;/p&gt;
    &lt;code&gt;git pull
# bd automatically detects JSONL is newer and imports on next command

bd ready  # Fresh data from git!
bd list   # Shows issues from other machines&lt;/code&gt;
    &lt;p&gt;Manual sync (optional):&lt;/p&gt;
    &lt;code&gt;bd sync  # Immediately flush pending changes and import latest JSONL&lt;/code&gt;
    &lt;p&gt;For zero-lag sync, install the git hooks:&lt;/p&gt;
    &lt;code&gt;cd examples/git-hooks &amp;amp;&amp;amp; ./install.sh&lt;/code&gt;
    &lt;p&gt;This adds:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;pre-commit - Immediate flush before commit (no 5-second wait)&lt;/item&gt;
      &lt;item&gt;post-merge - Guaranteed import after &lt;code&gt;git pull&lt;/code&gt;or&lt;code&gt;git merge&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disable auto-sync if needed:&lt;/p&gt;
    &lt;code&gt;bd --no-auto-flush create "Issue"   # Skip auto-export
bd --no-auto-import list            # Skip auto-import check&lt;/code&gt;
    &lt;p&gt;Version 0.20.1 introduces collision-resistant hash-based IDs to enable reliable multi-worker and multi-branch workflows.&lt;/p&gt;
    &lt;p&gt;Issue IDs now use short hexadecimal hashes instead of sequential numbers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Before (v0.20.0): &lt;code&gt;bd-1&lt;/code&gt;,&lt;code&gt;bd-2&lt;/code&gt;,&lt;code&gt;bd-152&lt;/code&gt;(sequential numbers)&lt;/item&gt;
      &lt;item&gt;After (v0.20.1): &lt;code&gt;bd-a1b2&lt;/code&gt;,&lt;code&gt;bd-f14c&lt;/code&gt;,&lt;code&gt;bd-3e7a&lt;/code&gt;(4-6 character hashes)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hash IDs use progressive length scaling based on database size:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;0-500 issues: 4-character hashes (e.g., &lt;code&gt;bd-a1b2&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;500-1,500 issues: 5-character hashes (e.g., &lt;code&gt;bd-f14c3&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;1,500-10,000 issues: 6-character hashes (e.g., &lt;code&gt;bd-3e7a5b&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The problem with sequential IDs: When multiple agents or branches create issues concurrently, sequential IDs collide:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Agent A creates &lt;code&gt;bd-10&lt;/code&gt;on branch&lt;code&gt;feature-auth&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Agent B creates &lt;code&gt;bd-10&lt;/code&gt;on branch&lt;code&gt;feature-payments&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Git merge creates duplicate IDs ‚Üí collision resolution required&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How hash IDs solve this: Hash IDs are generated from random data, making concurrent creation collision-free:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Agent A creates &lt;code&gt;bd-a1b2&lt;/code&gt;(hash of random UUID)&lt;/item&gt;
      &lt;item&gt;Agent B creates &lt;code&gt;bd-f14c&lt;/code&gt;(different hash, different UUID)&lt;/item&gt;
      &lt;item&gt;Git merge succeeds cleanly ‚Üí no collision resolution needed&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hash IDs use birthday paradox probability to determine length:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Hash Length&lt;/cell&gt;
        &lt;cell role="head"&gt;Total Space&lt;/cell&gt;
        &lt;cell role="head"&gt;50% Collision at N Issues&lt;/cell&gt;
        &lt;cell role="head"&gt;1% Collision at N Issues&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;4 chars&lt;/cell&gt;
        &lt;cell&gt;65,536&lt;/cell&gt;
        &lt;cell&gt;~304 issues&lt;/cell&gt;
        &lt;cell&gt;~38 issues&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;5 chars&lt;/cell&gt;
        &lt;cell&gt;1,048,576&lt;/cell&gt;
        &lt;cell&gt;~1,217 issues&lt;/cell&gt;
        &lt;cell&gt;~153 issues&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;6 chars&lt;/cell&gt;
        &lt;cell&gt;16,777,216&lt;/cell&gt;
        &lt;cell&gt;~4,869 issues&lt;/cell&gt;
        &lt;cell&gt;~612 issues&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Our thresholds are conservative: We switch from 4‚Üí5 chars at 500 issues (way before the 1% collision point at ~1,217) and from 5‚Üí6 chars at 1,500 issues.&lt;/p&gt;
    &lt;p&gt;Progressive extension on collision: If a hash collision does occur, bd automatically extends the hash to 7 or 8 characters instead of remapping to a new ID.&lt;/p&gt;
    &lt;p&gt;Existing databases continue to work - no forced migration. Run &lt;code&gt;bd migrate&lt;/code&gt; when ready:&lt;/p&gt;
    &lt;code&gt;# Inspect migration plan (for AI agents)
bd migrate --inspect --json

# Check schema and config state
bd info --schema --json

# Preview migration
bd migrate --dry-run

# Migrate database schema (removes obsolete issue_counters table)
bd migrate

# Show current database info
bd info&lt;/code&gt;
    &lt;p&gt;AI-supervised migrations: The &lt;code&gt;--inspect&lt;/code&gt; flag provides migration plan analysis for AI agents. The system verifies data integrity invariants (required config keys, foreign key constraints, issue counts) before committing migrations.&lt;/p&gt;
    &lt;p&gt;Note: Hash IDs require schema version 9+. The &lt;code&gt;bd migrate&lt;/code&gt; command detects old schemas and upgrades automatically.&lt;/p&gt;
    &lt;p&gt;Hash IDs support hierarchical children for natural work breakdown structures. Child IDs use dot notation:&lt;/p&gt;
    &lt;code&gt;bd-a3f8e9      [epic] Auth System
bd-a3f8e9.1    [task] Design login UI
bd-a3f8e9.2    [task] Backend validation
bd-a3f8e9.3    [epic] Password Reset
bd-a3f8e9.3.1  [task] Email templates
bd-a3f8e9.3.2  [task] Reset flow tests
&lt;/code&gt;
    &lt;p&gt;Benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Collision-free: Parent hash ensures unique namespace&lt;/item&gt;
      &lt;item&gt;Human-readable: Clear parent-child relationships&lt;/item&gt;
      &lt;item&gt;Flexible depth: Up to 3 levels of nesting&lt;/item&gt;
      &lt;item&gt;No coordination needed: Each epic owns its child ID space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common patterns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1 level: Epic ‚Üí tasks (most projects)&lt;/item&gt;
      &lt;item&gt;2 levels: Epic ‚Üí features ‚Üí tasks (large projects)&lt;/item&gt;
      &lt;item&gt;3 levels: Epic ‚Üí features ‚Üí stories ‚Üí tasks (complex projects)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example workflow:&lt;/p&gt;
    &lt;code&gt;# Create parent epic (generates hash ID automatically)
bd create "Auth System" -t epic -p 1
# Returns: bd-a3f8e9

# Create child tasks
bd create "Design login UI" -p 1       # Auto-assigned: bd-a3f8e9.1
bd create "Backend validation" -p 1    # Auto-assigned: bd-a3f8e9.2

# Create nested epic with its own children
bd create "Password Reset" -t epic -p 1  # Auto-assigned: bd-a3f8e9.3
bd create "Email templates" -p 1          # Auto-assigned: bd-a3f8e9.3.1&lt;/code&gt;
    &lt;p&gt;Note: Child IDs are automatically assigned sequentially within each parent's namespace. No need to specify parent manually - bd tracks context from git branch/working directory.&lt;/p&gt;
    &lt;p&gt;Check installation health: &lt;code&gt;bd doctor&lt;/code&gt; validates your &lt;code&gt;.beads/&lt;/code&gt; setup, database version, ID format, and CLI version. Provides actionable fixes for any issues found.&lt;/p&gt;
    &lt;code&gt;bd create "Fix bug" -d "Description" -p 1 -t bug
bd create "Add feature" --description "Long description" --priority 2 --type feature
bd create "Task" -l "backend,urgent" --assignee alice

# Get JSON output for programmatic use
bd create "Fix bug" -d "Description" --json

# Create from templates (built-in: epic, bug, feature)
bd create --from-template epic "Q4 Platform Improvements"
bd create --from-template bug "Auth token validation fails"
bd create --from-template feature "Add OAuth support"

# Override template defaults
bd create --from-template bug "Critical issue" -p 0  # Override priority

# Create multiple issues from a markdown file
bd create -f feature-plan.md&lt;/code&gt;
    &lt;p&gt;Options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-f, --file&lt;/code&gt;- Create multiple issues from markdown file&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--from-template&lt;/code&gt;- Use template (epic, bug, feature, or custom)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-d, --description&lt;/code&gt;- Issue description&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-p, --priority&lt;/code&gt;- Priority (0-4, 0=highest, default=2)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-t, --type&lt;/code&gt;- Type (bug|feature|task|epic|chore, default=task)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-a, --assignee&lt;/code&gt;- Assign to user&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-l, --labels&lt;/code&gt;- Comma-separated labels&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--id&lt;/code&gt;- Explicit issue ID (e.g.,&lt;code&gt;worker1-100&lt;/code&gt;for ID space partitioning)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--json&lt;/code&gt;- Output in JSON format&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See &lt;code&gt;bd template list&lt;/code&gt; for available templates and &lt;code&gt;bd help template&lt;/code&gt; for managing custom templates.&lt;/p&gt;
    &lt;code&gt;bd info                                    # Show database path and daemon status
bd show bd-a1b2                            # Show full details
bd list                                    # List all issues
bd list --status open                      # Filter by status
bd list --priority 1                       # Filter by priority
bd list --assignee alice                   # Filter by assignee
bd list --label=backend,urgent             # Filter by labels (AND)
bd list --label-any=frontend,backend       # Filter by labels (OR)

# Advanced filters
bd list --title-contains "auth"            # Search title
bd list --desc-contains "implement"        # Search description
bd list --notes-contains "TODO"            # Search notes
bd list --id bd-123,bd-456                 # Specific IDs (comma-separated)

# Date range filters (YYYY-MM-DD or RFC3339)
bd list --created-after 2024-01-01         # Created after date
bd list --created-before 2024-12-31        # Created before date
bd list --updated-after 2024-06-01         # Updated after date
bd list --updated-before 2024-12-31        # Updated before date
bd list --closed-after 2024-01-01          # Closed after date
bd list --closed-before 2024-12-31         # Closed before date

# Empty/null checks
bd list --empty-description                # Issues with no description
bd list --no-assignee                      # Unassigned issues
bd list --no-labels                        # Issues with no labels

# Priority ranges
bd list --priority-min 0 --priority-max 1  # P0 and P1 only
bd list --priority-min 2                   # P2 and below

# Combine multiple filters
bd list --status open --priority 1 --label-any urgent,critical --no-assignee

# JSON output for agents
bd info --json
bd list --json
bd show bd-a1b2 --json&lt;/code&gt;
    &lt;code&gt;bd update bd-a1b2 --status in_progress
bd update bd-a1b2 --priority 2
bd update bd-a1b2 --assignee bob
bd close bd-a1b2 --reason "Completed"
bd close bd-a1b2 bd-f14c bd-3e7a   # Close multiple

# JSON output
bd update bd-a1b2 --status in_progress --json&lt;/code&gt;
    &lt;code&gt;# Add dependency (bd-f14c depends on bd-a1b2)
bd dep add bd-f14c bd-a1b2
bd dep add bd-3e7a bd-a1b2 --type blocks

# Remove dependency
bd dep remove bd-f14c bd-a1b2

# Show dependency tree
bd dep tree bd-f14c

# Detect cycles
bd dep cycles&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;blocks: Hard blocker (default) - issue cannot start until blocker is resolved&lt;/item&gt;
      &lt;item&gt;related: Soft relationship - issues are connected but not blocking&lt;/item&gt;
      &lt;item&gt;parent-child: Hierarchical relationship (child depends on parent)&lt;/item&gt;
      &lt;item&gt;discovered-from: Issue discovered during work on another issue (automatically inherits parent's &lt;code&gt;source_repo&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Only &lt;code&gt;blocks&lt;/code&gt; dependencies affect ready work detection.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Issues created with&lt;/p&gt;&lt;code&gt;discovered-from&lt;/code&gt;dependencies automatically inherit the parent's&lt;code&gt;source_repo&lt;/code&gt;field, ensuring discovered work stays in the same repository as the parent task.&lt;/quote&gt;
    &lt;code&gt;# Show ready work (no blockers)
bd ready
bd ready --limit 20
bd ready --priority 1
bd ready --assignee alice

# Sort policies (hybrid is default)
bd ready --sort priority    # Strict priority order (P0, P1, P2, P3)
bd ready --sort oldest      # Oldest issues first (backlog clearing)
bd ready --sort hybrid      # Recent by priority, old by age (default)

# Show blocked issues
bd blocked

# Statistics
bd stats

# JSON output for agents
bd ready --json&lt;/code&gt;
    &lt;p&gt;Add flexible metadata to issues for filtering and organization:&lt;/p&gt;
    &lt;code&gt;# Add labels during creation
bd create "Fix auth bug" -t bug -p 1 -l auth,backend,urgent

# Add/remove labels
bd label add bd-a1b2 security
bd label remove bd-a1b2 urgent

# List labels
bd label list bd-a1b2            # Labels on one issue
bd label list-all                # All labels with counts

# Filter by labels
bd list --label backend,auth     # AND: must have ALL labels
bd list --label-any frontend,ui  # OR: must have AT LEAST ONE&lt;/code&gt;
    &lt;p&gt;See docs/LABELS.md for complete label documentation and best practices.&lt;/p&gt;
    &lt;code&gt;# Single issue deletion (preview mode)
bd delete bd-a1b2

# Force single deletion
bd delete bd-a1b2 --force

# Batch deletion
bd delete bd-a1b2 bd-f14c bd-3e7a --force

# Delete from file (one ID per line)
bd delete --from-file deletions.txt --force

# Cascade deletion (recursively delete dependents)
bd delete bd-a1b2 --cascade --force&lt;/code&gt;
    &lt;p&gt;The delete operation removes all dependency links, updates text references to &lt;code&gt;[deleted:ID]&lt;/code&gt;, and removes the issue from database and JSONL.&lt;/p&gt;
    &lt;p&gt;Manage per-project configuration for external integrations:&lt;/p&gt;
    &lt;code&gt;# Set configuration
bd config set jira.url "https://company.atlassian.net"
bd config set jira.project "PROJ"

# Get configuration
bd config get jira.url

# List all configuration
bd config list --json

# Unset configuration
bd config unset jira.url&lt;/code&gt;
    &lt;p&gt;See docs/CONFIG.md for complete configuration documentation.&lt;/p&gt;
    &lt;p&gt;Beads provides agent-driven compaction - your AI agent decides what to compress, no API keys required:&lt;/p&gt;
    &lt;code&gt;# Agent-driven workflow (recommended)
bd compact --analyze --json              # Get candidates with full content
bd compact --apply --id bd-42 --summary summary.txt

# Legacy AI-powered workflow (requires ANTHROPIC_API_KEY)
bd compact --auto --dry-run --all        # Preview candidates
bd compact --auto --all                  # Auto-compact all eligible issues&lt;/code&gt;
    &lt;p&gt;How it works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use &lt;code&gt;--analyze&lt;/code&gt;to export candidates (closed 30+ days) with full content&lt;/item&gt;
      &lt;item&gt;Summarize the content using any LLM (Claude, GPT, local model, etc.)&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;--apply&lt;/code&gt;to persist the summary and mark as compacted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is agentic memory decay - your database naturally forgets fine-grained details while preserving essential context. The agent has full control over compression quality.&lt;/p&gt;
    &lt;code&gt;# Export to JSONL (automatic by default)
bd export -o issues.jsonl

# Import from JSONL (automatic when JSONL is newer)
bd import -i issues.jsonl

# Handle missing parents during import
bd import -i issues.jsonl --orphan-handling resurrect  # Auto-recreate deleted parents
bd import -i issues.jsonl --orphan-handling skip       # Skip orphans with warning
bd import -i issues.jsonl --orphan-handling strict     # Fail on missing parents

# Manual sync
bd sync&lt;/code&gt;
    &lt;p&gt;Import Orphan Handling:&lt;/p&gt;
    &lt;p&gt;When importing hierarchical issues (e.g., &lt;code&gt;bd-abc.1&lt;/code&gt;, &lt;code&gt;bd-abc.2&lt;/code&gt;), bd needs to handle cases where the parent (&lt;code&gt;bd-abc&lt;/code&gt;) has been deleted:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;allow&lt;/code&gt;(default) - Import orphans without validation. Most permissive, ensures no data loss.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;resurrect&lt;/code&gt;- Search JSONL history for deleted parents and recreate them as tombstones (Status=Closed, Priority=4). Preserves hierarchy.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;skip&lt;/code&gt;- Skip orphaned children with warning. Partial import.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;strict&lt;/code&gt;- Fail import if parent is missing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Configure default behavior: &lt;code&gt;bd config set import.orphan_handling resurrect&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;See docs/CONFIG.md for complete configuration documentation.&lt;/p&gt;
    &lt;p&gt;Note: Auto-sync is enabled by default. Manual export/import is rarely needed.&lt;/p&gt;
    &lt;p&gt;bd runs a background daemon per workspace for auto-sync and RPC operations. Use &lt;code&gt;bd daemons&lt;/code&gt; to manage multiple daemons:&lt;/p&gt;
    &lt;code&gt;# List all running daemons
bd daemons list

# Check health (version mismatches, stale sockets)
bd daemons health

# Stop a specific daemon
bd daemons stop /path/to/workspace
bd daemons stop 12345  # By PID

# Restart a specific daemon
bd daemons restart /path/to/workspace
bd daemons restart 12345  # By PID

# View daemon logs
bd daemons logs /path/to/workspace -n 100
bd daemons logs 12345 -f  # Follow mode

# Stop all daemons
bd daemons killall
bd daemons killall --force  # Force kill if graceful fails&lt;/code&gt;
    &lt;p&gt;Common use cases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;After upgrading bd: Run &lt;code&gt;bd daemons health&lt;/code&gt;to check for version mismatches, then&lt;code&gt;bd daemons killall&lt;/code&gt;to restart all daemons with the new version&lt;/item&gt;
      &lt;item&gt;Debugging: Use &lt;code&gt;bd daemons logs &amp;lt;workspace&amp;gt;&lt;/code&gt;to view daemon logs&lt;/item&gt;
      &lt;item&gt;Cleanup: &lt;code&gt;bd daemons list&lt;/code&gt;auto-removes stale sockets&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See commands/daemons.md for complete documentation.&lt;/p&gt;
    &lt;p&gt;A standalone web interface for real-time issue monitoring is available as an example:&lt;/p&gt;
    &lt;code&gt;# Build the monitor-webui
cd examples/monitor-webui
go build

# Start web UI on localhost:8080
./monitor-webui

# Custom port and host
./monitor-webui -port 3000
./monitor-webui -host 0.0.0.0 -port 8080  # Listen on all interfaces&lt;/code&gt;
    &lt;p&gt;The monitor provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time table view of all issues with filtering by status and priority&lt;/item&gt;
      &lt;item&gt;Click-through details - Click any issue to view full details in a modal&lt;/item&gt;
      &lt;item&gt;Live updates - WebSocket connection for real-time changes via daemon RPC&lt;/item&gt;
      &lt;item&gt;Responsive design - Mobile-friendly card view on small screens&lt;/item&gt;
      &lt;item&gt;Statistics dashboard - Quick overview of issue counts and ready work&lt;/item&gt;
      &lt;item&gt;Clean UI - Simple, fast interface styled with milligram.css&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The monitor is particularly useful for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Team visibility - Share a dashboard view of project status&lt;/item&gt;
      &lt;item&gt;AI agent supervision - Watch your coding agent create and update issues in real-time&lt;/item&gt;
      &lt;item&gt;Quick browsing - Faster than CLI for exploring issue details&lt;/item&gt;
      &lt;item&gt;Mobile access - Check project status from your phone&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See examples/monitor-webui/ for complete documentation.&lt;/p&gt;
    &lt;p&gt;Check out the examples/ directory for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python agent - Full agent implementation in Python&lt;/item&gt;
      &lt;item&gt;Bash agent - Shell script agent example&lt;/item&gt;
      &lt;item&gt;Git hooks - Automatic export/import on git operations&lt;/item&gt;
      &lt;item&gt;Branch merge workflow - Handle ID collisions when merging branches&lt;/item&gt;
      &lt;item&gt;Claude Desktop MCP - MCP server for Claude Desktop&lt;/item&gt;
      &lt;item&gt;Claude Code Plugin - One-command installation with slash commands&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For advanced usage, see:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;docs/ADVANCED.md - Prefix renaming, merging duplicates, daemon configuration&lt;/item&gt;
      &lt;item&gt;docs/CONFIG.md - Configuration system for integrations&lt;/item&gt;
      &lt;item&gt;docs/EXTENDING.md - Database extension patterns&lt;/item&gt;
      &lt;item&gt;docs/ADVANCED.md - JSONL format and merge strategies&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;README.md - You are here! Core features and quick start&lt;/item&gt;
      &lt;item&gt;docs/INSTALLING.md - Complete installation guide for all platforms&lt;/item&gt;
      &lt;item&gt;docs/QUICKSTART.md - Interactive tutorial (&lt;code&gt;bd quickstart&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;docs/AGENT_MAIL_QUICKSTART.md - 5-minute Agent Mail setup guide&lt;/item&gt;
      &lt;item&gt;docs/AGENT_MAIL.md - Complete Agent Mail integration guide&lt;/item&gt;
      &lt;item&gt;docs/MULTI_REPO_MIGRATION.md - Multi-repo workflow guide (OSS, teams, multi-phase)&lt;/item&gt;
      &lt;item&gt;docs/MULTI_REPO_AGENTS.md - Multi-repo patterns for AI agents&lt;/item&gt;
      &lt;item&gt;docs/FAQ.md - Frequently asked questions&lt;/item&gt;
      &lt;item&gt;docs/TROUBLESHOOTING.md - Common issues and solutions&lt;/item&gt;
      &lt;item&gt;docs/ADVANCED.md - Advanced features and use cases&lt;/item&gt;
      &lt;item&gt;docs/LABELS.md - Complete label system guide&lt;/item&gt;
      &lt;item&gt;docs/CONFIG.md - Configuration system&lt;/item&gt;
      &lt;item&gt;docs/EXTENDING.md - Database extension patterns&lt;/item&gt;
      &lt;item&gt;docs/ADVANCED.md - JSONL format analysis&lt;/item&gt;
      &lt;item&gt;docs/PLUGIN.md - Claude Code plugin documentation&lt;/item&gt;
      &lt;item&gt;CONTRIBUTING.md - Contribution guidelines&lt;/item&gt;
      &lt;item&gt;SECURITY.md - Security policy&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;beads-ui - Local web interface with live updates, kanban board, and keyboard navigation. Zero-setup launch with &lt;code&gt;npx beads-ui start&lt;/code&gt;. Built by @mantoni.&lt;/item&gt;
      &lt;item&gt;bdui - Real-time terminal UI with kanban board, tree view, dependency graph, and statistics dashboard. Vim-style navigation, search/filter, themes, and native notifications. Built by @assimelha.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Have you built something cool with bd? Open an issue to get it featured here!&lt;/p&gt;
    &lt;code&gt;# Run tests
go test ./...

# Build
go build -o bd ./cmd/bd

# Run
./bd create "Test issue"

# Bump version
./scripts/bump-version.sh 0.9.3           # Update all versions, show diff
./scripts/bump-version.sh 0.9.3 --commit  # Update and auto-commit&lt;/code&gt;
    &lt;p&gt;See scripts/README.md for more development scripts.&lt;/p&gt;
    &lt;p&gt;MIT&lt;/p&gt;
    &lt;p&gt;Built with ‚ù§Ô∏è by developers who love tracking dependencies and finding ready work.&lt;/p&gt;
    &lt;p&gt;Inspired by the need for a simpler, dependency-aware issue tracker.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075616</guid><pubDate>Fri, 28 Nov 2025 04:50:57 +0000</pubDate></item><item><title>How to use Linux vsock for fast VM communication</title><link>https://popovicu.com/posts/how-to-use-linux-vsock-for-fast-vm-communication/</link><description>&lt;doc fingerprint="8f8d591d4d436ea4"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôve recently been experimenting with various ways to construct Linux VM images, but for these images to be practical, they need to interact with the outside world. At a minimum, they need to communicate with the host machine.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;vsock&lt;/code&gt; is a technology specifically designed with VMs in mind. It eliminates the need for a TCP/IP stack or network virtualization to enable communication with or between VMs. At the API level, it behaves like a standard socket but utilizes a specialized addressing scheme.&lt;/p&gt;
    &lt;p&gt;In the experiment below, we‚Äôll explore using &lt;code&gt;vsock&lt;/code&gt; as the transport mechanism for a gRPC service running on a VM. We‚Äôll build this project with Bazel for easy reproducibility. Check out this post if you need an intro to Bazel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;There are many use cases for efficient communication between a VM and its host (or between multiple VMs). One simple reason is to create a hermetic environment within the VM and issue commands via RPC from the host. This is the primary driver for using gRPC in this example, but you can easily generalize the approach shown here to build far more complex systems.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub repo&lt;/head&gt;
    &lt;p&gt;The complete repository is hosted here and serves as the source of truth for this experiment. While there may be minor inconsistencies between the code blocks below and the repository, please rely on GitHub as the definitive source.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code breakdown&lt;/head&gt;
    &lt;p&gt;Let‚Äôs break down the code step by step:&lt;/p&gt;
    &lt;head rend="h3"&gt;External dependencies&lt;/head&gt;
    &lt;p&gt;Here are the external dependencies listed as Bazel modules:&lt;/p&gt;
    &lt;code&gt;bazel_dep(name = "rules_proto", version = "7.1.0")
bazel_dep(name = "rules_cc", version = "0.2.14")
bazel_dep(name = "protobuf", version = "33.1", repo_name = "com_google_protobuf")
bazel_dep(name = "grpc", version = "1.76.0.bcr.1")&lt;/code&gt;
    &lt;p&gt;This is largely self-explanatory. The &lt;code&gt;protobuf&lt;/code&gt; repository is used for C++ proto-generation rules, and &lt;code&gt;grpc&lt;/code&gt; provides the monorepo for Bazel rules to generate gRPC code for the C++ family of languages.&lt;/p&gt;
    &lt;head rend="h3"&gt;gRPC library generation&lt;/head&gt;
    &lt;p&gt;The following Bazel targets generate the necessary C++ Protobuf and gRPC libraries:&lt;/p&gt;
    &lt;code&gt;load("@rules_proto//proto:defs.bzl", "proto_library")
load("@com_google_protobuf//bazel:cc_proto_library.bzl", "cc_proto_library")
load("@grpc//bazel:cc_grpc_library.bzl", "cc_grpc_library")

proto_library(
    name = "vsock_service_proto",
    srcs = ["vsock_service.proto"],
)

cc_proto_library(
    name = "vsock_service_cc_proto",
    deps = [
        ":vsock_service_proto",
    ],
    visibility = [
        "//server:__subpackages__",
        "//client:__subpackages__",
    ],
)

cc_grpc_library(
    name = "vsock_service_cc_grpc",
    grpc_only = True,
    srcs = [
        ":vsock_service_proto",
    ],
    deps = [
        ":vsock_service_cc_proto",
    ],
    visibility = [
        "//server:__subpackages__",
        "//client:__subpackages__",
    ],
)&lt;/code&gt;
    &lt;p&gt;The protocol definition is straightforward:&lt;/p&gt;
    &lt;code&gt;syntax = "proto3";

package popovicu_vsock;

service VsockService {
  rpc Addition(AdditionRequest) returns (AdditionResponse) {}
}

message AdditionRequest {
  int32 a = 1;
  int32 b = 2;
}

message AdditionResponse {
  int32 c = 1;
}&lt;/code&gt;
    &lt;p&gt;It simply exposes a service capable of adding two integers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Server implementation&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;BUILD&lt;/code&gt; file is straightforward:&lt;/p&gt;
    &lt;code&gt;load("@rules_cc//cc:defs.bzl", "cc_binary")

cc_binary(
    name = "server",
    srcs = [
        "server.cc",
    ],
    deps = [
        "@grpc//:grpc++",
        "//proto:vsock_service_cc_grpc",
        "//proto:vsock_service_cc_proto",
    ],
    linkstatic = True,
    linkopts = [
        "-static",
    ],
)&lt;/code&gt;
    &lt;p&gt;We want a statically linked binary to run on the VM. This choice simplifies deployment, allowing us to drop a single file onto the VM.&lt;/p&gt;
    &lt;p&gt;The code is largely self-explanatory:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;string&amp;gt;

#include &amp;lt;grpc++/grpc++.h&amp;gt;
#include "proto/vsock_service.grpc.pb.h"

using grpc::Server;
using grpc::ServerBuilder;
using grpc::ServerContext;
using grpc::Status;
using popovicu_vsock::VsockService;
using popovicu_vsock::AdditionRequest;
using popovicu_vsock::AdditionResponse;

// Service implementation
class VsockServiceImpl final : public VsockService::Service {
  Status Addition(ServerContext* context, const AdditionRequest* request,
                  AdditionResponse* response) override {
    int32_t result = request-&amp;gt;a() + request-&amp;gt;b();
    response-&amp;gt;set_c(result);
    std::cout &amp;lt;&amp;lt; "Addition: " &amp;lt;&amp;lt; request-&amp;gt;a() &amp;lt;&amp;lt; " + " &amp;lt;&amp;lt; request-&amp;gt;b()
              &amp;lt;&amp;lt; " = " &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl;
    return Status::OK;
  }
};

void RunServer() {
  // Server running on VM (guest)
  // vsock:-1:9999 means listen on port 9999, accept connections from any CID
  // CID -1 (VMADDR_CID_ANY) allows the host to connect to this VM server
  std::string server_address("vsock:3:9999");
  VsockServiceImpl service;

  ServerBuilder builder;
  builder.AddListeningPort(server_address, grpc::InsecureServerCredentials());
  builder.RegisterService(&amp;amp;service);

  std::unique_ptr&amp;lt;Server&amp;gt; server(builder.BuildAndStart());
  std::cout &amp;lt;&amp;lt; "Server listening on " &amp;lt;&amp;lt; server_address &amp;lt;&amp;lt; std::endl;

  server-&amp;gt;Wait();
}

int main() {
  RunServer();
  return 0;
}&lt;/code&gt;
    &lt;p&gt;The only part requiring explanation is the &lt;code&gt;server_address&lt;/code&gt;. The &lt;code&gt;vsock:&lt;/code&gt; prefix indicates that we‚Äôre using &lt;code&gt;vsock&lt;/code&gt; as the transport layer. gRPC supports various transports, including TCP/IP and Unix sockets.&lt;/p&gt;
    &lt;p&gt;The number &lt;code&gt;3&lt;/code&gt; is the CID, or Context ID. This functions similarly to an IP address. Certain CIDs have special meanings. For instance, CID 2 represents the VM host itself; if the VM needs to connect to a &lt;code&gt;vsock&lt;/code&gt; socket on the host, it targets CID 2. CID 1 is reserved for the loopback address. Generally, VMs are assigned CIDs starting from 3.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;9999&lt;/code&gt; is simply the port number, functioning just as it does in TCP/IP.&lt;/p&gt;
    &lt;head rend="h3"&gt;Client implementation&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;BUILD&lt;/code&gt; file is, again, quite simple:&lt;/p&gt;
    &lt;code&gt;load("@rules_cc//cc:defs.bzl", "cc_binary")

cc_binary(
    name = "client",
    srcs = [
        "client.cc",
    ],
    deps = [
        "@grpc//:grpc++",
        "//proto:vsock_service_cc_grpc",
        "//proto:vsock_service_cc_proto",
    ],
    linkstatic = True,
    linkopts = [
        "-static",
    ],
)&lt;/code&gt;
    &lt;p&gt;And the C++ code:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;memory&amp;gt;
#include &amp;lt;string&amp;gt;

#include &amp;lt;grpc++/grpc++.h&amp;gt;
#include "proto/vsock_service.grpc.pb.h"

using grpc::Channel;
using grpc::ClientContext;
using grpc::Status;
using popovicu_vsock::VsockService;
using popovicu_vsock::AdditionRequest;
using popovicu_vsock::AdditionResponse;

class VsockClient {
 public:
  VsockClient(std::shared_ptr&amp;lt;Channel&amp;gt; channel)
      : stub_(VsockService::NewStub(channel)) {}

  int32_t Add(int32_t a, int32_t b) {
    AdditionRequest request;
    request.set_a(a);
    request.set_b(b);

    AdditionResponse response;
    ClientContext context;

    Status status = stub_-&amp;gt;Addition(&amp;amp;context, request, &amp;amp;response);

    if (status.ok()) {
      return response.c();
    } else {
      std::cout &amp;lt;&amp;lt; "RPC failed: " &amp;lt;&amp;lt; status.error_code() &amp;lt;&amp;lt; ": "
                &amp;lt;&amp;lt; status.error_message() &amp;lt;&amp;lt; std::endl;
      return -1;
    }
  }

 private:
  std::unique_ptr&amp;lt;VsockService::Stub&amp;gt; stub_;
};

int main() {
  // Client running on host, connecting to VM server
  // vsock:3:9999 means connect to CID 3 (guest VM) on port 9999
  // CID 3 is an example - adjust based on your VM's actual CID
  std::string server_address("vsock:3:9999");

  VsockClient client(
      grpc::CreateChannel(server_address, grpc::InsecureChannelCredentials()));

  int32_t a = 5;
  int32_t b = 7;
  int32_t result = client.Add(a, b);

  std::cout &amp;lt;&amp;lt; "Addition result: " &amp;lt;&amp;lt; a &amp;lt;&amp;lt; " + " &amp;lt;&amp;lt; b &amp;lt;&amp;lt; " = " &amp;lt;&amp;lt; result
            &amp;lt;&amp;lt; std::endl;

  return 0;
}&lt;/code&gt;
    &lt;head rend="h2"&gt;Running it all together&lt;/head&gt;
    &lt;p&gt;Bazel shines here. You only need a working C++ compiler on your host system. Bazel automatically fetches and builds everything else on the fly, including the Protobuf compiler.&lt;/p&gt;
    &lt;p&gt;To get the statically linked server binary:&lt;/p&gt;
    &lt;code&gt;bazel build //server&lt;/code&gt;
    &lt;p&gt;Similarly, for the client:&lt;/p&gt;
    &lt;code&gt;bazel build //client&lt;/code&gt;
    &lt;p&gt;To create a VM image, I used &lt;code&gt;debootstrap&lt;/code&gt; on an &lt;code&gt;ext4&lt;/code&gt; image, as described in this post on X:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Stop downloading 4GB ISOs to create Linux VMs.&lt;/p&gt;‚Äî Uros Popovic (@popovicu94) November 25, 2025&lt;lb/&gt;You don't need an installer, a GUI, or a "Next &amp;gt; Next &amp;gt; Finish" wizard. You just need a directory of files.&lt;lb/&gt;Here is how I build custom, hacky, bootable Debian VMs in 60 seconds using debootstrap.&lt;lb/&gt;Your distro is just a kernel and a‚Ä¶ pic.twitter.com/l4mM02zPmr&lt;/quote&gt;
    &lt;p&gt;This is a quick, albeit hacky, solution for creating a runnable Debian instance.&lt;/p&gt;
    &lt;p&gt;Next, I copied the newly built server binary to &lt;code&gt;/opt&lt;/code&gt; within the image.&lt;/p&gt;
    &lt;p&gt;Now, the VM can be booted straight into the server binary as soon as the kernel runs:&lt;/p&gt;
    &lt;code&gt; qemu-system-x86_64 -m 1G -kernel /tmp/linux/linux-6.17.2/arch/x86/boot/bzImage \
  -nographic \
  -append "console=ttyS0 init=/opt/server root=/dev/vda rw" \
  --enable-kvm \
  -smp 8 \
  -drive file=./debian.qcow2,format=qcow2,if=virtio -device vhost-vsock-pci,guest-cid=3&lt;/code&gt;
    &lt;p&gt;As shown in the last line, a virtual device is attached to the QEMU VM acting as &lt;code&gt;vsock&lt;/code&gt; networking hardware, configured with CID 3.&lt;/p&gt;
    &lt;p&gt;The QEMU output shows:&lt;/p&gt;
    &lt;code&gt;[    1.581192] Run /opt/server as init process
[    1.889382] random: crng init done
Server listening on vsock:3:9999&lt;/code&gt;
    &lt;p&gt;To send an RPC to the server from the host, I ran the client binary:&lt;/p&gt;
    &lt;code&gt;bazel run //client&lt;/code&gt;
    &lt;p&gt;The output confirmed the result:&lt;/p&gt;
    &lt;code&gt;Addition result: 5 + 7 = 12&lt;/code&gt;
    &lt;p&gt;Correspondingly, the server output displayed:&lt;/p&gt;
    &lt;code&gt;Addition: 5 + 7 = 12&lt;/code&gt;
    &lt;p&gt;We have successfully invoked an RPC from the host to the VM!&lt;/p&gt;
    &lt;head rend="h2"&gt;Under the hood&lt;/head&gt;
    &lt;p&gt;I haven‚Äôt delved into the low-level system API for &lt;code&gt;vsock&lt;/code&gt;s, as frameworks typically abstract this away. However, &lt;code&gt;vsock&lt;/code&gt;s closely resemble TCP/IP sockets. Once created, they are used in the same way, though the creation API differs. Information on this is readily available online.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I believed it was more valuable to focus on a high-level RPC system over &lt;code&gt;vsock&lt;/code&gt; rather than raw sockets. With gRPC, you can invoke a structured RPC on a server running inside the VM. This opens the door to running interesting applications in sealed, isolated environments, allowing you to easily combine different OSes (e.g., a Debian host and an Arch guest) or any platform supporting &lt;code&gt;vsock&lt;/code&gt;. Additionally, gRPC allows you to write clients and servers in many different languages and technologies. This is achieved without network virtualization, resulting in increased efficiency.&lt;/p&gt;
    &lt;p&gt;I hope this was fun and useful to you as well.&lt;/p&gt;
    &lt;p&gt;Please consider following me on X and LinkedIn for further updates.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075746</guid><pubDate>Fri, 28 Nov 2025 05:19:45 +0000</pubDate></item><item><title>Show HN: Glasses to detect smart-glasses that have cameras</title><link>https://github.com/NullPxl/banrays</link><description>&lt;doc fingerprint="37a0505728a445d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Glasses to detect smart-glasses that have cameras&lt;/p&gt;
    &lt;p&gt;I'm experimenting with 2 main approaches:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optics: classify the camera using light reflections.&lt;/item&gt;
      &lt;item&gt;Networking: bluetooth and wi-fi analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So far fingerprinting specific devices based on bluetooth (BLE) is looking like easiest and most reliable approach. The picture below is the first version, which plays the legend of zelda 'secret found' jingle when it detects a BLE advertisement from Meta Raybans.&lt;/p&gt;
    &lt;p&gt;I'm essentially treating this README like a logbook, so it will have my current approaches/ideas.&lt;/p&gt;
    &lt;p&gt;By sending IR at camera lenses, we can take advantage of the fact that the CMOS sensor in a camera reflects light directly back at the source (called 'retro-reflectivity' / 'cat-eye effect') to identify cameras.&lt;/p&gt;
    &lt;p&gt;This isn't exactly a new idea. Some researchers in 2005 used this property to create 'capture-resistant environments' when smartphones with cameras were gaining popularity.&lt;/p&gt;
    &lt;p&gt;There's even some recent research (2024) that figured out how to classify individual cameras based on their retro-reflections.&lt;/p&gt;
    &lt;p&gt;Now we have a similar situation to those 2005 researchers on our hands, where smart glasses with hidden cameras seem to be getting more popular. So I want to create a pair of glasses to identify these. Unfortunately, from what I can tell most of the existing research in this space records data with a camera and then uses ML, a ton of controlled angles, etc. to differentiate between normal reflective surfaces and cameras.&lt;/p&gt;
    &lt;p&gt;I would feel pretty silly if my solution uses its own camera. So I'll be avoiding that. Instead I think it's likely I'll have to rely on being consistent with my 'sweeps', and creating a good classifier based on signal data. For example you can see here that the back camera on my smartphone seems to produce quick and large spikes, while the glossy screen creates a more prolonged wave.&lt;/p&gt;
    &lt;p&gt;After getting to test some Meta Raybans, I found that this setup is not going to be sufficient. Here's a test of some sweeps of the camera-area + the same area when the lens is covered. You can see the waveform is similar to what I saw in the earlier test (short spike for camera, wider otherwise), but it's wildly inconsistent and the strength of the signal is very weak. This was from about 4 inches away from the LEDs. I didn't notice much difference when swapping between 940nm and 850nm LEDs.&lt;/p&gt;
    &lt;p&gt;So at least with current hardware that's easy for me to access, this probably isn't enough to differentiate accurately.&lt;/p&gt;
    &lt;p&gt;Another idea I had is to create a designated sweep 'pattern'. The user (wearing the detector glasses) would perform a specific scan pattern of the target. Using the waveforms captured from this data, maybe we can more accurately fingerprint the raybans. For example, sweeping across the targets glasses in a 'left, right, up, down' approach. I tested this by comparing the results of the Meta raybans vs some aviators I had lying around. I think the idea behind this approach is sound (actually it's light), but it might need more workshopping.&lt;/p&gt;
    &lt;p&gt;For prototyping, I'm using:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arduino uno&lt;/item&gt;
      &lt;item&gt;a bunch of 940nm and 850nm IR LEDs&lt;/item&gt;
      &lt;item&gt;a photodiode as a receiver&lt;/item&gt;
      &lt;item&gt;a 2222A transistor&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;experiment with sweeping patterns&lt;/item&gt;
      &lt;item&gt;experiment with combining data from different wavelengths&lt;/item&gt;
      &lt;item&gt;collimation?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This has been more tricky than I first thought! My current approach here is to fingerprint the Meta Raybans over Bluetooth low-energy (BLE) advertisements. But, I have only been able to detect BLE traffic during 1) pairing 2) powering-on. I sometimes also see the advertisement as they are taken out of the case (while already powered on), but not consistently.&lt;/p&gt;
    &lt;p&gt;The goal is to detect them during usage when they're communicating with the paired phone, but to see this type of directed BLE traffic it seems like I would first need to see the &lt;code&gt;CONNECT_REQ&lt;/code&gt; packet which has information as to what which of the communication channels to hop between in sync. I don't think what I currently have (ESP32) is set up to do this kind of following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;potentially can use an nRF module for this&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For any of the bluetooth classic (BTC) traffic, unfortunately the hardware seems a bit more involved (read: expensive). So if I want to do down this route, I'll likely need a more clever solution here.&lt;/p&gt;
    &lt;p&gt;When turned on or put into pairing mode (or sometimes when taken out of the case), I can detect the device through advertised manufacturer data and service UUIDs. &lt;code&gt;0x01AB&lt;/code&gt; is a Meta-specific SIG-assigned ID (assigned by the Bluetooth standards body), and &lt;code&gt;0xFD5F&lt;/code&gt; in the Service UUID is assigned to Meta as well.&lt;/p&gt;
    &lt;p&gt;capture when the glasses are powered on:&lt;/p&gt;
    &lt;code&gt;[01:07:06] RSSI: -59 dBm
Address: XX:XX:XX:XX:XX:XX
Name: Unknown

META/LUXOTTICA DEVICE DETECTED!
  Manufacturer: Meta (0x01AB)
  Service UUID: Meta (0xFD5F) (0000fd5f-0000-1000-8000-00805f9b34fb)

Manufacturer Data:
  Company ID: Meta (0x01AB)
  Data: 020102102716e4

Service UUIDs: ['0000fd5f-0000-1000-8000-00805f9b34fb']
&lt;/code&gt;
    &lt;p&gt;IEEE assigns certain MAC address prefixes (OUI, 'Organizationally Unique Identifier'), but these addresses get randomized so I don't expect them to be super useful for BLE.&lt;/p&gt;
    &lt;p&gt;Here's some links to more data if you're curious:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://www.bluetooth.com/wp-content/uploads/Files/Specification/HTML/Assigned_Numbers/out/en/Assigned_Numbers.pdf&lt;/item&gt;
      &lt;item&gt;https://gitlab.com/wireshark/wireshark/-/blob/99df5f588b38cc0964f998a6a292e81c7dcf0800/epan/dissectors/packet-bluetooth.c&lt;/item&gt;
      &lt;item&gt;https://www.netify.ai/resources/macs/brands/meta&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TODO:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read: https://dl.acm.org/doi/10.1145/3548606.3559372&lt;/item&gt;
      &lt;item&gt;try active probing/interrogating&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks to Trevor Seets and Junming Chen for their advice in optics and BLE (respectively). Also to Sohail for lending me meta raybans to test with.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46075882</guid><pubDate>Fri, 28 Nov 2025 05:52:38 +0000</pubDate></item><item><title>Tech Titans Amass Multimillion-Dollar War Chests to Fight AI Regulation</title><link>https://www.wsj.com/tech/ai/tech-titans-amass-multimillion-dollar-war-chests-to-fight-ai-regulation-88c600e1</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46077038</guid><pubDate>Fri, 28 Nov 2025 09:21:55 +0000</pubDate></item><item><title>A Repository with 44 Years of Unix Evolution</title><link>https://www.spinellis.gr/pubs/conf/2015-MSR-Unix-History/html/Spi15c.html</link><description>&lt;doc fingerprint="b714999f6ea38799"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;head rend="h1"&gt;A Repository with 44 Years of Unix Evolution &lt;/head&gt;&lt;head rend="h2"&gt; Abstract&lt;/head&gt;&lt;p&gt; The evolution of the Unix operating system is made available as a version-control repository, covering the period from its inception in 1972 as a five thousand line kernel, to 2015 as a widely-used 26 million line system. The repository contains 659 thousand commits and 2306 merges. The repository employs the commonly used Git system for its storage, and is hosted on the popular GitHub archive. It has been created by synthesizing with custom software 24 snapshots of systems developed at Bell Labs, Berkeley University, and the 386BSD team, two legacy repositories, and the modern repository of the open source FreeBSD system. In total, 850 individual contributors are identified, the early ones through primary research. The data set can be used for empirical research in software engineering, information systems, and software archaeology. &lt;/p&gt;&lt;head rend="h2"&gt; 1 Introduction&lt;/head&gt;&lt;p&gt; The Unix operating system stands out as a major engineering breakthrough due to its exemplary design, its numerous technical contributions, its development model, and its widespread use. The design of the Unix programming environment has been characterized as one offering unusual simplicity, power, and elegance [&lt;/p&gt;1&lt;p&gt;]. On the technical side, features that can be directly attributed to Unix or were popularized by it include [&lt;/p&gt;2&lt;p&gt;]: the portable implementation of the kernel in a high level language; a hierarchical file system; compatible file, device, networking, and inter-process I/O; the pipes and filters architecture; virtual file systems; and the shell as a user-selectable regular process. A large community contributed software to Unix from its early days [&lt;/p&gt;3&lt;p&gt;], [&lt;/p&gt;4&lt;p&gt;,pp. 65-72]. This community grew immensely over time and worked using what are now termed open source software development methods [&lt;/p&gt;5&lt;p&gt;,pp. 440-442]. Unix and its intellectual descendants have also helped the spread of the C and C++ programming languages, parser and lexical analyzer generators (&lt;/p&gt;yacc&lt;p&gt;, &lt;/p&gt;lex&lt;p&gt;), document preparation tools (&lt;/p&gt;troff&lt;p&gt;, &lt;/p&gt;eqn&lt;p&gt;, &lt;/p&gt;tbl&lt;p&gt;), scripting languages (&lt;/p&gt;awk&lt;p&gt;, &lt;/p&gt;sed&lt;p&gt;, &lt;/p&gt;Perl&lt;p&gt;), TCP/IP networking, and configuration management systems (&lt;/p&gt;SCCS&lt;p&gt;, &lt;/p&gt;RCS&lt;p&gt;, &lt;/p&gt;Subversion&lt;p&gt;, &lt;/p&gt;Git&lt;p&gt;), while also forming a large part of the modern internet infrastructure and the web. &lt;/p&gt;&lt;p&gt; Luckily, important Unix material of historical importance has survived and is nowadays openly available. Although Unix was initially distributed with relatively restrictive licenses, the most significant parts of its early development have been released by one of its right-holders (Caldera International) under a liberal license. Combining these parts with software that was developed or released as open source software by the University of California, Berkeley and the FreeBSD Project provides coverage of the system's development over a period ranging from June 20th 1972 until today. &lt;/p&gt;&lt;p&gt; Curating and processing available snapshots as well as old and modern configuration management repositories allows the reconstruction of a new synthetic Git repository that combines under a single roof most of the available data. This repository documents in a digital form the detailed evolution of an important digital artefact over a period of 44 years. The following sections describe the repository's structure and contents (Section &lt;/p&gt;II&lt;p&gt;), the way it was created (Section &lt;/p&gt;III&lt;p&gt;), and how it can be used (Section &lt;/p&gt;IV&lt;p&gt;). &lt;/p&gt;&lt;head rend="h2"&gt; 2 Data Overview&lt;/head&gt;&lt;p&gt; The 1GB Unix history Git repository is made available for cloning on &lt;/p&gt;GitHub&lt;p&gt;.&lt;/p&gt;1&lt;p&gt; Currently&lt;/p&gt;2&lt;p&gt; the repository contains 659 thousand commits and 2306 merges from about 850 contributors. The contributors include 23 from the Bell Labs staff, 158 from Berkeley's Computer Systems Research Group (CSRG), and 660 from the FreeBSD Project. &lt;/p&gt;&lt;p&gt; The repository starts its life at a tag identified as &lt;/p&gt;Epoch&lt;p&gt;, which contains only licensing information and its modern README file. Various tag and branch names identify points of significance. &lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt; Research-VX tags correspond to six research editions that came out of Bell Labs. These start with Research-V1 (4768 lines of PDP-11 assembly) and end with Research-V7 (1820 mostly C files, 324kLOC). &lt;/item&gt;&lt;item&gt; Bell-32V is the port of the 7th Edition Unix to the DEC/VAX architecture. &lt;/item&gt;&lt;item&gt; BSD-X tags correspond to 15 snapshots released from Berkeley. &lt;/item&gt;&lt;item&gt; 386BSD-X tags correspond to two open source versions of the system, with the Intel 386 architecture kernel code mainly written by Lynne and William Jolitz. &lt;/item&gt;&lt;item&gt; FreeBSD-release/X tags and branches mark 116 releases coming from the FreeBSD project. &lt;/item&gt;&lt;/list&gt;&lt;p&gt; In addition, branches with a &lt;/p&gt;-Snapshot-Development&lt;p&gt; suffix denote commits that have been synthesized from a time-ordered sequence of a snapshot's files, while tags with a &lt;/p&gt;-VCS-Development&lt;p&gt; suffix mark the point along an imported version control history branch where a particular release occurred. &lt;/p&gt;&lt;p&gt; The repository's history includes commits from the earliest days of the system's development, such as the following. &lt;/p&gt;&lt;quote&gt; commit c9f643f59434f14f774d61ee3856972b8c3905b1 Author: Dennis Ritchie &amp;lt;research!dmr&amp;gt; Date: Mon Dec 2 18:18:02 1974 -0500 Research V5 development Work on file usr/sys/dmr/kl.c &lt;/quote&gt;&lt;p&gt; Merges between releases that happened along the system's evolution, such as the development of BSD 3 from BSD 2 and Unix 32/V, are also correctly represented in the Git repository as graph nodes with two parents. &lt;/p&gt;&lt;p&gt; More importantly, the repository is constructed in a way that allows &lt;/p&gt;git blame&lt;p&gt;, which annotates source code lines with the version, date, and author associated with their first appearance, to produce the expected code provenance results. For example, checking out the &lt;/p&gt;BSD-4&lt;p&gt; tag, and running &lt;/p&gt;git blame&lt;p&gt; on the kernel's &lt;/p&gt;pipe.c&lt;p&gt; file will show lines written by Ken Thompson in 1974, 1975, and 1979, and by Bill Joy in 1980. This allows the automatic (though computationally expensive) detection of the code's provenance at any point of time. &lt;/p&gt;&lt;p&gt;Figure 1: Code provenance across significant Unix releases.&lt;/p&gt;&lt;p&gt; As can be seen in Figure &lt;/p&gt;1&lt;p&gt;, a modern version of Unix (FreeBSD 9) still contains visible chunks of code from BSD 4.3, BSD 4.3 Net/2, and FreeBSD 2.0. Interestingly, the Figure shows that code developed during the frantic dash to create an open source operating system out of the code released by Berkeley (386BSD and FreeBSD 1.0) does not seem to have survived. The oldest code in FreeBSD 9 appears to be an 18-line sequence in the C library file &lt;/p&gt;timezone.c&lt;p&gt;, which can also be found in the 7th Edition Unix file with the same name and a time stamp of January 10th, 1979 - 36 years ago. &lt;/p&gt;&lt;head rend="h2"&gt; 3 Data Collection and Processing&lt;/head&gt;&lt;p&gt; The goal of the project is to consolidate data concerning the evolution of Unix in a form that helps the study of the system's evolution, by entering them into a modern revision repository. This involves collecting the data, curating them, and synthesizing them into a single Git repository. &lt;/p&gt;&lt;p&gt;Figure 2: Imported Unix snapshots, repositories, and their mergers.&lt;/p&gt;&lt;p&gt; The project is based on three types of data (see Figure &lt;/p&gt;2&lt;p&gt;). First, &lt;/p&gt;snapshots of early released versions&lt;p&gt;, which were obtained from the &lt;/p&gt;Unix Heritage Society archive&lt;p&gt;,&lt;/p&gt;3&lt;p&gt; the &lt;/p&gt;CD-ROM images&lt;p&gt; containing the full source archives of CSRG,&lt;/p&gt;4&lt;p&gt; the &lt;/p&gt;OldLinux site&lt;p&gt;,&lt;/p&gt;5&lt;p&gt; and the &lt;/p&gt;FreeBSD archive&lt;p&gt;.&lt;/p&gt;6&lt;p&gt; Second, &lt;/p&gt;past and current repositories&lt;p&gt;, namely the CSRG SCCS [&lt;/p&gt;6&lt;p&gt;] repository, the FreeBSD 1 CVS repository, and the &lt;/p&gt;Git mirror of modern FreeBSD development&lt;p&gt;.&lt;/p&gt;7&lt;p&gt; The first two were obtained from the same sources as the corresponding snapshots. &lt;/p&gt;&lt;p&gt; The last, and most labour intensive, source of data was &lt;/p&gt;primary research&lt;p&gt;. The release snapshots do not provide information regarding their ancestors and the contributors of each file. Therefore, these pieces of information had to be determined through primary research. The authorship information was mainly obtained by reading author biographies, research papers, internal memos, and old documentation scans; by reading and automatically processing source code and manual page markup; by communicating via email with people who were there at the time; by posting a query on the Unix &lt;/p&gt;StackExchange&lt;p&gt; site; by looking at the location of files (in early editions the kernel source code was split into &lt;/p&gt;usr/sys/dmr&lt;p&gt; and &lt;/p&gt;/usr/sys/ken&lt;p&gt;); and by propagating authorship from research papers and manual pages to source code and from one release to others. (Interestingly, the 1st and 2nd Research Edition manual pages have an "owner" section, listing the person (e.g. &lt;/p&gt;ken&lt;p&gt;) associated with the corresponding system command, file, system call, or library function. This section was not there in the 4th Edition, and resurfaced as the "Author" section in BSD releases.) Precise details regarding the source of the authorship information are documented in the project's files that are used for mapping Unix source code files to their authors and the corresponding commit messages. Finally, information regarding merges between source code bases was obtained from a &lt;/p&gt;BSD family tree maintained by the NetBSD project&lt;p&gt;.&lt;/p&gt;8 &lt;p&gt; The software and data files that were developed as part of this project, are &lt;/p&gt;available online&lt;p&gt;,&lt;/p&gt;9&lt;p&gt; and, with appropriate network, CPU and disk resources, they can be used to recreate the repository from scratch. The authorship information for major releases is stored in files under the project's &lt;/p&gt;author-path&lt;p&gt; directory. These contain lines with a regular expressions for a file path followed by the identifier of the corresponding author. Multiple authors can also be specified. The regular expressions are processed sequentially, so that a catch-all expression at the end of the file can specify a release's default authors. To avoid repetition, a separate file with a &lt;/p&gt;.au&lt;p&gt; suffix is used to map author identifiers into their names and emails. One such file has been created for every community associated with the system's evolution: Bell Labs, Berkeley, 386BSD, and FreeBSD. For the sake of authenticity, emails for the early Bell Labs releases are listed in UUCP notation (e.g. &lt;/p&gt;research!ken&lt;p&gt;). The FreeBSD author identifier map, required for importing the early CVS repository, was constructed by extracting the corresponding data from the project's modern Git repository. In total the commented authorship files (828 rules) comprise 1107 lines, and there are another 640 lines mapping author identifiers to names. &lt;/p&gt;&lt;p&gt; The curation of the project's data sources has been codified into a 168-line &lt;/p&gt;Makefile&lt;p&gt;. It involves the following steps. &lt;/p&gt; Fetching &lt;p&gt; Copying and cloning about 11GB of images, archives, and repositories from remote sites. &lt;/p&gt; Tooling &lt;p&gt; Obtaining an archiver for old PDP-11 archives from 2.9 BSD, and adjusting it to compile under modern versions of Unix; compiling the 4.3 BSD &lt;/p&gt;compress&lt;p&gt; program, which is no longer part of modern Unix systems, in order to decompress the 386BSD distributions. &lt;/p&gt; Organizing &lt;p&gt; Unpacking archives using &lt;/p&gt;tar&lt;p&gt; and &lt;/p&gt;cpio&lt;p&gt;; combining three 6th Research Edition directories; unpacking all 1 BSD archives using the old PDP-11 archiver; mounting CD-ROM images so that they can be processed as file systems; combining the 8 and 62 386BSD floppy disk images into two separate files. &lt;/p&gt; Cleaning &lt;p&gt; Restoring the 1st Research Edition kernel source code files, which were obtained from printouts through optical character recognition, into a format close to their original state; patching some 7th Research Edition source code files; removing metadata files and other files that were added after a release, to avoid obtaining erroneous time stamp information; patching corrupted SCCS files; processing the early FreeBSD CVS repository by removing CVS symbols assigned to multiple revisions with a custom Perl script, deleting CVS &lt;/p&gt;Attic&lt;p&gt; files clashing with live ones, and converting the CVS repository into a Git one using &lt;/p&gt;cvs2svn&lt;p&gt;. &lt;/p&gt;&lt;p&gt; An interesting part of the repository representation is how snapshots are imported and linked together in a way that allows &lt;/p&gt;git blame&lt;p&gt; to perform its magic. Snapshots are imported into the repository as sequential commits based on the time stamp of each file. When all files have been imported the repository is tagged with the name of the corresponding release. At that point one could delete those files, and begin the import of the next snapshot. Note that the &lt;/p&gt;git blame&lt;p&gt; command works by traversing backwards a repository's history, and using heuristics to detect code moving and being copied within or across files. Consequently, deleted snapshots would create a discontinuity between them, and prevent the tracing of code between them. &lt;/p&gt;&lt;p&gt; Instead, before the next snapshot is imported, all the files of the preceding snapshot are moved into a hidden look-aside directory named &lt;/p&gt;.ref&lt;p&gt; (reference). They remain there, until all files of the next snapshot have been imported, at which point they are deleted. Because every file in the &lt;/p&gt;.ref&lt;p&gt; directory matches exactly an original file, &lt;/p&gt;git blame&lt;p&gt; can determine how source code moves from one version to the next via the &lt;/p&gt;.ref&lt;p&gt; file, without ever displaying the &lt;/p&gt;.ref&lt;p&gt; file. To further help the detection of code provenance, and to increase the representation's realism, each release is represented as a merge between the branch with the incremental file additions (&lt;/p&gt;-Development&lt;p&gt;) and the preceding release. &lt;/p&gt;&lt;p&gt; For a period in the 1980s, only a subset of the files developed at Berkeley were under SCCS version control. During that period our unified repository contains imports of both the SCCS commits, and the snapshots' incremental additions. At the point of each release, the SCCS commit with the nearest time stamp is found and is marked as a merge with the release's incremental import branch. These merges can be seen in the middle of Figure &lt;/p&gt;2&lt;p&gt;. &lt;/p&gt;&lt;p&gt; The synthesis of the various data sources into a single repository is mainly performed by two scripts. A 780-line Perl script (&lt;/p&gt;import-dir.pl&lt;p&gt;) can export the (real or synthesized) commit history from a single data source (snapshot directory, SCCS repository, or Git repository) in the &lt;/p&gt;Git fast export&lt;p&gt; format. The output is a simple text format that Git tools use to import and export commits. Among other things, the script takes as arguments the mapping of files to contributors, the mapping between contributor login names and their full names, the commit(s) from which the import will be merged, which files to process and which to ignore, and the handling of "reference" files. A 450-line shell script creates the Git repository and calls the Perl script with appropriate arguments to import each one of the 27 available historical data sources. The shell script also runs 30 tests that compare the repository at specific tags against the corresponding data sources, verify the appearance and disappearance of look-aside directories, and look for regressions in the count of tree branches and merges and the output of &lt;/p&gt;git blame&lt;p&gt; and &lt;/p&gt;git log&lt;p&gt;. Finally, &lt;/p&gt;git&lt;p&gt; is called to garbage-collect and compress the repository from its initial 6GB size down to the distributed 1GB. &lt;/p&gt;&lt;head rend="h2"&gt; 4 Data Uses&lt;/head&gt;&lt;p&gt; The data set can be used for empirical research in software engineering, information systems, and software archeology. Through its unique uninterrupted coverage of a period of more than 40 years, it can inform work on software evolution and handovers across generations. With thousandfold increases in processing speed and million-fold increases in storage capacity during that time, the data set can also be used to study the co-evolution of software and hardware technology. The move of the software's development from research labs, to academia, and to the open source community can be used to study the effects of organizational culture on software development. The repository can also be used to study how notable individuals, such as Turing Award winners (Dennis Ritchie and Ken Thompson) and captains of the IT industry (Bill Joy and Eric Schmidt), actually programmed. Another phenomenon worthy of study concerns the longevity of code, either at the level of individual lines, or as complete systems that were at times distributed with Unix (Ingres, Lisp, Pascal, Ratfor, Snobol, TMG), as well as the factors that lead to code's survival or demise. Finally, because the data set stresses Git, the underlying software repository storage technology, to its limits, it can be used to drive engineering progress in the field of revision management systems. &lt;/p&gt;&lt;p&gt;Figure 3: Code style evolution along Unix releases.&lt;/p&gt;&lt;p&gt; Figure &lt;/p&gt;3&lt;p&gt;, which depicts trend lines (obtained with R's local polynomial regression fitting function) of some interesting code metrics along 36 major releases of Unix, demonstrates the evolution of code style and programming language use over very long timescales. This evolution can be driven by software and hardware technology affordances and requirements, software construction theory, and even social forces. The dates in the Figure have been calculated as the average date of all files appearing in a given release. As can be seen in it, over the past 40 years the mean length of identifiers and file names has steadily increased from 4 and 6 characters to 7 and 11 characters, respectively. We can also see less steady increases in the number of comments and decreases in the use of the &lt;/p&gt;goto&lt;p&gt; statement, as well as the virtual disappearance of the &lt;/p&gt;register&lt;p&gt; type modifier. &lt;/p&gt;&lt;head rend="h2"&gt; 5 Further Work&lt;/head&gt;&lt;p&gt; Many things can be done to increase the repository's faithfulness and usefulness. Given that the build process is shared as open source code, it is easy to contribute additions and fixes through GitHub pull requests. The most useful community contribution would be to increase the coverage of imported snapshot files that are attributed to a specific author. Currently, about 90 thousand files (out of a total of 160 thousand) are getting assigned an author through a default rule. Similarly, there are about 250 authors (primarily early FreeBSD ones) for which only the identifier is known. Both are listed in the build repository's &lt;/p&gt;unmatched&lt;p&gt; directory, and contributions are welcomed. Furthermore, the BSD SCCS and the FreeBSD CVS commits that share the same author and time-stamp can be coalesced into a single Git commit. Support can be added for importing the SCCS file comment fields, in order to bring into the repository the corresponding metadata. Finally, and most importantly, more branches of open source systems can be added, such as NetBSD OpenBSD, DragonFlyBSD, and &lt;/p&gt;illumos&lt;p&gt;. Ideally, current right holders of other important historical Unix releases, such as System III, System V, NeXTSTEP, and SunOS, will release their systems under a license that would allow their incorporation into this repository for study. &lt;/p&gt;&lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt; The author thanks the many individuals who contributed to the effort. Brian W. Kernighan, Doug McIlroy, and Arnold D. Robbins helped with Bell Labs login identifiers. Clem Cole, Era Eriksson, Mary Ann Horton, Kirk McKusick, Jeremy C. Reed, Ingo Schwarze, and Anatole Shaw helped with BSD login identifiers. The BSD SCCS import code is based on work by H. Merijn Brand and Jonathan Gray. This research has been co-financed by the European Union (European Social Fund - ESF) and Greek national funds through the Operational Program "Education and Lifelong Learning" of the National Strategic Reference Framework (NSRF) - Research Funding Program: Thalis - Athens University of Economics and Business - Software Engineering Research Platform. &lt;head rend="h2"&gt;References&lt;/head&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;[1]&lt;/item&gt;&lt;item rend="dd-1"&gt; M. D. McIlroy, E. N. Pinson, and B. A. Tague, "UNIX time-sharing system: Foreword," The Bell System Technical Journal, vol. 57, no. 6, pp. 1899-1904, July-August 1978. &lt;/item&gt;&lt;item rend="dt-2"&gt;[2]&lt;/item&gt;&lt;item rend="dd-2"&gt; D. M. Ritchie and K. Thompson, "The UNIX time-sharing system," Bell System Technical Journal, vol. 57, no. 6, pp. 1905-1929, July-August 1978. &lt;/item&gt;&lt;item rend="dt-3"&gt;[3]&lt;/item&gt;&lt;item rend="dd-3"&gt; D. M. Ritchie, "The evolution of the UNIX time-sharing system," AT&amp;amp;T Bell Laboratories Technical Journal, vol. 63, no. 8, pp. 1577-1593, Oct. 1984. &lt;/item&gt;&lt;item rend="dt-4"&gt;[4]&lt;/item&gt;&lt;item rend="dd-4"&gt; P. H. Salus, A Quarter Century of UNIX. Boston, MA: Addison-Wesley, 1994. &lt;/item&gt;&lt;item rend="dt-5"&gt;[5]&lt;/item&gt;&lt;item rend="dd-5"&gt; E. S. Raymond, The Art of Unix Programming. Addison-Wesley, 2003. &lt;/item&gt;&lt;item rend="dt-6"&gt;[6]&lt;/item&gt;&lt;item rend="dd-6"&gt; M. J. Rochkind, "The source code control system," IEEE Transactions on Software Engineering, vol. SE-1, no. 4, pp. 255-265, 1975.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Footnotes:&lt;/head&gt; 1https://github.com/dspinellis/unix-history-repo 2&lt;p&gt;Updates may add or modify material. To ensure replicability the repository's users are encouraged to fork it or archive it. &lt;/p&gt; 3http://www.tuhs.org/archive_sites.html 4https://www.mckusick.com/csrg/ 5http://www.oldlinux.org/Linux.old/distributions/386BSD 6http://ftp-archive.freebsd.org/pub/FreeBSD-Archive/old-releases/ 7https://github.com/freebsd/freebsd 8http://ftp.netbsd.org/pub/NetBSD/NetBSD-current/src/share/misc/bsd-family-tree 9https://github.com/dspinellis/unix-history-make &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46077106</guid><pubDate>Fri, 28 Nov 2025 09:32:38 +0000</pubDate></item><item><title>EU Council Approves New "Chat Control" Mandate Pushing Mass Surveillance</title><link>https://reclaimthenet.org/eu-council-approves-new-chat-control-mandate-pushing-mass-surveillance</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46077393</guid><pubDate>Fri, 28 Nov 2025 10:36:08 +0000</pubDate></item><item><title>Google denies 'misleading' reports of Gmail using your emails to train AI</title><link>https://www.theverge.com/news/826902/gmail-ai-training-data-opt-out</link><description>&lt;doc fingerprint="59a8a47295b2b2eb"&gt;
  &lt;main&gt;
    &lt;p&gt;Google is pushing back on viral social media posts and articles like this one by Malwarebytes, claiming Google has changed its policy to use your Gmail messages and attachments to train AI models, and the only way to opt out is by disabling ‚Äúsmart features‚Äù like spell checking.&lt;/p&gt;
    &lt;head rend="h1"&gt;Google denies ‚Äòmisleading‚Äô reports of Gmail using your emails to train AI&lt;/head&gt;
    &lt;p&gt;Viral posts claim you need to opt out of Gmail‚Äôs ‚Äòsmart features‚Äô to avoid having your emails used to train AI, but Google says it doesn‚Äôt use the content of your emails for AI training.&lt;/p&gt;
    &lt;p&gt;Viral posts claim you need to opt out of Gmail‚Äôs ‚Äòsmart features‚Äô to avoid having your emails used to train AI, but Google says it doesn‚Äôt use the content of your emails for AI training.&lt;/p&gt;
    &lt;p&gt;But Google spokesperson Jenny Thomson tells The Verge that ‚Äúthese reports are misleading ‚Äì we have not changed anyone‚Äôs settings, Gmail Smart Features have existed for many years, and we do not use your Gmail content for training our Gemini AI model.‚Äù&lt;/p&gt;
    &lt;p&gt;You may want to double-check your settings anyway, as one Verge staffer also says they had opted out of some of the Smart Features, but had been opted back in to having them on. In January, Google updated its smart feature personalization settings so that you could turn off the features for Google Workspace and for other Google products (like Maps and Wallet) independently of each other.&lt;/p&gt;
    &lt;p&gt;In addition to things like spell checking, having Gmail‚Äôs smart features turned on enables features like tracking orders or easily adding flights from Gmail to your calendar. Enabling the feature in Workspace says that ‚Äúyou agree to let Google Workspace use your Workspace content and activity to personalize your experience across Workspace,‚Äù according to the settings page, but according to Google, that does not mean handing over the content of your emails to use for AI training.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46077404</guid><pubDate>Fri, 28 Nov 2025 10:38:41 +0000</pubDate></item><item><title>Africa's forests have switched from absorbing to emitting carbon</title><link>https://phys.org/news/2025-11-africa-forests-absorbing-emitting-carbon.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46077445</guid><pubDate>Fri, 28 Nov 2025 10:45:29 +0000</pubDate></item><item><title>Switzerland: Data Protection Officers Impose Broad Cloud Ban for Authorities</title><link>https://www.heise.de/en/news/Switzerland-Data-Protection-Officers-Impose-Broad-Cloud-Ban-for-Authorities-11093477.html</link><description>&lt;doc fingerprint="76fc90f1b6ae2b51"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Switzerland: Data Protection Officers Impose Broad Cloud Ban for Authorities&lt;/head&gt;
    &lt;p&gt;According to the Data Protection Conference, federal offices may only use US hyperscalers like AWS, Google, or Microsoft to a limited extent.&lt;/p&gt;
    &lt;p&gt;The Conference of Swiss Data Protection Officers, Privatim, has severely restricted the usability of international cloud services ‚Äì particularly hyperscalers like AWS, Google, or Microsoft ‚Äì for federal authorities in a resolution. At its core, the resolution from Monday amounts to a de facto ban on the use of these services as comprehensive Software-as-a-Service (SaaS) solutions whenever particularly sensitive or legally confidential personal data is involved. For the most part, authorities will likely only be able to use applications like the widespread Microsoft 365 as online storage.&lt;/p&gt;
    &lt;p&gt;The background to the position is the special responsibility of public bodies for the data of their citizens. While cloud services appear extremely attractive due to their economies of scale and dynamic resource allocation, data protection officers see significant risks in outsourcing sensitive data to international public clouds. Regardless of the sensitivity of the information, authorities must always analyze and mitigate such risks, but for particularly sensitive or confidential data in SaaS solutions from large international providers, Privatim considers outsourcing inadmissible in most cases.&lt;/p&gt;
    &lt;p&gt;The experts cite a lack of protection due to insufficient encryption and the associated loss of control as the main reasons. Most SaaS solutions do not yet offer true end-to-end encryption that would exclude the cloud provider's access to plaintext data. However, this is the central demand: The use is therefore only permissible if the data is encrypted by the public body itself and the cloud provider has no access to the key.&lt;/p&gt;
    &lt;p&gt;Videos by heise&lt;/p&gt;
    &lt;head rend="h3"&gt;Concerns about Cloud Act&lt;/head&gt;
    &lt;p&gt;Another point is the low transparency of globally operating companies. Swiss authorities can hardly verify compliance with contractual obligations regarding data protection and security, it is stated. This concerns both the implementation of technical measures and the control of employees and subcontractors, who sometimes form long chains of external service providers. Compounding this is the fact that software providers periodically unilaterally adjust contract terms.&lt;/p&gt;
    &lt;p&gt;Privatim is particularly concerned about the US Cloud Act. This can obligate providers there to hand over customer data to national authorities, even if the data is stored in Swiss data centers. Rules of international legal assistance do not have to be observed, the controllers complain. This creates considerable legal uncertainty, especially for data subject to a duty of confidentiality.&lt;/p&gt;
    &lt;p&gt;According to lawyer Martin Steiger most authority data is subject to a duty of confidentiality. Furthermore, meaningful use of many cloud services with continuous encryption is hardly possible. However, it remains to be seen whether the supervisory authorities will follow their words with actions this time. Cantonal controllers had already declared the use of Microsoft 365 generally inadmissible in the past, which had hardly any consequences. Nevertheless, the resolution presents authorities with challenges regarding their IT strategy.&lt;/p&gt;
    &lt;p&gt;(vbr)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46077885</guid><pubDate>Fri, 28 Nov 2025 12:00:35 +0000</pubDate></item><item><title>A Tale of Four Fuzzers</title><link>https://tigerbeetle.com/blog/2025-11-28-tale-of-four-fuzzers/</link><description>&lt;doc fingerprint="7f7892312c01258d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Tale Of Four Fuzzers&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;Charles Darnay observed that the gate was held by a mixed guard of soldiers and patriots, the latter far outnumbering the former; and that while ingress into the city for peasants√¢ carts bringing in supplies, and for similar traffic and traffickers, was easy enough, egress, even for the homeliest people, was very difficult.&lt;/p&gt;&lt;lb/&gt;√¢ Complaining about egress fees goes back to at least the French Revolution.&lt;/quote&gt;
    &lt;p&gt;Some time ago we overhauled TigerBeetle√¢s routing algorithm to better handle varying network topologies in a cluster. That turned out to be an interesting case study of practical generative testing (or fuzzing) for non-trivial, real-world code. We ended up adding not one, not even two, but four very different new fuzzers to the system! Let√¢s talk about why just one fuzzer is not enough.&lt;/p&gt;
    &lt;p&gt;This is a good moment to brew some tea, the journey will take us awhile!&lt;/p&gt;
    &lt;p&gt;Although this post isn√¢t primarily about the new algorithm itself, we√¢ll start by covering the basics of replication. TigerBeetle provides transaction Atomicity, Consistency, Isolation and Durability (ACID). Out of the four letters, the D, Durability, is the most consequential. For, without Durability, there wouldn√¢t be any data at all to provide guarantees for!&lt;/p&gt;
    &lt;p&gt;You can get a decent chunk of durability by writing the data to a (single) hard drive. This works for many non-critical applications, but might still fail if you repeat the procedure often enough. Disks are faulty with non-zero probability, and it is fairly common to lose an entire machine (floods, fires and tripping over the power supply happen). If you really want your data to be durable, better to store several copies of it on different machines, to replicate.&lt;/p&gt;
    &lt;p&gt;All data in TigerBeetle is ultimately derived from an append-only hash-chained log of prepare messages, so the task of replication reduces to distributing the prepares (a MiB each) across the six replicas of the cluster.&lt;/p&gt;
    &lt;p&gt;The primary sends &lt;code&gt;.prepare&lt;/code&gt; messages down to the backups;
they reply &lt;code&gt;.prepare_ok&lt;/code&gt; back up once the prepare is locally
durable. When the primary receives a quorum of
&lt;code&gt;.prepare_ok&lt;/code&gt;s, it knows that the message is globally
durable.&lt;/p&gt;
    &lt;p&gt;The most straightforward way to implement that is for the primary to broadcast the prepare:&lt;/p&gt;
    &lt;p&gt;The problem with this approach is that the primary uses 5x the bandwidth of the backup. In other words, we are going only at 1/5th of the optimal performance. For this reason, our V1 routing used a simple ring topology, where most replicas need to send and receive one message:&lt;/p&gt;
    &lt;p&gt;The ring replication is simple and balances the bandwidth nicely. It served well for the first year of production use, despite some critical issues!&lt;/p&gt;
    &lt;p&gt;First, the fixed ring topology falls prey to one of the eight fallacies of distributed computing. The ring is fully static, and assumes that network topology doesn√¢t change. But this is not true. For example, if one replica crashes or becomes partitioned, it is a good idea to proactively route around it, rather than rely on retries to randomly pick a different replica.&lt;/p&gt;
    &lt;p&gt;Second, the ring doesn√¢t have what I like to call √¢there√¢s no (re)try√¢ property. Most messages exchanged in the process of ring replication are critical: if a single message is lost, then the whole chain of replication unravels until the retry timeout kicks in. This means that network errors are visible as elevated P100 latencies (bad), and, when they happen, we have to run rarely-executed retry code (worse!). Such √¢cold code√¢ is the preferred habitat for bugs! Ideally, a system should have built-in redundancy such that any operation completes without tripping timeouts even in the presence of errors.&lt;/p&gt;
    &lt;p&gt;Thus, Adaptive Replication Routing (or, how we affectionately call it, ARR) was born. It combines two ideas. First, while we keep the ring as our replication topology, we place the primary into the middle:&lt;/p&gt;
    &lt;p&gt;The small downside is slightly uneven network load, as the primary sends two messages. The big upside is that none of the messages are critical. If any single message is dropped, the prepare is still going to be replicated to at least half of the cluster, allowing the primary to commit without tripping timeouts (recall that TigerBeetle is using Heidi Howard√¢s Flexible Quorums, so 3 of 6 as replication quorum is enough for safety because the view change quorum, is 4 of 6, preserving the intersection property).&lt;/p&gt;
    &lt;p&gt;The second trick is that the ring itself is dynamic. At runtime, the cluster picks the order of replicas that minimizes the latency overall. If one replica becomes unreachable, the replicas are reshuffled along the ring to move the missing one to the very end.&lt;/p&gt;
    &lt;p&gt;How do you find the best route? One approach is to build a model of the system. For example, replicas can exchange heartbeat messages, note pairwise latencies, and then solve traveling salesman problem in the resulting small six-node graph to find the most perfect route.&lt;/p&gt;
    &lt;p&gt;This works algorithmically, but relies on a pretty big assumption √¢ that our model of the world is faithful. But imagine, for example, a network with a link with very low latency, but also very low throughput. Using (small) heartbeat messages to measure the link quality would give us a misleading model that breaks down for (much larger) prepares.&lt;/p&gt;
    &lt;p&gt;The problem here isn√¢t this particular case, but the entire class of √¢out of the distribution√¢ errors which make any indirect measurement suspect (c.f. Goodhart√¢s Law). As another example, consider a replica with a very slow disk. Although the &lt;code&gt;ping&lt;/code&gt; time for it is very fast, the replication
is going to be slow, as &lt;code&gt;.prepare_ok&lt;/code&gt; is only sent once the
&lt;code&gt;.prepare&lt;/code&gt; is durably persistent. Pings only measure network
latency, but we also care about storage latency (and throughput).&lt;/p&gt;
    &lt;p&gt;A different approach, inspired laterally by the PCC paper, is to avoid modeling altogether, and instead to just go and do something, and then measure the relevant result directly, Grace Hopper style. This is how ARR works: for every &lt;code&gt;.prepare&lt;/code&gt;, the primary tracks how long did it take to
replicate (via tracking &lt;code&gt;.prepare_ok&lt;/code&gt; messages). Every once
in a while, it runs an experiment, where a prepare follows a different,
experimental route. If that experimental route is measured to
be better than the route we are currently using, the topology is
switched. Over time, the cluster converges to the optimal route.&lt;/p&gt;
    &lt;p&gt;That√¢s ARR in a nutshell: replication topology is a ring with the primary in the middle, where the order of replicas in the ring is adjusted dynamically based on how well each specific permutation performs end-to-end.&lt;/p&gt;
    &lt;p&gt;As promised, this post is not about ARR, so assume that you√¢ve already implemented ARR for TigerBeetle. How would you apply Deterministic Simulation Testing principles to it?&lt;/p&gt;
    &lt;p&gt;One approach is to leverage our existing &lt;del&gt;game&lt;/del&gt; whole-system simulation, VOPR. This actually gets you quite far, but it is always possible to do better.&lt;/p&gt;
    &lt;p&gt;First, whole system simulation might not be as efficient at exercising deeper layers of the system. For every permutation of events affecting the target layer, the simulator also needs to handle all other events above and below. Furthermore, the permutations you get might be restricted by the way the subsystem is used by the larger system. In other words, the routing component might be working correctly if used in the exact same way as in the real database, but it might still have bugs under certain interactions of its public APIs.&lt;/p&gt;
    &lt;p&gt;Second, while checking √¢it doesn√¢t crash√¢ is easy enough through the VOPR, asserting that the route is good is much harder. Again, there√¢s just too much other stuff happening to focus just on the contribution of routing.&lt;/p&gt;
    &lt;p&gt;That√¢s why the general principle in TigerBeetle is that, in addition to the main whole-system fuzzer, each subsystem should also have a targeted fuzzer, and ARR is no exception.&lt;/p&gt;
    &lt;p&gt;There√¢s a fairly general recipe for how to fuzz a subsystem in isolation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identify all the connections between the target and the rest of the system,&lt;/item&gt;
      &lt;item&gt;abstract the connections behind an interface,&lt;/item&gt;
      &lt;item&gt;supply a stub implementation for fuzzing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With some ingenuity, you can even avoid modifying your source code at all, instead leveraging runtime support to materialize interfaces out of thin air. For example, you can use &lt;code&gt;LD_PRELOAD&lt;/code&gt; tricks to
intercept all libc-mediated syscalls.&lt;/p&gt;
    &lt;p&gt;But there√¢s a catch! With a large and intricate interface, it might be challenging to thoroughly explore the state space, especially as the interface itself changes over time (and large and intricate things also mysteriously love to be high-churn as well). For the long term, it pays to start with the minimal possible interface.&lt;/p&gt;
    &lt;p&gt;Did you notice that I tricked you in the first paragraph in this section? You don√¢t first build a system, and then add a fuzzer. The process is almost the reverse √¢ the starting point is sketching minimal interfaces that yield themselves to efficient fuzzing. This is a bit like Test Driven Design, though, not exactly. There√¢s relatively little incrementality and iteration. Instead, fuzzer√¢s input on architecture is felt at the very beginning, during √¢sketching on the mental napkin√¢ phase.&lt;/p&gt;
    &lt;p&gt;Let√¢s do this for ARR. Again, the idea is that we observe timing information during replication (the delay between sending &lt;code&gt;.prepare&lt;/code&gt; and receiving a set of &lt;code&gt;.prepare_ok&lt;/code&gt;s)
and use that to gradually discover the best possible route, where the
route is a permutation of replicas with the current primary in the
middle.&lt;/p&gt;
    &lt;p&gt;Note how little in the above description is related to TigerBeetle! This is a hint that the routing component can be fully independent! This is the core interface of &lt;code&gt;Routing&lt;/code&gt;
(simplified for the blog, but just a touch):&lt;/p&gt;
    &lt;code&gt;pub fn init(
: struct { replica: u8, replica_count: u8 },
     options
 ) Routing;
pub fn op_prepare(
: *Routing, op: u64, now: Instant) void;
     routing
pub fn op_prepare_ok(
: *Routing, op: u64, now: Instant, replica: u8) void;
     routing
pub fn op_next_hop(routing: *Routing, op: u64) []const u8;

pub fn view_change(routing: *Routing, view: u32) void;&lt;/code&gt;
    &lt;p&gt;To start routing, you need to know how many replicas are there, and which one is you. &lt;code&gt;op_prepare&lt;/code&gt; and &lt;code&gt;op_prepare_ok&lt;/code&gt;
are for tracking timing. The contract is simple: &lt;code&gt;op_prepare&lt;/code&gt;
is called once a &lt;code&gt;.prepare&lt;/code&gt; is ready to be replicated, and
&lt;code&gt;op_prepare_ok&lt;/code&gt; is called for every received
&lt;code&gt;.prepare_ok&lt;/code&gt; response. That is, the happy path is six calls
to &lt;code&gt;op_prepare_ok&lt;/code&gt; for every call to
&lt;code&gt;op_prepare&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;op_next_hop&lt;/code&gt; is the actual routing √¢ it tells which
replicas a freshly received prepare needs to be forwarded to. It might
return zero, one or two replicas.&lt;/p&gt;
    &lt;p&gt;Finally, routing needs to know which replica is the primary. When the primary changes, so do the routes! The primary is uniquely defined by the view, which can be changed via the &lt;code&gt;view_change&lt;/code&gt;
method.&lt;/p&gt;
    &lt;p&gt;Note how minimal this interface is! We are just passing integers around (&lt;code&gt;Instant&lt;/code&gt; is a newtyped &lt;code&gt;u64&lt;/code&gt;. It√¢s a
topic for a separate blog post why we newtype &lt;code&gt;Instant&lt;/code&gt; but
not &lt;code&gt;view&lt;/code&gt;√¢¬¶). But this is not natural, it√¢s a result of
deliberate design process!&lt;/p&gt;
    &lt;p&gt;For example, &lt;code&gt;Routing&lt;/code&gt; routes &lt;code&gt;Prepare&lt;/code&gt;s, so it
would be natural to pass in the whole &lt;code&gt;Prepare&lt;/code&gt; structure
with all dependencies on the rest of the VSR framework. It takes intellectual
control to know that &lt;code&gt;Routing&lt;/code&gt; only cares about
&lt;code&gt;Prepare&lt;/code&gt;√¢s identity, and that op number is a concise
representation of that identity.&lt;/p&gt;
    &lt;p&gt;Handling of time deserves an entire separate article (good thing that we did write that up). The source of time is ultimately a &lt;code&gt;Clock&lt;/code&gt;
instance, so the most natural thing to do would be to inject
&lt;code&gt;Clock&lt;/code&gt; dependency in the constructor. But a moment√¢s
thinking makes you realize that a fully general clock is unnecessary. We
only care about the time difference between a &lt;code&gt;.prepare&lt;/code&gt; and
the corresponding &lt;code&gt;.prepare_ok&lt;/code&gt;s, which you can get, simply,
by accepting an &lt;code&gt;Instant&lt;/code&gt; √¢ a &lt;code&gt;u64&lt;/code&gt; number of
nanoseconds since an unspecified start of the epoch. This is a
major simplification for fuzzing, as time is notoriously tricky
to model, and here we get it essentially for free.&lt;/p&gt;
    &lt;p&gt;Finally, in order to downsize the interface, &lt;code&gt;view_change&lt;/code&gt;
violates one of the best best practices. It adds the second source of
truth for the view number! The authority about the current view is the
&lt;code&gt;Replica&lt;/code&gt; struct (this
lovely 12k sloc file) with a &lt;code&gt;view: u32&lt;/code&gt; field.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Routing&lt;/code&gt; needs to be aware of the view, and the most
straightforward way to do that is to inject the entire
&lt;code&gt;Replica&lt;/code&gt; in &lt;code&gt;init&lt;/code&gt;, using banana-gorilla-jungle
pattern of Joe Armstrong. The textbook fix would be to abstract √¢thing
with a &lt;code&gt;get_view&lt;/code&gt; method√¢ behind an interface and inject
that. But that indirection makes the code more verbose and
harder to reason about. It also is not enough: not only
&lt;code&gt;Routing&lt;/code&gt; needs to know the current view, it must actively
react to changes in the view! This can be fixed via Observer pattern,
but Observer is notorious for destroying readability of control flow and
bring a host of problems of its own, including complicated lifetime
management, non-deterministic order of execution and potential for
feedback loops.&lt;/p&gt;
    &lt;p&gt;It indeed is much simpler to just let &lt;code&gt;Routing&lt;/code&gt;
have its own private copy of &lt;code&gt;view: u32&lt;/code&gt;. And the risk of
views desynchronizing is easy to mitigate. We already have
&lt;code&gt;invariants&lt;/code&gt; method on the &lt;code&gt;Replica&lt;/code&gt; which is
called frequently to catch various violations, and it can check view
consistency as well:&lt;/p&gt;
    &lt;code&gt;pub fn invariants(self: *const Replica) void {
// ...
     self.view == self.routing.view);
     assert( }&lt;/code&gt;
    &lt;p&gt;You get the idea! The trick to making the code more easily fuzzable is to minimize the interface. You want to get rid of accidental dependencies and leave only the essential ones. And to do that, it helps to apply data-oriented design principles √¢ thinking in terms of input data, output data, and the fundamental data transformation that the system implements.&lt;/p&gt;
    &lt;p&gt;When the primary decides to switch the route after a successful experiment, it needs to communicate the new route to the peers. It√¢s a serialization/deserialization task. As there are at most six replicas in the cluster, and a route is a permutation thereof, a route is encoded compactly as an &lt;code&gt;u64&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;pub fn route_encode(routing: *const Routing, route: Route) u64;
pub fn route_decode(routing: *const Routing, code: u64) ?Route;

pub fn route_active(self: *const Replica) Route;
pub fn route_activate(routing: *Routing, route: Route) void;&lt;/code&gt;
    &lt;p&gt;Serialization is a favorite vehicle for explaining property based testing: checking that serializing data and then deserializing it back doesn√¢t lose a bit is an obvious thing to do (&lt;code&gt;deserialize . serialize == id&lt;/code&gt;, if you speak pointfree). So
we can generate a random permutation and assert that it round-trips the
encoding correctly:&lt;/p&gt;
    &lt;code&gt;test route_encode {
var prng = stdx.PRNG.from_seed(std.testing.random_seed);
     const replica_count =
     .range_inclusive(u8, 1, constants.replicas_max);
         prng
// Start with a trivial permutation, then shuffle it.
     var route: Route = .trivial(replica_count);
     .shuffle(u8, &amp;amp;route.replicas);
     prng
const code = route_encode(route);
     const route_decoded = route_decode(code).?;
     
.meta.eql(route, route_decoded));
     assert(std }&lt;/code&gt;
    &lt;p&gt;And here√¢s &lt;code&gt;shuffle&lt;/code&gt; for the reference, nothing fancy:&lt;/p&gt;
    &lt;code&gt;pub fn shuffle(prng: *PRNG, T: type, slice: []T) void {
if (slice.len &amp;lt;= 1) return;
     
for (0..slice.len - 1) |i| {
     const j = prng.range_inclusive(u64, i, slice.len - 1);
         .mem.swap(T, &amp;amp;slice[i], &amp;amp;slice[j]);
         std
     } }&lt;/code&gt;
    &lt;p&gt;This already is a decent test, but we can make it even better. There are at most six replicas in a cluster. That means there are &lt;code&gt;1! + 2! + ... + 6!&lt;/code&gt; routes in total we need to
check. This is a tiny number of routes, computer-wise, and we can easily
check them all in no time!&lt;/p&gt;
    &lt;p&gt;The only catch is that writing code to generate all permutations needs somewhat tricky recursion, and then you need to also iterate over number of replicas√¢¬¶ But there√¢s a secret cheat code here. This is it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you wrote a function that takes a PRNG and generates a random object, you already have a function capable of enumerating all objects.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Just imagine how the above function executes, from the perspective of the PRNG. You are constantly being asked to generate random numbers, which are used to shuffle the initial identity permutation. But what if you always return zero? Well, the resulting permutation will be in some sense trivial! And you can get the next permutation if you change the last zero to be one. And then two. And, if, say, the last number you are asked to generate needs to lie between zero and two, then after two you wrap back to zero, but also increment the penultimate number:&lt;/p&gt;
    &lt;code&gt;0 0 0 0 0
0 0 0 0 1
0 0 0 0 2
0 0 0 1 0
0 0 0 1 1&lt;/code&gt;
    &lt;p&gt;If you can generate all sequences of random numbers, you can turn a function generating a random object into a function enumerating all objects! And here√¢s how you can generate all random number sequences:&lt;/p&gt;
    &lt;code&gt;: bool = false,
 started: [32]struct { value: u32, bound: u32 } = undefined,
 v: usize = 0,
 p: usize = 0,
 p_max
const Gen = @This();

pub fn done(g: *@This()) bool {
if (!g.started) {
     .started = true;
         greturn false;
         
     }var i = g.p_max;
     while (i &amp;gt; 0) {
     -= 1;
         i if (g.v[i].value &amp;lt; g.v[i].bound) {
         .v[i].value += 1;
             g.p_max = i + 1;
             g.p = 0;
             greturn false;
             
         }
     }return true;
     
 }
fn gen(g: *Gen, bound: u32) u32 {
.p &amp;lt; g.v.len);
     assert(gif (g.p == g.p_max) {
     .v[g.p] = .{ .value = 0, .bound = 0 };
         g.p_max += 1;
         g
     }.p += 1;
     g.v[g.p - 1].bound = bound;
     greturn g.v[g.p - 1].value;
     
 }

/// Public API, get a "random" number in bounds:
pub fn int_inclusive(g: *Gen, Int: type, bound: Int) Int {
return @intCast(g.gen(@intCast(bound)));
      }&lt;/code&gt;
    &lt;p&gt;Makes no sense? For me too! Every time I look at this code, I need to solve the puzzle afresh. Luckily, there√¢s a write up: Generate All The Things.&lt;/p&gt;
    &lt;p&gt;The bottom line is that we can just wrap our existing random test into a while loop, and magically get an exhaustive test for all routes:&lt;/p&gt;
    &lt;code&gt;test route_encode {
var prng: Gen = .{};
     while (!prng.done()) {
     const replica_count =
         .range_inclusive(u8, 1, constants.replicas_max);
             prng
// Start with a trivial permutation, then shuffle it.
         var route: Route = .trivial(replica_count);
         .shuffle(u8, &amp;amp;route.replicas);
         prng
const code = route_encode(route);
         const route_decoded = route_decode(code).?;
         
.meta.eql(route, route_decoded));
         assert(std
     } }&lt;/code&gt;
    &lt;p&gt;That√¢s it! Testing every &lt;code&gt;replica_count&lt;/code&gt;, and
every permutation of replicas!&lt;/p&gt;
    &lt;p&gt;This is our first fuzzer √¢ we test serialization by encoding and decoding a random route. We also notice that the total amount of routes is small, and adapt our random code to exhaustively cover the entire positive space, using a rigged PRNG.&lt;/p&gt;
    &lt;p&gt;Testing only positive space is a common pitfall. We want to check serialization and deserialization for routes. We do that by round-tripping the route. We even make sure to check every possible route, how can there be anything else left to test here?&lt;/p&gt;
    &lt;p&gt;This is an example of a positive space thinking, which sometimes gives us false confidence that everything is thoroughly tested, while we are failing to consider some cases off the happy path.&lt;/p&gt;
    &lt;p&gt;What we missed here is that not every code necessarily encodes a valid route. We only feed √¢valid√¢ data to deserialization routine, but who knows what bytes you can receive through the TCP socket?&lt;/p&gt;
    &lt;p&gt;Now, this is tricky: actually, TigerBeetle only talks to other TigerBeetle replicas, and all communication is protected by a strong checksum. So it is actually correct to assume that the encoding is valid, modulo bugs. But there might be bugs! And, if there√¢s a bug somewhere which manifests itself as an invalid encoding, we want to detect that and crash loudly, rather than silently misinterpret valid data.&lt;/p&gt;
    &lt;p&gt;That√¢s why the decode function returns a nullable &lt;code&gt;Route&lt;/code&gt;√¢¬¶&lt;/p&gt;
    &lt;code&gt;pub fn route_decode(routing: *const Routing, code: u64) ?Route;&lt;/code&gt;
    &lt;p&gt;but at the call-site the &lt;code&gt;Route&lt;/code&gt; is unwrapped:&lt;/p&gt;
    &lt;code&gt;const route = self.routing.route_decode(message.header.route).?;&lt;/code&gt;
    &lt;p&gt;This is offensive programming, we want to force bugs to jump into the spotlight, and not to lie hidden on odd cold paths.&lt;/p&gt;
    &lt;p&gt;The most straightforward way to test the negative space here is to run our test backwards, and to try deserialize and then serialize a random code:&lt;/p&gt;
    &lt;code&gt;test route_decode {
var prng = stdx.PRNG.from_seed(std.testing.random_seed);
     
const code = prng.int(u64);
     if (route_decode(code)) |route| {
     const code_encoded = route_encode(route);
         == code_encoded);
         assert(code else {
     } // Just make sure we don't crash!
         
     } }&lt;/code&gt;
    &lt;p&gt;There√¢s a subtle problem with a test above √¢ the √¢then√¢ branch of the if is dead code, and we√¢ll never get there, even if we repeat the test a hundred million times:&lt;/p&gt;
    &lt;code&gt;test route_decode {
var prng = stdx.PRNG.from_seed(std.testing.random_seed);
     
for (0..100_000_000) |_| {
     const code = prng.int(u64);
         if (route_decode(code)) |_| {
         false);
             assert(else {
         } // Just make sure we don't crash!
             
         }
     } }&lt;/code&gt;
    &lt;code&gt;$ t ./zig/zig build test --release -- route_decode

real 7.14s
cpu  7.16s (7.08s user + 76.41ms sys)
rss  34.97mb&lt;/code&gt;
    &lt;p&gt;Our completely random encoding never manages to generate a valid code!&lt;/p&gt;
    &lt;p&gt;As we have seen above, there are very few different routes, and, therefore, very few valid encodings. But our code is &lt;code&gt;u64&lt;/code&gt;.
The space of all possible codes is huge, but the subspace of all
valid codes is very sparse.&lt;/p&gt;
    &lt;p&gt;Is this a problem? We checked all valid codes, so it√¢s fine if we only look at the invalid ones? No! Given just how rarefied our encoding space is, purely random codes are going to be obviously invalid. The decoding routine will reject them very quickly, and we are likely to not exercise most of the logic there.&lt;/p&gt;
    &lt;p&gt;For effective fuzzing, you want to test the boundary: you want to check a valid code, and a code which is almost the same, but invalid.&lt;/p&gt;
    &lt;p&gt;For that, we bias our generator to prefer codes in the neighborhood of valid encodings:&lt;/p&gt;
    &lt;code&gt;var code_bytes: [8]u8 = @splat(0);
for (&amp;amp;code_bytes) |*byte| {
.* = if (prng.chance(ratio(replica_count + 1, 8)))
     byte.int_inclusive(u8, constants.replicas_max + 1)
         prngelse
     0xFF;
         
 }var code: u64 = @bitCast(code_bytes);

if (prng.chance(ratio(1, 20))) {
^= prng.bit(u64);
     code 
 }if (prng.chance(ratio(1, 20))) {
= prng.int(u64);
     code  }&lt;/code&gt;
    &lt;p&gt;The encoding is literally a permutation of replica indexes, where each replica index is a byte, padded by &lt;code&gt;0xFF&lt;/code&gt; bytes to
&lt;code&gt;u64&lt;/code&gt;. We generate a random mish-mash of those bytes (which
just might generate a valid code), then, to spice thing up, we
randomly corrupt a single bit of code. Finally, to make sure we don√¢t
just generate almost valid code, sometimes we throw everything
away and fall back to fully random.&lt;/p&gt;
    &lt;p&gt;This sounds plausible, but is this actually true? Do we actually hit the boundary here, generate both valid and invalid codes? And how do we make sure that our negative-space fuzzer continues to test interesting cases as the code itself evolves (it certainly looks like we can optimize the encoding to be more compact√¢¬¶)?&lt;/p&gt;
    &lt;p&gt;A good pattern here is to repeat the test many times, counting all the sad and happy cases, and assert that they are reasonable:&lt;/p&gt;
    &lt;code&gt;test route_decode {
const Counts =
     struct { total: u32, valid: u32, invalid: u32 };
         
var prng = stdx.PRNG.from_seed(std.testing.random_seed);
     
var counts: Counts =
     .{ .total = 200_000, .valid = 0, .invalid = 0 };
         
for (0..counts.total) |_| {
     //...
         if (route_decode(code)) |_| {
         .valid += 1;
             counts//...
             else {
         } .invalid += 1;
             counts
         }
     }
.total == counts.valid + counts.invalid);
     assert(counts.valid &amp;gt; 50);
     assert(counts.invalid &amp;gt; 100_000);
     assert(counts }&lt;/code&gt;
    &lt;p&gt;Due to randomness, we can√¢t check the exact values of counters, but we can assert that most of the encodings are invalid, and that at least some are valid (remember, our initial test generated zero valid encodings out of 100 000 000 attempts).&lt;/p&gt;
    &lt;p&gt;This brings me to another topic I want to cover √¢ treatment of determinism in tests. √¢Thy tests shall be deterministic√¢ is a reasonable commandment, but not an absolute one. I see that often people try to avoid randomness in tests at all costs, and always initialize PRNG with a hard-coded seed of 42. I don√¢t like that, for two reasons.&lt;/p&gt;
    &lt;p&gt;The practical reason is that, over its lifetime, the test is going to be re-run many thousand times over, and it is wasteful to not take advantage of that to explore more of the state space eventually, while keeping each individual test run very fast.&lt;/p&gt;
    &lt;p&gt;The purity reason is that, if there exists a seed value that makes the test fail, the test (or the code) is buggy and needs to be fixed! Sure, it√¢s unfortunate if you discover that bug while working on an unrelated change, but it is less unfortunate than not knowing about the bug at all!&lt;/p&gt;
    &lt;p&gt;However, just using genuinely random seeds for tests is pretty bad:&lt;/p&gt;
    &lt;code&gt;test route_decode {
const seed = std.crypto.random.int(u64);
      }&lt;/code&gt;
    &lt;p&gt;The problem with the above is that, when a test fails, you don√¢t know the seed! And, if it is one-in-a-million failure, it can be very a frustrating experience to reproduce it. This can be helped by printing the seed on failure, but that A) requires writing more code per test and, B) doesn√¢t work if the failure is not graceful. Imagine getting a mystery segfault on some random CI run, and then not being able to reproduce it because the process dies before the seed is printed!&lt;/p&gt;
    &lt;p&gt;Zig I think has the best design in this space. It provides you with the &lt;code&gt;std.testing.random_seed&lt;/code&gt; value, which is a ready-to-use
random seed that is different per run. Crucially, the seed is generated
outside of the test process itself and is passed to it on the CLI. It
doesn√¢t matter what happens with the test process. It can explode
completely, but the parent process will still print the seed on failure.
Conveniently, the seed is printed as a part of a CLI invocation which
you can immediately paste into your shell!&lt;/p&gt;
    &lt;code&gt;$ ./zig/zig build test

test
+- run test-vsr failure
thread 2285 panic: reached unreachable code
...
error: while executing test 'vsr.test.routing.route_decode'
error: the following command terminated with signal 6:

.zig-cache/o/14db484/test-vsr --seed=0x737929ed&lt;/code&gt;
    &lt;p&gt;So that√¢s why we√¢ve been using &lt;code&gt;PRNG.from_seed(testing.random_seed)&lt;/code&gt; throughout! And it has
been working perfectly, up until now. Here√¢s the problem:&lt;/p&gt;
    &lt;code&gt;var prng = stdx.PRNG.from_seed(std.testing.random_seed);

//...

.total == counts.valid + counts.invalid);
 assert(counts.valid &amp;gt; 50);
 assert(counts.invalid &amp;gt; 100_000); assert(counts&lt;/code&gt;
    &lt;p&gt;The seed is random, so, sooner or later, our assert will fire. We can make the probability of that negligible by increasing &lt;code&gt;total&lt;/code&gt; and increasing our tolerance, but that is
unsatisfactory. Larger iteration count slows down each individual test
run. And relaxing asserts tells us less about the average case,
what we actually care about. And we don√¢t know what√¢s the actual
probability of hitting the assert! It might be that the actual
probability is small, but not infinitesimal, such that you√¢ll be
debugging a random √¢failure√¢ five years from now! One in a
billion events do happen in CI!&lt;/p&gt;
    &lt;p&gt;A nice pattern here is to run the test twice: once with a hard-coded seed to capture the √¢average√¢ distribution and assert statistics, and once with a truly random seed for coverage:&lt;/p&gt;
    &lt;code&gt;test route_decode {
const T = struct {
     const Counts =
         struct { total: u32, valid: u32, invalid: u32 };
             
fn check(seed: u64) Counts {
         // ...
             
         }
     };
const counts = T.check(92);
     .total == counts.valid + counts.invalid);
     assert(counts.valid &amp;gt; 50);
     assert(counts.invalid &amp;gt; 100_000);
     assert(counts
= T.check(std.testing.random_seed);
     _  }&lt;/code&gt;
    &lt;p&gt;This is our second fuzzer √¢ testing for negative space by probing obviously invalid values, and then specifically values that cross the valid/invalid boundary, while collecting and asserting coverage information.&lt;/p&gt;
    &lt;p&gt;For this particular scenario, it would√¢ve been better to use a real coverage-guided fuzzer like libFuzzer, but, at the time of writing, Zig is only at the start of its fuzzing journey. It already has &lt;code&gt;std.testing.fuzz&lt;/code&gt;, but I wasn√¢t able to get that working on
my machine. Anyway the implementation of the fuzzer is a detail. What
matters is the principle of explicit testing of negative space, the
boundary space, and verifying that both ins and outs get tested!&lt;/p&gt;
    &lt;p&gt;Moreover, just like we got exhaustive test by driving PRNG interface via exhaustive enumeration from inside, we can drive a PRNG through a fuzzer. You can combine the best of both worlds: highly structured complex inputs of property based testing and introspective guided program state exploration of coverage-guided fuzzers. This again is worth a separate blog post, but I really need to do more research before it is ready. However, I√¢ll be sharing what I got so far on 1000x world tour on December 3 in Lisbon next week. Come, say hello if you are around: https://luma.com/7d47f4et!&lt;/p&gt;
    &lt;p&gt;Ok, the warmup is over! Serialization was a simple and boring part of Adaptive Replication Routing. Let√¢s tackle the actual logic. Similarly, we√¢ll start with a positive space, checking that ARR indeed converges to the best route in a scenario approximating what we expect to see in the real world.&lt;/p&gt;
    &lt;p&gt;This is going to be interesting, because it is not a local correctness property. We want to check that six instances of ARR on six different physical machines work in concert, such that, e.g., everyone agrees which operations are experiments, and what is the route of each experiment.&lt;/p&gt;
    &lt;p&gt;Here√¢s the plan. We arrange six replicas into a virtual ring, such that the network delay between replicas is proportional to the distance along the ring. The order of replicas is random, and correctly implemented ARR must be able to √¢unscramble√¢ the permutation in the end. Each √¢replica√¢ is just a &lt;code&gt;Routing&lt;/code&gt; instance. This is the
entire idea behind &lt;del&gt;The Matrix&lt;/del&gt; focused fuzzing, we don√¢t need to simulate anything else!&lt;/p&gt;
    &lt;code&gt;const T = struct {
: u8,
     replica_count: []u8,
     permutation
: u32,
     view: u8,
     primary: u8,
     prepare_ok_count
: []Routing,
     replicas
const T = @This();
     
pub fn init(gpa: Allocator, seed: u64) T { ... }
     
pub fn deinit(t: *T, gpa: Allocator) void { ... }
     
fn ring_index(t: *const T, replica: u8) i8 {
     return @intCast(t.permutation[replica]);
         
     }
fn distance(t: *const T, a: u8, b: u8) u8 {
     const a2b = @abs(t.ring_index(b) - t.ring_index(a));
         const b2a = t.replica_count - a2b;
         return @min(a2b, b2a);
         
     } };&lt;/code&gt;
    &lt;p&gt;An optimal route enumerates replicas in the order of &lt;code&gt;permutation&lt;/code&gt; in either of two directions (there are two
optimal routes!). We can check that by summing up pairwise
distances:&lt;/p&gt;
    &lt;code&gt;fn route_total_distance(t: *const T, route: Route) u8 {
var result: u8 = 0;
     for (
     .replicas[0 .. t.replica_count - 1],
         route.replicas[1..t.replica_count],
         route|a, b| {
     ) += t.distance(a, b);
         result 
     }return result;
     
 }
fn route_optimal(t: *const T, route: Route) bool {
return t.total_route_distance(route) == t.replica_count - 1;
      }&lt;/code&gt;
    &lt;p&gt;The overall flow of the fuzzer is as follows. We send prepares one by one. For each prepare, we run the simulation until the primary collects &lt;code&gt;prepare_ok&lt;/code&gt; messages from everybody.
&lt;code&gt;prepare_ok_count&lt;/code&gt; field tells us when we should start with
the next prepare. Submitting a prepare is modeled via sending a message
to the primary. When a set number of prepares is dealt with, we check
that the final route is optimal.&lt;/p&gt;
    &lt;p&gt;Note that this is not how the real replication works, reality is pipelined, and multiple prepares are in flight at the same time. However, the purpose of this particular fuzzer isn√¢t to check a √¢realistic√¢ scenario, the purpose is to check the idealized scenario, but be very strict in the acceptance criteria (that the route really is optimal).&lt;/p&gt;
    &lt;p&gt;The full code is a bit too much for this article, but the core logic of simulating replication process is this &lt;code&gt;message_delivered&lt;/code&gt;
function. It models what happens when replica &lt;code&gt;target&lt;/code&gt;
receives a &lt;code&gt;message&lt;/code&gt; from &lt;code&gt;source&lt;/code&gt;. Which is,
forward the message along the ring, and reply with
&lt;code&gt;.prepare_ok&lt;/code&gt; to the primary.&lt;/p&gt;
    &lt;code&gt;fn message_delivered(
: *T,
     t: u8,
     source: u8,
     target: union(enum) { prepare: u64, prepare_ok: u64 },
     messagevoid {
 ) switch (message) {
     .prepare =&amp;gt; |op| {
         // The initial prepare is injected by the fuzzer.
             if (target == t.primary) {
             == t.primary);
                 assert(source .prepare_ok_count == 0);
                 assert(t.replicas[t.primary].op_prepare(op, t.now());
                 t
             }
// Inform the primary that we got the prepare.
             .send(.{
             t.source = target,
                 .target = t.primary,
                 .message = .{ .prepare_ok = op },
                 
             });
// Forward prepare along the current replication ring.
             for (t.replicas[target].op_next_hop(op)) |target_next| {
             &amp;lt; t.replica_count);
                 assert(target_next .send(.{
                 t.source = target,
                     .target = target_next,
                     .message = .{ .prepare = op },
                     
                 });
             },
         }
.prepare_ok =&amp;gt; |op| {
         == t.primary);
             assert(target .prepare_ok_count += 1;
             t.replicas[t.primary].op_prepare_ok(op, t.now(), source);
             t,
         }
     } }&lt;/code&gt;
    &lt;p&gt;What√¢s fascinating about this fuzzer is not the implementation, but rather the bugs it was able to find. Writing the fuzzer was a relatively mechanical and mindless process, other than the initial idea of modeling a physical ring of replicas. But the two failures it found revealed my misunderstanding of the problem, and forced me to apply deeper thinking where I thought I understood everything.&lt;/p&gt;
    &lt;p&gt;To explain that, I need to talk about the ARR cost function. After an ARR experiment, the primary somehow needs to measure the quality of a the experimental route. The data we have are &lt;code&gt;.prepare_ok&lt;/code&gt; latencies for all replicas √¢ a vector of six
integers.&lt;/p&gt;
    &lt;p&gt;My initial cost function was a pair of the median and the maximum value of the vector, with some fuzz factor:&lt;/p&gt;
    &lt;code&gt;.of(.{
 Cost.ms(31), .ms(178), .ms(148),
     .ms(92), .ms(144), .ms(50),
     ==
 }) .{ .median = .ms(92), .maximum = .ms(178) }     &lt;/code&gt;
    &lt;p&gt;The median tracks the moment in time when a half of the cluster acknowledged the prepare, which, due to flexible quorums, is the moment where it is safe to commit prepare. The median replication time is a proxy for user-visible latency, and it is the primary number we are optimizing for.&lt;/p&gt;
    &lt;p&gt;After we replied to the user, we still want to replicate the prepare to the rest of the cluster, to maximize durability. The maximum replication time directly tracks full replication, and it√¢s the second most important metric to optimize.&lt;/p&gt;
    &lt;p&gt;Finally, we don√¢t want the cluster to oscillate between two nearly identical routes simply due to random delay noise, so we also add a fuzz factor and consider close enough numbers to be equal for comparison purposes.&lt;/p&gt;
    &lt;p&gt;Can you see the bug here? I didn√¢t, but the fuzzer I wrote did. After running for a short time, the fuzzer found the case where ARR failed to converge to the optimal path. Here√¢s the path that that run ended up with:&lt;/p&gt;
    &lt;p&gt;This is indeed an optimal path in terms of median,maximum cost function. The median is two hops, the maximum is three. But it is not actually optimal, because replicas between median and maximum take longer time to replicate, and we care about that as well, as that√¢s a proxy for us selecting the most efficient route for each replica. It doesn√¢t affect important latencies, but it still sends the electrons further away than they√¢d otherwise need to go.&lt;/p&gt;
    &lt;p&gt;The fix is easy √¢ add a third component to the cost function, the sum of all latencies.&lt;/p&gt;
    &lt;p&gt;The problem was fixed, but, after a few iterations more, I got another example that failed to converge to an optimal route. It took me an embarrassingly long time to debug that, but the explanation was really simple. My fuzz factor was too fuzzy, and made two different routes look the same. This fix also was simple, just tighten up the √¢almost equal√¢ condition.&lt;/p&gt;
    &lt;p&gt;But what bugged me is that, in my mental model, the old fuzz factor was fuzzy enough as is. So I tried to explain why it didn√¢t work, and realized that I had a completely wrong mental image of replication routes. And, yes, all the illustrations I√¢ve drawn so far also have this bug. Do you see it?&lt;/p&gt;
    &lt;p&gt;This is what the actual replication route looks like:&lt;/p&gt;
    &lt;p&gt;Prepares flow forward along the ring, but acknowledgements always flow directly to the primary, in a star topology. When the primary measures the replication latency, it captures both the time to send the &lt;code&gt;.prepare&lt;/code&gt; forward and the time to get the
corresponding &lt;code&gt;.prepare_ok&lt;/code&gt; back. And the time to receive all
&lt;code&gt;.prepare_ok&lt;/code&gt; is independent of the route!&lt;/p&gt;
    &lt;p&gt;In other words, changing the route can affect only half of the observed latencies, which makes relative difference between the routes smaller, and justifies tighter tolerances.&lt;/p&gt;
    &lt;p&gt;This was a huge shift in the mental model for me! I didn√¢t realize that we only observe latencies through the glass, darkly! I hadn√¢t thought about that myself, but the fuzzer did!&lt;/p&gt;
    &lt;p&gt;This is our third fuzzer. It is a whole subsystem positive space fuzzer. It√¢s actually an exuberantly optimistic fuzzer, as it sets up an ideal lab environment with extremely predictable network latencies. While not realistic, this setup ensures that there√¢s a clear answer to the question of which route is the best, and that allows us to verify that the algorithm is exactly correct, and not merely crash free. This is the catch √¢ in the real system with faults and variants, the notion of optimal route is ill-defined and constantly changes. The acceptance criteria has to be fuzzy in a realistic simulation, but can be very strict in the lab.&lt;/p&gt;
    &lt;p&gt;Finally, the fourth fuzzer. You might guess it, we√¢ll go for negative space this time. We no longer care about how the Routing should be used by the replica, we are trying to break it.&lt;/p&gt;
    &lt;p&gt;The fundamental difference here is that, for positive space, we modeled all six √¢replicas√¢ at the same time messages flowing between them. But any model of that sort necessarily restricts us to executions possible in the cluster. Now we won√¢t be trying to model anything in particular. We√¢ll have just a single instance of &lt;code&gt;Routing&lt;/code&gt;
and will be calling all public methods in random order, only obeying the
documented invariants:&lt;/p&gt;
    &lt;code&gt;// Simulate the entire cluster:
const PositiveSpace = struct {
: []Routing,
     replicas// ...
     
 };
// Hammer a single replica, hard:
const NegativeSpace = struct {
: Routing,
     replica// ...
      };&lt;/code&gt;
    &lt;p&gt;There isn√¢t much we can check here, but we can check something. At minimum, we should never crash. Additionally, we can check that whatever route we have, it √¢connects√¢. That is, if we follow the chain of &lt;code&gt;next_hop&lt;/code&gt;s, we√¢ll visit each replica
exactly once.&lt;/p&gt;
    &lt;p&gt;The code isn√¢t particularly illuminating here, but the overall shape looks similar to the technique described in the Swarm Testing Data Structures.&lt;/p&gt;
    &lt;p&gt;That√¢s it for today! This was a tale of four fuzzers!&lt;/p&gt;
    &lt;p&gt;Yeah√¢¬¶ At Fuzzer #3, I realized that we actually wrote five fuzzers for ARR, but the title and the Dickens quote had really grown on me by that time. Sorry for this, here√¢s a bonus fuzzer for you!&lt;/p&gt;
    &lt;p&gt;Our positive space ARR fuzzer explores a really specific network topology, which is roughly as far from a realistic scenario as the negative space fuzzer, but in the opposite direction √¢ everything√¢s too good, no one√¢s crashing, the network gives stable latencies.&lt;/p&gt;
    &lt;p&gt;What we are missing is the realistic fuzzer between the two extremes. A fuzzer that runs in a somewhat flaky network, and checks that the route is roughly optimal (or at least not bad). But that is the VOPR! As a whole system fuzzer, it is capable of simulating somewhat realistic distributions of network faults and delays.&lt;/p&gt;
    &lt;p&gt;Historically, VOPR was biased towards faulting as much things as hard as possible, as we want TigerBeetle to be correct and fast, in that order. Now that we started optimization work, we implemented &lt;code&gt;--performance&lt;/code&gt; mode for VOPR.&lt;/p&gt;
    &lt;p&gt;In the default mode, VOPR uses swarm testing to generate distribution of faults (during fuzzing, you generate random events. The idea of swarm testing is to also generate the distribution itself at random). In the performance mode, fault parameters are fixed to √¢realistic√¢ values, and the drastic faults (replicas crashing or becoming partitioned) are strictly controlled (e.g., you can request exactly one crash per run):&lt;/p&gt;
    &lt;code&gt;fn options_swarm(prng: *stdx.PRNG) Simulator.Options
fn options_performance() Simulator.Options&lt;/code&gt;
    &lt;p&gt;Furthermore, in performance mode VOPR tracks statistics about the number of network messages exchanged. ARR was verified by running different performance VOPR scenarios with and without ARR, and checking that ARR is an improvement across the board:&lt;/p&gt;
    &lt;code&gt;√é¬ª ./zig/zig build vopr -- --performance --replica-missing=2

          SEED=1044607978391563277

          replicas=6
          clients=4
          one_way_delay_mean=50ms ticks
          one_way_delay_min=0ns ticks
          packet_loss_probability=0
          path_maximum_capacity=10 messages
          packet_replay_probability=0
          crash_probability=0
          crash_stability=500 ticks
          restart_probability=0
          ...

Messages:
prepare                     1881    1.23MiB
prepare_ok                  1575  393.75KiB
request_prepare              795  198.75KiB
request                      730  510.08KiB
ping                         550  275.00KiB
reply                        503  363.76KiB
request_headers              466  116.50KiB
pong                         440  110.00KiB
headers                      328  320.75KiB
commit                       285   71.25KiB
start_view                    85  224.25KiB
do_view_change                25   12.50KiB
request_start_view            10    2.50KiB
total                       7673    3.77MiB


          PASSED (1741 ticks)&lt;/code&gt;
    &lt;p&gt;It√¢s a bit hard to turn these manual experiments into tests that fail only if there are bugs (and not due to randomness or unrelated code choices), but just tinkering with the setup is a great way to quickly test ideas. VOPR runs much faster than a real-world cluster would, so you can use it to collect a fairly long performance trace.&lt;/p&gt;
    &lt;p&gt;This was a long one, wasn√¢t it? Although it√¢s just one system and five fuzzers, no two fuzzers are alike, each illuminates its own corner of the design space. If you want a closer looks, here√¢s the source code, it√¢s almost exactly a thousand lines for the implementation plus the fuzzers.&lt;/p&gt;
    &lt;p&gt;To jolt the ideas back into the short term (and, who knows, maybe a long term) memory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You want both a whole system fuzzer AND subsystem (minor) fuzzers. Main fuzzer works out the seams between components, while minor fuzzers divide&amp;amp;conquerer the resulting combinatorial explosion.&lt;/item&gt;
      &lt;item&gt;Good fuzzing is tantamount to good interfaces.&lt;/item&gt;
      &lt;item&gt;Interfaces can be extracted mechanically, by introducing indirection whenever a dependency happens.&lt;/item&gt;
      &lt;item&gt;But such a mechanical interface extraction risks ossifying accidental dependencies.&lt;/item&gt;
      &lt;item&gt;Long-term more efficient approach is to think in terms of fundamental input and output data. Sometimes a little copying is better than a little dependency!&lt;/item&gt;
      &lt;item&gt;Data interfaces tend to be non-incremental. The best time to capture an interface is before the first line of code is written.&lt;/item&gt;
      &lt;item&gt;Fuzz positive space and negative space.&lt;/item&gt;
      &lt;item&gt;Given a PRNG interface, its easy to explore structured search space.&lt;/item&gt;
      &lt;item&gt;If the search space is small, you can use the same PRNG interface to walk it thoroughly and exhaustively.&lt;/item&gt;
      &lt;item&gt;And you can plug the same PRNG interface into coverage guided fuzzer.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;deserialize . serialize&lt;/code&gt;is positive space,&lt;code&gt;serialize . deserialize&lt;/code&gt;can be negative space.&lt;/item&gt;
      &lt;item&gt;Hard to breath in rarefied air! Purely random inputs can be uniformly boring and bounce off the edges of the system.&lt;/item&gt;
      &lt;item&gt;For negative space testing, you want to hew close to the valid/invalid boundary, poking out from both sides.&lt;/item&gt;
      &lt;item&gt;You still want some amount of purely random inputs, just in case.&lt;/item&gt;
      &lt;item&gt;You want to assert that both positive and negative cases actually happen with non-negligible probability.&lt;/item&gt;
      &lt;item&gt;Run fuzzer once with a fixed seed (I use &lt;code&gt;92&lt;/code&gt;), to sanity check the count of good and bad cases.&lt;/item&gt;
      &lt;item&gt;Run fuzzer again with a genuinely random seed to accumulate coverage over time.&lt;/item&gt;
      &lt;item&gt;Make sure to generate the seed outside of the test process itself, lest it gets lost during crash.&lt;/item&gt;
      &lt;item&gt;Mind the time! You want to make each individual CI run as quick as possible, while racking up the total fuzzing time over multiple runs.&lt;/item&gt;
      &lt;item&gt;Another quick and dirty way to check fuzzer coverage is adding &lt;code&gt;unreachable&lt;/code&gt;to various branches and check seeing if it crashes.&lt;/item&gt;
      &lt;item&gt;Fuzzers can test fairly sophisticated invariants (e.g., optimality of the routing), but that might require setting up a particularly favorable environment.&lt;/item&gt;
      &lt;item&gt;Writing a fuzzer is mostly boring mechanical work. However, not only fuzzers do find bugs, some bugs lead to large, fundamental mental shifts, and a deeper understanding of the domain!&lt;/item&gt;
      &lt;item&gt;Don√¢t write fuzzers to find bugs in the code, write fuzzers to find bugs in your understanding of the problem.&lt;/item&gt;
      &lt;item&gt;Positive space fuzzing tries to be realistic, negative space fuzzing tries to be un-realistic.&lt;/item&gt;
      &lt;item&gt;Simulate a real cluster for the positive space, simulate a single peer in a radioactive room for the negative space.&lt;/item&gt;
      &lt;item&gt;It might be hard to get intricate, flake-free assertions from the whole system fuzzer.&lt;/item&gt;
      &lt;item&gt;But whole-system fuzzer is still invaluable as an exploration tool.&lt;/item&gt;
      &lt;item&gt;You can fuzz for performance, at least on the high level protocol level (# messages exchanged).&lt;/item&gt;
      &lt;item&gt;Come to TigerBeetle 1000X to Lisbon (or the city nearest to you): https://tigerbeetle.com/event/1000x&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At√É¬© j√É¬°!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46077964</guid><pubDate>Fri, 28 Nov 2025 12:11:39 +0000</pubDate></item><item><title>The mysterious black fungus from Chernobyl that may eat radiation</title><link>https://www.bbc.com/future/article/20251125-the-mysterious-black-fungus-from-chernobyl-that-appears-to-eat-radiation</link><description>&lt;doc fingerprint="b398517fb824876a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The mysterious black fungus from Chernobyl that may eat radiation&lt;/head&gt;
    &lt;p&gt;Mould found at the site of the Chernobyl nuclear disaster appears to be feeding off the radiation. Could we use it to shield space travellers from cosmic rays?&lt;/p&gt;
    &lt;p&gt;In May 1997, Nelli Zhdanova entered one of the most radioactive places on Earth ‚Äì the abandoned ruins of Chernobyl's exploded nuclear power plant ‚Äì and saw that she wasn't alone.&lt;/p&gt;
    &lt;p&gt;Across the ceiling, walls and inside metal conduits that protect electrical cables, black mould had taken up residence in a place that was once thought to be detrimental to life.&lt;/p&gt;
    &lt;p&gt;In the fields and forest outside, wolves and wild boar had rebounded in the absence of humans. But even today there are hotspots where staggering levels of radiation can be found due to material thrown out from the reactor when it exploded.&lt;/p&gt;
    &lt;p&gt;The mould ‚Äì formed from a number of different fungi ‚Äì seemed to be doing something remarkable. It hadn't just moved in because workers at the plant had left. Instead, Zhdanova had found in previous surveys of soil around Chernobyl that the fungi were actually growing towards the radioactive particles that littered the area. Now, she found that they had reached into the original source of the radiation, the rooms within the exploded reactor building.&lt;/p&gt;
    &lt;p&gt;With each survey taking her close to harmful radiation, Zhdanova's work has also overturned our ideas about how radiation impacts life on Earth. Now her discovery offers hope of cleaning up radioactive sites and even provide ways of protecting astronauts from harmful radiation as they travel into space.&lt;/p&gt;
    &lt;p&gt;Eleven years before Zhdanova's visit, a routine safety test of reactor four at the Chernobyl Nuclear Power Plant had quickly turned into the world's worst nuclear accident. A series of errors both in the design of the reactor and its operation led to a huge explosion in the early hours of 26 April 1986. The result was a single, massive release of radionuclides. Radioactive iodine was a leading cause of death in the first days and weeks, and, later, of cancer.&lt;/p&gt;
    &lt;p&gt;In an attempt to reduce the risk of radiation poisoning and long-term health complications, a 30km (19 mile) exclusion zone ‚Äì also known as the "zone of alienation" ‚Äì was established to keep people at a distance from the worst of the radioactive remains of reactor four.&lt;/p&gt;
    &lt;p&gt;But while humans were kept away, Zhdanova's black mould had slowly colonised the area.&lt;/p&gt;
    &lt;p&gt;Like plants reaching for sunlight, Zhdanova's research indicated that the fungal hyphae of the black mould seemed attracted to ionising radiation. But "radiotropism", as Zhdanova called it, was a paradox: ionising radiation is generally far more powerful than sunlight, a barrage of radioactive particles that shreds through DNA and proteins like bullets puncture flesh. The damage it causes can trigger harmful mutations, destroy cells and kill organisms.&lt;/p&gt;
    &lt;p&gt;Along with the apparently radiotropic fungi, Zhdanova's surveys found 36 other species of ordinary, but distantly related, fungi growing around Chernobyl. Over the next two decades, her pioneering work on the radiotropic fungi she identified would reach far outside of Ukraine. It would add to knowledge of a potentially new foundation of life on Earth ‚Äì one that thrives on radiation rather than sunlight. And it would lead scientists at Nasa to consider surrounding their astronauts in walls of fungi for a durable form of life support.&lt;/p&gt;
    &lt;p&gt;At the centre of this story is a pigment found widely in life on Earth: melanin. This molecule, which can range from black to reddish brown, is what leads to different skin and hair colours in people. But it is also the reason why the various species of mould growing in Chernobyl were black. Their cell walls were packed with melanin.&lt;/p&gt;
    &lt;p&gt;Just as darker skin protects our cells from ultraviolet (UV) radiation, Zhdanova suspected that the melanin of these fungi was acting as a shield against ionising radiation.&lt;/p&gt;
    &lt;p&gt;It wasn't just fungi that were harnessing melanin's protective properties. In the ponds around Chernobyl, frogs with higher concentrations of melanin in their cells, and so darker in colour, were better able to survive and reproduce, slowly turning the local population living there black.&lt;/p&gt;
    &lt;p&gt;In warfare, a shield might protect a soldier from an arrow by deflecting the projectile away from their body. But melanin doesn't work like this. It isn't a hard or smooth surface. The radiation ‚Äì whether UV or radioactive particles ‚Äì is swallowed by its disordered structure, its energy dissipated rather than deflected. Melanin is also an antioxidant, a molecule that can turn the reactive ions that radiation produces in biological matter and return them to a stable state.&lt;/p&gt;
    &lt;p&gt;In 2007, Ekaterina Dadachova, a nuclear scientist at the Albert Einstein College of Medicine in New York, added to Zhdanova's work on Chernobyl's fungi, revealing that their growth wasn't just directional (radiotropic) but actually increased in the presence of radiation. Melanised fungi, just like those inside Chernobyl's reactor, grew 10% faster in the presence of radioactive Caesium compared to the same fungi cultured without radiation, she found. Dadachova and her team also found that the melanised fungi that were irradiated appeared to be using the energy to help drive its metabolism. In other words, they were using it to grow.&lt;/p&gt;
    &lt;p&gt;Zhdanova had suggested that these fungi could be harnessing the energy from radiation, and now Dadachova's research appeared to be building on this. These fungi weren't just growing towards radiation for warmth or some unknown reaction between radiation and its surroundings as Zhdanova had suggested. Dadachova believed the fungi were actively feeding on the radiation's energy. She called this process "radiosynthesis". And melanin was central to the theory.&lt;/p&gt;
    &lt;p&gt;"The energy of ionising radiation is around one million times higher than the energy of white light, which is used in photosynthesis," says Dadachova. "So you need a pretty powerful energy transducer, and this is what we think melanin is capable of doing ‚Äì to transduce [ionising radiation] into usable levels of energy."&lt;/p&gt;
    &lt;p&gt;Radiosynthesis is still just a theory, as it can only be proven if the precise mechanism between melanin and metabolism is discovered. Scientists would need to find the exact receptor ‚Äì or a particular nook in melanin's convoluted structure ‚Äì that is involved in converting radiation into energy for growth.&lt;/p&gt;
    &lt;p&gt;In more recent years, Dadachova and her colleagues have started to identify some of the pathways and proteins that might underlie the fungi's increase in growth with ionising radiation.&lt;/p&gt;
    &lt;p&gt;Not all melanised fungi show a tendency for radiotropism and positive growth in the presence of radiation. A 2006 study from Zhdanova and her colleagues, for example, found that only nine of the 47 species of melanised fungi they collected at Chernobyl grew towards a source of radioactive caesium (caesium-137).&lt;/p&gt;
    &lt;p&gt;Similarly, in 2022, scientists at Sandia National Laboratories in New Mexico found no difference in growth when two species of fungi (one melanised, one not) were exposed to UV radiation and caesium-137.&lt;/p&gt;
    &lt;p&gt;But that same year, the same tendency for fungal growth when exposed to radiation was found again ‚Äì in space.&lt;/p&gt;
    &lt;p&gt;Different from the radioactive decay found at Chernobyl, so-called galactic cosmic radiation is an invisible storm of charged protons, each travelling near the speed of light through the Universe. Originating from exploding stars outside our solar system, it even passes through lead without much trouble. On Earth, our atmosphere largely protects us from it but for astronauts travelling into deep-space it has been called "the greatest hazard" to their health.&lt;/p&gt;
    &lt;p&gt;But even galactic cosmic radiation was no problem for samples of Cladosporium sphaerospermum, the same strain that Zhdanova found growing throughout Chernobyl, according to a study that sent these fungi to the International Space Station in December 2018.&lt;/p&gt;
    &lt;p&gt;"What we showed is that it grows better in space," says Nils Averesch, a biochemist working at the University of Florida and co-author of the study.&lt;/p&gt;
    &lt;p&gt;Compared to control samples back on Earth, the researchers found that fungi that faced the galactic cosmic radiation for 26 days grew an average 1.21 times faster.&lt;/p&gt;
    &lt;p&gt;Even so, Averesch is still unconvinced that this is because C. sphaerospermum was harnessing the radiation in space. The increased levels of growth could also have been the result of zero gravity, he says, another factor that fungi back on Earth didn't experience. "Averesch is now conducting experiments using a random positioning machine that simulates zero gravity here on Earth to parse these two possibilities.&lt;/p&gt;
    &lt;p&gt;But Averesch and his colleagues also tested the protective potential of the melanin in C. sphaerospermum by putting a sensor underneath a sample of the fungi aboard the International Space Station. Compared to samples without fungi, the amount of radiation blocked increased as the fungi grew, and even a smear of mould in a petri dish seemed to be an effective shield.&lt;/p&gt;
    &lt;p&gt;"Considering the comparatively thin layer of biomass, this may indicate a profound ability of C. sphaerospermum to absorb space radiation in the measured spectrum," the researchers wrote.&lt;/p&gt;
    &lt;p&gt;Averesch says it's still possible the apparent radioprotective benefits of fungi are due to components of biological life other than melanin. Water, for example, a molecule with a high number of protons in its structure (eight in oxygen and one in each hydrogen), is one of the best ways to protect against the protons that zoom through space, an astrobiological equivalent of fighting fire with fire.&lt;/p&gt;
    &lt;p&gt;More like this:&lt;/p&gt;
    &lt;p&gt;‚Ä¢ The true toll of the Chernobyl disaster&lt;/p&gt;
    &lt;p&gt;‚Ä¢ How plants reclaimed Chernobyl's poisoned land&lt;/p&gt;
    &lt;p&gt;‚Ä¢ The guards caring for Chernobyl's abandoned dogs&lt;/p&gt;
    &lt;p&gt;Even so, the findings have opened intriguing prospects for solving a problem of space-based living. Both China and the US plan to have a base on the Moon in the coming decades, while Texas-based SpaceX aims to have its first mission to Mars blast off by the end of 2026, and land humans there three to five years later. Any people living on these bases will need to be protected from cosmic radiation. But using water or polyethylene plastic as a radioprotective cocoon for these bases might be far too heavy for liftoff.&lt;/p&gt;
    &lt;p&gt;Metal and glass present a similar problem. Lynn J Rothschild, an astrobiologist at Nasa's Ames Research Centre, has likened transporting these materials into space to build space bases to a turtle carrying its shell everywhere it goes. "[It's] a reliable plan, but with huge energy costs," she said in a 2020 Nasa release.&lt;/p&gt;
    &lt;p&gt;Her research has led to fungal based furniture and walls that could be grown on the Moon or Mars. Not only would such "myco-architecture" reduce the cost of lift-off, but ‚Äì if the findings from Dadachova and Averesch prove correct ‚Äì it could also be used to form a radiation shield, a self-regenerating barrier between the space-faring humans and the storm of galactic cosmic radiation outside.&lt;/p&gt;
    &lt;p&gt;Just as those black moulds colonised an abandoned world at Chernobyl, perhaps they could one day protect our first steps on new worlds elsewhere in the Solar System.&lt;/p&gt;
    &lt;p&gt;* Alex Riley is an award-winning science writer and author of Super Natural: How Life Thrives in Impossible Places. You can follow him on Instagram.&lt;/p&gt;
    &lt;p&gt;--&lt;/p&gt;
    &lt;p&gt;If you liked this story, sign up for The Essential List newsletter ‚Äì a handpicked selection of features, videos and can't-miss news, delivered to your inbox twice a week.&lt;/p&gt;
    &lt;p&gt;For more science, technology, environment and health stories from the BBC, follow us on Facebook and Instagram.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46077992</guid><pubDate>Fri, 28 Nov 2025 12:17:15 +0000</pubDate></item><item><title>A Remarkable Assertion from A16Z</title><link>https://nealstephenson.substack.com/p/a-remarkable-assertion-from-a16z</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46078138</guid><pubDate>Fri, 28 Nov 2025 12:41:32 +0000</pubDate></item></channel></rss>