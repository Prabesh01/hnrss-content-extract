<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 05 Dec 2025 14:42:17 +0000</lastBuildDate><item><title>How elites could shape mass preferences as AI reduces persuasion costs</title><link>https://arxiv.org/abs/2512.04047</link><description>&lt;doc fingerprint="849e200a55f594ac"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Economics &amp;gt; General Economics&lt;/head&gt;&lt;p&gt; [Submitted on 3 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Polarization by Design: How Elites Could Shape Mass Preferences as AI Reduces Persuasion Costs&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:In democracies, major policy decisions typically require some form of majority or consensus, so elites must secure mass support to govern. Historically, elites could shape support only through limited instruments like schooling and mass media; advances in AI-driven persuasion sharply reduce the cost and increase the precision of shaping public opinion, making the distribution of preferences itself an object of deliberate design. We develop a dynamic model in which elites choose how much to reshape the distribution of policy preferences, subject to persuasion costs and a majority rule constraint. With a single elite, any optimal intervention tends to push society toward more polarized opinion profiles - a ``polarization pull'' - and improvements in persuasion technology accelerate this drift. When two opposed elites alternate in power, the same technology also creates incentives to park society in ``semi-lock'' regions where opinions are more cohesive and harder for a rival to overturn, so advances in persuasion can either heighten or dampen polarization depending on the environment. Taken together, cheaper persuasion technologies recast polarization as a strategic instrument of governance rather than a purely emergent social byproduct, with important implications for democratic stability as AI capabilities advance.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;econ.GN&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46145180</guid><pubDate>Thu, 04 Dec 2025 08:38:17 +0000</pubDate></item><item><title>Fighting the age-gated internet</title><link>https://www.wired.com/story/age-verification-is-sweeping-the-us-activists-are-fighting-back/</link><description>&lt;doc fingerprint="3b8f985538b1b5e0"&gt;
  &lt;main&gt;
    &lt;p&gt;Members of Congress considered 19 online safety bills Tuesday that may soon have a major impact on the future of the internet as age-verification laws have spread to half of the US and around the world.&lt;/p&gt;
    &lt;p&gt;In response, digital and human rights organization Fight for the Future is hosting a week of events—across Reddit, LinkedIn, and various livestreams—to raise awareness of how it believes these bills are setting a dangerous precedent by making the internet more exploitative rather than safer. Many of the proposed bills include a clause for ID or age verification, which forces people to upload an ID, allow a face scan, or otherwise authenticate that they are not a minor before viewing adult content. Fight for the Future says the policies will lead to increased censorship and surveillance.&lt;/p&gt;
    &lt;p&gt;Among the 19 bills considered at the hearing conducted by the House Energy and Commerce Committee was the Kids Online Safety Act (KOSA), which passed with sweeping bipartisan approval in the Senate last year, and the Reducing Exploitative Social Media Exposure for Teens Act, which would ban tech companies from allowing minors under the age of 16 on their platforms. In addition to age verification, the bills raised concerns over issues of parental controls, consumer research of minors, AI, and data privacy.&lt;/p&gt;
    &lt;p&gt;“We’re seeing this huge wave toward ID checks being the norm in tech policy, and it felt like we needed to capture the already activated communities who are not feeling heard in Congress,” says Sarah Philips, a campaigner with Fight for the Future. “If you look on YouTube, if you see people making content about KOSA or responding to a lot of this legislation, it’s very unpopular with people. But it’s viewed on the Hill as very common-sense.”&lt;/p&gt;
    &lt;p&gt;Missouri’s age-gate law took effect earlier this week, meaning 25 US states have passed a form of age verification. The process usually involves third-party services, which can be especially prone to data breaches. This year, the UK also passed a mandate for age verification—the Online Safety Act—and Australia’s teen social media ban, which requires social media companies to deactivate the accounts of users under the age of 16, goes into effect on December 10. Instagram, YouTube, Snap, and TikTok are complying with the historic ban.&lt;/p&gt;
    &lt;p&gt;Philips believes the laws are a direct threat to democratic freedom. “These are censorship laws,” she says. “In the South, where I live, these same proposals mimic a lot of the arguments that you see behind book bans and behind laws that criminalize gender-affirming health care or abortion information.”&lt;/p&gt;
    &lt;p&gt;In March, over 90 human rights advocacy groups signed a coalition letter opposing online ID-check mandates. “The internet is not improved by treating its users like criminal suspects and our lives as opportunities for corporate profit,” David Swanson, campaign coordinator at RootsAction.org, wrote in the letter. “Legislators defunding education to invest in wars, police, prisons, borders, and constant surveillance should think hard before claiming to be acting on behalf of children.”&lt;/p&gt;
    &lt;p&gt;Though Tuesday’s hearing did not advance any legislation, it included testimonies from Joel Thayer, president of the Digital Progress Institute, and Kate Ruane, director of the Free Expression Project at the Center for Democracy and Technology. “The government and social media platforms should not be—indeed, with respect to the government, cannot be—the sole arbiters of the content children can see and services that they can access online,” Ruane said during her testimony.&lt;/p&gt;
    &lt;p&gt;The package of bills is indicative of how Congress has failed to deliver real solutions, Philips says. “We have repeatedly asked them to focus on comprehensive privacy legislation, on antitrust issues, and on things that actually protect us from the surveillance capitalist business model of big tech companies. Congress says they’re holding big tech accountable, but most of the options on the table just mandate verification.” According to The Verge, a revamped version of KOSA removes tech companies’ liability in mitigating potential harms caused by their platforms.&lt;/p&gt;
    &lt;p&gt;In an op-ed for Teen Vogue published in October, Fight for the Future director Evan Greer and campaigner Janus Rose criticized Democratic lawmakers who support KOSA, including the bill’s cowriter, Senator Richard Blumenthal of Connecticut. “KOSA takes the same logic of the bans on drag shows and LGBTQ+ books and applies it to the internet, allowing censorship of a broad range of information in the name of protecting kids from real online harm,” Greer noted.&lt;/p&gt;
    &lt;p&gt;But since KOSA and the Children and Teens’ Online Privacy Protection Act failed to gain approval last year, “it’ll be interesting to see what actually floats to the top right now,” Philips says, concerned that some of the bills could be attached to the National Defense Authorization Act or have the Trump administration’s 10-year moratorium on state AI regulations attached to them, “which is a disaster tornado of tech policies.”&lt;/p&gt;
    &lt;p&gt;Philips tells me she isn’t disheartened by the work, because she wants people to understand what’s really at stake in the fight ahead.&lt;/p&gt;
    &lt;p&gt;“The thing that people misunderstand most about age verification is that it actually applies to all of us,” she says. “A lot of the people pushing for age verification solely focus on kids, because that’s the discussion happening in Congress or on the Hill. But in actuality, if we age-gate the internet and implement mandates, that means that you have to prove that you’re not a child—whether you’re 18 or 50. Everyone will have to interact with this.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46147493</guid><pubDate>Thu, 04 Dec 2025 13:34:27 +0000</pubDate></item><item><title>Transparent leadership beats servant leadership</title><link>https://entropicthoughts.com/transparent-leadership-beats-servant-leadership</link><description>&lt;doc fingerprint="bce3d0111682d491"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Transparent Leadership Beats Servant Leadership&lt;/head&gt;
    &lt;p&gt;tl:dr: Parenting and leadership is similar. Teach a man to fish, etc.&lt;/p&gt;
    &lt;p&gt;I spent a couple of years managing a team, and I entered that role – like many – without knowing anything about how to do it. I tried to figure out how to be a good manager, and doing so I ended up reading a lot about servant leadership. It never quite sat right with me, though. Servant leadership seems to me a lot like curling parenting: the leader/parent anticipate problems and sweep the way for their direct reports/children.&lt;/p&gt;
    &lt;p&gt;To be clear, this probably feels very good (initially, anyway) for the direct reports/children. But the servant leader/curling parent quickly becomes an overworked single point of failure, and once they leave there is nobody else who knows how to handle the obstacles the leader moved out of the way for everyone. In the worst cases, they leave behind a group of people who have been completely isolated from the rest of the organisation, and has no idea what their purpose is and how to fit in with the rest of the world.&lt;/p&gt;
    &lt;p&gt;I would like to invent my own buzzword: transparent leadership. In my book, a good leader&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;coaches people,&lt;/item&gt;
      &lt;item&gt;connects people,&lt;/item&gt;
      &lt;item&gt;teaches people methodical problem solving,&lt;/item&gt;
      &lt;item&gt;explains values and principles embraced by the organisation to aid them in making aligned decisions on their own,&lt;/item&gt;
      &lt;item&gt;creates direct links between supply and demand (instead of deliberately making themselves a middle man),&lt;/item&gt;
      &lt;item&gt;allows their direct reports career growth by gradually taking over leadership responsibilities,&lt;/item&gt;
      &lt;item&gt;continuously trains their replacement, and&lt;/item&gt;
      &lt;item&gt;generally makes themselves redundant.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The middle manager that doesn’t perform any useful work is a fun stereotype, but I also think it’s a good target to aim for. The difference lies in what to do once one has rendered oneself redundant. A common response is to invent new work, ask for status reports, and add bureaucracy.&lt;/p&gt;
    &lt;p&gt;A better response is to go back to working on technical problems. This keeps the manager’s skills fresh and gets them more respect from their reports. The manager should turn into a high-powered spare worker, rather than a paper-shuffler.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46147540</guid><pubDate>Thu, 04 Dec 2025 13:40:00 +0000</pubDate></item><item><title>Multivox: Volumetric Display</title><link>https://github.com/AncientJames/multivox</link><description>&lt;doc fingerprint="87cd881f7921b9c9"&gt;
  &lt;main&gt;
    &lt;p&gt;This is the code I currently use to drive my volumetric displays.&lt;/p&gt;
    &lt;p&gt;It supports two closely related devices which are configured in the &lt;code&gt;src/driver/gadgets&lt;/code&gt; directory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rotovox is a 400mm Orb featuring two 128x64 panels arranged vertically side by side.&lt;/item&gt;
      &lt;item&gt;Vortex is a 300mm Orb featuring two 128x64 panels arranged horizontally, back to back.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rotovox has a higher vertical resolution and better horizontal density; Vortex is brighter and has a higher refresh rate.&lt;/p&gt;
    &lt;p&gt;The 3D printable parts for Vortex are available here.&lt;/p&gt;
    &lt;p&gt;This code was originally written for a single display, and the device specific code was later somewhat abstracted out to support a second similar gadget. There are assumptions about the hardware that are pretty well baked in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It consists of two HUB75 LED panels spinning around a vertical axis.&lt;/item&gt;
      &lt;item&gt;The panels use either ABCDE addressing or ABC shift register addressing.&lt;/item&gt;
      &lt;item&gt;It uses a single GPIO (a photodiode or similar) to sync to rotation - high for 180°, low for 180°.&lt;/item&gt;
      &lt;item&gt;It's running on a Raspberry Pi 4.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The GPIO mappings and panel layout are defined in &lt;code&gt;src/driver/gadgets/gadget_&amp;lt;name&amp;gt;.h&lt;/code&gt;. GPIO is via memory mapped
access - if you're using a different model of Pi you'll need to change &lt;code&gt;BCM_BASE&lt;/code&gt; in the GPIO code. I haven't tested
this, and you should probably assume it doesn't work.&lt;/p&gt;
    &lt;p&gt;Input is via a bluetooth gamepad - I've been using an Xbox controller, and the input system is based on the default mapping for that.&lt;/p&gt;
    &lt;p&gt;Audio out is also via bluetooth. I haven't had success with the higher quality codecs, but the headset protocol works.&lt;/p&gt;
    &lt;p&gt;There are two parts to this code - the driver, which creates a voxel buffer in shared memory and scans its contents out in sync with rotation, and the client code which generates content and writes it into the voxel buffer. Both driver and client code are designed to run on the same device, a Raspberry Pi embedded in the hardware and spinning at several hundred RPM. There is a demo included in the Python directory which streams point clouds from a PC over wifi to the device, but fundamentally it's designed as a self contained gadget, like an alternate timeline Vectrex. A bluetooth gamepad is used to control the demos.&lt;/p&gt;
    &lt;code&gt;├── src
│   ├── driver
│   │   ├── gadgets         -- the different volumetric display configurations
│   │   │   └──             
│   │   └── vortex.c        -- driver code - creates a voxel buffer in shared memory,
│   │                          and handles scanning it out to the led panels in sync with
│   │                          the rotation
│   ├── simulator
│   │   └── virtex.c        -- software simulator - presents the same voxel buffer as
│   │                          the driver would, but renders the contents into an X11 window
│   │
│   ├── multivox            -- front end / launcher for the various volumetric toys
│   │   └──
│   ├── platform            -- common client code
│   │   └──
│   └── toys                -- a collection of volumetric demos using the shared voxel buffer
│       ├── eighty          -- multiplayer light cycles
│       ├── fireworks.c     -- cheesy first demo
│       ├── flight.c        -- some kind of 70s scifi thing
│       ├── tesseract.c     -- a 4D cubube
│       ├── viewer.c        -- viewer for .obj and .png files
│       └── zander          -- lander/zarch/virus-esque
├── python  
│   ├── calibration.py      -
│   ├── grid.py             -- some pattern generators, useful when calibrating the device
│   ├── colourwheel.py      -
│   ├── obj2c.py            -- tool for embedding .obj models in a header file
│   ├── pointvision.py      -- receive point clouds streamed from vortexstream.py
│   └── vortexstream.py     -- stream point clouds to pointvision.py
└── README.md               -- you are here
&lt;/code&gt;
    &lt;p&gt;On the Raspberry Pi, clone the repository:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/AncientJames/multivox.git
&lt;/code&gt;
    &lt;p&gt;Configure the project for your hardware:&lt;/p&gt;
    &lt;code&gt;cd multivox
mkdir build
cd build
cmake -DMULTIVOX_GADGET=vortex ..
cmake --build .
&lt;/code&gt;
    &lt;p&gt;First, the driver has to be running:&lt;/p&gt;
    &lt;code&gt;sudo ./vortex
&lt;/code&gt;
    &lt;p&gt;When invoked from the command line it periodically outputs profiling information (frame rate, rotation rate), and accepts keyboard input for various diagnostics:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;esc&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;b&lt;/cell&gt;
        &lt;cell&gt;Bit depth - cycles through 1, 2 or 3 bits per channel. Higher bit depths result in lower refresh rates&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;u&lt;/cell&gt;
        &lt;cell&gt;Uniformity - cycles through different strategies for trading off brightness against uniformity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;t&lt;/cell&gt;
        &lt;cell&gt;Trails - adjusts how far back to accumulate skipped voxels when the rotation rate is too high for the refresh rate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;l&lt;/cell&gt;
        &lt;cell&gt;Lock - whether to adjust the rotation sync to keep it facing one way&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;d D&lt;/cell&gt;
        &lt;cell&gt;Drift - rotisserie mode. Introduces some explicit drift to the rotation sync&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;Panel - selectively disable the panels&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;xyz&lt;/cell&gt;
        &lt;cell&gt;Axis - When the display isn't spinning, it shows an othographic view. This lets you choose the axis&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;While that's running, try one of the toys:&lt;/p&gt;
    &lt;code&gt;./tesseract
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;viewer&lt;/code&gt; takes a list of .obj and .png files as arguments. You can scale, rotate and so on using the gamepad, and it
also accepts keyboard input when run remotely from the command line.&lt;/p&gt;
    &lt;code&gt;./viewer ~/Multivox/models/*.obj
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Control&lt;/cell&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;esc&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LB/RB&lt;/cell&gt;
        &lt;cell&gt;[ / ]&lt;/cell&gt;
        &lt;cell&gt;Cycle through models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Walkthrough / Orbit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;X&lt;/cell&gt;
        &lt;cell&gt;Zoom to fit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Y&lt;/cell&gt;
        &lt;cell&gt;Toggle wireframe&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you don't have a physical volumetric display, there's a simulator, &lt;code&gt;virtex&lt;/code&gt;, which you can run in place of &lt;code&gt;vortex&lt;/code&gt;. It exposes the same voxel buffer in shared memory, but renders the contents using OpenGL in an X11 window.&lt;/p&gt;
    &lt;p&gt;Run without command line arguments it creates a display compatible with the currently configured gadget, but there are some options to let you experiment with different geometries:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-s X&lt;/cell&gt;
        &lt;cell&gt;slice count - the number of vertical slices per revolution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-o X X&lt;/cell&gt;
        &lt;cell&gt;offsets - distance the front and back screens are offset from the axis, as a fraction of screen radius&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-b X&lt;/cell&gt;
        &lt;cell&gt;bits per channel (1 - 3)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;-w X Y&lt;/cell&gt;
        &lt;cell&gt;panel resolution&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;-g X&lt;/cell&gt;
        &lt;cell&gt;scan geometry - radial or linear. Linear looks better, but it's a lot harder to build.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;An idealised device with linear scanning and 3 bits per channel can be invoked like this:&lt;/p&gt;
    &lt;code&gt;./virtex -g l -s 128 -w 1280 1280 -b 3
&lt;/code&gt;
    &lt;p&gt;The simulator is fill rate intensive; if you're running it on a Raspberry Pi you'll probably want to reduce the slice count.&lt;/p&gt;
    &lt;p&gt;If you want it to start up automatically on boot, you can install &lt;code&gt;vortex&lt;/code&gt; as a service, and set &lt;code&gt;multivox&lt;/code&gt; to run on startup.&lt;/p&gt;
    &lt;p&gt;First install everything to its default location &lt;code&gt;~/Multivox&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;make install&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This will build the executable files and copy them into the destination directory, as well as creating &lt;code&gt;.mct&lt;/code&gt; files in &lt;code&gt;~/Multivox/carts&lt;/code&gt; for the built in toys.&lt;/p&gt;
    &lt;p&gt;Create the driver service:&lt;/p&gt;
    &lt;code&gt;sudo nano /usr/lib/systemd/system/vortex.service
&lt;/code&gt;
    &lt;p&gt;and fill in the following information:&lt;/p&gt;
    &lt;code&gt;[Unit]
Description=Vortex Display Driver
After=multi-user.target

[Service]
ExecStart=/home/pi/Multivox/bin/vortex

[Install]
WantedBy=multi-user.target
&lt;/code&gt;
    &lt;p&gt;Then start it up:&lt;/p&gt;
    &lt;code&gt;sudo systemctl daemon-reload
sudo systemctl enable vortex.service
&lt;/code&gt;
    &lt;p&gt;The driver assigns itself to core 3 - you can add &lt;code&gt;isolcpus=3&lt;/code&gt; to the end of &lt;code&gt;/boot/cmdline.txt&lt;/code&gt; to ensure it's the only thing running on that core.&lt;/p&gt;
    &lt;p&gt;You'll also want the launcher to start up on boot:&lt;/p&gt;
    &lt;code&gt;crontab -e
&lt;/code&gt;
    &lt;p&gt;And add the line:&lt;/p&gt;
    &lt;code&gt;@reboot /home/pi/Multivox/bin/multivox
&lt;/code&gt;
    &lt;p&gt;If everything goes smoothly, when you turn on the device it will boot up into &lt;code&gt;Multivox&lt;/code&gt;. This is a fantasy console which
acts as a launcher for all the games and demos you run on the hardware. The bundled toys are automatically installed in
the &lt;code&gt;~/Multivox/carts/&lt;/code&gt; directory as &lt;code&gt;.mct&lt;/code&gt; files, and external apps can be launched by adding a &lt;code&gt;.mct&lt;/code&gt; file containing
its command, path and arguments.&lt;/p&gt;
    &lt;p&gt;Each &lt;code&gt;.mct&lt;/code&gt; file appears as a cartridge in the Multivox front end. They should each have a label on the side; at the moment
all you can do to distinguish between them is change their colour in the &lt;code&gt;.mct&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When you exit an app back to the launcher, it saves a snapshot of the voxel volume, and this gives a preview of what you'll see when you launch a cart. This means there are two competing representations of the same information, and any future work on the front end will probably start with overhauling the entire approach.&lt;/p&gt;
    &lt;p&gt;Some basic UI for controls such as changing bit depth, rebooting and so on would also be a boon.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Control&lt;/cell&gt;
        &lt;cell role="head"&gt;Effect&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;LB/RB&lt;/cell&gt;
        &lt;cell&gt;Cycle through carts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Launch cart&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;⧉&lt;/cell&gt;
        &lt;cell&gt;Exit / resume running cart&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;△ ▽&lt;/cell&gt;
        &lt;cell&gt;Change bit depth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;☰ x5&lt;/cell&gt;
        &lt;cell&gt;Power off&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46149813</guid><pubDate>Thu, 04 Dec 2025 16:58:35 +0000</pubDate></item><item><title>I have been writing a niche history blog for 15 years</title><link>https://resobscura.substack.com/p/why-i-have-been-writing-a-niche-history</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46151299</guid><pubDate>Thu, 04 Dec 2025 18:49:20 +0000</pubDate></item><item><title>CUDA-l2: Surpassing cuBLAS performance for matrix multiplication through RL</title><link>https://github.com/deepreinforce-ai/CUDA-L2</link><description>&lt;doc fingerprint="df7c7dfdcca34d95"&gt;
  &lt;main&gt;
    &lt;p&gt;CUDA-L2 is a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used torch.matmul to state-of-the-art NVIDIA closed-source libraries (cuBLAS, cuBLASLt-heuristic, cuBLASLt-AutoTuning). Paper&lt;/p&gt;
    &lt;p&gt;Speedup of CUDA-L2 over torch.matmul, cuBLAS, cuBLASLt-heuristic, and cuBLASLt-AutoTuning across 1000 (M,N,K) configurations on A100.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;[Dec 2, 2025] Released A100 optimized HGEMM kernels across 1,000 configurations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Release HGEMM with 32-bit accumulator (SM80_16x8x16_F16F16F16F32 and F32F16F16F32 officially) for A100. Current version only support 16-bit accumulator (SM80_16x8x16_F16F16F16F16).&lt;/item&gt;
      &lt;item&gt;Support denser matrix configurations (more configurations).&lt;/item&gt;
      &lt;item&gt;Extend to more GPUs (Ada Lovelace, Hopper, Blackwell).&lt;/item&gt;
      &lt;item&gt;Easy deployment for open-source LLMs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Q: Do A100 kernels apply to other machines like RTX 3090 or H100?&lt;/p&gt;
    &lt;p&gt;A: Ideally, kernels trained on A100 should only be used on A100 if you are targeting speedup. They might have speedup on other machines, but it's not guaranteed. We will progressively release kernels trained on different machines.&lt;/p&gt;
    &lt;p&gt;Q: What if I need matrix dimensions (M, N, K) not found in your configurations?&lt;/p&gt;
    &lt;p&gt;A: 1. You can find the nearest neighbor configuration (larger than yours) and pad with zeros. 2. Feel free to post your dimensions on GitHub issues. We are happy to release kernels for your configuration.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python: Ensure you have a working Python environment.&lt;/item&gt;
      &lt;item&gt;PyTorch: This project requires PyTorch version 2.6.0 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project depends on NVIDIA CUTLASS. You must clone specific tag &lt;code&gt;v4.2.1&lt;/code&gt; into a directory named &lt;code&gt;cutlass&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;git clone -b v4.2.1 https://github.com/NVIDIA/cutlass.git cutlass&lt;/code&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Warning: Please ensure you download the correct CUTLASS version (&lt;code&gt;v4.2.1&lt;/code&gt;) and set the&lt;code&gt;CUTLASS_DIR&lt;/code&gt;environment variable correctly. Incorrect CUTLASS setup may cause the project to fail silently or produce no results.&lt;/quote&gt;
    &lt;p&gt;Before building or running the project, you must configure the following environment variables:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CUTLASS_DIR&lt;/code&gt;: Points to the directory where you cloned CUTLASS.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TORCH_CUDA_ARCH_LIST&lt;/code&gt;: Specifies the target GPU architecture (e.g., "8.0" for NVIDIA Ampere / A100 / RTX 30 series).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run the following commands:&lt;/p&gt;
    &lt;code&gt;export CUTLASS_DIR=/path/to/your/cutlass
export TORCH_CUDA_ARCH_LIST="8.0"&lt;/code&gt;
    &lt;p&gt;To run the evaluation, use the &lt;code&gt;eval_one_file.sh&lt;/code&gt; script. Below is an example command for offline mode:&lt;/p&gt;
    &lt;code&gt;./eval_one_file.sh --mnk 64_4096_64 --warmup_seconds 5 --benchmark_seconds 10 --base_dir ./results --gpu_device_id 7 --mode offline&lt;/code&gt;
    &lt;p&gt;For server mode, you need to specify &lt;code&gt;--target_qps&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;./eval_one_file.sh --mnk 64_4096_64 --warmup_seconds 5 --benchmark_seconds 10 --base_dir ./results --gpu_device_id 7 --mode server --target_qps 100&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Argument&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--mnk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Specifies the problem size (e.g., &lt;code&gt;64_4096_64&lt;/code&gt;).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--warmup_seconds&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Duration of warmup in seconds before timing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--benchmark_seconds&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Duration of benchmarking in seconds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--base_dir&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Directory to save the compile and output results.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--gpu_device_id&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;The ID of the GPU to use (e.g., &lt;code&gt;7&lt;/code&gt;).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--mode&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Execution mode. Options are:&lt;p&gt;•&lt;/p&gt;&lt;code&gt;offline&lt;/code&gt;: Runs the evaluation in offline/batch processing mode.&lt;p&gt;•&lt;/p&gt;&lt;code&gt;server&lt;/code&gt;: Runs the evaluation in server mode (simulating request-based scenarios).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--target_qps&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Target Queries Per Second (QPS) for server mode. Required if mode is &lt;code&gt;server&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you have any questions, please open a GitHub issue or reach out to us at jiwei_li@deep-reinforce.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46153058</guid><pubDate>Thu, 04 Dec 2025 21:04:29 +0000</pubDate></item><item><title>StardustOS: Library operating system for building light-weight Unikernels</title><link>https://github.com/StardustOS</link><description>&lt;doc fingerprint="d09a987b17a69106"&gt;
  &lt;main&gt;
    &lt;p&gt;Stardust is a unikernel operating system designed to run Cloud applications in a protected, single-address space environment. It delegates the management of physical resources to an underlying hypervisor which is treated as a trusted platform. Stardust has a small code base that can be maintained easily, and relies on static linking to combine a minimal kernel with a single application, along with the libraries and associated programming language run-time required for the execution of the application. Due to static linking, an executable binary of Stardust is packaged within an immutable single-purpose virtual machine image. Stardust supports multiple cores, preemptive threads, and basic block and networking drivers, and provides a collection of standard POSIX-compatible libraries.&lt;/p&gt;
    &lt;p&gt;Stardust is being used in supporting the teaching and research activities at the University of St Andrews.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stardust provides the unikernel implementation in C.&lt;/item&gt;
      &lt;item&gt;Stardust-oxide is a re-implementation of the unikernel in Rust.&lt;/item&gt;
      &lt;item&gt;Duster provides a small debugger for para-virtualised Unikernels written in C that run on the Xen hypervisor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jaradat, W., Dearle A. and Lewis, J. Unikernel Support for Lambda Functions. In the Fifth Annual UK System Research Challenges Workshop, United Kingdom, 2020. Accepted Talk&lt;/item&gt;
      &lt;item&gt;Ahmad, K., Dearle A., Lewis, J. and Jaradat, W. Debugging Unikernel Operating Systems (Slides). In the Fifth Annual UK System Research Challenges Workshop, United Kingdom, 2020. Accepted Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W. On Engineering Unikernels, Systems Seminars Series, University of St Andrews, United Kingdom, 2018. Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W., Dearle, A. and Lewis, J. Unikernel support for the deployment of light-weight, self-contained, and latency avoiding services. In the Third Annual UK System Research Challenges Workshop, United Kingdom, 2018. Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W. Towards Unikernel Support for Distributed Microservices. Adobe Tech Summit, San Francisco, United States of America, 2019. Talk&lt;/item&gt;
      &lt;item&gt;Jaradat, W., Dearle, A. and Lewis, J. The Case for Unikernels. In the Fourth Annual UK System Research Challenges Workshop, United Kingdom, 2019. Lightning Talk&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jaradat, W., Dearle, A. and Lewis, J. Unikernel support for the deployment of light-weight, self-contained, and latency avoiding services. In the Third Annual UK System Research Challenges Workshop, United Kingdom, 2018.&lt;/item&gt;
      &lt;item&gt;McKeogh, F., Stardust Oxide, Dissertation, University of St Andrews, United Kingdom.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46154344</guid><pubDate>Thu, 04 Dec 2025 22:56:08 +0000</pubDate></item><item><title>We gave 5 LLMs $100K to trade stocks for 8 months</title><link>https://www.aitradearena.com/research/we-ran-llms-for-8-months</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46154491</guid><pubDate>Thu, 04 Dec 2025 23:08:25 +0000</pubDate></item><item><title>Trick users and bypass warnings – Modern SVG Clickjacking attacks</title><link>https://lyra.horse/blog/2025/12/svg-clickjacking/</link><description>&lt;doc fingerprint="120802a0bcc92fb"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;SVG Filters - Clickjacking 2.0&lt;/head&gt;&lt;p&gt;Clickjacking is a classic attack that consists of covering up an iframe of some other website in an attempt to trick the user into unintentionally interacting with it. It works great if you need to trick someone into pressing a button or two, but for anything more complicated it’s kind of unrealistic.&lt;/p&gt;&lt;p&gt;I’ve discovered a new technique that turns classic clickjacking on its head and enables the creation of complex interactive clickjacking attacks, as well as multiple forms of data exfiltration.&lt;/p&gt;&lt;p&gt;I call this technique “SVG clickjacking”.&lt;/p&gt;&lt;head rend="h2"&gt;Liquid SVGs&lt;/head&gt;&lt;p&gt;The day Apple announced its new Liquid Glass redesign was pretty chaotic. You couldn’t go on social media without every other post being about the new design, whether it was critique over how inaccessible it seemed, or awe at how realistic the refraction effects were.&lt;/p&gt;&lt;p&gt;Drowning in the flurry of posts, a thought came to mind - how hard would it be to re-create this effect? Could I do this, on the web, without resorting to canvas and shaders? I got to work, and about an hour later I had a pretty accurate CSS/SVG recreation of the effect1.&lt;/p&gt;&lt;p&gt;EMERGENCY!&lt;/p&gt;&lt;p&gt;Girls Rituals&lt;/p&gt;&lt;p&gt;This Won't Be The Last Time&lt;/p&gt;&lt;p&gt;acloudyskye&lt;/p&gt;&lt;p&gt;SOUND BANDIT FUCKING LIVES&lt;/p&gt;&lt;p&gt;Sound Bandit&lt;/p&gt;&lt;p&gt;Love &amp;amp; Ponystep&lt;/p&gt;&lt;p&gt;Vylet Pony&lt;/p&gt;&lt;p&gt;I Love My Computer&lt;/p&gt;&lt;p&gt;Ninajirachi&lt;/p&gt;&lt;p&gt;You can drag around the effect with the bottom-right circle control thing in the demo above (chrome/firefox desktop, chrome mobile).&lt;/p&gt;&lt;p&gt;My little tech demo made quite a splash online, and even resulted in a news article with what is probably the wildest quote about me to date: “Samsung and others have nothing on her”.&lt;/p&gt;&lt;p&gt;A few days passed, and another thought came to mind - would this SVG effect work on top of an iframe?&lt;/p&gt;&lt;p&gt;Like, surely not? The way the effect “refracts light”2 is way too complex to work on a cross-origin document.&lt;/p&gt;&lt;p&gt;But, to my surprise, it did.&lt;/p&gt;&lt;p&gt;The reason this was so interesting to me is that my liquid glass effect uses the &lt;code&gt;feColorMatrix&lt;/code&gt; and &lt;code&gt;feDisplacementMap&lt;/code&gt; SVG filters - changing the colors of pixels, and moving them, respectively. And I could do that on a cross-origin document?&lt;/p&gt;&lt;p&gt;This got me wondering - do any of the other filters work on iframes, and could we turn that into an attack somehow? It turns out that it’s all of them, and yes!&lt;/p&gt;&lt;head rend="h2"&gt;Building blocks&lt;/head&gt;&lt;p&gt;I got to work, going through every &amp;lt;fe*&amp;gt; SVG element and figuring out which ones can be combined to build our own attack primitives.&lt;/p&gt;&lt;p&gt;These filter elements take in one or more input images, apply operations to them, and output a new image. You can chain a bunch of them together within a single SVG filter, and refer to the output of any of the previous filter elements in the chain.&lt;/p&gt;&lt;p&gt;Let’s take a look at some of the more useful base elements we can play with:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&amp;lt;feImage&amp;gt; - load an image file;&lt;/item&gt;&lt;item&gt;&amp;lt;feFlood&amp;gt; - draw a rectangle;&lt;/item&gt;&lt;item&gt;&amp;lt;feOffset&amp;gt; - move stuff around;&lt;/item&gt;&lt;item&gt;&amp;lt;feDisplacementMap&amp;gt; - move pixels according to a map;&lt;/item&gt;&lt;item&gt;&amp;lt;feGaussianBlur&amp;gt; - blur stuff;&lt;/item&gt;&lt;item&gt;&amp;lt;feTile&amp;gt; - tiling and cropping utility;&lt;/item&gt;&lt;item&gt;&amp;lt;feMorphology&amp;gt; - expand/grow light or dark areas;&lt;/item&gt;&lt;item&gt;&amp;lt;feBlend&amp;gt; - blend two inputs according to the mode;&lt;/item&gt;&lt;item&gt;&amp;lt;feComposite&amp;gt; - compositing utilities, can be used to apply an alpha matte, or do various arithmetics on one or two inputs;&lt;/item&gt;&lt;item&gt;&amp;lt;feColorMatrix&amp;gt; - apply a color matrix, this allows moving colors between channels and converting between alpha and luma mattes;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;That’s quite a selection of utilities!&lt;/p&gt;&lt;p&gt;If you’re a demoscener3 you’re probably feeling right at home. These are the fundamental building blocks for many kinds of computer graphics, and they can be combined into many useful primitives of our own. So let’s see some examples.&lt;/p&gt;&lt;head rend="h3"&gt;Fake captcha&lt;/head&gt;&lt;p&gt;I’ll start off with an example of basic data exfiltration. Suppose you’re targeting an iframe that contains some sort of sensitive code. You could ask the user to retype it by itself, but that’d probably seem suspicious.&lt;/p&gt;&lt;p&gt;What we can do instead is make use of &lt;code&gt;feDisplacementMap&lt;/code&gt; to make the text seem like a captcha! This way, the user is far more likely to retype the code.&lt;/p&gt;&lt;p&gt;Here is your secret code:&lt;/p&gt;&lt;p&gt;6c79 7261 706f 6e79&lt;/p&gt;&lt;p&gt;Don't share it with anyone!&lt;/p&gt;&lt;p&gt;Here is your secret code:&lt;/p&gt;&lt;p&gt;6c79 7261 706f 6e79&lt;/p&gt;&lt;p&gt;Don't share it with anyone!&lt;/p&gt;&lt;p&gt;(tapclick to edit if you're not a girl)&lt;/p&gt;&lt;p&gt;Note: Only the part inside the &lt;code&gt;&amp;lt;filter&amp;gt;&lt;/code&gt; block is relevant, the rest is just an example of using filters.&lt;/p&gt;&lt;p&gt;Add to this some color effects and random lines, and you’ve got a pretty convincing cap-tcha!&lt;/p&gt;&lt;p&gt;Out of all the attack primitives I’ll be sharing, this one is probably the least useful as sites rarely allow you to frame pages giving out magic secret codes. I wanted to show it though, as it’s a pretty simple introduction to the attack technique.&lt;/p&gt;&lt;p&gt;Still, it could come in handy because often times you’re allowed to frame read-only API endpoints, so maybe there’s an attack there to discover.&lt;/p&gt;&lt;head rend="h3"&gt;Grey text hiding&lt;/head&gt;&lt;p&gt;The next example is for situations where you want to trick someone into, for example, interacting with a text input. Oftentimes the inputs have stuff like grey placeholder text in them, so showing the input box by itself won’t cut it.&lt;/p&gt;&lt;p&gt;Let’s take a look at our example target (try typing in the box).&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;In this example we want to trick the user into setting an attacker-known password, so we want them to be able to see the text they’re entering, but not the grey placeholder text, nor the red “too short” text.&lt;/p&gt;&lt;p&gt;Let’s start off by using &lt;code&gt;feComposite&lt;/code&gt; with arithmetics to make the grey text disappear. The &lt;code&gt;arithmetic&lt;/code&gt; operation takes in two images, &lt;code&gt;i1&lt;/code&gt; (&lt;code&gt;in=...&lt;/code&gt;) and &lt;code&gt;i2&lt;/code&gt; (&lt;code&gt;in2=...&lt;/code&gt;), and lets us do per-pixel maths with &lt;code&gt;k1&lt;/code&gt;, &lt;code&gt;k2&lt;/code&gt;, &lt;code&gt;k3&lt;/code&gt;, &lt;code&gt;k4&lt;/code&gt; as the arguments according to this formula: 4.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Tip! You can leave out the in/in2 parameters if you just want it to be the previous output.&lt;/p&gt;&lt;p&gt;It’s getting there - by multiplying the brightness of the input we’ve made the grey text disappear, but now the black text looks a little suspicious and hard to read, especially on 1x scaling displays.&lt;/p&gt;&lt;p&gt;We could play around with the arguments to find the perfect balance between hiding the grey text and showing the black one, but ideally we’d still have the black text look the way usually does, just without any grey text. Is that possible?&lt;/p&gt;&lt;p&gt;So here’s where a really cool technique comes into play - masking. We’re going to create a matte to “cut out” the black text and cover up everything else. It’s going to take us quite a few steps to get to the desired result, so lets go through it bit-by-bit.&lt;/p&gt;&lt;p&gt;We start off by cropping the result of our black text filter with &lt;code&gt;feTile&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Note: Safari seems to be having some trouble with &lt;code&gt;feTile&lt;/code&gt;, so if the examples flicker or look blank, read this post in a browser such as Firefox or Chrome. If you're writing an attack for Safari, you can also achieve cropping by making a luma matte with &lt;code&gt;feFlood&lt;/code&gt; and then applying it.&lt;/p&gt;&lt;p&gt;Then we use &lt;code&gt;feMorphology&lt;/code&gt; to increase the thickness of the text.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Now we have to increase the contrast of the mask. I’m going to do it by first using &lt;code&gt;feFlood&lt;/code&gt; to create a solid white image, which we can then &lt;code&gt;feBlend&lt;/code&gt; with &lt;code&gt;difference&lt;/code&gt; to invert our mask. And then we can use &lt;code&gt;feComposite&lt;/code&gt; to multiply5 the mask for better contrast.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;We have a luma matte now! All that’s left is to convert it into an alpha matte with &lt;code&gt;feColorMatrix&lt;/code&gt;, apply it to the source image with &lt;code&gt;feComposite&lt;/code&gt;, and make the background white with &lt;code&gt;feBlend&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;Looks pretty good, doesn’t it! If you empty out the box (try it!) you might notice some artifacts that give away what we’ve done, but apart from that it’s a pretty good way to sort of sculpt and form various inputs around a bit for an attack.&lt;/p&gt;&lt;p&gt;There are all sorts of other effects you can add to make the input seem just right. Let’s combine everything together into a complete example of an attack.&lt;/p&gt;&lt;p&gt;Set a new password&lt;/p&gt;&lt;p&gt;You can see how the textbox is entirely recontextualized now to fit a different design while still being fully functional.&lt;/p&gt;&lt;head rend="h3"&gt;Pixel reading&lt;/head&gt;&lt;p&gt;And now we come to what is most likely the most useful attack primitive - pixel reading. That’s right, you can use SVG filters to read color data off of images and perform all sorts of logic on them to create really advanced and convincing attacks.&lt;/p&gt;&lt;p&gt;The catch is of course, that you’ll have to do everything within SVG filters - there is no way to get the data out6. Despite that, it is very powerful if you get creative with it.&lt;/p&gt;&lt;p&gt;On a higher level, what this lets us do is make everything in a clickjacking attack responsive - fake buttons can have hover effects, pressing them can show fake dropdowns and dialogs, and we can even have fake form validation.&lt;/p&gt;&lt;p&gt;Let’s start off with a simple example - detecting if a pixel is pure black, and using it to turn another filter on or off.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;For this target, we want to detect when the user clicks on the box to change its color, and use that to toggle a blur effect.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;Let’s start off by using two copies of the &lt;code&gt;feTile&lt;/code&gt; filter to first crop out the few pixels we’re interested in and then tile those pixels across the entire image.&lt;/p&gt;&lt;p&gt;The result is that we now have the entire screen filled with the color of the area we are interested in.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;We can turn this result into a binary on/off value by using &lt;code&gt;feComposite&lt;/code&gt;’s arithmetic the same way as in the last section, but with a way larger &lt;code&gt;k2&lt;/code&gt; value. This makes it so that the output image is either completely black or completely white.&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;And just as before, this can be used as a mask. We once again convert it into an alpha matte, but this time apply it to the blur filter.&lt;/p&gt;&lt;p&gt;So that’s how you can find out whether a pixel is black and use that to toggle a filter!&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;Uh oh! It seems that somebody has changed the target to have a pride-themed button instead!&lt;/p&gt;&lt;p&gt;How can we adapt this technique to work with arbitrary colors and textures?&lt;/p&gt;&lt;p&gt;&amp;lt;--- very cool! click to change color&lt;/p&gt;&lt;p&gt;The solution is pretty simple - we can simply use &lt;code&gt;feBlend&lt;/code&gt;’s difference combined with a &lt;code&gt;feColorMatrix&lt;/code&gt; to join the color channels to turn the image into a similar black/white matte as before. For textures we can use &lt;code&gt;feImage&lt;/code&gt;, and for non-exact colors we can use a bit of &lt;code&gt;feComposite&lt;/code&gt;’s arithmetic to make the matching threshold more lenient.&lt;/p&gt;&lt;p&gt;And that’s it, a simple example of how we can read a pixel value and use it to toggle a filter.&lt;/p&gt;&lt;head rend="h3"&gt;Logic gates&lt;/head&gt;&lt;p&gt;But here’s the part where it gets fun! We can repeat the pixel-reading process to read out multiple pixels, and then run logic on them to program an attack.&lt;/p&gt;&lt;p&gt;By using &lt;code&gt;feBlend&lt;/code&gt; and &lt;code&gt;feComposite&lt;/code&gt;, we can recreate all logic gates and make SVG filters functionally complete. This means that we can program anything we want, as long as it is not timing-based7 and doesn’t take up too many resources8.&lt;/p&gt;&lt;p&gt;Input: &lt;/p&gt;&lt;p&gt; NOT: &lt;code&gt;&amp;lt;feBlend mode=difference in2=white /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; AND: &lt;code&gt;&amp;lt;feComposite operator=arithmetic k1=1 /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; OR: &lt;code&gt;&amp;lt;feComposite operator=arithmetic k2=1 k3=1 /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; XOR: &lt;code&gt;&amp;lt;feBlend mode=difference in=a in2=b /&amp;gt;&lt;/code&gt;&lt;/p&gt;&lt;p&gt; NAND: &lt;code&gt;(AND + NOT)&lt;/code&gt;&lt;/p&gt;&lt;p&gt; NOR: &lt;code&gt;(OR + NOT)&lt;/code&gt;&lt;/p&gt;&lt;p&gt; XNOR: &lt;code&gt;(XOR + NOT)&lt;/code&gt;&lt;/p&gt;&lt;p&gt;These logic gates are what modern computers are made of. You could build a computer within an SVG filter if you wanted to. In fact, here’s a basic calculator I made:&lt;/p&gt;&lt;p&gt;SVG Adder&lt;/p&gt;&lt;p&gt;This is a full adder circuit. This filter implements the logic gates for the output and for the carry bit using the logic gates described above. There are more efficient ways to implement an adder in SVG filters, but this is meant to serve as proof of the ability to implement arbitrary logic circuits.&lt;/p&gt;&lt;p&gt;Anyways, for an attacker, what all of this means is that you can make a multi-step clickjacking attack with lots of conditions and interactivity. And you can run logic on data from cross-origin frames.&lt;/p&gt;&lt;p&gt;Securify&lt;/p&gt;&lt;p&gt;Welcome to this secure application!&lt;/p&gt;&lt;p&gt;Hack confirmation&lt;/p&gt;&lt;p&gt;Are you sure you'd like to get hacked?&lt;/p&gt;⌛&lt;p&gt;This is an example target where we want to trick the user into marking themselves as hacked, which requires a few steps:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Clicking a button to open a dialog&lt;/item&gt;&lt;item&gt;Waiting for the dialog to load&lt;/item&gt;&lt;item&gt;Clicking a checkbox within the dialog&lt;/item&gt;&lt;item&gt;Clicking another button in the dialog&lt;/item&gt;&lt;item&gt;Checking for the red text that appeared&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Securify&lt;/p&gt;&lt;p&gt;Welcome to this secure application!&lt;/p&gt;&lt;p&gt;Hack confirmation&lt;/p&gt;&lt;p&gt;Are you sure you'd like to get hacked?&lt;/p&gt;⌛&lt;p&gt;Win free iPod by following the steps below.&lt;/p&gt;&lt;p&gt;A traditional clickjacking attack against this target would be difficult to pull off. You’d need to have the user click on multiple buttons in a row with no feedback in the UI.&lt;/p&gt;&lt;p&gt;There are some tricks you could do to make a traditional attack more convincing than what you see above, but it’s still gonna look sketch af. And the moment you throw something like a text input into the mix, it’s just not gonna work.&lt;/p&gt;&lt;p&gt;Anyways, let’s build out a logic tree for a filter-based attack:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Is the dialog open? &lt;list rend="ul"&gt;&lt;item&gt;(No) Is the red text present? &lt;list rend="ul"&gt;&lt;item&gt;(No) Make the user press the button&lt;/item&gt;&lt;item&gt;(Yes) Show the end screen&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;(Yes) Is the dialog loaded? &lt;list rend="ul"&gt;&lt;item&gt;(No) Show loading screen&lt;/item&gt;&lt;item&gt;(Yes) Is the checkbox checked? &lt;list rend="ul"&gt;&lt;item&gt;(No) Make the user check the checkbox&lt;/item&gt;&lt;item&gt;(Yes) Make the user click the button&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;(No) Is the red text present? &lt;/item&gt;&lt;/list&gt;&lt;p&gt;Which can be expressed in logic gates9 as:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Inputs &lt;list rend="ul"&gt;&lt;item&gt;D (dialog visible) = check for background dim&lt;/item&gt;&lt;item&gt;L (dialog loaded) = check for the button in dialog&lt;/item&gt;&lt;item&gt;C (checkbox checked) = check whether the button is blue or grey&lt;/item&gt;&lt;item&gt;R (red text visible) = &lt;code&gt;feMorphology&lt;/code&gt;and check for red pixels&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Outputs &lt;list rend="ul"&gt;&lt;item&gt;(¬D) ∧ (¬R) =&amp;gt; button1.png&lt;/item&gt;&lt;item&gt;D ∧ (¬L) =&amp;gt; loading.png&lt;/item&gt;&lt;item&gt;D ∧ L ∧ (¬C) =&amp;gt; checkbox.png&lt;/item&gt;&lt;item&gt;D ∧ L ∧ C =&amp;gt; button2.png&lt;/item&gt;&lt;item&gt;(¬D) ∧ R =&amp;gt; end.png&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;And this is how we would implement it in SVG:&lt;/p&gt;&lt;p&gt;Securify&lt;/p&gt;&lt;p&gt;Welcome to this secure application!&lt;/p&gt;&lt;p&gt;Hack confirmation&lt;/p&gt;&lt;p&gt;Are you sure you'd like to get hacked?&lt;/p&gt;⌛&lt;p&gt;Play around with this and see just how much more convincing it is as an attack. And we could easily make it better by, for example, adding some extra logic to also add hover visuals to the buttons. The demo has debug visuals for the four inputs (D, L, C, R) in the bottom left as squares to make it easier to understand what’s going on.&lt;/p&gt;&lt;p&gt;But yeah, that’s how you can make complex and long clickjacking attacks that have not been realistic with the traditional clickjacking methods.&lt;/p&gt;&lt;p&gt;I kept this example here pretty short and simple, but real-world attacks can be a lot more involved and polished.&lt;/p&gt;&lt;p&gt;In fact…&lt;/p&gt;&lt;head rend="h2"&gt;The Docs bug&lt;/head&gt;&lt;p&gt;I’ve actually managed to pull off this attack against Google Docs!&lt;/p&gt;&lt;p&gt;Take a look at the demo videos here (alt links: bsky, twitter).&lt;/p&gt;&lt;p&gt;What this attack does is:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Makes the user click on the “Generate Document” button&lt;/item&gt;&lt;item&gt;Once pressed, detects the popup and shows a textbox for the user to type a “captcha” into &lt;list rend="ul"&gt;&lt;item&gt;The textbox starts off with a gradient animation, which must be handled&lt;/item&gt;&lt;item&gt;The textbox has focus states, which must also be present in the attack visuals, so they must be detected by the background color of the textbox&lt;/item&gt;&lt;item&gt;The textbox has grey text for both a placeholder AND suggestions, which must be hidden with the technique discussed earlier&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Once the captcha is typed, makes the user seemingly click on a button (or press enter), which causes a suggested Docs item to be added into the textbox &lt;list rend="ul"&gt;&lt;item&gt;This item must be detected by looking for its background color in the textbox&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Once the item is detected, the textbox must be hidden and another button must be shown instead &lt;list rend="ul"&gt;&lt;item&gt;Once that button is clicked, a loading screen appears, which must be detected&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;If the loading screen is present, or the dialog is not visible and the “Generate Document” button is not present, the attack is over and the final screen must be shown&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In the past, individual parts of such an attack could’ve been pulled off through traditional clickjacking and some basic CSS, but the entire attack would’ve been way too long and complex to be realistic. With this new technique of running logic inside SVG filters, such attacks become realistic.&lt;/p&gt;&lt;p&gt;Google VRP awarded me $3133.70 for the find. That was, of course, right before they introduced a novelty bonus for new vulnerability classes. Hmph!10&lt;/p&gt;&lt;head rend="h2"&gt;The QR attack&lt;/head&gt;&lt;p&gt;Something I see in online discussions often is the insistence on QR codes being dangerous. It kind of rubs me the wrong way because QR codes are not any more dangerous than links.&lt;/p&gt;&lt;p&gt;I don’t usually comment on this too much because it’s best to avoid suspicious links, and the same goes for QR codes, but it does nag me to see people make QR codes out to be this evil thing that can somehow immediately hack you.&lt;/p&gt;&lt;p&gt;I turns out though, that my SVG filters attack technique can be applied to QR codes as well!&lt;/p&gt;&lt;p&gt;The example from earlier in the blog with retyping a code becomes impractical once the user realizes they’re typing something they shouldn’t. We can’t stuff the data we exfiltrate into a link either, because an SVG filter cannot create a link.&lt;/p&gt;&lt;p&gt;But since an SVG filter can run logic and provide visual output, perhaps we could generate a QR code with a link instead?&lt;/p&gt;&lt;head rend="h3"&gt;Creating the QR&lt;/head&gt;&lt;p&gt;Creating a QR code within an SVG filter is easier said than done however. We can shape binary data into the shape of a QR code by using &lt;code&gt;feDisplacementMap&lt;/code&gt;, but for a QR code to be scannable it also needs error correction data.&lt;/p&gt;&lt;p&gt;QR codes use Reed-Solomon error correction, which is some fun math stuff that’s a bit more advanced than a simple checksum. It does math with polynomials and stuff and that is a bit annoying to reimplement in an SVG.&lt;/p&gt;&lt;p&gt;Luckily for us, I’ve faced the same problem before! Back in 2021 I was the first person11 to make a QR code generator in Minecraft, so I’ve already figured out the things necessary.&lt;/p&gt;&lt;p&gt;In my build I pre-calculated some lookup tables for the error correction, and used those instead to make the build simpler - and we can do the same with the SVG filter.&lt;/p&gt;&lt;p&gt;This post is already getting pretty long, so I’ll leave figuring out how this filter works as an exercise to the reader ;).&lt;/p&gt;&lt;p&gt;This is a demo that displays a QR code telling you how many seconds you’ve been on this page for. It’s a bit fiddly, so if it doesn’t work make sure that you aren’t using any &lt;/p&gt;&lt;p&gt;This demo &lt;/p&gt;&lt;p&gt;Similarly, in a real attack, the scaling and color profile issues could be worked around using some JavaScript tricks or simply by implementing the filter a bit differently - this here is just a proof of concept that’s a bit rough around the edges.&lt;/p&gt;&lt;p&gt;But yeah, that’s a QR code generator built inside an SVG filter!&lt;/p&gt;&lt;p&gt;Took me a while to make, but I didn’t want to write about it just being “theoretically possible”.&lt;/p&gt;&lt;head rend="h3"&gt;Attack scenario&lt;/head&gt;&lt;p&gt;So the attack scenario with the QR code is that you’d read pixels from a frame, process them to extract the data you want, encode them into a URL that looks something like https://lyra.horse/?ref=c3VwZXIgc2VjcmV0IGluZm8 and render it as a QR code.&lt;/p&gt;&lt;p&gt;Then, you prompt the user to scan the QR code for whatever reason (eg anti-bot check). To them, the URL will seem like just a normal URL with a tracking ID or something in it.&lt;/p&gt;&lt;p&gt;Once the user opens the URL, your server gets the request and receives the data from the URL.&lt;/p&gt;&lt;head rend="h2"&gt;And so on..&lt;/head&gt;&lt;p&gt;There are so many ways to make use of this technique I won’t have time to go over them all in this post. Some examples would be reading text by using the difference blend mode, or exfiltrating data by making the user click on certain parts of the screen.&lt;/p&gt;&lt;p&gt;You could even insert data from the outside to have a fake mouse cursor inside the SVG that shows the pointer cursor and reacts to fake buttons inside your SVG to make the exfiltration more realistic.&lt;/p&gt;&lt;p&gt;Or you could code up attacks with CSS and SVG where CSP doesn’t allow for any JS.&lt;/p&gt;&lt;p&gt;Anyways, this post is long as is, so I’ll leave figuring out these techniques as homework.&lt;/p&gt;&lt;head rend="h2"&gt;Novel technique&lt;/head&gt;&lt;p&gt;This is the first time in my security research I’ve found a completely new technique!&lt;/p&gt;&lt;p&gt;I introduced it briefly at my BSides talk in September, and this post here is a more in-depth overview of the technique and how it can be used.&lt;/p&gt;&lt;p&gt;Of course, you can never know 100% for sure that a specific type of attack has never been found by anyone else, but my extensive search of existing security research has come up with nothing, so I suppose I can crown myself as the researcher who discovered it?&lt;/p&gt;&lt;p&gt;Here’s some previous research I’ve found:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;You click, I steal: analyzing and detecting click hijacking attacks in web pages,&lt;lb/&gt;On the fragility and limitations of current Browser-provided Clickjacking protection schemes&lt;list rend="ul"&gt;&lt;item&gt;The papers mention SVG filters in clickjacking attacks, but only in the context of obscuring the underlying elements, not running logic.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Pixel Perfect Timing - Attacks with HTML5,&lt;lb/&gt;Security: SVG Filter Timing Attack&lt;list rend="ul"&gt;&lt;item&gt;Research on reading pixels through SVG filter timing attacks, which is a technique that is mitigated in modern browsers.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;The Human Side Channel &lt;list rend="ul"&gt;&lt;item&gt;Some pretty cool clickjacking techniques, though no multi-step attacks or SVG logic.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;SVG is turing-complete-ish &lt;list rend="ul"&gt;&lt;item&gt;Another example of logic gates in SVG I found after writing my blog. It’s fun because it comes with reddit and hn threads - I particularly like the comment asking about whether this turing completeness is useful or just a fun fact, which got a reply confirming the latter. I like turning fun facts into vulnerabilities ^^.&lt;/item&gt;&lt;item&gt;Note that whether SVG filters are actually turing complete is questionable because filters are implemented in constant-time and can’t run in a loop. This doesn’t mean they can’t be turing complete, but it also doesn’t prove that they are.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;I don’t think me discovering this technique was just luck though. I have a history of seeing things such as CSS as programming languages to exploit and be creative with. It wasn’t a stretch for me to see SVG filters as a programming language either.&lt;/p&gt;&lt;p&gt;That, and my overlap between security research and creative projects - I often blur the lines between the two, which is what Antonymph was born out of.&lt;/p&gt;&lt;p&gt;In any case, &lt;/p&gt;&lt;head rend="h2"&gt;afterword&lt;/head&gt;&lt;p&gt;whoa this post took such a long time for me to get done!&lt;/p&gt;&lt;p&gt;i started work on it in july, and was expecting to release it alongside my CSS talk in september, but it has taken me so much longer than expected to actually finish this thing. i wanted to make sure it was a good in-depth post, rather than something i just get out as soon as possible.&lt;/p&gt;&lt;p&gt;unlike my previous posts, i did unfortunately have to break my trend of using no images, since i needed a few data URIs within the SVG filters for demos. still, no images anywhere else in the post, no javascript, and just 42kB (gzip) of handcrafted html/css/svg.&lt;/p&gt;&lt;p&gt;also, i usually hide a bunch of easter eggs in my post that link to stuff i’ve enjoyed recently, but i have a couple links i didn’t want to include without content warnings. finding responsibility is a pretty dark talk about the ethics of making sure your work won’t end up killing people, and youre the one ive always wanted is slightly nsfw doggyhell vent art.&lt;/p&gt;&lt;p&gt;btw i’ll soon be giving talks at 39c3 and disobey 2026! the 39c3 one is titled “css clicker training” and will be about css crimes and making games in css. and the disobey one is the same talk as the bsides one about using css to hack stuff and get bug bounties, but i’ll make sure to throw some extra content in there to keep it fun.&lt;/p&gt;&lt;p&gt;see y’all around!!&lt;/p&gt;&lt;p&gt;&amp;lt;3&lt;/p&gt;&lt;p&gt;Note: I you’re making content (articles, videos etc) based on this post, feel free to reach out to me to ask for questions or feedback.&lt;/p&gt;&lt;p&gt;Discuss this post on: twitter, mastodon, lobsters&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;What I actually had after an hour was this, the Codepen link is an updated version that I added controls to later on. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;This is a fancy way of saying it does a basic displacement of pixels. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;…or After Effects/Blender/Fusion etc user. Or anything else computer graphics. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;result = k1*i1*i2 + k2*i1 + k3*i2 + k4 in programmer language (I just couldn’t resist trying out the &amp;lt;math&amp;gt; tag for fun). ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;The multiplication in this case is kind of the opposite of what you’d expect from the “multiply” blend mode - things will get lighter, not darker. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;It’s not possible to get the pixel data out of a SVG filter as they’re implemented in constant-time. If you can find a way to retrieve the data then it’s a browser bug and you can most likely get bounty for it. Happy to collaborate if you’d like to turn such a finding into a working proof of concept for a report :). ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;We can actually pass the current time into an SVG filter, but we can’t do attacks such as “if a pixel changes, wait 1 second and then show a dialog” unless we can piggyback off an animation in the source frame. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Since SVG filters are implemented in constant-time, they become pretty resource-intensive for complex filters on high-resolution targets. One optimization would be to have a full-resolution filter just for picking out the pixels, then a tiny-resolution backdrop-filter to run all the logic, and then another full-resolution filter to display the attack. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;¬ - NOT, ∧ - AND, ∨ - OR, ⊕ - XOR etc, see List of logic symbols. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;This is kind of similar to how I reported the Docs/YouTube/Slides chain right before they 5x’d the VRP rewards. I seem to have the worst luck with timing my reports… ↩︎&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;I released my QR code generator in 2021, making it the earliest publicly released Minecraft QR code generator. I know, however, that DavidJR was independently working on a QR code generator at the same time as I was, eventually releasing it in 2023. Then there’s one from Sep 2024 by 37meliodas, and lastly there’s the probably most well-known one by mattbatwings from Dec 2024. The latter has an awesome video explaining everything in-depth, so I definitely recommend checking it out if you’re interested in Minecraft redstone. ↩︎&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46155085</guid><pubDate>Fri, 05 Dec 2025 00:03:36 +0000</pubDate></item><item><title>Netflix’s AV1 Journey: From Android to TVs and Beyond</title><link>https://netflixtechblog.com/av1-now-powering-30-of-netflix-streaming-02f592242d80</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46155135</guid><pubDate>Fri, 05 Dec 2025 00:09:57 +0000</pubDate></item><item><title>BMW PHEV: Safety fuse replacement is extremely expensive</title><link>https://evclinic.eu/2025/12/04/2021-phev-bmw-ibmucp-21f37e-post-crash-recovery-when-eu-engineering-becomes-a-synonym-for-unrepairable-generating-waste/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46155619</guid><pubDate>Fri, 05 Dec 2025 01:05:57 +0000</pubDate></item><item><title>NeurIPS 2025 Best Paper Awards</title><link>https://blog.neurips.cc/2025/11/26/announcing-the-neurips-2025-best-paper-awards/</link><description>&lt;doc fingerprint="35151ad94c338bba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing the NeurIPS 2025 Best Paper Awards&lt;/head&gt;
    &lt;p&gt;The Best Paper Award Committee members were nominated by the Program Chairs and the Database and Benchmark track chairs, who selected leading researchers across machine learning topics. These nominations were approved by the General Chairs and Next Generation and Accessibility Chairs.&lt;/p&gt;
    &lt;p&gt;The best paper award committees were tasked with selecting a handful of highly impactful papers from the Main Track and the Datasets &amp;amp; Benchmark Track of the conference.&lt;/p&gt;
    &lt;p&gt;With that, we are excited to share the news that the best and runner-up paper awards this year go to seven groundbreaking papers, including four best papers (one of which is from the datasets and benchmarks track) and three runner-ups. The seven papers highlight advances in diffusion model theory, self-supervised reinforcement learning, attention mechanisms for large language models, reasoning capabilities in LLMs, online learning theory, neural scaling laws, and benchmarking methodologies for language model diversity.&lt;/p&gt;
    &lt;p&gt;The winners are presented here in alphabetical order by title.&lt;/p&gt;
    &lt;p&gt;Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)&lt;/p&gt;
    &lt;head rend="h1"&gt;Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Yejin Choi&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Large language models (LMs) often struggle to generate diverse, human-like creative content, raising concerns about the long-term homogenization of human thought through repeated exposure to similar outputs. Yet scalable methods for evaluating LM output diversity remain limited, especially beyond narrow tasks such as random number or name generation, or beyond repeated sampling from a single model. To address this gap, we introduce Infinity-Chat, a large-scale dataset of 26K diverse, real-world, open-ended user queries that admit a wide range of plausible answers with no single ground truth. We introduce the first comprehensive taxonomy for characterizing the full spectrum of open-ended prompts posed to LMs, comprising 6 top-level categories (e.g., creative content generation, brainstorm &amp;amp; ideation) that further breaks down to 17 subcategories. Using Infinity-Chat, we present a large-scale study of mode collapse in LMs, revealing a pronounced Artificial Hivemind effect in open-ended generation of LMs, characterized by (1) intra-model repetition, where a single model consistently generates similar responses, and more so (2) inter-model homogeneity, where different models produce strikingly similar outputs. Infinity-Chat also includes 31,250 human annotations, across absolute ratings and pairwise preferences, with 25 independent human annotations per example. This enables studying collective and individual-specific human preferences in response to open-ended queries. Our findings show that state-of-the-art LMs, reward models, and LM judges are less well calibrated to human ratings on model generations that elicit differing idiosyncratic annotator preferences, despite maintaining comparable overall quality. Overall, INFINITY-CHAT presents the first large-scale resource for systematically studying real-world open-ended queries to LMs, revealing critical insights to guide future research for mitigating long-term AI safety risks posed by the Artificial Hivemind.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;/p&gt;
    &lt;p&gt;This paper makes a substantial and timely contribution to the understanding of diversity, pluralism, and societal impact in modern language models. The authors introduce Infinity-Chat, a rigorously constructed benchmark of 26K real-world open-ended queries paired with 31K dense human annotations, enabling systematic evaluation of creative generation, ideation, and subjective preference alignment, dimensions historically underexamined in AI evaluation. Beyond releasing a valuable dataset, the paper provides deep analytical insights through the first comprehensive taxonomy of open-ended prompts and an extensive empirical study across more than 70 models, revealing the Artificial Hivemind effect: pronounced intra- and inter-model homogenization that raises serious concerns about long-term risks to human creativity, value plurality, and independent thinking. The findings expose critical miscalibration between current reward models, automated judges, and diverse human preferences, highlighting the tension between alignment and diversity and establishing a foundation for future work on preserving heterogeneity in AI systems. Overall, this work sets a new standard for datasets and benchmarks that advance scientific understanding and address pressing societal challenges rather than solely improving technical performance.&lt;/p&gt;
    &lt;p&gt;Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free&lt;/p&gt;
    &lt;head rend="h3"&gt;Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, Junyang Lin&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification—applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)—consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates massive activation, attention sink and enhances long-context extrapolation performance. We also release related codes (https://github.com/qiuzh20/gated_attention}) and models (https://huggingface.co/QwQZh/gated_attention) to facilitate future research. Furthermore, the most effective SDPA output gating is used in the Qwen3-Next models (https://huggingface.co/collections/Qwen/qwen3-next).&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;/p&gt;
    &lt;p&gt;The main finding of this paper is that the performance of large language models using softmax attention can be consistently improved by introducing head-specific sigmoid gating after the scaled dot product attention operation in both dense and mixture-of-experts (MoE) Transformer models. This finding is backed up by more than thirty experiments on different variants of gated softmax attention using 15B MoE and 1.7B dense models trained on large-scale datasets of 400B, 1T, or 3.5T tokens. The paper also includes careful analyses showing that the introduction of the authors’ recommended form of gating improves the training stability of large language models, reduces the “attention sink” phenomenon that has been widely reported in attention models, and enhances the performance of context length extension. The main recommendation of the paper is easily implemented, and given the extensive evidence provided in the paper for this modification to LLM architecture, we expect this idea to be widely adopted. This paper represents a substantial amount of work that is possible only with access to industrial scale computing resources, and the authors’ sharing of the results of their work, which will advance the community’s understanding of attention in large language models, is highly commendable, especially in an environment where there has been a move away from open sharing of scientific results around LLMs.&lt;/p&gt;
    &lt;p&gt;1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities&lt;/p&gt;
    &lt;head rend="h3"&gt;Kevin Wang , Ishaan Javali, Michał Bortkiewicz, Tomasz Trzcinski, Benjamin Eysenbach&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 — 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance. Our experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals. Evaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by — , outperforming other goal-conditioned baselines. Increasing the model depth not only increases success rates but also qualitatively changes the behaviors learned.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;lb/&gt;This paper challenges the conventional assumption that the information provided by reinforcement learning (RL) is insufficient to effectively guide the numerous parameters of deep neural networks, hence suggesting that large AI systems be predominantly trained through self-supervision, with RL reserved solely for fine-tuning. The work introduces a novel and easy-to-implement RL paradigm for the effective training of very deep neural networks, employing self-supervised and contrastive RL. The accompanying analysis demonstrates that RL can scale efficiently with increasing network depth, leading to the emergence of more sophisticated capabilities. In addition to presenting compelling results, the study includes several useful analyses, for example, for highlighting the important role of batch size scaling for deeper networks within contrastive RL. &lt;/p&gt;
    &lt;p&gt;Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training&lt;/p&gt;
    &lt;head rend="h3"&gt;Tony Bonnaire, Raphaël Urfin, Giulio Biroli, Marc Mezard&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time at which models begin to generate high-quality samples, and a later time beyond which memorization emerges. Crucially, we find that increases linearly with the training set size , while remaining constant. This creates a growing window of training times where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when it becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allows to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;/p&gt;
    &lt;p&gt;This paper presents foundational work on the implicit regularization dynamics of diffusion models, delivering a powerful result by unifying empirical observation with formal theory. The critical finding is the quantitative identification of two distinct, predictable timescales, an early, dataset-independent generalization phase followed by a linear, dataset-size-dependent memorization phase . This demonstration of an expanding window for effective generalization is not merely an empirical finding but is rigorously explained by deriving the spectral properties of the random features model using random matrix theory. By linking the practical success of diffusion models directly to a provable dynamical property (the implicit postponement of overfitting), the paper provides fundamental, actionable insight into the mechanisms governing modern generative AI, setting a new standard for analytical depth in the study of generalization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Runners Up&lt;/head&gt;
    &lt;p&gt;Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?&lt;/p&gt;
    &lt;head rend="h3"&gt;Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, Gao Huang&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly in mathematics and programming tasks. It is widely believed that, similar to how traditional RL helps agents to explore and learn new strategies, RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed the capacity of the corresponding base models. In this study, we take a critical look at \textit{the current state of RLVR} by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across diverse model families, RL algorithms, and math/coding/visual reasoning benchmarks, using pass@\textit{k} at large \textit{k} values as the evaluation metric. While RLVR improves sampling efficiency towards the correct path, we surprisingly find that current training does \emph{not} elicit fundamentally new reasoning patterns. We observe that while RLVR-trained models outperform their base models at smaller values of (\eg, =1), base models achieve higher pass@ score when is large. Moreover, we observe that the reasoning capability boundary of LLMs often narrows as RLVR training progresses. Further coverage and perplexity analysis shows that the reasoning paths generated by RLVR models are already included in the base models’ sampling distribution, suggesting that their reasoning abilities originate from and are \textit{bounded} by the base model. From this perspective, treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in fully leveraging the potential of the base model. In contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model’s reasoning capabilities. Taken together, our findings suggest that current RLVR methods have not fully realized the potential of RL to elicit genuinely novel reasoning abilities in LLMs. This underscores the need for improved RL paradigms—such as continual scaling and multi-turn agent-environment interaction—to unlock this potential.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;/p&gt;
    &lt;p&gt;This paper delivers a masterfully executed and critically important negative finding on a widely accepted, foundational assumption in Large Language Model (LLM) research: that Reinforcement Learning with Verifiable Rewards (RLVR) elicits genuinely new reasoning capabilities. The paper shows that RLVR training, across various model families, tasks, and algorithms, enhances sampling efficiency without expanding the reasoning capacity already present in base models. RL narrows exploration, rewarded trajectories are amplified, but the broader solution space shrinks, revealing that RLVR optimizes within, rather than beyond, the base distribution. This is an important finding which will hopefully incentivize fundamentally new RL paradigms able to navigate the vast action space and genuinely expand LLM reasoning capabilities.&lt;/p&gt;
    &lt;p&gt;Optimal Mistake Bounds for Transductive Online Learning&lt;/p&gt;
    &lt;head rend="h3"&gt;Zachary Chase, Steve Hanneke, Shay Moran, Jonathan Shafer&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. We prove that for every concept class with Littlestone dimension , the transductive mistake bound is at least . This establishes an exponential improvement over previous lower bounds of , , and , respectively due to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that our bound is tight: for every , there exists a class of Littlestone dimension with transductive mistake bound . Our upper bound also improves the previous best known upper bound from Ben-David et al. (1997). These results demonstrate a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advanced access to the unlabeled instance sequence. This stands in stark contrast to the PAC setting, where transductive and standard learning exhibit similar sample complexities.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee&lt;/p&gt;
    &lt;p&gt;This paper presents a breakthrough in learning theory, deserving the NeurIPS Best Paper Runner-Up award for its elegant, comprehensive, and definitive resolution of a 30-year-old open problem. The authors have not only precisely quantified the optimal mistake bound for transductive online learning as Ω(√d), but they have also achieved a tight match with an O(√d) upper bound. This establishes a quadratic gap between transductive and standard online learning, a result that represents an exponential leap beyond all previous logarithmic lower bounds and dramatically highlights the theoretical value of unlabeled data in this setting—a crucial insight distinct from its more limited role in PAC learning.&lt;lb/&gt;The novelty and ingenuity of their proof techniques are quite remarkable. For the lower bound, the adversary employs a sophisticated strategy that balances forcing mistakes with carefully managing the shrinking of the version space, leveraging the concept of “paths in trees” as a fundamental underlying structure. The upper bound, demonstrating the learnability within O(√d) mistakes, introduces an innovative hypothesis class construction that embeds a “sparse encoding” for off-path nodes – a probabilistic design where most off-path labels are zero, but the rare ones carry immense information. The learner’s strategy to exploit this class is equally brilliant, integrating several non-standard sophisticated techniques: “Danger Zone Minimization” to control the instance sequence presented by the adversary, “Splitting Experts” via a multiplicative weights approach to handle uncertainty about a node’s on-path status, and a strategic “Transition to Halving” once sufficient information is gathered from the sparsely encoded off-path labels. This intricate interplay between a cleverly constructed hypothesis class and a highly adaptive learning algorithm showcases a masterclass in theoretical analysis and design.&lt;/p&gt;
    &lt;p&gt;Superposition Yields Robust Neural Scaling&lt;/p&gt;
    &lt;head rend="h3"&gt;Yizhou Liu, Ziming Liu, Jeff Gore&lt;/head&gt;
    &lt;p&gt;Abstract&lt;/p&gt;
    &lt;p&gt;The success of today’s large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law, that loss decreases as a power law with model size, remains unclear. We propose that representation superposition, meaning that LLMs represent more features than they have dimensions, can be a key contributor to loss and cause neural scaling. Based on Anthropic’s toy model, we use weight decay to control the degree of superposition, allowing us to systematically study how loss scales with model size. When superposition is weak, the loss follows a power law only if data feature frequencies are power-law distributed. In contrast, under strong superposition, the loss generically scales inversely with model dimension across a broad class of frequency distributions, due to geometric overlaps between representation vectors. We confirmed that open-sourced LLMs operate in the strong superposition regime and have loss scaling inversely with model dimension, and that the Chinchilla scaling laws are also consistent with this behavior. Our results identify representation superposition as a central driver of neural scaling laws, providing insights into questions like when neural scaling laws can be improved and when they will break down.&lt;/p&gt;
    &lt;p&gt;Reflections from the Selection Committee:&lt;/p&gt;
    &lt;p&gt;This paper moves beyond observation of neural scaling laws—the empirically established phenomenon in which model loss exhibits a power-law decrease as model size, dataset size, or computational resources are increased—to demonstrate that representation superposition constitutes the primary mechanism governing these laws. Authors introduce a controlled “toy model” to examine how superposition and data structure affect the scaling of loss with model size and demonstrate that under strong superposition where features are overlapping, the loss scales consistently as an inverse power law with respect to the model dimension. The core findings are supported by a series of carefully designed experiments and offer fresh insights into an important research area.&lt;/p&gt;
    &lt;p&gt;The selection of these papers reflects the remarkable breadth of research presented at NeurIPS 2025, spanning generative modeling, reinforcement learning, natural language processing, learning theory, neural scaling, and benchmarking methodologies. The diversity of topics among the awarded papers demonstrates the vibrant and multifaceted nature of machine learning research.&lt;/p&gt;
    &lt;p&gt;We extend our congratulations to all the award recipients and look forward to seeing these works presented at the conference this December! Please note that the award certificates will be given out during the paper’s respective oral sessions by the session chairs.&lt;/p&gt;
    &lt;p&gt;We would also like to extend our gratitude and appreciation to the members of the Best Paper Award Committee listed here.&lt;/p&gt;
    &lt;p&gt;Best Paper Award Committee for Main Track and Database and Benchmark Tracks&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jacob Andreas (MIT, United States)&lt;/item&gt;
      &lt;item&gt;Sander Dieleman (Google DeepMind, UK)&lt;/item&gt;
      &lt;item&gt;Dilek Hakkani-Tur (University of Illinois Urbana-Champaign, United States)&lt;/item&gt;
      &lt;item&gt;Brian Kingsbury (IBM, United States)&lt;/item&gt;
      &lt;item&gt;Mirella Lapata (University of Edinburgh, Scotland)&lt;/item&gt;
      &lt;item&gt;Vincent Lepetit (Ecole des Ponts ParisTech, France)&lt;/item&gt;
      &lt;item&gt;Ulrich Paquet (AIMES &amp;amp; Google DeepMind, Africa)&lt;/item&gt;
      &lt;item&gt;Violet Peng (UCLA, United States)&lt;/item&gt;
      &lt;item&gt;Doina Precup (McGill University, Canada)&lt;/item&gt;
      &lt;item&gt;Masashi Sugiyama (RIKEN &amp;amp; University of Tokyo, Japan)&lt;/item&gt;
      &lt;item&gt;Vincent Tan (National University of Singapore, Singapore)&lt;/item&gt;
      &lt;item&gt;Yee Whye Teh (University of Oxford, United Kingdom)&lt;/item&gt;
      &lt;item&gt;Xing Xie (Microsoft, China)&lt;/item&gt;
      &lt;item&gt;Luke Zettlemoyer (University of Washington/Meta, United States)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46155701</guid><pubDate>Fri, 05 Dec 2025 01:15:42 +0000</pubDate></item><item><title>UniFi 5G</title><link>https://blog.ui.com/article/introducing-unifi-5g</link><description>&lt;doc fingerprint="3e818a12e14516a1"&gt;
  &lt;main&gt;
    &lt;p&gt;The UniFi 5G Max lineup was created with a clear goal in mind: deliver a sleek, versatile, and exceptionally powerful 5G internet experience that works effortlessly in any environment.&lt;/p&gt;
    &lt;p&gt;The UniFi 5G Max lineup was created with a clear goal in mind: deliver a sleek, versatile, and exceptionally powerful 5G internet experience that works effortlessly in any environment.&lt;/p&gt;
    &lt;p&gt;The UniFi 5G Max makes deployment easy, whether installed locally or at a remote site. Plug it into any PoE port and it instantly appears as a ready to use WAN interface, no matter whether plugged directly into your UniFi gateway or into your office switch. No new cable runs needed! It sits neatly on a desk, but you can reposition it for the best possible signal using the included wall or window mount.&lt;/p&gt;
    &lt;p&gt;The 5G Max delivers downlink speeds up to 2 Gbps with ultra low latency that makes it reliable as a primary connection and seamless as a backup WAN. UniFi routing policies and SLAs let you choose exactly how and when 5G is used, and for which clients and VLANs. Easily set per-SIM usage limits to avoid overage costs with just a few clicks.&lt;/p&gt;
    &lt;p&gt;For tougher environments or deployments with poor indoor cellular coverage, the outdoor model maintains the same high performance cellular connectivity with improved antenna performance in a durable IP67 rated enclosure. It is built for rooftop installs, off site locations, and mobile deployments where reliability is critical. Just like its indoor counterpart, you can also connect it via any PoE port, anywhere on your network, greatly simplifying cabling requirements.&lt;/p&gt;
    &lt;p&gt;If you want everything UniFi in one device, the DreamRouter 5G Max combines 5G connectivity with WiFi 7, local storage, and full UniFi OS application support. Deploy it anywhere 5G is available and run an entire high-performance and scalable network stack instantly.&lt;/p&gt;
    &lt;p&gt;Every device in the UniFi 5G lineup supports both physical SIMs and eSIM, giving you the freedom to choose your carrier and switch whenever needed with zero friction. All are equipped with dual SIM slots, with one SIM replaceable by eSIM, and are fully unlocked: any major carrier, any type of deployment, with one piece of hardware.&lt;/p&gt;
    &lt;p&gt;The UniFi 5G lineup brings sleek design, powerful performance, easy installation, and genuine WAN flexibility to every deployment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46157594</guid><pubDate>Fri, 05 Dec 2025 07:06:38 +0000</pubDate></item><item><title>Kenyan court declares law banning seed sharing unconstitutional</title><link>https://apnews.com/article/kenya-seed-sharing-law-ruling-ad4df5a364299b3a9f8515c0f52d5f80</link><description>&lt;doc fingerprint="d8e624ea1bbbd655"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Kenyan court declares law banning seed sharing unconstitutional&lt;/head&gt;
    &lt;p&gt;KISUMU, Kenya (AP) — A high court in Kenya on Thursday declared unconstitutional sections of a seed law that prevented farmers from sharing and selling indigenous seeds in what food campaigners have called a landmark win for food security.&lt;/p&gt;
    &lt;p&gt;Farmers in Kenya could face up to two years’ imprisonment and a fine of 1 million Kenya shillings ($7,700) for sharing seeds through their community seed banks, according to a seed law signed in 2012.&lt;/p&gt;
    &lt;p&gt;Justice Rhoda Rutto on Thursday said sections of the seed law that gave government officials powers to raid seed banks and seize seeds were also unconstitutional.&lt;/p&gt;
    &lt;p&gt;The law was introduced as a measure to curb growing sale of counterfeit seeds that were causing loses in the agricultural sector and gave sole seed trading rights to licensed companies.&lt;/p&gt;
    &lt;p&gt;The case had been filed by 15 smallholder farmers, who are members of community seed banks that have been in operation for years, preserving and sharing seeds among colleagues.&lt;/p&gt;
    &lt;p&gt;A farmer, Samuel Wathome, who was among the 15, said the old farming practices had been vindicated.&lt;/p&gt;
    &lt;p&gt;“My grandmother saved seeds, and today the court has said I can do the same for my grandchildren without fear of the police or of prison,” he said.&lt;/p&gt;
    &lt;p&gt;Elizabeth Atieno, a food campaigner at Greenpeace Africa, called the win a “victory for our culture, our resilience, and our future.”&lt;/p&gt;
    &lt;p&gt;“By validating indigenous seeds, the court has struck a blow against the corporate capture of our food system. We can finally say that in Kenya, feeding your community with climate-resilient, locally adapted seeds is no longer a crime,” she said.&lt;/p&gt;
    &lt;p&gt;Food campaigners have in the past encouraged governments to work with farmers to preserve indigenous seeds as a way of ensuring food security by offering farmers more plant varieties.&lt;/p&gt;
    &lt;p&gt;Indigenous seeds are believed to be drought resistant and adaptable to the climate conditions of their native areas, and hence often outperform hybrid seeds.&lt;/p&gt;
    &lt;p&gt;Kenya has a national seed bank based near the capital Nairobi where indigenous seeds are stored in cold rooms, but farmers say community seed banks are equally important for variety and proximity to the farmer.&lt;/p&gt;
    &lt;p&gt;The country has faced challenges in the seed sector where counterfeit seeds were sold to farmers, leading to losses amounting to millions of shillings in a country that relies on rain-fed agriculture.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46158578</guid><pubDate>Fri, 05 Dec 2025 09:09:25 +0000</pubDate></item><item><title>Sugars, Gum, Stardust Found in NASA's Asteroid Bennu Samples</title><link>https://www.nasa.gov/missions/osiris-rex/sugars-gum-stardust-found-in-nasas-asteroid-bennu-samples/</link><description>&lt;doc fingerprint="a654d1fe9bb70191"&gt;
  &lt;main&gt;
    &lt;p&gt;The asteroid Bennu continues to provide new clues to scientists’ biggest questions about the formation of the early solar system and the origins of life. As part of the ongoing study of pristine samples delivered to Earth by NASA’s OSIRIS-REx (Origins, Spectral Interpretation, Resource Identification, and Security-Regolith Explorer) spacecraft, three new papers published Tuesday by the journals Nature Geosciences and Nature Astronomy present remarkable discoveries: sugars essential for biology, a gum-like substance not seen before in astromaterials, and an unexpectedly high abundance of dust produced by supernova explosions.&lt;/p&gt;
    &lt;p&gt;Sugars essential to life&lt;/p&gt;
    &lt;p&gt;Scientists led by Yoshihiro Furukawa of Tohoku University in Japan found sugars essential for biology on Earth in the Bennu samples, detailing their findings in the journal Nature Geoscience. The five-carbon sugar ribose and, for the first time in an extraterrestrial sample, six-carbon glucose were found. Although these sugars are not evidence of life, their detection, along with previous detections of amino acids, nucleobases, and carboxylic acids in Bennu samples, show building blocks of biological molecules were widespread throughout the solar system.&lt;/p&gt;
    &lt;p&gt;For life on Earth, the sugars deoxyribose and ribose are key building blocks of DNA and RNA, respectively. DNA is the primary carrier of genetic information in cells. RNA performs numerous functions, and life as we know it could not exist without it. Ribose in RNA is used in the molecule’s sugar-phosphate “backbone” that connects a string of information-carrying nucleobases.&lt;/p&gt;
    &lt;p&gt;“All five nucleobases used to construct both DNA and RNA, along with phosphates, have already been found in the Bennu samples brought to Earth by OSIRIS-REx,” said Furukawa. “The new discovery of ribose means that all of the components to form the molecule RNA are present in Bennu.”&lt;/p&gt;
    &lt;p&gt;The discovery of ribose in asteroid samples is not a complete surprise. Ribose has previously been found in two meteorites recovered on Earth. What is important about the Bennu samples is that researchers did not find deoxyribose. If Bennu is any indication, this means ribose may have been more common than deoxyribose in environments of the early solar system.&lt;/p&gt;
    &lt;p&gt;Researchers think the presence of ribose and lack of deoxyribose supports the “RNA world” hypothesis, where the first forms of life relied on RNA as the primary molecule to store information and to drive chemical reactions necessary for survival.&lt;/p&gt;
    &lt;p&gt;“Present day life is based on a complex system organized primarily by three types of functional biopolymers: DNA, RNA, and proteins,” explains Furukawa. “However, early life may have been simpler. RNA is the leading candidate for the first functional biopolymer because it can store genetic information and catalyze many biological reactions.”&lt;/p&gt;
    &lt;p&gt;The Bennu samples also contained one of the most common forms of “food” (or energy) used by life on Earth, the sugar glucose, which is the first evidence that an important energy source for life as we know it was also present in the early solar system.&lt;/p&gt;
    &lt;p&gt;Mysterious, ancient ‘gum’&lt;/p&gt;
    &lt;p&gt;A second paper, in the journal Nature Astronomy led by Scott Sandford at NASA’s Ames Research Center in California’s Silicon Valley and Zack Gainsforth of the University of California, Berkeley, reveals a gum-like material in the Bennu samples never seen before in space rocks – something that could have helped set the stage on Earth for the ingredients of life to emerge. The surprising substance was likely formed in the early days of the solar system, as Bennu’s young parent asteroid warmed.&lt;/p&gt;
    &lt;p&gt;Once soft and flexible, but since hardened, this ancient “space gum” consists of polymer-like materials extremely rich in nitrogen and oxygen. Such complex molecules could have provided some of the chemical precursors that helped trigger life on Earth, and finding them in the pristine samples from Bennu is important for scientists studying how life began and whether it exists beyond our planet.&lt;/p&gt;
    &lt;head rend="h2"&gt;On this primitive asteroid that formed in the early days of the solar system, we’re looking at events near the beginning of the beginning.&lt;/head&gt;
    &lt;p&gt;Scott SandFord&lt;/p&gt;
    &lt;p&gt;Astrophysicist, NASA's Ames Research Center&lt;/p&gt;
    &lt;p&gt;Bennu’s ancestral asteroid formed from materials in the solar nebula – the rotating cloud of gas and dust that gave rise to the solar system – and contained a variety of minerals and ices. As the asteroid began to warm, due to natural radiation, a compound called carbamate formed through a process involving ammonia and carbon dioxide. Carbamate is water soluble, but it survived long enough to polymerize, reacting with itself and other molecules to form larger and more complex chains impervious to water. This suggests that it formed before the parent body warmed enough to become a watery environment.&lt;/p&gt;
    &lt;p&gt;“With this strange substance, we’re looking at, quite possibly, one of the earliest alterations of materials that occurred in this rock,” said Sandford. “On this primitive asteroid that formed in the early days of the solar system, we’re looking at events near the beginning of the beginning.”&lt;/p&gt;
    &lt;p&gt;Using an infrared microscope, Sandford’s team selected unusual, carbon-rich grains containing abundant nitrogen and oxygen. They then began what Sandford calls “blacksmithing at the molecular level,” using the Molecular Foundry at Lawrence Berkeley National Laboratory (Berkeley Lab) in Berkeley, California. Applying ultra-thin layers of platinum, they reinforced a particle, welded on a tungsten needle to lift the tiny grain, and shaved the fragment down using a focused beam of charged particles.&lt;/p&gt;
    &lt;p&gt;When the particle was a thousand times thinner than a human hair, they analyzed its composition via electron microscopy at the Molecular Foundry and X-ray spectroscopy at Berkeley Lab’s Advanced Light Source. The ALS’s high spatial resolution and sensitive X-ray beams enabled unprecedented chemical analysis.&lt;/p&gt;
    &lt;p&gt;“We knew we had something remarkable the instant the images started to appear on the monitor,” said Gainsforth. “It was like nothing we had ever seen, and for months we were consumed by data and theories as we attempted to understand just what it was and how it could have come into existence.”&lt;/p&gt;
    &lt;p&gt;The team conducted a slew of experiments to examine the material’s characteristics. As the details emerged, the evidence suggested the strange substance had been deposited in layers on grains of ice and minerals present in the asteroid.&lt;/p&gt;
    &lt;p&gt;It was also flexible – a pliable material, similar to used gum or even a soft plastic. Indeed, during their work with the samples, researchers noticed the strange material was bendy and dimpled when pressure was applied. The stuff was translucent, and exposure to radiation made it brittle, like a lawn chair left too many seasons in the sun.&lt;/p&gt;
    &lt;p&gt;“Looking at its chemical makeup, we see the same kinds of chemical groups that occur in polyurethane on Earth,” said Sandford, “making this material from Bennu something akin to a ‘space plastic.’”&lt;/p&gt;
    &lt;p&gt;The ancient asteroid stuff isn’t simply polyurethane, though, which is an orderly polymer. This one has more “random, hodgepodge connections and a composition of elements that differs from particle to particle,” said Sandford. But the comparison underscores the surprising nature of the organic material discovered in NASA’s asteroid samples, and the research team aims to study more of it.&lt;/p&gt;
    &lt;p&gt;By pursuing clues about what went on long ago, deep inside an asteroid, scientists can better understand the young solar system – revealing the precursors to and ingredients of life it already contained, and how far those raw materials may have been scattered, thanks to asteroids much like Bennu.&lt;/p&gt;
    &lt;p&gt;Abundant supernova dust&lt;/p&gt;
    &lt;p&gt;Another paper in the journal Nature Astronomy, led by Ann Nguyen of NASA’s Johnson Space Center in Houston, analyzed presolar grains – dust from stars predating our solar system – found in two different rock types in the Bennu samples to learn more about where its parent body formed and how it was altered by geologic processes. It is believed that presolar dust was generally well-mixed as our solar system formed. The samples had six-times the amount of supernova dust than any other studied astromaterial, suggesting the asteroid’s parent body formed in a region of the protoplanetary disk enriched in the dust of dying stars.&lt;/p&gt;
    &lt;p&gt;The study also reveals that, while Bennu’s parent asteroid experienced extensive alteration by fluids, there are still pockets of less-altered materials within the samples that offer insights into its origin.&lt;/p&gt;
    &lt;p&gt;“These fragments retain a higher abundance of organic matter and presolar silicate grains, which are known to be easily destroyed by aqueous alteration in asteroids,” said Nguyen. “Their preservation in the Bennu samples was a surprise and illustrates that some material escaped alteration in the parent body. Our study reveals the diversity of presolar materials that the parent accreted as it was forming.”&lt;/p&gt;
    &lt;p&gt;NASA’s Goddard Space Flight Center provided overall mission management, systems engineering, and the safety and mission assurance for OSIRIS-REx. Dante Lauretta of the University of Arizona, Tucson, is the principal investigator. The university leads the science team and the mission’s science observation planning and data processing. Lockheed Martin Space in Littleton, Colorado, built the spacecraft and provided flight operations. Goddard and KinetX Aerospace were responsible for navigating the OSIRIS-REx spacecraft. Curation for OSIRIS-REx takes place at NASA’s Johnson Space Center in Houston. International partnerships on this mission include the OSIRIS-REx Laser Altimeter instrument from CSA (Canadian Space Agency) and asteroid sample science collaboration with JAXA’s (Japan Aerospace Exploration Agency’s) Hayabusa2 mission. OSIRIS-REx is the third mission in NASA’s New Frontiers Program, managed by NASA’s Marshall Space Flight Center in Huntsville, Alabama, for the agency’s Science Mission Directorate in Washington.&lt;/p&gt;
    &lt;p&gt;For more information on the OSIRIS-REx mission, visit:&lt;/p&gt;
    &lt;p&gt;https://www.nasa.gov/osiris-rex&lt;/p&gt;
    &lt;p&gt;Karen Fox / Molly Wasser&lt;lb/&gt;Headquarters, Washington&lt;lb/&gt;202-285-5155 / 240-419-1732&lt;lb/&gt;karen.c.fox@nasa.gov / molly.l.wasser@nasa.gov&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46160239</guid><pubDate>Fri, 05 Dec 2025 12:12:52 +0000</pubDate></item><item><title>Netflix to Acquire Warner Bros</title><link>https://about.netflix.com/en/news/netflix-to-acquire-warner-bros</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46160315</guid><pubDate>Fri, 05 Dec 2025 12:21:19 +0000</pubDate></item><item><title>Making RSS More Fun</title><link>https://matduggan.com/making-rss-more-fun/</link><description>&lt;doc fingerprint="2b4893113586319f"&gt;
  &lt;main&gt;
    &lt;p&gt;I don't like RSS readers. I know, this is blasphemous especially on a website where I'm actively encouraging you to subscribe through RSS. As someone writing stuff, RSS is great for me. I don't have to think about it, the requests are pretty light weight, I don't need to think about your personal data or what client you are using. So as a protocol RSS is great, no notes.&lt;/p&gt;
    &lt;p&gt;However as something I'm going to consume, it's frankly a giant chore. I feel pressured by RSS readers, where there is this endlessly growing backlog of things I haven't read. I rarely want to read all of a websites content from beginning to end, instead I like to jump between them. I also don't really care if the content is chronological, like an old post about something interesting isn't less compelling to me than a newer post.&lt;/p&gt;
    &lt;p&gt;What I want, as a user experience, is something akin to TikTok. The whole appeal of TikTok, for those who haven't wasted hours of their lives on it, is that I get served content based on an algorithm that determines what I might think is useful or fun. However what I would like is to go through content from random small websites. I want to sit somewhere and passively consume random small creators content, then upvote some of that content and the service should show that more often to other users. That's it. No advertising, no collecting tons of user data about me, just a very simple "I have 15 minutes to kill before the next meeting, show me some random stuff."&lt;/p&gt;
    &lt;p&gt;In this case the "algorithm" is pretty simple: if more people like a thing, more people see it. But with Google on its way to replacing search results with LLM generated content, I just wanted to have something that let me play around with the small web the way that I used to.&lt;/p&gt;
    &lt;p&gt;There actually used to be a service like this called StumbleUpon which was more focused on pushing users towards popular sites. It has been taken down, presumably because there was no money in a browser plugin that sent users to other websites whose advertising you didn't control.&lt;/p&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;You can go download the Firefox extension now and try this out and skip the rest of this if you want. https://timewasterpro.xyz/ If you hate it or find problems, let me know on Mastodon. https://c.im/@matdevdug&lt;/p&gt;
    &lt;head rend="h3"&gt;Functionality&lt;/head&gt;
    &lt;p&gt;So I wanted to do something pretty basic. You hit a button, get served a new website. If you like the website, upvote it, otherwise downvote it. If you think it has objectionable content then hit report. You have to make an account (because I couldn't think of another way to do it) and then if you submit links and other people like it, you climb a Leaderboard.&lt;/p&gt;
    &lt;p&gt;On the backend I want to (very slowly so I don't cost anyone a bunch of money) crawl a bunch of RSS feeds, stick the pages in a database and then serve them up to users. Then I want to track what sites get upvotes and return those more often to other users so that "high quality" content shows up more often. "High quality" would be defined by the community or just me if I'm the only user.&lt;/p&gt;
    &lt;p&gt;It's pretty basic stuff, most of it copied from tutorials scattered around the Internet. However I really want to drive home to users that this is not a Serious Thing. I'm not a company, this isn't a new social media network, there are no plans to "grow" this concept beyond the original idea unless people smarter than me ping with me ideas. So I found this amazing CSS library: https://sakofchit.github.io/system.css/&lt;/p&gt;
    &lt;p&gt;The Apple's System OS design from the late-80s to the early 90s was one of my personal favorites and I think would send a strong signal to a user that this is not a professional, modern service.&lt;/p&gt;
    &lt;p&gt;Great, the basic layout works. Let's move on!&lt;/p&gt;
    &lt;head rend="h3"&gt;Backend&lt;/head&gt;
    &lt;p&gt;So I ended up doing FastAPI because it's very easy to write. I didn't want to spend a ton of time writing the API because I doubt I nailed the API design on the first round. I use sqlalchemy for the database. The basic API layout is as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;admin - mostly just generating read-only reports of like "how many websites are there"&lt;/item&gt;
      &lt;item&gt;leaderboard - So this is my first attempt at trying to get users involved. Submit a website that other people like? Get points, climb leaderboard.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The source for the RSS feeds came from the (very cool) Kagi small web Github. https://github.com/kagisearch/smallweb. Basically I assume that websites that have submitted their RSS feeds here are cool with me (very rarely) checking for new posts and adding them to my database. If you want the same thing as this does, but as an iFrame, that's the Kagi small web service.&lt;/p&gt;
    &lt;p&gt;The scraping work is straightforward. We make a background worker, they grab 5 feeds every 600 seconds, they check for new content on each feed and then wait until the 600 seconds has elapsed to grab 5 more from the smallweb list of RSS feeds. Since we have a lot of feeds, this ends up look like we're checking for new content less than once a day which is the interval that I want.&lt;/p&gt;
    &lt;p&gt;Then we write it out to a sqlite database and basically track "has this URL been reported", if so, put it into a review queue and then how many times this URL has been liked or disliked. I considered a "real" database but honestly sqlite is getting more and more scalable every day and its impossible to beat the immediate start up and functionality. Plus very easy to back up to encrypted object storage which is super nice for a hobby project where you might wipe the prod database at any moment.&lt;/p&gt;
    &lt;p&gt;In terms of user onboarding I ended up doing the "make an account with an email, I send a link to verify the email". I actually hate this flow and I don't really want to know a users email. I never need to contact you and there's not a lot associated with your account, which makes this especially silly. I have a ton of email addresses and no real "purpose" in having them. I'd switch to Login with Apple, which is great from a security perspective but not everybody has an Apple ID.&lt;/p&gt;
    &lt;p&gt;I also did a passkey version, which worked fine but the OSS passkey handling was pretty rough still and most people seem to be using a commercial service that handled the "do you have the passkey? Great, if not, fall back to email" flow. I don't really want to do a big commercial login service for a hobby application.&lt;/p&gt;
    &lt;p&gt;Auth is a JWT, which actually was a pain and I regret doing it. I don't know why I keep reaching for JWTs, they're a bad user experience and I should stop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Can I just have the source code?&lt;/head&gt;
    &lt;p&gt;I'm more than happy to release the source code once I feel like the product is in a somewhat stable shape. I'm still ripping down and rewriting relatively large chunks of it as I find weird behavior I don't like or just decide to do things a different way.&lt;/p&gt;
    &lt;p&gt;In the end it does seem to do whats on the label. We have over 600,000 individual pages indexed.&lt;/p&gt;
    &lt;head rend="h3"&gt;So how is it to use?&lt;/head&gt;
    &lt;p&gt;Honestly I've been pretty pleased. But there are some problems.&lt;/p&gt;
    &lt;p&gt;First I couldn't find a reliable way of switching the keyboard shortcuts to be Mac/Windows specific. I found some options for querying platform but they didn't seem to work, so I ended up just hardcoding them as Alt which is not great.&lt;/p&gt;
    &lt;p&gt;The other issue is that when you are making an extension, you spend a long time working with these manifests.json. The specific part I really wasn't sure about was:&lt;/p&gt;
    &lt;code&gt;"browser_specific_settings": {
    "gecko": {
      "id": "[email protected]",
      "strict_min_version": "80.0",
      "data_collection_permissions": {
        "required": ["authenticationInfo"]
      }
    }
  }&lt;/code&gt;
    &lt;p&gt;I'm not entirely sure if that's all I'm doing? I think so from reading the docs.&lt;/p&gt;
    &lt;p&gt;Anyway I built this mostly for me. I have no idea if anybody else will enjoy it. But if you are bored I encourage you to give it a try. It should be pretty light weight and straight-forward if you crack open the extension and look at it. I'm not loading any analytics into the extension so basically until people complain about it, I don't really know if its going well or not.&lt;/p&gt;
    &lt;head rend="h3"&gt;Future stuff&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I need to sort stuff into categories so that you get more stuff in genres you like. I don't 100% know how to do that, maybe there is a way to scan a website to determine the "types" of content that is on there with machine learning? I'm still looking into it.&lt;/item&gt;
      &lt;item&gt;There's a lot of junk in there. I think if we reach a certain number of downvotes I might put it into a special "queue".&lt;/item&gt;
      &lt;item&gt;I want to ensure new users see the "best stuff" early on but there isn't enough data to determine "best vs worst".&lt;/item&gt;
      &lt;item&gt;I wish there were more independent photography and science websites. Also more crafts. That's not really a "future thing", just me putting a hope out into the universe. Non-technical beta testers get overwhelmed by technical content.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46160698</guid><pubDate>Fri, 05 Dec 2025 13:00:28 +0000</pubDate></item><item><title>Most technical problems are people problems</title><link>https://blog.joeschrag.com/2023/11/most-technical-problems-are-really.html</link><description>&lt;doc fingerprint="2ed1d053672c65da"&gt;
  &lt;main&gt;
    &lt;p&gt;I once worked at a company which had an enormous amount of technical debt - millions of lines of code, no unit tests, based on frameworks that were well over a decade out of date. On one specific project, we had a market need to get some Windows-only modules running on Linux, and rather than cross-compiling, another team had simply copied &amp;amp; pasted a few hundred thousand lines of code, swapping Windows-specific components for Linux-specific.&lt;/p&gt;
    &lt;p&gt;For the non-technical reader, this is an enormous problem because now two versions of the code exist. So, all features &amp;amp; bug fixes must be solved in two separate codebases that will grow apart over time. When I heard about this, a young &amp;amp; naive version of me set out to fix the situation....&lt;/p&gt;
    &lt;head rend="h2"&gt;People Problems&lt;/head&gt;
    &lt;p&gt;Tech debt projects are always a hard sell to management, because even if everything goes flawlessly, the code just does roughly what it did before. This project was no exception, and the optics weren't great. I did as many engineers do and "ignored the politics", put my head down, and got it done. But, the project went long, and I lost a lot of clout in the process.&lt;/p&gt;
    &lt;p&gt;I realized I was essentially trying to solve a people problem with a technical solution. Most of the developers at this company were happy doing the same thing today that they did yesterday...and five years ago. As Andrew Harmel-Law points out, code tends to follow the personalities of the people that wrote it. The code was calcified because the developers were also. Personality types who dislike change tend not to design their code with future change in mind.&lt;/p&gt;
    &lt;p&gt;Most technical problems are really people problems. Think about it. Why does technical debt exist? Because requirements weren't properly clarified before work began. Because a salesperson promised an unrealistic deadline to a customer. Because a developer chose an outdated technology because it was comfortable. Because management was too reactive and cancelled a project mid-flight. Because someone's ego wouldn't let them see a better way of doing things.&lt;/p&gt;
    &lt;p&gt;The core issue with the project was that admitting the need for refactoring was also to admit that the way the company was building software was broken and that individual skillsets were sorely out of date. My small team was trying to fix one module of many, while other developers were writing code as they had been for decades. I had one developer openly tell me, "I don't want to learn anything new." I realized that you'll never clean up tech debt faster than others create it. It is like triage in an emergency room, you must stop the bleeding first, then you can fix whatever is broken.&lt;/p&gt;
    &lt;head rend="h2"&gt;An Ideal World&lt;/head&gt;
    &lt;p&gt;The project also disabused me of the engineer's ideal of a world in which engineering problems can be solved in a vacuum - staying out of "politics" and letting the work speak for itself - a world where deadlines don't exist...and let's be honest, neither do customers. This ideal world rarely exists. The vast majority of projects have non-technical stakeholders, and telling them "just trust me; we're working on it" doesn't cut it. I realized that the perception that your team is getting a lot done is just as important as getting a lot done.&lt;/p&gt;
    &lt;p&gt;Non-technical people do not intuitively understand the level of effort required or the need for tech debt cleanup; it must be communicated effectively by engineering - in both initial estimates &amp;amp; project updates. Unless leadership has an engineering background, the value of the technical debt work likely needs to be quantified and shown as business value.&lt;/p&gt;
    &lt;head rend="h2"&gt;Heads Up&lt;/head&gt;
    &lt;p&gt;Perhaps these are the lessons that prep one for more senior positions. In my opinion, anyone above senior engineer level needs to know how to collaborate cross-functionally, regardless of whether they choose a technical or management track. Schools teach Computer Science, not navigating personalities, egos, and personal blindspots.&lt;/p&gt;
    &lt;p&gt;I have worked with some incredible engineers, better than myself - the type that have deep technical knowledge on just about any technology you bring up. When I was younger, I wanted to be that engineer - the "engineer's engineer". But I realize now, that is not my personality. I'm too ADD for that. :)&lt;/p&gt;
    &lt;p&gt;For all of their (considerable) strengths, more often than not, those engineers shy away from the interpersonal. The tragedy is that they are incredibly productive ICs, but may fail with bigger initiatives because they are only one person - a single processor core can only go so fast. Perhaps equally valuable is the "heads up coder" - the person who is deeply technical, but also able to pick their head up &amp;amp; see project risks coming (technical &amp;amp; otherwise) and steer the team around them.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46160773</guid><pubDate>Fri, 05 Dec 2025 13:07:59 +0000</pubDate></item><item><title>Influential study on glyphosate safety retracted 25 years after publication</title><link>https://www.lemonde.fr/en/environment/article/2025/12/03/influential-study-on-glyphosate-safety-retracted-25-years-after-publication_6748114_114.html</link><description>&lt;doc fingerprint="364059ff18954fcc"&gt;
  &lt;main&gt;
    &lt;p&gt;A quarter-century after its publication, one of the most influential research articles on the potential carcinogenicity of glyphosate has been retracted for "several critical issues that are considered to undermine the academic integrity of this article and its conclusions." In a retraction notice dated Friday, November 28, the journal Regulatory Toxicology and Pharmacology announced that the study, published in April 2000 and concluding the herbicide was safe, has been removed from its archives. The disavowal comes 25 years after publication and eight years after thousands of internal Monsanto documents were made public during US court proceedings (the "Monsanto Papers"), revealing that the actual authors of the article were not the listed scientists – Gary M. Williams (New York Medical College), Robert Kroes (Ritox, Utrecht University, Netherlands), and Ian C. Munro (Intertek Cantox, Canada) – but rather Monsanto employees.&lt;/p&gt;
    &lt;p&gt;Known as "ghostwriting," this practice is considered a form of scientific fraud. It involves companies paying researchers to sign their names to research articles they did not write. The motivation is clear: When a study supports the safety of a pesticide or drug, it appears far more credible if not authored by scientists employed by the company marketing the product.&lt;/p&gt;
    &lt;p&gt;You have 73.89% of this article left to read. The rest is for subscribers only.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46161125</guid><pubDate>Fri, 05 Dec 2025 13:39:04 +0000</pubDate></item><item><title>Emerge Career (YC S22) Is Hiring</title><link>https://www.ycombinator.com/companies/emerge-career/jobs/qQhLEmC-founding-design-engineer</link><description>&lt;doc fingerprint="3d98101009444eae"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Who We Are:&lt;/head&gt;
      &lt;p&gt;Emerge Career’s mission is to break the cycle of poverty and incarceration. We’re not just building software; we’re creating pathways to real second chances. Through an all-in-one platform deeply embedded within the criminal justice system, we recruit, train, and place justice-impacted individuals into life-changing careers.&lt;/p&gt;
      &lt;p&gt;Our vision is to become the country’s unified workforce development system, replacing disconnected brick-and-mortar job centers with one integrated, tech-powered solution that meets low-income individuals exactly where they are. Today, the federal government spends billions annually on education and training programs, yet only about 70% of participants graduate, just 38.6% secure training-related employment, and average first-year earnings hover around $34,708.&lt;/p&gt;
      &lt;p&gt;By contrast, our seven-person team has already outperformed the job centers in two entire states (Vermont and South Dakota) in just the past year. With an 89% graduation rate and 92% of graduates securing training-related employment, our alumni aren’t just getting jobs—they’re launching new lives with average first-year earnings of $77,352. The results speak for themselves, and we’re just getting started.&lt;/p&gt;
      &lt;p&gt;Before Emerge, our founders Zo and Gabe co-founded Ameelio, an award-winning tech nonprofit that is dismantling the prison communication duopoly. Backed by tech luminaries like Reid Hoffman, Vinod Khosla, and Jack Dorsey, and by major criminal-justice philanthropies such as Arnold Ventures and the Mellon Foundation, Ameelio became a recognized leader in the space. Because of this experience both Zo and Gabe understood what it took to create change from within the system. After serving over 1M people impacted by incarceration, they witnessed firsthand the gap in second-chance opportunities and the chronic unemployment plaguing those impacted by the justice system. Emerge Career is committed to solving this issue.&lt;/p&gt;
      &lt;p&gt;Our students are at the heart of our work. Their journeys have captured national attention on CBS, NBC, and in The Boston Globe, and our programs now serve entire states and cities. And we’re not doing it alone: our vision has attracted support from Alexis Ohanian (776), Michael Seibel, Y Combinator, the Opportunity Fund, and public figures like Diana Taurasi, Deandre Ayton, and Marshawn Lynch. All of us believe that, with the right mix of technology and hands-on practice, we can redefine workforce development and deliver true second chances at scale.&lt;/p&gt;
      &lt;head rend="h1"&gt;Why We Do This:&lt;/head&gt;
      &lt;p&gt;Emerge Career was designed to tackle two systemic issues: recidivism, fueled by post-incarceration unemployment and poverty, and labor shortages in key industries. Over 60% of formerly incarcerated people remain unemployed a year after incarceration, seeking work but not finding it. The reality is shocking, workforce development programs are severely limited inside prison, with only one-third of incarcerated people ever participating. To worsen, the available prison jobs offer meager wages, often less than $1 per hour, and often do not equip individuals with the skills for long-term stable employment.&lt;/p&gt;
      &lt;head rend="h1"&gt;About the Role&lt;/head&gt;
      &lt;p&gt;We call this a Founding Design Engineer role—even three years in and with multiple contracts under our belt—for two reasons. First, you’ll be our very first engineer, joining our co-founder, who’s built the entire platform solo to date. Second, our growth is now outpacing our systems, and we can’t keep up on maintenance alone. We’re at a critical juncture: we can either hire someone to simply care for what exists, or we can bring on a talent who believes that, with the right blend of technology and hands-on practice, we can unify the workforce-development system and deliver second chances at true scale. We hope that can be you.&lt;/p&gt;
      &lt;p&gt;This is not a traditional engineering job. You’ll do high-impact technical work, but just as often you’ll be on the phone with a student, writing documentation, debugging support issues, or figuring out how to turn a one-off solution into a repeatable system. You’ll ship features, talk to users, and fix what’s broken, whether that’s in the product or in the process. You’ll build things that matter, not just things that are asked for.&lt;/p&gt;
      &lt;p&gt;This role blends engineering, product, support, and program operations. We’re looking for someone who is energized by ownership, obsessed with user outcomes, and excited to work across domains to make things better. If you’re the kind of person who wants to be hands-on with everything—students, code, strategy, and execution—you’ll thrive here.&lt;/p&gt;
      &lt;head rend="h1"&gt;Who You Are:&lt;/head&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;You love supporting other people’s growth. This role will feel like case work at times, and you’re drawn to that. You’ve dedicated your life volunteering, working in social impact, or finding ways to make the playing field more fair. You find joy in helping others rise. You don’t hesitate to call, text, or meet with a student who needs you. You show up consistently, personally, and with heart.&lt;/item&gt;
        &lt;item&gt;You believe everyone deserves a second chance. You treat everyone with dignity. You know how to meet people exactly where they are—with empathy and compassion—helping create a space where everyone feels seen and valued, regardless of their background..&lt;/item&gt;
        &lt;item&gt;You identify yourself as a cracked engineer. You love finding a way or making one. You take extreme ownership of ideas, driving them to completion even when others need convincing. Every time you hit a wall, you think of three new ways to solve the issue.&lt;/item&gt;
        &lt;item&gt;You are tech-forward, but not tech-first. You look for ways to automate and scale, but you know not everything should be automated. You believe that with the right builder mindset, one coach can support hundreds of individuals—but you also understand that in a program like ours, many moments require a human touch. You know when to hand it to a system, and when to pick up the phone.&lt;/item&gt;
        &lt;item&gt;You are entrepreneurial. You’re scrappy, resourceful, autonomous, and low-maintenance. You know process matters—but at this stage, speed and iteration matter more. You’re comfortable building quickly and changing procedures often to get to the right solution. You roll up your sleeves and solve problems. No job is too small.&lt;/item&gt;
        &lt;item&gt;You play to win. You stay optimistic when things get tough and keep moving when others slow down. You’re not rattled by change or new ideas. You don’t need to agree with everything, but you bring a “yes, and” mindset that helps ideas grow instead of shutting them down.&lt;/item&gt;
        &lt;item&gt;You work hard. You show up early, stay late, and do what needs to get done—no ego, no excuses. You don’t wait around or ask for permission. This isn’t a 9-to-5. The team puts in 10+ hour days because we care about the mission and each other. If that sounds miserable, this isn’t for you. If it sounds exciting, you’ll fit right in.&lt;/item&gt;
        &lt;item&gt;You are a straight shooter. You don’t shy away from hard conversations—internally or externally. You bring clarity, care, and accountability to every interaction.&lt;/item&gt;
        &lt;item&gt;You love learning. You understand that recent advancements in AI have shifted the way we work and what it means to be a high performer. You tinker with new tools. You enjoy being an early adopter. You’re always rethinking and optimizing how you work so you can keep leveling up. Nobody needs to tell you to keep upping your game.&lt;/item&gt;
        &lt;item&gt;You have an eye for operational detail. This doesn’t mean you’re simply organized. At Emerge, operational excellence isn’t just about efficiency—it’s about protecting the real lives and futures of the people in our programs. You have an almost paranoid attention to detail, because you understand that even small oversights have real, human consequences.&lt;/item&gt;
        &lt;item&gt;You are a clear writer. This doesn’t mean you need to craft the next great novel, but you must communicate ideas simply and clearly. You value precision, clarity, and brevity. You understand that good writing reduces confusion, accelerates decisions, and ensures everyone stays aligned, especially in high-stakes environments like ours.&lt;/item&gt;
        &lt;item&gt;You are a strong prompter. You’ve seen firsthand how a few thoughtful prompts can transform messy tasks into scalable, repeatable AI workflows. You love tinkering with prompts, contexts, and configurations to get exactly the right outputs, and you take pride in turning cutting-edge AI into practical processes that anyone can use.&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h1"&gt;Requirements&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Willing to relocate and work in-person in New York City&lt;/item&gt;
        &lt;item&gt;Experience taking a project from 0 to 1. You might have led a project, been a founder previously, built an impressive side project, or been one of the first 10 employees at an early stage startup&lt;/item&gt;
        &lt;item&gt;You love working with React and Typescript&lt;/item&gt;
        &lt;item&gt;Experience with Figma&lt;/item&gt;
        &lt;item&gt;Experience collaborating with operations or support teams&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Bonus Points&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Experience in ed-tech&lt;/item&gt;
        &lt;item&gt;Experience with UX research&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h1"&gt;What you will be doing&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Coaching students. You’ll support students throughout their training journey—not just by building tools, but by directly engaging with them. This means texting, calling, and helping students one-on-one when needed. At the same time, you’ll take what you learn from those interactions and turn it into scalable systems and smart automations. You’ll be doing both engineering and non-engineering work to make sure students succeed and the product keeps improving.&lt;/item&gt;
        &lt;item&gt;Talking to students. Good founding engineers read feedback and iterate quickly. Great founding engineers have users they're friendly with, talk with them frequently, bounce ideas off them, and iterate with them when they ship new things.&lt;/item&gt;
        &lt;item&gt;Doing support. This is an engineering role with key program management responsibilities. You’ll work directly with students every day. That requires patience, empathy, and a willingness to meet people where they are. You’ll help investigate and resolve product issues, and you’ll take ownership of making the product better through what you learn.&lt;/item&gt;
        &lt;item&gt;Documenting your work and its impact. Our work is complex, spans months, and involves multiple teams. Clear documentation and communication are critical. You’ll be responsible for creating awareness when a change impacts operations, and for helping others understand how features affect different parts of training and service delivery. Precision matters.&lt;/item&gt;
        &lt;item&gt;Owning products and features end to end. You won’t just take tickets. You’ll originate ideas based on user conversations, your own instincts, and our larger strategy. You’ll test MVPs in production, iterate based on real feedback, and stay accountable to the long-term success of your work. We build in React and TypeScript. If you like shipping for the sake of it, this role isn’t for you. If you like shipping with purpose and ownership, you’ll love it here.&lt;/item&gt;
        &lt;item&gt;Implementing AI features and operational workflows. This is last for a reason. We don’t jump to automation. We do things manually first to fully understand the problem—then we build. If you care about applying AI meaningfully, not just for hype, this is the right place.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Benefits You’ll Receive: link&lt;/p&gt;
      &lt;p&gt;Start Date: ASAP&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46161460</guid><pubDate>Fri, 05 Dec 2025 14:06:53 +0000</pubDate></item></channel></rss>