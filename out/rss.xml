<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 03 Oct 2025 03:42:35 +0000</lastBuildDate><item><title>How Israeli actions caused famine in Gaza, visualized</title><link>https://www.cnn.com/2025/10/02/middleeast/gaza-famine-causes-vis-intl</link><description>&lt;doc fingerprint="edaed098b2dab5fa"&gt;
  &lt;main&gt;
    &lt;p&gt;Israel’s nearly two-year war pushed parts of Gaza into “man-made” famine, according to a report published in August by a United Nations-backed initiative, deepening the Palestinians’ struggle for survival under relentless bombing, mass displacement and the spread of disease.&lt;/p&gt;
    &lt;p&gt;The report by the Integrated Food Security Phase Classification (IPC), a UN-backed expert panel that assesses global food insecurity and malnutrition, helped to fuel growing international outcry over Israel’s campaign in Gaza following the Hamas-led October 7, 2023, attacks – and was cited by some of countries that recently made moves towards formally recognizing a Palestinian state. The IPC forecast that by the end of September nearly a third of Gaza’s total population would face famine conditions, although it has not yet provided an update on that forecast.&lt;/p&gt;
    &lt;p&gt;In Gaza governorate alone – the largest by population of five in the Gaza Strip – more than half a million people were condemned to a cycle of “starvation, destitution and death,” the IPC added. The Israeli assault on Gaza City, which Israeli Prime Minister Benjamin Netanyahu says is targeting one of Hamas’ “remaining strongholds” has choked relief operations for starving Palestinians, according to rights workers.&lt;/p&gt;
    &lt;p&gt;Michael Fakhri, the UN’s special rapporteur on the right to food, accused Israel of using hunger “as a weapon against Palestinians,” in violation of international law.&lt;/p&gt;
    &lt;p&gt;“Israel is using food and aid as a weapon to humiliate, weaken, displace and kill Palestinians in Gaza,” Fakhri told CNN on August 28.&lt;/p&gt;
    &lt;p&gt;Israel rejected the IPC’s findings, with the Israeli agency that oversees the entry of aid into Gaza claiming the report was “false” and based on “partial, biased” data “originating from Hamas.” Netanyahu slammed the UN-backed report, in a statement from his office, adding that “Israel does not have a policy of starvation.”&lt;/p&gt;
    &lt;p&gt;Israel has since insisted that it has stepped up the entry of aid into Gaza. But aid agencies say that Israel’s intensification of the war, particularly around Gaza City, has compounded the misery faced by Palestinians. Here is a look, in five charts, at how the situation described by the IPC materialized.&lt;/p&gt;
    &lt;head rend="h2"&gt;Famine projected to spread to central, southern Gaza&lt;/head&gt;
    &lt;p&gt;The IPC projected that famine would spread to Deir Al-Balah, central Gaza and further south, in Khan Younis by the end of September, affecting nearly 641,000 people.&lt;/p&gt;
    &lt;p&gt;Up to June 2026, at least 132,000 children under the age of five are expected to suffer from acute malnutrition, including more than 41,000 severe cases of children at heightened risk of death, the IPC added.&lt;/p&gt;
    &lt;p&gt;Under the IPC – a five-phase indicator used to measure the severity of food insecurity – a famine can only be declared if three thresholds are met: at least 20% of households face extreme food shortages, the proportion of children assessed as acutely malnourished reaches a certain threshold, and at least two in every 10,000 people die each day from starvation, or from malnutrition and disease.&lt;/p&gt;
    &lt;p&gt;Israel accused the IPC of lowering the second threshold of acutely malnourished children for a famine declaration, which the IPC has denied.&lt;/p&gt;
    &lt;p&gt;Researchers use three methods for assessing child malnutrition – either a child’s height and weight, their BMI, or a child’s mid-upper arm circumference, known as MUAC. The IPC used the latter, a metric employed since 2019, to determine that at least 15% of children aged six to 59 months have a mid-upper arm circumference of less than 125mm or edema, the agency told CNN. The thresholds for famine classification are “standard and were not modified for Gaza,” the IPC told CNN, adding that the MUAC metric “is the measurement most frequently available and has strong correlation with mortality outcomes,” and was also used in famine classifications in Sudan and South Sudan this decade.&lt;/p&gt;
    &lt;p&gt;Human rights advocates say Israel’s destruction of health infrastructure and intensified hostilities have hampered efforts to document the full scope of famine in Gaza.&lt;/p&gt;
    &lt;p&gt;After more than 700 days of war, 455 Palestinians have died of malnutrition or starvation, including 151 children, the health ministry in Gaza reported on October 1. One hundred and seventy-seven of the total number have died of malnutrition or starvation since the IPC confirmed famine on August 15, it said.&lt;/p&gt;
    &lt;head rend="h2"&gt;How much UN aid is getting into Gaza?&lt;/head&gt;
    &lt;p&gt;Israel’s vast web of bureaucratic impediments, including delayed approvals, arduous border checks and the arbitrary rejection of items, throttles the amount of aid that makes it to the other side of the border and sends food costs soaring, the UN and aid agencies say.&lt;/p&gt;
    &lt;p&gt;After visiting the region in late August, US Senators Chris Van Hollen and Jeff Merkley, both Democrats, warned that Netanyahu’s government was “implementing a plan to ethnically cleanse Gaza of Palestinians” and accused Israel of using food “as a weapon of war.” Israel has denied the allegations.&lt;/p&gt;
    &lt;p&gt;“The findings from our trip lead to the inescapable conclusion that the Netanyahu government’s war in Gaza has gone far beyond the targeting of Hamas to imposing collective punishment on the Palestinians there, with the goal of making life for them unsustainable,” said the report, published on September 11. “That is why it restricts the delivery of humanitarian assistance.”&lt;/p&gt;
    &lt;p&gt;Israeli authorities have said trucks “remain uncollected” at the border with Gaza – accusing the UN of failing to coordinate the entry of vehicles into the strip.&lt;/p&gt;
    &lt;p&gt;But Sam Rose, the acting director of affairs for the UN agency for Palestinian refugees (UNRWA) in Gaza, says Israel – which has near-total jurisdiction over what goods enter and exit Gaza – has controlled “to the calorie” the volume, type and overall flow of food into the enclave. “The system is designed not to function smoothly,” he said.&lt;/p&gt;
    &lt;p&gt;Israeli authorities “know and analyze each truck that goes into Gaza, the weight and the calories,” a senior official with COGAT, the Israeli agency that controls the entry of aid into the enclave, said in September. According to a COGAT statement published in response to the IPC famine declaration, “analysis of contents of food aid trucks that entered the Gaza Strip reveal that 4,400 calories per person per day entered Gaza since the beginning of August.”&lt;/p&gt;
    &lt;p&gt;However, as of May, Palestinians were consuming just 1,400 calories per day – or “67 per cent of what a human body needs to survive,” at 2,300 calories, the UN reported in June.&lt;/p&gt;
    &lt;p&gt;Last October, Israel’s government banned UNRWA from operating in areas under its control, a prohibition that went into effect in January, having accused the agency of failing to stop Hamas’ alleged theft of aid. An internal US government review found no evidence of widespread theft by Hamas of US-funded humanitarian aid in Gaza.&lt;/p&gt;
    &lt;p&gt;When the trickle of relief does enter the strip, aid workers face intensified hostilities, damaged roads and limited fuel supplies – impeding internal distribution efforts, minimizing viable routes and blocking access to displaced Palestinians, said Rose.&lt;/p&gt;
    &lt;head rend="h2"&gt;What other ways are there to receive aid?&lt;/head&gt;
    &lt;p&gt;Israel says UN aid makes up only part of the relief that gets into Gaza. A senior COGAT official told a briefing in early September that 27% of the trucks entering Gaza are UN vehicles, claiming it was “a lie” that the UN had brought in 600 aid trucks a day before the war.&lt;/p&gt;
    &lt;p&gt;“There is no famine in Gaza. Period,” the official said, adding that “Israel and the IDF are trying to strengthen the humanitarian situation in Gaza with partners.”&lt;/p&gt;
    &lt;p&gt;In May, the US and Israeli-backed Gaza Humanitarian Foundation (GHF) established a program that now plans to operate up to five distribution sites in the enclave, all but one in southern Gaza – which rely on private military contractors and largely replaced 400 UN-led hubs.&lt;/p&gt;
    &lt;p&gt;Relief and health workers say these other methods of delivering food in Gaza, including the GHF sites and aid pallet drops from planes, are dehumanizing and inaccessible for many Palestinians, and expose them to injury or death.&lt;/p&gt;
    &lt;p&gt;At least 1,172 people were killed “near militarized supply sites” between May 27 and September 9, the UN said on September 10, with another 1,084 deaths along convoy supply routes. In August, UN experts called for the immediate closure of GHF-operated sites in Gaza and accused Israeli forces of opening “indiscriminate fire” on people seeking aid there. The advocates warned the hubs are “especially difficult” for women, children, people with disabilities and elderly Palestinians to access.&lt;/p&gt;
    &lt;p&gt;GHF has defended its work in Gaza and said earlier in September that it was the only organization in Gaza able to deliver food “at scale without interference.” The organization also said that it had “repeatedly sought collaboration with UN agencies and international NGOs to deliver aid side-by-side” but that the UN had “declined those offers.” The Israeli military has acknowledged firing warning shots toward crowds in some instances and denied responsibility for other casualties near aid hubs.&lt;/p&gt;
    &lt;p&gt;The US and Israel plan to set up 12 additional sites across the enclave, an Israeli official told CNN in August. However, there is no indication that the new sites have been established. In September, GHF said it had sought IDF permission to open sites in northern Gaza but that Israel had not granted the permission.&lt;/p&gt;
    &lt;p&gt;“With parents injured and their siblings starving, many teenagers and young adults are taking the risk,” Mohammed Khaleel, an American surgeon who was deployed to Gaza earlier this year, told CNN in August.&lt;/p&gt;
    &lt;p&gt;“We’ve even heard some people report that they will go and accept their fate. Dying from a gunshot may be preferable to dying from starvation,” he added.&lt;/p&gt;
    &lt;head rend="h2"&gt;Farmland is shrinking and becoming increasingly inaccessible&lt;/head&gt;
    &lt;p&gt;Israel’s two-year offensive in Gaza had left just 1.5% of cropland accessible and undamaged as of July 28, according to the UN – largely preventing Palestinians from cultivating produce.&lt;/p&gt;
    &lt;p&gt;That destruction, coupled with Israel’s fishing ban and intensified assault in the north, has further limited the sources of food available to hundreds of thousands of displaced Palestinians.&lt;/p&gt;
    &lt;p&gt;“It is not by chance that Israel has focused its starvation tactics in northern Gaza,” Fakhri, the UN special rapporteur, said. “They have announced their intent to push people from the north to the south of Gaza… Just as now, the focus of their starvation campaign on Gaza City correlates with their invasion plans.”&lt;/p&gt;
    &lt;p&gt;The military’s invasion of Gaza City will collapse an “already fragile” aid supply chain, warned Arif Husain, chief economist at the World Food Programme.&lt;/p&gt;
    &lt;p&gt;Relief agencies need a ceasefire, unimpeded humanitarian access, large-scale multi-sector aid, protection of civilians and infrastructure – and restoration of commercial and local food systems – to reverse famine in Gaza, said Husain.&lt;/p&gt;
    &lt;p&gt;“We are already at the brink. Another escalation – especially in Gaza City – could push the situation into unimaginable catastrophe,” he added. “It will not only result in more deaths but destroy any foundation for future recovery.”&lt;/p&gt;
    &lt;p&gt;CNN’s Ibrahim Dahman, Kareem Khadder and Eyad Kourdi contributed reporting.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45447699</guid><pubDate>Thu, 02 Oct 2025 09:23:50 +0000</pubDate></item><item><title>N8n added native persistent storage with DataTables</title><link>https://community.n8n.io/t/data-tables-are-here/192256</link><description>&lt;doc fingerprint="bfbc187f418403ca"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Hey everyone &lt;/head&gt;
        &lt;head rend="h2"&gt;We’re super excited to share that starting with v1.113 we’re rolling out data tables (beta) to all plans. &lt;/head&gt;
        &lt;p&gt;Since the very beginning of n8n we’ve heard many of you mention the need for a proper table inside n8n to store data between workflow executions without needing to switch platforms or setting up credentials and now it’s finally here.&lt;/p&gt;
        &lt;p&gt;With data tables you can:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Save specific data from your workflow runs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Keep data around between multiple executions&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Avoid duplicate runs by tracking execution status&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Store reusable prompts for different workflows&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Collect evaluation data for your AI workflows&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Do lookups, merges, enhancements…&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;…and honestly, probably 100 other creative things we haven’t even thought of yet &lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
        &lt;p&gt; To make sure your instance stays performant, we’ve set a 50MB limit for everyone. If you’re self-hosting (and know what you’re doing), you can change that via the ENV variable &lt;code&gt;N8N_DATA_TABLES_MAX_SIZE_BYTES&lt;/code&gt;&lt;/p&gt;
        &lt;p&gt; Upgrade to 1.113, give data tables a spin, and let us know what you think! What’s missing? What would make it even more useful for you? We’re really curious to hear your ideas and thoughts! &lt;/p&gt;
        &lt;p&gt; Read more about the data tables in the docs here.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 38 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;liam
2&lt;/div&gt;
      &lt;p&gt; 7 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;dszp
3&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is absolutely awesome to see, I can’t wait to use these! It’s probably been my number one frustration that saving even a small amount of data between executions for all sorts of purposes requires either integrating PostgreSQL and dealing with schemas, using a third party database or API like Supabase (as handy as they are), or using variables that are powerful but are somewhat clumsy to instantiate and track since they only work in Code nodes and only save data for production executions, making testing hard. Hoping data-tables makes a ton of these things easier! Probably won’t run the new version until it’s in final release rather than pre-release, but this is awesome to see!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 4 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;bartv
5&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is really great - when I migrated from “the other platform” almost 4 years ago, I really felt the pain of not having a simple in-app data storage. I played around with Data tables this weekend and it’s just SUCH a good and fast experience! Kudos to our Product and Engineering teams &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Hey all,&lt;/p&gt;
        &lt;p&gt;IMPORTANT NOTE: There is an issue with very large SQLite databases that is causing instances to slow down. Out of an abundance of caution, we are unfortunately removing version 1.113.0 until we fix this issue. We hope to have this released again with a fix within the next couple of days.&lt;/p&gt;
        &lt;p&gt;Very sorry about this!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 10 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;This is great!&lt;/p&gt;
        &lt;p&gt;It´d be cool for self hosting to be able to add a second DB, where n8n pulls the data from. So one could have performance without having to set up each time a postgres connection.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;bartv
10&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Data tables is back on!&lt;/p&gt;
        &lt;p&gt;A patch was released earlier today. It has now been tested and we have high confidence. Please update to 1.113.1 (which is still in beta) to try this feature.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 4 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;TH1
11&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;is that Data Tables only available for the Cloud version? local host will not have Data Tables?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;liam
12&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;It’s on all plans (cloud and self hosted) starting on version 1.113.1 &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;Sujit
13&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;I am unable to see the data tables in my local self hosted n8n. I’ve also updated the docker image to pick the latest one. What am I missing?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Does this mean we can share data between multiple workflows now? This would make splitting up complex workflows across multiple workflows so much easier.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;I believe this is still only available in the beta version?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jabbson
16&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;The fact that the “latest” is not “1.113.1”. The latest is “the latest stable”, where 1.113.1 is not that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Very happy to see this feature. I’ve been testing it out, and was commenting feedback on Reddit but someone in the Discord server said the forums is the best place to post this instead. Here’s my list(so far)&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;When going to the Data Tables tab, “Create Table” is not default on the upper right button, it’s defaulted to “Create Workflow” instead.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Cannot change the data type after a column is created.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Cannot set any of the column’s as primary or unique such as the ID column (To prevent duplicates)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;For some odd reason, setting a column data type to “number” then pushing data from JSON array into the table, physically opening the table and looking at the rows, the numbers in the “number” data type column are not all together. For example “29683389” shows in the table as “29 683 389”. This isn’t a one off either, ALL rows exhibit the same behavior and ALL columns set as “numbers” too.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Table page can only show 50 rows per page. Which I understand is probably for performance reasons. However, there really needs to be a “search” function for the table to search for data.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Are there any limitations for creating tables?&lt;lb/&gt; or we can create multiples/unlimited (in 50Mb limit)?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Love this list @compaholic, thanks so much for sharing it.&lt;lb/&gt; 1, 2 and 5 are all planned. For (4), I think that is just a highlighting to make it easier to read that it’s actually 29M. So the number should still be correct.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;You can created unlimited ones within the storage limit &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450044</guid><pubDate>Thu, 02 Oct 2025 14:26:23 +0000</pubDate></item><item><title>Work is not school: Surviving institutional stupidity</title><link>https://www.leadingsapiens.com/surviving-institutional-stupidity/</link><description>&lt;doc fingerprint="37111b1e29438f9c"&gt;
  &lt;main&gt;
    &lt;p&gt;For 16+ years, we master the rules of school. Study hard, get good grades, follow the formula and ultimately merit wins. Then we enter the workforce and none of it works quite like we thought. This becomes painfully obvious as you rise higher in the org.&lt;/p&gt;
    &lt;p&gt;Even seasoned veterans forget this. Recently, a director-level client hit a minor career bump and spiraled into crisis mode, their expectations still anchored in what I call "school rules".&lt;/p&gt;
    &lt;p&gt;Organizations don't run purely on merit or even clear criteria. Although they claim otherwise using buzzwords like “merit” and “data”. That’s only one part of the story, and also what’s visible.&lt;/p&gt;
    &lt;p&gt;The other part, often more consequential, runs on flawed psychology, imperfect decisions, and competing interests. You can call it organizational absurdities. Or more bluntly, institutional stupidity.&lt;/p&gt;
    &lt;p&gt;What follows is a reality check. It’s a “letter to frustrated high-performers” who keep bumping up against these unwritten rules of work. Consider it your guide to staying sane while playing the long game.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have to, blame stupidity not malice&lt;/item&gt;
      &lt;item&gt;Organizations are anything but meritocracies&lt;/item&gt;
      &lt;item&gt;Perception matters as much as performance&lt;/item&gt;
      &lt;item&gt;Don’t waste time fighting for “objective fairness.”&lt;/item&gt;
      &lt;item&gt;Positioning what you offer&lt;/item&gt;
      &lt;item&gt;Mind the gap: your standards vs their’s&lt;/item&gt;
      &lt;item&gt;Higher you go, more it’s an inverted funnel&lt;/item&gt;
      &lt;item&gt;Know which game you’re choosing to play&lt;/item&gt;
      &lt;item&gt;Watching your circle of control&lt;/item&gt;
      &lt;item&gt;Keep a balanced portfolio&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;If you have to, blame stupidity not malice&lt;/head&gt;
    &lt;p&gt;Most of what we chalk up to “politics” or “backstabbing”, aka bad intent, is often better explained by stupidity, inertia, bad incentives, fragmented attention, and misaligned maps of reality.&lt;/p&gt;
    &lt;p&gt;People are juggling too much, thinking too little, and rarely stepping back to ask, “What actually makes sense here?”&lt;/p&gt;
    &lt;p&gt;When you assume stupidity instead of malice, you stay above the fray, stop taking slights personally, or turning misjudgments into betrayals. This way we retain agency and choice.&lt;/p&gt;
    &lt;p&gt;Assuming malice turns you into a cynic. In contrast, assuming stupidity keeps you curious. Instead of fighting ghosts, you study the system and ask better questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What pressures is that person responding to?&lt;/item&gt;
      &lt;item&gt;What game are they trying to win?&lt;/item&gt;
      &lt;item&gt;What am I assuming as rational that’s not?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No one is out to get you; they’re just out to get through the week. The shift from malice to stupidity gives you just enough distance to be curious instead of reactive.&lt;/p&gt;
    &lt;head rend="h2"&gt;Organizations are anything but meritocracies&lt;/head&gt;
    &lt;p&gt;Managers will claim they are, and we want to believe them. But the reality is that the best don’t always rise. At least not as easily or automatically as we think they should.&lt;/p&gt;
    &lt;p&gt;Sometimes they do. But often, what gets rewarded isn’t performance but proximity to power, timing, perception, and political usefulness.&lt;/p&gt;
    &lt;p&gt;This doesn’t mean performance doesn’t matter. It means performance is necessary but not sufficient. It is the entry ticket that gets you through the door, but does not guarantee a seat at the table.&lt;/p&gt;
    &lt;p&gt;Assuming that excellence is obvious is the fatal error of the conscientious expert. Although it creates value, performance doesn’t automatically generate visibility, influence, or narrative. And those are the currencies that get traded when decisions are made by humans.&lt;/p&gt;
    &lt;p&gt;Merit matters, but it needs a stage and a spotlight. It doesn’t mean becoming a shameless self-promoter. Rather, your work needs a distribution strategy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Perception matters as much as performance&lt;/head&gt;
    &lt;p&gt;In school, everyone was evaluated against an objective criteria by someone paid to assess fairly.&lt;/p&gt;
    &lt;p&gt;In organizations, no such thing exists. Instead, perception is the “data”. And this data is constructed often haphazardly by busy people working off limited inputs. You have to manage the story by shaping impressions intentionally.&lt;/p&gt;
    &lt;p&gt;Not only does perception matter as much as performance but who’s doing the perceiving matters even more.&lt;/p&gt;
    &lt;p&gt;Not all perceivers are created equal. A peer may love your work but they might not be a critical node in the web of influence. Who gets consulted? Do they know what you’ve built, and have they heard your name in relevant contexts?&lt;/p&gt;
    &lt;p&gt;It’s not just “do great work.”; it’s also “do the work that’s perceived as valuable.” This means translating your work’s significance up the chain and shape its interpretation. If not, others will do it for you and they may not be generous, or even accurate.&lt;/p&gt;
    &lt;p&gt;For more, see my last two articles: Schrodinger’s Cat at Work Part I and Schrodinger’s Cat at Work Part II.&lt;/p&gt;
    &lt;head rend="h2"&gt;Don’t waste time fighting for “objective fairness.”&lt;/head&gt;
    &lt;p&gt;On paper, organizations love metrics: KPIs, OKRs, dashboards. They create the appearance of detached objectivity.&lt;/p&gt;
    &lt;p&gt;Meanwhile, subjective decisions are constantly happening behind the scenes. The decisions about who to trust, or who gets a shot are made through informal reputations and shared stories about your value. Then the “data” is used to justify them in retrospect.&lt;/p&gt;
    &lt;p&gt;Rather than rant against the system, get good at reading the underlying subjective logic:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Who does this person trust, and why?&lt;/item&gt;
      &lt;item&gt;What do they consider strategic vs tactical?&lt;/item&gt;
      &lt;item&gt;What would make them feel safe betting on me?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Subjectivity isn’t the enemy. It’s the underlying physics of it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Positioning what you offer&lt;/head&gt;
    &lt;p&gt;You may have had the right message but at the wrong moment, or in the wrong wrapper.&lt;/p&gt;
    &lt;p&gt;Positioning is the context around your contributions: Why now? Why you? Why this way? A good idea, or stellar performance, poorly positioned can seem irrelevant. In contrast, a mediocre one nicely positioned is deemed visionary.&lt;/p&gt;
    &lt;p&gt;It’s not just what you say but also when, how, and through whom you say it.&lt;/p&gt;
    &lt;p&gt;Persistence matters as well. Think in campaigns, not just one-time efforts. In my piece on effective conversations I wrote:&lt;/p&gt;
    &lt;quote&gt;It is the series of messages in different forms that over time makes the difference. More akin to waves shaping the shoreline rather than the occasional once in a lifetime tsunami.&lt;/quote&gt;
    &lt;p&gt;What messages are you sending, are they varied, and are you doing it consistently? This is as true for marketing products as it is for positioning yourself inside organizations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mind the gap: your standards vs their’s&lt;/head&gt;
    &lt;p&gt;Another obvious but forgotten reality of organizational life: Not everyone operates by the same playbook.&lt;/p&gt;
    &lt;p&gt;You prioritize substance and direct contribution, while others focus on visibility and relationship-building in ways that are uncomfortable to you. It’s that colleague who excels at positioning routine work as “strategic”, or the peer who builds influence through cultivating key relationships.&lt;/p&gt;
    &lt;p&gt;And yes, these approaches do yield results.&lt;/p&gt;
    &lt;p&gt;But this doesn’t mean dismissing it as pure politics or simply abandoning your principles. It means understanding the landscape you're operating in.&lt;/p&gt;
    &lt;p&gt;You can’t effectively participate in a game you refuse to see clearly. You're not at a disadvantage because you choose to act with integrity. It’s because you fail to recognize that influence flows through multiple channels and others are willing to play differently.&lt;/p&gt;
    &lt;p&gt;The key is not to match their behavior but to factor it in. Instead of expecting fairness, anticipate asymmetry. And then get creative about how you play.&lt;/p&gt;
    &lt;p&gt;Being ethical doesn’t mean being passive but tactically awake.&lt;/p&gt;
    &lt;head rend="h2"&gt;Higher you go, more it’s an inverted funnel&lt;/head&gt;
    &lt;p&gt;Perhaps the most obvious point but also easy to forget especially if your career has been on autopilot so far.&lt;/p&gt;
    &lt;p&gt;There’s a bottleneck up top: fewer seats, more ambiguity, less structure and high subjectivity. It’s not just hard to get in but even harder to stay clear on what “good” even looks like.&lt;/p&gt;
    &lt;p&gt;This means you can do everything right and still get passed over. That’s not a verdict on your worth or ability, just geometry.&lt;/p&gt;
    &lt;p&gt;It also means that staying the course when things don’t go your way isn’t just a virtue but a practice. To play the long game, you have to keep showing up even after crushing disappointment without getting cynical of the process. Put differently, you need high levels of frustration tolerance.&lt;/p&gt;
    &lt;p&gt;Cliched? Yes, very much so, but also uncommon. It means if you can pull it off, it’s a source of power.&lt;/p&gt;
    &lt;head rend="h2"&gt;Know which game you’re choosing to play&lt;/head&gt;
    &lt;p&gt;There is no one game being played. There are multiple, overlapping games with different scoring systems. Some are playing to build long-term credibility; others for short-term visibility.&lt;/p&gt;
    &lt;p&gt;You can’t play them all and neither should you try.&lt;/p&gt;
    &lt;p&gt;The real problem is we slide into playing someone else's game without realizing it. We adopt the norms and metrics of others without checking if that’s the game we actually want to play, let alone win. So we end up optimizing for a role we don’t respect, or chasing promotions that hollow us out.&lt;/p&gt;
    &lt;p&gt;Whatever you’re doing, own it outright. Not just the upside but also the downside. If you're focused on building something lasting like developing others, or robust systems, you need to accept that visible status markers (titles, promotions, recognition) might not happen right away.&lt;/p&gt;
    &lt;p&gt;The danger isn't which path you pick, whether it's chasing promotions or maintaining your autonomy. The real disaster is to sleepwalk down a path while pretending you had no choice in the matter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Watching your circle of control&lt;/head&gt;
    &lt;p&gt;An easy way to burn out is to focus relentlessly on things you care about but cannot actually influence. Over time, especially in large organizations, it's tempting to attribute everything to forces outside yourself. This induces organizational helplessness. A sense that nothing you do matters unless someone above says so.&lt;/p&gt;
    &lt;p&gt;Fight that, not with bluster, but with deliberate ownership of the space you control and influence. While experienced professionals often have more influence than they think, it's distributed differently than they expect.&lt;/p&gt;
    &lt;p&gt;The key is maintaining an internal locus of control which includes your positioning, relationships, and what you are building.&lt;/p&gt;
    &lt;head rend="h2"&gt;Keep a balanced portfolio&lt;/head&gt;
    &lt;p&gt;This is a well-understood concept in investing but often missing in the context of long careers. If all your identity is wrapped up in organizational validation, you're fragile. This means setbacks don't just rattle your job, it rattles your sense of self.&lt;/p&gt;
    &lt;p&gt;Many mid-career professionals learn this the hard way. To state the obvious: you are not your title, or your most recent performance review.&lt;/p&gt;
    &lt;p&gt;The anti-dote is diversification not of money, but meaning.&lt;/p&gt;
    &lt;p&gt;Rebalancing here means investing in other sources of connection and community. This includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Developing a craft that exists beyond a given employer.&lt;/item&gt;
      &lt;item&gt;Investing in communities that outlast org charts.&lt;/item&gt;
      &lt;item&gt;Projects, relationships, and sources of learning that replenish.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You are building adaptive capacity.&lt;/p&gt;
    &lt;p&gt;A balanced portfolio also helps to play the long game with more psychological courage because your whole life isn't riding on the next promotion cycle or external validation.&lt;/p&gt;
    &lt;head rend="h2"&gt;In closing&lt;/head&gt;
    &lt;p&gt;By recognizing the subjective currents that shape work environments, we can operate within them more skillfully.&lt;/p&gt;
    &lt;p&gt;This isn't cynicism or gaming the system. Rather, it's developing a nuanced understanding of how organizations actually function. This stance equips us to make more intentional choices about how to engage, contribute, and create meaning.&lt;/p&gt;
    &lt;p&gt;As ideal as it sounds, the goal isn't to eliminate organizational absurdities, but to work effectively within and around them. By staying in the game, you find ways to gradually improve the system from within.&lt;/p&gt;
    &lt;p&gt;Organizations are ultimately human constructs. Imperfect, but not immutable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450525</guid><pubDate>Thu, 02 Oct 2025 14:58:04 +0000</pubDate></item><item><title>Signal Protocol and Post-Quantum Ratchets</title><link>https://signal.org/blog/spqr/</link><description>&lt;doc fingerprint="ac708592c921c484"&gt;
  &lt;main&gt;
    &lt;p&gt;We are excited to announce a significant advancement in the security of the Signal Protocol: the introduction of the Sparse Post Quantum Ratchet (SPQR). This new ratchet enhances the Signal Protocol’s resilience against future quantum computing threats while maintaining our existing security guarantees of forward secrecy and post-compromise security.&lt;/p&gt;
    &lt;p&gt;The Signal Protocol is a set of cryptographic specifications that provides end-to-end encryption for private communications exchanged daily by billions of people around the world. After its publication in 2013, the open source Signal Protocol was adopted not only by the Signal application but also by other major messaging products. Technical information on the Signal Protocol can be found in the specifications section of our docs site.&lt;/p&gt;
    &lt;p&gt;In a previous blog post, we announced the first step towards advancing quantum resistance for the Signal Protocol: an upgrade called PQXDH that incorporates quantum-resistent cryptographic secrets when chat sessions are established in order to protect against harvest-now-decrypt-later attacks that could allow current chat sessions to become compromised if a sufficiently powerful quantum computer is developed in the future. However, the Signal Protocol isn’t just about protecting cryptographic material and keys at the beginning of a new chat or phone call; it’s also designed to minimize damage and heal from compromise as that conversation continues.&lt;/p&gt;
    &lt;p&gt;We refer to these security goals as Forward Secrecy (FS) and Post-Compromise Security (PCS). FS and PCS can be considered mirrors of each other: FS protects past messages against future compromise, while PCS protects future messages from past compromise. Today, we are happy to announce the next step in advancing quantum resistance for the Signal Protocol: an additional regularly advancing post-quantum ratchet called the Sparse Post Quantum Ratchet, or SPQR. On its own, SPQR provides secure messaging that provably achieves these FS and PCS guarantees in a quantum safe manner. We mix the output of this new ratcheting protocol with Signal’s existing Double Ratchet, in a combination we refer to as the Triple Ratchet.&lt;/p&gt;
    &lt;p&gt;What does this mean for you as a Signal user? First, when it comes to your experience using the app, nothing changes. Second, because of how we’re rolling this out and mixing it in with our existing encryption, eventually all of your conversations will move to this new protocol without you needing to take any action. Third, and most importantly, this protects your communications both now and in the event that cryptographically relevant quantum computers eventually become a reality, and it allows us to maintain our existing security guarantees of forward secrecy and post-compromise security as we proactively prepare for that new world.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Current State of the Signal Protocol&lt;/head&gt;
    &lt;p&gt;The original Signal ratchet uses hash functions for FS and a set of elliptic-curve Diffie Hellman (ECDH) secret exchanges for PCS. The hash functions are quantum safe, but elliptic-curve cryptography is not. An example is in order: our favorite users, Alice and Bob, establish a long-term connection and chat over it regularly. During that session’s lifetime, Alice and Bob regularly agree on new ECDH secrets and use them to “ratchet” their session. Mean ol’ Mallory records the entire (encrypted) communication, and really wants to know what Alice and Bob are talking about.&lt;/p&gt;
    &lt;p&gt;The concept of a “ratchet” is crucial to our current non-quantum FS/PCS protection. In the physical world, a ratchet is a mechanism that allows a gear to rotate forward, but disallows rotation backwards. In the Signal Protocol, it takes on a similar role. When Alice and Bob “ratchet” their session, they replace the set of keys they were using prior with a new set based on both the older secrets and a new one they agree upon. Given access to those new secrets, though, there’s no (non-quantum) way to compute the older secrets. By being “one-way”, this ratcheting mechanism provides FS.&lt;/p&gt;
    &lt;p&gt;The ECDH mechanism allows Alice and Bob to generate new, small (32 bytes) data blobs and attach them to every message. Whenever each party receives a message from the other, they can locally (and relatively cheaply) use this data blob to agree on a new shared secret, then use that secret to ratchet their side of the protocol. Crucially, ECDH also allows Alice and Bob to both agree on the new secret without sending that secret itself over their session, and in fact without sending anything over the session that Mallory could use to determine it. This description of Diffie-Hellman key exchange provides more details on the concepts of such a key exchange, and this description of ECDH provides specific details on the variant used by the current Signal protocol.&lt;/p&gt;
    &lt;p&gt;Sometime midway through the lifetime of Alice and Bob’s session, Mallory successfully breaches the defences of both Alice and Bob, gaining access to all of the (current) secrets used for their session at the time of request. Alice and Bob should have the benefits of Forward Secrecy - they’ve ratcheted sometime recently before the compromise, so no messages earlier than their last ratchet are accessible to Mallory, since ratcheting isn’t reversible. They also retain the benefits of Post-Compromise Security. Their ratcheting after Mallory’s secret access agrees upon new keys that can’t be gleaned just from the captured data they pass between each other, re-securing the session.&lt;/p&gt;
    &lt;p&gt;Should Mallory have access to a quantum computer, though, things aren’t so simple. Because elliptic curve cryptography is not quantum resistant, it’s possible that Mallory could glean access to the secret that Alice and Bob agreed upon, just by looking at the communication between them. Given this, Alice and Bob’s session will never “heal”; Mallory’s access to their network traffic from this point forward will allow her to decrypt all future communications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mixing In Quantum Security&lt;/head&gt;
    &lt;p&gt;In order to make our security guarantees stand up to quantum attacks, we need to mix in secrets generated from quantum secure algorithms. In PQXDH, we did this by performing an additional round of key agreement during the session-initiating handshake, then mixing the resulting shared secret into the initial secret material used to create Signal sessions. To handle FS and PCS, we need to do continuous key agreement, where over the lifetime of a session we keep generating new shared secrets and mixing those keys into our encryption keys.&lt;/p&gt;
    &lt;p&gt;Luckily there is a tool designed exactly for this purpose: the quantum-secure Key-Encapsulation Mechanism (KEM). KEMs share similar behavior to the Diffie-Hellman mechanisms we described earlier, where two clients provide each other with information, eventually deciding on a shared secret, without anyone who intercepts their communications being able to access that secret. However, there is one important distinction for KEMs - they require ordered, asymmetric messages to be passed between their clients. In ECDH, both clients send the other some public parameters, and both combine these parameters with their locally held secrets and come up with an identical shared secret. In the standardized ML-KEM key-encapsulation mechanism, though, the initiating client generates a pair of encapsulation key (EK) and decapsulation key (DK) (analogous to a public and private key respectively) and sends the EK. The receiving client receives it, generates a secret, and wraps it into a ciphertext (CT) with that key. The initiating client receives that CT and decapsulates with its previously generated DK. In the end, both clients have access to the new, shared secret, just through slightly different means.&lt;/p&gt;
    &lt;p&gt;Wanting to integrate this quantum-secure key sharing into Signal, we could take a simple, naive approach for each session. When Alice initiates a session with Bob,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alice, with every message she sends, sends an EK&lt;/item&gt;
      &lt;item&gt;Bob, with every message he receives, generates a secret and a CT, and sends the CT back&lt;/item&gt;
      &lt;item&gt;Alice, on receiving a CT, extracts the secret with her DK and mixes it in&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This initially simple-looking approach, though, quickly runs into a number of issues we’ll need to address to make our protocol actually robust. First, encapsulation keys and CTs are large - over 1000 bytes each for ML-KEM 768, compared to the 32 bytes required for ECDH. Second, while this protocol works well when both clients are online, what happens when a client is offline? Or a message is dropped or reordered? Or Alice wants to send 10 messages before Bob wants to send one?&lt;/p&gt;
    &lt;p&gt;Some of these problems have well-understood solutions, but others have trade-offs that may shine in certain circumstances but fall short in others. Let’s dive in and come to some conclusions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who Wants What When&lt;/head&gt;
    &lt;p&gt;How does Alice decide what to send based on what Bob needs next, and vice versa? If Bob hasn’t received an EK yet, she shouldn’t send the next one. What does Bob send when he hasn’t yet received an EK from Alice, or when he has, but he’s already responded to it? This is a common problem when remote parties send messages to communicate, so there’s a good, well-understood solution: a state machine. Alice and Bob both keep track of “what state am I in”, and base their decisions on that. When sending or receiving a message, they might also change their state. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alice wants to send a message, but she’s in a StartingA state, so she doesn’t have an EK. So, she generates an EK/DK pair, stores them locally, and transitions to the SendEK state&lt;/item&gt;
      &lt;item&gt;Alice wants to send a message and is in the SendEK state. She sends the EK along with the message&lt;/item&gt;
      &lt;item&gt;Alice wants to send another message, but she’s still in the SendEK state. So, she sends the EK with the new message as well&lt;/item&gt;
      &lt;item&gt;Bob receives the message with the EK. He generates a secret and uses the EK to create a CT. He transitions to the SendingCT state.&lt;/item&gt;
      &lt;item&gt;Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the message&lt;/item&gt;
      &lt;item&gt;Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the message&lt;/item&gt;
      &lt;item&gt;… etc …&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By crafting a set of states and transitions, both sides can coordinate what’s sent. Note, though, that even in this simple case, we see problems. For example, we’re sending our (large) EK and (large) CT multiple times.&lt;/p&gt;
    &lt;head rend="h2"&gt;Say (or Send) Less&lt;/head&gt;
    &lt;p&gt;We’ve already mentioned that the size of the data we’re sending has increased pretty drastically, from 32 bytes to over 1000 per message. But bandwidth is expensive, especially on consumer devices like client phones, that may be anywhere in the world and have extremely varied costs for sending bytes over the wire. So let’s discuss strategies for conserving that bandwidth.&lt;/p&gt;
    &lt;p&gt;First, the simplest approach - don’t send a new key with every message. Just, for example, send with every 50 messages, or once a week, or every 50 messages unless you haven’t sent a key in a week, or any other combination of options. All of these approaches tend to work pretty well in online cases, where both sides of a session are communicating in real-time with no message loss. But in cases where one side is offline or loss can occur, they can be problematic. Consider the case of “send a key if you haven’t sent one in a week”. If Bob has been offline for 2 weeks, what does Alice do when she wants to send a message? What happens if we can lose messages, and we lose the one in fifty that contains a new key? Or, what happens if there’s an attacker in the middle that wants to stop us from generating new secrets, and can look for messages that are 1000 bytes larger than the others and drop them, only allowing keyless messages through?&lt;/p&gt;
    &lt;p&gt;Another method is to chunk up a message. Want to send 1000 bytes? Send 10 chunks of 100 bytes each. Already sent 10 chunks? Resend the first chunk, then the second, etc. This smooths out the total number of bytes sent, keeping individual message sizes small and uniform. And often, loss of messages is handled. If chunk 1 was lost, just wait for it to be resent. But it runs into an issue with message loss - if chunk 99 was lost, the receiver has to wait for all of chunks 1-98 to be resent before it receives the chunk it missed. More importantly, if a malicious middleman wants to stop keys from being decided upon, they could always drop chunk 3, never allowing the full key to pass between the two parties.&lt;/p&gt;
    &lt;p&gt;We can get around all of these issues using a concept called erasure codes. Erasure codes work by breaking up a larger message into smaller chunks, then sending those along. Let’s consider our 1000 byte message being sent as 100 byte chunks again. After chunk #10 has been sent, the entirety of the original 1000 byte message has been sent along in cleartext. But rather than just send the first chunk over again, erasure codes build up a new chunk #11, and #12, etc. And they build them in such a way that, once the recipient receives any 10 chunks in any order, they’ll be able to reconstruct the original 1000 byte message.&lt;/p&gt;
    &lt;p&gt;When we put this concept of erasure code chunks together with our previous state machine, it gives us a way to send large blocks of data in small chunks, while handling messages that are dropped. Crucially, this includes messages dropped by a malicious middleman: since any N chunks can be used to recreate the original message, a bad actor would need to drop all messages after #N-1 to disallow the data to go through, forcing them into a complete (and highly noticeable) denial of service. Now, if Alice wants to send an EK to Bob, Alice will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transition from the StartingA state to the SendingEK state, by generating a new EK and chunking it&lt;/item&gt;
      &lt;item&gt;While in the SendingEK state, send a new chunk of the EK along with any messages she sends&lt;/item&gt;
      &lt;item&gt;When she receives confirmation that the recipient has received the EK (when she receives a chunk of CT), transition to the ReceivingCT state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On Bob’s side, he will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transition from the StartingB state to the ReceivingEK state when he receives its first EK chunk&lt;/item&gt;
      &lt;item&gt;Keep receiving EK chunks until he has enough to reconstruct the EK&lt;/item&gt;
      &lt;item&gt;At that point, reconstruct the EK, generate the CT, chunk the CT, and transition to the SendingCT state&lt;/item&gt;
      &lt;item&gt;From this point on, he will send a chunk of the CT with every message&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One interesting way of looking at this protocol so far is to consider the messages flowing from Alice to Bob as potential capacity for sending data associated with post-quantum ratcheting: each message that we send, we could also send additional data like a chunk of EK or of the CT. If we look at Bob’s side, above, we notice that sometimes he’s using that capacity (IE: in step 4 when he’s sending CT chunks) and sometimes he’s not (if he sends a message to Alice during step 2, he has no additional data to send). This capacity is pretty limited, so using more of it gives us the potential to speed up our protocol and agree on new secrets more frequently.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Meditation On How Faster Isn’t Always Better&lt;/head&gt;
    &lt;p&gt;We want to generate shared secrets, then use them to secure messages. So, does that mean that we want to generate shared secrets as fast as possible? Let’s introduce a new term: an epoch. Alice and Bob start their sessions in epoch 0, sending the EKs for epoch 1 (EK#1) and associated ciphertext (CT#1) to each other. Once that process completes, they have a new shared secret they use to enter epoch 1, after which all newly sent messages are protected by the new secret. Each time they generate a new shared secret, they use it to enter a new epoch. Surely, every time we enter a new epoch with a new shared secret, we protect messages before that secret (FS) and after that secret (PCS), so faster generation is better? It seems simple, but there’s an interesting complexity here that deserves attention.&lt;/p&gt;
    &lt;p&gt;First, let’s discuss how to do things faster. Right now, there’s a lot of capacity we’re not using: Bob sends nothing while Alice sends an EK, and Alice sends nothing while Bob sends a CT. Speeding this up isn’t actually that hard. Let’s change things so that Alice sends EK#1, and once Bob acknowledges its receipt, Alice immediately generates and sends EK#2. And once she notices Bob has received that, she generates and sends EK#3, etc. Whenever Alice sends a new message, she always has data to send along with it (new EK chunks), so she’s using its full capacity. Bob doesn’t always have a new CT to send, but he is receiving EKs as fast as Alice can send them, so he often has a new CT to send along.&lt;/p&gt;
    &lt;p&gt;But now let’s consider what happens when an attacker gains access to Alice. Let’s say that Alice has sent EK#1 and EK#2 to Bob, and she’s in the process of sending EK#3. Bob has acknowledged receipt of EK#1 and EK#2, but he’s still in the process of sending CT#1, since in this case Bob sends fewer messages to Alice than vice versa. Because Alice has already generated 3 EKs she hasn’t used, Alice needs to keep the associated DK#1, DK#2, and DK#3 around. So, if at this point someone maliciously gains control of Alice’s device, they gain access to both the secrets associated with the current epoch (here, epoch 0) and to the DKs necessary to reconstruct the secrets to other epochs (here, epochs 1, 2, and 3) using only the over-the-wire CT that Bob has yet to send. This is a big problem: by generating secrets early, we’ve actually made the in-progress epochs and any messages that will be sent within them less secure against this single-point-in-time breach.&lt;/p&gt;
    &lt;p&gt;To test this out, we at Signal built a number of different state machines, each sending different sets of data either in parallel or serially. We then ran these state machines in numerous simulations, varying things like the ratio of messages sent by Alice vs Bob, the amount of data loss or reordering, etc. And while running these simulations, we tracked what epochs’ secrets were exposed at any point in time, assuming an attacker were to breach either Alice’s or Bob’s secret store. The results showed that, in general, while simulations that handled multiple epochs’ secrets in parallel (IE: by sending EK#2 before receipt of CT#1) did generate new epochs more quickly, they actually made more messages vulnerable to a single breach.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Let’s Still Be Efficient&lt;/head&gt;
    &lt;p&gt;This still leaves us with a problem, though: the capacity present in messages we send in either direction is still a precious resource, and we want to use it as efficiently as possible. And our simple approach of Alice’s “send EK, receive CT, repeat” and Bob’s “receive EK, send CT, repeat” leaves lots of time where Alice and Bob have nothing to send, should that capacity be available.&lt;/p&gt;
    &lt;p&gt;To improve our use of our sending capacity, we decided to take a harder look into the ML-KEM algorithm we’re using to share secrets, to see if there was room to improve. Let’s break things down more and share some actual specifics on the ML-KEM algorithm.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates an EK of 1184 bytes to send to Bob, and an associated DK&lt;/item&gt;
      &lt;item&gt;Bob receives the EK&lt;/item&gt;
      &lt;item&gt;Bob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice&lt;/item&gt;
      &lt;item&gt;Alice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secret&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Diving in further, we can break out step #3 into some sub-steps&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates an EK of 1184 bytes to send to Bob, and an associated DK&lt;/item&gt;
      &lt;item&gt;Bob receives the EK&lt;/item&gt;
      &lt;item&gt;Bob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice1&lt;list rend="ol"&gt;&lt;item&gt;Bob creates a new shared secret S and sampled randomness R by sampling entropy and combining it with a hash of EK&lt;/item&gt;&lt;item&gt;Bob hashes the EK into a Hash&lt;/item&gt;&lt;item&gt;Bob pulls 32 bytes of the EK, a Seed&lt;/item&gt;&lt;item&gt;Bob uses the Seed and R to generate the majority of the CT&lt;/item&gt;&lt;item&gt;Bob then uses S and EK to generate the last portion of the CT&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Alice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secret&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Step 3.d, which generates 960 bytes of the 1088-byte CT, only needs 64 bytes of input: a Seed that’s 32 of EK’s bytes, and the hash of EK, which is an additional 32. If we combine these values and send them first, then most of EK and most of the CT can be sent in parallel from Alice to Bob and Bob to Alice respectively. Our more complicated but more efficient secret sharing now looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates EK and DK. Alice extracts the 32-byte Seed from EK&lt;/item&gt;
      &lt;item&gt;Alice sends 64 bytes EK1 (Seed + Hash(EK)) to Bob. Bob sends nothing during this time.&lt;/item&gt;
      &lt;item&gt;Bob receives the Seed and Hash, and generates the first, largest part of the CT from them (CT1)&lt;/item&gt;
      &lt;item&gt;After this point, Alice sends EK2 (the rest of the EK minus the Seed), while Bob sends CT1&lt;/item&gt;
      &lt;item&gt;Bob eventually receives EK2, and uses it to generate the final portion of the CT (CT2)&lt;/item&gt;
      &lt;item&gt;Once Alice tells Bob that she has received all of CT1, Bob sends Alice CT2. Alice sends nothing during this time.&lt;/item&gt;
      &lt;item&gt;With both sides having all of the pieces of EK and the CT that they need, they extract their shared secret and increment their epoch&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are still places in this algorithm (specifically steps 2 and 6) where one side has nothing to send. But during those times, the other side has only a very small amount of information to send, so the duration of those steps is minimal compared to the rest of the process. Specifically, while the full EK is 37 chunks and the full CT is 34, the two pieces of the new protocol which must be sent without data being received (EK1 and CT2) are 2 and 4 chunks respectively, while the pieces that can be sent while also receiving (EK2 and CT1) are the bulk of the data, at 36 and 30 chunks respectively. Far more of our sending capacity is actually used with this approach.&lt;/p&gt;
    &lt;p&gt;Remember that all of this is just to perform a quantum-safe key exchange that gives us a secret we can mix into the bigger protocol. To help us organize our code, our security proofs, and our understanding better we treat this process as a standalone protocol that we call the ML-KEM Braid.&lt;/p&gt;
    &lt;p&gt;This work was greatly aided by the authors of the libcrux-ml-kem Rust library, who graciously exposed the APIs necessary to work with this incremental version of ML-KEM 768. With this approach completed, we’ve been able to really efficiently use the sending capacity of messages sent between two parties to share secrets as quickly as possible without exposing secrets from multiple epochs to potential attackers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mixing Things Up - The Triple Ratchet&lt;/head&gt;
    &lt;p&gt;There are plenty of details to add to make sure that we reached every corner - check those out in our online protocol documentation - but this basic idea lets us build secure messaging that has post-quantum FS and PCS without using up anyone’s data. We’re not done, though! Remember, at the beginning of this post we said we wanted post-quantum security without taking away our existing guarantees.&lt;/p&gt;
    &lt;p&gt;While today’s Double Ratchet may not be quantum safe, it provides a high level of security today and we believe it will continue to be strong well into the future. We aren’t going to take that away from our users. So what can we do?&lt;/p&gt;
    &lt;p&gt;Our answer ends up being really simple: we run both the Double Ratchet and the Sparse Post Quantum Ratchet alongside each other and mix their keys together, into what we’re calling the Triple Ratchet protocol. When you want to send a message you ask both the Double Ratchet and SPQR “What encryption key should I use for the next message?” and they will both give you a key (along with some other data you need to put in a message header). Instead of either key being used directly, both are passed into a Key Derivation Function - a special function that takes random-enough inputs and produces a secure cryptographic key that’s as long as you need. This gives you a new “mixed” key that has hybrid security. An attacker has to break both our elliptic curve and ML-KEM to even be able to distinguish this key from random bits. We use that mixed key to encrypt our message.&lt;/p&gt;
    &lt;p&gt;Receiving messages is just as easy. We take the message header - remember it has some extra data in it - and send it to the Double Ratchet and SPQR and ask them “What key should I use to decrypt a message with this header?” They both return their keys and you feed them both into that Key Derivation Function to get your decryption key. After that, everything proceeds just like it always has.&lt;/p&gt;
    &lt;head rend="h2"&gt;Heterogeneous Rollout&lt;/head&gt;
    &lt;p&gt;So we’ve got this new, snazzy protocol, and we want to roll it out to all of our users across all of their devices… but none of the devices currently support that protocol. We roll it out to Alice, and Alice tries to talk to Bob, but Alice speaks SPQR and Bob doesn’t. Or we roll it out to Bob, but Alice wants to talk to Bob and Alice doesn’t know the new protocol Bob wants to use. How do we make this work?&lt;/p&gt;
    &lt;p&gt;Let’s talk about the simplest option: allowing downgrades. Alice tries to establish a session with Bob using SPQR and sends a message over it. Bob fails to read the message and establish the session, because Bob hasn’t been upgraded yet. Bob sends Alice an error, so Alice has to try again. This sounds fine, but in practice it’s not tenable. Consider what happens if Alice and Bob aren’t online at the same time… Alice sends a message at 1am, then shuts down. Bob starts up at 3am, sends an error, then shuts down. Alice gets that error when she restarts at 5am, then resends. Bob starts up at 7am and finally gets the message he should have received at 3am, 4 hours behind schedule.&lt;/p&gt;
    &lt;p&gt;To handle this, we designed the SPQR protocol to allow itself to downgrade to not being used. When Alice sends her first message, she attaches the SPQR data she would need to start up negotiation of the protocol. Noticing that downgrades are allowed for this session, Alice doesn’t mix any SPQR key material into the message yet. Bob ignores that data, because it’s in a location he glosses over, but since there’s no mixed in keys yet, he can still decrypt the message. He sends a response that lacks SPQR data (since he doesn’t yet know how to fill it in), which Alice receives. Alice sees a message without SPQR data, and understands that Bob doesn’t speak SPQR yet. So, she downgrades to not using it for that session, and they happily talk without SPQR protection.&lt;/p&gt;
    &lt;p&gt;There’s some scary potential problems here… let’s work through them. First off, can a malicious middleman force a downgrade and disallow Alice and Bob from using SPQR, even if both of them are able to? We protect against that by having the SPQR data attached to the message be MAC’d by the message-wide authentication code - a middleman can’t remove it without altering the whole message in such a way that the other party sees it, even if that other party doesn’t speak SPQR. Second, could some error cause messages to accidentally downgrade sometime later in their lifecycle, due either to bugs in the code or malicious activity? Crucially, SPQR only allows a downgrade when it first receives a message from a remote party. So, Bob can only downgrade if he receives his first message from Alice and notices that she doesn’t support SPQR, and Alice will only downgrade if she receives her first reply from Bob and notices that he doesn’t. After that first back-and-forth, SPQR is locked in and used for the remainder of the session.&lt;/p&gt;
    &lt;p&gt;Finally, those familiar with Signal’s internal workings might note that Signal sessions last a really long time, potentially years. Can we ever say “every session is protected by SPQR”, given that SPQR is only added to new sessions as they’re being initiated? To accomplish this, Signal will eventually (once all clients support the new protocol) roll out a code change that enforces SPQR for all sessions, and that archives all sessions which don’t yet have that protection. After the full rollout of that future update, we’ll be able to confidently assert complete coverage of SPQR.&lt;/p&gt;
    &lt;p&gt;One nice benefit to setting up this “maybe downgrade if the other side doesn’t support things” approach is that it also sets us up for the future: the same mechanisms that allow us to choose between SPQR or no-SPQR are designed to also allow us to upgrade from SPQR to some far-future (as yet unimagined) SPQRv2.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Sure We Get It Right&lt;/head&gt;
    &lt;p&gt;Complex protocols require extraordinary care. We have to ensure that the new protocol doesn’t lose any of the security guarantees the Double Ratchet gives us. We have to ensure that we actually get the post-quantum protection we’re aiming for. And even then, after we have full confidence in the protocol, we have to make sure that our implementation is correct and robust and stays that way as we maintain it. This is a tall order.&lt;/p&gt;
    &lt;p&gt;To make sure we got this right, we started by building the protocol on a firm foundation of fundamental research. We built on the years of research the academic community has put into secure messaging and we collaborated with researchers from PQShield, AIST, and NYU to explore what was possible with post-quantum secure messaging. In a paper at Eurocrypt 25 we introduced erasure code based chunking and proposed the high-level Triple Ratchet protocol, proving that it gave us the post-quantum security we wanted without taking away any of the security of the classic Double Ratchet. In a follow up paper at USENIX 25, we observed that there are many different ways to design a post-quantum ratchet protocol and we need to pick the one that protects user messages the best. We introduced and analyzed six different protocols and two stood out: one is essentially SPQR, the other is a protocol using a new KEM, called Katana, that we designed just for ratcheting. That second one is exciting, but we want to stick to standards to start!&lt;/p&gt;
    &lt;head rend="h2"&gt;Formal Verification From the Start&lt;/head&gt;
    &lt;p&gt;This research gave us the framework to think about protocol design and prove protocols are secure, but there is a big leap from an academic paper to code. Already when designing PQXDH - a much simpler protocol! - we found that formal verification was an important tool for getting the details right. With the Triple Ratchet we partnered with Cryspen and made formal verification part of the process from the beginning.&lt;/p&gt;
    &lt;p&gt;As we kept finding better protocol candidates - and we implemented around a dozen of them - we modeled them in ProVerif to prove that they had the security properties we needed. Rather than wrapping up a protocol design and performing formal verification as a last step we made it a core part of the design process. Now that the design is settled, this gives us machine verified proof that our protocol has the security properties we demand from it. We wrote our Rust code to closely match the ProVerif models, so it is easy to check that we’re modeling what we implement. In particular, ProVerif is very good at reasoning about state machines, which we’re already using, making the mapping from code to model much simpler.&lt;/p&gt;
    &lt;p&gt;We are taking formal verification further than that, though. We are using hax to translate our Rust implementation into F* on every CI run. Once the F* models are extracted, we prove that core parts of our highly optimized implementation are correct, that function pre-conditions and post-conditions cannot be violated, and that the entire crate is panic free. That last one is a big deal. It is great for usability, of course, because nobody wants their app to crash. But it also matters for correctness. We aggressively add assertions about things we believe must be true when the protocol is running correctly - and we crash the app if they are false. With hax and F*, we prove that those assertions will never fail.&lt;/p&gt;
    &lt;head rend="h2"&gt;Formal Verification Doesn’t Freeze Our Progress&lt;/head&gt;
    &lt;p&gt;Often when people think about formally verified protocol implementations, they imagine a one-time huge investment in verification that leaves you with a codebase frozen in time. This is not the case here. We re-run formal verification in our CI pipeline every time a developer pushes a change to GitHub. If the proofs fail then the build fails, and the developer needs to fix it. In our experience so far, this is usually as simple as adding a pre- or postcondition or returning an error when a value is out of bounds. For us, formal verification is a dynamic part of the development process and ensures that the quality is high on every merge.&lt;/p&gt;
    &lt;head rend="h2"&gt;TLDR&lt;/head&gt;
    &lt;p&gt;Signal is rolling out a new version of the Signal Protocol with the Triple Ratchet. It adds the Sparse Post-Quantum Ratchet, or SPQR, to the existing Double Ratchet to create a new Triple Ratchet which gives our users quantum-safe messaging without taking away any of our existing security promises. It’s being added in such a way that it can be rolled out without disruption. It’s relatively lightweight, not using much additional bandwidth for each message, to keep network costs low for our users. It’s resistant to meddling by malicious middlemen - to disrupt it, all messages after a certain time must be dropped, causing a noticeable denial of service for users. We’re rolling it out slowly and carefully now, but in such a way that we’ll eventually be able to say with confidence “every message sent by Signal is protected by this.” Its code has been formally verified, and will continue to be so even as future updates affect the protocol. It’s the combined effort of Signal employees and external researchers and contributors, and it’s only possible due to the continued work and diligence of the larger crypto community. And as a user of Signal, our biggest hope is that you never even notice or care. Except one day, when headlines scream “OMG, quantum computers are here”, you can look back on this blog post and say “oh, I guess I don’t have to care about that, because it’s already been handled”, as you sip your Nutri-Algae while your self-driving flying car wends its way through the floating tenements of Megapolis Prime.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Those that are interested can look at https://nvlpubs.nist.gov/nistpubs/fips/nist.fips.203.pdf and note that Algorithm 17 uses randomness plus the hash of EK to generate a shared secret and random value, then that random value is used in Algorithm 14 to create c1. The rest of ekPKE is only used by Algorithm 14 to generate c2. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451527</guid><pubDate>Thu, 02 Oct 2025 16:06:10 +0000</pubDate></item><item><title>Launch HN: Simplex (YC S24) – Browser automation platform for developers</title><link>https://www.simplex.sh/</link><description>&lt;doc fingerprint="f4968ef0cf13274b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Browser automation&lt;lb/&gt;for developers&lt;/head&gt;&lt;p&gt;Simplex provides all the infrastructure needed for modern browser automation. &lt;lb/&gt; Remote browsers, steerable web agents, and more.&lt;/p&gt;&lt;head rend="h3"&gt;A team from world class institutions&lt;/head&gt;&lt;head rend="h2"&gt;Watch a live demo of Simplex automating&lt;lb/&gt; a real billing portal.&lt;/head&gt;&lt;head rend="h3"&gt;Live Session Stream&lt;/head&gt;&lt;p&gt;Demo Preview&lt;/p&gt;&lt;head rend="h3"&gt;Live Demo Logs&lt;/head&gt;&lt;p&gt;Agent logs will appear here when demo is running&lt;/p&gt;&lt;head rend="h2"&gt;Engineered from the ground up to work with legacy systems.&lt;/head&gt;&lt;p&gt;Reliably automate every legacy portal your customers use.&lt;/p&gt;&lt;head rend="h3"&gt;Billing Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to log into a billing portal and download the list of invoices for a specified customer.&lt;/p&gt;&lt;head rend="h3"&gt;Prior Authorization Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to fill out complex, branching-logic prior authorization forms on medical provider portals.&lt;/p&gt;&lt;head rend="h3"&gt;ERPs&lt;/head&gt;&lt;p&gt;Simplex has been used to automate data entry and download report PDFs across different ERPs.&lt;/p&gt;&lt;head rend="h3"&gt;Government Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to search and extract structured information across public government portals.&lt;/p&gt;&lt;head rend="h3"&gt;TMS/WMS Software&lt;/head&gt;&lt;p&gt;Simplex has been used to log into a TMS portal, create and edit the information for a shipment, then dispatch the shipment.&lt;/p&gt;&lt;head rend="h3"&gt;... and more&lt;/head&gt;&lt;p&gt;with us to discuss your specific use case.&lt;/p&gt;&lt;table&gt;&lt;row span="6"&gt;&lt;cell role="head"&gt;Order ID&lt;/cell&gt;&lt;cell role="head"&gt;Customer&lt;/cell&gt;&lt;cell role="head"&gt;Status&lt;/cell&gt;&lt;cell role="head"&gt;Amount&lt;/cell&gt;&lt;cell role="head"&gt;Last Updated&lt;/cell&gt;&lt;cell role="head"&gt;Actions&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-001&lt;/cell&gt;&lt;cell&gt;John Smith&lt;/cell&gt;&lt;cell&gt;PENDING&lt;/cell&gt;&lt;cell&gt;$1,234.56&lt;/cell&gt;&lt;cell&gt;01/15/2024&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-002&lt;/cell&gt;&lt;cell&gt;Jane Doe&lt;/cell&gt;&lt;cell&gt;PROCESSING&lt;/cell&gt;&lt;cell&gt;$987.65&lt;/cell&gt;&lt;cell&gt;01/14/2024&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-003&lt;/cell&gt;&lt;cell&gt;Bob Johnson&lt;/cell&gt;&lt;cell&gt;COMPLETED&lt;/cell&gt;&lt;cell&gt;$2,345.67&lt;/cell&gt;&lt;cell&gt;01/13/2024&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;PO-2024-004&lt;/cell&gt;&lt;cell&gt;Alice Brown&lt;/cell&gt;&lt;cell&gt;ERROR&lt;/cell&gt;&lt;cell&gt;$876.54&lt;/cell&gt;&lt;cell&gt;01/12/2024&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Deploy reliably, scale easily.&lt;/head&gt;&lt;head rend="h3"&gt;Run consistent workflows&lt;/head&gt;&lt;p&gt;Simplex automatically caches agent actions. This increases reliability of runs and makes developing flows lightning fast.&lt;/p&gt;&lt;head rend="h3"&gt;Create realtime flows&lt;/head&gt;We've achieved realtime latency to handle complex workflows during phone calls.&lt;p&gt;if low latency flows are a priority for you.&lt;/p&gt;&lt;quote&gt;simplex.click(“New Order”)simplex.click(“Shipment Address”)simplex.type(“”)&lt;/quote&gt;&lt;head rend="h2"&gt;Simplex just works.&lt;/head&gt;&lt;head rend="h3"&gt;Production-ready&lt;/head&gt;&lt;p&gt;Eval harnesses to stress-test your workflows at scale and emulate production conditions.&lt;/p&gt;&lt;head rend="h3"&gt;Authentication Handling&lt;/head&gt;&lt;p&gt;Authentication SDK functions to handle 2FA, login data, and more on your customers' sites. See more here.&lt;/p&gt;&lt;head rend="h3"&gt;Scalable Headless Browsers&lt;/head&gt;&lt;p&gt;Headless browsers that can scale to 100s of concurrent sessions in seconds.&lt;/p&gt;&lt;head rend="h3"&gt;Stealth Mode&lt;/head&gt;&lt;p&gt;Automatic CAPTCHA solving, proxies, and anti-bot protections.&lt;/p&gt;&lt;head rend="h3"&gt;Controllable Workflows&lt;/head&gt;&lt;p&gt;Our web agents are constrained to only take the actions you tell it to.&lt;/p&gt;&lt;head rend="h3"&gt;Optimized Workflows&lt;/head&gt;&lt;p&gt;Automatically cache your workflows for fast and reliable execution in production.&lt;/p&gt;&lt;head rend="h3"&gt;Robust SDKs&lt;/head&gt;&lt;p&gt;Our SDKs are designed to be robust and easy to use. Available in both Python and TypeScript.&lt;/p&gt;&lt;head rend="h3"&gt;Detailed Logging and Replays&lt;/head&gt;&lt;p&gt;View a livestream and live logs of sessions as they happen. Share session replays and detailed agent logs with your team and customers.&lt;/p&gt;&lt;head rend="h2"&gt;Ready to get started?&lt;/head&gt;&lt;p&gt;Book a call with our team to discuss your use case and get started.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451547</guid><pubDate>Thu, 02 Oct 2025 16:07:23 +0000</pubDate></item><item><title>Playball – Watch MLB games from a terminal</title><link>https://github.com/paaatrick/playball</link><description>&lt;doc fingerprint="607031164bc1d92f"&gt;
  &lt;main&gt;
    &lt;p&gt;Watch MLB games from the comfort of your own terminal&lt;/p&gt;
    &lt;p&gt;MLB Gameday and MLB.tv are great, but sometimes you want to keep an eye on a game a bit more discreetly. &lt;code&gt;playball&lt;/code&gt; puts the game in a terminal window.&lt;/p&gt;
    &lt;p&gt;Just want to try it out?&lt;/p&gt;
    &lt;code&gt;$ npx playball
&lt;/code&gt;
    &lt;p&gt;Ready for the big leagues? Install the package globally&lt;/p&gt;
    &lt;code&gt;$ npm install -g playball
&lt;/code&gt;
    &lt;p&gt;Then run it&lt;/p&gt;
    &lt;code&gt;$ playball
&lt;/code&gt;
    &lt;code&gt;$ docker build -t playball .
$ docker run -it --rm --name playball playball:latest
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;q&lt;/cell&gt;
        &lt;cell&gt;quit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;c&lt;/cell&gt;
        &lt;cell&gt;go to schedule view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;s&lt;/cell&gt;
        &lt;cell&gt;go to standings view&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;↓/j, ↑/k, ←/h, →/l&lt;/cell&gt;
        &lt;cell&gt;change highlighted game&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;enter&lt;/cell&gt;
        &lt;cell&gt;view highlighted game&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;show previous day's schedule/results&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;n&lt;/cell&gt;
        &lt;cell&gt;show next day's schedule&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;t&lt;/cell&gt;
        &lt;cell&gt;return to today's schedule&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;↓/j, ↑/k&lt;/cell&gt;
        &lt;cell&gt;scroll list of all plays&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Playball can be configured using the &lt;code&gt;config&lt;/code&gt; subcommand. To list the current configuration values run the subcommand with no additional arguments:&lt;/p&gt;
    &lt;code&gt;playball config&lt;/code&gt;
    &lt;p&gt;You should see output similar to:&lt;/p&gt;
    &lt;code&gt;color.ball = green
color.favorite-star = yellow
color.in-play-no-out = blue
color.in-play-out = white
color.in-play-runs-bg = white
color.in-play-runs-fg = black
color.on-base = yellow
color.other-event = white
color.out = red
color.strike = red
color.strike-out = red
color.walk = green
favorites = 
&lt;/code&gt;
    &lt;p&gt;To get the value of a single setting pass the key as an additional argument:&lt;/p&gt;
    &lt;code&gt;playball config color.strike&lt;/code&gt;
    &lt;p&gt;To change a setting pass the key and value as arguments:&lt;/p&gt;
    &lt;code&gt;playball config color.strike blue&lt;/code&gt;
    &lt;p&gt;To revert a setting to its default value provide the key and the &lt;code&gt;--unset&lt;/code&gt; flag:&lt;/p&gt;
    &lt;code&gt;playball config color.strike --unset&lt;/code&gt;
    &lt;p&gt;This table summarizes the available settings:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
        &lt;cell role="head"&gt;default&lt;/cell&gt;
        &lt;cell role="head"&gt;allowed values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.ball&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing balls in top row of game view&lt;/cell&gt;
        &lt;cell&gt;green&lt;/cell&gt;
        &lt;cell&gt;One of the following: &lt;code&gt;black&lt;/code&gt;, &lt;code&gt;red&lt;/code&gt;, &lt;code&gt;green&lt;/code&gt;, &lt;code&gt;yellow&lt;/code&gt;, &lt;code&gt;blue&lt;/code&gt;, &lt;code&gt;magenta&lt;/code&gt;, &lt;code&gt;cyan&lt;/code&gt;, &lt;code&gt;white&lt;/code&gt;, &lt;code&gt;grey&lt;/code&gt;. Any of those colors may be prefixed by &lt;code&gt;bright-&lt;/code&gt; or &lt;code&gt;light-&lt;/code&gt; (for example &lt;code&gt;bright-green&lt;/code&gt;). The exact color used will depend on your terminal settings. The value &lt;code&gt;default&lt;/code&gt; may be used to specify the default text color for your terminal. Finally hex colors (e.g &lt;code&gt;#FFA500&lt;/code&gt;) can be specified. If your terminal does not support true color, the closest supported color may be used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.favorite-star&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of star indiciating favorite team in schedule and standing views&lt;/cell&gt;
        &lt;cell&gt;yellow&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-no-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where ball was put in play and no out was made (single, double, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;blue&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where ball was put in play and an out was made (flyout, fielder's choice, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-runs-bg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Background color for score update in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-runs-fg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Foreground color for score update in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;black&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.on-base&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of diamonds representing runners on base in top row of game view&lt;/cell&gt;
        &lt;cell&gt;yellow&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.other-event&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of other events (mound visit, injury delay, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing outs in top row of game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.strike&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing strikes in top row of game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.strike-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where play ends on a strike (strike out) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.walk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where play ends on a ball (walk, hit by pitch) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;green&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;favorites&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Teams to highlight in schedule and standings views&lt;/cell&gt;
        &lt;cell&gt;Any one of the following: &lt;code&gt;ATL&lt;/code&gt;, &lt;code&gt;AZ&lt;/code&gt;, &lt;code&gt;BAL&lt;/code&gt;, &lt;code&gt;BOS&lt;/code&gt;, &lt;code&gt;CHC&lt;/code&gt;, &lt;code&gt;CIN&lt;/code&gt;, &lt;code&gt;CLE&lt;/code&gt;, &lt;code&gt;COL&lt;/code&gt;, &lt;code&gt;CWS&lt;/code&gt;, &lt;code&gt;DET&lt;/code&gt;, &lt;code&gt;HOU&lt;/code&gt;, &lt;code&gt;KC&lt;/code&gt;, &lt;code&gt;LAA&lt;/code&gt;, &lt;code&gt;LAD&lt;/code&gt;, &lt;code&gt;MIA&lt;/code&gt;, &lt;code&gt;MIL&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;NYM&lt;/code&gt;, &lt;code&gt;NYY&lt;/code&gt;, &lt;code&gt;OAK&lt;/code&gt;, &lt;code&gt;PHI&lt;/code&gt;, &lt;code&gt;PIT&lt;/code&gt;, &lt;code&gt;SD&lt;/code&gt;, &lt;code&gt;SEA&lt;/code&gt;, &lt;code&gt;SF&lt;/code&gt;, &lt;code&gt;STL&lt;/code&gt;, &lt;code&gt;TB&lt;/code&gt;, &lt;code&gt;TEX&lt;/code&gt;, &lt;code&gt;TOR&lt;/code&gt;, &lt;code&gt;WSH&lt;/code&gt;. Or a comma-separated list of multiple (e.g. &lt;code&gt;SEA,MIL&lt;/code&gt;).&lt;p&gt;Note: in some terminals the list must be quoted:&lt;/p&gt;&lt;code&gt;playball config favorites "SEA,MIL"&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;git clone https://github.com/paaatrick/playball.git
cd playball
npm install
npm start
&lt;/code&gt;
    &lt;p&gt;Contributions are welcome!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451577</guid><pubDate>Thu, 02 Oct 2025 16:09:15 +0000</pubDate></item><item><title>Why I chose Lua for this blog</title><link>https://andregarzia.com/2025/03/why-i-choose-lua-for-this-blog.html</link><description>&lt;doc fingerprint="7f443511072312dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I chose Lua for this blog&lt;/head&gt;
    &lt;p&gt;This blog used to run using with a stack based on Racket using Pollen and lots of hacks on top of it. At some point I realised that my setup was working against me. The moving parts and workflow I created added too much friction to keep my blog active. That happened mostly because it was a static generator trying to behave as if it was dynamic website with an editing interface. That can be done really well â cue Grav CMS â but that was not the case for me.&lt;/p&gt;
    &lt;p&gt;Once I decided to rewrite this blog as a simpler system, I faced the dilema of what stack to choose. The obvious choice for me would be Javascript, it is the language I use more often and one that I am quite confortable with. Still, I don't think it is a wise choice for the kind of blog I want to maintain.&lt;/p&gt;
    &lt;p&gt;Talking to some friends recently, I noticed that many people I know that have implemented their own blogging systems face many challenges keeping them running over many years. Not because it is hard to keep software running, but because their stack of choice is moving faster than their codebase.&lt;/p&gt;
    &lt;p&gt;This problem is specially prevalent in the Javascript world. It is almost a crime that JS as understood by the browser is this beautiful language with extreme retrocompatibility, while JS as understood and used by the current tooling and workflows is this mess moving at lightspeed. Let me unpack that for a bit.&lt;/p&gt;
    &lt;p&gt;You can open a web page from 1995 on your browser of choice and it will just work because browser vendors try really hard to make sure they don't break the web.&lt;/p&gt;
    &lt;p&gt;Developers who built the whole ecosystem of NodeJS, NPM, and all those libraries and frameworks don't share the same ethos. They all make a big case of semantic versioning and thus being able to handle breaking changes, but they have breaking changes all the time. You'd be hardpressed to actually run some JS code from ten years ago based on NodeJS and NPM. There is a big chance that dependencies might be gone, broken, or it might be incompatible with the current NodeJS.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I know this sounds like FUD, and that for many many projects, maybe even most projects, that will not be the case. But I heard from many people that keeping their blogging systems up to date requires a lot more work than they would like to do and if they don't, then they're screwed.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That is also true about other languages even though many of them move at a slower speed. A friend recently complained about a blogging system he implemented that requires Ruby 2.0 and that keeping that running sucks.&lt;/p&gt;
    &lt;p&gt;I want a simpler blogging system; one that requires minimal changes over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Now we talk about Lua&lt;/head&gt;
    &lt;p&gt;Lua is a wonderful and nimble language that is often misunderstood.&lt;/p&gt;
    &lt;p&gt;One characteristic that I love about it, is that is evolves very slowly. Lua 5.1 was introduced in 2006, Lua 5.4 which is the current version initial release was in 2020. Yes, there are point released in between, but you can see how much slower it moves when compared to JS.&lt;/p&gt;
    &lt;p&gt;The differences between Lua 5.1 and Lua 5.4 are minimal when compared with how much other languages changed in the same time period.&lt;/p&gt;
    &lt;p&gt;Lua only requires a C89 compiler to bootstrap itself. It is very easy to make Lua work and even easier to make it interface with something.&lt;/p&gt;
    &lt;p&gt;JS is a lot larger than Lua, there is more to understand and more to remember. My blog needs are very simple and Lua can handle them with ease.&lt;/p&gt;
    &lt;head rend="h2"&gt;How this blog works&lt;/head&gt;
    &lt;p&gt;This is an old-school blog. I uses cgi-bin â aka Comon Gateway Interface â scripts to run it. It is a dynamic website with a SQLite database holding its data. When you open a page, it fetches the data from a database and assembles a HTML to send to the browser using Mustache templates.&lt;/p&gt;
    &lt;p&gt;One process per request. Like the old days.&lt;/p&gt;
    &lt;p&gt;You might argue that if I went with NodeJS, I'd be able to serve more requests using fewer resources. That is true. I don't need to serve that many requests though. My peak access was a couple years ago with 50k visitors on a week, even my old Racket blog could handle that fine. The Lua one should handle it too; and if it breaks it breaks. I'm a flawed human being, my code can be flawed too, we're in this together, holding hands.&lt;/p&gt;
    &lt;p&gt;Your blog is your place to experiment and program how you want it. You can drop the JS fatigue, you can drop your fancy Haskell types, you can just do whatever you find fun and keep going (and that includes JS and Haskell if that's your thing. You do you).&lt;/p&gt;
    &lt;p&gt;Cause I'm using Lua, I don't have as many libraries and frameworks available to me as JS people have, but I still have quite a large collection via Luarocks. I try not to add many dependencies to my blog. At the moment there are about ten and that is mostly because Lua is a batteries-not-included language so you start from a minimal core and build things up to suit your needs.&lt;/p&gt;
    &lt;p&gt;For a lot of things I went with the questionable choice of implementing things myself. I got my own little CGI library. It is 200 lines long and does the bare minimum to make this blog work. I got my own little libraries for many things. Micropub and IndieAuth were all implemented by hand.&lt;/p&gt;
    &lt;p&gt;At the moment I'm &lt;del&gt;despairing&lt;/del&gt;&lt;del&gt;frustrated&lt;/del&gt; having a lot of fun implementing WebMentions. Doing the Microformats2 &lt;del&gt;exorcism&lt;/del&gt; extraction on my own is teaching me a lot of things.&lt;/p&gt;
    &lt;p&gt;What I want to say is that by choosing a small language that moves very slowly and very few dependencies, I can keep all of my blogging system in my head. I can make sure it will run without too much change for the next ten or twenty years.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Lua is a lego set, a toolkit, it adapts to you and your needs. I don't need to keep chasing the new shiny or the latest framework du jour. I can focus on making the features I want and actually understanding how they work.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Instead of installing a single dependency in another language and it pulling a hundred of other small dependencies all of which were transpiled into something the engine understands to the point that understanding how all the pieces work and fit together takes more time than to learn a new language, I decided to keep things simple.&lt;/p&gt;
    &lt;p&gt;I got 29 Luarocks installed here and that is for all my Lua projects in this machine. That is my blog, my game development, my own work scripts for my day job. Not even half of those are for my blog.&lt;/p&gt;
    &lt;p&gt;I often see wisdom in websites such as Hacker News and Lobsters around the idea of "choosing boring" because it is proven, safe, easier to maintain. I think that boring is not necessarily applicable to my case. I don't find Lua boring at all, but all that those blog posts talk about that kind of mindset are all applicable to my own choices here.&lt;/p&gt;
    &lt;p&gt;Next time you're building your own blogging software, consider for a bit for how long do you want to maintain it. I first started blogging on macOS 8 in 2001. I choose badly many times and in the end couldn't keep my content moving forward in time with me as softwares I used or created became impossible to run. The last two changes: from JS to Racket and from Racket to Lua have been a lot safer and I managed to carry all my content forward into increasingly simpler setups and workflows.&lt;/p&gt;
    &lt;p&gt;My blogging system is not becoming more complex over the years, it is becoming smaller, because with each change I select a stack that is more nimble and smaller than the one I had before. I don't think I can go smaller than Lua though.&lt;/p&gt;
    &lt;p&gt;By small I mean:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A language I can fully understand and keep on my head.&lt;/item&gt;
      &lt;item&gt;A language that I know how to build the engine and can do it if needed.&lt;/item&gt;
      &lt;item&gt;An engine that requires very few resources and is easy to interface with third-party libraries in native code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I chose Lua because of all that, and I'm happy with it and hope this engine will see me through the next ten or so years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45452261</guid><pubDate>Thu, 02 Oct 2025 16:58:55 +0000</pubDate></item><item><title>Liva AI (YC S25) Is Hiring</title><link>https://www.ycombinator.com/companies/liva-ai/jobs/6xM8JYU-founding-operations-lead</link><description>&lt;doc fingerprint="a3de624f74d97bb3"&gt;
  &lt;main&gt;
    &lt;p&gt;Scale AI for video and voice data.&lt;/p&gt;
    &lt;p&gt;The mission at Liva AI (YC S25) is to make AI feel truly human. AI voices and faces today still feel flat and generic, missing emotion, nuance, and identity. We’re fixing that by building the world’s richest library of human voice and video data, fueling the next generation of realistic AI.&lt;/p&gt;
    &lt;p&gt;We’re hiring an extremely organized and committed operator to take on a full-time role. You’ll manage complex projects and people with efficiency, solve problems in uncertain situations, and help us scale fast. Over time, you’ll also play a key role in building the most automated operations system of any data company, translating the workflows you run today into scalable processes and overseeing the internal systems we’re developing.&lt;/p&gt;
    &lt;p&gt;This is a founding role: your work will directly fuel the next generation of AI in a tangible way, while shaping the foundation of how Liva runs at scale.&lt;/p&gt;
    &lt;p&gt;ABOUT THE ROLE&lt;/p&gt;
    &lt;p&gt;What you’ll do:&lt;/p&gt;
    &lt;p&gt;WHAT WE’RE LOOKING FOR&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;p&gt;Nice to have:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; BENEFITS:&lt;/p&gt;
    &lt;p&gt;Liva's mission is to make AI look and sound truly human. The AI voices and faces today feel off, and lack the capability to reflect diverse people across different ethnicities, races, accents, and career professions. We’re fixing that by building the world’s richest library of human voice and video data, fueling the next generation of realistic AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45452299</guid><pubDate>Thu, 02 Oct 2025 17:01:16 +0000</pubDate></item><item><title>Babel is why I keep blogging with Emacs</title><link>https://entropicthoughts.com/why-stick-to-emacs-blog</link><description>&lt;doc fingerprint="26d8bd04cf2550b8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I Keep Blogging With Emacs&lt;/head&gt;
    &lt;p&gt;Every time I look at someone’s simple static site generation setup for their blog, I feel a pang of envy. I’m sure I could make a decent blogging engine in 2,000 lines of code, and it would be something I’d understand, be proud over, able to extend, and willing to share with others.&lt;/p&gt;
    &lt;p&gt; Instead, I write these articles in Org mode, and use mostly the standard Org publishing functions to export them to html. This is sometimes brittle, but most annoyingly, I don’t understand it. I have been asked for details on how my publishing flow works, but the truth is I have no idea what happens when I run the &lt;code&gt;org-publish-current-file&lt;/code&gt; command.
&lt;/p&gt;
    &lt;p&gt; I could find out by tracing the evaluation of the Lisp code that runs on export, but I won’t, because just the html exporting code (&lt;code&gt;ox-html.el&lt;/code&gt;) is 5,000
lines of complexity. The general exporting framework (&lt;code&gt;ox-publish.el&lt;/code&gt; and
&lt;code&gt;ox.el&lt;/code&gt;) is 8,000 lines. The framework depends on Org parsing code
(&lt;code&gt;org-element.el&lt;/code&gt;) which is at least another 9,000 lines. This is over 20,000
lines of complexity I’d need to contend with.
&lt;/p&gt;
    &lt;p&gt;It might seem like a no-brainer to just write that 2,000 line custom static generator and use that instead.&lt;/p&gt;
    &lt;p&gt;Except one thing: Babel.&lt;/p&gt;
    &lt;p&gt;Any lightweight markup format (like Markdown or ReStructuredText or whatever) allows for embedding code blocks, but Org, through Babel, can run that code on export, and then display the output in the published document, even when the output is a table or an image. It supports sessions that lets code reuse definitions from earlier code blocks. It allows for injecting variables from the markup into the code, and vice versa. As a bonus, Org doesn’t require a JavaScript syntax highlighter, because it generates inline styles in the source code.&lt;/p&gt;
    &lt;p&gt;It does this for a large number of languages, although I mainly use it with R for drawing plots. Being able to do this is incredibly convenient, because it makes it trivial to draft data, illustrations, and text at the same time, adjusting both until the article coheres. Having tried it, I cannot see myself living without it.&lt;/p&gt;
    &lt;p&gt;A simple 2,000 line blogging engine would be a fun weekend project. Mirroring the features of Babel I use would turn it into a multi-month endeavour for someone with limited time such as myself. Not going to happen, and I will continue to beat myself up for overcomplicating my publishing workflow.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453222</guid><pubDate>Thu, 02 Oct 2025 18:06:41 +0000</pubDate></item><item><title>Gemini 3.0 Pro – early tests</title><link>https://twitter.com/chetaslua/status/1973694615518880236</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453448</guid><pubDate>Thu, 02 Oct 2025 18:26:57 +0000</pubDate></item><item><title>OpenAI's H1 2025: $4.3B in income, $13.5B in loss</title><link>https://www.techinasia.com/news/openais-revenue-rises-16-to-4-3b-in-h1-2025</link><description>&lt;doc fingerprint="d267d1111c03bee1"&gt;
  &lt;main&gt;
    &lt;p&gt;If you're seeing this message, that means JavaScript has been disabled on your browser.&lt;/p&gt;
    &lt;p&gt;Please enable JavaScript to make this website work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453586</guid><pubDate>Thu, 02 Oct 2025 18:37:28 +0000</pubDate></item><item><title>Why most product planning is bad and what to do about it</title><link>https://blog.railway.com/p/product-planning-improvement</link><description>&lt;doc fingerprint="ad29913538a777e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why most product planning is bad and what to do about it&lt;/head&gt;
    &lt;p&gt;TL;DR: We tried OKRs, they created more ceremony than clarity. Our solution: Problem Driven Development, a 4-day quarterly process focused on identifying problems (not solutions), prioritizing as a team, and committing publicly. It's kept us shipping at velocity even as we've scaled to 1.7M+ users.&lt;/p&gt;
    &lt;p&gt;For most of my friends and colleagues at mature software companies, there are usually three ways for an item of work to get put on the board to eventually be done.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;There is a giant ceremony that determines what gets done by a grab bag of metrics.&lt;/item&gt;
      &lt;item&gt;A deal gets blocked by a missing feature, and the engineering team scrambles jets to eliminate the blocker.&lt;/item&gt;
      &lt;item&gt;Founder feels like we have to build something.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thats not to say that every company is a disorganized mess or a bureaucratic hell scape but, I have never met any engineer who said: “Wow, I just love my company planning process.”&lt;/p&gt;
    &lt;p&gt;These words are seldom spoken in the english language.&lt;/p&gt;
    &lt;p&gt;Railway, was fast approaching 1. and 2. at the same time.&lt;/p&gt;
    &lt;p&gt;Despite us using excellent tools (shoutout Linear) planning is much as a cultural phenomena as well as an interesting engineering problem. From our perspective, we would finalize the features and the requirements what we would want to build once every 3 months and then at times get blindsided every now and then from a new business priority or an incident. Especially now serving 1.7M+ customers, we have to aggregate our taste, feedback, and opportunities and make a roadmap that will get us everything we ever wanted.&lt;/p&gt;
    &lt;p&gt;This was an issue.&lt;/p&gt;
    &lt;p&gt;Why write about something that you would read in Rand’s? It’s Q4 for those who celebrate, and we felt that reigniting the agile vs. waterfall armistice needed to be torn up. (Besides, we’re blogging like it’s 2005.)&lt;/p&gt;
    &lt;p&gt;…and we have spent the better part of 18 months to improve our planning so that we finally get to a process that is not bad, and if you’d like, you can steal it, so that you can deliver excellent products.&lt;/p&gt;
    &lt;p&gt;Mike Tyson asked about his fight plan against Evander Holyfield responded with; “Everyone has a plan until they get punched in the mouth.”&lt;/p&gt;
    &lt;p&gt;And that quote ever since would be used out of context to fight the implementation of Objectives, Key Results across the world. At Railway, we aren’t Anti-Planning, we are Anti-Bad-Planning and as such, we used to avoid a lot of it because we always felt that a bad process is massively negative to the status quo. However, we did get to the point where we needed to finally harness the collective attention spans of the company and work towards a goal.&lt;/p&gt;
    &lt;p&gt;Railway at the time was (and is) a vision led company. We fundamentally believe that most infrastructure boilerplate shouldn’t exist so that you can work on what matters. Back in ‘21, we went from a world where we could do our work in weeks, to work that required us to think in months. I implemented a somewhat lighter form of SAFE which splits work between “sprints”, which were short pieces of work, then initiatives which helped grouped “epics” which are long running work-streams to deliver a feature.&lt;/p&gt;
    &lt;p&gt;Despite us having a strong founder, we still wanted the ability to have employees bring projects that they would be excited to work on.&lt;/p&gt;
    &lt;p&gt;As with most young companies, we found quickly that new work would come in and unseat the old work and we would begin anew.&lt;/p&gt;
    &lt;p&gt;So then we turned to OKRs.&lt;/p&gt;
    &lt;p&gt;We thought that having some long term objectives will help us focus the company.&lt;/p&gt;
    &lt;p&gt;We implemented them faithfully, we all read the John Doer book. …and it worked… somewhat.&lt;/p&gt;
    &lt;p&gt;We can make this a whole blog post on OKRs. But, trying to hold back here from just ranting and to give you the relevant information.&lt;/p&gt;
    &lt;p&gt;OKRs work REALLY well when you have concrete goals and really simple ways to measure them. Its why they belong at their primary birthplace, the factory. Objectives work when you have something binary or “limited”, like a product existing… or not. Or a product meeting a benchmark… or not. It’s easy to rally a team around them and conquer the world.&lt;/p&gt;
    &lt;p&gt;Where OKRs start to falter, are when you need to use them to prioritize work to meet a “unlimited” objective. Like: “Increase conversion rate of landing page”&lt;/p&gt;
    &lt;p&gt;OKRheads will notice that’s a particularly weak objective, however, teaching an entire organization to all of a sudden be great goal setters, which is what OKRs require you to do made it difficult for engineers to 1) bring them and then 2) plan work around them. Which leads to the first big issue of OKRs- which is that it really depends on the human psychology of the team. Engineers straddle the line between concrete numbers such as uptime, and more creative endeavors such as figuring out how to get a feature implemented right. Whenever the work enters the creative realm, the wheels come off.&lt;/p&gt;
    &lt;p&gt;Which is why outside of the factory, OKRs are great for Sales. You set a number, you get alignment on hitting that number, and then you pick up the phone, email, or LinkedIn until you hit it. The psychology of an account executive matches the planning process.&lt;/p&gt;
    &lt;p&gt;Where OKRs work great, is for alignment. If you are able to set great goals and if you wanted to prescribe a bunch of work toward said goal. …and most startups reach for the High Output Management book (which I love) because there is the lack of alignment between different orgs at a startup. Which is to say, I don’t shame any company reaching for them, the same way newborns have the palmar grasp reflex. We yearn for the alignment.&lt;/p&gt;
    &lt;p&gt;The second big issue for OKRs is that they are, by design, inflexible, once you commit to them. If you spend a week or two planning for them for the year or quarter, and you are midway through realizing that the “KR” part is incorrect, thats a one way door.&lt;/p&gt;
    &lt;p&gt;So we felt pressure into making sure that the OKRs were indeed correct. Which is where you enter the issues of the performative aspects of planning. It felt “mature” for Railway to be having these discussions, even though… it wasn’t productive.&lt;/p&gt;
    &lt;p&gt;At Railway, we would spend a significant amount of time discussing what is a valid OKR, only for us to realize we were two days into the planning process and we’ve yet to decide what we should build for the OKR.&lt;/p&gt;
    &lt;p&gt;This is when Christian, our Head of Operations had an answer for us.&lt;/p&gt;
    &lt;p&gt;Instead of crowd sourcing the OKRs from the company and bubbling them up per function Jake and Christian would work on some top level guidance that would provide a macro view of our finances, priorities, and strategy that we have to keep in mind.&lt;/p&gt;
    &lt;p&gt;We then have a Hex dashboard with topline metrics from different sides of the product and business. From a GTM perspective, it’s engagement, signups, and revenue. From an Engineering perspective, it’s support tickets, uptime, and feature performance.&lt;/p&gt;
    &lt;p&gt;This data would give us a “theme”. (You may have seen them in our launch weeks.)&lt;/p&gt;
    &lt;p&gt;Whenever someone goes on Central Station and requests a feature, depending on how large the work is, that usually determines if that request becomes a “Project”.&lt;/p&gt;
    &lt;p&gt;Then around the start of 2024, Christian spun up a Notion DB, and punched in a simple template for each new entry.&lt;/p&gt;
    &lt;p&gt;Railway at this point and time was organized into three sections of the business. Product Engineering which delivers features and value to our customers via the UI and terminal. Platform which supports the product with the Infrastructure and the APIs to control that Infrastructure. Then Logistics, which is a synthetic team which includes the Support, Marketing, and Sales function working to be the voice of the customer.&lt;/p&gt;
    &lt;p&gt;We would then fill out a “Project Candidate” and then go to bat on trying to figure out if was a priority that tied to the “theme” that was presented.&lt;/p&gt;
    &lt;p&gt;If something is a P0: it’s an existential company risk, we MUST deliver it.&lt;/p&gt;
    &lt;p&gt;Then, a P1: something we need to deliver for this quarter&lt;/p&gt;
    &lt;p&gt;Lastly, P2: a nice to have.&lt;/p&gt;
    &lt;p&gt;This worked well until some cracks formed.&lt;/p&gt;
    &lt;p&gt;At the start of this new process, our team was small enough to list maybe 50 project candidates, discuss as a group and then be on with it but with 200. Spending all day on a call with your co-workers to discuss if something was worth it isn’t the best use of time.&lt;/p&gt;
    &lt;p&gt;Second, Project Candidates would sometimes have full on fleshed out RFC style sections where the author would have a solution ready to go. When a candidate got deprioritized, it’s understandable why someone would feel not great about their hard effort not being reciprocated.&lt;/p&gt;
    &lt;p&gt;Third, to deal with the larger amount of project candidates and additional sources of project candidates, we would set up additional meetings before the planning week that would eat into our engineering cycles. Not that we want to squeeze every engineer for what they are worth, but these negotiation calls would take longer than the main call sometimes. Worse even, was that we would completely reset the board quarter after quarter.&lt;/p&gt;
    &lt;p&gt;Lastly, which was starting to be a bigger issue, was that having Project Candidates being a bottom up process was great when the people who knew that work they needed to do can fit the whole work-stream in their head. But… we were about to embark on a multi-quarter effort that would span all parts of the business to deliver Railway Metal. This system was not great for uncovering planning gaps that required attention from a different team.&lt;/p&gt;
    &lt;p&gt;(With that said, we did ship some really good software thanks to this planning process.)&lt;/p&gt;
    &lt;p&gt;However, Jake, the team, aren’t ones to sit on their laurels so we refined it until we have our current process.&lt;/p&gt;
    &lt;p&gt;So we flipped the project system on it’s head.&lt;/p&gt;
    &lt;p&gt;Instead, we practice what I call: “Problem Driven Development”&lt;/p&gt;
    &lt;p&gt;Rather than spending a loaded amount of time picking feedback items from our feedback systems, and spending a loaded amount of time on trying to flesh out requirements. We collect problems on a continuous process that begins in earnest on the Friday before our planning week.&lt;/p&gt;
    &lt;p&gt;We stopped asking people to propose solutions and started asking them to articulate problems. No more half-baked RFCs that people felt attached to. No more solution-first thinking that locked us into approaches before we understood what we were solving. Just clear problem statements.&lt;/p&gt;
    &lt;p&gt;Here's what a problem looks like in our system:&lt;/p&gt;
    &lt;p&gt;Problem Title: "Users can't debug failed deployments without SSHing into containers"&lt;/p&gt;
    &lt;p&gt;Notice what's missing? Any mention of how we'd solve it. That comes later, after we've committed to solving the problem. Then we turned the two week ceremony into a lightning 4 day sprint.&lt;/p&gt;
    &lt;p&gt;After the problems are listed and filled out fully, each team on Day 1 enters problems on their own. If a problem isn't fleshed out, it's put in the Parking Lot and we kindly nudge the idea person to fill the template.&lt;/p&gt;
    &lt;p&gt;By the time we get into a room together, everyone's had time to process what's on the board.&lt;/p&gt;
    &lt;p&gt;Then on Day 2, the planning captain will have a closed session with the team (with spectators from other teams) to put a best guess priority from P0, P1, and P2. If we find that a problem is contentious or requires an external dependency, we mark it so the day after, we can discuss it.&lt;/p&gt;
    &lt;p&gt;By having each team prioritize independently first, we avoid the tragedy of the commons where everything becomes P0 because someone shouted loudest on the call. Platform can look at their problems and say "yes, API reliability is more important than API versioning right now" without having to negotiate with Product in real-time.&lt;/p&gt;
    &lt;p&gt;(We have spent countless hours talking about difficult questions on a call when we have been able to diffuse rough conversations ahead of time.)&lt;/p&gt;
    &lt;p&gt;On Day 3, we get to a 95 percent certainty on the priorities listed and tie break the dependencies. We also confirm that we have the capacity and staffing to deal with the problems listed on the board. If not, we add a problem for hiring for a specific role.&lt;/p&gt;
    &lt;p&gt;This is where cross-team dependencies surface naturally. When Platform marks "Multi-mount volumes" as P1 and Product marks "HA DBs" as P1, we can see that one blocks the other. We're not discovering this mid-quarter when someone's already three weeks into building the wrong thing.&lt;/p&gt;
    &lt;p&gt;The capacity check is crucial too. We look at people's current commitments, oncall rotations, and whether anyone's about to take parental leave. If we have eight P1 problems and capacity for five, we have an honest conversation about what drops to P2 or what we need to hire for.&lt;/p&gt;
    &lt;p&gt;Then lastly, Day 4, commitment. Everyone looks at the whole list, mention final objections if we should be working on something that we aren't— or vice versa. After we confirm priorities, we assign problems to people by giving them a DRI (Directly Responsible Individual).&lt;/p&gt;
    &lt;p&gt;Then, we look into each other's eyes, as much as anyone can on a video call, and commit.&lt;/p&gt;
    &lt;p&gt;After the starting gun is off, we then write RFCs on problems to see how we can best solve them which eventually become Linear tickets.&lt;/p&gt;
    &lt;p&gt;We're working on open-sourcing our Notion templates because honestly, this isn't rocket science. It's just:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cataloguing your problems, not solutions&lt;/item&gt;
      &lt;item&gt;Let teams prioritize independently before negotiating&lt;/item&gt;
      &lt;item&gt;Front-load the hard conversations&lt;/item&gt;
      &lt;item&gt;Commit publicly&lt;/item&gt;
      &lt;item&gt;Then, and only then, figure out how to solve it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the real hard part was building a culture where people feel safe bringing up problems without having to defend their pet solutions.&lt;/p&gt;
    &lt;p&gt;Keep process to a minimum, focus on performance of shipping, not performative work.&lt;/p&gt;
    &lt;p&gt;And like any good product, our planning process will keep evolving. Maybe in six months we'll realize Problem Driven Development has its own cracks. Theres always plenty to do on planning and I think we generally know how it’ll need to evolve, but we try to take small steps from cycle to cycle. If that happens, we'll write another blog post about what we learned and what we changed.&lt;/p&gt;
    &lt;p&gt;Until then, we're shipping.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45454374</guid><pubDate>Thu, 02 Oct 2025 19:34:08 +0000</pubDate></item><item><title>Anti-aging breakthrough: Stem cells reverse signs of aging in monkeys</title><link>https://www.nad.com/news/anti-aging-breakthrough-stem-cells-reverse-signs-of-aging-in-monkeys</link><description>&lt;doc fingerprint="3648d18e38a2aa99"&gt;
  &lt;main&gt;
    &lt;p&gt;Key Points:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Super stem cells” improve the memory of monkeys while protecting against neurodegeneration.&lt;/item&gt;
      &lt;item&gt;The super stem cells prevent age-related bone loss while rejuvenating over 50% of the 61 tissues analyzed.&lt;/item&gt;
      &lt;item&gt;Treatment with stem cells reduces inflammation and senescent cells (cells that accumulate to promote aging).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While small in number, our adult stem cells play a crucial role in regenerating our lost or damaged tissues, rebuilding our body cell by cell. However, with age, our bodies become riddled with inflammation, hardly providing an environment capable of keeping our stem cells healthy. Eventually, our stem cells lose their regenerative capacity, contributing to degenerative aging.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fox, O, Three&lt;/head&gt;
    &lt;p&gt;Hydra are a genus of immortal beings that live forever in freshwater environments like lakes and ponds. What scientists have called “nothing but an active stem cell community,” Hydra can escape death by infinitely regenerating. Their stem cells can continuously proliferate and renew by producing FoxO, a protein they share with humans.&lt;/p&gt;
    &lt;p&gt;In humans, the FoxO protein, specifically the FoxO3 isoform, responds to cellular stress by binding to DNA and turning genes on and off. These genes are involved in numerous cellular processes that promote healthy aging and extended lifespan. This is why the FoxO3 protein’s corresponding gene, FOXO3, is considered a longevity gene.&lt;/p&gt;
    &lt;head rend="h2"&gt;Experimenting with SRCs&lt;/head&gt;
    &lt;p&gt;As FoxO3 assists cells in resisting stressful environments, such as inflamed tissue, Chinese Academy of Sciences researchers engineered human stem cells to have enhanced FoxO3 activity. As published in Cell, these senescence-resistant stem cells (SRCs) were designed to exhibit greater resistance to age-related stress. To test this, cynomolgus monkeys, also known as crab-eating macaques, were first stratified into four groups based on age:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A1: 3-5 years (approximately equivalent to 9-15 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A2: 10-12 years (approximately equivalent to 30-36 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A3: 16-19 years (approximately equivalent to 48-57 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A4: 19-23 years (approximately equivalent to 57-69 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The oldest of the monkeys, the A4 group, were the focus of the study and were subdivided into three groups. One group was injected with saline (salt and water), another group with normal stem cells, and the third with SRCs. The aged monkeys were injected every two weeks for 44 weeks, approximately equivalent to the duration of three human years. On safety, there were no serious adverse events, such as immune system rejection or tumor growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;SRCs Improve Cognition&lt;/head&gt;
    &lt;p&gt;After 44 weeks of biweekly injections, a suite of biological indices was measured from the aged monkeys to assess whether SRCs slow down biological aging. One of these indices was memory retention. To assess memory, the researchers conducted a common experiment called the Wisconsin General Test Apparatus (WGTA).&lt;/p&gt;
    &lt;p&gt;For this experiment, each monkey was trained to retrieve food located outside of one of two identical boxes. During the test session, after each monkey was trained, food was placed next to one of the boxes to keep it hidden. Subsequently, a flap was placed in front of the monkey to block the boxes from view. Three seconds later, when the flap was reopened, each monkey had to remember which of the two boxes contained the food.&lt;/p&gt;
    &lt;p&gt;Remarkably, the monkeys injected with SRCs remembered the location of the food with higher accuracy than the monkeys injected with saline. Moreover, the monkeys injected with normal stem cells exhibited the same level of accuracy as the monkeys injected with saline. These findings suggest that SRCs, and not normal stem cells, improve the memory of aged monkeys.&lt;/p&gt;
    &lt;p&gt;Furthermore, MRI-based structural analysis showed that treatment with SRCs mitigated age-related brain shrinkage. MRI-based experiments also revealed that brain connectivity was restored to that of young (A1 group) monkeys. Namely, the structural connectivity between seven brain regions, including those important for working memory (prefrontal cortex), was rejuvenated with SRC treatment. Overall, these findings suggest that SRC injections improve memory by protecting against neurodegeneration.&lt;/p&gt;
    &lt;head rend="h2"&gt;SRCs Rejuvenate Many Organs and Tissues&lt;/head&gt;
    &lt;p&gt;In addition to the brain, the Academy researchers found that SRC treatment rejuvenated multiple organs and tissues. This is important because the rejuvenation of a given organ or tissue could lead to the reduced risk of its corresponding age-related chronic diseases. For example, the rejuvenating effects of SRCs on the brain could reduce the risk of neurodegenerative disorders like Alzheimer’s and Parkinson’s diseases.&lt;/p&gt;
    &lt;p&gt;One common age-related disease is osteoporosis, characterized by brittle and weak bones that make patients more prone to fractures and deadly falls. Using an X-ray imaging technique called micro-CT, the researchers found evidence for the reversal of age-related bone loss. Namely, while the aged monkeys treated with saline exhibited dental bone loss, the aged monkeys treated with SRCs had teeth more similar to those of young monkeys.&lt;/p&gt;
    &lt;p&gt;To conduct a body-wide assessment, the researchers measured the up- and down-regulation of genes from 10 systems and 61 tissues. Elevations and reductions in gene activation reflect the function (or dysfunction) of cells, tissues, and organ systems. With that said, SRC treatment was shown to rejuvenate over 50% of the tissues examined, with maximal rejuvenation achieved in areas like the hippocampus (the memory consolidation center of the brain), fallopian tubes, and colon. In contrast, the regular stem cells rejuvenated about 30% of the tissues examined.&lt;/p&gt;
    &lt;p&gt;Confirming some of the rejuvenating effects inferred by the gene experiments, the researchers also observed structural changes to various organs and tissues from aged monkeys treated with SRCs. For example, the vascularity of the lung and heart was improved while the thickening of the aorta was reduced. Neurons had longer projections and fewer proteins associated with Alzheimer’s disease (e.g., beta-amyloid and phosphorylated-tau), and the kidney and brain showed less mineralization [abnormal mineral (usually calcium) deposits].&lt;/p&gt;
    &lt;head rend="h2"&gt;SRCs Reduce Cellular Senescence and Inflammation&lt;/head&gt;
    &lt;p&gt;Modern scientists have begun to unravel the underlying causes of aging by identifying commonalities between age-related diseases at the cellular level. Two of the most prominent purported underlying causes of aging are chronic inflammation and senescent cells. With age, senescent cells accumulate throughout the body, promoting inflammation by secreting pro-inflammatory molecules.&lt;/p&gt;
    &lt;p&gt;Eliminating senescent cells, which can be achieved through genetic manipulation or compounds called senolytics, ameliorates age-related diseases and even extends the lifespan of model organisms. Now, the Academy researchers demonstrate that SRCs reduce senescent cells, measured using a blue dye called SA-β-Gal, in multiple organs, including the brain, heart, and lungs. Along those lines, SRC treatment also reduced markers of inflammation and other underlying causes of aging, like DNA damage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stem Cells Make Sense&lt;/head&gt;
    &lt;p&gt;When it comes to combating degenerative aging, it makes sense that regenerative stem cells are a promising solution. In fact, one of the underlying causes of aging is stem cell exhaustion, whereby stem cells lose their regenerative capacity. While normal stem cells have anti-aging effects, as shown by the Chinese Academy of Sciences researchers, they are not protected against stressors like age-related inflammation. This explains why SRCs provide enhanced regenerative capacity (they withstand the harsh microenvironments induced by aging and cellular senescence).&lt;/p&gt;
    &lt;p&gt;As no serious safety concerns were raised during the study, it would seem that SRCs are well tolerated. However, the long-term effects of the SRC treatment will need further evaluation. The primary concern with injecting stem cells into the bloodstream is that they can trigger the spread of cancer almost anywhere in the body. Nevertheless, the SRCs possess tumor suppression properties, suggesting they may not induce tumor growth. If this ends up being true, we may soon see SRCs being tested in humans.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45454460</guid><pubDate>Thu, 02 Oct 2025 19:39:08 +0000</pubDate></item><item><title>The strangest letter of the alphabet: The rise and fall of yogh</title><link>https://www.deadlanguagesociety.com/p/history-of-letter-yogh</link><description>&lt;doc fingerprint="b6728524aea13b82"&gt;
  &lt;main&gt;
    &lt;p&gt;English spelling has a reputation. And it’s not a good one.&lt;/p&gt;
    &lt;p&gt;It’s full of silent letters, as in numb, knee, and honour. A given sound can be spelled in multiple ways (farm, laugh, photo), and many letters make multiple sounds (get, gist, mirage).&lt;/p&gt;
    &lt;p&gt;English spelling is so complex that we’ve made mastering it into a competitive sport: what would be the point of a spelling bee in a language with a predictable spelling system? Where’s the fun unless you have to sweat a little as you struggle to recall whether this particular word is one where “‘i’ before ‘e’ except after ‘c’” doesn’t apply?&lt;/p&gt;
    &lt;p&gt;In short, English has a complicated writing system.&lt;/p&gt;
    &lt;p&gt;I’ve written about the origin of some of this complexity before, blaming everyone from the French to stingy printers and late medieval yuppies. But I’ve not yet plumbed the depths of this complexity. To do so, I will need to tell you the story of yogh,1 an obscure medieval letter whose rise and fall allows us to peer into this abyss.&lt;/p&gt;
    &lt;p&gt;But like an Icelandic family saga, we begin not with the story of yogh, but with the story of its parent. So allow me to introduce you to the letter ‘g,’ which, as you’ll soon see, is a complicated letter in its own right, dating back to Old English.&lt;/p&gt;
    &lt;p&gt;It starts with the shape of the letter. When modern editors print Old English today, they print nice, modern-looking ‘g’s — that is, the ones we use today, with an open or closed loop on the bottom, depending on the typeface.&lt;/p&gt;
    &lt;p&gt;This modern form of ‘g’ is called the Carolingian ‘g.’ It had its origin in the Carolingian minuscule, the script used by the scribes of the Carolingian Renaissance, the great revival of learning which flourished in the vast realm of Charlemagne (reigned 768–814).2&lt;/p&gt;
    &lt;p&gt;But Old English scribes didn’t write their g-sounds with a Carolingian ‘g.’ The Old English letter ‘g’ was written in a form called the insular ‘g.’ Here’s what it looked like: ‘ᵹ.’ It’s like a mix between a ‘z’ and a ‘3.’&lt;/p&gt;
    &lt;p&gt;Let’s see it in action. When the first lines of Beowulf are written in a modern edition, they look like this:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Hwæt, we Gardena in geardagum&lt;/p&gt;&lt;lb/&gt;þeodcyninga þrym gefrunon,&lt;p&gt;‘How we have heard of the glory of the kings of the spear-Danes in days of old’&lt;/p&gt;&lt;p&gt;(Beowulf 1–2)&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;But in the manuscript, they’re written like this:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;HǷÆT ǷE GARDE&lt;/p&gt;&lt;lb/&gt;na inᵹear daᵹum þeod cyninᵹa&lt;lb/&gt;þrym ᵹefrunon&lt;p&gt;London, British Library, Cotton MS Vitellius A XV, f. 132r.&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;Now, there’s clearly lots of other weird stuff going on in the manuscript, but focus on how the ‘g’ is represented. While the majuscule (capital) ‘G’ in ‘gardena’ is spelled more like a modern ‘g,’ all the others are insular ‘ᵹ.’&lt;/p&gt;
    &lt;p&gt;Interestingly, the Anglo-Saxons did use the Carolingian ‘g’ — just not for Old English. They used it when writing Latin, at least after the late 10th century. This was when the Church in England underwent a set of reforms, which caused a flowering of literature both in Latin and Old English.3 As part of these reforms, the Carolingian minuscule script was adopted for Latin texts.&lt;/p&gt;
    &lt;p&gt;So for a period, both ‘g’ and ‘ᵹ’ were used in England, but generally speaking, each was used for writing the ‘g’ sound in a different language. Simple enough, but the stage was set for things to get a lot more complicated.&lt;/p&gt;
    &lt;p&gt;But for that, we need the help of the Normans.&lt;/p&gt;
    &lt;p&gt;You're reading The Dead Language Society. I'm Colin Gorrie, linguist, ancient language teacher, and your guide through the history of the English language and its relatives.&lt;/p&gt;
    &lt;p&gt;Subscribe for a free issue every Wednesday, or upgrade to support my mission of bringing historical linguistics out of the ivory tower and receive two extra Saturday deep-dives per month.&lt;/p&gt;
    &lt;p&gt;If you upgrade, you’ll also be able to join our ongoing Beowulf Book Club and watch our discussion of the first 915 lines (part 1, part 2) right away.&lt;/p&gt;
    &lt;head rend="h1"&gt;Of course, they would have spelled it ȝoȝ&lt;/head&gt;
    &lt;p&gt;For the history of the English language, no single year was more momentous than 1066. In this year, William, Duke of Normandy, invaded and took the English throne, bringing with him Norman knights, and more importantly for our purposes, Norman scribes.&lt;/p&gt;
    &lt;p&gt;These Norman scribes inherited the writing traditions that the Carolingian renaissance had given birth to. This meant the latest, greatest, 11th-century French versions of the Carolingian minuscule.&lt;/p&gt;
    &lt;p&gt;These weren’t so different from the way Anglo-Saxon scribes had written Latin. But they were very different from the way they had written Old English, especially in the ‘g’ department.&lt;/p&gt;
    &lt;p&gt;But that wasn’t so much of an issue, since these Norman-trained scribes, and those of the generations that came after them, didn’t write much English at all. In fact, writing in English of any kind was very scarce up until the end of the 12th century.&lt;/p&gt;
    &lt;p&gt;Over the course of that tumultuous — and, for English, silent — century, the language had changed a great deal. All the scribes trained in the old, Anglo-Saxon traditions were long dead, so when a new generation of scribes turned their attention once again to English, they had to devise some new strategies for writing it.&lt;/p&gt;
    &lt;p&gt;And this, after a surprisingly long delay, is where we first meet the star of today’s issue: ‘ȝ,’ also known as yogh.&lt;/p&gt;
    &lt;p&gt;Yogh is descended from a variant form of the old insular ‘ᵹ.’ But, while the insular ‘ᵹ’ was thought of as the same letter as the Carolingian ‘g’ in Anglo-Saxon times, the yogh ‘ȝ’ of the 12th century was an entirely different letter from the Carolingian-derived ‘g.’&lt;/p&gt;
    &lt;p&gt;And, stranger still, ‘ȝ’ was used to write two completely different sounds in Middle English, the form of English spoken from around 1100–1450: the y-sound as in young or yesterday, and another sound that English has lost altogether.&lt;/p&gt;
    &lt;p&gt;The other sound that ‘ȝ’ once spelled is the “harsh” or “guttural” sound made in the back of the mouth, which you hear in Scots loch or German Bach.4 This sound is actually the reason for the most famous bit of English spelling chaos: the sometimes-silent, sometimes-not sequence ‘gh’ that you see in laugh, cough, night, and daughter. Maybe one day I’ll tell you that story too.&lt;/p&gt;
    &lt;p&gt;For today, however, just know that the spelling ‘gh,’ which causes spellers so much trouble, was originally a replacement for ‘ȝ’ in these words. But more on that later. Let’s dwell for a moment on the bizarre situation we had in Middle English, where the same letter ‘ȝ’ could represent either a y-sound or that now-vanished gh-sound.&lt;/p&gt;
    &lt;head rend="h1"&gt;But not as bizarre as this painting&lt;/head&gt;
    &lt;p&gt;Or is it actually so bizarre?&lt;/p&gt;
    &lt;p&gt;Modern English spelling is, of course, chaotic. So perhaps it shouldn’t surprise us that we too have a very yogh-like situation with two of our letters: ‘c’ and — wait for it — ‘g.’&lt;/p&gt;
    &lt;p&gt;Each of these regularly represents two not particularly similar sounds. The letter ‘c’ sometimes represents a k-sound, like in cat, and sometimes an s-sound, like in city. The letter ‘g’ is similar: sometimes it writes a true g-sound, like in good, but other times, what it represents is a j-sound, like in gem.&lt;/p&gt;
    &lt;p&gt;If these sounds seem similar to you, pay attention to your tongue as you make each one: the k-sound and the true g-sound are made with the back of your tongue hitting against your soft palate. The s-sound and the ‘j’ sound are made in slightly different places, but in both cases, they use the tip of your tongue coming up against (or close to) just behind your upper teeth.&lt;/p&gt;
    &lt;p&gt;Each of our two double-sounding letters, ‘c’ and ‘g,’ has two variants, which are made at completely different ends of the mouth. This is how yogh worked too: it had one variant made at the back of the mouth (the gh-sound) and the other made towards the front (that’s the y-sound).&lt;/p&gt;
    &lt;p&gt;There’s actually a good linguistic reason why this pattern keeps happening. It’s a sound change called palatalization: this happens when a sound made towards the back of the mouth, like a k- or g-sound, gets pulled forward because it’s next to another sound made at the front of the mouth. Often, these front-of-the-mouth sounds are vowels.&lt;/p&gt;
    &lt;p&gt;In Old English, these front-of-the-mouth vowels (front vowels for short) included the ones spelled ‘i’ and ‘e,’ which sounded like the vowels in bee and bay.5 Sometime in the deep prehistory of the English language, the ‘g’ sound got pulled forward in the mouth to sound like ‘y’ whenever it was next to these front vowels.&lt;/p&gt;
    &lt;p&gt;But when it came time to spell these ‘y’ sounds in the Latin alphabet, they still seemed to the Anglo-Saxon scribes to be versions of ‘g’ sounds. So they spelled them ‘ᵹ’ just like they spelled other ‘g’ sounds. This is why, when you read Old English, you can often replace ‘ᵹ’ (or ‘g’ in modern editions) with ‘y’ and get recognizable Modern English words: ᵹear is year, dæᵹ is day, weᵹ is way.6&lt;/p&gt;
    &lt;p&gt;A process like this happened in the ancestor of French too, just slightly differently. Instead of the ‘g’ being pulled forward into a ‘y’ sound before ‘i’ and ‘e,’ it got pulled forward into a ‘j’ sound.&lt;/p&gt;
    &lt;p&gt;This is why the Modern English ‘g’ represents two sounds, one before the letters ‘i’ and ‘e’, and the other before the other letters. It’s because we took our spelling conventions from how the Norman scribes wrote their language, which was an old form of French.7&lt;/p&gt;
    &lt;p&gt;While the Norman scribes, and the later English scribes they trained, were used to the letter ‘g’ writing two different sounds, neither was a y-sound. They needed another letter for that, and they found one: the old insular ‘ᵹ,’ or rather, its descendant, the yogh ‘ȝ.’&lt;/p&gt;
    &lt;p&gt;So that’s why ‘ȝ’ spelled a ‘y’ sound. To understand why ‘ȝ’ also spelled that vanished ‘gh’ sound, however, we need to go back into the distant history of English, long before it was ever written down. Actually, long before there even was an English.&lt;/p&gt;
    &lt;p&gt;Back then, there was just one single language, which would later split into English, Dutch, German, Swedish, and all the other Germanic languages.&lt;/p&gt;
    &lt;p&gt;In this Proto-Germanic language (as it’s called today), the sound that would become the English g-sound — which the Anglo-Saxons would later spell ‘ᵹ’ — was not the g-sound we have today in words like good or bag. That came later.&lt;/p&gt;
    &lt;p&gt;Instead, the Proto-Germanic ancestor of words like good had a sound very similar to the later Middle English gh-sound at the start.8 Only later would English harden that gh-sound into the g-sound we know today. But this only happened in certain places in the word, especially at the start. At the end of the word, the gh-sound remained. In Old English, both versions were spelled the same: ‘ᵹ.’&lt;/p&gt;
    &lt;p&gt;This means that ‘ᵹ’ actually had three pronunciations in Old English, not two: the y-sound (next to front vowels), the g-sound (at the start of words), and the gh-sound (in the middle of words). So when those Norman-trained scribes turned their attention to writing English in the 12th century, they had no problem writing the g-sound at the start of words with ‘g.’&lt;/p&gt;
    &lt;p&gt;But when they wanted to write the gh-sound, they ran into the same problem they had in writing the y-sound. They didn’t have a good way to write the gh-sound, which didn’t exist in French at the time, so they pressed yogh into service again.&lt;/p&gt;
    &lt;p&gt;And that’s why yogh has two sounds, each of which corresponds to a pronunciation of the Old English letter ‘ᵹ’ that the French scribal tradition couldn’t accept writing with ‘g.’&lt;/p&gt;
    &lt;head rend="h1"&gt;Wait, we’ve been saying ‘Mackenzie’ wrong?&lt;/head&gt;
    &lt;p&gt;When you’re reading Middle English, it can get a bit confusing: Which kind of yogh is which?&lt;/p&gt;
    &lt;p&gt;Look at this line from Sir Gawain and the Green Knight:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Þaȝ ȝe ȝourself be talenttyf, to take hit to yourseluen,&lt;/p&gt;&lt;lb/&gt;‘Though you yourself are eager to accept it [a challenge] personally,’ (Sir Gawain and the Green Knight 350)&lt;/quote&gt;
    &lt;p&gt;In this line, the first yogh makes the gh-sound, while the second and third represent y-sounds. But you just have to know the words in order to figure that out.&lt;/p&gt;
    &lt;p&gt;As inconvenient as this confusion is for us modern readers of Middle English, that isn’t the reason yogh disappeared from the English language. The fate of yogh was sealed by a conspiracy of factors.&lt;/p&gt;
    &lt;p&gt;One is that yogh was never the only alternative for writing the sounds it wrote. The y-sound could also be written in the way the French wrote it, that is, ‘i’ or ‘y.’ The latter is the spelling that English ended up using for this sound, hence yourself instead of ȝourself. The gh-sound could also be written ‘h,’ ‘ȝh,’ or ‘gh.’ The last of these, of course, is what English ended up adopting.&lt;/p&gt;
    &lt;p&gt;But the death blow dealt to yogh was the printing press. The earliest printing press in England was a Flemish import, as were the typefaces. But the yogh letter was unique to English, and like the other letters unique to English, it would be expensive to print. And, as we just saw, there were ready alternatives, so yogh disappeared without a trace… from English.&lt;/p&gt;
    &lt;p&gt;In Scotland, on the other hand, it stuck around for longer. Scots used the combination ‘lȝ’ to represent an ‘ly’ sequence like we have in million, and ‘nȝ’ to represent either the ‘ny’ sequence like in canyon, or an ‘ng’ sound like in singer.&lt;/p&gt;
    &lt;p&gt;And Scottish printers were more eager to keep it than English printers were. So they took advantage of the visual similarity between ‘ȝ’ and ‘z’ — most forms of cursive writing in English still write ‘z’ like ‘ȝ’ — to write their yoghs with ‘z’s.&lt;/p&gt;
    &lt;p&gt;You still see the results of this substitution, ‘lz’ and ‘nz,’ in certain Scottish names. But the ‘z’ has led them to be pronounced in ways that have nothing to do with their traditional forms. So Menzies and Mackenzie were meant to spell things that sounded more like Mingus and Mackenyie.&lt;/p&gt;
    &lt;p&gt;And that’s how one single letter of the Middle English alphabet ended up being pronounced like ‘y,’ ‘gh,’ or even eventually like ‘z.’ I warned you it would be complicated.&lt;/p&gt;
    &lt;p&gt;But the journey through the history of yogh has allowed us to peer down some interesting side alleys of the history of writing, from Carolingian scribal practices to the compromises of Scottish printers.&lt;/p&gt;
    &lt;p&gt;I don’t lament the loss of yogh myself, not nearly as much as I lament the fate of other lost letters. But if the cause of yogh is one ȝou fancy taking up ȝourself, there’s nothing standing in ȝour waȝ (it’s included in many modern fonts), althouȝ I can imagine hiȝer causes to aspire to.&lt;/p&gt;
    &lt;p&gt;The name of the letter yogh is pronounced today in many ways: you can say it to rhyme with log, loch, or brogue.&lt;/p&gt;
    &lt;p&gt;Technically, it was the double-storey, closed-loop ‘g’ that was associated with Carolingian minuscule. The open-loop, single-storey ‘g’ was a later development. Today, they’re seen as more or less interchangeable: in fact, I don’t even know what version you’re seeing when you read this, since it’ll look different on the web and in your email client.&lt;/p&gt;
    &lt;p&gt;If you know phonetic terminology, I’ll be more specific: this sound was the voiceless velar fricative, or [x] in the International Phonetic Alphabet. In some situations, it was likely the voiceless palatal fricative (IPA [ç]).&lt;/p&gt;
    &lt;p&gt;If you want to know why the names of these letters sound completely different in Modern English (‘i’ and ‘e’ sound more like the vowels in buy and bee), let me tell you all about it.&lt;/p&gt;
    &lt;p&gt;Conscientious modern editors (myself included, he said humbly) will spell the ‘g’ that you’re supposed to pronounce like ‘y’ with a little dot on top: ‘ġ.’&lt;/p&gt;
    &lt;p&gt;Ditto for the two pronunciations of ‘c.’&lt;/p&gt;
    &lt;p&gt;Note for nerds: this was the voiced velar fricative [ɣ]. Yogh would later be used for the voiceless velar fricative [x], which actually had a different origin in the ancestor of Old English. But the two sounds ended up sounding identical at the end of a word, so yogh ended up being used for both.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45455882</guid><pubDate>Thu, 02 Oct 2025 21:34:42 +0000</pubDate></item><item><title>10k Pushups and How I Got into the Best Shape of My Adult Life</title><link>https://wjgilmore.com/articles/10000-pushups</link><description>&lt;doc fingerprint="fed0d0575880177b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;10,000 Pushups And Other Silly Exercise Quests That Changed My Life&lt;/head&gt;
    &lt;p&gt;Headed into 2025 I was fat, out of shape, and lazy. My three young children were running circles around me, and I was increasingly concerned not only about my health in general but about the kind of example I was setting for them. My (very) sedentary job in front of a laptop wasn't helping, nor was the fact that my favorite hobby in the world outside of work is, well, sitting in front of the laptop.&lt;/p&gt;
    &lt;p&gt;Adding to the general anxiety was the fact I had spent the last two years watching my parents struggle with devastating health issues. My parents had me in their early 20's, so all said they really weren't that much older than I am. My thoughts regularly turned into worry that I'd eventually wind up with my own serious health problems if I didn't get my act together.&lt;/p&gt;
    &lt;p&gt;I wanted to do something about it, but what? Past attempts to go to a gym weren't successful, and I really did not want to drive any more than I already do serving alongside my wife as a kid taxi. Also, having made half-hearted attempts in the past to get into shape (Orange Theory, P90X, etc) and winding up spending less time exercising than researching the minutiae of max VO2, bicycle construction, and fasting benefits, I knew I had to keep things simple.&lt;/p&gt;
    &lt;p&gt;While on a post-Christmas family vacation down in Florida I concluded it made sense to set a goal that could help me get into better shape but which also could be completed in small chunks over a long period of time. It was also important that I could do the workout at any point in the day and even in my office if necessary. And thus began the quest to complete 10,000 pushups in one year. Almost 10 months later, this harebrained goal and the many positive effects that came from it changed my life in ways I never imagined.&lt;/p&gt;
    &lt;head rend="h2"&gt;January, 2025 - The Pushup Tracker&lt;/head&gt;
    &lt;p&gt;While still in Florida I fired up a Google Sheet and added two columns to it: Date and Pushups. And on January 1, 2025 I dropped down and knocked out 30. Well not 30 in a row, mind you. I never could have done that on day 1. It was more like 10, 10, 5, 5 or something like that. Then I wrote it down. On January 2 I upped my game a bit, doing 35 and again immediately logged into the sheet and wrote it down again.&lt;/p&gt;
    &lt;p&gt;In the days that followed, the reward very much became the opportunity to open that sheet. Can't write the pushup number down if I didn't do the pushups, right? I didn't want to break the chain (although you'll later see I did in fact break the chain plenty of times in the months ahead) and so in the first 31 days I did pushups on 24 of 31 days, logging 1,018 in total and averaging 32.84 per day.&lt;/p&gt;
    &lt;p&gt;I even worked up the motivation to run on a treadmill one day in January, logging 2.17 miles in 30 minutes on January 14, 2025. Other than that run and pushups, according to my spreadsheet I did no other notable exercise that month.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It was also in January that I stopped eating fast food of any type, and as of the day of this writing I've not reversed course on this decision. Long story short we were driving back from https://codemash.org/ and pulled through a McDonalds. I had at that point been eating McDonalds all of my life; nothing over the top mind you, but probably twice a month at least for as long as I can remember. Anyway, the food that day was rancid. Legitimately nauseating. I have no idea why it was that way but I was so turned off that right there and then I swore I'd never touch it again. Coincidentally, over this past weekend I was on a walk and reflecting on some of what I'd been writing in this blog post, and my thoughts turned towards diet. When was the last time you heard somebody (including yourself) say they feel better after eating fast food? We all know the answer to this question: never. This stuff is not food and I feel so much better staying away from this poison.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;February, 2025 - Winter Blues Set In&lt;/head&gt;
    &lt;p&gt;Whether it was due to the winter blues or that shiny New Year's resolution already starting to fade, I only logged 848 pushups on 21 of 28 days in February. But I definitely seemed to be getting stronger, averaging 44.63 pushups on those days, and managed to log a daily high of 117 pushups on February 9, 2025. By the end of February I had logged 1,876 pushups. According to my spreadsheet I also managed to lift weights on February 1, 4, and 10. I have a pretty basic weight set in the basement and although I can't recall the specifics, I was probably standing around listening to CNBC on my phone most of the time.&lt;/p&gt;
    &lt;head rend="h2"&gt;March, 2025 - Will the Sun Ever Shine Again?&lt;/head&gt;
    &lt;p&gt;I'm not going to sugarcoat it; March was bad, real bad. I only logged 206 pushups on 9 days, averaging 22.9 pushups on those days. It's unclear to me why I'd tailed off so much other than to imagine old man winter was really starting to weigh on me by that point. Even so, those 206 pushups took me to a total of 2,082 pushups for the year.&lt;/p&gt;
    &lt;head rend="h2"&gt;April, 2025 - The Sun is Shining Again&lt;/head&gt;
    &lt;p&gt;In April my pace picked back up along with the improving weather and increasing sunlight. I completed 375 pushups on 13 days, averaging 28.84 pushups on those days. However I also managed to lift weights on six days in April, went on a run on April 14, and even gave fasting a go for a 28 hour period between April 2-April 3 (not sure I'll do that again).&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Another lifestyle change unexpectedly happened in April: I basically quit drinking alcohol, wine in particular. This decision was a pretty simple one because as I've gotten older, the hangovers have gotten worse, and my sleep quality has gotten much worse, anytime I drank more than 1-2 drinks. As of this writing (September 28, 2025) I've had maybe 2-3 glasses of wine in almost 5 months. My new alcoholic drink of choice when I feel like having something? Miller Lite. It has low calories, low alcohol content, and you can buy a 12 pack for as much as one bottle of wine.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Adding 375 pushups to the pile took me to a total of 2,457 pushups for 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;May, 2025 - Can I Do a Split?&lt;/head&gt;
    &lt;p&gt;Likely due to fear I was going to enter yet another summer rocking the "dad bod", my exercise intensity soared in May. I completed 1,281 pushups over 25 days, averaging 51.24 pushups on those days. On five of those days I completed more than 100 pushups, and on May 18 completed a YTD single day high of 150.&lt;/p&gt;
    &lt;p&gt;I also became mildly obsessed with the idea of doing a split. While browsing Libby as I love to do at night, I found the book Even the Stiffest People Can Do the Splits. The cover showed the author smiling and doing a full split, and I thought well if Eiko says even stiff people can do it then maybe I can too.&lt;/p&gt;
    &lt;p&gt;Over the course of May I did the splits workout 15 times, and undoubtedly became far more flexible although I never did quite reach a complete split. This continued into June and early July however for reasons I'll explain in a moment I stopped doing the regiment out of fear I'd get hurt. However, to this day I stretch daily and of all the different exercise routines I've tried this year I think aggressive stretching has perhaps had the most ROI of them all.&lt;/p&gt;
    &lt;p&gt;On May 15 I ran a 5K with my daughter (well she sped ahead of me after mile 1), completing it in 32:50. Not too bad considering according to my log I ran exactly four times in 2025.&lt;/p&gt;
    &lt;p&gt;Headed into June I had completed a grand total of 3,738 pushups.&lt;/p&gt;
    &lt;head rend="h2"&gt;June, 2025 and Doing Pushups at Versailles&lt;/head&gt;
    &lt;p&gt;June is where things really started to get exciting. Every year Xenon Partners runs a friendly intercontinental pushup contest. "Friendly" is a relative term considering I work with numerous combat veterans, retired members of the United States and Australian military services, and a former Mr. Australia contestant. I also spent some time in France with the family, attending the 24 Hours of Le Mans race (amazing btw) and sightseeing around the country, meaning I had to fit pushups in whenever possible, including at Versailles:&lt;/p&gt;
    &lt;p&gt;In June my output soared to 2,014 pushups, and despite all of the traveling managed to do pushups on 24 of 30 days, averaging 91.55 pushups per day. I also set multiple PRs in June, doing 205 pushups on June 1, 222 on June 15, and then 300 on June 27. As of June 30 I had completed a total of 5,752 pushups.&lt;/p&gt;
    &lt;head rend="h2"&gt;July, 2025 and the 5/15/500 Race&lt;/head&gt;
    &lt;p&gt;Upon returning from Europe I got the bright idea to organize a race called the 5/15/500 Challenge. This involved running 5 miles, biking 15 miles, and then completing 500 body weight exercises. Nevermind that I'd run maybe four times in 2025 and hadn't been on my bike once. Many of my neighbors joined the fun, and we even had t-shirts printed for the occasion. Of course, I also created a website. I did this because I figured having an artificially imposed deadline was going to force me to exercise more often. Mission accomplished.&lt;/p&gt;
    &lt;p&gt;In July I completed 2,002 pushups, ran 48.88 miles, and biked 28.99 miles (this includes the race day numbers). The heat throughout the month was often unbearable, but I pushed through all the same knowing July 26 (race day) was coming up quick. During this period I also really began to dial in my diet, eating little more than fruit, eggs (lots of eggs), chicken, rice, and salad (lots of salad). It was during this period and August that my body began to change. I became noticeably larger and more muscular, and incredibly my abs began to show.&lt;/p&gt;
    &lt;p&gt;In this photo I'm completing race pushup #500. Don't judge the form, it was almost 90 degrees and the exhaustion was real from having already completed the run and bike segments. That said if you squint in the right light you can see I actually have muscles due to all the pushups and running!&lt;/p&gt;
    &lt;p&gt;Due to all of the July training and the 5/15/500 Challenge, my YTD pushup output soared to 7,754.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It was around this time that I went down a major rabbit hole regarding microplastics. A successful techie named Nat Friedman funded a study that looked into the prevalency of microplastics in food, vitamins, and other products, and published the results here. I'm not going to call out any products by name here (although I should because they are poisoning us), but take a moment to open this site in a new tab and search for protein for a glimpse into how you are being poisoned every time you take a bite of so-called health food. After spending a few weeks researching this topic I radically changed my diet and eliminated all of this nonsense. If you really want to go down a rabbit hole, look into the relationship between chocolate-infused health products and heavy metals.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;August, 2025 - Bought a Running Watch&lt;/head&gt;
    &lt;p&gt;In August I did exactly 1,000 pushups, and threw in 190 body weight squats just for fun. 525 of these pushups were completed in a single day (August 16) thanks to my neighbor, friend, and fellow 5/15/500 contestant Charlie having the bright idea that we should knock out what was originally supposed to be 400 pushups during our sons' soccer game. Of course, our competitive spirit got the best of us and I quit at 525 while Charlie pushed on to 600. I'll get him the next time!&lt;/p&gt;
    &lt;p&gt;The running sessions continued throughout August, with 37.48 miles completed. I started taking running much more seriously at this point because I signed up for the October 19 Columbus 1/2 Marathon. I've run 1/2 marathons before (poorly - my last finish time was 3:05) so I know what I'm getting into here, but this time around I want to actually finish at what I deem to be a respectable time which is around 2:20 (10:40/mile pace). Of course, in order to train for this I needed to know what pace I'm running in the first place, and so I bought a Garmin Forerunner 55 watch with GPS.&lt;/p&gt;
    &lt;p&gt;As mentioned before my proclivity for going down research rabbit holes hasn't really helped my previous attempts to get into shape so I chose this watch because compared to other watches it is relatively spartan in terms of features. Above all else I wanted a watch that can accurately track my running distance, pace, and route and so far I am so, so happy with this purchase. It is perfect, and the battery life is amazing.&lt;/p&gt;
    &lt;p&gt;On August 2 I received the watch and later that day took my son and his friend up to a local (Alum Creek) mountain bike park and while they were riding I decided to run the trails. I wound up running 4.69 miles on very hilly and bumpy trails, and paid for it dearly over the next week due to terrible foot and knee pain.&lt;/p&gt;
    &lt;p&gt;On August 21 I ran my first training 10K, completing it in 1:12:28. According to my fancy watch I completed the first 5K in 39:11 but then sped up and completed the second 5K in 33:11.&lt;/p&gt;
    &lt;p&gt;On August 25 I repeated the route, this time completing the 10K in 1:05:41. On August 28 I did it a third time, completing it in 1:02:47. Progress!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I brought some help to the the August 25 and 28 10K training runs: GU packs. In July I read the book Swim, Bike, Bonk: Confessions of a Reluctant Triathlete, by Will McGough. In this hilarious recounting of training and competing in an Ironman triathlon, the author mentions using these mysterious "gel" pack, of which the most popular is known as a "GU pack". I subsequently picked up a few at the local Walmart and can confirm they unquestionably gave me a boost on these long runs. Now anytime I plan on running a 10K or longer I put one in my running pouch and open it 5K into the route.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;With another 1,000 pushups in the book my YTD output sat at 8,754 on August 31.&lt;/p&gt;
    &lt;p&gt;Much better endurance aside, the most obvious visible outcome of the last few months is my clothes no longer fit. My polo shirts are so baggy they look like tents, and my t-shirts are too small because I'm so much more... muscular? What in the hell is going on? This seems to be working!&lt;/p&gt;
    &lt;head rend="h2"&gt;September, 2025 - I am a Beast&lt;/head&gt;
    &lt;p&gt;With 8,754 pushups complete, I only had 1,246 to go and concluded I'd meet the milestone in September. With the 1/2 marathon around the corner my running workouts picked up and I set multiple PRs, including a 29:51 5K PR on September 8, followed by another 28:10 5K PR on September 11.&lt;/p&gt;
    &lt;p&gt;On September 17 I got one of the biggest motivational boosts possible. I was in Chicago for a quarterly meeting, and one of the fellow board members who I've seen in person once every 3 months (but not 3 months ago because we were on the France trip) walked up to me and introduced himself. I stared back at him completely puzzled, and watched him walk away to greet the person next to me. He suddenly wheeled around with a look of shock on his face and said something to the effect of "Holy shit! I didn't even recognize you! You look amazing!".&lt;/p&gt;
    &lt;p&gt;On September 21 I completed the 10,000th pushup in unceremonious fashion on my living room floor:&lt;/p&gt;
    &lt;p&gt;On September 24 I gobbled up a GU pack and headed outside feeling like I could tear a phone book in half. My goal was to shatter the previous 28:10 5K record, and I was on track to do exactly that, running the first 2.1 kilometers in 18 minutes flat. Then out of nowhere I felt this terrible pain in my left calf and came to an immediate stop. It wasn't until September 29 that I could comfortably run again, and even then I only ran 1 mile because I'm terrified of a nagging injury setting me back for the October 19 1/2 marathon.&lt;/p&gt;
    &lt;p&gt;In September I added 1,501 pushups to the pile, bringing the YTD total to 10,245.&lt;/p&gt;
    &lt;head rend="h2"&gt;October and Beyond&lt;/head&gt;
    &lt;p&gt;Today is October 1, 2025 and the pushups continue. The aforementioned 1/2 marathon is on October 19, and my neighbor Charlie and I have already agreed to walk/run a full marathon (around our neighborhood) on November 29. Although it's almost 80 degrees today, in past years we've seen snow by the end of the month so I'm thinking about getting one of those fancy stationary bikes or maybe even a treadmill so I can keep this party going over the winter.&lt;/p&gt;
    &lt;head rend="h2"&gt;My Diet&lt;/head&gt;
    &lt;p&gt;In recent months I have started to look so different that friends have asked me for some diet details. As mentioned, I no longer eat fast food, nor overconsume alcohol. But I've also almost completely cut out processed foods, eating them only very sparingly. A few months ago I did manage to go down the microplastics and heavy metals rabbit hole, and now spend some time researching anything that I plan on eating on a regular basis. Believe me, a lot of the food you think is healthy is pure garbage.&lt;/p&gt;
    &lt;p&gt;Every morning I eat one of two things: either a gigantic fruit smoothie or four scrambled eggs and a salad. I do not deviate from this, only very occasionally eating some protein-powder pancakes made by my wife. My smoothie consists of milk, greek yogurt, 1.5 scoops of Optimum Nutrition protein powder, a huge scoop (probably two cups) of frozen organic berries, and an entire banana:&lt;/p&gt;
    &lt;p&gt;Here is the typical scrambled eggs and salad breakfast:&lt;/p&gt;
    &lt;p&gt;For lunch I eat some combination of chicken, rice, tuna, and salad. I almost never deviate from this. For dinner I eat whatever my wife decides to make, which is always healthy. Obviously we occasionally go out and I'll eat some garbage like wings or pizza, but this is pretty rare compared to the past. I also take a few vitamins and creatine daily.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Earlier in this post I mentioned researching the prevalency of microplastics, heavy metals, and other poison in food. This is particularly problematic in ironically protein powder, protein bars, protein shakes, etc. I settled on Optimum Nutrition because it is one of the few powders on the market that has been tested by numerous third-parties, including the Clean Label Project. It's pretty expensive compared to other products, but I'm happy to pay in order to avoid ingesting this garbage.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Some Analytics&lt;/head&gt;
    &lt;p&gt;Despite getting myself into incredibly good shape relative to the past, this wasn't really that hard. On 105 of 274 days (38.3%) I did no pushups at all. On 142 of 274 (51.8%) days I did between 1 and 100 pushups. On just 26 of 274 (9.4%) days did I do more than 100 pushups, and on only 8 of 274 (2.9%) days did I do 200 or greater. Interestingly, although I have no hard data to back this up I feel like my strength soared in the 67 days following the 5/15/500 race (July 26). Following that date I did more than 100 pushups on 11 days (16.4% of the days), and became noticeably more muscular. Here's a chart showing the pushup volume throughout the year:&lt;/p&gt;
    &lt;head rend="h2"&gt;In Conclusion&lt;/head&gt;
    &lt;p&gt;Headed into October, I feel like a million dollars and plan on continuing these off-the-wall exercise quests for the rest of my (hopefully long) life. I obviously have no idea what I'm doing, but am happy to answer any questions and help motivate you to get in the best shape of your life. Send me an email at wj@wjgilmore.com or DM me on Twitter/X at @wjgilmore!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45456188</guid><pubDate>Thu, 02 Oct 2025 22:06:57 +0000</pubDate></item><item><title>Self-supervised learning, JEPA, world models, and the future of AI [video]</title><link>https://www.youtube.com/watch?v=yUmDRxV0krg</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45457048</guid><pubDate>Thu, 02 Oct 2025 23:57:23 +0000</pubDate></item><item><title>Apple takes down ICE tracking apps after pressure from DOJ</title><link>https://www.foxbusiness.com/politics/apple-takes-down-ice-tracking-app-after-pressure-from-ag-bondi</link><description>&lt;doc fingerprint="d580da991927d11d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Apple takes down ICE tracking apps after pressure from Bondi DOJ&lt;/head&gt;
    &lt;head rend="h2"&gt;Bondi confirmed the department reached out to Apple Thursday to demand it remove ICEBlock&lt;/head&gt;
    &lt;p&gt;FIRST ON FOX: Apple dropped ICEBlock, a widely used tracking tool, from its App Store Thursday after the Department of Justice raised concerns with the big tech giant that the app put law enforcement officers at risk.&lt;/p&gt;
    &lt;p&gt;DOJ officials, at the direction of Attorney General Pam Bondi, asked Apple to take down ICEBlock, a move that comes as Trump administration officials have claimed the tool, which allows users to anonymously report ICE agents' presence, puts agents in danger and helps shield illegal immigrants.&lt;/p&gt;
    &lt;p&gt;"We reached out to Apple today demanding they remove the ICEBlock app from their App Store — and Apple did so," Bondi said in a statement to Fox News Digital.&lt;/p&gt;
    &lt;p&gt;"ICEBlock is designed to put ICE agents at risk just for doing their jobs, and violence against law enforcement is an intolerable red line that cannot be crossed," Bondi added. "This Department of Justice will continue making every effort to protect our brave federal law enforcement officers, who risk their lives every day to keep Americans safe."&lt;/p&gt;
    &lt;p&gt;BONDI DECLARES ‘NEW ERA’ OF POLITICAL VIOLENCE AS FEDERAL AGENTS DEPLOY TO ICE FACILITIES NATIONWIDE&lt;/p&gt;
    &lt;p&gt;Controversy surrounding ICE tracking apps intensified after last month’s deadly shooting at an ICE field office in Dallas, Texas, the latest in a series of attacks that appeared to be targeting immigration enforcement officers.&lt;/p&gt;
    &lt;p&gt;Authorities said the suspect, Joshua Jahn, searched his phone for tracking apps, including ICEBlock, before opening fire on the facility from a rooftop. Authorities said Jahn killed one detainee and left two critically injured but that the personnel were his intended targets, not the immigrants. One of the injured, a 32-year-old husband and father of four, died this week.&lt;/p&gt;
    &lt;p&gt;Marcos Charles, an acting director for ICE’s removal operations, said during a press conference that Jahn had intended to murder ICE employees and that attacks on them have skyrocketed.&lt;/p&gt;
    &lt;p&gt;"The evidence is clear that this was intended as an assault on ICE personnel who come to work everyday to do their job," Charles said. "Violent rhetoric has led to an over 1000% increase in assaults on ICE officers, and it has to stop."&lt;/p&gt;
    &lt;p&gt;Fox News Digital reached out to Apple and ICEBlock for comment.&lt;/p&gt;
    &lt;p&gt;Apple said in a statement it removed ICEBlock and other apps like it.&lt;/p&gt;
    &lt;p&gt;"We created the App Store to be a safe and trusted place to discover apps. Based on information we’ve received from law enforcement about the safety risks associated with ICEBlock, we have removed it and similar apps from the App Store," Apple said.&lt;/p&gt;
    &lt;p&gt;Joshua Aaron, ICEBlock's creator, said he was "incredibly disappointed by Apple's actions today."&lt;/p&gt;
    &lt;p&gt;"Capitulating to an authoritarian regime is never the right move," Aaron said. "Apple has claimed they received information from law enforcement that ICEBlock served to harm law enforcement officers. This is patently false."&lt;/p&gt;
    &lt;p&gt;CLICK HERE TO GET THE FOX NEWS APP&lt;/p&gt;
    &lt;p&gt;Aaron said ICEBlock, which has more than 1.1 million users, functions like other mapping applications that use crowd sourcing for speed traps, citing Apple's own map service as an example.&lt;/p&gt;
    &lt;p&gt;"We are determined to fight this with everything we have," Aaron said. "Our mission has always been to protect our neighbors from the terror this administration continues to reign down on the people of this nation."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45457333</guid><pubDate>Fri, 03 Oct 2025 00:34:29 +0000</pubDate></item><item><title>Researchers develop molecular qubits that communicate at telecom frequencies</title><link>https://chicagoquantum.org/news/researchers-develop-molecular-qubits-communicate-telecom-frequencies</link><description>&lt;doc fingerprint="793d90fa6ec5c048"&gt;
  &lt;main&gt;
    &lt;p&gt;A team of scientists from the University of Chicago, the University of California Berkeley, Argonne National Laboratory, and Lawrence Berkeley National Laboratory has developed molecular qubits that bridge the gap between light and magnetism—and operate at the same frequencies as telecommunications technology. The advance, published today in Science, establishes a promising new building block for scalable quantum technologies that can integrate seamlessly with existing fiber-optic networks.&lt;/p&gt;
    &lt;p&gt;Because the new molecular qubits can interact at telecom-band frequencies, the work points toward future quantum networks—sometimes called the “quantum internet.” Such networks could enable ultra-secure communication channels, connect quantum computers across long distances, and distribute quantum sensors with unprecedented precision. Molecular qubits could also serve as highly sensitive quantum sensors; their tiny size and chemical flexibility mean they could be embedded in unusual environments—such as biological systems—to measure magnetic fields, temperature, or pressure at the nanoscale. And because they are compatible with silicon photonics, these molecules could be integrated directly into chips, paving the way for compact quantum devices that could be used for computing, communication, or sensing.&lt;/p&gt;
    &lt;p&gt;The new molecular qubit contains erbium, a rare-earth element. Rare earths are used in classical technologies as well as emerging quantum technologies because they absorb and emit light very “cleanly” relative to other elements, but they also interact strongly with magnetic fields.&lt;/p&gt;
    &lt;p&gt;“These molecules can act as a nanoscale bridge between the world of magnetism and the world of optics,” said Leah Weiss, postdoctoral scholar at the University of Chicago Pritzker School of Molecular Engineering (UChicago PME) and co-first author on the paper. “Information could be encoded in the magnetic state of a molecule and then accessed with light at wavelengths compatible with well-developed technologies underlying optical fiber networks and silicon photonic circuits.”&lt;/p&gt;
    &lt;p&gt;On a quantum level, the relationship between light and magnetism is subtle and complex. Light is often how quantum information is transmitted and read; magnetism is deeply connected to “spin”, a unique quantum property that underlies a variety of quantum technologies such as sensors and certain types of quantum computers. This work builds on a foundation of two fields, quantum optics—with applications in lasers and quantum networks—and synthetic chemistry—which is responsible for the contrast agents used in magnetic resonance imaging (MRI) machines—to establish a molecular building block that can bridge the divide between them.&lt;/p&gt;
    &lt;p&gt;“Rare-earth chemistry provided a fortuitous combination of properties that allowed us to bring these capabilities to a molecular system,” said Grant Smith, graduate student at PME and another first author on the paper. “There were a lot of things pointing toward this as an exciting platform to advance the use of optical degrees of freedom in molecular spin qubits. One of the central focuses of this work, and the work in the lab more broadly, is that we want to really expand the gamut of quantum systems and materials that we can control and interact with.” By doing this, he says, “you can begin to think about new and unconventional ways to utilize and integrate them into technologies.”&lt;/p&gt;
    &lt;p&gt;Using optical spectroscopy and microwave techniques, the team demonstrated that the erbium molecular qubits used frequencies compatible with silicon photonics, which are used in telecommunications, high-performance computing, and advanced sensors. The researchers say this compatibility with mature technologies could accelerate the development of hybrid molecular–photonic platforms for quantum networks.&lt;/p&gt;
    &lt;p&gt;“By demonstrating the versatility of these erbium molecular qubits, we’re taking another step toward scalable quantum networks that can plug directly into today’s optical infrastructure,” said David Awschalom, the Liew Family Professor of Molecular Engineering and Physics at the University of Chicago and principal investigator of the study. “We’ve also demonstrated that these atomically engineered qubits have the capabilities necessary for multi-qubit architectures, which opens the door to a wide spectrum of applications, including quantum sensing and hybrid organic-inorganic quantum systems.”&lt;/p&gt;
    &lt;p&gt;Both Weiss and Smith emphasized the importance of their collaboration with chemists at UC Berkeley, especially their co-first author Ryan Murphy in the research group of Jeffrey Long, calling it “absolutely critical” to the work and “a privilege.”&lt;/p&gt;
    &lt;p&gt;“Synthetic molecular chemistry provides an opportunity for optimizing the electronic and optical properties of rare earth ions in ways that can be difficult to access in conventional solid-state substrates,” said Murphy. “This study is just scratching the surface of what we think we can accomplish.”&lt;/p&gt;
    &lt;p&gt;“Our work shows that synthetic chemistry can be used to design and control quantum materials at the molecular level,” said Long, Professor of Chemistry at UC Berkeley and co-principal investigator. “This points to a powerful route for creating tailor-made quantum systems with applications in networking, sensing, and computation.”&lt;/p&gt;
    &lt;p&gt;The study was supported by the U.S. Department of Energy’s Office of Science and Q-NEXT, a DOE National Quantum Information Science Research Center.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45457411</guid><pubDate>Fri, 03 Oct 2025 00:44:42 +0000</pubDate></item><item><title>Microcomputers – The Second Wave: Toward a Mass Market</title><link>https://technicshistory.com/2025/10/03/microcomputers-the-second-wave-towards-a-mass-market/</link><description>&lt;doc fingerprint="a568d34923a725fc"&gt;
  &lt;main&gt;
    &lt;p&gt;In 1977, three new microcomputers appeared on the scene that broke free from the industry’s hobbyist roots: the Apple II, the Commodore PET, and the Tandy/Radio Shack TRS-80. Much later, in the 1990s, journalists and historians began reverently referring to this group as “the Trinity.” Though all three machines had different origins and different trajectories (Apple, for example, appeared in 1978 to be an also-ran before rising to eclipse all of its rivals), the distinctiveness of the 1977 generation of computers is not merely a retrospective imputation by later writers. The hobby journalists of the time recognized that with the Trinity, something like an “appliance” computer had arrived on the scene, “a clean break from commercial and hobbyist computer systems requiring technical skill and dedication from their operators into a consumer market where no qualifications are required of the customer.”[1]&lt;/p&gt;
    &lt;p&gt;Three factors were required to join this holy ensemble: the technical expertise to design a capable and reliable microcomputer, a nose for the larger business opportunity latent in the hobby computer market, and the capital resources to produce, market, and sell thousands (or even tens of thousands) of computers per month. Most of all it required a certain measure of daring, a willingness to a take a leap in the dark.&lt;/p&gt;
    &lt;p&gt;After all, the transformation of the microcomputer hobby into a large-scale commercial enterprise came as a surprise to most outsiders. In 1977, the established mainframe and minicomputer makers remained cooly aloof from the microcomputer business. Clearly, computer enthusiasts had found in the Altair and its successors a fascinating gadget to occupy their spare hours. It did not necessarily follow that these toys had anything to do with the “real” computer business, any more than model rocketry had to do with putting a man on the moon. In a la of the leading minicomputer makers, Hewlett-Packard and Digital, were offered ready-made micro designs by computing-loving engineers within their ranks (Steve Wozniak and Dave Ahl, respectively), but both rejected the idea, unwilling to pursue a fringe market that seemed to have nothing to do with their business.[2]&lt;/p&gt;
    &lt;p&gt;Even many of the hobbyists themselves didn’t believe that the market for a home computer would extend much beyond the existing circle of electronic hobbyists and computer enthusiasts. But a few people with access to deep pockets (not necessarily their own) smelled an opportunity in the microcomputer, and decided to pursue it, and those people formed the breaking edge of the second wave of personal computers.&lt;/p&gt;
    &lt;p&gt;The obvious place to start to tell the story of the second wave is Apple Computer: not because it is, retrospectively, the most well-known of the three, but because it had the deepest roots in the first-wave hobby community. Commodore and Tandy were well-established companies, dragged into the computer business almost against their will by internal agitators who believed fervently in the idea of the personal computer. Apple was founded by and for hobbyists. If not for the fickle whims of fate and the chutzpah of Steve Jobs, it would have met a quiet demise in total obscurity, like so many other hobby computer companies of the day.&lt;/p&gt;
    &lt;head rend="h3"&gt;Apple Computer&lt;/head&gt;
    &lt;p&gt;The story of Apple Computer (later, simply Apple) continues to fascinate because of the company’s massive economic success and cultural impact (first in the early 1980s, then again in the twenty-first century), because of its meteoric rise from humble beginnings, and because of the vivid and contrasting personalities of its two primary co-founders: Steve Jobs and Steve Wozniak. No other topic I will cover in this series has a comparably extensive literature: all of the known details of the early years of the two Steves and their company could fill more than one book (and have). Here we can note only the most important highlights.&lt;/p&gt;
    &lt;p&gt;What became the first Apple computer began as an anonymous circuit board, the product of an intense burst of creative energy by Steven “Woz” Wozniak. Wozniak’s engineer father moved his family to Sunnyvale, California, at the southern end of San Francisco Bay, in the late 1950s, to take a job at Lockheed. The younger Wozniak developed an early fascination with electronics, and he came of age in the perfect environment to feed and reinforce that fascination: a suburban neighborhood teeming with engineer dads on every block, who had bins full of parts and minds full of expertise to lend to the eager young gadget enthusiasts who roamed the sidewalks.[3]&lt;/p&gt;
    &lt;p&gt;By the time Woz graduated from high school in 1968, he had grown into a true electronics genius, with a level of insight and skill far beyond the typical hobbyist. He could envision a design that would produce the desired effect in the most efficient way possible, with the elegant finality of a mathematical proof. Socially isolated, he lived an inward life of imagination, spending every spare moment at home and in school sketching designs for electronic systems. His social awkwardness dwelled side-by-side with a love of pranks and juvenile humor: in his early twenties he ran a Dial-a-Joke service out of his home that played pre-recorded Polack jokes from an answering machine.[4]&lt;/p&gt;
    &lt;p&gt;His high school electronics teacher gave Wozniak the opportunity to make weekly visits to a nearby corporate computer center, and he learned about minicomputers from the trade literature at the Stanford Linear Accelerator Center (SLAC). Like many other young men of his generation, these brushes with computing got him pining for his own computer; unlike most of them, he decided to do something about it. In 1971, while a student at University of California, Berkeley, he built a home computer with the help of a younger friend, a high school student named Bill Fernandez. This “Cream Soda Computer,” named for the beverage that fueled its creation, was roughly similar in character and capabilities to the Kenbak-1 sold by John Blankenbaker that same year: a very basic processor, a tiny memory, and a handful of lights for output. Meanwhile, Woz continued to pore over the brochures and manuals for dream machines like the Data General Nova minicomputer, and to work out schematics on paper for minicomputers of his own design.[5]&lt;/p&gt;
    &lt;p&gt;For years, Wozniak’s dream of a computer to call his own slumbered. Then, in the spring of 1975, a sudden shock jolted it awake. His friend Allen Baum took Wozniak to the first Homebrew Computer Club meeting under false pretenses—he told Woz that the people there were working on computer video terminals like Don Lancaster’s TV Typewriter. Wozniak knew all about that: he had already designed his own terminal to use with a local time-sharing service, Call Computer. He was unprepared, then, when he arrived at Don French’s Menlo Park garage and found everyone chattering about microprocessors. Accustomed to mastery of all things electronic, Wozniak initially recoiled at being thrown into an environment that exposed him as ignorant.[6]&lt;/p&gt;
    &lt;p&gt;After the meeting, though, he began looking into how these newfangled microprocessors worked, and found that they replicated the structure of his favorite minicomputers in miniature (indeed, the first Intel microprocessor was consciously modeled on the DEC PDP-8). Moreover, he saw that with a microprocessor, he could put a whole computer right in his TV terminal, and eliminate the need to actually “call” some outside computer.[7]&lt;/p&gt;
    &lt;p&gt;Wozniak designed and built his second computer over the following months. For the processor, he opted at first for the Motorola 6800, but then found a much cheaper device with a similar design: the MOS Technology 6502, a new product that he acquired for $25 at the Western Electric Show and Convention (WESCON) at San Francisco’s Cow Palace, in June 1975. He assembled a circuit board around the processor with controller chips for a keyboard, TV screen, 4K bytes of static RAM, and a tiny program in ROM to bring up the keyboard interface at power on. Later in the year he upgraded to cheaper (but more finicky) dynamic RAM, with the help of his friend Steve Jobs, a classmate of Bill Fernandez.[8]&lt;/p&gt;
    &lt;p&gt;Had it been only up to Wozniak, the story of his computer would end there, as a minor anecdote in the history of hobby microcomputers. Proud of his accomplishment but shy of self-promotion, he would set up his computer at Homebrew meetings and wait patiently to lure in fellow hobbyists, to whom he would offer free copies of the plans for the machine. He found few takers. Wozniak’s design was clean and self-contained, but others were cobbling together the same capabilities by combining an Altair, a Processor Technology memory board, and a TV Typewriter. Moreover, using the 6502 processor put him out of step with the mainline hobby culture of Intel 8080 machines based on Altair.&lt;/p&gt;
    &lt;p&gt;Wozniak didn’t mind that at all. He had not designed his computer to fill a market need, but because he wanted it. Comfortably settled into his dream job—designing calculators for Hewlett-Packard (HP)—and into his first real romantic relationship (with Alice Robertson, whom he met through his Dial-a-Joke line), he was content with the direction of his life and felt no pull to parlay his talent for computer design into fame, recognition, or money. His young friend Steve Jobs, on the other hand, bristled with restless energy. Though he found electronics interesting, he lacked Wozniak’s all-consuming passion for the field, and did not know where to turn instead for fulfillment. Dissatisfied with the well-worn patterns around which others organized their lives, he spent his late teens and early twenties in search of some kind of vital spiritual awakening, whether through bizarre diets inspired by German-born writer Arnold Ehret, primal scream therapy, the ancient wisdom of India, or the All One Farm, a hippie commune in Oregon.[9]&lt;/p&gt;
    &lt;p&gt;Jobs saw in Woz’s computer the potential for a business: instead of giving away the plans, they could sell the pre-assembled boards (with no case, keyboard, monitor, or other accessories). In early 1976, he cajoled his friend into a partnership; Woz agreed, but still considered this a sideline to his real work at HP. Inspired by the orchard at All One Farm where he had been spending much of his time, Jobs proposed the name Apple Computer. The partnership had a clear division of labor: Wozniak would design the hardware and software, Jobs would make the business decisions.&lt;/p&gt;
    &lt;p&gt;Financed by the sale of their most valuable property (Jobs’ Volkswagen bus and Wozniak’s HP calculator), and thirty-day credit with a parts supplier, they made a fifty-computer deal with Paul Terrell, owner of the Byte Shop computer store. But this promising beginning also turned out to also be the end of their success; very few other Apples were sold. By mid-1976, the barebones board offered by Apple didn’t cut it anymore. A hobbyist could order a Processor Technology Sol-20 that came in a handsome case with integrated power supply, keyboard and cassette interface. Even when outfitted with all of that (which Terrell had to do himself in order to make a saleable computer), the Apple lacked compatibility with the growing array of software and hardware designed for the 8080 processor and S-100 bus.[10]&lt;/p&gt;
    &lt;p&gt;Jobs knew that they would need a better computer and more money to build it. In late 1976, Woz lost himself in a second design. While he had built the first Apple so he could finally have a minicomputer of his own, he built the Apple II so he could create and play arcade games with full-color graphics. Woz knew the field well, because Jobs had an on-again, off-again job at Atari in Sunnyvale and Woz had taken a commission from his friend to design a Breakout game for the company—a single-player Pong variant, with the goal to break a whole array of blocks before losing the ball off the bottom of the screen. Unlike the Atari games of the time, though, Apple II games would be written in mutable BASIC code, not fossilized in hardware. As usual for Woz, the Apple II’s design was spare and efficient. The pièce de resistance was circuitry to time-share the memory bus so that the video card could read from memory while drawing each line of the screen, then hand control to the processor while waiting for the cathode ray beam to reset to the start of the next line.&lt;/p&gt;
    &lt;p&gt;Jobs, meanwhile, went to work on finding an investor. He tried selling Apple Computer to Atari and Commodore (a calculator firm about which we’ll have much more to say very shortly), but the former had no interest in the product and the latter balked at the price. Jobs asked Atari founder Bushnell (his former employer) for an investment, and was rebuffed again. But Bushnell did ask venture capitalist Don Valentine to come take a look at Apple. Valentine was turned off by Jobs’ hippie vibes, but put Jobs in touch with a former employee of his from his Fairchild Semiconductor days, Mike Markkula.[11]&lt;/p&gt;
    &lt;p&gt;Through this chain of happenstance, Apple Computer found exactly the right person to make it a success. Jobs exhibited taste and enterprise, but untempered by experience and marred by streaks of cruelty and pettiness. Wozniak possessed exceptional engineering talent, but entirely lacked business sense, and avoided any work that he didn’t find personal interesting.&lt;/p&gt;
    &lt;p&gt;Markkula was an even-keeled veteran of Fairchild and intel, living in semi-retirement in his thirties after making several million dollars off of his stock. He brought to Apple $250,000 of his own money, and the connections to bring in far more. He also brought knowledge about how to build and develop a company, and a deep belief in the market for something like the Apple. Markkula was already personally invested in the idea of computing at home: at Intel he had used a home teletype terminal to connect to an Intel time-sharing computer. He used it for company business but also to balance his personal checkbook, and had always wondered why Intel didn’t package the 8008 or 8080 into a computer and sell it.[12]&lt;/p&gt;
    &lt;p&gt;By the spring of 1977, the Apple II was ready to sell. Wozniak had finished the circuit design, Markkula had laid out a business plan, and Jobs had selected, a sleek, molded-plastic case that would make Apple II the most elegant-looking computer on the market. At a starting price of $1,298, it was far from the cheapest, however, and its novel processor and bus still put it outside the mainstream of hobby hardware and software. Sales were initially slow.&lt;/p&gt;
    &lt;p&gt;But Markkula kept the investment money flowing, and coaxed one last engineering miracle from Woz. Having ported his checkbook program to the Apple II, Markkula was annoyed at how long it took to load from cassette tape, and gave Wozniak the mandate to produce a magnetic disk drive for the Apple II, using the new 5.25 inch drives available from Shugart Associates. A floppy disk of the time could hold less data than a tape, but the computer could read and write it far faster and could instantly access any part of the rapidly-spinning disk (whereas tape could only be read or written at whatever position it was currently wound to).&lt;/p&gt;
    &lt;p&gt;The $595 Disk II immediately set Apple apart from its competitors, and set the stage for the company’s future growth. With the disk drive, color graphics, eight expansion slots, and the capacity for up to forty-eight kilobytes of memory, the Apple II was by far the most fully-featured and extensible of the Trinity. In 1977 and 1978, however, Apple Computer remained an also-ran. Other companies, with pre-existing manufacturing and distribution networks, raced ahead of with mass-market computers at half the price and many times the reach. They put up sales and production numbers far exceeding anything that the hobby computer market had ever seen. The first of these was Commodore.&lt;/p&gt;
    &lt;head rend="h3"&gt;Commodore&lt;/head&gt;
    &lt;p&gt;Commodore, co-founded by Jack Tramiel and Manfred Kapp, began life as a typewriter importer in Toronto, but pivoted into the calculator business in the late 1960s, selling re-branded Japanese imports from Casio and then later building its own factories, with the backing of Canadian financier Irving Gould.[13]&lt;/p&gt;
    &lt;p&gt;Tramiel, born Idek Trzmiel in Poland, survived Auschwitz and came to the U.S. in his early twenties, where he worked his way up from the bottom. Not much of an idea man or product innovator, he got ahead on hard work, a hard nose, and ruthlessness (this adjective is attributed to him seven times by various persons in Brian Bagnall’s book, Commodore: A Company on the Edge). Behind him lurked the shadowy Canadian financier Gould, who brought large quantities of cash, international contacts, and clever tax evasion schemes.&lt;/p&gt;
    &lt;p&gt;Like most North American calculator makers, Commodore found itself on the brink of disaster in 1974, after semiconductor firms like Texas Instruments began to flood the market with dirt-cheap calculators. Tramiel (now CEO of Commodore) decided to beat them by joining them: he vertically integrated by buying one of his suppliers, MOS Technology, the very firm that had built the 6502 processor used in the Apple I and II. Once again, we find a thread woven into the story of the personal computer out of the unraveling fabric of the North American calculator business.&lt;/p&gt;
    &lt;p&gt;Tramiel acquired MOS in September 1976 in exchange for a modest dollop of cash, a more substantial stake in Commodore, and a generous serving of pressure on his (also financially beleaguered) supplier.[14] In the bargain he had also acquired Charles “Chuck” Peddle, whose vision for the future of computing would pull Commodore in an entirely new direction.&lt;/p&gt;
    &lt;p&gt;A cocky striver from a poor family in Maine, Peddle studied electrical engineering at the state university, then began bouncing around the country in search of bigger and better opportunities: to California to take a job at General Electric (GE), then to GE’s time-sharing systems division offices in Arizona and Ohio, then back to Arizona to take a swing at starting a smart terminal business, miss, and join the semiconductor design team at Motorola, then to Pennsylvania to join MOS Technology as part of a breakaway group of engineers who had worked on Motorola’s 6800 microprocessor. In contrast to the introverted Woz and the otherworldly Jobs, Peddle was brash and frankly carnal—the type of man who would compare the pleasures of computer use to sex and boast that his wife had the figure of Zsa Zsa Gabor.[15]&lt;/p&gt;
    &lt;p&gt;Peddle helped to design the 6502 processor, but it was never targeted at microcomputers. It was designed as a controller for some larger application, such as an industrial machine, a traffic signal, or an automobile. At a price point of $25, it aimed to compete with the Intel 4040, the recently released successor to the 4004. MOS also sold the KIM-1, a single-board computer with a calculator screen and keypad built into it. It was intended as a marketing showpiece for the 6502, not a consumer product. Nonetheless, it sold in surprisingly large numbers to hobbyists who appreciated having a very inexpensive all-in-one computer.&lt;/p&gt;
    &lt;p&gt;In late 1975, Peddle was visiting the Miami suburbs to help Allied Leisure, a maker of electro-mechanical arcade games, design a microprocessor-powered pinball machine using the 6502. Peddle discovered that one of the Allied engineers, Bill Seiler, was a hobbyist who had bought a computer from the Digital Group in Denver, but then struggled to figure out what to do with it.[16]&lt;/p&gt;
    &lt;p&gt;Peddle decided he wanted to launch a full-fledged computer product: not because he had always dreamed of a computer of his own, but because he believed in the market opportunity, having witnessed both the not-quite-satisfied demand of hobbyists like Seiler for a computer that was both easy-to-use and able to do something useful, and the eagerness with which those same hobbyists had taken up the cheap, simple KIM-1. Up to this time, the microcomputer business was almost fully autochthonous, built by native hobbyists; Peddle was the first immigrant. With MOS scaling back under financial pressure, Peddle intended to jump ship for Allied Leisure, which had agreed to launch his planned computer. But then, in the fall of 1976, came the Commodore acquisition.[17]&lt;/p&gt;
    &lt;p&gt;Commodore’s vice president of engineering, Andre Sousan, later a major player at Apple, didn’t want to lose Peddle, and agreed to help him pitch his computer to Tramiel. Tramiel, in turn, agreed to give Peddle a computer division in Palo Alto. Sousan and Peddle succeeded with Tramiel because they framed the project in terms of the familiar calculator market: as historian Brian Bagnall writes, “[they] pitched the product as an evolution of the calculator which would surpass the HP65,” adding new features like a TV monitor and cassette deck. Tramiel especially appreciated the idea of having electronics retailer Radio Shack market the computer under its own brand, in exchange for more Commodore calculator distribution in their stores.[18]&lt;/p&gt;
    &lt;p&gt;Tramiel wanted something to show at the January 1977 Consumer Electronics Show (CES), just a few months away, so Peddle went to Apple, the only existing company with a working 6502-based computer, to try to acquire their design. As we have seen, those negotiations went nowhere, so Peddle and his team slapped the guts of a 6502-based sprinkler-system controller into a case with a rubberized keyboard sourced from Commodore’s existing calculator designs. Unimpressed by a demo of this barely-working machine at CES, Radio Shack decided they could do at least as well themselves, and broke off negotiations, too.[19]&lt;/p&gt;
    &lt;p&gt;Commodore went ahead alone, and announced the imminent arrival of the “Commodore PET 2001” – its name inspired by the pet rock fad, and its numerical designation borrowed from the space odyssey. Peddle built hype for the machine with an early prototype at the West Coast Computer Faire in San Francisco in April 1977, and the mob of over ten thousand attendees convinced Tramiel that this computer idea was something more than just a means of selling more calculators. As company revenues from calculators continued to dwindle, Tramiel kept the company afloat by transforming the excitement for the PET into cash flow with an advanced payment plan that secured eager buyers a place in line—once the computers were actually built.[20]&lt;/p&gt;
    &lt;p&gt;In the fall of 1977, Commodore finally began to ship PETs to customers. The resulting computer had serious shortcomings. Tramiel had refused to pay the up-front cost for a modern-looking plastic-molded exterior, instead funneling some revenue to his struggling file cabinet subsidiary by ordering sheet metal cases. He also insisted on a calculator-style keyboard with small rubberized keys, which proved unpleasantly cramped and flimsy. With black-and-white character graphics and no expansion slots or speaker, the PET was neither as handsome nor (in most ways) as capable as the Apple II. But it held two decided advantages: Commodore’s existing retail distribution network and a lower price. An 8KB model with integrated tape deck and monitor included cost just $795—two-thirds the price of an Apple II without those accessories.[21]&lt;/p&gt;
    &lt;p&gt;PET continued to attract attention from the press that fall, with an article in BYTE touting it as an “appliance computer,” extensive coverage in Personal Computing, and a cover feature in Popular Science. At first production failed to keep up with interest: “By the end of 1977,” Bagnall writes, “Commodore had only managed to assemble a meager 500 machines.” But Commodore straightened out its production lines, and by one estimate they sold about 25,000 PETs in 1978.[22]&lt;/p&gt;
    &lt;p&gt;Nonetheless, Commodore remained in a slightly uncomfortable position. Peddle had tried and failed to a launch a disk drive for the PET, in large part because his lack of understanding of the hobby market led him to overshoot, aiming for a complex dual-disk drive that would appeal to the kind of corporate mini-computer user that he used to be (His failure incurred Tramiel’s wrath, leading Peddle to rashly jump ship to Apple –a move that wouldn’t last.) So, on the one hand, with no color graphics, limited memory, and no disk drive, the PET could not compete for the high-end customers who wanted those capabilities and could find them in an Apple II. On the other, Commodore had failed to dominate the market for low-cost, mass-produced machines either: the PET’s 1978 sales would have made it the fastest-selling computer of all time, if not for the final entry in the 1977 Trinity, the TRS-80.[23]&lt;/p&gt;
    &lt;head rend="h3"&gt;Radio Shack&lt;/head&gt;
    &lt;p&gt;Tandy Radio Shack was the creation of Charles Tandy, a driven World War II veteran from Brownsville, Texas, who expanded his father’s Forth Worth shoe leather business into a nationally-known leather and leathercrafts empire. By the 1960s, Tandy felt that leather had taken him as far as it could; he wanted to expand into new markets with more growth potential. So, in 1963, he acquired an ailing electronics chain headquartered in Boston called Radio Shack. Pouring money from the leather business into his new venture, he expanded Radio Shack from just nine stores to several thousand over the following decade. Customers could walk into a Radio Shack in nearly every city in the United States, from Eugene, Oregon to Fort Lauderdale, Florida: even sparsely populated states like Montana and Wyoming boasted several stores each. This ubiquity provided the springboard for the TRS-80’s success.[24]&lt;/p&gt;
    &lt;p&gt;The impetus for Tandy’s entry into the computer business came from a hobbyist on the inside. Don French started his career as a teenage salesman in a Radio Shack store outside San Diego and rose rapidly through the ranks to become a project manager at the Fort Worth headquarters in 1973, while still in his mid-twenties. His fascination with computers was sparked by a course on the topic he took at Grossmont Junior College, and kindled anew by the arrival of the first hobby computers. He ordered the Mark-8 kit from Jonathan Titus in 1974, and then the MITS Altair when it became available the following year. French, enamored with his new toys, became convinced that Radio Shack should bring out its own kit computer, and tried earnestly to convince his bosses of the same.[25]&lt;/p&gt;
    &lt;p&gt;Leadership gradually came around to the idea that an expensive, innovative computer product offered an opportunity to revise Radio Shack’s image as a purveyor of low-cost, low-quality, imitative merchandise. According to French, the key turning point came in the fall of 1975, when audio equipment maker Advent wowed the market with a new speaker design. An infuriated Charles Tandy railed at the fact that his company couldn’t innovate like that, and latched onto French’s computer proposal as a way to make similar headlines. His passion for the idea could not have run too deep, however, because French’s plans for a Radio Shack computer went nowhere until the spring of 1976.[26]&lt;/p&gt;
    &lt;p&gt;At that time, a group of Tandy Radio Shack buyers including John Roach (Radio Shack’s Vice President) visited National Semiconductor in Santa Clara, California, to check out the latest chip offerings, including the company’s new SC/MP microprocessor. While there, they met a young electrical engineer named Steve Leininger. Then, when they visited the local Byte Shop to scope out the hobby computer scene, there was Leininger again, working a part-time gig behind the counter. Leininger, as it turned out, in addition to his engineering credentials, regularly attended Homebrew Computer Club meetings and was spending his spare hours building a hobby computer and writing his own BASIC. Tandy had found the perfect man to kick-off French’s computer project, combining a passion for hobby computers with the engineering chops to understand the nuances of integrated circuits and microprocessor design. The opportunity to design a computer as a commercial product and access to a better job market for his out-of-work geologist wife were sufficient enticements to bring Leininger to Texas.[27]&lt;/p&gt;
    &lt;p&gt;Leininger grew up near South Bend, Indiana, and earned a bachelor and master’s degree in electric engineering in just four years at Purdue University before accepting a job at National Semiconductor. He did not have the bold personality of Woz, Jobs, or even Peddle, but he certainly had the skills that Tandy needed. French’s notional computer project only began in earnest after Leininger’s move to Fort Worth in July 1976. However, most of the company’s leaders did not really believe that a computer would or could be a successful Radio Shack product, and they left Leininger toiling away in almost total isolation, first in a speaker plant at the Fort Worth stockyards, then in a saddle factory. [28]&lt;/p&gt;
    &lt;p&gt;It took several months before Leininger even knew what he should build. French had initially planned on a kit, as kits fell within his preserve at Tandy, making it relatively easy to sneak by leadership. But in October he and Leininger agreed to develop a pre-assembled computer instead, at the insistence of Radio Shack president Lew Kornfeld. Kornfeld felt burnt by a recent digital clock kit, which precipitated a high rate of returns from customers unable to put it together, and didn’t want to send and even more complicated kit project out the door. It is likely at this moment of uncertainty that French turned to Tramiel and Peddle to explore having Commodore build the computer instead, a notion which fell through at CES in January 1977, likely because the Tandy people saw that Peddle had nothing better to offer than what Leininger had prototyped to that point.[29]&lt;/p&gt;
    &lt;p&gt;Other than general skepticism about the viability of a Radio Shack computer in the first place, the clearest message French and Leininger got from Tandy leadership was to keep the price as low as possible: the original target (under the kit plan) was a $195 retail price. Cost control guided the rest of the design. For example, Leininger’s choice of the Zilog Z80 microprocessor, an 8080-compatible architected by the same Federico Faggin who had led the design of that seminal Intel chip. It offered more built-in circuitry than its Intel predecessor, including circuitry for refreshing dynamic RAM. Because dynamic RAM was cheaper than static, this lowered the cost of building the TRS-80. To save more money on hardware, Leininger wrote software to generate the tones for saving data to tape rather than using a physical tone-generator circuit. Meanwhile, because RCA offered to provide a cheap black-and-white TV already finished in a plastic “Mercedes silver” case, Radio Shack let the tail wag the dog, and adopted the same color for the case of the computer and keyboard, housed as a single unit.[30]&lt;/p&gt;
    &lt;p&gt;In February 1977 French, Leininger, and Roach presented the prototype to Charles Tandy and got the go-ahead to enter production. French thought they could sell 50,000 units; Lew Kornfeld found that laughable and proposed building 1,000; Roach finalized the number at 3,500 units, enough to get better economies of scale from the factory and to use a computer to manage inventory in each store if they found no buyers.[31]&lt;/p&gt;
    &lt;p&gt;Despite this continuing internal skepticism, the TRS-80 attracted tremendous interest when it was announced to the public at the Waldorf Hotel New York City on August 3rd, and even more so when French showed up with a working TRS-80s at the ComputerMania hobby computer convention in Boston, later that month. At just $600 for a monitor, keyboard, cassette recorder, four kilobytes of memory, and a simple built-in BASIC, it was substantially cheaper even than the recently-debuted PET. Just like Commodore, it took Radio Shack several more months to get their factory production lines humming, but once they did, Tandy sold TRS-80s at an astonishing rate: 100,000 in 1978, exceeding the number of minicomputers sold by all manufacturers combined that year (though certainly not coming close to their total dollar value). Rather than the flop that company leaders feared, it proved to be a vital new revenue source, accounting for 10% of Tandy’s revenue in 1978. The TRS-80 arrived just in time to take up the slack from the flagging citizens-band (CB) radio craze that had filled Radio Shack’s sails in the mid-1970s.[32]&lt;/p&gt;
    &lt;p&gt;These astonishing sales numbers are not explained by the TRS-80’s low price alone. It was also more physically accessible and visible to a broad audience than any computer before it. Radio Shack store managers across the United States set up a TRS-80, powered it on to the BASIC programming language prompt, and left it sitting out for anyone to play with. The experience that students, scientists, and engineers had been discovering and falling in love with in computer centers, labs, and offices for the previous decade became available to anyone who walked into a Radio Shack—and available to take home for just a few hundred dollars. Every electronic hobbyist who had not yet caught the computer bug was exposed every time they came into the store to pick up a few parts for a project, but so were millions of people who dropped in just to get some flashlight batteries or blank cassette tapes. One TRS-80 owner reported: “I discovered the magic of computers in a Radio Shack. My brother and I typed in a small sample program [10 INPUT “WHAT IS YOUR NAME”; A$:PRINT “HELLO,“A$ ] and I was absolutely astounded at what it did.” Others became Radio Shack bums, hanging out in the store all day for a chance to play with the computer.[33]&lt;/p&gt;
    &lt;p&gt;The TRS-80, however, did little to shake Radio Shack’s reputation for cut-rate products: the nickname “TRASH-80” appeared in print in late 1978, and would dog the product line for the rest of its existence, fairly or not. All of the personal computers of this era were clumsy and feeble compared to their minicomputer counterparts. The TRS-80 at least had a proper keyboard, unlike the early PETs. But much about it did exude a particular cheapness, including a mediocre BASIC written by Leininger (Radio Shack, like the rest of the Trinity, ended having to go to MicroSoft for a better version) and a finicky tape controller. Worst of all was the $300 Expansion Interface released in 1978, with ports for connecting a printer and a disk drive, and space for up to 32 kilobytes of additional memory and an expansion card. You could not do anything serious with the TRS-80 without an Expansion Interface, but it was notoriously unreliable, causing reboots, freezes, and monitor glitches. Even the TRS-80 advertisements were cheap knock-offs of Apple’s, with the attractive models in high-end homes replaced by unprepossessing Tandy employees posing awkwardly in Tandy offices or their suburban Fort Worth kitchens.[34]&lt;/p&gt;
    &lt;p&gt;The culture of cheapness extended to French and Leininger, as well. As mere cogs in the Tandy corporate machine, they would never see the kind of monetary rewards that awaited the founders of Apple. French, at least, expected a promotion to vice presidency and a bonus from his work on the TRS-80, but was blocked by Radio Shack president Lew Kornfeld after Charles Tandy’s death in late 1978. Disgruntled, he left to start his own company, implementing the CP/M disk operating system for the TRS-80. Leininger stayed on for several more years, designing Radio Shack computers.[35]&lt;/p&gt;
    &lt;head rend="h3"&gt;The Winnowing&lt;/head&gt;
    &lt;p&gt;In 1975 and 1976, a slew of hobby-entrepreneurs had founded companies to try to turn their love of computers into a living. As in so many new, innovation-driven markets, this early burst of entrepreneurial energy was followed by a brutal winnowing. The majority of the hobby computer companies collapsed by 1979, unable to survive in a suddenly much larger and more competitive market. [36]&lt;/p&gt;
    &lt;p&gt;MITS, though not founded as a hobby computer company, was the first to enter the market and also the first to leave. The company struggled to develop a focused and reliable line of products, and never really rose to the challenge of IMSAI, much less further waves of more powerful and easy-to-use computers. In May 1977, Roberts, who had tired of the business and wanted to cash in, sold MITS for six million dollars to Pertec, a maker of disk and tape drive systems. Most MITS employees disliked the new management. The best of them had already fled to MicroSoft or other ventures, or soon would (Dave Bunnell, for example, started the magazine Personal Computing). Pertec continued to make higher end microprocessor-based minicomputers into the 1980s, but retired the Altair in 1978, and never made another computer targeted at individuals.[37]&lt;/p&gt;
    &lt;p&gt;Bill Millard’s IMSAI, as we have already seen, launched a dud of a second product and was sucked dry of cash to help fund Millard’s chain of ComputerLand retail stores. It went bankrupt in 1979. Millard continued his financial shenanigans for decades, and in the 1990s became a wanted man for over $100 million in unpaid taxes. Processor Technology simply froze, failing to update its Sol product line in response to the threats posed by the rise of the Trinity, and also met its demise in 1979.[38]&lt;/p&gt;
    &lt;p&gt;The Digital Group of Denver had bet their company on a CPU-independent bus, the key product decision that made their computers attractive to hardcore electronic hobbyists who wanted to experiment with different processors. But this expensive and complicated setup also contributed to the company’s abysmal quality control, and new buyers began demanding to pay cash-on-delivery, not up front. This put Digital Group in a cash flow death spiral, and they, too declared bankruptcy in 1979 after a total of 3,000 computers sold.[39]&lt;/p&gt;
    &lt;p&gt;One hobby enterprise, as we know, survived by metamorphosing into a venture-capital-fueled bet on a large-scale personal computer market: Apple Computer. The other survivors found specialized niches unserved by the mass-market Trinity: Cromemco, for example, focused on delivering reliable, powerful, rigorously engineered hardware, and became the darling of the hardcore scientific computerist; Vector Graphics built turn-key business systems.&lt;/p&gt;
    &lt;p&gt;In 1975 and 1976, almost everyone entering the microcomputer business was attracted to it by their passion for the machines themselves; by 1978, with computers backed by deep-pocketed companies flying off of retail shelves, there was a new lure: the scent of money. Prior to that year, for example, almost every franchisee of the retailer ComputerLand was a computer-loving hobbyist. But after a Fortune magazine profile of the company ran in April, the franchise office was flooded with inquiries from businessmen and salesman with no prior interest in computers. For better and for worse, the microcomputer market was leaving its childhood, and its innocence was lost.[40]&lt;/p&gt;
    &lt;p&gt;Even more money was in the offing for those who could figure out how best to market these dazzling devices: make them more accessible, make them more fun, or even convince buyers that they were actually useful. Our next few installments will focus on who was buying these computers in such large quantities, and what they wanted them for.&lt;/p&gt;
    &lt;p&gt;[1] “Most Important Companies,” BYTE (September 1995), 100; Carl Helmers, “Reflections on Entry into Our Third Year,” BYTE (September 1977), 6; “Chuck Peddle on the PET Computer,” Personal Computing (September/October 1977), 31.&lt;/p&gt;
    &lt;p&gt;[2] Freiberger and Swaine, Fire in the Valley, 25-28.&lt;/p&gt;
    &lt;p&gt;[3] Michael Moritz, Return to the Little Kingdom: Steve Jobs, the Creation of Apple, and How it Changed the World (New York: Overlook Press, 2009), 31-32.&lt;/p&gt;
    &lt;p&gt;[4] Moritz, Return to the Little Kingdom, 49-52, 129.&lt;/p&gt;
    &lt;p&gt;[5] One important difference was the use of punched cards for input, though how exactly this worked is unclear. Punch card reader accessories for the computers of the time were typically large and expensive pieces of equipment, designed to read in hundreds of cards per minute. Steve Wozniak, iWoz: Computer Geek to Cult Icon (New York: W.W. Norton, 2006), 86-88. Moritz, Return to the Little Kingdom, 54-63.&lt;/p&gt;
    &lt;p&gt;[6] Wozniak, iWoz, 152-154. Wozniak also, at some point, began work on a video terminal design called the “Computer Conversor,” intended to be commercialized by Call Computer’s owner, Alex Kamradt. Whether this project began before or after the Homebrew meeting is unclear in the sources. Moritz, Return to the Little Kingdom, 124-126, 146-147. Wozniak briefly mentions the Call Computer project in his memoir (p. 170), but never mentions Kamradt, perhaps out of embarrassment that he ditched the never-finished Computer Conversor in favor of Apple Computer. Kamradt, a closeted homosexual, was murdered by a group of young men he picked up in 1991. Will Johnson, “Alex Kamradt,” 2010 (http://www.countyhistorian.com/knol/4hmquk6fx4gu-414-alex-kamradt.html).&lt;/p&gt;
    &lt;p&gt;[7] Wozniak, iWoz, 155-158.&lt;/p&gt;
    &lt;p&gt;[8] Wozniak, iWoz, 162-170; Mos Technology, “Mos 6502 Saves More Money,” September 1975 (https://upload.wikimedia.org/wikipedia/commons/1/14/MOS_6501_6502_Ad_Sept_1975.jpg).&lt;/p&gt;
    &lt;p&gt;[9] Moritz, Return to the Little Kingdom, 96-109.&lt;/p&gt;
    &lt;p&gt;[10] Moritz, Return to the Little Kingdom, 144-161.&lt;/p&gt;
    &lt;p&gt;[11] Moritz, Return to the Little Kingdom, 169-170, 182-185; Wozniak, iWoz, 194-196; Walter Isaacson, Steve Jobs (New York: Simon &amp;amp; Schuster, 2011), 72-76.&lt;/p&gt;
    &lt;p&gt;[12] Moritz, Return to the Little Kingdom, 183-185, 227-229; John Hollar, “Oral History of Armas Clifford (Mike) Markkula, Jr.,” Computer History Museum (May 1, 2012), 22-23.&lt;/p&gt;
    &lt;p&gt;[13] Gareth Edwards, “How Commodore Invented the Mass Market Computer,” Every (March 10, 2025), https://every.to/the-crazy-ones/the-first-king-of-home-computing.&lt;/p&gt;
    &lt;p&gt;[14] Brian Bagnall, Commodore: A Company on the Edge (Variant Press, 2011) 13, 55-58; “Calculator Maker Integrates Downward,” New Scientist (September 9, 1976), 541.&lt;/p&gt;
    &lt;p&gt;[15] Bagnall, Commodore, 4-13, 99, 115.&lt;/p&gt;
    &lt;p&gt;[16] Bagnall, Commodore, 43-44.&lt;/p&gt;
    &lt;p&gt;[17] Bagnall, Commodore, 51-54.&lt;/p&gt;
    &lt;p&gt;[18] Bagnall, Commodore, 59-61.&lt;/p&gt;
    &lt;p&gt;[19] Bagnall, Commodore, 70, 73-77, 84.&lt;/p&gt;
    &lt;p&gt;[20] Bganall, Commodore, 78, 97-101, 111, 117.&lt;/p&gt;
    &lt;p&gt;[21] Early promotional materials put the planned MSRP at $495, and some sources still report that price, but it was never offered that cheaply. A $495 4KB model was quickly discontinued when it was found to be a money loser. Bagnall, Commodore, 99, 101, 111, 114-115.&lt;/p&gt;
    &lt;p&gt;[22] “Commodore’s New PET Computer,” BYTE (October 1977), 50; “The PET Discussion,” Personal Computing (September/October 1977), 30-42; William J. Hawkins, “New Home Computers Can Change Your Lifestyle,” Popular Science (October 1977), 30-36; Bagnall, Commodore, 132; “BYTE News,” BYTE (May 1979), 117.&lt;/p&gt;
    &lt;p&gt;[23] Bagnall, Commodore, 161, 171-127.&lt;/p&gt;
    &lt;p&gt;[24] Irvin Farman, Tandy’s Money Machine: How Charles Tandy Built Radio Shack Into the World’s Largest Electronics Chain (Chicago: Mobium Press, 1992), 28-51, 154-163; Radio Shack, “1975 Electronics Catalog,” 84-85.&lt;/p&gt;
    &lt;p&gt;[25] Ira Goldklang, “TRS-80 Computers: Don French – The Father of the TRS-80,” June 13, 2021 (https://www.trs-80.com/wordpress/trs-80-computers-don-french/); “Interview with Don French, Co-designer of the TRS-80 Model I,” Floppy Days (February 21, 2016), https://floppydays.libsyn.com/floppy-days-53-interview-with-don-french-co-designer-of-the-trs-80-model-i.&lt;/p&gt;
    &lt;p&gt;[26] “Interview with Don French, Co-designer of the TRS-80 Model I,” Floppy Days (February 21, 2016).&lt;/p&gt;
    &lt;p&gt;[27] Farman, Tandy’s Money Machine, 402; David Welsh and Theresa Welsh, Priming the Pump: How TRS-80 Enthusiasts Helped Spark the PC Revolution (Ferndale, Michigan: The Seeker Books, 2013), 2-4.&lt;/p&gt;
    &lt;p&gt;[28] Welsh and Welsh, Priming the Pump, 4-5.&lt;/p&gt;
    &lt;p&gt;[29] The timeline of the TRS-80’s development has proved hard to pin down. My reconstruction of the timeline is anchored the very clear and precise dates given by Leininger in a 2024 interview: he started working for Tandy July 5th 1976, and demoed the computer to Tandy in February 1977. “Floppy Days 144 – Interview with Don French and Steve Leininger, Co-Designers of the TRS-80 Model I”, Floppy Days (Oct 27, 2024), https://floppydays.libsyn.com/floppy-days-144-interview-with-don-french-and-steve-leininger. In the same interview (and in other accounts), Don French places himself and John Roach at the West Coast Computer Faire before hiring Leininger to start working on the TRS-80, but with the first West Coast Computer Faire took place in April 1977, months after the prototype TRS-80 had already been demonstrated and approved for production by Charles Tandy. He also puts his conversations with Peddle at Commodore about possibly designing a computer for Radio Shack at a point before hiring, Leininger, but Peddle didn’t work for Commodore at that point. A 1981 article on Leininger gives an even more nonsensical timeline, in which Leininger leaves California to work for Tandy in the fall of 1975 (before the first Byte Shop, where he worked evenings, had even opened), chooses the Z80 in February 1976 (months before its release), and presents the prototype for Tandy’s approval in April 1976 for delivery in August of that same year (a year early). Jonathan Erickson, “The Men Behind the TRS-80,” Popular Computing (December 1981), 26-27.&lt;/p&gt;
    &lt;p&gt;[30] Welsh and Welsh, Priming the Pump, 4-5, 8.&lt;/p&gt;
    &lt;p&gt;[31] Welsh and Welsh, Priming the Pump, 6-7.&lt;/p&gt;
    &lt;p&gt;[32] Welsh and Welsh, priming the Pump, 25; “BYTE News,” BYTE (May 1979), 117; James L. Pelkey, “Chapter 7 – Data Communications: Market Order 1973-1979,” The History of Computer Communications (https://historyofcomputercommunications.info/section/7.1/Minicomputers,-Distributed-Data-Processing-and-Microprocessors).&lt;/p&gt;
    &lt;p&gt;[33] Welsh and Welsh, Priming the Pump, 30.&lt;/p&gt;
    &lt;p&gt;[34] Welsh and Welsh, Priming the Pump, 28, 36, 38; “Letters,” Kilobaud (December 1978), 18; Computer History Museum, “Radio Shack TRS-80 advertisement” (1977), https://www.computerhistory.org/revolution/personal-computers/17/298/1163; “Interview with Don French, Co-designer of the TRS-80 Model I”, Floppy Days (February 21, 2016), https://floppydays.libsyn.com/floppy-days-53-interview-with-don-french-co-designer-of-the-trs-80-model-i.&lt;/p&gt;
    &lt;p&gt;[35] Welsh and Welsh, Priming the Pump, 37.&lt;/p&gt;
    &lt;p&gt;[36] Apple and Cromemco are the only pre-1977 computer makers that survived long enough to be covered in Robert Levering, et al.’s The Computer Entrepreneurs, a 1984 collection of sixty-five industry founder profiles.&lt;/p&gt;
    &lt;p&gt;[37] Frieberger and Swaine, Fire in the Valley, 70-73, 153-155.&lt;/p&gt;
    &lt;p&gt;[38] Robert Frank, “After 20 Years, Missing CEO Reappears,” The Wall Street Journal (September 12, 2011).&lt;/p&gt;
    &lt;p&gt;[39] Robert Suding, “Digital Group Computers – The Real Story,” (ca. 2004), https://web.archive.org/web/20060820083602/www.ultimatecharger.com/dg.html.&lt;/p&gt;
    &lt;p&gt;[40] Jonathan Littman, Once Upon a Time in ComputerLand (New York: Touchstone, 1990), 133.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45457460</guid><pubDate>Fri, 03 Oct 2025 00:51:26 +0000</pubDate></item><item><title>I Trained a Small Language Model from Scratch</title><link>https://nwosunneoma.medium.com/how-i-trained-a-small-language-model-from-scratch-8af167479d1a</link><description>&lt;doc fingerprint="305905bb237aaed8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I Trained A Small Language Model From Scratch&lt;/head&gt;
    &lt;p&gt;With the artificial intelligence ecosystem rapidly growing with different systems being built, it has reached its inflection point. While billions are being poured into ever larger models, these expenses do not reflect the ROI with 42% of these projects delivering zero ROI, and 88% of proof of concept not getting to production. The issue isn’t with AI’s capabilities but with a mismatch on massive, general-purpose models and focused business needs.&lt;/p&gt;
    &lt;p&gt;This horrible phenomenon creates a need for a totally differnt approach to the situation: Small Language Models (SLMs). This approach trades breadth for depth where broad knowledge is traded for deep specialization and theoretical capability for measurable business outcomes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Less is More:&lt;/head&gt;
    &lt;p&gt;Large Language Models like GPT-4 contains 175+ billion parameters which require massive computational resource and cloud infrastructure, they excel at general knowledge but struggle with business contexts. Its inference costs are directly proportional to the model size, need for high quality GPUs and significant RAM and above all data privacy.&lt;/p&gt;
    &lt;p&gt;Away from the technical foundation, there’s the business problem: scaling. This isn’t a technical deficiency but a business one where organizations attempts to deploy general-purpose models for specific business processes leading to several inadequacy problems. There’s the integration complexity which require extensive infrastructure changes to connect with existing business systems, cost unpredictability due to token based pricing by providers leading to budgeting difficulty, and performance inconsistency as they excel at some tasks and fail in the other.&lt;/p&gt;
    &lt;p&gt;Small Language Models, typically ranging from 1 million to 10 billion parameters, address these constraints through focused specialization. Take for example a 16M parameter model trained on medical inbound call transcripts, rather than knowing everything about everything, it knows everything about medical inbound interactions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Architecture Efficiency in Practice:&lt;/head&gt;
    &lt;p&gt;To demonstrate the efficiency of SLMs over LLMs, I built a BYOD (Bring Your Own Data) pipeline. This pipeline takes in your text data, trains a model that is specialized to your data, and learns from it.&lt;/p&gt;
    &lt;p&gt;Data:&lt;/p&gt;
    &lt;p&gt;The data I used was a data from Huggingface which is a call transcript for automotive business customer service. I downloaded and added the zip file to my directory and ran the pipeline via the CLI, it supports archive files too, so you don’t need to worry about unzipping before usage.&lt;/p&gt;
    &lt;p&gt;Model Specifications:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;6 layers, 6 heads, 384 embedding dimensions&lt;/item&gt;
      &lt;item&gt;16 million parameters&lt;/item&gt;
      &lt;item&gt;50,257 vocabulary tokens&lt;/item&gt;
      &lt;item&gt;128 tokens for block size.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Performance:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Training loss improvement from 9.2 -&amp;gt; 2.2 indicating successful pattern learning&lt;/item&gt;
      &lt;item&gt;Domain specialization where it learned automotive service conversation structure&lt;/item&gt;
      &lt;item&gt;Preserved and maintained format of transcript metadata and speaker identification.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This model learned the specific patterns of automotive customer service calls, including technical vocabulary, conversation flow, and domain-specific terminology that a general-purpose model might miss or handle inefficiently.&lt;/p&gt;
    &lt;p&gt;Advantage:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Memory Efficiency: A 16M parameter model requires approximately 64MB of storage (using 32-bit precision) which fits comfortably on mobile devices or edge hardware.&lt;/item&gt;
      &lt;item&gt;Inference Speed: Smaller models generate tokens faster, crucial for real-time applications like customer service chatbots or technical support systems.&lt;/item&gt;
      &lt;item&gt;Fine-tuning Agility: Domain-specific training converges faster and requires less computational resources than attempting to specialize billion-parameter models.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Significance To Business Needs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Predictable Economics: A self-hosted 16M parameter model has fixed infrastructure costs regardless of usage volume. No token limits, no API rate limits, no scaling surprises.&lt;/item&gt;
      &lt;item&gt;Deep Integration: Can be embedded directly into business applications, CRM systems, or manufacturing equipment without architectural overhauls.&lt;/item&gt;
      &lt;item&gt;Consistent Performance: Maintains consistent quality within their specialization area, reducing the variability that plagues general-purpose deployments.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Honest Limitations Assessment:&lt;/head&gt;
    &lt;p&gt;Performance Trade-offs&lt;/p&gt;
    &lt;p&gt;SLMs sacrifice breadth for depth. A 16M parameter automotive model cannot discuss philosophy, write poetry, or solve complex mathematical problems. This limitation becomes a strength in business contexts where focused performance matters more than general capability.&lt;/p&gt;
    &lt;p&gt;Mitigation Strategy: Deploy multiple specialized models rather than one general model. The combined cost of several SLMs often remains lower than a single LLM deployment while providing superior domain performance.&lt;/p&gt;
    &lt;p&gt;Data Quality Requirements:&lt;/p&gt;
    &lt;p&gt;Small models amplify the importance of training data quality. The automotive customer service model I implemented learned JSON formatting alongside conversational patterns because the training data contained technical metadata. This highlights both a limitation and an opportunity.&lt;/p&gt;
    &lt;p&gt;Data Preprocessing Critical Path:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extract pure conversational content from technical wrappers&lt;/item&gt;
      &lt;item&gt;Normalize speaker identification and formatting&lt;/item&gt;
      &lt;item&gt;Remove system artifacts and metadata&lt;/item&gt;
      &lt;item&gt;Focus on business-relevant dialogue patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Scaling Considerations:&lt;/p&gt;
    &lt;p&gt;Individual SLMs excel within their domains but may create management overhead as organizations deploy multiple specialized models.&lt;/p&gt;
    &lt;p&gt;Management Solutions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Standardized deployment pipelines (like the BYOD framework)&lt;/item&gt;
      &lt;item&gt;Centralized model monitoring and updates&lt;/item&gt;
      &lt;item&gt;Consistent API interfaces across specialized models&lt;/item&gt;
      &lt;item&gt;Automated data pipeline management&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;The Future of Enterprise AI: Specialized Intelligence&lt;/head&gt;
    &lt;p&gt;The ecosystem’s obsession with scale has created an opportunity for specialization, pragmatic organizations can build competitive advantages through focused, efficient, and measurable AI systems deployments.&lt;/p&gt;
    &lt;p&gt;Small Language Models represent a return to engineering fundamentals: building solutions that solve specific problems efficiently rather than pursuing theoretical capabilities that may never translate to business value.&lt;/p&gt;
    &lt;p&gt;The 42% of organizations currently seeing zero ROI from AI investments have an alternative path. Instead of waiting for the next breakthrough in general AI, they can build specialized intelligence that delivers measurable value today.&lt;/p&gt;
    &lt;p&gt;The future of enterprise AI isn’t about having the biggest model, it’s about having the right model for the job.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45457929</guid><pubDate>Fri, 03 Oct 2025 01:49:52 +0000</pubDate></item></channel></rss>