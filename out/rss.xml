<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 28 Nov 2025 21:08:45 +0000</lastBuildDate><item><title>Meta hiding $27B in debt using advanced geometry</title><link>https://stohl.substack.com/p/exclusive-credit-report-shows-meta</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46079868</guid><pubDate>Fri, 28 Nov 2025 16:09:50 +0000</pubDate></item><item><title>AI Adoption Rates Starting to Flatten Out</title><link>https://www.apolloacademy.com/ai-adoption-rates-starting-to-flatten-out/</link><description>&lt;doc fingerprint="361896579877b344"&gt;
  &lt;main&gt;
    &lt;p&gt;Data from the Census Bureau and Ramp shows that AI adoption rates are starting to flatten out across all firm sizes, see charts below.&lt;/p&gt;
    &lt;p&gt;This presentation may not be distributed, transmitted or otherwise communicated to others in whole or in part without the express consent of Apollo Global Management, Inc. (together with its subsidiaries, ‚ÄúApollo‚Äù).&lt;/p&gt;
    &lt;p&gt;Apollo makes no representation or warranty, expressed or implied, with respect to the accuracy, reasonableness, or completeness of any of the statements made during this presentation, including, but not limited to, statements obtained from third parties. Opinions, estimates and projections constitute the current judgment of the speaker as of the date indicated. They do not necessarily reflect the views and opinions of Apollo and are subject to change at any time without notice. Apollo does not have any responsibility to update this presentation to account for such changes. There can be no assurance that any trends discussed during this presentation will continue.&lt;/p&gt;
    &lt;p&gt;Statements made throughout this presentation are not intended to provide, and should not be relied upon for, accounting, legal or tax advice and do not constitute an investment recommendation or investment advice. Investors should make an independent investigation of the information discussed during this presentation, including consulting their tax, legal, accounting or other advisors about such information. Apollo does not act for you and is not responsible for providing you with the protections afforded to its clients. This presentation does not constitute an offer to sell, or the solicitation of an offer to buy, any security, product or service, including interest in any investment product or fund or account managed or advised by Apollo.&lt;/p&gt;
    &lt;p&gt;Certain statements made throughout this presentation may be ‚Äúforward-looking‚Äù in nature. Due to various risks and uncertainties, actual events or results may differ materially from those reflected or contemplated in such forward-looking information. As such, undue reliance should not be placed on such statements. Forward-looking statements may be identified by the use of terminology including, but not limited to, ‚Äúmay‚Äù, ‚Äúwill‚Äù, ‚Äúshould‚Äù, ‚Äúexpect‚Äù, ‚Äúanticipate‚Äù, ‚Äútarget‚Äù, ‚Äúproject‚Äù, ‚Äúestimate‚Äù, ‚Äúintend‚Äù, ‚Äúcontinue‚Äù or ‚Äúbelieve‚Äù or the negatives thereof or other variations thereon or comparable terminology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46079987</guid><pubDate>Fri, 28 Nov 2025 16:21:09 +0000</pubDate></item><item><title>True P2P Email on Top of Yggdrasil Network</title><link>https://github.com/JB-SelfCompany/Tyr</link><description>&lt;doc fingerprint="bfbae441fb0022cf"&gt;
  &lt;main&gt;
    &lt;p&gt;English | –†—É—Å—Å–∫–∏–π&lt;/p&gt;
    &lt;p&gt;True P2P Email on top of Yggdrasil Network&lt;/p&gt;
    &lt;p&gt;We're taught that email must go through servers. Why? Because the Internet was built around centralized infrastructure. Every email you send travels through multiple servers - your provider's server, maybe a few relay servers, and finally your recipient's provider's server. Each hop is a potential point of surveillance, censorship, or failure.&lt;/p&gt;
    &lt;p&gt;Even "encrypted" email solutions still rely on these centralized servers. They encrypt the message content but the metadata - who you're talking to, when, how often - is visible to anyone watching the servers.&lt;/p&gt;
    &lt;p&gt;But there is a network, called Yggdrasil, that gives everyone a free IPv6 and doesn't need a blessing from your ISP. We finally have this possibility to use true P2P email. And moreover, this network has strong encryption to protect all data that flows from one IP to another.&lt;/p&gt;
    &lt;p&gt;So, Tyr brings true peer-to-peer email to your Android device using these unusual conditions. Unlike traditional email clients, Tyr doesn't need:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Centralized mail servers (the connections are straight P2P)&lt;/item&gt;
      &lt;item&gt;Message encryption layers (the network takes care of that)&lt;/item&gt;
      &lt;item&gt;Port forwarding or STUN/TURN servers (Yggdrasil handles NAT traversal)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Full integration with DeltaChat and ArcaneChat - the best decentralized messengers&lt;/item&gt;
      &lt;item&gt;Local SMTP/IMAP server running on your device&lt;/item&gt;
      &lt;item&gt;Automatic Ed25519 key generation for your mail identity&lt;/item&gt;
      &lt;item&gt;Connection to the Yggdrasil Network with configurable peers&lt;/item&gt;
      &lt;item&gt;Auto-start on boot for always-on availability&lt;/item&gt;
      &lt;item&gt;Encrypted backup &amp;amp; restore with password protection&lt;/item&gt;
      &lt;item&gt;Automatic recovery from Android Keystore issues (Samsung devices)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One of Tyr's strong points is censorship circumvention: you can connect to any of hundreds of available Yggdrasil nodes, host your own, or even build a private network. Email freedom is literally in your hands.&lt;/p&gt;
    &lt;p&gt;Tyr runs a complete email server right on your Android device, using the Yggdrasil network for transport. The Yggmail mail server (built in Go) is embedded as a library inside the app and runs as a foreground service.&lt;/p&gt;
    &lt;p&gt;On top of Yggdrasil, it provides standard SMTP and IMAP protocols on localhost (127.0.0.1:1025 and 127.0.0.1:1143). Any email client can connect to these ports - but we recommend DeltaChat or ArcaneChat for the best P2P messaging experience.&lt;/p&gt;
    &lt;p&gt;Every Tyr installation generates unique Ed25519 cryptographic keys. Your mail address is derived from your public key, making it: &lt;code&gt;&amp;lt;64-hex-characters&amp;gt;@yggmail&lt;/code&gt;. This means your identity is cryptographically verifiable and cannot be spoofed.&lt;/p&gt;
    &lt;p&gt;DeltaChat and ArcaneChat are perfect companions for Tyr. These are messengers that use email protocols but provide modern chat interfaces. When you configure DeltaChat/ArcaneChat to use Tyr's local server:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;DeltaChat/ArcaneChat sends messages via SMTP to Tyr&lt;/item&gt;
      &lt;item&gt;Tyr wraps them in Yggmail protocol and sends through Yggdrasil&lt;/item&gt;
      &lt;item&gt;The recipient's Tyr receives the message via Yggdrasil&lt;/item&gt;
      &lt;item&gt;Their DeltaChat/ArcaneChat fetches it via IMAP from their local Tyr&lt;/item&gt;
      &lt;item&gt;All this happens peer-to-peer, with no central servers&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install Tyr and complete the onboarding (set password, configure peers)&lt;/item&gt;
      &lt;item&gt;Start the Yggmail service in Tyr&lt;/item&gt;
      &lt;item&gt;Install DeltaChat or ArcaneChat from F-Droid or Google Play&lt;/item&gt;
      &lt;item&gt;In Tyr's main screen, tap "Setup DeltaChat/ArcaneChat"&lt;/item&gt;
      &lt;item&gt;Tyr will automatically open DeltaChat/ArcaneChat with pre-configured settings&lt;/item&gt;
      &lt;item&gt;Complete the setup and start chatting!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If automatic setup doesn't work:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install Tyr and complete the onboarding (set password, configure peers)&lt;/item&gt;
      &lt;item&gt;Start the Yggmail service in Tyr&lt;/item&gt;
      &lt;item&gt;Copy your mail address from the main screen (looks like &lt;code&gt;abc123...@yggmail&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Install DeltaChat or ArcaneChat from F-Droid or Google Play&lt;/item&gt;
      &lt;item&gt;In DeltaChat/ArcaneChat, tap "Create a new profile"&lt;/item&gt;
      &lt;item&gt;Enter a name and optionally select an avatar&lt;/item&gt;
      &lt;item&gt;Tap "Use a different server" (below the login fields)&lt;/item&gt;
      &lt;item&gt;Enter your Yggmail address and the password you set in Tyr&lt;/item&gt;
      &lt;item&gt;Tap "‚úì" in the top right corner to complete setup&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Important: Tyr must be running for DeltaChat/ArcaneChat to send and receive messages. Enable auto-start in Tyr settings for seamless experience.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Android Studio (latest version recommended)&lt;/item&gt;
      &lt;item&gt;JDK 17&lt;/item&gt;
      &lt;item&gt;Android SDK (API 23-36)&lt;/item&gt;
      &lt;item&gt;Go 1.21+ and gomobile (only if rebuilding yggmail.aar)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clone the repository:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone &amp;lt;repository-url&amp;gt;
cd Tyr&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build debug APK:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./gradlew assembleDebug&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install to connected device:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./gradlew installDebug&lt;/code&gt;
    &lt;p&gt;APKs will be in &lt;code&gt;app/build/outputs/apk/debug/&lt;/code&gt; or &lt;code&gt;app/build/outputs/apk/release/&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;If you need to rebuild the Yggmail library:&lt;/p&gt;
    &lt;code&gt;cd ../yggmail/mobile
# On Windows:
..\build-android.bat
# On Unix:
gomobile bind -target=android -androidapi 23 -javapkg=com.jbselfcompany.tyr -ldflags="-checklinkname=0" -o yggmail.aar .&lt;/code&gt;
    &lt;p&gt;Then copy &lt;code&gt;yggmail.aar&lt;/code&gt; to &lt;code&gt;Tyr/app/libs/&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Language: Kotlin 2.2.20&lt;/item&gt;
      &lt;item&gt;Min SDK: 23 (Android 6.0)&lt;/item&gt;
      &lt;item&gt;Target SDK: 33 (Android 13)&lt;/item&gt;
      &lt;item&gt;Architecture: Layered (UI ‚Üí Service ‚Üí Data)&lt;/item&gt;
      &lt;item&gt;Mail server: Yggmail (Go library, embedded via gomobile)&lt;/item&gt;
      &lt;item&gt;Network: Yggdrasil overlay network&lt;/item&gt;
      &lt;item&gt;Localization: English, Russian&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;üîí Security implementation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Passwords are encrypted using Android Keystore System (AES256-GCM encryption)&lt;/item&gt;
      &lt;item&gt;Automatic Keystore recovery: Handles Android Keystore issues on Samsung and other devices automatically&lt;/item&gt;
      &lt;item&gt;Network encryption provided by Yggdrasil Network for all peer-to-peer communications&lt;/item&gt;
      &lt;item&gt;Local-only access: SMTP/IMAP ports (1025/1143) are bound to localhost only, not accessible from network&lt;/item&gt;
      &lt;item&gt;Cryptographic identity: Ed25519 keys ensure your mail address cannot be spoofed&lt;/item&gt;
      &lt;item&gt;Encrypted backups: Configuration and keys can be backed up with password protection&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yggmail: The mail transfer agent that powers Tyr&lt;/item&gt;
      &lt;item&gt;Mimir: P2P messenger on Yggdrasil (sister project)&lt;/item&gt;
      &lt;item&gt;Yggdrasil Network: The mesh network infrastructure&lt;/item&gt;
      &lt;item&gt;DeltaChat: Recommended email-based messenger client&lt;/item&gt;
      &lt;item&gt;ArcaneChat: Alternative email-based messenger client&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Tyr is open source software. The Yggmail library uses Mozilla Public License v. 2.0.&lt;/p&gt;
    &lt;p&gt;See &lt;code&gt;LICENSE&lt;/code&gt; file for full details&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080143</guid><pubDate>Fri, 28 Nov 2025 16:35:24 +0000</pubDate></item><item><title>Stellantis Is Spamming Owners' Screens with Pop-Up Ads for New Car Discounts</title><link>https://www.thedrive.com/news/stellantis-is-spamming-owners-screens-with-pop-up-ads-for-new-car-discounts</link><description>&lt;doc fingerprint="acf89966da2f210a"&gt;
  &lt;main&gt;
    &lt;p&gt;The internet is raging right now over Stellantis pushing marketing pop-ups to owners‚Äô in-car screens. It‚Äôs obnoxious, to be sure, and it‚Äôs legit‚Äîwe have confirmation from a Jeep driver as well as Stellantis itself. But this isn‚Äôt even the first time it‚Äôs happened, as we reported in February about Jeep advertising extended warranties in the same way.&lt;/p&gt;
    &lt;p&gt;Auto writer and all-around car guy Zerin Dube posted earlier this week about his WL Grand Cherokee‚Äôs ‚Äúmarketing notification‚Äù on X. The photo started popping off, and before long, others were sharing the same ad that was sent to their screens. Dube labeled it as ‚Äúlate stage capitalism,‚Äù which feels like an accurate descriptor.&lt;/p&gt;
    &lt;p&gt;Funny enough, Dube was in the market for a new Wrangler anyhow, and he took advantage of the $1,500 loyalty offer Jeep sent straight to his Grand Cherokee. He drove off the lot Thursday night in a new Rubicon X, so I guess you can say it worked.&lt;/p&gt;
    &lt;p&gt;Others online were less accepting of the ploy. Practically every repost of Dube‚Äôs photo decried Jeep and Stellantis at large, with some Ram and Chrysler drivers corroborating the ad‚Äôs rollout that spread across multiple brands. Most of the comments said something along the lines of, ‚ÄúGuess what I‚Äôm never buying.‚Äù Some made the informed prediction that this type of promotion will become commonplace in the industry before long.&lt;/p&gt;
    &lt;p&gt;For its part, Stellantis told The Drive that it sends out these notifications to ‚Äústay in contact with our owners at critical points in their ownership.‚Äù The in-vehicle message system is also used to notify drivers of vehicle recalls and health monitor alerts. A spokesperson for the brand said:&lt;/p&gt;
    &lt;p&gt;‚ÄúRecently, a select group of owners received a special marketing notification in their vehicle, and we tailored this special offer to minimize any intrusions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The simple text message offering a $1,500 bonus incentive appears only on startup and while the vehicle is stationary&lt;/item&gt;
      &lt;item&gt;The message disappears when the vehicle begins moving, or the driver clicks the OK or X icon on the screen, or after 15 seconds&lt;/item&gt;
      &lt;item&gt;The message returns at the next key-on cycle only if the driver clicked on Remind Me Later, or they did not click OK or X&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;‚ÄúOur goal is to deliver the best vehicle experience for our customers. As a result of these efforts, we have seen our customers take advantage of this offer,‚Äù the spokesperson said.&lt;/p&gt;
    &lt;p&gt;The Stellantis spokesperson concluded by saying that owners can permanently opt out of in-vehicle messaging by calling the company‚Äôs customer care line at 800-777-3600.&lt;/p&gt;
    &lt;p&gt;The $1,500 loyalty bonus is just one of many discounts that Stellantis dealers are stacking in order to move more units. I reported last week that storming Ford Bronco sales are genuinely threatening the Wrangler‚Äôs spot atop the segment, and as Dube shared on X, he got something like $16,500 off his new Jeep. Apparently, now‚Äôs a good time to buy if you can stand the occasional spam on your car‚Äôs screen.&lt;/p&gt;
    &lt;p&gt;The last time this happened, Stellantis was advertising extended warranties to owners via their infotainment. Many were frustrated as the offer continued to show, even after acknowledging it by pressing ‚ÄúOK.‚Äù What‚Äôs more, others saw the ad despite their car exceeding the mileage limit mentioned in the promotion.&lt;/p&gt;
    &lt;p&gt;Connected cars, man. Gotta love ‚Äôem.&lt;/p&gt;
    &lt;p&gt;Got a tip or question for the author? Contact them directly: caleb@thedrive.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080161</guid><pubDate>Fri, 28 Nov 2025 16:37:32 +0000</pubDate></item><item><title>Lobsters Interview</title><link>https://susam.net/my-lobsters-interview.html</link><description>&lt;doc fingerprint="2409133128b73ad1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;My Lobsters Interview&lt;/head&gt;
    &lt;p&gt;I recently had an engaging conversation with Alex (@veqq) from the Lobsters community about computing, mathematics and a range of related topics. Our conversation was later published on the community website as Lobsters Interview with Susam.&lt;/p&gt;
    &lt;p&gt;I should mention the sections presented in that post are not in the same order in which we originally discussed them. The sections were edited and rearranged by Alex to improve the flow and avoid repetition of similar topics too close to each other.&lt;/p&gt;
    &lt;p&gt;This page preserves a copy of our discussion as edited by Alex, so I can keep an archived version on my website. In my copy, I have added a table of contents to make it easier to navigate to specific sections. The interview itself follows the table of contents. I hope you enjoy reading it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contents&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lisp and Other Things&lt;/item&gt;
      &lt;item&gt;Lisp, Emacs and Mathematics&lt;/item&gt;
      &lt;item&gt;Interests and Exploration&lt;/item&gt;
      &lt;item&gt;Computing for Fun&lt;/item&gt;
      &lt;item&gt;Computing Activities&lt;/item&gt;
      &lt;item&gt;Programming vs Domains&lt;/item&gt;
      &lt;item&gt;Old Functionality and New Problems&lt;/item&gt;
      &lt;item&gt;Designing for Composability&lt;/item&gt;
      &lt;item&gt;Small vs Large Functions&lt;/item&gt;
      &lt;item&gt;Domains and Projects&lt;/item&gt;
      &lt;item&gt;Double Spacing and Touch Typing&lt;/item&gt;
      &lt;item&gt;Approach to Learning&lt;/item&gt;
      &lt;item&gt;Managing Time and Distractions&lt;/item&gt;
      &lt;item&gt;Blogging&lt;/item&gt;
      &lt;item&gt;Forums&lt;/item&gt;
      &lt;item&gt;MathB Moderation Problems&lt;/item&gt;
      &lt;item&gt;Favourite Mathematics Textbooks&lt;/item&gt;
      &lt;item&gt;Mathematics and Computing&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Our Conversation&lt;/head&gt;
    &lt;p&gt;Hi @susam, I primarily know you as a Lisper, what other things do you use?&lt;/p&gt;
    &lt;p&gt;Yes, I use Lisp extensively for my personal projects and much of what I do in my leisure is built on it. I ran a mathematics pastebin for close to thirteen years. It was quite popular on some IRC channels. The pastebin was written in Common Lisp. My personal website and blog are generated using a tiny static site generator written in Common Lisp. Over the years I have built several other personal tools in it as well.&lt;/p&gt;
    &lt;p&gt;I am an active Emacs Lisp programmer too. Many of my software tools are in fact Emacs Lisp functions that I invoke with convenient key sequences. They help me automate repetitive tasks as well as improve my text editing and task management experience.&lt;/p&gt;
    &lt;p&gt;I use plenty of other tools as well. In my early adulthood, I spent many years working with C, C++, Java and PHP. My first substantial open source contribution was to the Apache Nutch project which was in Java and one of my early original open source projects was Uncap, a C program to remap keys on Windows.&lt;/p&gt;
    &lt;p&gt;These days I use a lot of Python, along with some Go and Rust, but Lisp remains important to my personal work. I also enjoy writing small standalone tools directly in HTML and JavaScript, often with all the code in a single file in a readable, unminified form.&lt;/p&gt;
    &lt;p&gt;How did you first discover computing, then end up with Lisp, Emacs and mathematics?&lt;/p&gt;
    &lt;p&gt;I got introduced to computers through the Logo programming language as a kid. Using simple arithmetic, geometry, logic and code to manipulate a two-dimensional world had a lasting effect on me.&lt;/p&gt;
    &lt;p&gt;I still vividly remember how I ended up with Lisp. It was at an airport during a long layover in 2007. I wanted to use the time to learn something, so I booted my laptop running Debian GNU/Linux 4.0 (Etch) and then started GNU CLISP 2.41. In those days, Wi-Fi in airports was uncommon. Smartphones and mobile data were also uncommon. So it was fortunate that I had CLISP already installed on my system and my laptop was ready for learning Common Lisp. I had it installed because I had wanted to learn Common Lisp for some time. I was especially attracted by its simplicity, by the fact that the entire language can be built up from a very small set of special forms. I use SBCL these days, by the way.&lt;/p&gt;
    &lt;p&gt;I discovered Emacs through Common Lisp. Several sources recommended using the Superior Lisp Interaction Mode for Emacs (SLIME) for Common Lisp programming, so that's where I began. For many years I continued to use Vim as my primary editor, while relying on Emacs and SLIME for Lisp development. Over time, as I learnt more about Emacs itself, I grew fond of Emacs Lisp and eventually made Emacs my primary editor and computing environment.&lt;/p&gt;
    &lt;p&gt;I have loved mathematics since my childhood days. What has always fascinated me is how we can prove deep and complex facts using first principles and clear logical steps. That feeling of certainty and rigour is unlike anything else.&lt;/p&gt;
    &lt;p&gt;Over the years, my love for the subject has been rekindled many times. As a specific example, let me share how I got into number theory. One day I decided to learn the RSA cryptosystem. As I was working through the RSA paper, I stumbled upon the Euler totient function \( \varphi(n) \) which gives the number of positive integers not exceeding n that are relatively prime to n. The paper first states that \[ \varphi(p) = p - 1 \] for prime numbers \( p. \) That was obvious since \( p \) has no factors other than \( 1 \) and itself, so every integer from \( 1 \) up to \( p - 1 \) must be relatively prime to it. But then it presents \[ \varphi(pq) = \varphi(p) \cdot \varphi(q) = (p - 1)(q - 1) \] for primes \( p \) and \( q. \) That was not immediately obvious to me back then. After a few minutes of thinking, I managed to prove it from scratch. By the inclusion-exclusion principle, we count how many integers from \( 1 \) up to \( pq \) are not divisible by \(p \) or \( q. \) There are \( pq \) integers in total. Among them, there are \( q \) integers divisible by \( p \) and \( p \) integers divisible by \( q. \) So we need to subtract \( p + q \) from \(pq. \) But since one integer (\( pq \) itself) is counted in both groups, we add \( 1 \) back. Therefore \[ \varphi(pq) = pq - (p + q) + 1 = (p - 1)(q - 1). \] Next I could also obtain the general formula for \( \varphi(n) \) for an arbitrary positive integer \( n \) using the same idea. There are several other proofs too, but that is how I derived the general formula for \( \varphi(n) \) when I first encountered it. And just like that, I had begun to learn number theory!&lt;/p&gt;
    &lt;p&gt;You've said you prefer computing for fun. What is fun to you? Do you have an idea of what makes something fun or not?&lt;/p&gt;
    &lt;p&gt;For me, fun in computing began when I first learnt IBM/LCSI PC Logo when I was nine years old. I had very limited access to computers back then, perhaps only about two hours per month in the computer laboratory at my primary school. Most of my Logo programming happened with pen and paper at home. I would "test" my programs by tracing the results on graph paper. Eventually I would get about thirty minutes of actual computer time in the lab to run them for real.&lt;/p&gt;
    &lt;p&gt;So back then, most of my computing happened without an actual computer. But even with that limited access to computers, a whole new world opened up for me: one that showed me the joy of computing and more importantly, the joy of sharing my little programs with my friends and teachers. One particular Logo program I still remember very well drew a house with animated dashed lines, where the dashes moved around the outline of the house. Everyone around me loved it, copied it and tweaked it to change the colours, alter the details and add their own little touches.&lt;/p&gt;
    &lt;p&gt;For me, fun in computing comes from such exploration and sharing. I enjoy asking "what happens if" and then seeing where it leads me. My Emacs package devil-mode comes from such exploration. It came from asking, "What happens if we avoid using the ctrl and meta modifier keys and use , (the comma key) or another suitable key as a leader key instead? And can we still have a non-modal editing experience?"&lt;/p&gt;
    &lt;p&gt;Sometimes computing for fun may mean crafting a minimal esoteric drawing language, making a small game or building a tool that solves an interesting problem elegantly. It is a bonus if the exploration results in something working well enough that I can share with others on the World Wide Web and others find it fun too.&lt;/p&gt;
    &lt;p&gt;How do you choose what to investigate? Which most interest you, with what commonalities?&lt;/p&gt;
    &lt;p&gt;For me, it has always been one exploration leading to another.&lt;/p&gt;
    &lt;p&gt;For example, I originally built MathB for my friends and myself who were going through a phase in our lives when we used to challenge each other with mathematical puzzles. This tool became a nice way to share solutions with each other. Its use spread from my friends to their friends and colleagues, then to schools and universities and eventually to IRC channels.&lt;/p&gt;
    &lt;p&gt; Similarly, I built TeXMe when I was learning neural networks and taking a lot of notes on the subject. I was not ready to share the notes online, but I did want to share them with my friends and colleagues who were also learning the same topic. Normally I would write my notes in LaTeX, compile them to PDF and share the PDF, but in this case, I wondered, what if I took some of the code from MathB and created a tool that would let me write plain Markdown (GFM) + LaTeX (MathJax) in a &lt;code&gt;.html&lt;/code&gt; file and have the tool render the file as soon
  as it was opened in a web browser?  That resulted in TeXMe, which
  has surprisingly become one of my most popular projects, receiving
  millions of hits in some months according to the CDN statistics.
&lt;/p&gt;
    &lt;p&gt;Another example is Muboard, which is a bit like an interactive mathematics chalkboard. I built this when I was hosting an analytic number theory book club and I needed a way to type LaTeX snippets live on screen and see them immediately rendered. That made me wonder: what if I took TeXMe, made it interactive and gave it a chalkboard look-and-feel? That led to Muboard.&lt;/p&gt;
    &lt;p&gt;So we can see that sharing mathematical notes and snippets has been a recurring theme in several of my projects. But that is only a small fraction of my interests. I have a wide variety of interests in computing. I also engage in random explorations, like writing IRC clients (NIMB, Tzero), ray tracing (POV-Ray, Java ray tracer), writing Emacs guides (Emacs4CL, Emfy), developing small single-file HTML games (Andromeda Invaders, Guess My RGB), purely recreational programming (FXYT, may4.fs, self-printing machine code, prime number grid explorer) and so on. The list goes on. When it comes to hobby computing, I don't think I can pick just one domain and say it interests me the most. I have a lot of interests.&lt;/p&gt;
    &lt;p&gt;What is computing, to you?&lt;/p&gt;
    &lt;p&gt;Computing, to me, covers a wide range of activities: programming a computer, using a computer, understanding how it works, even building one. For example, I once built a tiny 16-bit CPU along with a small main memory that could hold only eight 16-bit instructions, using VHDL and a Xilinx CPLD kit. The design was based on the Mano CPU introduced in the book Computer System Architecture (3rd ed.) by M. Morris Mano. It was incredibly fun to enter instructions into the main memory, one at a time, by pushing DIP switches up and down and then watch the CPU I had built execute an entire program. For someone like me, who usually works with software at higher levels of abstraction, that was a thrilling experience!&lt;/p&gt;
    &lt;p&gt;Beyond such experiments, computing also includes more practical and concrete activities, such as installing and using my favourite Linux distribution (Debian), writing software tools in languages like Common Lisp, Emacs Lisp, Python and the shell command language or customising my Emacs environment to automate repetitive tasks.&lt;/p&gt;
    &lt;p&gt;To me, computing also includes the abstract stuff like spending time with abstract algebra and number theory and getting a deeper understanding of the results pertaining to groups, rings and fields, as well as numerous number-theoretic results. Browsing the On-Line Encyclopedia of Integer Sequences (OEIS), writing small programs to explore interesting sequences or just thinking about them is computing too. I think many of the interesting results in computer science have deep mathematical foundations. I believe much of computer science is really discrete mathematics in action.&lt;/p&gt;
    &lt;p&gt;And if we dive all the way down from the CPU to the level of transistors, we encounter continuous mathematics as well, with non-linear voltage-current relationships and analogue behaviour that make digital computing possible. It is fascinating how, as a relatively new species on this planet, we have managed to take sand and find a way to use continuous voltages and currents in electronic circuits built with silicon and convert them into the discrete operations of digital logic. We have machines that can simulate themselves!&lt;/p&gt;
    &lt;p&gt;To me, all of this is fun. To study and learn about these things, to think about them, to understand them better and to accomplish useful or amusing results with this knowledge is all part of the fun.&lt;/p&gt;
    &lt;p&gt;How do you view programming vs. domains?&lt;/p&gt;
    &lt;p&gt;I focus more on the domain than the tool. Most of the time it is a problem that catches my attention and then I explore it to understand the domain and arrive at a solution. The problem itself usually points me to one of the tools I already know.&lt;/p&gt;
    &lt;p&gt;For example, if it is about working with text files, I might write an Emacs Lisp function. If it involves checking large sets of numbers rapidly for patterns, I might choose C++ or Rust. But if I want to share interactive visualisations of those patterns with others, I might rewrite the solution in HTML and JavaScript, possibly with the use of the Canvas API, so that I can share the work as a self-contained file that others can execute easily within their web browsers. When I do that, I prefer to keep the HTML neat and readable, rather than bundled or minified, so that people who like to 'View Source' can copy, edit and customise the code themselves to immediately see their changes take effect.&lt;/p&gt;
    &lt;p&gt; Let me share a specific example. While working on a web-based game, I first used &lt;code&gt;CanvasRenderingContext2D&lt;/code&gt;'s &lt;code&gt;fillText()&lt;/code&gt;
  to display text on the game canvas.  However, dissatisfied with the
  text rendering quality, I began looking for IBM PC OEM fonts and
  similar retro fonts online.  After downloading a few font packs, I
  wrote a little Python script to convert them to bitmaps (arrays of
  integers) and then used the bitmaps to draw text on the canvas using
  JavaScript, one cell at a time, to get pixel-perfect results!  These
  tiny Python and JavaScript tools were good enough that I felt
  comfortable sharing them together as a tiny toolkit called
  PCFace.
  This toolkit offers JavaScript bitmap arrays and tiny JavaScript
  rendering functions, so that someone else who wants to display text
  on their game canvas using PC fonts and nothing but plain HTML and
  JavaScript can do so without having to solve the problem from
  scratch!
&lt;/p&gt;
    &lt;p&gt;Has the rate of your making new Emacs functions has diminished over time (as if everything's covered) or do the widening domains lead to more? I'm curious how applicable old functionality is for new problems and how that impacts the APIs!&lt;/p&gt;
    &lt;p&gt;My rate of making new Emacs functions has definitely decreased. There are two reasons. One is that over the years my computing environment has converged into a comfortable, stable setup I am very happy with. The other is that at this stage of life I simply cannot afford the time to endlessly tinker with Emacs as I did in my younger days.&lt;/p&gt;
    &lt;p&gt;More generally, when it comes to APIs, I find that well-designed functionality tends to remain useful even when new problems appear. In Emacs, for example, many of my older functions continue to serve me well because they were written in a composable way. New problems can often be solved with small wrappers or combinations of existing functions. I think APIs that consist of functions that are simple, orthogonal and flexible age well. If each function in an API does one thing and does it well (the Unix philosophy), it will have long-lasting utility.&lt;/p&gt;
    &lt;p&gt;Of course, new domains and problems do require new functions and extensions to an API, but I think it is very important to not give in to the temptation of enhancing the existing functions by making them more complicated with optional parameters, keyword arguments, nested branches and so on. Personally, I have found that it is much better to implement new functions that are small, orthogonal and flexible, each doing one thing and doing it well.&lt;/p&gt;
    &lt;p&gt;What design methods or tips do you have, to increase composability?&lt;/p&gt;
    &lt;p&gt;For me, good design starts with good vocabulary. Clear vocabulary makes abstract notions concrete and gives collaborators a shared language to work with. For example, while working on a network events database many years ago, we collected data minute by minute from network devices. We decided to call each minute of data from a single device a "nugget". So if we had 15 minutes of data from 10 devices, that meant 150 nuggets.&lt;/p&gt;
    &lt;p&gt; Why "nugget"? Because it was shorter and more convenient than repeatedly saying "a minute of data from one device". Why not something less fancy like "chunk"? Because we reserved "chunk" for subdivisions within a nugget. Perhaps there were better choices, but "nugget" was the term we settled on and it quickly became shared terminology between the collaborators. Good terminology naturally carries over into code. With this vocabulary in place, function names like &lt;code&gt;collect_nugget()&lt;/code&gt;,
  &lt;code&gt;open_nugget()&lt;/code&gt;, &lt;code&gt;parse_chunk()&lt;/code&gt;,
  &lt;code&gt;index_chunk()&lt;/code&gt;, &lt;code&gt;skip_chunk()&lt;/code&gt;,
  etc. immediately become meaningful to everyone involved.
&lt;/p&gt;
    &lt;p&gt;Thinking about the vocabulary also ensures that we are thinking about the data, concepts and notions we are working with in a deliberate manner and that kind of thinking also helps when we design the architecture of software.&lt;/p&gt;
    &lt;p&gt;Too often I see collaborators on software projects jump straight into writing functions that take some input and produce some desired effect, with variable names and function names decided on the fly. To me, this feels backwards. I prefer the opposite approach. Define the terms first and let the code follow from them.&lt;/p&gt;
    &lt;p&gt;I also prefer developing software in a layered manner, where complex functionality is built from simpler, well-named building blocks. It is especially important to avoid layer violations, where one complex function invokes another complex function. That creates tight coupling between two complex functions. If one function changes in the future, we have to reason carefully about how it affects the other. Since both are already complex, the cognitive burden is high. A better approach, I think, is to identify the common functionality they share and factor that out into smaller, simpler functions.&lt;/p&gt;
    &lt;p&gt;To summarise, I like to develop software with a clear vocabulary, consistent use of that vocabulary, a layered design where complex functions are built from simpler ones and by avoiding layer violations. I am sure none of this is new to the Lobsters community. Some of these ideas also occur in domain-driven design (DDD). DDD defines the term ubiquitous language to mean, "A language structured around the domain model and used by all team members within a bounded context to connect all the activities of the team with the software." If I could call this approach of software development something, I would simply call it "vocabulary-driven development" (VDD), though of course DDD is the more comprehensive concept.&lt;/p&gt;
    &lt;p&gt;Like I said, none of this is likely new to the Lobsters community. In particular, I suspect Forth programmers would find it too obvious. In Forth, it is very difficult to begin with a long, poorly thought-out monolithic word and then break it down into smaller ones later. The stack effects quickly become too hard to track mentally with that approach. The only viable way to develop software in Forth is to start with a small set of words that represent the important notions of the problem domain, test them immediately and then compose higher-level words from the lower-level ones. Forth naturally encourages a layered style of development, where the programmer thinks carefully about the domain, invents vocabulary and expresses complex ideas in terms of simpler ones, almost in a mathematical fashion. In my experience, this kind of deliberate design produces software that remains easy to understand and reason about even years after it was written.&lt;/p&gt;
    &lt;p&gt;Not enhancing existing functions but adding new small ones seems quite lovely, but how do you come back to such a codebase later with many tiny functions? At points, I've advocated for very large functions, particularly traumatized by Java-esque 1000 functions in 1000 files approaches. When you had time, would you often rearchitecture the conceptual space of all of those functions?&lt;/p&gt;
    &lt;p&gt;The famous quote from Alan J. Perlis comes to mind:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It is better to have 100 functions operate on one data structure than 10 functions on 10 data structures.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Personally, I enjoy working with a codebase that has thousands of functions, provided most of them are small, well-scoped and do one thing well. That said, I am not dogmatically opposed to large functions. It is always a matter of taste and judgement. Sometimes one large, cohesive function is clearer than a pile of tiny ones.&lt;/p&gt;
    &lt;p&gt; For example, when I worked on parser generators, I often found that lexers and finite state machines benefited from a single top-level function containing the full tokenisation logic or the full state transition logic in one place. That function could call smaller helpers for specific tasks, but we still need the overall &lt;code&gt;switch&lt;/code&gt;-&lt;code&gt;case&lt;/code&gt; or
  &lt;code&gt;if&lt;/code&gt;-&lt;code&gt;else&lt;/code&gt; or &lt;code&gt;cond&lt;/code&gt; ladder
  somewhere.  I think trying to split that ladder into smaller
  functions would only make the code harder to follow.
&lt;/p&gt;
    &lt;p&gt;So while I lean towards small, composable functions, the real goal is to strike a balance that keeps code maintainable in the long run. Each function should be as small as it can reasonably be and no smaller.&lt;/p&gt;
    &lt;p&gt;Like you, I program as a tool to explore domains. Which do you know the most about?&lt;/p&gt;
    &lt;p&gt;For me too, the appeal of computer programming lies especially in how it lets me explore different domains. There are two kinds of domains in which I think I have gained good expertise. The first comes from years of developing software for businesses, which has included solving problems such as network events parsing, indexing and querying, packet decoding, developing parser generators, database session management and TLS certificate lifecycle management. The second comes from areas I pursue purely out of curiosity or for hobby computing. This is the kind I am going to focus on in our conversation.&lt;/p&gt;
    &lt;p&gt;Although computing and software are serious business today, for me, as for many others, computing is also a hobby.&lt;/p&gt;
    &lt;p&gt;Personal hobby projects often lead me down various rabbit holes and I end up learning new domains along the way. For example, although I am not a web developer, I learnt to build small, interactive single-page tools in plain HTML, CSS and JavaScript simply because I needed them for my hobby projects over and over again. An early example is QuickQWERTY, which I built to teach myself and my friends touch-typing on QWERTY keyboards. Another example is CFRS[], which I created because I wanted to make a total (non-Turing complete) drawing language that has turtle graphics like Logo but is absolutely minimal like P‚Ä≤‚Ä≤.&lt;/p&gt;
    &lt;p&gt;You use double spaces after periods which I'd only experienced from people who learned touch typing on typewriters, unexpected!&lt;/p&gt;
    &lt;p&gt;Yes, I do separate sentences by double spaces. It is interesting that you noticed this.&lt;/p&gt;
    &lt;p&gt;I once briefly learnt touch typing on typewriters as a kid, but those lessons did not stick with me. It was much later, when I used a Java applet-based touch typing tutor that I found online about two decades ago, that the lessons really stayed with me. Surprisingly, that application taught me to type with a single space between sentences. By the way, I disliked installing Java plugins into the web browser, so I wrote QuickQWERTY as a similar touch typing tutor in plain HTML and JavaScript for myself and my friends.&lt;/p&gt;
    &lt;p&gt; I learnt to use double spaces between sentences first with Vim and then later again with Emacs. For example, in Vim, the &lt;code&gt;joinspaces&lt;/code&gt; option is on by default, so when we join
  sentences with the normal mode command &lt;code&gt;J&lt;/code&gt; or format
  paragraphs with &lt;code&gt;gqap&lt;/code&gt;, Vim inserts two spaces after full
  stops.  We need to disable that behaviour with &lt;code&gt;:set
  nojoinspaces&lt;/code&gt; if we want single spacing.
&lt;/p&gt;
    &lt;p&gt; It is similar in Emacs. In Emacs, the &lt;code&gt;delete-indentation&lt;/code&gt; command (&lt;code&gt;M-^&lt;/code&gt;) and
  the &lt;code&gt;fill-paragraph&lt;/code&gt; command (&lt;code&gt;M-q&lt;/code&gt;) both
  insert two spaces between sentences by default.  Single spacing can
  be enabled with &lt;code&gt;(setq sentence-end-double-space nil)&lt;/code&gt;.
&lt;/p&gt;
    &lt;p&gt;Incidentally, I spend a good portion of the README for my Emacs quick-start DIY kit named Emfy discussing sentence spacing conventions under the section Single Space for Sentence Spacing. There I explain how to configure Emacs to use single spaces, although I use double spaces myself. That's because many new Emacs users prefer single spacing.&lt;/p&gt;
    &lt;p&gt;The defaults in Vim and Emacs made me adopt double spacing. The double spacing convention is also widespread across open source software. If we look at the Vim help pages, Emacs built-in documentation or the Unix and Linux man pages, double spacing is the norm. Even inline comments in traditional open source projects often use it. For example, see Vim's :h usr_01.txt, Emacs's (info "(emacs) Intro") or the comments in the GCC source code.&lt;/p&gt;
    &lt;p&gt;How do you approach learning a new domain?&lt;/p&gt;
    &lt;p&gt;When I take on a new domain, there is of course a lot of reading involved from articles, books and documentation. But as I read, I constantly try to test what I learn. Whenever I see a claim, I ask myself, "If this claim were wrong, how could I demonstrate it?" Then I design a little experiment, perhaps write a snippet of code or run a command or work through a concrete example, with the goal of checking the claim in practice.&lt;/p&gt;
    &lt;p&gt;Now I am not genuinely hoping to prove a claim wrong. It is just a way to engage with the material. To illustrate, let me share an extremely simple and generic example without going into any particular domain. Suppose I learn that Boolean operations in Python short-circuit. I might write out several experimental snippets like the following:&lt;/p&gt;
    &lt;code&gt;def t(): print('t'); return True
def f(): print('f'); return False
f() or t() or f()
&lt;/code&gt;
    &lt;p&gt; And then confirm that the results do indeed confirm short-circuit evaluation (&lt;code&gt;f&lt;/code&gt; followed by &lt;code&gt;t&lt;/code&gt; in this case).
&lt;/p&gt;
    &lt;p&gt;At this point, one could say, "Well, you just confirmed what the documentation already told you." And that's true. But for me, the value lies in trying to test it for myself. Even if the claim holds, the act of checking forces me to see the idea in action. That not only reinforces the concept but also helps me build a much deeper intuition for it.&lt;/p&gt;
    &lt;p&gt;Sometimes these experiments also expose gaps in my own understanding. Suppose I didn't properly know what "short-circuit" means. Then the results might contradict my expectations. That contradiction would push me to correct my misconception and that's where the real learning happens.&lt;/p&gt;
    &lt;p&gt; Occasionally, this process even uncovers subtleties I didn't expect. For example, while learning socket programming, I discovered that a client can successfully receive data using &lt;code&gt;recv()&lt;/code&gt; even
  after calling &lt;code&gt;shutdown()&lt;/code&gt;, contrary to what I had first
  inferred from the specifications.  See my Stack Overflow post
  Why can recv()
  receive messages after the client has invoked shutdown()? for
  more details if you are curious.
&lt;/p&gt;
    &lt;p&gt;Now this method cannot always be applied, especially if it is very expensive or unwieldy to do so. For example, if I am learning something in the finance domain, it is not always possible to perform an actual transaction. One can sometimes use simulation software, mock environments or sandbox systems to explore ideas safely. Still, it is worth noting that this method has its limitations.&lt;/p&gt;
    &lt;p&gt;In mathematics, though, I find this method highly effective. When I study a new branch of mathematics, I try to come up with examples and counterexamples to test what I am learning. Often, failing to find a counterexample helps me appreciate more deeply why a claim holds and why no counterexamples exist.&lt;/p&gt;
    &lt;p&gt;Do you have trouble not getting distracted with so much on your plate? I'm curious how you balance the time commitments of everything!&lt;/p&gt;
    &lt;p&gt;Indeed, it is very easy to get distracted. One thing that has helped over the years is the increase in responsibilities in other areas of my life. These days I also spend some of my free time studying mathematics textbooks. With growing responsibilities and the time I devote to mathematics, I now get at most a few hours each week for hobby computing. This automatically narrows down my options. I can explore perhaps one or at most two ideas in a month and that constraint makes me very deliberate about choosing my pursuits.&lt;/p&gt;
    &lt;p&gt;Many of the explorations do not evolve into something solid that I can share. They remain as little experimental code snippets or notes archived in a private repository. But once in a while, an exploration grows into something concrete and feels worth sharing on the Web. That becomes a short-term hobby project. I might work on it over a weekend if it is small or for a few weeks if it is more complex. When that happens, the goal of sharing the project helps me focus.&lt;/p&gt;
    &lt;p&gt;I try not to worry too much about making time. After all, this is just a hobby. Other areas of my life have higher priority. I also want to devote a good portion of my free time to learning more mathematics, which is another hobby I am passionate about. Whatever little spare time remains after attending to the higher-priority aspects of my life goes into my computing projects, usually a couple of hours a week, most of it on weekends.&lt;/p&gt;
    &lt;p&gt;How does blogging mix in? What's the development like of a single piece of curiosity through wrestling with the domain, learning and sharing it etc.?&lt;/p&gt;
    &lt;p&gt;Maintaining my personal website is another aspect of computing that I find very enjoyable. My website began as a loose collection of pages on a LAN site during my university days. Since then I have been adding pages to it to write about various topics that I find interesting. It acquired its blog shape and form much later when blogging became fashionable.&lt;/p&gt;
    &lt;p&gt;I usually write a new blog post when I feel like there is some piece of knowledge or some exploration that I want to archive in a persistent format. Now what the development of a post looks like depends very much on the post. So let me share two opposite examples to describe what the development of a single piece looks like.&lt;/p&gt;
    &lt;p&gt;One of my most frequently visited posts is Lisp in Vim. It started when I was hosting a Common Lisp programming club for beginners. Although I have always used Emacs and SLIME for Common Lisp programming myself, many in the club used Vim, so I decided to write a short guide on setting up something SLIME-like there. As a former long-time Vim user myself, I wanted to make the Lisp journey easier for Vim users too. I thought it would be a 30-minute exercise where I write up a README that explains how to install Slimv and how to set it up in Vim. But then I discovered a newer plugin called Vlime that also offered SLIME-like features in Vim! That detail sent me down a very deep rabbit hole. Now I needed to know how the two packages were different, what their strengths and weaknesses were, how routine operations were performed in both and so on. What was meant to be a short note turned into a nearly 10,000-word article. As I was comparing the two SLIME-like packages for Vim, I also found a few bugs in Slimv and contributed fixes for them (#87, #88, #89, #90). Writing this blog post turned into a month-long project!&lt;/p&gt;
    &lt;p&gt;At the opposite extreme is a post like Elliptical Python Programming. I stumbled upon Python's Ellipsis while reviewing someone's code. It immediately caught my attention. I wondered if, combined with some standard obfuscation techniques, one could write arbitrary Python programs that looked almost like Morse code. A few minutes of experimentation showed that a genuinely Morse code-like appearance was not possible, but something close could be achieved. So I wrote what I hope is a humorous post demonstrating that arbitrary Python programs can be written using a very restricted set of symbols, one of which is the ellipsis. It took me less than an hour to write this post. The final result doesn't look quite like Morse code as I had imagined, but it is quite amusing nevertheless!&lt;/p&gt;
    &lt;p&gt;What draws you to post and read online forums? How do you balance or allot time for reading technical articles, blogs etc.?&lt;/p&gt;
    &lt;p&gt;The exchange of ideas! Just as I enjoy sharing my own computing-related thoughts, ideas and projects, I also find joy in reading what others have to share.&lt;/p&gt;
    &lt;p&gt;Other areas of my life take precedence over hobby projects and hobby projects take precedence over technical forums.&lt;/p&gt;
    &lt;p&gt;After I've given time to the higher-priority parts of my life and to my own technical explorations, I use whatever spare time remains to read articles, follow technical discussions and occasionally add comments.&lt;/p&gt;
    &lt;p&gt;When you decided to stop with MathB due to moderation burdens, I offered to take over/help and you mentioned others had too. Did anyone end up forking it, to your knowledge?&lt;/p&gt;
    &lt;p&gt;I first thought of shutting down the MathB-based pastebin website in November 2019. The website had been running for seven years at that time. When I announced my thoughts to the IRC communities that would be affected, I received a lot of support and encouragement. A few members even volunteered to help me out with moderation. That support and encouragement kept me going for another six years. However, the volunteers eventually became busy with their own lives and moved on. After all, moderating user content for an open pastebin that anyone in the world can post to is a thankless and tiring activity. So most of the moderation activity fell back on me. Finally, in February 2025, I realised that I no longer want to spend time on this kind of work.&lt;/p&gt;
    &lt;p&gt;I developed MathB with a lot of passion for myself and my friends. I had no idea at the time that this little project would keep a corner of my mind occupied even during weekends and holidays. There was always a nagging worry. What if someone posted content that triggered compliance concerns and my server was taken offline while I was away? I no longer wanted that kind of burden in my life. So I finally decided to shut it down. I've written more about this in MathB.in Is Shutting Down.&lt;/p&gt;
    &lt;p&gt;To my knowledge, no one has forked it, but others have developed alternatives. Further, the Archive Team has archived all posts from the now-defunct MathB-based website. A member of the Archive Team reached out to me over IRC and we worked together for about a week to get everything successfully archived.&lt;/p&gt;
    &lt;p&gt;What're your favorite math textbooks?&lt;/p&gt;
    &lt;p&gt;I have several favourite mathematics books, but let me share three I remember especially fondly.&lt;/p&gt;
    &lt;p&gt;The first is Advanced Engineering Mathematics by Erwin Kreyszig. I don't often see this book recommended online, but for me it played a major role in broadening my horizons. I think I studied the 8th edition back in the early 2000s. It is a hefty book with over a thousand pages and I remember reading it cover to cover, solving every exercise problem along the way. It gave me a solid foundation in routine areas like differential equations, linear algebra, vector calculus and complex analysis. It also introduced me to Fourier transforms and Laplace transforms, which I found fascinating.&lt;/p&gt;
    &lt;p&gt;Of course, the Fourier transform has a wide range of applications in signal processing, communications, spectroscopy and more. But I want to focus on the fun and playful part. In the early 2000s, I was also learning to play the piano as a hobby. I used to record my amateur music compositions with Audacity by connecting my digital piano to my laptop with a line-in cable. It was great fun to plot the spectrum of my music on Audacity, apply high-pass and low-pass filters and observe how the Fourier transform of the audio changed and then hear the effect on the music. That kind of hands-on tinkering made Fourier analysis intuitive for me and I highly recommend it to anyone who enjoys both music and mathematics.&lt;/p&gt;
    &lt;p&gt;The second book is Introduction to Analytic Number Theory by Tom M. Apostol. As a child I was intrigued by the prime number theorem but lacked the mathematical maturity to understand its proof. Years later, as an adult, I finally taught myself the proof from Apostol's book. It was a fantastic journey that began with simple concepts like the M√É¬∂bius function and Dirichlet products and ended with quite clever contour integrals that proved the theorem. The complex analysis I had learnt from Kreyszig turned out to be crucial for understanding those integrals. Along the way I gained a deeper understanding of the Riemann zeta function \( \zeta(s). \) The book discusses zero-free regions where \( \zeta(s) \) does not vanish, which I found especially fascinating. Results like \( \zeta(-1) = -1/12, \) which once seemed mysterious, became obvious after studying this book.&lt;/p&gt;
    &lt;p&gt;The third is Galois Theory by Ian Stewart. It introduced me to field extensions, field homomorphisms and solubility by radicals. I had long known that not all quintic equations are soluble by radicals, but I didn't know why. Stewart's book taught me exactly why. In particular, it demonstrated that the polynomial \( t^5 - 6t + 3 \) over the field of rational numbers is not soluble by radicals. This particular result, although fascinating, is just a small part of a much larger body of work, which is even more remarkable. To arrive at this result, the book takes us through a wonderful journey that includes the theory of polynomial rings, algebraic and transcendental field extensions, impossibility proofs for ruler-and-compass constructions, the Galois correspondence and much more.&lt;/p&gt;
    &lt;p&gt;One of the most rewarding aspects of reading books like these is how they open doors to new knowledge, including things I didn't even know that I didn't know.&lt;/p&gt;
    &lt;p&gt;How does the newer math jell with or inform past or present computing, compared to much older stuff?&lt;/p&gt;
    &lt;p&gt;I don't always think explicitly about how mathematics informs computing, past or present. Often the textbooks I pick feel very challenging to me, so much so that all my energy goes into simply mastering the material. It is arduous but enjoyable. I do it purely for the fun of learning without worrying about applications.&lt;/p&gt;
    &lt;p&gt;Of course, a good portion of pure mathematics probably has no real-world applications. As G. H. Hardy famously wrote in A Mathematician's Apology:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have never done anything 'useful'. No discovery of mine has made or is likely to make, directly or indirectly, for good or ill, the least difference to the amenity of the world.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But there is no denying that some of it does find applications. Were Hardy alive today, he might be disappointed that number theory, his favourite field of "useless" mathematics, is now a crucial part of modern cryptography. Electronic commerce wouldn't likely exist without it.&lt;/p&gt;
    &lt;p&gt;Similarly, it is amusing how something as abstract as abstract algebra finds very concrete applications in coding theory. Concepts such as polynomial rings, finite fields and cosets of subspaces in vector spaces over finite fields play a crucial role in error-correcting codes, without which modern data transmission and storage would not be possible.&lt;/p&gt;
    &lt;p&gt;On a more personal note, some simpler areas of mathematics have been directly useful in my own work. While solving problems for businesses, information entropy, combinatorics and probability theory were crucial when I worked on gesture-based authentication about one and a half decades ago.&lt;/p&gt;
    &lt;p&gt;Similarly, when I was developing Bloom filter-based indexing and querying for a network events database, again, probability theory was crucial in determining the parameters of the Bloom filters (such as the number of hash functions, bits per filter and elements per filter) to ensure that the false positive rate remained below a certain threshold. Subsequent testing with randomly sampled network events confirmed that the observed false positive rate matched the theoretical estimate quite well. It was very satisfying to see probability theory and the real world agreeing so closely.&lt;/p&gt;
    &lt;p&gt;Beyond these specific examples, studying mathematics also influences the way I think about problems. Embarking on journeys like analytic number theory or Galois theory is humbling. There are times when I struggle to understand a small paragraph of the book and it takes me several hours (or even days) to work out the arguments in detail with pen and paper (lots of it) before I really grok them. That experience of grappling with dense reasoning teaches humility and also makes me sceptical of complex, hand-wavy logic in day-to-day programming.&lt;/p&gt;
    &lt;p&gt;Several times I have seen code that bundles too many decisions into one block of logic, where it is not obvious whether it would behave correctly in all circumstances. Explanations may sometimes be offered about why it works for reasonable inputs, but the reasoning is often not watertight. The experience of working through mathematical proofs, writing my own, making mistakes and then correcting them has taught me that if the reasoning for correctness is not clear and rigorous, something could be wrong. In my experience, once such code sees real-world usage, a bug is nearly always found.&lt;/p&gt;
    &lt;p&gt;That's why I usually insist either on simplifying the logic or on demonstrating correctness in a clear, rigorous way. Sometimes this means doing a case-by-case analysis for different types of inputs or conditions and showing that the code behaves correctly in each case. There is also a bit of an art to reducing what seem like numerous or even infinitely many cases to a small, manageable set of cases by spotting structure, such as symmetries, invariants or natural partitions of the input space. Alternatively, one can look for a simpler argument that covers all cases. These are techniques we employ routinely in mathematics and I think that kind of thinking and reasoning is quite valuable in software development too.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080218</guid><pubDate>Fri, 28 Nov 2025 16:42:46 +0000</pubDate></item><item><title>Airloom ‚Äì 3D Flight Tracker</title><link>https://objectiveunclear.com/airloom.html</link><description>&lt;doc fingerprint="77ed4fa771de6491"&gt;
  &lt;main&gt;
    &lt;p&gt;W/A/S/D - Move forward/left/back/right&lt;/p&gt;
    &lt;p&gt;Space - Move up&lt;/p&gt;
    &lt;p&gt;Shift - Move down&lt;/p&gt;
    &lt;p&gt;Mouse - Look around&lt;/p&gt;
    &lt;p&gt;Click anywhere to start flying&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080289</guid><pubDate>Fri, 28 Nov 2025 16:49:12 +0000</pubDate></item><item><title>So you wanna build a local RAG?</title><link>https://blog.yakkomajuri.com/blog/local-rag</link><description>&lt;doc fingerprint="fb5fccd27d06c5d6"&gt;
  &lt;main&gt;
    &lt;p&gt;When we launched Skald, we wanted it to not only be self-hostable, but also for one to be able to run it without sending any data to third-parties.&lt;/p&gt;
    &lt;p&gt;With LLMs getting better and better, privacy-sensitive organizations shouldn't have to choose between being left behind by not accessing frontier models and doing away with their committment (or legal requirement) for data privacy.&lt;/p&gt;
    &lt;p&gt;So here's what we did to support this use case and also some benchmarks comparing performance when using proprietary APIs vs self-hosted open-source tech.&lt;/p&gt;
    &lt;head rend="h2"&gt;RAG components and their OSS alternatives&lt;/head&gt;
    &lt;p&gt;A basic RAG usually has the following core components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A vector database&lt;/item&gt;
      &lt;item&gt;A vector embeddings model&lt;/item&gt;
      &lt;item&gt;An LLM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And most times it also has these as well:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A reranker&lt;/item&gt;
      &lt;item&gt;Document parsing (for PDFs, PowerPoints, etc)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What that means is that when you're looking to build a fully local RAG setup, you'll need to substitute whatever SaaS providers you're using for a local option for each of those components.&lt;/p&gt;
    &lt;p&gt;Here's a table with some examples of what we might use in a scenario where we can use third-party Cloud services and one where we can't:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Proprietary Options&lt;/cell&gt;
        &lt;cell role="head"&gt;Open-Source Options&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Vector Database&lt;/cell&gt;
        &lt;cell&gt;Pinecone, Turbopuffer, Weaviate Cloud, Qdrant Cloud&lt;/cell&gt;
        &lt;cell&gt;Qdrant, Weaviate, Postgres with pgvector&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Vector Embeddings Provider&lt;/cell&gt;
        &lt;cell&gt;OpenAI, Cohere, Voyage&lt;/cell&gt;
        &lt;cell&gt;Sentence Transformers, BGE, E5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LLM&lt;/cell&gt;
        &lt;cell&gt;GPT, Claude, Gemini&lt;/cell&gt;
        &lt;cell&gt;Llama, Mistral, GPT-OSS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reranker&lt;/cell&gt;
        &lt;cell&gt;Cohere, Voyage&lt;/cell&gt;
        &lt;cell&gt;BGE Reranker, Sentence Transformers Cross-Encoder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Document Parsing&lt;/cell&gt;
        &lt;cell&gt;Reducto, Datalab&lt;/cell&gt;
        &lt;cell&gt;Docling&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;Do note that running something locally does not mean it needs to be open-source, as one could pay for a license to self-host proprietary software. But at Skald our goal was to use fully open-source tech, which is what I'll be convering here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The table above is far from covering all available options on both columns, but basically it gives you an indication of what to research into in order to pick a tool that works for you.&lt;/p&gt;
    &lt;p&gt;As with anything, what works for you will greatly depend on your use case. And you need to be prepared to run a few more services than you're used to if you've just been calling APIs.&lt;/p&gt;
    &lt;p&gt;For our local stack, we went with the easiest setup for now to get it working (and it does! see writeup on this lower down) but will be running benchmarks on all other options to determine the best possible setup.&lt;/p&gt;
    &lt;p&gt;This is what we have today:&lt;/p&gt;
    &lt;p&gt;Vector DB: Postgres + pgvector. We already use Postgres and didn't want to bundle another service into our stack, but this is controversial and we will be running benchmarks to make a better informed decision here. Note that pgvector will serve a lot of use cases well all the way up to hundreds of thousands of documents, though.&lt;/p&gt;
    &lt;p&gt;Vector embeddings: Users can configure this in Skald and we use Sentence Transformers (all-MiniLM-L6-v2) as our default (solid all-around performer for speed and retrieval, English-only). I also ran Skald with bge-m3 (larger, multi-language) and share the results later in this post.&lt;/p&gt;
    &lt;p&gt;LLM: We don't even bundle a default with Skald and it's up to the users to run and manage this. I tested our setup with GPT-OSS 20B on EC2 (results shown below).&lt;/p&gt;
    &lt;p&gt;Reranker: Users can also configure this in Skald, and the default is the Sentence Transformers cross encoder (solid, English-only). I've also used bge-reranker-v2-m3 and mmarco-mMiniLMv2-L12-H384-v1 which offer multi-lingual support.&lt;/p&gt;
    &lt;p&gt;Document parsing: There isn't much of a question on this one. We're using Docling. It's great. We run it via docling-serve.&lt;/p&gt;
    &lt;head rend="h2"&gt;Does it perform though?&lt;/head&gt;
    &lt;p&gt;So the main goal here was first to get something working then ensure it worked well with our platform and could be easily deployed. From here we'll be running extensive benchmarks and working with our clients to provide a solid setup that both performs well but is also not a nightmare to deploy and manage.&lt;/p&gt;
    &lt;p&gt;From that perspective, this was a great success.&lt;/p&gt;
    &lt;p&gt;Deploying a production instance of Skald with this whole stack took me 8 minutes, and that comes bundled with the vector database (well, Postgres), a reranking and embedding service, and Docling.&lt;/p&gt;
    &lt;p&gt;The only thing I needed to run separately was the LLM, which I did via llama.cpp.&lt;/p&gt;
    &lt;p&gt;Having gotten this sorted, I imported all the content from the PostHog website [1] and set up a tiny dataset [2] of questions and expected answers inside of Skald, then used our Experiments feature to run the RAG over this dataset.&lt;/p&gt;
    &lt;p&gt;I explicitly kept the topK values really high (100 for the vector search and 50 for post-reranking), as I was mostly testing for accuracy and wanted to see the performance when questions required e.g. aggregating context over 15+ documents.&lt;/p&gt;
    &lt;head&gt;Full config&lt;/head&gt;
    &lt;p&gt;Here are the params configured in the Skald UI for the the experiment.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Config option&lt;/cell&gt;
        &lt;cell role="head"&gt;Selection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extra system prompt&lt;/cell&gt;
        &lt;cell&gt;Be really concise in your answers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Query rewriting&lt;/cell&gt;
        &lt;cell&gt;Off&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Vector search topK&lt;/cell&gt;
        &lt;cell&gt;100&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Vector search distance threshold&lt;/cell&gt;
        &lt;cell&gt;0.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Reranking&lt;/cell&gt;
        &lt;cell&gt;On&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Reranking topK&lt;/cell&gt;
        &lt;cell&gt;50&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;References&lt;/cell&gt;
        &lt;cell&gt;Off&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So without any more delay, here are the results of my not-very-scientific at all benchmark using the experimentation platform inside of Skald.&lt;/p&gt;
    &lt;head rend="h3"&gt;Voyage + Claude&lt;/head&gt;
    &lt;p&gt;This is our default Cloud setup. We use voyage-3-large and rerank-2.5 from Voyage AI as our embedding and reranking models respectively, and we default to Claude Sonnet 3.7 for responses (users can configure the model though).&lt;/p&gt;
    &lt;p&gt;It passed with flying colors.&lt;/p&gt;
    &lt;p&gt;Our LLM-as-a-Judge gave an average score of 9.45 to the responses, and I basically agree with the assessment. All answers were correct, with one missing a few extra bits of context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Voyage + GPT-OSS 20B&lt;/head&gt;
    &lt;p&gt;With the control experiment done, I then moved on to a setup where I kept Voyage as the embeddings provider and reranker, and then used GPT-OSS 20B running on a llama.cpp server on a g5.2xlarge EC2 instance as the LLM.&lt;/p&gt;
    &lt;p&gt;The goal here was to see how well the open-source LLM model itself stacked up against a frontier model accessed via API.&lt;/p&gt;
    &lt;p&gt;And it did great!&lt;/p&gt;
    &lt;p&gt;We don't yet support LLM-as-a-Judge on fully local deployments, so the only score we have here is mine. I scored the answers an average of 9.18 and they were all correct, with two of them just missing a few bits of information or highlighting less relevant information from the context.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fully local + GPT-OSS 20B&lt;/head&gt;
    &lt;p&gt;Lastly, it was time for the moment of truth: running a fully local setup.&lt;/p&gt;
    &lt;p&gt;For this I ran two tests:&lt;/p&gt;
    &lt;p&gt;1. Default sentence transformers embedding and reranking models&lt;/p&gt;
    &lt;p&gt;The most popular open-source models are all-MiniLM-L6-v2 for embeddings and ms-marco-MiniLM-L6-v2 as the reranker, so I used those for my first benchmark.&lt;/p&gt;
    &lt;p&gt;Here the average score was 7.10. Not bad, but definitely not great. However, when we dig into the results, we can get a better understanding of how this setup fails.&lt;/p&gt;
    &lt;p&gt;Basically, it got all point queries right, which are questions where the answer is somewhere in the mess of documents, but can be found from one specific place.&lt;/p&gt;
    &lt;p&gt;Where it failed was:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-english query: The embeddings model and the reranker are English-based, so my question in Portuguese obviously got no answer&lt;/item&gt;
      &lt;item&gt;An ambiguous question with very little context ("what's ch")&lt;/item&gt;
      &lt;item&gt;Aggregating information from multiple documents/chunks e.g. it only found 5 out of PostHog's 7 funding rounds, and only a subset of the PostHog competitors that offer session replay (as mentioned in the source data)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In my view, this is good news. That means that the default options will go a long way and should give you very good performance if your use case is only doing point queries in English. The other great thing is that these models are also fast.&lt;/p&gt;
    &lt;p&gt;Now, if you need to handle ambiguity better, or handle questions in other languages, then this setup is simply not for you.&lt;/p&gt;
    &lt;p&gt;2. Multi-lingual models&lt;/p&gt;
    &lt;p&gt;The next test I did used bge-m3 as the embeddings model and mmarco-mMiniLMv2-L12-H384-v1 as the reranker. The embeddings model is supposedly much better than the one used in the previous test and is also multi-lingual. The reranker on the other hand uses the same cross-encoder from the previous test as the base model but also adds multi-lingual support. The more standard option here would have been the much more popular bge-reranker-v2-m3 model, but I found it to be much slower. I intend to tweak my setup and test it again, however.&lt;/p&gt;
    &lt;p&gt;Anyway, onto the results! I scored it 8.63 on average, which is very good. There were no complete failures, and it handled the question in Portuguese well.&lt;/p&gt;
    &lt;p&gt;The mistakes it made were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This new setup also did not do the best job at aggregating information, missing 2 of PostHog's funding rounds, and a couple of its session replay competitors&lt;/item&gt;
      &lt;item&gt;It also answered a question correctly, but added incorrect additional context after it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So overall it performed quite well. Again what we what saw was the main problem is when the context needed for the response is scattered across multiple documents. There are various techniques to help with this and we'll be trialing some soon! They haven't been needed on the Cloud version because better models save you from having to add complexity for minimal performance gains, but as we're focused on building a really solid setup for local deploys, we'll be looking into this more and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Now what?&lt;/head&gt;
    &lt;p&gt;I hope this writeup has provided you with at least some insight and context into building a local RAG, and also the fact that it does work, it can serve a lot of use cases, and that the tendency is for this setup to get better and better as a) models improve b) we get more open-source models across the board, with both being things that we seem to be trending towards.&lt;/p&gt;
    &lt;p&gt;As for us at Skald, we intend to polish this setup further in order to serve even more use cases really well, as well as intend to soon be publishing more legitimate benchmarks for models in the open-source space, from LLMs to rerankers.&lt;/p&gt;
    &lt;p&gt;If you're a company that needs to run AI tooling in air-gapped infrastructure, let's chat -- feel free to email me at yakko [at] useskald [dot] com.&lt;/p&gt;
    &lt;p&gt;Lastly, if you want to get involved, feel free to chat to us over on our GitHub repo (MIT-licensed) or catch us on Slack.&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;[1] I used the PostHog website here because the website content is MIT-licensed (yes, wild) and readily-available as markdown on GitHub and having worked there I know a lot of answers off the top of my head making it a great dataset of ~2000 documents that I know well.&lt;/p&gt;
    &lt;p&gt;[2] The questions and answers dataset I used for the experiments was the following:&lt;/p&gt;
    &lt;head&gt;Dataset&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Question&lt;/cell&gt;
        &lt;cell role="head"&gt;Expected answer&lt;/cell&gt;
        &lt;cell role="head"&gt;Comments&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;How many raises did PostHog do?&lt;/cell&gt;
        &lt;cell&gt;PostHog has raised money 7 times: it raised $150k from YCombinator, then did a seed round ($3.025M), a series A ($12M), a series B ($15M), a series C ($10M), a series D ($70M), and a series E ($75M).&lt;/cell&gt;
        &lt;cell&gt;Requires aggregating context from at least 7 documents&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;When did group analytics launch?&lt;/cell&gt;
        &lt;cell&gt;December 16, 2021.&lt;/cell&gt;
        &lt;cell&gt;Point query, multiple mentions to "group analytics" in the source docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Why was the sessions page removed?&lt;/cell&gt;
        &lt;cell&gt;The sessions page was removed because it was confusing and limited in functionality. It was replaced by the 'Recordings' tab.&lt;/cell&gt;
        &lt;cell&gt;Point query, multiple mentions to "sessions" in the source docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;What's the difference between a product engineer and other roles?&lt;/cell&gt;
        &lt;cell&gt;Compared to product managers, product engineers focus more on building rather than deep research and planning. When it comes to software engineers, both product and software engineers write code, but software engineers focus on building great software, whereas product engineers focus on building great products.&lt;/cell&gt;
        &lt;cell&gt;Requires aggregating context from multiple docs + there are a ton of mentions of "product engineer" in the source docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;What were the main benefits of YC?&lt;/cell&gt;
        &lt;cell&gt;The main benefits of YC were: Network Access, Investor Reviews, Office Hours, Funding Opportunities, Hiring Resources, Angel Investing Opportunities, Accelerated Growth and Experience, Shift in Self-Perception, Customer Acquisition, Product Market Fit, Ambitious Goal Setting, Access to Thought Leaders, Community Support&lt;/cell&gt;
        &lt;cell&gt;Point query&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;quem foi o primeiro investidor da posthogg?&lt;/cell&gt;
        &lt;cell&gt;O primeiro investidor da PostHog foi o YCombinator.&lt;/cell&gt;
        &lt;cell&gt;Question in Portuguese, with PostHog misspelled&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;what posthog competitors also offer session replays&lt;/cell&gt;
        &lt;cell&gt;LogRocket, Smartlook, FullStory, Microsoft Clarity, Contentsquare, Mouseflow, Heap, Pendo, Hotjar, Glassbox, and Amplitude.&lt;/cell&gt;
        &lt;cell&gt;Requires aggregating content from at least 11 docs (more because I actually missed some in my expected answer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;top tips find client&lt;/cell&gt;
        &lt;cell&gt;1. Leverage your inner circle 2. Join relevant communities 3. Be laser-focused 4. Set achievable goals 5. Frame conversations properly&lt;/cell&gt;
        &lt;cell&gt;Point query, worded weirdly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;what's ch&lt;/cell&gt;
        &lt;cell&gt;CH most likely refers to ClickHouse, a column-oriented OLAP database.&lt;/cell&gt;
        &lt;cell&gt;Really ambiguous. I meant ClickHouse with my question.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;what is mixedpanel&lt;/cell&gt;
        &lt;cell&gt;Mixpanel is a popular product analytics tool that was founded in 2009&lt;/cell&gt;
        &lt;cell&gt;Mixpanel misspelled as Mixedpanel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;how was prpoerty filtering made faster?&lt;/cell&gt;
        &lt;cell&gt;Using materialized columns allowed ClickHouse to skip JSON parsing during queries and made queries with property filtering 25x faster.&lt;/cell&gt;
        &lt;cell&gt;Point query with a typo&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080364</guid><pubDate>Fri, 28 Nov 2025 16:54:56 +0000</pubDate></item><item><title>Apple and Intel Rumored to Partner on Mac Chips</title><link>https://www.macrumors.com/2025/11/28/intel-rumored-to-supply-new-mac-chip/</link><description>&lt;doc fingerprint="a4886974532b1b6c"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Apple and Intel Rumored to Partner on Mac Chips Again in a New Way&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;While all Macs are now powered by Apple's custom-designed chips, a new rumor claims that Apple may rekindle its partnership with Intel, albeit in a new and limited way.&lt;/p&gt;
          &lt;p&gt;&lt;lb/&gt;Apple supply chain analyst Ming-Chi Kuo today said Intel is expected to begin shipping Apple's lowest-end M-series chip as early as mid-2027. &lt;/p&gt;
          &lt;p&gt;Kuo said Apple plans to utilize Intel's 18A process, which is the "earliest available sub-2nm advanced node manufactured in North America."&lt;/p&gt;
          &lt;p&gt;If this rumor proves to be accurate, Intel could supply Apple with M6 or M7 chips for future MacBook Air, iPad Air, and iPad Pro models at a minimum. However, while previous Intel chips for Macs were designed by Intel and based on x86 architecture, M-series chips are designed by Apple and use Arm architecture. Intel would only assist with manufacturing.&lt;/p&gt;
          &lt;p&gt;TSMC would continue to supply the majority of Apple's M-series chips.&lt;/p&gt;
          &lt;p&gt;Kuo said that Apple choosing to have Intel supply its lowest-end M-series chip would appease the Trump administration's desire for "Made in USA" products, and it would also help Apple to diversify its supply chain for manufacturing.&lt;/p&gt;
          &lt;p&gt;Apple began transitioning away from Intel processors in Macs in 2020, and its own M-series chips continue to provide industry-leading performance per watt.&lt;/p&gt;
          &lt;p&gt;Apple previously announced that macOS Tahoe will be the final major macOS release that supports Intel-based Macs with x86 architecture.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Popular Stories&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple's online store is going down for a few hours on a rolling country-by-country basis right now, but do not get your hopes up for new products. Apple takes its online store down for a few hours ahead of Black Friday every year to tease/prepare for its annual gift card offer with the purchase of select products. The store already went down and came back online in Australia and New Zealand, ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple recently teamed up with Japanese fashion brand ISSEY MIYAKE to create the iPhone Pocket, a limited-edition knitted accessory designed to carry an iPhone. However, it is now completely sold out in all countries where it was released. iPhone Pocket became available to order on Apple's online store starting Friday, November 14, in the United States, France, China, Italy, Japan, Singapore, ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple's first foldable iPhone is expected to launch alongside the iPhone 18 Pro models in fall 2026, and it's shaping up to include three standout features that could set it apart from the competition. The book-style foldable will reportedly feature an industry-first 24-megapixel under-display camera built into the inner display, according to a recent JP Morgan equity research report. That...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;We've been focusing on deals on physical products over the past few weeks, but Black Friday is also a great time of year to purchase a streaming membership. Some of the biggest services have great discounts for new and select returning members this week, including Disney+, Hulu, Paramount+, Peacock, and more. Note: MacRumors is an affiliate partner with some of these vendors. When you click a...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;We've been focusing on deals on physical products over the past few weeks, but Black Friday is also a great time of year to purchase a streaming membership. Some of the biggest services have great discounts for new and select returning members this week, including Apple TV, Disney+, Hulu, Paramount+, Peacock, and more. Note: MacRumors is an affiliate partner with some of these vendors. When...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Black Friday is here, and as always this is the best time of the year to shop for great deals, including popular Apple products like AirPods, iPad, Apple Watch, and more. In this article, the majority of the discounts will be found on Amazon. Note: MacRumors is an affiliate partner with some of these vendors. When you click a link and make a purchase, we may receive a small payment, which...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Singapore has ordered Apple to block or filter messages on iMessage that impersonate government agencies, requiring the company to implement new anti-spoofing protections by December as part of efforts to curb rising online scams, the Straits Times reports. Singapore's Ministry of Home Affairs (MHA) said that it had issued an Implementation Directive to Apple under the Online Criminal Harms...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;While all Macs are now powered by Apple's custom-designed chips, a new rumor claims that Apple may rekindle its partnership with Intel, albeit in a new and limited way. Apple supply chain analyst Ming-Chi Kuo today said Intel is expected to begin shipping Apple's lowest-end M-series chip as early as mid-2027. Kuo said Apple plans to utilize Intel's 18A process, which is the "earliest...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080424</guid><pubDate>Fri, 28 Nov 2025 17:00:46 +0000</pubDate></item><item><title>Bringing Sexy Back. Internet surveillance has killed eroticism</title><link>https://lux-magazine.com/article/privacy-eroticism/</link><description>&lt;doc fingerprint="3cc0d15432a41fc2"&gt;
  &lt;main&gt;
    &lt;p&gt;I don‚Äôt remember when I first started noticing that people I knew out in the world had lost their sense of erotic privacy, but I do remember the day it struck me as a phenomenon that had escaped my timeline and entered my real, fleshy life. It was last year, when I was having a conversation with a friend of mine, who, for the record, is five years younger than me (I‚Äôm 31). I told my friend about an erotic encounter I‚Äôd just experienced and very much delighted in, in which I had my hair brushed at the same time by two very beautiful women at the hair salon ‚Äî one was teaching the other how to do it a certain way. When I finished my story, my friend looked at me, horrified.&lt;/p&gt;
    &lt;p&gt;‚ÄúThey had no idea you felt something sexual about them,‚Äù she said. ‚ÄúWhat if they found out? Lowkey, I hate to say this but: you took advantage of them.‚Äù I was shocked. I tried to explain ‚Äî and it felt extremely absurd to explain ‚Äî that this had happened in my body and in my thoughts, which were private to me and which nobody had the right to know about. But they did have the right, my friend argued. She demanded that I apologize to the women for sexualizing them. Offended at having been accused ‚Äî in my view, in extremely bad faith ‚Äî of being some kind of peep-show creep, I tried to argue that I‚Äôd simply responded in a physical way to an unexpected, direct, and involuntary stimulus. Back and forth, back and forth, we fought like this for a while. In fact, it ended the friendship.&lt;/p&gt;
    &lt;p&gt;There were other conversations, too, that suggested to me that conceptions of love and sex have changed fundamentally among people I know. Too many of my friends and acquaintances ‚Äî of varying degrees of ‚Äúonlineness,‚Äù from veteran discourse observers to casual browsers ‚Äî seem to have internalized the internet‚Äôs tendency to reach for the least charitable interpretation of every glancing thought and, as a result, to have pathologized what I would characterize as the normal, internal vagaries of desire.&lt;/p&gt;
    &lt;p&gt;Hence, there was the friend who justified her predilection for being praised in bed as a ‚Äúkink‚Äù inherited through the ‚Äútrauma‚Äù of her father always harping on her because of her grades. There was the friend who felt entitled to posting screenshots of intimate conversations on Twitter after a messy breakup so that she could get a ruling on ‚Äúwho was the crazy one.‚Äù Then there was the friend who bitterly described a man he was dating as a ‚Äúfuckboy‚Äù because he stood him up, claiming that their having enjoyed sex together beforehand was ‚Äúemotionally manipulative.‚Äù When I dug a bit deeper, it turned out the man in question had just gotten out of a seven-year relationship and realized he wasn‚Äôt ready to be sexually intimate, and while he was rude to stand my friend up, it shocked me how quick my friend was to categorize his rightfully hurt feelings as something pathological or sinister in the other person, and that he did this in order to preemptively shield himself from being cast as the villain in what was a multi-party experience. This last friend I asked: ‚ÄúWho are you defending yourself against?‚Äù To which he answered, to my astonishment: ‚ÄúI don‚Äôt know. The world.‚Äù&lt;/p&gt;
    &lt;p&gt;I choose these examples from my personal life because they express sentiments that were once the kind of stuff I encountered only in the messy battlegrounds of Twitter, amid discussions about whether Sabrina Carpenter is being oversexualized, whether kinks are akin to a sexual orientation, whether a woman can truly consent in an age-gap relationship, and whether exposure to sex scenes in movies violates viewer consent. It is quite easy to dismiss these ‚Äúdiscourse wars‚Äù as a ‚Äúpuritanism‚Äù afflicting the young, a reactionary current to be solved with a different, corrective discourse of pro-sex liberation, distributed via those same channels. If only it were so! To me, the reality goes deeper and is bleaker.&lt;/p&gt;
    &lt;p&gt;The fact is that our most intimate interactions with others are now governed by the expectation of surveillance and punishment from an online public. One can never be sure that this public or someone who could potentially expose us to it isn‚Äôt there, always secretly filming, posting, taking notes, ready to pounce the second one does something cringe or problematic (as defined by whom?). To claim that these matters are merely discursive in nature is to ignore the problem. Because love and sex are so intimate and vulnerable, the stakes of punishment are higher, and the fear of it penetrates deeper into the psyche and is harder to rationalize away than, say, fear of pushback from tweeting a divisive political opinion.&lt;/p&gt;
    &lt;p&gt;I should state at this point that this is not an essay about ‚Äúcancel culture going too far,‚Äù a topic which can now be historicized as little more than a rhetorical cudgel wielded successfully by the right to wrest cultural power back from an ascendant progressive liberalism. This was especially true after the prominence of organized campaigns such as #MeToo. #MeToo was smeared by liberals and conservatives alike (united, as they always are, in misogyny) as being inherently punitive in nature, meant to punish men who‚Äôd fallen into a rough patch of bad behavior, or who, perhaps, might not have done anything at all (the falsely accused or the misinterpreted man became the real victim, in this view). #MeToo did make use of the call-out ‚Äî the story shared in a spreadsheet anonymously or in a signed op-ed ‚Äî but the call-outs had a purpose: to end a long-standing and long-permitted norm of sexual abuse within institutions. Underlying this was a discursive practice and a form of solidarity building in which people believed that sharing their stories of trauma en masse could bring about structural change. As someone who participated myself, I too believed in this theory and saw it as necessary, cathartic, and political, and far from vigilante justice.&lt;/p&gt;
    &lt;p&gt;But the pushback against #MeToo reveals a certain peril to storytelling as politics, not only in the retraumatization evident in the practice of revealing one‚Äôs most intimate harms before an infinite online audience, which could always include those listening in bad faith. But also, a discursive market opened up in which trauma became a kind of currency of authenticity, resulting in a doubled exploitation. This idea, while not very nice, lingers in the use of harm as an authoritative form of rhetorical defense. The problem here is not what is said, but how it is used. A friction has since emerged between an awareness of weaponization of harm and emotion and the continued need to express oneself as vulnerably as possible in order to come off as sincere. This friction is unresolved.&lt;/p&gt;
    &lt;p&gt;The organized goals of the #MeToo movement are missing from the new puritanism. I think that the prudish revulsion I‚Äôve seen online and in my own life has as much to do with surveillance as with sex. Punishing strangers for their perceived perversion is a form of compensation for a process that is already completed: the erosion of erotic and emotional privacy through internet-driven surveillance practices, practices we have since turned inward on ourselves. In short, we have become our own panopticons.&lt;/p&gt;
    &lt;p&gt;On the rightmost side of the spectrum, punitive anti-erotic surveillance is very explicit and very real, especially for women. The Andrew Tates of the world and the practitioners of extreme forms of misogyny have no problem with using internet tools and social media websites for mass shaming and explicit harm. Covert filming of sex acts, AI deep fakes, extortion, and revenge porn are all realities one has to contend with when thinking about hooking up or going to public places such as nightclubs and gay bars. This is blackmail at its most explicit and extreme, meant to further solidify a link between sex and fear.&lt;/p&gt;
    &lt;p&gt;But that link between sex and fear is operating in more ‚Äúbenign‚Äù or common modes of internet practice. There is an online culture that thinks nothing of submitting screenshots, notes, videos, and photos with calls for collective judgement. When it became desirable and permissible to transform our own lives into content, it didn‚Äôt take long before a sense of entitlement emerged that extended that transformation to people we know and to strangers. My ex sent me this text, clearly she is the crazy one, right? Look at this dumb/funny/cringe Hinge profile! Look at this note some guy sent me, is this a red flag? Look at this random woman I photographed buying wine, coconut oil, and a long cucumber at the supermarket!&lt;/p&gt;
    &lt;p&gt;I think these kinds of posts sometimes amount to little more than common bullying, but they are on a continuum with a puritan discourse in which intimate questions, practices, and beliefs about queerness, sexuality, gender presentation, and desire are also subjected to days-long piles-on. In both instances, the instinct to submit online strangers to viral discipline is given a faux-radical sheen. It‚Äôs a kind of casual blackmail that warns everyone to conform or be exposed; a way of saying if you don‚Äôt cave to my point of view, redefine yourself in my image of what sexuality is or should be, and (most importantly) apologize to me and the public, I will subject you to my large following and there will be hell to pay. Such unproductive and antisocial behavior is justified as a step toward liberation from predation, misogyny, or any number of other harms. But the punitive mindset we‚Äôve developed towards relationships is indicative of an inability to imagine a future of gendered or sexual relations without subjugation. To couch that in the language of harm reduction and trauma delegitimizes both.&lt;/p&gt;
    &lt;p&gt;There are other ways the politics of surveillance have become a kind of funhouse mirror. It is seen as more and more normal to track one‚Äôs partner through Find My iPhone or an AirTag, even though the potential for abuse of this technology is staggering and obvious. There are all kinds of new products, such as a biometric ring that is allegedly able to tell you whether your partner is cheating, that expand this capability into more and more granular settings. That‚Äôs all before we get into the endless TikToks about ‚Äúwhy I go through my partner‚Äôs text messages.‚Äù That men use these tactics and tools to control women is a known threat. What is astonishing is the lengths to which some women will go to use these same technologies, claiming that they are necessary to prevent harm ‚Äî especially that caused by cheating, which is now seen as some kind of lifelong trauma or permanently damnable offense instead of one of the rather quotidian, if very painful, ways we hurt one another. Each of these surveillance practices operates from a feeling of entitlement and control over other people, their bodies, and what they do.&lt;/p&gt;
    &lt;p&gt;Pundits like to decree sexlessness as a Gen-Z problem, to argue no one is fucking because they are too on their phones. However, it is always too easy to blame the young. It was my generation that failed to instill the social norms necessary to prevent a situation where fear of strangers on the internet has successfully replaced the disciplinary apparatus more commonly held by religious or conservative doctrine. Even when, as in my experience in the salon, I am acting in the privacy of my own body, someone is always there watching, ready to interpret my actions, problematize them so as to share in the same sense of magical thinking, the same insecurities, and to be punished for not being insecure in the same way.&lt;/p&gt;
    &lt;p&gt;It‚Äôs only in retrospect that I‚Äôm able to realize the toll that constant, nagging interaction with my devices and the internet has taken on my thinking life and my sex life. I remember very viscerally when I‚Äôd just come out of the closet as bisexual in 2016. When I embarked on a journey to find the kind of lover I wanted to be, my only experience with the world of queerness was online through memes, articles, and others‚Äô social media presentation of themselves and of politics. Queer sex was not something that could be discovered through sensation, through physical interaction, but was rather a catalog of specific acts and roles one was already expected to know. I was terrified of making some kind of mistake, of being the wrong kind of bisexual, of misrepresenting myself in an offensive way (could I use the term ‚Äúsoft butch‚Äù if I wasn‚Äôt a lesbian?), of being exposed somehow as a fraud. When the time came for me to have sex for the first time, what should have been a joyous occasion was instead burdened with a sense of being watched. I could not let the natural processes of erotic discovery take their course, so caught up was I in judging myself from the perspective of strangers to whom I owed nothing.&lt;/p&gt;
    &lt;p&gt;But it wasn‚Äôt just a matter of queerness, either. When I hooked up with men, I could only perceive of sex the same way, not as situational but as a set of prescribed acts and scenes, many of which I wanted to explore. However, this time I interrogated these urges as being sociogenic in nature and somehow harmful to me, when they were, in fact, private, and I did not, in reality, feel harmed. Because I wanted, at one point in my life, to be tied up and gagged, the disempowering nature of such a want necessitated trying to justify it against invisible accusations with some kind of traumatogenic and immutable quality. Maybe it was because I was raped in college. Maybe I was just inherently submissive. One of the great ironies in the history of sex is that pathologization used to be a way of controlling sexual desire. (All are familiar with the many myths that masturbation would turn one blind.) Now it is a way of exempting oneself, of relinquishing control of one‚Äôs actions so as to absolve them of scrutiny. My little bondage moment couldn‚Äôt be problematic if it couldn‚Äôt be helped. It couldn‚Äôt be subjected to interrogation if there was something I could point to to say ‚Äúit‚Äôs beyond my control, don‚Äôt judge me!‚Äù One day, however, I came to an important revelation: The reality was much simpler. It was a passing phase, a desire that originated with a specific man and lost its charm after I moved on from him. There wasn‚Äôt some deterministic quality in myself that made me like this. My desire was not fixed in nature. My sexual qualities were transient and not inborn. What aroused me was wonderfully, entirely situational.&lt;/p&gt;
    &lt;p&gt;A situational eroticism is what is needed now, in our literalist times. It‚Äôs exhausting, how everything is so readily defined by types, acts, traumas, kinks, fetishes, pathology, and aesthetics. To me, our predilection for determinism is an expected psychological response to excessive surveillance. A situational eroticism decouples sensation from narrative and typology. It allows us to feel without excuse and to relate our feelings to our immediate embodied moment, grounded in a fundamental sense of personal privacy. While it is admirable to try and understand ourselves and important to protect ourselves from harm and investigate critically the ways in which what we want may put us at risk of that harm ‚Äî or at risk of doing harm to others ‚Äî sometimes desires just are, and they are not that way for long. Arousal is a matter of the self, which takes place within the body, a space no one can see into. It is often a mystery, a surprise, a discovery. It can happen at a small scale, say, the frisson of two sets of fingers in one‚Äôs hair at once. It is beautiful, unplanned and does not judge itself because it is an inert sensation, unimbued with premeditated meaning. This should liberate rather than frighten us. Maybe what it means doesn‚Äôt matter. Maybe we don‚Äôt have to justify it even to ourselves.&lt;/p&gt;
    &lt;p&gt;But in order to facilitate a return to situational eroticism, we need to kill the panopticon in our heads. That means first killing the panopticon we‚Äôve built for others. There is no purpose in vindictive or thoughtless exposure. Not everything needs to be subjected to public opinion, not every anecdote is worth sharing, not every debate needs engagement, especially those debates which have no material basis to them, no ask, no funnel for all that energy. We need to stop confusing vigilantism with justice and posting with politics. That does not mean we stop the work that #MeToo started, but that revenge is a weapon best utilized collectively against the enemies of liberation. We need to protect the vulnerable from exploitative technologies and practices, repeatedly denounce their use, and work towards a world without sexual coercion, digital or otherwise.&lt;/p&gt;
    &lt;p&gt;On an individual level, we need to abandon or reshape our relationships with our phones and regain a sense of our own personal and mental privacy. It‚Äôs a matter of existential, metaphysical importance. Only when this decoupling from ourselves and the mediated performance of ourselves is complete, can we begin the process of returning to our own bodies out there, in the world, with no one watching or reading our thoughts except those we want to. The truth is, we are very afraid not of sex, but of exposure. Only when we are unafraid can we begin to let desire flourish. Only when we return to ourselves can we really know what we want.&lt;/p&gt;
    &lt;p&gt;Kate Wagner is the architecture critic at The Nation. Her award-winning cultural writing has been featured in magazines ranging from The Baffler to the New Republic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080473</guid><pubDate>Fri, 28 Nov 2025 17:05:31 +0000</pubDate></item><item><title>C++ Web Server on my custom hobby OS</title><link>https://oshub.org/projects/retros-32/posts/getting-a-webserver-running</link><description>&lt;doc fingerprint="185cfc50ed2437dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Getting a Web Server running on my hobby OS&lt;/head&gt;
    &lt;p&gt;November 27, 2025joexbayerkernel, networking, tcp, http, web&lt;/p&gt;
    &lt;head rend="h1"&gt;Demo&lt;/head&gt;
    &lt;p&gt;After a long break from working on my hobby operating system, I finally got back into it and finished a very important milestone: a working web server.&lt;/p&gt;
    &lt;p&gt;Full Github repo: https://github.com/joexbayer/RetrOS-32&lt;/p&gt;
    &lt;p&gt;Networking was always integral to my hobby project. The first goal was getting the basic networking stack working: Ethernet, IP, ARP, UDP, TCP, DHCP and DNS. Besides TCP this was rather straightforward, but when moving onto HTTP things broke.&lt;/p&gt;
    &lt;p&gt;Networking stack code&lt;/p&gt;
    &lt;p&gt;This led to my first break from the project, but also left a nagging thought in my mind, wanting to make it work. I finally sat down and started debugging.&lt;/p&gt;
    &lt;p&gt;I eventually found the culprit after hours of dissecting my own code, the problem was a broken implementation of the terminal buffer, overwriting a lock in another process‚Ä¶ fun. Additionally, the E1000 network driver did not correctly handle incoming packets, which I finally got working by handling bursts of packets.&lt;/p&gt;
    &lt;head rend="h1"&gt;Performance and hardening&lt;/head&gt;
    &lt;p&gt;After getting an HTML page returned from the web engine I started noticing lots of performance errors and hangs from TCP, mainly because quickly refreshing the browser led to a spam of RST packets which were not handled correctly.&lt;/p&gt;
    &lt;p&gt;After a few hours of tinkering I finally got the RST packets working and the network stack is now able to handle a packet spam from the browser.&lt;/p&gt;
    &lt;head rend="h1"&gt;The HTTP Engine&lt;/head&gt;
    &lt;p&gt;Next step was actually implementing a HTTP engine, parsing the requests from the user. Before this engine I simply returned a static HTTP response no matter the actual request.&lt;lb/&gt;Keeping with the spirit of this hobby OS I want to write everything from scratch, luckily I already had implemented a pretty complete HTTP parser for my other project c-web-modules. So I extracted the HTTP parser as a standalone library and ported it to my OS.&lt;/p&gt;
    &lt;p&gt;Keeping with the spirit of this hobby OS I want to write everything from scratch, luckily I already had implemented a pretty complete HTTP parser for my other project c-web-modules. So I extracted the HTTP parser as a standalone library and ported it to my OS.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Web Engine.&lt;/head&gt;
    &lt;p&gt;After the HTTP engine was done I moved onto the web engine, focusing on something small, rather than big and fancy. Mainly routing was important and adding route handlers. Allowing the user to specify a route, method and lambda function handler.&lt;/p&gt;
    &lt;quote&gt;/* Simple routing */ engine.get("/", [](const http::Request&amp;amp; req, http::Response&amp;amp; res) { (void)req; res.setBody("hello world!"); .... });&lt;/quote&gt;
    &lt;p&gt;It‚Äôs a tiny example, but it mirrors how a lot of modern C++ and web frameworks think about routing: match a path + method, call a handler, build a response.&lt;/p&gt;
    &lt;quote&gt;#lang:plaintext [ Browser ] | v [ Web Server (userspace): WebEngine | HTTPEngine | FileRepository ] | v [ Network stack: TCP/UDP | IP | ARP | DHCP | DNS | Ethernet(E1000) ]&lt;/quote&gt;
    &lt;head rend="h1"&gt;The Web Server.&lt;/head&gt;
    &lt;p&gt;The last step was updating the userspace program with the new HTTP and Web engine. Finally I added a way to serve files using a FileRepository which supports caching. Now I can edit the files inside the operating system and then serve them with the web server.&lt;/p&gt;
    &lt;quote&gt;#lang:cpp WebEngine webEngine(80, 16); web::FileRepository fileRepo; /* Simple static pages */ webEngine.get("/home", [&amp;amp;fileRepo](const http::Request&amp;amp; req, http::Response&amp;amp; res) { (void)req; res.sendFile(fileRepo, "/web/index.htm"); }); webEngine.get("/about", [&amp;amp;fileRepo](const http::Request&amp;amp; req, http::Response&amp;amp; res) { (void)req; res.sendFile(fileRepo, "/web/about.htm"); }); webEngine.get("/status", [&amp;amp;fileRepo](const http::Request&amp;amp; req, http::Response&amp;amp; res) { (void)req; res.sendFile(fileRepo, "/web/status.htm"); }); webEngine.run();&lt;/quote&gt;
    &lt;head rend="h1"&gt;Next steps&lt;/head&gt;
    &lt;p&gt;The next thing on the TODO list will be to add a more fancy UI for the webserver and a way to close it gracefully.&lt;/p&gt;
    &lt;p&gt;(Graceful shutdown is one of those ‚Äúboring‚Äù features you don‚Äôt miss‚Ä¶ until the first time you corrupt something on exit.)&lt;/p&gt;
    &lt;p&gt;When this is finished, the biggest task so far will begin‚Ä¶ the web browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comments&lt;/head&gt;
    &lt;p&gt;2.01k views&lt;/p&gt;
    &lt;p&gt;No comments yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080579</guid><pubDate>Fri, 28 Nov 2025 17:15:20 +0000</pubDate></item><item><title>Anti-patterns while working with LLMs</title><link>https://instavm.io/blog/llm-anti-patterns</link><description>&lt;doc fingerprint="271e19d21f2dd1d0"&gt;
  &lt;main&gt;
    &lt;p&gt;After working with LLMs for the last 15 months, these are some of the anti-patterns I have discovered.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By anti-patterns, I simply mean patterns or behaviors we should avoid when working with LLMs.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;1. Did I tell you that already?&lt;/head&gt;
    &lt;p&gt;Context is a scarce resource and probably worth its weight in gold, we need to use it wisely. One of the learnings is to not send the same information/text multiple times in the same session.&lt;/p&gt;
    &lt;p&gt;For example, during computer-use sending each and every image frame when a mouse is going from point A to point B on the screen as screenshots with barely anything changing between a lot of consecutive frames (mouse pointer moving 1 millimeter for example) in each API call, when just one new and final screenshot showing current context is enough.&lt;/p&gt;
    &lt;p&gt;It's sort of an irony that the same company has come up with a context management tool/api, which helps you reduce/compress the context by removing redundant messages while it did exact opposite for computer-use and sent all previous almost duplicated screenshots in every new LLM api call again. We built open-source click3 which does it without sending any possibly duplicate screenshots in API calls - screenshots with significant differences (or taken at state changes) are enough for the LLM to decide next course of action.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Asking a fish to climb a tree&lt;/head&gt;
    &lt;p&gt;Should we ask the fish to climb a tree? Sure sometimes they can climb a tree, but better ask them do things they are good at. For example, asking Gemini Banana to generate an image on a wooden plank with a text starting with prefix 1AA..(notice the double A) always ended up with 1A.. (single A) after 13 tries or so, i decided to give up. Later, I had an idea - to write the text in a google doc, take its picture and then give the picture and ask it to merge it on a wooden plank picture (also given by me) -- It did it in 1 shot.&lt;/p&gt;
    &lt;p&gt;Similarly we should not ask LLMs how many Rs are there in BLUEBERRY - we should ask it to write a code which counts the Rs. Coding ability &amp;gt; Counting ability - atleast for the current LLMs.&lt;/p&gt;
    &lt;p&gt;Take another example, Cloudflare recently realised that tool calling is better when its written as &lt;code&gt;code&lt;/code&gt; that calls them. So, it seems we should ask it to generate code whenever we expect more accurate answers.&lt;/p&gt;
    &lt;p&gt;The climbing perch - A tree climbing fish&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Asking LLM to speak, when its drowning (in context)&lt;/head&gt;
    &lt;p&gt;LLMs do best when it's not nearly full with 128k tokens. For long running sessions, which go beyond the 128k token count - it can be even worse, we then depend on the ability of the Claude to compress or discard information based on its whim. For example, the other day, it completely forgot about a database connection URL I had given it and started spitting someone else's database URL in the same session. Thankfully(for them) that URL didn't work. Unfortunately, some tasks do need big contexts, my only advice in that case is to be aware of its accuracy decline.&lt;/p&gt;
    &lt;p&gt;Some random database url, from its memory&lt;/p&gt;
    &lt;head rend="h2"&gt;4. The squeaky wheel gets the grease&lt;/head&gt;
    &lt;p&gt;LLMs don't perform well on obscure topics. Similarly and as expected, on topics which were invented after their training cut-off dates, for the simple reason of them not being trained on those topics. They perform well on topics which have been widely discussed. So if your topic is an obscure one, assume less accuracy and figure out ways to make it accurate. Here is an instance of Claude-CLI giving up on Stripe integration which btw has one of the nicest documentation -&lt;/p&gt;
    &lt;head rend="h2"&gt;5. You don't want to be a vibe-coder&lt;/head&gt;
    &lt;p&gt;It's easy to slip into a manager (or as Andrej Karpathy calls it - a vibe-coder) mode with Claude Code like tool but in my observation if you lose the sight of what the LLM is writing, it will eventually be a net loss. Never lose the thread of what's going on. For example, in the &lt;code&gt;/invoices&lt;/code&gt; api, Claude decided it was fine to put the &lt;code&gt;User&lt;/code&gt; object in the response json, since it is part of the invoice object. Only I could see it was exposing the &lt;code&gt;password_hash&lt;/code&gt; unnecessarily. Although not a security issue immediately, but if something goes wrong, and the attackers gets access to the invoice jsons, this will only help the attackers get more important information. Or imagine someone not even hashing the password and getting exposed. You get the point.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080597</guid><pubDate>Fri, 28 Nov 2025 17:16:52 +0000</pubDate></item><item><title>Show HN: An LLM-Powered Tool to Catch PCB Schematic Mistakes</title><link>https://netlist.io/</link><description>&lt;doc fingerprint="91353e48075d4f2b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Catch PCB schematic mistakes before fabrication&lt;/head&gt;
    &lt;p&gt;AI-driven electrical design checks, powered by your datasheets and netlist from KiCad or Altium. Try it now, no credit card required.&lt;/p&gt;
    &lt;p&gt;Loading sample chat...&lt;/p&gt;
    &lt;p&gt;Ok cool. Thanks!&lt;/p&gt;
    &lt;p&gt;Effort&lt;/p&gt;
    &lt;p&gt;11.5%34.5k / 300k Per Chat&lt;/p&gt;
    &lt;p&gt;AI can make mistakes, check important info. Layout suggestions are based on logic-only; layout files aren't visible to Netlist.io.&lt;/p&gt;
    &lt;p&gt;Loading sample chat...&lt;/p&gt;
    &lt;p&gt;Ok cool. Thanks!&lt;/p&gt;
    &lt;p&gt;Effort&lt;/p&gt;
    &lt;p&gt;11.5%34.5k / 300k Per Chat&lt;/p&gt;
    &lt;p&gt;AI can make mistakes, check important info. Layout suggestions are based on logic-only; layout files aren't visible to Netlist.io.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080737</guid><pubDate>Fri, 28 Nov 2025 17:30:41 +0000</pubDate></item><item><title>JSON Schema Demystified: Dialects, Vocabularies and Metaschemas</title><link>https://www.iankduncan.com/engineering/2025-11-24-json-schema-demystified/</link><description>&lt;doc fingerprint="225d6903a4e42be0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;JSON Schema Demystified: Understanding Schemas, Dialects, Vocabularies, and Metaschemas&lt;/head&gt;
    &lt;p&gt;If you‚Äôve ever tried to dive into JSON Schema, you‚Äôve probably encountered a wall of terminology that makes your head spin: schemas, metaschemas, dialects, vocabularies, keywords, anchors, dynamic references. It feels like the community invented new words for things that already had perfectly good names, just to make the rest of us feel inadequate.&lt;/p&gt;
    &lt;p&gt;I‚Äôve been working on a Haskell JSON Schema library that‚Äôs actually fully spec-compliant, which meant I had to figure all of this out. The problem isn‚Äôt that the concepts are inherently difficult. The terminology creates artificial barriers to understanding.&lt;/p&gt;
    &lt;p&gt;This post will break down the key concepts in JSON Schema in a way that actually makes sense, connecting the dots between all these terms that seem designed to confuse. By the end, you‚Äôll understand not just what these words mean, but how they fit together into a coherent system.&lt;/p&gt;
    &lt;head rend="h2"&gt;Starting simple&lt;/head&gt;
    &lt;p&gt;Before we dive into terminology, let‚Äôs look at what we‚Äôre actually trying to accomplish. JSON Schema is fundamentally about describing the shape and constraints of JSON data. Here‚Äôs a simple example:&lt;/p&gt;
    &lt;code&gt;{
  "type": "object",
  "properties": {
    "name": { "type": "string" },
    "age": { "type": "number", "minimum": 0 }
  },
  "required": ["name"]
}&lt;/code&gt;
    &lt;p&gt;This schema says: ‚ÄúI expect a JSON object with a string &lt;code&gt;name&lt;/code&gt; field (required) and an optional numeric &lt;code&gt;age&lt;/code&gt; field that must be non-negative.‚Äù Simple enough, right?&lt;/p&gt;
    &lt;p&gt;Now here‚Äôs where it gets interesting: this schema is itself valid JSON. And since JSON can describe the structure of JSON documents, we can describe the structure of schemas using more schemas. This recursive property is what gives rise to metaschemas, and where the terminology starts to get confusing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What‚Äôs a schema anyway?&lt;/head&gt;
    &lt;p&gt;A schema is just a JSON document that describes constraints on other JSON documents. That‚Äôs it. The example above is a schema.&lt;/p&gt;
    &lt;p&gt;Schemas tell you what type a value should be (string, number, object, array), what values are allowed or disallowed, what properties must or may exist on an object, how many items should be in an array. When you write a schema, you‚Äôre essentially writing rules that say ‚Äúvalid JSON documents that I care about look like this.‚Äù&lt;/p&gt;
    &lt;code&gt;{
  "type": "string",
  "minLength": 1,
  "maxLength": 100
}&lt;/code&gt;
    &lt;p&gt;This schema says: ‚ÄúI want a string between 1 and 100 characters long.‚Äù Any JSON validator that understands JSON Schema can take this schema and your data and tell you whether your data follows the rules.&lt;/p&gt;
    &lt;p&gt;The confusing part is that schemas themselves are JSON documents. So naturally, you might ask: ‚ÄúWhat describes the structure of a schema?‚Äù And that leads us to the next layer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Metaschemas: schemas all the way down&lt;/head&gt;
    &lt;p&gt;A metaschema is a schema that describes the structure of other schemas. The ‚Äúschema of schemas,‚Äù if you will.&lt;/p&gt;
    &lt;p&gt;This sounds abstract and philosophical, but it‚Äôs actually quite practical. Remember how our simple schema used keywords like &lt;code&gt;"type"&lt;/code&gt;, &lt;code&gt;"properties"&lt;/code&gt;, and &lt;code&gt;"minimum"&lt;/code&gt;? The metaschema defines what those keywords mean, what values they can have, and how they work together.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a tiny excerpt of what a metaschema might look like:&lt;/p&gt;
    &lt;code&gt;{
  "$id": "https://json-schema.org/draft/2020-12/schema",
  "type": ["object", "boolean"],
  "properties": {
    "type": {
      "anyOf": [
        { "enum": ["null", "boolean", "object", "array", "number", "string"] },
        { "type": "array", "items": { "$ref": "#/properties/type/anyOf/0" } }
      ]
    },
    "properties": {
      "type": "object",
      "additionalProperties": { "$dynamicRef": "#meta" }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;This fragment says things like: ‚ÄúThe &lt;code&gt;type&lt;/code&gt; keyword can be a single type string or an array of type strings‚Äù and ‚ÄúThe &lt;code&gt;properties&lt;/code&gt; keyword should be an object where each value is itself a schema.‚Äù&lt;/p&gt;
    &lt;p&gt;Why does this matter? Well, you can validate that your schema is well-formed by checking it against the metaschema. If someone writes &lt;code&gt;"type": "stirng"&lt;/code&gt; (typo!), the metaschema validation will catch it. The metaschema is also the formal specification of what‚Äôs allowed in schemas. Tools that process schemas (validators, code generators, documentation generators) use the metaschema to understand what they‚Äôre working with.&lt;/p&gt;
    &lt;p&gt;The relationship is simple: schemas validate data, metaschemas validate schemas.&lt;/p&gt;
    &lt;code&gt;Data ‚Üí validated by ‚Üí Schema ‚Üí validated by ‚Üí Metaschema&lt;/code&gt;
    &lt;p&gt;Here‚Äôs where it gets recursive: since a metaschema is also a schema (JSON describing JSON structure), it can validate itself. The JSON Schema metaschema is designed to be self-describing. This is similar to how a compiler written in its own language can compile itself (bootstrapping).&lt;/p&gt;
    &lt;head rend="h2"&gt;Dialects: when versions matter&lt;/head&gt;
    &lt;p&gt;So we have schemas and metaschemas. But JSON Schema has evolved over time. Different versions have added new keywords, changed behavior, and deprecated old features. How do we keep track of which version of JSON Schema we‚Äôre using?&lt;/p&gt;
    &lt;p&gt;A dialect is a specific version or flavor of JSON Schema, defined by a particular metaschema. When someone says they‚Äôre using ‚ÄúDraft 2020-12‚Äù or ‚ÄúDraft 7,‚Äù they‚Äôre referring to specific dialects.&lt;/p&gt;
    &lt;p&gt;Each dialect has its own metaschema that defines which keywords are available, its own set of behaviors and validation rules, and is identified by a URI (usually something like &lt;code&gt;https://json-schema.org/draft/2020-12/schema&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;You declare which dialect your schema uses with the &lt;code&gt;$schema&lt;/code&gt; keyword:&lt;/p&gt;
    &lt;code&gt;{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object",
  "properties": {
    "name": { "type": "string" }
  }
}&lt;/code&gt;
    &lt;p&gt;This tells validators: ‚ÄúHey, interpret this schema according to the Draft 2020-12 rules.‚Äù&lt;/p&gt;
    &lt;p&gt;Different dialects can have different keywords and different behaviors. Draft 4 didn‚Äôt have the &lt;code&gt;const&lt;/code&gt; keyword, but Draft 6 added it. The &lt;code&gt;$id&lt;/code&gt; keyword worked differently in Draft 4 versus Draft 7. Draft 2019-09 introduced the concept of vocabularies (we‚Äôll get to that).&lt;/p&gt;
    &lt;p&gt;If you write a schema using Draft 2020-12 features and someone tries to validate it with a Draft 4 validator, things won‚Äôt work correctly. The &lt;code&gt;$schema&lt;/code&gt; keyword ensures everyone is on the same page.&lt;/p&gt;
    &lt;p&gt;Think of dialects like programming language versions. Python 2 and Python 3 are different dialects of Python. Your code needs to declare which one it‚Äôs written for, or chaos ensues.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vocabularies: the modular twist&lt;/head&gt;
    &lt;p&gt;Here‚Äôs where JSON Schema gets really interesting (and where my initial confusion peaked). Starting with Draft 2019-09, JSON Schema introduced the concept of vocabularies.&lt;/p&gt;
    &lt;p&gt;A vocabulary is a named collection of keywords that work together to provide a specific kind of functionality. Instead of having one monolithic metaschema that defines all possible keywords, you can compose metaschemas from smaller, focused vocabularies.&lt;/p&gt;
    &lt;p&gt;Think of vocabularies as modules or packages. Each vocabulary provides a set of related keywords. The core vocabulary has fundamental keywords like &lt;code&gt;$id&lt;/code&gt;, &lt;code&gt;$schema&lt;/code&gt;, &lt;code&gt;$ref&lt;/code&gt;, and &lt;code&gt;$defs&lt;/code&gt;. The applicator vocabulary has keywords that apply schemas to different parts of the data like &lt;code&gt;properties&lt;/code&gt;, &lt;code&gt;items&lt;/code&gt;, and &lt;code&gt;additionalProperties&lt;/code&gt;. The validation vocabulary has keywords for constraints like &lt;code&gt;minimum&lt;/code&gt;, &lt;code&gt;maxLength&lt;/code&gt;, &lt;code&gt;pattern&lt;/code&gt;, and &lt;code&gt;enum&lt;/code&gt;. The metadata vocabulary has keywords for human-readable information like &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, and &lt;code&gt;examples&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a schema using keywords from different vocabularies:&lt;/p&gt;
    &lt;code&gt;{
  "$id": "https://example.com/my-schema",
  "title": "User Name",
  "description": "The user's full name",
  "type": "string",
  "minLength": 5,
  "pattern": "^[A-Z]"
}&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;$id&lt;/code&gt; comes from the core vocabulary, &lt;code&gt;title&lt;/code&gt; and &lt;code&gt;description&lt;/code&gt; from the metadata vocabulary, &lt;code&gt;type&lt;/code&gt; from the applicator vocabulary, and &lt;code&gt;minLength&lt;/code&gt; and &lt;code&gt;pattern&lt;/code&gt; from the validation vocabulary.&lt;/p&gt;
    &lt;p&gt;Why vocabularies? They enable modularity and extensibility. You can pick and choose which vocabularies your dialect supports. Maybe you want validation but not format checking? Just include the vocabularies you need. You can define your own vocabulary with custom keywords specific to your domain. For example, a database schema dialect might add keywords like &lt;code&gt;indexed&lt;/code&gt; or &lt;code&gt;foreignKey&lt;/code&gt;. Each vocabulary is independently specified, making it easier to understand and implement different parts of JSON Schema.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs how a metaschema declares which vocabularies it uses:&lt;/p&gt;
    &lt;code&gt;{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$vocabulary": {
    "https://json-schema.org/draft/2020-12/vocab/core": true,
    "https://json-schema.org/draft/2020-12/vocab/applicator": true,
    "https://json-schema.org/draft/2020-12/vocab/validation": true,
    "https://json-schema.org/draft/2020-12/vocab/meta-data": false
  }
}&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;true&lt;/code&gt; versus &lt;code&gt;false&lt;/code&gt; values indicate whether the vocabulary is required or optional. If a validator doesn‚Äôt understand a required vocabulary, it should refuse to process the schema. If it doesn‚Äôt understand an optional vocabulary, it can safely ignore those keywords.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extending with your own keywords&lt;/head&gt;
    &lt;p&gt;Here‚Äôs where this all gets practical. Once you understand vocabularies, you realize you can extend JSON Schema with your own domain-specific keywords. This is incredibly powerful.&lt;/p&gt;
    &lt;p&gt;In fact, you‚Äôve probably already used extended JSON Schema without realizing it. OpenAPI (the spec for describing REST APIs) is exactly this: JSON Schema extended with custom keywords for HTTP-specific concerns like &lt;code&gt;operationId&lt;/code&gt;, &lt;code&gt;responses&lt;/code&gt;, &lt;code&gt;parameters&lt;/code&gt;, and so on. OpenAPI is JSON Schema plus a vocabulary for APIs. And you could extend OpenAPI further with your own vocabulary for framework-specific behaviors or company-specific conventions.&lt;/p&gt;
    &lt;p&gt;Say you‚Äôre building an API framework and you want to annotate your schemas with HTTP-specific metadata. Standard JSON Schema doesn‚Äôt have keywords for things like ‚Äúthis field comes from a query parameter‚Äù or ‚Äúthis response uses status code 201.‚Äù So you create your own vocabulary.&lt;/p&gt;
    &lt;p&gt;First, you define your custom keywords in a vocabulary document:&lt;/p&gt;
    &lt;code&gt;{
  "$id": "https://api.example.com/vocab/http",
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$vocabulary": {
    "https://json-schema.org/draft/2020-12/vocab/core": true
  },
  "type": "object",
  "properties": {
    "httpSource": {
      "enum": ["query", "path", "header", "body"]
    },
    "httpStatus": {
      "type": "integer",
      "minimum": 100,
      "maximum": 599
    }
  }
}&lt;/code&gt;
    &lt;p&gt;This vocabulary document describes the structure of your custom keywords. Now you can use them in your schemas:&lt;/p&gt;
    &lt;code&gt;{
  "$schema": "https://api.example.com/schema",
  "$vocabulary": {
    "https://json-schema.org/draft/2020-12/vocab/core": true,
    "https://json-schema.org/draft/2020-12/vocab/validation": true,
    "https://api.example.com/vocab/http": true
  },
  "type": "object",
  "properties": {
    "userId": {
      "type": "string",
      "httpSource": "path",
      "pattern": "^[0-9]+$"
    },
    "filter": {
      "type": "string",
      "httpSource": "query"
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Your validator needs to understand what to do with &lt;code&gt;httpSource&lt;/code&gt;, of course. When it encounters a schema using your custom vocabulary, it checks whether it supports that vocabulary. If the vocabulary is marked as required and the validator doesn‚Äôt support it, validation should fail with an error saying ‚ÄúI don‚Äôt understand this vocabulary.‚Äù If it‚Äôs optional, the validator can safely ignore those keywords.&lt;/p&gt;
    &lt;p&gt;The beauty of this approach is that your extensions are explicit and discoverable. Someone reading your schema can see exactly which vocabularies it uses. A validator can definitively say whether it supports your schema or not. You‚Äôre not just stuffing random properties into schemas and hoping validators ignore them.&lt;/p&gt;
    &lt;p&gt;You can extend validation rules too. Maybe you‚Äôre working with database schemas and want to validate that certain string fields match database identifier conventions. You could define a custom keyword like &lt;code&gt;dbIdentifier&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;{
  "type": "string",
  "dbIdentifier": true,
  "description": "Must be a valid PostgreSQL identifier"
}&lt;/code&gt;
    &lt;p&gt;Your validator would implement the logic to check PostgreSQL identifier rules (no leading numbers, only certain special characters, length limits, etc.). Standard JSON Schema validators would ignore this keyword if you mark the vocabulary as optional, or refuse to process the schema if you mark it as required.&lt;/p&gt;
    &lt;p&gt;This extensibility is why JSON Schema has all this vocabulary machinery. It‚Äôs not just academic complexity for its own sake. The vocabulary system lets you build domain-specific validation languages on top of JSON Schema‚Äôs foundation, while maintaining clear boundaries about what‚Äôs standard and what‚Äôs custom.&lt;/p&gt;
    &lt;head rend="h2"&gt;Putting it all together&lt;/head&gt;
    &lt;p&gt;Let‚Äôs connect all the dots. You write a schema that describes your data structure (like a User object). The schema uses keywords like &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;properties&lt;/code&gt;, and &lt;code&gt;minimum&lt;/code&gt; to express constraints. These keywords are defined by vocabularies (the validation vocabulary, applicator vocabulary). The vocabularies are bundled into a dialect (like Draft 2020-12). The dialect is defined by a metaschema that describes which keywords are available and how they work. Your schema declares its dialect using the &lt;code&gt;$schema&lt;/code&gt; keyword.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a visual:&lt;/p&gt;
    &lt;code&gt;Your Data (JSON)
    ‚Üì validated by
Your Schema (JSON)
    ‚Üì uses keywords from
Vocabularies (sets of related keywords)
    ‚Üì bundled into
Dialect (specific version/flavor)
    ‚Üì defined by
Metaschema (schema of schemas)&lt;/code&gt;
    &lt;p&gt;A concrete example showing all the layers:&lt;/p&gt;
    &lt;code&gt;{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://example.com/schemas/user",

  "title": "User",
  "description": "A registered user in the system",

  "type": "object",
  "properties": {
    "username": {
      "type": "string",
      "pattern": "^[a-zA-Z0-9_]+$",
      "minLength": 3,
      "maxLength": 20
    },
    "email": {
      "type": "string",
      "format": "email"
    },
    "age": {
      "type": "integer",
      "minimum": 13,
      "maximum": 120
    }
  },
  "required": ["username", "email"]
}&lt;/code&gt;
    &lt;p&gt;Breaking this down: &lt;code&gt;$schema&lt;/code&gt; declares we‚Äôre using the Draft 2020-12 dialect. &lt;code&gt;$id&lt;/code&gt; is a core vocabulary keyword that uniquely identifies this schema. &lt;code&gt;title&lt;/code&gt; and &lt;code&gt;description&lt;/code&gt; are metadata vocabulary keywords for documentation. &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;properties&lt;/code&gt;, and &lt;code&gt;required&lt;/code&gt; are applicator vocabulary keywords that apply constraints. &lt;code&gt;pattern&lt;/code&gt;, &lt;code&gt;minLength&lt;/code&gt;, &lt;code&gt;minimum&lt;/code&gt;, and &lt;code&gt;format&lt;/code&gt; are validation vocabulary keywords that enforce rules.&lt;/p&gt;
    &lt;p&gt;All of these keywords are defined in the Draft 2020-12 metaschema, which specifies their meaning and behavior.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other terms you‚Äôll encounter&lt;/head&gt;
    &lt;p&gt;While we‚Äôve covered the big four (schema, metaschema, dialect, vocabulary), there are a few other terms worth understanding.&lt;/p&gt;
    &lt;p&gt;A keyword is a specific property name with defined semantics in a schema. Examples: &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;properties&lt;/code&gt;, &lt;code&gt;minimum&lt;/code&gt;, &lt;code&gt;$ref&lt;/code&gt;. Keywords are the building blocks defined by vocabularies. Some keywords are universal (&lt;code&gt;type&lt;/code&gt;, &lt;code&gt;properties&lt;/code&gt;), while others are specific to certain vocabularies (&lt;code&gt;contentMediaType&lt;/code&gt; from the content vocabulary, &lt;code&gt;deprecated&lt;/code&gt; from the metadata vocabulary).&lt;/p&gt;
    &lt;p&gt;There‚Äôs also a distinction between annotations and assertions. Assertions are keywords that can make validation fail (like &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;minimum&lt;/code&gt;, &lt;code&gt;required&lt;/code&gt;, &lt;code&gt;pattern&lt;/code&gt;). If your data violates an assertion, validation fails. Annotations are keywords that just provide information and never cause validation to fail (like &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, &lt;code&gt;examples&lt;/code&gt;, &lt;code&gt;default&lt;/code&gt;). Annotations are useful for documentation and tooling but don‚Äôt affect validity. Some keywords can produce both annotations and assertions. For instance, &lt;code&gt;properties&lt;/code&gt; asserts the types of the properties while also annotating which properties were validated.&lt;/p&gt;
    &lt;p&gt;Anchors provide named locations within a schema that you can reference. They‚Äôre like bookmarks:&lt;/p&gt;
    &lt;code&gt;{
  "$defs": {
    "address": {
      "$anchor": "addr",
      "type": "object",
      "properties": {
        "street": { "type": "string" }
      }
    }
  },
  "properties": {
    "billingAddress": { "$ref": "#addr" },
    "shippingAddress": { "$ref": "#addr" }
  }
}&lt;/code&gt;
    &lt;p&gt;Dynamic anchors (&lt;code&gt;$dynamicAnchor&lt;/code&gt; and &lt;code&gt;$dynamicRef&lt;/code&gt;) are more advanced. They allow references to be resolved differently depending on the ‚Äúcontext‚Äù of evaluation. This is mostly useful for extending metaschemas and creating recursive schemas that can be overridden. Honestly, you can probably ignore dynamic anchors until you‚Äôre doing very advanced schema composition.&lt;/p&gt;
    &lt;p&gt;A bundled schema is a single schema document that contains multiple schema resources, usually via &lt;code&gt;$defs&lt;/code&gt;. This is handy for distributing related schemas together:&lt;/p&gt;
    &lt;code&gt;{
  "$id": "https://example.com/schemas/bundle",
  "$defs": {
    "user": {
      "$id": "user",
      "type": "object",
      "properties": { "name": { "type": "string" } }
    },
    "product": {
      "$id": "product",
      "type": "object",
      "properties": { "title": { "type": "string" } }
    }
  }
}&lt;/code&gt;
    &lt;p&gt;Now you can reference &lt;code&gt;https://example.com/schemas/user&lt;/code&gt; and &lt;code&gt;https://example.com/schemas/product&lt;/code&gt; from other schemas, even though they‚Äôre defined in the same document.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why is the terminology so confusing?&lt;/head&gt;
    &lt;p&gt;You might be wondering: why did they make this so complicated? The answer is that JSON Schema has evolved significantly over more than a decade, and the terminology evolved with it.&lt;/p&gt;
    &lt;p&gt;Early versions (Draft 3, Draft 4) had simpler, more monolithic metaschemas. As the specification matured, the community recognized the need for modularity, extensibility, and clearer versioning. That‚Äôs when concepts like dialects and vocabularies were formalized.&lt;/p&gt;
    &lt;p&gt;The terminology can feel academic because it comes from formal specification work. These are precise technical terms designed for specification writers and implementers, not necessarily for end users. Unfortunately, they leaked into the documentation that everyone reads, creating a steep learning curve.&lt;/p&gt;
    &lt;p&gt;But here‚Äôs the thing: you don‚Äôt need to think about most of this complexity to use JSON Schema effectively.&lt;/p&gt;
    &lt;head rend="h2"&gt;What do you actually need to know?&lt;/head&gt;
    &lt;p&gt;For 95% of JSON Schema usage, you need to understand that schemas describe data structure and constraints, &lt;code&gt;$schema&lt;/code&gt; declares which version (dialect) you‚Äôre using, and keywords like &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;properties&lt;/code&gt;, and &lt;code&gt;minimum&lt;/code&gt; define your rules.&lt;/p&gt;
    &lt;p&gt;That‚Äôs it. You can write perfectly good schemas for years without ever thinking about metaschemas or vocabularies in depth.&lt;/p&gt;
    &lt;p&gt;The deeper concepts matter when you‚Äôre building tools that process schemas (validators, code generators), extending JSON Schema with custom keywords, working on the specification itself, or debugging complex reference resolution issues. For everyone else, just know that these concepts exist and form a coherent system. If you encounter them in documentation, you‚Äôll know what they mean, but you probably won‚Äôt need to think about them day-to-day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some practical advice&lt;/head&gt;
    &lt;p&gt;Always specify &lt;code&gt;$schema&lt;/code&gt; to make it explicit which dialect you‚Äôre using:&lt;/p&gt;
    &lt;code&gt;{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "type": "object"
}&lt;/code&gt;
    &lt;p&gt;This ensures validators interpret your schema correctly.&lt;/p&gt;
    &lt;p&gt;Start with the latest stable dialect. As of this writing, that‚Äôs Draft 2020-12. It has the most features and best tooling support. Don‚Äôt worry about older drafts unless you‚Äôre maintaining legacy schemas.&lt;/p&gt;
    &lt;p&gt;Use clear, descriptive metadata. Even though &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;description&lt;/code&gt;, and &lt;code&gt;examples&lt;/code&gt; don‚Äôt affect validation, they make your schemas much more useful:&lt;/p&gt;
    &lt;code&gt;{
  "type": "object",
  "title": "User Account",
  "description": "Represents a user account in the system",
  "properties": {
    "username": {
      "type": "string",
      "description": "Unique username for login (alphanumeric and underscores only)",
      "examples": ["john_doe", "alice123"]
    }
  }
}&lt;/code&gt;
    &lt;p&gt;For complex schemas, use &lt;code&gt;$defs&lt;/code&gt; to break things into reusable pieces:&lt;/p&gt;
    &lt;code&gt;{
  "$defs": {
    "timestamp": {
      "type": "string",
      "format": "date-time"
    },
    "identifier": {
      "type": "string",
      "pattern": "^[a-z0-9-]+$"
    }
  },
  "type": "object",
  "properties": {
    "id": { "$ref": "#/$defs/identifier" },
    "createdAt": { "$ref": "#/$defs/timestamp" }
  }
}&lt;/code&gt;
    &lt;p&gt;If you‚Äôre ever confused about whether a keyword exists or how it works, check the metaschema. The Draft 2020-12 metaschema lives at: https://json-schema.org/draft/2020-12/schema&lt;/p&gt;
    &lt;p&gt;Test your schemas with online validators or schema testing tools to ensure they work as expected. The official JSON Schema website has a validator you can try: https://www.jsonschemavalidator.net/&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping up&lt;/head&gt;
    &lt;p&gt;JSON Schema‚Äôs terminology can feel intimidating, but the core ideas are straightforward. Schemas describe data. Metaschemas describe schemas. Dialects are specific versions of JSON Schema. Vocabularies are modular collections of keywords. Keywords are the actual properties you use in schemas.&lt;/p&gt;
    &lt;p&gt;The terminology exists to support a powerful, extensible system for describing JSON data structures. But for everyday use, you can mostly ignore the academic terminology and focus on writing clear, useful schemas.&lt;/p&gt;
    &lt;p&gt;The next time you see ‚Äúmetaschema‚Äù or ‚Äúvocabulary‚Äù in JSON Schema documentation, don‚Äôt panic. You know what these terms mean now, and more importantly, you understand how they fit together. That‚Äôs the real goal: building a mental model of how the system works, not memorizing definitions.&lt;/p&gt;
    &lt;p&gt;Now go forth and write some schemas. And remember: if you find yourself confused by JSON Schema terminology again, you‚Äôre not alone. The important thing is that underneath the jargon, there‚Äôs a well-designed system for a genuinely useful purpose.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080769</guid><pubDate>Fri, 28 Nov 2025 17:34:26 +0000</pubDate></item><item><title>Molly: An Improved Signal App</title><link>https://molly.im/</link><description>&lt;doc fingerprint="fa4c68be1bd036dc"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;An improved Signal app&lt;/head&gt;
    &lt;p&gt;Molly is an independent Signal fork for Android with improved features:&lt;/p&gt;
    &lt;p&gt; Fully FOSS Contains no proprietary blobs, unlike Signal &lt;/p&gt;
    &lt;p&gt; Encrypted Protects database with Passphrase Encryption &lt;/p&gt;
    &lt;p&gt; Multi-Device Pair multiple devices to a single account &lt;/p&gt;
    &lt;p&gt; Material You Extra theme that follows your device palette &lt;/p&gt;
    &lt;p&gt; UnifiedPush Ungoogled notification system &lt;/p&gt;
    &lt;p&gt; Automatic Locking When you are gone for a set period of time &lt;/p&gt;
    &lt;p&gt; RAM Shredding Securely shreds sensitive data &lt;/p&gt;
    &lt;p&gt; And more New and better features to come &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080916</guid><pubDate>Fri, 28 Nov 2025 17:48:25 +0000</pubDate></item><item><title>Rock Paper Scissors Solitaire</title><link>https://klezlab.it/rock-paper-scissors-solitaire.html</link><description>&lt;doc fingerprint="f63a208fde05911a"&gt;
  &lt;main&gt;
    &lt;p&gt;Are you bored? And I mean, really bored? Like "I'd do anything to pass time"-bored?&lt;/p&gt;
    &lt;p&gt;Do you also happen to be alone and without an internet connection?&lt;/p&gt;
    &lt;p&gt;You could play Rock Paper Scissors solitaire!&lt;/p&gt;
    &lt;p&gt;All you need is a dice. That's your opponent. Throw it from your hand and as soon as you throw it form the shape you choose with your hand and see the result of the dice. 1-2 is Rock, 3-4 is Paper, 5-6 is Scrissors.&lt;/p&gt;
    &lt;p&gt;Those numbers can be changed if you don't like them. For example I grew up calling it Paper Rock Scissors (Carta-Sasso-Forbici in Italian), so to me 1-2 being Paper and 3-4 being Rock is easier to mentally handle.&lt;/p&gt;
    &lt;p&gt;That's it.&lt;/p&gt;
    &lt;p&gt;Did I mention you need to be really bored to enjoy this?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46080972</guid><pubDate>Fri, 28 Nov 2025 17:54:05 +0000</pubDate></item><item><title>28M Hacker News comments as vector embedding search dataset</title><link>https://clickhouse.com/docs/getting-started/example-datasets/hackernews-vector-search-dataset</link><description>&lt;doc fingerprint="11938b322ad7669e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h3"&gt;Generate embeddings for search query&lt;/head&gt;
      &lt;p&gt;Sentence Transformers provide local, easy to use embedding models for capturing the semantic meaning of sentences and paragraphs.&lt;/p&gt;
      &lt;p&gt;The dataset in this HackerNews dataset contains vector emebeddings generated from the all-MiniLM-L6-v2 model.&lt;/p&gt;
      &lt;p&gt;An example Python script is provided below to demonstrate how to programmatically generate embedding vectors using &lt;code&gt;sentence_transformers1 Python package. The search embedding vector is then passed as an argument to the [&lt;/code&gt;cosineDistance()&lt;code&gt;](/sql-reference/functions/distance-functions#cosineDistance) function in the &lt;/code&gt;SELECT` query.&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;from sentence_transformers import SentenceTransformer
import sys

import clickhouse_connect

print("Initializing...")

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

chclient = clickhouse_connect.get_client() # ClickHouse credentials here

while True:
    # Take the search query from user
    print("Enter a search query :")
    input_query = sys.stdin.readline();
    texts = [input_query]

    # Run the model and obtain search vector
    print("Generating the embedding for ", input_query);
    embeddings = model.encode(texts)

    print("Querying ClickHouse...")
    params = {'v1':list(embeddings[0]), 'v2':20}
    result = chclient.query("SELECT id, title, text FROM hackernews ORDER BY cosineDistance(vector, %(v1)s) LIMIT %(v2)s", parameters=params)
    print("Results :")
    for row in result.result_rows:
        print(row[0], row[2][:100])
        print("---------")

&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;p&gt;An example of running the above Python script and similarity search results are shown below (only 100 characters from each of the top 20 posts are printed):&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;Initializing...

Enter a search query :
Are OLAP cubes useful

Generating the embedding for  "Are OLAP cubes useful"

Querying ClickHouse...

Results :

27742647 smartmic:
slt2021: OLAP Cube is not dead, as long as you use some form of:&amp;lt;p&amp;gt;1. GROUP BY multiple fi
---------
27744260 georgewfraser:A data mart is a logical organization of data to help humans understand the schema. Wh
---------
27761434 mwexler:&amp;amp;quot;We model data according to rigorous frameworks like Kimball or Inmon because we must r
---------
28401230 chotmat:
erosenbe0: OLAP database is just a copy, replica, or archive of data with a schema designe
---------
22198879 Merick:+1 for Apache Kylin, it&amp;amp;#x27;s a great project and awesome open source community. If anyone i
---------
27741776 crazydoggers:I always felt the value of an OLAP cube was uncovering questions you may not know to as
---------
22189480 shadowsun7:
_Codemonkeyism: After maintaining an OLAP cube system for some years, I&amp;amp;#x27;m not that
---------
27742029 smartmic:
gengstrand: My first exposure to OLAP was on a team developing a front end to Essbase that
---------
22364133 irfansharif:
simo7: I&amp;amp;#x27;m wondering how this technology could work for OLAP cubes.&amp;lt;p&amp;gt;An OLAP cube
---------
23292746 scoresmoke:When I was developing my pet project for Web analytics (&amp;lt;a href="https:&amp;amp;#x2F;&amp;amp;#x2F;github
---------
22198891 js8:It seems that the article makes a categorical error, arguing that OLAP cubes were replaced by co
---------
28421602 chotmat:
7thaccount: Is there any advantage to OLAP cube over plain SQL (large historical database r
---------
22195444 shadowsun7:
lkcubing: Thanks for sharing. Interesting write up.&amp;lt;p&amp;gt;While this article accurately capt
---------
22198040 lkcubing:Thanks for sharing. Interesting write up.&amp;lt;p&amp;gt;While this article accurately captures the issu
---------
3973185 stefanu:
sgt: Interesting idea. Ofcourse, OLAP isn't just about the underlying cubes and dimensions,
---------
22190903 shadowsun7:
js8: It seems that the article makes a categorical error, arguing that OLAP cubes were r
---------
28422241 sradman:OLAP Cubes have been disrupted by Column Stores. Unless you are interested in the history of
---------
28421480 chotmat:
sradman: OLAP Cubes have been disrupted by Column Stores. Unless you are interested in the
---------
27742515 BadInformatics:
quantified: OP posts with inverted condition: ‚ÄúOLAP != OLAP Cube‚Äù is the actual titl
---------
28422935 chotmat:
rstuart4133: I remember hearing about OLAP cubes donkey&amp;amp;#x27;s years ago (probably not far
---------
&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Summarization demo application&lt;/head&gt;
      &lt;p&gt;The example above demonstrated semantic search and document retrieval using ClickHouse.&lt;/p&gt;
      &lt;p&gt;A very simple but high potential generative AI example application is presented next.&lt;/p&gt;
      &lt;p&gt;The application performs the following steps:&lt;/p&gt;
      &lt;list class="custom-ol" rend="ol"&gt;
        &lt;item class="custom-li"&gt;Accepts a topic as input from the user&lt;/item&gt;
        &lt;item class="custom-li"&gt;Generates an embedding vector for the topic by using the &lt;code&gt;SentenceTransformers&lt;/code&gt; with model &lt;code&gt;all-MiniLM-L6-v2&lt;/code&gt;&lt;/item&gt;
        &lt;item class="custom-li"&gt;Retrieves highly relevant posts/comments using vector similarity search on the &lt;code&gt;hackernews&lt;/code&gt; table&lt;/item&gt;
        &lt;item class="custom-li"&gt;Uses &lt;code&gt;LangChain&lt;/code&gt; and OpenAI &lt;code&gt;gpt-3.5-turbo&lt;/code&gt; Chat API to summarize the content retrieved in step #3.
The posts/comments retrieved in step #3 are passed as context to the Chat API and are the key link in Generative AI.&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;An example from running the summarization application is first listed below, followed by the code for the summarization application. Running the application requires an OpenAI API key to be set in the environment variable &lt;code&gt;OPENAI_API_KEY&lt;/code&gt;. The OpenAI API key can be obtained after registering at https://platform.openai.com.&lt;/p&gt;
      &lt;p&gt;This application demonstrates a Generative AI use-case that is applicable to multiple enterprise domains like : customer sentiment analysis, technical support automation, mining user conversations, legal documents, medical records, meeting transcripts, financial statements, etc&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;$ python3 summarize.py

Enter a search topic :
ClickHouse performance experiences

Generating the embedding for ----&amp;gt;  ClickHouse performance experiences

Querying ClickHouse to retrieve relevant articles...

Initializing chatgpt-3.5-turbo model...

Summarizing search results retrieved from ClickHouse...

Summary from chatgpt-3.5:
The discussion focuses on comparing ClickHouse with various databases like TimescaleDB, Apache Spark,
AWS Redshift, and QuestDB, highlighting ClickHouse's cost-efficient high performance and suitability
for analytical applications. Users praise ClickHouse for its simplicity, speed, and resource efficiency
in handling large-scale analytics workloads, although some challenges like DMLs and difficulty in backups
are mentioned. ClickHouse is recognized for its real-time aggregate computation capabilities and solid
engineering, with comparisons made to other databases like Druid and MemSQL. Overall, ClickHouse is seen
as a powerful tool for real-time data processing, analytics, and handling large volumes of data
efficiently, gaining popularity for its impressive performance and cost-effectiveness.
&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;p&gt;Code for the above application :&lt;/p&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;quote&gt;
            &lt;code&gt;print("Initializing...")

import sys
import json
import time
from sentence_transformers import SentenceTransformer

import clickhouse_connect

from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains.summarize import load_summarize_chain
import textwrap
import tiktoken

def num_tokens_from_string(string: str, encoding_name: str) -&amp;gt; int:
    encoding = tiktoken.encoding_for_model(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

chclient = clickhouse_connect.get_client(compress=False) # ClickHouse credentials here

while True:
    # Take the search query from user
    print("Enter a search topic :")
    input_query = sys.stdin.readline();
    texts = [input_query]

    # Run the model and obtain search or reference vector
    print("Generating the embedding for ----&amp;gt; ", input_query);
    embeddings = model.encode(texts)

    print("Querying ClickHouse...")
    params = {'v1':list(embeddings[0]), 'v2':100}
    result = chclient.query("SELECT id,title,text FROM hackernews ORDER BY cosineDistance(vector, %(v1)s) LIMIT %(v2)s", parameters=params)

    # Just join all the search results
    doc_results = ""
    for row in result.result_rows:
        doc_results = doc_results + "\n" + row[2]

    print("Initializing chatgpt-3.5-turbo model")
    model_name = "gpt-3.5-turbo"

    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
        model_name=model_name
    )

    texts = text_splitter.split_text(doc_results)

    docs = [Document(page_content=t) for t in texts]

    llm = ChatOpenAI(temperature=0, model_name=model_name)

    prompt_template = """
Write a concise summary of the following in not more than 10 sentences:


{text}


CONSCISE SUMMARY :
"""

    prompt = PromptTemplate(template=prompt_template, input_variables=["text"])

    num_tokens = num_tokens_from_string(doc_results, model_name)

    gpt_35_turbo_max_tokens = 4096
    verbose = False

    print("Summarizing search results retrieved from ClickHouse...")

    if num_tokens &amp;lt;= gpt_35_turbo_max_tokens:
        chain = load_summarize_chain(llm, chain_type="stuff", prompt=prompt, verbose=verbose)
    else:
        chain = load_summarize_chain(llm, chain_type="map_reduce", map_prompt=prompt, combine_prompt=prompt, verbose=verbose)

    summary = chain.run(docs)

    print(f"Summary from chatgpt-3.5: {summary}")
&lt;/code&gt;
          &lt;/quote&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46081053</guid><pubDate>Fri, 28 Nov 2025 18:02:04 +0000</pubDate></item><item><title>Imgur Geo-Blocked the UK, So I Geo-Unblocked My Network</title><link>https://blog.tymscar.com/posts/imgurukproxy/</link><description>&lt;doc fingerprint="b582e1f56725aa92"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Imgur Geo-Blocked the UK, So I Geo-Unblocked My Entire Network&lt;/head&gt;
    &lt;p&gt;Imgur decided to block UK users. Honestly? I don‚Äôt really care that much. I haven‚Äôt actively browsed the site in years. But it used to be everywhere. Back when Reddit embedded everything on Imgur, maybe fifteen years ago, it was genuinely useful. Then Reddit built their own image hosting, Discord did the same, and Imgur slowly faded into the background.&lt;/p&gt;
    &lt;p&gt;Except it never fully disappeared. And since the block, I keep stumbling across Imgur links that just show ‚Äúunavailable.‚Äù It‚Äôs mildly infuriating.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Minecraft shader problem#&lt;/head&gt;
    &lt;p&gt;Here‚Äôs a concrete example. I was playing Minecraft with some work colleagues and wanted to try different shaders. Most shader pages embed preview images hosted on Imgur. So I‚Äôd click through shader after shader, and every single preview was just gone. I couldn‚Äôt see what any of them looked like without the images.&lt;/p&gt;
    &lt;p&gt;This kind of thing happens constantly now. Old forum posts, Reddit threads, documentation pages, random project READMEs. Imgur links are still scattered across the internet, and in the UK, they‚Äôre all broken.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why I didn‚Äôt just install a VPN#&lt;/head&gt;
    &lt;p&gt;The obvious solution is to use a VPN. Change your location, problem solved. But I have a few issues with that approach.&lt;/p&gt;
    &lt;p&gt;First, I just upgraded to 2.5 Gbps internet and I don‚Äôt want to route all my traffic through a VPN and take the speed hit. I have this bandwidth for a reason.&lt;/p&gt;
    &lt;p&gt;Second, even if I installed a VPN on my main machine, what about my phone? My laptop? My desktop? Every device would need the VPN running, and I‚Äôd have to remember to connect it before browsing. It‚Äôs messy.&lt;/p&gt;
    &lt;p&gt;I wanted something cleaner: a solution that works for every device on my network, automatically, without any client-side configuration.&lt;/p&gt;
    &lt;head rend="h2"&gt;The network-level approach#&lt;/head&gt;
    &lt;p&gt;I already run a homelab with Traefik as my reverse proxy, Pi-hole for DNS, and everything declaratively configured with NixOS. If you‚Äôve read my previous post on Docker containers with secrets, you‚Äôll recognise the pattern.&lt;/p&gt;
    &lt;p&gt;The idea was simple: intercept all requests to &lt;code&gt;i.imgur.com&lt;/code&gt; at the DNS level, route them through a VPN-connected container, and serve the images back. Every device on my network automatically uses Pi-hole for DNS via DHCP, so this would be completely transparent.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs the flow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Device requests &lt;code&gt;i.imgur.com&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Pi-hole returns my Traefik instance‚Äôs IP instead&lt;/item&gt;
      &lt;item&gt;Traefik sees the SNI hostname and routes to Gluetun&lt;/item&gt;
      &lt;item&gt;Gluetun tunnels the request through a VPN&lt;/item&gt;
      &lt;item&gt;Nginx (attached to Gluetun‚Äôs network) proxies to the real Imgur&lt;/item&gt;
      &lt;item&gt;Image comes back through the tunnel to the device&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why Nginx when I already have Traefik?#&lt;/head&gt;
    &lt;p&gt;Good question. Gluetun isn‚Äôt a reverse proxy. It‚Äôs a container that provides VPN connectivity to other containers attached to its network namespace. So I needed something inside Gluetun‚Äôs network to actually handle the proxying. Nginx was the simplest choice.&lt;/p&gt;
    &lt;p&gt;The Nginx config is minimal. It just does TCP passthrough with SNI:&lt;/p&gt;
    &lt;code&gt;user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
}

stream {
    resolver 127.0.0.1 valid=30s;
    resolver_timeout 5s;

    server {
        listen 443;
        ssl_preread on;
        proxy_pass i.imgur.com:443;
        proxy_connect_timeout 10s;
        proxy_timeout 60s;
    }
}
&lt;/code&gt;
    &lt;p&gt;This listens on port 443, reads the SNI header to confirm the destination, and passes the connection through to the real &lt;code&gt;i.imgur.com&lt;/code&gt;. The TLS handshake happens end-to-end; Nginx never sees the decrypted traffic.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Docker setup#&lt;/head&gt;
    &lt;p&gt;The compose file runs two containers. Gluetun handles the VPN connection, and Nginx attaches to Gluetun‚Äôs network:&lt;/p&gt;
    &lt;code&gt;version: '3.8'

services:
  gluetun:
    image: qmcgaw/gluetun:latest
    container_name: gluetun
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    environment:
      - VPN_SERVICE_PROVIDER=${VPN_SERVICE_PROVIDER}
      - VPN_TYPE=wireguard
      - WIREGUARD_PRIVATE_KEY=${WIREGUARD_PRIVATE_KEY}
      - SERVER_COUNTRIES=${SERVER_COUNTRIES}
      - FIREWALL=on
      - FIREWALL_INPUT_PORTS=443
      - FIREWALL_OUTBOUND_SUBNETS=10.0.0.0/8
      - DOT=on
      - DOT_PROVIDERS=cloudflare
      - HEALTH_VPN_DURATION_INITIAL=30s
    volumes:
      - ./gluetun:/gluetun
    restart: unless-stopped
    networks:
      - proxy
    healthcheck:
      test: ["CMD", "/gluetun-entrypoint", "healthcheck"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  imgur-proxy:
    image: nginx:alpine
    container_name: imgur-proxy
    depends_on:
      gluetun:
        condition: service_healthy
    network_mode: "service:gluetun"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    restart: unless-stopped

networks:
  proxy:
    external: true
&lt;/code&gt;
    &lt;p&gt;The key detail is &lt;code&gt;network_mode: "service:gluetun"&lt;/code&gt;. This makes Nginx share Gluetun‚Äôs network stack, so all its traffic automatically goes through the VPN tunnel.&lt;/p&gt;
    &lt;p&gt;I‚Äôm not going to mention which VPN provider I use. It‚Äôs one of the major ones with WireGuard support, but honestly I‚Äôm not thrilled with it. Use whatever you have.&lt;/p&gt;
    &lt;head rend="h2"&gt;Traefik routing#&lt;/head&gt;
    &lt;p&gt;The final piece is telling Traefik to route &lt;code&gt;i.imgur.com&lt;/code&gt; traffic to the Gluetun container. This uses TCP routing with TLS passthrough:&lt;/p&gt;
    &lt;code&gt;tcp:
  routers:
    imgur-router:
      rule: "HostSNI(`i.imgur.com`)"
      entryPoints:
        - https
      service: imgur-service
      tls:
        passthrough: true
  services:
    imgur-service:
      loadBalancer:
        servers:
          - address: "gluetun:443"
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;passthrough: true&lt;/code&gt; is important. It means Traefik doesn‚Äôt terminate TLS; it just inspects the SNI header and forwards the connection.&lt;/p&gt;
    &lt;head rend="h2"&gt;NixOS integration#&lt;/head&gt;
    &lt;p&gt;Following the same pattern from my Docker with secrets post, I created a systemd service that runs the compose stack with Agenix-managed secrets:&lt;/p&gt;
    &lt;code&gt;{ pkgs, config, ... }:
let
  docker-env = config.age.secrets.docker-imgur-proxy.path;
in
{
  systemd.services.imgur-proxy = {
    description = "Imgur Proxy with VPN";
    after = [
      "network.target"
      "docker.service"
      "docker-create-proxy-network.service"
    ];
    wants = [
      "docker.service"
      "docker-create-proxy-network.service"
    ];
    serviceConfig = {
      ExecStart = "${pkgs.docker}/bin/docker compose --env-file ${docker-env} -f docker-compose.yml up --force-recreate";
      ExecStop = "${pkgs.docker}/bin/docker compose -f docker-compose.yml down";
      WorkingDirectory = "/home/tymscar/dotfiles/apps/nixos/docker/imgur-proxy";
      Restart = "always";
    };
    wantedBy = [ "multi-user.target" ];
  };
}
&lt;/code&gt;
    &lt;p&gt;The VPN credentials are stored encrypted with Agenix, so my entire dotfiles repo stays public while keeping secrets safe.&lt;/p&gt;
    &lt;head rend="h2"&gt;The result#&lt;/head&gt;
    &lt;p&gt;Now when any device on my network requests an Imgur image, it works. My phone, my laptop, guest devices, everything. No VPN apps to install, no browser extensions, no manual configuration. Pi-hole intercepts the DNS, Traefik routes the connection, and Gluetun tunnels it through a non-UK exit point.&lt;/p&gt;
    &lt;p&gt;The latency increase is negligible for loading images, and it only affects Imgur traffic. Everything else still goes direct at full speed.&lt;/p&gt;
    &lt;p&gt;Is this overkill for viewing the occasional Imgur image? Probably. But it‚Äôs a clean solution that requires minimal ongoing maintenance, and it scratches the homelab itch. Plus I can finally see what those Minecraft shaders look like.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46081188</guid><pubDate>Fri, 28 Nov 2025 18:15:50 +0000</pubDate></item><item><title>Show HN: Pulse 2.0 ‚Äì Live co-listening rooms where anyone can be a DJ</title><link>https://473999.net/pulse</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46081443</guid><pubDate>Fri, 28 Nov 2025 18:39:34 +0000</pubDate></item><item><title>Effective harnesses for long-running agents</title><link>https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents</link><description>&lt;doc fingerprint="3d429ef368998aee"&gt;
  &lt;main&gt;
    &lt;p&gt;As AI agents become more capable, developers are increasingly asking them to take on complex tasks requiring work that spans hours, or even days. However, getting agents to make consistent progress across multiple context windows remains an open problem.&lt;/p&gt;
    &lt;p&gt;The core challenge of long-running agents is that they must work in discrete sessions, and each new session begins with no memory of what came before. Imagine a software project staffed by engineers working in shifts, where each new engineer arrives with no memory of what happened on the previous shift. Because context windows are limited, and because most complex projects cannot be completed within a single window, agents need a way to bridge the gap between coding sessions.&lt;/p&gt;
    &lt;p&gt;We developed a two-fold solution to enable the Claude Agent SDK to work effectively across many context windows: an initializer agent that sets up the environment on the first run, and a coding agent that is tasked with making incremental progress in every session, while leaving clear artifacts for the next session. You can find code examples in the accompanying quickstart.&lt;/p&gt;
    &lt;head rend="h2"&gt;The long-running agent problem&lt;/head&gt;
    &lt;p&gt;The Claude Agent SDK is a powerful, general-purpose agent harness adept at coding, as well as other tasks that require the model to use tools to gather context, plan, and execute. It has context management capabilities such as compaction, which enables an agent to work on a task without exhausting the context window. Theoretically, given this setup, it should be possible for an agent to continue to do useful work for an arbitrarily long time.&lt;/p&gt;
    &lt;p&gt;However, compaction isn‚Äôt sufficient. Out of the box, even a frontier coding model like Opus 4.5 running on the Claude Agent SDK in a loop across multiple context windows will fall short of building a production-quality web app if it‚Äôs only given a high-level prompt, such as ‚Äúbuild a clone of claude.ai.‚Äù&lt;/p&gt;
    &lt;p&gt;Claude‚Äôs failures manifested in two patterns. First, the agent tended to try to do too much at once‚Äîessentially to attempt to one-shot the app. Often, this led to the model running out of context in the middle of its implementation, leaving the next session to start with a feature half-implemented and undocumented. The agent would then have to guess at what had happened, and spend substantial time trying to get the basic app working again. This happens even with compaction, which doesn‚Äôt always pass perfectly clear instructions to the next agent.&lt;/p&gt;
    &lt;p&gt;A second failure mode would often occur later in a project. After some features had already been built, a later agent instance would look around, see that progress had been made, and declare the job done.&lt;/p&gt;
    &lt;p&gt;This decomposes the problem into two parts. First, we need to set up an initial environment that lays the foundation for all the features that a given prompt requires, which sets up the agent to work step-by-step and feature-by-feature. Second, we should prompt each agent to make incremental progress towards its goal while also leaving the environment in a clean state at the end of a session. By ‚Äúclean state‚Äù we mean the kind of code that would be appropriate for merging to a main branch: there are no major bugs, the code is orderly and well-documented, and in general, a developer could easily begin work on a new feature without first having to clean up an unrelated mess.&lt;/p&gt;
    &lt;p&gt;When experimenting internally, we addressed these problems using a two-part solution:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Initializer agent: The very first agent session uses a specialized prompt that asks the model to set up the initial environment: an &lt;code&gt;init.sh&lt;/code&gt;script, a claude-progress.txt file that keeps a log of what agents have done, and an initial git commit that shows what files were added.&lt;/item&gt;
      &lt;item&gt;Coding agent: Every subsequent session asks the model to make incremental progress, then leave structured updates.1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key insight here was finding a way for agents to quickly understand the state of work when starting with a fresh context window, which is accomplished with the claude-progress.txt file alongside the git history. Inspiration for these practices came from knowing what effective software engineers do every day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Environment management&lt;/head&gt;
    &lt;p&gt;In the updated Claude 4 prompting guide, we shared some best practices for multi-context window workflows, including a harness structure that uses ‚Äúa different prompt for the very first context window.‚Äù This ‚Äúdifferent prompt‚Äù requests that the initializer agent set up the environment with all the necessary context that future coding agents will need to work effectively. Here, we provide a deeper dive on some of the key components of such an environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feature list&lt;/head&gt;
    &lt;p&gt;To address the problem of the agent one-shotting an app or prematurely considering the project complete, we prompted the initializer agent to write a comprehensive file of feature requirements expanding on the user‚Äôs initial prompt. In the claude.ai clone example, this meant over 200 features, such as ‚Äúa user can open a new chat, type in a query, press enter, and see an AI response.‚Äù These features were all initially marked as ‚Äúfailing‚Äù so that later coding agents would have a clear outline of what full functionality looked like.&lt;/p&gt;
    &lt;code&gt;{
    "category": "functional",
    "description": "New chat button creates a fresh conversation",
    "steps": [
      "Navigate to main interface",
      "Click the 'New Chat' button",
      "Verify a new conversation is created",
      "Check that chat area shows welcome state",
      "Verify conversation appears in sidebar"
    ],
    "passes": false
  }
&lt;/code&gt;
    &lt;p&gt;We prompt coding agents to edit this file only by changing the status of a passes field, and we use strongly-worded instructions like ‚ÄúIt is unacceptable to remove or edit tests because this could lead to missing or buggy functionality.‚Äù After some experimentation, we landed on using JSON for this, as the model is less likely to inappropriately change or overwrite JSON files compared to Markdown files.&lt;/p&gt;
    &lt;head rend="h3"&gt;Incremental progress&lt;/head&gt;
    &lt;p&gt;Given this initial environment scaffolding, the next iteration of the coding agent was then asked to work on only one feature at a time. This incremental approach turned out to be critical to addressing the agent‚Äôs tendency to do too much at once.&lt;/p&gt;
    &lt;p&gt;Once working incrementally, it‚Äôs still essential that the model leaves the environment in a clean state after making a code change. In our experiments, we found that the best way to elicit this behavior was to ask the model to commit its progress to git with descriptive commit messages and to write summaries of its progress in a progress file. This allowed the model to use git to revert bad code changes and recover working states of the code base.&lt;/p&gt;
    &lt;p&gt;These approaches also increased efficiency, as they eliminated the need for an agent to have to guess at what had happened and spend its time trying to get the basic app working again.&lt;/p&gt;
    &lt;head rend="h3"&gt;Testing&lt;/head&gt;
    &lt;p&gt;One final major failure mode that we observed was Claude‚Äôs tendency to mark a feature as complete without proper testing. Absent explicit prompting, Claude tended to make code changes, and even do testing with unit tests or &lt;code&gt;curl&lt;/code&gt; commands against a development server, but would fail recognize that the feature didn‚Äôt work end-to-end.&lt;/p&gt;
    &lt;p&gt;In the case of building a web app, Claude mostly did well at verifying features end-to-end once explicitly prompted to use browser automation tools and do all testing as a human user would.&lt;/p&gt;
    &lt;p&gt;Providing Claude with these kinds of testing tools dramatically improved performance, as the agent was able to identify and fix bugs that weren‚Äôt obvious from the code alone.&lt;/p&gt;
    &lt;p&gt;Some issues remain, like limitations to Claude‚Äôs vision and to browser automation tools making it difficult to identify every kind of bug. For example, Claude can‚Äôt see browser-native alert modals through the Puppeteer MCP, and features relying on these modals tended to be buggier as a result.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting up to speed&lt;/head&gt;
    &lt;p&gt;With all of the above in place, every coding agent is prompted to run through a series of steps to get its bearings, some quite basic but still helpful:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Run &lt;code&gt;pwd&lt;/code&gt;to see the directory you‚Äôre working in. You‚Äôll only be able to edit files in this directory.&lt;/item&gt;
      &lt;item&gt;Read the git logs and progress files to get up to speed on what was recently worked on.&lt;/item&gt;
      &lt;item&gt;Read the features list file and choose the highest-priority feature that‚Äôs not yet done to work on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This approach saves Claude some tokens in every session since it doesn‚Äôt have to figure out how to test the code. It also helps to ask the initializer agent to write an init.sh script that can run the development server, and then run through a basic end-to-end test before implementing a new feature.&lt;/p&gt;
    &lt;p&gt;In the case of the claude.ai clone, this meant that the agent always started the local development server and used the Puppeteer MCP to start a new chat, send a message, and receive a response. This ensured that Claude could quickly identify if the app had been left in a broken state, and immediately fix any existing bugs. If the agent had instead started implementing a new feature, it would likely make the problem worse.&lt;/p&gt;
    &lt;p&gt;Given all this, a typical session starts off with the following assistant messages:&lt;/p&gt;
    &lt;code&gt;[Assistant] I'll start by getting my bearings and understanding the current state of the project.
[Tool Use] &amp;lt;bash - pwd&amp;gt;
[Tool Use] &amp;lt;read - claude-progress.txt&amp;gt;
[Tool Use] &amp;lt;read - feature_list.json&amp;gt;
[Assistant] Let me check the git log to see recent work.
[Tool Use] &amp;lt;bash - git log --oneline -20&amp;gt;
[Assistant] Now let me check if there's an init.sh script to restart the servers.
&amp;lt;Starts the development server&amp;gt;
[Assistant] Excellent! Now let me navigate to the application and verify that some fundamental features are still working.
&amp;lt;Tests basic functionality&amp;gt;
[Assistant] Based on my verification testing, I can see that the fundamental functionality is working well. The core chat features, theme switching, conversation loading, and error handling are all functioning correctly. Now let me review the tests.json file more comprehensively to understand what needs to be implemented next.
&amp;lt;Starts work on a new feature&amp;gt;&lt;/code&gt;
    &lt;p&gt;Agent failure modes and solutions&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Problem&lt;/cell&gt;
        &lt;cell role="head"&gt;Initializer Agent Behavior&lt;/cell&gt;
        &lt;cell role="head"&gt;Coding Agent Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude declares victory on the entire project too early.&lt;/cell&gt;
        &lt;cell&gt;Set up a feature list file: based on the input spec, set up a structured JSON file with a list of end-to-end feature descriptions.&lt;/cell&gt;
        &lt;cell&gt;Read the feature list file at the beginning of a session. Choose a single feature to start working on.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude leaves the environment in a state with bugs or undocumented progress.&lt;/cell&gt;
        &lt;cell&gt;An initial git repo and progress notes file is written.&lt;/cell&gt;
        &lt;cell&gt;Start the session by reading the progress notes file and git commit logs, and run a basic test on the development server to catch any undocumented bugs. End the session by writing a git commit and progress update.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Claude marks features as done prematurely.&lt;/cell&gt;
        &lt;cell&gt;Set up a feature list file.&lt;/cell&gt;
        &lt;cell&gt;Self-verify all features. Only mark features as ‚Äúpassing‚Äù after careful testing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Claude has to spend time figuring out how to run the app.&lt;/cell&gt;
        &lt;cell&gt;Write an &lt;code&gt;init.sh&lt;/code&gt; script that can run the development server.&lt;/cell&gt;
        &lt;cell&gt;Start the session by reading &lt;code&gt;init.sh&lt;/code&gt;.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Future work&lt;/head&gt;
    &lt;p&gt;This research demonstrates one possible set of solutions in a long-running agent harness to enable the model to make incremental progress across many context windows. However, there remain open questions.&lt;/p&gt;
    &lt;p&gt;Most notably, it‚Äôs still unclear whether a single, general-purpose coding agent performs best across contexts, or if better performance can be achieved through a multi-agent architecture. It seems reasonable that specialized agents like a testing agent, a quality assurance agent, or a code cleanup agent, could do an even better job at sub-tasks across the software development lifecycle.&lt;/p&gt;
    &lt;p&gt;Additionally, this demo is optimized for full-stack web app development. A future direction is to generalize these findings to other fields. It‚Äôs likely that some or all of these lessons can be applied to the types of long-running agentic tasks required in, for example, scientific research or financial modeling.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Written by Justin Young. Special thanks to David Hershey, Prithvi Rajasakeran, Jeremy Hadfield, Naia Bouscal, Michael Tingley, Jesse Mu, Jake Eaton, Marius Buleandara, Maggie Vo, Pedram Navid, Nadine Yasser, and Alex Notov for their contributions.&lt;/p&gt;
    &lt;p&gt;This work reflects the collective efforts of several teams across Anthropic who made it possible for Claude to safely do long-horizon autonomous software engineering, especially the code RL &amp;amp; Claude Code teams. Interested candidates who would like to contribute are welcome to apply at anthropic.com/careers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;1. We refer to these as separate agents in this context only because they have different initial user prompts. The system prompt, set of tools, and overall agent harness was otherwise identical.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46081704</guid><pubDate>Fri, 28 Nov 2025 19:05:16 +0000</pubDate></item><item><title>The original ABC language, Python's predecessor (1991)</title><link>https://github.com/gvanrossum/abc-unix</link><description>&lt;doc fingerprint="2fe8520fe9cb456a"&gt;
  &lt;main&gt;
    &lt;p&gt;ABC was Python's most direct predecessor; I worked on it from around 1983-1986.&lt;/p&gt;
    &lt;p&gt;I downloaded these sources from cwi.nl, specifically the abc-unix tarball.&lt;/p&gt;
    &lt;p&gt;Another copy of the ABC sources lives in Luciano Ramalho's GitHub. I hope one day to compare the two trees and unify them.&lt;/p&gt;
    &lt;p&gt;Most files have 1991 as their latest modification time in the tar ball; a few have 1996 or 2021.&lt;/p&gt;
    &lt;p&gt;The old README file has build instructions.&lt;/p&gt;
    &lt;p&gt;The current sources assume a 32-bit system where int and pointers have the same size. I hope to eventually upgrade the source code to work on 64-bit systems too (where int is 32 bits and pointers are 64 bits).&lt;/p&gt;
    &lt;p&gt;CWI never put a license on ABC, but it says:&lt;/p&gt;
    &lt;p&gt;Copyright (c) Stichting Mathematisch Centrum, Amsterdam, 1988-2011.&lt;/p&gt;
    &lt;p&gt;I'll try to negotiate with Steven Pemberton eventually (hopefully MIT).&lt;/p&gt;
    &lt;p&gt;Eddy Boeve, Frank van Dijk, Leo Geurts, Timo Krijnen, Lambert Meertens, Steven Pemberton, Guido van Rossum.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leo Geurts, Lambert Meertens and Steven Pemberton, The ABC Programmer's Handbook, Prentice-Hall, Englewood Cliffs, New Jersey, 1990, ISBN 0-13- 000027-2.&lt;/item&gt;
      &lt;item&gt;Steven Pemberton, An Alternative Simple Language and Environment for PCs, IEEE Software, Vol. 4, No. 1, January 1987, pp. 56-64. http://www,cwi.nl/~steven/abc.html&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46082204</guid><pubDate>Fri, 28 Nov 2025 19:58:13 +0000</pubDate></item></channel></rss>