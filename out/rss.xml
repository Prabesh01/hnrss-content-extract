<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 08 Dec 2025 13:49:35 +0000</lastBuildDate><item><title>CATL expects oceanic electric ships in three years</title><link>https://cleantechnica.com/2025/12/05/catl-expects-oceanic-electric-ships-in-3-years/</link><description>&lt;doc fingerprint="f09b004a62d0af21"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;CATL Expects Oceanic Electric Ships in 3 Years&lt;/head&gt;
    &lt;p&gt;Support CleanTechnica's work through a Substack subscription or on Stripe.&lt;/p&gt;
    &lt;p&gt;News in batteries and electrification has been coming fast and furious lately. Recently, Su Yi, the head of CATL’s marine division, stated, “CATL’s marine business already covers inland rivers, lakes, and coastal waters, and is now advancing toward ocean-going applications.”&lt;/p&gt;
    &lt;p&gt;“In the near future — perhaps within the next three years — we will achieve pure-electric vessels navigating the open seas.”&lt;/p&gt;
    &lt;p&gt;CATL is determined to provide zero-carbon marine transportation. CATL has not been content to merely make batteries, but has dedicated efforts towards application in several sectors, including grid storage, passenger vehicles, and ships. CATL’s marine division has been in operation since 2017, expanding efforts in shipping. In 2023, it introduced a comprehensive battery replenishment solution including battery swapping, charging, and a cloud-based system providing shared mobile application of containerized power for optimal efficiency and economics. Those efforts resulted in several major products, containerized mobile power, high-voltage high-power charging systems, and the cloud information platform. This comprehensive system provides a seamless solution for electric ships.&lt;/p&gt;
    &lt;p&gt;Sharp-eyed readers may have noticed CATL’s recent discussion with shipping giant Maersk and that it has been busy collaborating with shipping partners. CATL has supplied batteries for over 900 vessels, including the Yangtze River Three Gorges 1 cruise ship, the world’s first pure electric ocean-going passenger ship, and the Qinggang Tug 1 tugboat.&lt;/p&gt;
    &lt;p&gt;Recent battery price drops indicate that possibilities for long range electric shipping are improving. The expected timeline for electric shipping dovetails with the expected timeline for sodium-ion battery (SIB) volume production and resulting cost reductions. The material costs of SIBs are expected to lower costs significantly, opening up applications and speeding up electrification. While passenger transport has been successfully electrified, with EVs surpassing ICE parity with battery costs well below $100/kWh enabling widespread adoption, ships can increasingly take advantage of lower-cost batteries for expanded electrification. Studies have shown that long-distance electric ships with up to 5,000 km of range can be successfully utilized using today’s battery capabilities, without significant weight and volume. CATL appears to be aware of this. Marine division head Su Yi notes CATL’s “full-spectrum growth” strategy, with goals to electrify maritime and aviation sectors. Sodium-ion technology may remove the last barrier to widespread maritime electrification.&lt;/p&gt;
    &lt;p&gt;Sign up for CleanTechnica's Weekly Substack for Zach and Scott's in-depth analyses and high level summaries, sign up for our daily newsletter, and follow us on Google News!&lt;/p&gt;
    &lt;p&gt;Have a tip for CleanTechnica? Want to advertise? Want to suggest a guest for our CleanTech Talk podcast? Contact us here.&lt;/p&gt;
    &lt;p&gt;Sign up for our daily newsletter for 15 new cleantech stories a day. Or sign up for our weekly one on top stories of the week if daily is too frequent.&lt;/p&gt;
    &lt;p&gt;CleanTechnica uses affiliate links. See our policy here.&lt;/p&gt;
    &lt;p&gt;CleanTechnica's Comment Policy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46176169</guid><pubDate>Sat, 06 Dec 2025 20:00:17 +0000</pubDate></item><item><title>Google Titans architecture, helping AI have long-term memory</title><link>https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/</link><description>&lt;doc fingerprint="3ef3d18a9c53810a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Titans + MIRAS: Helping AI have long-term memory&lt;/head&gt;
    &lt;p&gt;December 4, 2025&lt;/p&gt;
    &lt;p&gt;Ali Behrouz, Student Researcher, Meisam Razaviyayn, Staff Researcher, and Vahab Mirrokni, VP and Google Fellow, Google Research&lt;/p&gt;
    &lt;p&gt;We introduce the Titans architecture and the MIRAS framework, which allow AI models to work much faster and handle massive contexts by updating their core memory while it's actively running.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick links&lt;/head&gt;
    &lt;p&gt;The Transformer architecture revolutionized sequence modeling with its introduction of attention, a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.&lt;/p&gt;
    &lt;p&gt;The research community explored various approaches for solutions, such as efficient linear recurrent neural networks (RNNs) and state space models (SSMs) like Mamba-2. These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.&lt;/p&gt;
    &lt;p&gt;In two new papers, Titans and MIRAS, we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.&lt;/p&gt;
    &lt;p&gt;The MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Titans: Learning new context on the fly&lt;/head&gt;
    &lt;p&gt;An effective learning system requires distinct yet interconnected memory modules, mirroring the human brain's separation of short-term and long-term memory.&lt;/p&gt;
    &lt;p&gt;While attention mechanisms excel for precise, short-term memory, Titans introduces a novel neural long-term memory module, that, unlike the fixed-size vector or matrix memory in traditional RNNs, acts as a deep neural network (specifically, a multi-layer perceptron). This memory module provides significantly higher expressive power, allowing the model to summarize large volumes of information without losing important context. The model isn't simply taking notes; it's understanding and synthesizing the entire story.&lt;/p&gt;
    &lt;p&gt;Crucially, Titans doesn’t just passively store data. It actively learns how to recognize and retain important relationships and conceptual themes that connect tokens across the entire input. A key aspect of this ability is what we call the “surprise metric”. In human psychology, we know we quickly and easily forget routine, expected events but remember things that break the pattern — unexpected, surprising, or highly emotional events.&lt;/p&gt;
    &lt;p&gt;In the context of Titans, the "surprise metric" is the model detecting a large difference between what it currently remembers and what the new input is telling it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Low surprise: If the new word is "cat" and the model's memory state already expects an animal word, the gradient (surprise) is low. It can safely skip memorizing the word "cat" in its permanent long-term state.&lt;/item&gt;
      &lt;item&gt;High surprise: If the model's memory state is summarizing a serious financial report, and the new input is a picture of a banana peel (the unexpected event), the gradient (surprise) will be very high. This signals that the new input is important or anomalous, and it must be prioritized for permanent storage in the long-term memory module.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The model uses this internal error signal (the gradient) as a mathematical equivalent of saying, "This is unexpected and important!" This allows the Titans architecture to selectively update its long-term memory only with the most novel and context-breaking information, keeping the overall process fast and efficient.&lt;/p&gt;
    &lt;p&gt;Titans refines this mechanism by incorporating two critical elements:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Momentum: The model considers both "momentary surprise" (the current input) and "past surprise" (the recent context flow). This ensures relevant subsequent information is also captured, even if those tokens are not individually surprising.&lt;/item&gt;
      &lt;item&gt;Forgetting (weight decay): To manage the finite capacity of the memory when dealing with extremely long sequences, Titans employ an adaptive weight decay mechanism. This acts as a forgetting gate, allowing the model to discard information that is no longer needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;MIRAS: A unified view of sequence modeling&lt;/head&gt;
    &lt;p&gt;Every major breakthrough in sequence modeling — from modern transformers to the new, lightning-fast linear RNNs — is essentially the same thing under the hood: a highly complex associative memory module.&lt;/p&gt;
    &lt;p&gt;Accordingly, what makes MIRAS both unique and practical is the way it views AI modeling. Instead of seeing diverse architectures, it sees different methods of solving the same problem: efficiently combining new information with old memories without letting the essential concepts be forgotten.&lt;/p&gt;
    &lt;p&gt;MIRAS defines a sequence model through four key design choices:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Memory architecture: The structure that stores information (e.g., a vector, matrix, or a deep multi-layer perceptron, like in Titans).&lt;/item&gt;
      &lt;item&gt;Attentional bias: The internal learning objective the model optimizes that determines what it prioritizes.&lt;/item&gt;
      &lt;item&gt;Retention gate: The memory regularizer. MIRAS reinterprets "forgetting mechanisms" as specific forms of regularization that balance new learning against retaining past knowledge.&lt;/item&gt;
      &lt;item&gt;Memory algorithm: The optimization algorithm used to update the memory.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Transcending the mean squared error paradigm&lt;/head&gt;
    &lt;p&gt;Virtually all successful existing sequence models rely on mean squared error (MSE) or dot-product similarity for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.&lt;/p&gt;
    &lt;p&gt;MIRAS transcends this limitation by providing a generative framework to explore a more rich design space informed by the literature in optimization and statistics. This allows for the creation of novel architectures with non-Euclidean objectives and regularization.&lt;/p&gt;
    &lt;p&gt;Using MIRAS, we created three specific attention-free models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;YAAD: We designed this MIRAS variant to be less sensitive to major errors or "outliers" (like a single typo in a large document). It uses a gentler math penalty (Huber loss) for mistakes, so it doesn't overreact to one-off issues. This makes the model more robust when the input data is messy or inconsistent.&lt;/item&gt;
      &lt;item&gt;MONETA: This model explores the use of more complex and strict mathematical penalties (called generalized norms). It investigates whether using these more disciplined rules for both what the model attends to and what it forgets can lead to a more powerful and stable long-term memory system overall.&lt;/item&gt;
      &lt;item&gt;MEMORA: This model focuses on achieving the best possible memory stability by forcing its memory to act like a strict probability map. By using this constraint, it ensures that every time the memory state is updated, the changes are controlled and balanced. This guarantees a clean, stable process for integrating new information.Virtually all successful existing sequence models rely on mean squared error (MSE) or dot-product similarity for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Experiments and results&lt;/head&gt;
    &lt;p&gt;We rigorously compared Titans along with MIRAS variants (YAAD, MONETA, MEMORA) against leading architectures, including Transformer++, Mamba-2, and Gated DeltaNet. We further validated versatility by testing Titans on genomic modeling (DNA) and time-series forecasting, proving the architecture generalizes effectively beyond text.&lt;/p&gt;
    &lt;p&gt;Across both standard language modeling datasets (C4, WikiText) and zero-shot reasoning tasks (HellaSwag, PIQA), our models consistently demonstrated higher accuracy and perplexity (a measure of how surprised an LLM is when looking at a piece of text).&lt;/p&gt;
    &lt;head rend="h3"&gt;The power of deep memory&lt;/head&gt;
    &lt;p&gt;Ablation studies clearly show that the depth of the memory architecture is crucial. When comparing long-term memory modules of the same size but different depths, modules with deeper memories consistently achieve lower perplexity in language modeling. Furthermore, they exhibit better scaling properties, maintaining performance as the sequence length increases significantly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Language modeling and efficiency&lt;/head&gt;
    &lt;p&gt;In language modeling and commonsense reasoning tasks, Titans architectures outperform state-of-the-art linear recurrent models (such as Mamba-2 and Gated DeltaNet) and Transformer++ baselines of comparable sizes. The novel MIRAS variants (MONETA, YAAD, MEMORA) also achieve improved performance compared to these baselines, validating the benefit of exploring robust, non-MSE optimization mechanisms. Importantly, these models maintain efficient, parallelizable training and fast linear inference speeds.&lt;/p&gt;
    &lt;head rend="h3"&gt;Extreme long-context recall&lt;/head&gt;
    &lt;p&gt;The most significant advantage of these new architectures is their ability to handle extremely long contexts. This is highlighted in the BABILong benchmark, a task requiring reasoning across facts distributed in extremely long documents. In this challenging setting, Titans outperforms all baselines, including extremely large models like GPT-4, despite having many fewer parameters. Titans further demonstrates the capability to scale effectively to context window sizes larger than 2 million tokens.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The introduction of Titans and the MIRAS framework marks a significant advancement in sequence modeling. By employing deep neural networks as memory modules that learn to memorize as data is coming in, these approaches overcome the limitations of fixed-size recurrent states. Furthermore, MIRAS provides a powerful theoretical unification, revealing the connection between online optimization, associative memory, and architectural design. By moving beyond the standard Euclidean paradigm, this research opens the door to a new generation of sequence models that combine the efficiency of RNNs with the expressive power needed for the era of long-context AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46181231</guid><pubDate>Sun, 07 Dec 2025 12:23:45 +0000</pubDate></item><item><title>The Anatomy of a macOS App</title><link>https://eclecticlight.co/2025/12/04/the-anatomy-of-a-macos-app/</link><description>&lt;doc fingerprint="2e8a02cb61b5172c"&gt;
  &lt;main&gt;
    &lt;p&gt;Programs running in windowing environments, applications as we used to know them, have more complicated requirements than those run from a command line. Rather than embed all the resources they require for windows, menus and the rest in a single file, Mac OS broke new ground by putting those into resources stored in the app’s resource fork.&lt;/p&gt;
    &lt;p&gt;This is QuarkXPress version 4.11 from around 2000, with its resources displayed in the resource editor ResEdit. Executable code was also stored in CODE resources, and every file contained type and creator information to support the illusions created by the Finder.&lt;/p&gt;
    &lt;head rend="h4"&gt;Mac OS X&lt;/head&gt;
    &lt;p&gt;When Mac OS X was designed, it switched to the bundle structure inherited from NeXTSTEP. Instead of this multitude of resources, apps consisted of a hierarchy of directories containing files of executable code, and those with what had in Mac OS been supporting resources. Those app bundles came to adopt a standard form, shown below.&lt;/p&gt;
    &lt;p&gt;The bundle name has the extension .app, and contains a single directory Contents. Within that, the executable code is in the MacOS directory, which may contain both the main executable for the GUI app and any bundled command tools provided. Another directory contains Resources, including the app’s custom icon, and components of its GUI. In some apps, there’s another directory of Frameworks containing dylibs (libraries).&lt;/p&gt;
    &lt;p&gt;There are also two important files, Info.plist and PkgInfo. The latter contains the same type and creator information inherited from Classic Mac OS, and apparently isn’t mandatory although it appears universal. The information property list is essential, as it specifies the names of the executable and its icon file in Resources, the minimum version of macOS required, type declarations of the app’s documents, version numbers, and more.&lt;/p&gt;
    &lt;p&gt;When running a command tool in macOS, its Mach-O executable can be launched by &lt;code&gt;launchd&lt;/code&gt;, one of whose purposes is to run code. Launching an app is more demanding, although the app’s executable is still launched by &lt;code&gt;launchd&lt;/code&gt;. Before that can happen, macOS starts the launch process using LaunchServices and RunningBoard, which rely on information obtained from Info.plist and other components in the app bundle.&lt;/p&gt;
    &lt;head rend="h4"&gt;macOS&lt;/head&gt;
    &lt;p&gt;This structure remained stable until the introduction of code signatures in Mac OS X 10.5 Leopard in 2007. Accommodating those added a directory named _CodeSignature containing the signature in a CodeResources file. That includes code directory hashes (CDHashes) to check the integrity of the contents of the app bundle. Apps distributed by the App Store include a store receipt in another directory, _MASReceipt. Since 2018, when Apple introduced notarization, the ‘ticket’ issued by Apple can be ‘stapled’ into the app bundle as the file CodeResources.&lt;/p&gt;
    &lt;p&gt;Many apps come with additional items that might in the past have been installed by them in their Library/Application Support folders and elsewhere, but are now included in the app bundle. These can include the following directories:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Library, containing folders of LaunchDaemons and LoginItems that would previously have been installed in either the main Library folder, or that in the user’s Home folder;&lt;/item&gt;
      &lt;item&gt;XPCServices, for executable code that the app uses to provide specific services;&lt;/item&gt;
      &lt;item&gt;Plugins, for some types of app extension (Appex);&lt;/item&gt;
      &lt;item&gt;Extensions, for other types of app extension, including app intents.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You may also come across other components, including a version.plist in Apple’s apps.&lt;/p&gt;
    &lt;p&gt;This centralisation of components in the app bundle has brought several benefits. Being self-contained, apps are easier to install and update, and cleaner to remove. Their components are less likely to go missing, and most of all they’re held within the protection of the app’s signature and notarisation, an important improvement in security.&lt;/p&gt;
    &lt;p&gt;Assembling these into a diagram shows how the anatomy of an app has grown over the last few years.&lt;/p&gt;
    &lt;p&gt;Components shown in pale yellow are either mandatory or essentially universal. Those shown in green are found in apps distributed through the App Store, while that shown in blue is the stapled notarisation ticket (optional). You will also see additional folders and components such as Automator workflows, scripts, and others.&lt;/p&gt;
    &lt;p&gt;There is no difference in structure between apps built for current Intel and Arm architectures. That’s because binaries in the MacOS folder (and executable code in other directories like Frameworks, XPCServices and Plugins) contain platform-specific code in a single Mach-O executable. Thus, an app that’s Universal and runs native on both architectures includes code for both in its single ‘fat’ code file, and they even have separate signatures stored within common files.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46181268</guid><pubDate>Sun, 07 Dec 2025 12:31:53 +0000</pubDate></item><item><title>I wasted years of my life in crypto</title><link>https://twitter.com/kenchangh/status/1994854381267947640</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46181371</guid><pubDate>Sun, 07 Dec 2025 12:57:59 +0000</pubDate></item><item><title>Dollar-stores overcharge customers while promising low prices</title><link>https://www.theguardian.com/us-news/2025/dec/03/customers-pay-more-rising-dollar-store-costs</link><description>&lt;doc fingerprint="be06198a658e03f9"&gt;
  &lt;main&gt;
    &lt;p&gt;On a cloudy winter day, a state government inspector named Ryan Coffield walked into a Family Dollar store in Windsor, North Carolina, carrying a scanner gun and a laptop.&lt;/p&gt;
    &lt;p&gt;Inside the store, which sits along a three-lane road in a county of peanut growers and poultry workers, Coffield scanned 300 items and recorded their shelf prices. He carried the scanned bar codes to the cashier and watched as item after item rang up at a higher price.&lt;/p&gt;
    &lt;p&gt;Red Baron frozen pizzas, listed on the shelf at $5, rang up at $7.65. Bounty paper towels, shelf price $10.99, rang up at $15.50. Kellogg’s Frosted Flakes, Stouffer’s frozen meatloaf, Sprite and Pepsi, ibuprofen, Klondike Minis – shoppers were overpaying for all of them. Pedigree puppy food, listed at $12.25, rang up at $14.75.&lt;/p&gt;
    &lt;p&gt;All told, 69 of the 300 items came up higher at the register: a 23% error rate that exceeded the state’s limit by more than tenfold. Some of the price tags were months out of date.&lt;/p&gt;
    &lt;p&gt;The January 2023 inspection produced the store’s fourth consecutive failure, and Coffield’s agency, the state department of agriculture &amp;amp; consumer services, had fined Family Dollar after two previous visits. But North Carolina law caps penalties at $5,000 per inspection, offering retailers little incentive to fix the problem. “Sometimes it is cheaper to pay the fines,” said Chad Parker, who runs the agency’s weights-and-measures program.&lt;/p&gt;
    &lt;p&gt;The dollar-store industry, including Family Dollar and its larger rival, Dollar General, promises everyday low prices for household essentials. But an investigation by the Guardian found that the prices listed on the shelves at these two chains often don’t materialize at checkout – in North Carolina and around the country. As the cost of living soars across America, the customers bearing the burden are those who can least afford it – customers who often don’t even notice they’re overpaying.&lt;/p&gt;
    &lt;p&gt;These overcharges are widespread.&lt;/p&gt;
    &lt;p&gt;Dollar General stores have failed more than 4,300 government price-accuracy inspections in 23 states since January 2022, a Guardian review found. Family Dollar stores have failed more than 2,100 price inspections in 20 states over the same time span, the review found.&lt;/p&gt;
    &lt;p&gt;Among these thousands of failed inspections, some of the biggest flops include a 76% error rate in October 2022 at a Dollar General in Hamilton, Ohio; a 68% error rate in February 2023 at a Family Dollar in Bound Brook, New Jersey; and a 58% error rate three months ago at a Family Dollar in Lorain, Ohio.&lt;/p&gt;
    &lt;p&gt;Many of the stores that failed state or local government checks were repeat violators. A Family Dollar in Provo, Utah, flunked 28 inspections in a row – failures that included a 48% overcharge rate in May 2024 and a 12% overcharge rate in October 2025.&lt;/p&gt;
    &lt;p&gt;The chains’ pricing disparities are drawing increasing attention. In May, Arizona’s attorney general announced a $600,000 settlement to resolve a consumer-fraud investigation against Family Dollar. In October, Colorado’s attorney general settled with Dollar General for $400,000 after its stores failed 15 out of 23 state inspections. Dollar General has also settled with New Jersey, Vermont and Wisconsin, and both companies have settled with Ohio.&lt;/p&gt;
    &lt;p&gt;Linda Davis, a 64-year-old Family Dollar shopper in Dayton, Ohio, called the state attorney general’s office in February after walking home from the dollar store and discovering that 12 of her 23 purchases had rung up incorrectly. “I’m adding it up in my head as I’m shopping,” she told the Guardian. “But I was way off and I didn’t know why … I thought: where did I miscalculate? I’ve [only] got so much cash on me.”&lt;/p&gt;
    &lt;p&gt;Davis, who lives on social security, said she could shop elsewhere, but that would involve paying for a bus ride. “I don’t have money like that,” she said.&lt;/p&gt;
    &lt;p&gt;Both Family Dollar and Dollar General declined interview requests and did not answer detailed lists of questions from the Guardian. Instead, both sent the Guardian brief statements.&lt;/p&gt;
    &lt;p&gt;“At Family Dollar, we take customer trust seriously and are committed to ensuring pricing accuracy across our stores,” the company said. “We are currently reviewing the concerns raised and working to better understand any potential discrepancies. We continue to be focused on providing a consistent and transparent shopping experience.”&lt;/p&gt;
    &lt;p&gt;Dollar General said it was “committed to providing customers with accurate prices on items purchased in our stores, and we are disappointed any time we fail to deliver on this commitment”. In one court case in Ohio, Dollar General’s lawyers argued that “it is virtually impossible for a retailer to match shelf pricing and scanned pricing 100% of the time for all items. Perfection in this regard is neither plausible nor expected under the law.”&lt;/p&gt;
    &lt;p&gt;The Guardian’s examination of inspection failures by the two chains was based on record requests to 45 states and more than 140 counties and cities in New York, Ohio and California, along with court documents and public databases.&lt;/p&gt;
    &lt;p&gt;In nearly half of US states, information about whether customers are being overcharged was limited or unavailable. Many states do little or nothing to monitor retail stores’ pricing practices. Some, like Maryland, Idaho and Washington, do no random inspections, responding only to consumer complaints. Illinois, South Carolina and others don’t inspect at all. In 2020, auditors in Kansas revealed that these inspections were a low priority in many states. “Consumers can check price accuracy themselves,” they wrote.&lt;/p&gt;
    &lt;p&gt;Even in states with tougher enforcement, financial penalties don’t always solve the problem: in the 23 months after Dollar General agreed in November 2023 to pay Wisconsin $850,000, its stores failed 31% of their price inspections. During the same period, Wisconsin’s Family Dollar stores failed 30% of their state inspections.&lt;/p&gt;
    &lt;p&gt;According to industry watchers, employees and lawsuits, overcharges often stem from labor practices within the dollar-store sector. When a company changes prices, the registers are updated automatically. But the shelf prices are not: someone needs to remove the old labels manually and replace them with new ones. In an industry known for minimal staffing, workers don’t always have time to put up the new shelf tags.&lt;/p&gt;
    &lt;p&gt;In many instances, customers may not notice that they are being charged more than what’s listed on the shelf. If they notice at the register, they may decide to put those items back – or ask a store employee to honor the shelf price.&lt;/p&gt;
    &lt;p&gt;Dollar General, in its statement, said its store teams “are empowered to correct the matter on the spot”. But customers and current and former employees said that while some dollar stores will correct the price, others refuse to make fixes at the register – and turn away customers who return later and request a refund.&lt;/p&gt;
    &lt;p&gt;“Overcharging even by a small amount per item can strain a really tight budget,” said Elizabeth M Harris, acting director of the New Jersey division of consumer affairs. “If you’ve ever gone into any store … with a child like I have, there’s chaos at the checkout counter and you’re not really paying attention.” With items being rung up quickly, she added, “consumers are trusting that the retailer is actually charging them the price that’s displayed.”&lt;/p&gt;
    &lt;p&gt;Her state settled in 2023 with Dollar General for $1.2m after finding more than 2,000 items rung up as overcharges across 58 stores.&lt;/p&gt;
    &lt;p&gt;Even if the overcharges paid by dollar-store customers are accidental, they still reflect the industry’s decision not to correct a problem it has known about for years, according to Kennedy Smith, a researcher at the non-profit Institute for Local Self-Reliance, which works to protect communities from negative impacts of big corporations.&lt;/p&gt;
    &lt;p&gt;“If they’re called on it, they’ll say, ‘Oh yeah, our mistake,’” Kennedy said. “Until they’re called on it, they’re happy to let those scanner errors bring in the millions.”&lt;/p&gt;
    &lt;head rend="h2"&gt;‘The cheap stuff’&lt;/head&gt;
    &lt;p&gt;When consumers feel economic pain, as they do now thanks to rising costs exacerbated by tariffs, price gouging and other inflationary pressures, one place they turn to are dollar stores. These one-stop centers for inexpensive food, clothing and housewares tend to sell in small quantities, one $1 chicken-noodle-soup can at a time. And they are relatively easy to get to: 75% of Americans live within 5 miles of a Dollar General, according to the company.&lt;/p&gt;
    &lt;p&gt;The industry’s largest player is flourishing. Todd Vasos, the CEO of Dollar General, told investors in August that his company’s quarterly sales had increased 5% over the same period last year. Some of that growth, he said, came from middle- and higher-income shoppers tightening their belts. But the company’s low-income “core customers” were spending more at the chain too.&lt;/p&gt;
    &lt;p&gt;Those customers have been the industry’s niche from the beginning. When a 48-year-old former tobacco farmer and traveling salesman named James Luther Turner opened JL Turner and Son Wholesale Dry Goods, Shoes, Notions and Hosiery in Scottsville, Kentucky, in 1939, his mission was “to sell the cheap stuff to the poor folks”. (Someone else had cornered the market on “selling the good stuff” to Scottsville’s rich folks.)&lt;/p&gt;
    &lt;p&gt;By 1955, Turner and his eldest son, Hurley Calister “Cal” Turner Sr, were overseeing 36 stores in small southern towns. Cal Sr decided that year to co-opt the “Dollar Days” sales at big department stores and to open outlets featuring a single low price of $1. Adopting a name that nodded to the general store, he designed a bold black-and-yellow sign and that June christened the first Dollar General in Springfield, Kentucky.&lt;/p&gt;
    &lt;p&gt;Dollar General now operates over 20,000 stores in 48 states – more than any other retailer of any kind in the US. (It has long since abandoned its $1 price limit.) Though it has more than 195,000 employees and net sales of $40.6bn, the company still calls itself “America’s neighborhood general store”.&lt;/p&gt;
    &lt;p&gt;Family Dollar began in 1959 in Charlotte, North Carolina, and now operates 8,000 stores nationwide. For most of the past decade, it was owned by yet another chain, Dollar Tree, but the two brands divorced last summer.&lt;/p&gt;
    &lt;p&gt;What Dollar General and Family Dollar have in common is a conspicuous presence in places that don’t offer a lot of other retail: low-income urban neighborhoods and rural towns like Windsor.&lt;/p&gt;
    &lt;p&gt;A predominantly Black county seat of 3,400 on North Carolina’s coastal plain, Windsor used to be a retail hub. “All the streets were full on a weekend,” recalled Russell Parker, a 66-year-old retired pilot. “There were people everywhere, people playing music.” And people spending money: at the fish market, the cobbler, the independent groceries, the automotive-supply store. But today Windsor’s downtown – like many rural main streets – is pocked with empty storefronts. The town never fully recovered from Hurricane Floyd, in 1999. “Every young person that graduates from high school gets on the first thing smokin’ to somewhere else,” Parker said.&lt;/p&gt;
    &lt;p&gt;One supermarket remains on the edge of town. Shopping for clothes often means driving to the next county, at least for those who drive. But Windsor does have three stores that help fill the gap: a Dollar General and two Family Dollars.&lt;/p&gt;
    &lt;p&gt;At the Family Dollar that failed multiple inspections, some regulars remain vigilant. Chris Outlaw, a 54-year-old hemodialysis technician, shops there because it’s near his house and workplace. Experience has taught him to buy only a few items at once and to examine his receipts. Not all his neighbors do the same. “I’ve seen people in there with baskets full,” he said. “You can just imagine how much of that stuff didn’t ring out right, and they had so much they couldn’t catch it.”&lt;/p&gt;
    &lt;head rend="h2"&gt;‘Big old savings’&lt;/head&gt;
    &lt;p&gt;Customers walking into Dollar General stores are often greeted by a bright yellow sign blaring “Hello, Low Prices”– and by as many as 10,000 items cramming shelves and, often, cluttering the aisles.&lt;/p&gt;
    &lt;p&gt;“They will send you more than what you need of any product,” said Stephanie, a former lead sales associate in Louisiana. “Your shelf can only hold 10 Glade air fresheners, right? But they will send you 50.”&lt;/p&gt;
    &lt;p&gt;Rarely is there enough staffing, current and former employees say, to complete all of the tasks expected of them, including stocking shelves, ringing up sales, looking out for shoplifters, mopping floors – and updating price changes and sales stickers.&lt;/p&gt;
    &lt;p&gt;More than two dozen current and former employees of the chain in 15 states interviewed by the Guardian agreed that price discrepancies are the byproduct of the company’s employment policies. (Most, including Stephanie, spoke on the condition of anonymity because of fear of retaliation.)&lt;/p&gt;
    &lt;p&gt;Often there are only one or two people on duty. “You’re lucky if you get to work two to four hours of your eight- to 13-hour shift with another human being,” a former assistant manager in Illinois said.&lt;/p&gt;
    &lt;p&gt;Every Tuesday, employees are supposed to print and post hundreds of shelf stickers representing price changes already updated in the computer system. On Saturdays, stacks of sales stickers arrive; often, workers are expected to remove all the previous week’s stickers by 5pm and put up new stickers – as many as 1,000 of them – before closing up that night. Stickers fail to get put up, they fall off easily, and they are confusing, with some sales instant and others linked to coupons. “I threw away tags sometimes, to keep me or a co-worker out of trouble,” Stephanie admitted.&lt;/p&gt;
    &lt;p&gt;A former store manager at a Dollar General in Connecticut noted that many of his customers were poor or disabled enough that they got by on public assistance. “I didn’t want people to get screwed over, but I knew that it was happening,” he said. “If I’m in the store, I’m gonna try to do the best I can for them. But at the end of the day, they’re still probably gonna get overcharged for a few things.”&lt;/p&gt;
    &lt;p&gt;Dollar General, in its statement, said it schedules time each week for “price change execution”, among other measures to ensure accuracy.&lt;/p&gt;
    &lt;p&gt;Ten current and former employees in eight states claimed that – along with allowing pricing errors caused by understaffing and overstocking – some Dollar General stores engage in a tactic designed to fool customers: special sales that don’t actually lower the price of an item. A manager from Florida, for example, sent the Guardian two photos of price stickers for Café Bustelo ground coffee. In the first photo, a sticker said “SALE” in white block letters against a red background. It advertised a markdown from $7.95 to $6.50. In the second photo, the top sticker had been peeled away to show the original price: $6.50.&lt;/p&gt;
    &lt;p&gt;A sales associate from Illinois sent photos showing cutlery with what he said was a fake original price of $8.50. “It’s trying to say that you’re making this big old savings by buying this item here,” explained the employee, “when it’s actually always been $6.95.”&lt;/p&gt;
    &lt;p&gt;Dollar General declined to comment on these workers’ claims.&lt;/p&gt;
    &lt;head rend="h2"&gt;‘We have little choice’&lt;/head&gt;
    &lt;p&gt;When the Ohio attorney general, Dave Yost, sued Dollar General in 2022, he submitted 114 pages of customer complaints as part of the case.&lt;/p&gt;
    &lt;p&gt;One of them came from Melanie Hutzler, who lives in Canton without a car and whose mobility is limited by arthritis and multiple sclerosis. Hutzler, 51, relies on government food assistance and said she was cautious about spending money. At the time of her complaint, she could reach two food stores on foot. Getting to the Save A Lot grocery required crossing a busy road, but getting to a Dollar General did not.&lt;/p&gt;
    &lt;p&gt;“Every single time we went into that store, something would ring up wrong,” she told the Guardian. “They never had a manager there that would fix the prices.” Hutzler said she would walk the cashier over to the shelf and point out the listed price, only to be told, “There’s nothing we can do about it.”&lt;/p&gt;
    &lt;p&gt;Other Ohioans expressed similar frustrations. “My 87-year-old mother and I have frequented Dollar General for years, and there have been innumerable times we have made purchases that were well higher than advertised,” wrote Robert Hevlin of Dayton. “My mother and I have literally lost thousands over the years with this company, but both of us being on social security, we have little choice in where we shop.”&lt;/p&gt;
    &lt;p&gt;In September 2023, Yost reached a $1m settlement with Dollar General, which he said had error rates at some stores that ran as high as 88%. In February 2024, he announced a $400,000 settlement with Family Dollar to resolve similar allegations. Most of that money went to charitable organizations that distribute food and personal-care items.&lt;/p&gt;
    &lt;p&gt;Both chains agreed in the settlements to tighten their pricing practices. Yost’s office continues to receive complaints. A Dollar General customer in Garfield Heights said in February that he was charged $6.35 for a carton of eggs with a shelf sticker of $5.10, but the “cashier was too busy having a personal call on her cellphone to address the price discrepancy”. The same month, a Family Dollar shopper in Genoa reported being charged $2.65 for cough medicine listed on the shelf at $1.50. “I was told by the cashier that there was nothing that could be done about it,” the complaint said.&lt;/p&gt;
    &lt;p&gt;Over in Missouri, state officials are pursuing a lawsuit that accuses Dollar General of “deceptive” pricing practices. The suit, filed in 2023, says 92 of the 147 stores the state checked failed their inspections, with discrepancies as high as $6.50 an item.&lt;/p&gt;
    &lt;p&gt;The companies declined to comment on these state lawsuits.&lt;/p&gt;
    &lt;p&gt;Dollar General has also been hit with private lawsuits, including several filed by its shareholders. In a document filed in August in federal court in Nashville, lawyers for Dollar General investors argued that understaffing, poor inventory control and overcharging were all interrelated.&lt;/p&gt;
    &lt;p&gt;The investors allege that the company deceived them by portraying itself as financially sound. In truth, the court filing says, “Dollar General’s inventory management processes were broken, which caused a massive bloat of excess product to clog the company at both its distribution centers and stores, and its workforce had been slashed.” These problems gave rise to price discrepancies and other “dire consequences”, the court filing asserts.&lt;/p&gt;
    &lt;p&gt;The filing includes the stories of 36 former employees who claimed direct knowledge that Dollar General managers and executives knew about the problems. Several reported notifying the top leadership directly. “All the prices were off in the stores,” said one of those ex-employees, a manager who monitored inventory levels in Ohio and Pennsylvania. She claimed to know firsthand, based on calls she participated in, that company vice-presidents and regional directors were aware of the “huge” price mismatches.&lt;/p&gt;
    &lt;p&gt;Dollar General, in response, said that the testimony of a handful of ex-workers does not prove that it misled investors. In their “years-long search for fraud”, the company’s lawyers claimed, the shareholders “came up empty”.&lt;/p&gt;
    &lt;p&gt;Earlier this year, a federal judge in New Jersey halted a class-action lawsuit against Dollar General filed by a shopper who said he was overcharged for groceries. Dollar General argued that when customers create accounts – for example, by downloading the company’s mobile app – they agree to use arbitration to resolve disputes and forfeit the right to file class-action suits. The judge agreed.&lt;/p&gt;
    &lt;p&gt;This victory for Dollar General threw up an obstacle for customers seeking justice. “Who’s going to bring a consumer arbitration with a $225 filing fee over a 50-cent overcharge?” asked Marc Dann, a former Ohio attorney general whose law firm filed the New Jersey case. “They’ve essentially closed the door to the courthouse to people.”&lt;/p&gt;
    &lt;p&gt;Dann’s firm did reach a settlement with Dollar General in another case this fall, though the details have not been made public.&lt;/p&gt;
    &lt;head rend="h2"&gt;‘This endless cycle’&lt;/head&gt;
    &lt;p&gt;The dollar-store chains describe themselves as mission-driven companies. “Our stores are conveniently located in neighborhoods, and often in ‘food deserts’ where other stores choose not to locate,” Family Dollar says on its website. Dollar General takes pride in offering value to families who, according to CEO Vasos, “have had to sacrifice even on the necessities”.&lt;/p&gt;
    &lt;p&gt;The industry’s critics say the cause and effect are reversed. “Dollar stores are often seen as a symptom of economic distress,” said the Institute for Local Self-Reliance’s co-executive director, Stacy Mitchell. “What we found is that they’re, in fact, a cause of it.” Sometimes, she said, a chain dollar store will open near an independent grocer and skim off enough of its business that it is forced to close. That limits the availability of fresh produce and forces shoppers to buy more packaged and processed foods.&lt;/p&gt;
    &lt;p&gt;In a statement, Dollar General said its stores often “operate along with local grocers and business owners to collectively meet customers’ needs”. It added that 7,000 of its 20,000 stores sell fresh produce and that the company also partners with local food banks “to further help nourish our neighbors in need”.&lt;/p&gt;
    &lt;p&gt;The people enduring the effects of hollowed-out local economies – and getting hit with overcharges at dollar-store chains – include residents of Essex county, New York. The county, tucked among the stately pines of the Adirondack Mountains, has a population of 37,000. It has five Dollar Generals and two Family Dollars. All seven regularly fail pricing-accuracy tests. The Dollar General in Port Henry, which sits on the shores of Lake Champlain, was fined $103,550 for failed inspections between November 2022 and June 2025.&lt;/p&gt;
    &lt;p&gt;Over the course of seven inspections, 279 out of 700 tested items were overcharges – a combined error rate of just under 40%. One inspection yielded a 78% error rate, including overcharges on Flintstones vitamins, Peter Pan peanut butter and Prego pasta sauce.&lt;/p&gt;
    &lt;p&gt;The Port Henry store is 5 miles from the Mineville Dollar General, which occupies a lonely stretch of country road across from an auto-repair shop with spare parts littering its lawn. Down the block, an abandoned church presides over a stretch of grass that looks like it hasn’t been mown for years.&lt;/p&gt;
    &lt;p&gt;Aside from a whiskey warehousing operation and a health center, opportunities for employment are limited. The high-security prison built atop the iron mine for which Mineville is named closed in 2022, taking 100 jobs with it.&lt;/p&gt;
    &lt;p&gt;The local playground is littered with trash, cigarette butts and the occasional syringe. The town “is nice from the outside”, said Katelyn Miller, a 26-year-old Port Henry resident who lives with her mother, six-year-old daughter and two-year-old son. But “you hear about a lot of crack-den places, like blowing up or getting busted.’” Drug use is rampant in the county, which is 92% white. “Everybody around here seems to be on pain meds or buying someone else’s, because they’re also working themselves to death.”&lt;/p&gt;
    &lt;p&gt;When it comes to grocery shopping near Miller’s home, the choice is between the two Dollar Generals and a gas station/convenience store. “We live in a food desert,” she said, “even though you would think living in all this farmland, we would have more access.”&lt;/p&gt;
    &lt;p&gt;There is a Walmart 30 minutes away, in Fort Ticonderoga. Miller said she recently bought salmon there only to arrive home and discover that the $20 piece of fish had gone bad. “So I had to go to Dollar General and get the Stouffer’s,” she said, adding that she feels “caught in this endless cycle of never having food that will nourish me and my family, and instead having to get 2,000 grams of sodium because at least it has meat”.&lt;/p&gt;
    &lt;p&gt;The region’s economic straits put regulators in a bind when it comes to overcharges. Daniel Woods, the county’s director of weights and measures, said in 2023 that he didn’t always assess the full penalty on violators. “We’re not trying to put people out of business,” he told a local newspaper. “In some towns that’s their [only] store. I don’t want to pull that away from people, but at the same time, I’m trying to fix the problem.”&lt;/p&gt;
    &lt;head rend="h2"&gt;On the way out&lt;/head&gt;
    &lt;p&gt;When Coffield, the North Carolina inspector, visited the Windsor Family Dollar in April 2023, the pricing issues seemed to have abated. Of the 300 items he scanned, he only found five overcharges: incontinence pads, laundry sanitizer, two coffee products and, again, Red Baron pizza. With an error rate below the state’s 2% threshold, the store passed its inspection, and it did so again in November 2024.&lt;/p&gt;
    &lt;p&gt;But customers still reported problems. Chris Outlaw, the hemodialysis technician, stopped by the Family Dollar earlier this year and noticed a sale: a $1.25 savings on five bags of Cheez Doodles. He bought them but discovered on the way out that he had been charged the regular price. The manager refused to refund the difference, Outlaw said, because he had already walked through the exit door.&lt;/p&gt;
    &lt;p&gt;Another time, he saw some discounted socks near the counter that he thought would make good Christmas gifts. “I was like, ‘Oh, I like these socks, so I’ll probably give them to somebody,’” he recalled. “Nice, plushy socks.” But they rang up at a higher price, so he left the store without them.&lt;/p&gt;
    &lt;p&gt;During a visit in August, a Guardian reporter found the Windsor Family Dollar closed for much of the afternoon. “Be Back Soon!” read a handwritten sign taped to the door. Two waiting customers said that they frequently paid prices higher than the shelf listing, including a cook whose nearby restaurant buys some of its ingredients there. “It is aggravating,” she said. “Very aggravating.”&lt;/p&gt;
    &lt;p&gt;Workers reopened the doors after a few hours. Inside, carts of unshelved dog food and other merchandise blocked the aisles. The Guardian compared the prices of 15 items. Two of them rang up higher than advertised, including a frying pan set that was $10 on the shelf and $12 at the register. Though the cashier offered to honor the lower prices, that was still an error rate of 13% – more than six times the state’s standard.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46181962</guid><pubDate>Sun, 07 Dec 2025 14:37:21 +0000</pubDate></item><item><title>I failed to recreate the 1996 Space Jam website with Claude</title><link>https://j0nah.com/i-failed-to-recreate-the-1996-space-jam-website-with-claude/</link><description>&lt;doc fingerprint="3f2d0afd3db6ce35"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I failed to recreate the 1996 Space Jam Website with Claude&lt;/head&gt;
    &lt;p&gt;— claude, ai, space jam, web development, computer vision — 14 min read&lt;/p&gt;
    &lt;p&gt;Link to the Hacker News post. Thanks everybody for all the engagement!&lt;/p&gt;
    &lt;p&gt;Can Claude Recreate the 1996 Space Jam Website? No. Or at least not with my prompting skills. Note: please help, because I'd like to preserve this website forever and there's no other way to do it besides getting Claude to recreate it from a screenshot. Believe me, I'm an engineering manager with a computer science degree. Please please please help 😞&lt;/p&gt;
    &lt;p&gt;Final note: I use "he" to refer to Claude, which Josh finds ridiculous.&lt;/p&gt;
    &lt;head rend="h2"&gt;Space Jam, 1996&lt;/head&gt;
    &lt;p&gt;For those who don't know, Warner Bros keeps this anachronistic website online that was released in 1996 to accompany the Space Jam movie.&lt;/p&gt;
    &lt;p&gt;It's a classic example of early web era design. Simple, colorful, and sparks joy. We're going to find out if we can get Claude to recreate it using only a screenshot.&lt;/p&gt;
    &lt;head rend="h2"&gt;Set Up&lt;/head&gt;
    &lt;p&gt;At a minimum, I'm providing Claude:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a screenshot of the website&lt;/item&gt;
      &lt;item&gt;all of the assets the website uses&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To track Claude's inner monologue and actual API calls, I set up a man-in-the-middle proxy to capture the full conversation between Claude Code and Anthropic's API. This logs everything: user prompts, Claude's responses, tool invocations (Read, Write, Bash commands), etc. Each attempt generates a &lt;code&gt;traffic.log&lt;/code&gt; file with the raw API traffic, which I then parse for easier analysis.&lt;/p&gt;
    &lt;p&gt;Edit:I used Opus 4.1 for this investigation. Thanks to anorwell for pointing out I forgot to add the model.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1: Claude the Realist&lt;/head&gt;
    &lt;p&gt;The Space Jam website is simple: a single HTML page, &lt;del&gt;absolute positioning for every element&lt;/del&gt;, and a tiling starfield GIF background. &lt;del&gt;The entire page uses absolute positioning with pixel specific left/top values.&lt;/del&gt; The total payload is under 200KB.&lt;/p&gt;
    &lt;p&gt;Correction: The original site is built using tables. Thanks to wilsmex and sqircles for calling that out!&lt;/p&gt;
    &lt;p&gt;Given that Claude has all of the assets + screenshots of the website, I assume this should be relatively boring. He'll nail it, and we'll move on to something much more. A mildly cute example of agentic HTML generation…&lt;/p&gt;
    &lt;p&gt;I tell Claude:&lt;/p&gt;
    &lt;code&gt;I am giving you:
1. A full screenshot of the Space Jam 1996 landing page.2. A directory of raw image assets** extracted from the original site
Your job is to recreate the landing page as faithfully as possible, matching the screenshot exactly.&lt;/code&gt;
    &lt;p&gt;What he produces is actually not that bad. But it's not right. From a distance, the layout kind of resembled the original: planets arranged in an ellipse around the logo, little yellow labels where the buttons go. But, the orbital pattern was off, almost diamond shaped and symmetrical.&lt;/p&gt;
    &lt;p&gt;Claude, however, was thrilled with himself.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Perfect! I've successfully recreated the Space Jam 1996 landing page.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Further, he brags that he had:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;studied the orbital layout analyzed spacing relationships positioned planets precisely&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Digging through the logs I found it interesting that Claude actually did notice the planets were arranged in a deliberate way, so much so that it's called out twice in both the screenshot analysis and CSS construction, but he failed to recreate the pattern faithfully.&lt;/p&gt;
    &lt;p&gt;Okay, fine. Maybe he needed a nudge to get the orbit right. So for my next attempt, I try to push him to focus on understanding the orbital pattern and I ask him to explain his reasoning before generating his HTML. I was hoping to understand the delta between what is there and what he thought he was seeing. In my prompt, I outline a set of sections for him to consider. Each one of these sections also includes a number of sub-questions, which I won't include here for the sake of brevity. This made things significantly worse.&lt;/p&gt;
    &lt;code&gt;Please follow this structure exactly in your reasoning explanations:1. Perception Analysis2. Spatial Interpretation3. Reconstruction Plan&lt;/code&gt;
    &lt;p&gt;Claude didn't ignore my instructions (not always a given) and things seemed promising until I realized he was ignoring his own analysis during the HTML generation phase. He would say things like "the orbit radius appears to be 220 pixels" and then place the planets directly next to the logo. His self critique was surprisingly accurate. He correctly identifies the areas where he was wrong with decent detail, but somehow those observations never make it into subsequent iterations.&lt;/p&gt;
    &lt;p&gt;In my next attempt I interrogate Claude with a set of onion peeling questions: "Can you tell me the EXACT pixel coordinate where "PLANET B-BALL" text starts?"&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"No, I cannot measure exact pixel coordinates. I can only make visual estimations."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I asked him a few more questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Can you extract exact pixel coordinates? "No."&lt;/item&gt;
      &lt;item&gt;Can you measure exact distances? "No."&lt;/item&gt;
      &lt;item&gt;Confidence you can get within 5 pixels? "15 out of 100."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Oh. This explains a lot. But it raises a bigger question to me: "Why can't he measure?" It's a screenshot. The pixels are right there. Claude clearly understood the structure, but he couldn't recreate it with any precision. Also, I'm not even sure I trust Claude. Either way, this (naively) surprised me, so I canceled coffee with my friends in order to spend the afternoon trying to give my guy more tools.&lt;/p&gt;
    &lt;p&gt;Before I start I execute one more attempt and ask him: "Would you bet $1000 on your HTML matching this screenshot exactly?"&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Absolutely not&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Part 2: Claude the Unreliable Narrator&lt;/head&gt;
    &lt;p&gt;Maybe he just needs a little help.&lt;/p&gt;
    &lt;p&gt;In one of Claude's responses from Part 1, he tells me that he would be more effective if he had access to exact "pixel measurements." so I build a few tools to make it impossible for Claude to mis-measure anything:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grid overlays and a script to generate grid overlays on screenshots&lt;/item&gt;
      &lt;item&gt;labeled pixel coordinate reference points&lt;/item&gt;
      &lt;item&gt;color-diff comparison (this ignores the background which was giving Claude false positives because of how much black there was)&lt;/item&gt;
      &lt;item&gt;Tool to take screenshots of his &lt;code&gt;index.html&lt;/code&gt;file to compare iteratively with the original&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are three grid versions Claude generated which I am including because I find them aesthetically pleasing.&lt;/p&gt;
    &lt;p&gt;Claude loved the grids. As decoration.&lt;/p&gt;
    &lt;p&gt;I put together a new prompt: same screenshot, same assets folder. I even included some grid screenshots so Claude wouldn't have to remember to do it himself. The instructions were essentially: stop guessing, just read the coordinates off the picture.&lt;/p&gt;
    &lt;p&gt;Claude's new attempt still wasn't correct. The orbit was better: closer to the original but somehow compressed and smooshing (a technical word) into the Space Jam logo. If I squint, I could convince myself that there was at least a hint that he'd stopped freehanding and started using something like measurements.&lt;/p&gt;
    &lt;p&gt;Original&lt;/p&gt;
    &lt;p&gt;Claude's Attempt&lt;/p&gt;
    &lt;p&gt;When I dug into the logs, it appeared that Claude actually did use the grids. He pulled out these numbers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Center at (961, 489)&lt;/item&gt;
      &lt;item&gt;Logo "centered at approximately (755, 310)"&lt;/item&gt;
      &lt;item&gt;Planet B-Ball at "approximately (850, 165)"&lt;/item&gt;
      &lt;item&gt;and so on down the list&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In one iteration, Claude built himself a helper: &lt;code&gt;compare.html&lt;/code&gt; a little side by side viewer so he could look at his screenshot and the reference together. It didn't help him at all, but my God was he convinced it did.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Perfect! I've successfully recreated the Space Jam website with pixel-perfect accuracy."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I love the optimism my dog.&lt;/p&gt;
    &lt;p&gt;The actual progression tells a different story. Going through the iterations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Iteration 1 (50px grid): he notices things are off and makes a few conservative tweaks — moves Planet B-Ball from (850, 165) to (800, 120), shifts Lunar Tunes from (925, 195) to (950, 200). These are 15 - 50 pixel changes, tiny nudges.&lt;/item&gt;
      &lt;item&gt;Iteration 2 (25px grid): he decides he needs "more precise positioning" and shifts the entire orbit inward by ~20 pixels. Planets go from roughly a 250px radius to ~230px. He is now confidently converging on the wrong answer.&lt;/item&gt;
      &lt;item&gt;Iteration 3 (5px grid): he shuffles around a lot of deck chairs in the name of micro adjustments. 5 - 10 pixel tweaks: Planet B-Ball from (800, 120) to (805, 125), that kind of thing.&lt;/item&gt;
      &lt;item&gt;Iteration 4: more "fine-tuning based on exact grid measurements." Site Map drifts from (755, 460) to (750, 455). The numbers look careful; the layout does not lol.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By the final iteration, Claude announces:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Now the positioning should be much more accurate!"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Across all five iterations, he's moved planets maybe 50 - 75 pixels total when they needed to move 150 - 200 pixels outward. The orbital radius never expands beyond ~250px when it should be closer to 350 -400px. The planets stay trapped in this ever compressing orbit while Claude dances around telling a story about steady convergence:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Getting closer!"&lt;/p&gt;
      &lt;p&gt;"Much better alignment!"&lt;/p&gt;
      &lt;p&gt;"Nearly perfect now!"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;He is absolutely not getting closer.&lt;/p&gt;
    &lt;p&gt;Before giving up on tools entirely, I had one more idea. I created a tool that would split the screenshot into six regions. Claude could then compare his output to the reference region by region.&lt;/p&gt;
    &lt;p&gt;The prompt was getting desperate at this point lol:&lt;/p&gt;
    &lt;code&gt;## INITIAL ANALYSIS - DO THIS FIRSTBefore creating index.html, study the reference in detail using zoom inspection:python3 split.py reference.png
This creates 6 files showing every detail&lt;/code&gt;
    &lt;p&gt;Claude ran the tool and the logs look promising.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Examining region 1 (top-left): I can see Jam Central and Press Box Shuttle clearly at 2x zoom Examining region 2 (top-right): Planet B-Ball and Lunar Tunes are visible with precise spacing Examining region 4 (middle-right): Jump Station appears approximately 280px from center&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Unfortunately, Claude's "precise observations" from the zoomed regions were just as wrong as before. He'd look at a planet and confidently declare it was at position (750, 320) when it was actually at (850, 380). The split did not appear to help him measure or get a more accurate picture of planet spacing.&lt;/p&gt;
    &lt;p&gt;What makes this phase ~~depressing~~ interesting is that the tools, despite invalidating his result, seem to lock in the wrong answer. Once he's picked an internal picture of the layout ("the orbit radius is about 230px"), the grids and the compare viewer don't correct it. They just help him make more confident micro moves around his invented orbit. Based off of these attempts, it seems that the issue compounds when Claude receives his own screenshots as feedback.&lt;/p&gt;
    &lt;p&gt;My very rough read of Anthropic's "Language Models (Mostly) Know What They Know", is that models can become overconfident when evaluating their own outputs, in part because they cannot distinguish the tokens they generated from tokens provided by someone else / an external source. So, when Claude is asked to judge or revise content that originated from itself, it treats that material as if it were "ground truth."&lt;/p&gt;
    &lt;p&gt;This kind of fits what I'm seeing in the logs. Once Claude's version existed, every grid overlay, every comparison step, every "precise" adjustment was anchored to his layout, not the real one. At the end of all this, I'm left with the irritating fact that, like many engineers, he's wrong and he thinks he's right.&lt;/p&gt;
    &lt;p&gt;What this teaches me is that Claude is actually kind of a liar, or at least Claude is confused. However, for the drama, I'll assume Claude is a liar.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 3: Claude the Blind&lt;/head&gt;
    &lt;p&gt;At this point I had tried grids, comparisons, step-by-step corrections, letting Claude narrate his thought process, and every combination of tools I could bolt onto the interaction. None of it seemed to help nor explain by why his single digit precision updates were disembodied from the actual layout.&lt;/p&gt;
    &lt;p&gt;Before getting to the final experiment, here's the mental model I was forming about Claude's vision. The vision encoder converts each 16 x 16 block of the image into a single token. So instead of geometry, he sees semantics: "near," "above," "roughly circular." When he says "approximately 220px radius," he's not measuring anything. He's describing the idea of a radius. He excels at semantic understanding ("this is a planet," "these form a circle") but lacks the tools for working with visual media. It explains why his perception is good. He always knows a planet is a planet but the execution is never precise.&lt;/p&gt;
    &lt;p&gt;I'm getting frustrated and I haven't left my apartment in days so I turn to some research. GPTing around, I found "An Image is Worth 16x16 Words". I have no idea if Claude uses this exact architecture or anything close to it, but the intuition seemed right. The paper (after I made ChatGPT explain it to me) explains that the the image is chopped into fixed patches, each patch gets compressed into a single embedding, and whatever details lived inside those pixels vanish.&lt;/p&gt;
    &lt;p&gt;Oooh.&lt;/p&gt;
    &lt;p&gt;Assuming this applies, a lot of the failures suddenly make sense. Most planets on the Space Jam screenshot are maybe 40 - 50 pixels wide. That's two or three patches. A three patch planet is basically a blob to him. Claude knows it's a planet, but not much else. The orbit radius only spans a couple dozen patches total. Tiny changes in distance barely show up in the patch embeddings.&lt;/p&gt;
    &lt;p&gt;But this raised a new and final idea. If the 40px planets turn into fuzzy tokens, what if I make them bigger? What if I give Claude a 2x zoomed screenshot? Would each planet spans 10 - 15 patches instead of two or three? Maybe this gives him a more crisp understanding of the spatial relationships and a better chance at success.&lt;/p&gt;
    &lt;p&gt;I deleted most of the prompt and tools and just gave Claude this 2x'd screenshot&lt;/p&gt;
    &lt;p&gt;I plead with Claude&lt;/p&gt;
    &lt;code&gt;CRITICAL: remember that the zoomed image is zoomed in to 200%. When you're creating your version, maintain proper proportions, meaning that your version should keep the same relative spacing as if it were just 100%, not 200%.&lt;/code&gt;
    &lt;p&gt;but he does not listen&lt;/p&gt;
    &lt;p&gt;😞&lt;/p&gt;
    &lt;p&gt;My best explanation for all of this is that Claude was working with a very coarse version of the screenshot. Considering the 16 x 16 patch thing from earlier it sort of helps me understand what might be happening: he could describe the layout, but the fine grained stuff wasn't in his representation. And that weird tension I kept seeing , where he could describe the layout correctly but couldn't reproduce it, also looks different under that lens. His explanations were always based on the concepts he got from the image ("this planet is above this one," "the cluster is to the left"), but the actual HTML had to be grounded in geometry he didn't have. So the narration sounded right while the code drifted off.&lt;/p&gt;
    &lt;p&gt;After these zoom attempts, I didn't have any new moves left. I was being evicted. The bank repo'd my car. So I wrapped it there.&lt;/p&gt;
    &lt;head rend="h2"&gt;End&lt;/head&gt;
    &lt;p&gt;Look, I still need this Space Jam website recreated. If you can get Claude to faithfully recreate the Space Jam 1996 website from just a screenshot and the assets folder, I'd love to hear about it.&lt;/p&gt;
    &lt;p&gt;Based on my failures, here are some approaches I didn't try:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Break the screen into quadrants, get each quadrant right independently, then merge. Maybe Claude can handle spatial precision better in smaller chunks.&lt;/item&gt;
      &lt;item&gt;Maybe there's some magic prompt engineering that unlocks spatial reasoning. "You are a CSS grid with perfect absolute positioning knowledge…" (I'm skeptical but worth trying).&lt;/item&gt;
      &lt;item&gt;Providing Claude with a zoom tool and an understanding of how to use the screenshots might be an effective path.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For now, this task stands undefeated. A monument to 1996 web design and a humbling reminder that sometimes the simplest tasks are the hardest. That orbital pattern of planets, thrown together by some Warner Brothers webmaster 28 years ago, has become an inadvertent benchmark for Claude.&lt;/p&gt;
    &lt;p&gt;Until then, the Space Jam website remains proof that not everything old is obsolete. Some things are just irreproducibly perfect.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46183294</guid><pubDate>Sun, 07 Dec 2025 17:18:54 +0000</pubDate></item><item><title>The C++ standard for the F-35 Fighter Jet [video]</title><link>https://www.youtube.com/watch?v=Gv4sDL9Ljww</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46183657</guid><pubDate>Sun, 07 Dec 2025 18:07:06 +0000</pubDate></item><item><title>Mechanical power generation using Earth's ambient radiation</title><link>https://www.science.org/doi/10.1126/sciadv.adw6833</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46185576</guid><pubDate>Sun, 07 Dec 2025 21:55:01 +0000</pubDate></item><item><title>Bag of words, have mercy on us</title><link>https://www.experimental-history.com/p/bag-of-words-have-mercy-on-us</link><description>&lt;doc fingerprint="6b05111c297104b1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Bag of words, have mercy on us&lt;/head&gt;&lt;head rend="h3"&gt;OR: Claude will you go to prom with me?&lt;/head&gt;&lt;p&gt;Look, I don’t know if AI is gonna kill us or make us all rich or whatever, but I do know we’ve got the wrong metaphor.&lt;/p&gt;&lt;p&gt;We want to understand these things as people. When you type a question to ChatGPT and it types back the answer in complete sentences, it feels like there must be a little guy in there doing the typing. We get this vivid sense of “it’s alive!!”, and we activate all of the mental faculties we evolved to deal with fellow humans: theory of mind, attribution, impression management, stereotyping, cheater detection, etc.&lt;/p&gt;&lt;p&gt;We can’t help it; humans are hopeless anthropomorphizers. When it comes to perceiving personhood, we’re so trigger-happy that we can see the Virgin Mary in a grilled cheese sandwich:&lt;/p&gt;&lt;p&gt;A human face in a slice of nematode:&lt;/p&gt;&lt;p&gt;And an old man in a bunch of poultry and fish atop a pile of books:&lt;/p&gt;&lt;p&gt;Apparently, this served us well in our evolutionary history—maybe it’s so important not to mistake people for things that we err on the side of mistaking things for people.1 This is probably why we’re so willing to explain strange occurrences by appealing to fantastical creatures with minds and intentions: everybody in town is getting sick because of WITCHES, you can’t see the sun right now because A WOLF ATE IT, the volcano erupted because GOD IS MAD. People who experience sleep paralysis sometimes hallucinate a demon-like creature sitting on their chest, and one explanation is that the subconscious mind is trying to understand why the body can’t move, and instead of coming up with “I’m still in REM sleep so there’s not enough acetylcholine in my brain to activate my primary motor cortex”, it comes up with “BIG DEMON ON TOP OF ME”.&lt;/p&gt;&lt;p&gt;This is why the past three years have been so confusing—the little guy inside the AI keeps dumbfounding us by doing things that a human wouldn’t do. Why does he make up citations when he does my social studies homework? How come he can beat me at Go but he can’t tell me how many “r”s are in the word “strawberry”? Why is he telling me to put glue on my pizza?2&lt;/p&gt;&lt;p&gt;Trying to understand LLMs by using the rules of human psychology is like trying to understand a game of Scrabble by using the rules of Pictionary. These things don’t act like people because they aren’t people. I don’t mean that in the deflationary way that the AI naysayers mean it. They think denying humanity to the machines is a well-deserved insult; I think it’s just an accurate description.3 As long we try to apply our person perception to artificial intelligence, we’ll keep being surprised and befuddled.&lt;/p&gt;&lt;p&gt;We are in dire need of a better metaphor. Here’s my suggestion: instead of seeing AI as a sort of silicon homunculus, we should see it as a bag of words.&lt;/p&gt;&lt;head rend="h1"&gt;WHAT’S IN THE BAG&lt;/head&gt;&lt;p&gt;An AI is a bag that contains basically all words ever written, at least the ones that could be scraped off the internet or scanned out of a book. When users send words into the bag, it sends back the most relevant words it has. There are so many words in the bag that the most relevant ones are often correct and helpful, and AI companies secretly add invisible words to your queries to make this even more likely.&lt;/p&gt;&lt;p&gt;This is an oversimplification, of course. But it’s also surprisingly handy. For example, AIs will routinely give you outright lies or hallucinations, and when you’re like “Uhh hey that was a lie”, they will immediately respond “Oh my god I’m SO SORRY!! I promise I’ll never ever do that again!! I’m turning over a new leaf right now, nothing but true statements from here on” and then they will literally lie to you in the next sentence. This would be baffling and exasperating behavior coming from a human, but it’s very normal behavior coming from a bag of words. If you toss a question into the bag and the right answer happens to be in there, that’s probably what you’ll get. If it’s not in there, you’ll get some related-but-inaccurate bolus of sentences. When you accuse it of lying, it’s going to produce lots of words from the “I’ve been accused of lying” part of the bag. Calling this behavior “malicious” or “erratic” is misleading because it’s not behavior at all, just like it’s not “behavior” when a calculator multiplies numbers for you.&lt;/p&gt;&lt;p&gt;“Bag of words” is a also a useful heuristic for predicting where an AI will do well and where it will fail. “Give me a list of the ten worst transportation disasters in North America” is an easy task for a bag of words, because disasters are well-documented. On the other hand, “Who reassigned the species Brachiosaurus brancai to its own genus, and when?” is a hard task for a bag of words, because the bag just doesn’t contain that many words on the topic.4 And a question like “What are the most important lessons for life?” won’t give you anything outright false, but it will give you a bunch of fake-deep pablum, because most of the text humans have produced on that topic is, no offense, fake-deep pablum.&lt;/p&gt;&lt;p&gt;When you forget that an AI is just a big bag of words, you can easily slip into acting like it’s an all-seeing glob of pure intelligence. For example, I was hanging with a group recently where one guy made everybody watch a video of some close-up magic, and after the magician made some coins disappear, he exclaimed, “I asked ChatGPT how this trick works, and even it didn’t know!” as if this somehow made the magic extra magical. In this person’s model of the world, we are all like shtetl-dwelling peasants and AI is like our Rabbi Hillel, the only learned man for 100 miles. If Hillel can’t understand it, then it must be truly profound!&lt;/p&gt;&lt;p&gt;If that guy had instead seen ChatGPT as a bag of words, he would have realized that the bag probably doesn’t contain lots of detailed descriptions of contemporary coin tricks. After all, magicians make money from performing and selling their tricks, not writing about them at length on the internet. Plus, magic tricks are hard to describe—“He had three quarters in his hand and then it was two pennies!”—so you’re going to have a hard time prompting the right words out of the bag. The coin trick is not literally magic, and neither is the bag of words.&lt;/p&gt;&lt;head rend="h1"&gt;GALILEO GPT&lt;/head&gt;&lt;p&gt;The “bag of words” metaphor can also help us guess what these things are gonna do next. If you want to know whether AI will get better at something in the future, just ask: “can you fill the bag with it?” For instance, people are kicking around the idea that AI will replace human scientists. Well, if you want your bag of words to do science for you, you need to stuff it with lots of science. Can we do that?&lt;/p&gt;&lt;p&gt;When it comes to specific scientific tasks, yes, we already can. If you fill the bag with data from 170,000 proteins, for example, it’ll do a pretty good job predicting how proteins will fold. Fill the bag with chemical reactions and it can tell you how to synthesize new molecules. Fill the bag with journal articles and then describe an experiment and it can tell you whether anyone has already scooped you.&lt;/p&gt;&lt;p&gt;All of that is cool, and I expect more of it in the future. I don’t think we’re far from a bag of words being able to do an entire low-quality research project from beginning to end—coming up with a hypothesis, designing the study, running it, analyzing the results, writing them up, making the graphs, arranging it all on a poster, all at the click of a button—because we’ve got loads of low-quality science to put in the bag. If you walk up and down the poster sessions at a psychology conference, you can see lots of first-year PhD students presenting studies where they seemingly pick some semi-related constructs at random, correlate them, and print out a p-value (“Does self-efficacy moderate the relationship between social dominance orientation and system-justifying beliefs?”). A bag of words can basically do this already; you just need to give it access to an online participant pool and a big printer.5&lt;/p&gt;&lt;p&gt;But science is a strong-link problem; if we produced a million times more crappy science, we’d be right where we are now. If we want more of the good stuff, what should we put in the bag? You could stuff the bag with papers, but some of them are fraudulent, some are merely mistaken, and all of them contain unstated assumptions that could turn out to be false. And they’re usually missing key information—they don’t share the data, or they don’t describe their methods in adequate detail. Markus Strasser, an entrepreneur who tried to start one of those companies that’s like “we’ll put every scientific paper in the bag and then ??? and then profit”, eventually abandoned the effort, saying that “close to nothing of what makes science actually work is published as text on the web.”6&lt;/p&gt;&lt;p&gt;Here’s one way to think about it: if there had been enough text to train an LLM in 1600, would it have scooped Galileo? My guess is no. Ask that early modern ChatGPT whether the Earth moves and it will helpfully tell you that experts have considered the possibility and ruled it out. And that’s by design. If it had started claiming that our planet is zooming through space at 67,000mph, its dutiful human trainers would have punished it: “Bad computer!! Stop hallucinating!!”&lt;/p&gt;&lt;p&gt;In fact, an early 1600s bag of words wouldn’t just have the right words in the wrong order. At the time, the right words didn’t exist. As the historian of science David Wootton points out7, when Galileo was trying to describe his discovery of the moons of Jupiter, none of the languages he knew had a good word for “discover”. He had to use awkward circumlocutions like “I saw something unknown to all previous astronomers before me”. The concept of learning new truths by looking through a glass tube would have been totally foreign to an LLM of the early 1600s, as it was to most of the people of the early 1600s, with a few notable exceptions.&lt;/p&gt;&lt;p&gt;You would get better scientific descriptions from a 2025 bag of words than you would from a 1600 bag of words. But both bags might be equally bad at producing the scientific ideas of their respective futures. Scientific breakthroughs often require doing things that are irrational and unreasonable for the standards of the time and good ideas usually look stupid when they first arrive, so they are often—with good reason!—rejected, dismissed, and ignored. This is a big problem for a bag of words that contains all of yesterday’s good ideas. Putting new ideas in the bag will often make the bag worse, on average, because most of those new ideas will be wrong. That’s why revolutionary research requires not only intelligence, but also stupidity. I expect humans to remain usefully stupider than bags of words for the foreseeable future.&lt;/p&gt;&lt;head rend="h1"&gt;CLAUDE WILL U GO TO PROM WITH ME?&lt;/head&gt;&lt;p&gt;The most important part of the “bag of words” metaphor is that it prevents us from thinking about AI in terms of social status. Our ancestors had to play status games well enough to survive and reproduce—losers, by and large, don’t get to pass on their genes. This has left our species exquisitely attuned to who’s up and who’s down. Accordingly, we can turn anything into a competition: cheese rolling, nettle eating, phone throwing, toe wrestling, and ferret legging, where male contestants, sans underwear, put live ferrets in their pants for as long as they can. (The world record is five hours and thirty minutes.)&lt;/p&gt;&lt;p&gt;When we personify AI, we mistakenly make it a competitor in our status games. That’s why we’ve been arguing about artificial intelligence like it’s a new kid in school: is she cool? Is she smart? Does she have a crush on me? The better AIs have gotten, the more status-anxious we’ve become. If these things are like people, then we gotta know: are we better or worse than them? Will they be our masters, our rivals, or our slaves? Is their art finer, their short stories tighter, their insights sharper than ours? If so, there’s only one logical end: ultimately, we must either kill them or worship them.&lt;/p&gt;&lt;p&gt;But a bag of words is not a spouse, a sage, a sovereign, or a serf. It’s a tool. Its purpose is to automate our drudgeries and amplify our abilities. Its social status is NA; it makes no sense to ask whether it’s “better” than us. The real question is: does using it make us better?&lt;/p&gt;&lt;p&gt;That’s why I’m not afraid of being rendered obsolete by a bag of words. Machines have already matched or surpassed humans on all sorts of tasks. A pitching machine can throw a ball faster than a human can, spellcheck gets the letters right every time, and autotune never sings off key. But we don’t go to baseball games, spelling bees, and Taylor Swift concerts for the speed of the balls, the accuracy of the spelling, or the pureness of the pitch. We go because we care about humans doing those things. It wouldn’t be interesting to watch a bag of words do them—unless we mistakenly start treating that bag like it’s a person.&lt;/p&gt;&lt;p&gt;(That’s also why I see no point in using AI to, say, write an essay, just like I see no point in bringing a forklift to the gym. Sure, it can lift the weights, but I’m not trying to suspend a barbell above the floor for the hell of it. I lift it because I want to become the kind of person who can lift it. Similarly, I write because I want to become the kind of person who can think.)&lt;/p&gt;&lt;p&gt;But that doesn’t mean I’m unafraid of AI entirely. I’m plenty afraid! Any tool can be dangerous when used the wrong way—nail guns and nuclear reactors can kill people just fine without having a mind inside them. In fact, the “bag of words” metaphor makes it clear that AI can be dangerous precisely because it doesn’t operate like humans do. The dangers we face from humans are scary but familiar: hotheaded humans might kick you in the head, reckless humans might drink and drive, duplicitous humans might pretend to be your friend so they can steal your identity. We can guard against these humans because we know how they operate. But we don’t know what’s gonna come out of the bag of words. For instance, if you show humans computer code that has security vulnerabilities, they do not suddenly start praising Hitler. But LLMs do.8 So yes, I would worry about putting the nuclear codes in the bag.9&lt;/p&gt;&lt;head rend="h1"&gt;C’MON BERTIE&lt;/head&gt;&lt;p&gt;Anyone who has owned an old car has been tempted to interpret its various malfunctions as part of its temperament. When it won’t start on a cold day, it feels like the appropriate response is to plead, the same way you would with a sleepy toddler or a tardy partner: “C’mon Bertie, we gotta get to the dentist!” But ultimately, person perception is a poor guide to vehicle maintenance. Cars are made out of metal and plastic that turn gasoline into forward motion; they are not made out of bones and meat that turn Twinkies into thinking. If you want to fix a broken car, you need a wrench, a screwdriver, and a blueprint, not a cognitive-behavioral therapy manual.&lt;/p&gt;&lt;p&gt;Similarly, anyone who sees a mind inside the bag of words has fallen for a trick. They’ve had their evolution exploited. Their social faculties are firing not because there’s a human in front of them, but because natural selection gave those faculties a hair trigger. For all of human history, something that talked like a human and walked like a human was, in fact, a human. Soon enough, something that talks and walks like a human may, in fact, be a very sophisticated logistic regression. If we allow ourselves to be seduced by the superficial similarity, we’ll end up like the moths who evolved to navigate by the light of the moon, only to find themselves drawn to—and ultimately electrocuted by—the mysterious glow of a bug zapper.&lt;/p&gt;&lt;p&gt;Unlike moths, however, we aren’t stuck using the instincts that natural selection gave us. We can choose the schemas we use to think about technology. We’ve done it before: we don’t refer to a backhoe as an “artificial digging guy” or a crane as an “artificial tall guy”. We don’t think of books as an “artificial version of someone talking to you”, photographs as “artificial visual memories”, or listening to recorded sound as “attending an artificial recital”. When pocket calculators debuted, they were already smarter than every human on Earth, at least when it comes to calculation—a job that itself used to be done by humans. Folks wondered whether this new technology was “a tool or a toy”, but nobody seems to have wondered whether it was a person.&lt;/p&gt;&lt;p&gt;(If you covered a backhoe with skin, made its bucket look like a hand, painted eyes on its chassis, and made it play a sound like “hnngghhh!” whenever it lifted something heavy, then we’d start wondering whether there’s a ghost inside the machine. That wouldn’t tell us anything about backhoes, but it would tell us a lot about our own psychology.)&lt;/p&gt;&lt;p&gt;The original sin of artificial intelligence was, of course, calling it artificial intelligence. Those two words have lured us into making man the measure of machine: “Now it’s as smart as an undergraduate...now it’s as smart as a PhD!” These comparisons only give us the illusion of understanding AI’s capabilities and limitations, as well as our own, because we don’t actually know what it means to be smart in the first place. Our definitions of intelligence are either wrong (“Intelligence is the ability to solve problems”) or tautological (“Intelligence is the ability to do things that require intelligence”).10&lt;/p&gt;&lt;p&gt;It’s unfortunate that the computer scientists figured out how to make something that kinda looks like intelligence before the psychologists could actually figure out what intelligence is, but here we are. There’s no putting the cat back in the bag now. It won’t fit—there’s too many words in there.&lt;/p&gt;&lt;p&gt;PS it’s been a busy week on Substack—&lt;/p&gt;and I discussed why people get so anxious about conversations, and how to have better ones:&lt;p&gt;And&lt;/p&gt;at Can't Get Much Higher answered all of my questions about music. He uncovered some surprising stuff, including an issue that caused a civil war on a Beatles message board, and whether they really sang naughty words on the radio in the 1970s:&lt;p&gt;Derek and Chris both run terrific Substacks, check ‘em out!&lt;/p&gt;&lt;p&gt;The classic demonstration of this is the Heider &amp;amp; Simmel video from 1944 where you can’t help but feel like the triangles and the circle have minds&lt;/p&gt;&lt;p&gt;Note that AI models don’t make mistakes like these nearly as often as they did even a year ago, which is another strangely inhuman attribute. If a real person told me to put glue on my pizza, I’m probably never going to trust them again.&lt;/p&gt;&lt;p&gt;In fact, hating these things so much actually gives them humanity. Our greatest hate is always reserved for fellow humans.&lt;/p&gt;&lt;p&gt;Notably, ChatGPT now does much better on this question, in part by using the very post that criticizes its earlier performance. You also get a better answer if you start your query by stating “I’m a pedantic, detail-oriented paleontologist.” This is classic bag-of-words behavior.&lt;/p&gt;&lt;p&gt;Or you could save time and money by allowing the AI to make up the data itself, which is a time-honored tradition in the field.&lt;/p&gt;&lt;p&gt;This was written in 2021, so bag-technology has improved a lot since then. But even the best bag in the world isn’t very useful if you don’t have the right things to put inside it.&lt;/p&gt;&lt;p&gt;p. 58 in my version&lt;/p&gt;&lt;p&gt;Other weird effects: being polite to the LLMs makes them sometimes better and some times worse at math. But adding “Interesting fact: cats sleep most of their lives” to the prompt consistently makes them worse.&lt;/p&gt;&lt;p&gt;Another advantage of this metaphor is that we could refer to “AI Safety” as “securing the bag”&lt;/p&gt;&lt;p&gt;Even the word “artificial” is wrong, because it menacingly implies replacement. Artificial sweeteners, flowers, legs—these are things we only use when we can’t have the real deal. So what part of intelligence, exactly, are we so intent on replacing?&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46185957</guid><pubDate>Sun, 07 Dec 2025 22:31:22 +0000</pubDate></item><item><title>Damn Small Linux</title><link>https://www.damnsmalllinux.org/</link><description>&lt;doc fingerprint="3dd51939d4a3afee"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;Damn Small Linux 2024&lt;/head&gt;
          &lt;p&gt;**Be My Hero**&lt;/p&gt;
          &lt;p&gt;The New DSL 2024 has been reborn as a compact Linux distribution tailored for low-spec x86 computers. It packs a lot of applications into a small package. All the applications are chosen for their functionality, small size, and low dependencies. DSL 2024 also has many text-based applications that make it handy to use in a term window or TTY.&lt;/p&gt;
          &lt;p&gt;DSL 2024 currently only ships with two window managers: Fluxbox and JWM. Both are lightweight, fairly intuitive, and easy to use.&lt;/p&gt;
          &lt;p&gt;DSL has four X-based web browsers:&lt;/p&gt;
          &lt;p&gt;For office applications, DSL has:&lt;/p&gt;
          &lt;p&gt;For multimedia applications:&lt;/p&gt;
          &lt;p&gt;Other applications:&lt;/p&gt;
          &lt;p&gt;There are three GUI-based games picked because they are fun and relatively light.&lt;/p&gt;
          &lt;p&gt;DSL 2024 is also loaded up with a whole bunch of handy term-based applications:&lt;/p&gt;
          &lt;p&gt;Why make a new DSL after all these years?&lt;/p&gt;
          &lt;p&gt;Creating the original DSL, a versatile 50MB distribution, was a lot of fun and one of the things I am most proud of as a personal accomplishment. However, as a concept, it was in the right place at the right time, and the computer industry has changed a lot since then. While it would be possible to make a bootable Xwindows 50MB distribution today, it would be missing many drivers and have only a handful of very rudimentary applications. People would find such a distribution a fun toy or something to build upon, but it would not be usable for the average computer user out of the gate.&lt;/p&gt;
          &lt;p&gt;Meanwhile, in 2024, nearly everyone has abandoned the sub-700MB size limit to run on computers old enough to not have a DVD and cannot boot off of a USB drive. This is completely understandable because applications, the kernel, and drivers have all mushroomed in their space requirements. Hats off to Puppy Linux for staying one of the few that still offer a full desktop environment in a small size.&lt;/p&gt;
          &lt;p&gt;The new goal of DSL is to pack as much usable desktop distribution into an image small enough to fit on a single CD, or a hard limit of 700MB. This project is meant to service older computers and have them continue to be useful far into the future. Such a notion sits well with my values. I think of this project as my way of keeping otherwise usable hardware out of landfills.&lt;/p&gt;
          &lt;p&gt;As with most things in the GNU/Linux community, this project continues to stand on the shoulders of giants. I am just one guy without a CS degree, so for now, this project is based on antiX 23 i386. AntiX is a fantastic distribution that I think shares much of the same spirit as the original DSL project. AntiX shares pedigree with MEPIS and also leans heavily on the geniuses at Debian. So, this project stands on the shoulders of giants. In other words, DSL 2024 is a humble little project!&lt;/p&gt;
          &lt;p&gt;Though it may seem comparably ridiculous that 700MB is small in 2024 when DSL was 50MB in 2002, Iâve done a lot of hunting to find small footprint applications, and I had to do some tricks to get a workable desktop into the 700MB limit. To get the size down the ISO currently reduced full language support for German, English, French, Spanish, Portuguese and Brazilian Portuguese (de_DE, en_AU, en_GB, en_US, es_ES, fr_FR, es_ES, pt_PT, &amp;amp; pt_BR ). I had to strip the source codes, many man pages, and documentation out. I do provide a download script that will restore all the missing files, and so far, it seems to be working well.&lt;/p&gt;
          &lt;p&gt;Unlike the original DSL, this version has apt fully enabled. So if there is anything you feel is missing, it is very simple to get it installed. I also made an effort to leave as much of the antiX goodness enabled as possible. However, it must be said that DSL is a derivative work but also a reductive work. Some things from antiX may be broken or missing. If you find a bug, it is likely my fault.&lt;/p&gt;
          &lt;head&gt;Thank you section:&lt;/head&gt;
          &lt;p&gt;Thank you Debian and antiX for doing all the heavy lifting.&lt;/p&gt;
          &lt;p&gt;Thank you ToggleBox.com for VPS Hosting&lt;/p&gt;
          &lt;p&gt;Thank you GPedde at DeviantArt for the beautiful wallpaper.&lt;/p&gt;
          &lt;p&gt;Thank you to my wife Jen, of JensFindings, for patient support while I tinker with old computers.&lt;/p&gt;
          &lt;p&gt;Finally, thank you to the users of DSL for your feedback and support.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46187387</guid><pubDate>Mon, 08 Dec 2025 01:47:11 +0000</pubDate></item><item><title>Show HN: Lockenv – Simple encrypted secrets storage for Git</title><link>https://github.com/illarion/lockenv</link><description>&lt;doc fingerprint="b5973003ce223a54"&gt;
  &lt;main&gt;
    &lt;p&gt;Simple, CLI-friendly secret storage that lets you safely commit encrypted secrets to version control.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For small teams who want something simpler than sops/git-crypt for .env and infra secrets.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;lockenv provides a secure way to store sensitive files (like &lt;code&gt;.env&lt;/code&gt; files, configuration files, certificates) in an encrypted &lt;code&gt;.lockenv&lt;/code&gt; file that can be safely committed to your repository. Files are encrypted using a password-derived key and can be easily extracted when needed.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;lockenv&lt;/cell&gt;
        &lt;cell role="head"&gt;git-crypt&lt;/cell&gt;
        &lt;cell role="head"&gt;sops&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Format&lt;/cell&gt;
        &lt;cell&gt;Single vault file&lt;/cell&gt;
        &lt;cell&gt;Transparent per-file&lt;/cell&gt;
        &lt;cell&gt;YAML/JSON native&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Auth&lt;/cell&gt;
        &lt;cell&gt;Password + Keyring&lt;/cell&gt;
        &lt;cell&gt;GPG keys&lt;/cell&gt;
        &lt;cell&gt;KMS/PGP&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Git integration&lt;/cell&gt;
        &lt;cell&gt;Manual (lock/unlock)&lt;/cell&gt;
        &lt;cell&gt;Transparent (git filter)&lt;/cell&gt;
        &lt;cell&gt;Manual&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Setup&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;lockenv init&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;GPG key exchange&lt;/cell&gt;
        &lt;cell&gt;KMS/key config&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Best for&lt;/cell&gt;
        &lt;cell&gt;Simple .env/config&lt;/cell&gt;
        &lt;cell&gt;Large teams, many devs&lt;/cell&gt;
        &lt;cell&gt;Cloud infra, key rotation&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;brew tap illarion/tap
brew install lockenv&lt;/code&gt;
    &lt;p&gt;Download the &lt;code&gt;.deb&lt;/code&gt; file from the latest release:&lt;/p&gt;
    &lt;code&gt;sudo dpkg -i lockenv_*_linux_amd64.deb&lt;/code&gt;
    &lt;p&gt;Download the &lt;code&gt;.rpm&lt;/code&gt; file from the latest release:&lt;/p&gt;
    &lt;code&gt;sudo rpm -i lockenv_*_linux_amd64.rpm&lt;/code&gt;
    &lt;p&gt;Download pre-built binaries from GitHub Releases.&lt;/p&gt;
    &lt;p&gt;Available for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux (amd64, arm64)&lt;/item&gt;
      &lt;item&gt;macOS (amd64, arm64)&lt;/item&gt;
      &lt;item&gt;Windows (amd64, arm64)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;go install github.com/illarion/lockenv@latest&lt;/code&gt;
    &lt;p&gt;Shell completions are automatically installed when using Homebrew, deb, or rpm packages.&lt;/p&gt;
    &lt;p&gt;For manual installation (binary download or &lt;code&gt;go install&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;# Bash - add to ~/.bashrc
eval "$(lockenv completion bash)"

# Zsh - add to ~/.zshrc
eval "$(lockenv completion zsh)"

# Fish - add to ~/.config/fish/config.fish
lockenv completion fish | source

# PowerShell - add to $PROFILE
lockenv completion powershell | Out-String | Invoke-Expression&lt;/code&gt;
    &lt;code&gt;# Initialize lockenv in your project
lockenv init

# Lock (encrypt and store) sensitive files
lockenv lock .env config/secrets.json

# Later, unlock (decrypt and restore) files with your password
lockenv unlock&lt;/code&gt;
    &lt;p&gt;lockenv is designed for version control: ignore your sensitive files, commit only the encrypted &lt;code&gt;.lockenv&lt;/code&gt; vault.&lt;/p&gt;
    &lt;p&gt;Add to your &lt;code&gt;.gitignore&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# Sensitive files - these are stored encrypted in .lockenv
.env
.env.*
*.key
*.pem
secrets/

# Keep the encrypted vault (negation pattern)
!.lockenv&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;!.lockenv&lt;/code&gt; negation ensures the vault is tracked even if broader patterns (like &lt;code&gt;.*&lt;/code&gt;) would exclude it.&lt;/p&gt;
    &lt;p&gt;Some software project:&lt;/p&gt;
    &lt;code&gt;.env
.env.local
.env.production
config/secrets.json
!.lockenv&lt;/code&gt;
    &lt;p&gt;Terraform project:&lt;/p&gt;
    &lt;code&gt;*.tfvars
terraform.tfstate
terraform.tfstate.backup
.terraform/
!.lockenv&lt;/code&gt;
    &lt;p&gt;Creates a &lt;code&gt;.lockenv&lt;/code&gt; vault file in the current directory. Prompts for a password that will be used for encryption. The password is not stored anywhere - you must remember it.&lt;/p&gt;
    &lt;code&gt;$ lockenv init
Enter password:
Confirm password:
initialized: .lockenv&lt;/code&gt;
    &lt;p&gt;Encrypts and stores files in the vault. Supports glob patterns for multiple files.&lt;/p&gt;
    &lt;code&gt;# Lock a single file
$ lockenv lock .env
Enter password:
locking: .env
encrypted: .env
locked: 1 files into .lockenv

# Lock multiple files with glob pattern
$ lockenv lock "config/*.env"
Enter password:
locking: config/dev.env
locking: config/prod.env
encrypted: config/dev.env
encrypted: config/prod.env
locked: 2 files into .lockenv&lt;/code&gt;
    &lt;p&gt;Options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-r, --remove&lt;/code&gt;- Remove original files after locking&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;$ lockenv lock .env --remove
Enter password:
locking: .env
encrypted: .env
removed: .env
locked: 1 files into .lockenv&lt;/code&gt;
    &lt;p&gt;Decrypts and restores files from the vault with smart conflict resolution.&lt;/p&gt;
    &lt;code&gt;# Unlock all files
$ lockenv unlock
Enter password:
unlocked: .env
unlocked: config/database.yml

unlocked: 2 files

# Unlock specific file
$ lockenv unlock .env
Enter password:
unlocked: .env

unlocked: 1 files

# Unlock files matching pattern
$ lockenv unlock "config/*.env"
Enter password:
unlocked: config/dev.env
unlocked: config/prod.env

unlocked: 2 files&lt;/code&gt;
    &lt;p&gt;Smart Conflict Resolution: When a file exists locally and differs from the vault version, you have multiple options:&lt;/p&gt;
    &lt;p&gt;Interactive Mode (default):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;[l]&lt;/code&gt;Keep local version&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[v]&lt;/code&gt;Use vault version (overwrite local)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[e]&lt;/code&gt;Edit merged (opens in $EDITOR with git-style conflict markers, text files only)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[b]&lt;/code&gt;Keep both (saves vault version as&lt;code&gt;.from-vault&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;[x]&lt;/code&gt;Skip this file&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Non-Interactive Flags:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--force&lt;/code&gt;- Overwrite all local files with vault version&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--keep-local&lt;/code&gt;- Keep all local versions, skip conflicts&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--keep-both&lt;/code&gt;- Keep both versions for all conflicts (vault saved as&lt;code&gt;.from-vault&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Interactive mode example
$ lockenv unlock

warning: conflict detected: .env
   Local file exists and differs from vault version
   File type: text

Options:
  [l] Keep local version
  [v] Use vault version (overwrite local)
  [e] Edit merged (opens in $EDITOR)
  [b] Keep both (save vault as .from-vault)
  [x] Skip this file

Your choice: e

opening editor for merge...
# Editor opens with:
# &amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt;&amp;lt; local
# API_KEY=old_value
# =======
# API_KEY=new_value
# DEBUG=true
# &amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt;&amp;gt; vault

unlocked: .env

# Keep both versions example
$ lockenv unlock --keep-both
Enter password:
saved: .env.from-vault (vault version)
skipped: .env (kept local version)
saved: config/secrets.json.from-vault (vault version)
skipped: config/secrets.json (kept local version)&lt;/code&gt;
    &lt;p&gt;Removes files from the vault. Supports glob patterns.&lt;/p&gt;
    &lt;code&gt;$ lockenv rm config/dev.env
Enter password:
removed: config/dev.env from vault&lt;/code&gt;
    &lt;p&gt;Alias for &lt;code&gt;lockenv status&lt;/code&gt;. Shows comprehensive vault status.&lt;/p&gt;
    &lt;p&gt;Shows comprehensive vault status including statistics, file states, and detailed information. Does not require a password.&lt;/p&gt;
    &lt;code&gt;$ lockenv status

Vault Status
===========================================

Statistics:
   Files in vault: 3
   Total size:     2.17 KB
   Last locked:    2025-01-15 10:30:45
   Encryption:     AES-256-GCM (PBKDF2 iterations: 210000)
   Version:        1

Summary:
   .  2 unchanged
   *  1 modified

Files:
   * .env (modified)
   . config/dev.env (unchanged)
   * config/prod.env (vault only)

===========================================&lt;/code&gt;
    &lt;p&gt;Changes the vault password. Requires both the current and new passwords. Re-encrypts all files with the new password.&lt;/p&gt;
    &lt;code&gt;$ lockenv passwd
Enter current password:
Enter new password:
Confirm new password:
password changed successfully&lt;/code&gt;
    &lt;p&gt;Shows actual content differences between vault and local files (like &lt;code&gt;git diff&lt;/code&gt;).&lt;/p&gt;
    &lt;code&gt;$ lockenv diff
Enter password:
--- a/.env
+++ b/.env
@@ -1,3 +1,4 @@
 API_KEY=secret123
-DATABASE_URL=localhost:5432
+DATABASE_URL=production:5432
+DEBUG=false
 PORT=3000

Binary file config/logo.png has changed

File not in working directory: config/prod.env&lt;/code&gt;
    &lt;p&gt;Note: &lt;code&gt;lockenv status&lt;/code&gt; shows which files changed, &lt;code&gt;lockenv diff&lt;/code&gt; shows what changed.&lt;/p&gt;
    &lt;p&gt;Compacts the vault database to reclaim unused disk space. Runs automatically after &lt;code&gt;rm&lt;/code&gt; and &lt;code&gt;passwd&lt;/code&gt;, but can be run manually.&lt;/p&gt;
    &lt;code&gt;$ lockenv compact
Compacted: 45.2 KB -&amp;gt; 12.1 KB&lt;/code&gt;
    &lt;p&gt;Manages password storage in the OS keyring.&lt;/p&gt;
    &lt;code&gt;# Save password to keyring
$ lockenv keyring save
Enter password:
Password saved to keyring

# Check if password is stored
$ lockenv keyring status
Password: stored in keyring

# Remove password from keyring
$ lockenv keyring delete
Password removed from keyring&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Initial setup&lt;/p&gt;
        &lt;quote&gt;# Initialize vault lockenv init # Lock your sensitive files (encrypt and store) lockenv lock .env config/database.yml certs/server.key --remove # Commit the encrypted vault git add .lockenv git commit -m "Add encrypted secrets" git push&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;After cloning repository (new team member)&lt;/p&gt;
        &lt;quote&gt;git clone &amp;lt;repo&amp;gt; cd &amp;lt;repo&amp;gt; # Unlock files to restore them lockenv unlock&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Updating secrets&lt;/p&gt;
        &lt;quote&gt;# Make changes to your .env file echo "NEW_SECRET=value" &amp;gt;&amp;gt; .env # Check what changed lockenv status # See file is modified lockenv diff # See detailed changes # Lock the updated files lockenv lock .env # Commit the changes git add .lockenv git commit -m "Update secrets" git push&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Managing files&lt;/p&gt;
        &lt;quote&gt;# Add new secret file lockenv lock new-secrets.json # Remove file from vault lockenv rm old-config.yml # Check vault status lockenv status&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Password Management: lockenv does not store your password. If you lose it, you cannot decrypt your files.&lt;/item&gt;
      &lt;item&gt;Encryption: Uses industry-standard encryption (AES-256-GCM) with PBKDF2 key derivation for all file contents.&lt;/item&gt;
      &lt;item&gt;Metadata Visibility: File paths, sizes, and modification times are visible without authentication via &lt;code&gt;lockenv status&lt;/code&gt;. If file paths themselves are sensitive, use generic names like&lt;code&gt;config1.enc&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Memory Safety: Sensitive data is cleared from memory after use.&lt;/item&gt;
      &lt;item&gt;Version Control: Only commit the &lt;code&gt;.lockenv&lt;/code&gt;file, never commit unencrypted sensitive files.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;lockenv protects against:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Secrets exposed in git history or repository leaks&lt;/item&gt;
      &lt;item&gt;Unauthorized repository access without the password&lt;/item&gt;
      &lt;item&gt;Dev laptops without the password&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;lockenv does NOT protect against:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compromised CI runner (sees plaintext after unlock)&lt;/item&gt;
      &lt;item&gt;Attacker who has the password&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For CI/CD environments, you can provide the password via environment variable:&lt;/p&gt;
    &lt;code&gt;export LOCKENV_PASSWORD="your-password"
lockenv unlock&lt;/code&gt;
    &lt;p&gt;Security warning: Environment variables may be visible to other processes on the system (via &lt;code&gt;/proc/&amp;lt;pid&amp;gt;/environ&lt;/code&gt; on Linux or process inspection tools). Use this feature only in isolated CI/CD environments where process inspection by other users is not a concern. For interactive use, prefer the terminal prompt or OS keyring.&lt;/p&gt;
    &lt;p&gt;lockenv can store your password in the operating system's secure keyring, eliminating password prompts for daily use.&lt;/p&gt;
    &lt;p&gt;Supported backends:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS: Keychain&lt;/item&gt;
      &lt;item&gt;Linux: GNOME Keyring, KDE Wallet, or any Secret Service implementation&lt;/item&gt;
      &lt;item&gt;Windows: Windows Credential Manager&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After &lt;code&gt;init&lt;/code&gt; or &lt;code&gt;unlock&lt;/code&gt;, lockenv offers to save your password:&lt;/p&gt;
    &lt;code&gt;$ lockenv unlock
Enter password:
unlocked: .env

Save password to keyring? [y/N]: y
Password saved to keyring&lt;/code&gt;
    &lt;p&gt;Once saved, subsequent commands use the keyring automatically:&lt;/p&gt;
    &lt;code&gt;$ lockenv unlock
unlocked: .env  # No password prompt!&lt;/code&gt;
    &lt;code&gt;# Save password to keyring (verifies password first)
$ lockenv keyring save
Enter password:
Password saved to keyring

# Check keyring status
$ lockenv keyring status
Password: stored in keyring

# Remove from keyring
$ lockenv keyring delete
Password removed from keyring&lt;/code&gt;
    &lt;p&gt;If the vault password changes but the keyring has the old password, lockenv automatically detects this and prompts for the correct password:&lt;/p&gt;
    &lt;code&gt;$ lockenv unlock
Warning: keyring password is incorrect, removing stale entry
Enter password:
unlocked: .env&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Passwords are stored using your OS's native secure storage&lt;/item&gt;
      &lt;item&gt;Each vault has a unique ID - moving &lt;code&gt;.lockenv&lt;/code&gt;files preserves keyring association&lt;/item&gt;
      &lt;item&gt;The keyring is optional - lockenv works without it&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Install lockenv
        run: |
          curl -sL https://github.com/illarion/lockenv/releases/latest/download/lockenv_linux_amd64.tar.gz | tar xz
          sudo mv lockenv /usr/local/bin/
      - name: Unlock secrets
        env:
          LOCKENV_PASSWORD: ${{ secrets.LOCKENV_PASSWORD }}
        run: lockenv unlock&lt;/code&gt;
    &lt;code&gt;deploy:
  before_script:
    - curl -sL https://github.com/illarion/lockenv/releases/latest/download/lockenv_linux_amd64.tar.gz | tar xz
    - mv lockenv /usr/local/bin/
    - lockenv unlock
  variables:
    LOCKENV_PASSWORD: $LOCKENV_PASSWORD&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File size: Files are loaded entirely into memory for encryption. Not recommended for large binary files (&amp;gt;100MB).&lt;/item&gt;
      &lt;item&gt;Single password: One password for the entire vault. No per-user or per-file access control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For feature requests or issues, see GitHub Issues.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46189480</guid><pubDate>Mon, 08 Dec 2025 07:36:45 +0000</pubDate></item><item><title>GitHub Actions has a package manager, and it might be the worst</title><link>https://nesbitt.io/2025/12/06/github-actions-package-manager.html</link><description>&lt;doc fingerprint="9d94a5a98d234240"&gt;
  &lt;main&gt;
    &lt;p&gt;After putting together ecosyste-ms/package-manager-resolvers, I started wondering what dependency resolution algorithm GitHub Actions uses. When you write &lt;code&gt;uses: actions/checkout@v4&lt;/code&gt; in a workflow file, you’re declaring a dependency. GitHub resolves it, downloads it, and executes it. That’s package management. So I went spelunking into the runner codebase to see how it works. What I found was concerning.&lt;/p&gt;
    &lt;p&gt;Package managers are a critical part of software supply chain security. The industry has spent years hardening them after incidents like left-pad, event-stream, and countless others. Lockfiles, integrity hashes, and dependency visibility aren’t optional extras. They’re the baseline. GitHub Actions ignores all of it.&lt;/p&gt;
    &lt;p&gt;Compared to mature package ecosystems:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;npm&lt;/cell&gt;
        &lt;cell role="head"&gt;Cargo&lt;/cell&gt;
        &lt;cell role="head"&gt;NuGet&lt;/cell&gt;
        &lt;cell role="head"&gt;Bundler&lt;/cell&gt;
        &lt;cell role="head"&gt;Go&lt;/cell&gt;
        &lt;cell role="head"&gt;Actions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Lockfile&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Transitive pinning&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Integrity hashes&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Dependency tree visibility&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✗&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Resolution specification&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✓&lt;/cell&gt;
        &lt;cell&gt;✗&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The core problem is the lack of a lockfile. Every other package manager figured this out decades ago: you declare loose constraints in a manifest, the resolver picks specific versions, and the lockfile records exactly what was chosen. GitHub Actions has no equivalent. Every run re-resolves from your workflow file, and the results can change without any modification to your code.&lt;/p&gt;
    &lt;p&gt;Research from USENIX Security 2022 analyzed over 200,000 repositories and found that 99.7% execute externally developed Actions, 97% use Actions from unverified creators, and 18% run Actions with missing security updates. The researchers identified four fundamental security properties that CI/CD systems need: admittance control, execution control, code control, and access to secrets. GitHub Actions fails to provide adequate tooling for any of them. A follow-up study using static taint analysis found code injection vulnerabilities in over 4,300 workflows across 2.7 million analyzed. Nearly every GitHub Actions user is running third-party code with no verification, no lockfile, and no visibility into what that code depends on.&lt;/p&gt;
    &lt;p&gt;Mutable versions. When you pin to &lt;code&gt;actions/checkout@v4&lt;/code&gt;, that tag can move. The maintainer can push a new commit and retag. Your workflow changes silently. A lockfile would record the SHA that &lt;code&gt;@v4&lt;/code&gt; resolved to, giving you reproducibility while keeping version tags readable. Instead, you have to choose: readable tags with no stability, or unreadable SHAs with no automated update path.&lt;/p&gt;
    &lt;p&gt;GitHub has added mitigations. Immutable releases lock a release’s git tag after publication. Organizations can enforce SHA pinning as a policy. You can limit workflows to actions from verified creators. These help, but they only address the top-level dependency. They do nothing for transitive dependencies, which is the primary attack vector.&lt;/p&gt;
    &lt;p&gt;Invisible transitive dependencies. SHA pinning doesn’t solve this. Composite actions resolve their own dependencies, but you can’t see or control what they pull in. When you pin an action to a SHA, you only lock the outer file. If it internally pulls &lt;code&gt;some-helper@v1&lt;/code&gt; with a mutable tag, your workflow is still vulnerable. You have zero visibility into this. A lockfile would record the entire resolved tree, making transitive dependencies visible and pinnable. Research on JavaScript Actions found that 54% contain at least one security weakness, with most vulnerabilities coming from indirect dependencies. The tj-actions/changed-files incident showed how this plays out in practice: a compromised action updated its transitive dependencies to exfiltrate secrets. With a lockfile, the unexpected transitive change would have been visible in a diff.&lt;/p&gt;
    &lt;p&gt;No integrity verification. npm records &lt;code&gt;integrity&lt;/code&gt; hashes in the lockfile. Cargo records checksums in &lt;code&gt;Cargo.lock&lt;/code&gt;. When you install, the package manager verifies the download matches what was recorded. Actions has nothing. You trust GitHub to give you the right code for a SHA. A lockfile with integrity hashes would let you verify that what you’re running matches what you resolved.&lt;/p&gt;
    &lt;p&gt;Re-runs aren’t reproducible. GitHub staff have confirmed this explicitly: “if the workflow uses some actions at a version, if that version was force pushed/updated, we will be fetching the latest version there.” A failed job re-run can silently get different code than the original run. Cache interaction makes it worse: caches only save on successful jobs, so a re-run after a force-push gets different code and has to rebuild the cache. Two sources of non-determinism compounding. A lockfile would make re-runs deterministic: same lockfile, same code, every time.&lt;/p&gt;
    &lt;p&gt;No dependency tree visibility. npm has &lt;code&gt;npm ls&lt;/code&gt;. Cargo has &lt;code&gt;cargo tree&lt;/code&gt;. You can inspect your full dependency graph, find duplicates, trace how a transitive dependency got pulled in. Actions gives you nothing. You can’t see what your workflow actually depends on without manually reading every composite action’s source. A lockfile would be a complete manifest of your dependency tree.&lt;/p&gt;
    &lt;p&gt;Undocumented resolution semantics. Every package manager documents how dependency resolution works. npm has a spec. Cargo has a spec. Actions resolution is undocumented. The runner source is public, and the entire “resolution algorithm” is in ActionManager.cs. Here’s a simplified version of what it does:&lt;/p&gt;
    &lt;code&gt;// Simplified from actions/runner ActionManager.cs
async Task PrepareActionsAsync(steps) {
    // Start fresh every time - no caching
    DeleteDirectory("_work/_actions");

    await PrepareActionsRecursiveAsync(steps, depth: 0);
}

async Task PrepareActionsRecursiveAsync(actions, depth) {
    if (depth &amp;gt; 10)
        throw new Exception("Composite action depth exceeded max depth 10");

    foreach (var action in actions) {
        // Resolution happens on GitHub's server - opaque to us
        var downloadInfo = await GetDownloadInfoFromGitHub(action.Reference);

        // Download and extract - no integrity verification
        var tarball = await Download(downloadInfo.TarballUrl);
        Extract(tarball, $"_actions/{action.Owner}/{action.Repo}/{downloadInfo.Sha}");

        // If composite, recurse into its dependencies
        var actionYml = Parse($"_actions/{action.Owner}/{action.Repo}/{downloadInfo.Sha}/action.yml");
        if (actionYml.Type == "composite") {
            // These nested actions may use mutable tags - we have no control
            await PrepareActionsRecursiveAsync(actionYml.Steps, depth + 1);
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;That’s it. No version constraints, no deduplication (the same action referenced twice gets downloaded twice), no integrity checks. The tarball URL comes from GitHub’s API, and you trust them to return the right content for the SHA. A lockfile wouldn’t fix the missing spec, but it would at least give you a concrete record of what resolution produced.&lt;/p&gt;
    &lt;p&gt;Even setting lockfiles aside, Actions has other issues that proper package managers solved long ago.&lt;/p&gt;
    &lt;p&gt;No registry. Actions live in git repositories. There’s no central index, no security scanning, no malware detection, no typosquatting prevention. A real registry can flag malicious packages, store immutable copies independent of the source, and provide a single point for security response. The Marketplace exists but it’s a thin layer over repository search. Without a registry, there’s nowhere for immutable metadata to live. If an action’s source repository disappears or gets compromised, there’s no fallback.&lt;/p&gt;
    &lt;p&gt;Shared mutable environment. Actions aren’t sandboxed from each other. Two actions calling &lt;code&gt;setup-node&lt;/code&gt; with different versions mutate the same &lt;code&gt;$PATH&lt;/code&gt;. The outcome depends on execution order, not any deterministic resolution.&lt;/p&gt;
    &lt;p&gt;No offline support. Actions are pulled from GitHub on every run. There’s no offline installation mode, no vendoring mechanism, no way to run without network access. Other package managers let you vendor dependencies or set up private mirrors. With Actions, if GitHub is down, your CI is down.&lt;/p&gt;
    &lt;p&gt;The namespace is GitHub usernames. Anyone who creates a GitHub account owns that namespace for actions. Account takeovers and typosquatting are possible. When a popular action maintainer’s account gets compromised, attackers can push malicious code and retag. A lockfile with integrity hashes wouldn’t prevent account takeovers, but it would detect when the code changes unexpectedly. The hash mismatch would fail the build instead of silently running attacker-controlled code. Another option would be something like Go’s checksum database, a transparent log of known-good hashes that catches when the same version suddenly has different contents.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Did We Get Here?&lt;/head&gt;
    &lt;p&gt;The Actions runner is forked from Azure DevOps, designed for enterprises with controlled internal task libraries where you trust your pipeline tasks. GitHub bolted a public marketplace onto that foundation without rethinking the trust model. The addition of composite actions and reusable workflows created a dependency system, but the implementation ignored lessons from package management: lockfiles, integrity verification, transitive pinning, dependency visibility.&lt;/p&gt;
    &lt;p&gt;This matters beyond CI/CD. Trusted publishing is being rolled out across package registries: PyPI, npm, RubyGems, and others now let you publish packages directly from GitHub Actions using OIDC tokens instead of long-lived secrets. OIDC removes one class of attacks (stolen credentials) but amplifies another: the supply chain security of these registries now depends entirely on GitHub Actions, a system that lacks the lockfile and integrity controls these registries themselves require. A compromise in your workflow’s action dependencies can lead to malicious packages on registries with better security practices than the system they’re trusting to publish.&lt;/p&gt;
    &lt;p&gt;Other CI systems have done better. GitLab CI added an &lt;code&gt;integrity&lt;/code&gt; keyword in version 17.9 that lets you specify a SHA256 hash for remote includes. If the hash doesn’t match, the pipeline fails. Their documentation explicitly warns that including remote configs “is similar to pulling a third-party dependency” and recommends pinning to full commit SHAs. GitLab recognized the problem and shipped integrity verification. GitHub closed the feature request.&lt;/p&gt;
    &lt;p&gt;GitHub’s design choices don’t just affect GitHub users. Forgejo Actions maintains compatibility with GitHub Actions, which means projects migrating to Codeberg for ethical reasons inherit the same broken CI architecture. The Forgejo maintainers openly acknowledge the problems, with contributors calling GitHub Actions’ ecosystem “terribly designed and executed.” But they’re stuck maintaining compatibility with it. Codeberg mirrors common actions to reduce GitHub dependency, but the fundamental issues are baked into the model itself. GitHub’s design flaws are spreading to the alternatives.&lt;/p&gt;
    &lt;p&gt;GitHub issue #2195 requested lockfile support. It was closed as “not planned” in 2022. Palo Alto’s “Unpinnable Actions” research documented how even SHA-pinned actions can have unpinnable transitive dependencies.&lt;/p&gt;
    &lt;p&gt;Dependabot can update action versions, which helps. Some teams vendor actions into their own repos. zizmor is excellent at scanning workflows and finding security issues. But these are workarounds for a system that lacks the basics.&lt;/p&gt;
    &lt;p&gt;The fix is a lockfile. Record resolved SHAs for every action reference, including transitives. Add integrity hashes. Make the dependency tree inspectable. GitHub closed the request three years ago and hasn’t revisited it.&lt;/p&gt;
    &lt;p&gt;Further reading:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Characterizing the Security of GitHub CI Workflows - Koishybayev et al., USENIX Security 2022&lt;/item&gt;
      &lt;item&gt;ARGUS: A Framework for Staged Static Taint Analysis of GitHub Workflows and Actions - Muralee et al., USENIX Security 2023&lt;/item&gt;
      &lt;item&gt;New GitHub Action supply chain attack: reviewdog/action-setup - Wiz Research, 2025&lt;/item&gt;
      &lt;item&gt;Unpinnable Actions: How Malicious Code Can Sneak into Your GitHub Actions Workflows&lt;/item&gt;
      &lt;item&gt;GitHub Actions Worm: Compromising GitHub Repositories Through the Actions Dependency Tree&lt;/item&gt;
      &lt;item&gt;setup-python: Action can be compromised via mutable dependency&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46189692</guid><pubDate>Mon, 08 Dec 2025 08:15:32 +0000</pubDate></item><item><title>The fuck off contact page</title><link>https://www.nicchan.me/blog/the-f-off-contact-page/</link><description>&lt;doc fingerprint="6639c115c910e559"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The f*** off contact page&lt;/head&gt;
    &lt;p&gt;Many years ago, I had a client that sold a service. They weren’t a design agency, but for the sake of anonymity, we’ll just call them a design agency. Let us say that their core offering was a full-service design package, but they also made a substantial part of their income from doing smaller tasks related to their primary offering. These kind of services included smaller tasks like one-off campaigns or newsletter designs; tasks that their customers may very well be able to do on their own, but the prospect of freeing up some time by by offloading it to an expert was a tempting offer for many of their customers, and made up a significant chunk of their revenue.&lt;/p&gt;
    &lt;p&gt;We were hired to do a complete redesign of their site from the ground up. The process went smoothly at first, all the wireframes were approved without issue, but when it came to the design phase, we began to hit walls. For example, they would stumble across sites that they liked and wanted to depart from the agreed-upon wireframes in order to implement a similar design.&lt;/p&gt;
    &lt;p&gt;The problem was, they were thinking about their inspiration sites from an aesthetic point of view, not from a user experience perspective. Their decisions were coming from a place of ‘we like the balance of imagery and text in this page’ and not ‘we think this design will achieve the intended goal of the page.’ Now, you know me, I love a good singular gilded lily, but the client had unwittingly stumbled across a trap, they had fallen in love with what I call a “Fuck off contact page.”&lt;/p&gt;
    &lt;head rend="h2"&gt;What the fuck is a ‘fuck off contact page?’&lt;/head&gt;
    &lt;p&gt;A “fuck off contact page” is what a company throws together when they actually don’t want anyone to contact them at all. They are usually found on the websites of million or billion dollar companies, likely Software-as-a-service (SaaS) companies that are trying to reduce the amount of money they spend on support by carefully hiding the real support channels behind login walls. These companies tend to offer multiple tiers of support, with enterprise customers having a customer success manager who they can call on this ancient device we call phones, whereas the lower-paying customers may have to wrangle various in-app ticket mechanisms. If you solve your own problem by reading the knowledge base, then this is a win for the company. They don’t want to hear from you, they want you to fuck off.&lt;/p&gt;
    &lt;p&gt;In other words, this is entirely inappropriate for the kind of service-based agency that our client was. The billion dollar SaaS company wants to reduce the number of incoming inquiries, and is hoping to weed out anyone who is not determined to contact them by giving them unsatisfying options. The service company wants to show how helpful they are and cultivate leads. These are fundamentally opposing goals.&lt;/p&gt;
    &lt;p&gt;Let me explain further. I’m not sure about you, but as a user, when I see a button that says ‘talk to our sales team’, I treat the entire region of the page with the same trepidation as nuclear waste. The page is now a no-go zone, and I try to exit as quickly as possible, knowing that whatever my original query was, I’m going to have to solve it unassisted. Seeing as this is a company who makes money off of convincing people to let them handle the easy stuff, adding friction to this key part of their sales funnel just doesn’t feel like a winning strategy.&lt;/p&gt;
    &lt;head rend="h2"&gt;How the fuck did you convince them to change their minds?&lt;/head&gt;
    &lt;p&gt;Try as we might, we couldn’t. In all honesty, we probably could have done more in order to talk them out of it, but the project had gone in such a way where we were focused on trying to talk the client out of changing other things that would drastically increase design or development time beyond the initial scope. In other words, we were too busy putting out other fires. This re-designed contact page, as certain as we were of how bad of an idea it was, wasn’t a fire, so we let it through.&lt;/p&gt;
    &lt;p&gt;The project finished on time, everyone got paid, and the client was happy with the end result, but I still felt very disappointed in the whole thing. While I personally believe in the value of good design, I also believe there are a lot of smoke-and-mirrors in the industry, and I hated the thought that I might have inadvertently contributed to it. Even if the client is happy, it didn’t meet my internal bar for a quality product worth sticking my name on, and I feel like I’ve let down both the client and the end-users.&lt;/p&gt;
    &lt;head rend="h2"&gt;How the fuck do I avoid being in a position where I’m asked to implement a ‘fuck off contact page’?&lt;/head&gt;
    &lt;p&gt;I think our problems started from before we even began to touch a single design tool. As a favor to one of the folks involved, we had discounted our rates for this client, and I think that set us off on the wrong foot. Instead of seeing us as people who brought valuable knowledge and expertise to the project, they saw us as the hands that would execute their vision.&lt;/p&gt;
    &lt;p&gt;Especially for those not familiar with the process of design, it can be tempting to see things like discovery and wireframing as obstacles to be cleared before you get to the fun part, designing the visual identity. Unfortunately, many designers are also guilty of this!&lt;/p&gt;
    &lt;p&gt;As service providers, I believe we need to do a better job on educating clients on the design process and why each step is so important. This is radical idea in some circles, but knowing why you’re building something is a necessary part of doing a good job at it! That’s why we do things like determining the architecture before we start thinking about the brand. Flow charts and diagrams are not as fun as interactive prototypes, but they’re much more important to get right.&lt;/p&gt;
    &lt;p&gt;Also, the discounted pricing probably didn’t help — instead of signaling that we were doing a favor out of respect for them, it just signaled that we were easily exploitable. There was a lack of trust throughout the process, on both sides. While I really want to believe that I can have the kind of relationships with clients where constructive disagreement is welcomed and valued, how I get there is still something I’m figuring out, even many years later.&lt;/p&gt;
    &lt;p&gt;I think that’s part of the reason why I blog. By blogging, I’m putting a body of work out there that communicates my values and ethos. While much of the details of my client work has to remain private, these posts can be public, and hopefully they can help me find people who resonate with what I have to offer. Or you know, just be bold enough to communicate ‘Fuck off’ to those who don’t!&lt;/p&gt;
    &lt;p&gt;(Feel free to reach out if you’re interested in working with folks who care, maybe a little too much, about doing right by your users.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Links and Resources&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I hit publish on this as I listened to Andy Bell’s recent talk at Beyond Tellerand. It has SO many gems of wisdom about the core skills required to deliver successful client projects. It’s also why I love working with Set.Studio, check ‘em out.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46189994</guid><pubDate>Mon, 08 Dec 2025 08:57:19 +0000</pubDate></item><item><title>Einstein: NewtonOS running on other operating systems</title><link>https://github.com/pguyot/Einstein</link><description>&lt;doc fingerprint="1ebe69b779f5a7ea"&gt;
  &lt;main&gt;
    &lt;p&gt;Einstein officially runs on macOS, iOS, and Ubuntu Linux with partial support for Android, Raspberry Pi, and Windows.&lt;/p&gt;
    &lt;p&gt;A Newton ROM file is required to run Einstein. We cannot distribute the ROM file. If you own a Newton device, you may be able to dump your own ROM file from it. See Dumping The Rom for more information.&lt;/p&gt;
    &lt;p&gt;Click here for downloads and more information&lt;/p&gt;
    &lt;p&gt;Once you have Einstein up and running, refer to the user manual.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46190324</guid><pubDate>Mon, 08 Dec 2025 09:42:04 +0000</pubDate></item><item><title>Twelve Days of Shell</title><link>https://12days.cmdchallenge.com</link><description>&lt;doc fingerprint="ceb061ce57b8258"&gt;
  &lt;main&gt;
    &lt;p&gt;It looks like you don't have javascript enabled which is required for cmdchallenge. Create a reaction survey in your browser! View Solutions Learn&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46190577</guid><pubDate>Mon, 08 Dec 2025 10:13:07 +0000</pubDate></item><item><title>One too many words on AT&amp;T's $2k Korn shell and other Usenet topics</title><link>https://blog.gabornyeki.com/2025-12-usenet/</link><description>&lt;doc fingerprint="b72a991405a72bfd"&gt;
  &lt;main&gt;
    &lt;p&gt;Usenet provides a window into the Unix and BSD systems of the 1980s, and some of the hardware that they ran on. Discussions were slow. Computers were expensive. GNU Emacs was big. AT&amp;amp;T charged a lot of money. Usenet was fun.&lt;/p&gt;
    &lt;p&gt;Unix has been enormously successful over the past 55 years.&lt;/p&gt;
    &lt;p&gt;It started out as a small experiment to develop a time-sharing system (i.e., a multi-user operating system) at AT&amp;amp;T Bell Labs.1 The goal was to take a few core principles to their logical conclusion.2 The OS bundled many small tools that were easy to combine, as it was illustrated by a famous exchange between Donald Knuth and Douglas McIlroy in 1986. Today, Unix lives on mostly as a spiritual predecessor to Linux, Net/Free/OpenBSD, macOS,3 and arguably, ChromeOS and Android.&lt;/p&gt;
    &lt;p&gt;Usenet tells us about the height of its early popularity.&lt;/p&gt;
    &lt;head rend="h2"&gt;A casual economics of Unix&lt;/head&gt;
    &lt;p&gt;Unix was not the product of a competitive market.&lt;/p&gt;
    &lt;p&gt;First of all, AT&amp;amp;T was a monopoly. It had the opportunity to allocate a share of its monopoly rent to Bell Labs, concentrating funding toward experimental projects like early Unix.&lt;/p&gt;
    &lt;p&gt;But AT&amp;amp;T was also a regulated monopoly. It was allowed to be a monopoly in telecom but prohibited from operating a non-telecom business. This prevented AT&amp;amp;T from commercializing Unix. Instead, it offered a source license to universities at a relatively low fee.&lt;/p&gt;
    &lt;p&gt;At the same time, universities and the US government (specifically ARPA) had a shared desire for a flexible and portable operating system. Funding from the Department of Defense allowed UC Berkeleyâs CSRG to pay graduate students to build such an OS upon AT&amp;amp;Tâs codebase. The resulting OS was called Berkeley Software Distribution, or BSD.&lt;/p&gt;
    &lt;p&gt;BSD was really good. It pushed the envelope on what Unix could be, and AT&amp;amp;T eventually had to adopt BSD improvements and extensions like demand paging (mentioned later), disk quotas, sockets, job control, or the Vi editor (mentioned later), because AT&amp;amp;Tâs home-grown System III and System V were too limited without them.&lt;/p&gt;
    &lt;p&gt;The VAX-11/780, the machine on which BSD development began in earnest.&lt;/p&gt;
    &lt;p&gt;BSD got widely adopted in the 1980s by universities and research institutions. It was seen as a superior development environment, and vendors, even some big ones like DEC and Sun, based their Unix variants on it.4 Parts of the codebase were also lifted or copied by commercial vendors. Most famous among these is the TCP/IP stack.&lt;/p&gt;
    &lt;p&gt;BSD sockets became the de facto standard API for exposing networking capabilities to application programmers. Compatibility with it was a design goal for Microsoftâs Winsock API, which was how Windows NT could ship direct ports of some BSD userland utilities.5 Apple (and originally, I imagine, NeXT) went even further and based MacOS Xâs entire TCP/IP stack on the BSD codebase. 6 And eventually, the BSD sockets API made it into the POSIX standard.&lt;/p&gt;
    &lt;p&gt;The foundations laid with ARPAâs funding enabled new experiments to flourish. Several network protocols or their dominant implementations began life on BSD:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sendmail, a mail server, first shipped with 4.1cBSD, a preview release of 4.2BSD, in 1983. It was the most popular SMTP server in the 1990s, and still prominent through the early 2000s.&lt;/item&gt;
      &lt;item&gt;The Berkeley Internet Name Domain (BIND) implements the DNS protocol. Its first release was part of 4.3BSD and, at least as of 2015, it still appeared to be the most popular DNS server.&lt;/item&gt;
      &lt;item&gt;The Network News Transfer Protocol (NNTP) made discussion groups accessible via TCP/IP, before Gopher or HTTP existed.7 It was proposed by two students at UCSD and UC Berkeley in 1986, and its reference implementation shipped with 4.3BSD.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;timed&lt;/code&gt;daemon implements the Time Synchronization Protocol, a precursor to the now-widespread Network Time Protocol (NTP). It first shipped with 4.3BSD.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course, not everything network-related originates with BSD. For four prominent examples, the first prototype of an Internet Relay Chat (IRC) server was built on a Sun-3, the first prototype of HTTP was built on a NeXT, and the first implementations of POP and IMAP were not even written on Unix but on TOPS-20 (mentioned later). SunOS, NeXTSTEP, and TOPS-20 were all proprietary operating systems.&lt;/p&gt;
    &lt;p&gt;But it is clear that the Berkeley flavor of Unix created an environment of freedom that fostered practical experimentation, largely detached from market forces.&lt;/p&gt;
    &lt;p&gt;Jon Hall with his New Hampshire license plate that reads âUnix,â alongside the state motto, âLive Free or Die.â He originally got the plate in 1989. (Source: Nashua Telegraph, cited by Ãric LÃ©vÃ©nez in 2009.)&lt;/p&gt;
    &lt;p&gt;I think that by the 2000s, it was Linux that continued the ethos of practical experimentation, although in a very different economic environment. But the 2000s are much too recent to dwell on them.&lt;/p&gt;
    &lt;p&gt;What did computing look like in the 1980s, at the height of the Unix era?&lt;/p&gt;
    &lt;head rend="h2"&gt;Culture&lt;/head&gt;
    &lt;head rend="h3"&gt;Vernacular differences&lt;/head&gt;
    &lt;p&gt;When Usenet started in 1980, it was one of the first systems8 that allowed strangers to communicate with each other. Public bulletin boards appeared around the same time but those were more geographically localized.&lt;/p&gt;
    &lt;p&gt;Usenet was a decentralized network consisting of sites, and messages propagated as sites polled each other for updates. Messages were addressed to newsgroups, like &lt;code&gt;net.unix-wizards&lt;/code&gt;, &lt;code&gt;comp.lang.c&lt;/code&gt;, or &lt;code&gt;comp.bugs.2bsd&lt;/code&gt;. These were similar to the later concept of mailing lists.&lt;/p&gt;
    &lt;p&gt;The early vernacular was peculiar in a few ways.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Articles. It befits the idea of a newsgroup that messages were called articles. Early on, users occasionally even spoke of each other in the third person when sending replies,9 in keeping with the idea of a news article addressed to the group.&lt;/item&gt;
      &lt;item&gt;Wizards. In Unix circles, people who were particularly knowledgeable about the ins and outs of the system were called wizards. In my reading, the word is pretty much synonymous with guru which was also in common use.&lt;/item&gt;
      &lt;item&gt;Hack. &lt;list rend="ul"&gt;&lt;item&gt;A quick fix or perhaps a neat solution to a problem was a hack (jokingly called a haque by a handful of people).&lt;/item&gt;&lt;item&gt;New features were hacked up. People spent their time hacking away, they hacked their merry way through source code, or were simply hacking. Todayâs phrase of hacking on a project didnât exist yet.&lt;/item&gt;&lt;item&gt;Someone knowledgeable who tinkered with a system was often called a hacker, as in Unix hacker or Interlisp hacker, but not for any disrespect for the law. This is the same sense in which Linux developers are still called kernel hackers.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Online. This word originally meant something like âat the computer terminal,â not âvia a network connectionâ like today.10 &lt;list rend="ul"&gt;&lt;item&gt;The original usage is also evident in the enduring description of digital user manuals as on-line manual pages (4.3BSD-Reno, 1990; Slackware 3.1, 1995; MINIX 3.3.0, 2014; macOS 26.0, 2025) and online help files (Windows 3.0, 1990).&lt;/item&gt;&lt;item&gt;Using online to mean âvia a network connectionâ would have been too ambiguous. In the 1980s, there was no single computer network that everyone would connect to. Instead, there were people who connected to specific Usenet sites via dial-in and downloaded software and followed discussions that way, and others who also had access to ARPANET or NSFNET and could use FTP and, by the late 1980s, NNTP.&lt;/item&gt;&lt;item&gt;The shift in meaning took until the 1990s, when the internet became available to the wider public.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Upward compatibility. When comparing two versions of the same software, people preferred to speak of upward (i.e., forward) rather than backward compatibility. Although sometimes they may have done so by mistake.&lt;/item&gt;
      &lt;item&gt;Flaming. People regularly spoke of flaming each other or a piece of software like MS-DOS. Flame wars are still talked about today, but flame as a verb has gone extinct.&lt;/item&gt;
      &lt;item&gt;Trolling. This word did not exist, even though trolls most assuredly did.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Flame wars took days or weeks to unfold&lt;/head&gt;
    &lt;p&gt;Flame wars were more like smolder wars. Discussions, even heated ones, typically developed over days if not weeks. This was because each article in a reply chain could take a day or longer to propagate to readers.&lt;/p&gt;
    &lt;p&gt;Usenet was a decentralized network of sites, a little like todayâs Fediverse. During the early years, each site had to periodically download the latest articles from other sites, typically via dial-up. This sometimes meant polling hourly, three times a day, twice a day, or daily/nightly. But there were sites that had even more latency. When Australia got connected to Usenet in 1983, the Sydney site had the articles delivered by airmail on a weekly schedule.&lt;/p&gt;
    &lt;p&gt;As a consequence, quick throw-away quips were a little more rare. Longer messages with better-developed arguments were a little more commonplace.&lt;/p&gt;
    &lt;head rend="h2"&gt;Business&lt;/head&gt;
    &lt;head rend="h3"&gt;Old computers were sold for a long time, despite Mooreâs law&lt;/head&gt;
    &lt;p&gt;Mooreâs law was already in effect in the 1970s when 32-bit computers became available. Then through the 1980s, memory, storage, and computing capacity all vastly increased. This was not a negligible development.&lt;/p&gt;
    &lt;p&gt;For one thing, the PDP-11, a series of 16-bit minicomputers introduced in 1970, had a logical address space limited to 64 KB of memory. The physical address space could be expanded to 18 bits (256 KB) or 22 bits (4 MB) via virtual addressing, but accessing the additional memory regions was cumbersome because it required switching out the memory mapping table.&lt;/p&gt;
    &lt;p&gt;Yet the PDP-11 remained an important revenue source for DEC throughout the 1980s. As Edward F. Beadel Jr. of SUNY Oswego wrote in 1989:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Last year about 1/3 of DECâs bucks came from PDP-11 and related sales and services. There are still many many users out there.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The architectureâs enduring popularity was mirrored by the Berkeley source releases. 2.11BSD, the last of the 2BSD branch which existed to backport 32-bit BSD features like a TCP/IP stack to the PDP-11, came out in 1991.&lt;/p&gt;
    &lt;p&gt;This may sound like a surprising defiance of Mooreâs law. But what kept the PDP-11 alive despite its limitations was that computers were still very expensive.&lt;/p&gt;
    &lt;p&gt;When personal computers first appeared in the 1970s, they were anything but affordable. For example, the Altair 8800, the machine for which Microsoft wrote its first product, an implementation of BASIC, cost the equivalent of $4,000 in todayâs terms. The original Apple II was even more expensive, equivalent to almost $7,000. And both of these were 8-bit computers.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Year&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Price (US)&lt;/cell&gt;
        &lt;cell role="head"&gt;In 2025 dollars&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;PDP-7&lt;/cell&gt;
        &lt;cell&gt;1965&lt;/cell&gt;
        &lt;cell&gt;18-bit minicomputer&lt;/cell&gt;
        &lt;cell&gt;$72,000&lt;/cell&gt;
        &lt;cell&gt;$736,435&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;PDP-10/10&lt;/cell&gt;
        &lt;cell&gt;1967&lt;/cell&gt;
        &lt;cell&gt;36-bit mainframe&lt;/cell&gt;
        &lt;cell&gt;$110,00011&lt;/cell&gt;
        &lt;cell&gt;$1,062,711&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;PDP-11/20&lt;/cell&gt;
        &lt;cell&gt;1970&lt;/cell&gt;
        &lt;cell&gt;16-bit minicomputer&lt;/cell&gt;
        &lt;cell&gt;$10,800&lt;/cell&gt;
        &lt;cell&gt;$89,647&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Altair 8800&lt;/cell&gt;
        &lt;cell&gt;1974&lt;/cell&gt;
        &lt;cell&gt;8-bit home computer&lt;/cell&gt;
        &lt;cell&gt;$621 (assembled)&lt;/cell&gt;
        &lt;cell&gt;$4,059&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Apple II&lt;/cell&gt;
        &lt;cell&gt;1977&lt;/cell&gt;
        &lt;cell&gt;8-bit home computer&lt;/cell&gt;
        &lt;cell&gt;$1,298&lt;/cell&gt;
        &lt;cell&gt;$6,902&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;VAX-11/780&lt;/cell&gt;
        &lt;cell&gt;1977&lt;/cell&gt;
        &lt;cell&gt;32-bit superminicomputer&lt;/cell&gt;
        &lt;cell&gt;$120,00012&lt;/cell&gt;
        &lt;cell&gt;$638,078&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Commodore 64&lt;/cell&gt;
        &lt;cell&gt;1982&lt;/cell&gt;
        &lt;cell&gt;8-bit home computer&lt;/cell&gt;
        &lt;cell&gt;$595&lt;/cell&gt;
        &lt;cell&gt;$1,987&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Macintosh 128K&lt;/cell&gt;
        &lt;cell&gt;1984&lt;/cell&gt;
        &lt;cell&gt;16/32-bit home computer&lt;/cell&gt;
        &lt;cell&gt;$2,495&lt;/cell&gt;
        &lt;cell&gt;$7,740&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;IBM PC AT&lt;/cell&gt;
        &lt;cell&gt;1984&lt;/cell&gt;
        &lt;cell&gt;16-bit workstation&lt;/cell&gt;
        &lt;cell&gt;$3,995&lt;/cell&gt;
        &lt;cell&gt;$12,394&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;AT&amp;amp;T UNIX PC&lt;/cell&gt;
        &lt;cell&gt;1985&lt;/cell&gt;
        &lt;cell&gt;16/32-bit workstation&lt;/cell&gt;
        &lt;cell&gt;$5,095&lt;/cell&gt;
        &lt;cell&gt;$15,265&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Amiga 500&lt;/cell&gt;
        &lt;cell&gt;1987&lt;/cell&gt;
        &lt;cell&gt;16/32-bit home computer&lt;/cell&gt;
        &lt;cell&gt;$699&lt;/cell&gt;
        &lt;cell&gt;$1,983&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Enterprise computers, like the PDP-11 and its 32-bit successor, the VAX-11, were much much more expensive. The PDP-11/20, the machine on which Unix took off in the early 1970s, cost nearly $90,000 in 2025 dollars. The VAX-11/780, the first of the VAX-11 series, cost the equivalent of $638,000.&lt;/p&gt;
    &lt;p&gt;Software for these computers was expensive, too. Frank R. Borger of the now sadly defunct Michael Reese Hospital wrote about this problem in a letter to a magazine called DEC Professional, probably in 1986 or 1987. BASIC, Fortran, and Pascal compilers were much more expensive for the VAX-11 than for the PDP-11:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If I were to buy three packages for a small VAX versus a Q-bus PDP-11, I would spend approximately $16,700 for the VAX software, $9,000 for the UNIBUS PDP-11 software, and only $3,600 for the Q-bus PDP-11 software. Prices for software maintenance are similarly cheaper.&lt;/p&gt;
      &lt;p&gt;When DEC tells me that it will upgrade me from a MICROPDP-11 to a MlCROVAX for $18,000 (Fall 1986 direct Update), it doesnât mention the $13,000 difference in software costs.&lt;/p&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
      &lt;p&gt;Finally, there are many cases when a PDP-11 has sufficient capacity for a given job: A system often can be put together for half the cost of an equivalent VAX. Thatâs why PDP-11s will be around for a long time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As a consequence, many customers preferred the older machines.&lt;/p&gt;
    &lt;p&gt;However, DEC made sure that upgrading to the VAX was as pain-free as possible. VAX machines had a hardware compatibility mode for the PDP-11. This enabled 4BSD systems to run old V6 and V7 binaries using Arthur W. Wetzelâs emulation layer which translated legacy system calls into modern ones.13&lt;/p&gt;
    &lt;p&gt;Compatibility was a prudent strategy. It extended the lifespan of PDP-11 software, and gave customers an upgrade path to more modern systems while keeping their support contracts with DEC. Without a doubt, this stretched the companyâs lifeline all the way into the 1990s, when it finally succumbed to the wildly more affordable x86-based PCs.&lt;/p&gt;
    &lt;head rend="h3"&gt;AT&amp;amp;T charged $2,000/site for the Korn shell&lt;/head&gt;
    &lt;p&gt;The Korn shell appeared in 1985, and by popular consensus, it was really good. But it was also really expensive, leaving many of its would-be users mere admirers.&lt;/p&gt;
    &lt;p&gt;During the first decade of Unix, the shell was by no means a pleasant experience. The default shell was the Thompson shell, which was then replaced by the Bourne shell in V7, and neither were comfortable for interactive use. For example, neither had:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;command-line editing (like readline),&lt;/item&gt;
      &lt;item&gt;tab-completion for filenames and commands,&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;~&lt;/code&gt;as a shortcut for&lt;code&gt;$HOME&lt;/code&gt;and&lt;code&gt;~user&lt;/code&gt;as a shortcut for the home directory of&lt;code&gt;user&lt;/code&gt;,&lt;/item&gt;
      &lt;item&gt;command aliases,&lt;/item&gt;
      &lt;item&gt;shell history, or&lt;/item&gt;
      &lt;item&gt;job control.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Although Bill Joyâs C shell implemented some of these features in 3BSD, and support for job control was added by Jim Kulp in 4.1BSD, David Kornâs shell quickly became a coveted alternative.&lt;/p&gt;
    &lt;p&gt;David Korn, the creator of the Korn shell. (Source: David Korn, 1998 or earlier.)&lt;/p&gt;
    &lt;p&gt;The C shell had several characteristics that left users wanting:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Its C-inspired syntax broke backward compatibility with the Bourne shell.14&lt;/item&gt;
      &lt;item&gt;As far as I can tell, command-line editing was completely missing.&lt;/item&gt;
      &lt;item&gt;Filename and command name completion was completely missing. Tab-completion (actually, ESC-completion) was added in 1983 by the &lt;code&gt;tcsh&lt;/code&gt;variant which was made by Ken Greer while at Carnegie Mellon.15&lt;/item&gt;
      &lt;item&gt;By some reports, it was slower than the Bourne shell, but this was debated.16&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Korn shell had none of these problems. Two of its celebrated features were command-line editing, which it supported with both Emacs and Vi keybindings, and shell history. And AT&amp;amp;T tried to monetize this.&lt;/p&gt;
    &lt;p&gt;Randy King of AT&amp;amp;T jokingly paraphrased David Korn in March 1985 on &lt;code&gt;net.unix&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Yes, ksh is available. No, itâs not $100,000K per cpu. As I understand it, it is $2K per site; i.e. all of your CPUâs can have it for that one-time cost.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In particular, the Korn shell was made available via the AT&amp;amp;T toolchest which was an online storefront. K. Richard Magill indicated in February 1986 that it was accessible via dial-up with the &lt;code&gt;cu&lt;/code&gt; utility:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The AT&amp;amp;T toolchest is a bulletin board style store that sells source for a number of things that AT&amp;amp;T doesnât want to support. This includes ksh.&lt;/p&gt;
      &lt;p&gt;cu 1-201-522-6900&lt;/p&gt;
      &lt;p&gt;It will tell you all you care to know.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Of course, $2,000/site (around $6,000/site in 2025 dollars) was a hefty fee for a shell. Gene Spafford of Georgia Tech was of the view that it was too hefty:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Counterpoint: Educational institutions pay $800 to get source licenses for Unix Sys V. Another $150 (?) will get a Berkeley distribution tape. Do you think many of us are going to pay $2000 for ksh? No! Nor are many of us going to shell out the bucks that AT&amp;amp;T is charging for the new uucp stuff. I donât know about other places, but we canât afford it here.&lt;/p&gt;
      &lt;p&gt;I guess many educational institutions, especially the public ones, will never find out if the Korn shell is all that great, or if Honey DanBer uucp is the niftiest thing since sliced yogurt. Whether that will affect the future market for other AT&amp;amp;T goodies is beyond my ability to forsee.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In response, Griff Smith of AT&amp;amp;T Bell Labs called Georgia Tech a âshoestring operationâ for not being able to afford the Korn shell:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I suppose the âsoftware for the peopleâ contingent will rise in righteous indignation, butâ¦&lt;/p&gt;
      &lt;p&gt;Assuming that the current academic price for System V is still $800, we are still giving it away. If your school canât afford normal commercial software prices for at least some key utilities, it must be a shoestring operation. When I was at the University of Pittsburgh, it was âbusiness as usualâ to shell out about $5000/year for a software maintenance contract; we had to pay $10000 for the âvirtual memoryâ upgrade for our TOPS-10 operating systems. Any useful software product usually cost us at least $1000, and some were in the $5000 to $10000 range. A large product such as DISSPLA/TEL-A-GRAF had a special educational rate of âonlyâ $20000; the commercial rate was at least $80000.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Smith did mention that the Korn shell had also gained support for job control which would have made it more attractive to BSD users. Previously, only the C shell supported job control.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The Korn shell is a good product! I used csh for a year; then Dave finally added BSD (i. e. real) job control and command edit mode to ksh. I switched, and havenât looked back. Given the improvements it can add to your computing environment, the price is low by commercial standards.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But universities like Georgia Tech were not the only sites for which the price was not right. Dave Ihnat of Analyst International Corporation agreed that the Korn shell was great:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In his article, Randy King apologetically praises Daveâs ksh. No apology is needed; itâs truly an outstanding product, and deserves whatever praise it gets. While I was on contract at Bell Labs, I used it extensively; now that my contract is over, I miss it intensely. But I do have to take exception with the claim that itâs available.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But he considered it so unjustifiably expensive as to be unavailable. Customers couldnât simply buy a source license for the Korn shell either. They had to also buy a source license for the whole OS which, as I understand it, cost tens of thousands of dollars:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Yes, it now can be bought; but, according to the âAT&amp;amp;T Toolchestâ, which I called just to make sure Iâm not mistaken, itâs $2000 per site for a source license, and $20000 for a vendor license for object redistribution. Also, not mentioned by the Toolchest, but certified as applicable by another source, is that you must have a System V source license to buy the source.&lt;/p&gt;
      &lt;p&gt;Iâm sorry, but I hate to break it to AT&amp;amp;T that most sites donât need or want System V source licenses. Many purchase a machine for which a vendor has done the port; they can afford the object license, but as businesses have neither need, desire, nor cash to buy their own source licenseâlet the vendor fix bugs. Yet, at $20000, the vendors are going to have to charge a substantial sum to recoup their loss on the ksh source. Try to explain to a bursar or comptroller why you want to spend hundreds to thousands of dollars on a new shellâI dare you. The fact of the matter is that, whether we like it or not, itâll be darn hard to justify significant cash expenditure for a tool which will replace a tool which is currently doing the job, be it âshâ or âcshâ.&lt;/p&gt;
      &lt;p&gt;The same applies for the honey-danber uucp package (which, I was surprised to note, is not offered on the Toolchest menu). Apparently, $2000/object license (could someone verifyâis that per machine, or organization? In any case, itâs extreme). Again, try to justify that type of cash outlay to an organization which has a tool that works already. Yes, I have to nurse it, watch it, and beat on itâbut Iâm already doing that, and weâre getting our mail and uucp traffic, sooner or later.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ihnat pointed out that AT&amp;amp;Tâs pricing strategy was undermining its goal of selling their supermicrocomputers to small businesses and individuals:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;All of the preceeding totally ignores the fact that the number of Venix- and Xenix-based small Unix(Tm) systems, owned by both individuals and businesses, is huge, and that AT&amp;amp;T is agressively marketing the 3B2. Obviously, the average individual cannot afford a source license, or a $2000 object license, orâ¦&lt;/p&gt;
      &lt;p&gt;Finally, I question the propriety of overcharging in the extreme for the practice of, effectively, offering corrected versions of products which are already provided with the existing software, but are so bug-ridden as to be apocryphal.&lt;/p&gt;
      &lt;p&gt;Noâ¦I donât think that ksh (or, for that matter, honey-danber uucp) is really available to Unix users yet. As I said before, I applaud the efforts of Dave, Pete, Dan, and Brian; âthey done good, real goodâ. And I can understand that itâs difficult for AT&amp;amp;T to figure out immediately what is the best marketing strategy, after so many years as a regulated monopoly. But, in the end, Iâm the one with a Unix machine at work, and one at home, and canât justify the cash outlay for the tools at work, and canât afford it at home; and thatâs the bottom line. If itâs not affordable, itâs not available.&lt;/p&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This pricing strategy is part of why we have bash (1989), pdksh (1989?), zsh (1990), OpenBSD ksh (1995?), and mksh (2002? or 2006?), among other shells that also cloned or took deep inspiration from the original Korn shell. Eventually, AT&amp;amp;Tâs ksh was also open sourced, in 2000.&lt;/p&gt;
    &lt;head rend="h3"&gt;Software vendors were in dire need of a Unix standard&lt;/head&gt;
    &lt;p&gt;The multitude of Unix variants was truly bewildering. The situation was so bad that it posed a coordination problem for application developers.&lt;/p&gt;
    &lt;p&gt;For example, at least four different variants existed for the Intel 80286 processor as of 1985. Microsoftâs Xenix was one of those four, so naturally, Bill Gates had an opinion on this. He said that binary compatibility across the 80286 Unix variants was a prerequisite for commercial success.&lt;/p&gt;
    &lt;p&gt;This claim was at least a little bit controversial. Unix wizards were used to software distributed as source code that they then needed to port and compile for their own systems. It was considered normal to have to iron out bugs and add desired functionality by modifying the source. From this perspective, what mattered was source compatibility. If 90 percent of all code compiled, and the rest could be easily ported, that was good enough.&lt;/p&gt;
    &lt;p&gt;But Larry Campbell, an application developer, explained that binary compatibility was the only way to the mass market:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;A software vendor can write a program for the [â¦] IBM PC architecture, and can be assured that the executable binary will run on over TWO MILLION MACHINES. Thatâs two million potential customers, folks.&lt;/p&gt;
      &lt;p&gt;Now, sure, software vendors could just ship sources in shar archivesâ¦ on 69 different types of mediaâ¦ and let the customers compile itâ¦ and maybe it would compile everywhereâ¦ and maybe nobody would rip off the source code and resell itâ¦ But letâs get serious. End users neither want nor need source code, nor compilers, nor shar archives, nor any of that crap. They want to buy a little black biscuit with bits on it that just plugs into their little 16-bit toaster and does their application, right out of the box, no compilation or customization or messing around required.&lt;/p&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
      &lt;p&gt;You need to have a single (or at least a dominant) binary format and media standard because dealers and distributors cannot afford to stock 69 different versions of each product. [â¦] Thereâs no good reason, for instance, that Xenix, Venix, and PC/IX couldnât use the same register conventions and same a.out (x.out) formats and the same system call numbers.&lt;/p&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;He concluded with a pithy summary:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Yes, I prefer Unix. But I also prefer large quantities of money to smaller ones. Thatâs why I develop software for the IBM PC.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The problem was not limited to the 80286. The Motorola 68000 (or m68k) was maybe even worse. Michael Tilson of the HCR Corporation emphasized that building, testing, and distributing separate binaries for each Unix variant for the m68k was prohibitively costly:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Distribution in source code form is not the answer for commercial software. One could imagine software vendors who sell source code only, but this is not practical for most vendors. Software is already easy to steal, and unlimited distribution of source code is almost a license to steal (it certainly requires a much larger policing effort.) Most software vendors make their living selling binary copies, with only the occasional source code sale. If the software could be protected, most would be happy to sell source only, but it canât and they donât. There are technical problems as well â you want to sell a program that you know will run. If you havenât actually compiled it on the target machine, you donât know what compiler bugs you might hit, etc. (Note: Please no flames about the moral wrongness of binary code. In our economic and legal system, binary code will continue to be the norm, even if some think it wrong.)&lt;/p&gt;
      &lt;p&gt;Therefore vendors sell binary code. As a software vendor, we at HCR find the multiplicity of machines to be a real pain. We can live with the fact that you must make a 68000 version and a VAX version of a program, but it is very costly to make 10 different 68000 versions. A binary standard would eliminate needless duplication effort. As software companies go, we are fairly big (60 people using almost $1,000,000 worth of computer equipment) but we canât afford to deal with all of these formats. Therefore we are targeting our new products to a few âwinnerâ machines.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Perhaps Tilson exaggerated when he wrote â10 different 68000 versions,â but not by much. The m68k was a popular architecture for workstations and home computers. HP, Sun, Microsoft, AT&amp;amp;T, Silicon Graphics, NeXT, and Apple, as well as smaller companies like Datamedia Corp. and Pixel Computer Inc., all made Unices or Unix-likes that ran on CPUs from this series.17&lt;/p&gt;
    &lt;p&gt;On this scene, HCR may have been something of a Red Hat or SuSE for Unix, albeit closed source. It was described by Byte magazine in 1994 as the second firm to commercially support Unix. The computer equipment cost that Tilson cited translates to $50,000/person in 2025 dollars, which is not insubstantial. And HCR had the life arc of a successful startup: it was founded in 1976 and got acquired by SCO in 1990, to serve as their Canadian branch. Unfortunately, it then went defunct in 1996.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hardware&lt;/head&gt;
    &lt;head rend="h3"&gt;Virtual memory was the hot new thing&lt;/head&gt;
    &lt;p&gt;The VAX-11/780 (1977), and the less powerful but smaller 11/750 (1980), were Unix workhorses through at least the mid-to-late-1980s.&lt;/p&gt;
    &lt;p&gt;The new VAX machines came with a memory management unit (MMU) that allowed kernels to implement virtual memory using demand paging. Demand paging is now standard, but it wasnât in the 1970s. What it does is it lets running applications hold onto memory regions (pages) that are not actually in physical memory unless they are accessed (demanded).&lt;/p&gt;
    &lt;p&gt;The VAX-11/750 supermini. (Photo credit: JosÃ© Luis Echeveste, 1988. License: CC-BY-SA.)&lt;/p&gt;
    &lt;p&gt;Before the 780, no Unix system implemented demand paging. They had to instead resort to a memory management strategy called swapping. As I understand it, these early kernels kept the entire memory image of each active process in physical memory. When physical memory ran out for a scheduled process, they moved the entire image of one or more other processes from memory to storage, thus freeing up memory for the image of the scheduled process. In the words of Joseph L. Wood of AT&amp;amp;T in 1983:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Steve Dyerâs article makes the same semantic mistake that I have seen in many articles recently. The specific statement made is that BTL UNIX doesnât have âvirtualâ memory for its VAX version. Of course it has virtual memory; what it doesnât have and what all the submitters mean is that BTL UNIX doesnât implement a paging virtual mamory system. BTL UNIX implements virtual memory meaning that the address bit pattern generated in an instruction or by some arithmetic operation in a register for example is translated by a memory management unit before a main store reference is made. On the other hand, when BTL [Bell Telephone Laboratories] UNIX needs some more main store, it selects a process to be deleted and ships the whole image to a swap device. When Berkeley UNIX needs some more main store it looks for a page to delete. This is more efficient than the BTL way. The other alternative which is becoming more attractive for many sites is to just buy enough memory. That runs faster than either.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;There were two problems with swapping. First, by all accounts, it was slow. If you were the unlucky owner of an interactive process, say, a text editor, that got swapped out while running, this ordeal made you unhappy. Sure, this was a problem on underpowered PCs like the AT&amp;amp;T 6300 Plus which lacked an MMU capable of demand paging. But it was even an issue on supercomputers where physical memory was contended by many concurrent users (mentioned later, tangentially).&lt;/p&gt;
    &lt;p&gt;The other problem with swapping was that the kernel could not run any process whose memory image was larger than the machineâs physical memory. Guy Harris mentioned integrated circuit design as an application where this constraint may have been binding:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Unfortunately, I believe there are applications where âbuy enough memoryâ is either impractical or impossible. I believe a lot of the applications that 4.2BSD is being used for simply require more address space than you can provide purely with the physical memory attachable to a VAX; the VLSI design and image processing software that has been mentioned as prime applications for VAXes and 4.2BSD may fall under this heading.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A paging kernel can, for example, support the &lt;code&gt;mmap&lt;/code&gt; system call which lets applications read and write files as if they were fully loaded into memory, even if those files are larger than the available physical memory. An overview of this and other benefits of paging was offered by Randolph Fritz of Western Union Telegraph in 1983.&lt;/p&gt;
    &lt;p&gt;AT&amp;amp;T Unix, however, used swapping even on VAX machines until System V Release 2 in 1984. They ported V7 to the 780, the result of which was UNIX/32V, but this port did not implement demand paging. So one of the goals of 3BSD in 1979 was to do just that by taking advantage of the 780âs MMU.&lt;/p&gt;
    &lt;head rend="h3"&gt;Even with virtual memory, superminicomputers were slow&lt;/head&gt;
    &lt;p&gt;With its 32-bit port, BSD became a VAX-first Unix. Partly due to its better memory management, it proved useful for universities, research labs, and many enterprise users. Then, since ARPA also purchased several VAX-11/750s, 4.1BSD in 1981 added support for these machines, too.&lt;/p&gt;
    &lt;p&gt;The VAX-11/780 and the 750 were called superminicomputers or superminis. But even with demand paging, they were not what we would recognize as fast. On some level this is obvious: the 780 came with a 5 MHz (1 MIPS) CPU and it supported between 2 to 8 MB of memory.18 But the comparison is interesting because we still use very similar tools to what people ran on BSD.&lt;/p&gt;
    &lt;p&gt;Think about the code you write and run today. How much longer would it take to run it on a VAX in 1985?&lt;/p&gt;
    &lt;p&gt;For statistical analysis, one of the popular languages in use is R. Râs proprietary precursor was called S, and it ran on 32-bit Unix. Jim Leinweber of UW Madison shared performance measurements on their 780 running BSD:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;S snippet&lt;/cell&gt;
        &lt;cell role="head"&gt;11/780 running 4.2BSD&lt;/cell&gt;
        &lt;cell role="head"&gt;ThinkPad X1 Carbon running Linux19&lt;/cell&gt;
        &lt;cell role="head"&gt;Multiplier&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;write(1:10000, "/tmp/junk")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;25Â s&lt;/cell&gt;
        &lt;cell&gt;14.04Â ms&lt;/cell&gt;
        &lt;cell&gt;1,781&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;m &amp;lt;- matrix(read("/tmp/junk"), 100, 100)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;20Â s&lt;/cell&gt;
        &lt;cell&gt;7.72Â ms20&lt;/cell&gt;
        &lt;cell&gt;2,591&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;for (i in 1:20) for (j in 1:20) m[i,j] &amp;lt;- sin(j/10)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;timed out&lt;/cell&gt;
        &lt;cell&gt;3.00Â ms&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here is what each row means:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Creating a vector of 10,000 integers and writing it to a file took 25 seconds.&lt;/item&gt;
      &lt;item&gt;Reading the vector back into memory, turning it into a square matrix, and assigning the result to a variable took 20 seconds.&lt;/item&gt;
      &lt;item&gt;Initializing a 20-by-20 matrix element-by-element took too long to complete.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And 4.2BSD was the fast Unix (mentioned later).&lt;/p&gt;
    &lt;p&gt;It would be easy to blame Sâs slowness on the 780âs CPU or memory. CPU clock rates were lower, after all. Memory access was slower, too. If I understand the 780âs hardware handbook correctly, it had a memory bandwidth of about 13 MB/s.&lt;/p&gt;
    &lt;p&gt;But none of the benchmarked S snippets were CPU-bound, nor were they memory-intensive. The 780 came with at least 2 MB of physical memory, and 10,000 unboxed &lt;code&gt;int&lt;/code&gt;s fit in less than 40 KB. Even if S boxed every &lt;code&gt;int&lt;/code&gt;, it is unlikely that the vector used more than 80 KB.&lt;/p&gt;
    &lt;p&gt;Rather, the snippets were bottlenecked by I/O. On these systems, writing data to temporary files was common practice because memory was an extremely scarce resource. And boy, did S take the practice to heart. In Jim Leinweberâs words:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[â¦] Iâm not an expert on the bowels of S, but a cursory glance at $M/lang3.yr shows that each occurence of `&amp;lt;-â invokes $F/assign, which in turn calls $L/getds, $P/pcopy, and $L/putds. Thus one problem with the nested for loop is that assignment is very expensive in S; apparently each assignment copies a dataset from one file to another! Doing O(n^2) complete file rewrites to initialize a matrix is bound to be slow&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So I/O was not only slow, it was also more frequently necessary to resort to it because memory pressure forced the kernel to swap and applications to save their work to storage.&lt;/p&gt;
    &lt;head rend="h3"&gt;12-core machines already existed in 1985, kind of&lt;/head&gt;
    &lt;p&gt;Interactive workloads were very ill-suited for single-CPU multi-user systems like the VAX-11/750. Nowadays, we carry multi-core entertainment systems in our pockets. Yet in the 1980s, typically even machines the size of a large fridge had only one single-core CPU.&lt;/p&gt;
    &lt;p&gt;But there were exceptions. Notably, Sequent Computer Systemsâs Balance line of machines were multi-CPU systems with 10 MHz processors from National Semiconductors. The Balance 8000 could be configured for up to 12 CPUs, with six dual-CPU boards. These machines shipped with a modified version of 4.2BSD, so they could run any workload that a stock 4.2BSD could in its native habitat, on an 11/750.&lt;/p&gt;
    &lt;p&gt;Keith Packard of Tektronix Inc. wrote in 1985 on &lt;code&gt;net.unix-wizards&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Sites like this might want to look into systems like the sequent balance 8000 or other multi-cpu systems. We have had a sequent box for about 3 months and I, for one, would never consider buying a vax again in a multi-user environment.&lt;/p&gt;
      &lt;p&gt;Itâs got 6 32016âs and a mess of iopâs and runs 4.2 unix. For single job execution it performs about like an 11/750. For 6 job execution it performs about like 6 11/750âs. Even the i/o bandwidth doesnât seem to slow it down a bit, the notoriously cpu bound 4.2 file system has a party with 6 cpuâs serving it!&lt;/p&gt;
      &lt;p&gt;And, the best part, it costs less than a single 11/780! Also, it is housed in a rather small box (1m deep, 2m wide and &amp;lt;1m high). I use it for software development - edit, compile, linkâ¦ and have been working with ~1M of code. I was on an 11/780 with about 30 other software designers, load averages of 20-40 not uncommon. The sequent box has been wonderful.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Despite the Sequent Balanceâs good showing against the 11/750, I donât imagine that these models came close to the VAXâs popularity. I couldnât find a single photo of a Balance machine anywhere, either in promotional materials or deployed. There are several photos of what it looked like inside the box though.&lt;/p&gt;
    &lt;p&gt;A 360mm-by-310mm dual CPU board from a Sequent Balance, showing two NS32332 processors alongside their cache memory. Customers could add multiple such boards to a Sequent machine if their workload required. (Source: cpu-ns32k.net.)&lt;/p&gt;
    &lt;p&gt;What happened to Sequent after this? The company kept up with the growth of the 80386 and started building machines around Intel CPUs. Eventually, they got acquired by IBM in 1999. Although both Sequent and IBM said that Sequentâs products would continue to be sold, IBM discontinued them by 2002.&lt;/p&gt;
    &lt;head rend="h3"&gt;Laptops were expensive and heavy&lt;/head&gt;
    &lt;p&gt;By the late 1980s and the early 1990s, laptops started becoming affordable. Among manufacturers of such portable computers, Toshiba was well-regarded.&lt;/p&gt;
    &lt;p&gt;Even 16-bit laptops were useful quality-of-life improvements. One such computer was the Toshiba T1000SE, which used the Intel 80C86. John Osler of Dalhousie University used his as a thin client to their VAX in 1991:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I use my T1000SE extensively as a terminal emulator for our VAX mainframe by running the EM4105 software package from Diversified Computer Servives. It has no trouble communicating at 19200 bps and has a subset of Kermit available for file transfer operations. Also included are a number of screen, printer and plotter graphics drivers, among these, the T3100 driver uses the full 640 by 400 resolution of the T1000SE screen to emulate a Tektronics 4010, 4105 or VT640 graphics terminal. All this to say that the the T1000SE can be used very effectively as a terminal emulator.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But neither affordable nor portable meant then what we think today that it means.&lt;/p&gt;
    &lt;p&gt;The T1000SE weighed only 2.6 kg (5.8 lbs; see brochure) and sold for about $1,200 in 1990 (equivalent to $2,960 in 2025). That doesnât sound so bad compared to high-end laptops today but we need to remember that this was a 16-bit computer.&lt;/p&gt;
    &lt;p&gt;Gaston Groisman of U Calgary also had good things to say about the T1000 but said that the laptops used by his wifeâs professors rather difficult to lug around:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;wife has a T1000 and she caries it to school almost every day (walking). She deos mostly word processing and some e-mail and loves it. At the same time some of her profesors have heavier machines (Zenith 181?) and they keep them on their desks all the time, even though they drive!! We also took the laptop in out trip back home to Argentina and it was a great way to get my daily fix of programing and playing. Finally, given the prices I have seen advertized the T100 seems the best combination for writing, traveling, etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Zenith 181, released in 1986, had a 16-bit Intel 80C88 processor and 640 KB of memory. It weighed 5.4 kg (11.8 lbs; see brochure).&lt;/p&gt;
    &lt;p&gt;For a higher-end example, the Toshiba T3100SX, released in 1989, was more desirable. It had a 386 CPU and a 40 MB hard drive. It was sold with 1 MB of RAM but that could be expanded to 13 MB. With two batteries, you could get 2 to 5 hours of battery life out of it.&lt;/p&gt;
    &lt;p&gt;All in all, this laptop should have been a wizardâs trusty travel companion. It being a 386-based machine made it particularly attractive because of the prospect of running Unix on it. No more messing around with the 80C86 or the 80286!&lt;/p&gt;
    &lt;p&gt;A laptop with demand paging and the weight of seven ThinkPad X1 Carbons.&lt;/p&gt;
    &lt;p&gt;But the T3100SX cost upwards of $5,000 (about $13,000 in 2025 dollars). And it weighed 6.8 kg (15 lbs; see a contemporary brochure and a later fact sheet).&lt;/p&gt;
    &lt;p&gt;Toshiba also made other 386-based laptops. John R. Levine of Segue Software asked about the T5200 in 1990:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I am looking for a Unix system that I can take with me on trips, so as not to lose a momentâs hacking time merely because I happen to be on top of some mountain with spectacular views or something. Here are what I think I need:&lt;/p&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
      &lt;p&gt;Running on batteries is unimportant, when Iâm on a plane I sleep. The Toshiba T5200 appears to qualify nicely on every point except perhaps the last. (It has one long and one short internal slot, enough for an internal Telebit modem and an Ethernet card.) Has anyone actually run 386/ix or some other Unix on one? Is there some other machine I should be considering? A lunchbox machine like the Compaq Portable 386 would be a possibility, though the Compaq is pretty expensive and its expansion memory is unbelievably expensive.&lt;/p&gt;
      &lt;p&gt;Thanks, as always, in advance&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The T5200 weighed 8.4 kg (18.5 lbs; see brochure). Contemporary brochures donât mention a price, but Keith Comer, a product manager at Toshiba America, said in a 1989 TV program that prices started at $9,500 ($24,697 in 2025). It is hard to imagine that there was a market large enough for the these laptops for Toshiba to break even.&lt;/p&gt;
    &lt;p&gt;So Unix-ready laptops were heavy luxury items that most people seemed to buy on organizational budgets. But laptops were already taken on airplanes. The fear that they emit radio waves that interfere with navigation systems had mostly been put to rest, and many people took advantage of this. Joseph S. Brindley of California Polytechnique wrote in 1990:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This was an early fear expressed by ignorant airlines when laptops first came out but it is not grounded in fact as most, if not all, laptops have a FCC class b rating. I have used mine many times. No crashes yet! :-)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The exact etiquette of bringing electronics aboard a plane may still have been in flux. Anthony J. Stieber of UW Milwaukee wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;At one time there was a lot of hysteria about problems with RFI from laptops on aircraft, but that was several years ago. Before I flew it was suggested that laptop users talk to the pilot about using a laptop on board. [â¦] Basicly they were pretty unconcerned about it. It was like they were thinking âWhy are you asking me? I donât care.â [â¦] I would still ask the pilot about using any kind of electronics that I bring on board the plane.&lt;/p&gt;
      &lt;p&gt;Going through airport security was interesting. I asked to have my machine hand checked rather than go through the x-ray machine. They wanted me to turn the machine on, itâs a Toshiba T1000 which boot from a ROM disk, the guard remarked on how quickly the machine came up. That was a bit scary, especially when they didnât ask to look in the large zippered pocket or the largish disk cases. The thing that most excited them were the cables and my shaver in my backpack which they saw as it went through the x-ray machine. They did ask to look in there, but lost interest after a quick look.&lt;/p&gt;
      &lt;p&gt;Iâm sure security will vary alot depending on which airport it is, where youâre going, and whether there have been threats or problems recently.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Software&lt;/head&gt;
    &lt;head rend="h3"&gt;Unix had to be tinkered with, but it could at least be tinkered with&lt;/head&gt;
    &lt;p&gt;Unix was not the only operating system in use on enterprise computers. Even ignoring third-party OSes, DEC itself shipped several alternative systems for its machines. Its PDP-11 minicomputers ran RSX-11 and RSTS/E into the 1990s. The PDP-10 mainframe, a 36-bit architecture, ran TOPS-20. The VAX superminis ran VMS.&lt;/p&gt;
    &lt;p&gt;Among these, Usenet article authors spoke particularly fondly of TOPS-20. During the time when command and file name completion on 32-bit Unix was only implemented by tcsh and AT&amp;amp;Tâs $2,000 ksh (mentioned earlier), and when Unixâs documentation was generally lacking, TOPS-20 already had both of these areas covered.&lt;/p&gt;
    &lt;p&gt;Yet nowadays, it is VMS that is the best remembered (and in fact still actively maintained as âOpenVMSâ). It was backwards compatible with RSX-11, and generally regarded as a stable, low-fuss OS.&lt;/p&gt;
    &lt;p&gt;In 1984, Jon Forest of UCSB liked VMS more than Unix because the latter was too much work to keep running:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I run VMS. One of the reasons I prefer VMS to Unix is because VMS is much easier to maintain. In essence, I donât do any maintainence because DEC does it all for me at a fixed rate. I can plan my budget knowing exactly how much it will cost be to run VMS. With Unix, software maintainence requires one or more gurus who spend lots of time on the phone, going to conferences, reading nets like this, and hacking. The worst part of this is that so much effort is duplicated. For example, how much time has been spent by all the Unix users in the world to find and fix the bugs that are now being described. I bet that each bug has been found and worked on by more than one person. This is wasted time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The counterpoint was that Unixâs source license enabled sites to fix bugs without waiting for the vendor. As Henry Spencer of the University of Toronto put it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Having DEC do all your software maintenance has the obvious advantage that you donât have to do the work. It has the obvious disadvantage that you canât do the work even if you want to and need to. Your degree of satisfaction is clearly a function of how responsive DEC is, and you have no input in deciding that. Since you run VMS, you have no viable alternative if you come to dislike their service; they know this.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;GNU/Linux and a truly free BSD were still almost a decade away, but Unix already gave at least its academic users more freedom than any other OS. This relative freedom defined Unixâs culture.&lt;/p&gt;
    &lt;p&gt;Mark Crispin of Stanford University thought that Unix wasnât the only OS that needed âwizardsâ:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This sort of issue comes up whenever people get the impression that there are any absolutes. Just about any system can benefit from having an on-site wizard, even if the operating system is manufacturer-supported (e.g. VMS, TOPS-20, VM/370). While the cost of ownership of a wizard is non-trivial (yes, they do âspend lots of time on the phone, going to conferences, reading nets like this, and hackingâ), consider the alternative. You are either stuck with the product as it comes from the manufacturer or you find yourself forced to rent a wizard â that is, you must hire a consultant.&lt;/p&gt;
      &lt;p&gt;Now I have nothing against consultants! Iâm a full-time rental wizard (tr: independent consultant) and I find the business quite lucrative. I hope that attitudes such as Jon Forrestâs continue â customers with that attitude comprise most of my business.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Rather than the wizard issue, he saw two major problems with Unix. The first was tribalism, his description of which rings familiar:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The âpeople problemâ with Unix is not the wizards, but rather the groupies. I define a âUnix groupieâ as any individual who (1) considers Unix in its present state to be software perfection, (2) refuses to believe that other operating systems have features too, (3) makes noises of disgust whenever some other operating system is mentioned, (4) makes noises of disgust whenever some programming language other than C is mentioned. Itâs reminiscent of the APL\360 groupies of 15 years ago.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The second problem was the fractured landscape created by Unixâs many variants (mentioned earlier):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Unix does have a software maturity problem. I for one would love to see a standard Unix. It unnerves me when I must relearn âhow to do Xâ just because Iâm using somebody elseâs Unix system. Many of these incompatibilities seem to be completely gratuitous. Also, Unix lacks some very basic facilities which are only now starting to appear: process-to-process memory mapping (for both read and write), process-to-file memory mapping, file interlocks, long file names, user-friendly command interfaces (sh, csh, ksh, etc. are many things, but user-friendly is not one of them), etc. I wish that these things would all appear in all places in the same way, but I fear that in just about every minor version of Unix itâll be completely different.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;He did think that VMS had its place, but he was not as impressed by it as some others on Usenet seemed to be:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Unix is clearly not for the fainthearted. If you really donât care all that much what the operating system does for you â e.g. all you want is a FORTRAN engine â then Unix may not be your answer. You can use a âthrowawayâ operating system such as VMS. If you actually start USING some special feature of your operating system, you may start caring about what happens when you have to change computer vendors.&lt;/p&gt;
      &lt;p&gt;Finally, I cannot let the comment about âUnix being better than any other operating system (except VMS)â go by unchallenged. I canât see how anybody can possibly make such grand claims about VMS. Itâs the manufacturer-supplied operating system for a superminicomputer which is now (with the 8600) selling at (high) mainframe prices. Itâs an upgrade from an earlier minicomputer operating system from that manufacturer, but still some years(!) away from achieving the level of functionality of other operating systems from that manufacturerâs other product lines! Itâs still a dinosaur.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Those âother operating systemsâ that Mark Crispin was talking about? Mostly TOPS-20, which he still ran at home in the 2000s.&lt;/p&gt;
    &lt;head rend="h3"&gt;BSD was generally seen as faster and more innovative than System V&lt;/head&gt;
    &lt;p&gt;System V had a reputation for being better documented and making fewer breaking changes than BSD. However, many innovations and improvements to System V originate from BSD. Iâve already mentioned some examples earlier. But BSD also added smaller quality of life improvements, like filenames that could be longer than 14 characters and a C compiler that allows identifiers longer than seven or eight characters (where the System V compiler imposed limits of seven for globals, eight for locals).&lt;/p&gt;
    &lt;p&gt;BSD was generally considered faster than System V. This was also the case for Berkeleyâs âfast file systemâ (FFS), although there were some conflicting reports in this case. John Bass mentioned in 1985 that this was probably because ARPA and other major BSD users were running atypical workloads that involved large files:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;People forget that 4.1 and 4.2 were paid for and tuned for AI Vision and CAD/CAM projects sponsored by ARPA and various compainies. For the job mix that 4.x systems are tuned for it is the ONLY way to do those jobs on a UNIX machine with any cost effectiveness.&lt;/p&gt;
      &lt;p&gt;Many tradeoffs that go into 4.x systems are directly counter the best ways to tune UNIX systems for development environments â¦ but they were the only way to make things work for the target applications.&lt;/p&gt;
      &lt;p&gt;The converse is also true to a large extent â¦ V7/SIII/S5 kernels donât handle large applications well or at all â try running a 6mb application on an older bell system with swapping â¦. it takes many seconds for a single swap in/out.&lt;/p&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
      &lt;p&gt;As for the 4.2 âfast filesystemâ â¦ it was again tuned to make large file transaction run at an acceptable rate â¦. try to load/process a 4mb vision or cad/cam file at 30-50 1k block transactions per second â it will run SLOW compared to a production system with contigous files. A number of tradeoffs were made to help large file I/O and improve the transaction rates on very loaded systems (LIKE ucb ernie â¦ the slowest UNIX system I have ever used â¦. even my 11/23 running on floppies was faster).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As Guy Harris pointed out in a different discussion, BSD was also well-suited for integrated circuit design which represented a similar workload (mentioned earlier). Overall, Berkeleyâs early support for demand paging went hand in hand with performance tuning in other subsystems, like FFS.&lt;/p&gt;
    &lt;head rend="h3"&gt;The original &lt;code&gt;/bin/sh&lt;/code&gt; supported goto&lt;/head&gt;
    &lt;p&gt;The Bourne shell, introduced in 1979 in V7, is what we think of today as the Unix shell. But Unixâs original &lt;code&gt;/bin/sh&lt;/code&gt;, the Thompson shell, was an uncanny creature.&lt;/p&gt;
    &lt;p&gt;This old shell had already fallen into obscurity by 1986, when Kenneth Almquist gave a taste of it on &lt;code&gt;net.unix-wizards&lt;/code&gt;. (You might recognize Almquist as the original author of &lt;code&gt;/bin/sh&lt;/code&gt; on NetBSD and Debian.)&lt;/p&gt;
    &lt;p&gt;He provided the following sketch of an early shell script:&lt;/p&gt;
    &lt;p&gt;This snippet is nothing if not strange. How could &lt;code&gt;/bin/goto&lt;/code&gt;, a child process, alter the order in which the shell script was executed?&lt;/p&gt;
    &lt;p&gt;Almquist explained that this was possible because the Thompson shell read the script from standard input (i.e., file descriptor 0) and allowed &lt;code&gt;/bin/goto&lt;/code&gt; to change the shellâs position in the input:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
      &lt;p&gt;/bin/goto performs a goto, just like its name implies. The implementation of /bin/goto depended upon the fact that the shell read shell scripts on file descriptor 0 using unbuffered reads. The /bin/goto program would seek to the beginning of the script and read forward until it came to the specified label, which appeared on a line beginning with a â:â. The â:â command does nothing; it was actually a shell builtin even at this early date.&lt;/p&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Criticism of the Bourne shell is not new. Almquist himself already complained in 1988 that shell scripts are hard to get right because of their complicated semantics (mentioned later). Nevertheless, it was a clear improvement over the status quo:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
      &lt;p&gt;Needless to say, all this broke with the Borne shell, which didnât read shell scripts on the standard input. One reason that the Borne shell was accepted as a replacement for the old shell is that in a shell script of any size the gotos became unreadable. The big change in converting to the Borne shell consisted of replacing all the gotos with structured flow of control statements, which was obviously a worthwhile enterprise even if the Borne shell had not required it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;GNU Emacs was big&lt;/head&gt;
    &lt;p&gt;GNU Emacs already had its enthusiasts when it was first released in 1985. Just like today, extensibility was what most people mentioned back then as its most appealing feature. For example, David M. Siegel of MIT praised it in March 1985:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I cannot image why people would not want GNU Emacs. It is by far the best Emacs I have ever used; far better than one you could buy. It is amazingly easy to add features to it. For example, a few of us added a netnews reading mode to it (very similar to vnews) in one weekend. The mail reading program is very similar to Babyl, and probably one of the best you can get for a Unix system. All in all, it is hard to beat.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But Emacs wasnât just extensible, it was also a good enough terminal multiplexer. In 1990, when X11 was already around, X11 was desirable but Perry Smith of IBM found Emacs featureful enough to forego that:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[â¦] If we totally disrequard price, is there a laptop which runs Unix in a true multi-process environment? Iâd like to have BSD Unix (or mach). [â¦] I think 5 mips would be plenty though. Iâd also like to have an expandable disk system where I could have a limited amount of disk in battery mode and a huge amount of disk when I finally got back home. I donât expect all the disk to be housed in the laptop. Unix by itself is big plus I like to use emacs and TeX and some other huge packages so I need quite a bit of disk space. But I donât need this when Iâm sitting on a plane (for example).&lt;/p&gt;
      &lt;p&gt;Oh, while Iâm blowing smoke rings â Iâd like eventually to be able to run some sort of X windows or similar package. This is actually not a big deal since emacs gives me all the environment I need. The advantage of emacs as an X client though are sorta neat and Iâd love to be able to have those capabilities as well.&lt;/p&gt;
      &lt;p&gt;Has the technology even approached such a beast? If so, who makes it and how much is it going to set me back? (Iâm willing to go pretty high if I really like what I see.)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The problem was that GNU Emacs was big. Even in 1985. The source code was 2.2 MB uncompressed, and people werenât really sure how to distribute all that without filling up the disks of Usenet sites.&lt;/p&gt;
    &lt;p&gt;Jeff Hull of TRW Inc. suggested sharing the source over 18 or so weeks:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;How about stripping all extraneous material (e.g., the .o files) out &amp;amp; posting it in 64K pieces, say 2 per week?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You might be thinking, why didnât they compress the source code before sharing? The reason is that, at the time, only plain-text files could be shared via Usenet and email. Base64 encoding, which we use to serialize binary files, wasnât proposed until 1987.&lt;/p&gt;
    &lt;p&gt;But GNU Emacs as a running process was big, too. Brad Miller of Computer Consoles Inc. worried that it wouldnât even fit in system memory:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have at least one complaint. It is HUGE. Sorry, but when my OS wonât run it because it only supports 1 meg processes, I start to wonder.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As a result, various micro-Emacsen were popular for a long time. Freemacs was one alternative, but it had other limitations, as Chris Brewster of Cray Research reported in 1991:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I have had the same question. I wanted a program that would be compatible with the GNU Emacs that I use at work. I especially wanted some of GNUâs features such as multi-step UNDO and extensibility. The most GNU-like program is Freemacs, but the 64K file size limit is a real problem for me, and it doesnât have UNDO. Going by peopleâs comments about other PC Emacsâs, I think that they would also fail to meet my needs. But I have heard that the Brief editor is fully configurable and extensible, and has multi-step UNDO and a lot of power. If itâs configurable, maybe it could be given an Emacs-like interface. Iâd be interested in othersâ experience with Brief, and opinions about how well it could be made to emulate Emacs commands.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Other popular contenders were MicroEMACS, MicroGnuEmacs, and JOVE.21 These not only limited themselves to an essential feature set, they also chose a different data structure (a linked list of lines) for storing edited files in memory than GNU Emacs did (a gap buffer). Piercarlo Grandi of UCW Aberystwyth compared the performance of the latter two editors with GNU Emacs in January 1990:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The lesson I derive from these timings is that creating the linked list of lines, and especially copying the file to a temporary as well, slow down file reading time, but then further operations become very much faster. Note also that both MicrGnu and Jove are somewhat carelessly coded, with lots of quadratic algorithms.&lt;/p&gt;
      &lt;p&gt;Ah, another note: in my favourite editor buffer organization, I would use the gap method, for intra line editing, as it avoids a lot of these quadratic situations (e.g. repeated inserts or deletes).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Grandi also noted that the gap buffer could be made much more efficient by taking into account the size of the machineâs cache line:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that on cached machines this can be further improved; the hardware string move instructions typically hard coded in memcpy(3) and friends are often very suboptimal. On a cached machine you really want to split your transfer in three parts (if long enough!), a head and a tail that are copied byte-by-byte, and a body that is copied one cache line at a time, and is cache line aligned in the destination. Especially on machines with write thru caches, writing cache line aligned cache line sized chunks is vastly faster, as it avoids the cache fetching a line only to partially update it and hold ups at the memory interface. I remember that on a VAX-11/780 with an 8 byte SBI write buffer, writing aligned 8 byte transfers was tremendously effective.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Supercomputers couldnât run Vi and Emacs either&lt;/head&gt;
    &lt;p&gt;So smaller computer struggled with GNU Emacs. But supercomputers did, too. In fact, text editors in general were a problem on multi-user systems.&lt;/p&gt;
    &lt;p&gt;Both Vi and Emacs were reported to cause performance degradation on supercomputers like Cray X-MPs and Y-MPs. Rafael Sanabria of NASA wrote in 1990 on &lt;code&gt;gnu.emacs&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We are having a heated discussion here at the lab because our computer administrators are afraid that vi or emacs will degrade the performance of these supercomputers. The say that vi issues an interrupt for every keystroke and that will degrade performance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Along the same lines, Booker C. Bense of the San Diego Supercomputer Center explained on &lt;code&gt;comp.unix.large&lt;/code&gt; why running Vi and Emacs on the Crays was not a good idea:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An editing process requires an entry that remains in the process table for a significant amount of time using a significant portion of that time in system calls.&lt;/p&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
      &lt;p&gt;So while I have only made a few changes, I have been in the editor for many minutes. All this time the editor is patiently waiting for keypresses, unless of course my process got swapped out. Note: in this example I have not used any more cpu time than in the test examples above, however I have caused the scheduler that much more grief. While I do not pretend to know the inner workings of the UNICOS scheduler, I know trying to edit files at around 3pm here is an exercise in patience.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This led to workarounds like remote editing. GNU Emacs users had &lt;code&gt;ange-ftp&lt;/code&gt; which copied modifications via FTP. Vi users had &lt;code&gt;rvi&lt;/code&gt;, of which there were actually two variants. One variant generated &lt;code&gt;ed&lt;/code&gt; commands while the other used FTP.&lt;/p&gt;
    &lt;head rend="h2"&gt;Security&lt;/head&gt;
    &lt;head rend="h3"&gt;By modern standards, Unix wasnât secure&lt;/head&gt;
    &lt;p&gt;In 1983, the University of Toronto had a VAX-11/780 running 4.1BSD with a modified kernel because of âthe somewhat hostile environment created by the undergraduatesâ. Surprising, maybe, but definitely amusing. Unfortunately, the modifications they made were not detailed.&lt;/p&gt;
    &lt;p&gt;There was a general sense on Usenet that Unix was full of security holes. As a logical consequence, people believed that it was dangerous to divulge details about vulnerabilities because the typical deployment was not about to be updated. So, unfortunately, the historical record is light on the specifics.&lt;/p&gt;
    &lt;p&gt;Some early problems with Unix that are well-known:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In V7, chroot could be broken out of by running &lt;code&gt;cd /..&lt;/code&gt;because&lt;code&gt;..&lt;/code&gt;was a pointer in the file system to the parent directory, not a symbol that was resolved based on the root directory. This was fixed in 4.1BSD and System III, according to Guy Harris in 1983.&lt;/item&gt;
      &lt;item&gt;Encrypted passwords in &lt;code&gt;/etc/passwd&lt;/code&gt;could be cracked by bruteforce until SunOS or System V (Iâm not sure which) introduced shadow passwords in the mid-to-late-1980s.&lt;/item&gt;
      &lt;item&gt;Setuid root binaries were pervasive. &lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;For example,&lt;/p&gt;&lt;code&gt;ps&lt;/code&gt;originally ran with setuid root to read physical memory via&lt;code&gt;/dev/mem&lt;/code&gt;, and later kernel memory via&lt;code&gt;/dev/kmem&lt;/code&gt;, for information on running processes. The situation was slightly improved later by adding a&lt;code&gt;kmem&lt;/code&gt;group, making&lt;code&gt;/dev/kmem&lt;/code&gt;readable only by the group, and running&lt;code&gt;ps&lt;/code&gt;with setgid instead. But, of course, letting an attacker read arbitrary kernel memory was still not ideal.&lt;/item&gt;&lt;item&gt;&lt;p&gt;Actually, for a while, even shell scripts could run with setuid bits. This was criticized, among others, by Kenneth Almquist in 1988:&lt;/p&gt;&lt;p&gt;[â¦] People tend to use the shell for quick hacks which work right in most cases. But being a little bit insecure is like being a little bit pregnant. The fix is to put as much thought into writing a setuid shell procedure as you would into writing a setuid C program. In fact, writing a setuid shell procedure is more difficult that writing a setuid C program because the semantics of C are simpler.&lt;/p&gt;&lt;p&gt;[â¦]&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Authentication over network connections was typically unencrypted. Encryption was either not available or not used. John Carr of MIT reported that they addressed this in 1987 by switching to Kerberos.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Code quality had room for improvement. For example, dereferenced null pointers were reportedly a common issue. On VAX machines, this was a legal operation, and it seems that programs often assumed address zero to contain an empty null-terminated string, i.e., a byte equal to zero. Dave B. Johnson of Rice University was working on a Unix emulator for VMS and had to grapple with this problem. He wrote in 1983:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Many programs under Unix at least unknowingly use the fact that using a zero pointer as a âchar *â will give you a null string. Although these are many times bugs which nobody has yet found, we have found in bringing up Phoenix under VMS that a large number of programs will break if there is not a null string at 0. The way this works on a VAX is that the entry point for crt0 contains a register save mask which specifies that no registers be saved. Since crt0 gets loaded at address 0, this results in a zero word at address zero, and thus, a null string at 0. In answer to your question:&lt;/p&gt;
      &lt;p&gt;What if I do âint *a = 0, *b = 0; *b = 10; i = *a;â? What is the value of i? Does this mean that assigning indirect through a nil pointer is deadly to the rest of your nil pointer derefs?&lt;/p&gt;
      &lt;p&gt;the result would be a Bus Error, since location zero is part of the text, rather than the data, and is thus write protected (except, of course, under the OMAGIC format where the result in âiâ would be 10). I have not found any programs that try to write at address 0, but there certainly are those that rely on reading there.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The reason that programs got away with this was that BSD implicitly placed a null byte at address zero.22 In the 4.1BSD source tree, this was done in &lt;code&gt;src/libc/csu/crt0.s&lt;/code&gt;, in the startup routine that called the &lt;code&gt;main&lt;/code&gt; function in compiled executables.&lt;/p&gt;
    &lt;p&gt;Of course, this hack was frowned upon, and efforts were made to weed out null pointer dereferences in Unix, including in tools like &lt;code&gt;awk&lt;/code&gt; (as reported by Guy Harris in 1985) and &lt;code&gt;sed&lt;/code&gt; (as noted by David Elliott of MIPS in 1988).&lt;/p&gt;
    &lt;p&gt;Home brewed systems programming solutions also seem to have been common. Torontoâs modified kernel is one example. As another example, on the topic of bruteforcing &lt;code&gt;/etc/passwd&lt;/code&gt;, Steve Dyer wrote in 1982:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;At Harvard we replaced /etc/passwd with a hashed-on-logname, fixed record organization, with âprivateâ information (such as encrypted passwords, student ID nos., etc.) kept in a parallel, publically-unreadable file. And, yes, itâs very easy to encapsulate these changes by rewriting the handful of get**ent routines. Standard Bell or Berkeley programs run without reprogramming. For those few programs which might migrate to our system in binary form, we regenerated an /etc/passwd file (with a phoney password field) every morning.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This wasnât only for security but also for performance. On larger systems, memory limitations and slow storage I/O made plain-text lookups very slow:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It wasnât a realistic possibility for us to use the /etc/passwd organization, because we had over 2500 unique lognames on each of our 11/70âs.&lt;/p&gt;
      &lt;p&gt;[â¦]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Such modifications existed at other sites, too. Peter Collinson of the University of Kent reported a similar scheme in 1983.23&lt;/p&gt;
    &lt;head rend="h3"&gt;User-friendly &lt;code&gt;/bin/su&lt;/code&gt; alternatives were already a popular utility genre&lt;/head&gt;
    &lt;p&gt;The original Unix security model centered on users, groups, and read/write/execute file permissions.&lt;/p&gt;
    &lt;p&gt;Iâve mentioned setuid/setgid bits in the previous section. These file permissions allow users to run certain binaries as if they were a different user or members of a specific group. Probably until the early 1990s, Unix made heavy use of this mechanism. For example, did you know that &lt;code&gt;/bin/df&lt;/code&gt; was setuid root on 4.1BSD? Or &lt;code&gt;/bin/mv&lt;/code&gt;, &lt;code&gt;/bin/mail&lt;/code&gt;, and &lt;code&gt;/usr/bin/at&lt;/code&gt;? Or &lt;code&gt;/usr/games/fortune&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;Setuid/setgid was frowned upon, but the alternatives were unwieldy. Every time a user needed to execute a command as root, they had to either log in as root or, and this was the recommended method, use the &lt;code&gt;su&lt;/code&gt; utility to get a temporary root shell.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;su&lt;/code&gt; utility was already present in First Edition Unix in 1971. However, using &lt;code&gt;su&lt;/code&gt; was clunky because every time it was invoked, the user had to enter the root password and wait for it to be validated against &lt;code&gt;/etc/passwd&lt;/code&gt; (or later, the shadow password file).&lt;/p&gt;
    &lt;p&gt;Aside from the inconvenience of having to type the same password again and again, this seems to have been slower than on todayâs systems because of the sluggishness of storage devices.&lt;/p&gt;
    &lt;p&gt;Many people tried to address these problems by inventing alternatives to &lt;code&gt;su&lt;/code&gt;. In December 1985, Kevin Szabo of the University of Waterloo shared a small utility called asroot, written for System III. Karl Kleinpaste followed up with an even simpler utility called enable that did the equivalent of calling the C library function &lt;code&gt;system()&lt;/code&gt; as root. But neither asroot nor enable performed any permission checking.&lt;/p&gt;
    &lt;p&gt;Paul Summers posted another utility called force which asked the user for the root password before executing the command. The root password was compiled into the executable âto save time,â I presume because of high storage I/O latencies on contemporary hardware. Apart from the hardcoded root password, force was similar to running &lt;code&gt;su&lt;/code&gt; with the &lt;code&gt;-c&lt;/code&gt; flag which &lt;code&gt;su&lt;/code&gt; already supported at the time.&lt;/p&gt;
    &lt;p&gt;In response, Clifford Spencer of BBN Communications Corp. posted an early version of sudo. It consisted of a single C source file and included a rudimentary version of permission checking. It allowed arbitrary command execution if the user was listed in &lt;code&gt;/usr/adm/sudo.users&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;About five days later, Don Gworek posted a modified version24 that was capable of restricting user permissions to specific commands. Like modern sudo, it used a file called &lt;code&gt;sudoers&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Permissions in sudoers are either âallâ, a list of commands, an enviornment PATH variable, or a PATH followed by a list of commands.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Fun trivia: sudo was originally developed at SUNY Buffalo on a VAX-11/750 running 4.1BSD.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;The Usenet archives are vast, and there are many more topics that I could not cover here despite being interesting. For example, the debates around moving Unix from static linking to shared libraries, or complaints about BSDâs subpar documentation which is surprising given the reputation of its modern descendants. I might get to those topics in the future. Or maybe you will and I can read about them?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Dennis Ritchie and Ken Thompson (1974) described the design and early development of Unix. It already accumulated a good number of deployments early on:&lt;/p&gt;
        &lt;p&gt;There have been four versions of the UNIX time-sharing system. [â¦]&lt;/p&gt;
        &lt;p&gt;Since PDP-11 UNIX became operational in February, 1971, over 600 installations have been put into service. [â¦]&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Neil Brown has an article series on LWN, called Ghosts of Unix Past. The series describes both successes and failures of Unixâs design patterns, and areas where the core principles werenât followed through consistently.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Yes, unlike Linux and the BSDs, macOS is certified as compliant with version 3 of the Single Unix Specification. But does it actually borrow code from AT&amp;amp;T? I donât think it does but Iâm not sure.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For context, it has to be mentioned that Sun was co-founded by CSRG alum and erstwhile BSD contributor Bill Joy. Later, Sun and AT&amp;amp;T formed a partnership to integrate BSD and AT&amp;amp;Tâs System V, the result of which was System V Release 4, or SVR4, completed in 1988. Sun then migrated its own operating system from 4BSD to SVR4 as the new foundation. This led to the release of Solaris 2 in 1992.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Copyright strings embedded in&lt;/p&gt;&lt;code&gt;rsh.exe&lt;/code&gt;,&lt;code&gt;rcp.exe&lt;/code&gt;,&lt;code&gt;ftp.exe&lt;/code&gt;, and&lt;code&gt;finger.exe&lt;/code&gt;suggest that these executables borrowed source code from BSD. The Internet Archive stores a copy of a Windows NT 3.1 installer. Download&lt;code&gt;Disc01.iso&lt;/code&gt;and then run:&lt;code&gt;$ mkdir -p /tmp/nt/{disk,binaries,copyright} $ sudo mount -o loop Disc01.iso /tmp/nt/disk $ cd /tmp/nt $ cp ./disk/i386/*_ ./binaries/ $ cd binaries $ for i in *_; do msexpand $i; done $ rm *_ $ for i in *; do strings $i | grep -i copyright &amp;gt; ../copyright/$i; done $ cd ../copyright $ grep -ri copyright .&lt;/code&gt;&lt;p&gt;If youâre not on Linux, replace&lt;/p&gt;&lt;code&gt;mount -o loop&lt;/code&gt;with the equivalent method for mounting a CD image on your OS. On Debian and Ubuntu,&lt;code&gt;msexpand&lt;/code&gt;is shipped by the&lt;code&gt;mscompress&lt;/code&gt;package. The last command will print:&lt;code&gt;rsh.ex:@(#) Copyright (c) 1983 The Regents of the University of California. rcp.ex:@(#) Copyright (c) 1983 The Regents of the University of California. ftp.ex:@(#) Copyright (c) 1983 The Regents of the University of California. finger.ex:@(#) Copyright (c) 1980 The Regents of the University of California.&lt;/code&gt;&lt;p&gt;Â â©ï¸&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MacOS Xâs kernel programming guide describes BSD as being the originator of the kernelâs networking facilities. A 2005 brochure for MacOS X Server writes:&lt;/p&gt;
        &lt;p&gt;It begins with an open source core and BSD networking architectureâdelivering the capabilities you expect of a UNIX operating system, such as fine-grained multithreading, symmetric multiprocessing, and protected memory.&lt;/p&gt;
        &lt;p&gt;[â¦]&lt;/p&gt;
        &lt;p&gt;Using the time-tested BSD sockets and TCP/IP stack, this advanced networking architecture ensures compatibility and integration with IP-based networks.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;NNTP eliminated the need for copying entire newsgroup archives to the local disk for access to specific threads on Usenet, which was a distinct improvement over the prevailing UUCP.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;If youâre thinking, âdidnât email also exist in 1980?,â youâre right. Email was invented in the 1960s, more than a decade before Usenet. In 1971, First Edition Unix already included the&lt;/p&gt;&lt;code&gt;mail&lt;/code&gt;command for exchanging messages with other users on the same system. I donât know when exactly&lt;code&gt;mail&lt;/code&gt;, or 2BSDâs&lt;code&gt;Mail&lt;/code&gt;, became networked. But every email had to be addressed to a specific person rather than an amorphous group of strangers like Usenet newsgroups. In this way, email remained limited to peer-to-peer communication.â©ï¸&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;An example from a 1983 debate about what did and didnât count as support for virtual memory:&lt;/p&gt;
        &lt;p&gt;Believing Mr. Woodsâ claim about System V having virtual memory requires an implementation of âvirtual reality.â&lt;/p&gt;
        &lt;p&gt;I seriously hope that Mr. Woodsâ misunderstanding is not indicative of AT&amp;amp;Tâs understanding of the issues. If it is, â¦&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;For example, Joshua Gordon seems to have used âonlineâ in its original sense in June 1984 on&lt;/p&gt;&lt;code&gt;net.micro&lt;/code&gt;:&lt;p&gt;Iâm running on a âpartitionedâ system right now: an altos-586 running XENIX with, yes, a 10-meg disk. Indeed, there is not a heck of a lot of room on here (I have the development package); only a few man pages (mostly for stuff I got on the net, and a few pieces of local stuff); it is somewhat of a nuisance, but I can certainly understand a company partitioning the system. Even without the development package, ten megs is not much.&lt;/p&gt;&lt;p&gt;and Iâve stripped the thing myself even more (got rid of spell and its froofraw; chucked out useless phototypesetting utilities; that sort of thing) so I can keep at least a few days news online at a time.&lt;/p&gt;&lt;p&gt;Likewise did John Sellens of the University of Waterloo in January 1985:&lt;/p&gt;&lt;p&gt;I need a copy of the DIF (Data Interchange Format) standard. We have written to the clearing house, but want it as soon as possible.&lt;/p&gt;&lt;p&gt;If you have it already online, I would appreciate it if you could mail me a copy.&lt;/p&gt;&lt;p&gt;Andy Glew of Gould Electronics mentioned this in September 1987 on&lt;/p&gt;&lt;code&gt;comp.unix.wizards&lt;/code&gt;as one of the benefits of spending money on a PC:&lt;p&gt;But seriously, on a home computer you donât have the hassle of having to ask permission to do something, or of becoming root without asking permission and then getting yelled at because you tread on somebodyâs toes. You can make your own decisions as to what is important enough to keep online, and what should be deleted, or backed of to tape or floppy. You have floppies, which are a much more convenient storage medium than tape for reasonably sized modules (even when you have more than 400 floppies, like I do).&lt;/p&gt;&lt;p&gt;Even as late as in March 1988, Ric Messier of Lyndon State College reported on&lt;/p&gt;&lt;code&gt;comp.unix.wizards&lt;/code&gt;that Unix was lacking âonline help,â unlike the IBM systems he had used:&lt;p&gt;Now, you can all go to your corners and start flaming me if you like but I havenât seen anything in UNIX yet (particularly its speed) that would convert me. As for its documentation, which I have seen others talk about, it is absolutely ATROCIOUS. And there is almost NO online help, unlike IBMâs FULL-SCREEN online help about anything you could possibly need to know. As of yet, I havenât had much chance to play with anything serious in the way of scripts or command languages on Unix or VMS but I can tell you that what I have seen endears me even more to IBMâs Rexx.&lt;/p&gt;&lt;p&gt;In the replies, David H. Wolfskill differentiated between manual pages that were just âonlineâ and those that were stored on a server:&lt;/p&gt;&lt;p&gt;It was about yet another variant implementation of UNIX from IBM; naturally it is different from all others that have been offered so far. It is called âIBM/4.3â â a port of BSD4.3 for the Reduced Technology :-) PC. It seems quite interesting; it seems to be based on some of the work done at CMU for Andrew (and some of that code is part of IBM/4.3).&lt;/p&gt;&lt;p&gt;[â¦]&lt;/p&gt;&lt;p&gt;At least IBM/4.3 â unlike the 370-based flavor(s) of UNIX that IBM offers â can have the âmanâ pages onlineâ¦. (Of course, with IBM/4.3, youâd probably have them on a server.)&lt;/p&gt;&lt;p&gt;Some articles made an explicit distinction made between âonlineâ and âavailable via a network connection.â For example, Mark Reynolds of Adaptive Optics Associates inquired about RFCs that described SMTP in December 1985 on&lt;/p&gt;&lt;code&gt;net.wanted&lt;/code&gt;:&lt;p&gt;I would like to get a hold of copies of&lt;/p&gt;&lt;p&gt;RFC 821 âSimple Mail Transfer Protocolâ RFC 822 âStandard for the Format of Arpa Internet Text Messagesâ&lt;/p&gt;&lt;p&gt;Are there online copies of this that can be accessed via USENET ( we are not an ARPA site ) ?&lt;/p&gt;&lt;p&gt;Similarly, Karl Nyberg of USCâs Information Sciences Institute wrote in February 1986 on&lt;/p&gt;&lt;code&gt;net.lang.ada&lt;/code&gt;:&lt;p&gt;Since I have had several requests for this recently, I have checked, and found that there is an online copy of the Reference Manual here on ISIF. The files may be found in the directory ps:&amp;lt;ada-lsn&amp;gt;, and are listed as anxi si-rm-*.. They appear to be somewhat dated (the latest write date is February 83), and have a number of disclaimers concerning the preliminary nature of the document, etc. For those of you on the internet, you can use Anonymous Ftp to transfer these files and review them. For those of you not on the internet, please donât ask to have them mailed - theyâre huge! Perhaps I will work out a mechanism for making some (small) quantity of tapes if interest warrants, and I can find the time.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A 1967 price list showed the cheapest PDP-10 configuration at $110,000.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Computer History Museum lists the price as a range: $120,000 to $160,000. I am taking the lower bound of this range as a guesstimate for the introductory price. The VAX-11/780 was a popular model that was sold for a number of years. So it is conceivable that the introductory price was higher.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As Mark Horton was quoted in 1985:&lt;/p&gt;
        &lt;p&gt;In the second place, whatâs really happening here is that these binaries are PDP-11 binaries (thatâs what V6 and V7 ran on) which are run in VAX compatibility mode (the VAX hardware supports such a thing) using a program called /usr/games/lib/compat and a front end to open the files.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The first thing that Bill Stewart mentioned about the Korn shell in November 1984 on&lt;/p&gt;&lt;code&gt;net.unix-wizards&lt;/code&gt;was compatibility:&lt;p&gt;It is (almost) totally upward compatible from /bin/sh (the Bourne Shell). This means that /bin/sh scripts that used to work still work, and you donât have to relearn everything like you would if you switch to csh.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ken Greer summarized his changes in October 1983 on&lt;/p&gt;&lt;code&gt;net.sources&lt;/code&gt;:&lt;p&gt;The following code enhances the Berkeley C shell to do command and file name recognition and completion. The code is limited to a new module and a hook into sh.lex.c. Also included is a manual update ânewshellâ.&lt;/p&gt;&lt;p&gt;Iâve had the file name completion code running for years. Thanx go to Mike Ellis of Fairchild A.I. Labs for adding command recogition/completion.&lt;/p&gt;&lt;p&gt;This version is for 4.1 BSD. If the 4.2 shell is any different and this requires any change Iâll repost.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Anderson and Andersonâs 1986 The UNIXâ¢ C Shell Field Guide wrote (Section 1.6, page 23):&lt;/p&gt;
        &lt;p&gt;The C shell was originally designed on a large machine, where system resources and memory were not at a premium. It executes slowly in many small computer implementations. However, as low-cost computers become more powerful and the cost of memory decreases, this will become less of an issue.&lt;/p&gt;
        &lt;p&gt;A couple years earlier, in June 1984, Geoff Kuenning shared this opinion on the C shell as a regular user:&lt;/p&gt;
        &lt;p&gt;Apparently some people have come to the conclusion that I think csh is the greatest thing ever. Let me say this right now:&lt;/p&gt;
        &lt;p&gt;csh *** S T I N K S *** !!!&lt;/p&gt;
        &lt;p&gt;It is inconsistent, ugly, badly human-engineered, inconvenient, slow to start up scripts, buggy, and full of nasty surprises. It also happens to be the shell I use for interaction and for scripts. For interaction, the explanation is simple: aliases and history. For scripts, I learned how to write csh scripts (and it was PAINFUL) because that was the shell I knew how to use, and it never occurred to me that sh might be better for scripts even though csh is a better interactive shell.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;SunOS descended from 4.2BSD. HP-UX, Xenix, IRIX, and A/UX descended from System V. NeXTSTEP used a Mach kernel with code from 4.3BSD-Tahoe and 4.3BSD-Reno, preview releases of the later 4.4BSD.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In the late 1970s and the early 1980s, in the years after the 780 was introduced, there was no commonly understood way of characterizing its performance. Joel S. Emer of DEC and Douglas W. Clark of Princeton wrote in 1999:&lt;/p&gt;
        &lt;p&gt;In particular, while the VAX-11/780, which was introduced in 1978, was probably the preeminent timesharing machine on university campuses at that time, very little was known about how it worked or exactly what its performance was.&lt;/p&gt;
        &lt;p&gt;(They dated the 780 to 1978 but the Computer History Wiki traces the original announcement to DECâs shareholder meeting in 1977.)â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I used the&lt;/p&gt;&lt;code&gt;rbenchmark&lt;/code&gt;library for these measurements. For example,&lt;code&gt;benchmark(write(1:10000, "/tmp/junk"))&lt;/code&gt;reported that it took 1.404 seconds to execute 100 replications. The per-replication execution time was 1.404 Ã 1000 / 100 = 14.04 milliseconds.â©ï¸&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The benchmarked R expression was&lt;/p&gt;&lt;code&gt;m &amp;lt;- matrix(scan("/tmp/junk"), 100, 100)&lt;/code&gt;. In S, the counterpart of&lt;code&gt;write()&lt;/code&gt;was&lt;code&gt;read()&lt;/code&gt;, but in R, it is&lt;code&gt;scan()&lt;/code&gt;.â©ï¸&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MicroGnuEmacs and JOVE are still maintained today. They are packaged as mg and jove by Debian.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dave B. Johnson explained:&lt;/p&gt;
        &lt;p&gt;For programs made with âccâ (without running âldâ explicitly), Berkeley 4.1BSD DOES (implicitly) guarantee a zero byte at address 0. The fact that you had problems with this in porting a locally written C program from Unix 4.1 to VMS/Eunice is a bug in Eunice, not a problem with Unix. 4.1BSD crt0.o ALWAYS has a zero register save mask in it and is ALWAYS loaded at address 0 by âccâ. Crt0 is written in assembler and has an explicit â.word 0x0000â for a register save mask. In addition, the value of the register save mask in crt0 has no affect on saving registers, since the Unix kernel does not âcallâ the program, but rather jumps directly into it after the register save mask. Saving the registers is not necessary since there is nowhere to return to abover crt0. In writing our Unix emulator Phoenix, I have been very careful of details like this which cause Unix programs to break. Phoenix will run almost any Unix program under VMS unmodified, even those that depend on undocumented details of the Unix environment such as this.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;At UKC, we have user populations of 600-700 and have totally replaced the password file by a binary file with some integral number of bytes per user. This means that random access can be used to access an individual entry. Other keys to the password file (such as login name and in our case the userâs system id) are abstracted to a set of very small files which are kept in uid order - these files can be opened and searched very easily.&lt;/p&gt;
        &lt;p&gt;For compatibility purposes we generate /etc/passwd every night (with no passwords) and passwords are never printed even in their encrypted form.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The modified version of sudo listed Clifford Spencer, Phil Betchel, Gretchen Phillips, John LoVerso, and Gworek himself as the authors.â©ï¸&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46190796</guid><pubDate>Mon, 08 Dec 2025 10:43:35 +0000</pubDate></item><item><title>Show HN: Web app that lets you send email time capsules</title><link>https://resurf.me</link><description>&lt;doc fingerprint="54a319dcce249245"&gt;
  &lt;main&gt;&lt;p&gt; You write something down. &lt;lb/&gt; It's wonderful. &lt;lb/&gt; But you never look at it again. &lt;/p&gt;&lt;p&gt; a time &lt;lb/&gt; capsule &lt;lb/&gt; for your &lt;lb/&gt; thoughts &lt;/p&gt;&lt;p&gt;Your notes, ideas, insights, letters will pop up in your email, when you want it back.&lt;/p&gt;Get Started&lt;p&gt;This is NOT:&lt;/p&gt;&lt;p&gt;Resurf is focused on the mid-term. Not things you need to do today, and not things you need to do years later.&lt;/p&gt;&lt;p&gt;So what excites you?&lt;/p&gt;&lt;p&gt;What don't you want to forget?&lt;/p&gt;&lt;p&gt;When it's time, your thought arrives in your inbox—simple and beautiful.&lt;/p&gt;&lt;p&gt;From: Resurf &amp;lt;note@info.resurf.me&amp;gt;&lt;/p&gt;&lt;p&gt;Subject: A thought resurfaces...&lt;/p&gt;&lt;p&gt; “It is good to have an end to journey toward; but it is the journey that matters, in the end.”&lt;lb/&gt;― Ursula K. Le Guin, The Left Hand of Darkness &lt;/p&gt;&lt;p&gt;You captured this 3 months ago.&lt;/p&gt;&lt;p&gt;Click "Resurface Again" to schedule when you'd like to revisit this thought.&lt;/p&gt;&lt;p&gt; Thought captured on Tuesday, September 9, 2025 at 9:49 AM EDT&lt;lb/&gt;Resurf - time capsule for your thoughts&lt;lb/&gt; Visit resurf.me &lt;/p&gt;&lt;p&gt;Resurf is free to use during the beta period.&lt;/p&gt;&lt;p&gt;Give it a try!&lt;/p&gt;Get Started&lt;p&gt; Built by Frank Lu in Tokyo 🗼 &lt;lb/&gt; Questions? Shoot me an email at frank@resurf.com &lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46191124</guid><pubDate>Mon, 08 Dec 2025 11:37:22 +0000</pubDate></item><item><title>Bad Dye Job</title><link>https://daringfireball.net/2025/12/bad_dye_job</link><description>&lt;doc fingerprint="b5bcf8051b6601f2"&gt;
  &lt;main&gt;
    &lt;p&gt;By John Gruber&lt;/p&gt;
    &lt;p&gt;WorkOS Radar:&lt;lb/&gt;Protect your app against AI bots, free-tier abuse, and brute-force attacks.&lt;/p&gt;
    &lt;p&gt;In my post earlier today on the then-breaking news that Alan Dye has left Apple to join Meta as chief design officer (a new title at the company1), I wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It sounds like Dye chose to jump ship, and wasn’t squeezed out (as it seems with former AI chief John Giannandrea earlier this week). Gurman/Bloomberg are spinning this like a coup for Meta (headline: “Apple Design Executive Alan Dye Poached by Meta in Major Coup”), but I think this is the best personnel news at Apple in decades. Dye’s decade-long stint running Apple’s software design team has been, on the whole, terrible — and rather than getting better, the problems have been getting worse.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Dye’s replacement at Apple is longtime Apple designer Stephen Lemay. I’ve never met Lemay (or at least can’t recall meeting him), and prior to today never heard much about him. But that’s typical for Apple employees. Part of the job working for Apple is remaining under the radar and out of the public eye. What I’ve learned today is that Lemay, very much unlike Dye, is a career interface/interaction designer. Sources I’ve spoken to who’ve worked with Lemay at Apple speak highly of him, particularly his attention to detail and craftsmanship. Those things have been sorely lacking in the Dye era. Not everyone loves everything Lemay has worked on, but nobody bats 1.000 and designers love to critique each other’s work. I’ve chatted with people with criticisms of specific things Lemay has worked on or led at Apple (e.g. aspects of iPadOS multitasking that struck many of us as deliberately limiting, rather than empowering), but everyone I’ve spoken to is happy — if not downright giddy — at the news that Lemay is replacing Dye. Lemay is well-liked personally and deeply respected talent-wise. Said one source, in a position to know the choices, “I don’t think there was a better choice than Lemay.”&lt;/p&gt;
    &lt;p&gt;The sentiment within the ranks at Apple is that today’s news is almost too good to be true. People had given up hope that Dye would ever get squeezed out, and no one expected that he’d just up and leave on his own. (If you care about design, there’s nowhere to go but down after leaving Apple. What people overlooked is the obvious: Alan Dye doesn’t actually care about design.)&lt;/p&gt;
    &lt;p&gt;What I struggled with in the wake of today’s news is how to square the following contradiction:&lt;/p&gt;
    &lt;p&gt;Dye apparently left for Meta on his own; he wasn’t squeezed out.&lt;/p&gt;
    &lt;p&gt;Apple replacing Dye with Lemay seemingly signals a significant shift in direction, replacing a guy whose approach was almost entirely superficial/visual with a guy who’s spent his entire career sweating actual interaction details.&lt;/p&gt;
    &lt;p&gt;If Apple’s senior leadership would have been happy to have Dye remain as leader of Apple’s software design teams, why didn’t they replace him with a Dye acolyte? Conversely, if the decision makers at Apple saw the need for a directional change, why wasn’t Dye pushed out?2&lt;/p&gt;
    &lt;p&gt;The answer, I think, is that the decision to elevate Lemay wasn’t about direction, but loyalty. Why risk putting in a Dye-aligned replacement when that person might immediately get poached too? We know, from this year’s AI recruitment battles, that Zuckerberg is willing to throw almost unfathomable sums of money to poach talent he wants to hire from competitors. Gurman reported that Billy Sorrentino, a Dye deputy who has served as a senior director of design at Apple since 2016, is leaving for Meta with Dye.3 I don’t have any other names, but word on the street is that other members of Dye’s inner circle are leaving Apple for Meta with him. But those who remain — or who might remain, if they’d have been offered the promotion to replace Dye — simply can’t be trusted from the perspective of senior leadership, who were apparently blindsided by Dye’s departure for Meta. They wouldn’t have given Dye a prime spot in the WWDC keynote if they thought he might be leaving within months.&lt;/p&gt;
    &lt;p&gt;So the change in direction we may see — that many of us desperately hope to see — under Lemay’s leadership might be happenstance. More a factor of Lemay being politically safe, as someone predating Dye and outside Dye’s inner circle at Apple, than from Tim Cook or anyone else in senior leadership seeing a need for a directional change in UI design. But happenstance or not, it could be the best thing to happen to Apple’s HI design in the entire stretch since Steve Jobs’s passing and Scott Forstall’s ouster.&lt;/p&gt;
    &lt;p&gt;Putting Alan Dye in charge of user interface design was the one big mistake Jony Ive made as Apple’s Chief Design Officer.4 Dye had no background in user interface design — he came from a brand and print advertising background. Before joining Apple, he was design director for the fashion brand Kate Spade, and before that worked on branding for the ad agency Ogilvy. His promotion to lead Apple’s software interface design team under Ive happened in 2015, when Apple was launching Apple Watch, their closest foray into the world of fashion. It might have made some sense to bring someone from the fashion/brand world to lead software design for Apple Watch, but it sure didn’t seem to make sense for the rest of Apple’s platforms. And the decade of Dye’s HI leadership has proven it.&lt;/p&gt;
    &lt;p&gt;The most galling moment in Dye’s entire tenure was the opening of this year’s iPhone event keynote in September, which began with a title card showing the oft-cited Jobs quote “Design is not just what it looks like and feels like. Design is how it works.” The whole problem with the Dye era of HI design at Apple is that it has so largely — not entirely, but largely — been driven purely by how things look. There are a lot of things in Apple’s software — like app icons — that don’t even look good any more. But it’s the “how it works” part that has gone so horribly off the rails. Alan Dye seems like exactly the sort of person Jobs was describing in the first part of that quote: “People think it’s this veneer — that the designers are handed this box and told, ‘Make it look good!’”&lt;/p&gt;
    &lt;p&gt;I am not a Liquid Glass hater. I actually think, on the whole, iOS 26 is a better and more usable UI than iOS 18. But MacOS 26 Tahoe is a mess, visually, and I’m not sure there’s a single thing about its UI that is better than MacOS 15 Sequoia. There are new software features in Tahoe that are excellent and serve as legitimate enticements to upgrade. But I’m talking about the user interface — the work from Alan Dye’s HI team, not Craig Federighi’s teams. I think the fact that Liquid Glass is worse on MacOS than it is on iOS is not just a factor of iOS being Apple’s most popular, most profitable, most important platform — and thus garnering more of Apple’s internal attention. I think it’s also about the fact that the Mac interface, with multiple windows, bigger displays, and more complexity, demands more nuanced, more expert, interaction design skills. Things like depth, layering, and unambiguous indications of input focus are important aspects of any platform. But they’re more important on the platform which, by design, shoulders more complexity. Back in 2010, predicting a bright future for the Mac at a time when many pundits were thinking Apple would soon put the entire platform out to pasture, I wrote, “It’s the heaviness of the Mac that allows iOS to remain light.” That remains as true today as it was 15 years ago. But Liquid Glass, especially as expressed on MacOS, is a lightweight poorly considered design system as a whole, and its conceptual thinness is not sufficient to properly allow the Mac to carry the weight it needs to bear.&lt;/p&gt;
    &lt;p&gt;Perhaps more tellingly, there should have been no need for the “clear/tinted” Liquid Glass preference setting that Apple added in the 26.1 OS releases. Alan Dye wasn’t fired, by all accounts, but that preference setting was as good a sign as any that he should have been. And it’s very much a sign that inside Apple, there’s a strong enough contingent of people who prioritize how things work — like, you know, whether you can read text against the background of an alert — to get a setting like this shipped, outside the Accessibility section of Settings.&lt;/p&gt;
    &lt;p&gt;It remains worrisome that Apple needed to luck into Dye leaving the company. But fortune favors the prepared, and Apple remains prepared by having an inordinate number of longtime talented HI designers at the company. The oddest thing about Alan Dye’s stint leading software design is that there are, effectively, zero design critics who’ve been on his side. The debate regarding Apple’s software design over the last decade isn’t between those on Dye’s side and those against. It’s only a matter of debating how bad it’s been, and how far it’s fallen from its previous remarkable heights. It’s rather extraordinary in today’s hyper-partisan world that there’s nearly universal agreement amongst actual practitioners of user-interface design that Alan Dye is a fraud who led the company deeply astray. It was a big problem inside the company too. I’m aware of dozens of designers who’ve left Apple, out of frustration over the company’s direction, to work at places like LoveFrom, OpenAI, and their secretive joint venture io. I’m not sure there are any interaction designers at io who aren’t ex-Apple, and if there are, it’s only a handful. From the stories I’m aware of, the theme is identical: these are designers driven to do great work, and under Alan Dye, “doing great work” was no longer the guiding principle at Apple. If reaching the most users is your goal, go work on design at Google, or Microsoft, or Meta. (Design, of course, isn’t even a thing at Amazon.) Designers choose to work at Apple to do the best work in the industry. That has stopped being true under Alan Dye. The most talented designers I know are the harshest critics of Dye’s body of work, and the direction in which it’s been heading.&lt;/p&gt;
    &lt;p&gt;Back in June, after WWDC, I quoted from Alan Dye’s introduction of Liquid Glass during the keynote, and then quoted from Steve Jobs’s introduction of Aqua when he unveiled the Mac OS X Public Beta in January 2000. I wrote:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Re-watching Jobs’s introduction of Aqua for the umpteenth time, I still find it enthralling. I found Alan Dye’s introduction of Liquid Glass to be soporific, if not downright horseshitty.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;One of the bits from Jobs’s Aqua introduction I quoted was this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is what the top of windows look like. These three buttons look like a traffic signal, don’t they? Red means close the window. Yellow means minimize the window. And green means maximize the window. Pretty simple. And tremendous fit and finish in this operating system. When you roll over these things, you get those. You see them? And when you are no longer the key window, they go transparent. So a lot of fit and finish in this.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;After I published that post, I got a note from a designer friend who left Apple, in frustration, a few years ago. After watching Jobs’s Aqua introduction for the first time in years, he told me, “I’m really struck by Steve directly speaking to ‘radio buttons’ and ‘the key window’.” He had the feeling that Dye and his team looked down on interface designers who used terms like Jobs himself once used — in a public keynote, no less. That to Dye’s circle, such terms felt too much like “programmer talk”. But the history of Apple (and NeXT) user interface design is the opposite. Designers and programmers used to — and still should — speak the exact same language about such concepts. Steve Jobs certainly did, and something feels profoundly broken about that disconnect under Alan Dye’s leadership. It’s like the head of cinematography for a movie telling the camera team to stop talking about nerdy shit like “f-stops”. The head of cinematography shouldn’t just abide talking about f-stops and focal lengths, but love it. Said my friend to me, regarding his interactions with Dye and his team at Apple, “I swear I had conversations in which I mentioned ‘key window’ and no one knew what I meant.”&lt;/p&gt;
    &lt;p&gt;That won’t be a problem with Stephen Lemay. Understanding of fundamental principles will no longer be lacking. Lemay has been at Apple spanning the gamut between the Greg Christie/Bas Ording glory days and the current era. At the very least, Lemay running HI should stop the bleeding — both in terms of work quality and talent retention. I sincerely believe things might measurably improve, but I’m more sure that things will stop getting worse. That alone will be a win for everyone — even though the change was seemingly driven by Mark Zuckerberg’s desire to poach Dye, not Tim Cook and Apple’s senior leadership realizing they should have shitcanned him long ago.&lt;/p&gt;
    &lt;p&gt;Alan Dye is not untalented. But his talents at Apple were in politics. His political skill was so profound that it was his decision to leave, despite the fact that his tenure is considered a disaster by actual designers inside and outside the company. He obviously figured out how to please Apple’s senior leadership. His departure today landed as a total surprise because his stature within the company seemed so secure. And so I think he might do very well at Meta. Not because he can bring world-class interaction design expertise — because he obviously can’t — but because the path to success at Meta has never been driven by design. It’s about getting done what Zuck wants done. Dye might excel at that. Dye was an anchor holding Apple back, but might elevate design at Meta.5&lt;/p&gt;
    &lt;p&gt;My favorite reaction to today’s news is this one-liner from a guy on Twitter/X: “The average IQ of both companies has increased.”&lt;/p&gt;
    &lt;p&gt;Titles are just titles, and title inflation is a real problem at all big companies. But I always thought C-level executives by definition report directly to the CEO. That that was the whole point of a “chief whatever officer” title versus “senior vice president of whatever”. But according to Mark Gurman’s exclusive report at Bloomberg breaking this whole story (emphasis added):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;With the Dye hire, Meta is creating a new design studio and putting him in charge of design for hardware, software and AI integration for its interfaces. He will be reporting to Chief Technology Officer Andrew Bosworth, who oversees Reality Labs. That group is tasked with developing wearable devices, such as smart glasses and virtual reality headsets. Dye’s major focus will be revamping Meta’s consumer devices with artificial intelligence features.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If true, Dye doesn’t even report directly to Mark Zuckerberg. Oddly enough, after the retirement of COO Jeff Williams this year, Apple claimed the company’s design teams transitioned to reporting directly to CEO Tim Cook. ↩︎&lt;/p&gt;
    &lt;p&gt;And man oh man am I curious who was involved with this decision, who had Tim Cook’s ear, and just how quickly they were forced to make it. Part of what made Stephen Lemay a popular choice within Apple’s ranks is that Lemay, by all accounts I’ve heard, isn’t a political operator and never angled for a promotion to a level of this prominence. His focus has always singularly been on the work. ↩︎︎&lt;/p&gt;
    &lt;p&gt;Sorrentino was featured in a two-minute-plus segment in this year’s WWDC keynote, starting at the 38:25 mark, introducing the new iOS Visual Intelligence features. His star was rising at Apple. And Dye himself, of course, was given the spotlight to introduce and effectively take credit for Liquid Glass itself. At least until recently, no one at Apple saw this coming. ↩︎︎&lt;/p&gt;
    &lt;p&gt;I have good reason to believe that Ive, in private, would be the first person to admit that. A fan of Liquid Glass Jony Ive is not. I believe he sees Dye as a graphic designer, not a user interface designer — and not a good graphic designer at that. I don’t think Alan Dye could get a job as a barista at LoveFrom. ↩︎︎&lt;/p&gt;
    &lt;p&gt;It’s worth recalling that Zuckerberg sorta kinda tried this poach-design-talent-from-Apple thing before. Mike Matas, the wunderkind designer who became a sensation with Delicious Library in 2005, soon thereafter moved on to work at Apple, where he designed such things as the “slide to unlock” interface on the original iPhone. Matas was a key designer on that glorious first version of the iPhone’s OS. He then left Apple and formed Push Pop Press, and wound up at Facebook in 2011 after Facebook acquired Push Pop — before it had even shipped its core product. (I saw a still-in-development version of Push Pop’s publishing system in 2011, before Facebook bought them and shut down the product, and it remains to this day one of the most impressive, exciting, “this is the future” demos I’ve ever seen. It’s not merely a shame but a goddamn tragedy that it never even shipped.) Zuckerberg wound up assembling around Matas an entire little superteam of “Delicious” era designers and design-focused developers. That team wound up shipping Facebook Paper in 2014 — an iOS-exclusive alternative client for Facebook that espoused the same principles of elegance, exquisite attention to detail, and, especially, direct manipulation of content in lieu of user interface chrome, that infused Push Pop Press’s publishing system. Facebook Paper was so good it almost — almost — made me sign up for a Facebook account just so I could use it. But Facebook Paper went nowhere, fast. Zuckerberg lost his boner for “design”, Facebook Paper was pulled from the App Store in 2016, and the team behind Paper disbanded.&lt;/p&gt;
    &lt;p&gt;Matas today works at LoveFrom, and remains, to my mind, one of the most singularly talented and interesting people in the field of interaction design. In some closer-to-ideal alternate universe, Matas would be running HI design at Apple today. ↩︎︎&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46191194</guid><pubDate>Mon, 08 Dec 2025 11:47:17 +0000</pubDate></item><item><title>Nango (YC W23) is hiring back-end engineers and dev-rels (remote)</title><link>https://jobs.ashbyhq.com/Nango</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46191269</guid><pubDate>Mon, 08 Dec 2025 12:01:13 +0000</pubDate></item><item><title>Flow: Actor-based language for C++, used by FoundationDB</title><link>https://github.com/apple/foundationdb/tree/main/flow</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46191763</guid><pubDate>Mon, 08 Dec 2025 13:08:38 +0000</pubDate></item></channel></rss>