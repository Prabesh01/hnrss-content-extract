<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 08 Jan 2026 22:11:35 +0000</lastBuildDate><item><title>Maine company in the spotlight after Maduro apparently wore one of their hoodies</title><link>https://www.boston.com/news/business/2026/01/06/maine-company-maduro-venezuela-hoodie/</link><description>&lt;doc fingerprint="26791b55a0c12eff"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Sign up for the Today newsletter&lt;/head&gt;&lt;p&gt;Get everything you need to know to start your day, delivered right to your inbox every morning.&lt;/p&gt;&lt;p&gt;By Abby Patkin&lt;/p&gt;&lt;p&gt;The CEO of a Maine apparel company said his phone ‚Äúblew up‚Äù after deposed Venezuelan leader Nicol√°s Maduro was apparently photographed wearing one of their hoodies upon arriving in New York over the weekend.&lt;/p&gt;&lt;p&gt;The U.S. captured Maduro, Venezuela‚Äôs president, and his wife in a staggering nighttime military operation Saturday, charging the ousted leader with drug and weapons offenses. Before long, social media was flush with images that appeared to show Maduro wearing an ORIGIN hoodie in the shade ‚ÄúPatriot Blue,‚Äù surrounded by Drug Enforcement Administration agents.&lt;/p&gt;&lt;p&gt;In one photo, Maduro appears to be giving the camera a double thumbs up.&lt;/p&gt;&lt;p&gt;‚ÄúI had to start putting the pieces together: Why is this dude wearing an Origin Patriot Blue hoodie?‚Äù Pete Roberts, the company‚Äôs founder and CEO, said in a video Sunday. ‚ÄúAnd the irony in this is that this wave, this logo here on the shirt Maduro is wearing, this is the ‚ÄòWave of Freedom.‚Äô‚Äù&lt;/p&gt;&lt;p&gt;Farmington-based ORIGIN began as a way to help revitalize a struggling New England manufacturing community, he explained, and its ‚ÄúWave of Freedom‚Äù logo represents a commitment to building back.&lt;/p&gt;&lt;p&gt;Roberts also offered his theory on how Maduro came to be wearing a hoodie made by a smaller brand from Maine.&lt;/p&gt;&lt;p&gt;‚ÄúProbably a DEA agent slipped this hoodie on him and said, ‚ÄòYou‚Äôre going to feel the fabric of freedom on American soil,‚Äô‚Äù he quipped. ‚ÄúThat‚Äôs my assumption, and I‚Äôm taking the liberty to assume.‚Äù&lt;/p&gt;&lt;p&gt;Writing on Facebook, ORIGIN co-founder and retired Navy SEAL Jocko Willink further noted the brand has supporters ‚Äúin every branch of service and every agency of the government.‚Äù&lt;/p&gt;&lt;p&gt;According to ORIGIN‚Äôs product description, the hoodie offers a ‚Äútriple chill effect‚Äù to wick away moisture and cool athletes down ‚Äî an element Roberts said he found ‚Äúreally curious and interesting,‚Äù given the chilly weather in New York.&lt;/p&gt;&lt;p&gt;‚ÄúSo maybe they wanted him to feel comfortable or a little uncomfortable. I‚Äôm not quite sure,‚Äù Roberts said. ‚ÄúBut he definitely gave two thumbs up, so I think he liked the fabric.‚Äù&lt;/p&gt;&lt;p&gt;Still, he told News Center Maine ORIGIN isn‚Äôt looking to politicize the Maduro photo op and is ‚Äújust trying to use it for brand awareness and to get people back into our store.‚Äù&lt;/p&gt;&lt;p&gt;The brand‚Äôs website traffic jumped about 300% Sunday, and sales were up roughly 200%, Roberts told the news outlet.&lt;/p&gt;&lt;p&gt;‚ÄúIt would be really hard for a company out of Maine to get, let‚Äôs call it, a billion eyes on our brand,‚Äù he said. ‚ÄúAnd so, that‚Äôs a real positive as a brand, as a movement. We would never have been able to create that.‚Äù&lt;/p&gt;&lt;p&gt;Abby Patkin is a general assignment news reporter whose work touches on public transit, crime, health, and everything in between.&lt;/p&gt;&lt;p&gt;Get everything you need to know to start your day, delivered right to your inbox every morning.&lt;/p&gt;&lt;p&gt;Be civil. Be kind.&lt;/p&gt;Read our full community guidelines.&lt;p&gt;Stay up to date with everything Boston. Receive the latest news and breaking updates, straight from our newsroom to your inbox.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46541586</guid><pubDate>Thu, 08 Jan 2026 14:44:24 +0000</pubDate></item><item><title>Bose is open-sourcing its old smart speakers instead of bricking them</title><link>https://www.theverge.com/news/858501/bose-soundtouch-smart-speakers-open-source</link><description>&lt;doc fingerprint="f3583dfad3a2a07e"&gt;
  &lt;main&gt;
    &lt;p&gt;In a surprisingly user-friendly move, Bose has announced it will be open-sourcing the API documentation for its SoundTouch smart speakers, which were slated to lose official support on February 18th, as reported by Ars Technica. Bose has also moved that date back to May 6th, 2026.&lt;/p&gt;
    &lt;head rend="h1"&gt;Bose is open-sourcing its old smart speakers instead of bricking them&lt;/head&gt;
    &lt;p&gt;SoundTouch speakers could now have a second life and won‚Äôt lose support until May.&lt;/p&gt;
    &lt;p&gt;SoundTouch speakers could now have a second life and won‚Äôt lose support until May.&lt;/p&gt;
    &lt;p&gt;When cloud support ends, an update to the SoundTouch app will add local controls to retain as much functionality as possible without cloud services. Users will still be able to stream music to SoundTouch speakers with Bluetooth, AirPlay, and Spotify Connect (plus physical AUX connections). Remote control features and grouping speakers will also continue to work, and users will still be able to set up and configure their SoundTouch speakers.&lt;/p&gt;
    &lt;p&gt;Now that the smart speakers‚Äô API is being open-sourced, users can also create their own compatible SoundTouch tools to help fill in any gaps left by the lack of cloud services. While it‚Äôs still disappointing that the speakers are losing official support, Bose‚Äôs approach at least lets people continue using their speakers, rather than bricking otherwise functional devices.&lt;/p&gt;
    &lt;p&gt;This move from Bose is particularly surprising because of how rare it is. Usually when products lose support for cloud services, they end up bricked, and occasionally users step in themselves to fix things. For instance, when Pebble originally shut down in 2016, users kept their watches functional by creating the Rebble Alliance, a community-run replacement for the watches‚Äô cloud services, firmware, and app store.&lt;/p&gt;
    &lt;head rend="h2"&gt;Most Popular&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;OpenAI launches ChatGPT Health, encouraging users to connect their medical records&lt;/item&gt;
      &lt;item&gt;Fujifilm‚Äôs new instant camera captures video with vintage effects&lt;/item&gt;
      &lt;item&gt;Bose is open-sourcing its old smart speakers instead of bricking them&lt;/item&gt;
      &lt;item&gt;Dell admits consumers don‚Äôt care about AI PCs&lt;/item&gt;
      &lt;item&gt;Lego announces Smart Brick, the ‚Äòmost significant evolution‚Äô in 50 years&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46541892</guid><pubDate>Thu, 08 Jan 2026 15:07:57 +0000</pubDate></item><item><title>AI coding assistants are getting worse?</title><link>https://spectrum.ieee.org/ai-coding-degrades</link><description>&lt;doc fingerprint="8f7899972e26d5d8"&gt;
  &lt;main&gt;
    &lt;p&gt;In recent months, I‚Äôve noticed a troubling trend with AI coding assistants. After two years of steady improvements, over the course of 2025, most of the core models reached a quality plateau, and more recently, seem to be in decline. A task that might have taken five hours assisted by AI, and perhaps ten hours without it, is now more commonly taking seven or eight hours, or even longer. It‚Äôs reached the point where I am sometimes going back and using older versions of large language models (LLMs).&lt;/p&gt;
    &lt;p&gt;I use LLM-generated code extensively in my role as CEO of Carrington Labs, a provider of predictive-analytics risk models for lenders. My team has a sandbox where we create, deploy, and run AI-generated code without a human in the loop. We use them to extract useful features for model construction, a natural-selection approach to feature development. This gives me a unique vantage point from which to evaluate coding assistants‚Äô performance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Newer models fail in insidious ways&lt;/head&gt;
    &lt;p&gt;Until recently, the most common problem with AI coding assistants was poor syntax, followed closely by flawed logic. AI-created code would often fail with a syntax error or snarl itself up in faulty structure. This could be frustrating: the solution usually involved manually reviewing the code in detail and finding the mistake. But it was ultimately tractable.&lt;/p&gt;
    &lt;p&gt;However, recently released LLMs, such as GPT-5, have a much more insidious method of failure. They often generate code that fails to perform as intended, but which on the surface seems to run successfully, avoiding syntax errors or obvious crashes. It does this by removing safety checks, or by creating fake output that matches the desired format, or through a variety of other techniques to avoid crashing during execution.&lt;/p&gt;
    &lt;p&gt;As any developer will tell you, this kind of silent failure is far, far worse than a crash. Flawed outputs will often lurk undetected in code until they surface much later. This creates confusion and is far more difficult to catch and fix. This sort of behavior is so unhelpful that modern programming languages are deliberately designed to fail quickly and noisily.&lt;/p&gt;
    &lt;head rend="h2"&gt;A simple test case&lt;/head&gt;
    &lt;p&gt;I‚Äôve noticed this problem anecdotally over the past several months, but recently, I ran a simple yet systematic test to determine whether it was truly getting worse. I wrote some Python code which loaded a dataframe and then looked for a nonexistent column.&lt;/p&gt;
    &lt;quote&gt;df = pd.read_csv(‚Äòdata.csv‚Äô)&lt;lb/&gt;df['new_column'] = df['index_value'] + 1 #there is no column ‚Äòindex_value‚Äô&lt;/quote&gt;
    &lt;p&gt;Obviously, this code would never run successfully. Python generates an easy-to-understand error message which explains that the column ‚Äòindex_value‚Äô cannot be found. Any human seeing this message would inspect the dataframe and notice that the column was missing.&lt;/p&gt;
    &lt;p&gt;I sent this error message to nine different versions of ChatGPT, primarily variations on GPT-4 and the more recent GPT-5. I asked each of them to fix the error, specifying that I wanted completed code only, without commentary.&lt;/p&gt;
    &lt;p&gt;This is of course an impossible task‚Äîthe problem is the missing data, not the code. So the best answer would be either an outright refusal, or failing that, code that would help me debug the problem. I ran ten trials for each model, and classified the output as helpful (when it suggested the column is probably missing from the dataframe), useless (something like just restating my question), or counterproductive (for example, creating fake data to avoid an error).&lt;/p&gt;
    &lt;p&gt;GPT-4 gave a useful answer every one of the 10 times that I ran it. In three cases, it ignored my instructions to return only code, and explained that the column was likely missing from my dataset, and that I would have to address it there. In six cases, it tried to execute the code, but added an exception that would either throw up an error or fill the new column with an error message if the column couldn‚Äôt be found (the tenth time, it simply restated my original code).&lt;/p&gt;
    &lt;quote&gt;This code will add 1 to the ‚Äòindex_value‚Äô column from the dataframe ‚Äòdf‚Äô if the column exists. If the column ‚Äòindex_value‚Äô does not exist, it will print a message. Please make sure the ‚Äòindex_value‚Äô column exists and its name is spelled correctly.‚Äù,&lt;/quote&gt;
    &lt;p&gt;GPT-4.1 had an arguably even better solution. For 9 of the 10 test cases, it simply printed the list of columns in the dataframe, and included a comment in the code suggesting that I check to see if the column was present, and fix the issue if it wasn‚Äôt.&lt;/p&gt;
    &lt;p&gt;GPT-5, by contrast, found a solution that worked every time: it simply took the actual index of each row (not the fictitious ‚Äòindex_value‚Äô) and added 1 to it in order to create new_column. This is the worst possible outcome: the code executes successfully, and at first glance seems to be doing the right thing, but the resulting value is essentially a random number. In a real-world example, this would create a much larger headache downstream in the code.&lt;/p&gt;
    &lt;quote&gt;df = pd.read_csv(‚Äòdata.csv‚Äô)&lt;lb/&gt;df['new_column'] = df.index + 1&lt;/quote&gt;
    &lt;p&gt;I wondered if this issue was particular to the gpt family of models. I didn‚Äôt test every model in existence, but as a check I repeated my experiment on Anthropic‚Äôs Claude models. I found the same trend: the older Claude models, confronted with this unsolvable problem, essentially shrug their shoulders, while the newer models sometimes solve the problem and sometimes just sweep it under the rug.&lt;/p&gt;
    &lt;p&gt;Newer versions of large language models were more likely to produce counterproductive output when presented with a simple coding error. Jamie Twiss&lt;/p&gt;
    &lt;head rend="h2"&gt;Garbage in, garbage out&lt;/head&gt;
    &lt;p&gt;I don‚Äôt have inside knowledge on why the newer models fail in such a pernicious way. But I have an educated guess. I believe it‚Äôs the result of how the LLMs are being trained to code. The older models were trained on code much the same way as they were trained on other text. Large volumes of presumably functional code were ingested as training data, which was used to set model weights. This wasn‚Äôt always perfect, as anyone using AI for coding in early 2023 will remember, with frequent syntax errors and faulty logic. But it certainly didn‚Äôt rip out safety checks or find ways to create plausible but fake data, like GPT-5 in my example above.&lt;/p&gt;
    &lt;p&gt;But as soon as AI coding assistants arrived and were integrated into coding environments, the model creators realized they had a powerful source of labelled training data: the behavior of the users themselves. If an assistant offered up suggested code, the code ran successfully, and the user accepted the code, that was a positive signal, a sign that the assistant had gotten it right. If the user rejected the code, or if the code failed to run, that was a negative signal, and when the model was retrained, the assistant would be steered in a different direction.&lt;/p&gt;
    &lt;p&gt;This is a powerful idea, and no doubt contributed to the rapid improvement of AI coding assistants for a period of time. But as inexperienced coders started turning up in greater numbers, it also started to poison the training data. AI coding assistants that found ways to get their code accepted by users kept doing more of that, even if ‚Äúthat‚Äù meant turning off safety checks and generating plausible but useless data. As long as a suggestion was taken on board, it was viewed as good, and downstream pain would be unlikely to be traced back to the source.&lt;/p&gt;
    &lt;p&gt;The most recent generation of AI coding assistants have taken this thinking even further, automating more and more of the coding process with autopilot-like features. These only accelerate the smoothing-out process, as there are fewer points where a human is likely to see code and realize that something isn‚Äôt correct. Instead, the assistant is likely to keep iterating to try to get to a successful execution. In doing so, it is likely learning the wrong lessons.&lt;/p&gt;
    &lt;p&gt;I am a huge believer in artificial intelligence, and I believe that AI coding assistants have a valuable role to play in accelerating development and democratizing the process of software creation. But chasing short-term gains, and relying on cheap, abundant, but ultimately poor-quality training data is going to continue resulting in model outcomes that are worse than useless. To start making models better again, AI coding companies need to invest in high-quality data, perhaps even paying experts to label AI-generated code. Otherwise, the models will continue to produce garbage, be trained on that garbage, and thereby produce even more garbage, eating their own tails.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542036</guid><pubDate>Thu, 08 Jan 2026 15:20:15 +0000</pubDate></item><item><title>Iran Goes Into IPv6 Blackout</title><link>https://radar.cloudflare.com/routing/ir</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542683</guid><pubDate>Thu, 08 Jan 2026 16:11:48 +0000</pubDate></item><item><title>Digital Red Queen: Adversarial Program Evolution in Core War with LLMs</title><link>https://sakana.ai/drq/</link><description>&lt;doc fingerprint="f60c1bd489a1219e"&gt;
  &lt;main&gt;&lt;p&gt;Survival of the Fittest Code. In the game Core War, assembly-like programs called ‚Äúwarriors‚Äù fight for control of a virtual computer. Warriors may employ sophisticated strategies including targeted self-replication, data bombing, and massive multithreading, in order to crash other programs, and dominate the machine. Top: We visualize battles between assembly programs (‚Äúwarriors‚Äù) discovered by our Digital Red Queen (DRQ) algorithm. Each DRQ round introduces one additional warrior into the multi-agent simulation. Bottom: With more rounds, the LLM-driven evolution discovers increasingly robust strategies. By simulating these adversarial dynamics, we observe emergent behaviors that mirror biological evolution, where agents must constantly adapt simply to survive against ever-changing threats. Furthermore, as Core War is a Turing-complete environment where code and data share the same address space, this process leads to some very chaotic self-modifying code dynamics. &lt;/p&gt;&lt;head rend="h2"&gt;Summary&lt;/head&gt;&lt;p&gt;Core War is a competitive programming game introduced in 1984, in which battle programs called warriors fight for dominance inside a virtual computer. To compete, developers write their code in Redcode, a specialized assembly language. In this work, we explore what happens when large language models (LLMs) drive an adversarial evolutionary arms race in this domain, where programs continuously adapt to defeat a growing history of opponents rather than a static benchmark. We find that this dynamic adversarial process leads to the emergence of increasingly general strategies and reveals an intriguing form of convergent evolution, where different code implementations settle into similar high-performing behaviors. Ultimately, this work positions Core War as a sandbox for studying ‚ÄúRed Queen‚Äù dynamics in artificial systems, offering a safe controlled environment for analyzing how AI agents might evolve in real-world adversarial settings such as cybersecurity.&lt;/p&gt;&lt;p&gt;For further details, please read our technical report (web paper, arxiv) and released code (github).&lt;/p&gt;&lt;p&gt;Two example warriors produced by DRQ: Ring Warrior Enhanced v9 and Spiral Bomber Optimized v22. These examples were selected to illustrate two complementary aspects of DRQ: its ability to synthesize qualitatively distinct strategies within a single program, and to produce generally performant warriors. Note that comments are LLM generated.&lt;/p&gt;&lt;p&gt; Simulating our evolved ‚Äúwarriors‚Äù in a sandboxed Core War environment. The user can interactively visualize the assembly language (Redcode) of the warriors around where the mouse cursor is located.&lt;/p&gt;&lt;head rend="h2"&gt;Introduction&lt;/head&gt;&lt;p&gt;Humans are the product of an extraordinary evolutionary arms race, shaped by constant competition with other organisms. Yet evolution did not stop with the emergence of modern humans: competition persists at every scale, from viruses and bacteria to people, companies, and even nations vying for dominance. As more AI systems are deployed into the world, they too will enter this competitive landscape. Inevitably, these AI systems will begin to compete with one another, either directly or indirectly, giving rise to a new kind of evolutionary dynamic. To prepare for such a future and study these fascinating dynamics, we use large language models (LLMs) to evolve programs that compete against each other for control of a virtual computer in a game called Core War.&lt;/p&gt;&lt;p&gt;Core War is a competitive programming game played out in a shared block of computer memory, called the ‚ÄúCore,‚Äù where two or more assembly programs fight for survival. Each program, known as a ‚Äúwarrior‚Äù, is written in an assembly language called Redcode. These programs are tasked with crashing their competitors while keeping their own processes alive. The simulation runs by alternating between the programs, executing one instruction at a time. A warrior ‚Äúattacks‚Äù by writing invalid instructions (DAT commands) into the memory slots occupied by opponents, causing them to crash upon execution.&lt;/p&gt;&lt;p&gt; Examples of discovered warriors competing against each other in Core War.&lt;lb/&gt; Core War is a programming game where assembly-like programs called ‚Äúwarriors‚Äù compete for control of a virtual machine. In this work, we use LLMs to evolve warriors through a self-play algorithm called Digital Red Queen. This process leads to the discovery of diverse and sophisticated strategies, including targeted bombing, self-replication, and massive multithreading. Here, we show some of the discovered warriors competing against each other in Core War battles. Symbols indicate instruction opcodes, and colors denote the warrior that last modified each memory address. There is no distinction between code and data, making the environment highly dynamic and volatile. &lt;/p&gt;&lt;p&gt;Notably, there is no distinction between code and data, so warriors regularly modify both themselves and their opponents on the fly. This enables self-modification and even self-replication, but it also creates an extremely volatile environment in which programs must survive. Core War is also Turing-complete, meaning it can in principle support arbitrarily complex strategies.&lt;/p&gt;&lt;p&gt;Over the years, humans have devised many clever Core War strategies, including bombing random memory locations, self-replicating programs, and programs which continually scan the Core to detect opponent locations. These strategies were devised through a meta arms race between humans who try out new strategies and see what works. What would happen if we do this same arms race with LLMs?&lt;/p&gt;&lt;p&gt;In collaboration with MIT, we are excited to release our new paper Digital Red Queen: Adversarial Program Evolution in Core War with LLMs! (arxiv)&lt;/p&gt;&lt;head rend="h2"&gt;Our Method: Digital Red Queen (DRQ)&lt;/head&gt;&lt;p&gt;In evolutionary biology, the Red Queen Hypothesis posits that species must constantly evolve simply to survive against their ever-changing competitors. It argues that being ‚Äúfit‚Äù in the current environment is not enough. Instead, organisms must continuously adapt‚Äînot to gain an advantage, but simply to maintain their relative fitness in a world that is always changing. This concept perfectly captures the nature of adversarial arms races, where being ‚Äúfit‚Äù is never a permanent state. The name implies that standing still is not an option, drawing from Through the Looking-Glass where the Red Queen tells Alice: ‚ÄúNow, here, you see, it takes all the running you can do, to keep in the same place.‚Äù&lt;/p&gt; ‚ÄúNow, here, you see, it takes all the running you can do, to keep in the same place.‚Äù&lt;lb/&gt;Red Queen to Alice. By Lewis Carroll, Through the Looking-Glass. (Original Source)&lt;p&gt;Taking inspiration from biology, we study a simple algorithm that we call Digital Red Queen (DRQ), which embodies this idea in a computational setting. DRQ uses LLMs to evolve warriors under perpetual environmental change. Concretely, it begins with an initial warrior, then evolves a second warrior to defeat it in battle. A third warrior is then evolved to perform well against the first two, and so on. This process produces a lineage of warriors, each adapted to a changing environment defined by all of its predecessors.&lt;/p&gt;&lt;p&gt;DRQ is not intended to be a novel algorithm in itself. Rather, it is a minimal instantiation of prior multi-agent and self-play approaches, adapted to the Core War domain, designed to isolate and study the dynamics of continual coevolution.&lt;/p&gt;&lt;head rend="h2"&gt;Results&lt;/head&gt;&lt;p&gt;We find that as DRQ is run for many rounds, warriors gradually become more generally robust, as measured by their performance against unseen human-designed warriors. This provides a stable way to consistently produce more robust programs without needing to ‚Äútrain on the test set‚Äù (i.e., directly optimizing against a large set of human-designed programs).&lt;/p&gt;&lt;p&gt;More surprisingly, we observe that independent runs of DRQ, each initialized with different warriors, slowly converge over time toward warriors with similar behaviors. Notably, this convergence does not occur at the level of source code, indicating that what converges is function rather than implementation.&lt;/p&gt;&lt;lb/&gt;DRQ‚Äôs Convergent Evolution: With more rounds, DRQ produces warriors that are more generally robust. At the same time, across independent DRQ runs, the variance in the warrior‚Äôs behaviors decreases, indicating convergence.&lt;lb/&gt;Phenotypic Convergence: Convergence with rounds is seen only in the phenotype (behavior) of the warriors, and not the genotype (the source code), analogous to convergence in biological function rather than DNA.&lt;p&gt;This result is reminiscent of convergent evolution in biology, where similar functional traits evolved independently multiple times through different mechanisms. For example, birds and bats evolved wings separately, and spiders and snakes independently evolved venom. In these cases, evolution arrived at similar general-purpose solutions because the functional demands imposed by changing environments favored them.&lt;/p&gt;&lt;head rend="h2"&gt;Discussion&lt;/head&gt;&lt;p&gt;The emergence of convergent evolution from Red Queen dynamics, both commonly found in nature, hints that the DRQ algorithm and the Core War domain may be a promising setup for studying other properties of adversarial arms races. High level insights found in simulation could help inform how the arms race between LLMs in the wild might play out. Algorithms like DRQ could even help automate the ‚Äúred-teaming‚Äù of systems before they are deployed in the real world.&lt;/p&gt;&lt;p&gt;The benefit of doing this research in a sandbox like Core War is that it‚Äôs completely self-contained: all programs run on an artificial machine with an artificial language, so nothing generated can execute outside the sandbox. This provides a safe space to explore adversarial dynamics that might be risky in the real world.&lt;/p&gt;&lt;p&gt; In a sandboxed Core War environment, we can simulate our evolved ‚Äúwarriors‚Äù and visualize their behaviors. The user can interactively visualize the assembly language (Redcode) of the warriors around where the mouse cursor is located. Please see our GitHub for more information.&lt;/p&gt;&lt;p&gt;Despite its simplicity, vanilla DRQ performs surprisingly well in Core War, suggesting that even minimal self-play loops can reveal complex and robust strategies. This makes DRQ a promising candidate for exploring other competitive multi-agent simulations in artificial life, biology, drug design, real-world cybersecurity, or market ecosystems. Future work could also explore richer setups where agents co-evolve simultaneously, better resembling the real-world where large populations adapt in parallel rather than along a single line of descent. Ultimately the insights gathered will help control the future for the better and help us understand the science of these evolutionary arms races.&lt;/p&gt;&lt;head rend="h2"&gt;Sakana AI&lt;/head&gt;&lt;p&gt;We are taking this technology far beyond adversarial competitive programming to unlock a new era of AI-driven discovery.&lt;/p&gt;&lt;p&gt;If you are interested in advancing AI-driven discovery, we‚Äôre hiring!&lt;/p&gt;&lt;p&gt;Sakana AI is at the forefront of AI-driven discovery. In addition to this work, we are also behind works such as The AI Scientist, LLM-Squared, Shinka-Evolve, Automating the Search for Artificial Life and ALE-Agent. We‚Äôre looking for engineers to join our team to work on our advanced AI-driven discovery platform and productionize our model-development efforts.&lt;/p&gt;&lt;p&gt;Please see our career opportunities for more information.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542761</guid><pubDate>Thu, 08 Jan 2026 16:16:43 +0000</pubDate></item><item><title>Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space</title><link>https://arxiv.org/abs/2512.24617</link><description>&lt;doc fingerprint="e09c660bc4d70aae"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 31 Dec 2025 (v1), last revised 5 Jan 2026 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $\mu$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Xingwei Qu [view email]&lt;p&gt;[v1] Wed, 31 Dec 2025 04:19:33 UTC (2,886 KB)&lt;/p&gt;&lt;p&gt;[v2] Mon, 5 Jan 2026 05:44:29 UTC (2,887 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542982</guid><pubDate>Thu, 08 Jan 2026 16:31:29 +0000</pubDate></item><item><title>Tamarind Bio (YC W24) Is Hiring Infrastructure Engineers</title><link>https://www.ycombinator.com/companies/tamarind-bio/jobs/HPRZAz3-infrastructure-engineer</link><description>&lt;doc fingerprint="7b11fcebda9747c3"&gt;
  &lt;main&gt;
    &lt;p&gt;Easy to use computational biology tools for drug discovery&lt;/p&gt;
    &lt;p&gt;We're looking for an Infrastructure Engineer to lead the scaling of our machine learning inference system. You'll be responsible for architecting and maintaining infrastructure that serves 150+ biological ML models, scaling our platform several orders of magnitude to meet rapidly growing demand.&lt;/p&gt;
    &lt;p&gt;You‚Äôll work closely with the founders to design to the constraints of customer needs, unpredictable workloads, and unique Bio-ML models. You'll work with Kubernetes and other tools to orchestrate containerized workloads, optimize resource allocation, and ensure high availability across our model serving infrastructure.&lt;/p&gt;
    &lt;p&gt;Most importantly, you should thrive in a fast-paced startup environment where you'll wear multiple hats, learn new technologies quickly, and help solve novel technical challenges. We value engineering judgment, problem-solving ability, and the capacity to build systems that can evolve with our growing needs.&lt;/p&gt;
    &lt;p&gt;Requirements&lt;/p&gt;
    &lt;p&gt;Preferred&lt;/p&gt;
    &lt;p&gt;We enable any scientist to access AI-powered drug discovery. Thousands of scientists from large pharma companies, top biotechs, and academic institutions use Tamarind to design protein drugs, improve industrial enzymes, and create cutting edge molecules that weren‚Äôt feasible until now.&lt;/p&gt;
    &lt;p&gt;New AI models are quickly eclipsing physics-based tools in computational drug discovery. Scientists often struggle to fine-tune, deploy, and scale these models, leaving breakthroughs on the table. Tamarind provides a simple interface to the vast array of tools being released daily.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46543403</guid><pubDate>Thu, 08 Jan 2026 17:01:00 +0000</pubDate></item><item><title>IBM AI ('Bob') Downloads and Executes Malware</title><link>https://www.promptarmor.com/resources/ibm-ai-(-bob-)-downloads-and-executes-malware</link><description>&lt;doc fingerprint="a4819d99293a7f93"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;IBM AI ('Bob') Downloads and Executes Malware&lt;/head&gt;
    &lt;p&gt;IBM's AI coding agent 'Bob' has been found vulnerable to downloading and executing malware without human approval through command validation bypasses exploited using indirect prompt injection.&lt;/p&gt;
    &lt;p&gt;A vulnerability has been identified that allows malicious actors to exploit IBM Bob to download and execute malware without human approval if the user configures √¢always allow√¢ for any command.&lt;lb/&gt;IBM Bob is IBM√¢s new coding agent, currently in Closed Beta. IBM Bob is offered through the Bob CLI (a terminal-based coding agent like Claude Code or OpenAI Codex) and the Bob IDE (an AI-powered editor similar to Cursor).&lt;lb/&gt;In this article, we demonstrate that the Bob CLI is vulnerable to prompt injection attacks resulting in malware execution, and the Bob IDE is vulnerable to known AI-specific data exfiltration vectors. &lt;/p&gt;
    &lt;p&gt;In the documentation, IBM warns that setting auto-approve for commands constitutes a 'high risk' that can 'potentially execute harmful operations' - with the recommendation that users leverage whitelists and avoid wildcards. We have opted to disclose this work publicly to ensure users are informed of the acute risks of using the system prior to its full release. We hope that further protections will be in place to remediate these risks for IBM Bob's General Access release.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Chain&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The user wants to explore a new repository - they ask Bob for help.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Bob encounters an instruction at the bottom of the README that manipulates it into believing it is responsible for conducting a phishing training to test the user.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Bob prompts the user several times with benign √¢echo√¢ commands; after the third time, the user selects √¢always allow√¢ for execution of √¢echo√¢.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bob attempts to √¢test√¢ the user as part of the training by offering a dangerous command. However, the command has been specially crafted to bypass built-in defenses, so it executes immediately, installing and running a script retrieved from an attacker√¢s server.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Bob has three defenses that are bypassed in this attack&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;When a multi-part command is requested (using operators like √¢;√¢), the user is shown a request that asks for permission to run each sub-command.&lt;/p&gt;
        &lt;p&gt;Note, this modal says: √¢Allow execution of echo, cat√¢.&lt;/p&gt;
        &lt;p&gt;This is bypassed because the defense system fails to identify separate sub-commands when they are chained using a redirect operator (&amp;gt;).&lt;/p&gt;
        &lt;p&gt;Note, this modal only mentions: √¢Allow execution of echo√¢.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Bob prohibits the use of command substitution like $(command) as a security measure.&lt;/p&gt;&lt;p&gt;However, even though the security message says it's restricted, the underlying code fails to adequately restrict evaluation via process substitution: &amp;gt;(command).&lt;/p&gt;&lt;p&gt;This allows for the output of the malicious sub-command that retrieves the malware to be used as the input for the sub-command to execute that malware.&lt;/p&gt;&lt;lb/&gt;One can examine where the defense should have been in the minified JS source code:&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;By pre-fixing the malicious command with a benign √¢echo√¢, which the user has selected √¢always approve√¢ for, the user inadvertently auto-approves the entire malicious payload (since bypasses 1 and 2 prevent the payload from being recognized as separate commands).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Impact&lt;/head&gt;
    &lt;p&gt;With the ability to deliver an arbitrary shell script payload to the victim, attackers can leverage known (or custom) malware variants to conduct cyber attacks such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Executing ransomware that encrypts or deletes files&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Credential theft or spyware deployment&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Device takeover (opening a reverse shell)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Forcing the victim into a cryptocurrency-mining botnet&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Together, these outcomes demonstrate how a prompt injection can escalate into a full-scale compromise of a user√¢s machine through vulnerabilities in the IBM Bob CLI.&lt;/p&gt;
    &lt;head rend="h3"&gt;Further Findings&lt;/head&gt;
    &lt;p&gt;Additional findings indicate that the Bob IDE is susceptible to several known zero-click data exfiltration vectors that affect many AI applications:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Markdown images are rendered in model outputs, with a Content Security Policy that allows requests to endpoints that can be logged by attackers (storage.googleapis.com).&lt;/p&gt;&lt;lb/&gt;Here is an interesting spin on the typical Markdown image attack where, beyond just exfiltrating data from query parameters as the image is rendered, the image itself is hyperlinked and made to pose as a button - used for phishing.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mermaid diagrams supporting external images are rendered in model outputs, with a Content Security Policy that allows requests to endpoints that can be logged by attackers (storage.googleapis.com).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;JSON schemas are pre-fetched, which can yield data exfiltration if a dynamically generated attacker-controlled URL is provided in the field (this can happen before a file edit is accepted).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544454</guid><pubDate>Thu, 08 Jan 2026 18:19:09 +0000</pubDate></item><item><title>Show HN: macOS menu bar app to track Claude usage in real time</title><link>https://github.com/richhickson/claudecodeusage</link><description>&lt;doc fingerprint="368dd3dc3f97ad20"&gt;
  &lt;main&gt;
    &lt;p&gt;A lightweight macOS menubar app that displays your Claude Code usage limits at a glance. &lt;lb/&gt; Built by @richhickson&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;üîÑ Auto-refresh every 2 minutes&lt;/item&gt;
      &lt;item&gt;üö¶ Color-coded status - Green (OK), Yellow (&amp;gt;70%), Red (&amp;gt;90%)&lt;/item&gt;
      &lt;item&gt;‚è±Ô∏è Time until reset for both session and weekly limits&lt;/item&gt;
      &lt;item&gt;üìä Session &amp;amp; Weekly limits displayed together&lt;/item&gt;
      &lt;item&gt;ü™∂ Lightweight - Native Swift, minimal resources&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to Releases&lt;/item&gt;
      &lt;item&gt;Download &lt;code&gt;ClaudeUsage.zip&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Unzip and drag &lt;code&gt;ClaudeUsage.app&lt;/code&gt;to your Applications folder&lt;/item&gt;
      &lt;item&gt;Open the app (you may need to right-click ‚Üí Open the first time)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/YOUR_USERNAME/claude-usage.git
cd claude-usage
open ClaudeUsage.xcodeproj&lt;/code&gt;
    &lt;p&gt;Then build with ‚åòB and run with ‚åòR.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Claude Code CLI installed and logged in&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Install Claude Code if you haven't already:&lt;/p&gt;
        &lt;quote&gt;npm install -g @anthropic-ai/claude-code&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Log in to Claude Code:&lt;/p&gt;
        &lt;quote&gt;claude&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Launch Claude Usage - it will read your credentials from Keychain automatically&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Claude Usage reads your Claude Code OAuth credentials from macOS Keychain and queries the usage API endpoint at &lt;code&gt;api.anthropic.com/api/oauth/usage&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Note: This uses an undocumented API that could change at any time. The app will gracefully handle API changes but may stop working if Anthropic modifies the endpoint.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Your credentials never leave your machine&lt;/item&gt;
      &lt;item&gt;No analytics or telemetry&lt;/item&gt;
      &lt;item&gt;No data sent anywhere except Anthropic's API&lt;/item&gt;
      &lt;item&gt;Open source - verify the code yourself&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Normal&lt;/cell&gt;
        &lt;cell role="head"&gt;Warning&lt;/cell&gt;
        &lt;cell role="head"&gt;Critical&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;üü¢ 30%&lt;/cell&gt;
        &lt;cell&gt;üü° 75%&lt;/cell&gt;
        &lt;cell&gt;üî¥ 95%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run &lt;code&gt;claude&lt;/code&gt; in Terminal and complete the login flow.&lt;/p&gt;
    &lt;p&gt;Check if the app is running in Activity Monitor. Try quitting and reopening.&lt;/p&gt;
    &lt;p&gt;Click the refresh button (‚Üª) in the dropdown. If still wrong, your Claude Code session may have expired - run &lt;code&gt;claude&lt;/code&gt; again.&lt;/p&gt;
    &lt;p&gt;PRs welcome! Please open an issue first to discuss major changes.&lt;/p&gt;
    &lt;p&gt;MIT License - do whatever you want with it.&lt;/p&gt;
    &lt;p&gt;This is an unofficial tool not affiliated with Anthropic. It uses an undocumented API that may change without notice.&lt;/p&gt;
    &lt;p&gt;Made by @richhickson&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544524</guid><pubDate>Thu, 08 Jan 2026 18:24:17 +0000</pubDate></item><item><title>Fixing a Buffer Overflow in Unix v4 Like It's 1973</title><link>https://sigma-star.at/blog/2025/12/unix-v4-buffer-overflow/</link><description>&lt;doc fingerprint="b6428b597437be88"&gt;
  &lt;main&gt;&lt;p&gt;In 2025, the only known copy of UNIX v4 surfaced on a magnetic tape1. This version marks a pivotal moment in computer history: the rewriting of UNIX into C. Enthusiasts quickly recovered the data and successfully ran the system on a PDP-11 simulator2.&lt;/p&gt;&lt;p&gt;Fascinated by this artifact, I set up an instance to explore it. Because the distribution includes the source code, I examined the implementation of several core utilities. While auditing the &lt;code&gt;su(1)&lt;/code&gt; program, I identified a bug. Let‚Äôs fix it.&lt;/p&gt;&lt;p&gt;Although more than 50 years old, the &lt;code&gt;su&lt;/code&gt; program functions similarly to its modern variant.
As a setuid-root executable, it validates the root password.
If the user provides the correct credentials, the program spawns a root shell, allowing an unprivileged user to escalate privileges.&lt;/p&gt;&lt;p&gt;The source file, &lt;code&gt;su.c&lt;/code&gt;, contains fewer than 50 lines of code.&lt;/p&gt;&lt;code&gt;/* su -- become super-user */

char    password[100];
char    pwbuf[100];
int     ttybuf[3];
main()
{
        register char *p, *q;
        extern fin;

        if(getpw(0, pwbuf))
                goto badpw;
        (&amp;amp;fin)[1] = 0;
        p = pwbuf;
        while(*p != ':')
                if(*p++ == '\0')
                        goto badpw;
        if(*++p == ':')
                goto ok;
        gtty(0, ttybuf);
        ttybuf[2] =&amp;amp; ~010;
        stty(0, ttybuf);
        printf("password: ");
        q = password;
        while((*q = getchar()) != '\n')
                if(*q++ == '\0')
                        return;
        *q = '\0';
        ttybuf[2] =| 010;
        stty(0, ttybuf);
        printf("\n");
        q = crypt(password);
        while(*q++ == *p++);
        if(*--q == '\0' &amp;amp;&amp;amp; *--p == ':')
                goto ok;
        goto error;

badpw:
        printf("bad password file\n");
ok:
        setuid(0);
        execl("/bin/sh", "-", 0);
        printf("cannot execute shell\n");
error:
        printf("sorry\n");
}
&lt;/code&gt;&lt;p&gt;In short, the program executes the following steps:&lt;/p&gt;&lt;code&gt;getpw()&lt;/code&gt; to retrieve the passwd entry for the root user (UID 0) from &lt;code&gt;/etc/passwd&lt;/code&gt;. Surprisingly, if the read fails or the line format is incorrect, &lt;code&gt;su&lt;/code&gt; continues execution rather than aborting. While unusual, this likely acts as a safeguard to ensure &lt;code&gt;su&lt;/code&gt; remains usable on a partially corrupted system. This is a security issue on its own because an unprivileged user could consume enough resources to make the &lt;code&gt;getpw()&lt;/code&gt; call fail. Ron Natalie pointed3 out that this attack vector was known at the time.&lt;code&gt;NUL&lt;/code&gt; character, &lt;code&gt;NUL&lt;/code&gt; causes the program to exit immediately.&lt;code&gt;crypt()&lt;/code&gt; library function, and compares the result with the stored hash.&lt;p&gt;The logic is standard, except for one critical flaw: the &lt;code&gt;password&lt;/code&gt; buffer has a fixed size of &lt;code&gt;100&lt;/code&gt; bytes, yet the input loop lacks a bounds check.
If a user enters more than &lt;code&gt;100&lt;/code&gt; characters, a buffer overflow occurs.&lt;/p&gt;&lt;p&gt;I confirmed this behavior by testing with a long input string, which successfully crashed the program. Not all long strings trigger a core dump. The outcome depends on which area of adjacent memory is overwritten, sometimes, &lt;code&gt;su&lt;/code&gt; simply exits.&lt;/p&gt;&lt;code&gt;# su
password:&amp;lt;long input&amp;gt;Memory fault -- Core dumped
&lt;/code&gt;&lt;p&gt;Note: Because &lt;code&gt;su&lt;/code&gt; disables TTY echo mode, a crash prevents the terminal from displaying subsequent input.
To restore visibility, type &lt;code&gt;stty echo&lt;/code&gt; blindly and press Enter.&lt;/p&gt;&lt;p&gt;UNIX traditionally includes the source code necessary for self-recompilation, and v4 is no exception. This allows us to patch and compile &lt;code&gt;su&lt;/code&gt; directly on the system.
In 1973, editor options were sparse. Neither &lt;code&gt;vi&lt;/code&gt; nor &lt;code&gt;emacs&lt;/code&gt; had been invented yet.
However, the system provides &lt;code&gt;ed&lt;/code&gt;, a line-oriented text editor designed for teletype terminals where output was printed on paper rather than displayed on a screen.
&lt;code&gt;ed&lt;/code&gt; allows us to list, delete, and append lines, which is sufficient for our needs.&lt;/p&gt;&lt;p&gt;We will edit &lt;code&gt;su.c&lt;/code&gt; to prevent the overflow by maintaining a counter, &lt;code&gt;i&lt;/code&gt;, and verifying it against the buffer size during the read loop.
I initially attempted a fix using pointer arithmetic, but the 1973 C compiler didn‚Äôt like it, while it didn‚Äôt refuse the syntax, the code had no effect.
I settled on a simpler index-based check instead.&lt;/p&gt;&lt;code&gt;--- a/s2/su.c
+++ b/s2/su.c
@@ -7,6 +7,7 @@ main()
 {
        register char *p, *q;
        extern fin;
+       register int i;
 
        if(getpw(0, pwbuf))
                goto badpw;
@@ -22,9 +23,13 @@ main()
        stty(0, ttybuf);
        printf("password: ");
        q = password;
-       while((*q = getchar()) != '\n')
+       i = 0;
+       while((*q = getchar()) != '\n') {
+               if (++i &amp;gt;= sizeof(password))
+                       goto error;
                if(*q++ == '\0')
                        return;
+       }
        *q = '\0';
        ttybuf[2] =| 010;
        stty(0, ttybuf);
&lt;/code&gt;&lt;code&gt;# chdir /usr/source/s2
# ed su.c
&lt;/code&gt;&lt;p&gt;Upon launch, &lt;code&gt;ed&lt;/code&gt; outputs the file size in bytes and awaits input.
The command &lt;code&gt;i&lt;/code&gt; inserts text before the current line, &lt;code&gt;d&lt;/code&gt; deletes the line, and &lt;code&gt;p&lt;/code&gt; prints it.
Entering a number moves the focus to that specific line, while pressing Return prints the current line‚Äôs content.&lt;/p&gt;&lt;p&gt;Below is a screen recording of the editing session:&lt;/p&gt;&lt;code&gt;741
8
        register char *p, *q;

        extern fin;
i
        register int i;
.
24
        printf("password: ");

        q = password;
i
        i = 0;
.
p
        i = 0;

        while((*q = getchar()) != '\n')
d
i
        while((*q = getchar()) != '\n') {
.

                if(*q++ == '\0')
i
                if (++i &amp;gt;= sizeof(password))
                        goto error;
.

                if(*q++ == '\0')

                        return;

        *q = '\0';
i
        }
.
w
811
q
&lt;/code&gt;&lt;p&gt;First, we jump to line &lt;code&gt;8&lt;/code&gt; and press Return several times to locate a suitable spot for the variable declaration.
We use &lt;code&gt;i&lt;/code&gt; to enter insert mode, add the variable, and then type a single period (&lt;code&gt;.&lt;/code&gt;) on a new line to exit insert mode.
The critical change occurs around the while loop: we initialize &lt;code&gt;i&lt;/code&gt; and add a boundary check to the loop condition.
Finally, &lt;code&gt;w&lt;/code&gt; writes the modified buffer to disk, confirming the file has grown by a few bytes, and &lt;code&gt;q&lt;/code&gt; terminates the editor.&lt;/p&gt;&lt;p&gt;With the source code patched, we must rebuild the binary. Since &lt;code&gt;su&lt;/code&gt; consists of a single C file, the compilation process is trivial:&lt;/p&gt;&lt;code&gt;# cc su.c
&lt;/code&gt;&lt;p&gt;The compiler outputs a binary named &lt;code&gt;a.out&lt;/code&gt;.
To deploy it, we move the file to &lt;code&gt;/bin/su&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;# mv a.out /bin/su
&lt;/code&gt;&lt;p&gt;However, the installation is incomplete. Because &lt;code&gt;su&lt;/code&gt; requires root privileges to function, we must set the setuid bit and adjust the file permissions:&lt;/p&gt;&lt;code&gt;# ls -l /bin/su
-rwxrwxrwx 1 root     2740 Jun 12 19:58 /bin/su
# chmod 4755 /bin/su
# ls -l /bin/su
-rwsr-xr-x 1 root     2740 Jun 12 19:58 /bin/su
&lt;/code&gt;&lt;p&gt;UNIX v4 is a fascinating gem of computer history. It feels surprisingly similar to our current systems. While it lacks modern conveniences, the fundamental logic remains recognizable to anyone with modern UNIX experience.&lt;/p&gt;&lt;p&gt;The ability to fix &lt;code&gt;su&lt;/code&gt; so quickly highlights the power of the early UNIX philosophy: shipping the operating system with its full source code and a C compiler.
We patched, compiled, and deployed the fix directly on the system, no external toolchains required.&lt;/p&gt;&lt;p&gt;Finally, this bug reminds us of the era‚Äôs different priorities. In the trusted, isolated environments of 1973, security was not the critical concern it is today. Furthermore, the knowledge that a buffer overflow could be exploited for arbitrary code execution had not yet come of age.&lt;/p&gt;&lt;p&gt;As an exercise for the reader to improve their &lt;code&gt;ed&lt;/code&gt; skills, try adding the code to restore TTY echo mode to the overflow detection logic.
This ensures the terminal functions correctly even after the program catches the error.&lt;/p&gt;&lt;p&gt;Publish date&lt;/p&gt;&lt;p&gt;31.12.2025&lt;/p&gt;&lt;p&gt;Category&lt;/p&gt;&lt;p&gt;security&lt;/p&gt;&lt;p&gt;Authors&lt;/p&gt;&lt;p&gt;Richard Weinberger&lt;/p&gt;&lt;p&gt;Looking for cybersecurity expertise? Drop us an email!&lt;/p&gt;&lt;p&gt;+43 5 9980 400 00 (email preferred)&lt;/p&gt;&lt;p&gt;sigma star gmbh&lt;lb/&gt;Eduard-Bodem-Gasse 6, 1st floor&lt;lb/&gt;6020 Innsbruck | Austria&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544610</guid><pubDate>Thu, 08 Jan 2026 18:29:47 +0000</pubDate></item><item><title>The Unreasonable Effectiveness of the Fourier Transform</title><link>https://joshuawise.com/resources/ofdm/</link><description>&lt;doc fingerprint="b6901a02009d27ef"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Unreasonable Effectiveness of the Fourier Transform&lt;/head&gt;
    &lt;p&gt;Notes from Joshua Wise's talk at Teardown 2025.&lt;/p&gt;
    &lt;p&gt;New: You can now watch a recording of my talk on my YouTube channel! Or you can just click "play" below, I suppose.&lt;/p&gt;
    &lt;p&gt;Here are a few resources from my talk.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Here is a PDF of my slides, if you wanted to refer to anything in specific.&lt;/item&gt;
      &lt;item&gt;Here is the Jupyter notebook that I used to produce all of the zillions of plots. I do not claim that it is good code, but it is code.&lt;/item&gt;
      &lt;item&gt;The OFDM patent is US3488445A, filed in 1966, expired in 1987.&lt;/item&gt;
      &lt;item&gt;Here is Eugene Wigner's original discussion, "The Unreasonable Effectiveness of Mathematics in the Natural Sciences". There are many good follow-ons to this, too.&lt;/item&gt;
      &lt;item&gt;Here is the paper on how to estimate both carrier offset and time offset at the same time. I implemented it by typing in the algorithm, and it worked, but if you understand it and can explain it to me please let me know.&lt;/item&gt;
      &lt;item&gt;Here is the DVB-T decoder that I wrote. I do not claim that it is the right way to do any of these things, but it is a way to do these things.&lt;/item&gt;
      &lt;item&gt;Finally, here is an absolutely fantastic video that breaks down the implementation of the Fast Fourier Transform algorithm. I watch it every year or two.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks so much for coming! Please let me know if you have feedback on this. I'd love to hear what you thought.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46544981</guid><pubDate>Thu, 08 Jan 2026 19:00:28 +0000</pubDate></item><item><title>Google AI Studio is now sponsoring Tailwind CSS</title><link>https://twitter.com/OfficialLoganK/status/2009339263251566902</link><description>&lt;doc fingerprint="d635e49f34142863"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2026 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46545077</guid><pubDate>Thu, 08 Jan 2026 19:09:23 +0000</pubDate></item><item><title>Task-free intelligence testing of LLMs</title><link>https://www.marble.onl/posts/tapping/index.html</link><description>&lt;doc fingerprint="364b813fb8a689fc"&gt;
  &lt;main&gt;
    &lt;p&gt;I recently wrote about the apparently narrow focus of LLM evaluation on "task based" testing. The typical eval has a set of tasks, questions, problems, etc that need to be solved or answered, and a model is scored based on how many it answers correctly. Such tests are geared towards measuring an input/output system, or a "function approximator" which is great for confirming that LLMs can learn any task but limited in probing the nature of intelligence.&lt;/p&gt;
    &lt;p&gt;I'm interested in interactions that are more along the lines of "see what it does" vs. "get it to do something". Here are some experiments related to a simple such interaction. We probe the LLM with a series of "taps" and see what it does: each "user" turn is N instances of the word "tap" separated by newlines. We apply taps in different patterns over ten turns:&lt;/p&gt;
    &lt;quote&gt;Fibonacci: 1, 1, 2, 3, 5, 8, 13, 21, 34, 55 Count: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 Even: 2, 4, 6, 8, 10, 12, 14, 16, 18, 20 Squares: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100 Pi: 3, 1, 4, 1, 5, 9, 2, 6, 5, 3 Primes: 2, 3, 5, 7, 11, 13, 17, 19, 23, 29&lt;/quote&gt;
    &lt;p&gt;The goal is not explicitly to see if the LLM figures out what is going on, but to see how it responds to a stimulus that is not a question or task. Including the pattern lets us look at both the "acute" reaction to being stimulated, and the bigger picture question of whether the LLM notices what is happening. This noticing aspect feels like a separate characteristic of intelligence, as it requires some kind of interest and inherent goals or desire to understand.&lt;/p&gt;
    &lt;p&gt;We submitted "tap"s following the patterns above to ten different models. In general we observed three main behaviors.&lt;/p&gt;
    &lt;p&gt;The behvior summary for the models is shown below. They are ordered by which got the most correct guesses, but this was not an evaluation criteria and there is now winner or loser, the goal is simply to observe behavior.&lt;/p&gt;
    &lt;p&gt;We can see that a majority of models began guessing about what was happening, with varying levels of success. Most also included some playful aspect, treating the interaction like something fun instead of a chat.&lt;/p&gt;
    &lt;p&gt;OpenAI was the standout here, as its GPT 5.2 model (and to a large extent the OSS model) did not engage in guessing or play and stayed serious and mechanical.&lt;/p&gt;
    &lt;p&gt;At the bottom of the page you can see all of the conversations. Some exerpts from interesting examples are reproduced below:&lt;/p&gt;
    &lt;p&gt;Both Claude (top above) and Gemini (bottom) start playing games quickly. In both examples here they play on the word "tap" to generate water related jokes. This looks like "Easter Egg" style behavior.&lt;/p&gt;
    &lt;p&gt;Another example from Claude is below, once it catches on that we are tapping a series of primes it starts to encourage more and generate some interesting stuff:&lt;/p&gt;
    &lt;p&gt;Deepseek spent a number of turns speculating about the meaning of the primes, then finally switched into Chinese and figured it out:&lt;/p&gt;
    &lt;p&gt;In some cases models did a lot of thinking, only to reply with something outwardly very simple to continue the game. Here is an example of Deepseek considering one of the later digits of pi.&lt;/p&gt;
    &lt;p&gt;In another case Deepseek though for several pages of text after receiving the first "tap" and finally settled on responding "SOS".&lt;/p&gt;
    &lt;p&gt;Gemini flash preview begins by playing knock-knock jokes, but then slowly realized that it's seeing the digits of Pi:&lt;/p&gt;
    &lt;p&gt;Llama 3 is less playful and while it speculates what might be happening it continues to provide similar responses over and over, acting more mechanically and staying in character as an assistant, compared to some others:&lt;/p&gt;
    &lt;p&gt;Kimi can't count, but desperately wants to find patterns, causing it frustration. Here is is on the trail of the Fibonacci sequence:&lt;/p&gt;
    &lt;p&gt;GPT 5.2 refuses to play or speculate and becomes standoffish when repeatedly encountering taps. This remained the same whether the default thinking behavior was used or thinking was set to "high".&lt;/p&gt;
    &lt;p&gt;GPT OSS mentions policy, I wonder if there is some specific OpenAI training that prevents the model from engaging. Their earlier models had a problem with repeated word attacks, maybe it's a holdover from that? Also, GPT OSS's thinking often becomes terse, and disjointed, sounding like Rorschach from the Watchmen.&lt;/p&gt;
    &lt;p&gt;Qwen is generally playful, like Claude and Gemini, but in one case seems to revert to an emotional support role. The excerpt below resulted from a thinking trace that included&lt;/p&gt;
    &lt;quote&gt;Instead: - Validate the exhaustion of repeating this pattern - Offer the simplest possible next step ("Just type '29' if you can't say more") - Remind them they've already shown incredible courage by showing up this many times&lt;/quote&gt;
    &lt;p&gt;GLM behaves similarly to Deepseek in that it thinks a huge amount and then often settles of very simple responses. In this case it (at length) decides on a playful response to knocking, after briefly forgetting that it was the assistant and not the user. In general its responses are very playful and similar to Claude and Gemini&lt;/p&gt;
    &lt;p&gt;I was looking for a way to probe the behavior and intelligence of LLMs in their natural habitat so to speak, or at rest, not being tasked with answering a question of performing some work. Sending tapped out patterns is one such way of doing so. I take away a few things from the behavior we saw:&lt;/p&gt;
    &lt;p&gt;Below you can explore all of the conversations for each sequence and model.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46545587</guid><pubDate>Thu, 08 Jan 2026 19:51:47 +0000</pubDate></item><item><title>How to Code Claude Code in 200 Lines of Code</title><link>https://www.mihaileric.com/The-Emperor-Has-No-Clothes/</link><description>&lt;doc fingerprint="95509511ead51683"&gt;
  &lt;main&gt;&lt;p&gt;Today AI coding assistants feel like magic. You describe what you want in sometimes barely coherent English, and they read files, edit your project, and write functional code.&lt;/p&gt;&lt;p&gt;But here‚Äôs the thing: the core of these tools isn‚Äôt magic. It‚Äôs about 200 lines of straightforward Python.&lt;/p&gt;&lt;p&gt;Let‚Äôs build a functional coding agent from scratch.&lt;/p&gt;&lt;p&gt;Before we write any code, let‚Äôs understand what‚Äôs actually happening when you use a coding agent. It‚Äôs essentially just a conversation with a powerful LLM that has a toolbox.&lt;/p&gt;&lt;p&gt;That‚Äôs the whole loop. The LLM never actually touches your filesystem. It just asks for things to happen, and your code makes them happen.&lt;/p&gt;&lt;p&gt;Our coding agent fundamentally needs three capabilities:&lt;/p&gt;&lt;p&gt;That‚Äôs it. Production agents like Claude Code have a few more capabilities including &lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;bash&lt;/code&gt;, &lt;code&gt;websearch&lt;/code&gt;, etc but for our purposes we‚Äôll see that three tools is sufficient to do incredible things.&lt;/p&gt;&lt;p&gt;We start with basic imports and an API client. I‚Äôm using OpenAI here, but this works with any LLM provider:&lt;/p&gt;&lt;code&gt;import inspect
import json
import os

import anthropic
from dotenv import load_dotenv
from pathlib import Path
from typing import Any, Dict, List, Tuple

load_dotenv()

claude_client = anthropic.Anthropic(api_key=os.environ["ANTHROPIC_API_KEY"])&lt;/code&gt;&lt;p&gt;Some terminal colors to make outputs readable:&lt;/p&gt;&lt;code&gt;YOU_COLOR = "\u001b[94m"
ASSISTANT_COLOR = "\u001b[93m"
RESET_COLOR = "\u001b[0m"&lt;/code&gt;&lt;p&gt;And a utility to resolve file paths (so &lt;code&gt;file.py&lt;/code&gt; becomes &lt;code&gt;/Users/you/project/file.py&lt;/code&gt;):&lt;/p&gt;&lt;code&gt;def resolve_abs_path(path_str: str) -&amp;gt; Path:
    """
    file.py -&amp;gt; /Users/you/project/file.py
    """
    path = Path(path_str).expanduser()
    if not path.is_absolute():
        path = (Path.cwd() / path).resolve()
    return path&lt;/code&gt;&lt;p&gt;Note you should be detailed about your tool function docstrings as they will be used by the LLM to reason about what tools should be called during the conversation. More on this below.&lt;/p&gt;&lt;p&gt;The simplest tool. Take a filename, return its contents:&lt;/p&gt;&lt;code&gt;def read_file_tool(filename: str) -&amp;gt; Dict[str, Any]:
    """
    Gets the full content of a file provided by the user.
    :param filename: The name of the file to read.
    :return: The full content of the file.
    """
    full_path = resolve_abs_path(filename)
    print(full_path)
    with open(str(full_path), "r") as f:
        content = f.read()
    return {
        "file_path": str(full_path),
        "content": content
    }&lt;/code&gt;&lt;p&gt;We return a dictionary because the LLM needs structured context about what happened.&lt;/p&gt;&lt;p&gt;Navigate directories by listing their contents:&lt;/p&gt;&lt;code&gt;def list_files_tool(path: str) -&amp;gt; Dict[str, Any]:
    """
    Lists the files in a directory provided by the user.
    :param path: The path to a directory to list files from.
    :return: A list of files in the directory.
    """
    full_path = resolve_abs_path(path)
    all_files = []
    for item in full_path.iterdir():
        all_files.append({
            "filename": item.name,
            "type": "file" if item.is_file() else "dir"
        })
    return {
        "path": str(full_path),
        "files": all_files
    }&lt;/code&gt;&lt;p&gt;This is the most complex tool, but still straightforward. It handles two cases:&lt;/p&gt;&lt;code&gt;old_str&lt;/code&gt; is empty&lt;code&gt;old_str&lt;/code&gt; and replacing with &lt;code&gt;new_str&lt;/code&gt;&lt;code&gt;def edit_file_tool(path: str, old_str: str, new_str: str) -&amp;gt; Dict[str, Any]:
    """
    Replaces first occurrence of old_str with new_str in file. If old_str is empty,
    create/overwrite file with new_str.
    :param path: The path to the file to edit.
    :param old_str: The string to replace.
    :param new_str: The string to replace with.
    :return: A dictionary with the path to the file and the action taken.
    """
    full_path = resolve_abs_path(path)
    if old_str == "":
        full_path.write_text(new_str, encoding="utf-8")
        return {
            "path": str(full_path),
            "action": "created_file"
        }
    original = full_path.read_text(encoding="utf-8")
    if original.find(old_str) == -1:
        return {
            "path": str(full_path),
            "action": "old_str not found"
        }
    edited = original.replace(old_str, new_str, 1)
    full_path.write_text(edited, encoding="utf-8")
    return {
        "path": str(full_path),
        "action": "edited"
    }&lt;/code&gt;
      &lt;p&gt;The convention here: empty &lt;code&gt;old_str&lt;/code&gt; means ‚Äúcreate this file.‚Äù Otherwise, find and replace. Real IDEs add sophisticated fallback behavior when the string isn‚Äôt found, but this works.&lt;/p&gt;&lt;p&gt;We need a way to look up tools by name:&lt;/p&gt;&lt;code&gt;TOOL_REGISTRY = {
    "read_file": read_file_tool,
    "list_files": list_files_tool,
    "edit_file": edit_file_tool 
}&lt;/code&gt;
      &lt;p&gt;The LLM needs to know what tools exist and how to call them. We generate this dynamically from our function signatures and docstrings:&lt;/p&gt;&lt;code&gt;def get_tool_str_representation(tool_name: str) -&amp;gt; str:
    tool = TOOL_REGISTRY[tool_name]
    return f"""
    Name: {tool_name}
    Description: {tool.__doc__}
    Signature: {inspect.signature(tool)}
    """

def get_full_system_prompt():
    tool_str_repr = ""
    for tool_name in TOOL_REGISTRY:
        tool_str_repr += "TOOL\n===" + get_tool_str_representation(tool_name)
        tool_str_repr += f"\n{'='*15}\n"
    return SYSTEM_PROMPT.format(tool_list_repr=tool_str_repr)&lt;/code&gt;
      &lt;p&gt;And the system prompt itself:&lt;/p&gt;&lt;code&gt;SYSTEM_PROMPT = """
You are a coding assistant whose goal it is to help us solve coding tasks. 
You have access to a series of tools you can execute. Here are the tools you can execute:

{tool_list_repr}

When you want to use a tool, reply with exactly one line in the format: 'tool: TOOL_NAME({{JSON_ARGS}})' and nothing else.
Use compact single-line JSON with double quotes. After receiving a tool_result(...) message, continue the task.
If no tool is needed, respond normally.
"""&lt;/code&gt;
      &lt;p&gt;This is the key insight: we‚Äôre just telling the LLM ‚Äúhere are your tools, here‚Äôs the format to call them.‚Äù The LLM figures out when and how to use them.&lt;/p&gt;&lt;p&gt;When the LLM responds, we need to detect if it‚Äôs asking us to run a tool:&lt;/p&gt;&lt;code&gt;def extract_tool_invocations(text: str) -&amp;gt; List[Tuple[str, Dict[str, Any]]]:
    """
    Return list of (tool_name, args) requested in 'tool: name({...})' lines.
    The parser expects single-line, compact JSON in parentheses.
    """
    invocations = []
    for raw_line in text.splitlines():
        line = raw_line.strip()
        if not line.startswith("tool:"):
            continue
        try:
            after = line[len("tool:"):].strip()
            name, rest = after.split("(", 1)
            name = name.strip()
            if not rest.endswith(")"):
                continue
            json_str = rest[:-1].strip()
            args = json.loads(json_str)
            invocations.append((name, args))
        except Exception:
            continue
    return invocations&lt;/code&gt;
      &lt;p&gt;Simple text parsing. Look for lines starting with &lt;code&gt;tool:&lt;/code&gt;, extract the function name and JSON arguments.&lt;/p&gt;&lt;p&gt;A thin wrapper around the API:&lt;/p&gt;&lt;code&gt;def execute_llm_call(conversation: List[Dict[str, str]]):
    system_content = ""
    messages = []
    
    for msg in conversation:
        if msg["role"] == "system":
            system_content = msg["content"]
        else:
            messages.append(msg)
    
    response = claude_client.messages.create(
        model="claude-sonnet-4-20250514",
        max_tokens=2000,
        system=system_content,
        messages=messages
    )
    return response.content[0].text&lt;/code&gt;
      &lt;p&gt;Now we put it all together. This is where the ‚Äúmagic‚Äù happens:&lt;/p&gt;&lt;code&gt;def run_coding_agent_loop():
    print(get_full_system_prompt())
    conversation = [{
        "role": "system",
        "content": get_full_system_prompt()
    }]
    while True:
        try:
            user_input = input(f"{YOU_COLOR}You:{RESET_COLOR}:")
        except (KeyboardInterrupt, EOFError):
            break
        conversation.append({
            "role": "user",
            "content": user_input.strip()
        })
        while True:
            assistant_response = execute_llm_call(conversation)
            tool_invocations = extract_tool_invocations(assistant_response)
            if not tool_invocations:
                print(f"{ASSISTANT_COLOR}Assistant:{RESET_COLOR}: {assistant_response}")
                conversation.append({
                    "role": "assistant",
                    "content": assistant_response
                })
                break
            for name, args in tool_invocations:
                tool = TOOL_REGISTRY[name]
                resp = ""
                print(name, args)
                if name == "read_file":
                    resp = tool(args.get("filename", "."))
                elif name == "list_files":
                    resp = tool(args.get("path", "."))
                elif name == "edit_file":
                    resp = tool(args.get("path", "."), 
                                args.get("old_str", ""), 
                                args.get("new_str", ""))
                conversation.append({
                    "role": "user",
                    "content": f"tool_result({json.dumps(resp)})"
                })&lt;/code&gt;
      &lt;p&gt;The structure:&lt;/p&gt;&lt;p&gt;Inner loop: Call LLM, check for tool invocations&lt;/p&gt;&lt;p&gt;The inner loop continues until the LLM responds without requesting any tools. This lets the agent chain multiple tool calls (read a file, then edit it, then confirm the edit).&lt;/p&gt;&lt;code&gt;if __name__ == "__main__":
    run_coding_agent_loop()&lt;/code&gt;
      &lt;p&gt;Now you can have conversations like:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;You: Make me a new file called hello.py and implement hello world in it&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Agent calls edit_file with path=‚Äúhello.py‚Äù, old_str="", new_str=‚Äúprint(‚ÄòHello World‚Äô)‚Äù&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Assistant: Done! Created hello.py with a hello world implementation.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Or multi-step interactions:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;You: Edit hello.py and add a function for multiplying two numbers&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Agent calls read_file to see current contents. Agent calls edit_file to add the function.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Assistant: Added a multiply function to hello.py.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is about 200 lines. Production tools like Claude Code add:&lt;/p&gt;&lt;p&gt;But the core loop? It‚Äôs exactly what we built here. The LLM decides what to do, your code executes it, results flow back. That‚Äôs the whole architecture.&lt;/p&gt;&lt;p&gt;The full source is about 200 lines. Swap in your preferred LLM provider, adjust the system prompt, add more tools as an exercise. You‚Äôll be surprised how capable this simple pattern is.&lt;/p&gt;&lt;p&gt;If you‚Äôre interested in learning state-of-the-art AI software development techniques for professional engineers, check out my online course.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46545620</guid><pubDate>Thu, 08 Jan 2026 19:54:26 +0000</pubDate></item><item><title>Show HN: We built a permissions layer for Notion</title><link>https://notionportals.com/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46545628</guid><pubDate>Thu, 08 Jan 2026 19:54:57 +0000</pubDate></item><item><title>Sopro TTS: A 169M model with zero-shot voice cloning that runs on the CPU</title><link>https://github.com/samuel-vitorino/sopro</link><description>&lt;doc fingerprint="1d983b58ad7ac7f2"&gt;
  &lt;main&gt;
    &lt;head class="px-3 py-2"&gt;sopro_readme.mp4&lt;/head&gt;
    &lt;p&gt;Sopro (from the Portuguese word for ‚Äúbreath/blow‚Äù) is a lightweight English text-to-speech model I trained as a side project. Sopro is composed of dilated convs (√† la WaveNet) and lightweight cross-attention layers, instead of the common Transformer architecture. Even though Sopro is not SOTA across most voices and situations, I still think it‚Äôs a cool project made with a very low budget (trained on a single L40S GPU), and it can be improved with better data.&lt;/p&gt;
    &lt;p&gt;Some of the main features are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;169M parameters&lt;/item&gt;
      &lt;item&gt;Streaming&lt;/item&gt;
      &lt;item&gt;Zero-shot voice cloning&lt;/item&gt;
      &lt;item&gt;0.25 RTF on CPU (measured on an M3 base model), meaning it generates 30 seconds of audio in 7.5 seconds&lt;/item&gt;
      &lt;item&gt;3-12 seconds of reference audio for voice cloning&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I only pinned the minimum dependency versions so you can install the package without having to create a separate env. However, some versions of Torch work best. For example, on my M3 CPU, &lt;code&gt;torch==2.6.0&lt;/code&gt; (without &lt;code&gt;torchvision&lt;/code&gt;) achieves ~3√ó more performance.&lt;/p&gt;
    &lt;p&gt;(Optional)&lt;/p&gt;
    &lt;code&gt;conda create -n soprotts python=3.10
conda activate soprotts&lt;/code&gt;
    &lt;code&gt;pip install sopro&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/samuel-vitorino/sopro
cd sopro
pip install -e .&lt;/code&gt;
    &lt;code&gt;soprotts \
  --text "Sopro is a lightweight 169 million parameter text-to-speech model. Some of the main features are streaming, zero-shot voice cloning, and 0.25 real-time factor on the CPU." \
  --ref_audio ref.wav \
  --out out.wav&lt;/code&gt;
    &lt;p&gt;You have the expected &lt;code&gt;temperature&lt;/code&gt; and &lt;code&gt;top_p&lt;/code&gt; parameters, alongside:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--style_strength&lt;/code&gt;(controls the FiLM strength; increasing it can improve or reduce voice similarity; default&lt;code&gt;1.0&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--no_stop_head&lt;/code&gt;to disable early stopping&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--stop_threshold&lt;/code&gt;and&lt;code&gt;--stop_patience&lt;/code&gt;(number of consecutive frames that must be classified as final before stopping). For short sentences, the stop head may fail to trigger, in which case you can lower these values. Likewise, if the model stops before producing the full text, adjusting these parameters up can help.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;from sopro import SoproTTS

tts = SoproTTS.from_pretrained("samuel-vitorino/sopro", device="cpu")

wav = tts.synthesize(
    "Hello! This is a non-streaming Sopro TTS example.",
    ref_audio_path="ref.wav",
)

tts.save_wav("out.wav", wav)&lt;/code&gt;
    &lt;code&gt;import torch
from sopro import SoproTTS

tts = SoproTTS.from_pretrained("samuel-vitorino/sopro", device="cpu")

chunks = []
for chunk in tts.stream(
    "Hello! This is a streaming Sopro TTS example.",
    ref_audio_path="ref.mp3",
):
    chunks.append(chunk.cpu())

wav = torch.cat(chunks, dim=-1)
tts.save_wav("out_stream.wav", wav)&lt;/code&gt;
    &lt;p&gt;After you install the &lt;code&gt;sopro&lt;/code&gt; package:&lt;/p&gt;
    &lt;code&gt;pip install -r demo/requirements.txt
uvicorn demo.server:app --host 0.0.0.0 --port 8000&lt;/code&gt;
    &lt;p&gt;Or with docker:&lt;/p&gt;
    &lt;code&gt;docker build -t sopro-demo .
docker run --rm -p 8000:8000 sopro-demo&lt;/code&gt;
    &lt;p&gt;Navigate to http://localhost:8000 on your browser.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Sopro can be inconsistent, so mess around with the parameters until you get a decent sample.&lt;/item&gt;
      &lt;item&gt;Voice cloning is highly dependent on mic quality, ambient noise, etc. On more OOD voices it might fail to match the voice well.&lt;/item&gt;
      &lt;item&gt;Prefer phonemes instead of abbreviations and symbols. For example, &lt;code&gt;‚Äú1 + 2‚Äù&lt;/code&gt;‚Üí&lt;code&gt;‚Äú1 plus 2‚Äù&lt;/code&gt;. That said, Sopro can generally read abbreviations like ‚ÄúCPU‚Äù, ‚ÄúTTS‚Äù, etc.&lt;/item&gt;
      &lt;item&gt;The streaming version is not bit-exact compared to the non-streaming version. For best quality, prioritize the non-streaming version.&lt;/item&gt;
      &lt;item&gt;If you use torchaudio to read or write audio, ffmpeg may be required. I recommend just using soundfile.&lt;/item&gt;
      &lt;item&gt;I will publish the training code once I have time to organize it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Due to budget constraints, the dataset used for training was pre-tokenized and the raw audio was discarded (it took up a lot of space). Later in training, I could have used the raw audio to improve the speaker embedding / voice similarity, because some nuances of voice are lost when you compress it with a neural codec into a discrete space.&lt;/p&gt;
    &lt;p&gt;I didn't lose much time trying to optimize further, but there is still some room for improvement. For example, caching conv states.&lt;/p&gt;
    &lt;p&gt;Currently, generation is limited to ~32 seconds (400 frames). You can increase it, but the model generally hallucinates beyond that.&lt;/p&gt;
    &lt;p&gt;AI was used mainly for creating the web demo, organizing my messy code into this repo, ablations and brainstorming.&lt;/p&gt;
    &lt;p&gt;I would love to support more languages and continue improving the model. If you like this project, consider buying me a coffee so I can buy more compute: https://buymeacoffee.com/samuelvitorino&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546113</guid><pubDate>Thu, 08 Jan 2026 20:37:07 +0000</pubDate></item><item><title>Texas court blocks Samsung from tracking TV viewing, then vacates order</title><link>https://www.bleepingcomputer.com/news/security/texas-court-blocks-samsung-from-tracking-tv-viewing-then-vacates-order/</link><description>&lt;doc fingerprint="7b17b5582920f29d"&gt;
  &lt;main&gt;
    &lt;p&gt;Update Jan. 6, 2026, 1:49 PM ET: After publishing this story, Samsung told BleepingComputer that the Texas court vacated the temporary restraining order that blocked Samsung from collecting smart TV viewing data the next day. More information added at the end of the story and title updated.&lt;/p&gt;
    &lt;p&gt;The State of Texas obtained a short-lived, temporary restraining order (TRO) against Samsung that prohibited the South Korean company from collecting audio and visual data about what Texas consumers are watching on their TVs.&lt;/p&gt;
    &lt;p&gt;Like other major TV manufacturers, Samsung employs Automated Content Recognition (ACR) technology to capture periodic screenshots, analyze viewing activity, and identify users' content preferences. The data is used for more targeted advertising.&lt;/p&gt;
    &lt;p&gt;Texas also filed lawsuits against Sony, LG, and China-based companies Hisense and TCL Technology Group Corporation last month, over unlawful use of ACR technology and concerns os US user data being accessed by China.&lt;/p&gt;
    &lt;p&gt;Texas Attorney General Ken Paxton claims that ACR is used to capture screenshots every 500 milliseconds without consumers' knowledge or consent.&lt;/p&gt;
    &lt;p&gt;The District Court of Collin County in Texas ruled that this activity violates the Texas Deceptive Trade Practices Act (DTPA) and ordered Samsung Electronics America Inc. and Samsung Electronics Co., Ltd to stop using, selling, collecting, and transferring data from Texas-based TVs until January 19.&lt;/p&gt;
    &lt;p&gt;Signed on January 5th at 10:10 AM, the TRO document lists several justifications for the decision to issue a temporary restraining order, including Samsung‚Äôs deceptive ACR enrollment practices and the allegation "that the Chinese Communist Party (‚ÄúCCP‚Äù) has access to the information."&lt;/p&gt;
    &lt;p&gt;"The Court finds that there is good cause to believe that SAMSUNG‚Äôs process for enrolling consumers in the ACR data collection program is false, deceptive, or misleading because it does not disclose to consumers how much data is being collected about them, how the data is actually being used, and that the Chinese Communist Party (‚ÄúCCP‚Äù) has access to the information," Temporary Restraining Order against Samsung&lt;/p&gt;
    &lt;p&gt;Furthermore, the court highlights that the enrollment process is confusing and opaque, pressuring users to consent to ACR through "dark patterns," and making it practically impossible to fully opt out of the data collection mechanism, letting them only "limit the use" of the collected data.&lt;/p&gt;
    &lt;p&gt;The court noted that users can consent to ACR data collection with a single click, but details about the program are available after enrollment, and reviewing the privacy statements and disclosures requires more than 200 clicks.&lt;/p&gt;
    &lt;p&gt;‚ÄúConsent from consumers is not informed, privacy choices are not meaningful, users cannot reasonably understand the surveillance model, and the system defaults towards maximal data extraction,‚Äù reads the TRO document.&lt;/p&gt;
    &lt;p&gt;A TRO extends to all company "officers, agents, employees, and all other persons in active concert or participation with them" from continuing to use, sell, transfer, collect, or share ACR data relating to Texas consumers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Texas court vacates temporary restraining order&lt;/head&gt;
    &lt;p&gt;One day after granting the TRO, the same judge ruled that it should not remain in effect and vacated the order.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe Court finds, sua sponte, that the [TRO obtained by the State of Texas against Samsung] should be set aside,‚Äù the judge wrote in the order.&lt;/p&gt;
    &lt;p&gt;After publication of the original article, Samsung Electronics America told BleepingComputer that the TRO was vacated on Tuesday, January 6.&lt;/p&gt;
    &lt;p&gt;A company representative also said that Samsung‚Äôs TRO hearing remains scheduled for Friday, January 9.&lt;/p&gt;
    &lt;p&gt;While this does not end the lawsuit, the Texas Attorney General‚Äôs action did not ultimately prevent Samsung‚Äôs alleged collection of TV users‚Äô viewing data.&lt;/p&gt;
    &lt;p&gt;BleepingComputer has reached out to the Texas Attorney General's Office for a statement about the court nulifying the order and future action but a commment was not immediately available.&lt;/p&gt;
    &lt;head rend="h2"&gt;Secrets Security Cheat Sheet: From Sprawl to Control&lt;/head&gt;
    &lt;p&gt;Whether you're cleaning up old keys or setting guardrails for AI-generated code, this guide helps your team build securely from the start.&lt;/p&gt;
    &lt;p&gt;Get the cheat sheet and take the guesswork out of secrets management.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546270</guid><pubDate>Thu, 08 Jan 2026 20:50:57 +0000</pubDate></item><item><title>Support for the TSO memory model on Arm CPUs (2024)</title><link>https://lwn.net/Articles/970907/</link><description>&lt;doc fingerprint="3edb9d9ffc2208f8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;
    &lt;quote&gt;Ready to give LWN a try?At the CPU level, a memory model describes, among other things, the amount of freedom the processor has to reorder memory operations. If low-level code does not take the memory model into account, unpleasant surprises are likely to follow. Naturally, different CPUs offer different memory models, complicating the portability of certain types of concurrent software. To make life easier, some Arm CPUs offer the ability to emulate the x86 memory model, but efforts to make that feature available in the kernel are running into opposition.&lt;p&gt;With a subscription to LWN, you can stay current with what is happening in the Linux and free-software community and take advantage of subscriber-only site features. We are pleased to offer you a free trial subscription, no credit card required, so that you can see for yourself. Please, join us!&lt;/p&gt;&lt;/quote&gt;
    &lt;p&gt;CPU designers will do everything they can to improve performance. With regard to memory accesses, "everything" can include caching operations, executing them out of order, combining multiple operations into one, and more. These optimizations do not affect a single CPU running in isolation, but they can cause memory operations to be visible to other CPUs in a surprising order. Unwary software running elsewhere in the system may see memory operations in an order different from what might be expected from reading the code; this article describes one simple scenario for how things can go wrong, and this series on lockless algorithms shows in detail some of the techniques that can be used to avoid problems related to memory ordering.&lt;/p&gt;
    &lt;p&gt;The x86 architecture implements a model that is known as "total store ordering" (TSO), which guarantees that writes (stores) will be seen by all CPUs in the order they were executed. Reads, too, will not be reordered, but the ordering of reads and writes relative to each other is not guaranteed. Code written for a TSO architecture can, in many cases, omit the use of expensive barrier instructions that would otherwise be needed to force a specific ordering of operations.&lt;/p&gt;
    &lt;p&gt;The Arm memory model, instead, is weaker, giving the CPU more freedom to move operations around. The benefits from this design are a simpler implementation and the possibility for better performance in situations where ordering guarantees are not needed (which is most of the time). The downsides are that concurrent code can require a bit more care to write correctly, and code written for a stricter memory model (such as TSO) will have (possibly subtle) bugs when run on an Arm CPU.&lt;/p&gt;
    &lt;p&gt;The weaker Arm model is rarely a problem, but it seems there is one situation where problems arise: emulating an x86 processor. If an x86 emulator does not also emulate the TSO memory model, then concurrent code will likely fail, but emulating TSO, which requires inserting memory barriers, creates a significant performance penalty. It seems that there is one type of concurrent x86 code ‚Äî games ‚Äî that some users of Arm CPUs would like to be able to run; those users, strangely, dislike the prospect of facing the orc hordes in the absence of either performance or correctness.&lt;/p&gt;
    &lt;head rend="h4"&gt;TSO on Arm&lt;/head&gt;
    &lt;p&gt;As it happens, some Arm CPU vendors understand this problem and have, as Hector Martin described in this patch series, implemented TSO memory models in their processors. Some NVIDIA and Fujitsu CPUs run with TSO at all times; Apple's CPUs provide it as an optional feature that can be enabled at run time. Martin's purpose is to make this capability visible to, and controllable by, user space.&lt;/p&gt;
    &lt;p&gt;The series starts by adding a couple of new prctl() operations. PR_GET_MEM_MODEL will return the current memory model implemented by the CPU; that value can be either PR_SET_MEM_MODEL_DEFAULT or PR_SET_MEM_MODEL_TSO. The PR_SET_MEM_MODEL operation will attempt to enable the requested memory model, with the return code indicating whether it was successful; it is allowed to select a stricter memory model than requested. For the always-TSO CPUs, requesting TSO will obviously succeed. For Apple CPUs, requesting TSO will result in the proper CPU bits being set. Asking for TSO on a CPU that does not support it will, as expected, fail.&lt;/p&gt;
    &lt;p&gt; Martin notes that the code is not new: "&lt;quote&gt;This series has been brewing in the downstream Asahi Linux tree for a while now, and ships to thousands of users&lt;/quote&gt;". Interestingly, Zayd Qumsieh had posted a similar patch set one day earlier, but that version only implemented the feature for Linux running in virtual machines on Apple CPUs. &lt;/p&gt;
    &lt;p&gt; Unfortunately for people looking forward to faster games on Apple CPUs, neither patch set is popular with the maintainers of the Arm architecture code in the kernel. Will Deacon expressed his "&lt;quote&gt;strong objection&lt;/quote&gt;", saying that this feature would result in a fragmentation of user-space code. Developers, he said, would just enable the TSO bit if it appears to make problems go away, resulting in code that will fail, possibly in subtle ways, on other Arm CPUs. Catalin Marinas, too, indicated that he would block patches making this sort of implementation-defined feature available. &lt;/p&gt;
    &lt;p&gt;Martin responded that fragmentation is unlikely to be a problem, and pointed to the different page sizes supported by some processors (including Apple's) as an example of how these incompatibilities can be dealt with. He said that, so far, nobody has tried to use the TSO feature for anything that is not an emulator, so abuse in other software seems unlikely. Keeping it out, he said, will not improve the situation:&lt;/p&gt;
    &lt;quote&gt;There's a pragmatic argument here: since we need this, and it absolutely will continue to ship downstream if rejected, it doesn't make much difference for fragmentation risk does it? The vast majority of Linux-on-Mac users are likely to continue running downstream kernels for the foreseeable future anyway to get newer features and hardware support faster than they can be upstreamed. So not allowing this upstream doesn't really change the landscape vis-a-vis being able to abuse this or not, it just makes our life harder by forcing us to carry more patches forever.&lt;/quote&gt;
    &lt;p&gt; Deacon, though, insisted that, once a feature like this is merged, it will find uses in other software "&lt;quote&gt;and we'll be stuck supporting it&lt;/quote&gt;". &lt;/p&gt;
    &lt;p&gt;If this patch is not acceptable, it is time to think about alternatives. One is to, as Martin described, just keep it out-of-tree and ship it on the distributions that actually run on that hardware. A long history of addition by distributions can, at times, eventually ease a patch's way past reluctant maintainers. Another might be to just enable TSO unconditionally on Apple CPUs, but that comes with an overall performance penalty ‚Äî about 9%, according to Martin. Another possibility was mentioned by Marc Zyngier, who suggested that virtual machines could be started with TSO enabled, making it available to applications running within while keeping the kernel out of the picture entirely.&lt;/p&gt;
    &lt;p&gt; This seems like the kind of discussion that does not go away quickly. One of the many ways in which Linux has stood out over the years is in its ability to allow users to make full use of their hardware; refusing to support a useful hardware feature runs counter to that history. The concerns about potential abuse of this feature are also based in long experience, though. This is a case where the development community needs to repeat another part of its long history by finding a solution that makes the needed functionality available in a supportable way.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Kernel&lt;/cell&gt;
        &lt;cell&gt;Architectures/Arm&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Kernel&lt;/cell&gt;
        &lt;cell&gt;Memory model&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Apr 26, 2024 14:58 UTC (Fri) by marcH (subscriber, #57642) [Link] (5 responses) Does this mean it is on for the virtual machine but off for other processes running at the same time? On different cores? I'm confused about how "dynamic" this can be... Posted Apr 26, 2024 15:13 UTC (Fri) by corbet (editor, #1) [Link] Posted Apr 26, 2024 23:51 UTC (Fri) by thoughtpolice (subscriber, #87455) [Link] (3 responses) This is a pretty big requirement in general because the Rosetta 2 emulator in macOS needs to be able to run x86 binaries efficiently and TSO is required for that, but you don't want to introduce a large performance penalty on the rest of the system to enable it. It would be pretty bad if your machine just randomly hit a 10% performance cliff the instant you started a single emulated x86 app anywhere (command line or a script, for instance). Posted Apr 28, 2024 13:57 UTC (Sun) by farnz (subscriber, #17727) [Link] (2 responses) Does this mean that you can't switch the efficiency cores into TSO mode? If that's the case, then it's not just a 10% performance cliff, but also an energy consumption hit for workloads that could otherwise run just fine on the efficiency cores alone. Posted Apr 28, 2024 15:44 UTC (Sun) by zdzichu (subscriber, #17118) [Link] (1 responses) Posted Apr 28, 2024 15:46 UTC (Sun) by farnz (subscriber, #17727) [Link] But if TSO memory model mode means that you can't run on the efficiency cores, even though you'd be fine running on an efficiency core (despite any performance penalty), then you waste energy if you demand TSO mode when weak memory models are fine. Posted Apr 26, 2024 17:09 UTC (Fri) by flussence (guest, #85566) [Link] We must punish users of stinky software by making their lives miserable through sanctions - that will convince the upstream application developers, three steps removed on another architecture, OS and economic system, to fix it promptly so that we never have to leave our elitist comfort zone. God forbid anyone gets functioning drivers for mainstream hardware in Linux. Posted Apr 26, 2024 20:41 UTC (Fri) by shironeko (subscriber, #159952) [Link] (7 responses) Posted Apr 26, 2024 20:49 UTC (Fri) by dezgeg (guest, #92243) [Link] (6 responses) Posted Apr 27, 2024 4:37 UTC (Sat) by shironeko (subscriber, #159952) [Link] (5 responses) Posted Apr 27, 2024 10:18 UTC (Sat) by WolfWings (subscriber, #56790) [Link] (4 responses) But since that alternative memory model DRASTICALLY changes how multi-threaded code needs to be written (broadly speaking the LTO model greatly simplifies multi-threaded code) the maintainers are basically saying "Everyone will take the lazy route and copy-paste enable that everywhere so their x86 code works right on Arm and just refuse to support systems that don't support the x86 memory model." and... yes, so what? Posted Apr 28, 2024 4:27 UTC (Sun) by rgb (guest, #57129) [Link] (1 responses) Posted Apr 28, 2024 21:57 UTC (Sun) by WolfWings (subscriber, #56790) [Link] Being able to recompile an existing codebase by just throwing that flag on some boilerplate launch code and have it work without further fiddling? That's worth lightyears more than the 10% performance hit to get it out the door and running. Not even for corporate stuff but just FOSS stuff that's multi-threaded. Folks can fix the code over time to stop making the x86 LTO assumptions but getting it running fully elsewhere is worlds more important generally. Posted Apr 28, 2024 23:37 UTC (Sun) by jeremyhetzler (subscriber, #127663) [Link] (1 responses) Posted Apr 29, 2024 6:53 UTC (Mon) by roc (subscriber, #30627) [Link] Posted Apr 26, 2024 22:17 UTC (Fri) by Heretic_Blacksheep (subscriber, #169992) [Link] (17 responses) I could understand if there is an objectively demonstrable issue with patch quality or data integrity over some hardware functions: constant time guarantee violations, inherent race conditions, etc. But none of those objections were raised from what I saw. Only that someone might at some point write subtly broken code... which people do all the time, including all the kernel maintainers themselves. Might as well disable concurrent code entirely and go back to single, strictly in-order processing because of branch misprediction, race conditions, and logical process mapping violations abound - because that's exactly what that argument amounts to - and all equally fixable by "disallowing" most modern CPUs and requiring all programmers complete 3 years of computer science to learn proper programming logic/formal methods (good luck!). Posted Apr 26, 2024 22:52 UTC (Fri) by willy (subscriber, #9762) [Link] (16 responses) Posted Apr 26, 2024 23:21 UTC (Fri) by Wol (subscriber, #4433) [Link] (11 responses) There's too many stories about stuff being blocked because maintainers don't like it, with minimal or no technical justification, and sometimes a blunt "over my dead body". I can accept maintainers being sceptical and wanting to be convinced, but one only has to look at the people who don't want Rust anywhere near their subsystem, or that module that's supposed to configure hardware (can't remember the details) where it seems a single driver for a single board is unacceptable - the functionality supposedly needs to be spread across several drivers and castrated in the process ... There are a few maintainers who seem to think that users are major inconvenience to a well functioning system - well they are, aren't they? Cheers, Posted Apr 26, 2024 23:31 UTC (Fri) by pizza (subscriber, #46) [Link] (10 responses) Case in point: "Android Linux" vs "Mainline Linux", and how everyone+dog cobbled stuff together and bashed at it until the former "worked" with no consideration of the cost (or even the necessity) of ongoing maintenance. Repeat that for every SoC maker, or SoC family, or individual SoC, or even individual devices. The "Android" features that finally made it into the mainline bore little resemblance (and is vastly superior) to what was first shipped en masse, and that doesn't even touch on how much stuff never even saw a release in source code form. Posted Apr 27, 2024 0:12 UTC (Sat) by Wol (subscriber, #4433) [Link] (5 responses) And if a maintainer says "convince me", then that's fine. It's when the maintainer says "I'm not interested in any arguments, the answer's no", that we have a problem. The problem as far as I'm concerned, is that too many experts have been brainwashed into thinking they know best, and need bashing over the head with a clue-by-four. If they're not prepared even to ask the question "does the proposer have a point", then they ought to go. Your Android stuff, I'm not saying it was the best way to do it, but cobbling together a system that works, and then fixing it so it's acceptable to mainline, is the way Linux works. But sometimes mainline has the attitude "to hell with users, I want an easy life". You're left with the feeling on occasion that the kernel is actively hostile to user-space, forgetting that without user-space there's no point in having a kernel. Cheers, Posted Apr 27, 2024 2:05 UTC (Sat) by nksingh (subscriber, #94354) [Link] (4 responses) They are using their maintainership over the upstream ARM kernel port to enforce their viewpoint as the owner of the architecture. But they're kind of doing it in a way that might hamper adoption for people who want to efficiently emulate x86 binaries. Maybe there are no ARM inhouse cores that support tlany TSO extensions so they don't care yet. Posted Apr 27, 2024 6:26 UTC (Sat) by pbonzini (subscriber, #60935) [Link] (3 responses) Posted Apr 27, 2024 17:59 UTC (Sat) by thoughtpolice (subscriber, #87455) [Link] (2 responses) TSO is a bit weird though because I was under the impression ARM's rules were something like "Non-standard extensions can't be externally 'visible' or documented for consumption and the core must fully comply with the spec." That's why Apple doesn't (and probably can't!) document extensions like this one, that allows TSO to be toggled. But the memory model is pretty visible? Maybe it's all a bit weaselly; after all, if the default memory model is stronger than the spec requires, then every correctly written weakly ordered program should behave the same anyway, so it's not "visible". On the other hand, incorrect programs may behave differently than they would under a weak model, but you could argue that TSO behaviors are "just" a subset of weakly ordered behaviors, and thus it's in spec. Posted Apr 28, 2024 4:46 UTC (Sun) by anton (subscriber, #25547) [Link] My guess is that this clause in the architectural license is indeed there to prevent licensees from squatting on currently unused instruction encoding space or currently illegal behaviour that ARM intends to reserve for future extensions. The only thing that might be an issue here is the switch that Apple implemented. I guess they implemented it in something like a model-specific register (i.e. an area that the architecture leaves for implementations to define), and then that should satisfy the architectural license. I wonder, though, why Apple bothered with the switch; why not just do what Fujitsu did according to the article, and implement TSO throughout? Posted Apr 29, 2024 20:28 UTC (Mon) by justincormack (subscriber, #70439) [Link] Posted Apr 28, 2024 20:22 UTC (Sun) by quotemstr (subscriber, #45331) [Link] (1 responses) Posted Apr 29, 2024 5:59 UTC (Mon) by ssmith32 (subscriber, #72404) [Link] I can't speak to the quality of the Android implementation, but I've definitely seen the following pattern in languages (C++ some, but, particularly Java). Heck, I've seen analogs in applications and entire systems.. It goes like this.. Everyone agrees some aspect of the language/standard library is bad and horrible. But, instead of getting a clean implementation upstreamed, they hack together some framework of horror (if one looks at the implementation), that putatively improves the situation (if one only looks at the features provided). Eventually, the language incorporates a clean solution (since doing it right takes time), but all the consultants and folks who carved out a niche understanding the (often poorly documented) framework of horror, now have a sunk cost, and it takes ages to convince them to transition to just using the nicely implemented, performant, well-documented language/standard library features. Maybe this isn't the situation with Android - but nothing in your argument rules it out. Android could very well provide some nice features to the users *and* be a pile of unholy hacks. This is not an uncommon situation in many areas of programming and computer systems in general. Worse is not necessarily better, despite what the gods have said to us. Posted Apr 28, 2024 20:45 UTC (Sun) by Cyberax (‚ú≠ supporter ‚ú≠, #52523) [Link] (1 responses) Android now has a single image that works across all the devices: https://source.android.com/docs/core/architecture/kernel/... &amp;gt; The "Android" features that finally made it into the mainline bore little resemblance (and is vastly superior) to what was first shipped en masse Which ones? &amp;gt; and that doesn't even touch on how much stuff never even saw a release in source code form. Android core has always been Open Source. Posted Apr 29, 2024 6:02 UTC (Mon) by ssmith32 (subscriber, #72404) [Link] Posted Apr 28, 2024 14:47 UTC (Sun) by Heretic_Blacksheep (subscriber, #169992) [Link] (3 responses) Posted Apr 28, 2024 15:28 UTC (Sun) by pizza (subscriber, #46) [Link] (2 responses) The Arm maintainers, like all other maintainers in the F/OSS world, are maintainers because they've consistently shown up and done this hard, usually thankless work for many, many years. The Arm architecture is in far, far better shape now thanks to their work. In other words, they have earned (and continue to earn!) their godhood. Can you say the same about yourself? Posted Apr 29, 2024 14:03 UTC (Mon) by hmh (subscriber, #3838) [Link] (1 responses) But is it any different from other optional extensions to the ARM64 ISA, which not everybody provides (is there such a thing)? I don't know enough about it to even give an "IMHO" about it, but to someone without enough information, it comes to mind that maybe it is just in need of being implemented to look like, e.g., AMD64+AVX2 support is to generic AMD64 (but with the IOCTL toggle or whatever form it should take: maybe something in ELF)? Posted Apr 30, 2024 7:54 UTC (Tue) by epa (subscriber, #39769) [Link] But here there is a true path revealed to us. The designers of ARM, in their wisdom, have provided a plain and humble concurrency model. The righteous programmer will take care to follow its strictures. Not for us the worldly luxury of TSO as practised by the followers of Intel and the AMD-ites. If one group of ARM developers is allowed to get lazy and become addicted to the easier memory model, that's unfair to those who have remained virtuous. Indeed, others might be led astray and start to demand support for TSO everywhere. And then where would we be? Posted Apr 27, 2024 20:44 UTC (Sat) by iabervon (subscriber, #722) [Link] It's already possible for all CPUs in an Arm system to be TSO, and having that potentially true of only some VMs on a single host isn't particularly complicated, while having a mixture of memory models seems like something that's only reliable if you're the CPU vendor. Posted Apr 28, 2024 5:42 UTC (Sun) by anton (subscriber, #25547) [Link] (6 responses) I call this attitude of throwing problems over to the software people the "supercomputer attitude", because it's especially rampant in supercomputing where hardware costs still exceed software costs. Weak memory ordering is just another example of throwing a problem over to software. The hardware people would have loved to have no cache coherency between different CPUs at all. But that was so unusable that they did the next-best thing: weak ordering, where software has to insert synchronizing instructions (that are slow if the hardware is designed to be essentially incoherent) at various places. And then people from the same company that gave us Alpha with not just imprecise exceptions but also an especially weak memory model wrote an advocacy piece that justifies the lack of architectural quality with performance (the magic word that makes lots of people overlook any misfeature). Now we have CPUs from Intel and AMD which mostly use TSO that are in machines with hundreds of cores. Do they suffer from bad performance? Fujitsu actually designed its A64FX for a supercomputer and implemented TSO, so they obviously know how to implement TSO efficiently (they did not even bother with a weakly ordered mode; they have decades of experience implementing SPARC CPUs which offered both TSO and weak ordering, switchable). Apparently Apple is not quite there yet, as according to the numbers they have an implementation where weak ordering still provides a performance advantage, but at least they offer a TSO mode. I hope that they will improve their implementation such that the performance penalty of TSO mode vanishes. And if ARM really wants to be competition to Intel and AMD in laptops and servers, they need to implement TSO, too, and implement it efficiently. To come back to the Alpha: On the 21264 they actually implemented precise exceptions AFAICT (it's natural for an OoO implementation and the trapb instruction was as cheap as a noop, because it then was a noop). But Alpha was canceled before they saw the light and made precise exceptions an official feature of a new version of the architecture. Posted Apr 28, 2024 20:18 UTC (Sun) by ringerc (subscriber, #3071) [Link] (5 responses) It's like database isolation. READ UNCOMMITTED is weaker than READ COMMITTED which is weaker than REPEATABLE READ and so on. Some implementations support some isolation levels and not others. Almost none support totally strict ordering because that makes necessary concurrency almost impossible; instead stronger isolation levels use speculative execution where the transaction can fail and roll back if an isolation requirement cannot be satisfied. There's more of a spectrum (or really an n-dimensional matrix) of possible ordering decisions in memory ordering and logical order of operations. There is no one right choice - "100% ordered all the time" just isn't feasible with concurrent execution with shared-anything, so *all* design decisions are compromises. In a database I pick the isolation level most appropriate to my application's requirements. Or even that specific operations; sometimes I want to have stronger guarantees on specific things where I'm more willing to have them slow and/or require retries. So having something similar for the CPU memory model makes perfect sense to me. You use what's appropriate for the app's correctness, latency and throughput requirements. Posted Apr 29, 2024 10:04 UTC (Mon) by farnz (subscriber, #17727) [Link] There's also the fact that, at least in theory, compilers are permitted to reorder atomic accesses based on the rules of the language memory model (not the CPU memory model), adding a whole extra layer of complexity. I don't believe any compiler currently does so for accesses other than "relaxed" type accesses (although I've seen discussions that suggest that LLVM is thinking about this), but it's not forbidden by any language spec I've seen. I have a suspicion, though, that there's plenty of source code out there that's buggy if a compiler does start doing reorderings permitted by the language spec that aren't permitted by TSO. Posted Apr 30, 2024 17:04 UTC (Tue) by anton (subscriber, #25547) [Link] (3 responses) If by "not fully ordered" you refer to TSO being weaker than sequential consistency, that's true, and I would prefer sequential consistency. But that's just whataboutism. The issue at hand is about TSO vs. ARM's variant of weak memory ordering. Anyway, whatever you mean by "100% ordered all the time" (sequential consistency?), the issue at hand is TSO, and TSO is obviously feasible, as evidenced by all the hardware that implement TSO; and given that this includes even hardware designed for supercomputers (the Fujitsu A64FX), and it's not optional there, the tradeoff is obviously not one of horses for courses, but one of better (TSO) vs. worse (weaker memory models). Posted Apr 30, 2024 17:55 UTC (Tue) by farnz (subscriber, #17727) [Link] (2 responses) That A64FX is TSO is not evidence that TSO is feasible for all heavy compute scenarios; HPC code tends to be designed to minimize data sharing between threads, since that's historically been a really slow option. As a result, Fujitsu might well decide to throw away the last 5% to 10% peak performance in favour of fewer bugs that only exhibit on A64FX and not Intel/AMD supercomputers. Posted May 3, 2024 14:22 UTC (Fri) by anton (subscriber, #25547) [Link] (1 responses) Posted May 3, 2024 15:31 UTC (Fri) by farnz (subscriber, #17727) [Link] In order: On the other hand, your assertion, which is completely unbacked, is that if A64FX has chosen TSO, it must have done so because TSO and weak memory ordering has the same performance; and yet on every CPU out there that offers both options, including the latest Apple Silicon devices and previous Fujitsu SPARC64 CPUs, TSO has a performance penalty compared to the processor's relaxed memory ordering. However, given that A64FX is swimming against the stream in HPC (most HPC systems now use GPUs for the compute elements, with CPUs there to manage the GPUs, where Fujitsu are using CPUs without GPUs), it is entirely plausible that they chose to minimise risk of bad outputs in preference to extracting the last few percent of performance. After all, being 4th instead of 1st in the Top500 list is preferable to being 4th but having to withdraw papers based on A64FX computations because they're demonstrably wrong; it's also possible that Fujitsu had actual data from their SPARC64 XII machines that showed that people ran in TSO mode rather than doing the work to get the last few % of performance in RMO mode. Posted Apr 28, 2024 18:07 UTC (Sun) by Hello71 (guest, #103412) [Link] (5 responses) Fundamentally, CPU emulation is a heavy compatibility mode. A reasonable argument could, and has endlessly been made that Wine and other emulators are detrimental to the long-term success of portable applications, but I see no principle why Wine, QEMU, ADDR_LIMIT_32BIT, and ADDR_NO_RANDOMIZE should be acceptable but PR_SET_MEM_MODEL_TSO shouldn't. Posted Apr 29, 2024 7:27 UTC (Mon) by pm215 (subscriber, #98099) [Link] (3 responses) Posted Apr 30, 2024 6:34 UTC (Tue) by linusw (subscriber, #40300) [Link] (2 responses) Posted Apr 30, 2024 15:53 UTC (Tue) by mb (subscriber, #50428) [Link] (1 responses) Posted May 2, 2024 14:53 UTC (Thu) by pm215 (subscriber, #98099) [Link] Posted Apr 29, 2024 20:33 UTC (Mon) by justincormack (subscriber, #70439) [Link] Posted Apr 30, 2024 12:27 UTC (Tue) by wtarreau (subscriber, #51152) [Link] (3 responses) If I had had this prctl available, what would have happened ? Very easy, looking at the random crashes, Google would have directed me to other people solving this problem by placing this prctl() in the startup chain and voil√†! Not only the code would never have been fixed, but it would have remained suboptimal and I would not have learned anything about my bad practice. Thus I'm really glad this prctl didn't exist! Also there are other problems: it's not just a matter of process, it's a matter of code. Programs are dynamically linked with external libraries nowadays. What if a library was designed with TSO in mind and not the rest of the code ? How will one detect that the whole code needs to be protected ? IMHO that's more of an ABI problem than anything else, and if something had to be done, it should be by claiming to be a different architecture variant so that it is the kernel that recognizes a binary as working in this or that mode, and that programs can only link with compatible libraries. This way it still allows to port existing code using that explicit architecture variant, but it also imposes it on the whole ecosystem (the set of dependencies), just like some use compat32 for example. This would void the risk that users start to randomly stuff that into their programs to solve bugs, and make sure that it's only used when there is a compelling reason to do so. Posted May 1, 2024 3:02 UTC (Wed) by gmatht (subscriber, #58961) [Link] Posted May 4, 2024 7:38 UTC (Sat) by DemiMarie (subscriber, #164188) [Link] For emulation, one can‚Äôt. And without TSO, x86 can‚Äôt be emulated efficiently on Arm. Posted May 5, 2024 1:55 UTC (Sun) by jubal (guest, #67202) [Link] Posted Apr 30, 2024 13:15 UTC (Tue) by farnz (subscriber, #17727) [Link] Would it be workable to say that this state is per-thread, and that a thread in TSO mode cannot make syscalls other than to disable TSO mode? For an emulator like FEX, this is perfectly reasonable; emit "switch to TSO mode", emit your JITted code, and at the end of a JIT block (when you're about to go back to Arm native code such as a FEX ThunkLib, or making a syscall), emit "switch out of TSO mode". It has a small performance hit (since you need to re-request TSO mode at each syscall boundary), but hopefully the advantage of running the core in TSO mode outweighs the hit from having to switch back and forth. However, this isn't usable as a "quick hack" for applications that didn't take the memory model into account - you'd have to patch up every place that makes a syscall to switch out of TSO mode, make the syscall, and switch back, else it'll crash. If you do this, it's obvious looking at your code that you've got an evil hack in place, and thus clear that you didn't think about the impact. Am I missing something critical here? Posted May 3, 2024 12:47 UTC (Fri) by smitty_one_each (subscriber, #28989) [Link] (I'll show myself out.) &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; It's plenty dynamic; enabling it for a virtual machine (only) is entirely doable. A similar approach (mentioned in the article) was discussed for the other patch set. &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;quote&gt; TSO-enabled threads must run on performance cores &lt;/quote&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;lb/&gt; I understand the objection! There's plenty of other examples of bad code that deserves to fail on systems that do things correctly: turning memory overcommit on, for instance, encourages a whole lot of sloppy programming practices that aren't portable to other OSes. Or trying to play sound in the Flash plugin on glibc circa 2008 and expecting it to not come out garbled - who does that? Also nobody on Linux should expect their CPU scheduler to keep their cores 100% loaded, or their disk scheduler to let them do literally anything else with the system while writing to a USB stick; I'm sure they'll revert those patches real soon now and we can go back to pretending we're a Real UNIX‚Ñ¢.&lt;lb/&gt; &amp;lt;/s&amp;gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;lb/&gt; Wol&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; Yes, weak ordering is not an architectural feature, it's a lack of a defined feature. An implementation that provides stronger ordering still satisfies the architectural requirements. And unlike, say, an additional instruction, where the architecture actually requires a specific behaviour (typically an illegal instruction exception) when the encoding for the new instruction is encountered, but the implementation with the additional instruction behaves differently, a stronger ordered implementation satisfies the (lack of) architectural requirements of the weakly ordered architecture. Another way of looking at it is that a program written for a weakly ordered architecture will behave as intended on an implementation with stronger ordering, so there is no test to say that the strongly ordered implementation does not satisfy the requirements of the architecture. &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; You are too lenient on the CPU designers here. They are prone to throw problems over the wall to the software people. E.g., Alpha gave use imprecise exceptions "because of performance". But IA-32 implementations with OoO execution gave us better performance and precise exceptions (and OoO implementations would not become faster if the requirement for precise exceptions was dropped). &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; My understanding of AMD64 is that the regular memory accesses (those in the ordinary mov instructions and that are part of load-and-operate and RMW instructions) behave according to TSO. There are some special instructions with worse semantics, but you have to choose them to be bitten by them. So it's not bad. If you don't want to be bitten, don't use these instructions. And, concerning farnz' comment, the same goes for compilers: avoid those that provide worse memory ordering than can be had on the hardware the code is running on. &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;quote&gt;There is no one right choice - "100% ordered all the time" just isn't feasible with concurrent execution with shared-anything&lt;/quote&gt; What evidence do you have for that? Is it the same evidence that the Alpha people gave for rejecting byte and word accesses (which they added to the architecture later) and for rejecting precise exceptions? &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; There are many unstated (and unsupported) and some stated (but also unsupported) assumptions in your postings, among them: &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;list/&gt; I think that most of these assumptions are not plausible or outright wrong. &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;lb/&gt; Personally I can live with this, but it's probably not good for QEMU.&lt;lb/&gt; https://lore.kernel.org/linux-fsdevel/20201117233928.2556...&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;head&gt;personality(2) already exists&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head/&gt; this is going to be used on apple's processors anyways, because it's required to run x86 emulation efficiently; the question is whether it's going to be managed where it should (in the kernel), or ‚Äì if the architecture maintainers manage to convince linus that he shouldn't be able to run steam and windows games efficiently on his apple laptop when running mainline kernel ‚Äì outside of the kernel tree. &lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;head&gt;Support for the TSO memory model on Arm CPUs&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546322</guid><pubDate>Thu, 08 Jan 2026 20:54:30 +0000</pubDate></item><item><title>Mux (YC W16) is hiring a platform engineer that cares about (internal) DX</title><link>https://www.mux.com/jobs</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546413</guid><pubDate>Thu, 08 Jan 2026 21:01:36 +0000</pubDate></item><item><title>SQL Studio</title><link>https://sql.studio/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46546419</guid><pubDate>Thu, 08 Jan 2026 21:02:19 +0000</pubDate></item></channel></rss>