<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 23 Dec 2025 13:08:53 +0000</lastBuildDate><item><title>Claude Code gets native LSP support</title><link>https://github.com/anthropics/claude-code/blob/main/CHANGELOG.md</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46355165</guid><pubDate>Mon, 22 Dec 2025 15:59:01 +0000</pubDate></item><item><title>Flock Exposed Its AI-Powered Cameras to the Internet. We Tracked Ourselves</title><link>https://www.404media.co/flock-exposed-its-ai-powered-cameras-to-the-internet-we-tracked-ourselves/</link><description>&lt;doc fingerprint="2643fb572e61a1da"&gt;
  &lt;main&gt;
    &lt;p&gt;I am standing on the corner of Harris Road and Young Street outside of the Crossroads Business Park in Bakersfield, California, looking up at a Flock surveillance camera bolted high above a traffic signal. On my phone, I am watching myself in real time as the camera records and livestreams me—without any password or login—to the open internet. I wander into the intersection, stare at the camera and wave. On the livestream, I can see myself clearly. Hundreds of miles away, my colleagues are remotely watching me too through the exposed feed.&lt;/p&gt;
    &lt;p&gt;Flock left livestreams and administrator control panels for at least 60 of its AI-enabled Condor cameras around the country exposed to the open internet, where anyone could watch them, download 30 days worth of video archive, and change settings, see log files, and run diagnostics.&lt;/p&gt;
    &lt;p&gt;Unlike many of Flock’s cameras, which are designed to capture license plates as people drive by, Flock’s Condor cameras are pan-tilt-zoom (PTZ) cameras designed to record and track people, not vehicles. Condor cameras can be set to automatically zoom in on people’s faces as they walk through a parking lot, down a public street, or play on a playground, or they can be controlled manually, according to marketing material on Flock’s website. We watched Condor cameras zoom in on a woman walking her dog on a bike path in suburban Atlanta; a camera followed a man walking through a Macy’s parking lot in Bakersfield; surveil children swinging on a swingset at a playground; and film high-res video of people sitting at a stoplight in traffic. In one case, we were able to watch a man rollerblade down Brookhaven, Georgia’s Peachtree Creek Greenway bike path. The Flock camera zoomed in on him and tracked him as he rolled past. Minutes later, he showed up on another exposed camera livestream further down the bike path. The camera’s resolution was good enough that we were able to see that, when he stopped beneath one of the cameras, he was watching rollerblading videos on his phone.&lt;/p&gt;
    &lt;p&gt;The exposure was initially discovered by YouTuber and technologist Benn Jordan and was shared with security researcher Jon “GainSec” Gaines, who recently found numerous vulnerabilities in several other models of Flock’s automated license plate reader (ALPR) cameras. They shared the details of what they found with me, and I verified many of the details seen in the exposed portals by driving to Bakersfield to walk in front of two cameras there while I watched myself on the livestream. I also pulled Flock’s contracts with cities for Condor cameras, pulled details from company presentations about the technology, and geolocated a handful of the cameras to cities and towns across the United States. Jordan also filmed himself in front of several of the cameras on the Peachtree Creek Greenway bike path. Jordan said he and Gaines discovered many of the exposed cameras with Shodan, an internet of things search engine that researchers regularly use to identify improperly secured devices.&lt;/p&gt;
    &lt;p&gt;After finding links to the feed, “immediately, we were just without any username, without any password, we were just seeing everything from playgrounds to parking lots with people, Christmas shopping and unloading their stuff into cars,” Jordan told me in an interview. “I think it was like the first time that I actually got like immediately scared … I think the one that affected me most was as playground. You could see unattended kids, and that’s something I want people to know about so they can understand how dangerous this is.” In a YouTube video about his research, Jordan said he was able to use footage pulled from the exposed feed to identify specific people using open source investigation tools in order to show how trivially an exposure like this could be abused.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46355548</guid><pubDate>Mon, 22 Dec 2025 16:31:40 +0000</pubDate></item><item><title>Uplane (YC F25) Is Hiring Founding Engineers (Full-Stack and AI)</title><link>https://www.useparallel.com/uplane1/careers</link><description>&lt;doc fingerprint="d37fffed7efd5e8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46355932</guid><pubDate>Mon, 22 Dec 2025 17:00:34 +0000</pubDate></item><item><title>NIST was 5 μs off UTC after last week's power cut</title><link>https://www.jeffgeerling.com/blog/2025/nist-was-5-μs-utc-after-last-weeks-power-cut</link><description>&lt;doc fingerprint="b418ddda077cc07f"&gt;
  &lt;main&gt;
    &lt;p&gt;If you were 5 microseconds late today, blame it on NIST.&lt;/p&gt;
    &lt;p&gt;Their facility in Boulder Colorado just had its power cut for multiple days. After a backup generator failed, their main ensemble clock lost track of UTC, or Universal Time Coordinated.&lt;/p&gt;
    &lt;p&gt;But even if you used the NTP timing servers they run, they were never off by more than 5 microseconds.&lt;/p&gt;
    &lt;p&gt;5 μs might seem insignificant. But it is significant for scientists and universities who rely on NIST's more specialized timing signals.&lt;/p&gt;
    &lt;p&gt;But no, you don't need to panic. And yes, they have it under control now.&lt;/p&gt;
    &lt;p&gt;But I thought I'd go over what happened, what it means, and what we can learn from NIST's near-outage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Video&lt;/head&gt;
    &lt;p&gt;This blog post is a lightly-edited transcript of my most recent YouTube video:&lt;/p&gt;
    &lt;head rend="h2"&gt;What happened&lt;/head&gt;
    &lt;p&gt;The NIST campus, which distributes Internet time on six of the most popular NTP servers, lost power last Wednesday.&lt;/p&gt;
    &lt;p&gt;The power company was forced to cut power because of wind gusts over 100 mph (160 km/h). Power lines were coming down and they didn't want to risk starting a wildfire.&lt;/p&gt;
    &lt;p&gt;The whole campus was locked down for safety, so nobody could enter or exit.&lt;/p&gt;
    &lt;p&gt;They have backup generators. And those were working... but apparently one of the generators failed after a couple days. Specifically, the generator that powered the main ensemble clock that's used by the NTP servers.&lt;/p&gt;
    &lt;p&gt;Things were dicey last Friday, and they couldn't get any more staff in to fix it.&lt;/p&gt;
    &lt;p&gt;It got to the point Jeff Sherman, the Group Leader for NIST's Time Realization and Distribution Group, considered shutting down the backup generator that powered the time servers. That would've prevented them from sending out inaccurate time, which would be worse than no time at all for a lot of applications.&lt;/p&gt;
    &lt;p&gt;NTP's designed so you have multiple servers you look at, and if one fails, it won't cause you to lose time.&lt;/p&gt;
    &lt;p&gt;And luckily for NIST, they have another building in their Boulder campus with more clocks, and that building could transfer time back to the one that had the power failure, if they needed to.&lt;/p&gt;
    &lt;p&gt;But yesterday Jeff posted another update: power was restored, and apparently there were still some staff on-site who saved the clocks.&lt;/p&gt;
    &lt;p&gt;They were able to re-route emergency power after the main backup generator went down.&lt;/p&gt;
    &lt;p&gt;Battery backups, I'm assuming some big UPSes, were able to bridge the gap, until they got the backup backup power going.&lt;/p&gt;
    &lt;p&gt;When all was said and done, their monitoring showed deviation from UTC was less than 5 μs.&lt;/p&gt;
    &lt;p&gt;Seeing all that, Jeff and the team at NIST decided to keep their time servers online.&lt;/p&gt;
    &lt;p&gt;But why would they do that, if they were off? Well, time scales are important here. If you're on a Mac like I am, go in the Terminal and run &lt;code&gt;sntp time-a-b.nist.gov&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This or a command like &lt;code&gt;ntpdate&lt;/code&gt; on Linux gives back an error bound, that shows latency between your computer and the NTP time servers.&lt;/p&gt;
    &lt;code&gt;$ sntp time-a-b.nist.gov
+0.005771 +/- 0.035081 time-a-b.nist.gov 132.163.96.1
&lt;/code&gt;
    &lt;p&gt;In my case, it's showing 0.035 seconds. That's 35 milliseconds, or 35 thousand microseconds. 5 microseconds isn't even a blip there.&lt;/p&gt;
    &lt;p&gt;So instead of taking down the servers, which could cause more problems, NIST kept them online.&lt;/p&gt;
    &lt;p&gt;But Jeff said NIST's time is usually about 5,000x more accurate. And if you're one of the universities or aerospace companies that relies on NIST for timing, a 5 μs difference probably does matter.&lt;/p&gt;
    &lt;p&gt;So they'll be working with those groups directly. But for most people, they'll never even notice.&lt;/p&gt;
    &lt;p&gt;Jeff finished off the email mentioning the US GPS system failed over successfully to the WWV-Ft. Collins campus. So again, for almost everyone, there was zero issue, and the redundancy designed into the system worked like it's supposed to.&lt;/p&gt;
    &lt;head rend="h2"&gt;Time is fragile&lt;/head&gt;
    &lt;p&gt;I was following this closely over the weekend. I have two Raspberry Pi GPS clocks in the studio. One runs my main Stratum 0 NTP server, and the other one I have running as a backup for testing. (Yes I know I should have 4+ going for good quorum.)&lt;/p&gt;
    &lt;p&gt;They both run off my outdoor GPS antenna, which is distributed in my rack room and in my studio for time research.&lt;/p&gt;
    &lt;p&gt;Like my studio, most places that need precise time rely on GPS. And that could be a problem!&lt;/p&gt;
    &lt;p&gt;I'm glad redundancies kept GPS from drifting—I don't know what would happen if GPS time goes away, but it wouldn't be good! But the main takeaway I think is this: timing infrastructure is fragile.&lt;/p&gt;
    &lt;p&gt;CISA identified a lot of risk in the US's over-dependence on GPS.&lt;/p&gt;
    &lt;p&gt;Because of that, the US announced it's trying to find good alternatives for PNT (Position, Navigation, and Timing) earlier this year.&lt;/p&gt;
    &lt;p&gt;I was actually at a meeting at the NAB where Jeff Sherman, the scientist who wrote the two NIST updates, was talking about BPS. The Broadcast Positioning System would give us redundancy even if GPS was down.&lt;/p&gt;
    &lt;p&gt;But even with multiple time sources, some places need more. I have two Rubidium atomic clocks in my studio, including the one inside a fancy GPS Disciplined Oscillator (GPSDO). That's good for holdover. Even if someone were jamming my signal, or my GPS antenna broke, I could keep my time accurate to nanoseconds for a while, and milliseconds for months. That'd be good enough for me.&lt;/p&gt;
    &lt;p&gt;(If I'm being truthful, it's actually overkill, but I'm in the time-nut rabbit hole now—if you know, you know.)&lt;/p&gt;
    &lt;p&gt;But some places do need nanoseconds, for science experiments, RF, media, or finance. And they might run their own even more precise clocks. But they still trace things back to NIST, at least most do here in the US.&lt;/p&gt;
    &lt;p&gt;So when NIST's disaster response is tested, everyone's watching.&lt;/p&gt;
    &lt;p&gt;Last week, when we were microseconds from disaster, the team at NIST fixed it so almost nobody noticed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;Jeff Sherman just posted a final update on the situation:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By 22:24 UTC (3:24 pm MST) on Sunday, Dec 21, NIST staff brought the offset of UTC(NIST) from UTC to the level of a few nanoseconds (billionths of a second), which is well within the normal range of precision for this system. This re-alignment was achieved using a backup time scale system at NIST Boulder. Assessment and other service repairs continue, but UTC(NIST) as furnished to the Boulder Internet Time Service is now providing accurate time.&lt;/p&gt;
      &lt;p&gt;We have determined that during the backup-power outage, the UTC(NIST) signal provided to the Boulder Internet Time Service did not deviate by more than 5 microseconds (five millionths of one second). As the typical uncertainty of time transfer over the public Internet is on the order of one millisecond (1/1000th of a second), we can say in retrospect that the accuracy of the Internet Time Service was not compromised and that users were not impacted by the time deviation.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46355949</guid><pubDate>Mon, 22 Dec 2025 17:01:28 +0000</pubDate></item><item><title>GLM-4.7: Advancing the Coding Capability</title><link>https://z.ai/blog/glm-4.7</link><description>&lt;doc fingerprint="674d8ad96d9bceda"&gt;
  &lt;main&gt;
    &lt;p&gt;GLM-4.7, your new coding partner, is coming with the following features:&lt;/p&gt;
    &lt;p&gt;You can also see significant improvements in many other scenarios such as chat, creative writing, and role-play scenario.&lt;/p&gt;
    &lt;p&gt;Benchmark Performance. More detailed comparisons of GLM-4.7 with other models GPT-5, GPT-5.1-High, Claude Sonnet 4.5, Gemini 3.0 Pro, DeepSeek-V3.2, Kimi K2 Thinking, on 17 benchmarks (including 8 reasoning, 5 coding, and 3 agents benchmarks) can be seen in the below table.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Benchmark&lt;/cell&gt;
        &lt;cell role="head"&gt;GLM-4.7&lt;/cell&gt;
        &lt;cell role="head"&gt;GLM-4.6&lt;/cell&gt;
        &lt;cell role="head"&gt;Kimi K2 Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek-V3.2&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini 3.0 Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Claude Sonnet 4.5&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5 High&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-5.1 High&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMLU-Pro&lt;/cell&gt;
        &lt;cell&gt;84.3&lt;/cell&gt;
        &lt;cell&gt;83.2&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;90.1&lt;/cell&gt;
        &lt;cell&gt;88.2&lt;/cell&gt;
        &lt;cell&gt;87.5&lt;/cell&gt;
        &lt;cell&gt;87.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;GPQA-Diamond&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;82.4&lt;/cell&gt;
        &lt;cell&gt;91.9&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;88.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;HLE&lt;/cell&gt;
        &lt;cell&gt;24.8&lt;/cell&gt;
        &lt;cell&gt;17.2&lt;/cell&gt;
        &lt;cell&gt;23.9&lt;/cell&gt;
        &lt;cell&gt;25.1&lt;/cell&gt;
        &lt;cell&gt;37.5&lt;/cell&gt;
        &lt;cell&gt;13.7&lt;/cell&gt;
        &lt;cell&gt;26.3&lt;/cell&gt;
        &lt;cell&gt;25.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;HLE (w/ Tools)&lt;/cell&gt;
        &lt;cell&gt;42.8&lt;/cell&gt;
        &lt;cell&gt;30.4&lt;/cell&gt;
        &lt;cell&gt;44.9&lt;/cell&gt;
        &lt;cell&gt;40.8&lt;/cell&gt;
        &lt;cell&gt;45.8&lt;/cell&gt;
        &lt;cell&gt;32.0&lt;/cell&gt;
        &lt;cell&gt;35.2&lt;/cell&gt;
        &lt;cell&gt;42.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AIME 2025&lt;/cell&gt;
        &lt;cell&gt;95.7&lt;/cell&gt;
        &lt;cell&gt;93.9&lt;/cell&gt;
        &lt;cell&gt;94.5&lt;/cell&gt;
        &lt;cell&gt;93.1&lt;/cell&gt;
        &lt;cell&gt;95.0&lt;/cell&gt;
        &lt;cell&gt;87.0&lt;/cell&gt;
        &lt;cell&gt;94.6&lt;/cell&gt;
        &lt;cell&gt;94.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;HMMT Feb. 2025&lt;/cell&gt;
        &lt;cell&gt;97.1&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;89.4&lt;/cell&gt;
        &lt;cell&gt;92.5&lt;/cell&gt;
        &lt;cell&gt;97.5&lt;/cell&gt;
        &lt;cell&gt;79.2&lt;/cell&gt;
        &lt;cell&gt;88.3&lt;/cell&gt;
        &lt;cell&gt;96.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;HMMT Nov. 2025&lt;/cell&gt;
        &lt;cell&gt;93.5&lt;/cell&gt;
        &lt;cell&gt;87.7&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;90.2&lt;/cell&gt;
        &lt;cell&gt;93.3&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;IMOAnswerBench&lt;/cell&gt;
        &lt;cell&gt;82.0&lt;/cell&gt;
        &lt;cell&gt;73.5&lt;/cell&gt;
        &lt;cell&gt;78.6&lt;/cell&gt;
        &lt;cell&gt;78.3&lt;/cell&gt;
        &lt;cell&gt;83.3&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;LiveCodeBench-v6&lt;/cell&gt;
        &lt;cell&gt;84.9&lt;/cell&gt;
        &lt;cell&gt;82.8&lt;/cell&gt;
        &lt;cell&gt;83.1&lt;/cell&gt;
        &lt;cell&gt;83.3&lt;/cell&gt;
        &lt;cell&gt;90.7&lt;/cell&gt;
        &lt;cell&gt;64.0&lt;/cell&gt;
        &lt;cell&gt;87.0&lt;/cell&gt;
        &lt;cell&gt;87.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Code Agent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SWE-bench Verified&lt;/cell&gt;
        &lt;cell&gt;73.8&lt;/cell&gt;
        &lt;cell&gt;68.0&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
        &lt;cell&gt;76.2&lt;/cell&gt;
        &lt;cell&gt;77.2&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;76.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SWE-bench Multilingual&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;53.8&lt;/cell&gt;
        &lt;cell&gt;61.1&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;68.0&lt;/cell&gt;
        &lt;cell&gt;55.3&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Terminal Bench Hard&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;23.6&lt;/cell&gt;
        &lt;cell&gt;30.6&lt;/cell&gt;
        &lt;cell&gt;35.4&lt;/cell&gt;
        &lt;cell&gt;39.0&lt;/cell&gt;
        &lt;cell&gt;33.3&lt;/cell&gt;
        &lt;cell&gt;30.5&lt;/cell&gt;
        &lt;cell&gt;43.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Terminal Bench 2.0&lt;/cell&gt;
        &lt;cell&gt;41.0&lt;/cell&gt;
        &lt;cell&gt;24.5&lt;/cell&gt;
        &lt;cell&gt;35.7&lt;/cell&gt;
        &lt;cell&gt;46.4&lt;/cell&gt;
        &lt;cell&gt;54.2&lt;/cell&gt;
        &lt;cell&gt;42.8&lt;/cell&gt;
        &lt;cell&gt;35.2&lt;/cell&gt;
        &lt;cell&gt;47.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;General Agent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BrowseComp&lt;/cell&gt;
        &lt;cell&gt;52.0&lt;/cell&gt;
        &lt;cell&gt;45.1&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;51.4&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;24.1&lt;/cell&gt;
        &lt;cell&gt;54.9&lt;/cell&gt;
        &lt;cell&gt;50.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BrowseComp (w/ Context Manage)&lt;/cell&gt;
        &lt;cell&gt;67.5&lt;/cell&gt;
        &lt;cell&gt;57.5&lt;/cell&gt;
        &lt;cell&gt;60.2&lt;/cell&gt;
        &lt;cell&gt;67.6&lt;/cell&gt;
        &lt;cell&gt;59.2&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BrowseComp-ZH&lt;/cell&gt;
        &lt;cell&gt;66.6&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
        &lt;cell&gt;62.3&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;42.4&lt;/cell&gt;
        &lt;cell&gt;63.0&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;τ²-Bench&lt;/cell&gt;
        &lt;cell&gt;87.4&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;85.3&lt;/cell&gt;
        &lt;cell&gt;90.7&lt;/cell&gt;
        &lt;cell&gt;87.2&lt;/cell&gt;
        &lt;cell&gt;82.4&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Coding: AGI is a long journey, and benchmarks are only one way to evaluate performance. While the metrics provide necessary checkpoints, the most important thing is still how it *feels*. True intelligence isn't just about acing a test or processing data faster; ultimately, the success of AGI will be measured by how seamlessly it integrates into our lives-"coding" this time.&lt;/p&gt;
    &lt;p&gt;GLM-4.7 enhances Interleaved Thinking, a feature introduced since GLM-4.5, and further introduces Preserved Thinking and Turn-level Thinking. By thinking between actions and staying consistent across turns, it makes complex tasks more stable and more controllable:&lt;/p&gt;
    &lt;p&gt;More details: https://docs.z.ai/guides/capabilities/thinking-mode&lt;/p&gt;
    &lt;p&gt;The Z.ai API platform offers the GLM-4.7 model. For comprehensive API documentation and integration guidelines, please refer to https://docs.z.ai/guides/llm/glm-4.7. At the same time, the model is also available worldwide through OpenRouter (https://openrouter.ai/).&lt;/p&gt;
    &lt;p&gt;GLM-4.7 is now available to use within coding agents (Claude Code, Kilo Code, Roo Code, Cline and more).&lt;/p&gt;
    &lt;p&gt;For GLM Coding Plan subscribers: You'll be automatically upgraded to GLM-4.7. If you've previously customized the app configs (like &lt;code&gt;~/.claude/settings.json&lt;/code&gt; in Claude Code), simply update the model name to "glm-4.7" to complete the upgrade.&lt;/p&gt;
    &lt;p&gt;For New users: Subscribing GLM Coding Plan means having access to a Claude-level coding model at a fraction of the cost — just 1/7th the price with 3x the usage quota. Start building today: https://z.ai/subscribe.&lt;/p&gt;
    &lt;p&gt;GLM-4.7 is accessible through Z.ai. Try to change the model option to GLM-4.7, if the system does not automatically do that (not like an AGI in that case :))&lt;/p&gt;
    &lt;p&gt;Model weights for GLM-4.7 are publicly available on HuggingFace and ModelScope. For local deployment, GLM-4.7 supports inference frameworks including vLLM and SGLang. Comprehensive deployment instructions are available in the official GitHub repository.&lt;/p&gt;
    &lt;p&gt;1: Default settings (most tasks): temperature 1.0, top-p 0.95, max new tokens 131072. For multi-turn agentic tasks (τ²-Bench and Terminal Bench 2), enable Preserved Thinking mode.&lt;/p&gt;
    &lt;p&gt;2: Terminal Bench and SWE-bench Verified settings: temperature 0.7, top-p 1.0, max new tokens 16384.&lt;/p&gt;
    &lt;p&gt;3: τ²-Bench settings: temperature 0, max new tokens 16384. For τ²-Bench, we added an extra prompt in the Retail and Telecom interactions to avoid failures caused by users ending the interaction incorrectly; for the Airline domain, we applied the domain fixes proposed in the Claude Opus 4.5 release report.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357287</guid><pubDate>Mon, 22 Dec 2025 18:46:32 +0000</pubDate></item><item><title>Universal Reasoning Model (53.8% pass 1 ARC1 and 16.0% ARC 2)</title><link>https://arxiv.org/abs/2512.14693</link><description>&lt;doc fingerprint="ac9d340ac5f784cf"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 16 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Universal Reasoning Model&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at this https URL.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357458</guid><pubDate>Mon, 22 Dec 2025 18:59:22 +0000</pubDate></item><item><title>The Illustrated Transformer</title><link>https://jalammar.github.io/illustrated-transformer/</link><description>&lt;doc fingerprint="37105b94113a1a90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Illustrated Transformer&lt;/head&gt;
    &lt;p&gt;Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) &lt;lb/&gt; Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese &lt;lb/&gt; Watch: MIT’s Deep Learning State of the Art lecture referencing this post &lt;lb/&gt; Featured in courses at Stanford, Harvard, MIT, Princeton, CMU and others&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Update: This post has now become a book! Check out LLM-book.com which contains (Chapter 3) an updated and expanded version of this post speaking about the latest Transformer models and how they've evolved in the seven years since the original Transformer (like Multi-Query Attention and RoPE Positional embeddings).&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.&lt;/p&gt;
    &lt;p&gt;The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.&lt;/p&gt;
    &lt;p&gt;2025 Update: We’ve built a free short course that brings the contents of this post up-to-date with animations:&lt;/p&gt;
    &lt;head rend="h2"&gt;A High-Level Look&lt;/head&gt;
    &lt;p&gt;Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.&lt;/p&gt;
    &lt;p&gt;Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.&lt;/p&gt;
    &lt;p&gt;The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.&lt;/p&gt;
    &lt;p&gt;The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:&lt;/p&gt;
    &lt;p&gt;The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.&lt;/p&gt;
    &lt;p&gt;The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.&lt;/p&gt;
    &lt;p&gt;The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models).&lt;/p&gt;
    &lt;head rend="h2"&gt;Bringing The Tensors Into The Picture&lt;/head&gt;
    &lt;p&gt;Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.&lt;/p&gt;
    &lt;p&gt;As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.&lt;/p&gt;
    &lt;p&gt;Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.&lt;/p&gt;
    &lt;p&gt;The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.&lt;/p&gt;
    &lt;p&gt;After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.&lt;/p&gt;
    &lt;p&gt;Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.&lt;/p&gt;
    &lt;p&gt;Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.&lt;/p&gt;
    &lt;head rend="h2"&gt;Now We’re Encoding!&lt;/head&gt;
    &lt;p&gt;As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.&lt;/p&gt;
    &lt;p&gt;The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-Attention at a High Level&lt;/head&gt;
    &lt;p&gt;Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.&lt;/p&gt;
    &lt;p&gt;Say the following sentence is an input sentence we want to translate:&lt;/p&gt;
    &lt;p&gt;”&lt;code&gt;The animal didn't cross the street because it was too tired&lt;/code&gt;”&lt;/p&gt;
    &lt;p&gt;What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.&lt;/p&gt;
    &lt;p&gt;When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.&lt;/p&gt;
    &lt;p&gt;As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.&lt;/p&gt;
    &lt;p&gt;If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.&lt;/p&gt;
    &lt;p&gt;As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The Animal", and baked a part of its representation into the encoding of "it".&lt;/p&gt;
    &lt;p&gt;Be sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-Attention in Detail&lt;/head&gt;
    &lt;p&gt;Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.&lt;/p&gt;
    &lt;p&gt;The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.&lt;/p&gt;
    &lt;p&gt;Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.&lt;/p&gt;
    &lt;p&gt;Multiplying x1 by the WQ weight matrix produces q1, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;What are the “query”, “key”, and “value” vectors? &lt;lb/&gt; They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.&lt;/p&gt;
    &lt;p&gt;The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.&lt;/p&gt;
    &lt;p&gt;The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.&lt;/p&gt;
    &lt;p&gt;The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.&lt;/p&gt;
    &lt;p&gt;This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.&lt;/p&gt;
    &lt;p&gt;The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).&lt;/p&gt;
    &lt;p&gt;The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).&lt;/p&gt;
    &lt;p&gt;That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.&lt;/p&gt;
    &lt;head rend="h2"&gt;Matrix Calculation of Self-Attention&lt;/head&gt;
    &lt;p&gt;The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).&lt;/p&gt;
    &lt;p&gt;Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)&lt;/p&gt;
    &lt;p&gt;Finally, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.&lt;/p&gt;
    &lt;p&gt;The self-attention calculation in matrix form&lt;/p&gt;
    &lt;head rend="h2"&gt;The Beast With Many Heads&lt;/head&gt;
    &lt;p&gt;The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices&lt;/p&gt;
    &lt;p&gt;This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.&lt;/p&gt;
    &lt;p&gt;How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.&lt;/p&gt;
    &lt;p&gt;That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place&lt;/p&gt;
    &lt;p&gt;Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:&lt;/p&gt;
    &lt;p&gt;As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".&lt;/p&gt;
    &lt;p&gt;If we add all the attention heads to the picture, however, things can be harder to interpret:&lt;/p&gt;
    &lt;head rend="h2"&gt;Representing The Order of The Sequence Using Positional Encoding&lt;/head&gt;
    &lt;p&gt;One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.&lt;/p&gt;
    &lt;p&gt;To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.&lt;/p&gt;
    &lt;p&gt;To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.&lt;/p&gt;
    &lt;p&gt;If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:&lt;/p&gt;
    &lt;p&gt;A real example of positional encoding with a toy embedding size of 4&lt;/p&gt;
    &lt;p&gt;What might this pattern look like?&lt;/p&gt;
    &lt;p&gt;In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.&lt;/p&gt;
    &lt;p&gt;A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.&lt;/p&gt;
    &lt;p&gt;The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in &lt;code&gt;get_timing_signal_1d()&lt;/code&gt;. This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).&lt;/p&gt;
    &lt;p&gt;July 2020 Update: The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. Here’s the code to generate it:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Residuals&lt;/head&gt;
    &lt;p&gt;One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.&lt;/p&gt;
    &lt;p&gt;If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:&lt;/p&gt;
    &lt;p&gt;This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Decoder Side&lt;/head&gt;
    &lt;p&gt;Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.&lt;/p&gt;
    &lt;p&gt;The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:&lt;/p&gt;
    &lt;p&gt;After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).&lt;/p&gt;
    &lt;p&gt;The following steps repeat the process until a special &lt;/p&gt;
    &lt;p&gt;The self attention layers in the decoder operate in a slightly different way than the one in the encoder:&lt;/p&gt;
    &lt;p&gt;In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to &lt;code&gt;-inf&lt;/code&gt;) before the softmax step in the self-attention calculation.&lt;/p&gt;
    &lt;p&gt;The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Final Linear and Softmax Layer&lt;/head&gt;
    &lt;p&gt;The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.&lt;/p&gt;
    &lt;p&gt;The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.&lt;/p&gt;
    &lt;p&gt;Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.&lt;/p&gt;
    &lt;p&gt;The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.&lt;/p&gt;
    &lt;p&gt;This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recap Of Training&lt;/head&gt;
    &lt;p&gt;Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.&lt;/p&gt;
    &lt;p&gt;During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.&lt;/p&gt;
    &lt;p&gt;To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “&amp;lt;eos&amp;gt;” (short for ‘end of sentence’)).&lt;/p&gt;
    &lt;p&gt;The output vocabulary of our model is created in the preprocessing phase before we even begin training.&lt;/p&gt;
    &lt;p&gt;Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:&lt;/p&gt;
    &lt;p&gt;Example: one-hot encoding of our output vocabulary&lt;/p&gt;
    &lt;p&gt;Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Loss Function&lt;/head&gt;
    &lt;p&gt;Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.&lt;/p&gt;
    &lt;p&gt;What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.&lt;/p&gt;
    &lt;p&gt;Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.&lt;/p&gt;
    &lt;p&gt;How do you compare two probability distributions? We simply subtract one from the other. For more details, look at cross-entropy and Kullback–Leibler divergence.&lt;/p&gt;
    &lt;p&gt;But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)&lt;/item&gt;
      &lt;item&gt;The first probability distribution has the highest probability at the cell associated with the word “i”&lt;/item&gt;
      &lt;item&gt;The second probability distribution has the highest probability at the cell associated with the word “am”&lt;/item&gt;
      &lt;item&gt;And so on, until the fifth output distribution indicates ‘&lt;code&gt;&amp;lt;end of sentence&amp;gt;&lt;/code&gt;’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The targeted probability distributions we'll train our model against in the training example for one sample sentence.&lt;/p&gt;
    &lt;p&gt;After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:&lt;/p&gt;
    &lt;p&gt;Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.&lt;/p&gt;
    &lt;p&gt;Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.&lt;/p&gt;
    &lt;head rend="h2"&gt;Go Forth And Transform&lt;/head&gt;
    &lt;p&gt;I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement.&lt;/item&gt;
      &lt;item&gt;Watch Łukasz Kaiser’s talk walking through the model and its details&lt;/item&gt;
      &lt;item&gt;Play with the Jupyter Notebook provided as part of the Tensor2Tensor repo&lt;/item&gt;
      &lt;item&gt;Explore the Tensor2Tensor repo.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Follow-up works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Depthwise Separable Convolutions for Neural Machine Translation&lt;/item&gt;
      &lt;item&gt;One Model To Learn Them All&lt;/item&gt;
      &lt;item&gt;Discrete Autoencoders for Sequence Models&lt;/item&gt;
      &lt;item&gt;Generating Wikipedia by Summarizing Long Sequences&lt;/item&gt;
      &lt;item&gt;Image Transformer&lt;/item&gt;
      &lt;item&gt;Training Tips for the Transformer Model&lt;/item&gt;
      &lt;item&gt;Self-Attention with Relative Position Representations&lt;/item&gt;
      &lt;item&gt;Fast Decoding in Sequence Models using Discrete Latent Variables&lt;/item&gt;
      &lt;item&gt;Adafactor: Adaptive Learning Rates with Sublinear Memory Cost&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post.&lt;/p&gt;
    &lt;p&gt;Please hit me up on Twitter for any corrections or feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357675</guid><pubDate>Mon, 22 Dec 2025 19:15:56 +0000</pubDate></item><item><title>The Garbage Collection Handbook</title><link>https://gchandbook.org/index.html</link><description>&lt;doc fingerprint="73791c0d628aee51"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Second edition&lt;/head&gt;
    &lt;p&gt;Richard Jonesâs Garbage Collection (Wiley, 1996) was a milestone book in the area of automatic memory management. Its widely acclaimed successor, The Garbage Collection Handbook: The Art of Automatic Memory Management captured the state of the field in 2012. However, technology developments have made memory management more challenging, interesting and important than ever. This second edition updates the handbook, bringing together a wealth of knowledge gathered by automatic memory management researchers and developers over the past sixty years. The authors compare the most important approaches and state-of-the-art techniques in a single, accessible framework.&lt;/p&gt;
    &lt;p&gt;The book addresses new challenges to garbage collection made by recent advances in hardware and software, and the environments in which programs are executed. It explores the consequences of these changes for designers and implementers of high performance garbage collectors. Along with simple and traditional algorithms, the book covers state-of-the-art parallel, incremental, concurrent and real-time garbage collection. Algorithms and concepts are often described with pseudocode and illustrations.&lt;/p&gt;
    &lt;p&gt;The nearly universal adoption of garbage collection by modern programming languages makes a thorough understanding of this topic essential for any programmer. This authoritative handbook gives expert insight on how different collectors work as well as the various issues currently facing garbage collectors. Armed with this knowledge, programmers can confidently select and configure the many choices of garbage collectors.&lt;/p&gt;
    &lt;head rend="h2"&gt;Features of the book&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Provides a complete, up-to-date, and authoritative sequel to the 1996 and 2012 books&lt;/item&gt;
      &lt;item&gt;Offers thorough coverage of parallel, concurrent and real-time garbage collection algorithms&lt;/item&gt;
      &lt;item&gt;Discusses in detail modern, high-performance commercial collectors&lt;/item&gt;
      &lt;item&gt;Explains some of the tricky aspects of garbage collection, including the interface to the run-time system&lt;/item&gt;
      &lt;item&gt;Over 90 more pages, including new chapters on persistence and energy-aware garbage collection&lt;/item&gt;
      &lt;item&gt;Backed by a comprehensive online database of nearly 3,400 garbage collection-related publications&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;e-book and translations&lt;/head&gt;
    &lt;p&gt;The e-book enhances the print versions with a rich collection of over 37,000 hyperlinks to chapters, sections, algorithms, figures, glossary entries, index items, original research papers and much more.&lt;/p&gt;
    &lt;p&gt;Chinese and Japanese translations of the first edition were published in 2016. We thank the translators for their work in bringing our book to a wider audience.&lt;/p&gt;
    &lt;head rend="h2"&gt;Web Resources&lt;/head&gt;
    &lt;p&gt;The online bibliographic database includes nearly 3,400 garbage collection-related publications. It contains abstracts for some entries and URLs or DOIs for most of the electronically available ones, and is continually being updated. The database can be searched online, or downloaded as BibTeX, PostScript or PDF.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357870</guid><pubDate>Mon, 22 Dec 2025 19:30:18 +0000</pubDate></item><item><title>Ultrasound Cancer Treatment: Sound Waves Fight Tumors</title><link>https://spectrum.ieee.org/ultrasound-cancer-treatment</link><description>&lt;doc fingerprint="f746e0824e2192b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ultrasound Treatment Takes on Cancer’s Toughest Tumors&lt;/head&gt;
    &lt;p&gt;HistoSonics turns its tumor-liquifying tech against pancreatic cancer&lt;/p&gt;
    &lt;p&gt;For many years, doctors and technicians who performed medical ultrasound procedures viewed bubbles with wary concern. The phenomenon of cavitation—the formation and collapse of tiny gas bubbles due to changes in pressure—was considered an undesirable and largely uncontrollable side effect. But in 2001, researchers at the University of Michigan began exploring ways to harness the phenomenon for the destruction of cancerous tumors and other problematic tissue.&lt;/p&gt;
    &lt;p&gt;The trouble was, creating and controlling cavitation generated heat, which harmed healthy tissue beyond the target area. Zhen Xu, who was working on a Ph.D. in biomedical engineering at the time, was bombarding pig heart tissue in a tank of water with ultrasound when she made a breakthrough.&lt;/p&gt;
    &lt;p&gt;The key was using extremely powerful ultrasound to produce negative pressure of more than 20 megapascals, delivered in short bursts measured in microseconds—but separated by relatively long gaps, between a millisecond and a full second long. These parameters created bubbles that quickly formed and collapsed, tearing apart nearby cells and turning the tissue into a kind of slurry, while avoiding heat buildup. The result was a form of incisionless surgery, a way to wipe out tumors without scalpels, radiation, or heat.&lt;/p&gt;
    &lt;p&gt;“The experiments worked,” says Xu, now a professor at Michigan, “but I also destroyed the ultrasound equipment that I used,” which was the most powerful available at the time. In 2009, she cofounded a company, HistoSonics, to commercialize more powerful ultrasound machines, test treatment of a variety of diseases, and make the procedure, called histotripsy, widely available.&lt;/p&gt;
    &lt;p&gt;So far, the killer app is fighting cancer. In 2023, HistoSonics’ Edison system received FDA approval for treatment of liver tumors. In 2026, clinicians will conclude a pivotal kidney cancer study and apply for regulatory approval. They’ll also launch a large-scale pivotal trial for pancreatic cancer, considered one of the deadliest forms of the disease with a five-year survival rate of just 13 percent. An effective treatment for pancreatic cancer would represent a major advance against one of the most lethal malignancies.&lt;/p&gt;
    &lt;head rend="h2"&gt;Histotripsy’s Benefits for Cancer Treatment&lt;/head&gt;
    &lt;p&gt;HistoSonics is not the only developer of histotripsy devices or techniques, but it is first to market with a purpose-built device. “What HistoSonics has developed is a symphony of technologies, which combines physics, biology, and biomedical engineering,” says Bradford Wood, an interventional radiologist at the National Institutes of Health, who is not affiliated with the company. Its engineering effort has spanned multiple disciplines to produce robotic, computer-guided systems that turn physical forces into therapeutic effects.&lt;/p&gt;
    &lt;p&gt;Over the past decade, research has confirmed or found other benefits of histotripsy. With precise calibration, fibrous tissue—such as blood vessels—can be spared from damage even in the target zone. And while other noninvasive techniques may leave scar tissue, the liquefied debris created by histotripsy is cleared away by the body’s natural processes.&lt;/p&gt;
    &lt;p&gt;In HistoSonics’ early trials for pancreatic cancer, doctors used focused ultrasound pulses to ablate, or destroy, tumors deep within the pancreas. “It’s a great achievement for the entire field to show that it is possible to ablate pancreatic tumors and that it’s well tolerated,” says Tatiana Khokhlova, a medical ultrasound researcher at the University of Washington, in Seattle, who has worked on alternative histotripsy techniques.&lt;/p&gt;
    &lt;p&gt;Khokhlova says the key to harnessing histotripsy’s benefits “will be combining ablation of the primary tumor in the pancreas with some other therapy.” Combination treatment could fight recurrent cancer and tiny tumors that ultrasound might miss, while also tapping into a surprising benefit.&lt;/p&gt;
    &lt;p&gt;Histotripsy generally seems to stimulate an immune response, helping the body attack cancer cells that weren’t targeted directly by ultrasound. The mechanical destruction of tumors likely leaves behind recognizable traces of cancer proteins that help the immune system learn to identify and destroy similar cells elsewhere in the body, explains Wood. Researchers are now exploring ways to pair histotripsy with immunotherapy to amplify that effect.&lt;/p&gt;
    &lt;p&gt;The company’s capacity to explore the treatment‘s potential for different conditions will only improve with time, says HistoSonics CEO Mike Blue. The company has fresh resources to accelerate R&amp;amp;D: A new ownership group, which includes billionaire Jeff Bezos, acquired HistoSonics in August 2025 at a valuation of US $2.25 billion.&lt;/p&gt;
    &lt;p&gt;Engineers are already testing a new guidance system that uses a form of X-rays rather than ultrasound imaging, which should expand use cases. The R&amp;amp;D team is also developing a feedback system that analyzes echoes from the therapeutic ultrasound to detect tissue destruction and integrates that information into the live display, says Blue.&lt;/p&gt;
    &lt;p&gt;If those advances pan out, histotripsy could move well beyond the liver, kidney, and pancreas in the fight against cancer. What started as a curiosity about bubbles might soon become a new pillar of noninvasive medicine—a future in which surgeons wield not scalpels, but sound waves.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46357945</guid><pubDate>Mon, 22 Dec 2025 19:37:34 +0000</pubDate></item><item><title>It's Always TCP_NODELAY</title><link>https://brooker.co.za/blog/2024/05/09/nagle.html</link><description>&lt;doc fingerprint="f78cd1300c0d0ff1"&gt;
  &lt;main&gt;&lt;p&gt;The first thing I check when debugging latency issues in distributed systems is whether TCP_NODELAY is enabled. And itâs not just me. Every distributed system builder I know has lost hours to latency issues quickly fixed by enabling this simple socket option, suggesting that the default behavior is wrong, and perhaps that the whole concept is outmoded.&lt;/p&gt;&lt;p&gt;First, letâs be clear about what weâre talking about. Thereâs no better source than John Nagleâs RFC896 from 19841. First, the problem statement:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;There is a special problem associated with small packets. When TCP is used for the transmission of single-character messages originating at a keyboard, the typical result is that 41 byte packets (one byte of data, 40 bytes of header) are transmitted for each byte of useful data. This 4000% overhead is annoying but tolerable on lightly loaded networks.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;In short, Nagle was interested in better amortizing the cost of TCP headers, to get better throughput out of the network. Up to 40x better throughput! These tiny packets had two main causes: human-interactive applications like shells, where folks were typing a byte at a time, and poorly implemented programs that dribbled messages out to the kernel through many &lt;code&gt;write&lt;/code&gt; calls. Nagleâs proposal for fixing this was simple and smart:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;A simple and elegant solution has been discovered.&lt;/p&gt;&lt;/quote&gt;&lt;quote&gt;&lt;p&gt;The solution is to inhibit the sending of new TCP segments when new outgoing data arrives from the user if any previously transmitted data on the connection remains unacknowledged.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;When many people talk about Nagleâs algorithm, they talk about timers, but RFC896 doesnât use any kind of timer other than the round-trip time on the network.&lt;/p&gt;&lt;p&gt;Nagleâs Algorithm and Delayed Acks&lt;/p&gt;&lt;p&gt;Nagleâs nice, clean, proposal interacted poorly with another TCP feature: delayed &lt;code&gt;ACK&lt;/code&gt;. The idea behind delayed &lt;code&gt;ACK&lt;/code&gt; is to delay sending the acknowledgement of a packet at least until thereâs some data to send back (e.g. a &lt;code&gt;telnet&lt;/code&gt; session echoing back the userâs typing), or until a timer expires. RFC813 from 1982 is that first that seems to propose delaying &lt;code&gt;ACKs&lt;/code&gt;:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;The receiver of data will refrain from sending an acknowledgement under certain circumstances, in which case it must set a timer which will cause the acknowledgement to be sent later. However, the receiver should do this only where it is a reasonable guess that some other event will intervene and prevent the necessity of the timer interrupt.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;which is then formalized further in RFC1122 from 1989. The interaction between these two features causes a problem: Nagleâs algorithm is blocking sending more data until an &lt;code&gt;ACK&lt;/code&gt; is received, but delayed ack is delaying that &lt;code&gt;ack&lt;/code&gt; until a response is ready. Great for keeping packets full, not so great for latency-sensitive pipelined applications.&lt;/p&gt;&lt;p&gt;This is a point Nagle has made himself several times. For example in this Hacker News comment:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;That still irks me. The real problem is not tinygram prevention. Itâs ACK delays, and that stupid fixed timer. They both went into TCP around the same time, but independently. I did tinygram prevention (the Nagle algorithm) and Berkeley did delayed ACKs, both in the early 1980s. The combination of the two is awful.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;As systems builders this is should be a familiar situation: two reasonable features of the system that interact to create an undesirable behavior. This kind of interaction is one of the things that makes protocol design so hard.&lt;/p&gt;&lt;p&gt;Is Nagle blameless?&lt;/p&gt;&lt;p&gt;Unfortunately, itâs not just delayed ACK2. Even without delayed ack and that stupid fixed timer, the behavior of Nagleâs algorithm probably isnât what we want in distributed systems. A single in-datacenter RTT is typically around 500Î¼s, then a couple of milliseconds between datacenters in the same region, and up to hundreds of milliseconds going around the globe. Given the vast amount of work a modern server can do in even a few hundred microseconds, delaying sending data for even one RTT isnât clearly a win.&lt;/p&gt;&lt;p&gt;To make a clearer case, letâs turn back to the justification behind Nagleâs algorithm: amortizing the cost of headers and avoiding that 40x overhead on single-byte packets. But does anybody send single byte packets anymore? Most distributed databases and systems donât. Partially thatâs because they simply have more to say, partially its because of additional overhead of protocols like TLS, and partially its because of encoding and serialization overhead. But mostly, they have more to say.&lt;/p&gt;&lt;p&gt;The core concern of not sending tiny messages is still a very real one, but weâve very effectively pushed that into the application layer. Sending a byte at a time wrapped in JSON isnât going to be very efficient, no matter what Nagleâs algorithm does.&lt;/p&gt;&lt;p&gt;Is Nagle needed?&lt;/p&gt;&lt;p&gt;First, the uncontroversial take: if youâre building a latency-sensitive distributed system running on modern datacenter-class hardware, enable &lt;code&gt;TCP_NODELAY&lt;/code&gt; (disable Nagleâs algorithm) without worries. You donât need to feel bad. Itâs not a sin. Itâs OK. Just go ahead.&lt;/p&gt;&lt;p&gt;More controversially, I suspect that Nagleâs algorithm just isnât needed on modern systems, given the traffic and application mix, and the capabilities of the hardware we have today. In other words, &lt;code&gt;TCP_NODELAY&lt;/code&gt; should be the default. Thatâs going to make some â&lt;code&gt;write&lt;/code&gt; every byteâ code slower than it would otherwise be, but those applications should be fixed anyway if we care about efficiency.&lt;/p&gt;&lt;p&gt;Footnotes&lt;/p&gt;&lt;code&gt;TCP_QUICKACK&lt;/code&gt;. I donât tend to reach for it for a few reasons, including lack of portability, and weird semantics (seriously, read the man page). The bigger problem is that &lt;code&gt;TCP_QUICKACK&lt;/code&gt; doesnât fix the fundamental problem of the kernel hanging on to data longer than my program wants it to. When I say &lt;code&gt;write()&lt;/code&gt;, I mean &lt;code&gt;write()&lt;/code&gt;.&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46359120</guid><pubDate>Mon, 22 Dec 2025 21:09:59 +0000</pubDate></item><item><title>Snitch – A friendlier ss/netstat</title><link>https://github.com/karol-broda/snitch</link><description>&lt;doc fingerprint="f4e6b86a0862bc9d"&gt;
  &lt;main&gt;
    &lt;p&gt;a friendlier &lt;code&gt;ss&lt;/code&gt; / &lt;code&gt;netstat&lt;/code&gt; for humans. inspect network connections with a clean tui or styled tables.&lt;/p&gt;
    &lt;code&gt;go install github.com/karol-broda/snitch@latest&lt;/code&gt;
    &lt;code&gt;# try it
nix run github:karol-broda/snitch

# install to profile
nix profile install github:karol-broda/snitch

# or add to flake inputs
{
  inputs.snitch.url = "github:karol-broda/snitch";
}
# then use: inputs.snitch.packages.${system}.default&lt;/code&gt;
    &lt;code&gt;# with yay
yay -S snitch-bin

# with paru
paru -S snitch-bin&lt;/code&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/karol-broda/snitch/master/install.sh | sh&lt;/code&gt;
    &lt;p&gt;installs to &lt;code&gt;~/.local/bin&lt;/code&gt; if available, otherwise &lt;code&gt;/usr/local/bin&lt;/code&gt;. override with:&lt;/p&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/karol-broda/snitch/master/install.sh | INSTALL_DIR=~/bin sh&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;macos: the install script automatically removes the quarantine attribute (&lt;/p&gt;&lt;code&gt;com.apple.quarantine&lt;/code&gt;) from the binary to allow it to run without gatekeeper warnings. to disable this, set&lt;code&gt;KEEP_QUARANTINE=1&lt;/code&gt;.&lt;/quote&gt;
    &lt;p&gt;download from releases:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;linux: &lt;code&gt;snitch_&amp;lt;version&amp;gt;_linux_&amp;lt;arch&amp;gt;.tar.gz&lt;/code&gt;or&lt;code&gt;.deb&lt;/code&gt;/&lt;code&gt;.rpm&lt;/code&gt;/&lt;code&gt;.apk&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;macos: &lt;code&gt;snitch_&amp;lt;version&amp;gt;_darwin_&amp;lt;arch&amp;gt;.tar.gz&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;tar xzf snitch_*.tar.gz
sudo mv snitch /usr/local/bin/&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;macos: if blocked with "cannot be opened because the developer cannot be verified", run:&lt;/p&gt;xattr -d com.apple.quarantine /usr/local/bin/snitch&lt;/quote&gt;
    &lt;code&gt;snitch              # launch interactive tui
snitch -l           # tui showing only listening sockets
snitch ls           # print styled table and exit
snitch ls -l        # listening sockets only
snitch ls -t -e     # tcp established connections
snitch ls -p        # plain output (parsable)&lt;/code&gt;
    &lt;p&gt;interactive tui with live-updating connection list.&lt;/p&gt;
    &lt;code&gt;snitch                  # all connections
snitch -l               # listening only
snitch -t               # tcp only
snitch -e               # established only
snitch -i 2s            # 2 second refresh interval&lt;/code&gt;
    &lt;p&gt;keybindings:&lt;/p&gt;
    &lt;code&gt;j/k, ↑/↓      navigate
g/G           top/bottom
t/u           toggle tcp/udp
l/e/o         toggle listen/established/other
s/S           cycle sort / reverse
w             watch/monitor process (highlight)
W             clear all watched
K             kill process (with confirmation)
/             search
enter         connection details
?             help
q             quit
&lt;/code&gt;
    &lt;p&gt;one-shot table output. uses a pager automatically if output exceeds terminal height.&lt;/p&gt;
    &lt;code&gt;snitch ls               # styled table (default)
snitch ls -l            # listening only
snitch ls -t -l         # tcp listeners
snitch ls -e            # established only
snitch ls -p            # plain/parsable output
snitch ls -o json       # json output
snitch ls -o csv        # csv output
snitch ls -n            # numeric (no dns resolution)
snitch ls --no-headers  # omit headers&lt;/code&gt;
    &lt;p&gt;json output for scripting.&lt;/p&gt;
    &lt;code&gt;snitch json
snitch json -l&lt;/code&gt;
    &lt;p&gt;stream json frames at an interval.&lt;/p&gt;
    &lt;code&gt;snitch watch -i 1s | jq '.count'
snitch watch -l -i 500ms&lt;/code&gt;
    &lt;p&gt;check for updates and upgrade in-place.&lt;/p&gt;
    &lt;code&gt;snitch upgrade              # check for updates
snitch upgrade --yes        # upgrade automatically
snitch upgrade -v 0.1.7     # install specific version&lt;/code&gt;
    &lt;p&gt;shortcut flags work on all commands:&lt;/p&gt;
    &lt;code&gt;-t, --tcp           tcp only
-u, --udp           udp only
-l, --listen        listening sockets
-e, --established   established connections
-4, --ipv4          ipv4 only
-6, --ipv6          ipv6 only
-n, --numeric       no dns resolution
&lt;/code&gt;
    &lt;p&gt;for more specific filtering, use &lt;code&gt;key=value&lt;/code&gt; syntax with &lt;code&gt;ls&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;snitch ls proto=tcp state=listen
snitch ls pid=1234
snitch ls proc=nginx
snitch ls lport=443
snitch ls contains=google&lt;/code&gt;
    &lt;p&gt;styled table (default):&lt;/p&gt;
    &lt;code&gt;  ╭─────────────────┬───────┬───────┬─────────────┬─────────────────┬────────╮
  │ PROCESS         │ PID   │ PROTO │ STATE       │ LADDR           │ LPORT  │
  ├─────────────────┼───────┼───────┼─────────────┼─────────────────┼────────┤
  │ nginx           │ 1234  │ tcp   │ LISTEN      │ *               │ 80     │
  │ postgres        │ 5678  │ tcp   │ LISTEN      │ 127.0.0.1       │ 5432   │
  ╰─────────────────┴───────┴───────┴─────────────┴─────────────────┴────────╯
  2 connections
&lt;/code&gt;
    &lt;p&gt;plain output (&lt;code&gt;-p&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;PROCESS    PID    PROTO   STATE    LADDR       LPORT
nginx      1234   tcp     LISTEN   *           80
postgres   5678   tcp     LISTEN   127.0.0.1   5432
&lt;/code&gt;
    &lt;p&gt;optional config file at &lt;code&gt;~/.config/snitch/snitch.toml&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[defaults]
numeric = false
theme = "auto"&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;linux or macos&lt;/item&gt;
      &lt;item&gt;linux: reads from &lt;code&gt;/proc/net/*&lt;/code&gt;, root or&lt;code&gt;CAP_NET_ADMIN&lt;/code&gt;for full process info&lt;/item&gt;
      &lt;item&gt;macos: uses system APIs, may require sudo for full process info&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46361229</guid><pubDate>Tue, 23 Dec 2025 01:03:57 +0000</pubDate></item><item><title>The Duodecimal Bulletin, Vol. 55, No. 1, Year 1209 [pdf]</title><link>https://dozenal.org/drupal/sites_bck/default/files/DuodecimalBulletinIssue551.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46361510</guid><pubDate>Tue, 23 Dec 2025 01:49:56 +0000</pubDate></item><item><title>FCC Updates Covered List to Include Foreign UAS and UAS Critical Components [pdf]</title><link>https://docs.fcc.gov/public/attachments/DOC-416839A1.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46362275</guid><pubDate>Tue, 23 Dec 2025 03:57:02 +0000</pubDate></item><item><title>Show HN: CineCLI – Browse and torrent movies directly from your terminal</title><link>https://github.com/eyeblech/cinecli</link><description>&lt;doc fingerprint="ba3c2bfa61c0f781"&gt;
  &lt;main&gt;
    &lt;p&gt; 🟢 API STATUS: ACTIVE &amp;amp; OPERATIONAL&lt;lb/&gt; All CineCLI features are fully functional. &lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Browse, inspect, and launch movie torrents directly from your terminal.&lt;/p&gt;&lt;lb/&gt;Fast. Cross-platform. Minimal. Beautiful.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🔍 Search movies from YTS&lt;/item&gt;
      &lt;item&gt;🎥 View detailed movie information&lt;/item&gt;
      &lt;item&gt;🧲 Launch magnet links directly into your torrent client&lt;/item&gt;
      &lt;item&gt;📦 Download &lt;code&gt;.torrent&lt;/code&gt;files if preferred&lt;/item&gt;
      &lt;item&gt;⚡ Auto-select best torrent (highest quality + healthy seeds)&lt;/item&gt;
      &lt;item&gt;🖥 Cross-platform (Linux, macOS, Windows)&lt;/item&gt;
      &lt;item&gt;🎨 Rich, clean terminal UI (powered by &lt;code&gt;rich&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;🧠 Smart defaults with full user control&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install cinecli
&lt;/code&gt;
    &lt;p&gt;Requires Python 3.9+&lt;/p&gt;
    &lt;code&gt;cinecli search matrix
&lt;/code&gt;
    &lt;p&gt;Displays matching movies with IDs:&lt;/p&gt;
    &lt;code&gt;ID     Title                 Year   Rating
3525   The Matrix            1999   8.7
3526   The Matrix Reloaded   2003   7.2

&lt;/code&gt;
    &lt;code&gt;cinecli watch 3525
&lt;/code&gt;
    &lt;p&gt;What happens:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Shows movie details&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Lists available torrents&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Auto-selects the best option (you can override)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Launches magnet or downloads&lt;/p&gt;
        &lt;code&gt;.torrent&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;cinecli interactive
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Search → select movie → choose torrent&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manual selection by design (safe &amp;amp; explicit)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CineCLI delegates magnet handling to your OS.&lt;/p&gt;
    &lt;p&gt;That means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Whatever torrent client is registered (&lt;/p&gt;&lt;code&gt;qBittorrent&lt;/code&gt;,&lt;code&gt;Transmission&lt;/code&gt;, etc.)&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;CineCLI will launch it directly&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example (Linux):&lt;/p&gt;
    &lt;code&gt;xdg-mime query default x-scheme-handler/magnet
&lt;/code&gt;
    &lt;p&gt;Full terminal walkthrough:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;demo.mov&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Python&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Typer — CLI framework&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Rich — terminal UI&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Requests — API communication&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;YTS API — movie data source&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT—see LICENSE.&lt;/p&gt;
    &lt;p&gt;Use it. Fork it. Improve it.&lt;/p&gt;
    &lt;p&gt;Built by eyeblech&lt;lb/&gt; 📧 0x1123@proton.me&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;STAR the repo if you like it! ⭐&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46362655</guid><pubDate>Tue, 23 Dec 2025 05:17:50 +0000</pubDate></item><item><title>10 years bootstrapped: €6.5M revenue with a team of 13</title><link>https://www.datocms.com/blog/a-look-back-at-2025</link><description>&lt;doc fingerprint="2f3948de99b426d0"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The DatoCMS Blog&lt;/head&gt;
    &lt;head rend="h1"&gt;A look back at 2025&lt;/head&gt;
    &lt;p&gt;As 2025 comes to a close, it's once again time to reflect. Itâs been another packed twelve months, and itâs great to look back at everything we achieved, day by day. (Yes, we're patting ourselves on the back. It's our blog, we're allowed to.)&lt;/p&gt;
    &lt;p&gt;Want to take a walk down memory lane? Here are previous editions: 2024, 2023, 2022, 2021, 2020.&lt;/p&gt;
    &lt;head rend="h2"&gt;Financials: Strong growth, with best-in-class margins&lt;/head&gt;
    &lt;p&gt;This year, we reached â¬6.5 million in revenue, a solid 10% year-over-year growth. Not that many companies still have double-digit growth after ten years! Most are either dead, laying off half their teams, acqui-hired, or pivoting to AI-something.&lt;/p&gt;
    &lt;p&gt;With our continued focus on sustainable operations and disciplined execution, we achieved an EBIT margin of 65%. To put this in perspective: while most SaaS companies celebrate 20-30% margins, and industry leaders hover around 40%, DatoCMS has reached a level of profitability that places us in the top 5% of SaaS companies globally.&lt;/p&gt;
    &lt;p&gt;For those familiar with SaaS metrics, the "Rule of 40" states that growth rate plus profit margin should exceed 40%. Ours is 75%. We're not bragging (okay, we're bragging a little) but it turns out that not burning through VC cash on ping-pong tables and "growth at all costs" actually works.&lt;/p&gt;
    &lt;head rend="h2"&gt;Partners: More and more of you are joining us&lt;/head&gt;
    &lt;p&gt;With 185 agency partners now fully enrolled in our partner network (!!!), we're genuinely blown away. These are people who build websites for a living, with real deadlines and real clients breathing down their necks. They don't have time for tools that get in the way â and they chose us. We don't take that for granted.&lt;/p&gt;
    &lt;p&gt;This year, we doubled down on making your work more visible. All that real work for real clients? It adds up â we now have 340 projects in the showcase (63 added this year alone!), enough that we had to revamp the page with proper filters so people can actually find things.&lt;/p&gt;
    &lt;p&gt;And what projects they are. Youâve used DatoCMS to power offline wayfinding. Youâve helped shape the early days of the entire GraphQL community. Heck, one of you even took a day to graffiti the streets of Switzerland about us â which is either peak brand loyalty or a cry for help, we're not sure. Either way, never felt so loved.&lt;/p&gt;
    &lt;p&gt;If you're an agency and you're not in the partner program yet â come on. We're not collecting logos here. We want to build a real relationship, learn what's slowing you down, and give you the perfect tool to ship quality work fast and painlessly. Half the features we shipped this year came from partner feedback. You're literally shaping the product. That's the whole point. No awkward sales calls, promise, we hate those too.&lt;/p&gt;
    &lt;head rend="h2"&gt;Product: Another incredible round of improvements&lt;/head&gt;
    &lt;p&gt;2025 has been another year of relentless shipping. We didn't just focus on one area â we improved the entire stack, from the way developers write code to how editors manage content, all while hardening security and preparing for the AI era (gosh, we said it, now we need to wash our mouths).&lt;/p&gt;
    &lt;p&gt;Here is an exhaustive look at everything we shipped this year, grouped by how they help you:&lt;/p&gt;
    &lt;head rend="h6"&gt;Type Safety &amp;amp; Developer Confidence&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Records, finally typed â The biggest DX win of the year. The JavaScript client now supports full end-to-end type safety, generating types directly from your schema for real autocomplete and compile-time safety. No more&lt;/p&gt;&lt;code&gt;any&lt;/code&gt;types haunting your dreams.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reactive Plugins â Plugin settings are now synced in real-time across users, preventing configuration conflicts when multiple people are working on complex setups simultaneously.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;AI &amp;amp; LLM Readiness&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;LLM-Ready Documentation â We made our docs AI-friendly with&lt;/p&gt;&lt;code&gt;llms-full.txt&lt;/code&gt;and a "Copy as Markdown" feature on every page, so you can easily feed context to ChatGPT or Claude. Because let's be honest, that's how half of you read documentation now anyway.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MCP Server â We released a Model Context Protocol (MCP) server that enables AI assistants to interact directly with your DatoCMS projects. It works. Sometimes. We wrote a whole blog post about the "sometimes" part.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;AI Translations â Bulk-translate entire records with OpenAI, Claude, Gemini, or DeepL. Finally, a reason to stop copy-pasting into Google Translate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Structured Text to Markdown â A new package that turns Structured Text fields back into clean, CommonMark-compatible Markdown: perfect for LLM pipelines or migration scripts.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;Content Editing Experience&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Inline Blocks in Structured Text â One of our most requested features! You can now insert blocks directly inside Structured Text fields â perfect for inline links, mentions, or notes â unlocking infinite nesting possibilities.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tabular View for Trees â Hierarchical models got a massive upgrade with a new Tabular View, bringing custom columns, pagination, and sorting to tree structures.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Favorite Locales â Editors can now pin their most-used languages to the top of the UI, hiding the noise of unused locales in massive multi-language projects. Finally, some peace for the people managing 40+ locales.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enhanced Previews â We introduced inline previews for blocks and link fields, letting you see colors, dates, and images directly in the list view without clicking through.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Single Block Presentation â You can now use a Single Block field as a model's presentation title or image, perfect for models where the main info is nested inside a block.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved Link Field Filtering â Link fields now correctly filter records by the current locale, eliminating confusion when referencing localized content.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fixed Headers â We unified the UI with fixed headers across all sections, ensuring that save and publish buttons are always within reach. A small change that sounds boring until you realize how much scrolling it saves.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;API &amp;amp; Tooling Power&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;New CLI cma:call command â You can now call any API method directly from the terminal without writing custom scripts, thanks to dynamic discovery of API resources.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Filter uploads by path â We added a new path filter to the GraphQL API, allowing you to query assets based on their storage path with inclusion, exclusion, and exact matching.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Increased GraphQL Pagination â We bumped the maximum number of items you can fetch in a single GraphQL query from 100 to 500, reducing the number of requests needed for large datasets. Five times more stuff in one go â you're welcome.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Site Search Decoupled â Site Search is now an independent entity, separate from Build Triggers. You can control indexing explicitly and access detailed crawler logs to debug robots.txt and sitemap issues.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enhanced Build Triggers Activity â We enhanced the Activity view to show events beyond the 30-item limit, with better filtering and detailed logs for every operation.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;Security &amp;amp; Governance&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Access to CDA Playground with Limited Permissions â Developers can now use the GraphQL Playground without needing full API token management permissions, safer for contractors and temporary access.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;All API Tokens are Deletable â For better security hygiene, you can now delete any API token, including the default read-only ones generated by the system.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;API Token Last Used Time â You can now see when each API token was last used directly in Project Settings, making it easy to identify stale tokens and clean up ones that haven't been active in months. Or years. We don't judge.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No Default Full-Access Token â New projects no longer come with a full-access API token by default, encouraging the principle of least privilege from day one.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Improved Roles &amp;amp; Permissions â We revamped the roles interface to clearly show inherited permissions and human-readable summaries of what a user can actually do.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h6"&gt;Workflow &amp;amp; Quality Control&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;DatoCMS Recipes &amp;amp; Import/Export â We launched a marketplace of reusable project "recipes" â pre-built models and blocks you can install into any project to save setup time, powered by the new Schema Import/Export plugin.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dedicated SEO Fallback Options â We decoupled SEO metadata from internal preview fields, allowing you to set specific fallbacks for SEO titles and images without affecting the CMS UI.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Force Validations on Publishing â You can now prevent the publishing of records that don't meet current validation rules â crucial when you've tightened schema requirements on existing content.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Save Invalid Drafts â Conversely, you can now save drafts even if they are invalid, allowing editors to save their work-in-progress without being blocked by strict validation rules until they are ready to publish. Because sometimes "half-done" is better than "lost."&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Draft Mode by Default â To encourage better editorial workflows, "Draft/Published" mode is now the default setting for all new models.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Smart Confirmation Guardrails â Destructive actions now calculate their impact before execution. If you're about to delete something used in 10+ records, we force a typed confirmation to prevent accidents. We've all been there. This is us protecting you from yourself.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;...and we also cleaned up some tech debt by sunsetting legacy batch endpoints and removing unused CI triggers, keeping the platform lean and fast.&lt;/p&gt;
    &lt;head rend="h2"&gt;Plugins: The ecosystem keeps growing&lt;/head&gt;
    &lt;p&gt;30 new public plugins landed in the marketplace this year â plus countless private ones we'll never see. The community (and our support team!) keeps surprising us with stuff we didn't even know we needed.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;AI Translations â Bulk-translate entire records with OpenAI, Claude, Gemini, or DeepL. Finally, a reason to stop copy-pasting into Google Translate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Schema Import/Export â Move models between projects without losing your mind. The backbone of our new Recipes feature.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Asset Optimization â Mass-optimize your media library and watch your storage bill shrink.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom Text Styles â Add custom marks and styles to Structured Text. Your designers will love you.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Phone Number, Zoned DateTime Picker, Bulk Change Author â Small tools that solve real annoyances.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Infrastructure: The journey to independence&lt;/head&gt;
    &lt;p&gt;This year, DatoCMS handled an average of 3.5B API calls/month (+80%), while serving 500TB of traffic/month and 4.5M optimized video views/month. At the same time, we executed the most ambitious engineering project in our history: a complete migration from Heroku to a custom Kubernetes cluster on AWS.&lt;/p&gt;
    &lt;p&gt;For almost ten years, managed hosting served us well â but by mid-2024, we had hit a ceiling. Costs were rising while our need for granular control grew. We realized we were paying a premium for convenience we no longer needed. It was time to build our own home.&lt;/p&gt;
    &lt;p&gt;The journey began back in October 2024, kicking off a nine-month marathon. We spent the winter prototyping (experimenting with everything from bare metal to alternative PaaS providers â some of which shall remain unnamed to protect the guilty), the spring architecting, and the early summer stress-testing.&lt;/p&gt;
    &lt;p&gt;After months of planning, we flipped the switch on Saturday, June 7th. We prepared for a battle, but we mostly ended up watching dashboards. Aside from a tiny detail that cost us exactly 1 minute of downtime, the transition was flawless. By the time we turned the writes back on, every byte of data had been successfully secured in AWS.&lt;/p&gt;
    &lt;p&gt;The results were immediate and startling:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Speed: Response times for the Content Delivery API (CDA) were halved instantly.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Efficiency: We are now running on 64GB RAM database instances on AWS that handle traffic better than the 256GB instances we used on Heroku. Yes, you read that right. Four times less RAM, better performance.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It was a massive bet, but looking at the metrics today, it is undeniably one of the best wins of our year.&lt;/p&gt;
    &lt;p&gt;We didn't just move servers and DBs; while moving our core applications to AWS EKS was the main event, we executed a total overhaul of the ecosystem surrounding it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Infrastructure as Code: We codified our entire environment using Terraform, giving us a reproducible, version-controlled blueprint of our infrastructure that eliminates manual configuration drift.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;CDN Caching: We switched from Fastly to Cloudflare for our CDN cache, implementing smarter caching rules that improved our hit ratio from 85% to 97%.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Storage: We migrated from AWS S3 to Cloudflare R2, eliminating massive egress fees and optimizing asset delivery. Goodbye, AWS data transfer bills. We won't miss you.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Observability: We ditched expensive CloudWatch logs for a custom Prometheus &amp;amp; Loki stack, slashing our monitoring bills to near zero while improving data quality.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Developer Experience: To tame Kubernetes complexity, we built cubo, a custom&lt;/p&gt;&lt;code&gt;kubectl&lt;/code&gt;wrapper tailored around our needs that handles everything from generating K8S manifests and orchestrating rollouts to managing cronjobs, real-time logs, and one-off commands, preserving the "git push" and CLI simplicity we loved on Heroku.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Bottom Line: We lowered overall infrastructure costs by over 25%, reduced Content Delivery API latency by 50%, expanded Realtime API capacity by 10Ã, and gained full control across every infrastructure layer. And we kept our sanity. Mostly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond code: Taking control of the books&lt;/head&gt;
    &lt;p&gt;While liberating ourselves from managed hosting, we made another quiet move: we fully internalized our accounting. For years, we outsourced this to external firms â the typical setup where you hand over receipts and hope for the best. But as we grew, flying blind between quarterly reports became untenable.&lt;/p&gt;
    &lt;p&gt;Now we run everything in-house with full visibility into our finances at any moment. No more waiting for external accountants to reconcile things. Same philosophy as the infrastructure migration: control beats convenience when you're building for the long term.&lt;/p&gt;
    &lt;head rend="h2"&gt;Team: Still small by design&lt;/head&gt;
    &lt;p&gt;This year marked our 10th anniversary â a decade of surviving frontend trends, CMS wars, and the occasional existential crisis about whether "headless" is still a cool term. To celebrate, we flew our entire team to the Tuscan countryside to eat, drink, and ride quad bikes. You can read the full story of our trip (and our "25% Matteo concentration rate") here: Dato Turns 10.&lt;/p&gt;
    &lt;p&gt;Despite our growth in revenue and traffic, we remain a team of just 13 people. This isn't an accident â it's a deliberate choice.&lt;/p&gt;
    &lt;p&gt;As we wrote in "How can you be eight people?" (well, now thirteen), building a massive organization is optional. We choose to ignore the pressure to maximize headcount or chase VC funding. Instead, we focus on what actually matters: a solid product, a healthy work-life balance, and staying profitable on our own terms. We don't mind "leaving a little water in the cloth" if it means we get to keep building the software we love, the way we want to build it.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next?&lt;/head&gt;
    &lt;p&gt;No idea. And honestly, we like it that way.&lt;/p&gt;
    &lt;p&gt;We're not going to pretend we have a five-year vision carved in stone or a slide deck about "the future of content." We'll keep shipping what matters, keep ignoring the hype cycles, and keep cashing checks instead of burning through runway.&lt;/p&gt;
    &lt;p&gt;That said... we may have a few things cooking that we're genuinely excited about. But we're not going to jinx it by overpromising â you'll see them when they ship.&lt;/p&gt;
    &lt;p&gt;Well, see you in 2026. We'll still be here. Probably still 13 people. Definitely still not taking ourselves too seriously. ð§¡&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Financials: Strong growth, with best-in-class margins&lt;/item&gt;
      &lt;item&gt;Partners: More and more of you are joining us&lt;/item&gt;
      &lt;item&gt;Product: Another incredible round of improvements&lt;/item&gt;
      &lt;item&gt;Type Safety &amp;amp; Developer Confidence&lt;/item&gt;
      &lt;item&gt;AI &amp;amp; LLM Readiness&lt;/item&gt;
      &lt;item&gt;Content Editing Experience&lt;/item&gt;
      &lt;item&gt;API &amp;amp; Tooling Power&lt;/item&gt;
      &lt;item&gt;Security &amp;amp; Governance&lt;/item&gt;
      &lt;item&gt;Workflow &amp;amp; Quality Control&lt;/item&gt;
      &lt;item&gt;Plugins: The ecosystem keeps growing&lt;/item&gt;
      &lt;item&gt;Infrastructure: The journey to independence&lt;/item&gt;
      &lt;item&gt;Beyond code: Taking control of the books&lt;/item&gt;
      &lt;item&gt;Team: Still small by design&lt;/item&gt;
      &lt;item&gt;What's next?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46363319</guid><pubDate>Tue, 23 Dec 2025 07:50:03 +0000</pubDate></item><item><title>Instant database clones with PostgreSQL 18</title><link>https://boringsql.com/posts/instant-database-clones/</link><description>&lt;doc fingerprint="bd19a93f8d981b8a"&gt;
  &lt;main&gt;
    &lt;p&gt;Have you ever watched long running migration script, wondering if it's about to wreck your data? Or wish you can "just" spin a fresh copy of database for each test run? Or wanted to have reproducible snapshots to reset between runs of your test suite, (and yes, because you are reading boringSQL) needed to reset the learning environment?&lt;/p&gt;
    &lt;p&gt;When your database is a few megabytes, &lt;code&gt;pg_dump&lt;/code&gt; and restore works fine. But
what happens when you're dealing with hundreds of megabytes/gigabytes - or more?
Suddenly "just make a copy" becomes a burden.&lt;/p&gt;
    &lt;p&gt;You've probably noticed that PostgreSQL connects to &lt;code&gt;template1&lt;/code&gt; by default. What
you might have missed is that there's a whole templating system hiding in plain
sight. Every time you run&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE dbname;
&lt;/code&gt;
    &lt;p&gt;PostgreSQL quietly clones standard system database &lt;code&gt;template1&lt;/code&gt; behind the
scenes. Making it same as if you would use&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE dbname TEMPLATE template1;
&lt;/code&gt;
    &lt;p&gt;The real power comes from the fact that you can replace &lt;code&gt;template1&lt;/code&gt; with any
database. You can find more at Template Database
documentation.&lt;/p&gt;
    &lt;p&gt;In this article, we will cover a few tweaks that turn this templating system into an instant, zero-copy database cloning machine.&lt;/p&gt;
    &lt;head rend="h2"&gt;CREATE DATABASE ... STRATEGY&lt;/head&gt;
    &lt;p&gt;Before PostgreSQL 15, when you created a new database from a template, it operated strictly on the file level. This was effective, but to make it reliable, Postgres had to flush all pending operations to disk (using &lt;code&gt;CHECKPOINT&lt;/code&gt;) before taking a consistent snapshot. This created a massive I/O
spike - a "Checkpoint Storm" - that could stall your production traffic.&lt;/p&gt;
    &lt;p&gt;Version 15 of PostgreSQL introduced new parameter &lt;code&gt;CREATE DATABASE ... STRATEGY = [strategy]&lt;/code&gt; and at the same time changed the default behaviour how the new
databases are created from templates. The new default become &lt;code&gt;WAL_LOG&lt;/code&gt; which
copies block-by-block via the Write-Ahead Log (WAL), making I/O sequential (and
much smoother) and support for concurrency without facing latency spike. This
prevented the need to CHECKPOINT but made the database cloning operation
potentially significantly slower. For an empty &lt;code&gt;template1&lt;/code&gt;, you won't notice the
difference. But if you try to clone a 500GB database using WAL_LOG, you are
going to be waiting a long time.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;STRATEGY&lt;/code&gt; parameter allows us to switch back to the original method
&lt;code&gt;FILE_COPY&lt;/code&gt; to keep the behaviour, and speed. And since PostgreSQL 18, this
opens the whole new set of options.&lt;/p&gt;
    &lt;head rend="h2"&gt;FILE_COPY&lt;/head&gt;
    &lt;p&gt;Because the &lt;code&gt;FILE_COPY&lt;/code&gt; strategy is a proxy to operating system file operations,
we can change how the OS handles those files.&lt;/p&gt;
    &lt;p&gt;When using standard file system (like &lt;code&gt;ext4&lt;/code&gt;), PostgreSQL reads every byte of
the source file and writes it to a new location. It's a physical copy. However
starting with PostgreSQL 18 - &lt;code&gt;file_copy_method&lt;/code&gt; gives you options to switch
that logic; while default option remains &lt;code&gt;copy&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;With modern filesystems (like ZFS, XFS with reflinks, APFS, etc.) you can switch it to &lt;code&gt;clone&lt;/code&gt; and leverage &lt;code&gt;CLONE&lt;/code&gt; (&lt;code&gt;FICLONE&lt;/code&gt; on Linux) operation for almost
instant operation. And it won't take any additional space.&lt;/p&gt;
    &lt;p&gt;All you have to do is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux with XFS or ZFS support (we will use XFS for the demostration) or similar operating system. MacOS APFS is also fully supported. FreeBSD with ZFS also supported (which normally would be my choice, but haven't got time to test so far)&lt;/item&gt;
      &lt;item&gt;PostgreSQL cluster on that file system&lt;/item&gt;
      &lt;item&gt;update the configuration &lt;code&gt;file_copy_method = clone&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;and reload the configuration&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The benchmark&lt;/head&gt;
    &lt;p&gt;We need some dummy data to copy. This is the only part of the tutorial where you have to wait. Let's generate a ~6GB database.&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE source_db;
\c source_db

CREATE TABLE boring_data (
    id serial PRIMARY KEY,
    payload text
);

-- generate 50m rows
INSERT INTO boring_data (payload)
SELECT md5(random()::text) || md5(random()::text)
FROM generate_series(1, 50000000);

-- force a checkpoint
CHECKPOINT;
&lt;/code&gt;
    &lt;p&gt;You can verify the database now has roughly 6GB of data.&lt;/p&gt;
    &lt;code&gt;Name              | source_db
Owner             | postgres
Encoding          | UTF8
Locale Provider   | libc
Collate           | en_US.UTF-8
Ctype             | en_US.UTF-8
Locale            |
ICU Rules         |
Access privileges |
Size              | 6289 MB
Tablespace        | pg_default
Description       |
&lt;/code&gt;
    &lt;p&gt;While enabling &lt;code&gt;\timing&lt;/code&gt; you can test the default (WAL_LOG) strategy. And on my
test volume (relatively slow storage) I get&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE slow_copy TEMPLATE source_db;
CREATE DATABASE
Time: 67000.615 ms (01:07.001)
&lt;/code&gt;
    &lt;p&gt;Now, let's verify our configuration is set for speed:&lt;/p&gt;
    &lt;code&gt;show file_copy_method;
 file_copy_method
------------------
 clone
(1 row)
&lt;/code&gt;
    &lt;p&gt;Let's request the semi-instant clone of the same database, without taking extra disk space at the same time.&lt;/p&gt;
    &lt;code&gt;CREATE DATABASE fast_clone TEMPLATE source_db STRATEGY=FILE_COPY;
CREATE DATABASE
Time: 212.053 ms
&lt;/code&gt;
    &lt;p&gt;That's a quite an improvement, isn't it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Working with cloned data&lt;/head&gt;
    &lt;p&gt;That was the simple part. But what is happening behind the scenes?&lt;/p&gt;
    &lt;p&gt;When you clone a database with &lt;code&gt;file_copy_method = clone&lt;/code&gt;, PostgreSQL doesn't
duplicate any data. The filesystem creates new metadata entries that point to
the same physical blocks. Both databases share identical storage.&lt;/p&gt;
    &lt;p&gt;This can create some initial confusion. If you ask PostgreSQL for the size:&lt;/p&gt;
    &lt;code&gt;SELECT pg_database_size('source_db') as source,
       pg_database_size('fast_clone') as clone;
&lt;/code&gt;
    &lt;p&gt;PostgreSQL reports both as ~6GB because that's the logical size - how much data each database "contains" - i.e. logical size.&lt;/p&gt;
    &lt;code&gt;-[ RECORD 1 ]------
source | 6594041535
clone  | 6594041535
&lt;/code&gt;
    &lt;p&gt;The interesting part happens when you start writing. PostgreSQL doesn't update tuples in place. When you UPDATE a row, it writes a new tuple version somewhere (often a different page entirely) and marks the old one as dead. The filesystem doesn't care about PostgreSQL internals - it just sees writes to 8KB pages. Any write to a shared page triggers a copy of that entire page.&lt;/p&gt;
    &lt;p&gt;A single UPDATE will therefore trigger copy-on-write on multiple pages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the page holding the old tuple&lt;/item&gt;
      &lt;item&gt;the page receiving the new tuple&lt;/item&gt;
      &lt;item&gt;index pages if any indexed columns changed&lt;/item&gt;
      &lt;item&gt;FSM and visibility map pages as PostgreSQL tracks free space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And later, VACUUM touches even more pages while cleaning up dead tuples. In this case diverging quickly from the linked storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;XFS proof&lt;/head&gt;
    &lt;p&gt;Using the database OID and relfilenode we can verify the both databases are now sharing physical blocks.&lt;/p&gt;
    &lt;code&gt;root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16402/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16402/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..    2031:   10471550..  10473581:   2032:             shared
   1:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   2:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   3:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   4:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   5:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   6:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16402/16404: 7 extents found
root@clone-demo:/var/lib/postgresql#
root@clone-demo:/var/lib/postgresql#
root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16418/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16418/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..    2031:   10471550..  10473581:   2032:             shared
   1:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   2:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   3:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   4:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   5:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   6:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16418/16404: 7 extents found
&lt;/code&gt;
    &lt;p&gt;All it takes is to update some rows using&lt;/p&gt;
    &lt;code&gt;update boring_data set payload = 'new value' || id where id IN (select id from boring_data limit 20);
&lt;/code&gt;
    &lt;p&gt;and the situation will start to change.&lt;/p&gt;
    &lt;code&gt;root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16402/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16402/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..      39:   10471550..  10471589:     40:
   1:       40..    2031:   10471590..  10473581:   1992:             shared
   2:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   3:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   4:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   5:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   6:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   7:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16402/16404: 7 extents found
root@clone-demo:/var/lib/postgresql# sudo filefrag -v /var/lib/postgresql/18/main/base/16418/16404
Filesystem type is: 58465342
File size of /var/lib/postgresql/18/main/base/16418/16404 is 1073741824 (262144 blocks of 4096 bytes)
 ext:     logical_offset:        physical_offset: length:   expected: flags:
   0:        0..      39:   10297326..  10297365:     40:
   1:       40..    2031:   10471590..  10473581:   1992:   10297366: shared
   2:     2032..   16367:   10474098..  10488433:  14336:   10473582: shared
   3:    16368..   32751:   10497006..  10513389:  16384:   10488434: shared
   4:    32752..   65519:   10522066..  10554833:  32768:   10513390: shared
   5:    65520..  129695:   10571218..  10635393:  64176:   10554834: shared
   6:   129696..  195231:   10635426..  10700961:  65536:   10635394: shared
   7:   195232..  262143:   10733730..  10800641:  66912:   10700962: last,shared,eof
/var/lib/postgresql/18/main/base/16418/16404: 8 extents found
root@clone-demo:/var/lib/postgresql#
&lt;/code&gt;
    &lt;p&gt;In this case extent 0 no longer has shared flag, first 40 blocks size (with default size 4KB) now diverge, making it total of 160KB. Each database now has its own copy at different physical address. The remaining extents are still shared.&lt;/p&gt;
    &lt;head rend="h2"&gt;Things to be aware of&lt;/head&gt;
    &lt;p&gt;Cloning is tempting but there's one serious limitation you need to be aware if you ever attempt to do it in production. The source database can't have any active connections during cloning. This is a PostgreSQL limitation, not a filesystem one. For production use, this usually means you create a dedicated template database rather than cloning your live database directly. Or given the relatively short time the operation takes you have to schedule the cloning in times where you can temporary block/terminate all connections.&lt;/p&gt;
    &lt;p&gt;Other limitation is that the cloning only works within a single filesystem. If your databases spans multiple table spaces on different mount points, cloning will fall back to regular physical copy.&lt;/p&gt;
    &lt;p&gt;Finally, in most managed cloud environments (AWS RDS, Google Cloud SQL), you will not have access to the underlying filesystem to configure this. You are stuck with their proprietary (and often billed) functionality. But for your own VMs or bare metal? Go ahead and try it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46363360</guid><pubDate>Tue, 23 Dec 2025 07:58:25 +0000</pubDate></item><item><title>Carnap – A formal logic framework for Haskell</title><link>https://carnap.io/</link><description>&lt;doc fingerprint="ef09550348c11b2f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome to Carnap.io&lt;/head&gt;
    &lt;p&gt;A formal logic framework for Haskell&lt;/p&gt;
    &lt;head rend="h2"&gt;Background&lt;/head&gt;
    &lt;p&gt;Carnap is a free and open software framework written in Haskell for teaching and studying formal logic. Carnap powers logic courses at dozens of colleges and universities around the world.&lt;/p&gt;
    &lt;p&gt;If you're a student in a course that uses Carnap, please follow the links at the top of the page to log in and to access course materials.&lt;/p&gt;
    &lt;p&gt;If you're just curious about Carnap, you can find some general information on our about page. If you're interested in the project, and would like to use Carnap in a class you're teaching, or get involved in some other way, please feel free to get in touch!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46363751</guid><pubDate>Tue, 23 Dec 2025 09:17:42 +0000</pubDate></item><item><title>Ask HN: What are the best engineering blogs with real-world depth?</title><link>https://news.ycombinator.com/item?id=46363921</link><description>&lt;doc fingerprint="113795e1f2b7ec68"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I’m looking for examples of high-quality engineering blog posts—especially from tech company blogs, that go beyond surface-level explanations.&lt;/p&gt;
      &lt;p&gt;Specifically interested in posts that: 1. Explain technical concepts clearly and concisely 2. Show real implementation details, trade-offs, and failures 3. Are well-structured and readable 4. Tie engineering decisions back to business or product outcomes&lt;/p&gt;
      &lt;p&gt;Any standout blogs, posts, or platforms you regularly learn from?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46363921</guid><pubDate>Tue, 23 Dec 2025 09:50:31 +0000</pubDate></item><item><title>Font with Built-In Syntax Highlighting (2024)</title><link>https://blog.glyphdrawing.club/font-with-built-in-syntax-highlighting/</link><description>&lt;doc fingerprint="a22f9c78ab11c3ad"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Syntax Highlighting in Hand-Coded Websites&lt;/head&gt;
    &lt;head rend="h3"&gt;The problem&lt;/head&gt;
    &lt;p&gt;I have been trying to identify practical reasons why hand-coding websites with HTML and CSS is so hard (by hand-coding, I mean not relying on frameworks, generators or 3rd party scripts that modify the DOM).&lt;/p&gt;
    &lt;p&gt;Let's say, I want to make a blog. What are the actual things that prevent me from making—and maintaining—it by hand? What would it take to clear these roadblocks?&lt;/p&gt;
    &lt;p&gt;There are many, of course, but for a hand-coded programming oriented blog one of these roadblocks is syntax highlighting.&lt;/p&gt;
    &lt;p&gt;When I display snippets of code, I want to make the code easy to read and understand by highlighting it with colors. To do that, I would normally need to use a complex syntax highlighter library, like Prism or highlight.js. These scripts work by scanning and chopping up the code into small language-specific patterns, then wrapping each part in tags with special styling that creates the highlighted effect, and then injecting the resulting HTML back into the page.&lt;/p&gt;
    &lt;p&gt;But, I want to write code by hand. I don't want any external scripts to inject things I didn't write myself. Syntax highlighters also add to the overall complexity and bloat of each page, which I'm trying to avoid. I want to keep things as simple as possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leveraging OpenType features to build a simple syntax highlighter inside the font&lt;/head&gt;
    &lt;p&gt;This lead me to think: could it be possible to build syntax highlighting directly into a font, skipping JavaScript altogether? Could I somehow leverage OpenType features, by creating colored glyphs with the COLR table, and identifying and substituting code syntax with contextual alternates?&lt;/p&gt;
    &lt;code&gt;&amp;lt;div class="spoilers"&amp;gt;
  &amp;lt;strong&amp;gt;Yes, it's possible!&amp;lt;/strong&amp;gt;
  &amp;lt;small&amp;gt;...to some extent =)&amp;lt;/small&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;
    &lt;p&gt;The colors in the HTML snippet above comes from within the font itself, the code is plain text, and requires no JavaScript.&lt;/p&gt;
    &lt;p&gt;To achieve that, I modified an open source font Monaspace Krypton to include colored versions of each character, and then used OpenType contextual alternates to essentially find &amp;amp; replace specific strings of text based on HTML, CSS and JS syntax. The result is a simple syntax highlighter, built-in to the font itself.&lt;/p&gt;
    &lt;p&gt;If you want to try it yourself, download the font: FontWithASyntaxHighlighter-Regular.woff2&lt;/p&gt;
    &lt;p&gt;And include the following bits of CSS:&lt;/p&gt;
    &lt;code&gt;@font-face {
  font-family: 'FontWithASyntaxHighlighter';
  src: 
    url('/FontWithASyntaxHighlighter-Regular.woff2') 
    format('woff2')
  ;
}
code {
  font-family: "FontWithASyntaxHighlighter", monospace;
}
&lt;/code&gt;
    &lt;p&gt;And that's it!&lt;/p&gt;
    &lt;head rend="h2"&gt;What are the Pros and Cons of this method?&lt;/head&gt;
    &lt;p&gt;This method opens up some interesting possibilities...&lt;/p&gt;
    &lt;head rend="h3"&gt;Pros&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install is as easy as using any custom font.&lt;/item&gt;
      &lt;item&gt;Works without JavaScript.&lt;/item&gt;
      &lt;item&gt;Works without CSS themes.&lt;/item&gt;
      &lt;item&gt;...but can be themed with CSS.&lt;/item&gt;
      &lt;item&gt;It's fast.&lt;/item&gt;
      &lt;item&gt;Snippets of code can be put into &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt;and&lt;code&gt;&amp;lt;code&amp;gt;&lt;/code&gt;, with no extra classes or&lt;code&gt;&amp;lt;span&amp;gt;&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;Clean HTML source code.&lt;/item&gt;
      &lt;item&gt;Works everywhere that supports OpenType features, like InDesign.&lt;/item&gt;
      &lt;item&gt;Doesn't require maintenance or updating.&lt;/item&gt;
      &lt;item&gt;Works in &lt;code&gt;&amp;lt;textarea&amp;gt;&lt;/code&gt;and&lt;code&gt;&amp;lt;input&amp;gt;&lt;/code&gt;! Syntax highlighting inside&lt;code&gt;&amp;lt;textarea&amp;gt;&lt;/code&gt;has been previously impossible, because textareas and inputs can only contain plain text. This is where the interesting possibilities lie. As a demo, I made this tiny HTML, CSS &amp;amp; JS sandbox, with native undo and redo, in a single, web component.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;tiny HTML &amp;amp; CSS sandbox =)&lt;/p&gt;
    &lt;head rend="h3"&gt;Cons&lt;/head&gt;
    &lt;p&gt;There are, of course, some limitations to this method. It is not a direct replacement to the more robust syntax highligting libraries, but works well enough for simple needs.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Making modifications to the syntax highligher, like adding more language supports or changing the look of the font, requires modifying the font file. This requires some knowledge of font production, which most people don't have.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It only works where OpenType is supported. Fortunately, that's all major browsers and most modern programs. However, something like PowerPoint doesn't support OpenType.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Finding patterns in text with OpenType contextual alternates is quite basic, and is no match for scripts that use regular expressions. For example, words within&lt;/p&gt;&lt;code&gt;&amp;lt;p&amp;gt;&lt;/code&gt;tags that are JS keywords will be always highlighted:&lt;code&gt;&amp;lt;p&amp;gt;if I throw this Object through the window, catch it, for else it’ll continue to Infinity &amp;amp; break&amp;lt;/p&amp;gt;&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiline highlighting with manual line breaks will sadly not work.&lt;/p&gt;
        &lt;p&gt;This is common, for example, in comment blocks and template literals:&lt;/p&gt;
        &lt;code&gt;&amp;lt;!-- This line gets highlighted... but not this, because I made a manual line break... --&amp;gt;&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;How does it actually work?&lt;/head&gt;
    &lt;p&gt;Here's roughly how it works. There are two features in OpenType that make this possible: OpenType COLR table and contextual alternates.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenType COLR table&lt;/head&gt;
    &lt;p&gt;OpenType COLR table makes multi-colored fonts possible. There is a good guide on creating a color font using Glyphs.&lt;/p&gt;
    &lt;p&gt;I made a palette with 8 colors.&lt;/p&gt;
    &lt;p&gt;I duplicated letters &lt;code&gt;A&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;Z&lt;/code&gt;, numbers &lt;code&gt;0&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;9&lt;/code&gt; and the characters &lt;code&gt;.&lt;/code&gt; &lt;code&gt;#&lt;/code&gt; &lt;code&gt;*&lt;/code&gt; &lt;code&gt;-&lt;/code&gt; and &lt;code&gt;_&lt;/code&gt; four times. Each duplicated character is then suffixed with .alt, .alt2, .alt3 or .alt4, and then assigned a color from the palette. For example, all .alt1 glyphs are &lt;code&gt;this&lt;/code&gt; color.&lt;/p&gt;
    &lt;p&gt;I also duplicated all characters twice, and gave them suffices .alt1 and .alt5 and assigned them colors used in &lt;code&gt;&amp;lt;!-- comment blocks --&amp;gt;&lt;/code&gt; and &lt;code&gt;"strings within quotes"&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;The two other colors I used for symbols &lt;code&gt;&amp;amp; | $ + − = ~ [] () {} / ; : " @ %&lt;/code&gt; and &lt;code&gt;'&lt;/code&gt;, and they are always in one color. Numbers &lt;code&gt;0 1 2 3 4 5 6 7 8 9&lt;/code&gt; are also always a certain color, unless overriden by other rules.&lt;/p&gt;
    &lt;head rend="h3"&gt;OpenType contextual alternates&lt;/head&gt;
    &lt;p&gt;The second required feature is OpenType contextual alternates. Here's a great introductory guide to advanced contextual alternates for Glyphs.&lt;/p&gt;
    &lt;p&gt;Contextual alternates makes characters "aware" of their adjacent characters. An example would be fonts that emulate continuous hand writing, where how a letter connects depends on which letter it connects to. There is a nice article covering possible uses here.&lt;/p&gt;
    &lt;head rend="h4"&gt;JavaScript syntax rules&lt;/head&gt;
    &lt;p&gt;The core feature of contextual alternates is substituting glyphs. Here is a simplified code for finding the JavaScript keyword &lt;code&gt;if&lt;/code&gt; and substituting the letters i and f with their colored variant:&lt;/p&gt;
    &lt;code&gt;sub i' f by i.alt2;
sub i.alt2 f' by f.alt2;
&lt;/code&gt;
    &lt;p&gt;In English:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;When i is followed by f, substitute the default i with an alternate (i.alt2).&lt;/item&gt;
      &lt;item&gt;When i.alt2 is followed by f, substitute the default f with an alternate (f.alt2).&lt;/item&gt;
      &lt;item&gt;As a result, every "if" in text gets substituted with &lt;code&gt;if&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;OpenType doesn't support many-to-many substitutions directly, but @behdad on Mastodon had a great suggestion: keywords could be elegantly colored by chaining contextual substitutions.&lt;/p&gt;
    &lt;p&gt;To do this, I made a lookup which substitutes each letter with its colored variant.&lt;/p&gt;
    &lt;code&gt;lookup ALT_SUBS {
    sub a by a.alt; 
    sub b by b.alt; 
    sub c by c.alt; 
    [etc.]
    sub Y by Y.alt;
    sub Z by Z.alt;
} ALT_SUBS;
&lt;/code&gt;
    &lt;p&gt;I moved this lookup rule to the Prefix section, which just means it doesn't get applied automatically unlike the other lookups.&lt;/p&gt;
    &lt;p&gt;Then, I made a lookup rule for each keyword in the contextual alternates section. Here's one for &lt;code&gt;console&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;lookup console {
    ignore sub @AllLetters c' o' n' s' o' l' e';
    ignore sub c' o' n' s' o' l' e' @AllLetters;
    sub c' lookup ALT_SUBS
        o' lookup ALT_SUBS
        n' lookup ALT_SUBS
        s' lookup ALT_SUBS
        o' lookup ALT_SUBS
        l' lookup ALT_SUBS
        e' lookup ALT_SUBS;
} console;
&lt;/code&gt;
    &lt;p&gt;First two lines tells it to ignore strings like &lt;code&gt;Xconsole&lt;/code&gt; or &lt;code&gt;consoles&lt;/code&gt;, but not if there's a period like &lt;code&gt;console.log()&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The third line starts by replacing the first letter 'c' with its colored variant &lt;code&gt;c&lt;/code&gt;, by using definitions from the other lookup table "ALT_SUBS". This repeats until each letter is replaced by its color variant, and the result is &lt;code&gt;console&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Identifying JavaScript keywords is fairly straightforward. Logic is the same for each keyword, and I used a python script to generate them.&lt;/p&gt;
    &lt;head rend="h4"&gt;HTML &amp;amp; CSS syntax rules&lt;/head&gt;
    &lt;p&gt;But for HTML and CSS... I had to get a bit more creative. There are simply too many keywords for both HTML and CSS combined. Making a separate rule for each keyword would inflate the file size.&lt;/p&gt;
    &lt;p&gt;Instead, I came up with this monstrosity. Here's how I find CSS value functions:&lt;/p&gt;
    &lt;code&gt;lookup CssParamCalt useExtension {
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' @CssParam parenleft by @CssParamAlt4;
  sub @CssParam' parenleft by @CssParamAlt4;
} CssParamCalt;
&lt;/code&gt;
    &lt;p&gt;@CssParam is a custom OpenType glyph class I've set up. It includes the characters &lt;code&gt;A&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;Z&lt;/code&gt;, &lt;code&gt;a&lt;/code&gt; &lt;code&gt;–&lt;/code&gt; &lt;code&gt;z&lt;/code&gt;, and &lt;code&gt;-&lt;/code&gt;, which are all the possible characters used in CSS value function names. Because the longest possible CSS value function name is &lt;code&gt;repeating-linear-gradient()&lt;/code&gt;, with 25 letters, the first line of the lookup starts with @CssParam repeated 25 times, followed by parenleft (&lt;code&gt;(&lt;/code&gt;). This lookup will match any word up to 25 letters long, if it's immediately followed by an opening parenthesis. When a match occurs, it substitutes the matched text with its alternate color form (@CssParamAlt4).&lt;/p&gt;
    &lt;p&gt;This lookup works for both CSS and JavaScript. It will colorize standard CSS functions like &lt;code&gt;rgb()&lt;/code&gt; as well as custom JavaScript functions like &lt;code&gt;myFunction()&lt;/code&gt;. The result is a semi-flexible syntax highlighter that doesn't require complex parsing. The downside is that if you have a really long function name, it stops working midway: &lt;code&gt;aReallyLongFunctionNameStopsWorkingMidway()&lt;/code&gt;. I've repeated the same principle for finding HTML tags and attributes, and for CSS selectors and parameters.&lt;/p&gt;
    &lt;head rend="h4"&gt;Unknown length rules&lt;/head&gt;
    &lt;p&gt;Comment blocks and strings between quotes also required extra care, because their length can be anything. OpenType doesn't support loops or anything resembling regular expressions. For example, I can't just tell it to simply substitute everything it finds between two quotes.&lt;/p&gt;
    &lt;p&gt;However, I got a great suggestion from @penteract on Hacker News to use a finite state machine for these kinds of situations. Here our aim is to colorize eveything between /* and */ gray:&lt;/p&gt;
    &lt;code&gt;lookup CSScomment useExtension {
  // stop if we encounter a colored */
  ignore sub asterisk.alt1 slash.alt1 @All';

  // color first letter after /*
  sub slash asterisk @All' by @AllAlt1;
  sub slash asterisk space @All' by @AllAlt1;
  
  // color /* itself
  sub slash' asterisk by slash.alt1;
  sub slash.alt1 asterisk' by asterisk.alt1;
  
  // finite state machine to color rest of the characters
  // or until ignore condition is met
  sub @AllAlt1 @All' by @AllAlt1;
} CSScomment;
&lt;/code&gt;
    &lt;p&gt;The last line is the important one. The lookup will just continue replacing characters if the previous character is already colored.&lt;/p&gt;
    &lt;head rend="h3"&gt;End note&lt;/head&gt;
    &lt;p&gt;The full process is a little bit too convoluted to go into step-by-step, but if you're a type designer who wants to do this with their own font, don't hesitate to contact me.&lt;/p&gt;
    &lt;p&gt;I'm also not an OpenType expert, so I'm sure the substitution logics could be improved upon. If you're interested in learning more about OpenType, I recommend reading The OpenType Cookbook and the complete OpenType™ Feature File Specification.&lt;/p&gt;
    &lt;p&gt;If you have any ideas, suggestions or feedback, let me know. You can reach me at &lt;code&gt;hlotvonen@gmail.com&lt;/code&gt; or leave a comment on Mastodon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Changing the color theme&lt;/head&gt;
    &lt;p&gt;You can change the color theme with CSS &lt;code&gt;override-colors&lt;/code&gt;! Browser support is great at ~94%.&lt;/p&gt;
    &lt;code&gt;
        var const let for while
        function() linear-gradient()
        .myDiv{ background-color: pink; }
        console.log("hello", true)
        /* comment */
        &amp;amp; | $ + − = ~ [] () {} / ; : " @ % 
        0 1 2 3 4 5 6 7 8 9
      &lt;/code&gt;
    &lt;head rend="h2"&gt;Alternative built-in color themes&lt;/head&gt;
    &lt;p&gt;Additionally, two alternative color themes Night Owl, and Light Owl were added by niutech. You can download them from the FontWithASyntaxHighlighter GitHub page. Night Owl theme is made by Sarah Drasner.&lt;/p&gt;
    &lt;p&gt;In order to modify the built-in color palette, you have to edit the font source file. To do so, you can edit the color palettes values in lines 112-120 of the FontWithASyntaxHighlighter.glyphs file and then build the font with fontmake.&lt;/p&gt;
    &lt;head rend="h2"&gt;Projects using this font&lt;/head&gt;
    &lt;p&gt;Here's some cool projects that are using or are inspired by this font:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Holograph is a visual coding tool built on tldraw &amp;amp; its GitHub page&lt;/item&gt;
      &lt;item&gt;@celine/celine is library for building reactive HTML notebooks &amp;amp; its GitHub page &amp;amp; blogpost&lt;/item&gt;
      &lt;item&gt;Shaders art made with pure CSS &amp;amp; its GitHub Page&lt;/item&gt;
      &lt;item&gt;Mdit, a simple Markdown previewer &amp;amp; its GitHub Page&lt;/item&gt;
      &lt;item&gt;Web Component for making a Textarea element into a syntax highlighted codeblock&lt;/item&gt;
      &lt;item&gt;It might also be used as an example for displaying the potential uses for color fonts in the W3C CSS Fonts Module Level 4 specification&lt;/item&gt;
      &lt;item&gt;
        &lt;code-pen&gt;Web Component with syntax highlighting&lt;/code-pen&gt;
      &lt;/item&gt;
      &lt;item&gt;An OpenType font with built-in TEX syntax highlighting&lt;/item&gt;
      &lt;item&gt;Labelary ZPL viewer &amp;amp; editor&lt;/item&gt;
      &lt;item&gt;garten.salat.dev blog&lt;/item&gt;
      &lt;item&gt;L’invasion du HTML mutant&lt;/item&gt;
      &lt;item&gt;(Did you make a project using this font, or know a project that uses it? Let me know please!)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Potential future&lt;/head&gt;
    &lt;p&gt;Many people suggested that this concept could be taken one step further with harfbuzz-wasm. With harfbuzz-wasm a real parser could be used instead of my crazy opentype lookup rules. Essentially, all the cons could be eliminated... Any harfbuzz-wasm experts who wants to take this on?&lt;/p&gt;
    &lt;head rend="h2"&gt;Licence&lt;/head&gt;
    &lt;p&gt;The original font (MonaSpace) has SIL open font license v1.1, which carries over to my modified version. So, you're free to use the font in any way that the SIL v1.1 license permits.&lt;/p&gt;
    &lt;p&gt;As for the code examples, they are MIT licensed. The tiny sandbox web component can be found here: https://github.com/hlotvonen/tinybox&lt;/p&gt;
    &lt;head rend="h2"&gt;Source&lt;/head&gt;
    &lt;p&gt;The original source .glyphs file is hosted in this GitHub repository. UFO files were kindly added by niutech. Or, you can modify the font with some scripting &amp;amp; build with fontmake.&lt;/p&gt;
    &lt;head rend="h2"&gt;More examples&lt;/head&gt;
    &lt;code&gt;as, in, of, if, for, while, finally, var, new, function,
do, return, void, else, break, catch, instanceof, with,
throw, case, default, try, switch, continue, typeof, delete,
let, yield, const, class, get, set, debugger, async, await,
static, import, from, export, extends

true, false, null, undefined, NaN, Infinity

Object, Function, Boolean, Symbol, Math, Date, Number, BigInt, 
String, RegExp, Array, Float32Array, Float64Array, Int8Array, 
Uint8Array, Uint8ClampedArray, Int16Array, Int32Array, Uint16Array, 
Uint32Array, BigInt64Array, BigUint64Array, Set, Map, WeakSet,
WeakMap, ArrayBuffer, SharedArrayBuffer, Atomics, DataView, 
JSON, Promise, Generator, GeneratorFunction, AsyncFunction, 
Reflect, Proxy, Intl, WebAssembly, Error, EvalError, InternalError, 
RangeError, ReferenceError, SyntaxError, TypeError, URIError, 
setInterval, setTimeout, clearInterval, clearTimeout, require, 
exports, eval, isFinite, isNaN, parseFloat, parseInt, decodeURI, 
decodeURIComponent, encodeURI, encodeURIComponent, escape, 
unescape, arguments, this, super, console, window, document, 
localStorage, sessionStorage, module, global
&lt;/code&gt;
    &lt;code&gt;&amp;lt;!-- this is a comment! --&amp;gt;
/* and this */
// and this
&amp;lt;!-- however...
it breaks when your code goes to a newline.
there's no way to keep context line to line...
--&amp;gt;
&lt;/code&gt;
    &lt;code&gt;&amp;lt;!-- can't disable highlighting JS keywords in between tags --&amp;gt;
&amp;lt;p&amp;gt;
  give me a break...
&amp;lt;/p&amp;gt;
&lt;/code&gt;
    &lt;code&gt;&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html lang="en"&amp;gt;
&amp;lt;head&amp;gt;
  &amp;lt;meta charset="UTF-8"&amp;gt;
  &amp;lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&amp;gt;
  &amp;lt;title&amp;gt;Syntax Highlighter Example&amp;lt;/title&amp;gt;
  &amp;lt;style&amp;gt;
    body {
      background-color: rgb(255, 0, 0);
      font-family: 'Arial Narrow', sans-serif;
      line-height: 1.44;
      color: #333;
    }
  &amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
  &amp;lt;header&amp;gt;
    &amp;lt;h1&amp;gt;Welcome to the Syntax Highlighter Test&amp;lt;/h1&amp;gt;
  &amp;lt;/header&amp;gt;
  &amp;lt;nav&amp;gt;
    &amp;lt;ul&amp;gt;
      &amp;lt;li&amp;gt;&amp;lt;a href="#section1"&amp;gt;Section 1&amp;lt;/a&amp;gt;
    &amp;lt;/ul&amp;gt;
  &amp;lt;/nav&amp;gt;
  &amp;lt;main&amp;gt;
    &amp;lt;section id="section1"&amp;gt;
      &amp;lt;h2&amp;gt;Section 1&amp;lt;/h2&amp;gt;
      &amp;lt;p&amp;gt;This is a &amp;lt;span class="highlight"&amp;gt;highlighted&amp;lt;/span&amp;gt; paragraph.&amp;lt;/p&amp;gt;
      &amp;lt;img src="/api/placeholder/300/200" alt="Placeholder image"&amp;gt;
    &amp;lt;/section&amp;gt;
  &amp;lt;/main&amp;gt;
  &amp;lt;script&amp;gt;
    console.log("This is a JavaScript comment");
    function greet(name) {
      return `Hello, ${name}!`;
    }
    document.addEventListener('DOMContentLoaded', () =&amp;gt; {
      console.log(greet('Syntax Highlighter'));
    });
  &amp;lt;/script&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;
    &lt;code&gt;.crazyBackground {
  /* don't try this at home */
  background:
    radial-gradient(
      100% 50% at 50% 50%,
      hsl(90 90% 45%) 0% 5%,
      hsl(250 70% 40%) 50%,
      hsl(50 50% 50%)
    ),
    radial-gradient(
      100% 100% at 50% 25%,
      hsl(90 40% 85%) 30%,
      hsl(40 80% 20%) 60% 90%,
      transparent
    ),
    linear-gradient(
      90deg,
      hsl(150 90% 90%) 0 10%,
      hsl(10 10% 20%),
      hsl(150 90% 90%) 90% 100%
    )
  ;
  background-size:
    5% 10%,
    10% 200%,
    25% 100%
  ;
  background-blend-mode:
    color-dodge,
    difference,
    normal
  ;
  animation: fire2 60s linear infinite;
}

@keyframes fire2 {
  from {
    background-position: 0% 0%, 0 30%, 0 0;
  }

  to {
    background-position: 0% -100%, -100% 30%, 200% 0%;
  }
}
&lt;/code&gt;
    &lt;code&gt;// Variables and constants
let variable = 'Hello';
const CONSTANT = 42;

// Template literals
const name = 'World';
console.log(`${variable}, ${name}!`);

// Function declaration
function greet(name) {
  return `Hello, ${name}!`;
}

// Arrow function
const multiply = (a, b) =&amp;gt; a * b;

// Class definition
class Person {
  constructor(name, age) {
    this.name = name;
    this.age = age;
  }
  sayHello() {
    console.log(`Hello, my name is ${this.name}`);
  }
}

// Object literal
const config = {
  apiKey: 'abc123',
  maxRetries: 3,
  timeout: 5000
};

// Array methods
const numbers = [1, 2, 3, 4, 5];
const doubled = numbers.map(num =&amp;gt; num * 2);
const sum = numbers.reduce((acc, curr) =&amp;gt; acc + curr, 0);

// Async/await
async function fetchData(url) {
  try {
    const response = await fetch(url);
    const data = await response.json();
    return data;
  } catch (error) {
    console.error('Error fetching data:', error);
  }
}

// Destructuring
const { apiKey, maxRetries } = config;
const [first, second, ...rest] = numbers;

// Spread operator
const newArray = [...numbers, 6, 7, 8];

// Conditional (ternary) operator
const isAdult = age &amp;gt;= 18 ? 'Adult' : 'Minor';

// Switch statement
function getDayName(dayNumber) {
  switch (dayNumber) {
    case 0: return 'Sunday';
    case 1: return 'Monday';
    // ... other cases
    default: return 'Invalid day';
  }
}

// Regular expression
const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;

// Symbol
const uniqueKey = Symbol('description');

// Set and Map
const uniqueNumbers = new Set(numbers);
const userRoles = new Map([['admin', 'full'], ['user', 'limited']]);

// Promises
const promise = new Promise((resolve, reject) =&amp;gt; {
  setTimeout(() =&amp;gt; resolve('Done!'), 1000);
});

// Export statement
export { greet, Person };
&lt;/code&gt;
    &lt;head rend="h2"&gt;Discussion&lt;/head&gt;
    &lt;p&gt;I received a lot of great feedback from the discussions at Mastodon and Hacker News.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Thanks to jfk13 on hn, and @pixelambacht on Mastodon for pointing out that 'calt' is turned on by default, and that 'colr' is not an opentype feature that needs to be "turned on".&lt;/p&gt;
    &lt;p&gt;Thanks to penteract on hn and @behdad on Mastodon for suggesting better substitution rules.&lt;/p&gt;
    &lt;p&gt;Thanks to @kizu and @pixelambacht on Mastodon for suggesting color theming with &lt;code&gt;override-colors&lt;/code&gt; CSS rule.&lt;/p&gt;
    &lt;p&gt;As said earlier, if you have any ideas, suggestions or feedback, let me know. You can reach me at &lt;code&gt;hlotvonen@gmail.com&lt;/code&gt; or leave a comment on Mastodon.&lt;/p&gt;
    &lt;p&gt;Thanks to all who sent emails, messages and commented!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46364131</guid><pubDate>Tue, 23 Dec 2025 10:28:09 +0000</pubDate></item><item><title>Postponed '60 Minutes' segment on Salvadoran prison is streamed by Canadian news</title><link>https://www.nbcnews.com/news/us-news/cbs-news-el-salvador-cecot-prison-sharyn-alfonsi-bari-weiss-rcna250618</link><description>&lt;doc fingerprint="c8212149da390e3"&gt;
  &lt;main&gt;
    &lt;p&gt;While the furor over CBS News’ decision to delay a planned “60 Minutes” report about deportees sent by the Trump administration to a notorious Salvadoran prison continued Monday, the intended segment was already circulating online, having been streamed in Canada.&lt;/p&gt;
    &lt;p&gt;The report, titled “Inside CECOT,” was streamed by Canada’s Global Television Network. In the U.S., its broadcast was postponed by CBS under its new editor-in-chief, Bari Weiss.&lt;/p&gt;
    &lt;p&gt;It includes interviews from people who were deported from the U.S. to the Center for the Confinement of Terrorism, or CECOT, under the Trump administration. The interviewees described torture and physical and sexual abuse at the complex.&lt;/p&gt;
    &lt;p&gt;“When we got there, the CECOT director was talking to us. The first thing he told us was that we would never see the light of day or night again,” Luis Munoz Pinto, a college student in Venezuela who went to the U.S. to seek asylum, told the TV news magazine.&lt;/p&gt;
    &lt;p&gt;“He said, ‘Welcome to hell. I’ll make sure you never leave,’” said Munoz, who the report noted has since been released.&lt;/p&gt;
    &lt;p&gt;He told the program that he was awaiting a decision on his asylum claim when he was deported to CECOT this year — one of 252 Venezuelans sent there between March and April.&lt;/p&gt;
    &lt;p&gt;Neither CBS nor Global Television Network immediately responded to respective requests for comment late Monday and early Tuesday.&lt;/p&gt;
    &lt;p&gt;The segment featured a clip of President Donald Trump describing El Salvador’s prisons as “great facilities, very strong facilities, and they don’t play games,” while seated next to Salvadoran President Nayib Bukele during a meeting at the White House earlier this year. It also showed Homeland Security Secretary Kristi Noem’s visit to CECOT in March in which she thanked Bukele and El Salvador for their “partnership” with the U.S. to incarcerate what she called “terrorists” at the facility.&lt;/p&gt;
    &lt;p&gt;Neither the White House nor the Department of Homeland Security immediately responded outside regular business hours early Tuesday to emailed requests for comment about the contents of the segment that aired in Canada.&lt;/p&gt;
    &lt;p&gt;“Inside CECOT” was anchored by correspondent Sharyn Alfonsi, who was critical of the decision to delay the segment’s broadcast. In a note to colleagues seen by NBC News, she accused the network of pulling the segment for “political” reasons.&lt;/p&gt;
    &lt;p&gt;In the note, she said it was pulled because the Trump administration refused requests for comment — a standard that she said, if adopted, would amount a government “kill switch” to stop publication of a story.&lt;/p&gt;
    &lt;p&gt;“Our story was screened five times and cleared by both CBS attorneys and Standards and Practices,” Alfonsi wrote in the note.&lt;/p&gt;
    &lt;p&gt;“It is factually correct. In my view, pulling it now, after every rigorous internal check has been met, is not an editorial decision, it is a political one,” she said.&lt;/p&gt;
    &lt;p&gt;Weiss is a former opinion writer and editor at The New York Times who launched the website The Free Press in 2021. Paramount Skydance, which owns CBS, acquired The Free Press and hired Weiss as editor-in-chief of CBS News in October.&lt;/p&gt;
    &lt;p&gt;Critics at the time noted that The Free Press was an online outlet that portrayed itself as being against “ideological narratives,” particularly on the political left, and that Weiss has worked mainly in the opinion sphere.&lt;/p&gt;
    &lt;p&gt;She resigned from the New York Times in 2020 after three years, writing that there was an “illiberal environment” there and that “My own forays into Wrongthink have made me the subject of constant bullying by colleagues who disagree with my views.”&lt;/p&gt;
    &lt;p&gt;Among the critics of the decision to pull Sunday’s “60 Minutes” segment were the free speech nonprofit PEN America and FCC Commissioner Anna M. Gomez, an appointee of former President Joe Biden who called it “deeply alarming.”&lt;/p&gt;
    &lt;p&gt;“CBS journalists, among the best in this country, appropriately made an outreach effort to get the government to weigh in on a deeply reported story out of El Salvador,” Tim Richardson, journalism and disinformation program director at PEN America said in a statement.&lt;/p&gt;
    &lt;p&gt;“Pulling it back at the last minute because the government chose not to respond is an insult not only to the integrity of the journalists but to core principles of independent news gathering,” he said.&lt;/p&gt;
    &lt;p&gt;Weiss in a statement said that the piece was only held, which she said is not unusual.&lt;/p&gt;
    &lt;p&gt;“Holding stories that aren’t ready for whatever reason — that they lack sufficient context, say, or that they are missing critical voices — happens every day in every newsroom. I look forward to airing this important piece when it’s ready,” Weiss said in the statement.&lt;/p&gt;
    &lt;p&gt;In an editorial call Monday morning, Weiss said that “I held a 60 Minutes story because it was not ready,” according to a source.&lt;/p&gt;
    &lt;p&gt;“While the story presented powerful testimony of torture at CECOT, it did not advance the ball — The Times and other outlets have previously done similar work,” she said, according to that source.&lt;/p&gt;
    &lt;p&gt;President Trump ran on a platform of mass deportations, and his administration began deporting people to El Salvador and CECOT in March, citing the previously rarely used Alien Enemies Act of 1798.&lt;/p&gt;
    &lt;p&gt;Among those sent to CECOT was Kilmar Abrego Garcia, who was mistakenly deported there contrary to a judge’s order, and who was subsequently returned to the U.S. and who was ordered released from immigration custody on Dec. 11. On Monday, a federal judge allowed him to remain free while she considers immigration proceedings in Abrego Garcia’s case.&lt;/p&gt;
    &lt;p&gt;The 252 Venezuelan men referred to in the “60 Minutes” report were released from CECOT in July in exchange for the release of 10 Americans held in Venezuela.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46364783</guid><pubDate>Tue, 23 Dec 2025 12:30:21 +0000</pubDate></item></channel></rss>