<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 08 Jan 2026 18:15:38 +0000</lastBuildDate><item><title>Tailscale state file encryption no longer enabled by default</title><link>https://tailscale.com/changelog</link><description>&lt;doc fingerprint="868f307d7f22bbd0"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Tailscale v1.92.5&lt;/head&gt;Update instructions&lt;head rend="h5"&gt;Linux&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;State file encryption and hardware attestation keys are no longer enabled by default.&lt;/item&gt;&lt;item&gt;Failure to load hardware attestation keys no longer prevents the client from starting. This could happen when the TPM device is reset or replaced.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h5"&gt;Windows&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;State file encryption and hardware attestation keys are no longer enabled by default.&lt;/item&gt;&lt;item&gt;Failure to load hardware attestation keys no longer prevents the client from starting. This could happen when the TPM device is reset or replaced.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale container image v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale container image is available. You can download it from Docker Hub or from our GitHub packages repository.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Hardware attestation keys are no longer added to Kubernetes state &lt;code&gt;Secrets&lt;/code&gt;, making it possible to change the Kubernetes node the Tailscale containers are deployed on.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale Kubernetes Operator v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale Kubernetes Operator is available. For guidance on installing and updating, refer to our installation instructions.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Certificate renewal is no longer done as an ARI order by default to avoid renewal failure if ACME account keys are recreated.&lt;/item&gt;&lt;item&gt;Hardware attestation keys are no longer added to Kubernetes state &lt;code&gt;Secrets&lt;/code&gt;, making it possible to change the Kubernetes node the Tailscale Kubernetes Operator is deployed on.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Tailscale tsrecorder v1.92.5&lt;/head&gt;&lt;p&gt;A new release of the Tailscale &lt;code&gt;tsrecorder&lt;/code&gt; is available. You can download it from Docker Hub.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Note: This version contains no changes except for library updates.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46531925</guid><pubDate>Wed, 07 Jan 2026 20:16:50 +0000</pubDate></item><item><title>Kernel bugs hide for 2 years on average. Some hide for 20</title><link>https://pebblebed.com/blog/kernel-bugs</link><description>&lt;doc fingerprint="52dac1dfd8b24a36"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Kernel bugs hide for 2 years on average. Some hide for 20.&lt;/head&gt;
    &lt;p&gt;There are bugs in your kernel right now that won't be found for years. I know because I analyzed 125,183 of them, every bug with a traceable &lt;code&gt;Fixes:&lt;/code&gt; tag in the Linux kernel's 20-year git history.&lt;/p&gt;
    &lt;p&gt;The average kernel bug lives 2.1 years before discovery. But some subsystems are far worse: CAN bus drivers average 4.2 years, SCTP networking 4.0 years. The longest-lived bug in my dataset, a buffer overflow in ethtool, sat in the kernel for 20.7 years. The one which I'll dissect in detail is refcount leak in netfilter, and it lasted 19 years.&lt;/p&gt;
    &lt;p&gt;I built a tool that catches 92% of historical bugs in a held-out test set at commit time. Here's what I learned.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key findings at a glance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;125,183&lt;/cell&gt;
        &lt;cell&gt;Bug-fix pairs with traceable &lt;code&gt;Fixes:&lt;/code&gt; tags&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;123,696&lt;/cell&gt;
        &lt;cell&gt;Valid records after filtering (0 &amp;lt; lifetime &amp;lt; 27 years)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2.1 years&lt;/cell&gt;
        &lt;cell&gt;Average time a bug hides before discovery&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;20.7 years&lt;/cell&gt;
        &lt;cell&gt;Longest-lived bug (ethtool buffer overflow)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;0% â 69%&lt;/cell&gt;
        &lt;cell&gt;Bugs found within 1 year (2010 vs 2022)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;92.2%&lt;/cell&gt;
        &lt;cell&gt;Recall of VulnBERT on held-out 2024 test set&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1.2%&lt;/cell&gt;
        &lt;cell&gt;False positive rate (vs 48% for vanilla CodeBERT)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;The initial discovery&lt;/head&gt;
    &lt;p&gt;I started by mining the most recent 10,000 commits with &lt;code&gt;Fixes:&lt;/code&gt; tags from the Linux kernel. After filtering out invalid references (commits that pointed to hashes outside the repo, malformed tags, or merge commits), I had 9,876 valid vulnerability records. For the lifetime analysis, I excluded 27 same-day fixes (bugs introduced and fixed within hours), leaving 9,849 bugs with meaningful lifetimes.&lt;/p&gt;
    &lt;p&gt;The results were striking:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Bugs analyzed&lt;/cell&gt;
        &lt;cell&gt;9,876&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Average lifetime&lt;/cell&gt;
        &lt;cell&gt;2.8 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Median lifetime&lt;/cell&gt;
        &lt;cell&gt;1.0 year&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Maximum&lt;/cell&gt;
        &lt;cell&gt;20.7 years&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Almost 20% of bugs had been hiding for 5+ years. The networking subsystem looked particularly bad at 5.1 years average. I found a refcount leak in netfilter that had been in the kernel for 19 years.&lt;/p&gt;
    &lt;p&gt;Initial findings: Half of bugs found within a year, but 20% hide for 5+ years.&lt;/p&gt;
    &lt;p&gt;But something nagged at me: my dataset only contained fixes from 2025. Was I seeing the full picture, or just the tip of the iceberg?&lt;/p&gt;
    &lt;head rend="h2"&gt;Going deeper: Mining the full history&lt;/head&gt;
    &lt;p&gt;I rewrote my miner to capture every &lt;code&gt;Fixes:&lt;/code&gt; tag since Linux moved to git in 2005. Six hours later, I had 125,183 vulnerability records which was 12x larger than my initial dataset.&lt;/p&gt;
    &lt;p&gt;The numbers changed significantly:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;2025 Only&lt;/cell&gt;
        &lt;cell role="head"&gt;Full History (2005-2025)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bugs analyzed&lt;/cell&gt;
        &lt;cell&gt;9,876&lt;/cell&gt;
        &lt;cell&gt;125,183&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Average lifetime&lt;/cell&gt;
        &lt;cell&gt;2.8 years&lt;/cell&gt;
        &lt;cell&gt;2.1 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Median lifetime&lt;/cell&gt;
        &lt;cell&gt;1.0 year&lt;/cell&gt;
        &lt;cell&gt;0.7 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;5+ year bugs&lt;/cell&gt;
        &lt;cell&gt;19.4%&lt;/cell&gt;
        &lt;cell&gt;13.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;10+ year bugs&lt;/cell&gt;
        &lt;cell&gt;6.6%&lt;/cell&gt;
        &lt;cell&gt;4.2%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Full history: 57% of bugs found within a year. The long tail is smaller than it first appeared.&lt;/p&gt;
    &lt;p&gt;Why the difference? My initial 2025-only dataset was biased. Fixes in 2025 include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New bugs introduced recently and caught quickly&lt;/item&gt;
      &lt;item&gt;Ancient bugs that finally got discovered after years of hiding&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The ancient bugs skewed the average upward. When you include the full history with all the bugs that were introduced AND fixed within the same year, the average drops from 2.8 to 2.1 years.&lt;/p&gt;
    &lt;head rend="h2"&gt;The real story: We're getting faster (but it's complicated)&lt;/head&gt;
    &lt;p&gt;The most striking finding from the full dataset: bugs introduced in recent years appear to get fixed much faster.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Year Introduced&lt;/cell&gt;
        &lt;cell role="head"&gt;Bugs&lt;/cell&gt;
        &lt;cell role="head"&gt;Avg Lifetime&lt;/cell&gt;
        &lt;cell role="head"&gt;% Found &amp;lt;1yr&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2010&lt;/cell&gt;
        &lt;cell&gt;1,033&lt;/cell&gt;
        &lt;cell&gt;9.9 years&lt;/cell&gt;
        &lt;cell&gt;0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2014&lt;/cell&gt;
        &lt;cell&gt;3,991&lt;/cell&gt;
        &lt;cell&gt;3.9 years&lt;/cell&gt;
        &lt;cell&gt;31%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2018&lt;/cell&gt;
        &lt;cell&gt;11,334&lt;/cell&gt;
        &lt;cell&gt;1.7 years&lt;/cell&gt;
        &lt;cell&gt;54%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2022&lt;/cell&gt;
        &lt;cell&gt;11,090&lt;/cell&gt;
        &lt;cell&gt;0.8 years&lt;/cell&gt;
        &lt;cell&gt;69%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Bugs introduced in 2010 took nearly 10 years to find and bugs introduced in 2024 are found in 5 months. At first glance it looks like a 20x improvement!&lt;/p&gt;
    &lt;p&gt;But here's the catch: this data is right-censored. Bugs introduced in 2022 can't have a 10-year lifetime yet since we're only in 2026. We might find more 2022 bugs in 2030 that bring the average up.&lt;/p&gt;
    &lt;p&gt;The fairer comparison is "% found within 1 year" and that IS improving: from 0% (2010) to 69% (2022). That's real progress, likely driven by:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Syzkaller (released 2015)&lt;/item&gt;
      &lt;item&gt;KASAN, KMSAN, KCSAN sanitizers&lt;/item&gt;
      &lt;item&gt;Better static analysis&lt;/item&gt;
      &lt;item&gt;More contributors reviewing code&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But there's a backlog. When I look at just the bugs fixed in 2024-2025:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;60% were introduced in the last 2 years (new bugs, caught quickly)&lt;/item&gt;
      &lt;item&gt;18% were introduced 5-10 years ago&lt;/item&gt;
      &lt;item&gt;6.5% were introduced 10+ years ago&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We're simultaneously catching new bugs faster AND slowly working through ~5,400 ancient bugs that have been hiding for over 5 years.&lt;/p&gt;
    &lt;head rend="h2"&gt;The methodology&lt;/head&gt;
    &lt;p&gt;The kernel has a convention: when a commit fixes a bug, it includes a &lt;code&gt;Fixes:&lt;/code&gt; tag pointing to the commit that introduced the bug.&lt;/p&gt;
    &lt;code&gt;commit de788b2e6227
Author: Florian Westphal &amp;lt;fw@strlen.de&amp;gt;
Date:   Fri Aug 1 17:25:08 2025 +0200

    netfilter: ctnetlink: fix refcount leak on table dump

    Fixes: d205dc40798d ("netfilter: ctnetlink: ...")
&lt;/code&gt;
    &lt;p&gt;I wrote a miner that:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Runs &lt;code&gt;git log --grep="Fixes:"&lt;/code&gt;to find all fixing commits&lt;/item&gt;
      &lt;item&gt;Extracts the referenced commit hash from the &lt;code&gt;Fixes:&lt;/code&gt;tag&lt;/item&gt;
      &lt;item&gt;Pulls dates from both commits&lt;/item&gt;
      &lt;item&gt;Classifies subsystem from file paths (70+ patterns)&lt;/item&gt;
      &lt;item&gt;Detects bug type from commit message keywords&lt;/item&gt;
      &lt;item&gt;Calculates the lifetime&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;fixes_pattern = r'Fixes:\s*([0-9a-f]{12,40})'
match = re.search(fixes_pattern, commit_message)
if match:
    introducing_hash = match.group(1)
    lifetime_days = (fixing_date - introducing_date).days
&lt;/code&gt;
    &lt;p&gt;Dataset details:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Kernel version&lt;/cell&gt;
        &lt;cell&gt;v6.19-rc3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Mining date&lt;/cell&gt;
        &lt;cell&gt;January 6, 2026&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Fixes mined since&lt;/cell&gt;
        &lt;cell&gt;2005-04-16 (git epoch)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Total records&lt;/cell&gt;
        &lt;cell&gt;125,183&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Unique fixing commits&lt;/cell&gt;
        &lt;cell&gt;119,449&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Unique bug-introducing authors&lt;/cell&gt;
        &lt;cell&gt;9,159&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;With CVE ID&lt;/cell&gt;
        &lt;cell&gt;158&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;With Cc: stable&lt;/cell&gt;
        &lt;cell&gt;27,875 (22%)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Coverage note: The kernel has ~448,000 commits mentioning "fix" in some form, but only ~124,000 (28%) use proper &lt;code&gt;Fixes:&lt;/code&gt; tags. My dataset captures the well-documented bugs aka the ones where maintainers traced the root cause.&lt;/p&gt;
    &lt;head rend="h2"&gt;It varies by subsystem&lt;/head&gt;
    &lt;p&gt;Some subsystems have bugs that persist far longer than others:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Subsystem&lt;/cell&gt;
        &lt;cell role="head"&gt;Bug Count&lt;/cell&gt;
        &lt;cell role="head"&gt;Avg Lifetime&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;drivers/can&lt;/cell&gt;
        &lt;cell&gt;446&lt;/cell&gt;
        &lt;cell&gt;4.2 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;networking/sctp&lt;/cell&gt;
        &lt;cell&gt;279&lt;/cell&gt;
        &lt;cell&gt;4.0 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;networking/ipv4&lt;/cell&gt;
        &lt;cell&gt;1,661&lt;/cell&gt;
        &lt;cell&gt;3.6 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;usb&lt;/cell&gt;
        &lt;cell&gt;2,505&lt;/cell&gt;
        &lt;cell&gt;3.5 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tty&lt;/cell&gt;
        &lt;cell&gt;1,033&lt;/cell&gt;
        &lt;cell&gt;3.5 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;netfilter&lt;/cell&gt;
        &lt;cell&gt;1,181&lt;/cell&gt;
        &lt;cell&gt;2.9 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;networking&lt;/cell&gt;
        &lt;cell&gt;6,079&lt;/cell&gt;
        &lt;cell&gt;2.9 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;memory&lt;/cell&gt;
        &lt;cell&gt;2,459&lt;/cell&gt;
        &lt;cell&gt;1.8 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;gpu&lt;/cell&gt;
        &lt;cell&gt;5,212&lt;/cell&gt;
        &lt;cell&gt;1.4 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;bpf&lt;/cell&gt;
        &lt;cell&gt;959&lt;/cell&gt;
        &lt;cell&gt;1.1 years&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;CAN bus and SCTP bugs persist longest. BPF and GPU bugs get caught fastest.&lt;/p&gt;
    &lt;p&gt;CAN bus drivers and SCTP networking have bugs that persist longest probably because both are niche protocols with less testing coverage. GPU (especially Intel i915) and BPF bugs get caught fastest, probably thanks to dedicated fuzzing infrastructure.&lt;/p&gt;
    &lt;p&gt;Interesting finding from comparing 2025-only vs full history:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Subsystem&lt;/cell&gt;
        &lt;cell role="head"&gt;2025-only Avg&lt;/cell&gt;
        &lt;cell role="head"&gt;Full History Avg&lt;/cell&gt;
        &lt;cell role="head"&gt;Difference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;networking&lt;/cell&gt;
        &lt;cell&gt;5.2 years&lt;/cell&gt;
        &lt;cell&gt;2.9 years&lt;/cell&gt;
        &lt;cell&gt;-2.3 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;filesystem&lt;/cell&gt;
        &lt;cell&gt;3.8 years&lt;/cell&gt;
        &lt;cell&gt;2.6 years&lt;/cell&gt;
        &lt;cell&gt;-1.2 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;drivers/net&lt;/cell&gt;
        &lt;cell&gt;3.3 years&lt;/cell&gt;
        &lt;cell&gt;2.2 years&lt;/cell&gt;
        &lt;cell&gt;-1.1 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;gpu&lt;/cell&gt;
        &lt;cell&gt;1.4 years&lt;/cell&gt;
        &lt;cell&gt;1.4 years&lt;/cell&gt;
        &lt;cell&gt;0 years&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Networking looked terrible in the 2025-only data (5.2 years!) but is actually closer to average in the full history (2.9 years). The 2025 fixes were catching a backlog of ancient networking bugs. GPU looks the same either way, and those bugs get caught consistently fast.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some bug types hide longer than others&lt;/head&gt;
    &lt;p&gt;Race conditions are the hardest to find, averaging 5.1 years to discovery:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Bug Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Count&lt;/cell&gt;
        &lt;cell role="head"&gt;Avg Lifetime&lt;/cell&gt;
        &lt;cell role="head"&gt;Median&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;race-condition&lt;/cell&gt;
        &lt;cell&gt;1,188&lt;/cell&gt;
        &lt;cell&gt;5.1 years&lt;/cell&gt;
        &lt;cell&gt;2.6 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;integer-overflow&lt;/cell&gt;
        &lt;cell&gt;298&lt;/cell&gt;
        &lt;cell&gt;3.9 years&lt;/cell&gt;
        &lt;cell&gt;2.2 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;use-after-free&lt;/cell&gt;
        &lt;cell&gt;2,963&lt;/cell&gt;
        &lt;cell&gt;3.2 years&lt;/cell&gt;
        &lt;cell&gt;1.4 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;memory-leak&lt;/cell&gt;
        &lt;cell&gt;2,846&lt;/cell&gt;
        &lt;cell&gt;3.1 years&lt;/cell&gt;
        &lt;cell&gt;1.4 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;buffer-overflow&lt;/cell&gt;
        &lt;cell&gt;399&lt;/cell&gt;
        &lt;cell&gt;3.1 years&lt;/cell&gt;
        &lt;cell&gt;1.5 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;refcount&lt;/cell&gt;
        &lt;cell&gt;2,209&lt;/cell&gt;
        &lt;cell&gt;2.8 years&lt;/cell&gt;
        &lt;cell&gt;1.3 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;null-deref&lt;/cell&gt;
        &lt;cell&gt;4,931&lt;/cell&gt;
        &lt;cell&gt;2.2 years&lt;/cell&gt;
        &lt;cell&gt;0.7 years&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;deadlock&lt;/cell&gt;
        &lt;cell&gt;1,683&lt;/cell&gt;
        &lt;cell&gt;2.2 years&lt;/cell&gt;
        &lt;cell&gt;0.8 years&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Why do race conditions hide so long? They're non-deterministic and only trigger under specific timing conditions that might occur once per million executions. Even sanitizers like KCSAN can only flag races they observe.&lt;/p&gt;
    &lt;p&gt;30% of bugs are self-fixes where the same person who introduced the bug eventually fixed it. I guess code ownership matters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why some bugs hide longer&lt;/head&gt;
    &lt;p&gt;Less fuzzing coverage. Syzkaller excels at syscall fuzzing but struggles with stateful protocols. Fuzzing netfilter effectively requires generating valid packet sequences that traverse specific connection tracking states.&lt;/p&gt;
    &lt;p&gt;Harder to trigger. Many networking bugs require:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Specific packet sequences&lt;/item&gt;
      &lt;item&gt;Race conditions between concurrent flows&lt;/item&gt;
      &lt;item&gt;Memory pressure during table operations&lt;/item&gt;
      &lt;item&gt;Particular NUMA topologies&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Older code with fewer eyes. Core networking infrastructure like &lt;code&gt;nf_conntrack&lt;/code&gt; was written in the mid-2000s. It works, so nobody rewrites it. But "stable" means fewer developers actively reviewing.&lt;/p&gt;
    &lt;head rend="h2"&gt;Case study: 19 years in the kernel&lt;/head&gt;
    &lt;p&gt;One of the oldest networking bug in my dataset was introduced in August 2006 and fixed in August 2025:&lt;/p&gt;
    &lt;code&gt;// ctnetlink_dump_table() - the buggy code path
if (res &amp;lt; 0) {
    nf_conntrack_get(&amp;amp;ct-&amp;gt;ct_general);  // increments refcount
    cb-&amp;gt;args[1] = (unsigned long)ct;
    break;
}
&lt;/code&gt;
    &lt;p&gt;The irony: Commit &lt;code&gt;d205dc40798d&lt;/code&gt; was itself a fix: "[NETFILTER]: ctnetlink: fix deadlock in table dumping". Patrick McHardy was fixing a deadlock by removing a &lt;code&gt;_put()&lt;/code&gt; call. In doing so, he introduced a refcount leak that would persist for 19 years.&lt;/p&gt;
    &lt;p&gt;The bug: the code doesn't check if &lt;code&gt;ct == last&lt;/code&gt;. If the current entry is the same as the one we already saved, we've now incremented its refcount twice but will only decrement it once. The object never gets freed.&lt;/p&gt;
    &lt;code&gt;// What should have been checked:
if (res &amp;lt; 0) {
    if (ct != last)  // &amp;lt;-- this check was missing for 19 years
        nf_conntrack_get(&amp;amp;ct-&amp;gt;ct_general);
    cb-&amp;gt;args[1] = (unsigned long)ct;
    break;
}
&lt;/code&gt;
    &lt;p&gt;The consequence: Memory leaks accumulate. Eventually &lt;code&gt;nf_conntrack_cleanup_net_list()&lt;/code&gt; waits forever for the refcount to hit zero. The netns teardown hangs. If you're using containers, this blocks container cleanup indefinitely.&lt;/p&gt;
    &lt;p&gt;Why it took 19 years: You had to run &lt;code&gt;conntrack_resize.sh&lt;/code&gt; in a loop for ~20 minutes under memory pressure. The fix commit says: "This can be reproduced by running conntrack_resize.sh selftest in a loop. It takes ~20 minutes for me on a preemptible kernel." Nobody ran that specific test sequence for two decades.&lt;/p&gt;
    &lt;head rend="h2"&gt;Incomplete fixes are common&lt;/head&gt;
    &lt;p&gt;Here's a pattern I keep seeing: someone notices undefined behavior, ships a fix, but the fix doesn't fully close the hole.&lt;/p&gt;
    &lt;p&gt;Case study: netfilter set field validation&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Date&lt;/cell&gt;
        &lt;cell role="head"&gt;Commit&lt;/cell&gt;
        &lt;cell role="head"&gt;What happened&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Jan 2020&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;f3a2181e16f1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Stefano Brivio adds support for sets with multiple ranged fields. Introduces &lt;code&gt;NFTA_SET_DESC_CONCAT&lt;/code&gt; for specifying field lengths.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Jan 2024&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;3ce67e3793f4&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Pablo Neira notices the code doesn't validate that field lengths sum to the key length. Ships a fix. Commit message: "I did not manage to crash nft_set_pipapo with mismatch fields and set key length so far, but this is UB which must be disallowed."&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Jan 2025&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;1b9335a8000f&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Security researcher finds a bypass. The 2024 fix was incompleteâthere were still code paths that could mismatch. Real fix shipped.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The 2024 fix was an acknowledgment that something was wrong, but Pablo couldn't find a crash, so the fix was conservative. A year later, someone found the crash.&lt;/p&gt;
    &lt;p&gt;This pattern suggests a detection opportunity: commits that say things like "this is undefined behavior" or "I couldn't trigger this but..." are flags. The author knows something is wrong but hasn't fully characterized the bug. These deserve extra scrutiny.&lt;/p&gt;
    &lt;head rend="h2"&gt;The anatomy of a long-lived bug&lt;/head&gt;
    &lt;p&gt;Looking at the bugs that survive 10+ years, I see common patterns:&lt;/p&gt;
    &lt;p&gt;1. Reference counting errors&lt;/p&gt;
    &lt;code&gt;kref_get(&amp;amp;obj-&amp;gt;ref);
// ... error path returns without kref_put()
&lt;/code&gt;
    &lt;p&gt;These don't crash immediately. They leak memory slowly. In a long-running system, you might not notice until months later when OOM killer starts firing.&lt;/p&gt;
    &lt;p&gt;2. Missing NULL checks after dereference&lt;/p&gt;
    &lt;code&gt;struct foo *f = get_foo();
f-&amp;gt;bar = 1;              // dereference happens first
if (!f) return -EINVAL;  // check comes too late
&lt;/code&gt;
    &lt;p&gt;The compiler might optimize away the NULL check since you already dereferenced. These survive because the pointer is rarely NULL in practice.&lt;/p&gt;
    &lt;p&gt;3. Integer overflow in size calculations&lt;/p&gt;
    &lt;code&gt;size_t total = n_elements * element_size;  // can overflow
buf = kmalloc(total, GFP_KERNEL);
memcpy(buf, src, n_elements * element_size);  // copies more than allocated
&lt;/code&gt;
    &lt;p&gt;If &lt;code&gt;n_elements&lt;/code&gt; comes from userspace, an attacker can cause allocation of a small buffer followed by a large copy.&lt;/p&gt;
    &lt;p&gt;4. Race conditions in state machines&lt;/p&gt;
    &lt;code&gt;spin_lock(&amp;amp;lock);
if (state == READY) {
    spin_unlock(&amp;amp;lock);
    // window here where another thread can change state
    do_operation();  // assumes state is still READY
}
&lt;/code&gt;
    &lt;p&gt;These require precise timing to hit. They might manifest as rare crashes that nobody can reproduce.&lt;/p&gt;
    &lt;head rend="h2"&gt;Can we catch these bugs automatically?&lt;/head&gt;
    &lt;p&gt;Every day a bug lives in the kernel is another day millions of devices are vulnerable. Android phones, servers, embedded systems, cloud infrastructure, all running kernel code with bugs that won't be found for years.&lt;/p&gt;
    &lt;p&gt;I built VulnBERT, a model that predicts whether a commit introduces a vulnerability.&lt;/p&gt;
    &lt;p&gt;Model evolution:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Recall&lt;/cell&gt;
        &lt;cell role="head"&gt;FPR&lt;/cell&gt;
        &lt;cell role="head"&gt;F1&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Random Forest&lt;/cell&gt;
        &lt;cell&gt;76.8%&lt;/cell&gt;
        &lt;cell&gt;15.9%&lt;/cell&gt;
        &lt;cell&gt;0.80&lt;/cell&gt;
        &lt;cell&gt;Hand-crafted features only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CodeBERT (fine-tuned)&lt;/cell&gt;
        &lt;cell&gt;89.2%&lt;/cell&gt;
        &lt;cell&gt;48.1%&lt;/cell&gt;
        &lt;cell&gt;0.65&lt;/cell&gt;
        &lt;cell&gt;High recall, unusable FPR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;VulnBERT&lt;/cell&gt;
        &lt;cell&gt;92.2%&lt;/cell&gt;
        &lt;cell&gt;1.2%&lt;/cell&gt;
        &lt;cell&gt;0.95&lt;/cell&gt;
        &lt;cell&gt;Best of both approaches&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The problem with vanilla CodeBERT: I first tried fine-tuning CodeBERT directly. Results: 89% recall but 48% false positive rate (measured on the same test set). Unusable, flagging half of all commits.&lt;/p&gt;
    &lt;p&gt;Why so bad? CodeBERT learns shortcuts: "big diff = dangerous", "lots of pointers = risky". These correlations exist in training data but don't generalize. The model pattern-matches on surface features, not actual bug patterns.&lt;/p&gt;
    &lt;p&gt;The VulnBERT approach: Combine neural pattern recognition with human domain expertise.&lt;/p&gt;
    &lt;code&gt;âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ
â                            INPUT: Git Diff                          â
âââââââââââââââââââââââââââââââââ¬ââââââââââââââââââââââââââââââââââââââ
                                â
                âââââââââââââââââ´ââââââââââââââââ
                â¼                               â¼
âââââââââââââââââââââââââââââ   âââââââââââââââââââââââââââââââââââââ
â   Chunked Diff Encoder    â   â   Handcrafted Feature Extractor   â
â   (CodeBERT + Attention)  â   â   (51 engineered features)        â
âââââââââââââââ¬ââââââââââââââ   âââââââââââââââââââ¬ââââââââââââââââââ
              â [768-dim]                         â [51-dim]
              âââââââââââââââââ¬ââââââââââââââââââââ
                              â¼
              âââââââââââââââââââââââââââââââââ
              â     Cross-Attention Fusion    â
              â     "When code looks like X,  â
              â      feature Y matters more"  â
              âââââââââââââââââ¬ââââââââââââââââ
                              â¼
              âââââââââââââââââââââââââââââââââ
              â        Risk Classifier        â
              âââââââââââââââââââââââââââââââââ
&lt;/code&gt;
    &lt;p&gt;Three innovations that drove performance:&lt;/p&gt;
    &lt;p&gt;1. Chunked encoding for long diffs. CodeBERT's 512-token limit truncates most kernel diffs (often 2000+ tokens). I split into chunks, encode each, then use learned attention to aggregate:&lt;/p&gt;
    &lt;code&gt;# Learnable attention over chunks
chunk_attention = nn.Sequential(
    nn.Linear(hidden_size, hidden_size // 4),
    nn.Tanh(),
    nn.Linear(hidden_size // 4, 1)
)
attention_weights = F.softmax(chunk_attention(chunk_embeddings), dim=1)
pooled = (attention_weights * chunk_embeddings).sum(dim=1)
&lt;/code&gt;
    &lt;p&gt;The model learns which chunks matter aka the one with &lt;code&gt;spin_lock&lt;/code&gt; without &lt;code&gt;spin_unlock&lt;/code&gt;, not the boilerplate.&lt;/p&gt;
    &lt;p&gt;2. Feature fusion via cross-attention. Neural networks miss domain-specific patterns. I extract 51 handcrafted features using regex and AST-like analysis of the diff:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Features&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Basic (4)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;lines_added&lt;/code&gt;, &lt;code&gt;lines_removed&lt;/code&gt;, &lt;code&gt;files_changed&lt;/code&gt;, &lt;code&gt;hunks_count&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Memory (3)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;has_kmalloc&lt;/code&gt;, &lt;code&gt;has_kfree&lt;/code&gt;, &lt;code&gt;has_alloc_no_free&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Refcount (5)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;has_get&lt;/code&gt;, &lt;code&gt;has_put&lt;/code&gt;, &lt;code&gt;get_count&lt;/code&gt;, &lt;code&gt;put_count&lt;/code&gt;, &lt;code&gt;unbalanced_refcount&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Locking (5)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;has_lock&lt;/code&gt;, &lt;code&gt;has_unlock&lt;/code&gt;, &lt;code&gt;lock_count&lt;/code&gt;, &lt;code&gt;unlock_count&lt;/code&gt;, &lt;code&gt;unbalanced_lock&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Pointers (4)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;has_deref&lt;/code&gt;, &lt;code&gt;deref_count&lt;/code&gt;, &lt;code&gt;has_null_check&lt;/code&gt;, &lt;code&gt;has_deref_no_null_check&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Error handling (6)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;has_goto&lt;/code&gt;, &lt;code&gt;goto_count&lt;/code&gt;, &lt;code&gt;has_error_return&lt;/code&gt;, &lt;code&gt;has_error_label&lt;/code&gt;, &lt;code&gt;error_return_count&lt;/code&gt;, &lt;code&gt;has_early_return&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Semantic (13)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;var_after_loop&lt;/code&gt;, &lt;code&gt;iterator_modified_in_loop&lt;/code&gt;, &lt;code&gt;list_iteration&lt;/code&gt;, &lt;code&gt;list_del_in_loop&lt;/code&gt;, &lt;code&gt;has_container_of&lt;/code&gt;, &lt;code&gt;has_cast&lt;/code&gt;, &lt;code&gt;cast_count&lt;/code&gt;, &lt;code&gt;sizeof_type&lt;/code&gt;, &lt;code&gt;sizeof_ptr&lt;/code&gt;, &lt;code&gt;has_arithmetic&lt;/code&gt;, &lt;code&gt;has_shift&lt;/code&gt;, &lt;code&gt;has_copy&lt;/code&gt;, &lt;code&gt;copy_count&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Structural (11)&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;if_count&lt;/code&gt;, &lt;code&gt;else_count&lt;/code&gt;, &lt;code&gt;switch_count&lt;/code&gt;, &lt;code&gt;case_count&lt;/code&gt;, &lt;code&gt;loop_count&lt;/code&gt;, &lt;code&gt;ternary_count&lt;/code&gt;, &lt;code&gt;cyclomatic_complexity&lt;/code&gt;, &lt;code&gt;max_nesting_depth&lt;/code&gt;, &lt;code&gt;function_call_count&lt;/code&gt;, &lt;code&gt;unique_functions_called&lt;/code&gt;, &lt;code&gt;function_definitions&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The key bug-pattern features:&lt;/p&gt;
    &lt;code&gt;'unbalanced_refcount': 1,    # kref_get without kref_put â leak
'unbalanced_lock': 1,        # spin_lock without spin_unlock â deadlock
'has_deref_no_null_check': 0,# *ptr without if(!ptr) â null deref
'has_alloc_no_free': 0,      # kmalloc without kfree â memory leak
&lt;/code&gt;
    &lt;p&gt;Cross-attention learns conditional relationships. When CodeBERT sees locking patterns AND &lt;code&gt;unbalanced_lock=1&lt;/code&gt;, that's HIGH risk. Neither signal alone is sufficient, it's the combination.&lt;/p&gt;
    &lt;code&gt;# Feature fusion via cross-attention
feature_embedding = feature_projection(handcrafted_features)  # 51 â 768
attended, _ = cross_attention(
    query=code_embedding,      # What patterns does the code have?
    key=feature_embedding,     # What do the hand-crafted features say?
    value=feature_embedding
)
fused = fusion_layer(torch.cat([code_embedding, attended], dim=-1))
&lt;/code&gt;
    &lt;p&gt;3. Focal loss for hard examples. The training data is imbalanced where most commits are safe. Standard cross-entropy wastes gradient updates on easy examples. Focal loss:&lt;/p&gt;
    &lt;code&gt;Standard loss when p=0.95 (easy):  0.05
Focal loss when p=0.95:            0.000125  (400x smaller)
&lt;/code&gt;
    &lt;p&gt;The model focuses on ambiguous commits: the hard 5% that matter.&lt;/p&gt;
    &lt;p&gt;Impact of each component (estimated from ablation experiments):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;F1 Score&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;CodeBERT baseline&lt;/cell&gt;
        &lt;cell&gt;~76%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;+ Focal loss&lt;/cell&gt;
        &lt;cell&gt;~80%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;+ Feature fusion&lt;/cell&gt;
        &lt;cell&gt;~88%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;+ Contrastive learning&lt;/cell&gt;
        &lt;cell&gt;~91%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Full VulnBERT&lt;/cell&gt;
        &lt;cell&gt;95.4%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: Individual component impacts are approximate; interactions between components make precise attribution difficult.&lt;/p&gt;
    &lt;p&gt;The key insight: neither neural networks nor hand-crafted rules alone achieve the best results. The combination does.&lt;/p&gt;
    &lt;p&gt;Results on temporal validation (train â¤2023, test 2024):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Target&lt;/cell&gt;
        &lt;cell role="head"&gt;Result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Recall&lt;/cell&gt;
        &lt;cell&gt;90%&lt;/cell&gt;
        &lt;cell&gt;92.2% â&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FPR&lt;/cell&gt;
        &lt;cell&gt;&amp;lt;10%&lt;/cell&gt;
        &lt;cell&gt;1.2% â&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Precision&lt;/cell&gt;
        &lt;cell&gt;â&lt;/cell&gt;
        &lt;cell&gt;98.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;F1&lt;/cell&gt;
        &lt;cell&gt;â&lt;/cell&gt;
        &lt;cell&gt;95.4%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;AUC&lt;/cell&gt;
        &lt;cell&gt;â&lt;/cell&gt;
        &lt;cell&gt;98.4%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What these metrics mean:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Recall (92.2%): Of all actual bug-introducing commits, we catch 92.2%. Missing 7.8% of bugs.&lt;/item&gt;
      &lt;item&gt;False Positive Rate (1.2%): Of all safe commits, we incorrectly flag 1.2%. Low FPR = fewer false alarms.&lt;/item&gt;
      &lt;item&gt;Precision (98.7%): Of commits we flag as risky, 98.7% actually are. When we raise an alarm, we're almost always right.&lt;/item&gt;
      &lt;item&gt;F1 (95.4%): Harmonic mean of precision and recall. Single number summarizing overall performance.&lt;/item&gt;
      &lt;item&gt;AUC (98.4%): Area under ROC curve. Measures ranking qualityâhow well the model separates bugs from safe commits across all thresholds.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The model correctly differentiates the same bug at different stages:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Commit&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Risk&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;acf44a2361b8&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fix for UAF in xe_vfio&lt;/cell&gt;
        &lt;cell&gt;12.4% LOW â&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;1f5556ec8b9e&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Introduced the UAF&lt;/cell&gt;
        &lt;cell&gt;83.8% HIGH â&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;What the model sees: The 19-year bug&lt;/head&gt;
    &lt;p&gt;When analyzing the bug-introducing commit &lt;code&gt;d205dc40798d&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;-    if (ct == last) {
-        nf_conntrack_put(&amp;amp;last-&amp;gt;ct_general);  // removed!
-    }
+    if (ct == last) {
+        last = NULL;
         continue;
     }
     if (ctnetlink_fill_info(...) &amp;lt; 0) {
         nf_conntrack_get(&amp;amp;ct-&amp;gt;ct_general);  // still here
&lt;/code&gt;
    &lt;p&gt;Extracted features:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;cell role="head"&gt;Signal&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_count&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;nf_conntrack_get()&lt;/code&gt; present&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;put_count&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;nf_conntrack_put()&lt;/code&gt; was removed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;unbalanced_refcount&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Mismatch detected&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;has_lock&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Uses &lt;code&gt;read_lock_bh()&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;list_iteration&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;Uses &lt;code&gt;list_for_each_prev()&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Model prediction: 72% risk: HIGH&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;unbalanced_refcount&lt;/code&gt; feature fires because &lt;code&gt;_put()&lt;/code&gt; was removed but &lt;code&gt;_get()&lt;/code&gt; remains. Classic refcount leak pattern.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;Dataset limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Only captures bugs with &lt;code&gt;Fixes:&lt;/code&gt;tags (~28% of fix commits). Selection bias: well-documented bugs tend to be more serious.&lt;/item&gt;
      &lt;item&gt;Mainline only, doesn't include stable-branch-only fixes or vendor patches&lt;/item&gt;
      &lt;item&gt;Subsystem classification is heuristic-based (regex on file paths)&lt;/item&gt;
      &lt;item&gt;Bug type detection based on keyword matching in commit messages and many bugs are "unknown" type&lt;/item&gt;
      &lt;item&gt;Lifetime calculation uses author dates, not commit dates, rebasing can skew timestamps&lt;/item&gt;
      &lt;item&gt;Some "bugs" may be theoretical (comments like "fix possible race" without confirmed trigger)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Model limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;92.2% recall is on a held-out 2024 test set, not a guarantee for future bugs&lt;/item&gt;
      &lt;item&gt;Can't catch semantic bugs (logic errors with no syntactic signal)&lt;/item&gt;
      &lt;item&gt;Cross-function blind spots (bug spans multiple files)&lt;/item&gt;
      &lt;item&gt;Training data bias (learns patterns from bugs that were found, novel patterns may be missed)&lt;/item&gt;
      &lt;item&gt;False positives on intentional patterns (init/cleanup in different commits)&lt;/item&gt;
      &lt;item&gt;Tested only on Linux kernel code, may not generalize to other codebases&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Statistical limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Survivorship bias in year-over-year comparisons (recent bugs can't have long lifetimes yet)&lt;/item&gt;
      &lt;item&gt;Correlation â causation for subsystem/bug-type lifetime differences&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What this means: VulnBERT is a triage tool, not a guarantee. It catches 92% of bugs with recognizable patterns. The remaining 8% and novel bug classes still need human review and fuzzing.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;92.2% recall with 1.2% FPR is production-ready. But there's more to do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RL-based exploration: Instead of static pattern matching, train an agent to explore code paths and find bugs autonomously. The current model predicts risk; an RL agent could generate triggering inputs.&lt;/item&gt;
      &lt;item&gt;Syzkaller integration: Use fuzzer coverage as a reward signal. If the model flags a commit and Syzkaller finds a crash in that code path, that's strong positive signal.&lt;/item&gt;
      &lt;item&gt;Subsystem-specific models: Networking bugs have different patterns than driver bugs. A model fine-tuned on netfilter might outperform the general model on netfilter commits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The goal isn't to replace human reviewers but to point them at the 10% of commits most likely to be problematic, so they can focus attention where it matters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Reproducing this&lt;/head&gt;
    &lt;p&gt;The dataset extraction uses the kernel's &lt;code&gt;Fixes:&lt;/code&gt; tag convention. Here's the core logic:&lt;/p&gt;
    &lt;code&gt;def extract_fixes_tag(commit_msg: str) -&amp;gt; Optional[str]:
    """Extract the commit ID from a Fixes: tag"""
    pattern = r'Fixes:\s*([a-f0-9]{12,40})'
    match = re.search(pattern, commit_msg, re.IGNORECASE)
    return match.group(1) if match else None

# Mine all Fixes: tags from git history
git log --since="2005-04-16" --grep="Fixes:" --format="%H"

# For each fixing commit:
#   - Extract introducing commit hash
#   - Get dates from both commits
#   - Calculate lifetime
#   - Classify subsystem from file paths
&lt;/code&gt;
    &lt;p&gt;Full miner code and dataset: github.com/quguanni/kernel-vuln-data&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;125,183 bugs analyzed from 20 years of Linux kernel git history (123,696 with valid lifetimes)&lt;/item&gt;
      &lt;item&gt;Average bug lifetime: 2.1 years (2.8 years in 2025-only data due to survivorship bias in recent fixes)&lt;/item&gt;
      &lt;item&gt;0% â 69% of bugs found within 1 year (2010 vs 2022) (real improvement from better tooling)&lt;/item&gt;
      &lt;item&gt;13.5% of bugs hide for 5+ years (these are the dangerous ones)&lt;/item&gt;
      &lt;item&gt;Race conditions hide longest (5.1 years average)&lt;/item&gt;
      &lt;item&gt;VulnBERT catches 92.2% of bugs on held-out 2024 test set with only 1.2% FPR (98.4% AUC)&lt;/item&gt;
      &lt;item&gt;Dataset: github.com/quguanni/kernel-vuln-data&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you're working on kernel security, vulnerability detection, or ML for code analysis, I'd love to talk: jenny@pebblebed.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46536340</guid><pubDate>Thu, 08 Jan 2026 02:18:09 +0000</pubDate></item><item><title>Open Infrastructure Map</title><link>https://openinframap.org</link><description>&lt;doc fingerprint="6d0a85331c526182"&gt;
  &lt;main&gt;
    &lt;p&gt;You must have Javascript enabled to view Open Infrastructure Map Open Infrastructure Map about stats exports&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46536866</guid><pubDate>Thu, 08 Jan 2026 03:31:12 +0000</pubDate></item><item><title>Go.sum is not a lockfile</title><link>https://words.filippo.io/gosum/</link><description>&lt;doc fingerprint="b6383f772da5a398"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;go.sum Is Not a Lockfile&lt;/head&gt;
    &lt;p&gt;I need everyone to stop looking at &lt;code&gt;go.sum&lt;/code&gt;, especially to analyze dependency graphs. It is not a “lockfile,”1 and it has zero semantic effects on version resolution. There is truly no use case for ever parsing it outside of cmd/go.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;go.sum&lt;/code&gt; is only a local cache for the Go Checksum Database. It’s a map of module versions to their cryptographic hashes. Those versions may or may not be in use; it doesn’t matter to package resolution.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;go.sum&lt;/code&gt; was not even enabled by default in the original modules design, precisely because it has no observable effect on builds!2 Its (important) purpose is exclusively tightening the security story: the Checksum Database ensures the whole ecosystem shares the same contents for a given module version, regardless of how it is downloaded, and &lt;code&gt;go.sum&lt;/code&gt; makes that guarantee local and self-contained.&lt;/p&gt;
    &lt;p&gt;Instead, just look at &lt;code&gt;go.mod&lt;/code&gt;. It lists the precise version at which all dependencies are built. Since Go 1.17 (released August 2021), it includes all transitive dependencies needed to build the main module and its tests.3&lt;/p&gt;
    &lt;p&gt;You can either parse &lt;code&gt;go.mod&lt;/code&gt; with golang.org/x/mod/modfile, run &lt;code&gt;go mod edit -json&lt;/code&gt; to get its JSON representation,4 or parse it according to its specification.&lt;/p&gt;
    &lt;p&gt;This is the end of the Public Service Announcement. Read on for some &lt;code&gt;go.mod&lt;/code&gt; nerdery.&lt;/p&gt;
    &lt;head rend="h2"&gt;Manifests and lockfiles&lt;/head&gt;
    &lt;p&gt;The enduring confusion around &lt;code&gt;go.mod&lt;/code&gt; and &lt;code&gt;go.sum&lt;/code&gt; is due to the fact that most other languages also have two package-related files, but theirs both matter to version resolution. These two files are usually called manifest and lockfile.&lt;/p&gt;
    &lt;p&gt;The manifest (e.g. &lt;code&gt;pyproject.toml&lt;/code&gt;, &lt;code&gt;package.json&lt;/code&gt;, &lt;code&gt;Cargo.toml&lt;/code&gt;) usually lists some dependencies along with potentially complex rules for which versions are supported. These rules usually apply transitively to dependents, making version resolution extremely hard and/or slow in the general case, and sometimes unsolvable. The manifest is not always5 guaranteed to list all direct dependencies, and no automated mechanism ensures your code actually works with e.g. the minimum allowed manifest version of its dependencies.6&lt;/p&gt;
    &lt;p&gt;The lockfile (e.g. &lt;code&gt;uv.lock&lt;/code&gt;, &lt;code&gt;package-lock.json&lt;/code&gt;, &lt;code&gt;Cargo.lock&lt;/code&gt;) is a relatively recent innovation in some ecosystems, and it lists the actual versions used in the most recent build. It is not really human-readable, and is ignored by dependents, allowing the rapid spread of supply-chain attacks.&lt;/p&gt;
    &lt;p&gt;I honestly find the manifest version ranges essentially useless (because the lower bounds are not normally tested and hence largely incorrect, while I never had a use case for complex upper bounds). I also get endlessly confused trying to remember which commands modify the lockfile (and when/why) and which ones respect it.&lt;/p&gt;
    &lt;p&gt;In Go, &lt;code&gt;go.mod&lt;/code&gt; serves as both manifest and lockfile, and more: it lists all dependencies, direct and transitive, and their exact version to be used when the module is the main module. Semantic versioning is assumed, and those versions are also the minimum versions applied to dependents’ module graphs. Different major versions of the same module are considered essentially separate modules.&lt;/p&gt;
    &lt;p&gt;Notice how there is no way to accidentally use a feature introduced in a version that your dependents won’t have. Also, when adding a dependency, you don’t automatically get the latest—potentially untested/compromised—version of all its dependencies. Finally, there can’t be diamond dependency conflicts.&lt;/p&gt;
    &lt;p&gt;All that with a single, human-readable file: &lt;code&gt;go.mod&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;All &lt;code&gt;go&lt;/code&gt; commands take a &lt;code&gt;-mod&lt;/code&gt; flag. If set to &lt;code&gt;mod&lt;/code&gt;, missing dependencies can be added to &lt;code&gt;go.mod&lt;/code&gt; automatically if necessary, and partial manual changes are reconciled. If set to &lt;code&gt;readonly&lt;/code&gt;, those are errors. &lt;code&gt;go mod tidy&lt;/code&gt; and (effectively) &lt;code&gt;go get&lt;/code&gt; default to &lt;code&gt;mod&lt;/code&gt;; all other commands default to &lt;code&gt;readonly&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Go modules truly don’t get enough credit for how much simpler they are compared to the alternatives. In other ecosystems, package resolution time going down below 1s is celebrated (and is indeed an impressive technical achievement given the design’s requirements!). In Go, no one ever noticed package resolution happening, so there is nothing to celebrate.&lt;/p&gt;
    &lt;p&gt;For more ecosystem feature appreciation posts, follow me on Bluesky at @filippo.abyssdomain.expert or on Mastodon at @filippo@abyssdomain.expert.&lt;/p&gt;
    &lt;head rend="h2"&gt;The picture&lt;/head&gt;
    &lt;p&gt;I had a great time at 39c3 during the holidays. The Chaos Communication Congress is a magical place with a very strict photo policy, so it’s pretty hard to convey its atmosphere. This is the best I could do without recognizable humans in the frame. In Fairy Dust we trust!&lt;/p&gt;
    &lt;p&gt;My work is made possible by Geomys, an organization of professional Go maintainers, which is funded by Smallstep, Ava Labs, Teleport, Tailscale, and Sentry. Through our retainer contracts, they ensure the sustainability and reliability of our open source maintenance work and get a direct line to my expertise and that of the other Geomys maintainers. (Learn more in the Geomys announcement.) Here are a few words from some of them!&lt;/p&gt;
    &lt;p&gt;Teleport — For the past five years, attacks and compromises have been shifting from traditional malware and security breaches to identifying and compromising valid user accounts and credentials with social engineering, credential theft, or phishing. Teleport Identity is designed to eliminate weak access patterns through access monitoring, minimize attack surface with access requests, and purge unused permissions via mandatory access reviews.&lt;/p&gt;
    &lt;p&gt;Ava Labs — We at Ava Labs, maintainer of AvalancheGo (the most widely used client for interacting with the Avalanche Network), believe the sustainable maintenance and development of open source cryptographic protocols is critical to the broad adoption of blockchain technology. We are proud to support this necessary and impactful work through our ongoing sponsorship of Filippo and his team.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;In the sense of the word as introduced by the original&lt;/p&gt;&lt;code&gt;Gemfile.lock&lt;/code&gt;, which only locked the selected versions. Most lockfiles these days also include the cryptographic checksums of version contents, which in Go is instead handled by the Go Checksum Database and&lt;code&gt;go.sum&lt;/code&gt;. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I still think it’s important and it was the first thing I remember advocating for when I joined the Go team, because it makes the module cryptographically self-contained, and because the Go Checksum Database transparency story is not great in ephemeral environments like CI. These are security effects, though, not semantic ones. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;These are the only dependencies you care about, even for security. If the main module imports&lt;/p&gt;&lt;code&gt;example.com/mod1/pkg1&lt;/code&gt;and a separate&lt;code&gt;example.com/mod1/pkg2&lt;/code&gt;imports&lt;code&gt;example.com/mod2&lt;/code&gt;, there is no way for&lt;code&gt;example.com/mod2&lt;/code&gt;to affect the build or run code on the developer’s machine, so you don’t need to consider it a dependency. This is actually very powerful, allowing libraries to segregate dependencies (e.g. the AWS SDK) in optional packages, reducing the transitive trust tree of dependents that don’t use that feature. ↩↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Why not&lt;/p&gt;&lt;code&gt;go list -m all&lt;/code&gt;, you ask? Because that prints the whole module graph, which includes modules that don’t contribute to the build3 and are not included in&lt;code&gt;go.mod&lt;/code&gt;. A closer approximation would be&lt;code&gt;go list -f '{{.Module}}' all&lt;/code&gt;, but this command applies the local build constraints, like GOOS/GOARCH. There is an open proposal for a flag to do&lt;code&gt;go.mod&lt;/code&gt;-like resolution in&lt;code&gt;go list&lt;/code&gt;. ↩&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I am told that Cargo enforces it, and some other package managers have a warning. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cargo has an opt-in, unstable, not recommended mode for it. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46537095</guid><pubDate>Thu, 08 Jan 2026 04:10:34 +0000</pubDate></item><item><title>Project Patchouli: Open-source electromagnetic drawing tablet hardware</title><link>https://patchouli.readthedocs.io/en/latest/</link><description>&lt;doc fingerprint="2791b911676f82d7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Project Patchouli&lt;/head&gt;
    &lt;p&gt;Open-Source EMR Pen Tablet Hardware Implementation and Documentation&lt;/p&gt;
    &lt;p&gt;Patchouli is an open-source electro-magnetic drawing tablet hardware implementation, including a coil array, an RF front end built using commercially available parts, and digital signal processing algorithms. The design is compatible with most commercial pens from different vendors, offering an ultra-low-latency pen input experience for your customized hardware projects.&lt;/p&gt;
    &lt;p&gt;In addition, this project aims to provide a comprehensive documentation of the EMR technology, including the mechanism, circuit implementation, signal processing algorithms, and the pen protocol of different product lines from different vendors.&lt;/p&gt;
    &lt;p&gt;Project Code / Hardware Repository: GitLab&lt;/p&gt;
    &lt;head rend="h2"&gt;Updates&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;January 2024, The project started.&lt;/item&gt;
      &lt;item&gt;March 2024, the first small-scale hardware prototype was successfully tested.&lt;/item&gt;
      &lt;item&gt;January 2025, the documentation page was hosted on Read the Docs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Maintainers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Project Lead: Yukidama&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Community&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reaching out to the maintainers: prj.patchouli@gmail.com&lt;/item&gt;
      &lt;item&gt;Join our public Discord Server&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Sponsorship&lt;/head&gt;
    &lt;p&gt;This project is sponsored by the NLnet Foundation NGI Zero Core Fund. Learn more about it here: Project Patchouli&lt;/p&gt;
    &lt;head rend="h2"&gt;License&lt;/head&gt;
    &lt;p&gt;Project Patchouli Documentation by Yukidama and other project members is licensed under Creative Commons Attribution 4.0 International&lt;/p&gt;
    &lt;p&gt;All images and other resource files in this project, unless otherwise specified, are created by the project team and are licensed under the same CC BY 4.0 license.&lt;/p&gt;
    &lt;p&gt;The hardware design is released under the CERN Open Source Hardware License strongly-reciprocal variant, CERN-OHL-S. A copy of the license is provided in the source repository. Additionally, a user guide of the license is provided on ohwr.org.&lt;/p&gt;
    &lt;p&gt;All program code, unless otherwise specified, is licensed under the GPLv3 license.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;This project is under active development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46537489</guid><pubDate>Thu, 08 Jan 2026 05:20:05 +0000</pubDate></item><item><title>A closer look at a BGP anomaly in Venezuela</title><link>https://blog.cloudflare.com/bgp-route-leak-venezuela/</link><description>&lt;doc fingerprint="b6439bf1a821c86b"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;As news unfolds surrounding the U.S. capture and arrest of Venezuelan leader NicolÃ¡s Maduro, a cybersecurity newsletter examined Cloudflare Radar data and took note of a routing leak in Venezuela on January 2.&lt;/p&gt;
      &lt;p&gt;We dug into the data. Since the beginning of December there have been eleven route leak events, impacting multiple prefixes, where AS8048 is the leaker. Although it is impossible to determine definitively what happened on the day of the event, this pattern of route leaks suggests that the CANTV (AS8048) network, a popular Internet Service Provider (ISP) in Venezuela, has insufficient routing export and import policies. In other words, the BGP anomalies observed by the researcher could be tied to poor technical practices by the ISP rather than malfeasance.&lt;/p&gt;
      &lt;p&gt;In this post, weâll briefly discuss Border Gateway Protocol (BGP) and BGP route leaks, and then dig into the anomaly observed and what may have happened to cause it.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Background: BGP route leaks&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;First, letâs revisit what a BGP route leak is. BGP route leaks cause behavior similar to taking the wrong exit off of a highway. While you may still make it to your destination, the path may be slower and come with delays you wouldnât otherwise have traveling on a more direct route.&lt;/p&gt;
      &lt;p&gt;Route leaks were given a formal definition in RFC7908 as âthe propagation of routing announcement(s) beyond their intended scope.â Intended scope is defined using pairwise business relationships between networks. The relationships between networks, which in BGP we represent using Autonomous Systems (ASes), can be one of the following:Â &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;customer-provider: A customer pays a provider network to connect them and their own downstream customers to the rest of the Internet&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;peer-peer: Two networks decide to exchange traffic between one another, to each othersâ customers, settlement-free (without payment)&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;In a customer-provider relationship, the provider will announce all routes to the customer. The customer, on the other hand, will advertise only the routes from their own customers and originating from their network directly.&lt;/p&gt;
      &lt;p&gt;In a peer-peer relationship, each peer will advertise to one another only their own routes and the routes of their downstream customers.Â &lt;/p&gt;
      &lt;p&gt;These advertisements help direct traffic in expected ways: from customers upstream to provider networks, potentially across a single peering link, and then potentially back down to customers on the far end of the path from their providers.Â &lt;/p&gt;
      &lt;p&gt;A valid path would look like the following that abides by the valley-free routing rule:Â &lt;/p&gt;
      &lt;p&gt;A route leak is a violation of valley-free routing where an AS takes routes from a provider or peer and redistributes them to another provider or peer. For example, a BGP path should never go through a âvalleyâ where traffic goes up to a provider, and back down to a customer, and then up to a provider again. There are different types of route leaks defined in RFC7908, but a simple one is the Type 1: Hairpin route leak between two provider networks by a customer.Â &lt;/p&gt;
      &lt;p&gt;In the figure above, AS64505 takes routes from one of its providers and redistributes them to their other provider. This is unexpected, since we know providers should not use their customer as an intermediate IP transit network. AS64505 would become overwhelmed with traffic, as a smaller network with a smaller set of backbone and network links than its providers. This can become very impactful quickly.Â &lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Route leak by AS8048 (CANTV)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Now that we have reminded ourselves what a route leak is in BGP, letâs examine what was hypothesizedÂ in the newsletter post. The post called attention to a few route leak anomalies on Cloudflare Radar involving AS8048. On the Radar page for this leak, we see this information:&lt;/p&gt;
      &lt;p&gt;We see the leaker AS, which is AS8048 â CANTV, Venezuelaâs state-run telephone and Internet Service Provider. We observe that routes were taken from one of their providers AS6762 (Sparkle, an Italian telecom company) and then redistributed to AS52320 (V.tal GlobeNet, a Colombian network service provider). This is definitely a route leak.Â &lt;/p&gt;
      &lt;p&gt;The newsletter suggests âBGP shenanigansâ and posits that such a leak could be exploited to collect intelligence useful to government entities.Â &lt;/p&gt;
      &lt;p&gt;While we canât say with certainty what caused this route leak, our data suggests that its likely cause was more mundane. Thatâs in part because BGP route leaks happen all of the time, and they have always been part of the Internet â most often for reasons that arenât malicious.&lt;/p&gt;
      &lt;p&gt;To understand more, letâs look closer at the impacted prefixes and networks. The prefixes involved in the leak were all originated by AS21980 (Dayco Telecom, a Venezuelan company):&lt;/p&gt;
      &lt;p&gt;The prefixes are also all members of the same 200.74.224.0/20 subnet, as noted by the newsletter author. Much more intriguing than this, though, is the relationship between the originating network AS21980 and the leaking network AS8048: AS8048 is a provider of AS21980.Â &lt;/p&gt;
      &lt;p&gt;The customer-provider relationship between AS8048 and AS21980 is visible in both Cloudflare Radar and bgp.tools AS relationship interference data. We can also get a confidence score of the AS relationship using the monocle tool from BGPKIT, as you see here:Â &lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;âÂ  ~ monocle as2rel 8048 21980
Explanation:
- connected: % of 1813 peers that see this AS relationship
- peer: % where the relationship is peer-to-peer
- as1_upstream: % where ASN1 is the upstream (provider)
- as2_upstream: % where ASN2 is the upstream (provider)&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;Data source: https://data.bgpkit.com/as2rel/as2rel-latest.json.bz2&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;
        &lt;code&gt;ââââââââ¬ââââââââ¬ââââââââââââ¬âââââââ¬âââââââââââââââ¬âââââââââââââââ®
âÂ asn1 â asn2Â  â connectedÂ â peer â as1_upstreamÂ âÂ as2_upstream â
ââââââââ¼ââââââââ¼ââââââââââââ¼âââââââ¼âââââââââââââââ¼âââââââââââââââ¤
â 8048 â 21980 â    9.9% Â  â 0.6%Â â     9.4%Â   Â  â 0.0% Â  Â  Â  Â  â
â°âââââââ´ââââââââ´ââââââââââââ´âââââââ´âââââââââââââââ´âââââââââââââââ¯&lt;/code&gt;
      &lt;/p&gt;
      &lt;p&gt;While only 9.9% of route collectors see these two ASes as adjacent, almost all of the paths containing them reflect AS8048 as an upstream provider for AS21980, meaning confidence is high in the provider-customer relationship between the two.&lt;/p&gt;
      &lt;p&gt;Many of the leaked routes were also heavily prepended with AS8048, meaning it would have been potentially less attractive for routing when received by other networks. Prepending is the padding of an AS more than one time in an outbound advertisement by a customer or peer, to attempt to switch traffic away from a particular circuit to another. For example, many of the paths during the leak by AS8048 looked like this: â52320,8048,8048,8048,8048,8048,8048,8048,8048,8048,23520,1299,269832,21980â.Â &lt;/p&gt;
      &lt;p&gt;You can see that AS8048 has sent their AS multiple times in an advertisement to AS52320, because by means of BGP loop prevention the path would never actually travel in and out of AS8048 multiple times in a row. A non-prepended path would look like this: â52320,8048,23520,1299,269832,21980â.Â &lt;/p&gt;
      &lt;p&gt;If AS8048 was intentionally trying to become a man-in-the-middle (MITM) for traffic, why would they make the BGP advertisement less attractive instead of more attractive? Also, why leak prefixes to try and MITM traffic when youâre already a provider for the downstream AS anyway? That wouldnât make much sense.Â &lt;/p&gt;
      &lt;p&gt;The leaks from AS8048 also surfaced in multiple separate announcements, each around an hour apart on January 2, 2026 between 15:30 and 17:45 UTC, suggesting they may have been having network issues that surfaced in a routing policy issue or a convergence-based mishap.Â &lt;/p&gt;
      &lt;p&gt;It is also noteworthy that these leak events begin over twelve hours prior to the U.S. military strikes in Venezuela. Leaks that impact South American networks are common, and we have no reason to believe, based on timing or the other factors I have discussed, that the leak is related to the capture of Maduro several hours later.&lt;/p&gt;
      &lt;p&gt;In fact, looking back the past two months, we can see plenty of leaks by AS8048 that are just like this one, meaning this is not a new BGP anomaly:&lt;/p&gt;
      &lt;p&gt;You can see above in the history of Cloudflare Radarâs route leak alerting pipeline that AS8048 is no stranger to Type 1 hairpin route leaks. Since the beginning of December alone there have been eleven route leak events where AS8048 is the leaker.&lt;/p&gt;
      &lt;p&gt;From this we can draw a more innocent possible explanation about the route leak: AS8048 may have configured too loose of export policies facing at least one of their providers, AS52320. And because of that, redistributed routes belong to their customer even when the direct customer BGP routes were missing. If their export policy toward AS52320 only matched on IRR-generated prefix list and not a customer BGP community tag, for example, it would make sense why an indirect path toward AS6762 was leaked back upstream by AS8048.Â &lt;/p&gt;
      &lt;p&gt;These types of policy errors are something RFC9234 and the Only-to-Customer (OTC) attribute would help with considerably, by coupling BGP more tightly to customer-provider and peer-peer roles, when supported by all routing vendors. I will save the more technical details on RFC9234 for a follow-up blog post.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;The difference between origin and path validation&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;The newsletter also calls out as ânotableâ that Sparkle (AS6762) does not implement RPKI (Resource Public Key Infrastructure) Route Origin Validation (ROV). While it is true that AS6762 appears to have an incomplete deployment of ROV and is flagged as âunsafeâ on isbgpsafeyet.com because of it, origin validation would not have prevented this BGP anomaly in Venezuela.Â &lt;/p&gt;
      &lt;p&gt;It is important to separate BGP anomalies into two categories: route misoriginations, and path-based anomalies. Knowing the difference between the two helps to understand the solution for each. Route misoriginations, often called BGP hijacks, are meant to be fixed by RPKI Route Origin Validation (ROV) by making sure the originator of a prefix is who rightfully owns it. In the case of the BGP anomaly described in this post, the origin AS was correct as AS21980 and only the path was anomalous. This means ROV wouldnât help here.&lt;/p&gt;
      &lt;p&gt;Knowing that, we need path-based validation. This is what Autonomous System Provider Authorization (ASPA), an upcoming draft standard in the IETF, is going to provide. The idea is similar to RPKI Route Origin Authorizations (ROAs) and ROV: create an ASPA object that defines a list of authorized providers (upstreams) for our AS, and everyone will use this to invalidate route leaks on the Internet at various vantage points. Using a concrete example, AS6762 is a Tier-1 transit-free network, and they would use the special reserved âAS0â member in their ASPA signed object to communicate to the world that they have no upstream providers, only lateral peers and customers. Then, AS52320, the other provider of AS8048, would see routes from their customer with â6762â in the path and reject them by performing an ASPA verification process.&lt;/p&gt;
      &lt;p&gt;ASPA is based on RPKI and is exactly what would help prevent route leaks similar to the one we observed in Venezuela.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;A safer BGP, built togetherÂ &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;We felt it was important to offer an alternative explanation for the BGP route leak by AS8048 in Venezuela that was observed on Cloudflare Radar. It is helpful to understand that route leaks are an expected side effect of BGP historically being based entirely on trust and carefully-executed business relationship-driven intent.Â &lt;/p&gt;
      &lt;p&gt;While route leaks could be done with malicious intent, the data suggests this event may have been an accident caused by a lack of routing export and import policies that would prevent it. This is why to have a safer BGP and Internet we need to work together and drive adoption of RPKI-based ASPA, for which RIPE recently released object creation, on the wide Internet. It will be a collaborative effort, just like RPKI has been for origin validation, but it will be worth it and prevent BGP incidents such as the one in Venezuela.Â &lt;/p&gt;
      &lt;p&gt;In addition to ASPA, we can all implement simpler mechanisms such as Peerlock and Peerlock-lite as operators, which sanity-checks received paths for obvious leaks. One especially promising initiative is the adoption of RFC9234, which should be used in addition to ASPA for preventing route leaks with the establishing of BGP roles and a new Only-To-Customer (OTC) attribute. If you havenât already asked your routing vendors for an implementation of RFC9234 to be on their roadmap: please do. You can help make a big difference.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46538001</guid><pubDate>Thu, 08 Jan 2026 06:46:26 +0000</pubDate></item><item><title>Mothers (YC X26) Is Hiring</title><link>https://jobs.ashbyhq.com/9-mothers</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46540078</guid><pubDate>Thu, 08 Jan 2026 12:00:35 +0000</pubDate></item><item><title>The Jeff Dean Facts</title><link>https://github.com/LRitzdorf/TheJeffDeanFacts</link><description>&lt;doc fingerprint="2f40dcb1cd7a06c7"&gt;
  &lt;main&gt;
    &lt;p&gt;The "Jeff Dean facts" are a set of jokes that revolve around the extraordinary programming prowess of their titular Google employee. Simply put, they are the coding equivalent of Chuck Norris-style jokes (for example, "Chuck Norris can slam a revolving door").&lt;/p&gt;
    &lt;p&gt;I first encountered the Facts on a random Quora page, though as of recently many of them appear to have been removed. Thus, in order to preserve this invaluable cache of programmer humor for posterity, I decided to create this repository. It is a combined list of various versions of the Facts, beginning with a text file that I copied from the Quora post sometime in 2019, when the original answer still existed. It's been expanded from other sources, which are listed (and linked, if possible) at the end of this document.&lt;/p&gt;
    &lt;p&gt;And now, without further ado...&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Jeff Dean proved that P=NP when he solved all NP problems in polynomial time on a whiteboard.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's PIN is the last 4 digits of pi.&lt;/item&gt;
      &lt;item&gt;When Jeff gives a seminar at Stanford, it's so crowded Don Knuth has to sit on the floor. (TRUE)&lt;/item&gt;
      &lt;item&gt;Jeff Dean once bit a spider, the spider got super powers and C++ readability.&lt;/item&gt;
      &lt;item&gt;Once, in early 2002, when the index servers went down, Jeff Dean answered user queries manually for two hours. Evals showed a quality improvement of 5 points.&lt;/item&gt;
      &lt;item&gt;Jeff Dean got promoted to level 11 in a system where max level is 10. (TRUE)&lt;/item&gt;
      &lt;item&gt;Google Search was Jeff Dean's Noogler Project.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has punch card readability.&lt;/item&gt;
      &lt;item&gt;Jeff Dean puts his pants on one leg at a time, but if he had more than two legs, you would see that his approach is actually &lt;code&gt;O(log n)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Jeff Dean acquired Sawzall readability after writing 58 lines of Sawzall code. As part of his readability review, he pointed out a flaw in the style guide which was promptly corrected by the reviewer.&lt;/item&gt;
      &lt;item&gt;Sanjay once asked Jeff Dean if he could keep the entire web in his memory. Due to the noise from his keyboard cooling fan, Jeff Dean misheard slightly and wrote Mustang instead of simply answering "Yes".&lt;/item&gt;
      &lt;item&gt;Jeff Dean compiles and runs his code before submitting, but only to check for compiler and CPU bugs.&lt;/item&gt;
      &lt;item&gt;Unsatisfied with constant time, Jeff Dean created the world's first &lt;code&gt;O(1/n)&lt;/code&gt;algorithm.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has binary readability.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has binary writability.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean goes on vacation, production services across Google mysteriously stop working within a few days. This is actually true.1&lt;/item&gt;
      &lt;item&gt;Jeff Dean once shifted a bit so hard it ended up on another computer.&lt;/item&gt;
      &lt;item&gt;During his own Google interview, Jeff Dean was asked the implications if P=NP were true. He said "P = 0 or N = 1." Then, before the interviewer had even finished laughing, Jeff examined Google's public certificate and wrote the private key on the whiteboard.&lt;/item&gt;
      &lt;item&gt;You use 10% of your brain. The other 90% is running one of Jeff's mapreduce jobs.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's resume lists the things he hasn't done; it's shorter that way.&lt;/item&gt;
      &lt;item&gt;To Jeff Dean, "NP" means "No Problemo".&lt;/item&gt;
      &lt;item&gt;Jeff Dean wrote an &lt;code&gt;O(n^2)&lt;/code&gt;algorithm once. It was for the Traveling Salesman Problem.&lt;/item&gt;
      &lt;item&gt;You don't explain your work to Jeff Dean. Jeff Dean explains your work to you.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's resume has so many accomplishments, it has a table of contents.&lt;/item&gt;
      &lt;item&gt;Jeff Dean was forced to invent asynchronous APIs one day when he optimized a function so that it returned before it was invoked.&lt;/item&gt;
      &lt;item&gt;The rate at which Jeff Dean produces code jumped by a factor of 40 in late 2000 when he upgraded his keyboard to USB2.0.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean designs software, he first codes the binary and then writes the source as documentation.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's Peer Review is what got Larry promoted to CEO.&lt;/item&gt;
      &lt;item&gt;When God said: "Let there be light!", Jeff Dean was there to do the code review.&lt;/item&gt;
      &lt;item&gt;When Graham Bell invented the telephone, he saw a missed call from Jeff Dean&lt;/item&gt;
      &lt;item&gt;Compilers don't warn Jeff Dean. Jeff Dean warns compilers.&lt;/item&gt;
      &lt;item&gt;Jeff Dean doesn't exist, he's actually an advanced AI created by Jeff Dean.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's IDE doesn't do code analysis, it does code appreciation.&lt;/item&gt;
      &lt;item&gt;Jeff Dean doesn't use ECC memory. He anticipates cosmic rays and uses them to improve performance.&lt;/item&gt;
      &lt;item&gt;Jeff Dean once failed a Turing test when he correctly identified the 203rd Fibonacci number in less than a second.&lt;/item&gt;
      &lt;item&gt;Jeff Dean invented Bigtable so that he would have a place to send his weekly snippets.&lt;/item&gt;
      &lt;item&gt;On the zeroth day, Jeff Dean created God.&lt;/item&gt;
      &lt;item&gt;Jeff Dean once implemented a web server in a single printf() call. Other engineers added thousands of lines of explanatory comments but still don't understand exactly how it works. Today that program is known as Google Web Server.&lt;/item&gt;
      &lt;item&gt;When Jeff has an ergonomic evaluation, it is for the protection of his keyboard.&lt;/item&gt;
      &lt;item&gt;Jeff Dean can beat you at connect four. In three moves.&lt;/item&gt;
      &lt;item&gt;Jeff Dean invented BigTable because his resume was too big to fit anywhere else.&lt;/item&gt;
      &lt;item&gt;Jeff Dean took the bite out of Apple's logo.&lt;/item&gt;
      &lt;item&gt;Chuck Norris can kill you. Jeff Dean can &lt;code&gt;kill -9&lt;/code&gt;you.&lt;/item&gt;
      &lt;item&gt;Jeff Dean can parse HTML with a regular expression...correctly.&lt;/item&gt;
      &lt;item&gt;When Jeff has trouble sleeping, he MapReduces sheep.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean fires up the profiler, loops unroll themselves in fear.&lt;/item&gt;
      &lt;item&gt;When your code has undefined behavior, you get a seg fault and corrupted data. When Jeff Dean's code has undefined behavior, a unicorn rides in on a rainbow and gives everybody free ice cream.&lt;/item&gt;
      &lt;item&gt;Jeff doesn't sleep, he just sends SIGSUSPEND to the universe.&lt;/item&gt;
      &lt;item&gt;Jeff got Java readability with only 8 lines of code.&lt;/item&gt;
      &lt;item&gt;Jeff Dean can instantiate abstract classes.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gcc -O4&lt;/code&gt;sends your code to Jeff Dean for a complete rewrite.&lt;/item&gt;
      &lt;item&gt;Jeff can recite 20,000 digits of pi in 5 hours. He doesn't remember them; he just recomputes them on the fly using only &lt;code&gt;O(log n)&lt;/code&gt;space.&lt;/item&gt;
      &lt;item&gt;Jeff Dean remembers only one password. For each site, he concatenates it with the site name, computes its SHA-256 hash, and then types the result.&lt;/item&gt;
      &lt;item&gt;Jeff Dean is still waiting for mathematicians to discover the joke he hid in the digits of pi.&lt;/item&gt;
      &lt;item&gt;There is no &lt;code&gt;Ctrl&lt;/code&gt;key on Jeff Dean's keyboard. Jeff Dean is always in control.&lt;/item&gt;
      &lt;item&gt;Jeff Dean was born on December 31, 1969 at 11:48 PM. It took him twelve minutes to implement his first time counter.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean says "Hello World", the world says "Hello Jeff".&lt;/item&gt;
      &lt;item&gt;Jeff Dean can get 1s out of &lt;code&gt;/dev/zero&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Jeff Dean simply walks into Mordor.&lt;/item&gt;
      &lt;item&gt;Jeff Dean spent some 20% time on an AI project. That produced Urs Hoelzle.&lt;/item&gt;
      &lt;item&gt;Google once had to move out of a datacenter after Jeff Dean accidentally compressed the index so densely that a black hole was formed.&lt;/item&gt;
      &lt;item&gt;Jeff starts his programming sessions with &lt;code&gt;cat &amp;gt; /dev/mem&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The speed of light in a vacuum used to be about 35 mph. Then Jeff Dean spent a weekend optimizing physics.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean sends you a code review, it's because he thinks there's something in it you should learn.&lt;/item&gt;
      &lt;item&gt;Jeff Dean does not &lt;code&gt;sleep()&lt;/code&gt;, he&lt;code&gt;wait()&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;Jeff Dean invented MapReduce so he could sort his fan mail.&lt;/item&gt;
      &lt;item&gt;Once Jeff Dean ordered a list, and the list obeyed him.&lt;/item&gt;
      &lt;item&gt;Chuck Norris is Jeff Dean's 20% project.&lt;/item&gt;
      &lt;item&gt;When your code is killed by SIGJEFF, it never runs again.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's calendar goes straight from March 31st to April 2nd; no one fools Jeff Dean.&lt;/item&gt;
      &lt;item&gt;Jeff Dean never has the wrong number; you have the wrong phone.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has exactly two keys on his keyboard: &lt;code&gt;0&lt;/code&gt;and&lt;code&gt;1&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Errors treat Jeff Dean as a warning.&lt;/item&gt;
      &lt;item&gt;Cricket matches used to take 5 days, until Jeff optimized them.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's watch displays seconds since January 1st, 1970. He is never late.&lt;/item&gt;
      &lt;item&gt;Jeff's code is so fast the assembly code needs three HALT opcodes to stop it.&lt;/item&gt;
      &lt;item&gt;Emacs' preferred editor is Jeff Dean.&lt;/item&gt;
      &lt;item&gt;Google: it's basically a Jeff Dean's side project.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has to unoptimize his code so that reviewers believe it was written by a human.&lt;/item&gt;
      &lt;item&gt;Websearch is just a large unittest Jeff wrote for his real app.&lt;/item&gt;
      &lt;item&gt;Jeff Dean doesn't need speakers or headphones. He just types &lt;code&gt;cat *.mp3&lt;/code&gt;, glances at the screen, and his brain decodes the music in the background while he works.&lt;/item&gt;
      &lt;item&gt;Jeff Dean has Perl Readability. (TRUE)&lt;/item&gt;
      &lt;item&gt;Jeff Dean quicksorts his laundry.&lt;/item&gt;
      &lt;item&gt;The OR ELSE construct had to be removed from ISO C after Jeff Dean used it in Mustang and kernels started panicking in terror.&lt;/item&gt;
      &lt;item&gt;Jeff Dean is not afraid of evil constructors. They are afraid of him.&lt;/item&gt;
      &lt;item&gt;Jeff Dean doesn't write bugs, just features you are unable to understand.&lt;/item&gt;
      &lt;item&gt;Jeff Dean eschews both Emacs and VI. He types his code into &lt;code&gt;zcat&lt;/code&gt;, because it's faster that way.&lt;/item&gt;
      &lt;item&gt;When Jeff Dean sends an Ethernet frame, there are no collisions because the competing frames retreat back up into the buffer memory on their source network cards.&lt;/item&gt;
      &lt;item&gt;Jeff once simultaneously reduced all binary sizes by 3% and raised the severity of a previously known low-priority Python bug to critical-priority in a single change that contained no Python code.&lt;/item&gt;
      &lt;item&gt;One day, Jeff Dean grabbed his Etch-a-Sketch instead of his laptop on his way out the door. On his way back home to get his real laptop, he programmed the Etch-a-Sketch to play Tetris.&lt;/item&gt;
      &lt;item&gt;The x86-64 spec includes several undocumented instructions marked private use. They are actually for Jeff Dean's use.&lt;/item&gt;
      &lt;item&gt;Knuth mailed a copy of The Art of Computer Programming to Google. Jeff Dean autographed it and mailed it back.&lt;/item&gt;
      &lt;item&gt;When he heard that Jeff Dean's autobiography would be exclusive to the platform, Richard Stallman bought a Kindle.&lt;/item&gt;
      &lt;item&gt;Jeff Dean can losslessly compress random data.&lt;/item&gt;
      &lt;item&gt;When asked if the facts about him are true, Jeff Dean responded with "111111". While the interviewer was still trying to figure out what he meant, he clarified, "every single bit of it is true."&lt;/item&gt;
      &lt;item&gt;Jeff Dean mines bitcoins. In his head.&lt;/item&gt;
      &lt;item&gt;Jeff Dean traps the KILL signal.&lt;/item&gt;
      &lt;item&gt;Jeff Dean's programs don't SEGFAULT. The memory rearranges itself in order to put data and code where it belongs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This list was compiled from several sources, with duplicate (or near-duplicate) Facts removed. The relevant sources are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Quora - the original question seems to still exist, at https://www.quora.com/What-are-all-the-Jeff-Dean-facts, but the exact response I downloaded all those years ago is nowhere to be found.&lt;/item&gt;
      &lt;item&gt;infO(N), a Bulgarian website that appears to provide schedules and results for coding competitions (post)&lt;/item&gt;
      &lt;item&gt;A now-deleted Google+ thread, quoted by a Reddit user&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It's not clear whether this fact is really true, or whether this line is simply part of the joke, so I've omitted the usual &lt;code&gt;(TRUE)&lt;/code&gt;identifier here. Interpret this as you see fit :)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46540498</guid><pubDate>Thu, 08 Jan 2026 13:02:05 +0000</pubDate></item><item><title>Show HN: DeepDream for Video with Temporal Consistency</title><link>https://github.com/jeremicna/deepdream-video-pytorch</link><description>&lt;doc fingerprint="3f194f592fcd7ab8"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a fork of neural-dream, a PyTorch implementation of DeepDream. This fork introduces optical flow estimation and occlusion masking to apply DeepDream to videos with temporal consistency.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Standard DeepDream: The original single-image implementation.&lt;/item&gt;
      &lt;item&gt;Video DeepDream: New CLI (&lt;code&gt;video_dream.py&lt;/code&gt;) that uses RAFT Optical Flow to warp previous dream frames into the current frame, ensuring smooth transitions and object tracking.&lt;/item&gt;
      &lt;item&gt;Occlusion Masking: Automatically detects when objects move in front of one another to prevent "ghosting" artifacts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head class="px-3 py-2"&gt;mallard_demo.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;highway_demo.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;mallard_independent_demo.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;highway_independent_demo.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;mallard.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;highway.mp4&lt;/head&gt;
    &lt;p&gt;This project requires the following key packages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PyTorch&lt;/item&gt;
      &lt;item&gt;torchvision&lt;/item&gt;
      &lt;item&gt;OpenCV&lt;/item&gt;
      &lt;item&gt;NumPy&lt;/item&gt;
      &lt;item&gt;Pillow&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Install Dependencies:&lt;/p&gt;
    &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;
    &lt;p&gt;Download Models: Run the download script to fetch the standard Inception/GoogLeNet models:&lt;/p&gt;
    &lt;code&gt;python models/download_models.py&lt;/code&gt;
    &lt;p&gt;To download all compatible models:&lt;/p&gt;
    &lt;code&gt;python models/download_models.py -models all-caffe-googlenet&lt;/code&gt;
    &lt;p&gt;To dream on a video, use the &lt;code&gt;video_dream.py&lt;/code&gt; script. This wrapper accepts specific video arguments and any argument accepted by the standard image dreamer (e.g., layers, octaves, iterations).&lt;/p&gt;
    &lt;p&gt;Basic Video Command:&lt;/p&gt;
    &lt;code&gt;python video_dream.py -content_video input.mp4 -output_video output.mp4 -num_iterations 1&lt;/code&gt;
    &lt;p&gt;Note: For video processing, we recommend using &lt;code&gt;-num_iterations 1&lt;/code&gt;. The temporal consistency from optical flow means each frame builds on the previous dream, so fewer iterations per frame are needed compared to single images.&lt;/p&gt;
    &lt;p&gt;Video-Specific Arguments:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Argument&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-content_video&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;input.mp4&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to the source video file.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-output_video&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;output.mp4&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path where the final video will be saved.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-blend&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;0.5&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;(0.0 - 1.0): Mix ratio between the raw video frame and the warped previous dream. Higher values (closer to 1.0) use more of the raw frame; lower values (closer to 0.0) preserve more of the previous hallucinations.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-independent&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Flag: If set, disables temporal consistency (Optical Flow). Every frame is dreamed on independently (causes flickering).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-update_interval&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;5&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Updates the output video file on disk every N frames (allows you to preview progress while running).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-temp_dir&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;temp&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Directory to store extracted frames, flow data, and masks during processing.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;-keep_temp&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Flag: If set, the temporary directory is not deleted after processing finishes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;-verbose&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;False&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Flag: Enable detailed logs (prints DeepDream iteration logs for every frame).&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All of the following arguments are from the single frame implementation, and you can mix and match any of these with the video-specific arguments above. Refer to neural-dream for more information on single frame parameters.&lt;/p&gt;
    &lt;p&gt;Example combining video and standard args:&lt;/p&gt;
    &lt;code&gt;python video_dream.py -content_video test.mp4 -dream_layers inception_4d -num_iterations 1 -octave_scale 0.7 -image_size 512&lt;/code&gt;
    &lt;p&gt;For single image processing only:&lt;/p&gt;
    &lt;code&gt;python neural_dream.py -content_image &amp;lt;image.jpg&amp;gt; -dream_layers inception_4c -num_iterations 10&lt;/code&gt;
    &lt;p&gt;Note: Paths to images should not contain the &lt;code&gt;~&lt;/code&gt; character; use relative or absolute paths.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-image_size&lt;/code&gt;: Maximum side length (in pixels) of the generated image. Default is 512.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-gpu&lt;/code&gt;: Zero-indexed ID of the GPU to use; for CPU mode set&lt;code&gt;-gpu&lt;/code&gt;to&lt;code&gt;c&lt;/code&gt;; for MPS mode (Apple Silicon) set&lt;code&gt;-gpu&lt;/code&gt;to&lt;code&gt;mps&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-dream_weight&lt;/code&gt;: How much to weight DeepDream. Default is&lt;code&gt;1e3&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-tv_weight&lt;/code&gt;: Weight of total-variation (TV) regularization; helps smooth the image. Default&lt;code&gt;0&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-l2_weight&lt;/code&gt;: Weight of latent state regularization. Default&lt;code&gt;0&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-num_iterations&lt;/code&gt;: Number of iterations. Default is&lt;code&gt;10&lt;/code&gt;. For video, use&lt;code&gt;1&lt;/code&gt;(temporal consistency reduces the need for multiple iterations per frame).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-init&lt;/code&gt;: Initialization method:&lt;code&gt;image&lt;/code&gt;(content image) or&lt;code&gt;random&lt;/code&gt;(noise). Default&lt;code&gt;image&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-jitter&lt;/code&gt;: Apply jitter to image. Default&lt;code&gt;32&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-layer_sigma&lt;/code&gt;: Apply gaussian blur to image. Default&lt;code&gt;0&lt;/code&gt;(disabled).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-optimizer&lt;/code&gt;:&lt;code&gt;lbfgs&lt;/code&gt;or&lt;code&gt;adam&lt;/code&gt;. Default&lt;code&gt;adam&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-learning_rate&lt;/code&gt;: Learning rate (step size). Default&lt;code&gt;1.5&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-normalize_weights&lt;/code&gt;: Divide dream weights by the number of channels.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-loss_mode&lt;/code&gt;: Loss mode:&lt;code&gt;bce&lt;/code&gt;,&lt;code&gt;mse&lt;/code&gt;,&lt;code&gt;mean&lt;/code&gt;,&lt;code&gt;norm&lt;/code&gt;, or&lt;code&gt;l2&lt;/code&gt;. Default&lt;code&gt;l2&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-output_image&lt;/code&gt;: Name of the output image. Default&lt;code&gt;out.png&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-output_start_num&lt;/code&gt;: Number to start output image names at. Default&lt;code&gt;1&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-print_iter&lt;/code&gt;: Print progress every N iterations.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-save_iter&lt;/code&gt;: Save image every N iterations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-dream_layers&lt;/code&gt;: Comma-separated list of layer names to use.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-channels&lt;/code&gt;: Comma-separated list of channels to use.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-channel_mode&lt;/code&gt;: Selection mode:&lt;code&gt;all&lt;/code&gt;,&lt;code&gt;strong&lt;/code&gt;,&lt;code&gt;avg&lt;/code&gt;,&lt;code&gt;weak&lt;/code&gt;, or&lt;code&gt;ignore&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-channel_capture&lt;/code&gt;:&lt;code&gt;once&lt;/code&gt;or&lt;code&gt;octave_iter&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-num_octaves&lt;/code&gt;: Number of octaves per iteration. Default&lt;code&gt;4&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-octave_scale&lt;/code&gt;: Resize value. Default&lt;code&gt;0.6&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-octave_iter&lt;/code&gt;: Iterations (steps) per octave. Default&lt;code&gt;50&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-octave_mode&lt;/code&gt;:&lt;code&gt;normal&lt;/code&gt;,&lt;code&gt;advanced&lt;/code&gt;,&lt;code&gt;manual_max&lt;/code&gt;,&lt;code&gt;manual_min&lt;/code&gt;, or&lt;code&gt;manual&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-lap_scale&lt;/code&gt;: Number of layers in laplacian pyramid. Default&lt;code&gt;0&lt;/code&gt;(disabled).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-sigma&lt;/code&gt;: Strength of gaussian blur in pyramids. Default&lt;code&gt;1&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-zoom&lt;/code&gt;: Amount to zoom in.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-zoom_mode&lt;/code&gt;:&lt;code&gt;percentage&lt;/code&gt;or&lt;code&gt;pixel&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-tile_size&lt;/code&gt;: Desired tile size. Default&lt;code&gt;0&lt;/code&gt;(disabled).&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-overlap_percent&lt;/code&gt;: Percentage of overlap for tiles. Default&lt;code&gt;50&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;-original_colors&lt;/code&gt;: Set to&lt;code&gt;1&lt;/code&gt;to keep content image colors.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-model_file&lt;/code&gt;: Path to&lt;code&gt;.pth&lt;/code&gt;file. Default is VGG-19.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-model_type&lt;/code&gt;:&lt;code&gt;caffe&lt;/code&gt;,&lt;code&gt;pytorch&lt;/code&gt;,&lt;code&gt;keras&lt;/code&gt;, or&lt;code&gt;auto&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-backend&lt;/code&gt;:&lt;code&gt;nn&lt;/code&gt;,&lt;code&gt;cudnn&lt;/code&gt;,&lt;code&gt;openmp&lt;/code&gt;, or&lt;code&gt;mkl&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;-cudnn_autotune&lt;/code&gt;: Use built-in cuDNN autotuner (slower start, faster run).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Problem: The program runs out of memory (OOM) Solution:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reduce &lt;code&gt;-image_size&lt;/code&gt;(e.g., to 512 or 256).&lt;/item&gt;
      &lt;item&gt;If using GPU, use &lt;code&gt;-backend cudnn&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;For video: Reduce the input video resolution before processing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Problem: Video processing is very slow Solution: Video DeepDreaming is computationally expensive. It runs the full DeepDream process per frame, plus Optical Flow calculations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use &lt;code&gt;-num_iterations 1&lt;/code&gt;(recommended for video; temporal consistency means fewer iterations are needed).&lt;/item&gt;
      &lt;item&gt;Reduce &lt;code&gt;-octave_iter&lt;/code&gt;(e.g., to 10 or 20).&lt;/item&gt;
      &lt;item&gt;Use a smaller &lt;code&gt;-image_size&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By default, &lt;code&gt;neural-dream&lt;/code&gt; uses the &lt;code&gt;nn&lt;/code&gt; backend.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Use cuDNN: &lt;code&gt;-backend cudnn&lt;/code&gt;(GPU only, reduces memory).&lt;/item&gt;
      &lt;item&gt;Reduce Size: &lt;code&gt;-image_size 256&lt;/code&gt;(Halves memory usage).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With default settings, standard execution uses ~1.3 GB GPU memory.&lt;/p&gt;
    &lt;p&gt;You can use multiple devices with &lt;code&gt;-gpu&lt;/code&gt; and &lt;code&gt;-multidevice_strategy&lt;/code&gt;.
Example: &lt;code&gt;-gpu 0,1,2,3 -multidevice_strategy 3,6,12&lt;/code&gt; splits layers across 4 GPUs. See ProGamerGov/neural-dream for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46540660</guid><pubDate>Thu, 08 Jan 2026 13:21:59 +0000</pubDate></item><item><title>Maine company in the spotlight after Maduro apparently wore one of their hoodies</title><link>https://www.boston.com/news/business/2026/01/06/maine-company-maduro-venezuela-hoodie/</link><description>&lt;doc fingerprint="26791b55a0c12eff"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;Sign up for the Today newsletter&lt;/head&gt;&lt;p&gt;Get everything you need to know to start your day, delivered right to your inbox every morning.&lt;/p&gt;&lt;p&gt;By Abby Patkin&lt;/p&gt;&lt;p&gt;The CEO of a Maine apparel company said his phone “blew up” after deposed Venezuelan leader Nicolás Maduro was apparently photographed wearing one of their hoodies upon arriving in New York over the weekend.&lt;/p&gt;&lt;p&gt;The U.S. captured Maduro, Venezuela’s president, and his wife in a staggering nighttime military operation Saturday, charging the ousted leader with drug and weapons offenses. Before long, social media was flush with images that appeared to show Maduro wearing an ORIGIN hoodie in the shade “Patriot Blue,” surrounded by Drug Enforcement Administration agents.&lt;/p&gt;&lt;p&gt;In one photo, Maduro appears to be giving the camera a double thumbs up.&lt;/p&gt;&lt;p&gt;“I had to start putting the pieces together: Why is this dude wearing an Origin Patriot Blue hoodie?” Pete Roberts, the company’s founder and CEO, said in a video Sunday. “And the irony in this is that this wave, this logo here on the shirt Maduro is wearing, this is the ‘Wave of Freedom.’”&lt;/p&gt;&lt;p&gt;Farmington-based ORIGIN began as a way to help revitalize a struggling New England manufacturing community, he explained, and its “Wave of Freedom” logo represents a commitment to building back.&lt;/p&gt;&lt;p&gt;Roberts also offered his theory on how Maduro came to be wearing a hoodie made by a smaller brand from Maine.&lt;/p&gt;&lt;p&gt;“Probably a DEA agent slipped this hoodie on him and said, ‘You’re going to feel the fabric of freedom on American soil,’” he quipped. “That’s my assumption, and I’m taking the liberty to assume.”&lt;/p&gt;&lt;p&gt;Writing on Facebook, ORIGIN co-founder and retired Navy SEAL Jocko Willink further noted the brand has supporters “in every branch of service and every agency of the government.”&lt;/p&gt;&lt;p&gt;According to ORIGIN’s product description, the hoodie offers a “triple chill effect” to wick away moisture and cool athletes down — an element Roberts said he found “really curious and interesting,” given the chilly weather in New York.&lt;/p&gt;&lt;p&gt;“So maybe they wanted him to feel comfortable or a little uncomfortable. I’m not quite sure,” Roberts said. “But he definitely gave two thumbs up, so I think he liked the fabric.”&lt;/p&gt;&lt;p&gt;Still, he told News Center Maine ORIGIN isn’t looking to politicize the Maduro photo op and is “just trying to use it for brand awareness and to get people back into our store.”&lt;/p&gt;&lt;p&gt;The brand’s website traffic jumped about 300% Sunday, and sales were up roughly 200%, Roberts told the news outlet.&lt;/p&gt;&lt;p&gt;“It would be really hard for a company out of Maine to get, let’s call it, a billion eyes on our brand,” he said. “And so, that’s a real positive as a brand, as a movement. We would never have been able to create that.”&lt;/p&gt;&lt;p&gt;Abby Patkin is a general assignment news reporter whose work touches on public transit, crime, health, and everything in between.&lt;/p&gt;&lt;p&gt;Get everything you need to know to start your day, delivered right to your inbox every morning.&lt;/p&gt;&lt;p&gt;Be civil. Be kind.&lt;/p&gt;Read our full community guidelines.&lt;p&gt;Stay up to date with everything Boston. Receive the latest news and breaking updates, straight from our newsroom to your inbox.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46541586</guid><pubDate>Thu, 08 Jan 2026 14:44:24 +0000</pubDate></item><item><title>Bose is open-sourcing its old smart speakers instead of bricking them</title><link>https://www.theverge.com/news/858501/bose-soundtouch-smart-speakers-open-source</link><description>&lt;doc fingerprint="f3603ffad3a2806c"&gt;
  &lt;main&gt;
    &lt;p&gt;In a surprisingly user-friendly move, Bose has announced it will be open-sourcing the API documentation for its SoundTouch smart speakers, which were slated to lose official support on February 18th, as reported by Ars Technica. Bose has also moved that date back to May 6th, 2026.&lt;/p&gt;
    &lt;head rend="h1"&gt;Bose is open-sourcing its old smart speakers instead of bricking them&lt;/head&gt;
    &lt;p&gt;SoundTouch speakers could now have a second life and won’t lose support until May.&lt;/p&gt;
    &lt;p&gt;SoundTouch speakers could now have a second life and won’t lose support until May.&lt;/p&gt;
    &lt;p&gt;When cloud support ends, an update to the SoundTouch app will add local controls to retain as much functionality as possible without cloud services. Users will still be able to stream music to SoundTouch speakers with Bluetooth, AirPlay, and Spotify Connect (plus physical AUX connections). Remote control features and grouping speakers will also continue to work, and users will still be able to set up and configure their SoundTouch speakers.&lt;/p&gt;
    &lt;p&gt;Now that the smart speakers’ API is being open-sourced, users can also create their own compatible SoundTouch tools to help fill in any gaps left by the lack of cloud services. While it’s still disappointing that the speakers are losing official support, Bose’s approach at least lets people continue using their speakers, rather than bricking otherwise functional devices.&lt;/p&gt;
    &lt;p&gt;This move from Bose is particularly surprising because of how rare it is. Usually when products lose support for cloud services, they end up bricked, and occasionally users step in themselves to fix things. For instance, when Pebble originally shut down in 2016, users kept their watches functional by creating the Rebble Alliance, a community-run replacement for the watches’ cloud services, firmware, and app store.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46541892</guid><pubDate>Thu, 08 Jan 2026 15:07:57 +0000</pubDate></item><item><title>Japanese electronics store pleads for old PCs amid ongoing hardware shortage</title><link>https://www.tomshardware.com/desktops/pc-building/major-japanese-electronics-store-begs-customers-for-their-old-pcs-as-hardware-drought-continues-we-pretty-much-buy-any-pc-pleads-the-akihabara-outlet</link><description>&lt;doc fingerprint="7eb91d9c38b2a9cf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Major Japanese electronics store begs customers for their old PCs as hardware drought continues — ‘we pretty much buy any PC’ pleads the Akihabara outlet&lt;/head&gt;
    &lt;p&gt;Old becomes gold in Akihabara Electric Town.&lt;/p&gt;
    &lt;p&gt;A major Japanese PC and electronics store is pleading with customers to sell their old PC gear. “As a favor, if you buy a new one, please sell your gaming PC to our company,” begged the X-account of Sofmap Gaming in Akihabara, the Electric Town district of Tokyo (machine translation, h/t PC-Watch). The store shared a photo of some almost barren shelves, presumably taken at its triple-floor retail establishment.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;ゲーミングPC、中古も本当に在庫なくて今これあの、お願いなので買い替えたらぜひ弊社にゲーミングPCを売ってください...結構高く買い取っていますので...ゲーミングのデスクでもノートでも、もちろんゲーミングじゃない普通のでもPCなら大体買い取っているので... pic.twitter.com/IinBuGgRV7January 7, 2026&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;“Gaming PCs, even used ones, are really out of stock right now,” wrote Sofmap, as an explanation for its call for old rigs. In the above Tweet, it asks customers to come in and sell their old PCs, highlighting that “We buy them back at pretty high prices...”&lt;/p&gt;
    &lt;p&gt;Moreover, the company underlined that it wasn’t going to be fussy. “Whether it's a gaming desktop or a laptop, or even a regular non-gaming one, we pretty much buy any PC...”&lt;/p&gt;
    &lt;p&gt;These are clearly the words of a PC retailer facing consumer demand that it just can’t meet. We reported on Akihabara store trying to limit new RAM, SSD, and HDD sales back in November.&lt;/p&gt;
    &lt;head rend="h2"&gt;Old becomes gold&lt;/head&gt;
    &lt;p&gt;The memory supply crunch impacted the PC industry faster and more deeply than many would have predicted. The insatiable demand for memory from AI data center makers, with their deep circular-funded pockets, caused the first pricing jolts in the PC memory market. That’s reasonable, as consumers and industry both need to be fed product from the same big-three memory makers.&lt;/p&gt;
    &lt;p&gt;Consumers saw the first impacts on modern DDR5 pricing. Some DDR5 kits, if you can find them in stock, like this Corsair Vengeance RGB DDR5-5200 16GB (2x8GB) on Amazon is now $235. That price is more than 3.5X what it cost last October ($66).&lt;/p&gt;
    &lt;p&gt;However, there remains some hope that DDR4 pricing and availability, thanks to old stocks and upgraders already having DIMMs, could provide a safe haven for continued PC building. This perception even seems to permeate PC component makers, with more DDR4-supporting motherboards being manufactured, plus hints about new processors for DDR4 platforms.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;However, we are continuing to feel RAM crunch aftershocks. Prices of pre-built PCs were the next market affected. Graphics cards with more generous VRAM quotas are also strongly rumored to be facing constraints. We should at least expect a price rise for GPU-restocks, with next-gen GPUs rumored to be delayed…&lt;/p&gt;
    &lt;p&gt;Now, underlined by this Japan retail report, it even seems like stocks of old used PCs are being snapped up by consumers.&lt;/p&gt;
    &lt;head rend="h2"&gt;How old is too old?&lt;/head&gt;
    &lt;p&gt;Of course, some old PCs are too old for retailers like Sofmap, even during today’s PC drought. We’d expect retailers that dabble in used PCs for non-enthusiast users to limit their purchases to DDR4 platforms, with hardware support that slots above the Windows 11 minimum requirements (Intel 8th Gen, AMD Ryzen 2000).&lt;/p&gt;
    &lt;p&gt;There’s an entirely different market for really old PCs, though. Vintage computers of certain eras have been increasingly pricey for quite a long time now. I was in Japan this time last year and astonished by the bountiful supplies of old PCs at used electronics retailers like Hard-Off. Hopefully, these computing gems (see the above picture), many of which live in the awkward zone between vintage and modern, will remain plentiful and affordable for PC retro-fans and tinkerers alike.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;bit_user&lt;/header&gt;I'm waiting for the article about someone making a $20k handbag covered in DDR5 DIMMs, or covering some designer shoes with DDR5 chips.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Gururu&lt;/header&gt;Reply&lt;quote/&gt;LOL that is a great question. Was just there a few months ago, wild place.ezst036 said:Where is new demand coming from?&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;Eximo&lt;/header&gt;End of Windows 10 might be part of it. But I suspect this is more about hitting a price point for their average customers who are used to bargain shopping used hardware. With more people who would normally buy new buying used, that would squeeze the availabilty of used hardware.Reply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542015</guid><pubDate>Thu, 08 Jan 2026 15:18:39 +0000</pubDate></item><item><title>Our Changing Planet, as Seen from Space</title><link>https://e360.yale.edu/digest/nasa-satellite-images-2025</link><description>&lt;doc fingerprint="6553206b04e87c6a"&gt;
  &lt;main&gt;
    &lt;p&gt;Humans are altering the planet on an unthinkable scale, both by converting vast tracts of wilderness into farms and cities and by pouring huge volumes of heat-trapping gas into the atmosphere. The impact of these enormous changes can be seen from space.&lt;/p&gt;
    &lt;p&gt;The satellite photos below, shared by NASA’s Earth Observatory over the past year, show the growing human imprint on planet Earth.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542046</guid><pubDate>Thu, 08 Jan 2026 15:20:52 +0000</pubDate></item><item><title>Iran Goes Into IPv6 Blackout</title><link>https://radar.cloudflare.com/routing/ir</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542683</guid><pubDate>Thu, 08 Jan 2026 16:11:48 +0000</pubDate></item><item><title>Digital Red Queen: Adversarial Program Evolution in Core War with LLMs</title><link>https://sakana.ai/drq/</link><description>&lt;doc fingerprint="f60c1bd489a1219e"&gt;
  &lt;main&gt;&lt;p&gt;Survival of the Fittest Code. In the game Core War, assembly-like programs called “warriors” fight for control of a virtual computer. Warriors may employ sophisticated strategies including targeted self-replication, data bombing, and massive multithreading, in order to crash other programs, and dominate the machine. Top: We visualize battles between assembly programs (“warriors”) discovered by our Digital Red Queen (DRQ) algorithm. Each DRQ round introduces one additional warrior into the multi-agent simulation. Bottom: With more rounds, the LLM-driven evolution discovers increasingly robust strategies. By simulating these adversarial dynamics, we observe emergent behaviors that mirror biological evolution, where agents must constantly adapt simply to survive against ever-changing threats. Furthermore, as Core War is a Turing-complete environment where code and data share the same address space, this process leads to some very chaotic self-modifying code dynamics. &lt;/p&gt;&lt;head rend="h2"&gt;Summary&lt;/head&gt;&lt;p&gt;Core War is a competitive programming game introduced in 1984, in which battle programs called warriors fight for dominance inside a virtual computer. To compete, developers write their code in Redcode, a specialized assembly language. In this work, we explore what happens when large language models (LLMs) drive an adversarial evolutionary arms race in this domain, where programs continuously adapt to defeat a growing history of opponents rather than a static benchmark. We find that this dynamic adversarial process leads to the emergence of increasingly general strategies and reveals an intriguing form of convergent evolution, where different code implementations settle into similar high-performing behaviors. Ultimately, this work positions Core War as a sandbox for studying “Red Queen” dynamics in artificial systems, offering a safe controlled environment for analyzing how AI agents might evolve in real-world adversarial settings such as cybersecurity.&lt;/p&gt;&lt;p&gt;For further details, please read our technical report (web paper, arxiv) and released code (github).&lt;/p&gt;&lt;p&gt;Two example warriors produced by DRQ: Ring Warrior Enhanced v9 and Spiral Bomber Optimized v22. These examples were selected to illustrate two complementary aspects of DRQ: its ability to synthesize qualitatively distinct strategies within a single program, and to produce generally performant warriors. Note that comments are LLM generated.&lt;/p&gt;&lt;p&gt; Simulating our evolved “warriors” in a sandboxed Core War environment. The user can interactively visualize the assembly language (Redcode) of the warriors around where the mouse cursor is located.&lt;/p&gt;&lt;head rend="h2"&gt;Introduction&lt;/head&gt;&lt;p&gt;Humans are the product of an extraordinary evolutionary arms race, shaped by constant competition with other organisms. Yet evolution did not stop with the emergence of modern humans: competition persists at every scale, from viruses and bacteria to people, companies, and even nations vying for dominance. As more AI systems are deployed into the world, they too will enter this competitive landscape. Inevitably, these AI systems will begin to compete with one another, either directly or indirectly, giving rise to a new kind of evolutionary dynamic. To prepare for such a future and study these fascinating dynamics, we use large language models (LLMs) to evolve programs that compete against each other for control of a virtual computer in a game called Core War.&lt;/p&gt;&lt;p&gt;Core War is a competitive programming game played out in a shared block of computer memory, called the “Core,” where two or more assembly programs fight for survival. Each program, known as a “warrior”, is written in an assembly language called Redcode. These programs are tasked with crashing their competitors while keeping their own processes alive. The simulation runs by alternating between the programs, executing one instruction at a time. A warrior “attacks” by writing invalid instructions (DAT commands) into the memory slots occupied by opponents, causing them to crash upon execution.&lt;/p&gt;&lt;p&gt; Examples of discovered warriors competing against each other in Core War.&lt;lb/&gt; Core War is a programming game where assembly-like programs called “warriors” compete for control of a virtual machine. In this work, we use LLMs to evolve warriors through a self-play algorithm called Digital Red Queen. This process leads to the discovery of diverse and sophisticated strategies, including targeted bombing, self-replication, and massive multithreading. Here, we show some of the discovered warriors competing against each other in Core War battles. Symbols indicate instruction opcodes, and colors denote the warrior that last modified each memory address. There is no distinction between code and data, making the environment highly dynamic and volatile. &lt;/p&gt;&lt;p&gt;Notably, there is no distinction between code and data, so warriors regularly modify both themselves and their opponents on the fly. This enables self-modification and even self-replication, but it also creates an extremely volatile environment in which programs must survive. Core War is also Turing-complete, meaning it can in principle support arbitrarily complex strategies.&lt;/p&gt;&lt;p&gt;Over the years, humans have devised many clever Core War strategies, including bombing random memory locations, self-replicating programs, and programs which continually scan the Core to detect opponent locations. These strategies were devised through a meta arms race between humans who try out new strategies and see what works. What would happen if we do this same arms race with LLMs?&lt;/p&gt;&lt;p&gt;In collaboration with MIT, we are excited to release our new paper Digital Red Queen: Adversarial Program Evolution in Core War with LLMs! (arxiv)&lt;/p&gt;&lt;head rend="h2"&gt;Our Method: Digital Red Queen (DRQ)&lt;/head&gt;&lt;p&gt;In evolutionary biology, the Red Queen Hypothesis posits that species must constantly evolve simply to survive against their ever-changing competitors. It argues that being “fit” in the current environment is not enough. Instead, organisms must continuously adapt—not to gain an advantage, but simply to maintain their relative fitness in a world that is always changing. This concept perfectly captures the nature of adversarial arms races, where being “fit” is never a permanent state. The name implies that standing still is not an option, drawing from Through the Looking-Glass where the Red Queen tells Alice: “Now, here, you see, it takes all the running you can do, to keep in the same place.”&lt;/p&gt; “Now, here, you see, it takes all the running you can do, to keep in the same place.”&lt;lb/&gt;Red Queen to Alice. By Lewis Carroll, Through the Looking-Glass. (Original Source)&lt;p&gt;Taking inspiration from biology, we study a simple algorithm that we call Digital Red Queen (DRQ), which embodies this idea in a computational setting. DRQ uses LLMs to evolve warriors under perpetual environmental change. Concretely, it begins with an initial warrior, then evolves a second warrior to defeat it in battle. A third warrior is then evolved to perform well against the first two, and so on. This process produces a lineage of warriors, each adapted to a changing environment defined by all of its predecessors.&lt;/p&gt;&lt;p&gt;DRQ is not intended to be a novel algorithm in itself. Rather, it is a minimal instantiation of prior multi-agent and self-play approaches, adapted to the Core War domain, designed to isolate and study the dynamics of continual coevolution.&lt;/p&gt;&lt;head rend="h2"&gt;Results&lt;/head&gt;&lt;p&gt;We find that as DRQ is run for many rounds, warriors gradually become more generally robust, as measured by their performance against unseen human-designed warriors. This provides a stable way to consistently produce more robust programs without needing to “train on the test set” (i.e., directly optimizing against a large set of human-designed programs).&lt;/p&gt;&lt;p&gt;More surprisingly, we observe that independent runs of DRQ, each initialized with different warriors, slowly converge over time toward warriors with similar behaviors. Notably, this convergence does not occur at the level of source code, indicating that what converges is function rather than implementation.&lt;/p&gt;&lt;lb/&gt;DRQ’s Convergent Evolution: With more rounds, DRQ produces warriors that are more generally robust. At the same time, across independent DRQ runs, the variance in the warrior’s behaviors decreases, indicating convergence.&lt;lb/&gt;Phenotypic Convergence: Convergence with rounds is seen only in the phenotype (behavior) of the warriors, and not the genotype (the source code), analogous to convergence in biological function rather than DNA.&lt;p&gt;This result is reminiscent of convergent evolution in biology, where similar functional traits evolved independently multiple times through different mechanisms. For example, birds and bats evolved wings separately, and spiders and snakes independently evolved venom. In these cases, evolution arrived at similar general-purpose solutions because the functional demands imposed by changing environments favored them.&lt;/p&gt;&lt;head rend="h2"&gt;Discussion&lt;/head&gt;&lt;p&gt;The emergence of convergent evolution from Red Queen dynamics, both commonly found in nature, hints that the DRQ algorithm and the Core War domain may be a promising setup for studying other properties of adversarial arms races. High level insights found in simulation could help inform how the arms race between LLMs in the wild might play out. Algorithms like DRQ could even help automate the “red-teaming” of systems before they are deployed in the real world.&lt;/p&gt;&lt;p&gt;The benefit of doing this research in a sandbox like Core War is that it’s completely self-contained: all programs run on an artificial machine with an artificial language, so nothing generated can execute outside the sandbox. This provides a safe space to explore adversarial dynamics that might be risky in the real world.&lt;/p&gt;&lt;p&gt; In a sandboxed Core War environment, we can simulate our evolved “warriors” and visualize their behaviors. The user can interactively visualize the assembly language (Redcode) of the warriors around where the mouse cursor is located. Please see our GitHub for more information.&lt;/p&gt;&lt;p&gt;Despite its simplicity, vanilla DRQ performs surprisingly well in Core War, suggesting that even minimal self-play loops can reveal complex and robust strategies. This makes DRQ a promising candidate for exploring other competitive multi-agent simulations in artificial life, biology, drug design, real-world cybersecurity, or market ecosystems. Future work could also explore richer setups where agents co-evolve simultaneously, better resembling the real-world where large populations adapt in parallel rather than along a single line of descent. Ultimately the insights gathered will help control the future for the better and help us understand the science of these evolutionary arms races.&lt;/p&gt;&lt;head rend="h2"&gt;Sakana AI&lt;/head&gt;&lt;p&gt;We are taking this technology far beyond adversarial competitive programming to unlock a new era of AI-driven discovery.&lt;/p&gt;&lt;p&gt;If you are interested in advancing AI-driven discovery, we’re hiring!&lt;/p&gt;&lt;p&gt;Sakana AI is at the forefront of AI-driven discovery. In addition to this work, we are also behind works such as The AI Scientist, LLM-Squared, Shinka-Evolve, Automating the Search for Artificial Life and ALE-Agent. We’re looking for engineers to join our team to work on our advanced AI-driven discovery platform and productionize our model-development efforts.&lt;/p&gt;&lt;p&gt;Please see our career opportunities for more information.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542761</guid><pubDate>Thu, 08 Jan 2026 16:16:43 +0000</pubDate></item><item><title>Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space</title><link>https://arxiv.org/abs/2512.24617</link><description>&lt;doc fingerprint="e09c660bc4d70aae"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 31 Dec 2025 (v1), last revised 5 Jan 2026 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Dynamic Large Concept Models: Latent Reasoning in an Adaptive Semantic Space&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large Language Models (LLMs) apply uniform computation to all tokens, despite language exhibiting highly non-uniform information density. This token-uniform regime wastes capacity on locally predictable spans while under-allocating computation to semantically critical transitions. We propose $\textbf{Dynamic Large Concept Models (DLCM)}$, a hierarchical language modeling framework that learns semantic boundaries from latent representations and shifts computation from tokens to a compressed concept space where reasoning is more efficient. DLCM discovers variable-length concepts end-to-end without relying on predefined linguistic units. Hierarchical compression fundamentally changes scaling behavior. We introduce the first $\textbf{compression-aware scaling law}$, which disentangles token-level capacity, concept-level reasoning capacity, and compression ratio, enabling principled compute allocation under fixed FLOPs. To stably train this heterogeneous architecture, we further develop a $\textbf{decoupled $\mu$P parametrization}$ that supports zero-shot hyperparameter transfer across widths and compression regimes. At a practical setting ($R=4$, corresponding to an average of four tokens per concept), DLCM reallocates roughly one-third of inference compute into a higher-capacity reasoning backbone, achieving a $\textbf{+2.69$\%$ average improvement}$ across 12 zero-shot benchmarks under matched inference FLOPs.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Xingwei Qu [view email]&lt;p&gt;[v1] Wed, 31 Dec 2025 04:19:33 UTC (2,886 KB)&lt;/p&gt;&lt;p&gt;[v2] Mon, 5 Jan 2026 05:44:29 UTC (2,887 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46542982</guid><pubDate>Thu, 08 Jan 2026 16:31:29 +0000</pubDate></item><item><title>The Waymo Ojai Will Soon Offer Autonomous Rides Around the U.S.</title><link>https://www.caranddriver.com/news/a69938250/waymo-ojai-autonomous-robotaxi-details/</link><description>&lt;doc fingerprint="7cebd04de7b4b894"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Waymo is expanding its operations to more U.S. cities, which will see a new electric van join the Jaguar I-Paces currently in Waymo's fleet, as reported by InsideEVs.&lt;/item&gt;
      &lt;item&gt;Called the Waymo Ojai, the van is built in China by Zeekr and features a suite of 13 cameras, six radar sensors, and four lidar sensors.&lt;/item&gt;
      &lt;item&gt;Certification documents show a single rear-mounted electric motor producing 268 horsepower and hooked up to a 93-kWh battery.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Waymo has been operating a fleet of autonomous robotaxis for some time now, first in San Francisco and then expanding last year to Los Angeles, Phoenix, Austin, and Atlanta. Now the Alphabet-owned company is preparing to update its lineup, supplementing its modified Jaguar I-Paces with a new van built by Chinese automaker Zeekr. Waymo confirmed the news to InsideEVs at CES 2026, spilling details about how the vans are built and how they will fit into Waymo's existing operations.&lt;/p&gt;
    &lt;head rend="h2"&gt;For Sale Near You&lt;/head&gt;
    &lt;p&gt;See all results for used cars for sale near 75247&lt;/p&gt;
    &lt;p&gt;The new autonomous van had previously been shown off as the Zeekr C1Me and the Zeekr RT, but now it bears a new name: Waymo Ojai. Named for a Californian city just north of Los Angeles, Waymo spokesperson Chris Bonelli told InsideEVs that the name was chosen because U.S. riders don't know the Zeekr brand, a subsidiary of automotive giant Geely.&lt;/p&gt;
    &lt;p&gt;Waymo plans to add the Ojai to its growing operations later this year, although the company wouldn't specify which cities will see the new van. Built exclusively for Waymo, Zeekr constructs the body of the van in China before shipping it to the U.S., where Waymo fits its software suite and array of sensors. Bonelli said that the Ojai is not affected by the U.S. government's regulations that aim to prevent Chinese cars from being sold to customers in the U.S.&lt;/p&gt;
    &lt;p&gt;A series of 13 cameras, six radar sensors, and four lidar sensors dot the Ojai's exterior, and are fitted with onboard heaters to reduce ice buildup and small wipers and fluid to clear away dirt. These features will be critical as Waymo expands beyond warm-weather cities and into gnarlier climates in the northeast United States.&lt;/p&gt;
    &lt;p&gt;While there are no official details on the powertrain, previous certification documents from the National Highway Traffic Safety Administration for the Zeekr CM1e indicate a single rear-mounted electric motor producing 268 horsepower and 252 pound-feet of torque. Documents from the EPA show a 93-kWh lithium-ion battery, but there is no official range figure. The Ojai also reportedly features an 800-volt electrical architecture, allowing it to charge faster than the I-Pace.&lt;/p&gt;
    &lt;p&gt;Along with the Ojai, Waymo plans to add modified Hyundai Ioniq 5s to its fleet that are also equipped with the company's newest software and hardware. The Ioniqs and Ojais will coexist with the I-Paces for at least a few years, as the Jaguars still have hundreds of thousands of miles left on their duty cycles.&lt;/p&gt;
    &lt;head rend="h2"&gt;➡️ Skip the lot. Let Car and Driver help you find your next car.&lt;/head&gt;
    &lt;p&gt;Caleb Miller began blogging about cars at 13 years old, and he realized his dream of writing for a car magazine after graduating from Carnegie Mellon University and joining the Car and Driver team. He loves quirky and obscure autos, aiming to one day own something bizarre like a Nissan S-Cargo, and is an avid motorsports fan.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46543200</guid><pubDate>Thu, 08 Jan 2026 16:46:45 +0000</pubDate></item><item><title>Tamarind Bio (YC W24) Is Hiring Infrastructure Engineers</title><link>https://www.ycombinator.com/companies/tamarind-bio/jobs/HPRZAz3-infrastructure-engineer</link><description>&lt;doc fingerprint="7b11fcebda9747c3"&gt;
  &lt;main&gt;
    &lt;p&gt;Easy to use computational biology tools for drug discovery&lt;/p&gt;
    &lt;p&gt;We're looking for an Infrastructure Engineer to lead the scaling of our machine learning inference system. You'll be responsible for architecting and maintaining infrastructure that serves 150+ biological ML models, scaling our platform several orders of magnitude to meet rapidly growing demand.&lt;/p&gt;
    &lt;p&gt;You’ll work closely with the founders to design to the constraints of customer needs, unpredictable workloads, and unique Bio-ML models. You'll work with Kubernetes and other tools to orchestrate containerized workloads, optimize resource allocation, and ensure high availability across our model serving infrastructure.&lt;/p&gt;
    &lt;p&gt;Most importantly, you should thrive in a fast-paced startup environment where you'll wear multiple hats, learn new technologies quickly, and help solve novel technical challenges. We value engineering judgment, problem-solving ability, and the capacity to build systems that can evolve with our growing needs.&lt;/p&gt;
    &lt;p&gt;Requirements&lt;/p&gt;
    &lt;p&gt;Preferred&lt;/p&gt;
    &lt;p&gt;We enable any scientist to access AI-powered drug discovery. Thousands of scientists from large pharma companies, top biotechs, and academic institutions use Tamarind to design protein drugs, improve industrial enzymes, and create cutting edge molecules that weren’t feasible until now.&lt;/p&gt;
    &lt;p&gt;New AI models are quickly eclipsing physics-based tools in computational drug discovery. Scientists often struggle to fine-tune, deploy, and scale these models, leaving breakthroughs on the table. Tamarind provides a simple interface to the vast array of tools being released daily.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46543403</guid><pubDate>Thu, 08 Jan 2026 17:01:00 +0000</pubDate></item><item><title>ICE's Tool to Monitor Phones in Neighborhoods</title><link>https://www.404media.co/inside-ices-tool-to-monitor-phones-in-entire-neighborhoods/</link><description>&lt;doc fingerprint="cddcd9061f5b13c8"&gt;
  &lt;main&gt;
    &lt;p&gt;A social media and phone surveillance system ICE bought access to is designed to monitor a city neighborhood or block for mobile phones, track the movements of those devices and their owners over time, and follow them from their places of work to home or other locations, according to material that describes how the system works obtained by 404 Media.&lt;/p&gt;
    &lt;p&gt;Commercial location data, in this case acquired from hundreds of millions of phones via a company called Penlink, can be queried without a warrant, according to an internal ICE legal analysis shared with 404 Media. The purchase comes squarely during ICE’s mass deportation effort and continued crackdown on protected speech, alarming civil liberties experts and raising questions on what exactly ICE will use the surveillance system for.&lt;/p&gt;
    &lt;p&gt;“This is a very dangerous tool in the hands of an out-of-control agency. This granular location information paints a detailed picture of who we are, where we go, and who we spend time with,” Nathan Freed Wessler, deputy project director of the American Civil Liberties Union’s (ACLU) Speech, Privacy, and Technology Project, told 404 Media.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46543420</guid><pubDate>Thu, 08 Jan 2026 17:01:58 +0000</pubDate></item><item><title>Chinese AI models have lagged the US frontier by 7 months on average since 2023</title><link>https://epoch.ai/data-insights/us-vs-china-eci</link><description>&lt;doc fingerprint="3f3b95d8a43414da"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Chinese AI models have lagged the US frontier by 7 months on average since 2023&lt;/head&gt;
    &lt;p&gt;Since 2023, every model at the frontier of AI capabilities, as measured by the Epoch Capabilities Index, has been developed in the United States. Over that same period, Chinese models have trailed US capabilities by an average of seven months, with a minimum gap of four months and a maximum gap of 14.&lt;/p&gt;
    &lt;p&gt;This gap closely resembles the broader gap between proprietary and open-weight models. This is unsurprising since nearly all leading Chinese models are open-weight, while frontier US models remain closed.&lt;/p&gt;
    &lt;p&gt;Authors&lt;/p&gt;
    &lt;p&gt;Published&lt;/p&gt;
    &lt;p&gt;January 2, 2026&lt;/p&gt;
    &lt;head rend="h2"&gt;Learn more&lt;/head&gt;
    &lt;head rend="h4"&gt;Overview&lt;/head&gt;
    &lt;p&gt;We visualize the gap in capabilities between US and Chinese models, using the Epoch Capabilities Index (ECI). Since 2023, the gap has ranged from 4 to 14 months, with a mean gap of 7 months.&lt;/p&gt;
    &lt;head rend="h4"&gt;Analysis&lt;/head&gt;
    &lt;p&gt;To calculate the gap between US and Chinese models, we first find the set of models that had the highest ECI among models from their country upon release. We then drop the first of these models (LLaMA-65B for the US, and Baichuan1-7B for China), since these first models were likely not at the true frontier (ECI data starts in January 2023).&lt;/p&gt;
    &lt;p&gt;To quantify the gap on each day, we look at the ECI of the best Chinese model on that day, and then calculate how long it has been since the last time the leading US model was the same or worse than that score. We consider models to be the same performance if their scores are within 1 ECI point difference. We repeat this process for each day where values exist for both the US and China. In practice, the first point where a Chinese model surpasses GPT-4 is May 2024 (a gap of 14 months), and no Chinese model has yet surpassed the ECI of OpenAI’s o3 model, released in April 2025.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46543933</guid><pubDate>Thu, 08 Jan 2026 17:40:02 +0000</pubDate></item></channel></rss>