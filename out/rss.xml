<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 29 Sep 2025 13:03:35 +0000</lastBuildDate><item><title>Show HN: Toolbrew – Free little tools without signups or ads</title><link>https://toolbrew.co/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45404667</guid><pubDate>Sun, 28 Sep 2025 14:40:46 +0000</pubDate></item><item><title>Scm2wasm: A Scheme to WASM compiler in 600 lines of C, making use of WASM GC</title><link>https://git.lain.faith/iitalics/scm2wasm</link><description>&lt;doc fingerprint="9fa21c334d8329a0"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;milo 7cbcaf8ccd&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;.gitignore&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Makefile&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;README.md&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;input.scm&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;scm2wasm.c&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h4"&gt; README.md &lt;/head&gt;
    &lt;head rend="h1"&gt;scm2wasm&lt;/head&gt;
    &lt;p&gt;really bad minimal scheme compiler&lt;/p&gt;
    &lt;head rend="h2"&gt;building&lt;/head&gt;
    &lt;code&gt;$ make
&lt;/code&gt;
    &lt;head rend="h2"&gt;running&lt;/head&gt;
    &lt;code&gt;$ ./scm2wasm &amp;lt; input.scm &amp;gt; output.wasm
$ wasm-tools validate output.wasm
$ wasm-tools print output.wasm -o output.wat
$ wasmtime -Wgc --invoke start output.wasm
...
30
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405175</guid><pubDate>Sun, 28 Sep 2025 15:43:25 +0000</pubDate></item><item><title>The AI coding trap</title><link>https://chrisloy.dev/post/2025/09/28/the-ai-coding-trap</link><description>&lt;doc fingerprint="a549d23b2ab12319"&gt;
  &lt;main&gt;
    &lt;p&gt;If you ever watch someone “coding”, you might see them spending far more time staring into space than typing on their keyboard. No, they (probably) aren’t slacking off. Software development is fundamentally a practice of problem-solving, and so, as with solving a tricky crossword, most of the work is done in your head.&lt;/p&gt;
    &lt;p&gt;In the software development lifecycle, coding is the letters filled into the crossword, only a small amount of effort compared to all the head scratching and scribbled notes. The real work usually happens alongside coding, as the developer learns the domain, narrows down requirements, maps out relevant abstractions, considers side effects, tests features incrementally, and finally squashes bugs that survived this rigorous process. It looks something like this:&lt;/p&gt;
    &lt;p&gt;But with AI-driven coding, things play out very differently.&lt;/p&gt;
    &lt;head rend="h2"&gt;“Code first, ask questions later”&lt;/head&gt;
    &lt;p&gt;AI coding agents such as Claude Code are making it astonishingly fast to write code in isolation. But most software lives within complex systems, and since LLMs can't yet hold the full context of an application in memory at once, human review, testing, and integration needs will remain. And that is a lot harder when the code has been written without the human thinking about it. As a result, for complex software, much of the time will be spent on post hoc understanding of what code the AI has written.&lt;/p&gt;
    &lt;p&gt;This is the root of the difference between marketing copy that boasts of the paradigm shifting speed of writing code with AI (often framed as “10X faster”), and the marginal productivity gains in delivering working software seen in the wild (usually closer to 10%).&lt;/p&gt;
    &lt;p&gt;An even more dispiriting upshot of this is that, as developers, we spend an ever greater proportion of our time merely fixing up the output of these wondrous babbling machines. While the LLMs get to blast through all the fun, easy work at lightning speed, we are then left with all the thankless tasks: testing to ensure existing functionality isn’t broken, clearing out duplicated code, writing documentation, handling deployment and infrastructure, etc. Very little time is actually dedicated to the thing that developers actually love doing: coding.&lt;/p&gt;
    &lt;p&gt;Fortunately, help is at hand. While LLMs are shaking up how software development is performed, this issue in itself is not actually new. In fact, it is merely a stark example of an age-old problem, which I call:&lt;/p&gt;
    &lt;head rend="h2"&gt;The tech lead’s dilemma&lt;/head&gt;
    &lt;p&gt;As engineers progress in their careers, they will eventually step into the role of tech lead. They might be managing a team, or they could be a principal engineer, driving technical delivery without the people management. In either case, they are responsible for the team’s technical delivery. They are also usually the most experienced developer in the team: either in their career, in the specialised domain of the team, or in both.&lt;/p&gt;
    &lt;p&gt;Software delivery is a team effort, but one in which experience can have a highly imbalancing effect on individual contribution velocity. As such, when the tech lead’s primary job is to maximise delivery, they will often face an internal conflict between two ways to deliver software:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fair delegation across the team, maximising learning and ownership opportunities for junior team members, but allowing delivery to be bottlenecked by the speed of the least productive team members.&lt;/item&gt;
      &lt;item&gt;Mollycoddling the team, by delegating only the easy or non-critical work to juniors, and keeping the hardest work for themselves, as the person on the team most capable of delivering at speed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately, while we shall see that mollycoddling is extremely harmful to long-term team health, it is also often a very effective way to accelerate delivery. The higher bandwidth of the tech lead is often most efficiently deployed by eating up all the hardest work:&lt;/p&gt;
    &lt;p&gt;As such, I have seen this pattern repeated time and again over the course of my career. And, of course, it comes at a cost. Siloing of experience in the tech lead makes the team brittle, it makes support harder, and it places ever greater pressure on the tech lead as a single point of failure. What follows next is predictable: burnout, departure, and ensuing crisis as the team struggles to survive without the one person who really knows how everything works.&lt;/p&gt;
    &lt;p&gt;As is usually the case, the solution lies in a third way that avoids these two extremes and balances delivery with team growth. We might frame it as something like:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Implement team practices that allow engineers to deliver working code within a framework that minimises rework, maximises effective collaboration, and promotes personal growth and learning.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When I was CTO of Datasine, we enshrined this attitude in a simple tech team motto:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Learn. Deliver. Have fun.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Good tech leads expose their engineers to work at the limit of their capabilities, using processes and practices that minimise delivery risk while also enabling each team member to grow their skills, knowledge, and domain expertise. This is, in fact, the essence of good technical leadership.&lt;/p&gt;
    &lt;p&gt;There are many ways to accomplish it, from strict codified frameworks such as the Extreme Programming rules, through to looser sets of principles which we might broadly refer to as “best practices”:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Code reviews&lt;/item&gt;
      &lt;item&gt;Incremental delivery&lt;/item&gt;
      &lt;item&gt;Modular design&lt;/item&gt;
      &lt;item&gt;Test-driven development&lt;/item&gt;
      &lt;item&gt;Pair programming&lt;/item&gt;
      &lt;item&gt;Quality documentation&lt;/item&gt;
      &lt;item&gt;Continuous integration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, for experienced engineers today, an urgent question is: how can we translate these practices into a world of AI-driven coding?&lt;/p&gt;
    &lt;head rend="h2"&gt;LLMs are lightning fast junior engineers&lt;/head&gt;
    &lt;p&gt;In 2025, many engineers are finding themselves for the first time in a position familiar to every tech lead: overseeing a brilliant but unpredictable junior engineer. Harnessing and controlling such talent, in a way that benefits effective team collaboration, is one of the primary challenges of engineering leadership. But AI coding agents need different management to junior engineers, because the nature of their productivity and growth is fundamentally different.&lt;/p&gt;
    &lt;p&gt;As software engineers gain experience, we tend to improve our productivity in multiple ways at the same time: writing more robust code, using better abstractions, spending less time writing and fixing bugs, understanding more complex architectures, covering edge cases more effectively, spotting repeated patterns earlier, etc. Engineering is a rich and complex discipline with many avenues for specialisation, but for simplicity we might group these dimensions into two broad themes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Quality: ability to deliver more complex, more performant, more maintainable code&lt;/item&gt;
      &lt;item&gt;Velocity: ability to develop working, bug-free code in a shorter space of time&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Over time, good engineers will improve in both axes.&lt;/p&gt;
    &lt;p&gt;Early LLMs were fast to write code, but time spent fixing bugs and removing hallucinations meant they were slow to complete bug-free code. Over time, smarter LLMs and better use of context engineering and tools have meant that modern AI coding agents are much better at “one shot” writing of code. The current generation of commercially available agents can be incredibly fast at producing working code for problems that would challenge some mid-level engineers, though they cannot yet match the expertise of senior engineers:&lt;/p&gt;
    &lt;p&gt;So we can think of the current generation of AI coding agents as junior engineers, albeit with two fundamental differences:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;LLMs deliver code much, much faster than junior engineers, constrained neither by thinking nor writing time;&lt;/item&gt;
      &lt;item&gt;LLMs have no true capacity to learn, and instead only improve through more effective context engineering or the arrival of new foundation models.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As with junior engineering talent, there are broadly two ways that you can deploy them, depending on whether your focus is long-term or short-term:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI-driven engineering: employing best practices, foregrounding human understanding of the code, moving slowly to make development sustainable.&lt;/item&gt;
      &lt;item&gt;Vibe coding: throwing caution to the wind and implementing at speed, sacrificing understanding for delivery velocity, hitting an eventual wall of unsalvageable, messy code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As might be expected, the long-term trajectories of choosing between these two approaches follow much the same pattern as choosing between parallel delegation and mollycoddling of a junior team:&lt;/p&gt;
    &lt;p&gt;This is why the vibe coding approach is great for tiny projects or throwaway prototypes: applications of sufficient simplicity can be delivered without the need for any human thinking at all. By limiting the complexity of our projects and leaning into the capabilities of the tools, we can deliver end-to-end working software in no time at all.&lt;/p&gt;
    &lt;p&gt;But you will hit a wall of complexity that AI is incapable of scaling alone.&lt;/p&gt;
    &lt;p&gt;Building prototypes is now easier than ever. But if we want to effectively use LLMs to accelerate delivery of real, complex, secure, working software, and to realise more than marginal efficiency gains, we need to write a new playbook of engineering practices tailored to maximise collaboration between engineering teams that include both humans and LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to avoid the AI coding trap&lt;/head&gt;
    &lt;p&gt;AI coding agents are dazzlingly productive, but lack in-depth knowledge of your business, codebase, or roadmap. Left unchecked, they will happily churn out thousands of lines of code with no heed paid to design, consistency, or maintainability. The job of the engineer, then, is to act as a tech lead to these hotshots: to provide the structure, standards, and processes that convert raw speed into sustainable delivery.&lt;/p&gt;
    &lt;p&gt;We need a new playbook for how to deliver working software efficiently, and we can look to the past to learn how to do that. By treating LLMs as lightning-fast junior engineers, we can lean on best practices from the software development lifecycle to build systems that scale.&lt;/p&gt;
    &lt;p&gt;Just as tech leads don't just write code but set practices for the team, engineers now need to set practices for AI agents. That means bringing AI into every stage of the lifecycle:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Specification: exploring, analysing, and refining feature specifications to cover edge cases and narrow focus.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Documentation: generating and reviewing documentation up front to provide reusable guardrails and lasting evidence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Modular Design: scaffolding modular architectures to control context scope and maximise comprehension.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Test-Driven Development: generating extensive test cases prior to implementation to guide implementation and prevent regression.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Coding Standards: applying house styles and best practice when generating code, through context engineering.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Monitoring &amp;amp; Introspection: analysing logs and extracting insights faster than any human ever could.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;By understanding that delivering software is so much more than just writing code, we can avoid the AI coding trap and instead hugely amplify our ability to deliver working, scalable software.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45405177</guid><pubDate>Sun, 28 Sep 2025 15:43:33 +0000</pubDate></item><item><title>Bayesian Data Analysis, Third edition (2013) [pdf]</title><link>https://sites.stat.columbia.edu/gelman/book/BDA3.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45406109</guid><pubDate>Sun, 28 Sep 2025 17:23:21 +0000</pubDate></item><item><title>Roe (YC W24) Is Hiring</title><link>https://news.ycombinator.com/item?id=45407951</link><description>&lt;doc fingerprint="272cfbb0f5843336"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;ROE is building AI Agents for risk and compliance. We are trusted by companies like eBay, Affirm and Tier 1 banks.&lt;/p&gt;
      &lt;p&gt;Hiring ambitious, talented founding engineers. Base $150K-250K, 0.75-2% options.&lt;/p&gt;
      &lt;p&gt;San Mateo office, 3 days hybrid working mode. Free lunch.&lt;/p&gt;
      &lt;p&gt;We sponsor H1B / PERM.&lt;/p&gt;
      &lt;p&gt;Link to apply https://www.ycombinator.com/companies/roe/jobs/OFFxite-found...&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45407951</guid><pubDate>Sun, 28 Sep 2025 21:00:07 +0000</pubDate></item><item><title>Play snake in the URL address bar</title><link>https://demian.ferrei.ro/snake/</link><description>&lt;doc fingerprint="5cfba17d98173e2c"&gt;
  &lt;main&gt;
    &lt;p&gt;⚠ Sorry, this game requires JavaScript. URL: ? Use the arrow keys or WASD to control the snake on the URL. Use the arrows to control the snake on the URL. Click here if you can't see the page URL or if it looks messed up with . 〈 ! Your highest score is points! Share 〈 ▲︎ ◀︎ ▼︎ ▶︎&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45408021</guid><pubDate>Sun, 28 Sep 2025 21:08:15 +0000</pubDate></item><item><title>Go ahead, write the “stupid” code</title><link>https://spikepuppet.io/posts/write-the-stupid-code/</link><description>&lt;doc fingerprint="3b719074d3a44c98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Go Ahead - Write the “stupid” code&lt;/head&gt;
    &lt;p&gt;When I finished school in 2010 (yep, along time ago now), I wanted to go try and make it as a musician. I figured if punk bands could just learn on the job, I could too. But my mum insisted that I needed to do something, just in case. So I went down to the local TAFE (this is like a trade school in Australia, though it has pathways into uni, it’s pretty neat!) and signed up for whatever looked good. I had always loved computers and gaming, I did all the courses for computing short of programming in school (the school didn’t offer it), and had an interest so I signed up.&lt;/p&gt;
    &lt;p&gt;It wasn’t love at first sight, as I still remember after a week freaking out in my room that I couldn’t do this. But I sat down with my massive VB.NET textbook we had to buy and pushed through it. And once I made it through, it clicked. I fell in love with programming after that, and it became something I was both good at and started growing a passion for.&lt;/p&gt;
    &lt;p&gt;From there, going through my games diploma, and my bachelors in games design and development (think a comp sci degree with game design elements, it’s pretty neat and I’m happy to answer questions about it), I wrote a lot of stupid code. Like a lot of it. In my courses, in my game jams (god good times), in my spare time when I was learning things both in uni and early in my career. It helped me refine my skills, but also learn a lot.&lt;/p&gt;
    &lt;p&gt;Fast forward to today. I’ve been doing a dive on JavaScript/TypeScript and different runtimes like NodeJS and Deno, amongst a bunch of other stuff. At first, I was looking into a deep dive into node with this talk by James Snell and wanted to try out the Streams API. Part of me wanted to start writing straight away, but held back because I didn’t think I had anything to use it on. After being unable to resist the urge to write some code after a few minutes, I just made the dumbest stock ticker I could so I could try streams out in an arbitrary way. But it left me thinking, “why didn’t I hold back”.&lt;/p&gt;
    &lt;p&gt;As I’m writing this now, I came up with the answer. As I was writing a little app to output inspirational quotes, I started umm’ing and ahh’ing over if I should make this. It’s small, it’s dumb, and there were probably plenty of options out there. But I wanted to write some code, and was interested in trying out Deno and seeing how it compiles binaries. So I did it. And I was happy (I’m very excited to use it), and I realised that I was scared to write something dumb. All my years of doing this helped refine my own abilities, but also made me much more harsh on myself. Harsh on my own code, harsh on just trying things.&lt;/p&gt;
    &lt;p&gt;After coming to this realisation, I’ve decided I’m going to give myself more grace when it comes to writing software for myself, and I encourage you all dear readers to do the same if you’ve been feeling this. There is no stupid code. There’s only code. Enjoy writing it, it doesn’t have to be nice or pretty if it’s for you. Have fun, try out that new runtime or language. Poke around and see what breaks. Keep that learning mindset, and keep feeding your curiosity. It’ll help you continue to grow across your career, and if you enjoy this kind of thing as a hobby like me, it’ll keep stoking your own enjoyment and passion.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45408617</guid><pubDate>Sun, 28 Sep 2025 22:20:59 +0000</pubDate></item><item><title>Lockless MPSC/SPMC/MPMC queues are not queues</title><link>https://alexsaveau.dev/blog/opinions/performance/lockness/lockless-queues-are-not-queues</link><description>&lt;doc fingerprint="6e5ad83788ac8ac5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Your MPSC/SPMC/MPMC queue is not a queue&lt;/head&gt;
    &lt;head rend="h2"&gt;Rethinking our approach to lockless channels&lt;/head&gt;
    &lt;p&gt;Published Aug 16, 2025 • Last updated Sep 25, 2025 • 10 min read&lt;/p&gt;
    &lt;p&gt;Lockless queues let multiple cores communicate with each other without mutexes, typically to move work around for parallel processing. They come in four variants: &lt;code&gt;{single,multi}&lt;/code&gt;-producer &lt;code&gt;{single,multi}&lt;/code&gt;-consumer. A producer gives data to a consumer, each of which can be limited to a single thread (i.e. a single-&lt;code&gt;{producer,consumer}&lt;/code&gt;) or shared across multiple threads. But only the single-producer single-consumer (SPSC) queue is actually a queue!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This article is part of a series building Lockness, a high-performance blocking task executor.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;SPMC/MPMC queues are broken #&lt;/head&gt;
    &lt;p&gt;Consider the so-called SPMC queue. By definition, received messages cannot be processed in a total global order without additional external synchronization. First, the single, ordered input stream is arbitrarily split amongst each consumer. Then, each message is removed from the queue in the same order. But from this moment onwards, the consumer thread can be paused at any moment (even within the library implementation that is still copying the data into your code). Consequently, another thread can process an element from the future before your code has even had a chance to see that it claimed an element, now from the past.&lt;/p&gt;
    &lt;p&gt;Acausality within consumers is the only upheld invariant: a consumer will not see any elements prior to the last element it has seen. This guarantee is almost certainly too weak to be useful as consumers have no control over which set of elements are seen, meaning arbitrary elements from the future may have been processed on other threads.&lt;/p&gt;
    &lt;p&gt;Similar logic applies to MPMC channels with the additional weakening that different producer streams are processed in no particular order. To work around this, some implementations use many SPMC channels to make a MPMC channel. They introduce the concept of a token which lets consumers optionally choose a specific producer to consume. Were this token to guarantee exclusive access to the producer, you’ve just created a poor man’s SPSC queue. Without exclusivity, you get all the same problems as SPMC channels (items being processed out-of-order by other threads).&lt;/p&gt;
    &lt;head rend="h2"&gt;MPSC queues are special #&lt;/head&gt;
    &lt;p&gt;While additional synchronization can be applied on top of SPMC and MPMC channels to provide ordering guarantees, the more useful abstraction is a stream. MPSC channels are special in that each producer can be thought of as its own stream, even though no ordering guarantees are provided between streams (producers). The consumer will see each stream in order with interleavings between streams. In hardware terms, it’s a multiplexer.&lt;/p&gt;
    &lt;p&gt;Consumer threads can then be set up for specific purposes. For example, Ringboard uses two consumer threads following the actor model in its UI implementations. Any thread can request state changes and/or submit view updates, but state changes and view updates are each processed serially on their own threads. Since I only have two consumer threads, this is effectively a mini model-view-controller framework: the controller thread handles model updates and the main thread updates the view. Notice that order within streams is important: the controller should process user input in the order in which actions occurred. However, other updates (i.e. other streams/producers) like an image loader having finished retrieving an image from a background thread can be interleaved arbitrarily with the user input stream.&lt;/p&gt;
    &lt;p&gt;Thus, MPSC channels as a whole aren’t queues, but each producer is its own queue which provides useful guarantees.&lt;/p&gt;
    &lt;head rend="h2"&gt;The rundown #&lt;/head&gt;
    &lt;p&gt;To summarize, SPMC queues and by extension MPMC queues don’t have useful ordering guarantees—calling them queues is silly. MPSC queues can be thought of as a set of producer queues multiplexed together.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: I’ve left SPSC queues out of this discussion because they are real queues with a generally agreed upon optimal implementation: power-of-2 queue capacity backed by duplicated mmaps with cached head/tail pointers expressed in terms of elements written/read, and optional get_robust_list support to handle multiprocess shared memory dead counterparty notification.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Lockless queues are slow #&lt;/head&gt;
    &lt;p&gt;Lockless queues are so named by virtue of being implemented with a queue, namely circular buffers or variations on linked lists. This is problematic because the queue linearizes updates to the channel where no such global ordering can be observed as explained above.&lt;/p&gt;
    &lt;p&gt;The implementation of a lockless queue can be conceptualized through four pointers:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The tail: producers increment it to reserve a slot to work within.&lt;list rend="ul"&gt;&lt;item&gt;Slots in red are being written to without interfering with consumers.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The committed pointer: producers have finished writing to all slots past this offset.&lt;list rend="ul"&gt;&lt;item&gt;Slots in orange should be ready to be consumed, but the shaded slot hasn’t finished its write, thereby blocking consumers from accessing subsequent elements.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The head: consumers increment it to claim a slot for consumption.&lt;list rend="ul"&gt;&lt;item&gt;Slots in green are ready to be read without interfering with producers.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The consumed pointer: consumers have finished reading all slots past this offset.&lt;list rend="ul"&gt;&lt;item&gt;Slots in blue should be ready to be written to, but the shaded slot hasn’t finished its read, thereby blocking producers from writing to subsequent elements.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This approach is not wait-free: a context-switched producer or consumer in the middle of writing or reading a value will prevent further progress.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lockless algorithm fundamentals #&lt;/head&gt;
    &lt;p&gt;The core problem in lockless algorithms is mediating access to shared memory. SPSC queues have it easy: they can prepare work and only commit it once they’re ready. Once you allow multiple threads to compete for the ability to access the same memory, they must go through stages:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A thread must exclude other competitors accessing a chunk of memory.&lt;/item&gt;
      &lt;item&gt;The thread uses the memory (non-atomically). This stage should be as fast as possible and is typically just a &lt;code&gt;memcpy&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The thread commits (publishes) its change.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Producers reserve memory to publish to consumers while consumers claim memory to be read and then released back to publishers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lockless bags as a new approach #&lt;/head&gt;
    &lt;p&gt;We’ve established that queue-based lockless channels pay the cost of linearizability without being able to take advantage of it. We’ve also seen that the only true requirement for a lockless channel is the ability to lock a region of memory.&lt;/p&gt;
    &lt;p&gt;Instead of a queue, let’s use a bag! What’s a bag? Well, uhhhh… It’s a bag. You can put stuff in, rummage around, and take stuff out. Notice that I said nothing about what you get out—if it’s in the bag, it’s a valid item to be taken out at any time (i.e. in an unspecified order).&lt;/p&gt;
    &lt;p&gt;The fastest single-threaded bag implementation is of course a stack. But this is the multithreaded world, so let’s instead use an array and two bitvectors. The first bitvector will be our reservations: producers atomically set bits to gain exclusive access to the corresponding array slot. The second bitvector is the list of committed slots: once producers are done with a slot, they set its corresponding commit bit. Conversely, consumers unset commit bits to read the slot and unset the reservation bit to return the slot to producers.&lt;/p&gt;
    &lt;p&gt;In this scheme, every producer and consumer operates independently. If a thread is stuck between stages, it has no effect on the progress of other threads. We have ourselves a wait-free MPMC channel!&lt;/p&gt;
    &lt;head rend="h2"&gt;You don’t need unbounded channels #&lt;/head&gt;
    &lt;p&gt;Limitless anything doesn’t exist in the real world, as much as we love to pretend it does. Unbounded channels introduce a lot of complexity in an attempt to paper over poorly engineered systems. If consumers cannot keep up, producers must slow down. The best way to go about this is to apply backpressure, but sadly this is rarely an option. Dropping the messages is a possibility, though a distasteful one. Alternatively, producers can cheat and buffer messages locally while waiting for space to free up when consumers are full. This last approach is the one taken throughout Lockness, minimizing communication and therefore contention when it is most critical (the channel is overloaded).&lt;/p&gt;
    &lt;p&gt;For this reason (and let’s be real mostly because unbounded channels are hard), the lockless bags I’ve implemented are unconfigurably bounded.&lt;/p&gt;
    &lt;head rend="h3"&gt;But here’s an idea to support unbounded lockless bags #&lt;/head&gt;
    &lt;p&gt;Make it a tree! Right now, the bitset represents data slots in an array, but it could additionally allow pointers to sub-nodes. This feels like a reasonable approach, though I haven’t thought too hard about a precise implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing Lockness Bags #&lt;/head&gt;
    &lt;p&gt;Lockness Bags implement the ideas described above and might be used to build the Lockness Executor should benchmarking show an advantage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Are Lockness bags fast? #&lt;/head&gt;
    &lt;p&gt;No :(&lt;/p&gt;
    &lt;p&gt;Queue-based channel implementations win over bags on current hardware by disjointing their producer and consumer memory writes. Remember the &lt;code&gt;consumed&lt;/code&gt; and &lt;code&gt;committed&lt;/code&gt; pointers from the diagram? In practice, these are implemented in a distributed fashion: each slot has a flag that marks it as readable or writable. To start, all slots are writable and transition to readable and back as you write and read the slots. The head pointer can only advance if its current slot is readable and conversely for the tail pointer. Crucially, this means consuming a slot almost never touches a cache line producers are actively working with.&lt;/p&gt;
    &lt;p&gt;On the other hand, lockless bags are implemented with two atomics each representing a bitvector. To produce and consume a value, you must always update both bitvectors. This means producers and consumers are all contending over the same two cache lines. On the other hand, lockless queues limit contention for producers to the tail cache line and similarly consumers only contend on the head cache line.&lt;/p&gt;
    &lt;head rend="h2"&gt;The future: hardware accelerating lockless bags #&lt;/head&gt;
    &lt;p&gt;The next article in the series explores the idea of novel instructions that would hardware accelerate lockless bags to significantly outperform all possible software channel implementations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix: alternative approaches and their pitfalls #&lt;/head&gt;
    &lt;p&gt;This problem has been itching the back of my brain for close to five years now. As part of the journey, I’ve toyed with many different approaches that were rejected.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why doesn’t a stack work for MPSC channels? #&lt;/head&gt;
    &lt;p&gt;Instead of a circular buffer, couldn’t we use a stack? Producers would compete to place items at the top and the single consumer takes items down. Unfortunately, the consumer would need to block producers from raising the stack, otherwise the consumer could end up in a situation where it is trying to out an already stacked upon element. You can hack around this, but it doesn’t seem better than a queue.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why not use many SPSC queues to make a MPSC channel? #&lt;/head&gt;
    &lt;p&gt;Generalizing the question, why not use multiple stricter channels to build a weaker one? On the surface, this appears to be a straightforward solution, but you run into two problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Load balancing: it is difficult to share resources. For example, consider using many SPMC channels to make a MPMC channel. If one producer has a spike in load while the others remain quiet, there is no way to use the available capacity of the other producers’ channels.&lt;/item&gt;
      &lt;item&gt;Poor scaling with high core counts: either producing or consuming values must scale linearly with the number of threads to scan across the individual queues. That said, this can be worked around by developing affinities, e.g. a consumer can keep reading from the same producer if it always has values. But if your load is so well-balanced that consumers could just pair with producers, you may as well do that instead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, orchestrating the addition/removal of individual queues in the channel and supporting sleeping becomes difficult.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why are tunnel channels bad? #&lt;/head&gt;
    &lt;p&gt;Tunnels are the simplest MPMC channel: they hold no values and thus require a pairing between producer and consumer to transfer a value onto the consumer’s stack. Consequently, either the producer or consumer must sleep to accept the next value, every time. This is painfully slow.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why not store machine-word sized elements in the channel? #&lt;/head&gt;
    &lt;p&gt;Instead of supporting arbitrarily sized values in the channel, what if we only accepted values that could fit in an atomic? More specifically pointers? Surprisingly, this doesn’t really help. If your only state is the array of atomic pointers, there’s no easy way to find free/filled slots. Thus, you need to go back to a circular buffer which has the same contention problems when the head/tail are updated but the slot hasn’t been atomically swapped to its new value. An alternative could be to scan the array for empty/filled slots until one is found, but under contention you’ll be fighting over the same slots.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45409258</guid><pubDate>Mon, 29 Sep 2025 00:21:01 +0000</pubDate></item><item><title>Cleaning house in Nx monorepo, how i removed unused deps safely</title><link>https://johnjames.blog/posts/cleaning-house-in-nx-monorepo-how-i-removed-120-unused-deps-safely</link><description>&lt;doc fingerprint="27bd23de5b31fdb1"&gt;
  &lt;main&gt;
    &lt;p&gt;Short version, I ran Knip across our Nx repo, took the “unused” list as a hint, deleted candidates, built, tested, booted apps, and put a few back when they were secretly used. Net, about 120 packages gone. Yarn install dropped by roughly a minute. Fewer CVE nags. Everyone slept better.&lt;/p&gt;
    &lt;head rend="h3"&gt;the situation&lt;/head&gt;
    &lt;p&gt;We got a chunky Nx monorepo. Roughly 500 deps scattered across apps and packages/libs, not all living in the root. Installs felt slow. Security alerts felt noisy. I wanted to clean house without breaking anything or making dev life worse.&lt;/p&gt;
    &lt;head rend="h3"&gt;why i ditched depcheck and tried knip&lt;/head&gt;
    &lt;p&gt;I used to reach for depcheck. It’s been on life support for years and doesn’t love modern setups. Knip looked current, understands monorepos, and actually sniffs entry points for common stacks. Depcheck recommends it too. It builds a little graph from imports and config refs, then compares it to package.json. Good enough for a first pass.&lt;/p&gt;
    &lt;head rend="h3"&gt;what i actually did&lt;/head&gt;
    &lt;p&gt;Baseline scan first:&lt;/p&gt;
    &lt;code&gt;yarn dlx knip&lt;/code&gt;
    &lt;p&gt;Then I ran the usual suspects to see what would scream if I yanked packages:&lt;/p&gt;
    &lt;code&gt;yarn nx affected -t build test lint
# I also spun up the app locally
yarn nx run &amp;lt;app&amp;gt;:serve   # or :dev&lt;/code&gt;
    &lt;p&gt;Knip’s pass flagged a ton of stuff on the first scan. About 40% of what it called “unused” turned out to be false positives in my setup. Totally fine, that’s expected.&lt;/p&gt;
    &lt;head rend="h3"&gt;how i treated the results&lt;/head&gt;
    &lt;p&gt;Knip is a signal, not the judge. For each package it flagged:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;uninstall it&lt;/item&gt;
      &lt;item&gt;build, test, lint, e2e, codegen/typegen, and then boot the owning app&lt;/item&gt;
      &lt;item&gt;if something broke, put it back and document why in my Knip ignore list&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Most of the false positives were “used but not imported” stuff:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;strings in config files, for example Jest preset or runner names&lt;/item&gt;
      &lt;item&gt;CLI tools only used in scripts or CI&lt;/item&gt;
      &lt;item&gt;plugin discovery patterns&lt;/item&gt;
      &lt;item&gt;type-only or toolchain stuff&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I kept a running ignore list with little comments so future me/dev isn’t confused.&lt;/p&gt;
    &lt;head rend="h3"&gt;the knip setup&lt;/head&gt;
    &lt;p&gt;I made the config monorepo-aware and added a few ignores that always trip scanners in our stack. Yours will vary, but this is just a taste without exposing too much:&lt;/p&gt;
    &lt;code&gt;/** @type {import('knip').KnipConfig} */
const config = {
  include: ["dependencies", "devDependencies"],
  ignoreWorkspaces: ["packages/eslint-config"],
  ignoreDependencies: [
    "ts-node", // referenced by name in jest config
    "cross-env", // scripts only
  ],
  workspaces: {
    "apps/cms": {
      ignoreDependencies: ["@sanity/vision"],
    },
    "packages/ui": {
      ignoreDependencies: [
        "tw-animate-css", // weird @import in global.css
        "@tailwindcss/typography", // same as above
      ],
    },
  },
}

export default config&lt;/code&gt;
    &lt;head rend="h3"&gt;verification loop&lt;/head&gt;
    &lt;p&gt;Delete the thing. Build. Test. Yadda, yadda. Quick smoke in dev. If it’s green, ship it. If not, restore and ignore with a one-liner note.&lt;/p&gt;
    &lt;p&gt;I also did a preview deploy and watched for dumb stuff like missing assets or new console errors. Nothing exciting showed up, which is the best possible outcome.&lt;/p&gt;
    &lt;head rend="h3"&gt;numbers&lt;/head&gt;
    &lt;p&gt;Before, about 510 unique packages across the workspace. After, about 390. Roughly 120 gone. Yarn install shaved off around a minute on my machine and in CI. Exactly what I wanted.&lt;/p&gt;
    &lt;head rend="h3"&gt;what knip nailed, and where it didn’t&lt;/head&gt;
    &lt;p&gt;Good at common React and server app entry points, and lots of config conventions. Not great when usage is indirect or only happens in scripts or CI. That’s fine. Humans still have jobs.&lt;/p&gt;
    &lt;head rend="h3"&gt;how i merged it without ruining anyone’s day&lt;/head&gt;
    &lt;p&gt;Small PRs are safer, but I batched this one, deployed to a preview branch, then merged during a quiet slot so rollback would only touch my PR. I left it live while I clicked through a few user flows and tailed logs. All quiet.&lt;/p&gt;
    &lt;head rend="h3"&gt;the extra bit&lt;/head&gt;
    &lt;p&gt;Knip can also flag unused files, enums, types. Nice for dead code hunts. Same rule, treat it as a hint and verify with real builds and tests.&lt;/p&gt;
    &lt;head rend="h3"&gt;what i’d do next&lt;/head&gt;
    &lt;p&gt;Wire Knip into CI as a gentle report first. Let it run for a sprint while you tune the ignore list, then consider failing on new unused deps. Keeps the bloat from creeping back in.&lt;/p&gt;
    &lt;head rend="h3"&gt;that’s it&lt;/head&gt;
    &lt;p&gt;I didn’t reinvent anything here. Knip found low-hanging fruit, I did the human check, and we shipped a smaller, cleaner repo without drama.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45409526</guid><pubDate>Mon, 29 Sep 2025 01:12:45 +0000</pubDate></item><item><title>Primer on FedEx's Distribution Network (2024)</title><link>https://ontheseams.substack.com/p/a-brief-primer-on-fedexs-distribution</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45409552</guid><pubDate>Mon, 29 Sep 2025 01:18:14 +0000</pubDate></item><item><title>F-Droid and Google’s developer registration decree</title><link>https://f-droid.org/2025/09/29/google-developer-registration-decree.html</link><description>&lt;doc fingerprint="8f6091199887549d"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;F-Droid and Google's Developer Registration Decree&lt;/head&gt;Posted on Sep 29, 2025 by marcprux&lt;p&gt;For the past 15 years1, F-Droid has provided a safe and secure haven for Android users around the world to find and install free and open source apps. When contrasted with the commercial app stores â of which the Google Play store is the most prominent â the differences are stark: they are hotbeds of spyware and scams, blatantly promoting apps that prey on their users through attempts to monetize their attention and mine their intimate information through any means necessary, including trickery and dark patterns.&lt;/p&gt;&lt;p&gt;F-Droid is different. It distributes apps that have been validated to work for the userâs interests, rather than for the interests of the appâs distributors. The way F-Droid works is simple: when a developer creates an app and hosts the source code publicly somewhere, the F-Droid team reviews it, inspecting it to ensure that it is completely open source and contains no undocumented anti-features such as advertisements or trackers[^antifeatures]. Once it passes inspection, the F-Droid build service compiles and packages the app to make it ready for distribution. The package is then signed either with F-Droidâs cryptographic key, or, if the build is reproducible, enables distribution using the original developerâs private key. In this way, users can trust that any app distributed through F-Droid is the one that was built from the specified source code and has not been tampered with.&lt;/p&gt;&lt;p&gt;Do you want a weather app that doesnât transmit your every movement to a shadowy data broker? Or a scheduling assistant that doesnât siphon your intimate details into an advertisement network? F-Droid has your back. Just as sunlight is the best disinfectant against corruption, open source is the best defense against software acting against the interests of the user.&lt;/p&gt;&lt;head rend="h3"&gt;Googleâs move to break free app distribution&lt;/head&gt;&lt;p&gt;The future of this elegant and proven system was put in jeopardy last month, when Google unilaterally decreed that Android developers everywhere in the world are going to be required to register centrally with Google. In addition to demanding payment of a registration fee and agreement to their (non-negotiable and ever-changing) terms and conditions, Google will also require the uploading of personally identifying documents, including government ID, by the authors of the software, as well as enumerating all the unique âapplication identifiersâ for every app that is to be distributed by the registered developer.&lt;/p&gt;&lt;p&gt;The F-Droid project cannot require that developers register their apps through Google, but at the same time, we cannot âtake overâ the application identifiers for the open-source apps we distribute, as that would effectively seize exclusive distribution rights to those applications.&lt;/p&gt;&lt;p&gt;If it were to be put into effect, the developer registration decree will end the F-Droid project and other free/open-source app distribution sources as we know them today, and the world will be deprived of the safety and security of the catalog of thousands of apps that can be trusted and verified by any and all. F-Droidâs myriad users[^howmanyusers] will be left adrift, with no means to install â or even update their existing installed â applications.&lt;/p&gt;&lt;head rend="h3"&gt;The Security Canard&lt;/head&gt;&lt;p&gt;While directly installing â or âsideloadingâ[^sideloading] â software can be construed as carrying some inherent risk, it is false to claim that centralized app stores are the only safe option for software distribution. Google Play itself has repeatedly hosted malware, proving that corporate gatekeeping doesnât guarantee user protection. By contrast, F-Droid offers a trustworthy and transparent alternative approach to security: every app is free and open source, the code can be audited by anyone, the build process and logs are public, and reproducible builds ensure that what is published matches the source code exactly. This transparency and accountability provides a stronger basis for trust than closed platforms, while still giving users freedom to choose. Restricting direct app installation not only undermines that choice, it also erodes the diversity and resilience of the open-source ecosystem by consolidating control in the hands of a few corporate players.&lt;/p&gt;&lt;p&gt;Furthermore, Googleâs framing that they need to mandate developer registration in order to defend against malware is disingenuous because they already have a remediation mechanism for malware they identify on a device: the Play Protect service[^playprotect] that is enabled on all Android Certified devices already scans and disables apps that have been identified as malware, regardless of their provenience. Any perceived risks associated with direct app installation can be mitigated through user education, open-source transparency, and existing security measures without imposing exclusionary registration requirements.&lt;/p&gt;&lt;p&gt;We do not believe that developer registration is motivated by security. We believe it is about consolidating power and tightening control over a formerly open ecosystem.&lt;/p&gt;&lt;head rend="h3"&gt;The Right to Run&lt;/head&gt;&lt;p&gt;If you own a computer, you should have the right to run whatever programs you want on it. This is just as true with the apps on your Android/iPhone mobile device as it is with the applications on your Linux/Mac/Windows desktop or server. Forcing software creators into a centralized registration scheme in order to publish and distribute their works is as egregious as forcing writers and artists to register with a central authority in order to be able to distribute their creative works. It is an offense to the core principles of free speech and thought that are central to the workings of democratic societies around the world.&lt;/p&gt;&lt;p&gt;By tying application identifiers to personal ID checks and fees, Google is building a choke point that restricts competition and limits user freedom. It must find a solution which preserves user rights, freedom of choice, and a healthy, competitive ecosystem.&lt;/p&gt;&lt;head rend="h3"&gt;What do we propose?&lt;/head&gt;&lt;p&gt;Regulatory and competition authorities should look carefully at Googleâs proposed activities, and ensure that policies designed to improve security are not abused to consolidate monopoly control. We urge regulators to safeguard the ability of alternative app stores and open-source projects to operate freely, and to protect developers who cannot or will not comply with exclusionary registration schemes and demands for personal information.&lt;/p&gt;&lt;p&gt;If you are a developer or user who values digital freedom, you can help. Write to your Member of Parliament, Congressperson or other representative, sign petitions in defense of sideloading, and contact the European Commissionâs Digital Markets Act (DMA) team to express why preserving open distribution matters. By making your voice heard, you help defend not only F-Droid, but the principle that software should remain a commons, accessible and free from unnecessary corporate gatekeeping.&lt;/p&gt;&lt;p&gt;https://f-droid.org/2025/09/04/twif.html [^antifeatures]: F-Droid Anti-Features overview: https://f-droid.org/docs/Anti-Features/ [^howmanyusers]: How many F-Droid users are there, exactly? We donât know, because we donât track users or have any registration. âNo user accounts, by designâ: https://f-droid.org/2022/02/28/no-user-accounts-by-design.html [^sideloading]: ââSideloadâ is a weird euphemism that the mobile duopoly came up with; it means âinstalling software without our permission,â which we used to just call âinstalling softwareâ (because you donât need a manufacturerâs permission to install software on your computer).â â Pluralistic: Darth Android: https://pluralistic.net/2025/09/01/fulu/ [^playprotect]: âGoogle Play Protect checks your apps and devices for harmful behaviorâ: https://support.google.com/googleplay/answer/2812853&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;âFor fifteen moreâ:Â ↩&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45409794</guid><pubDate>Mon, 29 Sep 2025 02:10:20 +0000</pubDate></item><item><title>Zero ASIC releases Wildebeest, the highest performance FPGA synthesis tool</title><link>https://www.zeroasic.com/blog/wildebeest-launch</link><description>&lt;doc fingerprint="b749739f3ad424b2"&gt;
  &lt;main&gt;
    &lt;p&gt;Zero ASIC ·&lt;/p&gt;
    &lt;head rend="h3"&gt;Zero ASIC releases Wildebeest, the world’s highest performance FPGA synthesis tool.&lt;/head&gt;
    &lt;p&gt;Cambridge, MA – September 17, 2025 – Zero ASIC, a U.S. semiconductor startup on a mission to democratize silicon, today announced the release of WildebeestTM, the world’s highest performance FPGA synthesis tool.&lt;/p&gt;
    &lt;head rend="h3"&gt;Background&lt;/head&gt;
    &lt;p&gt;The software world has largely moved away from proprietary, vendor-locked compilers in favor of open source alternatives such as LLVM1 and GCC. Early on, these open source compilers lagged behind in performance, but over time, through the collective effort of the community, they caught up and even surpassed their proprietary counterparts.&lt;/p&gt;
    &lt;p&gt;In hardware, a similar transformation has been unfolding. Thanks to the pioneering work of Alan Mishchenko (ABC2), Claire Xenia Wolf (Yosys3), and the broader open source EDA community, FPGA developers have had access to a full-featured Verilog RTL synthesis toolchain for years. Recently, SystemVerilog support has since been added through Mike Popoloski’s excellent Slang parser4. Thanks to strong community involvement, the Yosys project now supports FPGA synthesis for a number of commercial and academic FPGA architectures.&lt;/p&gt;
    &lt;p&gt;Unfortunately, funding for open source FPGA synthesis has been minimal, and as a result a large QoR gap between open source and proprietary synthesis remains. Industrial users, who care obsessively about performance, have thus been stuck between a rock and a hard place: “Freedom or Performance”.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attribute&lt;/cell&gt;
        &lt;cell role="head"&gt;Vendor Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Yosys&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FPGA Support&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Lock-in&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Open Source&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Free&lt;/cell&gt;
        &lt;cell&gt;Yes/No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Binary Size&lt;/cell&gt;
        &lt;cell&gt;Large&lt;/cell&gt;
        &lt;cell&gt;Small&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;QoR&lt;/cell&gt;
        &lt;cell&gt;Great&lt;/cell&gt;
        &lt;cell&gt;Good&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Robustness&lt;/cell&gt;
        &lt;cell&gt;Great&lt;/cell&gt;
        &lt;cell&gt;Good&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Wildebeest Intro&lt;/head&gt;
    &lt;p&gt;Wildebeest introduces a number of critical optimization techniques to open source. Some of these techniques are standard practice in commercial compilers, but this is the first time they have been demonstrated in an open source FPGA synthesis tool.&lt;/p&gt;
    &lt;p&gt;The most important Wildebeest strategy is the use of circuit size as a primary feature for selecting the synthesis algorithms. Existing single script solutions don’t work well because they either fail to converge for large circuits or sacrifice performance for robustness. Using a carefully selected set of size appropriate optimization scripts, Wildebeest achieves robustness and high performance for a wide range of benchmark (up to 1M LUT designs).&lt;/p&gt;
    &lt;p&gt;Another important aspect of the Wildebeest approach is the effective use of the most advanced &lt;code&gt;abc9&lt;/code&gt; commands for speculative synthesis and logic depth minimization. ABC is an incredibly powerful logic synthesis library, but making effective use of all commands is a non-trivial task that requires deep expertise in logic synthesis, the ABC architecture, and Yosys, and software development.&lt;/p&gt;
    &lt;p&gt;Logic optimization is only as good as the benchmark data that grounds the algorithms used. Wildebeest adopted an industrial approach to development from day one, developing an internal suite of 150+ carefully selected benchmarks and automated profiling utilities. The open source LogikBench benchmarks suite was created to enable independent evaluation and benchmarking.&lt;/p&gt;
    &lt;p&gt;Logic synthesis has been around for over 50 years. During this time, basic synthesis algorithms and approaches have been openly published by the synthesis R&amp;amp;D community, but many of the “outer loop” tricks of the trade have been kept hidden by practitioners within proprietary tools. The lead Wildebeest developer, Dr. Thierry Besson is an industry insider with 30 years of experience in developing state of the art commercial logic synthesis solutions. Dr. Besson has previously contributed the fastest/smallest results on a number of the EPFL logic synthesis benchmarks and with Wildebeest he is releasing many of these techniques into the wild.5&lt;/p&gt;
    &lt;head rend="h3"&gt;Benchmark Results&lt;/head&gt;
    &lt;p&gt;The table below shows how Wildebeest compares against both open-source and proprietary synthesis tools for the picorv32 CPU design. To run Wildebeest across a broader set of benchmarks, see the LogikBench project.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Device&lt;/cell&gt;
        &lt;cell role="head"&gt;Arch&lt;/cell&gt;
        &lt;cell role="head"&gt;Tool&lt;/cell&gt;
        &lt;cell role="head"&gt;Synthesis Command&lt;/cell&gt;
        &lt;cell role="head"&gt;LUTs&lt;/cell&gt;
        &lt;cell role="head"&gt;Logic Depth&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;z1060&lt;/cell&gt;
        &lt;cell&gt;LUT6&lt;/cell&gt;
        &lt;cell&gt;wildebeest&lt;/cell&gt;
        &lt;cell&gt;synth_fpga&lt;/cell&gt;
        &lt;cell&gt;2312&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;z1060&lt;/cell&gt;
        &lt;cell&gt;LUT6&lt;/cell&gt;
        &lt;cell&gt;wildebeest&lt;/cell&gt;
        &lt;cell&gt;synth_fpga -opt delay&lt;/cell&gt;
        &lt;cell&gt;2677&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Vendor-1&lt;/cell&gt;
        &lt;cell&gt;LUT6&lt;/cell&gt;
        &lt;cell&gt;vendor&lt;/cell&gt;
        &lt;cell&gt;(proprietary)&lt;/cell&gt;
        &lt;cell&gt;2870&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Vendor-2&lt;/cell&gt;
        &lt;cell&gt;LUT6&lt;/cell&gt;
        &lt;cell&gt;vendor&lt;/cell&gt;
        &lt;cell&gt;(proprietary)&lt;/cell&gt;
        &lt;cell&gt;2947&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;xc7&lt;/cell&gt;
        &lt;cell&gt;LUT6&lt;/cell&gt;
        &lt;cell&gt;yosys (0.56)&lt;/cell&gt;
        &lt;cell&gt;synth_xilinx -nocarry&lt;/cell&gt;
        &lt;cell&gt;3072&lt;/cell&gt;
        &lt;cell&gt;17&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;z1010&lt;/cell&gt;
        &lt;cell&gt;LUT4&lt;/cell&gt;
        &lt;cell&gt;wildebeest&lt;/cell&gt;
        &lt;cell&gt;synth_fpga&lt;/cell&gt;
        &lt;cell&gt;3593&lt;/cell&gt;
        &lt;cell&gt;39&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;z1010&lt;/cell&gt;
        &lt;cell&gt;LUT4&lt;/cell&gt;
        &lt;cell&gt;wildebeest&lt;/cell&gt;
        &lt;cell&gt;synth_fpga -opt delay&lt;/cell&gt;
        &lt;cell&gt;4112&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ice40&lt;/cell&gt;
        &lt;cell&gt;LUT4&lt;/cell&gt;
        &lt;cell&gt;yosys (0.56)&lt;/cell&gt;
        &lt;cell&gt;synth_ice40 -dsp -nocarry&lt;/cell&gt;
        &lt;cell&gt;4378&lt;/cell&gt;
        &lt;cell&gt;33&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The results show that Wildebeest QoR exceeds both proprietary and open source FPGA synthesis solutions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Future Work&lt;/head&gt;
    &lt;p&gt;This initial Wildebeest release is only the beginning of the journey. The development team has a pipeline of optimization techniques in development with QoR that is expected to exceed current proprietary tools by a wide margin.&lt;/p&gt;
    &lt;p&gt;The long term goal of the Wildebeest project is to help bring forth an era of “LLVM for synthesis” by working with the community to develop high performance open source FPGA tools, robust standard IRs and file formats, and broad hardware vendor adoption.&lt;/p&gt;
    &lt;head rend="h3"&gt;Demo&lt;/head&gt;
    &lt;p&gt;To try out the &lt;code&gt;Wildebeest&lt;/code&gt;, follow these installation instructions, download the picorv32 CPU example, launch &lt;code&gt;yosys&lt;/code&gt;, and run the command sequence below.&lt;/p&gt;
    &lt;code&gt;plugin -i wildebeest
read_verilog picorv32.v
hierarchy -check -top picorv32
synth_fpga -partname z1010&lt;/code&gt;
    &lt;head rend="h3"&gt;Availability&lt;/head&gt;
    &lt;p&gt;The Wildebeest source code was officially released on September 17, 2025 and can be downloaded via github:&lt;/p&gt;
    &lt;p&gt;https://github.com/zeroasiccorp/wildebeest&lt;/p&gt;
    &lt;head rend="h3"&gt;About Zero ASIC&lt;/head&gt;
    &lt;p&gt;Zero ASIC is a semiconductor startup based in Cambridge, Massachusetts. The company mission is to democratize access to silicon through chiplets and design automation. Zero ASIC is building the world’s first composable chiplet platform, enabling billions of unique silicon systems to be assembled in hours from a catalog of off-the-shelf chiplets.&lt;/p&gt;
    &lt;head rend="h3"&gt;References&lt;/head&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;C. Lattner, V. Adve, “LLVM: A Compilation Framework for Lifelong Program Analysis &amp;amp; Transformation”, Proc. International Symposium on Code Generation and Optimization 2004 ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;R. Brayton and A. Mishchenko, “ABC: An academic industrial-strength verification tool”, Proc. CAV 2010 ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;C. Wolf, Johann Glaser., “Yosys - A Free Verilog Synthesis Suite”, Proc. Austrochip 2013 ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;M. Popoloski, “Slang: a SystemVerilog Compiler”, https://github.com/MikePopoloski/slang ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;EPFL Benchmark Suite Best Results, https://github.com/lsils/benchmarks/tree/master/best_results ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45410155</guid><pubDate>Mon, 29 Sep 2025 03:45:40 +0000</pubDate></item><item><title>What is "good taste" in software engineering?</title><link>https://www.seangoedecke.com/taste/</link><description>&lt;doc fingerprint="addef8d1ea05c59d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What is "good taste" in software engineering?&lt;/head&gt;
    &lt;p&gt;Technical taste is different from technical skill. You can be technically strong but have bad taste, or technically weak with good taste. Like taste in general, technical taste sometimes runs ahead of your ability: just like you can tell good food from bad without being able to cook, you can know what kind of software you like before you’ve got the ability to build it. You can develop technical ability by study and repetition, but good taste is developed in a more mysterious way.&lt;/p&gt;
    &lt;p&gt;Here are some indicators of software taste:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;What kind of code “looks good” to you? What kind of code “looks ugly”?&lt;/item&gt;
      &lt;item&gt;Which design decisions you feel really good about, and which ones are just fine?&lt;/item&gt;
      &lt;item&gt;Which software problems really bother you, to the point where you’re worrying about them outside of work? Which problems can you just brush off?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I think taste is the ability to adopt the set of engineering values that fit your current project.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why taste is different from skill&lt;/head&gt;
    &lt;p&gt;Aren’t the indicators above just a part of skill? For instance, doesn’t code look good if it’s good code? I don’t think so.&lt;/p&gt;
    &lt;p&gt;Let’s take an example. Personally, I feel like code that uses map and filter looks nicer than using a for loop. It’s tempting to think that this is a case of me being straightforwardly correct about a point of engineering. For instance, map and filter typically involve pure functions, which are easier to reason about, and they avoid an entire class of off-by-one iterator bugs. It feels to me like this isn’t a matter of taste, but a case where I’m right and other engineers are wrong.&lt;/p&gt;
    &lt;p&gt;But of course it’s more complicated than that. Languages like Golang don’t contain map and filter at all, for principled reasons. Iterating with a for loop is easier to reason about from a performance perspective, and is more straightforward to extend to other iteration strategies (like taking two items at a time). I don’t care about these reasons as much as I care about the reasons in favour of map and filter - that’s why I don’t write a lot of for loops - but it would be far too arrogant for me to say that engineers who prefer for loops are simply less skilled. In many cases, they have technical capabilites that I don’t have. They just care about different things.&lt;/p&gt;
    &lt;p&gt;In other words, our disagreement comes down to a difference in values. I wrote about this point in I don’t know how to build software and you don’t either. Even if the big technical debates do have definite answers, no working software engineer is ever in a position to know what those answers are, because you can only fit so much experience into one career. We are all at least partly relying on our own personal experience: on our particular set of engineering values.&lt;/p&gt;
    &lt;head rend="h3"&gt;What engineering taste actually is&lt;/head&gt;
    &lt;p&gt;Almost every decision in software engineering is a tradeoff. You’re rarely picking between two options where one is strictly better. Instead, each option has its own benefits and downsides. Often you have to make hard tradeoffs between engineering values: past a certain point, you cannot easily increase performance without harming readability, for instance1.&lt;/p&gt;
    &lt;p&gt;Really understanding this point is (in my view) the biggest indicator of maturity in software engineering. Immature engineers are rigid about their decisions. They think it’s always better to do X or Y. Mature engineers are usually willing to consider both sides of a decision, because they know that both sides come with different benefits. The trick is not deciding if technology X is better than Y, but whether the benefits of X outweigh Y in this particular case.&lt;/p&gt;
    &lt;p&gt;In other words, immature engineers are too inflexible about their taste. They know what they like, but they mistake that liking for a principled engineering position. What defines a particular engineer’s taste?&lt;/p&gt;
    &lt;p&gt;In my view, your engineering taste is composed of the set of engineering values you find most important. For instance:&lt;/p&gt;
    &lt;p&gt;Resiliency. If an infrastructure component fails (a service dies, a network connection becomes unavailable), does the system remain functional? Can it recover without human intervention?&lt;/p&gt;
    &lt;p&gt;Speed. How fast is the software, compared to the theoretical limit? Is work being done in the hot path that isn’t strictly necessary?&lt;/p&gt;
    &lt;p&gt;Readability. Is the software easy to take in at a glance and to onboard new engineers to? Are functions relatively short and named well? Is the system well-documented?&lt;/p&gt;
    &lt;p&gt;Correctness. Is it possible to represent an invalid state in the system? How locked-down is the system with tests, types, and asserts? Do the tests use techniques like fuzzing? In the extreme case, has the program been proven correct by formal methods like Alloy?&lt;/p&gt;
    &lt;p&gt;Flexibility. Can the system be trivially extended? How easy is it to make a change? If I need to change something, how many different parts of the program do I need to touch in order to do so?&lt;/p&gt;
    &lt;p&gt;Portability. Is the system tied down to a particular operational environment (say, Microsoft Windows, or AWS)? If the system needs to be redeployed elsewhere, can that happen without a lot of engineering work?&lt;/p&gt;
    &lt;p&gt;Scalability. If traffic goes up 10x, will the system fall over? What about 100x? Does the system have to be over-provisioned or can it scale automatically? What bottlenecks will require engineering intervention?&lt;/p&gt;
    &lt;p&gt;Development speed. If I need to extend the system, how fast can it be done? Can most engineers work on it, or does it require a domain expert?&lt;/p&gt;
    &lt;p&gt;There are many other engineering values: elegance, modern-ness, use of open source, monetary cost of keeping the system running, and so on. All of these are important, but no engineer cares equally about all of these things. Your taste is determined by which of these values you rank highest. For instance, if you value speed and correctness more than development speed, you are likely to prefer Rust over Python. If you value scalability over portability, you are likely to argue for a heavy investment in your host’s (e.g. AWS) particular quirks and tooling. If you value resiliency over speed, you are likely to want to split your traffic between different regions. And so on2.&lt;/p&gt;
    &lt;p&gt;It’s possible to break these values down in a more fine-grained way. Two engineers who both deeply care about readability could disagree because one values short functions and the other values short call-stacks. Two engineers who both care about correctness could disagree because one values exhaustive test suites and the other values formal methods. But the principle is the same - there are lots of possible engineering values to care about, and because they are often in tension, each engineer is forced to take some more seriously than others.&lt;/p&gt;
    &lt;head rend="h3"&gt;How to identify bad taste&lt;/head&gt;
    &lt;p&gt;I’ve said that all of these values are important. Despite that, it’s possible to have bad taste. In the context of software engineering, bad taste means that your preferred values are not a good fit for the project you’re working on.&lt;/p&gt;
    &lt;p&gt;Most of us have worked with engineers like this. They come onto your project evangelizing about something - formal methods, rewriting in Golang, Ruby meta-programming, cross-region deployment, or whatever - because it’s worked well for them in the past. Whether it’s a good fit for your project or not, they’re going to argue for it, because it’s what they like. Before you know it, you’re making sure your internal metrics dashboard has five nines of reliability, at the cost of making it impossible for any junior engineer to understand.&lt;/p&gt;
    &lt;p&gt;In other words, most bad taste comes from inflexibility. I will always distrust engineers who justify decisions by saying “it’s best practice”. No engineering decision is “best practice” in all contexts! You have to make the right decision for the specific problem you’re facing.&lt;/p&gt;
    &lt;p&gt;One interesting consequence of this is that engineers with bad taste are like broken compasses. If you’re in the right spot, a broken compass will still point north. It’s only when you start moving around that the broken compass will steer you wrong. Likewise, many engineers with bad taste can be quite effective in the particular niche where their preferences line up with what the project needs. But when they’re moved between projects or jobs, or when the nature of the project changes, the wheels immediately come off. No job stays the same for long, particularly in these troubled post-2021 times.&lt;/p&gt;
    &lt;head rend="h3"&gt;How to identify good taste&lt;/head&gt;
    &lt;p&gt;Good taste is a lot more elusive than technical ability. That’s because, unlike technical ability, good taste is the ability to select the right set of engineering values for the particular technical problem you’re facing. It’s thus much harder to identify if someone has good taste: you can’t test it with toy problems, or by asking about technical facts. You need there to be a real problem, with all of its messy real-world context.&lt;/p&gt;
    &lt;p&gt;You can tell you have good taste if the projects you’re working on succeed. If you’re not meaningfully contributing to the design of a project (maybe you’re just doing ticket-work), you can tell you have good taste if the projects where you agree with the design decisions succeed, and the projects where you disagree are rocky. Importantly, you need a set of different kinds of projects. If it’s just the one project, or the same kind of project over again, you might just be a good fit for that. Even if you go through many different kinds of projects, that’s no guarantee that you have good taste in domains you’re less familiar with3.&lt;/p&gt;
    &lt;p&gt;How do you develop good taste? It’s hard to say, but I’d recommend working on a variety of things, paying close attention to which projects (or which parts of the project) are easy and which parts are hard. You should focus on flexibility: try not to acquire strong universal opinions about the right way to write software. What good taste I have I acquired pretty slowly. Still, I don’t see why you couldn’t acquire it fast. I’m sure there are prodigies with taste beyond their experience in programming, just as there are prodigies in other domains.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Of course this isn’t always true. There are win-win changes where you can improve several usually-opposing values at the same time. But mostly we’re not in that position.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Like I said above, different projects will obviously demand a different set of values. But the engineers working on those projects will still have to draw the line somewhere, and they’ll rely on their own taste to do that.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;That said, I do think good taste is somewhat transferable. I don’t have much personal experience with this so I’m leaving it in a footnote, but if you’re flexible and attentive to the details in domain A, you’ll probably be flexible and attentive to the details in domain B.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;September 28, 2025 │ Tags: good engineers, software design&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45410940</guid><pubDate>Mon, 29 Sep 2025 06:41:27 +0000</pubDate></item><item><title>Google just erased 7 years of our political history</title><link>https://www.thebriefing.ie/google-just-erased-7-years-of-our-political-history/</link><description>&lt;doc fingerprint="8926994b73f4b481"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Google just erased 7 years of our political history&lt;/head&gt;
    &lt;p&gt;Google appears to have deleted its political ad archive for the EU; so the last 7 years of ads, of political spending, of messaging, of targeting - on YouTube, on Search and for display ads - for countless elections across 27 countries - is all gone.&lt;/p&gt;
    &lt;p&gt;We had been told that Google would try to stop people placing political ads, a "ban" that was to come into effect this week. I did not read anywhere that this would mean the erasure of this archive of our political history.&lt;/p&gt;
    &lt;p&gt;When you go to the Google Ad Archive (which you can here: https://adstransparency.google.com/) until last week you could search all political ads shown in your country by a date range of your choosing going back to 2018. You could browse all the ads in that range, or search for keywords, candidates, parties. You could view each ad - watch the video, see the images - who had been targeted, how much had been spent etc.&lt;/p&gt;
    &lt;p&gt;Now when you try to click on "political ads" you get re-directed to a page asking you to select from a small number of countries - the US, of course, UK, India, Australia, Brazil, Israel - but not one EU country (see below):&lt;/p&gt;
    &lt;p&gt;The political ad archive - now deleted? - allowed people like me (and many others) to understand what happened in elections, like this longer piece I was able to write during the European &amp;amp; Local elections last year on the use of YouTube by a far right party, Sinn Féin's big push on search result ads, and the growth of attacks ads in Ireland:&lt;/p&gt;
    &lt;p&gt;Now you need the specific name of an advertiser, and when I looked for, for example, "Sinn Fein", it (a) only gave me the option of searching for their website, and (b) showed zero results. This is despite Sinn Fein spending upwards of €10k a day during some of the elections last year.&lt;/p&gt;
    &lt;p&gt;Some good things have been written about the impact that an ad "ban" might have on campaigning, especially when the algorithm still dominates all major social platforms (see this great piece by the Civil Liberties Union for Europe),&lt;/p&gt;
    &lt;p&gt;But the ad archives were introduced 7 years ago for a reason - in no small part because of the chaos of the Brexit and Trump 2016 votes, and our own advocacy here in Ireland about interference in the 2018 8th amendment referendum.&lt;/p&gt;
    &lt;p&gt;They were introduced to allow for scrutiny of campaigns, and also to provide a historical record so we could go back and look at what had been promised, and what had been spent, and to see if this lined up with what happened later.&lt;/p&gt;
    &lt;p&gt;This erasure of our political past feels dangerous, for scrutiny, for accountability, for shared memory, for enforcement of our rules - for our democracy.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45411332</guid><pubDate>Mon, 29 Sep 2025 07:57:39 +0000</pubDate></item><item><title>Optimizing a 6502 image decoder, from 70 minutes to 1 minute</title><link>https://www.colino.net/wordpress/en/archives/2025/09/28/optimizing-a-6502-image-decoder-from-70-minutes-to-1-minute/</link><description>&lt;doc fingerprint="12331abc36380bc8"&gt;
  &lt;main&gt;
    &lt;p&gt;When I set out to write a program that would allow me to do basic digital photography on the Apple II, I decided I would do it with the Quicktake cameras. It seemed the obvious choice as they were Apple cameras, and their interface to the computer is via serial port.&lt;/p&gt;
    &lt;p&gt;The scope creeped a bit after managing to decode Quicktake 100 photos. I wanted it to be able to decode Quicktake 150 and Quicktake 200 pictures too. This threw me into image processing much more than I initially wanted to. This article explains the process of how I got the Quicktake 150 decoder to a reasonabl-ish speed on a 6502 at 1MHz.&lt;/p&gt;
    &lt;p&gt;The Quicktake 150 format is proprietary and undocumented. Free software decoders exist though, in the dcraw project. This was my source for the initial Apple II decoder. Sadly, it is are written in C, is extremely non-documented, and extremely (to me!) hard to read and understand. The compression is based on Huffman coding, with variable-length codes (which means bit-shifting), and the image construction involves a lot of 16-bits math. None of this is good on a 6502.&lt;/p&gt;
    &lt;p&gt;But first I had to rework the original algorithm to work with bands of 20 pixels, for memory reasons. Once I had a functional decoder, it ran perfectly, but it took… seventy minutes to decode a single picture.&lt;/p&gt;
    &lt;p&gt;Of course, I didn’t get there that fast. The first implementation was released two years ago, in November 2023. Getting where I’m now took, I think, five or six deep dives with each time, one or two weeks worth of late evenings and full week-ends dedicated to progressing, wading through hundreds or thousands of debug printf()s, gdb’ing, variables and offsets comparisons, etc.&lt;/p&gt;
    &lt;p&gt;Follow me through the algorithmic iterations that allowed me to get that decoding time to under one minute. My implementation is now full assembly, but the commits I will link to here are to the general decoding algorithm, that is easier to read in C.&lt;/p&gt;
    &lt;p&gt;I have noticed that hand-optimizing assembler yields good results, but usually optimizing the algorithm itself leads to much more impressive speed gains. Doing too many things faster is not as good as doing the minimum faster. And that Quicktake 150 decoder sure did useless things, especially in my case where I don’t care about color and end up with a 256×192 image!&lt;/p&gt;
    &lt;p&gt;I have made a specific repository to track these algorithmic changes. It started here (already a little bit deobfuscated compared to dcraw), at 301 millions x86_64 instructions.&lt;/p&gt;
    &lt;p&gt;Dropping color&lt;/p&gt;
    &lt;p&gt;I didn’t have to decode color at all, as I was going to drop it, anyway. I added a flag to only decode the green pixels out of the Bayer matrix, and drop the rest. 264M instructions.&lt;/p&gt;
    &lt;p&gt;Understanding the buffers&lt;/p&gt;
    &lt;p&gt;I then set out to understand the use of the various temporary buffers: the more buffers, the more intermediary steps, the more copy and looping. I wanted to drop as much of them as possible. The first step towards it was unrolling some little imbricated loops that worked on y [1,2], x [col+1,col]. 238M instructions.&lt;/p&gt;
    &lt;p&gt;I figured I still had extra processing I didn’t need, removed it, dropped a buffer (and dropped the #ifdef COLOR conditional to make things clearer). 193M instructions.&lt;/p&gt;
    &lt;p&gt;At that point, my implementation still outputted green pixels only in a Bayer matrix to a 640×480 buffer, and then interpolated them. It was useless, so I dropped that entirely. 29M instructions.&lt;/p&gt;
    &lt;p&gt;I still had half the pixels black in the destination buffer, so I dropped them earlier rather than later, by outputting a 320×240 images with only the relevant pixels. 25M instructions.&lt;/p&gt;
    &lt;p&gt;At this point I was able to figure out that out of the three buf_m[3], only buf_m[1] was used to construct the picture, that buf_m[2] was only used to feed back into buf_m[0] at the start of a row, that I could construct the image from the buf_m[1] values on the fly instead of doing an extra loop on it, and that I could entirely drop it too. This allowed me to rename the last remaining buffer for more clarity. 22M instructions.&lt;/p&gt;
    &lt;p&gt;Optimizing divisions&lt;/p&gt;
    &lt;p&gt;That was about it for the buffers. The rework of the code, at that point, made clear that every final pixel value was computed by dividing the 16-bits values computed from the image data by a common factor, changing once every two rows only. The result of that division was then clamped to [0-255]. This allowed me to precompute a division table every two rows, storing the final result, pre-clamped, in a simple array. This also came with a bit of non-visible precision loss. On an x86_64, still 22M instructions, but on 6502, this was a huge gain, transforming 153600 divisions into less than 2000.&lt;/p&gt;
    &lt;p&gt;Output index&lt;/p&gt;
    &lt;p&gt;So far we set the output buffer using the usual buffer[y*WIDTH+x] access method, which is really slow on a processor with no multiplication support. I changed that to a much simpler line-by-line indexing. (Even on x86_64, the change is notable: 20M instructions).&lt;/p&gt;
    &lt;p&gt;Huffman decoding&lt;/p&gt;
    &lt;p&gt;The algorithm initialized full tables so that it was possible to get a Huffman code by just looking at the bitbuffer: for code 10001, for example, all codes from 10001000 to 10001111 were matched to the correct value, then the bitbuffer shifted &amp;lt;&amp;lt;5. This seems good at first, but not on 6502, as this requires a 16-bits bitbuffer to make sure we always have a full byte to look at. I reworked that to get bits one at a time. This made the x86_64 implementation slower, but the 6502 one 20 seconds faster, spending 9 seconds shifting bits instead of 29. It also allowed me to pack the tables more tight, freeing up some memory for the cache.&lt;/p&gt;
    &lt;p&gt;Assembly&lt;/p&gt;
    &lt;p&gt;This algorithm still performs very poorly when compiled by cc65, but is far easier to manually translate into optimized 6502 assembly. There are also a lot of ad-hoc optimisations, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The division factor for final pixel values for a pair of rows is 48 more than 50% of the time, on any image I tested. So the 6502 implementation has two divisions lookup tables, one for 48 that is never recomputed, one for another factor, recomputed if needed at the start of a pair of rows.&lt;/item&gt;
      &lt;item&gt;The row initialization multiplies all 320 next_line values by a factor, which is more than half the time 255. In this case, instead of multiplying a = a*255, the assembly version does (a*256)-a, which is (a&amp;lt;&amp;lt;8)-a, which is much faster.&lt;/item&gt;
      &lt;item&gt;There is a whole lot of &amp;lt;&amp;lt;4 going on in the main loop, which is lookup-table based in the assembly implementation. &amp;lt;&amp;lt;4 is larger than 8 bits, so there are two tables needed, but it still is worth the memory usage.&lt;/item&gt;
      &lt;item&gt;Half the Huffman codes read are discarded (they are used for blue and red pixels), so “discarder” functions are used in that case, only shifting the bitbuffer without fetching the value.&lt;/item&gt;
      &lt;item&gt;Buffers accesses (to next_line and output buffer) are patched in self-modifying code rather than using zero-page pointers, which require to keep track and patch about 54 labels on each page cross. This is ugly as hell, but this requires about 50k cycles per image, but spares 9M cycles overall.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The final code&lt;/p&gt;
    &lt;p&gt;I have pointed to commits to my “test” repository so far, but if you’re interested in the actual 6502 implementation, you can find it in my repository: the decoder, and the bitbuffer.&lt;/p&gt;
    &lt;p&gt;Bonus: first and current implementation video&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45412022</guid><pubDate>Mon, 29 Sep 2025 10:11:20 +0000</pubDate></item><item><title>Avalanche Studios NYC Retrospective – An Ambitious Company Ruined</title><link>https://probablydance.com/2025/09/28/avalanche-studios-nyc-retrospective-an-ambitious-company-ruined-by-bad-development-practices/</link><description>&lt;doc fingerprint="762951542ee71ab9"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Avalanche Studios NYC Retrospective – An Ambitious Company Ruined by Bad Development Practices&lt;/head&gt;
    &lt;head rend="h4"&gt;by Malte Skarupke&lt;/head&gt;
    &lt;p&gt;I’ve wanted to write about this since the studio closed a year ago. Now that Contraband is also canceled I think it’s time, especially since Contraband was one of the big reasons why I left the company. The blog post turned out much bigger than expected though. There was a lot to get off my chest…&lt;/p&gt;
    &lt;p&gt;I worked at Avalanche Studios NYC from July 2012 to December 31st 2019, seven and a half years. I wasn’t there for most of Contraband’s development but it was obvious early on that it was going to be a very difficult project. If anything I’m surprised it lasted that long before being canceled.&lt;/p&gt;
    &lt;p&gt;The studio was born out of ambition. It failed because it could not deliver on that ambition. So this will necessarily be negative. But we had a good run and made two good games. I have so many memories and thoughts that I need to get written down somewhere, so lets celebrate the good and talk about the troubles.&lt;/p&gt;
    &lt;head rend="h2"&gt;Founding a Studio to Make Better Games&lt;/head&gt;
    &lt;p&gt;I wasn’t there for the founding. The studio opened at the end of 2011. It was an expansion for Avalanche Studios in Stockholm. (not to be confused with Avalanche Software in Utah) That company was doing well: Both Just Cause 2 and Renegade Ops were widely considered to be surprisingly good games. theHunter was a profitable franchise, and Mad Max and Iron Man (later canceled) were under development. The story I heard was that Christofer Sundberg (co-founder of Avalanche Studios) loved New York and liked the idea of starting a studio there. Then the stars must have aligned where they got a contract from Square to develop Just Cause 3 and at the same time were able to hire a team from Kaos Studios who had just shut down. One motivation was that the NYC studio would hire talent that they didn’t have access to in Stockholm, because apparently a lot of talent isn’t interested in moving to Sweden.&lt;/p&gt;
    &lt;p&gt;Also the studio had grown like crazy after Just Cause 1 to do several different projects at once, only for all but JC2 to fail. So I think they were scared to have three projects in one studio, so why not try starting a second studio?&lt;/p&gt;
    &lt;head rend="h2"&gt;Joining as a Junior Among Seniors&lt;/head&gt;
    &lt;p&gt;I joined as a tools-programmer in 2012, straight out of college. I remember being shown around the office and introduced to people as a tools-programmer and designer Jesse Johansen’s reaction was “oh thank god”, in a voice that made it clear how frustrated he had been with the tools.&lt;/p&gt;
    &lt;p&gt;The team was still fairly new. We had inherited an engine that went from Just Cause 2 to Mad Max to us. So a lot of Just Cause 2 features had been ripped out (water, animals) or had broken. When I joined, a basic version of water was working again but there really wasn’t much in the game. The most exciting thing was an art test where they had set up a small town with high quality buildings. (no one knew what “next-gen” would look like and the town looks significantly better in the finished game)&lt;/p&gt;
    &lt;p&gt;There had been no tools-programmer before, so the level editor had been kept barely working by random devs helping out a little bit, so productivity for designers and artists wasn’t exactly great. My first commit on my first day was to disable a “create road” button because it would just crash whenever you clicked it.&lt;/p&gt;
    &lt;p&gt;The studio had made a point of only hiring “senior” people. I’d guess a third of the team had moved over from the Stockholm office, another third came from Kaos Studios, and another third were from other places. I was the first exception to the “only seniors” rule as someone straight out of college. I felt flattered until I learned in my second week that the level editor I was supposed to maintain was scheduled to be deprecated and deleted because the “engine team” in Stockholm was working on a new level editor together with a new terrain system that would get the engine ready for next-gen hardware. (this would all fail and we would stick with the old editor for a long time)&lt;/p&gt;
    &lt;head rend="h2"&gt;The Old Editor Was a Mess&lt;/head&gt;
    &lt;p&gt;Learning that I would maintain dead-end software was a bit demotivating but there was plenty of work to do. When I started, the editor would crash 30% of the time on startup. When I tracked this down to some kind of graphics bug, the graphics programmer told me that he thinks Mad Max had a fix for this already and he would try to bring it over. While we shared code with Mad Max in theory, in practice the codebases were two separate copies that were constantly diverging. After Iron Man was canceled, Mad Max had two tools-programmers, so for a while I tried to bring over improvements that they had made, but this got more and more tedious over time as the code was becoming more different, so eventually it stopped.&lt;/p&gt;
    &lt;p&gt;The editor was also just in a terrible state. The 30% crash was the least of my problems. It was very difficult to make any changes to the editor because you’d constantly break some state that wasn’t kept in sync. When opening a “location” (the term used for anything that streams in together, like a town or a military base) all the objects in the location would be created, then deleted, then created, then deleted, then created and finally kept. This happened because different pieces of code in the editor didn’t trust each other. There were observers that watched for state changes and recreated objects, but these observers could be turned off sometimes for performance reasons, and then there was a backup call that would just recreate everything. And different pieces of code would trigger this backup, just in case, so it happened twice before you even made any changes to the location. And it didn’t get better when you actually started interacting with it. This took months of incremental improvements to untangle. It was obvious what the correct thing was (just create the objects once on startup and recreate them once for changes) but if you just stopped turning off observers, you’d get infinite loops or terrible performance. Whoever wrote the editor before had simply given up and piled on more bandaids, which just made the problem worse. (they were following the “make it work, make it fast, make it good” rule by undoing step 1 in step 2 and never getting to step 3)&lt;/p&gt;
    &lt;p&gt;I’m making it sound simpler than it really was. To illustrate the mess, know that there is a thing called “object-references” where things can refer to each other. (like “when the player enters this trigger, start that cutscene”) When the editor needed to know whether an object-reference was actually pointing at an object, it would walk the Windows tree view to see if an item existed at that position in the tree. Why did it ask the view instead of the data structure that created that view? Because the data structure (if you can call it that) was such a mess that you could not look things up in it. You needed to combine several different methods just to iterate it, and this iteration code was copy+pasted many times and was doing things incorrectly as often as not. And you certainly could not just ask if an object identified by a path existed.&lt;/p&gt;
    &lt;p&gt;Previous maintainers were also terrified of deleting code. The “create road” button that I disabled on my first day belonged to a road-system that had last been used in Just Cause 1. There was a new system where you create roads in “locations” through a different interface. The old system was still needed though because it knew how to flatten the terrain wherever you place a road, so there was a brittle set of steps you had to follow to convert roads from the new system to the old system. I made those steps not crash, then automated them, then got rid of all of the old road system except for the part that did the terrain flattening. I also ripped out several other big systems where I was certain that they had not been used in Mad Max or Just Cause 2. My lines of code for the first year were almost negative, which I was quite proud of.&lt;/p&gt;
    &lt;p&gt;All this is to say that I could see a microcosm of how Just Cause 1 and 2 ended up as buggy games.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Game Was Almost Canceled&lt;/head&gt;
    &lt;p&gt;The game was coming along well actually. We were prototyping lots of things and the big important things (like the wingsuit) were working out. I can’t really comment on how decisions were made because I was too low-level. I think mostly this game went well because we had pretty clear direction (“next gen Just Cause 2”) with the occasional ambition that fit well. (“make crazy vertical military bases”) Sure, plenty of things were cut (underwater gameplay, multiplayer) but we kept the important things.&lt;/p&gt;
    &lt;p&gt;Also the right amount of restraint. Like the wingsuit was definitely overpowered at points during development. It still is a little overpowered in the shipped version, but you will smash your face into the ground several times before you get it right. Also we gave the player unlimited explosives but kept reloading of weapons. Need to draw the line somewhere.&lt;/p&gt;
    &lt;p&gt;The first big problem was when Square had financial troubles and almost canceled us. Somehow we avoided that fate by switching to being “free to play” (F2P) because that was a buzzword at the time. The idea was that the first island would be free and we would make people pay for extra islands and live service stuff, somehow. I have no idea how Roland and David got this through, but it kept the studio alive. Luckily Square’s finances recovered and that whole plan was scrapped. You can still see leftovers of it in the final game in that the first island is just the best island. The large island in particular was a victim of having not enough design due to being essentially canceled during most of the development.&lt;/p&gt;
    &lt;p&gt;I actually don’t have too much insight about the game itself. I was not involved in important decisions. When I boot it up now I just see all the bugs I caused. If you ever drive a car and get random unexplained bumps in the roads at intersections, that’s because road intersections required special handling (not my fault) and the tool for that was confusing (my fault). I could write a lot about why you don’t want to use Catmull-Rom splines for roads.&lt;/p&gt;
    &lt;head rend="h2"&gt;The New Editor and Terrain Were a Bigger Mess&lt;/head&gt;
    &lt;p&gt;Talking about my bugs means I have to talk about the new editor and new terrain. This was a giant mess. To the point where, when I left Avalanche, the new editor was going to be deleted again even though it had years of work by some of the most experienced developers in the studio. The new terrain was also a mess at first, but it could be saved with enough work.&lt;/p&gt;
    &lt;p&gt;I think the goal was that Avalanche Studios was going to license out its engine, like Unreal engine. But they recognized that the old tools were a mess and invested a lot of work into making new tools that were designed properly.&lt;/p&gt;
    &lt;p&gt;The new editor was written in Python. This was a huge mistake. The history is that, before I joined, the “location” compiler was rewritten in Python and the code was cleaner and actually ran faster than the old C++ code. So people thought Python was great and decided to use it for the new editor. I can see a case for wanting to use a memory-safe language for the editor, and I can see that Python is fun to write, but dammit Python is hard to maintain. I wasn’t there, but my theory was that the old location compiler was written by the same people who wrote the old editor. And obviously if you compare their code to a new implementation written in anything whatsoever, the new implementation will be better.&lt;/p&gt;
    &lt;p&gt;The code for the new editor was very designed. It followed lots of OOP best design practices. Everything was supposed to be a plugin. If you’re an experienced programmer you know how this went.&lt;/p&gt;
    &lt;p&gt;We hired a second tools-programmer, also a junior, whose main job was to work on the new editor once we started using it. He was constantly fighting fires. He was doing this so much that he was actually quite good at hiding how much of a problem we had on our hands. I knew, and I tried to keep my distance to the new editor. (this was easy to justify because there was enough work to do that for every month of work I finished, I would fall two months behind on my tasks) But I remember Andrew Yount (Tech Director) getting angry when he realized how this guy was spending his time. It’s weird to see Andrew get angry, that doesn’t happen.&lt;/p&gt;
    &lt;p&gt;People were constantly losing work. The new editor would crash and corrupt files, even late in development. Occasionally I would try to help out and see where the issues were and would immediately get completely lost in the layers. The way this thing wrote files to disk was so complicated that the devs thought it was a good idea if they took over version control (not a good idea if you’re also corrupting files). The team wasn’t that big, so in the old terrain you’d just make sure not to work on the same map tiles as other devs. The new editor tried to magic this away and failed miserably.&lt;/p&gt;
    &lt;p&gt;So our second tools-programmer spent a good amount of time recovering lost or corrupted files. Where in any sane environment you’d say “let me help you recover this file and then make sure this never happens again” half of his life was just “let me help you recover this file” because nobody could fix this editor. Unfortunately the fact that he was recovering the files also took the pressure off the engine team.&lt;/p&gt;
    &lt;p&gt;We had help from the “engine team” in Stockholm (who had written this editor) so they would actually ship features in the editor that we needed for the game. E.g. the new terrain had to be flattened for roads in a different way. This took much longer than in the old terrain because volumetric terrain is slower than height maps. But still this was interactive and nice at first. Then later in development it became slow and annoying to do because when you have a bad project it’s hard to keep nice things working.&lt;/p&gt;
    &lt;p&gt;Overall this whole effort was net-negative for development tools, and maybe even net-negative for JC3.&lt;/p&gt;
    &lt;p&gt;The terrain barely became shippable just before release. It almost caused us to run much too slow. I just booted up the game again on PS4 and we are nowhere near a stable 30fps. Is that entirely the terrain’s fault? No, there are other slow things, but the terrain alone was almost slow enough that we couldn’t ship, and required lots of effort that would have otherwise gone into optimizing other places.&lt;/p&gt;
    &lt;p&gt;The lesson there is to do one big rewrite at a time. Write a true 3D terrain or write a new editor. Don’t do both at the same time. Plus all the other lessons that ultimately caused the studio to fail, which I’ll get into later.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Culture Was Actually Great&lt;/head&gt;
    &lt;p&gt;I remember the culture being pretty great. Great work/life balance and lots of young people together in NYC. The Swedes that had come over from Stockholm were great at having fun in the city. Also our address was on the corner of Broadway and Spring, which is a fantastic neighborhood.&lt;/p&gt;
    &lt;p&gt;I lived further out, in Flatbush. There were kids dealing drugs out of the front door of my building. Someone got shot outside my door in the first three months of moving to the city. I lived with a girl who was hyper-active and was constantly smoking weed to make up for that. I thought she was OCD and a little intense, but the girl I was dating explained to me that no, my roommate was actually crazy. Later I moved to Chinatown to a seventh floor walkup with five roommates who were constantly rotating. I lived there for three years and I had more than thirty different roommates in that time. It was actually great.&lt;/p&gt;
    &lt;p&gt;The development culture also felt vibrant. Cool things were constantly being added to the game. At the same time there was no pressure to stay late. When we had to do a bit of overtime at the end of development, the leadership felt so conscious about making sure that we didn’t work crazy hours, that the whole effort actually felt kinda pointless (I never worked more than 50 hours in a week).&lt;/p&gt;
    &lt;p&gt;The biggest issue is that things were also always breaking. Nobody but me wrote automated tests. (I had added the googletest library to our engine, which is one of the reasons why my “lines of code” was not negative in my first year) It was normal for the build to be broken for half a day. We also didn’t have any infrastructure around running older versions. If you synced latest and it was broken, you got to spend an hour however you like (clear out your email backlog?) until someone fixed the build. (this obviously didn’t apply to programmers who were expected to fix the build when they sync to a bad version)&lt;/p&gt;
    &lt;p&gt;At some point the company started giving out awards for doing well on certain criteria and I’m still mad that the first “craftsmanship” award went to a guy who was constantly putting out fires that he had caused. It’s not good to have a culture where you reward the guy who is causing the fires when he then puts out those fires. (this wasn’t the other tools programmer. There were multiple people who seemed to always be busy putting out fires)&lt;/p&gt;
    &lt;p&gt;But people saw how hard he was working on difficult bugs, and that was rewarded. Even as a junior I could tell that this was silly because there is clearly a better way of working where you follow better coding practices and don’t constantly write difficult bugs, and then have more time to do more productive things. Like Jacques Kerner (physics and vehicle programmer) steadily shipped one feature after another throughout all of JC3 and JC4 development. But that kind of consistency is almost boring, so it doesn’t get rewarded.&lt;/p&gt;
    &lt;p&gt;With that I want to call out some other contributors. I’m afraid I’ll forget to mention most. I remember Michael Knowland stood out as a fantastic character artist. Zach Schläppi was an art director with great taste but for some reason the team didn’t like him. (I never worked directly with him so I don’t know why) But in my mind he is the main reason why JC3 looked better than JC4. On design I remember liking Hamish Young because he always wanted things to be done better and had lots of ideas. (he was a bit of an “ideas guy” but his ideas were actually good) On programming Per Hugoson (my first boss) and Dave Barrett stood out as being able to write really difficult code that somehow never caused issues. Roland Lesterlin was probably a great game director in hindsight, but I didn’t notice it at the time. I’m still friends with Rob Meyer (though more because of JC4) and Clint Levijoki because they were great at what they do and they care about the whole project.&lt;/p&gt;
    &lt;head rend="h2"&gt;We Released to Good Reviews&lt;/head&gt;
    &lt;p&gt;JC3 was delayed a bit and released in 2015. Mad Max was delayed a lot and also released in 2015. This was a bit awkward because it created a bit of competition between the Stockholm office and the NYC office. Everyone in the company was pretty good about trying to not make it a competition, but it was unavoidable to notice that JC3 got better reviews. We felt pretty good in the New York office. I liked that John Walker (from Rock, Paper Shotgun) gave us their “RPS Recommended” award because I’ve been reading that site almost since it started.&lt;/p&gt;
    &lt;p&gt;I don’t know exactly what happened next but for some reason the leadership of the NYC office got fired because of some conflict with the Stockholm management. Roland Lesterlin and David Grijns had basically been the founders of the NYC office and their departure left a bit of a vacuum. They also took some devs with then but most people stayed. At this point I had been at the company long enough where I probably could have been in the know about what happened, but having started as the only junior on the team, this still felt like it was outside of my pay-grade so I only remember that there was “some conflict with Stockholm.” I heard that Stockholm didn’t appreciate the hustling where JC3 almost went free to play, and the NYC leadership didn’t like that they always had to save money. (e.g. free snacks were replaced by a vending machine)&lt;/p&gt;
    &lt;p&gt;I think both Stockholm and NYC could be a bit unyielding. Iron Man had been canceled because Stockholm refused to follow a too-tight schedule, and Mad Max had been massively delayed because there were disagreements about direction with Cory Barlog, which led to his departure and a big redesign of the whole game.&lt;/p&gt;
    &lt;p&gt;When you have such evidence of wanting things to be done a specific way, that’s going to clash with the NYC leadership who called their new studio “Defiant”.&lt;/p&gt;
    &lt;head rend="h2"&gt;Switching Things Up for the Next Game&lt;/head&gt;
    &lt;p&gt;I remember that Stockholm wanted to develop JC4 because they wanted their baby back but surprisingly we ended up with it again. Maybe Square really wanted to work with us again? And it superficially made sense that Stockholm would do Rage 2 after Mad Max, them both being desert games.&lt;/p&gt;
    &lt;p&gt;I was very optimistic for this game. The biggest issues in JC3 felt like they were externally imposed (going F2P for a significant amount of time, starting from Mad Max instead of JC2, the new terrain that caused so much work for so many people) and we were finally in a state where things weren’t constantly breaking. Just before shipping we had an engine where people could be really productive, and they were. We filled out that giant northern island in no time at all.&lt;/p&gt;
    &lt;p&gt;For me though I knew that I couldn’t do another round as a tools-programmer. This just wasn’t a studio that valued this work. For the entire development of JC3 we had a role unfilled for another tools-programmer. I thought the role desperately needed filling, but it wasn’t a priority. The “old editor” was still the tool used for almost everything and maintaining it really was way too much work for just me. Our second tools-programmer (who worked on the editor that could only do terrain) was going back to Stockholm, partially for health reasons but also probably because you can’t work on a terrible project like that for too long. If I stayed in my role I just knew that I would once again be worked very hard. It doesn’t feel good to constantly be falling hopelessly behind on all your tasks. Yes I shipped some good things, but I also knew that people were mostly unhappy with our tools.&lt;/p&gt;
    &lt;p&gt;As an example: I had written the “time of day editor” which was a tool for editing settings that change throughout the day. Like “color of the sky” or “fog thickness” or “wave height.” From things I had learned in college I knew that the best approach for something like this is to get a simple version working early, get it in the hand of users, and then iterate on the feedback. So I got my very first version out quickly. It was a bit buggy and had clear flaws but it got the basic job done, and it had one nice feature: hot reloading. Then I had to work on something else for a while but I was scheduled to get back to this tool in a few weeks. Then another thing came up and the time slot got moved back on the schedule a bit more. And then it got moved again. And again. And again. This tool stayed at version 0.1 throughout all of development. I got to squeeze in some small bugfixes by stealing time from other projects, and at some point a frustrated technical artist was able to help out a little bit to at least do the clear easy wins, but I never got to do the proper second pass that was needed on that tool. I was embarrassed when people used it. But when you’re sitting in a meeting to decide on the schedule and say “this tool really needs a second pass, it’s barely working” the answer is always something like “sure, but people can get work done in it and there is this other new feature that we really needed two months ago.” Which means you never get to do a really good job at anything.&lt;/p&gt;
    &lt;head rend="h2"&gt;Passing the Editor Torch&lt;/head&gt;
    &lt;p&gt;Between projects I had a bit of time to work on what I wanted, so I did two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I shipped a testing framework that made it really easy to write tests that can run in the editor, so that things wouldn’t constantly break.&lt;/item&gt;
      &lt;item&gt;I added a single Qt button. The old editor was written in MFC which was so old that Visual Studio wasn’t properly supporting it anymore. For the longest time I would repurpose existing menu items because nobody knew how to add new menu items or buttons. I figured that out eventually but the UI was clearly unmaintainable. There was an old project online that allowed interoperability between Qt and Windows MFC UIs. I got that working again and as a prototype shipped exactly one button. Just to prove that we could incrementally move to something more maintainable, like Qt. (it was also a great button: physics settling. A feature where you can select objects in the editor, hold the button and the physics simulation runs just for those objects. This allows the object to settle so that you can e.g. place a group of exploding barrels on uneven ground then hold the button for two seconds to let them settle. (It was a hotkey before it became a button))&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This would in theory set up the editor for future success. We would have a more modern Qt UI and we would have tests!&lt;/p&gt;
    &lt;p&gt;I would move on to be an AI programmer. It had always been an interest of mine. And there was a slot freeing up because one programmer left to work at a different company.&lt;/p&gt;
    &lt;p&gt;I kept eyes on the tools work a little bit. A great tools-programmer (Viktor Arvidsson) had started in Stockholm and was driving the work to transition the editor to Qt. In fact the new terrain editor was going to be deprecated and the terrain-editing was going to be moved back into the old editor. The code had branched off from my state (as opposed to Mad Max) so I was happy that all my work of cleaning up was going to survive.&lt;/p&gt;
    &lt;p&gt;Sadly the “physics settling” feature broke pretty quickly and the editor tests were broken by a QA person who was trying to help out by doing some programming. I was aware of what his intentions were but I did not understand that he was submitting broken things. I only understood when the Stockholm programmer reached out to ask about the testing and I said “just run this thing” and then realized that it was completely broken and that I couldn’t easily get it working again.&lt;/p&gt;
    &lt;p&gt;Did I mention that Avalanche Studios didn’t really do code review? I’ll have to talk about that later. But it wasn’t that strange that somebody could submit something that was straight up breaking things that worked before. Since most people didn’t do testing, nobody cared that the editor tests didn’t work any more.&lt;/p&gt;
    &lt;head rend="h2"&gt;JC4 Tech Quickly Turned Bad&lt;/head&gt;
    &lt;p&gt;Anyway we were working on JC4. Starting off from a highly productive engine. It was going to be great. Sadly the engine became highly unproductive within months. People went back to their old habits right away. Turns out we were only stable near the end of JC3 because people mostly weren’t adding features. Most people were doing bugfixes or adding content or iterating on details. That makes it easy to keep things working. But for JC4 we were working on new functionality and immediately broke everything.&lt;/p&gt;
    &lt;p&gt;One disaster I remember was the gameplay scripting system. JC3 had shipped with two gameplay scripting systems. A “event” system which was obviously bad but was well-integrated with the engine. It led to constant bugs and because nobody could follow what even the simplest scripts were doing. And then we had another system that was inspired by Kismet from Unreal Engine. This system was written by someone on the Mad Max team over a holiday break. It was not well integrated with the engine so it could never possibly replace “events”. But at least you could see which objects were talking to which other objects and you could see in which order things were supposed to happen.&lt;/p&gt;
    &lt;p&gt;These led to so many problems and bugs in JC3 that it was an obvious point of attack for improving everyone’s work. As a result we wrote three and a half new scripting systems for JC4. If you thought that having two systems was a problem, wait until you ship with four and a half systems. Because “events” was the only system that was actually well integrated with the whole engine so we still used it.&lt;/p&gt;
    &lt;p&gt;How does this happen? Really bad planning and really bad design and not even properly thinking about the problem. Which can only happen because they didn’t have enough eyes on the proposals. (when I found out that we were doing the first two new scripting systems I immediately realized the problem and proposed a design for making “events” be graph-based to make it less error-prone, but my input came too late – the decisions had already been made)&lt;/p&gt;
    &lt;p&gt;Or we had a new “input system” to handle controls. It was outrageously bad. The person who wrote it left and nobody else could figure out how to implement remapping of controls. We almost shipped without being able to remap controls. I had continued to maintain my testing framework and was using it extensively to test AI behaviors and at some point I thought it would be a small additional lift to create tests for player behavior. But it was impossible, completely impossible, to send inputs from code. That might sound silly because clearly some code had to read the controller state and translate it into game inputs, but I could not figure it out at all. So I couldn’t write automated tests that control the player character for the silliest reason ever: because I couldn’t send inputs from code.&lt;/p&gt;
    &lt;p&gt;Other than the big screw-ups we actually improved lots of things. We were adding cool features to the grappling hook, we had more enemy variety, more vehicles, the game ran at a better framerate and had shorter loading times, it was easy to ship large content (the big city in JC3 was a major lift, in JC4 this was easy enough that we shipped several big cities) and ambitious features like the frontlines were done almost entirely by me, while they would have been very difficult in JC3.&lt;/p&gt;
    &lt;head rend="h2"&gt;JC4 Design Mysteriously Turned Bad&lt;/head&gt;
    &lt;p&gt;JC4 was weird because even though we improved on many things, the game came out worse. The design also had lots of improvements, going far beyond the “just blow everything up” gameplay of JC3.&lt;/p&gt;
    &lt;p&gt;When the game shipped I felt like something was off with it but I didn’t know what, so I thought we’d get better reviews than JC3 because we had improved so many things. The reviews disagreed, and reading the reviews clarified a lot for me.&lt;/p&gt;
    &lt;p&gt;The biggest problem was that where in JC3 the big bets worked out (wingsuit), in JC4 they didn’t. Like the “extreme weather” wasn’t terrible, but it also just didn’t carry a lot of weight. Also the frontlines never had any real design behind it beyond “these should exist.” I had implemented them but didn’t know what to do with them either so you can’t do much in them in the game.&lt;/p&gt;
    &lt;p&gt;The biggest bet that didn’t work out was the move away from “chaos objects”: The main criticism of JC3 was that it was repetitive. It was clear that we could never get great reviews as long as we didn’t address that. So we wanted to have more variety in the bases. And we prototyped so many different things for this. But it turns out to be impossible to design missions for Rico Rodriguez, the protagonist. Anything you put in his way can either be taken over or just walked around. How can you possibly design a mission if the protagonist can just fly over every obstacle you put in his way? Mostly we had two answers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Restrict the player to a small area. Either “hold this button without being interrupted” or “don’t leave this zone for 1 minute.” I don’t know how many of these missions we shipped with but I played way too many of these during development.&lt;/item&gt;
      &lt;item&gt;Escort Quests. If you can’t restrict Rico, make him protect someone who is restricted. Now you have to clear all the obstacles for NPCs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Players hate escort quests. The AI team (which I was now a member of, remember) hated escort quests. You can’t have escort quests in a Just Cause game because there is constantly debris everywhere. There is always some car wreck or a broken giant fuel tank blocking the way. Early in development the designers on our team made it clear to the rest of the design team that we would not give any support to escort quests. Our message was: don’t make these. And yet half the game is escort quests and we were very busy supporting escort quests. I think the other AI programmer on the team, Anders Südow, still has nightmares because one early mission involves escorting a car half-way across the map while it’s being chased by AI enemies and while there are roadblocks in the way. And it’s mostly systemic, only the road blocks are scripted. A crazy amount of work went into that mission because (surprise!) it’s not good to block the road that AI is supposed to drive on. And the mission is not even all that good.&lt;/p&gt;
    &lt;p&gt;How does this happen? A decision was made and we stuck with it. We weren’t going to do chaos objects any more. At first there was a notion that if things didn’t work out, we could always fall back on chaos objects. And then we never did that when things didn’t work out. Instead you get lots of missions of “go there then press the button.”&lt;/p&gt;
    &lt;p&gt;Maybe nobody realized the fundamental impossibility that you can’t design missions for Rico Rodriguez. I certainly didn’t put it together until after we shipped. (but it also wasn’t my job)&lt;/p&gt;
    &lt;p&gt;I personally liked our game director, Francesco, quite a lot. He was the lead designer on JC3 and he is clearly a good designer. But he is a bit too technical in that he likes to come up with systems and rules in the game. Somehow he missed that the basics were often not there. Like these missions aren’t fun. And a game director is also supposed to have his eyes on other things like graphics (characters look worse, water looks worse) or story. Watch a random cutscene from JC3 and it’s people having fun and is short and to the point. Watch a random cutscene from JC4 and it’s not people having fun and it’s not short and not to the point.&lt;/p&gt;
    &lt;p&gt;The main thing I remember him doing was managing Square, but I don’t actually know what that involves.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nobody Ever Gets Credit for Fixing Problems that Never Happened&lt;/head&gt;
    &lt;p&gt;My main issue was that things were constantly broken during development. We’d ship some new feature to Square for a “milestone” and then a month later it was broken and nobody cared to get it working again because we were busy shipping the next “milestone.” Usually you couldn’t play missions anymore that were made two months ago. Only in the last months before release did everything actually work and stay working.&lt;/p&gt;
    &lt;p&gt;I was still the only person writing automated tests. There was good infrastructure for this in place now. I remember giving a talk internally where I tried to convince people to write more tests. I asked before the presentation “who already thinks we should write more tests?” and almost everyone’s hand went up. Which left me a bit surprised because what the heck are we doing then? In practice it was completely impossible to get these people to write tests even if they thought they should be doing it. One time there was an animation bug that was causing issues for me. So I wrote a little test to reproduce the issue and I showed it to the other programmer. “Look how easy this was to set up” and “look, you can restart the test from the menu or run it on repeat to make it super easy to debug the animation issue” I’d say. And he’d nod appreciatively, fix the issue (two months later) and then never write a test of his own.&lt;/p&gt;
    &lt;p&gt;I was reading a lot of project management literature at this time because I couldn’t figure out what was going wrong. The problems were so obvious. The solutions, too. Just take a moment and think about what you need for gameplay scripting before you write three and a half different systems that all don’t solve the issue. Just take like ten minutes to write an automated test so that a new feature doesn’t break in a month. Invest a little bit of time to save a lot of time.&lt;/p&gt;
    &lt;p&gt;I found the answer in the classic paper “Nobody Ever Gets Credit for Fixing Problems that Never Happened“. Man were we ever stuck in a capability trap. Or a Red Queen’s race: You have to run as fast as you can just to stay in place. Here is a quote from the paper that could have applied perfectly to Avalanche Studios:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Supervisors never had time to make improvements or do preventative maintenance on their lines . . . they had to spend all their time just trying to keep the line going, but this meant it was always in a state of flux, which, in turn, caused them to want to hold lots of protective inventory, because everything was so unpredictable. A quality problem might not be discovered until we had produced a pile of defective parts. This of course meant we didn’t have time to figure out why the problem happened in the first place, since we were now really behind our production schedule. It was a kind of snowball effect that just kept getting worse.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You see we never had time to keep things working because we were behind on shipping the next milestone because so many things were broken.&lt;/p&gt;
    &lt;p&gt;I remember, years later, talking to another programmer who had been on the same team. And when I talked about testing he said something like “Avalanche was always doing big games with a small team. We just didn’t have enough programmers to have time to write tests.” This blew my mind. It explained a lot actually. People thought that if you want to write tests, you have to hire more people so you have more time to write tests. For me it was always obvious that tests save you so much time because you’re not constantly breaking things. Instead of being the guy who is constantly putting out fires, you can be the guy who steadily improves things and actually ships features. Meaning a team who writes tests needs fewer programmers to achieve the same thing.&lt;/p&gt;
    &lt;p&gt;But it wasn’t just the programmers. Everyone was constantly behind on everything. There were many things in JC4 that were actively worse than in JC3. Like more things were destructible in JC3. Much of the art is worse, some environment art was ridiculously bad. And yeah, it’s kinda silly that people on reddit were making fun of us for having blurry low-res low-poly rocks in the game. Because who really cares about rocks? But it was just a symptom of this wider issue that can be felt all throughout the game. And when you have a clear example, it makes sense to highlight it.&lt;/p&gt;
    &lt;p&gt;When there are constant issues all throughout development, nobody gets to ask for really high quality things. At the time I read the literature and watched the talks and found all the quotes: John Romero talking about the early days of id software and how they would constantly fix bugs as they saw them, Rob Pardo talking about how Blizzard doesn’t just polish at the end, you polish all throughout development. Read that linked paper to see how process improvements can drastically improve output in other industries. Here is another quote that could have applied to Avalanche:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;a BP team reduced butane flare-off to zero, saving $1.5 million/year and reducing pollution. The effort took two weeks and cost $5000, a return on investment of 30,000%/year. Members of the team had known about the problem and how to solve it for eight years. They already had all the engineering know-how they needed, and most of the equipment and materials were already on site. What had stopped them from solving the problem long ago? The only barrier was the mental model that there were no resources or time for improvement, that these problems were outside their control, and that they could never make a difference.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Avalanche had a couple of these lying around. E.g. I tried to convince people to implement the “not rocket science” rule of software engineering which is to “automatically maintain a repository of code that always passes all the tests”. We had almost no tests, so I would have been happy to automatically maintain a repository that builds. This is not difficult (it’s called “not rocket science” for a reason) it would have taken a few days of figuring things out in Perforce and Jenkins, and it would have paid for itself immediately, but we were too busy.&lt;/p&gt;
    &lt;p&gt;And in that culture you just cant ask for the really nice things that actually make a game good.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Obvious Physics Bugs Ship&lt;/head&gt;
    &lt;p&gt;Here’s a fun one for learned helplessness: When John Walker posted his JC4 review it was clear that he picked up on all the issues. He pointed out so many things that were not only bad, but pointlessly bad. Like how the game always shows the controls for honking the horn when in a helicopter, and there is no way to turn that off. And at the same time it doesn’t tell you how to actually fly the damn thing. (how does that happen? It got submitted somehow, maybe as a first version, and it wasn’t bad enough that it required someone’s full attention, and it was related to the bad new input system which had a big “ugh field” around it, so it stayed)&lt;/p&gt;
    &lt;p&gt;But he also posted a video with a bunch of physics bugs. I looked at that and thought “that looks worse than it should.” Sure you could break our physics, but it shouldn’t be that broken. I thought the JC4 physics was generally better than in JC3. I had an immediate suspicion though. I went into work the next day and found the code for clamping physics impulses.&lt;/p&gt;
    &lt;p&gt;Any physics engine has problems when you present it with impossible situations. Like a car inside a rock. It really wants to push the car out, but since it’s an impossible situation, it is going to apply crazy forces. And then those crazy forces cause more impossible situations to happen so things can never calm down. And in JC4 it’s easy to cause impossible situations because we give you lots of tools to mess with the physics. To solve this we clamped all impulses that the physics engine applies to some reasonable intensity. Meaning it can’t push the car out in one step any more, but things are more stable overall. I looked at that code and found that it was not that complicated. It just had some if/else statements to check if an impulse was too big, and to log the crazy impulse during development so that we can identify what’s causing them. Except the if/else was wrong. It was supposed to disable that logging in release builds, but leave the clamping enabled. It disabled both the logging and the clamping. Meaning only in the released game could you get unclamped crazy physics impulses that we never saw during development.&lt;/p&gt;
    &lt;p&gt;I walked over to Jacques, our physics programmer (who was usually great, see above) and we had it fixed in less than five minutes. It made it into the game in the next patch, but too late for reviews.&lt;/p&gt;
    &lt;p&gt;How does something like this get through QA? How can they not catch that the physics can go completely crazy in the released game? My guess is that QA did catch this and there were probably several entries in the bug tracker for crazy broken physics. But remember how things were always kinda broken during development? We probably had hundreds of bugs for broken physics in the bug tracker. Most weren’t as bad as the things we saw in release builds, but nobody could quite tell what level of badness was acceptable and what level wasn’t. So we shipped a game where you regularly saw physics go completely crazy.&lt;/p&gt;
    &lt;head rend="h2"&gt;The DLC Was More Fun to Develop Than to Play&lt;/head&gt;
    &lt;p&gt;In JC3 I didn’t work on the DLCs, but in JC4 I did. Once again this was better because the engine was now in a good state, not constantly broken. When you work on the DLC you have to make absolutely sure that you don’t break the main game, so people are more careful and don’t constantly break things. So everyone is actually productive.&lt;/p&gt;
    &lt;p&gt;My favorite part of working on JC4 was the “Getting Over It” easter egg. Rob Meyer is friends with Bennett Foddy, who lives in NYC so we got him to come in. For the Racing DLC I made a second bigger version of it. It’s great, even though almost nobody has played it. While writing this I discovered that someone finally uploaded a video to Youtube a year ago.&lt;/p&gt;
    &lt;p&gt;The concept of the Demon DLC was “bring back chaos objects.” Which is a great idea. I did all the enemy AI. Sadly the DLC didn’t really work out. Since I worked on the enemies, and you could probably call me the main combat designer, I mostly blame this on the combat. Enemies keep on respawning, so many players adopt the strategy of “try to clear the base even though I’m still being attacked” which means you end up playing a game where you’re constantly being pestered by the enemies, which is just unpleasant. The best way to play the DLC is to keep on killing the enemies because they eventually switch to a mode where they respawn more slowly, at which point you can do the chaos objects, but players don’t realize that. (I had to hide this “slow respawn” mode a little to get it through) We saw the problems during internal playtests and I didn’t want to do the infinite respawning, but I was overruled because it felt weird for the towns to be empty. So once again a decision got made, we saw it was bad, and we stuck with it because we couldn’t come up with anything better.&lt;/p&gt;
    &lt;p&gt;The hoverboard DLC was great fun in that it solved the design problem we had in the main game: Turns out you can design varied content for Rico Rodriguez by reframing them as chaos objects. I didn’t work on it for long though, and my main contribution is that I added the really long hoverboard tracks. I also worked on the boss fight though, once again, I am apparently not a good combat designer because that boss fight is not that great… (I’m better as a programmer)&lt;/p&gt;
    &lt;p&gt;I tried to internally pitch that we should make a hoverboard racing game, but it was rejected because the NYC studio wasn’t supposed to work on small titles.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lack of Faith Made me Leave&lt;/head&gt;
    &lt;p&gt;I never worked on Contraband since I was busy with JC4 DLC and a bit of Rage 2 DLC. I saw early prototypes of Contraband and heard all about the design, and it was clear to me that it was going to be trouble.&lt;/p&gt;
    &lt;p&gt;I didn’t want to work on another troubled multi-year project, so I left before joining the team. Another contributing factor was that a bunch of my code was lost, including all the work I had done on automated testing. This time the engine would continue from Call of the Wild, so if you wanted to keep something, you had to merge it into that code base. I had actually convinced people that automated testing was important, and apparently there was a meeting to decide what to do about it, but they decided to go with a different, less fleshed-out system that someone in Stockholm had recently written. It wasn’t a bad system, I just thought they made it unnecessarily hard to write and run tests, and it clearly was at best a first version of testing infrastructure, where my work had actually been used extensively and shipped in a game. But I didn’t get to make my case because I wasn’t in the meeting. Yes, they decided on the future of automated testing without involving the one person at the company (in both NYC and Stockholm) who actually had experience of doing extensive automated testing. I’m still bitter about that… I had briefly thought about adding all the nice features that my testing infrastructure had to the new system (like writing tests entirely in content using “events”, which I had actually convinced one designer to do; or being able to pause tests in the middle to debug something; or running a test on repeat), but the new system was so far from supporting nice features that my motivation quickly got deflated by how I would not only have to redo my work, but would have to put in extra work because the design of the new system wasn’t there yet.&lt;/p&gt;
    &lt;p&gt;If there’s one lesson in this, it’s that not sharing code and throwing away code at the end of a game can really hurt your engine, both in all the obvious ways and also in several non-obvious ways.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contraband Was Going to be Very Very Difficult&lt;/head&gt;
    &lt;p&gt;I still saw lots of Contraband before leaving. The company had pitched a lot of different projects, but Contraband actually got signed. Contraband was a lot of things to a lot of people. It was going to be the justification for having a NYC studio, going back to the original motivation from 2011: A big impressive AAA game that justifies having an expensive studio. In fact the game was big enough that NYC and Stockholm would work together on it, somehow.&lt;/p&gt;
    &lt;p&gt;It was going to be a multiplayer co-op open-world heist game. The first three things made total sense: We could see how much money GTA IV was making and we had wanted to make a multiplayer game for ages (JC3 had some work done towards multiplayer in the F2P era). Co-op makes sense for that because it’s easier to make a co-op multiplayer game than a competitive one, especially in an open world. Open-world made sense because that was our strength as a studio. But what the heck is a heist game? I think the idea was to appeal to a similar audience as GTA so it had to be more grounded. The proposed gameplay sounded a bit like Hitman where you have to scout out a location and then steal something. Or maybe like Teardown, though that wouldn’t come out until years later. There were also talks of smuggling using truck convoys, both driving them in co-op and trying to steal things from AI-driven ones. There were neat ideas but I don’t think anyone actually understood what the core of the game was.&lt;/p&gt;
    &lt;p&gt;So I don’t know what the game was trying to be, but if it wanted to be a multiplayer open-world Hitman, that sounds good in theory but I don’t know a single studio in the world that could pull that off. It sounds really really difficult. Even a single-player Hitman is really really difficult and requires very strong designers, which we had just demonstrated with JC4 that we don’t have.&lt;/p&gt;
    &lt;p&gt;The game director on the project was going to be the former narrative director from JC4. This made no sense because the narrative was not good and in fact was a bit mismanaged (though we had problems with a vendor, and things could be blamed on that). The reason he ran the project was that somebody from the NYC studio had to lead this game because it would have sucked to just be a subsidiary helping out Stockholm, but Francesco had just been game director on JC4 which wasn’t good, and nobody else made sense either. The person who eventually was the lead only made sense because everyone liked him. And he was really good at selling things. So he pitched a great game concept to Microsoft and got the project signed. (and also no project pitch from the Stockholm office got signed, so there were no alternatives)&lt;/p&gt;
    &lt;p&gt;I remember talking to him explaining that the game he was trying to make sounded really difficult from a gameplay perspective. Especially how it wasn’t supposed to be a shooter. Like everyone knows how to make a racing game because racing is inherently fun. Or a platformer or a shooter because platforming and shooting are fun interactions on a computer. And while this game had bits of driving and bits of shooting and maybe even bits of platforming, it wasn’t going to be about any of those things and I couldn’t see which part would be inherently fun. The various ideas that I had heard all sounded really difficult, like they weren’t going to work out. And after they wouldn’t work out they would be replaced with shooting, which would have gone against the design. The response I got did not make me think that he appreciated just how difficult of a project he was taking on.&lt;/p&gt;
    &lt;p&gt;I only know how it went from second-hand reports of people who stayed longer. Apparently they kept on trying new things saying “now the game will be this” for six months, then when that didn’t work out they said “now the game will be like that instead” for six months. I don’t know if they ever got out of that mode.&lt;/p&gt;
    &lt;p&gt;Apparently there were negotiations about starting development on another project, but that fell through. So eventually the studio shut down.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s All About Culture, in a Wider Sense&lt;/head&gt;
    &lt;p&gt;In my mind this is all about culture. You couldn’t fulfill the ambition of the NYC office with the culture of the NYC office.&lt;/p&gt;
    &lt;p&gt;To explain what I mean by that, let’s talk about countries for a second. The main reason why China in the 21st century is doing much better than China in the middle of the 20th century is culture. I mean culture in the Joseph Heinrich sense. So not just “books” or “music” or “art” but also things like “how companies are run” and “what you learn in school” and “what politicians care about” and what I talk about in the “culture” chapter above, as “work/life balance” and other unmeasurable things. You can invoke lots of other reasons but the culture of China in the middle of the 20th century was very limiting. You just can’t become a top tier country with that culture.&lt;/p&gt;
    &lt;p&gt;What specifically is holding you back? Lots and lots of little things, and then a few big things like the Great Leap Forward. But even without that, the lots of little things will just constantly drag you down. They’ll act like a mix of gravity and friction, keeping you from the top tier countries.&lt;/p&gt;
    &lt;p&gt;This is also obvious if look at other examples. You might compare Denmark and Afghanistan and say that Denmark got lucky with Novo Nordisk and Afghanistan got all kinds of unlucky. But really the main difference is culture and all the millions of little things that Denmark does right and Afghanistan does wrong. Afghanistan is being dragged down by its culture and many other issues could be overcome with better culture.&lt;/p&gt;
    &lt;p&gt;When Avalanche Studios NYC was founded, Avalanche Studios as whole wanted to step up to the next tier of game developers. There were big investments to make the engine ready for next gen, maybe even license out the engine. Use that money to reinvest and then grow the studio. But we didn’t have the culture to level up. I don’t think Avalanche Studios Stockholm had it either.&lt;/p&gt;
    &lt;p&gt;There are also no secrets about how to do this. The paper to read here is “The Mundanity of Excellence.” It talks about excellence in the context of olympic swimmers. Having now worked at other places, I think its thesis for how excellence is achieved is spot on:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Excellence is mundane. Superlative performance is really a confluence of dozens of small skills or activities, each one learned or stumbled upon, which have been carefully drilled into habit and then are fitted together in a synthesized whole. There is nothing extraordinary or super-human in any one of those actions; only the fact that they are done consistently and correctly, and all together, produce excellence. When a swimmer learns a proper flip turn in the freestyle races, she will swim the race a bit faster; then a streamlined push off from the wall, with the arms squeezed together over the head, and a little faster; then how to place the hands in the water so no air is cupped in them, then how to lift them over the water; then how to lift weights to properly build strength, and how to eat the right foods, and to wear the best suits for racing, and on and on. Each of those tasks seems small in itself, but each allows the athlete to swim a bit faster. And having learned and consistently practiced all of them together, and many more besides, the swimmer may compete in the Olympic Games. The winning of a gold medal is nothing more than the synthesis of a countless number of such little things – even if some of them are done unwittingly or by others, and thus called “luck.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I left video games because I was looking for a place that can ship high quality software and found it outside of video games. (I would have stayed in video games if there had been such options in NYC) Having now worked at a place that has a better culture, there really are no tricks. I think the new place writes just as many bugs as Avalanche Studios NYC did. But the rest of the culture means the bugs don’t cause as many problems. Most of them get caught before being submitted to main, and the ones that do make it through don’t cause as much damage because there are layers of defense. It just takes lots of small activities and “the fact that they are done consistently and correctly, and all together” as in the quote above. My advice for Avalanche would be to do small things, like write automated tests, do code review properly, actually share code between projects instead of copy-pasting the engine and diverging, run the autobuilder before a change gets submitted to make sure the build can’t break, do postmortems when there was a big enough issue that others noticed and actually do most of the follow-up actions that you agreed on, be curious about bugs when you run into them and try to fix them proactively. No magic, just small improvements done consistently for a better process.&lt;/p&gt;
    &lt;p&gt;I remember wasting days at Avalanche on trying to track down this weird crash. It was something about the GPU doing delayed work and crashing in a spot where you couldn’t see what originally caused the issue. Weeks later a technical artist was also trying to track this one down and together we made some good progress and finally found out “it must be something with X,” where X was written by one of our graphics programmers. We talk to him and he says right away “oh yeah X crashes sometimes.” He had known. And just left it in. And this guy wasn’t even the bad guy I had talked about before who was causing all the fires. Nothing happened as a result of this episode. The bug got fixed and that’s it. (this is obviously crazy, but at Avalanche Studios this was so normal that I have to say what to do instead: at least have a post-mortem about how this could have happened. Then decide on actions to prevent this from happening again and do those actions)&lt;/p&gt;
    &lt;p&gt;With the culture that Avalanche NYC had, you could either throw more money/people at the problem or work crazy overtime, or you could be stuck at a certain quality level. We didn’t have lots of people and we didn’t want to work crazy overtime, so we were stuck. And the companies that just throw more money at the problem are also a bit stuck. When things break at the same pace that you’re fixing them, you end up with some parts of the next game being better and some parts being worse, but ultimately you end up at the same quality level.&lt;/p&gt;
    &lt;p&gt;I think JC3 was our limit. It was a better version of JC2, and that’s all we could do. In JC4 we had ambition to do better than in JC3 and as a result we ended up in a Red Queen’s race where we just couldn’t make progress and actually ended up worse than where we started. Contraband wanted to be even better and as a result just couldn’t ship.&lt;/p&gt;
    &lt;p&gt;I actually think the company strategy was kinda smart. I think it made sense to expand to the US and to develop the new terrain at the time when they did. I think it made sense to try to redo the terrible editor code and to try to license out the engine. It made sense to go multiplayer for Contraband and to do it as a co-op game. But then there was lots of bad execution which led to us constantly being in trouble and constantly being behind, which led to us sticking to bad decisions.&lt;/p&gt;
    &lt;p&gt;I am also worried about the rest of Avalanche Studios. They were growing a lot when I left, and were working on several self-published titles like Second Extinction and Ravenbound and The Angler. Those games have shipped, to various levels of disappointment (I haven’t played any, but if I had to guess, The Angler might actually be good, the others not), and I haven’t heard about anything since. It’s not good that it’s been almost six years and they have only shipped some middling games that were already in development six years ago. (and I literally have no inside information about what they are working on) And also the track record wasn’t great before that. Rage 2 didn’t get great reviews either, and neither did theHunter: Primal or Generation Zero. They weren’t terrible games, just disappointing compared to what they could have been. And internally there wasn’t much reflection on that, the official messaging was always “they made their money back and were a success for us” which isn’t that satisfying.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lessons Learned&lt;/head&gt;
    &lt;p&gt;Speaking of which, lets get to the lessons we learned here. One of them is that you can’t develop games any more like we did for old console generations. And one of them was that Avalanche was not good at learning lessons. It’s not like they didn’t react to mistakes, but they often drew the wrong conclusions. Lets get to these in order.&lt;/p&gt;
    &lt;head rend="h3"&gt;Short-Lived Code&lt;/head&gt;
    &lt;p&gt;Games used to have development cycles where it made sense to not write super high quality code. When going from SNES to Playstation to Playstation 2 to Playstation 3 you were probably going to rewrite much of your engine anyway. This stopped being true with the Playstation 4, just as I joined the company. Not just because you have to live with the same code for multiple projects, but also because each project takes longer and has more code, so the crappy practices bite you before you even ship the first game. Avalanche was stuck in the old mindset for way too long. Programmers as a craft know how to do this better, but game developers were very reluctant to adopt practices that slow down individual changes in order for the project as a whole to move faster. They like to point out how things also are done badly in other industries (Slow buggy web apps, OOP design-patterns, micro-services) and as a result they reject the successful practices, too.&lt;/p&gt;
    &lt;p&gt;It was also clear that others in the games industry had the same problem. Mass Effect: Andromeda came out the year prior and got panned for bad character art and bad animations compared to earlier Mass Effect games (from earlier console generations). Or look at that comparison video of Far Cry 2 and Far Cry 5, which came out the same year as JC4.&lt;/p&gt;
    &lt;p&gt;What can be done about this? The easiest thing is to stop having your own engine. That limits how much damage your programmers can do. The more difficult thing is figuring out how to learn the right lessons.&lt;/p&gt;
    &lt;head rend="h3"&gt;Postmortems&lt;/head&gt;
    &lt;p&gt;Going back to that “culture of countries” comparison, while we obviously didn’t have anything nearly as bad as the “Great Leap Forward”, it’s helpful to think about how such a colossal fuckup can happen. It’s almost calming to see that a country that can evidently be very competent can also have one of the biggest screw-ups in history. But thinking about it, I think one essential ingredient for messing up really badly is that you don’t learn lessons.&lt;/p&gt;
    &lt;p&gt;We actually did try a bunch of improvements. We tried code review for a while but we did it badly and eventually it fizzled out. We kept on trying to share code between games and we kept on doing it badly, causing us to do very little of it. I did convince some people to write automated tests, but they would write one or two bad tests and then give up. Remember that QA person who was ruining the editor tests? He also wrote some other tests that were worse than useless. They would break all the time on nonsense issues and wouldn’t actually tell you what the problem is. This is actually normal when you first start writing tests, because it’s a bit of a skill. So you need to learn your lessons, delete those tests and write better ones. This never happened.&lt;/p&gt;
    &lt;p&gt;And when we did learn lessons, it was non-public and it was not clear how we came to exactly those lessons. E.g. the “old editor” was clearly a problem when I joined and that was the reason why there was going to be a new editor. The lesson they learned was to not write it in C++ and to write it in Python instead. This certainly was going to decrease crashes due to memory-safety issues. But would it fix anything else? And wouldn’t you increase crashes due to lack of type-safety? In hindsight the correct thing was to instead do the tedious work of slowly fixing all the issues with the editor over a couple years.&lt;/p&gt;
    &lt;p&gt;Coming to that conclusion earlier is difficult, but I know how to not do it: If you don’t have a postmortem, you certainly won’t learn from last time. If there was a postmortem for the Python decision, it was never shared with me, and I was the only other tools-programmer on the team, so that would be very strange. I remember walking towards lunch with Linus Blomberg when he was visiting NYC and he was very reluctant to talk about the whole thing. He certainly wasn’t going to write a postmortem. And if you don’t write a postmortem about bad decisions, you make more bad decisions like writing multiple new scripting systems that all didn’t solve the issue. And there were no postmortems shared for any of those either.&lt;/p&gt;
    &lt;p&gt;Even if people “learned their lesson” from these mistakes, who’s to say that they were the right lessons? Part of the value of a postmortem that’s widely shared is that you get lots of comments on the document and people will tell you all the important things you forgot. (as I expect will happen with this blog post, because my view is necessarily one-sided. E.g. I remember designers being frustrated that even if we learned lessons from playtests, we didn’t act on them. But that has to be someone else’s perspective)&lt;/p&gt;
    &lt;head rend="h3"&gt;Reviews&lt;/head&gt;
    &lt;p&gt;Another aspect of this was the lack of serious review. Meaning code review, but I’m not just talking about programmers, but also review for designers and artists.&lt;/p&gt;
    &lt;p&gt;I now spend more than 10% of my time doing review. It easily pays for itself. When it gets to be more than 30% I try to reduce my review obligations, so the right number is somewhere between those two.&lt;/p&gt;
    &lt;p&gt;Code review done properly helps with so many issues that we had. Your code has to actually be up to a certain standard to be possible to be reviewed at all. I remember people submitting things that were just straightforwardly wrong. Like multi-threaded code where a background thread writes to a data structure and the main thread reads it, and there is no synchronization between the two, at all. When reviewing multi-threaded code, if the synchronization is not clear, I leave a blocking comment that forbids release. If it’s too confusing to review, it shouldn’t be submitted.&lt;/p&gt;
    &lt;p&gt;I remember talking to a designer who had left Avalanche to work somewhere else, and hearing his appreciation that the new place challenged him, really, to explain why an idea would be fun. I remember hearing the description for a mission in a weekly demo of new features, and all I could think was “but why is this interesting?” I don’t think the designers at Avalanche were really challenged to explain why things were good.&lt;/p&gt;
    &lt;p&gt;I don’t know the process for artists, though I’m pretty sure some of the bad assets that made it into the finished game were not reviewed.&lt;/p&gt;
    &lt;p&gt;Review as a practice just helps all around. Spreads knowledge and responsibilities, raises standards, prevents heroes or leads from acting on their own. Just make sure that every change is reviewed by at least one other person and that it doesn’t get released until the review is finished. Then iterate on the process until everyone’s productive within those rules. (e.g. if you put a change up for me to review, it should be easy for me to run your version of the build)&lt;/p&gt;
    &lt;head rend="h2"&gt;Thesis&lt;/head&gt;
    &lt;p&gt;My overall thesis here has been that we were hampered by bad development practices and couldn’t achieve our ambition. If you have followed Avalanche Studios and looked at all their games, this might sound obvious. The development on the inside was exactly like you expect it to be from what you see in the shipped games. But I don’t think this awareness existed on the inside, or if people were aware they saw it as an unchangeable fact of life. If I had to guess, internally the number one reason would probably have been small team sizes and budgets compared to the size of game we were making. Which is a dangerous justification because sure, Generation Zero (random game developed by a small team) would have been better with a team twice the size, or with twice the time, but you don’t get to have twice the team size if your previous game was no good. You can either be stuck in a never-ending cycle of mediocrity, or you can figure out how to level up.&lt;/p&gt;
    &lt;p&gt;Video games as an industry will eventually figure this out. Just by evolution. Studios with bad culture will die out and studios with better culture will survive a little longer. If I ever get back into gaming I would try to optimize for productivity. Games don’t have to be as robust as the software running on the Mars rovers, and aiming for that would be a drag on productivity. But if something easily pays for itself, like having a build that’s not broken, or having automated tests for the main features of your game, then you should prioritize that. Don’t even get close to a capability trap. It makes sense that the tools programmer thinks productivity is a top priority, but it made no sense that everyone else always had something more important to do.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45412075</guid><pubDate>Mon, 29 Sep 2025 10:21:23 +0000</pubDate></item><item><title>DeepSeek-v3.2-Exp</title><link>https://github.com/deepseek-ai/DeepSeek-V3.2-Exp</link><description>&lt;doc fingerprint="fd4a9951d11b6a96"&gt;
  &lt;main&gt;
    &lt;p&gt;We are excited to announce the official release of DeepSeek-V3.2-Exp, an experimental version of our model. As an intermediate step toward our next-generation architecture, V3.2-Exp builds upon V3.1-Terminus by introducing DeepSeek Sparse Attention—a sparse attention mechanism designed to explore and validate optimizations for training and inference efficiency in long-context scenarios.&lt;/p&gt;
    &lt;p&gt;This experimental release represents our ongoing research into more efficient transformer architectures, particularly focusing on improving computational efficiency when processing extended text sequences.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;DeepSeek Sparse Attention (DSA) achieves fine-grained sparse attention for the first time, delivering substantial improvements in long-context training and inference efficiency while maintaining virtually identical model output quality.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To rigorously evaluate the impact of introducing sparse attention, we deliberately aligned the training configurations of DeepSeek-V3.2-Exp with V3.1-Terminus. Across public benchmarks in various domains, DeepSeek-V3.2-Exp demonstrates performance on par with V3.1-Terminus.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Benchmark&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek-V3.1-Terminus&lt;/cell&gt;
        &lt;cell role="head"&gt;DeepSeek-V3.2-Exp&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Reasoning Mode w/o Tool Use&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MMLU-Pro&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPQA-Diamond&lt;/cell&gt;
        &lt;cell&gt;80.7&lt;/cell&gt;
        &lt;cell&gt;79.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Humanity's Last Exam&lt;/cell&gt;
        &lt;cell&gt;21.7&lt;/cell&gt;
        &lt;cell&gt;19.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LiveCodeBench&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;74.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AIME 2025&lt;/cell&gt;
        &lt;cell&gt;88.4&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;HMMT 2025&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;83.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Codeforces&lt;/cell&gt;
        &lt;cell&gt;2046&lt;/cell&gt;
        &lt;cell&gt;2121&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Aider-Polyglot&lt;/cell&gt;
        &lt;cell&gt;76.1&lt;/cell&gt;
        &lt;cell&gt;74.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Agentic Tool Use&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;BrowseComp&lt;/cell&gt;
        &lt;cell&gt;38.5&lt;/cell&gt;
        &lt;cell&gt;40.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;BrowseComp-zh&lt;/cell&gt;
        &lt;cell&gt;45.0&lt;/cell&gt;
        &lt;cell&gt;47.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SimpleQA&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
        &lt;cell&gt;97.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SWE Verified&lt;/cell&gt;
        &lt;cell&gt;68.4&lt;/cell&gt;
        &lt;cell&gt;67.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SWE-bench Multilingual&lt;/cell&gt;
        &lt;cell&gt;57.8&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Terminal-bench&lt;/cell&gt;
        &lt;cell&gt;36.7&lt;/cell&gt;
        &lt;cell&gt;37.7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For TileLang kernels with better readability and research-purpose design, please refer to TileLang.&lt;/p&gt;
    &lt;p&gt;For high-performance CUDA kernels, indexer logit kernels (including paged versions) are available in DeepGEMM. Sparse attention kernels are released in FlashMLA.&lt;/p&gt;
    &lt;p&gt;We provide an updated inference demo code in the inference folder to help the community quickly get started with our model and understand its architectural details.&lt;/p&gt;
    &lt;p&gt;First convert huggingface model weights to the the format required by our inference demo. Set &lt;code&gt;MP&lt;/code&gt; to match your available GPU count:&lt;/p&gt;
    &lt;code&gt;cd inference
export EXPERTS=256
python convert.py --hf-ckpt-path ${HF_CKPT_PATH} --save-path ${SAVE_PATH} --n-experts ${EXPERTS} --model-parallel ${MP}&lt;/code&gt;
    &lt;p&gt;Launch the interactive chat interface and start exploring DeepSeek's capabilities:&lt;/p&gt;
    &lt;code&gt;export CONFIG=config_671B_v3.2.json
torchrun --nproc-per-node ${MP} generate.py --ckpt-path ${SAVE_PATH} --config ${CONFIG} --interactive&lt;/code&gt;
    &lt;code&gt;# H200
docker pull lmsysorg/sglang:dsv32

# MI350
docker pull lmsysorg/sglang:dsv32-rocm

# NPUs
docker pull lmsysorg/sglang:dsv32-a2
docker pull lmsysorg/sglang:dsv32-a3
&lt;/code&gt;
    &lt;code&gt;python -m sglang.launch_server --model deepseek-ai/DeepSeek-V3.2-Exp --tp 8 --dp 8 --page-size 64&lt;/code&gt;
    &lt;p&gt;vLLM provides day-0 support of DeepSeek-V3.2-Exp. See the recipes for up-to-date details.&lt;/p&gt;
    &lt;p&gt;This repository and the model weights are licensed under the MIT License.&lt;/p&gt;
    &lt;code&gt;@misc{deepseekai2024deepseekv32,
      title={DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention}, 
      author={DeepSeek-AI},
      year={2025},
}
&lt;/code&gt;
    &lt;p&gt;If you have any questions, please raise an issue or contact us at service@deepseek.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45412098</guid><pubDate>Mon, 29 Sep 2025 10:26:39 +0000</pubDate></item><item><title>What if I don't want videos of my hobby time available to the world?</title><link>https://neilzone.co.uk/2025/09/what-if-i-dont-want-videos-of-my-hobby-time-available-to-the-entire-world/</link><description>&lt;doc fingerprint="2c5ad9592ce694fd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What if I don't want videos of my hobby time available to the entire world?&lt;/head&gt;
    &lt;p&gt;I am very much enjoying my newly-resurrected hobby of Airsoft.&lt;/p&gt;
    &lt;p&gt;Running around in the woods, firing small plastic pellets at other people, in pursuit of a contrived-to-be-fun mission, turns out to be, well, fun.&lt;/p&gt;
    &lt;p&gt;I have also had to accept that, for some other players, part of that fun comes from making videos of their game days, and uploading them to YouTube.&lt;/p&gt;
    &lt;p&gt;They often have quite impressive setups, with multiple cameras - head, rear-facing from barrel of weapon, and scope cam - and clearly put time, money, and effort into doing this.&lt;/p&gt;
    &lt;p&gt;Great! Just like someone taking photos on their holidays, or when out and about, I can see the fun in it.&lt;/p&gt;
    &lt;p&gt;It is the “non-consensually publishing it online for the world to see” aspect which bugs me a bit.&lt;/p&gt;
    &lt;p&gt;In the handful of games that I have played, no-one has ever asked about consent of other participants.&lt;/p&gt;
    &lt;p&gt;There has been no “put on this purple lanyard if you don’t want to be included in the public version of the video” rule, which I’ve seen work pretty well at conferences I have attended (even if it is opt-out rather than consent).&lt;/p&gt;
    &lt;p&gt;I could, I suppose, ask each person that I see with a camera “would you mind not including me in anything you upload, please?”. And, since everyone with whom I’ve spoken at games, so far anyway, has been perfectly pleasant and friendly, I’d be hopeful that they would at least consider my request. I have not done this.&lt;/p&gt;
    &lt;p&gt;The impression I get is that this is just seen as part and parcel of the hobby: by running around in the woods of northern Newbury on a Sunday morning, I need to accept that I may well appear on YouTube, for the world to see.&lt;/p&gt;
    &lt;p&gt;I don’t love it, but it is not a big enough deal for me to make a fuss.&lt;/p&gt;
    &lt;head rend="h2"&gt;Other notes&lt;/head&gt;
    &lt;p&gt;I occasionally see people saying “well, if you don’t want to be in photos published online, don’t be in public spaces”.&lt;/p&gt;
    &lt;p&gt;This is nonsense, for a number of reasons. Clearly, one should be able to exist in society, including going outside one’s own home, without needing to accept this kind of thing.&lt;/p&gt;
    &lt;p&gt;In any case, here, the issue is somewhat different, since it is a private site, where people engage in private activity (a hobby).&lt;/p&gt;
    &lt;p&gt;But then I’ve seen the same at (private) conferences, with people saying “Of course I’m free to take photos of identifiable individuals without their consent and publish them online”.&lt;/p&gt;
    &lt;p&gt;Publishing someone’s photo online, without their consent, without another strong justification, just because they happen to be in view of one’s camera lens, feels wrong to me.&lt;/p&gt;
    &lt;p&gt;This isn’t about what is legal (although, in some cases, claims of legality may be poorly conceived), but around my own perceptions of a private life, and a dislike for the fact that, just because one can publish such things, that one should.&lt;/p&gt;
    &lt;head rend="h2"&gt;[Updated] Some more notes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I’m just blogging. Sharing my thoughts. I’m not trying to set anyone’s policy, demand that anyone takes anything down or stops doing anything, or change anyone’s view.&lt;/item&gt;
      &lt;item&gt;I am in the UK. Different places may well have different norms, laws, and expectations.&lt;/item&gt;
      &lt;item&gt;Yes, biodegradable BBs are available, although it is not a site requirement, and the site shop does not sell them. I have used them a couple of times, and I haven’t found them to shatter on impact as much as (non-biodegradable) tracer BBs. I tend to buy BBs from the site’s own shop, to support them.&lt;/item&gt;
      &lt;item&gt;Yes, I wear two-part face covering; goggles/glasses (depending on the heat and humity), and a lower face and ear mask. I prefer this to a full face mask. But players are still pretty obviously distinguishable, given differences in loadouts, patches they wear, and people shouting names.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;You may also like:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;My third Airsoft game day and perhaps I am finally getting the hang of it&lt;/item&gt;
      &lt;item&gt;My first Airsoft game day (Red Alert, Newbury)&lt;/item&gt;
      &lt;item&gt;Getting back into Airsoft (or at least thinking about it) via laser tag&lt;/item&gt;
      &lt;item&gt;No, you can't have my attention for free&lt;/item&gt;
      &lt;item&gt;Downloading YouTube subscriptions and channels automatically&lt;/item&gt;
      &lt;item&gt;How public is 'public'?&lt;/item&gt;
      &lt;item&gt;RevK's privacy-friendly GPS logger&lt;/item&gt;
      &lt;item&gt;CCTV or IP cameras outside your home, and the (UK) GDPR. It's easier than you think&lt;/item&gt;
      &lt;item&gt;Online safety, doing good, and inconvenient fundamental rights&lt;/item&gt;
      &lt;item&gt;Brave browser: less privacy-respectful than I was expecting&lt;/item&gt;
      &lt;item&gt;Detecting child sex abuse imagery in end-to-end encrypted communications in a privacy-respectful manner&lt;/item&gt;
      &lt;item&gt;Time for your compulsory home camera installation&lt;/item&gt;
      &lt;item&gt;Are you intruding on someoneâs privacy is you are actively doing OSINT on someone?&lt;/item&gt;
      &lt;item&gt;Online speech-to-text transcription and the ePrivacy directive&lt;/item&gt;
      &lt;item&gt;DNS-over-https on macOS and iOS&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45412419</guid><pubDate>Mon, 29 Sep 2025 11:28:06 +0000</pubDate></item><item><title>Tuning async IO in PostgreSQL 18</title><link>https://vondra.me/posts/tuning-aio-in-postgresql-18/</link><description>&lt;doc fingerprint="763af07f3cb210bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tuning AIO in PostgreSQL 18&lt;/head&gt;
    &lt;p&gt;PostgreSQL 18 was stamped earlier this week, and as usual there’s a lot of improvements. One of the big architectural changes is asynchronous I/O (AIO), allowing asynchronous scheduling of I/O, giving the database more control and better utilizing the storage.&lt;/p&gt;
    &lt;p&gt;I’m not going to explain how AIO works, or present detailed benchmark results. There have been multiple really good blog posts about that. There’s also a great talk from pgconf.dev 2025 about AIO, and a recent “Talking Postgres” podcast episode with Andres, discussing various aspects of the whole project. I highly suggest reading / watching those.&lt;/p&gt;
    &lt;p&gt;I want to share a couple suggestions on how to tune the AIO in Postgres 18, and explain some inherent (but not immediately obvious) trade-offs and limitations.&lt;/p&gt;
    &lt;p&gt;Ideally, this tuning advice would be included in the docs. But that requires a clear consensus on the suggestions, usually based on experience from the field. And because AIO is a brand new feature, it’s too early for that. We have done a fair amount of benchmarking during development, and we used that to pick the defaults. But that can’t substitute experience from running actual production systems.&lt;/p&gt;
    &lt;p&gt;So here’s a blog post with my personal opinions on how to (maybe) tweak the defaults, and what trade offs you’ll have to consider.&lt;/p&gt;
    &lt;head rend="h2"&gt;io_method / io_workers&lt;/head&gt;
    &lt;p&gt;There’s a handful of parameters relevant to AIO (or I/O in general). But you probably need to worry about just these two, introduced in Postgres 18:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;io_method = worker&lt;/code&gt;(options:&lt;code&gt;sync&lt;/code&gt;,&lt;code&gt;io_uring&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;io_workers = 3&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The other parameters (like &lt;code&gt;io_combine_limit&lt;/code&gt;) have reasonable defaults.
I don’t have great suggestions on how to tune them, so just leave those
alone. In this post I’ll focus on the two important ones.&lt;/p&gt;
    &lt;head rend="h3"&gt;io_method&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;io_method&lt;/code&gt; determines AIO actually handles requests - what process
performs the I/O, and how is the I/O scheduled. It has three possible
values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;sync&lt;/code&gt;- This is a “backwards compatibility” option, doing synchronous I/O with&lt;code&gt;posix_fadvice&lt;/code&gt;where supported. This prefetches data into page cache, not into shared buffers.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;worker&lt;/code&gt;- Creates a pool of “IO workers”, doing the actual I/O. When a backend needs to read a block from a data file, it inserts a request into a queue in shared memory. An I/O worker wakes up, does the&lt;code&gt;pread&lt;/code&gt;, puts it into shared buffers and notifies the backend.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;io_uring&lt;/code&gt;- Each backend has a&lt;code&gt;io_uring&lt;/code&gt;instance (a pair of queues) and uses it to perform the I/O. Except that instead of doing&lt;code&gt;pread&lt;/code&gt;it submits the requests through&lt;code&gt;io_uring&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The default is &lt;code&gt;io_method = worker&lt;/code&gt;. We did consider defaulting both to
&lt;code&gt;sync&lt;/code&gt; or &lt;code&gt;io_uring&lt;/code&gt;, but I think &lt;code&gt;worker&lt;/code&gt; is the right choice. It’s
actually “asynchronous”, and it’s available everywhere (because it’s
our implementation).&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;sync&lt;/code&gt; was seen as a “fallback” choice, in case we run into issues
during beta/RC. But we did not, and it’s not certain using &lt;code&gt;sync&lt;/code&gt; would
actually help, because it still goes through the AIO infrastructure.
You can still use &lt;code&gt;sync&lt;/code&gt; if you prefer to mimic older releases.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;io_uring&lt;/code&gt; is a popular way to do async I/O (and not just disk I/O!).
And it’s great, very efficient and lightweight. But it’s specific to
Linux, while we support a lot of platforms. We could have used
platform-specific defaults (similarly to &lt;code&gt;wal_sync_method&lt;/code&gt;). But it
seemed like unnecessary complexity.&lt;/p&gt;
    &lt;p&gt;Note: Even on Linux it’s hard to verify &lt;code&gt;io_uring&lt;/code&gt;. Some container
runtimes (e.g. containerd)
disabled &lt;code&gt;io_uring&lt;/code&gt; support a while back, because of security risks.&lt;/p&gt;
    &lt;p&gt;None of the &lt;code&gt;io_method&lt;/code&gt; options is “universally superior.” There’ll
always be workloads where A outperforms B and vice versa. In the end,
we wanted most systems to use AIO and get the benefits, and we wanted
to keep things simple, so we kept &lt;code&gt;worker&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Advice: My advice is to stick to &lt;code&gt;io_method = worker&lt;/code&gt;, and to adjust
the &lt;code&gt;io_workers&lt;/code&gt; value (as explained in the following section).&lt;/p&gt;
    &lt;head rend="h3"&gt;io_workers&lt;/head&gt;
    &lt;p&gt;The Postgres defaults are very conservative. It will start even on a tiny machine like Raspberry Pi. Which is great! The flip side is it’s terrible for typical database servers which tend to have much more RAM/CPU. To get good performance on such larger machines, you need to adjust a couple parameters (&lt;code&gt;shared_buffers&lt;/code&gt;, &lt;code&gt;max_wal_size&lt;/code&gt;, &amp;amp;mldr;).&lt;/p&gt;
    &lt;p&gt;I wish we had an automated way to pick “good” initial values for these basic parameters, but it’s way harder than it looks. It depends a lot on the context (e.g. other stuff might be running on the same system). At least there are tools like PGTune that will recommend sensible values &amp;amp;mldr;&lt;/p&gt;
    &lt;p&gt;This certainly applies to the &lt;code&gt;io_workers = 3&lt;/code&gt; default, which creates
just 3 I/O workers. That may be fine on a small machine with 8 cores,
but it’s definitely not enough for 128 cores.&lt;/p&gt;
    &lt;p&gt;I can actually demonstrate this using results from a benchmark I did as input for picking the &lt;code&gt;io_method&lt;/code&gt; default. The benchmark generates
a synthetic data set, and then runs queries matching parts of the data
(while forcing a particular scan type).&lt;/p&gt;
    &lt;p&gt;Note: The benchmark (along with scripts, a lot of results and a much more detailed explanation) was originally shared in the pgsql-hackers thread about the &lt;code&gt;io_method&lt;/code&gt; default. Look at that thread for more
details and feedback from various other people. The presented results
are from a small workstation with Ryzen 9900X (12 cores/24 threads),
and 4 NVMe SSDs (in RAID0).&lt;/p&gt;
    &lt;p&gt;Here’s a chart comparing query timing for different &lt;code&gt;io_method&lt;/code&gt; options
[PDF]:&lt;/p&gt;
    &lt;p&gt;Each color is a different &lt;code&gt;io_method&lt;/code&gt; value (17 is “Postgres 17”).
There are two data data series for “worker”, with different numbers
of workers (3 and 12). This is for two data sets:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;uniform - uniform distribution (so the I/O is entirely random)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;linear_10 - sequential with a bit of randomness (imperfect correlation)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The charts show a couple very interesting things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;index scans -&lt;/p&gt;&lt;code&gt;io_method&lt;/code&gt;has no effect, which makes perfect sense because index scans do not use AIO yet (all the I/O is synchronous).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;bitmap scans - The behavior is a lot messier. The&lt;/p&gt;&lt;code&gt;worker&lt;/code&gt;method performs best, but only with 12 workers. With the default 3 workers it actually performs poorly for low selectivity queries.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;sequential scans - There’s a clear difference between the methods.&lt;/p&gt;&lt;code&gt;worker&lt;/code&gt;is the fastest, about twice as faster than&lt;code&gt;sync&lt;/code&gt;(and PG17).&lt;code&gt;io_uring&lt;/code&gt;is somewhere in between.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The poor performance of &lt;code&gt;worker&lt;/code&gt; with 3 I/O workers for bitmap scans is
even more visible with log-scale y-axis [PDF]:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;io_workers=3&lt;/code&gt; configuration is consistently the slowest (in the
linear chart this was almost impossible to notice).&lt;/p&gt;
    &lt;p&gt;The good thing is that while I/O workers are not free, they are not too expensive either. So if you have extra workers, that’s probably better than having too few.&lt;/p&gt;
    &lt;p&gt;In the future, we’ll probably make this “adaptive” by starting/stopping workers based on demand. So we’d always have just the right number. There’s even a WIP patch, but it didn’t make it into Postgres 18. (This would be a good time to take a look and review it!)&lt;/p&gt;
    &lt;p&gt;Advice: Consider increasing &lt;code&gt;io_workers&lt;/code&gt;. I don’t have a great value
or formula to use, maybe something like 1/4 of cores would work?&lt;/p&gt;
    &lt;head rend="h2"&gt;Trade offs&lt;/head&gt;
    &lt;p&gt;There’s no universally optimal configuration. I saw suggestions to “use io_uring for maximum efficiency”, but the earlier benchmark clearly shows &lt;code&gt;io_uring&lt;/code&gt; being significantly slower than &lt;code&gt;worker&lt;/code&gt; for sequential
scans.&lt;/p&gt;
    &lt;p&gt;Don’t get me wrong. I love &lt;code&gt;io_uring&lt;/code&gt;, it’s a great interface. And the
advice is not “wrong” either. Any tuning advice is a simplification,
and there will be cases contradicting it. The world is never as simple
as the advice makes it seem. It hides the grotty complexity behind a
much simpler rule, that’s the whole point of having such advice.&lt;/p&gt;
    &lt;p&gt;So what are the trade offs and differences between the AIO methods?&lt;/p&gt;
    &lt;head rend="h3"&gt;bandwidth&lt;/head&gt;
    &lt;p&gt;One big difference between &lt;code&gt;io_uring&lt;/code&gt; and &lt;code&gt;worker&lt;/code&gt; is where the work
happens. With &lt;code&gt;io_uring&lt;/code&gt;, all the work happens in the backend itself,
while with &lt;code&gt;worker&lt;/code&gt; this happens in a separate process.&lt;/p&gt;
    &lt;p&gt;This may have some interesting consequences on bandwidth, depending on how expensive it’s to handle the I/O. And it can be fairly expensive, because it involves:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the actual I/O&lt;/item&gt;
      &lt;item&gt;verifying checksums (which are enabled by default in Postgres 18)&lt;/item&gt;
      &lt;item&gt;copying the data into shared buffers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With &lt;code&gt;io_uring&lt;/code&gt;, all of this happens in the backend itself. The I/O part
may be more efficient, but the checksums / &lt;code&gt;memcpy&lt;/code&gt; can be a bottleneck.
With &lt;code&gt;worker&lt;/code&gt;, this work is effectively divided between the workers. If
you have one backend and 3 workers, the limits are 3x higher.&lt;/p&gt;
    &lt;p&gt;Of course, this goes the other way too. If you have 16 connections, then with &lt;code&gt;io_uring&lt;/code&gt; this is 16 processes that can verify checksums, etc.
With &lt;code&gt;worker&lt;/code&gt;, the limit is whatever &lt;code&gt;io_workers&lt;/code&gt; is set to.&lt;/p&gt;
    &lt;p&gt;This is where my advice to set &lt;code&gt;io_workers&lt;/code&gt; to ~25% of the cores comes
from. I can imagine going higher, possibly up to one IO worker per core.
In any case, 3 seems clearly too low.&lt;/p&gt;
    &lt;p&gt;Note: I believe the ability to spread costs over multiple processes is why &lt;code&gt;worker&lt;/code&gt; outperforms &lt;code&gt;io_uring&lt;/code&gt; for sequential scans. The ~20%
difference seems about right for checksums and memcpy in this benchmark.&lt;/p&gt;
    &lt;head rend="h3"&gt;signals&lt;/head&gt;
    &lt;p&gt;Another important detail is the cost of inter-process communication between the backend and the IO worker(s), which is based on UNIX signals. Performing an I/O looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;backend adds a read request to a queue in shared memory&lt;/item&gt;
      &lt;item&gt;backend sends a &lt;code&gt;signal&lt;/code&gt;to a IO worker, to wake it up&lt;/item&gt;
      &lt;item&gt;IO worker performs the I/O requested by the backend, and copies the data into shared buffers&lt;/item&gt;
      &lt;item&gt;IO worker sends a &lt;code&gt;signal&lt;/code&gt;the backend, notifying it about the I/O completion&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In the worst case, this means a round trip with 2 signals per 8K block. The trouble is, signals are not free - a process can only do a finite number of those per second.&lt;/p&gt;
    &lt;p&gt;I wrote a simple benchmark, sending signals between two processes. On my machines, this reports 250k-500k round trips per second. If each 8K block needs a round trip, this means 2-4GB/s. That’s not a lot, especially considering the data may already be in page cache, not just for cold data read from storage. According to a test copying data from page cache, a process can do 10-20GB/s, so about 4x more. Clearly, signals may be a bottleneck.&lt;/p&gt;
    &lt;p&gt;Note: The exact limits are hardware-specific, and may be much lower on older machines. But the general observation holds on all machines I have access to.&lt;/p&gt;
    &lt;p&gt;The good thing is this only affects a “worst case” workload, reading 8KB pages one by one. Most regular workloads don’t look like this. Backends usually find a lot of buffers in shared memory already (and then no I/O is needed). Or the I/O happens in larger chunks thanks to look-ahead, which amortizes the signal cost over many blocks. I don’t expect this to be a serious problem.&lt;/p&gt;
    &lt;p&gt;There’s a longer discussion about the AIO overheads (not just due to signals) in the index prefetching thread.&lt;/p&gt;
    &lt;head rend="h3"&gt;file limit&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;io_uring&lt;/code&gt; doesn’t need any IPC, so it’s not affected by the signal
overhead, or anything like that. But &lt;code&gt;io_uring&lt;/code&gt; has limits too, just in
a different place.&lt;/p&gt;
    &lt;p&gt;For example, each process is subject to per-process bandwidth limits (e.g. how much memcpy can a single process do). But judging by the page-cache test, those limits are fairly high - 10-20GB/s, or so.&lt;/p&gt;
    &lt;p&gt;Another thing to consider is that &lt;code&gt;io_uring&lt;/code&gt; may need a fair number of
file descriptors. As explained in this pgsql-hackers
thread:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The issue is that, with io_uring, we need to create one FD for each possible child process, so that one backend can wait for completions for IO issued by another backend [1]. Those io_uring instances need to be created in postmaster, so they’re visible to each backend. Obviously that helps to much more quickly run into an unadjusted soft RLIMIT_NOFILE, particularly if max_connections is set to a higher value.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So if you decide to use &lt;code&gt;io_uring&lt;/code&gt;, you may need to adjust &lt;code&gt;ulimit -n&lt;/code&gt;
too.&lt;/p&gt;
    &lt;p&gt;Note: This is not the only place in Postgres code where you may run into the limit on file descriptors. About a year ago I posted a patch idea related to file descriptor cache. Each backend keeps up to &lt;code&gt;max_files_per_process&lt;/code&gt; open file descriptors,
and by default that GUC is set to 1000. That used to be enough, but with
partitioning (or schema per tenant) it’s fairly easy to trigger a storm
of expensive open/close calls. That’s a separate (but similar) issue.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;AIO is a massive architectural change, and in Postgres 18 it has various limitations. It only supports reads, and some operations still use the old synchronous I/O. Those limitations are not permanent, and should be addressed in future releases.&lt;/p&gt;
    &lt;p&gt;Based on the discussion in this blog post, my tuning advice is to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Keep the&lt;/p&gt;&lt;code&gt;io_method = worker&lt;/code&gt;default, unless you can demonstrate the&lt;code&gt;io_uring&lt;/code&gt;actually works better for your workload. Use&lt;code&gt;sync&lt;/code&gt;only if you need a behavior as close to Postgres 17 as possible (even if it means being slower in some cases).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Increase&lt;/p&gt;&lt;code&gt;io_workers&lt;/code&gt;to a value considering the total number of cores. Something like 25% cores seems reasonable, possibly even 100% in extreme cases.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you come up with some interesting observations, please report them either to me or (even better) to the pgsql-hackers, so that we can consider that when adding tuning advice to the docs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45412494</guid><pubDate>Mon, 29 Sep 2025 11:42:04 +0000</pubDate></item><item><title>To AI or Not to AI</title><link>https://antropia.studio/blog/to-ai-or-not-to-ai/</link><description>&lt;doc fingerprint="67a49b141ba70ddf"&gt;
  &lt;main&gt;
    &lt;p&gt;To AI or not to AI&lt;/p&gt;
    &lt;p&gt;A two-week experiment building an app with full AI assistance, exploring both the promise and frustrations of LLM-based development workflows.&lt;/p&gt;
    &lt;p&gt;There is no denying that LLMs are the new kid on the block. Our marketing director (that’d be me) said that if we don’t write something about it, we will be left behind, so we ran an experiment, we went hardcore on it, and… TL;DR, we don’t fully buy it, yet.&lt;/p&gt;
    &lt;p&gt;What a way to discourage you from reading the article, huh? Well, the devil is in the details, so if you want to understand why we aren’t jumping on the full AI wagon yet, we recommend you read the post.&lt;/p&gt;
    &lt;head rend="h1"&gt;The experiment&lt;/head&gt;
    &lt;p&gt;We have been brewing (strange choice of a word, you’ll understand why in a moment) a prototype for a couple of months with one of our dearest friends, Bernardo. While the two of us were helping my wife with her shop, we struggled with her Facebook Ads account. I don’t know if you ever had the opportunity to play with Facebook Ads, but our experience was quite terrible.&lt;/p&gt;
    &lt;p&gt;Look at that! Once you get pass the big chaos this dashboard is, you start noticing the little inconsistencies here and there, like why are there two different tones of red for notifications? 🙃&lt;/p&gt;
    &lt;p&gt;We are not here to judge, though. In general, ads are one hell of a problem to solve, especially if you have the size of Facebook. One of the (possibly multiple) reasons the UI is so cluttered is that it’s trying to do everything. Filtering, reports, ad creation, etc. It’s pure chaos. That’s when we decided to make our own simplified version of Facebook Ads by using their API. Something that would help our use case and our use case only.&lt;/p&gt;
    &lt;p&gt;And so adbrew was born. What a great opportunity to uncover the full potential of (generative) AIs, or so we thought.&lt;/p&gt;
    &lt;head rend="h1"&gt;The approach&lt;/head&gt;
    &lt;p&gt;We started following some AI-related accounts and studying their workflows. Chose a solid tech stack (Remix a.k.a. React Router v7, not very happy with it, but that’s for another post) and subscribed to Claude Code.&lt;/p&gt;
    &lt;p&gt;We spent our first hours tweaking the prompts, setting all the usual DX tools in place (in hopes it’d help the AI) and started defining issues.&lt;/p&gt;
    &lt;p&gt;Our daily routine would become something like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Define issues.&lt;/item&gt;
      &lt;item&gt;Ask the AI to implement the issue at hand.&lt;/item&gt;
      &lt;item&gt;Back and forth with the AI, refining the requirements.&lt;/item&gt;
      &lt;item&gt;Review generated code in detail.&lt;/item&gt;
      &lt;item&gt;Commit code, push and deploy.&lt;/item&gt;
      &lt;item&gt;Repeat.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From time to time we’d refine the Guidelines file, added MCPs, automatic checks, and more.&lt;/p&gt;
    &lt;head rend="h1"&gt;The problems&lt;/head&gt;
    &lt;p&gt;This experiment lasted 2 weeks. And as days passed, we grew more and more frustrated with it. At first, we justified it by the fact that this was an entirely new way of (VIBE) coding, and we were just not used to it. But we kept tweaking the flow, adjusting expectations, trusting the process, and… the frustration would only grow.&lt;/p&gt;
    &lt;p&gt;We want to pinpoint some of the problems we had with it, in an attempt to fully understand if this is something that can be solvable in the future, if it’s inherent to vibing, or if it’s just us doing it wrong.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;There is never enough context. We learned quickly that the more context we provided and the smaller the issues, the better the results. However, no matter how much context we provided, the AI would still mess things up because it didn’t ask us for feedback. AI would just not understand if it didn’t have enough information to finish a task, it would assume, a lot, and fail.&lt;/item&gt;
      &lt;item&gt;No maintainability. We couldn’t make it abstract things away or reuse code at all. If we asked the AI to solve a task that was already partially solved, it would just replicate code all over the project. We’d end up with three different card components. Yes, this is where reviews are important, but it’s very tiring to tell the AI for the nth time that we already have a &lt;code&gt;Text&lt;/code&gt;component with defined sizes and colors. Adding this information to the guidelines didn’t work BTW.&lt;/item&gt;
      &lt;item&gt;No flow. As the AI was solving an issue, we stared at the screen, trying to catch a mistake in their reasoning to stop it as soon as possible. We tried YOLOing as well, and keep writing issues at the same time, but we couldn’t find ~30-minute slots dedicated to one single task. This killed any momentum we tried to build.&lt;/item&gt;
      &lt;item&gt;Hallucinations. The Facebook API is a complex one by nature. On top of that, there are endpoints that are not documented (thanks to StackOverflow answers for clearing this up), and their SDK is poorly typed (They truly love &lt;code&gt;Record&amp;lt;string, any&amp;gt;&lt;/code&gt;). If we mix this with the confidence of the AI, we have a recipe for disaster. It would make up parameters, endpoints and more. Now multiply that for every other framework/library/API we use (Tailwind, React Router, dayjs, pino, etc.).&lt;/item&gt;
      &lt;item&gt;Pareto is more present than ever. We like to think of tasks as trees. We’d start with a coarse-grained idea (the root) and turn that into more concrete ideas and tasks (the branches). It’s at that moment when corner cases, cross-feature interactions, transversal tasks (logging, tracking, etc.), and more appear. In our experiment, we lost the ability to uncover those, and so we ended up with something that looked like it worked, but was full of inconsistencies and bugs. It’s relatively easy to get the 80% of the solution done with the AI, but we still had to spend 80% of our time to make it truly work.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;The outcome&lt;/head&gt;
    &lt;p&gt;After these two weeks, we decided to stop trying. The code was getting larger and messy each day, and we were losing control of it. More importantly, we were not enjoying the process, and the results were simply not there.&lt;/p&gt;
    &lt;p&gt;We spent another two weeks back with our classic workflow, cleaning up when needed and marveling at the things we just missed in the reviews (not the AI to blame, but ourselves).&lt;/p&gt;
    &lt;p&gt;Not perfect, but a great improvement over the original, especially for our workflow. If you work with FB ads, visit getadbrew.com.&lt;/p&gt;
    &lt;head rend="h1"&gt;Our opinion&lt;/head&gt;
    &lt;p&gt;We already use AI in our daily work. And we use it for many things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Powerful search engine. One that, if it gets the response right, speeds up the search process and is even able to adapt the solutions to your specific context (and that’s mind-blowing by itself). But when it fails (and it often does), we can dismiss it quickly and go back to our regular approach of RTFM.&lt;/item&gt;
      &lt;item&gt;Rubber ducking. Throwing ideas and asking for alternative solutions, just to make sure we haven’t missed anything. One thing it is particularly good at is revealing keywords to deepen your research on certain topics. The results are much better if we look for “fibonacci lattices”, “geodesics” or “the golden spiral” rather than “ways to distribute points in a sphere”.&lt;/item&gt;
      &lt;item&gt;Code snippet assistant. The fifth time we write a &lt;code&gt;chunkify&lt;/code&gt;,&lt;code&gt;clamp&lt;/code&gt;or&lt;code&gt;mapValues&lt;/code&gt;function it gets tiring. The AI can help with those tiny snippets we use in every project and make the rest of our work more enjoyable.&lt;/item&gt;
      &lt;item&gt;Tests. While we still have a word on the scope, techniques and libraries we use for tests, we let LLMs to write some of them for us. If only, to uncover scenarios we didn’t think of initially.&lt;/item&gt;
      &lt;item&gt;Language-related tasks. We use it as a copy editor for commit messages, posts, issues and PRs. In all of these use cases, we have actually reversed the relationship with the AI: we ask the AI to review our work, rather than us reviewing its work.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So we will keep using the AI, and we will keep favoring local LLMs over cloud ones in an attempt to keep control of our data, even when that is not always possible.&lt;/p&gt;
    &lt;p&gt;We just don’t think we will incorporate AI to do more than that, given the current state of things. We will, however, keep an eye in case the technology changes fundamentally.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45412592</guid><pubDate>Mon, 29 Sep 2025 11:55:52 +0000</pubDate></item></channel></rss>