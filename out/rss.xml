<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 18 Sep 2025 13:01:13 +0000</lastBuildDate><item><title>Optimizing ClickHouse for Intel's 280 core processors</title><link>https://clickhouse.com/blog/optimizing-clickhouse-intel-high-core-count-cpu</link><description>&lt;doc fingerprint="fb5f1c762d32084a"&gt;
  &lt;main&gt;&lt;quote&gt;&lt;p&gt;This is a guest post from Jiebin Sun, Zhiguo Zhou, Wangyang Guo and Tianyou Li, performance optimization engineers at Intel Shanghai.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Intel's latest processor generations are pushing the number of cores in a server to unprecedented levels - from 128 P-cores per socket in Granite Rapids to 288 E-cores per socket in Sierra Forest, with future roadmaps targeting 200+ cores per socket. These numbers multiply on multi-socket systems, such servers may consist of 400 and more cores. The paradigm of "more, not faster cores" is driven by physical limitations. Since the end of Dennard scaling in the mid-2000s, power density concerns made it increasingly difficult to push single-thread performance further.&lt;/p&gt;&lt;p&gt;For analytical databases like ClickHouse, ultra-high core counts represent a huge opportunity and a complex challenge at the same time. While more cores theoretically mean more power to process tasks in parallel, most databases struggle to utilize the available hardware fully. Bottlenecks for parallel processing like lock contention, cache coherence, non-uniform memory access (NUMA), memory bandwidth, and coordination overhead become significantly worse as the core count increases.&lt;/p&gt;&lt;head rend="h2"&gt;Optimizing for ultra-high core counts #&lt;/head&gt;&lt;p&gt;Over the past three years, I dedicated a part of my professional life to understand and optimize ClickHouse's scalability on Intel Xeon ultra-high core count processors. My work focused on using various profiling and analysis tools - including perf, emon, and Intel VTune - to analyze all 43 ClickBench queries on ultra-high core count servers systematically, identifying bottlenecks, and optimizing the ClickHouse accordingly.&lt;/p&gt;&lt;p&gt;The results have been exciting: individual optimizations routinely deliver speedups of multiple times for individual queries, in some cases up to 10x. The geometric mean of all 43 ClickBench queries consistently improved between 2% and 10% per optimization. The results demonstrate that ClickHouse can be made to scale very well on ultra-high core count systems.&lt;/p&gt;&lt;head rend="h2"&gt;The core scaling challenge #&lt;/head&gt;&lt;p&gt;Beyond single-thread performance, several key challenges must be addressed to optimize performance in ultra-high core count systems.&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Cache coherence overhead: Bouncing cache lines costs CPU cycles.&lt;/item&gt;&lt;item&gt;Lock contention: Amdahl's Law becomes brutal for serialized code sections as little as 1% of the overall code.&lt;/item&gt;&lt;item&gt;Memory bandwidth: Utilizing the memory bandwidth effectively is a persistent challenge for data-intensive systems. Proper memory reuse, management and caching becomes critical.&lt;/item&gt;&lt;item&gt;Thread coordination: The cost of synchronizing threads grows super-linearly with the number of threads.&lt;/item&gt;&lt;item&gt;NUMA effects: The memory latency and bandwidth on multi-socket systems differs for local or remote memory.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;This blog post summarizes our optimizations for ClickHouse on ultra-high core count servers. All of them were merged into the main codeline and they now help to speed up queries in ClickHouse deployments around the globe.&lt;/p&gt;&lt;p&gt;Hardware setup: Our work was conducted on Intel's latest generation platforms, including 2 x 80 vCPUs Ice Lake (ICX), 2 x 128 vCPUs Sapphire Rapids (SPR), 1 x 288 vCPUs Sierra Forest (SRF), and 2 x 240 vCPUs Granite Rapids (GNR). SMT (Hyper-threading) was enabled, except on SRF which doesn't support SMT, and high-memory-bandwidth configurations.&lt;/p&gt;&lt;p&gt;Software setup: We used perf, Intel VTune, pipeline visualization, and other custom profiling infrastructure.&lt;/p&gt;&lt;head rend="h2"&gt;The five optimization areas #&lt;/head&gt;&lt;p&gt;Through a systematic analysis of ClickHouse's performance on ultra-high core count systems, I identified five areas with a high potential for optimization. Each area addresses a different aspect of scalability, and together they form a comprehensive approach to unlocking the full potential of ultra-high core count systems.&lt;/p&gt;&lt;p&gt;My journey began with the most fundamental challenge: lock contention.&lt;/p&gt;&lt;head rend="h2"&gt;Bottleneck 1: Lock contention #&lt;/head&gt;&lt;p&gt;According to queue theory, if N threads compete for the same lock, the cycles grows quadratically (N^2). For example, if we go from 8 to 80 cores, lock wait times increase by (80/8)² = 100x. Furthermore, cache coherence traffic for the mutex itself grows linearly with the core count, and the overhead for context switching compounds the problem. In such settings, every mutex becomes a potential scalability obstacle, and seemingly innocent synchronization patterns can bring entire systems to their knee.&lt;/p&gt;&lt;p&gt;The key insight is that lock contention isn't just about removing locks - it's about rethinking more fundamentally how threads coordinate and share state. This requires a multi-pronged approach: reducing the duration of critical sections, replacing exclusive locks (mutexes) with more granular synchronization primitives, and in some cases, eliminating shared state entirely.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 1.1: Query condition cache (PR #80247) #&lt;/head&gt;&lt;p&gt;After resolving jemalloc page faults (an optimization detailed below), a new hotspot appeared in &lt;code&gt;native_queued_spin_lock_slowpath&lt;/code&gt; which consumed 76% of the CPU time. This function was called from &lt;code&gt;QueryConditionCache::write&lt;/code&gt; on 2×240 vCPU systems.&lt;/p&gt;&lt;p&gt;What is the query condition cache?&lt;/p&gt;&lt;p&gt;ClickHouse’s query condition cache stores the results of WHERE filters, enabling the database to skip irrelevant data. In each SELECT query, multiple threads check if cache entries must be updated based on different criteria:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;the hash of the filter condition (as cache key)&lt;/item&gt;&lt;item&gt;the read mark ranges&lt;/item&gt;&lt;item&gt;whether the currently read part has a final mark&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The query condition cache is read-heavy, i.e. there are far more reads than writes, but the original implementation used exclusive locking for all operations.&lt;/p&gt;&lt;p&gt;Reducing critical paths in read-heavy workloads&lt;/p&gt;&lt;p&gt;This optimization demonstrates the importance of reducing the time spent holding locks, especially write locks in read-heavy code.&lt;/p&gt;&lt;p&gt;With 240 threads within a single query, the original code created a perfect storm:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Unnecessary write locks: All threads acquired exclusive locks, even when they only read cache entries.&lt;/item&gt;&lt;item&gt;Long critical sections: Expensive updates of cache entries were performed inside exclusive locks.&lt;/item&gt;&lt;item&gt;Redundant work: Multiple threads updated the same cache entries potentially multiple times.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Our optimization uses double-checked locking with atomic operations to resolve these bottlenecks:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;The code now first checks with atomic reads (no locking), respectively under a shared lock if an update is needed at all (fast path).&lt;/item&gt;&lt;item&gt;Next, the code checks immediately after acquiring an exclusive lock (slow path) if an update is actually required - another thread may have performed the same update in the meantime.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Implementation&lt;/p&gt;&lt;p&gt;Based on PR #80247, the optimization introduces a fast path which checks if an update is needed before acquiring the expensive write lock.&lt;/p&gt;&lt;code&gt;/// Original code
void updateCache(mark_ranges, has_final_mark)
{
    acquire_exclusive_lock(cache_mutex);  /// 240 threads wait here!

    /// Always update marks, even if already in desired state
    for (const auto &amp;amp; range : mark_ranges)
        set_marks_to_false(range.begin, range.end);

    if (has_final_mark):
        set_final_mark_to_false();

    release_lock(cache_mutex);
}
&lt;/code&gt;&lt;code&gt;
/// Optimized code
void updateCache(mark_ranges, has_final_mark)
{
    /// Fast path: Check if update is needed with a cheap shared lock
    acquire_shared_lock(cache_mutex);  /// Multiple threads can read simultaneously

    need_update = false;
    for (const auto &amp;amp; range : mark_ranges)
    {
        if (any_marks_are_true(range.begin, range.end))
        {
            need_update = true;
            break;
        }
    }

    if (has_final_mark &amp;amp;&amp;amp; final_mark_is_true())
        need_update = true;

    release_shared_lock(cache_mutex);

    if (!need_update)
        return;  /// Early out - no expensive lock needed!

    /// Slow path: Actually need to update, acquire exclusive lock
    acquire_exclusive_lock(cache_mutex);

    /// Double-check: verify update is still needed after acquiring lock
    need_update = false;
    for (const auto &amp;amp; range : mark_ranges)
    {
        if (any_marks_are_true(range.begin, range.end))
        {
            need_update = true;
            break;
        }
    }

    if (has_final_mark &amp;amp;&amp;amp; final_mark_is_true())
        need_update = true;

    if (need_update)
    {
        // Perform the actual updates only if still needed
        for (const auto &amp;amp; range : mark_ranges)
            set_marks_to_false(range.begin, range.end);

        if (has_final_mark)
            set_final_mark_to_false();
    }

    release_lock(cache_mutex);
}
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;The optimized code delivered impressive performance improvements:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;CPU cycles spend for &lt;code&gt;native_queued_spin_lock_slowpath&lt;/code&gt;reduced from 76% to 1%&lt;/item&gt;&lt;item&gt;The QPS of ClickBench queries Q10 and Q11 improved by 85% and 89%&lt;/item&gt;&lt;item&gt;The geometric mean of all ClickBench queries improved by 8.1%&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Optimization 1.2: Thread-Local Timer ID (PR #48778) #&lt;/head&gt;&lt;p&gt;ClickHouse's query profiler was frequently creating and deleting a global timer_id variable, causing lock contention during query profiling.&lt;/p&gt;&lt;p&gt;Query profiler timer usage&lt;/p&gt;&lt;p&gt;ClickHouse's query profiler uses POSIX timers to sample thread stacks in periodic intervals for performance analysis. The original implementation:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;created and deleted timer_id frequently during profiling, and&lt;/item&gt;&lt;item&gt;required global synchronization for all operations that read or write the timer.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Usage of shared data structures that needed protection with locks caused significant overhead.&lt;/p&gt;&lt;p&gt;Eliminating global state with thread-local storage&lt;/p&gt;&lt;p&gt;Here, we eliminated lock contention by thread-local storage, removing the need for shared state. Now, each thread has its own timer_id. This avoids shared state and the overhead of thread synchronization. To update a timer, it is no longer required to acquire locks.&lt;/p&gt;&lt;p&gt;Technical solution&lt;/p&gt;&lt;code&gt;/// Original code
class QueryProfiler
{
    static global_mutex timer_management_lock

    void startProfiling()
    {
        timer_id = create_new_timer();  /// Expensive system call

        acquire_exclusive_lock(timer_management_lock);  /// Global lock!
        update_shared_timer_state(timer_id);  /// Modify shared state
        release_lock(timer_management_lock);
    }

    void stopProfiling()
    {
        acquire_exclusive_lock(timer_management_lock);
        cleanup_shared_timer_state(timer_id);
        release_lock(timer_management_lock);

        delete_timer(timer_id);
    }
}
&lt;/code&gt;&lt;code&gt;/// Optimized code
class QueryProfiler
{
    static thread_local timer_id per_thread_timer;
    static thread_local boolean timer_initialized;

    void startProfiling()
    {
        if (!timer_initialized)
        {
            per_thread_timer = create_new_timer();  /// Once per thread
            timer_initialized = true;
        }

        /// Reuse existing timer - no locks, no system calls!
        enable_timer(per_thread_timer);
    }

    void stopProfiling()
    {
        /// Just disable timer - no deletion, no locks!
        disable_timer(per_thread_timer);
    }
}
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;The new implementation has the following advantages:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;It eliminated timer-related lock contention hotspots from profiling traces&lt;/item&gt;&lt;item&gt;It reduced timer create/delete system calls through reuse&lt;/item&gt;&lt;item&gt;It makes profiling on ultra-high core count servers more scalable.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Thread-local storage can eliminate lock contention by removing the need for shared state. Global synchronization becomes unnecessary if threads maintain their own state.&lt;/p&gt;&lt;head rend="h2"&gt;Bottleneck 2: Memory management #&lt;/head&gt;&lt;p&gt;Memory optimization on ultra-high core count systems differs a lot from single-threaded memory management. Memory allocators themselves become contention points, memory bandwidth is divided across more cores, and allocation patterns that work fine on small systems can create cascading performance problems at scale. It is crucial to be mindful of how much memory is allocated and how memory is used.&lt;/p&gt;&lt;p&gt;This class of optimizations involves the allocator’s behavior, reducing pressure on memory bandwidth, and sometimes completely rethinking algorithms to eliminate memory-intensive operations altogether.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 2.1: Jemalloc Memory Reuse Optimization (PR #80245) #&lt;/head&gt;&lt;p&gt;This optimization is motivated by high page fault rates and excessive resident memory usage which we observed for certain aggregation queries on ultra-high core count systems.&lt;/p&gt;&lt;p&gt;Understanding two-level hash tables in ClickHouse&lt;/p&gt;&lt;p&gt;Aggregation in ClickHouse uses different hash tables, depending on the data type, data distribution and data size. Large aggregation states are maintained in ephemeral hash tables.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;The 1st level consists of 256 static buckets, each pointing to a 2nd level hash table.&lt;/item&gt;&lt;item&gt;2nd level hash tables grow independently of each other.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Memory reuse for two-level hash tables&lt;/p&gt;&lt;p&gt;At the end of an aggregation query, all hash tables used by the query are deallocated. In particular, the 256 sub-hash tables are deallocated and their memory is merged into larger free memory blocks.&lt;/p&gt;&lt;p&gt;jemalloc (as ClickHouse’s memory allocator) unfortunately prevented the reuse of merged memory blocks for future smaller allocations. This is because by default, only memory from blocks up to 64x larger than the requested size can be reused. This issue in jemalloc is very subtle but critical on ultra-high core count systems.&lt;/p&gt;&lt;p&gt;Based on jemalloc issue #2842, we noticed a fundamental problem with jemalloc’s memory reuse for the irregularly-sized allocations typical in two-level hash tables:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Extent management issue: When large allocations are freed, jemalloc fails to efficiently track and reuse these memory extents.&lt;/item&gt;&lt;item&gt;Size class fragmentation: Memory gets trapped in size classes that don't match future allocation patterns.&lt;/item&gt;&lt;item&gt;Metadata overhead: Excessive metadata structures prevent efficient memory coalescing.&lt;/item&gt;&lt;item&gt;Page fault amplification: New allocations trigger page faults instead of reusing existing committed pages.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;We identified jemalloc's &lt;code&gt;lg_extent_max_active_fit&lt;/code&gt; parameter as the root cause - it was too restrictive for ClickHouse's allocation patterns.&lt;/p&gt;&lt;p&gt;We contributed the fix to jemalloc PR #2842, but jemalloc didn’t have new stable releases for an extended period. Fortunately, we could resolve this issue through jemalloc's configuration parameters at compilation time.&lt;/p&gt;&lt;p&gt;Based on ClickHouse PR #80245, the fix involved tuning jemalloc's configuration parameters:&lt;/p&gt;&lt;code&gt;/// Original jemalloc configuration
JEMALLOC_CONFIG_MALLOC_CONF = "oversize_threshold:0,muzzy_decay_ms:0,dirty_decay_ms:5000"
/// lg_extent_max_active_fit defaults to 6, meaning memory can be reused from extents up to 64x larger than the requested allocation size
&lt;/code&gt;&lt;code&gt;/// Optimized jemalloc configuration
JEMALLOC_CONFIG_MALLOC_CONF = "oversize_threshold:0,muzzy_decay_ms:0,dirty_decay_ms:5000,lg_extent_max_active_fit:8"
/// lg_extent_max_active_fit is set to 8.
/// This allows memory reuse from extents up to 256x larger
/// than the requested allocation size (2^8 = 256x vs default 2^6 = 64x).
/// The 256x limit matches ClickHouse's two-level hash table structure (256 buckets).
/// This enables efficient reuse of merged hash table memory blocks.
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;The optimization improved&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;the performance of ClickBench query Q35 by 96.1%,&lt;/item&gt;&lt;item&gt;memory usage (VmRSS, resident memory) and page faults reduced for the same query went down by 45.4% and 71%, respectively.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The behavior of the memory allocator can have a dramatic impact on ultra-high core count systems.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 2.2: AST Query Rewriting for Memory Reduction (PR #57853) #&lt;/head&gt;&lt;p&gt;ClickBench query Q29 was memory-bound and bottlenecked in excessive memory accesses caused by redundant computations of the form &lt;code&gt;sum(column + literal)&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Understanding the memory bottleneck&lt;/p&gt;&lt;p&gt;ClickBench query Q29 contains multiple sum expressions with literals:&lt;/p&gt;&lt;code&gt;SELECT SUM(ResolutionWidth), SUM(ResolutionWidth + 1), SUM(ResolutionWidth + 2), 
       SUM(ResolutionWidth + 3), SUM(ResolutionWidth + 4), SUM(ResolutionWidth + 5), 
       SUM(ResolutionWidth + 6), SUM(ResolutionWidth + 7), SUM(ResolutionWidth + 8), 
       SUM(ResolutionWidth + 9), SUM(ResolutionWidth + 10), SUM(ResolutionWidth + 11), 
       SUM(ResolutionWidth + 12), SUM(ResolutionWidth + 13), SUM(ResolutionWidth + 14), 
       SUM(ResolutionWidth + 15), SUM(ResolutionWidth + 16), SUM(ResolutionWidth + 17), 
       SUM(ResolutionWidth + 18), SUM(ResolutionWidth + 19), SUM(ResolutionWidth + 20),
       -- ... continues up to SUM(ResolutionWidth + 89)
FROM hits;
&lt;/code&gt;&lt;p&gt;The original query execution&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Loaded column “ResolutionWidth” from storage once,&lt;/item&gt;&lt;item&gt;Compute expressions - 90 times, creating 90 temporary columns (one per expression),&lt;/item&gt;&lt;item&gt;Sum values performing 90 separate aggregation operations on each computed column.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Creating 90 temporary columns and running 90 redundant aggregations obviously created massive memory pressure.&lt;/p&gt;&lt;p&gt;Frontend query optimization for memory efficiency&lt;/p&gt;&lt;p&gt;This optimization demonstrates how better optimizer rules can reduce memory pressure by eliminating redundant computations. The key insight is that many analytical queries contain patterns that can be algebraically simplified.&lt;/p&gt;&lt;p&gt;The optimization recognizes that &lt;code&gt;sum(column + literal)&lt;/code&gt; can be rewritten to &lt;code&gt;sum(column) + count(column) * literal&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;ClickBench query Q29 sped up by 11.5x on a 2×80 vCPU system.&lt;/item&gt;&lt;item&gt;The geometric mean of all ClickBench queries saw a 5.3% improvement overall.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;More intelligent query plans can be more effective than optimizing execution itself. Avoiding work is better than doing work efficiently.&lt;/p&gt;&lt;head rend="h2"&gt;Bottleneck 3: Increase parallelism #&lt;/head&gt;&lt;p&gt;Fast aggregation is a core promise of any analytical database. From a database perspective, aggregating data in parallel threads is only one part of the equation. It is equally important to merge the local results in parallel.&lt;/p&gt;&lt;p&gt;ClickHouse's aggregation operator has two phases: In the first phase, each thread processes its portion of the data in parallel, creating a local and partial result. In the second phase, all partial results must be merged. If the merge phase is not properly parallelized, it becomes a bottleneck. More threads can actually make this issue worse by creating more partial results to merge.&lt;/p&gt;&lt;p&gt;Solving this issue requires careful algorithm design, smart data structure choices, and a deep understanding how hash tables behave under different load patterns. The goal is to eliminate the serial merge phase and enable linear scaling even for the most complex aggregation queries.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 3.1: Hash Table Conversion (PR #50748) #&lt;/head&gt;&lt;p&gt;ClickBench query Q5 showed a severe performance degradation as the core count increased from 80 to 112 threads. Our pipeline analysis revealed serial processing in the hash table conversion.&lt;/p&gt;&lt;p&gt;Understanding hash tables in ClickHouse&lt;/p&gt;&lt;p&gt;ClickHouse uses two types of hash tables for hash aggregation:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Single-level hash tables: This is a flat hash table that is suitable (= faster) for smaller datasets.&lt;/item&gt;&lt;item&gt;Two-level hash tables: This is a hierarchical hash table with 256 buckets. Two-level hash tables are more amendable to large datasets.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The database chooses the right hash table type based on the size of the processed data: Once a single-level hash table reaches a certain threshold during aggregation, it is automatically converted to a two-level hash table. The code to merge hash tables of different types was serialized.&lt;/p&gt;&lt;p&gt;The serial bottleneck&lt;/p&gt;&lt;p&gt;When merging hash tables from different threads,&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;single-level hash tables were serially merged in a pair-wise manner, e.g. ht1 / ht2 → result, then result / ht3, etc.&lt;/item&gt;&lt;item&gt;two-level hash tables are merged one-by-one as well but the merge is parallelized across buckets.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;In the case of mixed single/two-level hash tables, the single-level hash tables had to be converted to two-level hash tables first (this was a serial process). Once the was done, the resulting two-level hash tables could be merged in parallel.&lt;/p&gt;&lt;p&gt;With Q5, increasing the number of threads from 80 to 112 meant that each thread processes less data. With 80 threads, all hash tables were two-level. With 112 threads, the aggregation ended up with the mixed scenario: some hash tables remained single-level while others became two-level. This caused serialization - all single-level hash tables had to be converted to two-level before parallel merging could take place.&lt;/p&gt;&lt;p&gt;To diagnose the issue, pipeline visualization was a crucial tool. The telltale sign was that the merge phase duration increased with thread count - this is the opposite of what should happen.&lt;/p&gt;&lt;p&gt;Performance degradation with increased core count&lt;/p&gt;Pipeline visualization (max_threads=80) - the merge phase is reasonable Pipeline visualization (max_threads=112) - the merge phase takes 3.2x longer&lt;p&gt;Our optimization parallelizes the conversion phase: instead of converting all single-level hash tables to two-level hash tables one by one (serially), we now convert them in parallel. As each hash table can be converted independently, this eliminates the serial bottleneck.&lt;/p&gt;&lt;code&gt;/// Original code
void mergeHashTable(left_table, right_table)
{
    if (left_table.is_single_level() &amp;amp;&amp;amp; right_table.is_two_level())    
        left_table.convert_to_two_level();  /// Serial conversion blocks threads

    /// Now merge
    merge_sets(left_table, right_table);
}
&lt;/code&gt;&lt;code&gt;/// Optimized code
void mergeHashTableParallel(all_tables)
{
    /// Phase 1: Parallel conversion
    parallel_tasks = [];
    for (const auto &amp;amp; table : all_tables)
    {
        if (table.is_single_level())
        {
            /// Parallel conversion!
            task = create_parallel_task(table.convert_to_two_level());
            parallel_tasks.add(task);
        }
    }

    /// Wait for all conversions to complete
    wait_for_all_tasks(parallel_tasks);

    /// Phase 2: Now all sets are two-level, merge efficiently.
    for (const auto &amp;amp; pair : all_tables)
        merge_sets(pair.left_table, pair.right_table);
}
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;The performance did not improve only for Q5 - the optimization enabled linear scaling for any aggregation-heavy query on ultra-high core count systems.&lt;/p&gt;&lt;p&gt;Performance improvement after parallel conversion - Q5 achieves 264% improvement&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;ClickBench query Q5 improved by a 264% on a 2×112 vCPU system,&lt;/item&gt;&lt;item&gt;24 queries achieved &amp;gt;5% improvement,&lt;/item&gt;&lt;item&gt;the overall geometric mean improved by 7.4%&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The optimization demonstrates that scalability isn't just about making things more parallel - it's about eliminating serial sections that grow with parallelism. Sometimes you need to restructure algorithms on a more deep level, not just add more threads.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 3.2: Single-Level Hash Table Merging (PR #52973) #&lt;/head&gt;&lt;p&gt;We noticed that the performance was also subpar when all hash tables were single-level.&lt;/p&gt;&lt;p&gt;Extending parallel merge to single-level cases&lt;/p&gt;&lt;p&gt;Building on PR #50748, this optimization recognizes that the benefits of parallel merging are not limited to mixed hash tables. Even when all hash tables are single-level, parallel merging can improve performance if the total data size is large enough.&lt;/p&gt;&lt;p&gt;The challenge was to determine when single-level hash tables should be merged in parallel parallel:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;If datasets are too small, parallelization introduces extra overhead.&lt;/item&gt;&lt;item&gt;If datasets are too large, parallelization may not be beneficial enough.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Based on the implementation in PR #52973, the optimization added parallel merges to all single-level cases:&lt;/p&gt;&lt;code&gt;/// Before: Only parallelize mixed-level merges
void parallelizeMergePrepare(hash_tables)
{
    single_level_count = 0;

    for (const auto &amp;amp; hash_table : hash_tables)
        if hash_table.is_single_level():
            single_level_count++;

    /// Only convert if mixed levels (some single, some two-level)
    if single_level_count &amp;gt; 0 and single_level_count &amp;lt; hash_tables.size():
        convert_to_two_level_parallel(hash_tables);
}
&lt;/code&gt;&lt;code&gt;/// Optimized code
void parallelizeMergePrepare(hash_tables):
{
    single_level_count = 0;
    all_single_hash_size = 0;

    for (const auto &amp;amp; hash_table : hash_tables)
        if (hash_table.is_single_level())
            single_level_count++

    /// Calculate total size if all hash tables are single-level
    if (single_level_count == hash_tables.size())
        for (const auto &amp;amp; hash_table : hash_tables)
            all_single_hash_size += hash_table.size();

    /// Convert if mixed levels OR if all single-level with average size &amp;gt; THRESHOLD
    if (single_level_count &amp;gt; 0 and single_level_count &amp;lt; hash_tables.size())
        ||
       (all_single_hash_size / hash_tables.size() &amp;gt; THRESHOLD)
        convert_to_two_level_parallel(hash_tables);
}
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Performance for single-level merge scenarios improved by 235%&lt;/item&gt;&lt;item&gt;The optimal threshold was determined through systematic testing&lt;/item&gt;&lt;item&gt;There were no regressions on small datasets&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Optimization 3.3: Parallel Merge with Key Support (PR #68441) #&lt;/head&gt;&lt;p&gt;GROUP BY operations with large hash tables were merged serially.&lt;/p&gt;&lt;p&gt;Extending parallelization to keyed aggregations&lt;/p&gt;&lt;p&gt;The previous two optimizations (3.1 and 3.2) addressed merges without key - simple hash table operations like &lt;code&gt;COUNT(DISTINCT)&lt;/code&gt;. We applied the same optimization to merges with key where hash tables contain both keys and aggregated values that must be combined, e.g. general &lt;code&gt;GROUP BY&lt;/code&gt; semantics.&lt;/p&gt;&lt;p&gt;Performance Impact:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;ClickBench query Q8 improved by 10.3%, Q9 by 7.6%&lt;/item&gt;&lt;item&gt;There were no regressions in other queries&lt;/item&gt;&lt;item&gt;CPU utilization during the merge phase improved&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Parallel merging can be extended to complex aggregation scenarios with careful attention to cancellation and error handling.&lt;/p&gt;&lt;head rend="h2"&gt;Bottleneck 4: Algorithm optimization #&lt;/head&gt;&lt;p&gt;Harnessing the full potential of SIMD instructions is notoriously difficult. Compilers are conservative about vectorization, and database workloads often have complex control flows that inhibit auto-vectorization.&lt;/p&gt;&lt;p&gt;Effective usage of SIMD instructions in databases requires thinking beyond traditional vectorization. Besides processing N data items simultaneously instead of one, one can also utilize parallel SIMD comparisons for smart pruning strategies which lead to less work done overall. This idea is particularly powerful for string operations. These are at the same time frequently used in practice and computationally expensive.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 4.1: Two-character SIMD string search (PR #46289) #&lt;/head&gt;&lt;p&gt;String search (e.g. plain substring search or LIKE pattern search) is a bottleneck in a lot of queries, for example in ClickBench query Q20.&lt;/p&gt;&lt;p&gt;Understanding string search in analytical queries&lt;/p&gt;&lt;p&gt;Clickbench query 20 evaluates a LIKE pattern on millions of URLs, making fast string search crucial.&lt;/p&gt;&lt;code&gt;SELECT COUNT(*) FROM hits WHERE URL LIKE '%google%'
&lt;/code&gt;&lt;p&gt;Reducing false positives with two-character filtering&lt;/p&gt;&lt;p&gt;PR #46289 is based on the insight that SIMD instructions can be used in a smart way beyond brute-force parallelization. The original code already leveraged SIMD instructions but it only considered the search pattern’s first character, leading to expensive false positives. We rewrite the code to check the second character as well. This improved selectivity dramatically while adding only a negligible amount of new SIMD operations.&lt;/p&gt;&lt;code&gt;/// Original code
class StringSearcher
{
    first_needle_character = needle[0];
    first_needle_character_vec = broadcast_to_simd_vector(first_needle_character);

    void search()
    {
        for (position in haystack; step by 16 bytes)
        {
            haystack_chunk = load_16_bytes(haystack + position);
            first_matches = simd_compare_equal(haystack_chunk, first_needle_character_vec);
            match_mask = extract_match_positions(first_matches);

            for (const auto &amp;amp; match : match_mask)
                /// High false positive rate - many expensive verifications
                if (full_string_match(haystack + match_pos, needle))
                    return match_pos;
        }
    }
}
&lt;/code&gt;&lt;code&gt;// Optimized code
class StringSearcher
{
    first_needle_character = needle[0];
    second_needle_character = needle[1];  /// Second character
    first_needle_character_vec = broadcast_to_simd_vector(first_needle_character);
    second_needle_character_vec = broadcast_to_simd_vector(second_needle_character);

    void search()
    {
        for (position : haystack, step by 16 bytes)
        {
            haystack_chunk1 = load_16_bytes(haystack + position);
            haystack_chunk2 = load_16_bytes(haystack + position + 1);

            /// Compare both characters simultaneously
            first_matches = simd_compare_equal(haystack_chunk1, first_needle_character_vec);
            second_matches = simd_compare_equal(haystack_chunk2, second_needle_character_vec);
            combined_matches = simd_and(first_matches, second_matches);

            match_mask = extract_match_positions(combined_matches);

            for (const auto &amp;amp; match : match_mask)
                // Dramatically fewer false positives - fewer expensive verifications
                if full_string_match(haystack + match_pos, needle):
                    return match_pos;
        }
    }
}
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;Two-character SIMD filtering improved performance significantly:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;ClickBench query Q20 sped up by 35%&lt;/item&gt;&lt;item&gt;Other queries which perform substring matching saw an overall improvement of ~10%&lt;/item&gt;&lt;item&gt;The geometric mean of all queries improved by 4.1%&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The performance improvements are a result of fewer false positives, better cache locality and more efficient branch prediction.&lt;/p&gt;&lt;p&gt;Two-character SIMD filtering demonstrates that effective SIMD optimization isn't just about processing more data per instruction - it's about using SIMD's parallel comparison capabilities to improve the algorithmic efficiency. The two-character approach shows how a small number of additional SIMD operations can in some cases yield massive performance gains.&lt;/p&gt;&lt;head rend="h2"&gt;Bottleneck 5: False Sharing #&lt;/head&gt;&lt;p&gt;False sharing occurs when multiple threads access variables in the same cache. The CPU's cache coherence protocol works at cache line granularity, meaning that any cache line modifications - including modifications of two different variables - are treated as conflicts which require expensive synchronization between cores. On a 2 x 240 vCPUs system, false sharing can turn simple counter increments into system-wide performance disasters.&lt;/p&gt;&lt;p&gt;Eliminating false sharing requires how CPU cache coherence is implemented at the hardware level. It's not enough to optimize algorithms - to avoid false sharing, one must also optimize the memory layout to make sure that frequently-accessed data structures don't accidentally interfere with each other through cache line conflicts. This involves for example a strategic data layout and use of alignment and padding.&lt;/p&gt;&lt;head rend="h3"&gt;Optimization 5.1: Profile Event Counter Alignment (PR #82697) #&lt;/head&gt;&lt;p&gt;ClickBench query Q3 showed 36.6% of CPU cycles spent in &lt;code&gt;ProfileEvents::increment&lt;/code&gt; on a 2×240 vCPU system. Performance profiling revealed a severe cache line contention.&lt;/p&gt;&lt;p&gt;ProfileEvents counters at scale&lt;/p&gt;&lt;p&gt;Profile event counters refer to ClickHouse's internal eventing system - profile events track all internal operations, from detailed query execution steps to memory allocations. In a typical analytical query, these counters are incremented millions of times across all threads. The original implementation organized multiple counters in the same memory region without considering cache line boundaries.&lt;/p&gt;&lt;p&gt;This creates three challenges:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;Cache line physics: Modern Intel processors use 64-byte cache lines. When any byte in a cache line is modified, the entire line must be invalidated in the other cores' caches.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;False sharing amplification: With 240 threads, each counter update triggers a cache line invalidation across potentially dozens of cores. What should be independent operations become serialized through the cache coherence protocol.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Exponential degradation: As the number of cores increases, the probability of a simultaneous access to the same cache line grows exponentially, compounding the impact of cache misses.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Using perf, I discovered that &lt;code&gt;ProfileEvents::increment&lt;/code&gt; was generating massive cache coherence traffic. The smoking gun was the cache line utilization report that showed eight different counters packed into a single cache line. We also added new capabilities to Linux’s perf c2c tool and worked with the community to help developers more easily identify false sharing issues like this.&lt;/p&gt;&lt;p&gt;Proper cache line alignment ensures that each counter gets its own 64-byte cache line. This transforms false sharing (bad) into true sharing (manageable). When a thread updates its counter, now only a single cache line wil be affected.&lt;/p&gt;&lt;p&gt;Based on our implementation in PR #82697, the fix improved the cache line alignment for the profile event counters:&lt;/p&gt;&lt;code&gt;// Before: Counters packed without alignment
struct ProfileEvents:
    atomic_value counters[NUM_EVENTS]  // Multiple counters per cache line
    // 8 counters sharing single 64-byte cache lines

// After: Cache line aligned counters  
struct ProfileEvents:
    struct alignas(64) AlignedCounter:
        atomic_value value
        // Padding automatically added to reach 64 bytes
    
    AlignedCounter counters[NUM_EVENTS]  // Each counter gets own cache line
    // Now each counter has exclusive cache line ownership
&lt;/code&gt;&lt;p&gt;Performance impact&lt;/p&gt;&lt;p&gt;This optimization pattern applies to any frequently updated shared and compact data structure. The lesson is that the memory layout becomes critical at scale - what works fine on eight cores can be excruciatingly slow on 240 cores.&lt;/p&gt;After optimization: ProfileEvents::increment drops to 8.5% (from 36.6%)&lt;p&gt;As a result of our optimization, ClickBench query Q3 saw a 27.4% improvement on ultra-high core count systems. The performance gain increases with the number of cores because the cache coherence overhead grows super-linearly. This optimization therefore doesn't merely fix a bottleneck - it changes the scalability curve.&lt;/p&gt;ClickBench Q3: 27.4% improvement, with larger gains on higher core count systems&lt;head rend="h2"&gt;Building a foundation that scales #&lt;/head&gt;&lt;p&gt;In this post I covered optimizations for five performance bottlenecks:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Lock contention - The coordination overhead grows exponentially with core count.&lt;/item&gt;&lt;item&gt;Memory optimization - The memory bandwidth per core decreases as the core count increases.&lt;/item&gt;&lt;item&gt;Increased parallelism - Serial phases become the dominant bottleneck.&lt;/item&gt;&lt;item&gt;SIMD optimization - Smarter algorithms like two-character filtering beyond brute-force vectorization can improve performance significantly.&lt;/item&gt;&lt;item&gt;False sharing - False sharing is caused by the granularity of cache line size.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The bottlenecks and optimizations presented here are not just about ClickHouse - they represent a fundamental shift in how we must approach database optimization in the ultra-high core count era. As processors continue to evolve toward higher core counts, these techniques will become essential for any system that needs to scale.&lt;/p&gt;&lt;p&gt;Our optimizations enable ClickHouse to achieve close-to-linear scalability as the core count increases. This enables ClickHouse to thrive as an analytics database in a future world where Intel and other hardware manufacturers push the core count into the thousands.&lt;/p&gt;&lt;head rend="h2"&gt;References and Resources #&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Source Code: All optimizations available in ClickHouse main branch&lt;/item&gt;&lt;item&gt;Slide Deck: 2025 Shanghai Meetup Presentation&lt;/item&gt;&lt;item&gt;Pull Requests: Individual PRs linked throughout this post with detailed performance analysis&lt;/item&gt;&lt;item&gt;Intel Intrinsics Guide: Intel® Intrinsics Guide&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Acknowledgments #&lt;/head&gt;&lt;p&gt;Special thanks to the ClickHouse community for rigorous code review and performance validation. These optimizations represent collaborative effort between Intel and ClickHouse teams to unlock the full potential of modern ultra-high core count processors.&lt;/p&gt;&lt;p&gt;For questions about implementation details or performance reproduction, please refer to the individual PR discussions linked throughout this post.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45279792</guid><pubDate>Wed, 17 Sep 2025 18:46:03 +0000</pubDate></item><item><title>A postmortem of three recent issues</title><link>https://www.anthropic.com/engineering/a-postmortem-of-three-recent-issues</link><description>&lt;doc fingerprint="8d3083968c38d9d6"&gt;
  &lt;main&gt;
    &lt;p&gt;Between August and early September, three infrastructure bugs intermittently degraded Claude's response quality. We've now resolved these issues and want to explain what happened.&lt;/p&gt;
    &lt;p&gt;In early August, a number of users began reporting degraded responses from Claude. These initial reports were difficult to distinguish from normal variation in user feedback. By late August, the increasing frequency and persistence of these reports prompted us to open an investigation that led us to uncover three separate infrastructure bugs.&lt;/p&gt;
    &lt;p&gt;To state it plainly: We never reduce model quality due to demand, time of day, or server load. The problems our users reported were due to infrastructure bugs alone.&lt;/p&gt;
    &lt;p&gt;We recognize users expect consistent quality from Claude, and we maintain an extremely high bar for ensuring infrastructure changes don't affect model outputs. In these recent incidents, we didn't meet that bar. The following postmortem explains what went wrong, why detection and resolution took longer than we would have wanted, and what we're changing to prevent similar future incidents.&lt;/p&gt;
    &lt;p&gt;We don't typically share this level of technical detail about our infrastructure, but the scope and complexity of these issues justified a more comprehensive explanation.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we serve Claude at scale&lt;/head&gt;
    &lt;p&gt;We serve Claude to millions of users via our first-party API, Amazon Bedrock, and Google Cloud's Vertex AI. We deploy Claude across multiple hardware platforms, namely AWS Trainium, NVIDIA GPUs, and Google TPUs. This approach provides the capacity and geographic distribution necessary to serve users worldwide.&lt;/p&gt;
    &lt;p&gt;Each hardware platform has different characteristics and requires specific optimizations. Despite these variations, we have strict equivalence standards for model implementations. Our aim is that users should get the same quality responses regardless of which platform serves their request. This complexity means that any infrastructure change requires careful validation across all platforms and configurations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timeline of events&lt;/head&gt;
    &lt;p&gt;The overlapping nature of these bugs made diagnosis particularly challenging. The first bug was introduced on August 5, affecting approximately 0.8% of requests made to Sonnet 4. Two more bugs arose from deployments on August 25 and 26.&lt;/p&gt;
    &lt;p&gt;Although initial impacts were limited, a load balancing change on August 29 started to increase affected traffic. This caused many more users to experience issues while others continued to see normal performance, creating confusing and contradictory reports.&lt;/p&gt;
    &lt;head rend="h2"&gt;Three overlapping issues&lt;/head&gt;
    &lt;p&gt;Below we describe the three bugs that caused the degradation, when they occurred, and how we resolved them:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Context window routing error&lt;/head&gt;
    &lt;p&gt;On August 5, some Sonnet 4 requests were misrouted to servers configured for the upcoming 1M token context window. This bug initially affected 0.8% of requests. On August 29, a routine load balancing change unintentionally increased the number of short-context requests routed to the 1M context servers. At the worst impacted hour on August 31, 16% of Sonnet 4 requests were affected.&lt;/p&gt;
    &lt;p&gt;Approximately 30% of Claude Code users who made requests during this period had at least one message routed to the wrong server type, resulting in degraded responses. On Amazon Bedrock, misrouted traffic peaked at 0.18% of all Sonnet 4 requests from August 12. Incorrect routing affected less than 0.0004% of requests on Google Cloud's Vertex AI between August 27 and September 16.&lt;/p&gt;
    &lt;p&gt;However, some users were affected more severely, as our routing is "sticky". This meant that once a request was served by the incorrect server, subsequent follow-ups were likely to be served by the same incorrect server.&lt;/p&gt;
    &lt;p&gt;Resolution: We fixed the routing logic to ensure short- and long-context requests were directed to the correct server pools. We deployed the fix on September 4. A rollout to our first-party platforms and Google Cloud’s Vertex was completed by September 16. The fix is in the process of being rolled out on Bedrock.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Output corruption&lt;/head&gt;
    &lt;p&gt;On August 25, we deployed a misconfiguration to the Claude API TPU servers that caused an error during token generation. An issue caused by a runtime performance optimization occasionally assigned a high probability to tokens that should rarely be produced given the context, for example producing Thai or Chinese characters in response to English prompts, or producing obvious syntax errors in code. A small subset of users that asked a question in English might have seen "สวัสดี" in the middle of the response, for example.&lt;/p&gt;
    &lt;p&gt;This corruption affected requests made to Opus 4.1 and Opus 4 on August 25-28, and requests to Sonnet 4 August 25–September 2. Third-party platforms were not affected by this issue.&lt;/p&gt;
    &lt;p&gt;Resolution: We identified the issue and rolled back the change on September 2. We've added detection tests for unexpected character outputs to our deployment process.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Approximate top-k XLA:TPU miscompilation&lt;/head&gt;
    &lt;p&gt;On August 25, we deployed code to improve how Claude selects tokens during text generation. This change inadvertently triggered a latent bug in the XLA:TPU[1] compiler, which has been confirmed to affect requests to Claude Haiku 3.5.&lt;/p&gt;
    &lt;p&gt;We also believe this could have impacted a subset of Sonnet 4 and Opus 3 on the Claude API. Third-party platforms were not affected by this issue.&lt;/p&gt;
    &lt;p&gt;Resolution: We first observed the bug affecting Haiku 3.5 and rolled it back on September 4. We later noticed user reports of problems with Opus 3 that were compatible with this bug, and rolled it back on September 12. After extensive investigation we were unable to reproduce this bug on Sonnet 4 but decided to also roll it back out of an abundance of caution.&lt;/p&gt;
    &lt;p&gt;Simultaneously, we have (a) been working with the XLA:TPU team on a fix for the compiler bug and (b) rolled out a fix to use exact top-k with enhanced precision. For details, see the deep dive below.&lt;/p&gt;
    &lt;head rend="h2"&gt;A closer look at the XLA compiler bug&lt;/head&gt;
    &lt;p&gt;To illustrate the complexity of these issues, here's how the XLA compiler bug manifested and why it proved particularly challenging to diagnose.&lt;/p&gt;
    &lt;p&gt;When Claude generates text, it calculates probabilities for each possible next word, then randomly chooses a sample from this probability distribution. We use "top-p sampling" to avoid nonsensical outputs—only considering words whose cumulative probability reaches a threshold (typically 0.99 or 0.999). On TPUs, our models run across multiple chips, with probability calculations happening in different locations. To sort these probabilities, we need to coordinate data between chips, which is complex.[2]&lt;/p&gt;
    &lt;p&gt;In December 2024, we discovered our TPU implementation would occasionally drop the most probable token when temperature was zero. We deployed a workaround to fix this case.&lt;/p&gt;
    &lt;p&gt;The root cause involved mixed precision arithmetic. Our models compute next-token probabilities in bf16 (16-bit floating point). However, the vector processor is fp32-native, so the TPU compiler (XLA) can optimize runtime by converting some operations to fp32 (32-bit). This optimization pass is guarded by the &lt;code&gt;xla_allow_excess_precision&lt;/code&gt; flag which defaults to true.&lt;/p&gt;
    &lt;p&gt;This caused a mismatch: operations that should have agreed on the highest probability token were running at different precision levels. The precision mismatch meant they didn't agree on which token had the highest probability. This caused the highest probability token to sometimes disappear from consideration entirely.&lt;/p&gt;
    &lt;p&gt;On August 26, we deployed a rewrite of our sampling code to fix the precision issues and improve how we handled probabilities at the limit that reach the top-p threshold. But in fixing these problems, we exposed a trickier one.&lt;/p&gt;
    &lt;p&gt;Our fix removed the December workaround because we believed we'd solved the root cause. This led to a deeper bug in the approximate top-k operation—a performance optimization that quickly finds the highest probability tokens.[3] This approximation sometimes returned completely wrong results, but only for certain batch sizes and model configurations. The December workaround had been inadvertently masking this problem.&lt;/p&gt;
    &lt;p&gt;The bug's behavior was frustratingly inconsistent. It changed depending on unrelated factors such as what operations ran before or after it, and whether debugging tools were enabled. The same prompt might work perfectly on one request and fail on the next.&lt;/p&gt;
    &lt;p&gt;While investigating, we also discovered that the exact top-k operation no longer had the prohibitive performance penalty it once did. We switched from approximate to exact top-k and standardized some additional operations on fp32 precision.[4] Model quality is non-negotiable, so we accepted the minor efficiency impact.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why detection was difficult&lt;/head&gt;
    &lt;p&gt;Our validation process ordinarily relies on benchmarks alongside safety evaluations and performance metrics. Engineering teams perform spot checks and deploy to small "canary" groups first.&lt;/p&gt;
    &lt;p&gt;These issues exposed critical gaps that we should have identified earlier. The evaluations we ran simply didn't capture the degradation users were reporting, in part because Claude often recovers well from isolated mistakes. Our own privacy practices also created challenges in investigating reports. Our internal privacy and security controls limit how and when engineers can access user interactions with Claude, in particular when those interactions are not reported to us as feedback. This protects user privacy but prevents engineers from examining the problematic interactions needed to identify or reproduce bugs.&lt;/p&gt;
    &lt;p&gt;Each bug produced different symptoms on different platforms at different rates. This created a confusing mix of reports that didn't point to any single cause. It looked like random, inconsistent degradation.&lt;/p&gt;
    &lt;p&gt;More fundamentally, we relied too heavily on noisy evaluations. Although we were aware of an increase in reports online, we lacked a clear way to connect these to each of our recent changes. When negative reports spiked on August 29, we didn't immediately make the connection to an otherwise standard load balancing change.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we're changing&lt;/head&gt;
    &lt;p&gt;As we continue to improve our infrastructure, we're also improving the way we evaluate and prevent bugs like those discussed above across all platforms where we serve Claude. Here's what we're changing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More sensitive evaluations: To help discover the root cause of any given issue, we’ve developed evaluations that can more reliably differentiate between working and broken implementations. We’ll keep improving these evaluations to keep a closer eye on model quality.&lt;/item&gt;
      &lt;item&gt;Quality evaluations in more places: Although we run regular evaluations on our systems, we will run them continuously on true production systems to catch issues such as the context window load balancing error.&lt;/item&gt;
      &lt;item&gt;Faster debugging tooling: We'll develop infrastructure and tooling to better debug community-sourced feedback without sacrificing user privacy. Additionally, some bespoke tools developed here will be used to reduce the remediation time in future similar incidents, if those should occur.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Evals and monitoring are important. But these incidents have shown that we also need continuous signal from users when responses from Claude aren't up to the usual standard. Reports of specific changes observed, examples of unexpected behavior encountered, and patterns across different use cases all helped us isolate the issues.&lt;/p&gt;
    &lt;p&gt;It remains particularly helpful for users to continue to send us their feedback directly. You can use the &lt;code&gt;/bug&lt;/code&gt; command in Claude Code or you can use the "thumbs down" button in the Claude apps to do so. Developers and researchers often create new and interesting ways to evaluate model quality that complement our internal testing. If you'd like to share yours, reach out to feedback@anthropic.com.&lt;/p&gt;
    &lt;p&gt;We remain grateful to our community for these contributions.&lt;/p&gt;
    &lt;p&gt;[1] XLA:TPU is the optimizing compiler that translates XLA High Level Optimizing language—often written using JAX—to TPU machine instructions.&lt;/p&gt;
    &lt;p&gt;[2] Our models are too large for single chips and are partitioned across tens of chips or more, making our sorting operation a distributed sort. TPUs (just like GPUs and Trainium) also have different performance characteristics than CPUs, requiring different implementation techniques using vectorized operations instead of serial algorithms.&lt;/p&gt;
    &lt;p&gt;[3] We had been using this approximate operation because it yielded substantial performance improvements. The approximation works by accepting potential inaccuracies in the lowest probability tokens, which shouldn't affect quality—except when the bug caused it to drop the highest probability token instead.&lt;/p&gt;
    &lt;p&gt;[4] Note that the now-correct top-k implementation may result in slight differences in the inclusion of tokens near the top-p threshold, and in rare cases users may benefit from re-tuning their choice of top-p.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45281139</guid><pubDate>Wed, 17 Sep 2025 20:41:07 +0000</pubDate></item><item><title>One Token to rule them all – Obtaining Global Admin in every Entra ID tenant</title><link>https://dirkjanm.io/obtaining-global-admin-in-every-entra-id-tenant-with-actor-tokens/</link><description>&lt;doc fingerprint="3d63029d0fa3c0af"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;One Token to rule them all - obtaining Global Admin in every Entra ID tenant via Actor tokens&lt;/head&gt;
    &lt;p&gt;While preparing for my Black Hat and DEF CON talks in July of this year, I found the most impactful Entra ID vulnerability that I will probably ever find. This vulnerability could have allowed me to compromise every Entra ID tenant in the world (except probably those in national cloud deployments1). If you are an Entra ID admin reading this, yes that means complete access to your tenant. The vulnerability consisted of two components: undocumented impersonation tokens, called “Actor tokens”, that Microsoft uses in their backend for service-to-service (S2S) communication. Additionally, there was a critical flaw in the (legacy) Azure AD Graph API that failed to properly validate the originating tenant, allowing these tokens to be used for cross-tenant access.&lt;/p&gt;
    &lt;p&gt;Effectively this means that with a token I requested in my lab tenant I could authenticate as any user, including Global Admins, in any other tenant. Because of the nature of these Actor tokens, they are not subject to security policies like Conditional Access, which means there was no setting that could have mitigated this for specific hardened tenants. Since the Azure AD Graph API is an older API for managing the core Azure AD / Entra ID service, access to this API could have been used to make any modification in the tenant that Global Admins can do, including taking over or creating new identities and granting them any permission in the tenant. With these compromised identities the access could also be extended to Microsoft 365 and Azure.&lt;/p&gt;
    &lt;p&gt;I reported this vulnerability the same day to the Microsoft Security Response Center (MSRC). Microsoft fixed this vulnerability on their side within days of the report being submitted and has rolled out further mitigations that block applications from requesting these Actor tokens for the Azure AD Graph API. Microsoft also issued CVE-2025-55241 for this vulnerability.&lt;/p&gt;
    &lt;head rend="h1"&gt;Impact&lt;/head&gt;
    &lt;p&gt;These tokens allowed full access to the Azure AD Graph API in any tenant. Requesting Actor tokens does not generate logs. Even if it did they would be generated in my tenant instead of in the victim tenant, which means there is no record of the existence of these tokens.&lt;/p&gt;
    &lt;p&gt;Furthermore, the Azure AD Graph API does not have API level logging. Its successor, the Microsoft Graph, does have this logging, but for the Azure AD Graph this telemetry source is still in a very limited preview and I’m not aware of any tenant that currently has this available. Since there is no API level logging, it means the following Entra ID data could be accessed without any traces:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;User information including all their personal details stored in Entra ID.&lt;/item&gt;
      &lt;item&gt;Group and role information.&lt;/item&gt;
      &lt;item&gt;Tenant settings and (Conditional Access) policies.&lt;/item&gt;
      &lt;item&gt;Applications, Service Principals, and any application permission assignment.&lt;/item&gt;
      &lt;item&gt;Device information and BitLocker keys synced to Entra ID.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This information could be accessed by impersonating a regular user in the victim tenant. If you want to know the full impact, my tool roadrecon uses the same API, if you run it then everything you find in the GUI of the tool could have been accessed and modified by an attacker abusing this flaw.&lt;/p&gt;
    &lt;p&gt;If a Global Admin was impersonated, it would also be possible to modify any of the above objects and settings. This would result in full tenant compromise with access to any service that uses Entra ID for authentication, such as SharePoint Online and Exchange Online. It would also provide full access to any resource hosted in Azure, since these resources are controlled from the tenant level and Global Admins can grant themselves rights on Azure subscriptions. Modifying objects in the tenant does (usually) result in audit logs being generated. That means that while theoretically all data in Microsoft 365 could have been compromised, doing anything other than reading the directory information would leave audit logs that could alert defenders, though without knowledge of the specific artifacts that modifications with these Actor tokens generate, it would appear as if a legitimate Global Admin performed the actions.&lt;/p&gt;
    &lt;p&gt;Based on Microsoft’s internal telemetry, they did not detect any abuse of this vulnerability. If you want to search for possible abuse artifacts in your own environment, a KQL detection is included at the end of this post.&lt;/p&gt;
    &lt;head rend="h1"&gt;Technical details&lt;/head&gt;
    &lt;head rend="h2"&gt;Actor tokens&lt;/head&gt;
    &lt;p&gt;Actor tokens are tokens that are issued by the “Access Control Service”. I don’t know the exact origins of this service, but it appears to be a legacy service that is used for authentication with SharePoint applications and also seems to be used by Microsoft internally. I came across this service while investigating hybrid Exchange setups. These hybrid setups used to provision a certificate credential on the Exchange Online Service Principal (SP) in the tenant, with which it can perform authentication. These hybrid attacks were the topic of some talks I did this summer, the slides are on the talks page. In this case the hybrid part is not relevant, as in my lab I could also have added a credential on the Exchange Online SP without the complete hybrid setup. Exchange is not the only app which can do this, but since I found this in Exchange we will keep talking about these tokens in the context of Exchange.&lt;/p&gt;
    &lt;p&gt;Exchange will request Actor tokens when it wants to communicate with other services on behalf of a user. The Actor token allows it to “act” as another user in the tenant when talking to Exchange Online, SharePoint and as it turns out the Azure AD Graph. The Actor token (a JSON Web Token / JWT) looks as follows when decoded:&lt;/p&gt;
    &lt;code&gt;{
    "alg": "RS256",
    "kid": "_jNwjeSnvTTK8XEdr5QUPkBRLLo",
    "typ": "JWT",
    "x5t": "_jNwjeSnvTTK8XEdr5QUPkBRLLo"
}
{
    "aud": "00000002-0000-0000-c000-000000000000/graph.windows.net@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "exp": 1752593816,
    "iat": 1752507116,
    "identityprovider": "00000001-0000-0000-c000-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "iss": "00000001-0000-0000-c000-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "nameid": "00000002-0000-0ff1-ce00-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "nbf": 1752507116,
    "oid": "a761cbb2-fbb6-4c80-aa50-504962316eb2",
    "rh": "1.AXQAj_KHYn9PIkOWUahpfY_hvAIAAAAAAAAAwAAAAAAAAACtAQB0AA.",
    "sub": "a761cbb2-fbb6-4c80-aa50-504962316eb2",
    "trustedfordelegation": "true",
    "xms_spcu": "true"
}.[signature from Entra ID]
&lt;/code&gt;
    &lt;p&gt;There are a few fields here that differ from regular Entra ID access tokens:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The &lt;code&gt;aud&lt;/code&gt;field contains the GUID of the Azure AD Graph API, as well as the URL&lt;code&gt;graph.windows.net&lt;/code&gt;and the tenant it was issued to&lt;code&gt;6287f28f-4f7f-4322-9651-a8697d8fe1bc&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The expiry is exactly 24 hours after the token was issued.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;iss&lt;/code&gt;contains the GUID of the Entra ID token service itself, called “Azure ESTS Service”, and again the tenant GUID where it was issued.&lt;/item&gt;
      &lt;item&gt;The token contains the claim &lt;code&gt;trustedfordelegation&lt;/code&gt;, which is&lt;code&gt;True&lt;/code&gt;in this case, meaning we can use this token to impersonate other identities. Many Microsoft apps could request such tokens. Non-Microsoft apps requesting an Actor token would receive a token with this field set to&lt;code&gt;False&lt;/code&gt;instead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When using this Actor token, Exchange would embed this in an unsigned JWT that is then sent to the resource provider, in this case the Azure AD graph. In the rest of the blog I call these impersonation tokens since they are used to impersonate users.&lt;/p&gt;
    &lt;code&gt;{
    "alg": "none",
    "typ": "JWT"
}
{
    "actortoken": "eyJ0eXAiOiJKV1Qi&amp;lt;snip&amp;gt;TxeLkNB8v2rWWMLGpaAaFJlhA",
    "aud": "00000002-0000-0000-c000-000000000000/graph.windows.net@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "exp": 1756926566,
    "iat": 1756926266,
    "iss": "00000002-0000-0ff1-ce00-000000000000@6287f28f-4f7f-4322-9651-a8697d8fe1bc",
    "nameid": "10032001E2CBE43B",
    "nbf": 1756926266,
    "nii": "urn:federation:MicrosoftOnline",
    "sip": "doesnt@matter.com",
    "smtp": "doesnt@matter.com",
    "upn": "doesnt@matter.com"
}.[no signature]
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;sip&lt;/code&gt;, &lt;code&gt;smtp&lt;/code&gt;, &lt;code&gt;upn&lt;/code&gt; fields are used when accessing resources in Exchange online or SharePoint, but are ignored when talking to the Azure AD Graph, which only cares about the &lt;code&gt;nameid&lt;/code&gt;. This &lt;code&gt;nameid&lt;/code&gt; originates from an attribute of the user that is called the &lt;code&gt;netId&lt;/code&gt; on the Azure AD Graph. You will also see it reflected in tokens issued to users, in the &lt;code&gt;puid&lt;/code&gt; claim, which stands for Passport UID. I believe these identifiers are an artifact from the original codebase which Microsoft used for its Microsoft Accounts (consumer accounts or MSA). They are still used in Entra ID, for example to map guest users to the original identity in their home tenant.&lt;/p&gt;
    &lt;p&gt;As I mentioned before, these impersonation tokens are not signed. That means that once Exchange has an Actor token, it can use the one Actor token to impersonate anyone against the target service it was requested for, for 24 hours. In my personal opinion, this whole Actor token design is something that never should have existed. It lacks almost every security control that you would want:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;There are no logs when Actor tokens are issued.&lt;/item&gt;
      &lt;item&gt;Since these services can craft the unsigned impersonation tokens without talking to Entra ID, there are also no logs when they are created or used.&lt;/item&gt;
      &lt;item&gt;They cannot be revoked within their 24 hours validity.&lt;/item&gt;
      &lt;item&gt;They completely bypass any restrictions configured in Conditional Access.&lt;/item&gt;
      &lt;item&gt;We have to rely on logging from the resource provider to even know these tokens were used in the tenant.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Microsoft uses these tokens to talk to other services in their backend, something that Microsoft calls service-to-service (S2S) communication. If one of these tokens leaks, it can be used to access all the data in an entire tenant without any useful telemetry or mitigation. In July of this year, Microsoft did publish a blog about removing these insecure legacy practices from their environment, but they do not provide any transparency about how many services still use these tokens.&lt;/p&gt;
    &lt;head rend="h2"&gt;The fatal flaw leading to cross-tenant compromise&lt;/head&gt;
    &lt;p&gt;As I was refining my slide deck and polished up my proof-of-concept code for requesting and generating these tokens, I tested more variants of using these tokens, changing various fields to see if the tokens still worked with the modified information. As one of the tests I changed the tenant ID of the impersonation token to a different tenant in which none of my test accounts existed. The Actor tokens tenant ID was my &lt;code&gt;iminyour.cloud&lt;/code&gt; tenant, with tenant ID &lt;code&gt;6287f28f-4f7f-4322-9651-a8697d8fe1bc&lt;/code&gt; and the unsigned JWT generated had the tenant ID &lt;code&gt;b9fb93c1-c0c8-4580-99f3-d1b540cada32&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;I sent this token to &lt;code&gt;graph.windows.net&lt;/code&gt; using my CLI tool &lt;code&gt;roadtx&lt;/code&gt;, expecting a generic access denied since I had a tenant ID mismatch. However, I was instead greeted by a curious error message:&lt;/p&gt;
    &lt;p&gt;Note that these are the actual screenshots I made during my research, which is why the formatting may not work as well in this blog&lt;/p&gt;
    &lt;p&gt;The error message suggested that while my token was valid, the identity could not be found in the tenant. Somehow the API seemed to accept my token even with the mismatching tenant. I quickly looked up the &lt;code&gt;netId&lt;/code&gt; of a user that did exist in the target tenant, crafted a token and the Azure AD Graph happily returned the data I requested. I tested this in a few more test tenants I had access to, to make sure I was not crazy, but I could indeed access data in other tenants, as long as I knew their tenant ID (which is public information) and the &lt;code&gt;netId&lt;/code&gt; of a user in that tenant.&lt;/p&gt;
    &lt;p&gt;To demonstrate the vulnerability, here I am using a Guest user in the target tenant to query the &lt;code&gt;netId&lt;/code&gt; of a Global Admin. Then I impersonate the Global Admin using the same Actor token, and can perform any action in the tenant as that Global Admin over the Azure AD Graph.&lt;/p&gt;
    &lt;p&gt;First I craft an impersonation token for a Guest user in my victim tenant:&lt;/p&gt;
    &lt;p&gt;I use this token to query the &lt;code&gt;netId&lt;/code&gt; of a Global Admin:&lt;/p&gt;
    &lt;p&gt;Then I create an impersonation token for this Global Admin (the UPN is kept the same since it is not validated by the API):&lt;/p&gt;
    &lt;p&gt;And finally this token is used to access the tenant as the Global Admin, listing the users, something the guest user was not able to do:&lt;/p&gt;
    &lt;p&gt;I can even run roadrecon with this impersonation token, which queries all Azure AD Graph API endpoints to enumerate the available information in the tenant.&lt;/p&gt;
    &lt;p&gt;None of these actions would generate any logs in the victim tenant.&lt;/p&gt;
    &lt;head rend="h1"&gt;Practical abuse&lt;/head&gt;
    &lt;p&gt;With this vulnerability it would be possible to compromise any Entra ID tenant. Starting with an Actor token from an attacker controlled tenant, the following steps would lead to full control over the victim tenant:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Find the tenant ID for the victim tenant, this can be done using public APIs based on the domain name.&lt;/item&gt;
      &lt;item&gt;Find a valid &lt;code&gt;netId&lt;/code&gt;of a regular user in the tenant. Methods for this will be discussed below.&lt;/item&gt;
      &lt;item&gt;Craft an impersonation token with the Actor token from the attacker tenant, using the tenant ID and &lt;code&gt;netId&lt;/code&gt;of the user in the victim tenant.&lt;/item&gt;
      &lt;item&gt;List all Global Admins in the tenant and their &lt;code&gt;netId&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Craft an impersonation token for the Global Admin account.&lt;/item&gt;
      &lt;item&gt;Perform any read or write action over the Azure AD Graph API.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If an attacker makes any modifications in the tenant in step 6, that would be the only event in this chain that generates any telemetry in the victim tenant. An attacker could for example create new user accounts, grant these Global Admin privileges and then sign in interactively to any Entra ID, Microsoft 365 or third party application that integrates with the victim tenant. Alternatively they could add credentials on existing applications, grant these apps API permissions and use that to exfiltrate emails or files from Microsoft 365, a technique that is popular among threat actors. An attacker could also add credentials to Microsoft Service Principals in the victim tenant, several of which can request Actor tokens that allow impersonation against SharePoint or Exchange. For my DEF CON and Black Hat talks I made a demo video about using these Actor tokens to obtain Global Admin access. The video uses Actor tokens within a tenant, but the same technique could have been applied to any other tenant by abusing this vulnerability.&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding netIds&lt;/head&gt;
    &lt;p&gt;Since tenant IDs can be resolved when the domain name of a tenant is known, the only identifier that is not immediately available to the attacker is a valid &lt;code&gt;netId&lt;/code&gt; for a user in that specific tenant. As I mentioned above, these IDs are added to Entra ID access tokens as the &lt;code&gt;puid&lt;/code&gt; claim. Any token found online, in screenshots, examples or logs, even those that are long expired or with an obfuscated signature, would provide an attacker with enough information to breach the tenant. Threat actors that still have old tokens for any tenant from previous breaches can immediately access those tenants again as long as the victim account still exists.&lt;/p&gt;
    &lt;p&gt;The above is probably not a very common occurrence. What is a more realistic attack is simply brute-forcing the &lt;code&gt;netId&lt;/code&gt;. Unlike object IDs, which are randomly generated, netIds are actually incremental. Looking at the differences in netIds between my tenant and those of some tenants I analyzed, I found the difference between a newly created user in my tenant and their newest user to be in the range of 100.000 to 100 million. Simply brute forcing the &lt;code&gt;netId&lt;/code&gt; could be accomplished in minutes to hours for any target tenant, and the more user exist in a tenant the easier it is to find a match. Since this does not generate any logs it isn’t a noisy attack either. Because of the possibility to brute force these netIds I would say this vulnerability could have been used to take over any tenant without any prerequisites. There is however a third technique which is even more effective (and more fun from a technical level).&lt;/p&gt;
    &lt;head rend="h2"&gt;Compromising tenants by hopping over B2B trusts&lt;/head&gt;
    &lt;p&gt;I previously mentioned that a users &lt;code&gt;netId&lt;/code&gt; is used to establish links between a user account in multiple tenants. This is something that I researched a few years ago when I gave a talk at Black Hat USA 22 about external identities. The below screenshot is taken from one of my slides, which illustrates this:&lt;/p&gt;
    &lt;p&gt;The way this works is as follows. Suppose we have tenant A and tenant B. A user in tenant B is invited into tenant A. In the new guest account that is created in tenant A, their &lt;code&gt;netId&lt;/code&gt; is stored on the &lt;code&gt;alternativeSecurityIds&lt;/code&gt; attribute. That means that an attacker wanting to abuse this bug can simply read that attribute in tenant A, put it in an impersonation token for tenant B and then impersonate the victim in their home tenant. It should be noted that this works against the direction of invite. Any user in any tenant where you accept an invite will be able to read your &lt;code&gt;netId&lt;/code&gt;, and with this bug could have impersonated you in your home tenant. In your home tenant you have a full user account, which can enumerate other users. This is not a bug or risk with B2B trusts, but is simply an unintended consequence of the B2B design mechanism. A guest account in someone else’s tenant would also be sufficient with the default Entra ID guest settings because the default settings allow users to query the &lt;code&gt;netId&lt;/code&gt; of a user as long as the UPN is known.&lt;/p&gt;
    &lt;p&gt;To abuse this, a threat actor could perform the following steps, given that they have access to at least one tenant with a guest user:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Query the guest users and their &lt;code&gt;alternativeSecurityIds&lt;/code&gt;attribute which gives the&lt;code&gt;netId&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Query the tenant ID of the guest users home tenant based on the domain name in their UPN.&lt;/item&gt;
      &lt;item&gt;Create an impersonation token, impersonating the victim in their home tenant.&lt;/item&gt;
      &lt;item&gt;Optionally list Global Admins and impersonate those to compromise the entire tenant.&lt;/item&gt;
      &lt;item&gt;Repeat step 1 for each tenant that was compromised.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The steps above can be done in 2 API calls per tenant, which do not generate any logs. Most tenants will have guest users from multiple distinct other tenants. This means the number of tenants you compromise with this scales exponentially and the information needed to compromise the majority of all tenants worldwide could have been gathered within minutes using a single Actor token. After at least 1 user is known per victim tenant, the attacker can selectively perform post-compromise actions in these tenants by impersonating Global Admins.&lt;/p&gt;
    &lt;p&gt;Looking at the list of guest users in the tenants of some of my clients, this technique would be extremely powerful. I also observed that one of the first tenants you will likely compromise is Microsoft’s own tenant, since Microsoft consultants often get invited to customer tenants. Many MSPs and Microsoft Partners will have a guest account in the Microsoft tenant, so from the Microsoft tenant a compromise of most major service provider tenants is one step away.&lt;/p&gt;
    &lt;p&gt;Needless to say, as much as I would have liked to test this technique in practice to see how fast this would spread out, I only tested the individual steps in my own tenants and did not access any data I’m not authorized to.&lt;/p&gt;
    &lt;head rend="h1"&gt;Detection&lt;/head&gt;
    &lt;p&gt;While querying data over the Azure AD Graph does not leave any logs, modifying data does (usually) generate audit logs. If modifications are done with Actor tokens, these logs look a bit curious.&lt;/p&gt;
    &lt;p&gt;Since Actor tokens involve both the app and the user being impersonated, it seems Entra ID gets confused about who actually made the change, and it will log the UPN of the impersonated Global Admin, but the display name of Exchange. Luckily for defenders this creates a nice giveaway when Actor tokens are used in the tenant. After some testing and filtering with some fellow researchers that work on the blue side (thanks to Fabian Bader and Olaf Hartong) we came up with the following detection query:&lt;/p&gt;
    &lt;code&gt;AuditLogs
| where not(OperationName has "group")
| where not(OperationName == "Set directory feature on tenant")
| where InitiatedBy has "user"
| where InitiatedBy.user.displayName has_any ( "Office 365 Exchange Online", "Skype for Business Online", "Dataverse", "Office 365 SharePoint Online", "Microsoft Dynamics ERP")
&lt;/code&gt;
    &lt;p&gt;The exclusion for group operations is there because some of these products do actually use Actor tokens to perform operations on your behalf. For example creating specific groups via the Exchange Online PowerShell module will make Exchange use an Actor token on your behalf and create the group in Entra ID.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;This blog discussed a critical token validation failure in the Azure AD Graph API. While the vulnerability itself was a bad oversight in the token handling, the whole concept of Actor tokens is a protocol that was designed to behave with all the properties mentioned in the paragraphs above. If it weren’t for the complete lack of security measures in these tokens, I don’t think such a big impact with such limited telemetry would have been possible.&lt;/p&gt;
    &lt;p&gt;Thanks to the people at MSRC who immediately picked up the vulnerability report, searched for potential variants in other resources, and to the engineers who followed up with fixes for the Azure AD Graph and blocked Actor tokens for the Azure AD Graph API requested with credentials stored on Service Principals, essentially restricting the usage of these Actor tokens to only Microsoft internal services.&lt;/p&gt;
    &lt;head rend="h2"&gt;Disclosure timeline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;July 14, 2025 - reported issue to MSRC.&lt;/item&gt;
      &lt;item&gt;July 14, 2025 - MSRC case opened.&lt;/item&gt;
      &lt;item&gt;July 15, 2025 - reported further details on the impact.&lt;/item&gt;
      &lt;item&gt;July 15, 2025 - MSRC requested to halt further testing of this vulnerability.&lt;/item&gt;
      &lt;item&gt;July 17, 2025 - Microsoft pushed a fix for the issue globally into production.&lt;/item&gt;
      &lt;item&gt;July 23, 2025 - Issue confirmed as resolved by MSRC.&lt;/item&gt;
      &lt;item&gt;August 6, 2025 - Further mitigations pushed out preventing Actor tokens being issued for the Azure AD Graph with SP credentials.&lt;/item&gt;
      &lt;item&gt;September 4, 2025 - CVE-2025-55241 issued.&lt;/item&gt;
      &lt;item&gt;September 17, 2025 - Release of this blogpost.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;I do not have access to any tenants in a national cloud deployment, so I was not able to test whether the vulnerability existed there. Since national cloud deployments use their own token signing keys, it is unlikely that it would have been possible to execute this attack from a tenant in the public cloud to one of these national clouds. I do consider it likely that this attack would have worked across tenants in the same national cloud deployments, but that is speculation. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45282497</guid><pubDate>Wed, 17 Sep 2025 23:03:21 +0000</pubDate></item><item><title>Stepping Down as Libxml2 Maintainer</title><link>https://discourse.gnome.org/t/stepping-down-as-libxml2-maintainer/31398</link><description>&lt;doc fingerprint="1f9f5cdbd006b6ce"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;nwellnhof
(Nick Wellnhofer)
1&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;I’m stepping down as maintainer of libxml2 which means that this project is more or less unmaintained for now.&lt;/p&gt;
        &lt;p&gt;I will fix regressions in the 2.15 release until the end of 2025.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 18 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Thank you for your hard work!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;mcatanzaro
(Michael Catanzaro)
3&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Yes, thank you for maintaining libxml2 for such a long time!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;imcsk8
(Iván Chavero)
4&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Hello, since I’ve stepped in as libxslt maintainer I’ve been studying both libxslt and libxml2 codebases. I have the time to maintain the library I just need to get familiar with the latest changes you introduced like:&lt;/p&gt;
        &lt;p&gt;I haven’t find how to manage both output and input buffers. I found functions like: xmlOutputBufferCreateIO but by the places in which I’ve found them is not clear on how to use them.&lt;/p&gt;
        &lt;p&gt;Should I send you an email with my questions or do you prefer other means of communication?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 7 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;sri
(sri)
5&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Thank you Nick for maintaining the key libraries of the internet and used in millions of products globaly. Best of luck to you.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;I am one of those millions of people that use this library on the behalf of us Thank you very much!!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45283196</guid><pubDate>Thu, 18 Sep 2025 00:17:20 +0000</pubDate></item><item><title>Meta Ray-Ban Display</title><link>https://www.meta.com/blog/meta-ray-ban-display-ai-glasses-connect-2025/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45283306</guid><pubDate>Thu, 18 Sep 2025 00:30:44 +0000</pubDate></item><item><title>Hypervisor 101 in Rust</title><link>https://tandasat.github.io/Hypervisor-101-in-Rust/</link><description>&lt;doc fingerprint="8eb71d9bf88743ae"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Welcome to Hypervisor 101 in Rust&lt;/head&gt;
    &lt;p&gt;This is a day long course to quickly learn the inner working of hypervisors and techniques to write them for high-performance fuzzing.&lt;/p&gt;
    &lt;p&gt;This course covers foundation of hardware-assisted virtualization technologies, such as VMCS/VMCB, guest-host world switches, EPT/NPT, as well as useful features and techniques such as exception interception for virtual machine introspection for fuzzing.&lt;/p&gt;
    &lt;p&gt;The class is made up of lectures using the materials within this directory and hands-on exercises with source code under the &lt;code&gt;Hypervisor-101-in-Rust/hypervisor&lt;/code&gt; directory.&lt;/p&gt;
    &lt;p&gt;This lecture materials are written for the &lt;code&gt;gcc2023&lt;/code&gt; branch, which notionally have incomplete code for step-by-step exercises. Check out the starting point of the branch as below to go over hands-on exercises before you start.&lt;/p&gt;
    &lt;code&gt;git checkout b17a59dd634a7b0c2b9a6d493fc9b0ff22dcfce5
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45283731</guid><pubDate>Thu, 18 Sep 2025 01:18:58 +0000</pubDate></item><item><title>Slack has raised our charges by $195k per year</title><link>https://skyfall.dev/posts/slack</link><description>&lt;doc fingerprint="314880df7fbeeee1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Slack is extorting us with a $195k/yr bill increase&lt;/head&gt;
    &lt;p&gt;An open letter, or something&lt;/p&gt;
    &lt;p&gt;For nearly 11 years, Hack Club - a nonprofit that provides coding education and community to teenagers worldwide - has used Slack as the tool for communication. We weren’t freeloaders. A few years ago, when Slack transitioned us from their free nonprofit plan to a $5,000/year arrangement, we happily paid. It was reasonable, and we valued the service they provided to our community.&lt;/p&gt;
    &lt;p&gt;However, two days ago, Slack reached out to us and said that if we don’t agree to pay an extra $50k this week and $200k a year, they’ll deactivate our Slack workspace and delete all of our message history.&lt;/p&gt;
    &lt;p&gt;One could argue that Slack is free to stop providing us the nonprofit offer at any time, but in my opinion, a six month grace period is the bare minimum for a massive hike like this, if not more. Essentially, Salesforce (a $230 billion company) is strong-arming a small nonprofit for teens, by providing less than a week to pony up a pretty massive sum of money, or risk cutting off all our communications. That’s absurd.&lt;/p&gt;
    &lt;head rend="h2"&gt;The impact&lt;/head&gt;
    &lt;p&gt;The small amount of notice has also been catastrophic for the programs that we run. Dozens of our staff and volunteers are now scrambling to update systems, rebuild integrations and migrate years of institutional knowledge. The opportunity cost of this forced migration is simply staggering.&lt;/p&gt;
    &lt;p&gt;Anyway, we’re moving to Mattermost. This experience has taught us that owning your data is incredibly important, and if you’re a small business especially, then I’d advise you move away too.&lt;/p&gt;
    &lt;p&gt;This post was rushed out because, well, this has been a shock! If you’d like any additional details then feel free to send me an email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45283887</guid><pubDate>Thu, 18 Sep 2025 01:37:11 +0000</pubDate></item><item><title>Show HN: The text disappears when you screenshot it</title><link>https://unscreenshottable.vercel.app/?text=Hello</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45284311</guid><pubDate>Thu, 18 Sep 2025 02:18:45 +0000</pubDate></item><item><title>A QBasic Text Adventure Still Expanding in 2025</title><link>https://the-ventureweaver.itch.io/</link><description>&lt;doc fingerprint="72a7387f83d3e415"&gt;
  &lt;main&gt;
    &lt;p&gt;Welcome to the world of text adventures! Here, we dive into the art of creating interactive stories that immerse players in thrilling, choice-driven narratives. Whether you're a fellow developer, a fan of retro games, or just curious about text-based adventures, this channel has something for you. Join me as I share insights, tips, and updates on my latest text games. Get ready to shape worlds, solve mysteries, and experience stories where your choices lead the way. Let's bring adventures to life, one line at a time!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45284398</guid><pubDate>Thu, 18 Sep 2025 02:25:59 +0000</pubDate></item><item><title>Towards a Physics Foundation Model</title><link>https://arxiv.org/abs/2509.13805</link><description>&lt;doc fingerprint="1cdc341adbb7c22e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 17 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Towards a Physics Foundation Model&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Foundation models have revolutionized natural language processing through a ``train once, deploy anywhere'' paradigm, where a single pre-trained model adapts to countless downstream tasks without retraining. Access to a Physics Foundation Model (PFM) would be transformative -- democratizing access to high-fidelity simulations, accelerating scientific discovery, and eliminating the need for specialized solver development. Yet current physics-aware machine learning approaches remain fundamentally limited to single, narrow domains and require retraining for each new system. We present the General Physics Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that demonstrates foundation model capabilities are achievable for physics. Our key insight is that transformers can learn to infer governing dynamics from context, enabling a single model to simulate fluid-solid interactions, shock waves, thermal convection, and multi-phase dynamics without being told the underlying equations. GPhyT achieves three critical breakthroughs: (1) superior performance across multiple physics domains, outperforming specialized architectures by up to 29x, (2) zero-shot generalization to entirely unseen physical systems through in-context learning, and (3) stable long-term predictions through 50-timestep rollouts. By establishing that a single model can learn generalizable physical principles from data alone, this work opens the path toward a universal PFM that could transform computational science and engineering.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.LG&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45284766</guid><pubDate>Thu, 18 Sep 2025 03:06:08 +0000</pubDate></item><item><title>CERN Animal Shelter for Computer Mice</title><link>https://computer-animal-shelter.web.cern.ch/index.shtml</link><description>&lt;doc fingerprint="d059938848eb287e"&gt;
  &lt;main&gt;
    &lt;p&gt;We are back!!! After the disaster early 2012, we have been able to secure new funds and are happy to annouce the reopening of the CERN Animal Shelter for Computer Mice on the lawn in front of the CERN Computer Centre. Our shelter is open all week-days from 8:30 to 17:30.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;In the hay...&lt;/cell&gt;
        &lt;cell&gt;Eating...&lt;/cell&gt;
        &lt;cell&gt;Drinking...&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;Cuddling...&lt;/cell&gt;
        &lt;cell&gt;Playing...&lt;/cell&gt;
        &lt;cell&gt;Panicking...&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;A message from our sponsor --- A message from our sponsor --- A message from our sponsor&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;"Stop — Think — Click"...&lt;/head&gt;
          &lt;p&gt;...is the basic recommendation for securely browsing the Internet and for securely reading emails. Users who have followed this recommendation in the past were less likely to have their computer infected or their computing account compromised. However, still too many users click on malicious web-links, and put their computer and account at risk.&lt;/p&gt;
          &lt;p&gt;Therefore, in order to avoid clicking at all, all CERN users are asked to disconnect their computer mice from CERN computers, and bring them to the CERN Animal Shelter for Computer Mice.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Let us help you:&lt;lb/&gt; visit https://cern.ch/Computer.Security or contact Computer.Security@cern.ch&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45286369</guid><pubDate>Thu, 18 Sep 2025 06:53:08 +0000</pubDate></item><item><title>History of the Gem Desktop Environment</title><link>https://nemanjatrifunovic.substack.com/p/history-of-the-gem-desktop-environment</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45286394</guid><pubDate>Thu, 18 Sep 2025 06:55:28 +0000</pubDate></item><item><title>Pnpm has a new setting to stave off supply chain attacks</title><link>https://pnpm.io/blog/releases/10.16</link><description>&lt;doc fingerprint="7674f93dd943850b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;pnpm 10.16&lt;/head&gt;
    &lt;head rend="h2"&gt;Minor Changes&lt;/head&gt;
    &lt;head rend="h3"&gt;New setting for delayed dependency updates&lt;/head&gt;
    &lt;p&gt;There have been several incidents recently where popular packages were successfully attacked. To reduce the risk of installing a compromised version, we are introducing a new setting that delays the installation of newly released dependencies. In most cases, such attacks are discovered quickly and the malicious versions are removed from the registry within an hour.&lt;/p&gt;
    &lt;p&gt;The new setting is called &lt;code&gt;minimumReleaseAge&lt;/code&gt;. It specifies the number of minutes that must pass after a version is published before pnpm will install it. For example, setting &lt;code&gt;minimumReleaseAge: 1440&lt;/code&gt; ensures that only packages released at least one day ago can be installed.&lt;/p&gt;
    &lt;p&gt;If you set &lt;code&gt;minimumReleaseAge&lt;/code&gt; but need to disable this restriction for certain dependencies, you can list them under the &lt;code&gt;minimumReleaseAgeExclude&lt;/code&gt; setting. For instance, with the following configuration pnpm will always install the latest version of webpack, regardless of its release time:&lt;/p&gt;
    &lt;code&gt;minimumReleaseAgeExclude:&lt;/code&gt;
    &lt;p&gt;Related issue: #9921.&lt;/p&gt;
    &lt;head rend="h3"&gt;Advanced dependency filtering with finder functions&lt;/head&gt;
    &lt;p&gt;Added support for &lt;code&gt;finders&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In the past, &lt;code&gt;pnpm list&lt;/code&gt; and &lt;code&gt;pnpm why&lt;/code&gt; could only search for dependencies by name (and optionally version). For example:&lt;/p&gt;
    &lt;code&gt;pnpm why minimist&lt;/code&gt;
    &lt;p&gt;prints the chain of dependencies to any installed instance of &lt;code&gt;minimist&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;verdaccio 5.20.1&lt;/code&gt;
    &lt;p&gt;What if we want to search by other properties of a dependency, not just its name? For instance, find all packages that have &lt;code&gt;react@17&lt;/code&gt; in their peer dependencies?&lt;/p&gt;
    &lt;p&gt;This is now possible with "finder functions". Finder functions can be declared in &lt;code&gt;.pnpmfile.cjs&lt;/code&gt; and invoked with the &lt;code&gt;--find-by=&amp;lt;function name&amp;gt;&lt;/code&gt; flag when running &lt;code&gt;pnpm list&lt;/code&gt; or &lt;code&gt;pnpm why&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let's say we want to find any dependencies that have React 17 in peer dependencies. We can add this finder to our &lt;code&gt;.pnpmfile.cjs&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;module.exports = {&lt;/code&gt;
    &lt;p&gt;Now we can use this finder function by running:&lt;/p&gt;
    &lt;code&gt;pnpm why --find-by=react17&lt;/code&gt;
    &lt;p&gt;pnpm will find all dependencies that have this React in peer dependencies and print their exact locations in the dependency graph.&lt;/p&gt;
    &lt;code&gt;@apollo/client 4.0.4&lt;/code&gt;
    &lt;p&gt;It is also possible to print out some additional information in the output by returning a string from the finder. For example, with the following finder:&lt;/p&gt;
    &lt;code&gt;module.exports = {&lt;/code&gt;
    &lt;p&gt;Every matched package will also print out the license from its &lt;code&gt;package.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;@apollo/client 4.0.4&lt;/code&gt;
    &lt;p&gt;Related PR: #9946.&lt;/p&gt;
    &lt;head rend="h2"&gt;Patch Changes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fix deprecation warning printed when executing pnpm with Node.js 24 #9529.&lt;/item&gt;
      &lt;item&gt;Throw an error if &lt;code&gt;nodeVersion&lt;/code&gt;is not set to an exact semver version #9934.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pnpm publish&lt;/code&gt;should be able to publish a&lt;code&gt;.tar.gz&lt;/code&gt;file #9927.&lt;/item&gt;
      &lt;item&gt;Canceling a running process with Ctrl-C should make &lt;code&gt;pnpm run&lt;/code&gt;return a non-zero exit code #9626.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45286526</guid><pubDate>Thu, 18 Sep 2025 07:12:56 +0000</pubDate></item><item><title>John Grisham Still Wonders: Will Texas Kill Robert Roberson?</title><link>https://www.dmagazine.com/frontburner/2025/09/author-john-grisham-still-wonders-will-texas-kill-robert-roberson/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45286905</guid><pubDate>Thu, 18 Sep 2025 08:08:05 +0000</pubDate></item><item><title>This Website Has No Class</title><link>https://aaadaaam.com/notes/no-class/</link><description>&lt;doc fingerprint="2bdc203e2ba735d9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This website has no class&lt;/head&gt;
    &lt;p&gt;In my recent post, “There’s no such thing as a CSS reset”, I wrote this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Think of elements like components, but ones that come packed in the browser. Custom elements, without the “custom” part. You can just like, use them.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The line continued to rattle around in my head, and a few weeks later when I was digging into some cleanup work I came to an uncomfortable realization; I wasn’t really taking my own advice. Sure, I was setting some default element styles, but I was leaving a lot on the table. I felt attacked. Called out even. Present me, positively roasted by past me. There was only one possible solution; refactor my website.&lt;/p&gt;
    &lt;p&gt;I like to apply severe constraints in designing and building this site – I think constraints lead to interesting, creative solutions – and it was no different this time around. Instead of relying on built in elements a bit more, I decided to banish classes from my website completely. I haven’t used a class-free approach since the CSS Zen Garden days, and wanted to se how it felt with modern HTML and CSS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Doubling down on styled defaults&lt;/head&gt;
    &lt;p&gt;CSS for the site was structured around 3 cascade layers; &lt;code&gt;base&lt;/code&gt;, &lt;code&gt;components&lt;/code&gt;, and &lt;code&gt;utilities&lt;/code&gt;. Everything in &lt;code&gt;base&lt;/code&gt; was already tag selectors, so the task at hand was to change my approach for components, and eliminate utilities completely.&lt;/p&gt;
    &lt;p&gt;Step 1? Mitigation. There was plenty of code that could have been styled defaults but wasn’t, so I gave all my markup a thorough review, increasing use of semantic elements, extracting common patterns in the form of new element defaults, and making more use of contextual element styling. By contextual styling, I mean going from something like this:&lt;/p&gt;
    &lt;code&gt;.header-primary {
  margin-block: clamp(var(--size-sm), 4vw, var(--size-lg)) var(--size-flex);
}&lt;/code&gt;
    &lt;p&gt;To something like this:&lt;/p&gt;
    &lt;code&gt;body {
  background-color: var(--color-sheet);

  &amp;amp; &amp;gt; header {
    margin-block: clamp(var(--size-sm), 4vw, var(--size-lg)) var(--size-flex);
  }
}&lt;/code&gt;
    &lt;p&gt;It was a good start, and modern features like nesting, &lt;code&gt;:where()&lt;/code&gt;, and &lt;code&gt;:has()&lt;/code&gt; made this feel better that it did 20 years ago, but I took things way too far with contextual styles. Taken to the extreme, you end up with overloaded selector definitions and progressively more esoteric selector patterns. I knew I was down the rabbit hole when I did something like this:&lt;/p&gt;
    &lt;code&gt;li {
  &amp;amp;:has( &amp;gt; a + p) {
    padding-block: var(--size-lg);
    border-block-end: var(--border-default);
    text-wrap: balance;

    &amp;amp; &amp;gt; a {
      font-size: var(--font-xxl);
    }

    &amp;amp; &amp;gt; p {
      margin-block: var(--size-sm);
    }
  }
}&lt;/code&gt;
    &lt;p&gt;I still needed a “real” solution for components, and a way to manage variants.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom tags &amp;amp; custom attributes&lt;/head&gt;
    &lt;p&gt;I had an inkling of a solution, which is to leverage patterns from custom elements and web components, sans js. By virtue of their progressively enhanced nature, custom tag names and custom attributes are 100% valid HTML, javascript or no. That inkling turned into fervent belief after reading Keith Cirkel’s excellent post “CSS classes considered harmful”.&lt;/p&gt;
    &lt;p&gt;Revisiting the example above, now we’ve got a pattern like this:&lt;/p&gt;
    &lt;code&gt;note-pad {
  padding-block: var(--size-lg);
  border-block-end: var(--border-default);
  text-wrap: balance;

  &amp;amp; a {
    font-size: var(--font-xxl);
  }

  &amp;amp; p {
    margin-block: var(--size-sm);
  }
}&lt;/code&gt;
    &lt;p&gt;Custom attributes become a go-to for handling former BEM modifiers, but instead of relying on stylistic writing convention to fake a key-value pair, you get an actual key-value pair.&lt;/p&gt;
    &lt;code&gt;random-pattern {
  &amp;amp; [shape-type="1"] {
    border: 0.1rem solid var(--color-sheet);
    background-color: var(--color-sheet);
    filter: url("#noise1");
  }

  &amp;amp; [shape-type="2"] {
    background: var(--pattern-lines-horizontal);
    background-size: var(--pattern-scale);
  }
}&lt;/code&gt;
    &lt;p&gt;Now, you can use &lt;code&gt;data-whatever&lt;/code&gt; for attributes, but really, any two dash-separated words are safe. Personally, I think dropping the &lt;code&gt;data&lt;/code&gt; prefix feels better and allows for richer semantics.&lt;/p&gt;
    &lt;p&gt;You can argue that both of these techniques are re-inventing classes in various ways. Kind of! You can use custom element names in lieu of semantic tags, just like you can slap a class on a div. But these techniques, particularly with how you can seamlessly enhance to true custom elements or web components, feels like a coherent end-to-end system in a way that class-based approaches don’t. It’s tags and attributes, all the way down.&lt;/p&gt;
    &lt;head rend="h2"&gt;Would I do this again?&lt;/head&gt;
    &lt;p&gt;On the plus side, the user outcomes are decidedly positive; I removed a non-trivial amount of CSS (now about ~5KB of CSS over the wire for the entire site), and accessibility is without question better due to having to paid much closer attention to markup. Also, just look at that markup. So clean. So shiny.&lt;/p&gt;
    &lt;p&gt;On the flipside, this feels like an approach that simply asks more of authors. It requires more careful planning compared to pure component approaches; you can’t think of things in purely isolated terms. All to say, I’m very happy to ship this on my personal website, I’d be less likely to advocate for this approach on a large project with varied levels of frontend knowledge.&lt;/p&gt;
    &lt;p&gt;There’s a variation here that’s more encapsulated (use custom tag names with abandon), but that pulls on what feels like an unresolved thread; replacing a semantic element with a custom tag name that has no semantic value feels bad, and adding extra wrappers around everything also feels bad.&lt;/p&gt;
    &lt;p&gt;All to say, I’m not quite ready to say that this is The One True Way I’ll build all sites from now on, but I also can’t help but feel like I’ve crossed some kind of threshold. I used to think classes were fine. Now I’m not so sure. I don’t know exactly where it’ll lead yet, but this feels like one of those exercises that’ll have a lasting influence on my work.&lt;/p&gt;
    &lt;p&gt;A mea culpa; I only got 99% of the way there. I use 11ty’s syntax highlighting plugin, which uses classes for styling. I gave syntax-highlight a hard look, but I don’t love the idea of introducing client-side js where none need exist, and the authoring experience would be a step back, so I begrudgingly left it alone for now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45287155</guid><pubDate>Thu, 18 Sep 2025 08:41:32 +0000</pubDate></item><item><title>Fast Fourier Transforms Part 1: Cooley-Tukey</title><link>https://connorboyle.io/2025/09/11/fft-cooley-tukey.html</link><description>&lt;doc fingerprint="a499143b0d082236"&gt;
  &lt;main&gt;&lt;p&gt;Find me on:&lt;/p&gt;&lt;p&gt;Posts:&lt;/p&gt;&lt;p&gt;by Connor Boyle&lt;/p&gt;&lt;p&gt;tags: mathematicssoftware&lt;/p&gt;&lt;p&gt;I’m planning to write a series of posts about fast Fourier transform algorithms. This first post covers the Cooley-Tukey algorithm, which is the original and most well-known FFT algorithm.&lt;/p&gt;&lt;p&gt;If \(x\) is a sequence of complex numbers with a length \(\lvert x \rvert\) and a starting index of 0, then the discrete Fourier transform of \(x\), \(\mathcal{F} \{ x \}\), is defined as follows:&lt;/p&gt;\[|\mathcal{F} \{ x \}| = |x|\] \[\mathcal{F} \{ x \}[k] = \sum_{j=0}^{|x|-1} x[j] \cdot e^{-i 2 \pi jk \frac{1}{|x|}}\]&lt;p&gt;Since complex exponentation is so commonly used in Fourier transforms, we’ll define a helpful term \(W_N\) as follows:&lt;/p&gt;\[W_N \triangleq e^{-i 2 \pi \frac{1}{N}}\]&lt;p&gt;i.e. \(W_N = W_N^1\) is a \(\frac{1}{N}\)-turn rotation in the complex plane (starting at 1). \(W_N^2\) is a \(\frac{2}{N}\)-turn rotation in the complex plane, etc. Substituting to the original discrete Fourier transform definition, we get:&lt;/p&gt;\[\mathcal{F} \{ x \}[k] = \sum_{j=0}^{|x|-1} x[j] \cdot W_{|x|}^{jk}\]&lt;p&gt;Naïvely evaluating this equation for each of the \(\lvert x \rvert\) different output frequency buckets of the DFT (\(k = 0, 1, \ldots, \lvert x \rvert - 2, \lvert x \rvert - 1\)) requires a summation of complex products of the \(\lvert x \rvert\) samples in the signal, thus giving any naïve DFT algorithm a time complexity of \(O(\lvert x \rvert^2)\).&lt;/p&gt;&lt;p&gt;If \(\lvert x \rvert\) is a composite number, we can pick two natural numbers \(r\) and \(d\), such that:&lt;/p&gt;\[\lvert x \rvert = r \cdot d\]&lt;p&gt;This allows us to change the single summation over \(j\) into nested summations:&lt;/p&gt;\[\mathcal{F} \{ x \}[k] = \sum_{j_1=0}^{d-1} \sum_{j_0=0}^{r-1} x[j_1 r + j_0] \cdot W_{\lvert x \rvert}^{(j_1 r + j_0)k}\] \[= \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{\lvert x \rvert}^{(j_1 r + j_0)k}\]&lt;p&gt;Similarly, we can define variables \(k_0\) and \(k_1\) such that \(k = k_1 d + k_0\). Let:&lt;/p&gt;\[k_1 \triangleq \lfloor \frac{k}{d} \rfloor\] \[k_0 \triangleq k - k_1 d\]&lt;p&gt;In other words, \(k_1\) is the quotient and \(k_0\) is the remainder of the Euclidean division1 of \(k\) by \(d\).&lt;/p&gt;&lt;p&gt;This allows us to again re-formulate the discrete Fourier transform:&lt;/p&gt;\[\mathcal{F} \{ x \}[k] = \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{|x|}^{(j_1 r + j_0) (k_1 d + k_0)}\] \[= \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{|x|}^{j_1 r k_1 d + j_1 r k_0 + j_0 (k_1 d + k_0)}\] \[= \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{|x|}^{j_1 k_1 |x| + j_1 r k_0 + j_0 k}\] \[= \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{|x|}^{j_1 k_1 |x|} \cdot W_{|x|}^{j_1 r k_0} \cdot W_{|x|}^{j_0 k}\]&lt;p&gt;Since \(W_{\lvert x \rvert}^{j_1 k_1 \lvert x \rvert} = (e^{-i \frac{2 \pi \lvert x \rvert}{\lvert x \rvert}})^{j_1 k_1} = 1^{j_1 k_1} = 1\), therefore:&lt;/p&gt;\[\mathcal{F} \{ x \}[k] = \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{\lvert x \rvert}^{j_1 r k_0} \cdot W_{\lvert x \rvert}^{j_0 k}\]&lt;p&gt;At this point, we can split the elements of \(x\) into sub-sequences corresponding to modulo classes. Let \(x_{r}^{j_0}\) be a sequence whose elements are equal to the elements of \(x\) whose indices are equivalent \(j_0\) modulo \(r\). More formally, these sequences (of which there are \(r\) total) can be defined as follows:&lt;/p&gt;\[x_r^{j_0}[j_1] = x[j_1 r + j_0]\] \[|x_r^{j_0}| = \frac{|x|}{r} = d\]&lt;p&gt;Substituting this sequence definition, we get:2&lt;/p&gt;\[\mathcal{F} \{ x \}[k] = \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x_r^{j_0}[j_1] W_{|x|}^{k_0 j_1 r} W_{|x|}^{j_0 k}\] \[= \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x_r^{j_0}[j_1] W_d^{k_0 j_1} W_{|x|}^{j_0 k}\] \[= \sum_{j_0=0}^{r-1} \mathcal{F} \{ x_r^{j_0} \}[k_0] W_{|x|}^{j_0 k}\]&lt;p&gt;Let’s consider how long it will take to evaluate the discrete Fourier transform in this formulation:&lt;/p&gt;&lt;p&gt;Added together, these two sub-routines require \(O(\lvert x \rvert \cdot d + \lvert x \rvert \cdot r) = O(\lvert x \rvert \cdot (d + r))\) operations, possibly a significant improvement from the original \(O(\lvert x \rvert^2)\) complexity of the original naive formulation, depending on the values of \(r\) &amp;amp; \(d\). More importantly, this manipulation can be applied recursively. Specifically, each of the \(r\) discrete Fourier transforms of the \(d\)-length sequences \(x_r^{j_0}\) can be broken down into \(r'\) Fourier transforms of length \(d'\), assuming that two natural numbers exist such that \(d = r' \cdot d'\).3 In the ideal4 case where \(\lvert x \rvert = 2^n\), \(n \in \mathbb{N}\), calculating the Cooley-Tukey algorithm will require \(O(\lvert x \rvert \cdot (2 + 2 + \ldots + 2)) = O(\lvert x \rvert \cdot 2 \cdot \log_2(\lvert x \rvert)) = O (\lvert x \rvert \cdot \log(\lvert x \rvert))\) operations.&lt;/p&gt;&lt;p&gt;The Cooley-Tukey algorithm can also be used to calculate the inverse discrete Fourier transform with only very slight modification. In fact, the original Cooley-Tukey paper (see “related reading”) specifically described an algorithm to compute the inverse discrete Fourier transform, not the “forward” DFT. I will leave the Cooley-Tukey iDFT algorithm as an exercise for the reader.&lt;/p&gt;&lt;p&gt;However, note that the Cooley-Tukey algorithm gives no speed-up for input sequences of prime length, and provides relatively little speed-up when the factors of the input length contain large primes. To efficiently compute the DFT for sequences of prime or even non-highly-composite lengths, we will need additional algorithms. Ultimately, however, these other FFT algorithms generally depend on Cooley-Tukey for part of the computation. I plan to cover at least one of these techniques—Bluestein’s algorithm—in a future blog post(s).&lt;/p&gt;&lt;p&gt;This visualization shows how the discrete Fourier transform of some signal \(x\) is computed using the Cooley-Tukey algorithm. The black boxes at the very bottom are the input signal. While the DFT can be applied to complex signals, I’ve restricted the sample values of the input signal to be real numbers, for simplicity’s sake (this mimics some real-world applications, such as performing a DFT on an audio recording). You can click and drag on the input boxes to change their values.&lt;/p&gt;&lt;p&gt;The grey circles and the sometimes visible white “clock hands” inside of them represent the complex exponent \(W_N^x = e^{-2 i \pi \frac{x}{N}}\) for some \(N\) (e.g. \(\lvert x \rvert\), \(d\), \(d'\), etc.) and some \(x\). These complex exponents, which are equivalent to rotations in the complex plane, are applied to the relevant input value. Unlike the usual convention, I’ve decided to show the real component of the complex plane as vertical (“up” is positive-real) and the imaginary component as horizontal (“right” is imaginary-positive). You can see where the input value is drawn from by hovering the mouse over a given “rotation” box.&lt;/p&gt;&lt;p&gt;The sum of those rotated input values is added together to calculate one element of a discrete Fourier transform. Hover over a white “output” box of a discrete Fourier transform in the visualization to highlight the column of “rotation” boxes that it was summed from.&lt;/p&gt;&lt;p&gt;\(|x| =\)&lt;/p&gt;&lt;p&gt;Available factors:&lt;/p&gt;&lt;p&gt;I’ve noticed an irritating and confusing tendency among many people–including published authors–when talking about discrete Fourier transforms. I find they often use the phrase “fast Fourier transform” (or perhaps more often, the abbreviation “FFT”) when they mean “discrete Fourier transform” (or “DFT”). I think this is wrong and confusing; to understand why, imagine you have a list:&lt;/p&gt;&lt;code&gt;x = [10, 3, 2, 19, -2]
&lt;/code&gt;&lt;p&gt;would it make sense to refer to the following list as the “mergesort” of the previous list?&lt;/p&gt;&lt;code&gt;y = [-2, 2, 3, 10, 19]
&lt;/code&gt;&lt;p&gt;I think most people would find that very strange. We can say that the second list is the result of sorting the first list, but we don’t know anything about the specific algorithm that was used to sort the list. It could have been sorted using mergesort, quicksort, heapsort, bubblesort, bogosort, or any other sorting algorithm (in reality, I just sorted this one in my head). However, even if I had used mergesort, &lt;code&gt;y&lt;/code&gt; still wouldn’t be the “mergesort” of
&lt;code&gt;x&lt;/code&gt;, it would still just be “the result of sorting &lt;code&gt;x&lt;/code&gt;”.&lt;/p&gt;&lt;p&gt;Similarly, the output of an FFT algorithm should not be referred to as “an/the FFT of” anything. In theory, calculating the DFT of a sequence using Cooley-Tukey gives the exact same result as calculating that DFT using a naïvely-implemented DFT algorithm. In practice, Cooley-Tukey will probably give a slightly more accurate result since there are fewer total calculations and therefore fewer opportunities for floating point round-off error.&lt;/p&gt;&lt;p&gt;This is not just irritating to me; I think it causes confusion among the public. A friend of mine–an intelligent mathematics major and software engineer–recently asked me “what would I lose by taking the fast Fourier transform instead of the ‘normal’ Fourier transform? I’ve always just taken the normal Fourier transform”. He probably inferred from the way many people throw around the phrase “FFT” that the “fast Fourier transform” is some kind of approximation or related concept yet distinct from the discrete Fourier transform or Fourier transforms in general, rather than an algorithm for calculating such.&lt;/p&gt;&lt;p&gt;Thank you to my friend Andre Archer, who helped to proofread an earlier version of this post. Any mistakes are my own.&lt;/p&gt;&lt;p&gt;Footnotes:&lt;/p&gt;&lt;p&gt;As far as I am aware, there is no widely-standardized notation for Euclidean division in mathematics. Personally, I find this quite silly. ↩&lt;/p&gt;&lt;p&gt;If \(r\) is chosen to be equal to \(\lvert x \rvert\), and therefore \(d = 1\), then calculating \(\mathcal{F} \{ x \}\) using Cooley-Tukey is equivalent to calculating \(\mathcal{F} \{ x \}\) naïvely, taking \(O(\lvert x \rvert^2)\) steps. ↩&lt;/p&gt;&lt;p&gt;By “ideal” I do not necessarily mean optimal, in any sense. \(|x| = 2^n\) is “ideal” in that it is very simple to reason about. ↩&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45287513</guid><pubDate>Thu, 18 Sep 2025 09:28:40 +0000</pubDate></item><item><title>CircuitHub (YC W12) Is Hiring Operations Research Engineers (UK/Remote)</title><link>https://www.ycombinator.com/companies/circuithub/jobs/UM1QSjZ-operations-research-engineer</link><description>&lt;doc fingerprint="29da4270258e37c0"&gt;
  &lt;main&gt;
    &lt;p&gt;On-Demand Electronics Manufacturing&lt;/p&gt;
    &lt;p&gt;CircuitHub is reshaping electronics manufacturing with The Grid , a factory-scale robotics platform designed to make small-batch, high-mix electronics assembly radically more efficient. Think semiconductor-fab levels of precision applied to the chaotic world of prototyping and low-volume production. The result? A 10x throughput improvement in one of the world's most foundational industries.&lt;/p&gt;
    &lt;p&gt;We've raised $20M from top-tier investors, including Y Combinator and Google Ventures , and we're already profitable. Our customers include industry leaders like Tesla, Meta, and Zipline.&lt;/p&gt;
    &lt;p&gt;The Grid isn't a prototype. It's live, scaling fast, and delivering real revenue. We're now building the engineering core that will scale it further.&lt;/p&gt;
    &lt;p&gt;We're looking for an engineer to found our Operations Research Team. You'll tackle our trickiest scheduling and pricing optimization problems to help make the world's most efficient electronics factory. This is an incredibly high-impact role that will directly contribute to 3x revenue growth over the next year.&lt;/p&gt;
    &lt;p&gt;This role is only suitable for someone with direct experience with operations research problems.&lt;/p&gt;
    &lt;p&gt;Decide what our factory should build and when&lt;/p&gt;
    &lt;p&gt;Develop novel quoting algorithms&lt;/p&gt;
    &lt;p&gt;Predict forward-looking revenue based on capacity&lt;/p&gt;
    &lt;p&gt;Remote or work from one of our labs in the UK (London, Cambridge) or USA (Boston)&lt;/p&gt;
    &lt;p&gt;The work of the operations research team will be critical to making electronics manufacturing economically viable in the USA and Europe. This is a career-defining opportunity for a high-agency engineer. If you thrive on ownership and solving real problems using advanced research techniques, there's no better place to be.&lt;/p&gt;
    &lt;p&gt;Python&lt;/p&gt;
    &lt;p&gt;Google OR-Tools&lt;/p&gt;
    &lt;p&gt;Gurobi&lt;/p&gt;
    &lt;p&gt;MiniZinc&lt;/p&gt;
    &lt;p&gt;We're building a future where hardware companies can design and iterate as fast as software companies&lt;/p&gt;
    &lt;p&gt;CircuitHub is on a mission to fix rapid electronics prototyping. We are the first automated electronics factory built around a modern tech stack. We help hardware companies producing self driving cars, satellites, 3D printers, robotics, &amp;amp; more to rapidly prototype electronics and get to market faster.&lt;/p&gt;
    &lt;p&gt;We've raised $20M from top investors that include Y Combinator , Google Ventures, &amp;amp; more. With business growing fast we are looking to fill roles in Massachusetts, USA and London, UK .&lt;/p&gt;
    &lt;p&gt;Join us to solve real world problems while shaping the future of automated manufacturing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45287551</guid><pubDate>Thu, 18 Sep 2025 09:33:07 +0000</pubDate></item><item><title>Nvidia buys $5B in Intel stock in seismic deal</title><link>https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal</link><description>&lt;doc fingerprint="27f84771ddaa5063"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nvidia and Intel announce jointly developed 'Intel x86 RTX SOCs' for PCs with Nvidia graphics, also custom Nvidia data center x86 processors — Nvidia buys $5 billion in Intel stock in seismic deal&lt;/head&gt;
    &lt;p&gt;Cats and Dogs, living together!&lt;/p&gt;
    &lt;p&gt;In a surprising announcement that finds two long-time rivals working together, Nvidia and Intel announced today that the companies would jointly develop multiple new generations of x86 products together. The products include x86 Intel CPUs tightly fused with an Nvidia RTX graphics chiplet for the consumer gaming PC market, named the ‘Intel x86 RTX SOCs.’ Nvidia will also have Intel build custom x86 data center CPUs for its AI products for hyperscale and enterprise customers. Additionally, Nvidia announced that it will buy $5 billion in Intel common stock at $23.28 per share, representing a roughly 5% ownership stake in Intel. (Intel stock is now up 33% in premarket trading.) We spoke with Nvidia representatives to learn more details about the company’s plans.&lt;/p&gt;
    &lt;p&gt;Nvidia says that the partnership between the two companies is in the very early stages, so the timeline for product releases, along with any product specifications, will be disclosed at a later, unspecified date. (Given the traditionally long lead-times for new processors, it is rational to expect these products will take at least a year, and likely longer, to come to market.)&lt;/p&gt;
    &lt;p&gt;Nvidia emphasized that the companies are committed to multi-generation roadmaps for the co-developed products, which represents a strong investment in the x86 ecosystem. However, Nvidia tells us it also remains fully committed to its other announced product roadmaps and architectures, including for its Arm-based GB10 Grace Blackwell processors for workstations and the Nvidia Grace CPUs for data centers and the next-gen Vera CPUs. Nvidia says it also remains committed to products on its internal roadmaps that haven’t been publicly disclosed yet, indicating that the new roadmap with Intel will merely be additive to its existing initiatives.&lt;/p&gt;
    &lt;p&gt;Nvidia hasn’t disclosed whether it will use Intel Foundry to produce any of the products yet. However, while Intel has used TSMC to manufacture some of its recent products, its goal is to bring production of most of its high-performance products back into its own foundries, and some of its products never left. For instance, Intel’s existing Granite Rapids data center processors use the ‘Intel 3’ node, and the upcoming Clearwater Forest Xeons will use Intel’s own 18A process node for compute. This suggests that at least some of the Nvidia-custom x86 silicon, particularly for the data center, could be fabbed on Intel nodes. However, Intel also uses TSMC to fabricate many of its client x86 processors now, so we won’t know for sure until official announcements are made — particularly for the RTX GPU chiplet.&lt;/p&gt;
    &lt;p&gt;In either case, Nvidia has been mulling using Intel Foundry since 2022, has fabbed test chips there, and participates in the U.S. Defense Dept's RAMP-C project with Intel that involves Nvidia already making chips on Intel's 18A process node, so it wouldn't be a total surprise.&lt;/p&gt;
    &lt;p&gt;While the two companies have engaged in heated competition in some market segments, Intel and Nvidia have partnered for decades, ensuring interoperability between their hardware and software for products spanning both the client and data center markets. However, these products have long used the PCIe interface to connect Intel CPUs and Nvidia’s GPUs. The new partnership will find tighter integration using the NVLink interface for CPU-to-GPU communication, which affords up to 14 times more bandwidth along with lower latency than PCIe, thus granting the new x86 products access to the highest performance possible when paired with GPUs. Let’s dive into the details we’ve learned so far.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intel x86 RTX SOCs for the PC gaming market&lt;/head&gt;
    &lt;p&gt;For the PC market, the Intel x86 RTX SoC chips will come with an x86 CPU chiplet tightly connected with an Nvidia RTX GPU chiplet via the NVLink interface. This type of processor will have both the CPU and GPU units merged into one compact chip package that externally looks much like a standard CPU, rivaling AMD’s competing APU products.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;This type of tight integration packs all the gaming prowess into one package without an external discrete GPU, providing power and footprint advantages. As such, these chips will be heavily focused on thin-and-light gaming laptops and small form-factor PCs, much like today’s APUs from AMD. However, it’s possible the new Nvidia/Intel chips could come in multiple flavors and permeate further into the Intel stack over time.&lt;/p&gt;
    &lt;p&gt;Intel has worked on a similar type of chip before with AMD; however, there is at least one significant technical difference between these initiatives. Intel launched its Kaby Lake-G chip in 2017 with an Intel processor fused into the same package with an AMD Radeon GPU chiplet, much the same as the description of the new Nvidia/Intel chips. You can see an image of the Intel/AMD chip below.&lt;/p&gt;
    &lt;p&gt;This SoC had a CPU at one end connected via a PCIe connection to the separate AMD GPU chiplet, which is flanked by a small, dedicated memory package. This separate memory package was only usable by the GPU. The Nvidia/Intel products will have an RTX GPU chiplet connected to the CPU chiplet via the faster and more efficient NVLink interface, and we’re told it will have uniform memory access (UMA), meaning both the CPU and GPU will be able to access the same pool of memory.&lt;/p&gt;
    &lt;p&gt;Intel notoriously axed the Kaby Lake-G products in 2019, and the existing systems were left without proper driver support for quite some time, in part because Intel was responsible for validating the drivers, and then finger-pointing ensued. We’re told that both Intel and Nvidia will be responsible for their respective drivers for the new models, with Nvidia naturally providing its own GPU drivers. However, Intel will build and sell the consumer processors.&lt;/p&gt;
    &lt;p&gt;We haven’t spoken with Intel yet, but the limited scope of this project means that Intel’s proprietary Xe graphics architecture will most assuredly live on as the primary integrated GPU (iGPU) for its mass-market products.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nvidia's first x86 data center CPUs&lt;/head&gt;
    &lt;p&gt;Intel will fabricate custom x86 data center CPUs for Nvidia, which Nvidia will then sell as its own products to enterprise and data center customers. However, the entirety and extent of the modification are currently unknown. We do know that Nvidia will employ its NVLink interface, which tells us the chips could leverage Nvidia’s new NVLink Fusion tech that enables custom CPUs and accelerators to enable faster, more efficient communication with Nvidia’s GPUs than found with the PCIe interface.&lt;/p&gt;
    &lt;p&gt;Intel has long offered custom Xeons to its customers, primarily hyperscalers, often with relatively minor tweaks to clock rates, cache capacities, and other specifications. In fact, these mostly slightly-modified custom Xeon models once comprised more than 50% of Intel’s Xeon shipments. Intel has endured several years of market share erosion due to AMD’s advances, most acutely in the hyperscale market. Therefore, it is unclear if the 50% number still holds true, as hyperscalers were the primary customers for custom models.&lt;/p&gt;
    &lt;p&gt;Intel has long said that it will design completely custom x86 chips for customers as part of its IDM 2.0 strategy. However, aside from a recent announcement of custom AWS chips that sound like the slightly modified Xeons mentioned above, we haven’t heard of any large-scale uptake for significantly modified custom x86 processors. Intel announced a new custom chip design unit just two weeks ago, so it will be interesting to learn the extent of the customization for Nvidia’s x86 data center CPUs.&lt;/p&gt;
    &lt;p&gt;Nvidia already uses Intel’s Xeons in several of its systems, like the Nvidia DGX B300, but these systems still use the PCIe interface to communicate with the CPU. Intel’s new collaboration with Nvidia will obviously open up new opportunities, given the tighter integration with NVLink and all the advantages it brings with it. The likelihood of AMD adopting NVLink Fusion is somewhere around zero, as the company is heavily invested in its own Infinity Fabric (XGMI) and Ultra Accelerator Link (UALink) initiatives, which aim to provide an open-standard interconnect to rival NVLink and democratize rack-scale interconnect technologies. Intel is also a member of UALink, which uses AMD’s Infinity Fabric protocol as the foundation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dollar and Cents, Geopolitics&lt;/head&gt;
    &lt;p&gt;Nvidia’s $5 billion purchase of Intel common stock will come at $23.28 a share, roughly 6% below the current market value, but several aspects of this investment remain unclear. Nvidia hasn’t stated whether it will have a seat on the board (which is unlikely) or how it will vote on matters requiring shareholder approval. It is also unclear if Intel will issue new stock (primary issuance) for Nvidia to purchase, as it did when the U.S. government recently became an Intel shareholder (that is likely). Naturally, the investment is subject to approval from regulators.&lt;/p&gt;
    &lt;p&gt;Nvidia’s buy-in comes on the heels of the U.S government buying $10 billion of newly-created Intel stock, granting the country a 9.9% ownership stake at $20.47 per share. The U.S. government won’t have a seat on the board and agreed to vote with Intel’s board on matters requiring shareholder approval “with limited exceptions.” Softbank has also recently purchased $2 billion worth of primary issuance Intel stock at $23 per share.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Row 0 - Cell 0&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Total&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Share Price&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Stake in Intel&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Nvidia&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$5 Billion&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$23.28&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~5%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;U.S. Government&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$9 Billion&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$20.47&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~9.9%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Softbank&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$2 Billion&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$23&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Row 3 - Cell 3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The U.S. government says it invested in Intel with the goal of bolstering US technology, manufacturing, and national security, and the investments from the private sector also help solidify the struggling Intel. Altogether, these investments represent a significant cash influx for Intel as it attempts to maintain the heavy cap-ex investments required to compete with TSMC, all while struggling with a negative amount of free cash flow.&lt;/p&gt;
    &lt;p&gt;“AI is powering a new industrial revolution and reinventing every layer of the computing stack — from silicon to systems to software. At the heart of this reinvention is Nvidia’s CUDA architecture,” said Nvidia CEO Jensen Huang. “This historic collaboration tightly couples NVIDIA’s AI and accelerated computing stack with Intel’s CPUs and the vast x86 ecosystem—a fusion of two world-class platforms. Together, we will expand our ecosystems and lay the foundation for the next era of computing.”&lt;/p&gt;
    &lt;p&gt;“Intel’s x86 architecture has been foundational to modern computing for decades – and we are innovating across our portfolio to enable the workloads of the future,” said Intel CEO Lip-Bu Tan. “Intel’s leading data center and client computing platforms, combined with our process technology, manufacturing and advanced packaging capabilities, will complement Nvidia's AI and accelerated computing leadership to enable new breakthroughs for the industry. We appreciate the confidence Jensen and the Nvidia team have placed in us with their investment and look forward to the work ahead as we innovate for customers and grow our business.”&lt;/p&gt;
    &lt;p&gt;We’ll learn more details of the new partnership later today when Nvidia CEO Jensen Huang and Intel CEO Lip-Bu Tan hold a webcast press conference at 10 am PT.&lt;/p&gt;
    &lt;p&gt;This is breaking news…more to come.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!&lt;/p&gt;
    &lt;p&gt;Paul Alcorn is the Editor-in-Chief for Tom's Hardware US. He also writes news and reviews on CPUs, storage, and enterprise hardware.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;bolweval&lt;/header&gt;WOW, didn't see that coming, but it makes good business sense, especially for Intel..Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;usertests&lt;/header&gt;Wow.Reply&lt;lb/&gt;So this is still kind of like an integrated dGPU, unlike the chiplet/tile-based CPUs with integrated graphics they're already making?&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;watzupken&lt;/header&gt;AMD must be sweating now. An Intel and Nvidia tag team is going to give AMD some tough competition.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;hotaru251&lt;/header&gt;Reply&lt;quote/&gt;pretty much.... basically it keeps intel afloat by riding on nvidia and nvidia gets to take its monopoly even further.watzupken said:AMD must be sweating now. An Intel and Nvidia tag team is going to give AMD some tough competition.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;usertests&lt;/header&gt;Reply&lt;quote/&gt;What products are they trying to compete with?watzupken said:AMD must be sweating now. An Intel and Nvidia tag team is going to give AMD some tough competition.&lt;lb/&gt;Intel's mobile graphics were already competitive with AMD's phoned-in 128-bit APUs, with Lunar Lake being around Strix Point for example. They don't need an Nvidia chiplet to compete.&lt;lb/&gt;Intel never cared to make a desktop APU from the aforementioned graphics.&lt;lb/&gt;So that leaves Strix Halo. And Apple products.&lt;lb/&gt;They also get to outsource drivers and software to Nvidia, and CUDA will work with these. That could be a bigger deal than any hardware match-up.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;rluker5&lt;/header&gt;Reply&lt;quote/&gt;I wonder how much this will help transitioning common CPU tasks to GPU like OneAPI was supposed to?usertests said:What products are they trying to compete with?&lt;lb/&gt;Intel's mobile graphics were already competitive with AMD's phoned-in 128-bit APUs, with Lunar Lake being around Strix Point for example. They don't need an Nvidia chiplet to compete.&lt;lb/&gt;Intel never cared to make a desktop APU from the aforementioned graphics.&lt;lb/&gt;So that leaves Strix Halo. And Apple products.&lt;lb/&gt;They also get to outsource drivers and software to Nvidia, and CUDA will work with these. That could be a bigger deal than any hardware match-up.&lt;quote/&gt;Isn't buying common shares is the opposite of dilution? It sounds like same amount of shares enriched with more money.dalek1234 said:So another share dilution, yet Intel stock price is going up.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;aldaia&lt;/header&gt;"Nvidia announced that it will buy $5 billion in Intel common stock"Reply&lt;lb/&gt;Wondering if this is the start of a takeover process.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;blppt&lt;/header&gt;Wait a minute, if they start moving in this direction, wouldn't that make Intel's growing GPU division superfluous or redundant?Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;tamalero&lt;/header&gt;So an outright overpowered monopoly trying to prop up a falling monopoly by setting up joint products.. what can go wrong?Reply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45288161</guid><pubDate>Thu, 18 Sep 2025 11:04:48 +0000</pubDate></item><item><title>Nvidia to Invest $5B in Intel</title><link>https://www.ft.com/content/be8d4c0c-66ff-4dfd-9b43-af6c0b290ada</link><description>&lt;doc fingerprint="9e7b0571dfaad1e9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;&lt;quote&gt;Nvidia to invest $5bn in rival Intel&lt;/quote&gt;&lt;/head&gt;&lt;head rend="h2"&gt;Save 40% on Standard Digital&lt;/head&gt;was $540 now $319 for your first year&lt;p&gt;Save now on essential digital access to quality FT journalism on any device. Saving based on monthly annualised price.&lt;/p&gt;&lt;head rend="h2"&gt;Explore more offers.&lt;/head&gt;&lt;head rend="h3"&gt;Trial&lt;/head&gt;&lt;p&gt;Then $75 per month. Complete digital access to quality FT journalism on any device. Cancel or change your plan anytime during your trial.&lt;/p&gt;&lt;head rend="h3"&gt;Premium Digital&lt;/head&gt;&lt;p&gt;Complete digital access to quality FT journalism with expert analysis from industry leaders. Pay a year upfront and save 20%.&lt;/p&gt;&lt;p&gt;FT newspaper delivered Monday-Saturday, plus FT Digital Edition delivered to your device Monday-Saturday.&lt;/p&gt;&lt;p&gt;Check whether you already have access via your university or organisation.&lt;/p&gt;&lt;p&gt;Terms &amp;amp; Conditions apply&lt;/p&gt;&lt;head rend="h2"&gt;Explore our full range of subscriptions.&lt;/head&gt;&lt;head rend="h3"&gt;For individuals&lt;/head&gt;&lt;p&gt;Discover all the plans currently available in your country&lt;/p&gt;&lt;head rend="h3"&gt;For multiple readers&lt;/head&gt;&lt;p&gt;Digital access for organisations. Includes exclusive features and content.&lt;/p&gt;&lt;head rend="h2"&gt;Why the FT?&lt;/head&gt;&lt;p&gt;See why over a million readers pay to read the Financial Times.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45288271</guid><pubDate>Thu, 18 Sep 2025 11:24:34 +0000</pubDate></item><item><title>You Had No Taste Before AI</title><link>https://matthewsanabria.dev/posts/you-had-no-taste-before-ai/</link><description>&lt;doc fingerprint="a49ea3d18a83c5cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You Had No Taste Before AI&lt;/head&gt;
    &lt;head class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;There’s been an influx of people telling others to develop taste to use AI. Designers. Marketers. Developers. All of them touting the same message. It’s ironic, though. These are the same people who never questioned why their designs all look identical, never iterated beyond the first draft, and never asked if their work actually solved the problem at hand.&lt;/p&gt;
    &lt;p&gt;They’re not alone. The loudest voices preaching about taste and AI are often the ones who never demonstrated taste before AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Taste? #&lt;/head&gt;
    &lt;p&gt;The technology industry has a tendency to use words that mean multiple things without describing which definition they are referring to. When I read about taste and AI I usually see people referring to the following definition.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Critical judgment, discernment, or appreciation of aesthetic quality.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In the context of AI, this definition manifests itself in several ways.&lt;/p&gt;
    &lt;p&gt;Contextual Appropriateness: Knowing when AI-generated content fits the situation and when it doesn’t. Put another way, knowing when a human touch is needed (e.g., a message to a loved one).&lt;/p&gt;
    &lt;p&gt;Quality Recognition: Being able to distinguish between useful AI-generated content and slop. This requires domain knowledge to truly discern aesthetic quality rather than just functional quality.&lt;/p&gt;
    &lt;p&gt;Iterative Refinement: Understanding that AI is a starting point that requires further iteration. This point is most similar to how culinary taste is applied to refine a dish by iterating on the recipe and presentation.&lt;/p&gt;
    &lt;p&gt;Ethical Boundaries: Recognizing when AI crosses the lines of authenticity, legality, and respect. Basically, don’t use AI to do bad things.&lt;/p&gt;
    &lt;p&gt;None of these skills are new. These are the same skills we should have been applying to our work all along. Why are we asking about taste and AI now when we should have been applying taste the whole time? Perhaps people advocating for taste are telling on themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;Being Tasteless #&lt;/head&gt;
    &lt;p&gt;Some people have no taste. In the best case that may be due to lack of experience but in the worst case it may be due to ignorance. I’m noticing that many people worried about tasteless AI-generated content are often guilty of producing tasteless content themselves, usually manifesting as the following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Copying and pasting code without understanding it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sending resumes and emails that aren’t proofread and edited.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Asking others to review code without giving it a self review.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Noticing a quality issue and failing to document or fix it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Designing websites that look exactly like every other company’s website.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Regurgitating content from the trending influencer of the week.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Where’s the taste here? Where’s the critical judgment, discernment, or appreciation of aesthetic quality that separates mediocrity from excellence?&lt;/p&gt;
    &lt;p&gt;It’s not there because most people haven’t developed their taste yet. AI didn’t create this tasteless problem. People did. Now that everyone can generate content at the speed of thought we’re noticing that not all content is actually good. To play on a popular quote from Ratatouille, anyone can cook, but not everyone is a chef. Don’t complain about mediocre work when you’re producing mediocre work yourself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spectrum of Taste #&lt;/head&gt;
    &lt;p&gt;What about the nature of taste itself? Should people focus on developing depth of taste in specific domains or breadth of taste across many domains? My short answer is a bit of both, if possible.&lt;/p&gt;
    &lt;p&gt;Depth of taste means becoming an expert within a particular domain. We’ve all met such experts and even asked them for help on tricky, bespoke topics within their domain. A person with depth of taste can recognize when AI-generated content is refined and of high quality versus merely functional. This kind of taste comes from years of experience in a specific role coupled with deep domain knowledge.&lt;/p&gt;
    &lt;p&gt;Breadth of taste means becoming knowledgeable across multiple domains and understanding how those domains interface with one another. A person with breadth of taste can recognize when AI-generated content is contextually appropriate, authentic, and of enough quality to use for their needs. This kind of taste comes from years of experience across multiple roles coupled with moderate domain knowledge.&lt;/p&gt;
    &lt;p&gt;Breadth of taste is more valuable with AI. When using AI, you’re constantly switching between domains: a software engineer writing documentation, a marketer creating designs. Breadth lets you maintain quality across these contexts while recognizing when you need domain expertise. You iterate faster because you have opinions about what “good enough” looks like across multiple domains.&lt;/p&gt;
    &lt;p&gt;The people I see being most effective with AI developed a breadth of taste that they use to determine what good AI-generated content looks like, regardless of domain. They can recognize when something feels off, even if they can’t articulate exactly why. They understand their own limitations and know when to seek expertise in a specific domain. That’s not to say those with depth of taste can’t be successful with AI, but I see those people reluctant to use AI because they are more knowledgeable than AI in a particular domain.&lt;/p&gt;
    &lt;head rend="h2"&gt;It Tastes Bitter #&lt;/head&gt;
    &lt;p&gt;If you’re reading this thinking you have to spend time developing your taste, good! Perhaps I’ve left a bitter taste in your mouth. The good news is you’re not alone. There are many people that need to hear this to better their taste, myself included. The challenge here is recognizing that it’s not about developing taste for AI but rather about developing taste, period. If you’ve had poor taste before AI you’ll have poor taste with AI. If you’ve had good taste before AI, you’ll be able to apply that taste with AI.&lt;/p&gt;
    &lt;p&gt;Instead of treating AI taste as some mystical new skill, focus on the fundamentals that were always important. Here are some actionable ways to develop your taste.&lt;/p&gt;
    &lt;p&gt;Tomorrow: Pick one piece of work you’re proud of and one you’re not. Write down specifically what makes them different. That’s taste in action.&lt;/p&gt;
    &lt;p&gt;This week: Find three examples of excellence in a domain you work in. Study them. What patterns emerge? What choices did the creators make?&lt;/p&gt;
    &lt;p&gt;This month: Take something you’ve created with or without AI and iterate on it a few times. Each iteration should have a specific improvement based on a specific critique.&lt;/p&gt;
    &lt;p&gt;Always: When someone preaches about AI taste, ask them to show you their work from before AI. If they can’t demonstrate taste in their pre-AI work, they’re not qualified to lecture you about it now.&lt;/p&gt;
    &lt;p&gt;The people succeeding with AI aren’t the ones who suddenly discovered taste. They’re the ones who already had it and simply adapted their standards to a new tool. Develop your taste with or without AI. The medium doesn’t matter, the fundamentals do.&lt;/p&gt;
    &lt;p&gt;Stop waiting for AI to force you to develop taste. Start now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45288551</guid><pubDate>Thu, 18 Sep 2025 12:00:44 +0000</pubDate></item></channel></rss>