<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 30 Jan 2026 22:49:24 +0000</lastBuildDate><item><title>How AI assistance impacts the formation of coding skills</title><link>https://www.anthropic.com/research/AI-assistance-coding-skills</link><description>&lt;doc fingerprint="b720e598aba307c3"&gt;
  &lt;main&gt;
    &lt;p&gt;Research shows AI helps people do parts of their job faster. In an observational study of Claude.ai data, we found AI can speed up some tasks by 80%. But does this increased productivity come with trade-offs? Other research shows that when people use AI assistance, they become less engaged with their work and reduce the effort they put into doing it—in other words, they offload their thinking to AI.&lt;/p&gt;
    &lt;p&gt;It’s unclear whether this cognitive offloading can prevent people from growing their skills on the job, or—in the case of coding—understanding the systems they’re building. Our latest study, a randomized controlled trial with software developers as participants, investigates this potential downside of using AI at work.&lt;/p&gt;
    &lt;p&gt;This question has broad implications—for how to design AI products that facilitate learning, for how workplaces should approach AI policies, and for broader societal resilience, among others. We focused on coding, a field where AI tools have rapidly become standard. Here, AI creates a potential tension: as coding grows more automated and speeds up work, humans will still need the skills to catch errors, guide output, and ultimately provide oversight for AI deployed in high-stakes environments. Does AI provide a shortcut to both skill development and increased efficiency? Or do productivity increases from AI assistance undermine skill development?&lt;/p&gt;
    &lt;p&gt;In a randomized controlled trial, we examined 1) how quickly software developers picked up a new skill (in this case, a Python library) with and without AI assistance; and 2) whether using AI made them less likely to understand the code they’d just written.&lt;/p&gt;
    &lt;p&gt;We found that using AI assistance led to a statistically significant decrease in mastery. On a quiz that covered concepts they’d used just a few minutes before, participants in the AI group scored 17% lower than those who coded by hand, or the equivalent of nearly two letter grades. Using AI sped up the task slightly, but this didn’t reach the threshold of statistical significance.&lt;/p&gt;
    &lt;p&gt;Importantly, using AI assistance didn’t guarantee a lower score. How someone used AI influenced how much information they retained. The participants who showed stronger mastery used AI assistance not just to produce code but to build comprehension while doing so—whether by asking follow-up questions, requesting explanations, or posing conceptual questions while coding independently.&lt;/p&gt;
    &lt;head rend="h2"&gt;Study design&lt;/head&gt;
    &lt;p&gt;We recruited 52 (mostly junior) software engineers, each of whom had been using Python at least once a week for over a year. We also made sure they were at least somewhat familiar with AI coding assistance, and were unfamiliar with Trio, the Python library on which our tasks were based.&lt;/p&gt;
    &lt;p&gt;We split the study into three parts: a warm-up; the main task consisting of coding two different features using Trio (which requires understanding concepts related to asynchronous programming, a skill often learned in a professional setting); and a quiz. We told participants that a quiz would follow the task, but encouraged them to work as quickly as possible.&lt;/p&gt;
    &lt;p&gt;We designed the coding task to mimic how someone might learn a new tool through a self-guided tutorial. Each participant was given a problem description, starter code, and a brief explanation of the Trio concepts needed to solve it. We used an online coding platform with an AI assistant in the sidebar which had access to participants’ code and could at any time produce the correct code if asked.1&lt;/p&gt;
    &lt;head rend="h3"&gt;Evaluation design&lt;/head&gt;
    &lt;p&gt;In our evaluation design, we drew on research in computer science education to identify four types of questions commonly used to assess mastery of coding skills:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debugging: The ability to identify and diagnose errors in code. This skill is crucial for detecting when AI-generated code is incorrect and understanding why it fails.&lt;/item&gt;
      &lt;item&gt;Code reading: The ability to read and comprehend what code does. This skill enables humans to understand and verify AI-written code before deployment.&lt;/item&gt;
      &lt;item&gt;Code writing: The ability to write or select the correct approach to writing code. Low-level code writing, like remembering the syntax of functions, will be less important with the further integration of AI coding tools than high-level system design.&lt;/item&gt;
      &lt;item&gt;Conceptual: The ability to understand the core principles behind tools and libraries. Conceptual understanding is critical for assessing whether AI-generated code uses appropriate software design patterns that adhere to how the library is intended to be used.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our assessment focused most heavily on debugging, code reading, and conceptual problems, as we considered these the most important for providing oversight of what is increasingly likely to be AI-generated code.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;On average, participants in the AI group finished about two minutes faster, although the difference was not statistically significant. There was, however, a significant difference in test scores: the AI group averaged 50% on the quiz, compared to 67% in the hand-coding group—or the equivalent of nearly two letter grades (Cohen's d=0.738, p=0.01). The largest gap in scores between the two groups was on debugging questions, suggesting that the ability to understand when code is incorrect and why it fails may be a particular area of concern if AI impedes coding development.&lt;/p&gt;
    &lt;head rend="h3"&gt;Qualitative analysis: AI interaction modes&lt;/head&gt;
    &lt;p&gt;We were particularly interested in understanding how participants went about completing the tasks we designed. In our qualitative analysis, we manually annotated screen recordings to identify how much time participants spent composing queries, what types of questions they asked, the types of errors they made, and how much time they spent actively coding.&lt;/p&gt;
    &lt;p&gt;One surprising result was how much time participants spent interacting with the AI assistant. Several took up to 11 minutes (30% of the total time allotted) composing up to 15 queries. This helped to explain why, on average, participants using AI finished faster although the productivity improvement was not statistically significant. We expect AI would be more likely to significantly increase productivity when used on repetitive or familiar tasks.&lt;/p&gt;
    &lt;p&gt;Unsurprisingly, participants in the No AI group encountered more errors. These included errors in syntax and in Trio concepts, the latter of which mapped directly to topics tested on the evaluation. Our hypothesis is that the participants who encountered more Trio errors (namely, the control group) likely improved their debugging skills through resolving these errors independently.&lt;/p&gt;
    &lt;p&gt;We then grouped participants by how they interacted with AI, identifying distinct patterns that led to different outcomes in completion time and learning.&lt;/p&gt;
    &lt;p&gt;Low-scoring interaction patterns: The low-scoring patterns generally involved a heavy reliance on AI, either through code generation or debugging. The average quiz scores in this group were less than 40%. They showed less independent thinking and more cognitive offloading. We further separated them into:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AI delegation (n=4): Participants in this group wholly relied on AI to write code and complete the task. They completed the task the fastest and encountered few or no errors in the process.&lt;/item&gt;
      &lt;item&gt;Progressive AI reliance (n=4): Participants in this group started by asking one or two questions but eventually delegated all code writing to the AI assistant. They scored poorly on the quiz largely due to not mastering any of the concepts on the second task.&lt;/item&gt;
      &lt;item&gt;Iterative AI debugging (n=4): Participants in this group relied on AI to debug or verify their code. They asked more questions, but relied on the assistant to solve problems, rather than to clarify their own understanding. They scored poorly as a result, and were also slower at completing the two tasks.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;High-scoring interaction patterns: We considered high-scoring quiz patterns to be behaviors where the average quiz score was 65% or higher. Participants in these clusters used AI both for code generation and conceptual queries.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generation-then-comprehension (n=2): Participants in this group first generated code and then manually copied or pasted the code into their work. After their code was generated, they asked the AI assistant follow-up questions to improve understanding. These participants were not particularly fast when using AI, but showed a higher level of understanding on the quiz. Interestingly, this approach looked nearly the same as that of the AI delegation group, except for the fact that they used AI to check their own understanding.&lt;/item&gt;
      &lt;item&gt;Hybrid code-explanation (n=3): Participants in this group composed hybrid queries in which they asked for code generation along with explanations of the generated code. Reading and understanding the explanations they asked for took more time, but helped in their comprehension.&lt;/item&gt;
      &lt;item&gt;Conceptual inquiry (n=7): Participants in this group only asked conceptual questions and relied on their improved understanding to complete the task. Although this group encountered many errors, they also independently resolved them. On average, this mode was the fastest among high-scoring patterns and second fastest overall, after AI delegation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our qualitative analysis does not draw a causal link between interaction patterns and learning outcomes, but it does point to behaviors associated with different learning outcomes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Our results suggest that incorporating AI aggressively into the workplace, particularly with respect to software engineering, comes with trade-offs. The findings highlight that not all AI-reliance is the same: the way we interact with AI while trying to be efficient affects how much we learn. Given time constraints and organizational pressures, junior developers or other professionals may rely on AI to complete tasks as fast as possible at the cost of skill development—and notably the ability to debug issues when something goes wrong.&lt;/p&gt;
    &lt;p&gt;Though preliminary, these results suggest important considerations as companies transition to a greater ratio of AI-written to human-written code. Productivity benefits may come at the cost of skills necessary to validate AI-written code if junior engineers’ skill development has been stunted by using AI in the first place. Managers should think intentionally about how to deploy AI tools at scale, and consider systems or intentional design choices that ensure engineers continue to learn as they work—and are thus able to exercise meaningful oversight over the systems they build.&lt;/p&gt;
    &lt;p&gt;For novice workers in software engineering or any other industry, our study can be viewed as a small piece of evidence toward the value of intentional skill development with AI tools. Cognitive effort—and even getting painfully stuck—is likely important for fostering mastery. This is also a lesson that applies to how individuals choose to work with AI, and which tools they use. Major LLM services also provide learning modes (e.g., Claude Code Learning and Explanatory mode or ChatGPT Study Mode) designed to foster understanding. Knowing how people learn when using AI can also help guide how we design it; AI assistance should enable humans to work more efficiently and develop new skills at the same time.&lt;/p&gt;
    &lt;p&gt;Prior studies have found mixed results on whether AI helps or hinders coding productivity. Our own research found that AI can reduce the time it takes to complete some work tasks by 80%—a result that may seem in tension with the findings presented here. But the two studies ask different questions and use different methods: our earlier observational work measured productivity on tasks where participants already had the relevant skills, while this study examines what happens when people are learning something new. It is possible that AI both accelerates productivity on well-developed skills and hinders the acquisition of new ones, though more research is needed to understand this relationship.&lt;/p&gt;
    &lt;p&gt;This study is only a first step towards uncovering how human-AI collaboration affects the experience of workers. Our sample was relatively small, and our assessment measured comprehension shortly after the coding task. Whether immediate quiz performance predicts longer-term skill development is an important question this study does not resolve. There remain many unanswered questions we hope future studies will investigate, for example: the effects of AI on tasks beyond coding, whether this effect dissipates longitudinally as engineers develop greater fluency, and whether AI assistance differs from human assistance while learning.&lt;/p&gt;
    &lt;p&gt;Ultimately, to accommodate skill development in the presence of AI, we need a more expansive view of the impacts of AI on workers. In an AI-augmented workplace, productivity gains matter, but so does the long-term development of the expertise those gains depend on.&lt;/p&gt;
    &lt;p&gt;Read the full paper for details.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Acknowledgments&lt;/head&gt;
    &lt;p&gt;This project was led by Judy Hanwen Shen and Alex Tamkin. Editorial support for this blog post was provided by Jake Eaton, Stuart Ritchie, and Sarah Pollack.&lt;/p&gt;
    &lt;p&gt;We would like to thank Ethan Perez, Miranda Zhang, and Henry Sleight for making this project possible through the Anthropic Safety Fellows Program. We would also like to thank Matthew Jörke, Juliette Woodrow, Sarah Wu, Elizabeth Childs, Roshni Sahoo, Nate Rush, Julian Michael, and Rose Wang for experimental design feedback.&lt;/p&gt;
    &lt;code&gt;@misc{aiskillformation2026,
  author = {Shen, Judy Hanwen and Tamkin, Alex},
  title = {How AI Impacts Skill Formation},
  year = {2026},
  eprint = {2601.20245},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  eprinttype = {arxiv}
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Importantly, this setup is different from agentic coding products like Claude Code; we expect that the impacts of such programs on skill development are likely to be more pronounced than the results here.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46820924</guid><pubDate>Fri, 30 Jan 2026 05:41:23 +0000</pubDate></item><item><title>Netflix Animation Studios Joins the Blender Development Fund as Corporate Patron</title><link>https://www.blender.org/press/netflix-animation-studios-joins-the-blender-development-fund-as-corporate-patron/</link><description>&lt;doc fingerprint="f8491175711529fd"&gt;
  &lt;main&gt;
    &lt;p&gt;Blender Foundation is thrilled to announce that Netflix Animation Studios is joining the Blender Development Fund as Corporate Patron.&lt;/p&gt;
    &lt;p&gt;This support will be dedicated towards general Blender core development, to continuously improve content creation tools for individuals and teams working in media and entertainment-related workflows.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This membership is a significant acknowledgement of Blender becoming more embedded in high-end animation studios’ workflows. I deeply appreciate this strategic initiative from Netflix Animation Studios as an investment in a diverse, public, and open-source friendly ecosystem of creative tools that will benefit the global community of content creators.&lt;/p&gt;
      &lt;p&gt;Francesco Siddi, CEO at Blender&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Netflix Animation Studios’ corporate membership with Blender reflects our ongoing support for open-source software in the animation community. We are proud to be the first major animation studio to support Blender’s continued development and growing adoption by current and future generations of animation professionals.&lt;/p&gt;
      &lt;p&gt;Darin Grant, SVP Global Technology at Netflix Animation Studios&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;About Netflix&lt;/head&gt;
    &lt;p&gt;Netflix is one of the world’s leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time. Discover more about Netflix Animation Studios at https://www.netflixanimation.com/&lt;/p&gt;
    &lt;head rend="h2"&gt;About Blender&lt;/head&gt;
    &lt;p&gt;Blender, the world’s most popular free and open-source 3D creation software, offers a comprehensive solution for modelling, animation, VFX, and more. Maintained by the Blender Foundation, it’s the tool of choice for a vast global community of professional artists and enthusiasts, committed to open collaboration and 3D technology innovation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46821134</guid><pubDate>Fri, 30 Jan 2026 06:19:36 +0000</pubDate></item><item><title>Photoroom (YC S20) Is Hiring a Head of Cross-Platform (Rust) in Paris</title><link>https://jobs.ashbyhq.com/photoroom/dc994a7c-e104-46e1-81c3-b88d635398b9</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46821326</guid><pubDate>Fri, 30 Jan 2026 07:00:08 +0000</pubDate></item><item><title>BoldVoice (YC S21) Is Hiring Fullstack and Machine Learning Engineers</title><link>https://boldvoice.notion.site/careers-page?p=2e871a9bf729806c81f6e47f32e32622&amp;pm=s</link><description>&lt;doc fingerprint="10f452a104a33a8"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript must be enabled in order to use Notion. Please enable JavaScript to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823430</guid><pubDate>Fri, 30 Jan 2026 12:00:12 +0000</pubDate></item><item><title>Email experiments: filtering out external images</title><link>https://www.terracrypt.net/posts/email-experiments-image-filtering.html</link><description>&lt;doc fingerprint="679d9d1a3d50dfad"&gt;
  &lt;main&gt;
    &lt;p&gt;I had a realization the other day that, on almost every email in my inbox, my mail client has a "show external images" option. Most email I receive references externally loaded images via HTML. There are good reasons for this (not needing to send a copy of an image in every email on a newsletter) as well as nefarious reasons (the remote server can track where/when you load the image). So most mail clients I've used don't load them by default for obvious privacy reasons.&lt;/p&gt;
    &lt;p&gt;This got me thinking about the reverse, though: what email do I receive that does not include external images? And the answer is, mostly, email sent manually by a real human! I'm fairly certain I've never sent an email to another person in my personal life with an externally loaded image in it. When I have, it's been work email with a standard corporate email signature (that I'm sure was being tracked, natch). Mostly, when sending images to someone, they're sent as attachments to the email.&lt;/p&gt;
    &lt;p&gt;So I had a realization that, if I wanted to naturally filter email that was sent by hand from email sent from an automated system, this might be a decent proxy for that. Here's the sieve rule I landed on for now:&lt;/p&gt;
    &lt;code&gt;if body :regex "&amp;lt;img[^&amp;gt;]*src=\"https" {
  fileinto "Inbox.Automated";
}&lt;/code&gt;
    &lt;p&gt;(Yes, it's typically folly to regex on HTML. This is a simple enough match though that I hope it's fine!)&lt;/p&gt;
    &lt;p&gt;So far, after a day or so of usage, it's been shockingly effective. There's enough potentially important stuff in my Automated folder that I will need to check that fairly frequently, but the one email that's made it through this filter into my normal inbox is one sent by hand to a mailman list. Honestly, I'll take it! This makes the volume of email in my normal inbox much much more manageable, and I expect most mail that lands there will be things I actually do want to pay attention to.&lt;/p&gt;
    &lt;p&gt;Probably you'll want to have another rule that prevents you from running this for email from people in your contacts. I haven't done that yet, but that'll be a natural next step.&lt;/p&gt;
    &lt;p&gt;This is part of my December Adventure 2025 series.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823445</guid><pubDate>Fri, 30 Jan 2026 12:01:36 +0000</pubDate></item><item><title>Code is cheap. Show me the talk</title><link>https://nadh.in/blog/code-is-cheap/</link><description>&lt;doc fingerprint="cf52105ea8a337bd"&gt;
  &lt;main&gt;
    &lt;p&gt;30 January 2026&lt;/p&gt;
    &lt;head rend="h1"&gt;Code is cheap. Show me the talk.&lt;/head&gt;
    &lt;p&gt;TLDR; Software development, as it has been done for decades, is over. LLM coding tools have changed it fundamentally for the better or worse.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Talk is cheap. Show me the code.” — Linus Torvalds, August 2000&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When Linus Torvalds, the creator of Linux, made this quip in response to a claim about a complex piece of programming in the Linux kernel, [1] I was an oblivious, gangly, fledgling teenage n00b coder learning by copy-pasting open source Perl and VB snippets over dialup internet.&lt;/p&gt;
    &lt;p&gt;The quip has since become an adage in the software world. The gist of it back then was that, it was easy to talk about all the software stuff one would like to do, or could be hypothetically done, but unless one actually put in the effort and proved it, talk wasn’t of much value. Writing and proving good software was a high-effort, high-cost, high-skill endeavour.&lt;/p&gt;
    &lt;p&gt;Even when armed with a crystal clear software development plan and the exact know-how to implement it, any sufficiently complex piece of programming is high-effort, tedious, and time consuming to actually write and get to a form where it is functional, reliable, and at least reasonably future-ready. In the process of developing software, any number of unforeseen complexities and gotchas can arise with many unresolvable trade-offs,[2] both technical and external. It is not uncommon for software architectures to change mid-way multiple times. The cost of just trying things out is so exponentially high that the significant majority of ideas are simply never tried out.&lt;/p&gt;
    &lt;p&gt;After all, the real bottleneck is good old physical and biological human constraints—cognitive bandwidth, personal time and resources, and most importantly, the biological cost and constraints of having to sit for indefinite periods, writing code with one’s own hands line by line even if it is all in one’s head, while juggling and context-switching through the mental map of large systems. And if it is more than one individual, a whole host of interpersonal coordination and communication dynamics come into play. It is thus very difficult to prototype and try out not just grand ideas, but even reasonably simple ones. As many of us have done, most ideas are generally appended to a bottomless wishlist where they very likely stay forever. That’s how I have programmed and written software on a regular basis and enjoyed it—from hobby stuff to critical systems that millions of people depend on—for about 25 years.&lt;/p&gt;
    &lt;p&gt;All that has now been thrown out of the window, of course, for better or worse.&lt;/p&gt;
    &lt;p&gt;Coming back to Linus, fast-forward 25 years, when he merges a chunk of AI-generated code into his toy project and comments “Is this much better than I could do by hand? Sure is.”, [3] I, no longer the fledgling n00b, but someone with decades of software development scars and calluses (both physical and metaphorical), am able to grasp its implications. Not only that, now with a sizeable amount of first-hand experience with LLM-assisted coding, I am compelled to say, software development, as it has been done for decades, is over. Along with that, many other things are too.&lt;/p&gt;
    &lt;p&gt;I say that with the full awareness that it smacks of Fukuyama’s The End of History, [4] but I will reiterate:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Software development, as it has been done for decades, is over.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;I&lt;/head&gt;
    &lt;p&gt;I was lucky to be in the transitionary Goldilocks era to witness and to partake in the breakneck evolution of the internet and software landscape—dialup to DSL to gigabit; Basic, Visual Basic 4/5/6 and Delphi; rise and fall of cgi-bin; Altavista to Google; XMLHttpRequest kicking off Web 2.0; rise and fall of Flash; death of IE and the rise of Chrome; WAP to Symbian to Android and smartphone apps; the demise of SourceForge and the massive proliferation and success of FOSS (Free and Open-source Software); git and GitHub; rise of SaaS; ExpertsExchange to StackOverflow; the growth of the Linux world; sysadmin to devops to whateverOps; the ominous birthing of Node.js and MongoDB in the same year; microservices; the explosion of VC-funded software “unicorns”; crypto and web3 shams; the rapid darkening of patterns; widespread enshittification and monetisation of privacy, attention, and dignity; and the monumental bloating of software that has since become the norm.&lt;/p&gt;
    &lt;p&gt;All throughout this, I have been writing, maintaining, and deploying software both as a professional developer and as a FOSS hobbyist dabbling in a gazillion languages, frameworks, tools, and methodologies. From thinking that “indenting code is lame” (cringe) as a teen, from copy-pasting to CVS to svn to git, fighting space vs. tab battles, to maturing to “whatever floats your boat” and still regularly compressing PNGs to shave off a few KBs, I have been a dabbler, dilettante, and an addict, someone who has unconditionally enjoyed writing code and developing software.&lt;/p&gt;
    &lt;p&gt;But now? How I develop software now is not how I have done it all these years, all the right, wrong, good, bad, easy and hard bits combined. With the advent of code-assisting LLMs, it has been completely flipped on its head, and I don’t think there is any going back.&lt;/p&gt;
    &lt;p&gt;Now, that is some “Tears in rain”-esque [5] monologue right there.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code&lt;/head&gt;
    &lt;p&gt;Barring a bunch of obvious objective 101s, there is no universal measure of what makes a codebase good or great. Styles, idioms, patterns, architectures all vary greatly. Even objectively provable technical choices are subject to trade-offs that defy consensus. For a software developer like me, historically, there have been a few rule-of-thumb indicators for quick evaluation of software. When I evaluate a FOSS project, I look at a bunch of factors, all a mix of objective and subjective, weighted differently under different contexts—the project’s age; is the commit activity overly sparse or frantic; frameworks and dependencies; is code consistently organised and commented without being over-abstracted; is there a community around it; are maintainers responsive; can I actually get it up and running quickly from a clear README; the quality and depth of its documentation …&lt;/p&gt;
    &lt;p&gt;Many of these rule-of-thumb signals give a reasonable glimpse of the mental model and the style of working of the maintainers and the likely future trajectory of the project. For example, concise comments, README, and documentation indicate thoughtfulness, extra effort, and empathy for other developers (and self). Mainly because, for mortal developers like me, documentation and tests are a necessity, but unpleasant, boring, and tedious things to write and maintain.&lt;/p&gt;
    &lt;p&gt;Well, those notions have now been abruptly and violently defenestrated by LLMs. They can now one-shot generate stunning looking documentation pages, dense (ironically, pedantically detailed) READMEs, build great looking user interfaces, neatly organise code with proper idioms, patterns, and comments. One can no longer know whether such a repository was “vibe” coded by a non-technical person who has never written a single line of code, or an experienced developer, who may or may not have used LLM assistance. These no longer indicate the quality of a codebase. On the contrary, the more stunning or perfect looking something is, the more suspicious it is now—was it low-effort, one-shot vibe coded?&lt;/p&gt;
    &lt;p&gt;With the tell-tale, rule-of-thumb measures of code and software quality being outright dead, without a much closer inspection and a bit of expert forensic analysis, it is now difficult to tell the wheat from the “slop”. One is now slowly being compelled to also look much more closely at the provenance of software—the who, why, their track record, and plans of governance.&lt;/p&gt;
    &lt;head rend="h2"&gt;Effort&lt;/head&gt;
    &lt;p&gt;Historically, it would take a reasonably long period of consistent effort and many iterations of refinement for a good developer to produce 10,000 lines of quality code that not only delivered meaningful results, but was easily readable and maintainable. While the number of lines of code is not a measure of code quality—it is often the inverse—a codebase with good quality 10,000 lines of code indicated significant time, effort, focus, patience, expertise, and often, skills like project management that went into it. Human traits.&lt;/p&gt;
    &lt;p&gt;Now, LLMs can not only one-shot generate that in seconds, they can handle many technical aspects of the software development workflow, from testing to sysadmin to publishing. Unlike the unpredictable outcomes of frenzied vibe coding, when steered with human expertise, the output can be high quality and highly effective.[6] This has been my personal experience as well. On a regular basis, I have been compressing work that would have taken me weeks and months to mere days and even hours. That too, without vibe coding, an AGENT.md file, or any fancy multi-agent workflows or orchestration. Just an LLM agent CLI at arm’s length.&lt;/p&gt;
    &lt;p&gt;As a developer with a bottomless wishlist of things I wish I could have done or tried out, I have been able to use LLM tools to not just rapidly prototype and validate complex ideas, but actually write good quality production-grade software (my own subjective metric, of course) with better code than I could have written manually—things where I knew exactly what I had to do, but was constrained by physical limits, and also things that were unclear to me and needed novel ideas, approaches, and leaps. All the while, learning and bettering my own understanding of things.&lt;/p&gt;
    &lt;p&gt;The physiological, cognitive, and emotional cost I generally incur to achieve the software outcomes I want or am capable of engineering, has undoubtedly reduced by several orders of magnitude. The time and bandwidth this has freed up, I now spend on engineering, architecting, debating, tinkering, trying to expand my imagination, and writing much more concise and meaningful code that I actually want to write.&lt;/p&gt;
    &lt;p&gt;Remember the old adage, “programming is 90% thinking and 10% typing”? It is now, for real.&lt;/p&gt;
    &lt;head rend="h2"&gt;Slop&lt;/head&gt;
    &lt;p&gt;Given all that, what is the value of code as an artefact, when it can be generated at an industrial scale within seconds by someone who has never written any code? Barring obviously bad LLM-generated code, when code is neatly structured and functional (yes, LLMs can write good code when steered competently), what makes it valuable or not? We wouldn’t want LLM-generated code in systems out there in the real world, but would instead prefer pure unadulterated human code, yes? Well, that would be a wonderful joke.[7] [8] [9] [10] [11]&lt;/p&gt;
    &lt;p&gt;The reality is that the significant majority of the code written by humans globally on a daily basis, is likely borderline junk.[12] Software development is not even a discipline that has reached any objective level of maturity. Medical doctors and civil engineers go through rigorous training to be issued licenses that are contingent on real world ramifications of their work. How about software developers and engineers? The world runs on shoddily engineered, poorly cobbled together, bloated systems with garbage code that humans have written, mostly directed by people in positions of power with perverse incentives who have absolutely no technical know-how or have any grounding in the humanities—the tyranny of non-tech “tech leaders”.[13]&lt;/p&gt;
    &lt;p&gt;One could, to trigger emotions, argue that AI slop is at least neatly formatted, well documented, and more syntactically consistent than the vast majority of human-written code. ( ͡° ͜ʖ ͡°)&lt;/p&gt;
    &lt;p&gt;Kidding aside, I am no fan of AI slop. Reading those obvious soulless LLM-generated messages and articles on the (dead) internet[14] is a waste of neuronal activation in the amygdala, if there is any activation at all. That so many people across the world LLM-speak and emote in the exact same manner on the internet, is creepy self-Pluribus-ification.[15] Without human creation, perfection and flaws, language, literature, art, music etc. are unenjoyable (to most). Infinite, instantly-generatable stuff without human constraints and limits, is actually very difficult to value.&lt;/p&gt;
    &lt;p&gt;As is code, then? Well, code is a bit different from art, literature, or any form of direct communication and evocation. Code was always a means to an end. Unlike poetry or prose, end users don’t read or care about code. They don’t care what language or framework or the architecture the hundred systems running behind a portal are made of. Code is hidden. They interact with the effect and outcomes of code through various forms of UX. I say that, slightly begrudgingly, as someone who enjoys writing, organising, and even nurturing code. For those who are immersed in it, there is an element of creativity and art in it, and many like me, are borderline curmudgeons on all things software.[16]&lt;/p&gt;
    &lt;p&gt;Ignoring outright bad code, in a world where functional code is so abundant that “good” and “bad” are indistinguishable, ultimately, what makes functional AI code slop or non-slop? I am strongly inclined to think that it is the framework of accountability, and ironically, the element of humanness. That is, all things (code) being equal, the ability to hold someone accountable at least emotionally and morally (and sometimes legally), for an artefact, instills value.&lt;/p&gt;
    &lt;p&gt;When one gets that big pull request (PR) on an open source repository, irrespective of its quality, if it is handwritten by a human, there is an intrinsic value and empathy for the human time and effort that is likely ascribed to it. It is known that there is a physical and cognitive cost that has been paid writing a lot of code before raising a PR. That is what makes that code “expensive” and not cheap.&lt;/p&gt;
    &lt;p&gt;When a PR is obviously LLM-generated, irrespective of how good it is, the first reaction is likely to be “slop!”, because it is no longer possible to instantly ascertain the human effort behind it. On the other hand, the effort required to read and validate it is disproportionately and exponentially high—setting aside people who have also offloaded reading of code to LLMs. It may very well be the best possible functional code, but it is one out of an infinite possible variation that could have been generated with no human cost or effort. Emotionally, it feels wrong and unfair to be burdened by such code dumps.&lt;/p&gt;
    &lt;p&gt;And, at that point, our reality has become a version of Borges’ Library of Babel.[17]&lt;/p&gt;
    &lt;head rend="h2"&gt;FOSS&lt;/head&gt;
    &lt;p&gt;Speaking of libraries, FOSS is perhaps the greatest public commons that humanity has created. The genesis of FOSS and its predecessors, various schemes for sharing code, can be traced to the fundamental premise that software was prohibitively expensive and required immense specialist skills to create. Only a tiny handful of people in the world had the ability to do that, and everyone else was naturally forced to use the creations of the few (proprietary or not). While the global developer ecosystem has exploded since then, the ratio of makers to users has largely remained the same. Largescale FOSS collaboration and community dynamics all stem from that—codebases as valuable shared artefacts.&lt;/p&gt;
    &lt;p&gt;What happens in a world where code is cheap and small to medium-sized software libraries and modules can be quickly created by an expert, perfectly customised and attuned to their needs, no matter how niche? Forget expertise, a world where anyone reasonably savvy can vibe code the small things they need for their private use, however they please. I see this happening everywhere. What is happening to StackOverflow[18] is also happening to software, although not as dramatically. This seems to strike at the very heart of the human dynamics, societal conditions, and incentives that drive FOSS collaboration and sharing. Add to that, if one considers the impending Cambrian explosion of FOSS projects manufactured at an unprecedented scale, the high-quality FOSS projects that remain and thrive, expert governance, curation, and trust are likely to become more valuable than the code itself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Missing the forest for the trees&lt;/head&gt;
    &lt;p&gt;Humans have produced amazing software when there was no syntax highlighting, IDEs, or any kind of tooling. And humans also produce trash despite all the tooling and resources in the world. A good competent developer with good articulation skills and care for quality will use LLMs, or any other tools, in their own ways to produce quality outcomes. An incompetent developer with poor articulation skills or one with a lack of care for quality, will produce bad stuff, LLMs or not.&lt;/p&gt;
    &lt;p&gt;Thus, the extreme proponents of manic “agentic” vibe coding,[19] and the outright denouncers of LLMs, are both missing the forest for the trees. That there is a pragmatic middle path, where people who have the experience, expertise, competence, and the ability to articulate can use these tools to get the outcomes they desire with the right sets of trade-offs.&lt;/p&gt;
    &lt;p&gt;Vibe coding has its place, especially for non-technical people, who, for the first time, can tinker, explore, have fun, and empower themselves with software. I see this happening all around me. However, the fanatical acolytes of vibe coding are missing a very important thing that makes humans take artefacts seriously—finitude. They’re generating a vast Borgesian library where they themselves are likely to be lost in an ocean of slop generated by sycophantic agents. Slop, not because the code is of poor quality, but because anything that can be generated infinitely without effort and has no meaningful provenance, is very hard to value or take seriously. Humans fundamentally do not deal well with an infinite supply of anything, especially choices. Completely unsurprising because we are heavily constrained biological beings that have evolved on a finite planet with finite resources to live out finite lifetimes.&lt;/p&gt;
    &lt;p&gt;And then, the denouncers, they can’t seem to get past the argument from incredulity.[20] They denounce LLMs because they don’t personally like them for whatever reason, or have been unable to get desirable outcomes, or had the wrong expectations about them, or have simply gotten sick of them. But that is immaterial because there is a sizeable population who are using the exact same tools fruitfully and have the opposite experience. I am one of them.&lt;/p&gt;
    &lt;p&gt;All that said, the widespread braindead and outright stupid and harmful implementations of these technologies fuelled by hype, frenzy, and greed are an unfortunate reality and a massive cause of concern. The AI-business bubble is perhaps one of the biggest in history. The rise of FOSS AI technologies makes one hopeful. However, to incorrectly conflate bad actors, bad actions, bean-counting, and nonsensical implementations with fundamental, physical capabilities of these technologies—not theoretical, but the regular, proven, and practical—is irrational. It is missing the forest for the trees.&lt;/p&gt;
    &lt;head rend="h2"&gt;The human cost&lt;/head&gt;
    &lt;p&gt;All of this has been from the perspective of an experienced developer and engineer. For someone who has been weathered and bruised enough, these AI technologies provide extremely effective and powerful assistance.&lt;/p&gt;
    &lt;p&gt;But what about the young folks who are just starting out? If one does not have their fundamentals in place, if one has not developed an innate and nuanced understanding of systems and the process of software development, then these technologies are unreliable, dangerous genies. One asks for code, it gives code. One asks for changes, it gives changes. Soon, one is stuck with a codebase whose workings one doesn’t understand, and one is forced to go back to the genie and depend on it helplessly. And because one is hooked on and dependent on the genie, the natural circumstances that otherwise would allow for foundational and fundamental skills and understanding to develop, never arise, to the point of cognitive decline.[21] What then happens to an entire generation of juniors, who never get an opportunity to become seniors meaningfully?&lt;/p&gt;
    &lt;p&gt;Personally, I don’t care about the extreme vibe coders or denouncers or even slop. We are all going to drown in a deluge of slop, from which, many islands of sanity, recovery, and a new order of software will emerge. The real concern is for generations of learners who are being robbed of the opportunity to acquire the expertise to objectively discern what is slop and what is not. Even worse, the possibility that experienced folks who use these tools effectively, will feel disincentivised from mentoring and training junior folks in foundational ways, something that was a natural part of societal evolution. And not just with software development, but the wholesale offloading of agency and decision-making to black boxes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Talk&lt;/head&gt;
    &lt;p&gt;At this point, for a hands-on developer, reading and critically evaluating code have become more important than learning syntax and typing it out line by line. Of course, that is still an important skill, because the ability to read code effectively comes from that in the first place. But, the daily software development workflows have flipped over completely.&lt;/p&gt;
    &lt;p&gt;An experienced developer who can talk well, that is, imagine, articulate, define problem statements, architect and engineer, has a massive advantage over someone who cannot, more disproportionately than ever. Knowledge of specific language, syntax, and frameworks—code—is no longer a bottleneck. The physiological constraints of yore are no longer impediments. The machinery for instantly creating code at scale is now a commodity and available to everyone, just a &lt;code&gt;pip install&lt;/code&gt; equivalent away. It requires no special training, no new language or framework to learn, and has practically no entry barriers—just good old critical thinking and foundational human skills, and competence to run the machinery.&lt;/p&gt;
    &lt;p&gt;Conventional software development methodologies and roles—Waterfall[22] to Agile,[23] developer to tester, senior to junior—have fundamentally changed with traditional boundaries consolidating into unimaginably fast, compressed, blurry, iterative “agentic” loops. The dynamics of people, organisations, and public communities in software development, the very human incentives for sharing and collaboration,[24] [25] [26] are all changing.&lt;/p&gt;
    &lt;p&gt;For the first time ever, good talk is exponentially more valuable than good code. The ramifications of this are significant and disruptive. This time, it is different.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823485</guid><pubDate>Fri, 30 Jan 2026 12:05:50 +0000</pubDate></item><item><title>Pangolin (YC S25) is hiring software engineers (open-source, Go, networking)</title><link>https://docs.pangolin.net/careers/join-us</link><description>&lt;doc fingerprint="578d4934f0eed0f"&gt;
  &lt;main&gt;
    &lt;div&gt;Skip to main content&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;We are looking for talented engineers to join our team and help build secure remote access. If you’re passionate about open-source software, networking, and security, we’d love to hear from you.&lt;head rend="h2"&gt;About Pangolin&lt;/head&gt; Pangolin delivers identity-aware remote access to internal apps and services. Our platform replaces legacy VPNs and simplifies secure access to infrastructure, applications, and developer environments. We build in the open and are self‑hosted by default so teams retain control over data and infrastructure. The system is policy‑driven, integrates with standard IdPs, exposes clear observability and health, and provides an API for automation. If you’re interested in open-source auth and networking infrastructure, we’d love to chat. &lt;head rend="h2"&gt;Open Roles&lt;/head&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823544</guid><pubDate>Fri, 30 Jan 2026 12:11:49 +0000</pubDate></item><item><title>HTTP Cats</title><link>https://http.cat/</link><description>&lt;doc fingerprint="edf6e74fda190558"&gt;
  &lt;main&gt;
    &lt;p&gt;https://http.cat/[status_code]&lt;/p&gt;
    &lt;p&gt;Note: If you need an extension at the end of the URL just add .jpg.&lt;/p&gt;
    &lt;p&gt;.jpg&lt;/p&gt;
    &lt;p&gt;Continue&lt;/p&gt;
    &lt;p&gt;Switching Protocols&lt;/p&gt;
    &lt;p&gt;Processing&lt;/p&gt;
    &lt;p&gt;Early Hints&lt;/p&gt;
    &lt;p&gt;OK&lt;/p&gt;
    &lt;p&gt;Created&lt;/p&gt;
    &lt;p&gt;Accepted&lt;/p&gt;
    &lt;p&gt;Non-Authoritative Information&lt;/p&gt;
    &lt;p&gt;No Content&lt;/p&gt;
    &lt;p&gt;Reset Content&lt;/p&gt;
    &lt;p&gt;Partial Content&lt;/p&gt;
    &lt;p&gt;Multi-Status&lt;/p&gt;
    &lt;p&gt;Already Reported&lt;/p&gt;
    &lt;p&gt;Transformation Applied&lt;/p&gt;
    &lt;p&gt;IM Used&lt;/p&gt;
    &lt;p&gt;Multiple Choices&lt;/p&gt;
    &lt;p&gt;Moved Permanently&lt;/p&gt;
    &lt;p&gt;Found&lt;/p&gt;
    &lt;p&gt;See Other&lt;/p&gt;
    &lt;p&gt;Not Modified&lt;/p&gt;
    &lt;p&gt;Use Proxy&lt;/p&gt;
    &lt;p&gt;Temporary Redirect&lt;/p&gt;
    &lt;p&gt;Permanent Redirect&lt;/p&gt;
    &lt;p&gt;Bad Request&lt;/p&gt;
    &lt;p&gt;Unauthorized&lt;/p&gt;
    &lt;p&gt;Payment Required&lt;/p&gt;
    &lt;p&gt;Forbidden&lt;/p&gt;
    &lt;p&gt;Not Found&lt;/p&gt;
    &lt;p&gt;Method Not Allowed&lt;/p&gt;
    &lt;p&gt;Not Acceptable&lt;/p&gt;
    &lt;p&gt;Proxy Authentication Required&lt;/p&gt;
    &lt;p&gt;Request Timeout&lt;/p&gt;
    &lt;p&gt;Conflict&lt;/p&gt;
    &lt;p&gt;Gone&lt;/p&gt;
    &lt;p&gt;Length Required&lt;/p&gt;
    &lt;p&gt;Precondition Failed&lt;/p&gt;
    &lt;p&gt;Payload Too Large&lt;/p&gt;
    &lt;p&gt;Request-URI Too Long&lt;/p&gt;
    &lt;p&gt;Unsupported Media Type&lt;/p&gt;
    &lt;p&gt;Request Range Not Satisfiable&lt;/p&gt;
    &lt;p&gt;Expectation Failed&lt;/p&gt;
    &lt;p&gt;Iâm a teapot&lt;/p&gt;
    &lt;p&gt;Page Expired&lt;/p&gt;
    &lt;p&gt;Enhance Your Calm&lt;/p&gt;
    &lt;p&gt;Misdirected Request&lt;/p&gt;
    &lt;p&gt;Unprocessable Entity&lt;/p&gt;
    &lt;p&gt;Locked&lt;/p&gt;
    &lt;p&gt;Failed Dependency&lt;/p&gt;
    &lt;p&gt;Too Early&lt;/p&gt;
    &lt;p&gt;Upgrade Required&lt;/p&gt;
    &lt;p&gt;Precondition Required&lt;/p&gt;
    &lt;p&gt;Too Many Requests&lt;/p&gt;
    &lt;p&gt;Request Header Fields Too Large&lt;/p&gt;
    &lt;p&gt;No Response&lt;/p&gt;
    &lt;p&gt;Blocked by Windows Parental Controls&lt;/p&gt;
    &lt;p&gt;Unavailable For Legal Reasons&lt;/p&gt;
    &lt;p&gt;SSL Certificate Error&lt;/p&gt;
    &lt;p&gt;SSL Certificate Required&lt;/p&gt;
    &lt;p&gt;HTTP Request Sent to HTTPS Port&lt;/p&gt;
    &lt;p&gt;Token expired/invalid&lt;/p&gt;
    &lt;p&gt;Client Closed Request&lt;/p&gt;
    &lt;p&gt;Internal Server Error&lt;/p&gt;
    &lt;p&gt;Not Implemented&lt;/p&gt;
    &lt;p&gt;Bad Gateway&lt;/p&gt;
    &lt;p&gt;Service Unavailable&lt;/p&gt;
    &lt;p&gt;Gateway Timeout&lt;/p&gt;
    &lt;p&gt;Variant Also Negotiates&lt;/p&gt;
    &lt;p&gt;Insufficient Storage&lt;/p&gt;
    &lt;p&gt;Loop Detected&lt;/p&gt;
    &lt;p&gt;Bandwidth Limit Exceeded&lt;/p&gt;
    &lt;p&gt;Not Extended&lt;/p&gt;
    &lt;p&gt;Network Authentication Required&lt;/p&gt;
    &lt;p&gt;Web Server Is Down&lt;/p&gt;
    &lt;p&gt;Connection Timed Out&lt;/p&gt;
    &lt;p&gt;Origin Is Unreachable&lt;/p&gt;
    &lt;p&gt;SSL Handshake Failed&lt;/p&gt;
    &lt;p&gt;Site Frozen&lt;/p&gt;
    &lt;p&gt;Network Connect Timeout Error&lt;/p&gt;
    &lt;p&gt;Developed by @rogeriopvl&lt;/p&gt;
    &lt;p&gt;Original Images by Tomomi Imura (@girlie_mac)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46824422</guid><pubDate>Fri, 30 Jan 2026 13:56:51 +0000</pubDate></item><item><title>Show HN: Amla Sandbox – WASM bash shell sandbox for AI agents</title><link>https://github.com/amlalabs/amla-sandbox</link><description>&lt;doc fingerprint="a5369d909929b8a8"&gt;
  &lt;main&gt;
    &lt;p&gt;Every popular agent framework runs LLM-generated code via &lt;code&gt;subprocess&lt;/code&gt; or &lt;code&gt;exec()&lt;/code&gt;. That's arbitrary code execution on your host. One prompt injection and you're done.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Framework&lt;/cell&gt;
        &lt;cell role="head"&gt;Execution Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LangChain&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;exec(command, globals, locals)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;CVE-2025-68664, GitHub #5294&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AutoGen&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;subprocess.run()&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Code Executors docs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SWE-Agent&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;subprocess.run(["bash", ...])&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;SWE-ReX&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Some frameworks offer Docker isolation (OpenHands, AutoGen), but that requires running a Docker daemon and managing container infrastructure.&lt;/p&gt;
    &lt;p&gt;amla-sandbox is a WASM sandbox with capability enforcement. Agents can only call tools you explicitly provide, with constraints you define. Sandboxed virtual filesystem. No network. No shell escape.&lt;/p&gt;
    &lt;code&gt;uv pip install "git+https://github.com/amlalabs/amla-sandbox"&lt;/code&gt;
    &lt;p&gt;No Docker. No VM. One binary, works everywhere.&lt;/p&gt;
    &lt;code&gt;from amla_sandbox import create_sandbox_tool

sandbox = create_sandbox_tool(tools=[stripe_api, database])

# Agent writes one script instead of 10 tool calls (JavaScript)
result = sandbox.run('''
    const txns = await stripe.listTransactions({customer: "cus_123"});
    const disputed = txns.filter(t =&amp;gt; t.disputed);
    console.log(disputed[0]);
''', language="javascript")

# Or with shell pipelines
result = sandbox.run('''
    tool stripe.listTransactions --customer cus_123 | jq '[.[] | select(.disputed)] | .[0]'
''', language="shell")&lt;/code&gt;
    &lt;p&gt;Tool-calling is expensive. Every MCP call is a round trip through the model:&lt;/p&gt;
    &lt;code&gt;LLM → tool → LLM → tool → LLM → tool → ...
&lt;/code&gt;
    &lt;p&gt;Ten tool calls = ten LLM invocations. Code mode collapses this:&lt;/p&gt;
    &lt;code&gt;LLM → script that does all 10 things → result
&lt;/code&gt;
    &lt;p&gt;But you can't just eval whatever the model spits out. So people either pay the token tax or run unsafe code. This gives you both: code-mode efficiency with actual isolation.&lt;/p&gt;
    &lt;p&gt;The sandbox runs inside WebAssembly with WASI for a minimal syscall interface. WASM provides memory isolation by design—linear memory is bounds-checked, and there's no way to escape to the host address space. The wasmtime runtime we use is built with defense-in-depth and has been formally verified for memory safety.&lt;/p&gt;
    &lt;p&gt;On top of WASM isolation, every tool call goes through capability validation:&lt;/p&gt;
    &lt;code&gt;from amla_sandbox import Sandbox, MethodCapability, ConstraintSet, Param

sandbox = Sandbox(
    capabilities=[
        MethodCapability(
            method_pattern="stripe/charges/*",
            constraints=ConstraintSet([
                Param("amount") &amp;lt;= 10000,
                Param("currency").is_in(["USD", "EUR"]),
            ]),
            max_calls=100,
        ),
    ],
    tool_handler=my_handler,
)

# This works
sandbox.execute('await stripe.charges.create({amount: 500, currency: "USD"})')

# This fails - amount exceeds capability
sandbox.execute('await stripe.charges.create({amount: 50000, currency: "USD"})')&lt;/code&gt;
    &lt;p&gt;The design draws from capability-based security as implemented in systems like seL4—access is explicitly granted, not implicitly available. Agents don't get ambient authority just because they're running in your process. This matters because prompt injection is a fundamental unsolved problem; defense in depth through capability restriction limits the blast radius.&lt;/p&gt;
    &lt;code&gt;from amla_sandbox import create_sandbox_tool

sandbox = create_sandbox_tool()

# JavaScript
sandbox.run("console.log('hello'.toUpperCase())", language="javascript")  # -&amp;gt; "HELLO"

# Shell
sandbox.run("echo 'hello' | tr 'a-z' 'A-Z'", language="shell")  # -&amp;gt; "HELLO"

# With tools
def get_weather(city: str) -&amp;gt; dict:
    return {"city": city, "temp": 72}

sandbox = create_sandbox_tool(tools=[get_weather])
sandbox.run("const w = await get_weather({city: 'SF'}); console.log(w);", language="javascript")&lt;/code&gt;
    &lt;p&gt;With constraints:&lt;/p&gt;
    &lt;code&gt;sandbox = create_sandbox_tool(
    tools=[transfer_money],
    constraints={
        "transfer_money": {
            "amount": "&amp;lt;=1000",
            "currency": ["USD", "EUR"],
        },
    },
    max_calls={"transfer_money": 10},
)&lt;/code&gt;
    &lt;p&gt;Tools require object syntax:&lt;/p&gt;
    &lt;code&gt;// WORKS - tools always take an object argument
await get_weather({city: "SF"});
await transfer({to: "alice", amount: 500});

// FAILS - positional arguments don't work
await get_weather("SF");  // Error: argument after ** must be a mapping&lt;/code&gt;
    &lt;p&gt;Use &lt;code&gt;return&lt;/code&gt; or &lt;code&gt;console.log()&lt;/code&gt; for output:&lt;/p&gt;
    &lt;code&gt;// Return value is captured and output
return await get_weather({city: "SF"});  // -&amp;gt; {"city":"SF","temp":72}
return {a: 1, b: 2};  // -&amp;gt; {"a":1,"b":2}
return "hello";  // -&amp;gt; hello (strings not double-quoted)

// console.log also works
console.log(JSON.stringify({a: 1}));  // -&amp;gt; {"a":1}

// No return = no output
const x = 42;  // -&amp;gt; (no output)&lt;/code&gt;
    &lt;p&gt;VFS is writable only under /workspace and /tmp:&lt;/p&gt;
    &lt;code&gt;// WORKS - /workspace and /tmp are ReadWrite
await fs.writeFile('/workspace/data.json', '{}');
await fs.mkdir('/tmp/cache');

// FAILS - root is read-only
await fs.mkdir('/mydir');  // EACCES: Permission denied&lt;/code&gt;
    &lt;p&gt;For LangGraph integration:&lt;/p&gt;
    &lt;code&gt;from langgraph.prebuilt import create_react_agent
from langchain_anthropic import ChatAnthropic
from amla_sandbox import create_sandbox_tool

sandbox = create_sandbox_tool(tools=[get_weather, search_db])
agent = create_react_agent(
    ChatAnthropic(model="claude-sonnet-4-20250514"),
    [sandbox.as_langchain_tool()]  # LLM writes JS/shell that calls your tools
)&lt;/code&gt;
    &lt;p&gt;For fine-grained capability control:&lt;/p&gt;
    &lt;code&gt;from amla_sandbox import SandboxTool, MethodCapability, ConstraintSet, Param

caps = [
    MethodCapability(
        method_pattern="mcp:search_db",
        constraints=ConstraintSet([Param("query").starts_with("SELECT")]),
        max_calls=5,
    )
]

sandbox_tool = SandboxTool.from_functions([search_db], capabilities=caps)
agent = create_react_agent(model, [sandbox_tool.as_langchain_tool()])&lt;/code&gt;
    &lt;code&gt;┌────────────────────────────────────────────────┐
│              WASM Sandbox                      │
│  ┌──────────────────────────────────────────┐  │
│  │         Async Scheduler                  │  │
│  │   tasks waiting/running/ready            │  │
│  └──────────────────────────────────────────┘  │
│  ┌────────────┐ ┌──────────┐ ┌──────────────┐  │
│  │  VFS       │ │ Shell    │ │ Capabilities │  │
│  │ /workspace │ │ builtins │ │ validation   │  │
│  └────────────┘ └──────────┘ └──────────────┘  │
│                    ↓ yield                     │
└════════════════════════════════════════════════┘
                     │
                     ▼
┌─────────────────────────────────────────────┐
│              Python Host                    │
│                                             │
│   while sandbox.has_work():                 │
│       req = sandbox.step()  # tool call     │
│       sandbox.resume(execute(req))          │
│                                             │
└─────────────────────────────────────────────┘
&lt;/code&gt;
    &lt;p&gt;The sandbox yields on tool calls. Host executes them (after capability checks) and resumes. QuickJS runs inside WASM for the JS runtime.&lt;/p&gt;
    &lt;p&gt;First run compiles the WASM module (~300ms). Cache it:&lt;/p&gt;
    &lt;code&gt;amla-precompile&lt;/code&gt;
    &lt;p&gt;Subsequent loads: ~0.5ms.&lt;/p&gt;
    &lt;code&gt;from amla_sandbox import Param, ConstraintSet

constraints = ConstraintSet([
    Param("amount") &amp;gt;= 100,
    Param("amount") &amp;lt;= 10000,
    Param("currency").is_in(["USD", "EUR"]),
    Param("path").starts_with("/api/"),
])&lt;/code&gt;
    &lt;p&gt;Pattern matching for method names:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;stripe/charges/create&lt;/code&gt;— exact match&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;stripe/charges/*&lt;/code&gt;— single path segment&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;stripe/**&lt;/code&gt;— zero or more segments&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What you get: Isolation without infrastructure. Capability enforcement. Token efficiency.&lt;/p&gt;
    &lt;p&gt;What you don't get: Full Linux environment. Native module support. GPU access. Infinite loop protection (a &lt;code&gt;while(true){}&lt;/code&gt; will hang - the step limit only counts WASM yields, not JS instructions).&lt;/p&gt;
    &lt;p&gt;If you need a real VM with persistent state and arbitrary dependencies, use e2b or Modal. amla-sandbox is for the common case: agents running generated code with controlled tool access.&lt;/p&gt;
    &lt;p&gt;Python code is MIT. The WASM binary is currently proprietary—you can use it freely with this package, but you can't extract or redistribute it separately. We're working on open sourcing the WASM runtime.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46824877</guid><pubDate>Fri, 30 Jan 2026 14:34:32 +0000</pubDate></item><item><title>Buttered Crumpet, a custom typeface for Wallace and Gromit</title><link>https://jamieclarketype.com/case-study/wallace-and-gromit-font/</link><description>&lt;doc fingerprint="4dde0173bbd23507"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Wallace and Gromit Font&lt;/head&gt;
    &lt;head rend="h2"&gt;Categories:&lt;/head&gt;
    &lt;p&gt;A new typeface for Aardman’s iconic duo – meet Buttered Crumpet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Overview&lt;/head&gt;
    &lt;p&gt;I was thrilled to be selected to design a custom typeface for Wallace &amp;amp; Gromit – Aardman’s most beloved and recognisable characters.&lt;/p&gt;
    &lt;p&gt;The brief called for a font with a distinct tone of voice that could work seamlessly across film, print and digital, while bringing warmth and continuity to their next chapter.&lt;/p&gt;
    &lt;head rend="h2"&gt;Method&lt;/head&gt;
    &lt;p&gt;We began by exploring warm, characterful styles, taking inspiration from Oswald Cooper’s original drawings for Cooper Black. We then took a creative turn, developing a softer, low-contrast design with a distinctly hand-crafted feel.&lt;/p&gt;
    &lt;p&gt;Each letterform was carefully shaped to feel expressive yet balanced, with serifs that resemble loaves of bread – a nod to Aardman’s tactile, playful world.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outcomes&lt;/head&gt;
    &lt;p&gt;The finished typeface – Buttered Crumpet – gives Aardman a timeless, familiar tone of voice with bundles of charm. It includes over 200 characters, covering all Western European languages, and was designed in a single, carefully crafted weight with room for future expansion.&lt;/p&gt;
    &lt;p&gt;As a Bristol-based designer, it was a joy to create a lasting connection with my home city and one of its most renowned creative studios.&lt;/p&gt;
    &lt;quote&gt;I’ve loved rolling out this typeface and we’re starting to see it in action now. There have been lots of compliments.&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46825415</guid><pubDate>Fri, 30 Jan 2026 15:19:28 +0000</pubDate></item><item><title>Self Driving Car Insurance</title><link>https://www.lemonade.com/car/explained/self-driving-car-insurance/</link><description>&lt;doc fingerprint="4400d9e790b4d31"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How does Lemonade Autonomous Car insurance work?&lt;/head&gt;
    &lt;p&gt;Lemonade is the first to reward safer self-driving miles with real discounts.&lt;/p&gt;
    &lt;p&gt;Lemonade is the first to reward safer self-driving miles with real discounts.&lt;/p&gt;
    &lt;p&gt;Lemonade Autonomous Car insurance is the first car insurance designed specifically for self-driving cars. It offers Tesla owners 50% off every mile driven using Full Self-Driving technology by automatically tracking FSD miles versus manual miles through direct Tesla integration.&lt;/p&gt;
    &lt;p&gt;Lemonade Autonomous Car insurance is built on a simple principle: Tesla’s data shows that Full Self-Driving miles are twice as safe as manual driving, so they should cost 50% less on your insurance premiums.&lt;/p&gt;
    &lt;p&gt;When you drive using Tesla’s FSD technology, our system automatically tracks those miles and applies the discount. Manual miles are priced normally, while your FSD miles get the full 50% reduction, creating a pricing model that actually reflects the safety benefits of autonomous driving.&lt;/p&gt;
    &lt;p&gt;Unlike traditional car insurance that relies on estimates, Lemonade connects directly to your Tesla using Tesla’s Fleet API, which gives Lemonade access to vehicle data with a customer’s permission. This means:&lt;/p&gt;
    &lt;p&gt;The technology handles everything behind the scenes, so you just drive and save.&lt;/p&gt;
    &lt;p&gt;Lemonade Autonomous Car insurance is currently available for Tesla drivers in Arizona and launching in Oregon on February 26, 2026, with more states coming soon. To qualify, your Tesla needs:&lt;/p&gt;
    &lt;p&gt;Traditional insurers might offer small discounts for safety features like adaptive cruise control or lane-keeping assistance, but they don’t distinguish between these basic driver assistance systems and sophisticated autonomous driving technology.&lt;/p&gt;
    &lt;p&gt;Lemonade recognizes that Tesla’s Full Self-Driving represents a fundamentally different level of automation, one that deserves a fundamentally different insurance approach.&lt;/p&gt;
    &lt;p&gt;Tesla reports that FSD technology leads to:&lt;/p&gt;
    &lt;p&gt;These aren’t theoretical benefits, they’re real safety improvements that Lemonade’s pricing reflects. When autonomous driving technology demonstrably reduces risk, insurance costs should follow.&lt;/p&gt;
    &lt;p&gt;Instead of estimating your annual mileage and hoping for the best, Lemonade’s system knows exactly how many miles you drive in FSD mode versus manual mode. This usage-based insurance model means you pay precisely for the risk you represent.&lt;/p&gt;
    &lt;p&gt;As car manufacturers like Ford, GM, and others advance their autonomous driving technology, Lemonade is building the infrastructure to support them. Our approach with Tesla creates the foundation for insuring all types of self-driving vehicles as they become available.&lt;/p&gt;
    &lt;p&gt;If you own a Tesla with Hardware 4.0 or higher in an eligible state, you can start saving immediately with Lemonade Autonomous Car insurance. The 50% discount applies to every mile you drive using Full Self-Driving, potentially reducing your overall insurance costs significantly depending on how often you use the technology.&lt;/p&gt;
    &lt;p&gt;Lemonade Autonomous Car insurance works alongside our other products, so Tesla owners can bundle their autonomous coverage with homeowners, renters, pet, or term life insurance for additional discounts.&lt;/p&gt;
    &lt;p&gt;Getting started with Lemonade Autonomous Car insurance is straightforward:&lt;/p&gt;
    &lt;p&gt;Everything is managed through the Lemonade app, with transparent tracking of your FSD miles and savings.&lt;/p&gt;
    &lt;p&gt;Whether you’re a current Tesla owner interested in Lemonade Autonomous Car insurance or considering buying a Tesla to take advantage of this innovative coverage, Lemonade makes the process simple.&lt;/p&gt;
    &lt;p&gt;Our autonomous car insurance represents the future of auto insurance, pricing that actually reflects the safety benefits of new technology, seamless integration with cutting-edge vehicles, and transparent, fair coverage that evolves with your driving.&lt;/p&gt;
    &lt;p&gt;Ready to see how much you could save with the first insurance designed specifically for autonomous vehicles? Get a quote today and discover why safer miles should cost less.&lt;/p&gt;
    &lt;p&gt;You’ll save 50% on every mile driven using Tesla’s Full Self-Driving technology. Your total savings depend on how often you use FSD versus manual driving, but many drivers see significant reductions in their overall insurance premiums.&lt;/p&gt;
    &lt;p&gt;No special policies are required: Lemonade Autonomous Car insurance integrates with regular car insurance requirements. We simply price your FSD miles at 50% less than manual miles because they’re demonstrably safer.&lt;/p&gt;
    &lt;p&gt;Your Tesla needs Hardware 4.0 or higher and firmware version 2025.44.25.5 or newer. The firmware update is free and easy to install through your Tesla app or directly in your car.&lt;/p&gt;
    &lt;p&gt;Existing customers can add autonomous coverage at their next renewal. We don’t recommend canceling your current policy early, as you’d pay unnecessary fees for a new policy setup.&lt;/p&gt;
    &lt;p&gt;We’re working to expand to more states as quickly as possible while ensuring compliance with local regulations. Follow our updates for announcements about new state availability.&lt;/p&gt;
    &lt;p&gt;Please note: Lemonade articles and other editorial content are meant for educational purposes only, and should not be relied upon instead of professional legal, insurance or financial advice. The content of these educational articles does not alter the terms, conditions, exclusions, or limitations of policies issued by Lemonade, which differ according to your state of residence. While we regularly review previously published content to ensure it is accurate and up-to-date, there may be instances in which legal conditions or policy details have changed since publication. Any hypothetical examples used in Lemonade editorial content are purely expositional. Hypothetical examples do not alter or bind Lemonade to any application of your insurance policy to the particular facts and circumstances of any actual claim.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46825828</guid><pubDate>Fri, 30 Jan 2026 15:50:15 +0000</pubDate></item><item><title>Ask HN: Do you also "hoard" notes/links but struggle to turn them into actions?</title><link>https://news.ycombinator.com/item?id=46826277</link><description>&lt;doc fingerprint="6eced5152467dfab"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Hi HN — I’m exploring an idea and would love your feedback.&lt;/p&gt;
      &lt;p&gt;I’m a builder and user of Obsidian, validating a concept called Concerns. Today it’s only a landing page + short survey (no product yet) to test whether this pain is real.&lt;/p&gt;
      &lt;p&gt;The core idea (2–3 bullets):&lt;/p&gt;
      &lt;p&gt;- Many of us capture tons of useful info (notes/links/docs), but it rarely becomes shipped work.&lt;/p&gt;
      &lt;p&gt;- Instead of better “organization” (tags/folders), I’m exploring an “action engine” that:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  1.detects what you’re actively targetting/working on (“active projects”)

  2.surfaces relevant saved material at the right moment

  3.proposes a concrete next action (ideally pushed into your existing task tool)
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; My own “second brain” became a graveyard of good intentions: the organizing tax was higher than the value I got back. I’m trying to validate whether the real bottleneck is execution, not capture.&lt;/p&gt;
      &lt;p&gt;Before writing code, I’m trying to pin down two things:&lt;/p&gt;
      &lt;p&gt;- Project context signals (repo/PRs? issues? tasks? calendar? a “project doc”?)&lt;/p&gt;
      &lt;p&gt;- How to close the loop: ingest knowledge → rank against active projects → emit a small set of next-actions into an existing todo tool → learn from outcomes (done/ignored/edited) and optionally write back the minimal state. The open question: what’s the cleanest feedback signal without creating noise or privacy risk? (explicit ratings vs completion events vs doc-based write-back)&lt;/p&gt;
      &lt;p&gt;What I’m asking from you:&lt;/p&gt;
      &lt;p&gt;1.Where does your “second brain” break down the most?&lt;/p&gt;
      &lt;p&gt;capture / organization / retrieval / execution (If you can, share a concrete recent example.)&lt;/p&gt;
      &lt;p&gt;2.What best represents “active project context” for you today?&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  task project (Todoist/Things/Reminders)

  issues/boards (GitHub/Linear/Jira)

  a doc/wiki page (Notion/Docs)

  calendar

  "in my head"
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt; Which one would you actually allow a tool to read?&lt;/p&gt;
      &lt;p&gt;3.What’s your hard “no” for an AI that suggests actions from your notes/links? (pick 1–2)&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;  privacy/data retention

  noisy suggestions / interruption

  hallucinations / wrong suggestions

  workflow change / migration cost

  pricing

  others&lt;/code&gt;
      &lt;/quote&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46826277</guid><pubDate>Fri, 30 Jan 2026 16:22:05 +0000</pubDate></item><item><title>Kimi K2.5 Technical Report [pdf]</title><link>https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46826597</guid><pubDate>Fri, 30 Jan 2026 16:43:50 +0000</pubDate></item><item><title>Antirender: remove the glossy shine on architectural renderings</title><link>https://antirender.com/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829147</guid><pubDate>Fri, 30 Jan 2026 20:05:24 +0000</pubDate></item><item><title>Silver plunges 30% in worst day since 1980, gold tumbles</title><link>https://www.cnbc.com/2026/01/30/silver-gold-fall-price-usd-dollar-fed-warsh-chair-trump-metals.html</link><description>&lt;doc fingerprint="5a669b173db667ae"&gt;
  &lt;main&gt;
    &lt;p&gt;Gold and silver prices plunged Friday, as President Donald Trump's nomination for the next chair of the Federal Reserve, Kevin Warsh, appeared to relieve concerns about the central bank's independence and sent the dollar soaring.&lt;/p&gt;
    &lt;p&gt;Spot silver was down 28% at $83.45 an ounce, trading near its lows of the day. Silver futures plummeted 31.4% to settle at $78.53, marking its worst day since March 1980.&lt;/p&gt;
    &lt;p&gt;Meanwhile, spot gold shed around 9% to trade at $4,895.22 an ounce. Gold futures dropped 11.4% to settle at $4,745.10.&lt;/p&gt;
    &lt;p&gt;The sharp moves down were initially triggered by reports of Warsh's nomination. However, they gained steam in afternoon U.S. trading as investors who piled into the metals raced to book profits. Metals were also under pressure as the dollar spiked higher, making it more expensive for foreign investors to buy gold and silver and spoiling the theory that metals would replace the greenback as the globe's reserve currency.&lt;/p&gt;
    &lt;p&gt;The dollar index last traded around 0.8% higher.&lt;/p&gt;
    &lt;p&gt;"This is getting crazy," said Matt Maley, equity strategist at Miller Tabak. "Most of this is probably 'forced selling.' This has been the hottest asset for day traders and other short-term traders recently. So, there has been some leverage built up in silver. With the huge decline today, the margin calls went out."&lt;/p&gt;
    &lt;head rend="h2"&gt;Trump picks Warsh&lt;/head&gt;
    &lt;p&gt;National Economic Council Director Kevin Hassett had been the favorite to replace Powell for some time, but Warsh became the front-runner in prediction markets in recent days.&lt;/p&gt;
    &lt;p&gt;In a note on Friday morning, Evercore ISI's Krishna Guha said the market was "trading Warsh hawkish."&lt;/p&gt;
    &lt;p&gt;"The Warsh pick should help stabilize the dollar some and reduce (though not eliminate) the asymmetric risk of deep extended dollar weakness by challenging debasement trades – which is also why gold and silver are sharply lower," the firm's vice chairman said.&lt;/p&gt;
    &lt;p&gt;"But, we advise against overdoing the Warsh hawkish trade across asset markets – and even see some risk of a whipsaw. We see Warsh as a pragmatist not an ideological hawk in the tradition of the independent conservative central banker."&lt;/p&gt;
    &lt;p&gt;Claudio Wewel, FX strategist at J. Safra Sarasin Sustainable Asset Management, told CNBC's "Squawk Box Europe" on Friday that a "perfect storm" of geopolitical tensions had helped precious metals move higher this year, pointing to the U.S. capture of Venezuelan President Nicolás Maduro and Washington's threats to use military force in Greenland and Iran.&lt;/p&gt;
    &lt;p&gt;More recently, he said, speculation over who would be nominated as the next Fed chair had been influencing metals markets.&lt;/p&gt;
    &lt;p&gt;"The market has clearly been pricing the risk of a much more dovish contender, that's been largely helping the gold price along with other precious metal prices. Over the last 24 hours, the news flow has changed a little bit," Wewel said, prior to Trump's announcement.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Even good assets can sell-off'&lt;/head&gt;
    &lt;p&gt;Gold and silver both enjoyed record-smashing rallies in 2025, surging 66% and 135%, respectively, over the course of the year.&lt;/p&gt;
    &lt;p&gt;Coeur Mining lost 17%. Silver ETFs were dragged into the action, with the ProShares Ultra Silver fund last seen more than 62% lower. The iShares Silver Trust ETF lost 31%. Both funds were headed for their worst days on record.&lt;/p&gt;
    &lt;p&gt;Precious metals have been on a stellar rally over the past 12 months, amid broader market volatility, the decline of the U.S. dollar, bubbling geopolitical tensions and concerns about the independence of the Federal Reserve.&lt;/p&gt;
    &lt;p&gt;Katy Stoves, investment manager at British wealth management firm Mattioli Woods, told CNBC on Friday morning that the moves were likely "a market-wide reassessment of concentration risk."&lt;/p&gt;
    &lt;p&gt;"Just as tech stocks — particularly AI-related names — have dominated market attention and capital flows, gold has similarly seen intense positioning and crowding," she said. "When everyone is leaning the same way, even good assets can sell off as positions get unwound. The parallel isn't accidental: both represent areas where capital has flooded in based on powerful narratives, and concentrated positions eventually face their day of reckoning."&lt;/p&gt;
    &lt;p&gt;Meanwhile, Toni Meadows, head of investment at BRI Wealth Management, contended that gold's run to the $5,000 mark had happened "too easily." He noted that the unwinding of the greenback had supported gold prices, but that the dollar had appeared to stabilize.&lt;/p&gt;
    &lt;p&gt;"Central bank buying has driven the longer-term rally but this has tailed off in recent months," he said. "The case for further reserve diversification is still there though as Trump's trade policies and intervention in foreign affairs will make a lot of countries nervous about holding U.S. assets, especially those countries in the emerging markets or aligned to China or Russia. Silver will mirror the direction of gold, so it is not surprising to see falls there."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829548</guid><pubDate>Fri, 30 Jan 2026 20:37:48 +0000</pubDate></item><item><title>Peerweb: Decentralized website hosting via WebTorrent</title><link>https://peerweb.lol/</link><description>&lt;doc fingerprint="868e3ff18d2cd634"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;🪐 PeerWeb&lt;/head&gt;
    &lt;head rend="h2"&gt;Decentralized Website Hosting via WebTorrent&lt;/head&gt;
    &lt;head rend="h3"&gt;🤔 What is PeerWeb?&lt;/head&gt;
    &lt;p&gt;PeerWeb is a revolutionary way to host and share websites using WebTorrent technology. Instead of relying on centralized servers, websites are distributed across a peer-to-peer network, making them censorship-resistant and always available. 🌍✨&lt;/p&gt;
    &lt;head rend="h3"&gt;📤 Quick Upload&lt;/head&gt;
    &lt;head rend="h4"&gt;Drag &amp;amp; Drop Your Website&lt;/head&gt;
    &lt;p&gt;Drop a folder with your website files&lt;/p&gt;
    &lt;head rend="h3"&gt;📚 How to Use PeerWeb&lt;/head&gt;
    &lt;head rend="h3"&gt;💡 Load Existing Site&lt;/head&gt;
    &lt;p&gt;To load a website from a torrent hash, enter it below:&lt;/p&gt;
    &lt;p&gt;🎯 Just the hash! PeerWeb automatically adds the magnet link prefix and trackers.&lt;/p&gt;
    &lt;head rend="h3"&gt;🧪 Demos&lt;/head&gt;
    &lt;p&gt; Functionality test page: &lt;lb/&gt;https://peerweb.lol/?orc=90c020bd252639622a14895a0fad713b91e0130c &lt;/p&gt;
    &lt;p&gt; SomaFM on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=908d19242ae1461f333a516d1f8b89c13ef2d259 &lt;/p&gt;
    &lt;p&gt; Chess on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=1e14b1ba7fcd03e5f165d53ed8223a333349db04 &lt;/p&gt;
    &lt;p&gt; Text Editor app on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=4e5f1204dcec68195bfcc89f9410a0b70a0ddfac &lt;/p&gt;
    &lt;head rend="h3"&gt;🐛 Debug Mode&lt;/head&gt;
    &lt;p&gt;For developers and troubleshooting, add &amp;amp;debug=true to see detailed progress:&lt;/p&gt;
    &lt;code&gt;https://peerweb.lol?orc=ABC123DEF456...&amp;amp;debug=true&lt;/code&gt;
    &lt;head rend="h3"&gt;🚀 Advanced Options&lt;/head&gt;
    &lt;head rend="h3"&gt;💾 Smart Caching&lt;/head&gt;
    &lt;p&gt;PeerWeb caches visited sites for lightning-fast loading! 🚀&lt;/p&gt;
    &lt;head rend="h3"&gt;🛡️ Security Features&lt;/head&gt;
    &lt;p&gt;Enhanced security with DOMPurify integration! 🔒&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829582</guid><pubDate>Fri, 30 Jan 2026 20:40:00 +0000</pubDate></item><item><title>P vs. NP and the Difficulty of Computation: A ruliological approach</title><link>https://writings.stephenwolfram.com/2026/01/p-vs-np-and-the-difficulty-of-computation-a-ruliological-approach/</link><description>&lt;doc fingerprint="fe12981443a180ed"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Empirical Theoretical Computer Science&lt;/head&gt;
    &lt;p&gt;“Could there be a faster program for that?” It’s a fundamental type of question in theoretical computer science. But except in special cases, such a question has proved fiendishly difficult to answer. And, for example, in half a century, almost no progress has been made even on the rather coarse (though very famous) P vs. NP question—essentially of whether for any nondeterministic program there will always be a deterministic one that is as fast. From a purely theoretical point of view, it’s never been very clear how to even start addressing such a question. But what if one were to look at the question empirically, say in effect just by enumerating possible programs and explicitly seeing how fast they are, etc.?&lt;/p&gt;
    &lt;p&gt;One might imagine that any programs one could realistically enumerate would be too small to be interesting. But what I discovered in the early 1980s is that this is absolutely not the case—and that in fact it’s very common for programs even small enough to be easily enumerated to show extremely rich and complex behavior. With this intuition I already in the 1990s began some empirical exploration of things like the fastest ways to compute functions with Turing machines. But now—particularly with the concept of the ruliad—we have a framework for thinking more systematically about the space of possible programs, and so I’ve decided to look again at what can be discovered by ruliological investigations of the computational universe about questions of computational complexity theory that have arisen in theoretical computer science—including the P vs. NP question.&lt;/p&gt;
    &lt;p&gt;We won’t resolve the P vs. NP question. But we will get a host of definite, more restricted results. And by looking “underneath the general theory” at explicit, concrete cases we’ll get a sense of some of the fundamental issues and subtleties of the P vs. NP question, and why, for example, proofs about it are likely to be so difficult.&lt;/p&gt;
    &lt;p&gt;Along the way, we’ll also see lots of evidence of the phenomenon of computational irreducibility—and the general pattern of the difficulty of computation. We’ll see that there are computations that can be “reduced”, and done more quickly. But there are also others where we’ll be able to see with absolute explicitness that—at least within the class of programs we’re studying—there’s simply no faster way to get the computations done. In effect this is going to give us lots of proofs of restricted forms of computational irreducibility. And seeing these will give us ways to further build our intuition about the ever-more-central phenomenon of computational irreducibility—as well as to see how in general we can use the methodology of ruliology to explore questions of theoretical computer science.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Basic Setup&lt;/head&gt;
    &lt;p&gt;Click any diagram to get Wolfram Language code to reproduce it.&lt;/p&gt;
    &lt;p&gt;How can we enumerate possible programs? We could pick any model of computation. But to help connect with traditional theoretical computer science, I’ll use a classic one: Turing machines.&lt;/p&gt;
    &lt;p&gt;Often in theoretical computer science one concentrates on yes/no decision problems. But here it’ll typically be convenient instead to think (more “mathematically”) about Turing machines that compute integer functions. The setup we’ll use is as follows. Start the Turing machine with the digits of some integer n on its tape. Then run the Turing machine, stopping if the Turing machine head goes further to the right than where it started. The value of the function with input n is then read off from the binary digits that remain on its tape when the Turing machine stops. (There are many other “halting” criteria we could use, but this is a particularly robust and convenient one.)&lt;/p&gt;
    &lt;p&gt;So for example, given a Turing machine with rule&lt;/p&gt;
    &lt;p&gt;we can feed it successive integers as input, then run the machine to find the successive values it computes:&lt;/p&gt;
    &lt;p&gt;In this case, the function that the Turing machine computes is&lt;/p&gt;
    &lt;p&gt;or in graphical form:&lt;/p&gt;
    &lt;p&gt;For each input, the Turing machine takes a certain number of steps to stop and give its output (i.e. the value of the function):&lt;/p&gt;
    &lt;p&gt;But this particular Turing machine isn’t the only one that can compute this function. Here are two more:&lt;/p&gt;
    &lt;p&gt;The outputs are the same as before, but the runtimes are different:&lt;/p&gt;
    &lt;p&gt;Indicating these respectively by and plotting them together, we see that there are definite trends—but no clear winner for “fastest program”:&lt;/p&gt;
    &lt;p&gt;In computational complexity theory, it’s common to discuss how runtime varies with input size—which here means taking each block of inputs with a given number of digits, and just finding its maximum:&lt;/p&gt;
    &lt;p&gt;And what we see is that in this case the first Turing machine shown is “systematically faster” than the other two—and in fact provides the fastest way to compute this particular function among Turing machines of the size we’re using.&lt;/p&gt;
    &lt;p&gt;Since we’ll be dealing with lots of Turing machines here, it’s convenient to be able to specify them just with numbers—and we’ll do it the way TuringMachine in the Wolfram Language does. And with this setup, the machines we’ve just considered have numbers 261, 3333 and 1285.&lt;/p&gt;
    &lt;p&gt;In thinking about functions computed by Turing machines, there is one immediate subtlety to consider. We’ve said that we find the output by reading off what’s on the Turing machine tape when the Turing machine stops. But what if the machine never stops? (Or in our case, what if the head of the Turing machine never reaches the right-hand end?) Well, then there’s no output value defined. And in general, the functions our Turing machines compute will only be partial functions—in the sense that for some of their inputs, there may be no output value defined (as here for machine 2189):&lt;/p&gt;
    &lt;p&gt;When we plot such partial functions, we’ll just have a gap where there are undefined values:&lt;/p&gt;
    &lt;p&gt;In what follows, we’ll be exploring Turing machines of different “sizes”. We’ll assume that there are two possible colors for each position on the tape—and that there are s possible states for the head. The total number of possible Turing machines with k = 2 colors and s states is (2ks)ks—which grows rapidly with s:&lt;/p&gt;
    &lt;p&gt;For any given function we’ll then be able to ask what machine (or machines) up to a given size compute it the fastest. In other words, by explicitly studying possible Turing machines, we’ll be able to establish an absolute lower bound on the computational difficulty of computing a function, at least when that computation is done by a Turing machine of at most a given size. (And, yes, the size of the Turing machine can be thought of as characterizing its “algorithmic information content”.)&lt;/p&gt;
    &lt;p&gt;In traditional computational complexity theory, it’s usually been very difficult to establish lower bounds. But our ruliological approach here will allow us to systematically do it (at least relative to machines of a given size, i.e. with given algorithmic information content). (It’s worth pointing out that if a machine is big enough, it can include a lookup table for any number of cases of any given function—making questions about the difficulty of computing at least those cases rather moot.)&lt;/p&gt;
    &lt;head rend="h2"&gt;The s = 1, k = 2 Turing Machines&lt;/head&gt;
    &lt;p&gt;To begin our systematic investigation of possible programs, let’s consider what is essentially the simplest possible case: Turing machines with one state and two possible colors of cells on their tape &lt;/p&gt;
    &lt;p&gt;Here’s what each of these machines does for successive integer inputs:&lt;/p&gt;
    &lt;p&gt;Looking at the outputs in each case, we can plot the functions these compute:&lt;/p&gt;
    &lt;p&gt;And here are the corresponding runtimes:&lt;/p&gt;
    &lt;p&gt;Out of all 16 machines, 8 compute total functions (i.e. the machines always terminate, so the values of the functions are defined for every input), and 8 don’t. Four machines produce “complicated-looking” functions; an example is machine 14, which computes the function:&lt;/p&gt;
    &lt;p&gt;There are a variety of representations for this function, including&lt;/p&gt;
    &lt;p&gt;and:&lt;/p&gt;
    &lt;p&gt;The way the function is computed by the Turing machine is&lt;/p&gt;
    &lt;p&gt;and the runtime is given by&lt;/p&gt;
    &lt;p&gt;which is simply:&lt;/p&gt;
    &lt;p&gt;For input of size n, this implies the worst-case time complexity for computing this function is &lt;/p&gt;
    &lt;p&gt;Each one of the 1-state machines works at least slightly differently. But in the end, all of them are simple enough in their behavior that one can readily give a “closed-form formula” for the value of f[i] for any given i:&lt;/p&gt;
    &lt;p&gt;One thing that’s notable is that—except in the trivial case where all values are undefined—there are no examples among &lt;/p&gt;
    &lt;head rend="h2"&gt;s = 2, k = 2 Turing Machines&lt;/head&gt;
    &lt;p&gt;There are a total of 4096 possible 2-state, 2-color Turing machines. Running all these machines, we find that they compute a total of 350 distinct functions—of which 189 are total. Here are plots of these distinct total functions—together with a count of how many machines generate them (altogether 2017 of the 4096 machines always terminate, and therefore compute total functions):&lt;/p&gt;
    &lt;p&gt;Plotting the values of all these functions in 3D, we see that the vast majority have values f[i] that are close to their inputs i—indicating that in a sense the Turing machines usually “don’t do much” to their input:&lt;/p&gt;
    &lt;p&gt;To see more clearly what the machines “actually do”, we can look at the quantity &lt;/p&gt;
    &lt;p&gt;though in 6 cases it is 2, and in 3 cases (which include the “most popular” case &lt;/p&gt;
    &lt;p&gt;Dropping periodic cases, the remaining distinct &lt;/p&gt;
    &lt;p&gt;Some of what we see here is similar to the 1-state case. An example of different behavior occurs for machine 2223&lt;/p&gt;
    &lt;p&gt;which gives for &lt;/p&gt;
    &lt;p&gt;In this case f[i] turns out to be expressible simply as&lt;/p&gt;
    &lt;p&gt;or:&lt;/p&gt;
    &lt;p&gt;Another example is machine 2079&lt;/p&gt;
    &lt;p&gt;which gives for &lt;/p&gt;
    &lt;p&gt;This function once again turns out to be expressible in “closed form”:&lt;/p&gt;
    &lt;p&gt;Some functions grow rapidly. For example, machine 3239&lt;/p&gt;
    &lt;p&gt;has values:&lt;/p&gt;
    &lt;p&gt;These have the property that &lt;/p&gt;
    &lt;p&gt;There are many subtleties even in dealing with 2-state Turing machines. For example, different machines may “look like” they’re generating the same function f[i] up to a certain value of i, and only then deviate. The most extreme example of such a “surprise” among machines generating total functions occurs among:&lt;/p&gt;
    &lt;p&gt;Up to &lt;/p&gt;
    &lt;p&gt;What about partial functions? At least for 2-state machines, if undefined values in f[i] are ever going to occur, they always already occur for small i. The “longest holdouts” are machines 1960 and 2972, which are both first undefined for input 8&lt;/p&gt;
    &lt;p&gt;but which “become undefined” in different ways: in machine 1960, the head systematically moves to the left, while in machine 2972, it moves periodically back and forth forever, without ever reaching the right-hand end. (Despite their different mechanisms, both rules share the feature of being undefined for all inputs that are multiples of 8.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Runtimes in s = 2, k = 2 Machines&lt;/head&gt;
    &lt;p&gt;What about runtimes? If a function f[i] is computed by several different Turing machines, the details of how it’s computed by each machine will normally be at least slightly different. Still, in many cases the mechanisms are similar enough that their runtimes are the same. And in the end, among all the 2017 machines that compute our 189 distinct total functions, there are only 103 distinct “profiles” of runtime vs. input (and indeed many of these are very similar):&lt;/p&gt;
    &lt;p&gt;The picture gets simpler if, rather than plotting runtimes for each specific input value, we instead plot the worst-case runtime for all inputs of a given size. (In effect we’re plotting against IntegerLength[i, 2] or Ceiling[Log2[i + 1]].) There turn out to be just 71 distinct profiles for such worst-case time complexity&lt;/p&gt;
    &lt;p&gt;and indeed all of these have fairly simple closed forms—which for even n are (with directly analogous forms for odd n):&lt;/p&gt;
    &lt;p&gt;If we consider the behavior of these worst-case runtimes for large input lengths n, we find that fairly few distinct growth rates occur—notably with linear, quadratic and exponential cases, but nothing in between:&lt;/p&gt;
    &lt;p&gt;The machines with the fastest growth &lt;/p&gt;
    &lt;p&gt;For a size-n input, the maximum value of the function is just the maximum integer with n digits, or &lt;/p&gt;
    &lt;p&gt;And at these maxima, the machine is effectively operating like a binary counter, generating all the states it can, with the head moving in a very regular nested pattern:&lt;/p&gt;
    &lt;p&gt;It turns out that for &lt;/p&gt;
    &lt;p&gt;Of the 8 machines with runtimes growing like &lt;/p&gt;
    &lt;p&gt;The two machines with asymptotic runtime growth &lt;/p&gt;
    &lt;p&gt;Here’s the actual behavior of these machines when given inputs 1 through 10:&lt;/p&gt;
    &lt;p&gt;(The lack of runtimes intermediate between quadratic and exponential is notable—and perhaps reminiscent of the rarity of “intermediate growth” seen for example in the cases of finitely generated groups and multiway systems.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Runtime Distributions&lt;/head&gt;
    &lt;p&gt;Our emphasis so far has been on worst-case runtimes: the largest runtimes required for inputs of any given size. But we can also ask about the distribution of runtimes within inputs of a given size.&lt;/p&gt;
    &lt;p&gt;So, for example, here are the runtimes for all size-11 inputs for a particular, fairly typical Turing machine (&lt;/p&gt;
    &lt;p&gt;The maximum (“worst-case”) value here is 43—but the median is only 13. In other words, while some computations take a while, most run much faster—so that the runtime distribution is peaked at small values:&lt;/p&gt;
    &lt;p&gt;(The way our Turing machines are set up, they always run for an even number of steps before terminating—since to terminate, the head must move one position to the right for every position it moved to the left.)&lt;/p&gt;
    &lt;p&gt;If we increase the size of the inputs, we see that the distribution, at least in this case, is close to exponential:&lt;/p&gt;
    &lt;p&gt;It turns out that this kind of exponential distribution is typical of what we see in almost all Turing machines. (It’s notable that this is rather different from the t –1/2 “stopping time” distribution we’d expect if the Turing machine head was “on average” executing a random walk with an absorbing boundary.) There are nevertheless machines whose distributions deviate significantly from exponential, examples being:&lt;/p&gt;
    &lt;p&gt;Some simply have long tails to their exponentials. Others, however, have an overall non-exponential form.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Fast Can Functions Be Computed?&lt;/head&gt;
    &lt;p&gt;We’ve now seen lots of functions—and runtime profiles—that &lt;/p&gt;
    &lt;p&gt;We’ve seen that there are machines that compute functions quite slowly—like in exponential time. But are these machines the fastest that compute those particular functions? It turns out the answer is no.&lt;/p&gt;
    &lt;p&gt;And if we look across all 189 total functions computed by &lt;/p&gt;
    &lt;p&gt;In other words, there are 8 functions that are the “most difficult to compute” for &lt;/p&gt;
    &lt;p&gt;What are these functions? Here’s one of them (computed by machine 1511):&lt;/p&gt;
    &lt;p&gt;If we plot this function, it seems to have a nested form&lt;/p&gt;
    &lt;p&gt;which becomes somewhat more obvious on a log-log plot:&lt;/p&gt;
    &lt;p&gt;As it turns out, there’s what amounts to a “closed form” for this function&lt;/p&gt;
    &lt;p&gt;though unlike the closed forms we saw above, this one involves Nest, and effectively computes its results recursively:&lt;/p&gt;
    &lt;p&gt;How about machine 1511? Well, here’s how it computes this function—in effect visibly using recursion:&lt;/p&gt;
    &lt;p&gt;The runtimes are&lt;/p&gt;
    &lt;p&gt;giving worst-case runtimes for inputs of size n of the form:&lt;/p&gt;
    &lt;p&gt;It turns out all 8 functions with minimum runtimes growing like &lt;/p&gt;
    &lt;p&gt;For the functions with fastest computation times &lt;/p&gt;
    &lt;p&gt;So what can we conclude? Well, we now know some functions that cannot be computed by &lt;/p&gt;
    &lt;head rend="h2"&gt;Computing the Same Functions at Different Speeds&lt;/head&gt;
    &lt;p&gt;We now know the fastest that certain functions can be computed by &lt;/p&gt;
    &lt;p&gt;And in fact it’s common for there to be only one machine that computes a given function. Out of the 189 total functions that can be computed by &lt;/p&gt;
    &lt;p&gt;OK, but if multiple machines compute the same function, we can then ask how their speeds compare. Well, it turns out that for 145 of our 189 total functions all the different machines that compute the same function do so with the same “runtime profile” (i.e. with the same runtime for each input i). But that leaves 44 functions for which there are multiple runtime profiles:&lt;/p&gt;
    &lt;p&gt;Here are all these 44 functions, together with the distinct runtime profiles for machines that compute them:&lt;/p&gt;
    &lt;p&gt;Much of the time we see that the possible runtime profiles for computing a given function differ only very little. But sometimes the difference is more significant. For example, for the identity function &lt;/p&gt;
    &lt;p&gt;Within these 10 profiles, there are 3 distinct rates of growth for the worst-case runtime by input size: constant, linear, and exponential&lt;/p&gt;
    &lt;p&gt;exemplified by machines 3197, 3589 and 3626 respectively:&lt;/p&gt;
    &lt;p&gt;Of course, there’s a trivial way to compute this particular function—just by having a Turing machine that doesn’t change its input. And, needless to say, such a machine has runtime 1 for all inputs:&lt;/p&gt;
    &lt;p&gt;It turns out that for &lt;/p&gt;
    &lt;p&gt;But although there are not different “orders of growth” for worst-case runtimes among any other (total) functions computed by &lt;/p&gt;
    &lt;p&gt;by slightly different methods&lt;/p&gt;
    &lt;p&gt;with different worst-case runtime profiles&lt;/p&gt;
    &lt;p&gt;or:&lt;/p&gt;
    &lt;p&gt;By the way, if we consider partial instead of total functions, nothing particularly different happens, at least with &lt;/p&gt;
    &lt;p&gt;that are again essentially computing the identity function.&lt;/p&gt;
    &lt;p&gt;Another question is how &lt;/p&gt;
    &lt;p&gt;But how fast are the computations? This compares the possible worst-case runtimes for &lt;/p&gt;
    &lt;p&gt;There must always be &lt;/p&gt;
    &lt;p&gt;But can &lt;/p&gt;
    &lt;head rend="h2"&gt;Absolute Lower Bounds and the Efficiency of Machines&lt;/head&gt;
    &lt;p&gt;We’ve seen that different Turing machines can take different times to compute particular functions. But how fast can any conceivable Turing machine—even in principle—compute a given function?&lt;/p&gt;
    &lt;p&gt;There’s an obvious absolute lower bound to the runtime: with the way we’ve set things up, if a Turing machine is going to take input i and generate output j, its head has to at least be able to go far enough to the left to reach all the bits that need to change in going from i to j—as well as making it back to the right-hand end so that the machine halts. The number of steps required for this is&lt;/p&gt;
    &lt;p&gt;which for values of i and j up to 8 bits is:&lt;/p&gt;
    &lt;p&gt;So how do the runtimes of actual Turing machine computations compare with these absolute lower bounds?&lt;/p&gt;
    &lt;p&gt;Here’s the behavior of s = 1, k = 2 machines 1 and 3, where for each input we’re giving the actual runtime along with the absolute lower bound:&lt;/p&gt;
    &lt;p&gt;In the second case, the machine is always as efficient as it absolutely can be; in the first case, it only sometimes is—though the maximum slowdown is only 2 steps.&lt;/p&gt;
    &lt;p&gt;For s = 2, k = 2 machines, the differences can be much larger. For example, machine 378 can take exponential time—even though the absolute lower bound in this case is just 1 step, since this machine computes the identity function:&lt;/p&gt;
    &lt;p&gt;Here’s another example (machine 1447) in which the actual runtime is always roughly twice the absolute lower bound:&lt;/p&gt;
    &lt;p&gt;But how does the smallest (worst-case) runtime for any s = 2 Turing machine to compute a given function compare to the absolute lower bound? Well, in a result that presages what we’ll see later in discussing the P vs. NP question, the difference can be increasingly large:&lt;/p&gt;
    &lt;p&gt;The functions being computed here are&lt;/p&gt;
    &lt;p&gt;and the fastest s = 2 Turing machines that do this are (machines 2205, 3555 and 2977):&lt;/p&gt;
    &lt;p&gt;Our absolute lower bound determines how fast a Turing machine can possibly generate a given output. But one can also think of it as something that measures how much a Turing machine has “achieved” when it generates a given output. If the output is exactly the same as the input, the Turing machine has effectively “achieved nothing”. The more they differ, the more one can think of the machine having “achieved”.&lt;/p&gt;
    &lt;p&gt;So now a question one can ask is: are there functions where little is achieved in the transformation from input to output, but where the minimum runtime to perform this transformation is still long? One might wonder about the identity function—where in effect “nothing is achieved”. And indeed we’ve seen that there are Turing machines that compute this function, but only slowly. However, there are also machines that compute it quickly—so in a sense its computation doesn’t need to be slow.&lt;/p&gt;
    &lt;p&gt;The function above computed by machine 2205 is a somewhat better example. The (worst-case) “distance” between input and output grows like 2n with the input size n, but the fastest the function can be computed is what machine 2205 does, with a runtime that grows like 10n. Yes, these are still both linear in n. But at least to some extent this is an example of a function that “doesn’t need to be slow to compute”, but is at least somewhat slow to compute—at least for any &lt;/p&gt;
    &lt;head rend="h2"&gt;Space Complexity&lt;/head&gt;
    &lt;p&gt;How difficult is it to compute the value of a function, say with a Turing machine? One measure of that is the time it takes, or, more specifically, how many Turing machine steps it takes. But another measure is how much “space” it takes, or, more specifically, with our setup, how far to the left the Turing machine head goes—which determines how much “Turing machine memory” or “tape” has to be present.&lt;/p&gt;
    &lt;p&gt;Here’s a typical example of the comparison between “space” and “time” used in a particular Turing machine:&lt;/p&gt;
    &lt;p&gt;If we look at all possible space usage profiles as a function of input size we see that—at least for &lt;/p&gt;
    &lt;p&gt;(One could also consider different measures of “complexity”—perhaps appropriate for different kinds of idealized hardware. Examples include seeing the total length of path traversed by the head, the total area of the region delimited by the head, the number of times 1 is written to the tape during the computation, etc.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Runtime Distributions for Particular Inputs across Machines&lt;/head&gt;
    &lt;p&gt;We’ve talked quite a lot about how runtime varies with input (or input size) for a particular machine. But what about the complementary question: given a particular input, how does runtime vary across different machines? Consider, for example, the &lt;/p&gt;
    &lt;p&gt;The runtimes for these machines are:&lt;/p&gt;
    &lt;p&gt;Here’s what we see if we continue to larger inputs:&lt;/p&gt;
    &lt;p&gt;The maximum (finite) runtime across all &lt;/p&gt;
    &lt;p&gt;or in closed form:&lt;/p&gt;
    &lt;p&gt;For s = 2, k = 2 machines, the distribution of runtimes with input 1 is&lt;/p&gt;
    &lt;p&gt;where the maximum value of 17 is achieved for machine 1447. For larger inputs the maximum runtimes are:&lt;/p&gt;
    &lt;p&gt;Plotting these maximum runtimes&lt;/p&gt;
    &lt;p&gt;we see a big peak at input 127, corresponding to runtime 509 (achieved by machines 378 and 1351). And, yes, plotting the distribution for input 127 of runtimes for all machines, we see that this is a significant outlier:&lt;/p&gt;
    &lt;p&gt;If one computes runtimes maximized over all machines and all inputs for successively larger sizes of inputs, one gets (once again dominated by machines 378 and 1351):&lt;/p&gt;
    &lt;p&gt;By the way, one can compute not only runtimes but also values and widths maximized across machines:&lt;/p&gt;
    &lt;p&gt;And, no, the maximum value isn’t always of the form &lt;/p&gt;
    &lt;head rend="h2"&gt;s = 3, k = 2 Turing Machines and the Problem of Undecidability&lt;/head&gt;
    &lt;p&gt;We’ve so far looked at &lt;/p&gt;
    &lt;p&gt;The issue—as so often—is of computational irreducibility. Let’s say you have a machine and you’re trying to figure out if it computes a particular function. Or you’re even just trying to figure out if for input i it gives output j. Well, you might say, why not just run the machine? And of course you can do that. But the problem is: how long should you run it for? Let’s say the machine has been running for a million steps, and still hasn’t generated any output. Will the machine eventually stop, producing either output j or some other output? Or will the machine just keep running forever, and never generate any output at all?&lt;/p&gt;
    &lt;p&gt;If the behavior of the machine was computationally reducible, then you could expect to be able to “jump ahead” and figure out what it would do, without following all the steps. But if it’s computationally irreducible, then you can’t expect to do that. It’s a classic halting problem situation. And you have to conclude that the general problem of determining whether the machine will generate, say, output j is undecidable.&lt;/p&gt;
    &lt;p&gt;Of course, in lots of particular cases (say, for lots of particular inputs) it may be easy enough to tell what’s going to happen, either just by running for some number of steps, or by using some kind of proof or other abstract derivation. But the point is that—because of computational irreducibility—there’s no upper bound on the amount of computational effort that could be needed. And so the problem of “always getting an answer” has to be considered formally undecidable.&lt;/p&gt;
    &lt;p&gt;But what happens in practice? Let’s say we look at the behavior of all &lt;/p&gt;
    &lt;p&gt;And we then conclude that a bit more than half the machines halt—with the largest finite runtime being the fairly modest 53, achieved by machine 630283 (essentially equivalent to 718804):&lt;/p&gt;
    &lt;p&gt;But is this actually correct? Or do some of the machines we think don’t halt based on running for a million steps actually eventually halt—but only after more steps?&lt;/p&gt;
    &lt;p&gt;Here are a few examples of what happens:&lt;/p&gt;
    &lt;p&gt;And, yes, in all these cases we can readily see that the machines will never halt—and instead, potentially after some transient, their heads just move essentially periodically forever. Here’s the distribution of periods one finds&lt;/p&gt;
    &lt;p&gt;with the longest-period cases being:&lt;/p&gt;
    &lt;p&gt;And here’s the distribution of transients&lt;/p&gt;
    &lt;p&gt;with the longest-transient cases being:&lt;/p&gt;
    &lt;p&gt;But this doesn’t quite account for all the machines that don’t halt after a million steps: there are still 1938 left over. There are 91 distinct patterns of growth—and here are samples of what happens:&lt;/p&gt;
    &lt;p&gt;All of these eventually have a fundamentally nested structure. The patterns grow at different rates—but always in a regular succession of steps. Sometimes the spacings between these steps are polynomials, sometimes exponentials—implying either fractional power or logarithmic growth of the corresponding pattern. But the important point for our purposes here is that we can be confident that—at least with input 1—we know which &lt;/p&gt;
    &lt;p&gt;But what happens if we increase the input value we provide? Here are the first 20 maximum finite lifetimes we get:&lt;/p&gt;
    &lt;p&gt;In the “peak case” of input 10, the distribution of runtimes is&lt;/p&gt;
    &lt;p&gt;with, yes, the maximum value being a somewhat strange outlier.&lt;/p&gt;
    &lt;p&gt;What is that outlier? It’s machine 600720 (along with the related machine 670559)—and we’ll be discussing it in more depth in the next section. But suffice it to say now that 600720 shows up repeatedly as the &lt;/p&gt;
    &lt;p&gt;What about for larger inputs? Well, things get wilder then. Like, for example, consider the case of machine 1955095. For all inputs up to 41, the machine halts after a modest number of steps:&lt;/p&gt;
    &lt;p&gt;But then, at input 42, there’s suddenly a surprise—and the machine never halts:&lt;/p&gt;
    &lt;p&gt;And, yes, we can immediately tell it never halts, because we can readily see that the same pattern of growth repeats periodically—every 24 steps. (A more extreme example is &lt;/p&gt;
    &lt;p&gt;And, yes, things like this are the “long arm” of undecidability reaching in. But by successively investigating both larger inputs and longer runtimes, one can develop reasonable confidence that—at least most of the time—one is correctly identifying both cases that lead to halting, and ones that do not. And from this one can estimate that of all the 2,985,984 possible &lt;/p&gt;
    &lt;p&gt;Summarizing our results we find that—somewhat surprisingly—the halting fraction is quite similar for different numbers of states, and always close to 1/2:&lt;/p&gt;
    &lt;p&gt;And based on our census of halting machines, we can then conclude that the number of distinct total functions computed by &lt;/p&gt;
    &lt;head rend="h2"&gt;Machine 600720&lt;/head&gt;
    &lt;p&gt;In looking at the runtimes of &lt;/p&gt;
    &lt;p&gt;I actually first noticed this machine in the 1990s as part of my work on A New Kind of Science—and with considerable effort was able to give a rather elaborate analysis of at least some of its behavior:&lt;/p&gt;
    &lt;p&gt;The first remarkable thing about the machine is the dramatic peaks it exhibits in the output values it generates:&lt;/p&gt;
    &lt;p&gt;These peaks are accompanied by corresponding (somewhat less dramatic) peaks in runtime:&lt;/p&gt;
    &lt;p&gt;The first of the peaks shown here occurs at input i = 34—with runtime 315,391, and output &lt;/p&gt;
    &lt;p&gt;but the basic point is that the machine seems to behave in a very “deliberate” way that one might imagine could be analyzed.&lt;/p&gt;
    &lt;p&gt;It turns out, though, that the analysis is surprisingly complicated. Here’s a table of maximum (worst-case) runtimes (and corresponding inputs and outputs):&lt;/p&gt;
    &lt;p&gt;For odd n &amp;gt; 3, the maximum runtime occurs when the input value i is:&lt;/p&gt;
    &lt;p&gt;The corresponding initial states for the Turing machine are of the form:&lt;/p&gt;
    &lt;p&gt;The output value with such an input (for odd n &amp;gt; 3) is then&lt;/p&gt;
    &lt;p&gt;while the runtime—derived effectively by “mathematicizing” what the Turing machine does for these inputs—is given by the bizarrely complex formula:&lt;/p&gt;
    &lt;p&gt;What is the asymptotic behavior? It’s roughly 6αn where α varies with n according to:&lt;/p&gt;
    &lt;p&gt;So this is how long it can take the Turing machine to compute its output. But can we find that output faster, say just by finding a “mathematical formula” for it? For inputs i with some particular forms (like the one above) it is indeed possible to find such formulas:&lt;/p&gt;
    &lt;p&gt;But in the vast majority of cases there doesn’t seem to be any simple mathematical-style formula. And indeed one can expect that this Turing machine is a typical computationally irreducible system: you can always find its output (here the value f[i]) by explicitly running the machine, but there’s no general way to shortcut this, and to systematically get to the answer by some reduced, shorter computation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Runtimes in s = 3, k = 2 Turing Machines&lt;/head&gt;
    &lt;p&gt;We discussed above that out of the 2.99 million possible &lt;/p&gt;
    &lt;p&gt;There are machines that give asymptotically constant runtime&lt;/p&gt;
    &lt;p&gt;with all odd asymptotic runtime values up to 21 (along with 25) being possible:&lt;/p&gt;
    &lt;p&gt;Then there are machines that give asymptotically linear runtimes, with even coefficients from 2 to 20 (along with 24)—for example:&lt;/p&gt;
    &lt;p&gt;By the way, note that (as we mentioned before) some machines realize their worst-case runtimes for many specific inputs, while in other machines such runtimes are rare (here illustrated for machines with asymptotic runtimes 24n):&lt;/p&gt;
    &lt;p&gt;Sometimes there are machines whose worst-case runtimes increase linearly, but in effect with fractional slopes:&lt;/p&gt;
    &lt;p&gt;There are many machines whose worst-case runtimes increase in an ultimately linear way—but with “oscillations”:&lt;/p&gt;
    &lt;p&gt;Averaging out the oscillations gives an overall growth rate of the form αn, where α is an integer or rational number with (as it turns out) denominator 2 or 3; the possible values for α are:&lt;/p&gt;
    &lt;p&gt;There are also machines with worst-case runtimes growing like αn2, with α an integer from 1 to 10 (though missing 7):&lt;/p&gt;
    &lt;p&gt;And then there are a few machines (such as 129559 and 1166261) with cubic growth rates.&lt;/p&gt;
    &lt;p&gt;The next—and, in fact, single largest—group of machines have worst-case runtimes that asymptotically grow exponentially, following linear recurrences. The possible asymptotic growth rates seem to be (ϕ is the golden ratio ):&lt;/p&gt;
    &lt;p&gt;Some particular examples of machines with these growth rates include (we’ll see 5n/2 and 4n examples in the next section):&lt;/p&gt;
    &lt;p&gt;The first of these is machine 1020827, and the exact worst-case runtime for input size n in this case is:&lt;/p&gt;
    &lt;p&gt;The second case shown (machine 117245) has exact worst-case runtime&lt;/p&gt;
    &lt;p&gt;which satisfies the linear recurrence:&lt;/p&gt;
    &lt;p&gt;The third case (machine 1007039) has exact worst-case runtime:&lt;/p&gt;
    &lt;p&gt;It’s notable that in all of these cases, the maximum runtime for input size n occurs for input &lt;/p&gt;
    &lt;p&gt;Continuing and squashing the results, it becomes clear that there’s a nested structure to these patterns:&lt;/p&gt;
    &lt;p&gt;By the way, it’s certainly not necessary that the worst-case runtime must occur at the largest input of a given size. Here’s an example (machine 888388) where that’s not what happens&lt;/p&gt;
    &lt;p&gt;and where in the end the 2n/2 growth is achieved by having the same worst-case runtime for input sizes n and n + 1 for all even n:&lt;/p&gt;
    &lt;p&gt;One feature of everything we’ve seen here is the runtimes we’ve deduced are either asymptotically powers or asymptotically exponentials. There’s nothing in between—for example nothing like nLog[n] or 4Sqrt[n]:&lt;/p&gt;
    &lt;p&gt;No doubt there are Turing machines with such intermediate growth, but apparently none with &lt;/p&gt;
    &lt;head rend="h2"&gt;Functions and Their Runtimes in s = 3, k = 2 Turing Machines&lt;/head&gt;
    &lt;p&gt;As we discussed above, out of the 2.99 million possible &lt;/p&gt;
    &lt;p&gt;The functions computed by the most machines are (where, not surprisingly, the identity function &lt;/p&gt;
    &lt;p&gt;The minimum number of machines that can compute a given function is always 2—because there’s always one machine with a transition, and another with a transition, as in:&lt;/p&gt;
    &lt;p&gt;But altogether there are about 13,000 of these “isolate” machines, where no other &lt;/p&gt;
    &lt;p&gt;So what are these functions—and how long do they take to compute? And remember, these are functions that are computed by isolate machines—so whatever the runtime of those machines is, this can be thought of as defining a lower bound on the runtime to compute that function, at least by any &lt;/p&gt;
    &lt;p&gt;So what are the functions with the longest runtimes computed by isolate machines? The overall winner seems to be the function computed by machine 600720 that we discussed above.&lt;/p&gt;
    &lt;p&gt;Next appears to come machine 589111&lt;/p&gt;
    &lt;p&gt;with its asymptotically 4n runtime:&lt;/p&gt;
    &lt;p&gt;And although the values here, say for &lt;/p&gt;
    &lt;p&gt;Next appear to come machines like 599063&lt;/p&gt;
    &lt;p&gt;with asymptotic &lt;/p&gt;
    &lt;p&gt;Despite the seemingly somewhat regular pattern of values for this function, the machine that computes it is an isolate, so we know that at least among &lt;/p&gt;
    &lt;p&gt;What about the other machines with asymptotically exponential runtimes that we saw in the previous section? Well, the particular machines we used as examples there aren’t even close to isolates. But there are other machines that have the same exponentially growing runtimes, and that are isolates. And, just for once, there’s a surprise.&lt;/p&gt;
    &lt;p&gt;For asymptotic runtime 2n, it turns out that there is just a single isolate machine: 1342057:&lt;/p&gt;
    &lt;p&gt;But look at how simple the function this machine computes is. In fact, &lt;/p&gt;
    &lt;p&gt;But despite the simplicity of this, it still takes the Turing machine worst-case runtime &lt;/p&gt;
    &lt;p&gt;And, yes, after a transient at the beginning, all the machine is ultimately doing is to compute &lt;/p&gt;
    &lt;p&gt;Going on to asymptotic runtimes of the form 3n/2, it turns out there’s only one function for which there’s a machine (1007039) with this asymptotic runtime—and this function can be computed by over a hundred machines, many with faster runtimes, though some with slower (2n) runtimes (e.g. 879123).&lt;/p&gt;
    &lt;p&gt;What about asymptotic runtimes of order ? It’s more or less the same story as with 3n/2. There are 48 functions which can be computed by machines with this worst-case runtime. But in all cases there are also many other machines, with many other runtimes, that compute the same functions.&lt;/p&gt;
    &lt;p&gt;But now there’s another surprise. For asymptotic runtime 2n/2 there are two functions computed only by isolate machines (889249 and 1073017):&lt;/p&gt;
    &lt;p&gt;So, once again, these functions have the feature that they can’t be computed any faster by any other &lt;/p&gt;
    &lt;p&gt;When we looked at &lt;/p&gt;
    &lt;p&gt;Among &lt;/p&gt;
    &lt;p&gt;There are, of course, many more &lt;/p&gt;
    &lt;p&gt;Isolate machines immediately define lower bounds on runtime for the functions they compute. But in general (as we saw above) there can be many machines that compute a given function. For example, as mentioned above, there are 210,792 &lt;/p&gt;
    &lt;p&gt;with asymptotic runtimes ranging from constant to linear, quadratic and exponential. (The most rapidly increasing runtime is ~2n.)&lt;/p&gt;
    &lt;p&gt;For each function that can be computed, there’s a slightly different collection of runtime profiles; here are the ones for the functions computed by the next largest numbers of machines:&lt;/p&gt;
    &lt;head rend="h2"&gt;Can Bigger Machines Compute Functions Faster?&lt;/head&gt;
    &lt;p&gt;We saw above that there are functions which cannot be computed asymptotically faster than particular bounds by, say, any &lt;/p&gt;
    &lt;p&gt;The first thing to say is that (as we discussed before for &lt;/p&gt;
    &lt;p&gt;Among &lt;/p&gt;
    &lt;p&gt;where the exact worst-case runtime is:&lt;/p&gt;
    &lt;p&gt;But now we can ask whether this function can be computed faster by any &lt;/p&gt;
    &lt;p&gt;An example is machine 1069163:&lt;/p&gt;
    &lt;p&gt;We can think of what’s happening as being that we start from the &lt;/p&gt;
    &lt;p&gt;and in effect optimize this by using a slightly more complicated “instruction set”:&lt;/p&gt;
    &lt;p&gt;In looking at &lt;/p&gt;
    &lt;p&gt;As an example, consider the function computed by the isolate machine 1342057:&lt;/p&gt;
    &lt;p&gt;This has asymptotic runtime 4n. But now if we look at &lt;/p&gt;
    &lt;p&gt;There are also machines with linearly and quadratically increasing runtimes—though, confusingly, for the first few input sizes, they seem to be increasing just as fast as our original &lt;/p&gt;
    &lt;p&gt;Here are the underlying rules for these particular Turing machines:&lt;/p&gt;
    &lt;p&gt;And here’s the full spectrum of runtime profiles achieved by &lt;/p&gt;
    &lt;p&gt;There are runtimes that are easy to recognize as exponentials—though with bases like 2,, 3/2, that are smaller than 4. Then there are linear and polynomial runtimes of the kind we just saw. And there’s some slightly exotic “oscillatory” behavior, like with machine 1418699063&lt;/p&gt;
    &lt;p&gt;that seems to settle down to a periodic sequence of ratios, growing asymptotically like 2n/4.&lt;/p&gt;
    &lt;p&gt;What about other functions that are difficult to compute by &lt;/p&gt;
    &lt;p&gt;One of these follows exactly the runtimes of 600720; the other is not the same, but is very close, with about half the runtimes being the same, and the other half having maximal differences that grow linearly with n.&lt;/p&gt;
    &lt;p&gt;And what this means is that—unlike the function computed by &lt;/p&gt;
    &lt;p&gt;Looking at other functions that are “hard to compute” with &lt;/p&gt;
    &lt;head rend="h2"&gt;With a Sufficiently Large Turing Machine…&lt;/head&gt;
    &lt;p&gt;We’ve been talking so far about very small Turing machines—with at most a handful of distinct cases in their rules. But what if we consider much larger Turing machines? Would these allow us to systematically do computations much faster?&lt;/p&gt;
    &lt;p&gt;Given a particular (finite) mapping from input to output values, say&lt;/p&gt;
    &lt;p&gt;it’s quite straightforward to construct a Turing machine&lt;/p&gt;
    &lt;p&gt;whose state transitions in effect just “immediately look up” these values:&lt;/p&gt;
    &lt;p&gt;(If we try to compute a value that hasn’t been defined, the Turing machine will simply not halt.)&lt;/p&gt;
    &lt;p&gt;If we stay with a fixed value of k, then for a “function lookup table” of size v, the number of states we need for a “direct representation” of the lookup table is directly proportional to v. Meanwhile, the runtime is just equal to the absolute lower bound we discussed above, which is linearly proportional to the sizes of input and output.&lt;/p&gt;
    &lt;p&gt;Of course, with this setup, as we increase v we increase the size of the Turing machine. And we can’t guarantee to encode a function defined, say, for all integers, with anything less than an infinite Turing machine.&lt;/p&gt;
    &lt;p&gt;But by the time we’re dealing with an infinite Turing machine we don’t really need to be computing anything; we can just be looking everything up. And indeed computation theory always in effect assumes that we’re limiting the size of our machines. And as soon as we do this, there starts to be all sorts of richness in questions like which functions are computable, and what runtime is required to compute them.&lt;/p&gt;
    &lt;p&gt;In the past, we might just have assumed that there is some arbitrary bound on the size of Turing machines, or, in effect, a bound on their “algorithmic information content” or “program size”. But the point of what we’re doing here is to explore what happens not with arbitrary bounds, but with bounds that are small enough to allow us to do exhaustive empirical investigations.&lt;/p&gt;
    &lt;p&gt;In other words, we’re restricting ourselves to low algorithmic (or program) complexity and&lt;lb/&gt; asking what then happens with time complexity, space complexity, etc. And what we find is that even in that domain, there’s remarkable richness in the behavior we’re able to see. And from the Principle of Computational Equivalence we can expect that this richness is already characteristic of what we’d see even with much larger Turing machines, and thus larger algorithmic complexity. &lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond Binary Turing Machines&lt;/head&gt;
    &lt;p&gt;In everything we’ve done so far, we’ve been looking at “binary” (i.e. &lt;/p&gt;
    &lt;p&gt;The setup we’ve been using translates immediately:&lt;/p&gt;
    &lt;p&gt;The simplest case is &lt;/p&gt;
    &lt;p&gt;Of these machines, 88 always halt—and compute 77 distinct functions. The possible runtimes are:&lt;/p&gt;
    &lt;p&gt;And unlike what we saw even for &lt;/p&gt;
    &lt;p&gt;For s = 2, k = 3, we have &lt;/p&gt;
    &lt;p&gt;In both cases, of the machines that don’t halt, the vast majority become periodic. For &lt;/p&gt;
    &lt;p&gt;Just as for &lt;/p&gt;
    &lt;p&gt;or in squashed form:&lt;/p&gt;
    &lt;p&gt;If we look beyond input 1, we find that about 1.12 million &lt;/p&gt;
    &lt;p&gt;A notable feature is that the tail consists of functions computed by only one machine. In the &lt;/p&gt;
    &lt;p&gt;What about runtimes? The results for &lt;/p&gt;
    &lt;p&gt;In a sense it should not be surprising that there is so much similarity between the behavior of &lt;/p&gt;
    &lt;p&gt;However, if we look not at the kind of “one-sided” (or “halt if you go to the right”) Turing machines we are considering here, but instead at Turing machines where the head can go freely in either direction, then one difference emerges. Starting with a blank tape, all &lt;/p&gt;
    &lt;p&gt;And this fact provides a clue that such a machine (or, actually, the 14 essentially equivalent machines of which this is one example) might be capable of universal computation. And indeed it can be shown that—at least with appropriate (infinite) initial conditions, the machine can successfully be “programmed” to emulate systems that are known to be universal, thereby proving that it itself is universal.&lt;/p&gt;
    &lt;p&gt;How does this machine fare with our one-sided setup? Here’s what it does with the first few inputs:&lt;/p&gt;
    &lt;p&gt;And what one finds is that for any input, the head of the machine eventually goes to the right, so with our one-sided setup we consider the machine to halt:&lt;/p&gt;
    &lt;p&gt;It turns out that the worst-case runtime for input of size n grows according to:&lt;/p&gt;
    &lt;p&gt;But if we look at the function computed by this machine we can ask whether there are ways to compute it faster. And it turns out there are 11 other s = 2, k = 3 machines (though, for example, no &lt;/p&gt;
    &lt;p&gt;one might think they would be simple enough to have shorter runtimes. But in fact in the one-sided setup their behavior is basically identical to our original machine.&lt;/p&gt;
    &lt;p&gt;OK, but what about &lt;/p&gt;
    &lt;head rend="h2"&gt;Recognizable Functions&lt;/head&gt;
    &lt;p&gt;We’ve been talking a lot about how fast Turing machines can compute functions. But what can we say about what functions they compute? With appropriate encoding of inputs and decoding of outputs, we know that (essentially by definition) any computable function can be computed by some Turing machine. But what about the simple Turing machines we’ve been using here? And what about “without encodings”?&lt;/p&gt;
    &lt;p&gt;The way we’ve set things up, we’re taking both the input and the output to our Turing machines to be the sequences of values on their tapes—and we’re interpreting these values as digits of integers. So that means we can think of our Turing machines as defining functions from integers to integers. But what functions are they?&lt;/p&gt;
    &lt;p&gt;Here are two &lt;/p&gt;
    &lt;p&gt;There are a total of 17 &lt;/p&gt;
    &lt;p&gt;Still, for &lt;/p&gt;
    &lt;p&gt;If we restrict ourselves to even inputs, then we can compute &lt;/p&gt;
    &lt;p&gt;Similarly, there are &lt;/p&gt;
    &lt;p&gt;What about other “mathematically simple” functions, say &lt;/p&gt;
    &lt;p&gt;We’ve already seen a variety of examples where our Turing machines can be interpreted as evaluating bitwise functions of their inputs. A more minimal case would be something like a single bitflip—and indeed there is an &lt;/p&gt;
    &lt;p&gt;To be able to flip a higher-order digit, one needs a Turing machine with more states. There are two &lt;/p&gt;
    &lt;p&gt;And in general—as these pictures suggest—flipping the mth bit can be done with a Turing machine with at least &lt;/p&gt;
    &lt;p&gt;What about Turing machines that compute periodic functions? Strict (nontrivial) periodicity seems difficult to achieve. But here, for example, is an &lt;/p&gt;
    &lt;p&gt;With both &lt;/p&gt;
    &lt;p&gt;Another thing one might ask is whether one Turing machine can emulate another. And indeed that’s what we see happening—very directly—whenever one Turing machine computes the same function as another.&lt;/p&gt;
    &lt;p&gt;(We also know that there exist universal Turing machines—the simplest having &lt;/p&gt;
    &lt;head rend="h2"&gt;Empirical Computational Irreducibility&lt;/head&gt;
    &lt;p&gt;Computational irreducibility has been central to much of the science I’ve done in the past four decades or so. And indeed it’s guided our intuition in much of what we’ve been exploring here. But the things we’ve discussed now also allow us to take an empirical look at the core phenomenon of computational irreducibility itself.&lt;/p&gt;
    &lt;p&gt;Computational irreducibility is ultimately about the idea that there can be computations where in effect there is no shortcut: there is no way to systematically find their results except by running each of their steps. In other words, given an irreducible computation, there’s basically no way to come up with another computation that gives the same result, but in fewer steps. Needless to say, if one wants to tighten up this intuitive idea, there are lots of detailed issues to consider. For example, what about just using a computational system that has “bigger primitives”? Like many other foundational concepts in theoretical science, it’s difficult to pin down exactly how one should set things up—so that one doesn’t either implicitly assume what one’s trying to explain, or so restrict things that everything becomes essentially trivial.&lt;/p&gt;
    &lt;p&gt;But using what we’ve done here, we can explore a definite—if restricted—version of computational irreducibility in a very explicit way. Imagine we’re computing a function using a Turing machine. What would it mean to say that that function—and the underlying behavior of the Turing machine that computes it—is computationally irreducible? Essentially it’s that there’s no other faster way to compute that function.&lt;/p&gt;
    &lt;p&gt;But if we restrict ourselves to computation by a certain size of Turing machine, that’s exactly what we’ve studied at great length here. And, for example, whenever we have what we’ve called an “isolate” Turing machine, we know that no other Turing machine of the same size can compute the same function. So that means one can say that the function is computationally irreducible with respect to Turing machines of the given size.&lt;/p&gt;
    &lt;p&gt;How robust is such a notion? We’ve seen examples above where a given function can be computed, say, only in exponential time by an &lt;/p&gt;
    &lt;p&gt;But the important point here is that we can already see a restricted version of computational irreducibility just by looking explicitly at Turing machines of a given size. And this allows us to get concrete results about computational irreducibility, or at least about this restricted version of it.&lt;/p&gt;
    &lt;p&gt;One of the remarkable discoveries in looking at lots of kinds of systems over the years has been just how common the phenomenon of computational irreducibility seems to be. But usually we haven’t had a way to rigorously say that we’re seeing computational irreducibility in any particular case. All we typically know is that we can’t “visually decode” what’s going on, nor can particular methods we try. (And, yes, the fact that a wide variety of different methods almost always agree about what’s “compressible” and what’s not encourages our conclusions about the presence of computational irreducibility.)&lt;/p&gt;
    &lt;p&gt;In looking at Turing machines here, we’re often seeing “visual complexity”, not so much in the detailed—often ponderous—behavior with a particular initial condition, but more, for example, in what we get by plotting function values against inputs. But now we have a more rigorous—if restricted—test for computational irreducibility: we can ask whether the function that’s being computed is irreducible with respect to this size of Turing machine, or, typically equivalently, whether the Turing machine we’re looking at is an isolate.&lt;/p&gt;
    &lt;p&gt;So now we can, for example, explore how common irreducibility defined in this way might be. Here are results for some of the classes of small Turing machines we’ve studied above:&lt;/p&gt;
    &lt;p&gt;And what we see is that—much like our impression from computational systems like cellular automata—computational irreducibility is indeed very common among small Turing machines, where now we’re using our rigorous, if restricted, notion of computational irreducibility.&lt;/p&gt;
    &lt;p&gt;(It’s worth commenting that while “global” features of Turing machines—like the functions they compute—may be computationally irreducible, there can still be lots of computational reducibility in their more detailed properties. And indeed what we’ve seen here is that there are plenty of features of the behavior of Turing machines—like the back-and-forth motion of their heads—that look visually simple, and that we can expect to compute in dramatically faster ways than just running the Turing machine itself.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Nondeterministic (Multiway) Turing Machines&lt;/head&gt;
    &lt;p&gt;So far, we’ve made a fairly extensive study of ordinary, deterministic (“single-way”) Turing machines. But the P vs. NP question is about comparing the capabilities of such deterministic Turing machines with the capabilities of nondeterministic—or multiway—Turing machines.&lt;/p&gt;
    &lt;p&gt;An ordinary (deterministic) Turing machine has a rule such as&lt;/p&gt;
    &lt;p&gt;that specifies a unique sequence of successive configurations for the Turing machine&lt;/p&gt;
    &lt;p&gt;which we can represent as:&lt;/p&gt;
    &lt;p&gt;A multiway Turing machine, on the other hand, can have multiple rules, such as&lt;/p&gt;
    &lt;p&gt;which are applied in all possible ways to generate a whole multiway graph of successive configurations for the Turing machine&lt;/p&gt;
    &lt;p&gt;where we have indicated edges in the multiway graph associated with the application of each rule respectively by and , and where identical Turing machine configurations are merged.&lt;/p&gt;
    &lt;p&gt;Just as we have done for ordinary (deterministic) Turing machines, we take multiway Turing machines to reach a halting configuration whenever the head goes further to the right than it started—though now this may happen on multiple branches—so that the Turing machine in effect can generate multiple outputs.&lt;/p&gt;
    &lt;p&gt;With the way we have set things up, we can think of an ordinary (deterministic) Turing machine as taking an input i and giving as output some value f[i] (where that value might be undefined if the Turing machine doesn’t halt for a given i). In direct analogy, we can think of a multiway Turing machine as taking an input i and giving potentially a whole collection of corresponding outputs:&lt;/p&gt;
    &lt;p&gt;Among the immediate complications is the fact that the machine may not halt, at least on some branches—as happens for input 3 here, indicated by a red dot in the plot above:&lt;/p&gt;
    &lt;p&gt;(In addition, we see that there can be multiple paths that lead to a given output, in effect defining multiple runtimes for that output. There can also be cycles, but in defining “runtimes” we ignore these.)&lt;/p&gt;
    &lt;p&gt;When we construct a multiway graph we are effectively setting up a representation for all possible paths in the evolution of a (multiway) system. But when we talk about nondeterministic evolution we are typically imagining that just a single path is going to be followed, but we don’t know which.&lt;/p&gt;
    &lt;p&gt;Let’s say that we have a multiway Turing machine that for every given input generates a certain set of outputs. If we were to pick just one of the outputs from each of these sets, we would effectively in each case be picking one path in the multiway Turing machine. Or, in other words, we would be “doing a nondeterministic computation”, or in effect getting output from a nondeterministic Turing machine.&lt;/p&gt;
    &lt;p&gt;As an example, let’s take our multiway Turing machine from above. Here is an example of how this machine—thought of as a nondeterministic Turing machine—can generate a certain sequence of output values:&lt;/p&gt;
    &lt;p&gt;Each of these output values is achieved by following a certain path in the multiway graph obtained with each input:&lt;/p&gt;
    &lt;p&gt;Keeping only the path taken (and including the underlying Turing machine configuration) this represents how each output value was “derived”:&lt;/p&gt;
    &lt;p&gt;The length of the path can then be thought of as the runtime required for the nondeterministic Turing machine to reach the output value. (When there are multiple paths to a given output value, we’ll typically consider “the runtime” to be the length of the shortest of these paths.) So now we can summarize the runtimes from our example as follows:&lt;/p&gt;
    &lt;p&gt;The core of the P vs. NP problem is to compare the runtime for a particular function obtained by deterministic and nondeterministic Turing machines.&lt;/p&gt;
    &lt;p&gt;So, for example, given a deterministic Turing machine that computes a certain function, we can ask whether there is a nondeterministic Turing machine which—if you picked the right branch—can compute that same function, but faster.&lt;/p&gt;
    &lt;p&gt;In the case of the example above, there are two possible underlying Turing machine rules indicated by and . For each input we can choose at each step a different rule to apply in order to get to the output:&lt;/p&gt;
    &lt;p&gt;The possibility of using different rules at different steps in effect allows much more freedom in how our computation can be done. The P vs. NP question concerns whether this freedom allows one to fundamentally speed up the computation of a given function.&lt;/p&gt;
    &lt;p&gt;But before we explore that question further, let’s take a look at what multiway (nondeterministic) Turing machines typically do; in other words, let’s study their ruliology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Multiway (Nondeterministic) s = 1, k = 2 Turing Machines&lt;/head&gt;
    &lt;p&gt;As our first example of doing ruliology for multiway Turing machines, let’s consider the case of pairs of &lt;/p&gt;
    &lt;p&gt;Sometimes, as in machine {1,9}, there turns out to be a unique output value for every input:&lt;/p&gt;
    &lt;p&gt;Sometimes, as in machine {5,9}, there is usually a unique value, but sometimes not:&lt;/p&gt;
    &lt;p&gt;Something similar happens with {3,7}:&lt;/p&gt;
    &lt;p&gt;There are cases—like {1,3}—where for some inputs there’s a “burst” of possible outputs:&lt;/p&gt;
    &lt;p&gt;There are also plenty of cases where for some inputs&lt;/p&gt;
    &lt;p&gt;or for all inputs, there are branches that do not halt:&lt;/p&gt;
    &lt;p&gt;What about runtimes? Well, for each possible output in a nondeterministic Turing machine, we can see how many steps it takes to reach that output on any branch of the multiway graph, and we can consider that minimum number to be the “nondeterministic runtime” needed to compute that output.&lt;/p&gt;
    &lt;p&gt;It’s the quintessential setup for NP computations: if you can successfully guess what branch to follow, you can potentially get to an answer quickly. But if you have to explicitly check each branch in turn, that can be a slow process.&lt;/p&gt;
    &lt;p&gt;Here’s an example showing possible outputs and possible runtimes for a sequence of inputs for the {3,7} nondeterministic machine&lt;/p&gt;
    &lt;p&gt;or, combined in 3D:&lt;/p&gt;
    &lt;p&gt;So what functions can a nondeterministic machine like this “nondeterministically” generate? For each input we have to pick one of the possible corresponding (“multiway”) outputs. And in effect the possible functions correspond to possible “threadings” through these values&lt;/p&gt;
    &lt;p&gt;or:&lt;/p&gt;
    &lt;p&gt;To each function one can then associate a “nondeterministic runtime” for each output, here:&lt;/p&gt;
    &lt;head rend="h2"&gt;Nondeterministic vs. Deterministic Machines&lt;/head&gt;
    &lt;p&gt;We’ve seen how a nondeterministic machine can in general generate multiple functions, with each output from the function being associated with a minimum (“nondeterministic”) runtime. But how do the functions that a particular nondeterministic machine can generate compare with the functions that deterministic machines can generate? Or, put another way, given a function that a nondeterministic machine can generate (or “compute”), what deterministic machine is required to compute the same function?&lt;/p&gt;
    &lt;p&gt;Let’s look at the &lt;/p&gt;
    &lt;p&gt;Can we find a deterministic &lt;/p&gt;
    &lt;p&gt;so that it inevitably gives the same results as deterministic machine 3.&lt;/p&gt;
    &lt;p&gt;But (apart from the other trivial case based on following “machine 7” branches) none of the other functions we can generate from this nondeterministic machine can be reproduced by any &lt;/p&gt;
    &lt;p&gt;What about &lt;/p&gt;
    &lt;p&gt;And here are the paths through the multiway graphs for that machine that get to these values&lt;/p&gt;
    &lt;p&gt;with the “paths on their own” being&lt;/p&gt;
    &lt;p&gt;yielding “nondeterministic runtimes”:&lt;/p&gt;
    &lt;p&gt;This is how the deterministic &lt;/p&gt;
    &lt;p&gt;Here are the pair of underlying rules for the nondeterministic machine&lt;/p&gt;
    &lt;p&gt;and here is the deterministic machine that reproduces a particular function it can generate:&lt;/p&gt;
    &lt;p&gt;This example is rather simple, and has the feature that even the deterministic machine always has a very small runtime. But now the question we can ask is whether a function that takes a deterministic machine of a certain class a certain time to compute can be computed in a smaller time if its results are “picked out of” a nondeterministic machine.&lt;/p&gt;
    &lt;p&gt;We saw above that &lt;/p&gt;
    &lt;p&gt;But what about a nondeterministic machine? How fast can this be?&lt;/p&gt;
    &lt;p&gt;It turns out that there are 15 nondeterministic machines based on pairs of &lt;/p&gt;
    &lt;p&gt;Here are the paths within the multiway graph for the nondeterministic machine that are sampled to generate the deterministic Turing machine result:&lt;/p&gt;
    &lt;p&gt;And here are these “paths on their own”:&lt;/p&gt;
    &lt;p&gt;We can compare these with the computations needed in the deterministic machine:&lt;/p&gt;
    &lt;p&gt;With our rendering, the lengths of the nondeterministic paths might look longer. But in fact they are considerably shorter, as we see by plotting them (in orange) along with the deterministic runtimes (in gray):&lt;/p&gt;
    &lt;p&gt;Looking now at the worst-case runtimes for inputs of size n, we get:&lt;/p&gt;
    &lt;p&gt;For the deterministic machine we found above for input size n, this worst-case runtime is given by:&lt;/p&gt;
    &lt;p&gt;But now the runtime in the nondeterministic machine turns out to be:&lt;/p&gt;
    &lt;p&gt;In other words, we’re seeing that nondeterminism makes it substantially faster to compute this particular function—at least by small Turing machines.&lt;/p&gt;
    &lt;p&gt;In a deterministic machine, it’s always the same underlying rule that’s applied at each step. But in a nondeterministic machine with the setup we’re using, we’re independently choosing one of two different rules to apply at each step. The result is that for every function value we compute, we’re making a sequence of choices:&lt;/p&gt;
    &lt;p&gt;And the core question that underlies things like the P vs. NP problem is how much advantage the freedom to make these choices conveys—and whether, for example, it allows us to “nondeterministically” compute in polynomial time what takes more than polynomial (say, exponential) time to compute deterministically.&lt;/p&gt;
    &lt;p&gt;As a first example, let’s look at the function computed by the &lt;/p&gt;
    &lt;p&gt;Well, it turns out that the &lt;/p&gt;
    &lt;p&gt;And indeed, while the deterministic machine takes exponentially increasing runtime, the nondeterministic machine has a runtime that quickly approaches the fixed constant value of 5:&lt;/p&gt;
    &lt;p&gt;But is this somehow trivial? As the plot above suggests, the nondeterministic machine (at least eventually) generates all possible odd output values (and for even input i, also generates &lt;/p&gt;
    &lt;p&gt;What makes the runtime end up being constant, however, is that in this particular case, the output f[i] is always close to i (in fact, &lt;/p&gt;
    &lt;p&gt;There are actually no fewer than &lt;/p&gt;
    &lt;p&gt;And while all of them are in a sense straightforward in their operation, they illustrate the point that even when a function requires exponential time for a deterministic Turing machine, it can require much less time for a nondeterministic machine—and even a nondeterministic machine that has a much smaller rule.&lt;/p&gt;
    &lt;p&gt;What about other cases of functions that require exponential time for deterministic machines? The functions computed by the &lt;/p&gt;
    &lt;p&gt;Something slightly different happens with &lt;/p&gt;
    &lt;head rend="h2"&gt;The Limit of Nondeterminism and the Ruliad&lt;/head&gt;
    &lt;p&gt;A deterministic Turing machine has a single, definite rule that’s applied at each step. In the previous sections we’ve explored what’s in a sense a minimal case of nondeterminism in Turing machines—where we allow not just one, but two different possible rules to be applied at each step. But what if we increase the nondeterminism—say by allowing more possible rules at each step?&lt;/p&gt;
    &lt;p&gt;We’ve seen that there’s a big difference between determinism—with one rule—and even our minimal case of nondeterminism, with two rules. But if we add in, say, a third rule, it doesn’t seem to typically make any qualitative difference. So what about the limiting case of adding in all conceivable rules?&lt;/p&gt;
    &lt;p&gt;We can think of what we get as an “everything machine”—a machine that has every possible rule case for any possible Turing machine, say for &lt;/p&gt;
    &lt;p&gt;Running this “everything machine” for one step starting with input 1 we get:&lt;/p&gt;
    &lt;p&gt;Four of the rule cases just lead back to the initial state. Then of the other four, two lead to halting states, and two do not. Dropping self-loops, going another couple of steps, and using a different graph rendering, we see that outputs 2 and 3 now appear:&lt;/p&gt;
    &lt;p&gt;Here are the results for input 2:&lt;/p&gt;
    &lt;p&gt;So where can the “everything machine” reach, and how long does it take? The answer is that from any input i it can eventually reach absolutely any output value j. The minimum number of steps required (i.e. the minimum path length in the multiway graph) is just the absolute lower bound that we found for runtimes in deterministic machines above:&lt;/p&gt;
    &lt;p&gt;Starting with input 1, the nondeterministic runtime to reach output j is then&lt;/p&gt;
    &lt;p&gt;which grows logarithmically with output value, or linearly with output size.&lt;/p&gt;
    &lt;p&gt;So what this means is that the “everything machine” lets one nondeterministically go from a given input to a given output in the absolutely minimum number of steps structurally possible. In other words, with enough nondeterminism every function becomes nondeterministically “easy to compute”.&lt;/p&gt;
    &lt;p&gt;An important feature of the “everything machine” is that we can think of it as being a fragment of the ruliad. The full ruliad—which appears at the foundations of physics, mathematics and much more—is the entangled limit of all possible computations. There are many possible bases for the ruliad; Turing machines are one. In the full ruliad, we’d have to consider all possible Turing machines, with all possible sizes. The “everything machine” we’ve been discussing here gives us just part of that, corresponding to all possible Turing machine rules with a specific number of states and colors.&lt;/p&gt;
    &lt;p&gt;In representing all possible computations, the ruliad—like the “everything machine”—is maximally nondeterministic, so that it in effect includes all possible computational paths. But when we apply the ruliad in science (and even mathematics) we are interested not so much in its overall form as in particular slices of it which are sampled by observers that, like us, are computationally bounded. And indeed in the past few years it’s become clear that there’s a lot to say about the foundations of many fields by thinking in this way.&lt;/p&gt;
    &lt;p&gt;And one feature of computationally bounded observers is that they’re not maximally nondeterministic. Instead of following all possible paths in the multiway system, they tend to follow specific paths or bundles of paths—for example reflecting the single thread of experience that characterizes our human perception of things. So—when it comes to observers—the “everything machine” is somehow too nondeterministic. An actual (computationally bounded) observer will be concerned with one or just a few “threads of history”. In other words, if we’re interested in slices of the ruliad that observers will sample, what will be relevant is not so much the “everything machine” but rather deterministic machines, or at most machines with the kind of limited nondeterminism that we’ve studied the past few sections.&lt;/p&gt;
    &lt;p&gt;But just how does what the “everything machine” can do compare with what all possible deterministic machines can do? In some ways, this is a core question in the comparison between determinism and nondeterminism. And it’s straightforward to start studying it empirically.&lt;/p&gt;
    &lt;p&gt;For example, here are successive steps in the multiway graph for the (&lt;/p&gt;
    &lt;p&gt;In a sense these pictures illustrate the “reach” of deterministic vs. nondeterministic computation. In this particular case, with &lt;/p&gt;
    &lt;p&gt;For &lt;/p&gt;
    &lt;p&gt;and the values that can be reached by deterministic machines are:&lt;/p&gt;
    &lt;p&gt;But how long does it take to reach these values? This shows as dots the possible (deterministic) runtimes; the filling represents the minimum (nondeterministic) runtimes for the “everything machine”:&lt;/p&gt;
    &lt;p&gt;The most dramatic outlier occurs with value 31, which is reached deterministically only by machine 1447, in 15 steps, but which can be reached in 9 (nondeterministic) steps by the “everything machine”:&lt;/p&gt;
    &lt;p&gt;For &lt;/p&gt;
    &lt;head rend="h2"&gt;What Does It All Mean for P vs. NP?&lt;/head&gt;
    &lt;p&gt;The P vs. NP question asks whether every computation that can be done by any nondeterministic Turing machine with a runtime that increases at most polynomially with input size can also be done by some deterministic Turing machine with a runtime that also increases at most polynomially. Or, put more informally, it asks whether introducing nondeterminism can fundamentally speed up computation.&lt;/p&gt;
    &lt;p&gt;In its full form, this is an infinite question, that talks about limiting behavior over all possible inputs, in all possible Turing machines. But within this infinite question, there are definite, finite subquestions we can ask. And one of the things we’ve done here is in effect to explore some of these questions in an explicit, ruliological way. Looking at these finite subquestions won’t in any direct way be able to resolve the full P vs. NP question.&lt;/p&gt;
    &lt;p&gt;But it can give us important intuition about the P vs. NP question, and what some of the difficulties and subtleties involved in it are. When one analyzes specific, constructed algorithms, it’s common to see that their runtimes vary quite smoothly with input size. But one of the things we’ve seen here is that for arbitrary Turing machines “in the wild”, it’s very typical for the runtimes to jump around in complicated ways. It’s also not uncommon to see dramatic outliers that occur only for very specific inputs.&lt;/p&gt;
    &lt;p&gt;If there was just one outlier, then in the limit of arbitrarily large input size it would eventually become irrelevant. But what if there were an unending sequence of outliers, of unpredictable sizes at unpredictable positions? Ultimately we expect all sorts of computational irreducibility, which in the limit can make it infinitely difficult to determine in any particular case the limiting behavior of the runtime—and, for example, to find out if it’s growing like a polynomial or not.&lt;/p&gt;
    &lt;p&gt;One might imagine, though, that if one looked at enough inputs, enough Turing machines, etc. then somehow any wildness would get in some way averaged out. But our ruliological results don’t encourage that idea. And indeed they tend to show that “there’s always more wildness”, and it’s somehow ubiquitous. One might have imagined that computational irreducibility—or undecidability—would be sufficiently rare that it wouldn’t affect investigations of “global” questions like the P vs. NP one. But our results suggest that, to the contrary, there are all sorts of complicated details and “exceptions” that seem to get in the way of general conclusions.&lt;/p&gt;
    &lt;p&gt;Indeed, there seem to be issues at every turn. Some are related to unexpected behavior and outliers in runtimes. Some are related to the question of whether a particular machine ever even halts at all for certain inputs. And yet others are related to taking limits of sizes of inputs versus sizes of Turing machines, or amounts of nondeterminism. What our ruliological explorations have shown is that such issues are not obscure corner cases; rather they are generic and ubiquitous.&lt;/p&gt;
    &lt;p&gt;One has the impression, though, that they are more pronounced in deterministic than in nondeterministic machines. Nondeterministic machines in some sense “aggregate” over paths, and in doing so, wash out the “computational coincidences” which seem ubiquitous in determining the behavior of deterministic machines.&lt;/p&gt;
    &lt;p&gt;Certainly the specific experiments we’ve done on machines of limited size do seem to support the idea that there are indeed computations that can be done quickly by a nondeterministic machine, but for which in deterministic machines there are for example at least occasional large runtime outliers, which imply longer general runtimes.&lt;/p&gt;
    &lt;p&gt;I had always suspected that the P vs. NP question would ultimately get ensnared in issues of computational irreducibility and undecidability. But from our explicit ruliological explorations we get an explicit sense of how this can happen. Will it nevertheless ultimately be possible to resolve the P vs. NP question with a finite mathematical-style proof based, say, on standard mathematical axioms? The results here make me doubt it.&lt;/p&gt;
    &lt;p&gt;Yes, it will be possible to get at least certain restricted global results—in effect by “mining” pockets of computational reducibility. And, as we already know from what we have seen repeatedly here, it’s also possible to get definite results for, say, specific (ultimately finite) classes of Turing machines.&lt;/p&gt;
    &lt;p&gt;I’ve only scratched the surface here of the ruliological results that can be found. In some cases to find more just requires expending more computer time. In other cases, though, we can expect that new methodologies, particularly around “bulk” automated theorem proving, will be needed.&lt;/p&gt;
    &lt;p&gt;But what we’ve seen here already makes it clear that there is much to be learned by ruliological methods about questions of theoretical computer science—P vs. NP among them. In effect, we’re seeing that theoretical computer science can be done not only “purely theoretically”—say with methods from traditional mathematics—but also “empirically”, finding results and developing intuition by doing explicit computational experiments and enumerations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some Personal Notes&lt;/head&gt;
    &lt;p&gt;My efforts on what I now call ruliology started at the beginning of the 1980s, and in the early years I almost exclusively studied cellular automata. A large part of the reason was just that these were the first types of simple programs I’d investigated, and in them I had made a series of discoveries. I was certainly aware of Turing machines, but viewed them as less connected than cellular automata to my goal of studying actual systems in nature and elsewhere—though ultimately theoretically equivalent.&lt;/p&gt;
    &lt;p&gt;It wasn’t until 1991, when I started systematically studying different types of simple programs as I embarked on my book A New Kind of Science that I actually began to do simulations of Turing machines. (Despite their widespread use in theoretical science for more than half a century, I think almost nobody else—from Alan Turing on—had ever actually simulated them either.) At first I wasn’t particularly enamored of Turing machines. They seemed a little less elegant than mobile automata, and had much less propensity to show interesting and complex behavior than cellular automata.&lt;/p&gt;
    &lt;p&gt;Towards the end of the 1990s, though, I was working to connect my discoveries in what became A New Kind of Science to existing results in theoretical computer science—and Turing machines emerged as a useful bridge. In particular, as part of the final chapter of A New Kind of Science—“The Principle of Computational Equivalence”—I had a section entitled “Undecidability and Intractability”. And in that section I used Turing machines as a way to explore the relation of my results to existing results on computational complexity theory.&lt;/p&gt;
    &lt;p&gt;And it was in the process of that effort that I invented the kind of one-sided Turing machines I’ve used here:&lt;/p&gt;
    &lt;p&gt;I concentrated on the s = 2, k = 2 machines (for some reason I never looked at s = 1, k = 2), and found classes of machines that compute the same function—sometimes at different speeds:&lt;/p&gt;
    &lt;p&gt;And even though the computers I was using at the time were much slower than the ones I use today, I managed to extend what I was doing to s = 3, k = 2. At every turn, though, I came face to face with computational irreducibility and undecidability. I tried quite hard do things like resolve the exact number of distinct functions for &lt;/p&gt;
    &lt;p&gt;Nearly three decades later, I think I finally have the exact number. (Note that some of the details from A New Kind of Science are also different from what I have here, because in A New Kind of Science I included partial functions in my enumeration; here I’m mostly insisting on total functions, that halt and give a definite result for all inputs.)&lt;/p&gt;
    &lt;p&gt;After A New Kind of Science was released in 2002, I made another foray into Turing machines in 2007, putting up a prize on the fifth anniversary of the book for a proof (or refutation) of my suspicion that s = 2, k = 3 machine 596440 was capable of universal computation. The prize was soon won, establishing this machine as the very simplest universal Turing machine:&lt;/p&gt;
    &lt;p&gt;Many years passed. I occasionally suggested projects on Turing machines to students at the summer research program we started in 2003 (more on that later…). And I participated in celebrations of Alan Turing’s centenary in 2012. Then in 2020 we announced the Wolfram Physics Project—and I looked at Turing machines again, now as an example of a computational system that could be encoded with hypergraph rewriting, and studied using physics-inspired causal graphs, etc.:&lt;/p&gt;
    &lt;p&gt;Less than two months after the launch of our Physics Project I was studying what I now call the ruliad—and I decided to use Turing machines as a model for it:&lt;/p&gt;
    &lt;p&gt;A crucial part of this was the idea of multiway Turing machines:&lt;/p&gt;
    &lt;p&gt;I’d introduced multiway systems in A New Kind of Science, and had examples close to multiway Turing machines in the book. But now multiway Turing machines were more central to what I was doing—and in fact I started studying essentially what I’ve here called the “everything machine” (though the details were different, because I wasn’t considering Turing machines that can halt):&lt;/p&gt;
    &lt;p&gt;I also started looking at the comparison between what can be reached deterministically and nondeterministically—and discussed the potential relation of this to the P vs. NP question:&lt;/p&gt;
    &lt;p&gt;By the next year, I was expanding my study of multiway systems, and exploring many different examples—with one of them being multiway Turing machines:&lt;/p&gt;
    &lt;p&gt;Soon I realized that the general approach I was taking could be applied not only to the foundations of physics, but also to foundations of other fields. I studied the foundations of mathematics, of thermodynamics, of machine learning and of biology. But what about the foundations of theoretical computer science?&lt;/p&gt;
    &lt;p&gt;Over the years, I’d explored the ruliology of many kinds of systems studied in theoretical computer science—doing deep dives into combinators for their centenary in 2020, as well as (last year) into lambdas. In all these investigations, I was constantly seeing concrete versions of phenomena discussed in theoretical computer science—even though my emphasis tended to be different. But I was always curious what one might be able to say about central questions in theoretical computer science—like P vs. NP.&lt;/p&gt;
    &lt;p&gt;I had imagined that the principal problem in doing an empirical investigation of something like P vs. NP would just be to enumerate enough cases. But when I got into it, I realized that the shadow of computational irreducibility loomed even larger than I’d imagined—and that even within particular cases it could be irreducibly difficult to figure out what one needed to know about their behavior.&lt;/p&gt;
    &lt;p&gt;Fairly late in the project I was trying to look up some “conventional wisdom” about NP problems. Most of it was couched in rather traditional mathematical terms, and didn’t seem likely to have too much to say about what I was doing. But then I found a paper entitled “Program-size versus Time Complexity: Slowdown and Speed-up Phenomena in the Micro-cosmos of Small Turing Machines”—and I was excited to see that it was following up on what I’d done in A New Kind of Science, and doing ruliology. But then I realized: the lead author of the paper, Joost Joosten, had been an (already-a-professor) student at our summer program in 2009, and I’d in fact suggested the original version of the project (though the paper had taken it further, and in some slightly different directions than I’d anticipated).&lt;/p&gt;
    &lt;p&gt;Needless to say, what I’ve now done here raises a host of new questions, which can now be addressed by future projects done at our summer programs, and beyond….&lt;/p&gt;
    &lt;p&gt;Note: For general historical background see my related writings from 2002 and 2021.&lt;/p&gt;
    &lt;head rend="h2"&gt;Thanks&lt;/head&gt;
    &lt;p&gt;Thanks to Willem Nielsen, Nik Murzin and Brian Mboya of the Wolfram Institute for extensive help. Thanks also to Wolfram Institute affiliate Anneline Daggelinckx, as well as to Richard Assar and Pavel Hajek of the Wolfram Institute for additional help. Work at the Wolfram Institute on this project was supported in part by the John Templeton Foundation.&lt;/p&gt;
    &lt;p&gt;Additional input on the project was provided by Lenore &amp;amp; Manuel Blum, Christopher Gilbert, Josh Grochow, Don Knuth and Michael Sun. Matthew Szudzik also contributed relevant work in 1999 during the development of A New Kind of Science.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46830027</guid><pubDate>Fri, 30 Jan 2026 21:17:21 +0000</pubDate></item><item><title>Iran rounds up thousands in mass arrest campaign after crushing unrest</title><link>https://www.reuters.com/world/middle-east/iran-rounds-up-thousands-mass-arrest-campaign-after-crushing-unrest-sources-say-2026-01-29/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46830593</guid><pubDate>Fri, 30 Jan 2026 22:04:47 +0000</pubDate></item><item><title>Vitamin D supplements cut heart attack risk by 52%. Why?</title><link>https://www.empirical.health/blog/vitamin-d-heart/</link><description>&lt;doc fingerprint="f2e94a07698b437a"&gt;
  &lt;main&gt;
    &lt;p&gt;A new study showed that Vitamin D supplementation can reduce heart attacks by 52%. This is important since heart attacks are the #1 cause of death, about one billion people worldwide are deficient in Vitamin D, and supplementing vitamin D is cheap.&lt;/p&gt;
    &lt;p&gt;TARGET-D is a randomized trial in people with who previously had a heart attack, presented at American Heart Association (AHA) Scientific Sessions. In the study, they measured vitamin D blood levels every 3 months, and adjusted vitamin D3 doses to keep vitamin D levels within a target range of 40–80 ng/mL.&lt;/p&gt;
    &lt;p&gt;Participants in the experiment arm who stayed within 40-80 ng/mL of vitamin D had a 52% lower risk of a repeat heart attack.&lt;/p&gt;
    &lt;p&gt;Why would that be? Vitamin D is not part of the standard six biomarkers for cardiovascular health (blood pressure, ApoB, Lp(a), hs-CRP, eGFR, and A1c) that we’ve written about extensively here.&lt;/p&gt;
    &lt;p&gt;In the rest of this post, we’ll try to explain why. We’ll cover the physiological mechanisms that link Vitamin D₃ and D₂ to heart health, how to measure vitamin D, specific ways to get more vitamin D, and potential future work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mechanisms behind Vitamin D’s impact on heart health&lt;/head&gt;
    &lt;p&gt;Vitamin D comes in two main forms. Vitamin D₃ (cholecalciferol) is made in your skin when UVB sunlight hits it, and also in animal foods. Vitamin D₂ (ergocalciferol) is from plants and fungi.&lt;/p&gt;
    &lt;p&gt;Neither is active on its own, but is converted by the liver and kidneys into a calcitriol, a form that binds to Vitamin D receptors in the nucleus of many cells.&lt;/p&gt;
    &lt;p&gt;Multiple chemical pathways absorb or create vitamin D.&lt;/p&gt;
    &lt;p&gt;Vitamin D then regulates hundreds of genes with broad effects. It increases calcium absorption in the intestines (children without vitamin D get rickets). It regulates adaptive immunity (inflammation). It regulates blood pressure through effects on the renin-angiotensin system.&lt;/p&gt;
    &lt;p&gt;Vitamin D also stabilizes plaques in arteries by reducing macrophage activation. Plaque is the buildup of fats (e.g., cholesterol), immune cells (Macrophages, T-lymphocytes), calcium, and a few other components. Soft plaques have more inflammatory cells and are more vulnerable to rupture. When plaque ruptures and travels to the point where it blocks a coronary artery, that causes a heart attack.&lt;/p&gt;
    &lt;p&gt;One mechanism is that Vitamin D stabilizes plaques by reducing inflammatory cells. Other mechanisms include calcium and blood pressure regulation&lt;/p&gt;
    &lt;p&gt;One way to think about this: vitamin D doesn’t prevent heart disease from starting, but may reduce the chance that existing disease suddenly turns into another heart attack.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Measure Vitamin D&lt;/head&gt;
    &lt;p&gt;Vitamin D levels are measured with a 25-hydroxyvitamin D blood test (sometimes written as 25(OH)D).&lt;/p&gt;
    &lt;p&gt;In the U.S., results are reported in ng/mL. The TARGET-D study aimed for a 40–80 ng/mL range, which is higher than the threshold for “deficiency” but within a safe range for most adults.&lt;/p&gt;
    &lt;p&gt;An Empirical Health membership includes vitamin D testing along with a suite of cardiovascular and nutritional biomarkers (cholesterol, ApoB, Lp(a), hs-CRP, ferritin, eGFR, and more).&lt;/p&gt;
    &lt;head rend="h2"&gt;You can get Vitamin D through sunlight, food, and supplements.&lt;/head&gt;
    &lt;p&gt;There are many sources of vitamin D: food, supplements, sunlight.&lt;/p&gt;
    &lt;p&gt;Most of use could use more vitamin D.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sunlight: Your skin makes vitamin D when exposed to UVB rays from the sun. Even short periods (10-30 minutes with arms and legs exposed) can be enough for many people.This depends on your skin tone, where you live, and the season.&lt;/item&gt;
      &lt;item&gt;Food sources: Fatty fish (like salmon, mackerel, sardines), egg yolks, cod liver oil, and fortified foods (milk, orange juice, some cereals) provide vitamin D. However, diet alone is rarely enough to reach optimal blood levels.&lt;/item&gt;
      &lt;item&gt;Supplements: Vitamin D3 (cholecalciferol) is the preferred supplemental form. Typical doses range from 1,000 to 4,000 IU per day, but individual needs vary.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;TARGET-D study caveats&lt;/head&gt;
    &lt;p&gt;There are two main caveats to the TARGET-D study. First, this was presented at the American Heart Association scientific sessions, but the full manuscript isn’t out yet. It’s possible the results will end up not being statistically significant, having a methodological flaw, and so on. In the presented results, the reduction in heart attack risk was statistically significant but the change in overall death and stroke risk had a p value &amp;gt; 0.05. Second, while Vitamin D seems to be an effective intervention to reduce heart attack risk, we don’t yet know whether Vitamin D is an independent marker of heart disease risk or whether it’s reflecting known mechanisms such as inflammation and calcification.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;In a randomized control trial, adjusting vitamin D supplement dosages based on blood test results led to a 52% reduction in heart attacks. If confirmed in further research, optimizing vitamin D could join cholesterol and blood pressure as a key intervention for reducing heart attack risk, especially for people with a history of heart disease.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get your free 30-day heart health guide&lt;/head&gt;
    &lt;p&gt;Evidence-based steps to optimize your heart health.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46830667</guid><pubDate>Fri, 30 Jan 2026 22:13:05 +0000</pubDate></item><item><title>Show HN: I built an AI conversation partner to practice speaking languages</title><link>https://apps.apple.com/us/app/talkbits-speak-naturally/id6756824177</link><description>&lt;doc fingerprint="f62fead7c3b2d384"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TalkBits â Speak Naturally&lt;/head&gt;
    &lt;head rend="h2"&gt;Practice real conversations&lt;/head&gt;
    &lt;p&gt;Only for iPhone&lt;/p&gt;
    &lt;p&gt;Free Â· InâApp Purchases Â· Designed for iPhone&lt;/p&gt;
    &lt;p&gt;iPhone&lt;/p&gt;
    &lt;p&gt; TalkBits helps you practice real conversations â quickly, naturally, and without pressure. Whether in everyday life, while traveling, or at work, TalkBits helps you speak a language the way itâs actually used. 1. Real Conversations Instead of Lessons TalkBits focuses on natural conversation: Everyday language instead of textbook phrases Short, realistic responses Common expressions and casual speech You learn how people really speak. 2. Speak Press and hold to speak naturally Instant voice responses Hear correct pronunciation in real context 3. Many Languages Practice conversations in: English (US, UK, Australia) German French Spanish Italian Dutch Portuguese Arabic And more European languages 4. Learn Quickly â Speak for Real TalkBits is made for everyday use: 30 seconds or 5 minutes Perfect for on the go No long study sessions Every conversation moves you forward. 5. Intelligent AI Conversation Partner Adapts to your language level Responds naturally and friendly Gently corrects mistakes within the conversation Feels like a real interaction 6. Private &amp;amp; Pressure-Free No public profiles No ratings No awkward situations Practice entirely on your own terms. &lt;/p&gt;
    &lt;head rend="h2"&gt;Ratings &amp;amp; Reviews&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;article&gt;This app hasnât received enough ratings or reviews to display an overview.&lt;/article&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; UI modifications and Bug fixes &lt;/p&gt;
    &lt;p&gt;The developer, Omar Muhammad Omar, indicated that the appâs privacy practices may include handling of data as described below. For more information, see the developerâs privacy policy .&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h2"&gt;Data Not Collected&lt;/head&gt;
        &lt;p&gt;The developer does not collect any data from this app.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Accessibility&lt;/head&gt;
    &lt;p&gt;The developer has not yet indicated which accessibility features this app supports. Learn More&lt;/p&gt;
    &lt;head rend="h2"&gt;Information&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Seller&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Omar Muhammad Omar&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Size&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;28 MB&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Category&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Education&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Compatibility&lt;/item&gt;
        &lt;head class="svelte-lyqho4"&gt;Requires iOS 15.1 or later.&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;iPhone&lt;lb/&gt;Requires iOS 15.1 or later.&lt;/item&gt;
          &lt;item&gt;iPod touch&lt;lb/&gt;Requires iOS 15.1 or later.&lt;/item&gt;
          &lt;item&gt;Mac&lt;lb/&gt;Requires macOS 12.0 or later and a Mac with Apple M1 chip or later.&lt;/item&gt;
          &lt;item&gt;Apple Vision&lt;lb/&gt;Requires visionOS 1.0 or later.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Languages&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;English&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Age Rating&lt;/item&gt;
        &lt;head class="svelte-lyqho4"&gt;4+&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;4+&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;In-App Purchases&lt;/item&gt;
        &lt;head class="svelte-lyqho4"&gt;Yes&lt;/head&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Premium $29.99&lt;/item&gt;
          &lt;item&gt;Basic $14.99&lt;/item&gt;
          &lt;item&gt;Pro $49.99&lt;/item&gt;
          &lt;item&gt;Starter $9.99&lt;/item&gt;
          &lt;item&gt;Plus $19.99&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;item class="svelte-z7zy89" rend="dt-1"&gt;Copyright&lt;/item&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Â© Alemey 2025&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46830698</guid><pubDate>Fri, 30 Jan 2026 22:16:19 +0000</pubDate></item></channel></rss>