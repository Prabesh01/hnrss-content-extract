<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 21 Jan 2026 17:33:58 +0000</lastBuildDate><item><title>Infracost (YC W21) Is Hiring Sr Back End Eng (Node.js+SQL) to Shift FinOps Left</title><link>https://www.ycombinator.com/companies/infracost/jobs/Sr9rmHs-senior-backend-engineer-node-js-sql</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46702045</guid><pubDate>Wed, 21 Jan 2026 07:00:23 +0000</pubDate></item><item><title>The percentage of Show HN posts is increasing, but their scores are decreasing</title><link>https://snubi.net/posts/Show-HN/</link><description>&lt;doc fingerprint="22881157ac720865"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The percentage of Show HN posts is increasing, but their scores are decreasing&lt;/head&gt;
    &lt;p&gt;Last update: 2026-01-14&lt;/p&gt;
    &lt;p&gt;Recently, I felt like I was seeing more âShow HNâ stories, and many of which were generated with LLMs. So I analyzed the data to see if that was true. Also I included the average score per month to see if people enjoy seeing them (because I donât :P).&lt;/p&gt;
    &lt;head rend="h2"&gt;Charts&lt;/head&gt;
    &lt;p&gt;Stories in 2026 was omitted. 1) Itâs only 13 days, 2) Scores are not stable yet.&lt;/p&gt;
    &lt;p&gt;Left axis: &lt;code&gt;show_hn_ratio&lt;/code&gt;(&lt;code&gt;show_hn / story * 100&lt;/code&gt;)&lt;/p&gt;
    &lt;p&gt;Right axis: &lt;code&gt;average_show_hn_score&lt;/code&gt; and &lt;code&gt;average_story_score&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;With LLM timeline&lt;/p&gt;
    &lt;head rend="h2"&gt;Analysis&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Disclaimer: I am neither a data scientist nor a statistician. Some nuances may have been lost in translation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Percentage&lt;/head&gt;
    &lt;p&gt;For about ten years (2012~2022), the percentage of Show HN stories was around 2-3%. Then, with the appearance of LLMs that can code, itâs been increasing. Claude Code and Cursor 1.0 accelerated it even more. As of December 2025, over 12% of all stories are Show HNs. Itâs safe to say that there is a correlation between the increase in Show HN posts and LLM. People can create great things even if they donât know how to code at all.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scores&lt;/head&gt;
    &lt;p&gt;Show HN stories used to receive similar scores (around 15-18) to those of all stories until 2023~2024. However, itâs been declining while percentage of them are going up. As of December 2025, the average Show HN score is 10 points lower (9.04 vs 19.53).&lt;/p&gt;
    &lt;p&gt;Does it mean LLM-generated Show HNs are lower quality? Iâm not sure. Maybe people are tired of seeing too many Show HNs.&lt;/p&gt;
    &lt;p&gt;Also I have no idea why the average score was increased in 2022. A lot of new users?&lt;/p&gt;
    &lt;head rend="h2"&gt;Data and codes&lt;/head&gt;
    &lt;p&gt;You can find python code and csv in https://github.com/plastic041/hackernews.&lt;/p&gt;
    &lt;p&gt;I exported BigQuery hacker news data to csv using this query:&lt;/p&gt;
    &lt;code&gt;SELECT
  `time`,
  `title`,
  `type`,
  `score`,
  `id`
FROM
  `bigquery-public-data.hacker_news.full`
WHERE
  (`type` IN ('story')) and title IS NOT NULL;&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;type&lt;/code&gt; field in BigQuery does not have a &lt;code&gt;show_hn&lt;/code&gt; attribute like the Algolia API, so I lowercased titles and filtered using &lt;code&gt;starts_with("show_hn: ")&lt;/code&gt; to determine if a post is a Show HN story.&lt;/p&gt;
    &lt;p&gt;I didnât commit to the repo the original CSV because it was too big (~400 MB) but you can download it from BigQuery for free (I didnât set billing account). I ran SQL above, exported it to google drive, and downloaded it.&lt;/p&gt;
    &lt;p&gt;I would like to analyze the percentage of Show HN stories generated with LLMs but I couldnât find the way to do this, because many Show HN stories donât mention that theyâve used LLMs in their text.&lt;/p&gt;
    &lt;p&gt;Iâll try to update this article every few months.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46702099</guid><pubDate>Wed, 21 Jan 2026 07:09:03 +0000</pubDate></item><item><title>SETI@home is in hiberation</title><link>https://setiathome.berkeley.edu/</link><description>&lt;doc fingerprint="9ad0fc014f2203f2"&gt;
  &lt;main&gt;
    &lt;p&gt;Thanks to everyone for your support over the years. We encourage you to keep crunching for science.&lt;/p&gt;
    &lt;p&gt;SETI@home is a scientific experiment, based at UC Berkeley, that uses Internet-connected computers in the Search for Extraterrestrial Intelligence (SETI). You can participate by running a free program that downloads and analyzes radio telescope data.&lt;/p&gt;
    &lt;p&gt;Already joined? Log in.&lt;/p&gt;
    &lt;p&gt;SETI@home papers accepted for publication&lt;/p&gt;
    &lt;p&gt;Website outage&lt;lb/&gt; Multiple disk failure resulted in a web site outage. We think we've recovered almost everything from the web site, so it should be back up and running. &lt;lb/&gt; 3 Apr 2025, 20:49:48 UTC · Discuss &lt;/p&gt;
    &lt;p&gt;RIP Jimmy Carter&lt;lb/&gt; Carter wrote the following on June 16, 1977 and placed it in Voyager 1, which is the most distant human-made object from Earth:&lt;lb/&gt; This Voyager spacecraft was constructed by the United States of America. We are a community of 240 million human being among the more than 4 billion who inhabit the planet Earth. We human beings are still divided into nation states, but these states are rapidly becoming a single global civilization.&lt;lb/&gt; We cast this message into the cosmos. It is likely to survive a billion years into our future, when our civilization is profoundly altered and the surface of the Earth may be vastly changed. Of the 200 billion stars in the Milky Way galaxy, some – perhaps many – may have inhabited planet and spacefaring civilizations. If one such civilization intercepts Voyager and can understand these recorded contents, here is our message:&lt;lb/&gt; “This is a present from a small distant world, a token of our sounds, our science, our images, our music, our thoughts and our feelings. We are attempting to survive our time so we may live into yours. We hope someday, having solved the problem we face, to join a community of galactic civilizations. This record represents our hope and our determination, and our good will in a vast and awesome universe.”&lt;lb/&gt; --- Jimmy Carter, President of the United States of America, the White House, June 16, 1977 &lt;lb/&gt; 30 Dec 2024, 9:27:36 UTC · Discuss &lt;/p&gt;
    &lt;p&gt;Nebula progress report&lt;lb/&gt; Check out our latest newsletter: Final update. &lt;lb/&gt; 3 Mar 2023, 4:59:42 UTC · Discuss &lt;/p&gt;
    &lt;p&gt;Citizen Science SETI Project at UCLA.&lt;lb/&gt; Jean Luc Margot, a SETI Researcher at UCLA has started a Citizen Science project at UCLA. Participants will help identify and classify types of Radio Frequency Interference (RFI) seen in the data that they have taken at the Green Bank Telescope. This is an important step in identifying any signals that don't look like RFI.&lt;lb/&gt; You can join at https://www.zooniverse.org/projects/ucla-seti-group/are-we-alone-in-the-universe. &lt;lb/&gt; 15 Feb 2023, 19:40:18 UTC · Discuss &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; ©2026 University of California &lt;lb/&gt; SETI@home and Astropulse are funded by grants from the National Science Foundation, NASA, and donations from SETI@home volunteers. AstroPulse is funded in part by the NSF through grant AST-0307956. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46703301</guid><pubDate>Wed, 21 Jan 2026 09:49:34 +0000</pubDate></item><item><title>I Made Zig Compute 33M Satellite Positions in 3 Seconds. No GPU Required</title><link>https://atempleton.bearblog.dev/i-made-zig-compute-33-million-satellite-positions-in-3-seconds-no-gpu-required/</link><description>&lt;doc fingerprint="ef1e7cfce90c1280"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Made Zig Compute 33 Million Satellite Positions in 3 Seconds. No GPU Required.&lt;/head&gt;
    &lt;p&gt;I've spent the past month optimizing SGP4 propagation and ended up with something interesting: astroz is now the fastest general purpose SGP4 implementation I'm aware of, hitting 11-13M propagations per second in native Zig and ~7M/s through Python with just &lt;code&gt;pip install astroz&lt;/code&gt;. This post breaks down how I got there.&lt;/p&gt;
    &lt;p&gt;A note on "general purpose": heyoka.py can be faster for batch-processing many satellites simultaneously (16M/s vs 7.5M/s). But it's a general ODE integrator with SGP4 as a module, requiring LLVM for JIT compilation and a C++ dependency stack that conda-forge recommends over pip. For time batched propagation, many time points for one satellite, astroz is 2x faster (8.5M/s vs 3.8M/s). Full comparison below. I'm also skipping GPU accelerated SGP4 implementations. They can be faster for massive batch workloads, but require CUDA/OpenCL setup and aren't what I'd consider "general purpose."&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Bother Optimizing SGP4?&lt;/head&gt;
    &lt;p&gt;SGP4 is the standard algorithm for predicting satellite positions from TLE data. It's been around since the 80s and most implementations are straightforward ports of the original reference code. They work fine. You can read the implementation that I followed from SpaceTrack Report No. 3.&lt;/p&gt;
    &lt;p&gt;But "fine" starts to feel slow when you need dense time resolution. Generating a month of ephemeris data at one-second intervals is 2.6 million propagations per satellite. Pass prediction over a ground station network might need sub-second precision across weeks. Trajectory analysis for conjunction screening wants fine-grained time steps to catch close approaches. At 2-3M propagations per second (typical for a good implementation), these workloads take seconds per satellite—that adds up fast when you're doing iterative analysis or building interactive tools.&lt;/p&gt;
    &lt;p&gt;I wanted to see how fast I could make it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Starting Point: Already Faster Than Expected&lt;/head&gt;
    &lt;p&gt;Before I even started thinking about SIMD, the scalar implementation was already matching or beating the Rust sgp4 crate, the fastest open-source implementation I could find (general purpose). I hadn't done anything clever yet; the speed came from design choices that happened to play well with how Zig compiles.&lt;/p&gt;
    &lt;p&gt;Two things mattered most:&lt;/p&gt;
    &lt;p&gt;Branchless hot paths. The SGP4 algorithm has a lot of conditionals. Deep space vs near earth, different perturbation models, and convergence checks in the Kepler solver. I wrote these as branchless expressions where possible, not for performance reasons initially, but because it made the code easier to reason about. It happened to be a happy accident that modern CPUs love predictable instruction streams.&lt;/p&gt;
    &lt;p&gt;Comptime precomputation. Zig's &lt;code&gt;comptime&lt;/code&gt; lets you run arbitrary code at compile time. A lot of SGP4's setup work, ie. gravity constants, polynomial coefficients, derived parameters can be computed once and baked into the binary. No runtime initialization, and no repeated calculations. I didn't have to do anything special. Zig is smart enough where if you mark a variable as &lt;code&gt;const&lt;/code&gt; it treats it as comptime automatically.&lt;/p&gt;
    &lt;code&gt;const j2: comptime_float = 1.082616e-3; // explicit comptime (not needed)
const j2 = 1.082616e-3; // implied comptime because of `const` (what I used)
&lt;/code&gt;
    &lt;p&gt;The result was a scalar implementation running at ~5.2M propagations per second, which was already slightly faster than Rust's ~5.0M, but within a margin of error. But I started to see some room to go faster. SGP4, by design, doesn't rely on state to calculate positions: each satellite and each time point is independent. This algorithm feels tailor made for something I have always been too afraid to try: SIMD.&lt;/p&gt;
    &lt;head rend="h2"&gt;Discovering Zig's SIMD Superpowers&lt;/head&gt;
    &lt;p&gt;I have heard the nightmares about implementing SIMD. Most of the time its never worth it, it adds too much complexity, you have to build for different platforms, and the syntax itself is weird to write and think about.&lt;/p&gt;
    &lt;p&gt;I was pleasantly surprised to learn that Zig, whether on purpose or not, makes SIMD a first class citizen. This is all enabled by a powerful and sane standard library that has builtins that handle the weird stuff for me. Now I just had to tackle the thought process for the basic flow of things in SIMD.&lt;/p&gt;
    &lt;p&gt;I started with this foundation; a simple type declaration:&lt;/p&gt;
    &lt;code&gt;const Vec4 = @Vector(4, f64);
&lt;/code&gt;
    &lt;p&gt;That's all you need to start. I now have a 4-wide vector of 64-bit floats. No intrinsics, no platform detection, no conditional compilation. The LLVM backend handles targeting the right instruction set for wherever the code runs.&lt;/p&gt;
    &lt;p&gt;The builtin operations for vector operations are equally simple:&lt;/p&gt;
    &lt;code&gt;// Broadcast a scalar to all lanes
const twoPiVec: Vec4 = @splat(constants.twoPi);

// Auto-vectorized transcendentals through LLVM
pub fn sinSIMD(x: Vec4) Vec4 {
    return @sin(x);
}

pub fn cosSIMD(x: Vec4) Vec4 {
    return @cos(x);
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;@sin&lt;/code&gt; and &lt;code&gt;@cos&lt;/code&gt; builtins map directly to LLVM intrinsics, which use platform optimal implementations like libmvec on Linux x86_64. No manual work required.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learning to Think in Lanes&lt;/head&gt;
    &lt;p&gt;The arithmetic was easy. What took me a while to internalize was branching.&lt;/p&gt;
    &lt;p&gt;In scalar code, you write &lt;code&gt;if&lt;/code&gt; statements and the CPU takes one path or the other. In SIMD, all four lanes execute together. If lane 0 needs the "true" branch and lane 2 needs the "false" branch, you can't just branch, you have to compute both outcomes and then pick per lane.&lt;/p&gt;
    &lt;p&gt;Here's a concrete example. The scalar SGP4 code has a check like this:&lt;/p&gt;
    &lt;code&gt;// Scalar version
if (eccentricity &amp;lt; 1.0e-4) {
    result = simple_calculation(x);
} else {
    result = complex_calculation(x);
}
&lt;/code&gt;
    &lt;p&gt;In SIMD, this becomes:&lt;/p&gt;
    &lt;code&gt;// SIMD version - compute both, select per-lane
const simple_result = simple_calculation(x);
const complex_result = complex_calculation(x);
const mask = eccentricity &amp;lt; @as(Vec4, @splat(1.0e-4));
const result = @select(f64, mask, simple_result, complex_result);
&lt;/code&gt;
    &lt;p&gt;This felt wasteful at first. Why compute both paths? But modern CPUs are so fast at arithmetic that computing both and selecting is often faster than branch misprediction. Plus, for SGP4, most satellites take the same path anyway, so we're rarely doing truly "wasted" work.&lt;/p&gt;
    &lt;p&gt;The trickier case was convergence loops. SGP4's Kepler solver iterates until each result converges. In scalar code:&lt;/p&gt;
    &lt;code&gt;// Scalar Kepler solver
while (@abs(delta) &amp;gt; tolerance) {
    // iterate...
}
&lt;/code&gt;
    &lt;p&gt;But in SIMD, different lanes converge at different rates. Lane 0 might converge in 3 iterations while lane 3 needs 5. You can't exit early for just one lane. The solution is to track convergence per lane with a mask and use &lt;code&gt;@reduce&lt;/code&gt; to check if everyone's done:&lt;/p&gt;
    &lt;code&gt;// SIMD Kepler solver
var converged: @Vector(4, bool) = @splat(false);
while (!@reduce(.And, converged)) {
    // iterate...
    converged = @abs(delta) &amp;lt;= tolerance_vec;
}
&lt;/code&gt;
    &lt;p&gt;Once I understood this pattern, compute everything, mask the results, reduce to check completion, the rest of the conversion was methodical. I went through the scalar implementation line by line, keeping the original untouched so my test suite could compare outputs.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Three Propagation Modes&lt;/head&gt;
    &lt;p&gt;With the core SIMD patterns figured out, I built three different propagation strategies for different use cases.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Time Batched: &lt;code&gt;propagateV4&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The first question I asked: what's the most common workload? For me, it was generating ephemeris data, and propagating a single satellite across many time points. Pass prediction, trajectory analysis, conjunction screening: they all want dense time series for one object.&lt;/p&gt;
    &lt;p&gt;Time batched propagation processes 4 time points for one satellite simultaneously:&lt;/p&gt;
    &lt;code&gt;pub fn propagateV4(self: *const Sgp4, times: [4]f64) Error![4][2][3]f64 {
    const el = &amp;amp;self.elements;
    const timeV4 = Vec4{ times[0], times[1], times[2], times[3] };

    const secular = updateSecularV4(el, timeV4);
    const nm: Vec4 = @as(Vec4, @splat(el.grav.xke)) / simdMath.pow15V4(secular.a);
    const kepler = solveKeplerV4(el, secular);
    const corrected = applyShortPeriodCorrectionsV4(el, kepler, nm);
    return computePositionVelocityV4(el, corrected);
}
&lt;/code&gt;
    &lt;p&gt;Usage is straightforward:&lt;/p&gt;
    &lt;code&gt;const sat = try Sgp4.init(tle);
const times = [4]f64{ 0.0, 1.0, 2.0, 3.0 }; // minutes since epoch
const results = try sat.propagateV4(times);
// results[0] = position/velocity at t=0, results[1] at t=1, etc.
&lt;/code&gt;
    &lt;p&gt;This mode gave me the biggest initial speedup because it's the most cache friendly: the satellite's orbital elements stay in registers while we compute four outputs.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Satellite Batched: &lt;code&gt;propagateSatellitesV4&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The opposite workload: what if you have many satellites and need their positions at one specific time? Collision screening snapshots, catalog-wide visibility checks, that sort of thing.&lt;/p&gt;
    &lt;p&gt;Satellite batched propagation processes 4 different satellites at the same time point:&lt;/p&gt;
    &lt;code&gt;pub inline fn propagateSatellitesV4(el: *const ElementsV4, tsince: f64) Error![4][2][3]f64 {
    const tsinceVec: Vec4 = @splat(tsince);
    const secular = updateSecularSatV4(el, tsinceVec);
    const nm: Vec4 = el.xke / simdMath.pow15V4(secular.a);
    const kepler = solveKeplerSatV4(el, secular);
    const corrected = applyShortPeriodCorrectionsSatV4(el, kepler, nm);
    return computePositionVelocitySatV4(el, corrected);
}
&lt;/code&gt;
    &lt;p&gt;This required a different data layout. Instead of one satellite with 4 time points, I needed 4 satellites packed together. That's where &lt;code&gt;ElementsV4&lt;/code&gt; comes in. Its a struct where each field is a &lt;code&gt;Vec4&lt;/code&gt; holding values for 4 different satellites. More on that layout later.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Constellation Mode: &lt;code&gt;propagateConstellationV4&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The third workload combines both: propagate many satellites across many time points. This is what the live demo does: 13,000 satellites across 1,440 time points.&lt;/p&gt;
    &lt;p&gt;The naive approach would be: for each satellite, compute all time points. But that thrashes the cache. By the time you finish satellite 1's 1,440 points and move to satellite 2, all the time related data has been evicted.&lt;/p&gt;
    &lt;p&gt;Constellation mode uses cache conscious tiling:&lt;/p&gt;
    &lt;code&gt;// Time tile size tuned for L1 cache (~32KB)
const TILE_SIZE: usize = 64;

// Process in tiles over time to keep data in L1/L2 cache
var timeStart: usize = 0;
while (timeStart &amp;lt; numTimes) {
    const timeEnd = @min(timeStart + TILE_SIZE, numTimes);

    for (batches, 0..) |*batch, batchIdx| {
        for (timeStart..timeEnd) |timeIdx| {
            const satResults = try propagateSatellitesV4(batch, times[timeIdx]);
            // Store results...
        }
    }
    timeStart = timeEnd;
}
&lt;/code&gt;
    &lt;p&gt;The idea here is to process 64 time points for all satellites, then the next 64, and so on. The time values stay hot in L1 cache while we sweep through the satellite batches. The tile size (64) isn't magic, it's roughly &lt;code&gt;L1_size / sizeof(working_data)&lt;/code&gt; rounded to a SIMD-friendly number.&lt;/p&gt;
    &lt;p&gt;In practice, constellation mode is about 15-20% faster than calling satellite batched propagation in a naive loop for large catalogs.&lt;/p&gt;
    &lt;head rend="h2"&gt;The atan2 Problem (and Solution)&lt;/head&gt;
    &lt;p&gt;Here's where things got interesting. SGP4's Kepler solver needs &lt;code&gt;atan2&lt;/code&gt;, but LLVM doesn't provide a vectorized builtin for it. Calling the scalar function would break the SIMD implementation.&lt;/p&gt;
    &lt;p&gt;The solution I picked: a polynomial approximation. The key insight is that for SGP4's accuracy requirements (which are inherently limited by the model), we don't need perfect precision.&lt;/p&gt;
    &lt;code&gt;pub fn atan2SIMD(y: Vec4, x: Vec4) Vec4 {
    const abs_x = @abs(x);
    const abs_y = @abs(y);

    // Keep argument in [0, 1] for better polynomial accuracy
    const max_xy = @max(abs_x, abs_y);
    const min_xy = @min(abs_x, abs_y);
    const t = min_xy / @max(max_xy, @as(Vec4, @splat(1.0e-30)));
    const t2 = t * t;

    const c1: Vec4 = @splat(1.0);
    const c3: Vec4 = @splat(-0.3333314528);
    const c5: Vec4 = @splat(0.1999355085);
    // ... more coefficients

    var atan_t = c17;
    atan_t = atan_t * t2 + c15;
    atan_t = atan_t * t2 + c13;
    // ... Horner's method continues
    atan_t = atan_t * t;

    // Quadrant correction using branchless selects
    const swap_mask = abs_y &amp;gt; abs_x;
    atan_t = @select(f64, swap_mask, halfPiVec - atan_t, atan_t);

    // ... more quadrant handling with @select
    return result;
}
&lt;/code&gt;
    &lt;p&gt;This polynomial approximation is accurate to ~1e-7 radians, which translates to about 10mm position error at LEO distances. That's well within SGP4's inherent accuracy limits. The algorithm itself has kilometers of uncertainty over multi-day propagations built into it.&lt;/p&gt;
    &lt;p&gt;To be honest, this math was tricky for me to wrap my head around. I had to ask AI to help me here because I was really struggling with it.&lt;/p&gt;
    &lt;head rend="h2"&gt;"Struct of Arrays" for Multi Satellite Processing&lt;/head&gt;
    &lt;p&gt;For processing multiple satellites, I use a "struct of arrays" layout:&lt;/p&gt;
    &lt;code&gt;pub const ElementsV4 = struct {
    grav: constants.Sgp4GravityModel,

    // Each field is a Vec4 holding values for 4 satellites
    ecco: Vec4,
    inclo: Vec4,
    nodeo: Vec4,
    argpo: Vec4,
    mo: Vec4,
    // ...

    // Pre-splatted constants (computed once at init)
    xke: Vec4,
    j2: Vec4,
    one: Vec4,
    half: Vec4,
    // ...
};
&lt;/code&gt;
    &lt;p&gt;"Pre-splatting" constants eliminates repeated &lt;code&gt;@splat&lt;/code&gt; calls in the hot path. It's a small optimization, but in code running millions of times per second, everything counts, and its an easy win.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Benchmark Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Native Implementation Comparison (Zig vs Rust)&lt;/head&gt;
    &lt;p&gt;First, let's compare apples to apples. Native compiled implementations:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;astroz (Zig)&lt;/cell&gt;
        &lt;cell role="head"&gt;Rust sgp4&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1 day (minute)&lt;/cell&gt;
        &lt;cell&gt;0.27 ms&lt;/cell&gt;
        &lt;cell&gt;0.31 ms&lt;/cell&gt;
        &lt;cell&gt;1.16x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1 week (minute)&lt;/cell&gt;
        &lt;cell&gt;1.99 ms&lt;/cell&gt;
        &lt;cell&gt;2.04 ms&lt;/cell&gt;
        &lt;cell&gt;1.03x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2 weeks (minute)&lt;/cell&gt;
        &lt;cell&gt;3.87 ms&lt;/cell&gt;
        &lt;cell&gt;4.03 ms&lt;/cell&gt;
        &lt;cell&gt;1.04x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2 weeks (second)&lt;/cell&gt;
        &lt;cell&gt;222 ms&lt;/cell&gt;
        &lt;cell&gt;234 ms&lt;/cell&gt;
        &lt;cell&gt;1.05x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1 month (minute)&lt;/cell&gt;
        &lt;cell&gt;8.37 ms&lt;/cell&gt;
        &lt;cell&gt;8.94 ms&lt;/cell&gt;
        &lt;cell&gt;1.07x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Both implementations achieve around 5M propagations/sec for scalar (single-satellite) processing. The Zig implementation edges out Rust slightly. This is most likely hot path optimizations and &lt;code&gt;comptime&lt;/code&gt; being quite aggressive with its pre compute.&lt;/p&gt;
    &lt;head rend="h3"&gt;Native SIMD Throughput Comparison&lt;/head&gt;
    &lt;p&gt;The real gains come from SIMD. When processing multiple satellites or time points in parallel using &lt;code&gt;@Vector(4, f64)&lt;/code&gt;, throughput jumps to 11-13M propagations/sec, more than 2x faster than scalar implementations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Python Bindings Performance&lt;/head&gt;
    &lt;p&gt;For Python users, here's how astroz compares to python-sgp4:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;astroz&lt;/cell&gt;
        &lt;cell role="head"&gt;python-sgp4&lt;/cell&gt;
        &lt;cell role="head"&gt;Speedup&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;2 weeks (second)&lt;/cell&gt;
        &lt;cell&gt;160 ms&lt;/cell&gt;
        &lt;cell&gt;464 ms&lt;/cell&gt;
        &lt;cell&gt;2.9x&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1 month (minute)&lt;/cell&gt;
        &lt;cell&gt;5.9 ms&lt;/cell&gt;
        &lt;cell&gt;16.1 ms&lt;/cell&gt;
        &lt;cell&gt;2.7x&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Python Bindings Throughput&lt;/head&gt;
    &lt;head rend="h3"&gt;A Note on heyoka.py&lt;/head&gt;
    &lt;p&gt;As mentioned in the intro, heyoka.py deserves attention. Here are the single-threaded benchmarks:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Test&lt;/cell&gt;
        &lt;cell role="head"&gt;heyoka.py&lt;/cell&gt;
        &lt;cell role="head"&gt;astroz&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;8 sats × 1440 times&lt;/cell&gt;
        &lt;cell&gt;16.2M/s&lt;/cell&gt;
        &lt;cell&gt;7.5M/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;1 sat × 1440 times&lt;/cell&gt;
        &lt;cell&gt;3.8M/s&lt;/cell&gt;
        &lt;cell&gt;8.5M/s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;100 sats × 100 times&lt;/cell&gt;
        &lt;cell&gt;15.5M/s&lt;/cell&gt;
        &lt;cell&gt;8.4M/s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;heyoka.py wins on multi satellite batches; astroz wins on time batched workloads. Which one you pick depends on your use case:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How many satellites at once? If you're propagating hundreds of satellites at the same time point (collision screening snapshots), heyoka.py wins this easily.&lt;/item&gt;
      &lt;item&gt;How many time points per satellite? If you're generating ephemerides, predicting passes, or doing trajectory analysis for individual satellites across many time steps, astroz is 2x faster.&lt;/item&gt;
      &lt;item&gt;Do you need easy deployment? astroz is &lt;code&gt;pip install astroz&lt;/code&gt;with just NumPy. heyoka.py requires LLVM and a C++ stack that conda-forge recommends over pip.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Seeing It at Scale&lt;/head&gt;
    &lt;p&gt;Speed numbers are abstract until you see what they enable. When propagation is this fast, you stop thinking about batching and scheduling, you just compute what you need, when you need it.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Live Demo&lt;/head&gt;
    &lt;p&gt;To show what this looks like, I built an interactive Cesium visualization that propagates the entire active catalog from CelesTrak (~13,000 satellites) across 1440 time points (a full day at minute resolution). That's ~19 million propagations completing in about 2.7 seconds. Add ~0.6 seconds for TEME→ECEF coordinate conversion, and you get a full day of orbital data for every tracked satellite in about 3.3 seconds. (This demo runs through Python bindings at ~7M/sec; native Zig hits 11-13M/sec.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking Forward&lt;/head&gt;
    &lt;p&gt;Next up: SDP4 for deep space objects (the current implementation only handles near-earth satellites with periods under 225 minutes), and multithreading to scale across cores. The SIMD work here was single threaded, there's another multiplier waiting.&lt;/p&gt;
    &lt;head rend="h2"&gt;Try It Yourself&lt;/head&gt;
    &lt;p&gt;astroz is available on PyPI:&lt;/p&gt;
    &lt;code&gt;pip install astroz
&lt;/code&gt;
    &lt;p&gt;Or add it to your Zig project:&lt;/p&gt;
    &lt;code&gt;zig fetch --save git+https://github.com/ATTron/astroz/#HEAD
&lt;/code&gt;
    &lt;p&gt;The code is open source on GitHub. Stars, issues, and contributions welcome.&lt;/p&gt;
    &lt;p&gt;Browse the examples to integrate astroz into your own projects, or try the live demo to see it in action.&lt;/p&gt;
    &lt;p&gt;Who Am I?&lt;/p&gt;
    &lt;p&gt;Anthony Templeton is a software engineer passionate about high-performance computing and aerospace applications. You can connect with me on LinkedIn or check out more of my work on GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46703317</guid><pubDate>Wed, 21 Jan 2026 09:51:27 +0000</pubDate></item><item><title>EU–INC – A new pan-European legal entity</title><link>https://www.eu-inc.org/</link><description>&lt;doc fingerprint="a1be153739b2f6ef"&gt;
  &lt;main&gt;
    &lt;p&gt;WHAT IS EUâINC&lt;/p&gt;
    &lt;head rend="h2"&gt;EUâINC â A true pan-European solution&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;One new pan-European legal entity&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;One central EU-level registry&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Standardized investment documents&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Standardized EU-wide stock options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Local taxes &amp;amp; employment&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For every founder&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are already working with Brussels. This can become reality. But we need your help!&lt;/p&gt;
    &lt;p&gt;Read the in-detail proposal, made in collaboration with the best startup legal teams, funds and founders in Europe.&lt;/p&gt;
    &lt;p&gt;Welcome to improving europe&lt;/p&gt;
    &lt;p&gt;Why the EUâINC?&lt;/p&gt;
    &lt;p&gt;Europe has the talent, ambition, and ecosystems to create innovative companies, but fragmentation between European nations is holding us back.&lt;/p&gt;
    &lt;p&gt;"A startup from California can expand and raise money all across the United States. But our companies still face way too many national barriers that make it hard to work Europa-wide, and way too much regulatory burden."&lt;/p&gt;
    &lt;p&gt;Political Will&lt;/p&gt;
    &lt;p&gt;Will this actually happen?&lt;/p&gt;
    &lt;p&gt;Yes! But we need your help!&lt;/p&gt;
    &lt;p&gt;So far, we submitted our proposal to Justice Commissioner McGrath and Startup Commissioner Zaharieva.&lt;/p&gt;
    &lt;p&gt;President Von der Leyen has setup a dedicated working group in the Commission with whom we are in regular contact.&lt;/p&gt;
    &lt;p&gt;Additionally, the European Council and Parliament have each signaled interest in the EUâINC, or what in Brussel is called the "28th regime" (for 28th virtual state).&lt;/p&gt;
    &lt;p&gt;ROADMAP&lt;/p&gt;
    &lt;p&gt;What comes next?&lt;/p&gt;
    &lt;p&gt;The entire community is currently influencing the upcoming European Commission legislative proposal for a pan-European legal entity which is set to be released in Q1 2026. We need your help, see below!&lt;/p&gt;
    &lt;p&gt;Afterwards, the European Parliament and the European Council (made up of the 27 national governments) agree on the legislative details. The final implementation of the EUâINC would then happen in 2027.&lt;/p&gt;
    &lt;p&gt;For more details of what happened so far and what comes next, read our roadmap.&lt;/p&gt;
    &lt;p&gt;Join US&lt;/p&gt;
    &lt;p&gt;How you can help: talk to national politicians and press&lt;/p&gt;
    &lt;p&gt;In Europe, laws are still decided on national level, meaning we need to convince all 27 EU member state governments to back the EUâINC.&lt;/p&gt;
    &lt;p&gt;Thus we need YOU to activate your contacts, talk to your national politicians about the urgency of the EUâINC, talk to the press about how crucial the EUâINC is for European startups.&lt;/p&gt;
    &lt;p&gt;National governments need to understand the necessity of EUâINC for the future of Europe. Read more in FAQ.&lt;/p&gt;
    &lt;p&gt;Press&lt;/p&gt;
    &lt;p&gt;Spreading the word&lt;/p&gt;
    &lt;p&gt;You help us immensely if you share this page with your peers, follow &amp;amp; repost us on X &amp;amp; Linkedin and write about the EUâINC â see the FAQ for more infos.&lt;/p&gt;
    &lt;p&gt;You help us immensely if you share this page with your peers, follow &amp;amp; repost us on X &amp;amp; Linkedin and write about the EU-INC â see the FAQ for more infos.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46703763</guid><pubDate>Wed, 21 Jan 2026 10:49:20 +0000</pubDate></item><item><title>The super-slow conversion of the U.S. to metric (2025)</title><link>https://www.thefabricator.com/thefabricator/blog/testingmeasuring/the-super-slow-conversion-of-the-us-to-metric</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46704223</guid><pubDate>Wed, 21 Jan 2026 11:36:09 +0000</pubDate></item><item><title>Nukeproof: Manifesto for European Data Sovereignty</title><link>https://nukeproof.org/</link><description>&lt;doc fingerprint="7dfebda9b1c62e4c"&gt;
  &lt;main&gt;
    &lt;p&gt;NukeProof® Alliance&lt;/p&gt;
    &lt;head rend="h1"&gt;Manifesto for European Data Sovereignty&lt;/head&gt;
    &lt;p&gt;There’s a lot of talk about European data sovereignty. Rather than add to the chorus of complaints and inactivity, we are building a strategic alliance that will lead Europe to true data independence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Escape the chokehold of hyperscalers&lt;/head&gt;
    &lt;p&gt;European data is increasingly at the mercy of foreign control. Google, Microsoft, and Amazon now account for more than 60% of the cloud market, while Chinese companies are pushing their own interests. Laws such as the US CLOUD Act undermine the very idea of sovereign data, totally disregarding where the data physically resides.&lt;/p&gt;
    &lt;head rend="h2"&gt;It’s time to take back control together&lt;/head&gt;
    &lt;p&gt;Europe has the talent, the technology and the willpower. What we lack is cohesive action.&lt;/p&gt;
    &lt;p&gt;A patchwork of local providers, startups, MSPs, and telcos struggles to compete with global hyperscalers on scale, capability and cost. NukeProof exists to bring these players together, forming a coalition that enables Europe to stand strong on its own terms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Europe is strongest together&lt;/head&gt;
    &lt;p&gt;This is Europe’s moment of truth. Collaboration is part of our DNA, from shared markets and infrastructure to collective regulation and values.&lt;/p&gt;
    &lt;p&gt;NukeProof channels that spirit and unites independent European actors to create a sovereign cloud of our own.&lt;/p&gt;
    &lt;head rend="h2"&gt;Join the NukeProof Alliance&lt;/head&gt;
    &lt;p&gt;Join a new era of European data independence built on cooperation, resilience and control.&lt;/p&gt;
    &lt;p&gt;The time is now.&lt;/p&gt;
    &lt;head rend="h2"&gt;Engagement by Country&lt;/head&gt;
    &lt;p&gt;Share of total engagement score&lt;/p&gt;
    &lt;head rend="h2"&gt;FAQ&lt;/head&gt;
    &lt;p&gt;NukeProof Alliance is an initiative by SpaceTime, a Finnish organisation pushing back on hyperscalers to provide data storage services for European companies on European soil. A patchwork of local providers, startups, MSPs, and telcos struggles to compete with global hyperscalers on scale, capability and cost. NukeProof exists to bring these players together, forming a coalition that enables Europe to stand strong on its own terms.&lt;/p&gt;
    &lt;p&gt;The name NukeProof is an acknowledgement of why the internet was first built: to survive nuclear war. It was decentralised by design and resilient by necessity with no single point of failure. Somewhere along the way, we forgot that.&lt;/p&gt;
    &lt;p&gt;Today, much of the world's digital infrastructure depends on a handful of hyperscalers. When one region goes down, services across continents fail. An architecture designed for resilience has been replaced by fragile concentration. NukeProof returns the focus to the origins of the internet.&lt;/p&gt;
    &lt;p&gt;Your data is increasingly at the mercy of foreign control. Google, Microsoft, and Amazon now account for more than 60% of the cloud market, while Chinese companies are pushing their own interests. Laws such as the US CLOUD Act undermine the very idea of sovereign data, totally disregarding where the data physically resides. NukeProof Alliance is developing a collaborative approach to offer Europe-based cloud services to regain our continent's digital ownership.&lt;/p&gt;
    &lt;p&gt;NukeProof Alliance was started by SpaceTime, a Finland-based storage provider company, and it is open to all companies involved in the data, storage and digital industries.&lt;/p&gt;
    &lt;p&gt;Fill out the form on nukeproof.org. The forms ask about your role, whether you want to sponsor, provide technology, become an advocacy partner or just support the alliance silently, you can be a part of NukeProof. Follow us on LinkedIn, too.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46704310</guid><pubDate>Wed, 21 Jan 2026 11:44:48 +0000</pubDate></item><item><title>Hightouch (YC S19) Is Hiring</title><link>https://hightouch.com/careers</link><description>&lt;doc fingerprint="1afc511d6c36362c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We are a small team of kind and talented people&lt;/head&gt;
    &lt;head rend="h2"&gt;Let’s grow together&lt;/head&gt;
    &lt;p&gt;At Hightouch, we’re committed to helping our customers, business, and employees grow. As a series C startup backed by top investors, we are determined to continuously raise the bar and provide the best product in the market. Grow your career in a fast paced environment that values creative thinking and innovation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our values&lt;/head&gt;
    &lt;head rend="h3"&gt;Forever hungry&lt;/head&gt;
    &lt;p&gt;We are hungry and ambitious. We celebrate our accomplishments, but we’re never fully satisfied. We’re always figuring out how to collectively push ourselves further and do more.If we think we can grow the company 5x this year, the first question should be “why not 10x?”&lt;/p&gt;
    &lt;head rend="h3"&gt;Kindness&lt;/head&gt;
    &lt;p&gt;We want to create an environment where people feel actively welcomed, encouraged, and supported. People who aren’t kind aren’t tolerated — it’s just not worth it.We intrinsically believe in a deeper kindness as a core value, aside from its obvious benefits to the business&lt;/p&gt;
    &lt;head rend="h3"&gt;Efficient execution&lt;/head&gt;
    &lt;p&gt;Speed matters. We don’t have time for endless deliberation — most decisions are two-way doors. Move fast, adapt quickly.We take inspiration from others and don’t innovate where we don’t need to. We communicate clearly because time is precious. We parallelize to the greatest extent possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compassion&lt;/head&gt;
    &lt;p&gt;We listen to everyone and try to put ourselves in their shoes, regardless of our initial reaction to what they say.This applies to everyone — customers, prospects, partners, peers, etc.&lt;/p&gt;
    &lt;head rend="h3"&gt;Impact driven&lt;/head&gt;
    &lt;p&gt;Everyone should be intrinsically motivated by business impact. We minimize distractions and prioritize our time based on what’s actually impactful to the business.We value people at all levels based on their impact above anything else.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raising the bar&lt;/head&gt;
    &lt;p&gt;We have high expectations for performance and believe in having exceptional talent in every position. We understand the value of great people.We look deeper than credentials, prioritize slope over y-intercept, and put in the hard work to find those that truly raise the bar.&lt;/p&gt;
    &lt;head rend="h3"&gt;Humility&lt;/head&gt;
    &lt;p&gt;We are humble. Listening is mission critical — we are open to others’ perspectives and ideas. No work is beneath us. We also believe that humility leads to foresight.If we are not grounded and open minded, we blind ourselves from key opportunities and risks in every aspect of business.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benefits&lt;/head&gt;
    &lt;head rend="h3"&gt;Competitive compensation&lt;/head&gt;
    &lt;p&gt;We offer competitive compensation and meaningful equity. We also offer 401k for our US employees and retirement plans for our international employees.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hub &amp;amp; Remote friendly&lt;/head&gt;
    &lt;p&gt;Join our global team, either remotely or in one of our four in-person offices (San Francisco, NYC, Charlotte, and London). For those near an office, enjoy complimentary lunches Monday through Friday.&lt;/p&gt;
    &lt;head rend="h3"&gt;Flexible PTO&lt;/head&gt;
    &lt;p&gt;Downtime is just as important as on time and your teammates will support you while you relax and recharge.&lt;/p&gt;
    &lt;head rend="h3"&gt;Core benefits&lt;/head&gt;
    &lt;p&gt;For full-time US-based employees, we cover all health benefit premiums, 80% for dependents, and life insurance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parental leave&lt;/head&gt;
    &lt;p&gt;We value and support the family planning process. We provide up to 16 weeks for parents.&lt;/p&gt;
    &lt;head rend="h3"&gt;Professional development&lt;/head&gt;
    &lt;p&gt;We support employees for all learning resources needed to grow in their role (classes, books, conferences, etc).&lt;/p&gt;
    &lt;head rend="h3"&gt;Connectivity&lt;/head&gt;
    &lt;p&gt;We offer a $50 per month cell-phone or wifi connectivity stipend to all employees.&lt;/p&gt;
    &lt;head rend="h3"&gt;Commuter&lt;/head&gt;
    &lt;p&gt;Commuter benefits of $150 are offered to both employees who come into the offices and to remote workers working outside of their home.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open positions&lt;/head&gt;
    &lt;head rend="h3"&gt;Customer Success&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Account Manager&lt;/p&gt;
        &lt;p&gt;New York City, New York&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customer Success Engineer&lt;/p&gt;
        &lt;p&gt;Remote (Europe)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Head of Customer Success Engineering (West)&lt;/p&gt;
        &lt;p&gt;Remote (Mountain/Pacific North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Implementation Manager&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manager, Technical Account Management&lt;/p&gt;
        &lt;p&gt;New York City, New York&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Technical Account Manager&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Engineering&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Backend Engineer, AI Content Agents&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Developer Productivity Engineer&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Forward Deployed Data Scientist&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Full Stack Product Engineer&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Head of Machine Learning&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Machine Learning Engineer, AI Decisioning&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Principal Engineer, Streaming Systems&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Software Engineer, AI Agents&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Software Engineer, Control Plane&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Software Engineer, Customer Studio Backend&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Software Engineer, Distributed Systems&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Software Engineer, Journeys&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Software Engineer, Streaming Systems&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Web Engineer&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Finance / Data / Operations&lt;/head&gt;
    &lt;head rend="h3"&gt;Marketing&lt;/head&gt;
    &lt;head rend="h3"&gt;Partnerships&lt;/head&gt;
    &lt;head rend="h3"&gt;People&lt;/head&gt;
    &lt;head rend="h3"&gt;Sales&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Account Executive (APAC)&lt;/p&gt;
        &lt;p&gt;Remote (APAC)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enterprise Account Executive- East Region&lt;/p&gt;
        &lt;p&gt;Remote (East Coast)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enterprise Account Executive, Mountain West Region&lt;/p&gt;
        &lt;p&gt;Remote (Mountain West)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enterprise Account Executive, North EMEA&lt;/p&gt;
        &lt;p&gt;Remote (Europe)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enterprise Account Executive, South EMEA&lt;/p&gt;
        &lt;p&gt;Remote (Europe)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enterprise Account Executive, West Region&lt;/p&gt;
        &lt;p&gt;Remote (West Coast)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enterprise Solutions Engineer, East&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Enterprise Solutions Engineer, West&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manager, Sales Development&lt;/p&gt;
        &lt;p&gt;Denver, Colorado&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manager, Sales Development&lt;/p&gt;
        &lt;p&gt;New York City, New York&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manager, Solutions Engineering&lt;/p&gt;
        &lt;p&gt;New York, NY or San Francisco, CA&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mid-Market Account Executive, East Region&lt;/p&gt;
        &lt;p&gt;Remote (East Coast)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mid-Market Account Executive, EMEA&lt;/p&gt;
        &lt;p&gt;London, United Kingdom&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Regional VP of Enterprise Sales&lt;/p&gt;
        &lt;p&gt;New York, New York/ Boston, Massachusetts&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;RVP, Mid-Market Sales&lt;/p&gt;
        &lt;p&gt;Remote (North America)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sales Development Representative&lt;/p&gt;
        &lt;p&gt;New York, NY / Denver, CO / San Francisco, CA&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sales Development Representative, EMEA&lt;/p&gt;
        &lt;p&gt;London, United Kingdom&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Solutions Engineer, EMEA&lt;/p&gt;
        &lt;p&gt;Remote Europe&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Solutions Engineer (France)&lt;/p&gt;
        &lt;p&gt;Paris, France&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hightouch has been named America’s #3 best startup employer by Forbes&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46704465</guid><pubDate>Wed, 21 Jan 2026 12:02:09 +0000</pubDate></item><item><title>Vibecoding #2</title><link>https://matklad.github.io/2026/01/20/vibecoding-2.html</link><description>&lt;doc fingerprint="332988960f858b6b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Vibecoding #2&lt;/head&gt;
    &lt;p&gt;I feel like I got substantial value out of Claude today, and want to document it. I am at the tail end of AI adoption, so I don’t expect to say anything particularly useful or novel. However, I am constantly complaining about the lack of boring AI posts, so it’s only proper if I write one.&lt;/p&gt;
    &lt;head rend="h2"&gt;Problem Statement&lt;/head&gt;
    &lt;p&gt;At TigerBeetle, we are big on deterministic simulation testing. We even use it to track performance, to some degree. Still, it is crucial to verify performance numbers on a real cluster in its natural high-altitude habitat.&lt;/p&gt;
    &lt;p&gt; To do that, you need to procure six machines in a cloud, get your custom version of &lt;code&gt;tigerbeetle&lt;/code&gt;
            binary on them, connect cluster’s replicas together and hit them
            with load. It feels like, quarter of a century into the third
            millennium, “run stuff on six machines” should be a problem just a
            notch harder than opening a terminal and typing &lt;code&gt;ls&lt;/code&gt;, but
            I personally don’t know how to solve it without wasting a day. So, I
            spent a day vibecoding my own square wheel.
          &lt;/p&gt;
    &lt;p&gt;The general shape of the problem is that I want to spin a fleet of ephemeral machines with given specs on demand and run ad-hoc commands in a SIMD fashion on them. I don’t want to manually type slightly different commands into a six-way terminal split, but I also do want to be able to ssh into a specific box and poke it around.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solution&lt;/head&gt;
    &lt;p&gt;My idea for the solution comes from these three sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://github.com/catern/rsyscall&lt;/item&gt;
      &lt;item&gt;https://peter.bourgon.org/blog/2011/04/27/remote-development-from-mac-to-linux.html&lt;/item&gt;
      &lt;item&gt;https://github.com/dsherret/dax&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; The big idea of &lt;code&gt;rsyscall&lt;/code&gt; is that you can program
            distributed system in direct style. When programming locally, you do
            things by issuing syscalls:
          &lt;/p&gt;
    &lt;p&gt;This API works for doing things on remote machines, if you specify which machine you want to run the syscall on:&lt;/p&gt;
    &lt;p&gt;Direct manipulation is the most natural API, and it pays to extend it over the network boundary.&lt;/p&gt;
    &lt;p&gt;Peter’s post is an application of a similar idea to a narrow, mundane task of developing on Mac and testing on Linux. Peter suggests two scripts:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;remote-sync&lt;/code&gt; synchronizes a local and remote projects.
            If you run &lt;code&gt;remote-sync&lt;/code&gt; inside &lt;code&gt;~/p/tb&lt;/code&gt;
            folder, then &lt;code&gt;~/p/tb&lt;/code&gt; materializes on the remote machine.
            &lt;code&gt;rsync&lt;/code&gt; does the heavy lifting, and the wrapper script
            implements &lt;code&gt;DWIM&lt;/code&gt; behaviors.
          &lt;/p&gt;
    &lt;p&gt; It is typically followed by &lt;code&gt;remote-run some --command&lt;/code&gt;,
            which runs command on the remote machine in the matching directory,
            forwarding output back to you.
          &lt;/p&gt;
    &lt;p&gt; So, when I want to test local changes to &lt;code&gt;tigerbeetle&lt;/code&gt; on
            my Linux box, I have roughly the following shell session:
          &lt;/p&gt;
    &lt;p&gt; The killer feature is that shell-completion works. I first type the command I want to run, taking advantage of the fact that local and remote commands are the same, paths and all, then hit &lt;code&gt;^A&lt;/code&gt; and prepend &lt;code&gt;remote-run&lt;/code&gt; (in reality, I have
            &lt;code&gt;rr&lt;/code&gt; alias that combines sync&amp;amp;run).
          &lt;/p&gt;
    &lt;p&gt; The big thing here is not the commands per se, but the shift in the mental model. In a traditional ssh &amp;amp; vim setup, you have to juggle two machines with a separate state, the local one and the remote one. With &lt;code&gt;remote-sync&lt;/code&gt;, the state is the same
            across the machines, you only choose whether you want to run
            commands here or there.
          &lt;/p&gt;
    &lt;p&gt;With just two machines, the difference feels academic. But if you want to run your tests across six machines, the ssh approach fails — you don’t want to re-vim your changes to source files six times, you really do want to separate the place where the code is edited from the place(s) where the code is run. This is a general pattern — if you are not sure about a particular aspect of your design, try increasing the cardinality of the core abstraction from 1 to 2.&lt;/p&gt;
    &lt;p&gt; The third component, &lt;code&gt;dax&lt;/code&gt; library, is pretty mundane —
            just a JavaScript library for shell scripting. The notable aspects
            there are:
          &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;JavaScript’s template literals, which allow implementing command interpolation in a safe by construction way. When processing&lt;/p&gt;&lt;code&gt;$`ls ${paths}`&lt;/code&gt;, a string is never materialized, it’s arrays all the way to the&lt;code&gt;exec&lt;/code&gt;syscall ( more on the topic).&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;JavaScript’s async/await, which makes managing concurrent processes (local or remote) natural:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Additionally, deno specifically valiantly strives to impose process-level structured concurrency, ensuring that no processes spawned by the script outlive the script itself, unless explicitly marked&lt;/p&gt;&lt;code&gt;detached&lt;/code&gt;— a sour spot of UNIX.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Combining the three ideas, I now have a deno script, called &lt;code&gt;box&lt;/code&gt;, that provides a multiplexed interface for running
            ad-hoc code on ad-hoc clusters.
          &lt;/p&gt;
    &lt;p&gt;A session looks like this:&lt;/p&gt;
    &lt;p&gt;I like this! Haven’t used in anger yet, but this is something I wanted for a long time, and now I have it&lt;/p&gt;
    &lt;head rend="h2"&gt;Structure&lt;/head&gt;
    &lt;p&gt;The problem with implementing above is that I have zero practical experience with modern cloud. I only created my AWS account today, and just looking at the console interface ignited the urge to re-read The Castle. Not my cup of pu-erh. But I had a hypothesis that AI should be good at wrangling baroque cloud API, and it mostly held.&lt;/p&gt;
    &lt;p&gt;I started with a couple of paragraphs of rough, super high-level description of what I want to get. Not a specification at all, just a general gesture towards unknown unknowns. Then I asked ChatGPT to expand those two paragraphs into a more or less complete spec to hand down to an agent for implementation.&lt;/p&gt;
    &lt;p&gt;This phase surfaced a bunch of unknowns for me. For example, I wasn’t thinking at all that I somehow need to identify machines, ChatGPT suggested using random hex numbers, and I realized that I do need 0,1,2 naming scheme to concisely specify batches of machines. While thinking about this, I realized that sequential numbering scheme also has an advantage that I can’t have two concurrent clusters running, which is a desirable property for my use-case. If I forgot to shutdown a machine, I’d rather get an error on trying to re-create a machine with the same name, then to silently avoid the clash. Similarly, turns out the questions of permissions and network access rules are something to think about, as well as what region and what image I need.&lt;/p&gt;
    &lt;p&gt;With the spec document in hand, I turned over to Claude code for actual implementation work. The first step was to further refine the spec, asking Claude if anything is unclear. There were couple of interesting clarifications there.&lt;/p&gt;
    &lt;p&gt; First, the original ChatGPT spec didn’t get what I meant with my “current directory mapping” idea, that I want to materialize a local &lt;code&gt;~/p/tb/work&lt;/code&gt; as remote &lt;code&gt;~/p/tb/work&lt;/code&gt;, even if
            &lt;code&gt;~&lt;/code&gt; are different. ChatGPT generated an incorrect
            description and an incorrect example. I manually corrected
            example, but wasn’t able to write a concise and correct description.
            Claude fixed that working from the example. I feel like I need to
            internalize this more — for current crop of AI, examples seem to be
            far more valuable than rules.
          &lt;/p&gt;
    &lt;p&gt;Second, the spec included my desire to auto-shutdown machines once I no longer use them, just to make sure I don’t forget to turn the lights off when leaving the room. Claude grilled me on what precisely I want there, and I asked it to DWIM the thing.&lt;/p&gt;
    &lt;p&gt;The spec ended up being 6KiB of English prose. The final implementation was 14KiB of TypeScript. I wasn’t keeping the spec and the implementation perfectly in sync, but I think they ended up pretty close in the end. Which means that prose specifications are somewhat more compact than code, but not much more compact.&lt;/p&gt;
    &lt;p&gt;My next step was to try to just one-shot this. Ok, this is embarrassing, and I usually avoid swearing in this blog, but I just typoed that as “one-shit”, and, well, that is one flavorful description I won’t be able to improve upon. The result was just not good (more on why later), so I almost immediately decided to throw it away and start a more incremental approach.&lt;/p&gt;
    &lt;p&gt;In my previous vibe-post, I noticed that LLM are good at closing the loop. A variation here is that LLMs are good at producing results, and not necessarily good code. I am pretty sure that, if I had let the agent to iterate on the initial script and actually run it against AWS, I would have gotten something working. I didn’t want to go that way for three reasons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spawning VMs takes time, and that significantly reduces the throughput of agentic iteration.&lt;/item&gt;
      &lt;item&gt;No way I let the agent run with a real AWS account, given that AWS doesn’t have a fool-proof way to cap costs.&lt;/item&gt;
      &lt;item&gt;I am fairly confident that this script will be a part of my workflow for at least several years, so I care more about long-term code maintenance, than immediate result.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And, as I said, the code didn’t feel good, for these specific reasons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It wasn’t the code that I would have written, it lacked my character, which made it hard for me to understand it at a glance.&lt;/item&gt;
      &lt;item&gt;The code lacked any character whatsoever. It could have worked, it wasn’t “naively bad”, like the first code you write when you are learning programming, but there wasn’t anything good there.&lt;/item&gt;
      &lt;item&gt;I never know what the code should be up-front. I don’t design solutions, I discover them in the process of refactoring. Some of my best work was spending a quiet weekend rewriting large subsystems implemented before me, because, with an implementation at hand, it was possible for me to see the actual, beautiful core of what needs to be done. With a slop-dump, I just don’t get to even see what could be wrong.&lt;/item&gt;
      &lt;item&gt;In particular, while you are working the code (as in “wrought iron”), you often go back to requirements and change them. Remember that ambiguity of my request to “shut down idle cluster”? Claude tried to DWIM and created some horrific mess of bash scripts, timestamp files, PAM policy and systemd units. But the right answer there was “lets maybe not have that feature?” (in contrast, simply shutting the machine down after 8 hours is a one-liner).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; The incremental approach worked much better, Claude is good at filling-in the blanks. The very first thing I did for &lt;code&gt;box-v2&lt;/code&gt; was manually typing-in:
          &lt;/p&gt;
    &lt;p&gt; Then I asked Claude to complete the &lt;code&gt;CLIParse&lt;/code&gt; function,
            and I was happy with the result. Note
            Show, Don’t Tell
          &lt;/p&gt;
    &lt;p&gt; I am not asking Claude to avoid throwing an exception and fail fast instead. I just give &lt;code&gt;fatal&lt;/code&gt;
            function, and it code-completes the rest.
          &lt;/p&gt;
    &lt;p&gt; I can’t say that the code inside &lt;code&gt;CLIParse&lt;/code&gt; is
            top-notch. I’d probably written something more spartan. But the
            important part is that, at this level, I don’t care. The abstraction
            for parsing CLI arguments feel right to me, and the details I can
            always fix later. This is how this overall vibe-coding session
            transpired — I was providing structure, Claude was painting by the
            numbers.
          &lt;/p&gt;
    &lt;p&gt; In particular, with that CLI parsing structure in place, Claude had little problem adding new subcommands and new arguments in a satisfactory way. The only snag was that, when I asked to add an optional path to &lt;code&gt;sync&lt;/code&gt;, it went with &lt;code&gt;string |
              null&lt;/code&gt;, while I strongly prefer &lt;code&gt;string |
              undefined&lt;/code&gt;. Obviously, its better to pick your null in
            JavaScript and stick with it. The fact that &lt;code&gt;undefined&lt;/code&gt;
            is unavoidable predetermines the winner. Given that the argument was
            added as an incremental small change, course-correcting was trivial.
          &lt;/p&gt;
    &lt;p&gt; The null vs undefined issue perhaps illustrates my complaint about the code lacking character. &lt;code&gt;| null&lt;/code&gt; is the default non-choice. &lt;code&gt;|
              undefined&lt;/code&gt; is an insight, which I personally learned from VS
            Code LSP implementation.
          &lt;/p&gt;
    &lt;p&gt;The hand-written skeleton/vibe-coded guts worked not only for the CLI. I wrote&lt;/p&gt;
    &lt;p&gt;and then asked Claude to write the body of a particular function according to the SPEC.md.&lt;/p&gt;
    &lt;p&gt; Unlike with the CLI, Claude wasn’t able to follow this pattern itself. With one example it’s not obvious, but the overall structure is that &lt;code&gt;instanceXXX&lt;/code&gt; is the AWS-level operation on a
            single box, and
            &lt;code&gt;mainXXX&lt;/code&gt; is the CLI-level control flow that deals with
            looping and parallelism. When I asked Claude to implement &lt;code&gt;box
              run&lt;/code&gt;, without myself doing the &lt;code&gt;main&lt;/code&gt; /
            &lt;code&gt;instance&lt;/code&gt; split, Claude failed to noticed it and needed
            a course correction.
          &lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;However, Claude was massively successful with the actual logic. It would have taken me hours to acquire specific, non-reusable knowledge to write:&lt;/p&gt;
    &lt;p&gt;I want to be careful — I can’t vouch for correctness and especially completeness of the above snippet. However, given that the nature of the problem is such that I can just run the code and see the result, I am fine with it. If I were writing this myself, trial-and-error would totally be my approach as well.&lt;/p&gt;
    &lt;p&gt;Then there’s synthesis — with several instance commands implemented, I noticed that many started with querying AWS to resolve symbolic machine name, like “1”, to the AWS name/IP. At that point I realized that resolving symbolic names is a fundamental part of the problem, and that it should only happen once, which resulting in the following refactored shape of the code:&lt;/p&gt;
    &lt;p&gt;Claude was ok with extracting the logic, but messed up the overall code layout, so the final code motions were on me. “Context” arguments go first, not last, common prefix is more valuable than common suffix because of visual alignment.&lt;/p&gt;
    &lt;p&gt;The original “one-shotted” implementation also didn’t do up-front querying. This is an example of a shape of a problem I only discover when working with code closely.&lt;/p&gt;
    &lt;p&gt;Of course, the script didn’t work perfectly the first time and we needed quite a few iterations on the real machines both to fix coding bugs, as well gaps in the spec. That was an interesting experience of speed-running rookie mistakes. Claude made naive bugs, but was also good at fixing them.&lt;/p&gt;
    &lt;p&gt; For example, when I first tried to &lt;code&gt;box ssh&lt;/code&gt; after &lt;code&gt;box create&lt;/code&gt;, I got an error. Pasting it into Claude
            immediately showed the problem. Originally, the code was doing
            &lt;code&gt;aws ec2 wait
                instance-running&lt;/code&gt;
            and not
            &lt;code&gt;aws ec2 wait
                instance-status-ok&lt;/code&gt;.
          &lt;/p&gt;
    &lt;p&gt;The former checks if instance is logically created, the latter waits until the OS is booted. It makes sense that these two exist, and the difference is clear (and its also clear that OS booted != SSH demon started). Claude’s value here is in providing specific names for the concepts I already know to exist.&lt;/p&gt;
    &lt;p&gt; Another fun one was about the disk. I noticed that, while the instance had an SSD, it wasn’t actually used. I asked Claude to mount it as home, but that didn’t work. Claude immediately asked me to run &lt;code&gt;$ box run 0 cat
                /var/some/unintuitive/long/path.log&lt;/code&gt;
            and that log immediately showed the problem. This is remarkable! 50%
            of my typical Linux debugging day is wasted not knowing that a
            useful log exists, and the other 50% is for searching for the log I
            know should exist somewhere.
          &lt;/p&gt;
    &lt;p&gt; After the fix, I lost the ability to SSH. Pasting the error immediately gave the answer — by mounting over &lt;code&gt;/home&lt;/code&gt;,
            we were overwriting ssh keys configured prior.
          &lt;/p&gt;
    &lt;p&gt;There were couple of more iterations like that. Rookie mistakes were made, but they were debugged and fixed much faster than my personal knowledge allows (and again, I feel that is trivia knowledge, rather than deep reusable knowledge, so I am happy to delegate it!).&lt;/p&gt;
    &lt;p&gt;It worked satisfactorily in the end, and, what’s more, I am happy to maintain the code, at least to the extent that I personally need it. Kinda hard to measure productivity boost here, but, given just the sheer number of CLI flags required to make this work, I am pretty confident that time was saved, even factoring the writing of the present article!&lt;/p&gt;
    &lt;head rend="h2"&gt;Coda&lt;/head&gt;
    &lt;p&gt;I’ve recently read The Art of Doing Science and Engineering by Hamming (of distance and code), and one story stuck with me:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46704943</guid><pubDate>Wed, 21 Jan 2026 12:46:27 +0000</pubDate></item><item><title>Nested Code Fences in Markdown</title><link>https://susam.net/nested-code-fences.html</link><description>&lt;doc fingerprint="a0821a8b3cfb87a4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nested Code Fences in Markdown&lt;/head&gt;
    &lt;p&gt;Today, we will meet a spiky-haired nerd named Corey Dumm, who normally lives within Markdown code fences. We will get to know him a bit, smile with him when his fences hold and weep quietly when misfortune strikes.&lt;/p&gt;
    &lt;p&gt;One of the caveats of the Markdown universe is the wide variety of Markdown implementations available. In these parallel universes, the rules of Markdown rendering differ subtly. In this post, we will focus only on the CommonMark specification. Since GitHub Flavoured Markdown (GFM) is a strict superset of CommonMark, whatever we discuss here applies equally well to both CommonMark and GFM.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Basic Code Fences&lt;/head&gt;
    &lt;p&gt;Corey had a knack for working with computers ever since he was a kid.&lt;/p&gt;
    &lt;code&gt;Corey at his computer:

```
(o_o)--.|[_]|
```&lt;/code&gt;
    &lt;p&gt;Everything was perfect in Corey's world. The CommonMark renderer would convert the Markdown above to the following HTML:&lt;/p&gt;
    &lt;p&gt;Corey at his computer:&lt;/p&gt;
    &lt;code&gt;(o_o)--.|[_]|
&lt;/code&gt;
    &lt;head&gt;View HTML&lt;/head&gt;
    &lt;code&gt;&amp;lt;p&amp;gt;Corey at his computer:&amp;lt;/p&amp;gt;
&amp;lt;pre&amp;gt;&amp;lt;code&amp;gt;(o_o)--.|[_]|
&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;&lt;/code&gt;
    &lt;p&gt;At this point, all was well. Corey grew quickly. Before long, he had a head full of spiky hair. Then the fences began to matter.&lt;/p&gt;
    &lt;code&gt;Corey, all grown up:

```
 ```
(o_o)--.|[_]|
```&lt;/code&gt;
    &lt;p&gt;Let us see how this renders. I must warn you that during the Markdown-to-HTML translation, Corey loses his hair. Some viewers may find the following scene disturbing. Viewer discretion is advised. Here is the rendered HTML:&lt;/p&gt;
    &lt;p&gt;Corey, all grown up:&lt;/p&gt;
    &lt;p&gt;(o_o)--.|[_]|&lt;/p&gt;
    &lt;head&gt;View HTML&lt;/head&gt;
    &lt;code&gt;&amp;lt;p&amp;gt;Corey, all grown up:&amp;lt;/p&amp;gt;
&amp;lt;pre&amp;gt;&amp;lt;code&amp;gt;&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;
&amp;lt;p&amp;gt;(o_o)--.|[_]|&amp;lt;/p&amp;gt;
&amp;lt;pre&amp;gt;&amp;lt;code&amp;gt;&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;&lt;/code&gt;
    &lt;p&gt;Corey's hair is gone! What a catastrophic accident! Corey is alright, though. He is still quite afraid of Markdown fences, but otherwise well and bouncing back. Why did this happen? The second set of triple backticks immediately ends the fenced code block started by the first set of triple backticks. As a result, Corey's smiley face ends up outside the fenced code block. The triple backticks that were once Corey's hair are now woven into the fabric of the surrounding HTML. Fortunately, CommonMark offers a few ways to avoid such accidents.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fancy Code Fences&lt;/head&gt;
    &lt;p&gt;In CommonMark, there are two main ways to include triple backticks within fenced code blocks. First, we can use tildes as the code fence:&lt;/p&gt;
    &lt;code&gt;Corey, all grown up:

~~~
 ```
(o_o)--.|[_]|
~~~&lt;/code&gt;
    &lt;p&gt;In fact, a code fence need not consist of exactly three backticks or tildes. Any number of backticks or tildes is allowed, as long as that number is at least three. The following is therefore equivalent:&lt;/p&gt;
    &lt;code&gt;Corey, all grown up:

~~~~~
 ```
(o_o)--.|[_]|
~~~~~&lt;/code&gt;
    &lt;p&gt;And so is this:&lt;/p&gt;
    &lt;code&gt;Corey, all grown up:

`````
 ```
(o_o)--.|[_]|
`````&lt;/code&gt;
    &lt;p&gt;All three examples render like this:&lt;/p&gt;
    &lt;p&gt;Corey, all grown up:&lt;/p&gt;
    &lt;code&gt; ```
(o_o)--.|[_]|
&lt;/code&gt;
    &lt;head&gt;View HTML&lt;/head&gt;
    &lt;code&gt;&amp;lt;p&amp;gt;Corey, all grown up:&amp;lt;/p&amp;gt;
&amp;lt;pre&amp;gt;&amp;lt;code&amp;gt; ```
(o_o)--.|[_]|
&amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt;&lt;/code&gt;
    &lt;p&gt;No hair is lost in translation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basic Code Spans&lt;/head&gt;
    &lt;p&gt;A similar problem arises with inline code spans. Most Markdown users know to use backticks to delimit inline code spans. For example:&lt;/p&gt;
    &lt;code&gt;An old picture of Corey at his computer: `(o_o)--.|[_]|`&lt;/code&gt;
    &lt;p&gt;This produces the following output:&lt;/p&gt;
    &lt;p&gt;An old picture of Corey at his computer: &lt;code&gt;(o_o)--.|[_]|&lt;/code&gt;&lt;/p&gt;
    &lt;head&gt;View HTML&lt;/head&gt;
    &lt;code&gt;&amp;lt;p&amp;gt;An old picture of Corey at his computer: &amp;lt;code&amp;gt;(o_o)--.|[_]|&amp;lt;/code&amp;gt;&amp;lt;/p&amp;gt;&lt;/code&gt;
    &lt;p&gt;However, what do we do when we need to put Corey's dear friend Becky Trace within an inline code span? Becky has short, straight hair tucked neatly on either side of her face. Here's a picture of her:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;`(o_o)`&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;I believe you can already see the difficulty here. Inline code spans use backticks as delimiters. So when we put Becky within a code span, the first backtick in Corey's face would terminate the code span immediately and then the rest of Becky would lie outside it. CommonMark offers solutions for this kind of situation as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fancy Code Spans&lt;/head&gt;
    &lt;p&gt; An inline code span delimiter need not consist of exactly one backtick. It can consist of any number of backticks. So &lt;code&gt;`foo`&lt;/code&gt; and &lt;code&gt;``foo``&lt;/code&gt; produce identical HTML.
  There is another important but less well-known detail.  When the
  text inside an inline code span begins and ends with spaces, one
  space is removed from each end before rendering.  So
  &lt;code&gt;`foo`&lt;/code&gt; and &lt;code&gt;` foo `&lt;/code&gt; are equivalent.
  Therefore, when we need to put backticks within an inline code span,
  we can start the code span using multiple backticks and a space.
  For example:
&lt;/p&gt;
    &lt;code&gt;Meet Corey's friend Becky Trace: `` `(o_o)` ``&lt;/code&gt;
    &lt;p&gt;Here is the rendered output:&lt;/p&gt;
    &lt;p&gt;Meet Corey's friend Becky Trace: &lt;code&gt;`(o_o)`&lt;/code&gt;&lt;/p&gt;
    &lt;head&gt;View HTML&lt;/head&gt;
    &lt;code&gt;&amp;lt;p&amp;gt;Meet Corey's friend Becky Trace: &amp;lt;code&amp;gt;`(o_o)`&amp;lt;/code&amp;gt;&amp;lt;/p&amp;gt;&lt;/code&gt;
    &lt;p&gt;Becky has her hair intact too. We have avoided the mishap that once caused great distress to Corey. That, my friends, is how backticks survive nesting in Markdown.&lt;/p&gt;
    &lt;head rend="h2"&gt;Specification&lt;/head&gt;
    &lt;p&gt;Before I finish this post, let us take a look at the CommonMark specification to see where these details are defined. The excerpts quoted below are taken from CommonMark Spec Version 0.30, which is by now over four years old.&lt;/p&gt;
    &lt;p&gt;From section 4.5 Fenced Code Blocks:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A code fence is a sequence of at least three consecutive backtick characters (&lt;/p&gt;&lt;code&gt;`&lt;/code&gt;) or tildes (&lt;code&gt;~&lt;/code&gt;). (Tildes and backticks cannot be mixed.)&lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;The content of the code block consists of all subsequent lines, until a closing code fence of the same type as the code block began with (backticks or tildes), and with at least as many backticks or tildes as the opening code fence.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;From section 6.1 Code Spans:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A backtick string is a string of one or more backtick characters (&lt;/p&gt;&lt;code&gt;`&lt;/code&gt;) that is neither preceded nor followed by a backtick.&lt;p&gt;A code span begins with a backtick string and ends with a backtick string of equal length. The contents of the code span are the characters between these two backtick strings, normalized in the following ways:&lt;/p&gt;&lt;item&gt;First, line endings are converted to spaces.&lt;/item&gt;&lt;item&gt;If the resulting string both begins and ends with a space character, but does not consist entirely of space characters, a single space character is removed from the front and back. This allows you to include code that begins or ends with backtick characters, which must be separated by whitespace from the opening or closing backtick strings.&lt;/item&gt;&lt;/quote&gt;
    &lt;p&gt;I hope these little nuggets of Markdown trivialities will one day prove useful in your own Markdown misfortunes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46705201</guid><pubDate>Wed, 21 Jan 2026 13:08:35 +0000</pubDate></item><item><title>The first 100 days as a Renovate maintainer</title><link>https://www.jvt.me/posts/2026/01/21/renovate-100-days/</link><description>&lt;doc fingerprint="3f563115cdf74736"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The first 100 days as a Renovate maintainer: the shocking inside view of a popular Open Source project&lt;/head&gt;
    &lt;p&gt;Roughly 100 days ago, I joined Mend to work on the Renovate project as a maintainer and community manager. In a vague homage to the style of politics' "first hundred days", I'll talk about some things that have happened since I've joined the project, and some of the interesting (and mundane) things I've learned since joining the project.&lt;/p&gt;
    &lt;p&gt;This was originally going to be a talk presented at FOSDEM 2026, but there was a lot of strong competition to the half-day Package Management track so I didn't get through to speaking.&lt;/p&gt;
    &lt;p&gt;Instead of losing out on the opportunity to share this, I thought I'd write it up as a blog post instead, as well as it being quite a nice opportunity to reflect back on the last few months.&lt;/p&gt;
    &lt;p&gt;Note that this post is focussing on the Mend Renovate CLI, the Open Source project that is commonly referred to as Renovate, and I won't be going into anything internal to Mend.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's Renovate?&lt;/head&gt;
    &lt;p&gt;First, before we dig into the learnings I've had, let's first explore what Renovate is. It's likely you have some inkling if you're reading this, but it's worth making sure you have the context before reading further.&lt;/p&gt;
    &lt;p&gt;There's a good primer on what Renovate is on a recent post of mine, but a good TL;DR is that Renovate is an Open Source project (&lt;code&gt;AGPL-3.0-only&lt;/code&gt;) owned by Mend, which boasts the best support + extensibility in the ecosystem for dependency updates, and we had over 300 human contributors and 1599 releases last year alone.&lt;/p&gt;
    &lt;p&gt;Since starting the project in 2017, we've gone through a few versions of how we operate the project.&lt;/p&gt;
    &lt;p&gt;Right now, we have three classes of folks who interact with the project:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Maintainers: the team who have final say on the direction of the project and new features, and whom have merge rights&lt;/item&gt;
      &lt;item&gt;Contributors: people who regularly contribute to Renovate's features, fix bugs, improve documentation and help answer user questions&lt;/item&gt;
      &lt;item&gt;Users: the folks who use Renovate, or are responsible for their Renovate deployment. They may infrequently raise PRs for documentation of bug fixes&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Shipping despite a tiny team&lt;/head&gt;
    &lt;p&gt;As noted in my recent post about why we use GitHub Discussions as our triage process, I noted that we optimise for the people doing the work and making sure that Maintainers and Contributors are able to do their best work.&lt;/p&gt;
    &lt;p&gt;The most surprising thing I've found since joining the project is seeing how lean the maintainers team is. I felt like I did intellectually know that, but seeing it in practice has been another thing.&lt;/p&gt;
    &lt;p&gt;Within Renovate's maintainer team are 3 of us - Michael Kriese (who is contracted through Mend, but also does Renovate-y things in his free time), Sebastian Poxhofer (an external maintainer who is 100% not associated with Mend and works independently) and myself (who is 100% paid by Mend).&lt;/p&gt;
    &lt;p&gt;Alongside the maintainers, we have two contributors who are paid by Mend to work part-time on the project - Rahul Gautam Singh and Sergei Zharinov, who do great work on Issues and Discussions that come in from the community, as well as work coming in from Mend customers.&lt;/p&gt;
    &lt;p&gt;We also have a number of contributors who have Triage access on the project and get involved when they're able to, but there's no expectation of any time, given it's a volunteer basis.&lt;/p&gt;
    &lt;p&gt;When you put together the size of the team behind Renovate (in and outside of Mend), you have to admit it's impressive how we've been able to ship all these changes over 2025!&lt;/p&gt;
    &lt;p&gt;Especially when you also factor in the fact that none of the maintainers nor contributors work full-time on Renovate. I try to spend at least 50% of my time on the Open Source project, but it's worth bearing in mind.&lt;/p&gt;
    &lt;p&gt;In the first 100 days since I joined, we've had:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;95 contributors (excluding &lt;code&gt;renovate[bot]&lt;/code&gt;and&lt;code&gt;github-actions[bot]&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;187 contributions from me&lt;/item&gt;
      &lt;item&gt;735 from &lt;code&gt;renovate[bot]&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;314 from contributors (including me)&lt;/item&gt;
      &lt;item&gt;419 releases&lt;/item&gt;
      &lt;item&gt;1 major release&lt;/item&gt;
      &lt;item&gt;Renovate's GitHub repo hit 20k stars&lt;/item&gt;
      &lt;item&gt;Renovate GitHub repo hit 40k Issues/PRs/Discussions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(these aren't things that I'm taking credit for being responsible for - aside from my own contributions - but I'm showing how much we've got done in a small period of time)&lt;/p&gt;
    &lt;p&gt;So with all these changes coming in, how do we keep on top of them?&lt;/p&gt;
    &lt;p&gt;As I'll talk about a bit more, community management for our Discussions is primarily taken by Rahul and I, working with users to help answer their questions.&lt;/p&gt;
    &lt;p&gt;When things promote up to Pull Requests, a mix of Michael and I will do a lot of the code review (including my many PRs!), and merge them as they're ready to be shipped.&lt;/p&gt;
    &lt;p&gt;Dependency bump PRs from Renovate are automagically approved + merged through PR-based automerge, allowing us to keep an eye on changes going in, and so we can action a failing build, but don't require human reviews given the number of dependencies being bumped. We require our external dependencies are at least 7 days old, via Minimum Release Age, to make sure that stability and security aren't lost by automated merges.&lt;/p&gt;
    &lt;p&gt;Without using PR-based automerge, we'd be unlikely to ever get to reviewing all the dependency bumps on top of user contributions, and considering Renovate is many projects, we can't scale our minimal team quite that far.&lt;/p&gt;
    &lt;p&gt;(And yes, I realise the irony of us not having time to review our own Renovate PRs ð)&lt;/p&gt;
    &lt;head rend="h3"&gt;Too much shipping&lt;/head&gt;
    &lt;p&gt;At one point, we found that the pace of delivery of shipping new releases regularly over the years would actually come back to bite us!&lt;/p&gt;
    &lt;p&gt;In my first full week at Mend, publishes to the npm registry failed due to:&lt;/p&gt;
    &lt;code&gt;npm error code E406
npm error 406 Not Acceptable - PUT https://registry.npmjs.org/renovate - Package publish failed.
npm error Your package metadata is too large (100.01 MB &amp;gt; 100 MB).
npm error The likely cause is that you have too many versions (10451).
npm error Resolve this problem by unpublishing older versions of your package.
&lt;/code&gt;
    &lt;p&gt;This wasn't down to an individual &lt;code&gt;package.json&lt;/code&gt; being too large, but down to the fact that we had so many releases that the npm registry APIs would no longer allow publishing.&lt;/p&gt;
    &lt;p&gt;I feel that's a pretty cool "badge of honour" to note that we've been shipping consistently for so long that we end up hitting a rate limit on the npm registy side!&lt;/p&gt;
    &lt;p&gt;(Note that not every commit has its own release - we perform a batched release if there are multiple commits landing on &lt;code&gt;main&lt;/code&gt; within the same timeframe, but sometimes it does end up being several releases a day that are a single commit worth of changes)&lt;/p&gt;
    &lt;p&gt;This was also a "fun" incident because we weren't even able to manually unpublish any versions ourselves, as we're a popular package. We needed to work with npm support, who could perform some backend magic to force some unpublishes for us, saving us some time.&lt;/p&gt;
    &lt;p&gt;We're also working to periodically clean up old tags, to make sure that we don't hit this again in the future.&lt;/p&gt;
    &lt;head rend="h3"&gt;There's no "I" in team&lt;/head&gt;
    &lt;p&gt;Working on a large project like this isn't possible to do on your own - it really "takes a village".&lt;/p&gt;
    &lt;p&gt;The primary projects I've been maintaining over the years have largely been down to a single maintainer (me) who has been trying to fulfill all the roles necessary, as well as fitting it in with all the other things I'm working on.&lt;/p&gt;
    &lt;p&gt;For instance, &lt;code&gt;oapi-codegen&lt;/code&gt; is a widely used OpenAPI-to-Go code generator. But right now, my co-maintainer is mostly unable to work on the project, which means that triage, user questions and PR review, let alone being able to do forward thinking product-y thinking is all down to me - that gets hard over time, especially as the Issues and PRs pile up.&lt;/p&gt;
    &lt;p&gt;I noticed in the first couple of weeks of my time in the project that it was really nice to come in on a morning and see that Rahul had already triaged and/or resolved Discussions raised by users, or that Michael and Sebastian had already reviewed + merged Pull Requests.&lt;/p&gt;
    &lt;p&gt;It makes a huge difference mentally to know that there are other Contributors and Maintainers who are also working to support the project and that it's not all on me, unlike other projects I maintain.&lt;/p&gt;
    &lt;p&gt;I've also found that this gives me more ability to think about what I want to delegate, for instance a number of Mend customer requests may go to Rahul and Sergei, which frees me up to do a "bigger picture" piece of work, or frees me up to focus my time on code review instead.&lt;/p&gt;
    &lt;head rend="h3"&gt;A welcoming bunch&lt;/head&gt;
    &lt;p&gt;I'd also like to say another thank you for folks being so welcoming! It definitely helps being more of a familiar face, but it's still been nice that everyone's been lovely and welcoming.&lt;/p&gt;
    &lt;head rend="h2"&gt;Community Management&lt;/head&gt;
    &lt;p&gt;I find it really important to build empathy with my users, and so working on the community management is super valuable to me, and allows me to continue helping others as I've been helped over the years.&lt;/p&gt;
    &lt;p&gt;It's also fortunate I enjoy it because it's core part of my role at Mend as the community manager for the project, and so it's been good that it feels much more sustainable than other projects I've maintained in the past.&lt;/p&gt;
    &lt;p&gt;This was especially something I was a little nervous about, as I was stepping into Rhys' shoes of being the community manager, but I've found I've settled well into the role.&lt;/p&gt;
    &lt;p&gt;As I've written about, Renovate uses GitHub Discussions for the community to get in touch about questions, possible bugs and feature requests. I recommend you read the post for the full discussion (pun intended) about why it's worked so well for us.&lt;/p&gt;
    &lt;p&gt;As my role as Community Manager on the project, I spend a lot of my time working with GitHub Discussions, and have found that although Discussions does work for us, there are a few areas where it's not the best experience (as an offering from GitHub).&lt;/p&gt;
    &lt;p&gt;To improve the experience for (primarily) me, I've ended up building a sync-to-local-SQLite-and-web-application which I'm calling the "maintainer dashboard", where I can now get separate views of the data, such as the below information, which isn't possible to get out of GitHub otherwise:&lt;/p&gt;
    &lt;p&gt;Another key benefit of getting raw access to the data means that we can look at other interesting queries for things that we care about as a project, such as "where have we not replied to a user request for some time" or "is someone trying to bump an old thread, instead of creating a fresh Discussion?".&lt;/p&gt;
    &lt;p&gt;I'll soon be sharing about this in a bit more detail, as well as releasing it under an Open Source license.&lt;/p&gt;
    &lt;head rend="h2"&gt;Minimal reproductions take time&lt;/head&gt;
    &lt;p&gt;As with many Open Source projects and commercial projects alike, before you can fix a bug you need to find a way to reproduce it.&lt;/p&gt;
    &lt;p&gt;With Renovate this is no different, and it's a large part of the "Request Help" flow our users go through, where if we can't reasonably work out what's going on from debug logs provided by the user, we require a minimal reproduction repository.&lt;/p&gt;
    &lt;p&gt;While debugging Mend customer requests, thinking about features I'd like to have in Renovate, and sometimes being extra nice and going the extra mile for a given user request, I'll put together the minimal reproductions, as our users do, which can take time.&lt;/p&gt;
    &lt;p&gt;But like with writing a design document or a blog post, going through the process of breaking down the issue into the smallest possible unit can help determine "oh, it was actually something unrelated", or can make it much clearer that the behaviour you're trying to achieve doesn't make sense.&lt;/p&gt;
    &lt;p&gt;It's hugely valuable, and was something I was doing at Elastic as our resident Renovate expert, too. I also have a separate post I've been working on, which will go into what I find works best for my own testing of Renovate configuration changes, which feeds into this.&lt;/p&gt;
    &lt;p&gt;Although I've not yet had a chance to fully play around with it, I've been planning to get an LLM Agent set up with being able to take a bug report and convert it into a Minimal Reproduction - even if it gets a maximum of 80% of the way there, that's a good start!&lt;/p&gt;
    &lt;head rend="h2"&gt;Hitting the ground running&lt;/head&gt;
    &lt;p&gt;One of the key reasons I joined Mend to work on Renovate was because - in my humble opinion - I was the best person for the job. I had a lot of context with Renovate, some of Mend's offerings on top of the Renovate CLI, as well as the wider package management + dependency insights ecosystem Renovate sits in.&lt;/p&gt;
    &lt;p&gt;One of the really nice things was being able to join a company and - while being cautious about over-exertion and burnout - get going straight away, without needing to onboard.&lt;/p&gt;
    &lt;p&gt;Literally on my second day, I was reviewing Discussions and PRs, and drafting my own code changes.&lt;/p&gt;
    &lt;p&gt;Going back 2 weeks before I joined, I proposed something that would take up a considerable chunk of the next couple of months - the enablement of Minimum Release Age across the npm ecosystem.&lt;/p&gt;
    &lt;p&gt;As I'll mention below, this is something I was very proud of - and there was a lot of work for a few of us making the feature as stable and predictable as possible before enabling it for a significant percentage of our userbase.&lt;/p&gt;
    &lt;p&gt;This isn't the sort of thing that you'd expect a new-starter at a company to be picking up, so it was pretty great to know that I was able to come in and immediately "push the needle".&lt;/p&gt;
    &lt;head rend="h2"&gt;Renovate isn't one project&lt;/head&gt;
    &lt;p&gt;Another thing that I learned fairly quickly on the job is that when we talk about "Renovate", most folks think of the Renovate CLI as the thing, but there's many more things behind it that we don't consider.&lt;/p&gt;
    &lt;p&gt;Coming into the project I feel like I knew this, and I knew that the Renovate Docker image was built on top of "Containerbase", but I'd not fully appreciated how the two interacted, as well as the many other projects that make up the offering that is Renovate.&lt;/p&gt;
    &lt;p&gt;A simplified list of some of the more interesting projects that fit under "Renovate" are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;osv-offline&lt;/item&gt;
      &lt;item&gt;Containerbase a command-line tool for being able to i.e. &lt;code&gt;install-tool node 22.22.0&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Containerbase's pre-built tools (i.e. Node, the Go toolchain)&lt;/item&gt;
      &lt;item&gt;The official Helm charts&lt;/item&gt;
      &lt;item&gt;The official GitHub Actions&lt;/item&gt;
      &lt;item&gt;GitLab CI&lt;/item&gt;
      &lt;item&gt;A number of packages under our npm user&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each of these projects needs updates, they all have their own backlogs and sets of documentation to improve and keep on top of.&lt;/p&gt;
    &lt;p&gt;As noted above, there has to be some level of automation in place for dependency updates, otherwise we'd spend all of the Maintainer + Contributor time reviewing dependency update PRs!&lt;/p&gt;
    &lt;p&gt;Although we're primarily focussing on Renovate, there are times that to implement a feature in Renovate, it requires changes across multiple components before we can implement it upstream - so sometimes this can lead to a slightly more complex contribution process, as we're fairly well documented on the Renovate CLI, but not as many of the related projects.&lt;/p&gt;
    &lt;head rend="h2"&gt;Typescript is great&lt;/head&gt;
    &lt;p&gt;This isn't a new opinion of mine, but I very much love Typescript. Although builds aren't quite as speedy as Go, I very much enjoy the stronger type system, and the way Renovate is configured as a project works very nicely.&lt;/p&gt;
    &lt;p&gt;It's really quite nice to have real enum types - opposed to the hacks we have to do in Go - and the Language Server is much more powerful that &lt;code&gt;gopls&lt;/code&gt;, which has become more stark a contrast as I flip between the two languages.&lt;/p&gt;
    &lt;p&gt;I'm not likely to migrate to Typescript for personal projects - due to the beauty of the single static binary from Go - but it is nice to be working on what I find to be a really nice codebase with a strong set of tooling.&lt;/p&gt;
    &lt;head rend="h2"&gt;
      &lt;code&gt;await response()&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;Open Source is naturally an asynchronous model of collaboration, with the globally distributed nature of users, contributors and maintainers, and Renovate is no different.&lt;/p&gt;
    &lt;p&gt;We do fortunately have our maintainers all in Europe, so we have a significant timezone overlap, and cross-continent working with our contributors still works well.&lt;/p&gt;
    &lt;p&gt;After years of working at Elastic, where my team at one point was across a couple of timezones in the US, the UK, Spain, Germany and two timezones in Australia, I'm very used to - and enjoy - more asynchronous working.&lt;/p&gt;
    &lt;p&gt;I've managed to video call with a few folks, and I plan to try and meet with all of our core contributors + maintainers over the next few months, even if it's a "hey, let's make it easier to remember we're both humans" and get to know each other slightly more.&lt;/p&gt;
    &lt;p&gt;The majority of interactions are done over GitHub, but we do have a Renovate developers Slack for contributors and maintainers, so every so often we can have some high bandwidth calls.&lt;/p&gt;
    &lt;head rend="h2"&gt;So many package managers&lt;/head&gt;
    &lt;p&gt;Although we've not yet merged support for new package managers (there are a couple of PRs waiting on me, sorry!), it's been really interesting finding out how many package managers there are out there.&lt;/p&gt;
    &lt;p&gt;I knew that Renovate supports many package managers, but it's been eye-opening getting a chance to see what's in use, and get a gut-feel for what our users are actually using.&lt;/p&gt;
    &lt;p&gt;With user requests coming in, it's also a little bit of a learning curve trying to work out how the package manager works, getting minimal reproductions set up, and this is another area where I need to try and lean on LLMs a little bit more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Lots of "work in progress"&lt;/head&gt;
    &lt;p&gt;As someone with ADHD, I've noted before that I'll often (but not always) thrive more when there's lots of stuff to do at a given time, rather than only having a single task to do at a given time.&lt;/p&gt;
    &lt;p&gt;Working on the Renovate project is no different - I have several backlogs of "TODOs", coming in from community Discussions, Pull Requests, my own TODO list of "I'd love to have that done" and "that'd improve Mend's offerings if we had this" through to Mend customer requests.&lt;/p&gt;
    &lt;p&gt;Having a strong level of control over how I spend the time towards these is good, but as users, it's good to remember that there's often a lot of background work going on which may be why you're not getting a response - as noted above, I'm not even 100% full time on the Renovate Open Source project, and if you saw my email inbox from Renovate Discussions, you'd understand the delays ð&lt;/p&gt;
    &lt;p&gt;A lot of the time, this means that I'm working through 10+ Discussions at a given time, trying to provide an answer, the right documentation, or getting an Issue raised for changes to the codebase.&lt;/p&gt;
    &lt;head rend="h2"&gt;Big deliveries&lt;/head&gt;
    &lt;p&gt;To take a bit of a self-congratulatory note, I'd like to talk about some of the things I'm most proud of us delivering since I've joined.&lt;/p&gt;
    &lt;p&gt;Since joining, there have been a few things I've been most proud of leading + delivering:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enabling Minimum Release Age to secure the npm ecosystem&lt;list rend="ul"&gt;&lt;item&gt;In particular, this involved a tonne of work getting ready to being able to turn it on for a significant portion of Renovate users. Although the feature (previously known as "stability days") has been in Renovate since 2019(!), enabling this for all users of the npm datasource who use &lt;code&gt;config:best-practices&lt;/code&gt;meant that we needed to make it more predictable, highly documented, and introduce additional log lines for folks to understand what was going on&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;In particular, this involved a tonne of work getting ready to being able to turn it on for a significant portion of Renovate users. Although the feature (previously known as "stability days") has been in Renovate since 2019(!), enabling this for all users of the npm datasource who use &lt;/item&gt;
      &lt;item&gt;Significantly improved OpenTelemetry tracing, so it's much clearer what's now happening under-the-hood&lt;/item&gt;
      &lt;item&gt;First major release (and a lot of learnings around how the process works, which is good ahead of next week's next major release!)&lt;/item&gt;
      &lt;item&gt;Working to provide "secure by default" configuration like disabling unsafe execution of Gradle scripts or clarifying additional risks of self-hosting&lt;/item&gt;
      &lt;item&gt;Handling my first GitHub Security Advisory report, and closing off a few other fixed reports&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are also a number of other smaller bug fixes, features and community contributions I've helped get shipped, as well as shaping a number of medium-/long-term routes for the project.&lt;/p&gt;
    &lt;p&gt;One of the great benefits of being on the maintainer team means that I've been able to look at some of my previously raised feature requests, and I can now get them delivered because I now have much more experience, and ability to explain why the features should get done - including a few I've raised in the past that are super useful for Mend to have for our hosted platform!&lt;/p&gt;
    &lt;head rend="h2"&gt;There's always more to do&lt;/head&gt;
    &lt;p&gt;As noted on my interview on Open Source Security, package management is a pretty large space and there's so much to it.&lt;/p&gt;
    &lt;p&gt;Renovate aims to make it possible to keep all the things updated, with safe defaults and a workflow that is configurable to work for you.&lt;/p&gt;
    &lt;p&gt;To do this, however, is a balancing act to make sure that we're not adding features that make the maintenance of the project more difficult, or to make sure that we don't introduce too many more configuration options, while also continuing to move forward as a project.&lt;/p&gt;
    &lt;p&gt;I've really enjoyed the last 100 days, and the work we've done so far. I'm very excited to see what the next 100 days bring, and onwards into the future!&lt;/p&gt;
    &lt;p&gt;If you've got to the end - well done! Was this interesting? Anything you're interested in learning more about?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46705512</guid><pubDate>Wed, 21 Jan 2026 13:35:41 +0000</pubDate></item><item><title>Ireland wants to give its cops spyware, ability to crack encrypted messages</title><link>https://www.theregister.com/2026/01/21/ireland_wants_to_give_police/</link><description>&lt;doc fingerprint="fc3c993136b2f35d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ireland wants to give its cops spyware, ability to crack encrypted messages&lt;/head&gt;
    &lt;head rend="h2"&gt;Its very own Snooper’s Charter comes a month after proposed biometric tech expansion&lt;/head&gt;
    &lt;p&gt;The Irish government is planning to bolster its police's ability to intercept communications, including encrypted messages, and provide a legal basis for spyware use.&lt;/p&gt;
    &lt;p&gt;The Communications (Interception and Lawful Access) Bill is being framed as a replacement for the current legislation that governs digital communication interception.&lt;/p&gt;
    &lt;p&gt;The Department of Justice, Home Affairs, and Migration said in an announcement this week the existing Postal Packets and Telecommunications Messages (Regulation) Act 1993 "predates the telecoms revolution of the last 20 years."&lt;/p&gt;
    &lt;p&gt;As well as updating laws passed more than two decades ago, the government was keen to emphasize that a key ambition for the bill is to empower law enforcement to intercept of all forms of communications.&lt;/p&gt;
    &lt;p&gt;The Bill will bring communications from IoT devices, email services, and electronic messaging platforms into scope, "whether encrypted or not."&lt;/p&gt;
    &lt;p&gt;In a similar way to how certain other governments want to compel encrypted messaging services to unscramble packets of interest, Ireland's announcement also failed to explain exactly how it plans to do this.&lt;/p&gt;
    &lt;p&gt;However, it promised to implement a robust legal framework, alongside all necessary privacy and security safeguards, if these proposals do ultimately become law. It also vowed to establish structures to ensure "the maximum possible degree of technical cooperation between state agencies and communication service providers."&lt;/p&gt;
    &lt;p&gt;The government said it will follow the EU Commission's (EC) roadmap for law enforcement data interception, including a section on encryption issues, which it published last year.&lt;/p&gt;
    &lt;p&gt;"There is an urgent need for a new legal framework for lawful interception which can be used to confront serious crime and security threats," said justice minister Jim O'Callaghan, announcing the news.&lt;/p&gt;
    &lt;p&gt;"The new legislation will also include robust legal safeguards to provide continued assurance that the use of such powers is necessary and proportionate.&lt;/p&gt;
    &lt;p&gt;He said new legislation is "long overdue", following "significant changes" to digital comms over the past twenty years that "existing legislation does not comprehend."&lt;/p&gt;
    &lt;head rend="h3"&gt;Spyware provision&lt;/head&gt;
    &lt;p&gt;Ireland will also take the EU's lead on spyware, establishing a legal provision for its use, only in cases of strict necessity.&lt;/p&gt;
    &lt;p&gt;The EC's 2024 paper [PDF] examining the legality of spyware noted it could be used by member states, but only where situations absolutely require it. Programs must be used proportionally, with a judge's approval, and with stringent oversight.&lt;/p&gt;
    &lt;p&gt;The justice ministry said it would take this paper into consideration when developing Ireland's legal provision for using spyware. Example cases could include accessing data on a device or network, or covert recordings of communications on a device, or over a network, the government said.&lt;/p&gt;
    &lt;p&gt;In addition to spyware, Ireland is looking to establish a legal power for police to scan electronic equipment in a specific location to identify people of interest and their associates in relation to serious crime investigations. Examples of this technology in action include police camping outside a single location, and operating IMSI catchers to identify those inside.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Rather than add a backdoor, Apple decides to kill iCloud encryption for UK peeps&lt;/item&gt;
      &lt;item&gt;AWS flips switch on Euro cloud as customers fret about digital sovereignty&lt;/item&gt;
      &lt;item&gt;IPv6 just turned 30 and still hasn't taken over the world, but don't call it a failure&lt;/item&gt;
      &lt;item&gt;UK surveillance law still full of holes, watchdog warns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Olga Cronin, surveillance and human rights senior policy officer at the Irish Council for Civil Liberties (ICCL), said the nonprofit "has very serious concerns about this shopping list of surveillance powers," despite the proposals still being in their infancy.&lt;/p&gt;
    &lt;p&gt;"These are surveillance tools and powers of extraordinary reach, with sweeping implications for people's rights and freedoms, and come in the context of An Garda Síochána already expanding their 'eyes and ears' via the Recording Devices Bill," Cronin added.&lt;/p&gt;
    &lt;p&gt;The separate but related Recording Devices Bill was introduced in December 2025, proposing expanded police use of biometric recognition technology.&lt;/p&gt;
    &lt;p&gt;It did not say exactly how this would be implemented, but ministers describing the Bill's ambitions suggested that both live and retrospective facial recognition could become widely used across Ireland's police force.&lt;/p&gt;
    &lt;p&gt;"Once powers of this magnitude are normalised, the damage to rights and freedoms can be extremely difficult to reverse," said Cronin.&lt;/p&gt;
    &lt;p&gt;"We must also remember that measures introduced for exceptional or serious crimes tend, over time, to be used for much less serious crimes because there is institutional pressure to use them more frequently. What was once exceptional becomes routine." ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46705715</guid><pubDate>Wed, 21 Jan 2026 13:52:27 +0000</pubDate></item><item><title>Show HN: ChartGPU – WebGPU-powered charting library (1M points at 60fps)</title><link>https://github.com/ChartGPU/ChartGPU</link><description>&lt;doc fingerprint="a905bd6501bc13b2"&gt;
  &lt;main&gt;
    &lt;p&gt;High-performance charts powered by WebGPU&lt;/p&gt;
    &lt;p&gt;Documentation | Live Demo | Examples&lt;/p&gt;
    &lt;p&gt;ChartGPU is a TypeScript charting library built on WebGPU for smooth, interactive rendering—especially when you have lots of data.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🚀 WebGPU-accelerated rendering for high FPS with large datasets&lt;/item&gt;
      &lt;item&gt;📈 Multiple series types: line, area, bar, scatter, pie, candlestick&lt;/item&gt;
      &lt;item&gt;🧭 Built-in interaction: hover highlight, tooltip, crosshair&lt;/item&gt;
      &lt;item&gt;🔁 Streaming updates via &lt;code&gt;appendData(...)&lt;/code&gt;(cartesian series)&lt;/item&gt;
      &lt;item&gt;🔍 X-axis zoom (inside gestures + optional slider UI)&lt;/item&gt;
      &lt;item&gt;🎛️ Theme presets (&lt;code&gt;'dark' | 'light'&lt;/code&gt;) and custom theme support&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At a high level, &lt;code&gt;ChartGPU.create(...)&lt;/code&gt; owns the canvas + WebGPU lifecycle, and delegates render orchestration (layout/scales/data upload/render passes + internal overlays) to the render coordinator. For deeper internal notes, see &lt;code&gt;docs/API.md&lt;/code&gt; (especially “Render coordinator”).&lt;/p&gt;
    &lt;code&gt;flowchart TB
  UserApp["Consumer app"] --&amp;gt; PublicAPI["src/index.ts (Public API exports)"]

  PublicAPI --&amp;gt; ChartCreate["ChartGPU.create(container, options)"]
  PublicAPI --&amp;gt; SyncAPI["connectCharts(charts)"]

  subgraph ChartInstance["Chart instance (src/ChartGPU.ts)"]
    ChartCreate --&amp;gt; SupportCheck["checkWebGPUSupport()"]
    ChartCreate --&amp;gt; Canvas["Create canvas + mount into container"]
    ChartCreate --&amp;gt; Options["resolveOptions(options)"]
    ChartCreate --&amp;gt; GPUInit["GPUContext.create(canvas)"]
    ChartCreate --&amp;gt; Coordinator["createRenderCoordinator(gpuContext, resolvedOptions)"]

    ChartCreate --&amp;gt; InstanceAPI["ChartGPUInstance APIs"]
    InstanceAPI --&amp;gt; RequestRender["requestAnimationFrame (coalesced)"]
    RequestRender --&amp;gt; Coordinator

    InstanceAPI --&amp;gt; SetOption["setOption(...)"]
    InstanceAPI --&amp;gt; AppendData["appendData(...)"]
    InstanceAPI --&amp;gt; Resize["resize()"]

    subgraph PublicEvents["Public events + hit-testing (ChartGPU.ts)"]
      Canvas --&amp;gt; PointerHandlers["Pointer listeners"]
      PointerHandlers --&amp;gt; PublicHitTest["findNearestPoint() / findPieSlice()"]
      PointerHandlers --&amp;gt; EmitEvents["emit('click'/'mouseover'/'mouseout')"]
    end

    DataZoomSlider["dataZoom slider UI (DOM)"] --&amp;gt; Coordinator
  end

  subgraph WebGPUCore["WebGPU core (src/core/GPUContext.ts)"]
    GPUInit --&amp;gt; AdapterDevice["navigator.gpu.requestAdapter/device"]
    GPUInit --&amp;gt; CanvasConfig["canvasContext.configure(format)"]
  end

  subgraph RenderCoordinatorLayer["Render coordinator (src/core/createRenderCoordinator.ts)"]
    Coordinator --&amp;gt; Layout["GridArea layout"]
    Coordinator --&amp;gt; Scales["xScale/yScale (clip space for render)"]
    Coordinator --&amp;gt; DataUpload["createDataStore(device) (GPU buffer upload/caching)"]
    Coordinator --&amp;gt; RenderPass["Encode + submit render pass"]

    subgraph InternalOverlays["Internal interaction overlays (coordinator)"]
      Coordinator --&amp;gt; Events["createEventManager(canvas, gridArea)"]
      Events --&amp;gt; OverlayHitTest["hover/tooltip hit-testing"]
      Events --&amp;gt; InteractionX["interaction-x state (crosshair)"]
      Coordinator --&amp;gt; OverlaysDOM["DOM overlays: legend / tooltip / text labels"]
    end
  end

  subgraph Renderers["GPU renderers (src/renderers/*)"]
    RenderPass --&amp;gt; GridR["Grid"]
    RenderPass --&amp;gt; AreaR["Area"]
    RenderPass --&amp;gt; BarR["Bar"]
    RenderPass --&amp;gt; ScatterR["Scatter"]
    RenderPass --&amp;gt; LineR["Line"]
    RenderPass --&amp;gt; PieR["Pie"]
    RenderPass --&amp;gt; CandlestickR["Candlestick"]
    RenderPass --&amp;gt; CrosshairR["Crosshair overlay"]
    RenderPass --&amp;gt; HighlightR["Hover highlight overlay"]
    RenderPass --&amp;gt; AxisR["Axes/ticks"]
  end

  subgraph Shaders["WGSL shaders (src/shaders/*)"]
    GridR --&amp;gt; gridWGSL["grid.wgsl"]
    AreaR --&amp;gt; areaWGSL["area.wgsl"]
    BarR --&amp;gt; barWGSL["bar.wgsl"]
    ScatterR --&amp;gt; scatterWGSL["scatter.wgsl"]
    LineR --&amp;gt; lineWGSL["line.wgsl"]
    PieR --&amp;gt; pieWGSL["pie.wgsl"]
    CandlestickR --&amp;gt; candlestickWGSL["candlestick.wgsl"]
    CrosshairR --&amp;gt; crosshairWGSL["crosshair.wgsl"]
    HighlightR --&amp;gt; highlightWGSL["highlight.wgsl"]
  end

  subgraph ChartSync["Chart sync (src/interaction/createChartSync.ts)"]
    SyncAPI --&amp;gt; ListenX["listen: 'crosshairMove'"]
    SyncAPI --&amp;gt; DriveX["setCrosshairX(...) on peers"]
  end

  InteractionX --&amp;gt; ListenX
  DriveX --&amp;gt; InstanceAPI
&lt;/code&gt;
    &lt;p&gt;Financial OHLC (open-high-low-close) candlestick rendering with classic/hollow style toggle and color customization.&lt;/p&gt;
    &lt;code&gt;import { ChartGPU } from 'chartgpu';
const container = document.getElementById('chart')!;
await ChartGPU.create(container, {
  series: [{ type: 'line', data: [[0, 1], [1, 3], [2, 2]] }],
});&lt;/code&gt;
    &lt;p&gt;
      &lt;code&gt;npm install chartgpu&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;React bindings are available via &lt;code&gt;chartgpu-react&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;npm install chartgpu-react&lt;/code&gt;
    &lt;code&gt;import { ChartGPUChart } from 'chartgpu-react';

function MyChart() {
  return (
    &amp;lt;ChartGPUChart
      options={{
        series: [{ type: 'line', data: [[0, 1], [1, 3], [2, 2]] }],
      }}
    /&amp;gt;
  );
}&lt;/code&gt;
    &lt;p&gt;See the chartgpu-react repository for full documentation and examples.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Chrome 113+ or Edge 113+ (WebGPU enabled by default)&lt;/item&gt;
      &lt;item&gt;Safari 18+ (WebGPU enabled by default)&lt;/item&gt;
      &lt;item&gt;Firefox: not supported (WebGPU support in development)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Full documentation: Getting Started&lt;/item&gt;
      &lt;item&gt;API reference: &lt;code&gt;docs/API.md&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browse examples: &lt;code&gt;examples/&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Run locally: &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;npm install&lt;/code&gt;&lt;/item&gt;&lt;item&gt;&lt;code&gt;npm run dev&lt;/code&gt;(opens&lt;code&gt;http://localhost:5176/examples/&lt;/code&gt;)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See &lt;code&gt;CONTRIBUTING.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;MIT — see &lt;code&gt;LICENSE&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46706528</guid><pubDate>Wed, 21 Jan 2026 14:54:56 +0000</pubDate></item><item><title>Skip Is Now Free and Open Source</title><link>https://skip.dev/blog/skip-is-free/</link><description>&lt;doc fingerprint="3f0ad3111315ee7e"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Skip Is Now Free and Open Source&lt;/head&gt;&lt;p&gt;Since launching Skip in 2023, we’ve pursued one mission: enable developers to create premium mobile apps for iOS and Android from a single Swift and SwiftUI codebase — without any of the compromises that have encumbered cross-platform development tools since, well, forever.&lt;/p&gt;&lt;p&gt;Over the past three years, Skip has evolved significantly. We started with a Swift-to-Kotlin transpiler and Android support for the most common SwiftUI APIs. We then founded the Swift Android Workgroup ↗ and released the Swift Android SDK to compile Swift natively for Android. We now have dozens of popular integration frameworks, interoperate with thousands of cross-platform Swift packages, and feature the most complete independent SwiftUI implementation available.&lt;/p&gt;&lt;head rend="h3"&gt;The Challenge of Paid Developer Tools&lt;/head&gt;Section titled “The Challenge of Paid Developer Tools”&lt;p&gt;Until today, Skip has required a paid subscription and license key to build apps. While free apps and indie developers below a revenue threshold were exempt, businesses were expected to subscribe. This model helped us bootstrap Skip without outside investment, but we’ve always known that to truly compete with legacy cross-platform tools and achieve widespread adoption, Skip would need to become freely available.&lt;/p&gt;&lt;p&gt;The plain truth is that developers expect to get their tools free of charge. First-party IDEs like Xcode and Android Studio, popular integration frameworks, and essential dev tools are all given away at no (direct) cost. The platform vendors monetize through developer program fees, app store commissions, and cloud services. Framework providers typically monetize through complementary services. But developer tools? Those have historically required the patronage of massive tech companies in order to fund their ongoing development, support, and infrastructure costs.&lt;/p&gt;&lt;p&gt;Beyond pricing, there’s a deeper concern about durability. Developers are understandably wary of building their entire app strategy on a small company’s paid, closed-source tool. What if the company goes under? Gets acquired and shut down? What happens to their apps? We get it. While Skip’s innate ejectability offers some risk mitigation, product teams need absolute confidence that their chosen technologies will be around next week, next year, and beyond. They must remain immune from the dreaded “rug pull” that so often accompanies a “pivot”.&lt;/p&gt;&lt;p&gt;To keep the development community’s trust and achieve mass adoption, Skip needs a completely free and open foundation. Even if the core team disappeared, the community could continue supporting the technology and the apps that depend on it.&lt;/p&gt;&lt;head rend="h3"&gt;What’s Changing&lt;/head&gt;Section titled “What’s Changing”&lt;p&gt;As of Skip 1.7, all licensing requirements have been removed. No license keys, no end-user license agreements, no trial or evaluation period.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Current Skip developers: Your setup remains completely unchanged, except you will no longer need your license key after upgrading.&lt;/item&gt;&lt;item&gt;New Skip users: You can start building immediately — no evaluation license required.&lt;/item&gt;&lt;item&gt;Open source skipstone: We’ve open-sourced the Skip engine, known as “skipstone”. This is the tool that handles all the critical build-time functionality: Project creation and management, Xcode and SwiftPM plugin logic, iOS-to-Android project transformation, resource and localization bundling, JNI bridge creation, source transpilation, app packaging, and project export. It is now available as a public GitHub repository at https://github.com/skiptools ↗ under a free and open-source license.&lt;/item&gt;&lt;item&gt;Migrate skip.tools to skip.dev: As part of this process, we are launching our new home at https://skip.dev ↗! This new site hosts our documentation, blog, and case studies, and it is also open-source and welcomes contributions at https://github.com/skiptools/skip.dev ↗. We will eventually be migrating the entirety of https://skip.tools ↗ to https://skip.dev ↗.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Supporting Skip’s Future&lt;/head&gt;Section titled “Supporting Skip’s Future”&lt;p&gt;Since day one, Skip has been bootstrapped. We haven’t taken venture capital or private equity investment, nor are we controlled by big tech. This independence means we control our destiny and can make the best decisions for Skip’s developers and users — a unique position in the cross-platform development space.&lt;/p&gt;&lt;p&gt;But independence requires community support. And that is where you come in.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Current subscribers: Your Small Business or Professional plan will automatically transition to an Individual ↗ or Supporter ↗ tier, respectively. You can cancel any time with no consequences (other than making us sad), but we hope you’ll consider staying on, at least throughout this transition period.&lt;/item&gt;&lt;item&gt;Individual developers: If you believe in Skip’s mission, please consider supporting us through GitHub Sponsors ↗ with a monthly contribution.&lt;/item&gt;&lt;item&gt;Companies and organizations: For businesses that want to see Skip flourish, we offer corporate sponsorship tiers with visibility on our homepage and in our documentation. Your sponsorship directly funds development of the integration frameworks essential to production apps, as well as the ongoing maintenance, support, and infrastructure. Sponsorship comes with some compelling perks! Please visit https://skip.dev/sponsor ↗ to see the sponsorship tiers.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Investing in Skip is also investing in your own team’s capabilities and competitive advantage. Your support accelerates Skip’s development and ensures its long-term success, enabling your developers to build exceptional native experiences efficiently, today and into the future.&lt;/p&gt;&lt;head rend="h3"&gt;What Comes Next&lt;/head&gt;Section titled “What Comes Next”&lt;p&gt;We’re at a pivotal moment in the app development field. Legacy cross‑platform frameworks are struggling to keep pace with the rapid evolution of modern UI systems like Liquid Glass on iOS and Material Expressive on Android. The compromises that once felt acceptable in exchange for a unified codebase now result in dated interfaces, weaker user experiences, and real competitive disadvantages. Teams ready to move beyond those trade‑offs can count on Skip to champion what matters most: delivering truly native, uncompromised experiences on both major mobile platforms.&lt;/p&gt;&lt;p&gt;Opening Skip to the community marks the next step in its evolution. Software is never finished — especially a tool that supports modern Swift and Kotlin, SwiftPM and Gradle, Xcode and Android Studio, iOS and Android, and the ongoing growth of SwiftUI and Jetpack Compose. It’s a demanding pursuit, and we’re committed to it. But sustaining and expanding this work depends on the support of developers who believe in Skip’s mission.&lt;/p&gt;&lt;p&gt;Together, we will continue building toward Skip’s vision: a genuinely no‑compromise, cross‑platform foundation for universal mobile apps.&lt;/p&gt;&lt;p&gt;Thank you for your support, and as always, Happy Skipping!&lt;/p&gt;&lt;p&gt;Ready to get started? Get started with Skip 1.7 today and join the community building the future of native cross-platform development.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46706906</guid><pubDate>Wed, 21 Jan 2026 15:20:53 +0000</pubDate></item><item><title>SmartOS</title><link>https://docs.smartos.org/</link><description>&lt;doc fingerprint="3653195c723d5ed3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Home&lt;/head&gt;
    &lt;p&gt;Welcome to the SmartOS Documentation. Here you'll find everything you need to get started using SmartOS and participating in the community. Information about what's new in recent releases can be found in the SmartOS Changelog.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick Start¶&lt;/head&gt;
    &lt;p&gt;Not sure where to begin? Try the SmartOS Quick Start Guide!&lt;/p&gt;
    &lt;head rend="h2"&gt;SmartOS In a Nutshell¶&lt;/head&gt;
    &lt;p&gt;SmartOS is a specialized Type 1 Hypervisor platform based on illumos.Â It supports two types of virtualization:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OS Virtual Machines (Zones, Containers): A light-weight virtualization solution offering a complete and secure userland environment on a single global kernel, offering true bare metal performance and all the features illumos has, namely dynamic introspection via DTrace&lt;/item&gt;
      &lt;item&gt;Hardware Virtual Machines (KVM, Bhyve): A full virtualization solution for running a variety of guest OS's including Linux, Windows, *BSD, Plan9 and more&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SmartOS is a "live OS", it is always booted via PXE, ISO, or USB Key and runs entirely from memory, allowing the local disks to be used entirely for hosting virtual machines without wasting disks for the root OS.Â This architecture has a variety of advantages including increased security, no need for patching, fast upgrades and recovery.&lt;/p&gt;
    &lt;p&gt;Virtualization in SmartOS builds on top of the foundational illumos technologies inherited from OpenSolaris, namely:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ZFS for storage virtualization&lt;/item&gt;
      &lt;item&gt;Crossbow (&lt;code&gt;dladm&lt;/code&gt;) for network virtualization&lt;/item&gt;
      &lt;item&gt;Zones for virtualization and containment&lt;/item&gt;
      &lt;item&gt;DTrace for introspection&lt;/item&gt;
      &lt;item&gt;SMF for service management&lt;/item&gt;
      &lt;item&gt;RBAC/BSM for auditing and role based security&lt;/item&gt;
      &lt;item&gt;And more&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;SmartOS is typically "installed" by downloading and copying the OS image onto a USB key and then booting that key.Â On the first boot a configuration utility will configure your base networking, allow you to set the root password, and allow you to select which disks to use to create the ZFS Zpool which will provide persistent storage.&lt;/p&gt;
    &lt;p&gt;When you log into SmartOS you will enter the hypervisor, aka "global zone". From here you can download VM Images using the &lt;code&gt;imgadm&lt;/code&gt; tool, which are
pre-configured Container and HVM virtual machines.Â  You can then use the
&lt;code&gt;vmadm&lt;/code&gt; tool to create and manage both containers and hardware virtual
machines.&lt;/p&gt;
    &lt;p&gt;An important aspect of SmartOS is that both OS (Zones) and hardware virtual machines are both built on Zones technology.Â In the case of OS virtualization, the guest virtual machine is provided with a complete userland environment on which to run applications directly. In the case of HVM virtualization, the &lt;code&gt;qemu&lt;/code&gt; or &lt;code&gt;bhyve&lt;/code&gt;  process
will run within a stripped down Zone.Â  This offers a variety of
advantages for administration, including a common method for managing
resource controls, network interfaces, and administration.Â  It also
provides HVM guests with an additional layer of security and isolation
not offered by other virtualization platforms.&lt;/p&gt;
    &lt;p&gt;Finally, instances are described in JSON.Â Both administrative tools, &lt;code&gt;imgadm&lt;/code&gt; and &lt;code&gt;vmadm&lt;/code&gt;, accept and return all data in JSON
format.Â  This provides a simple, consistent, and programmatic
interface for creating and managing VM's.&lt;/p&gt;
    &lt;head rend="h2"&gt;Code of Conduct¶&lt;/head&gt;
    &lt;p&gt;As a participant of the illumos community, all projects related to Triton (including SmartOS, Triton, Manta, etc.) we have adopted the illumos Code of Conduct.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scope¶&lt;/head&gt;
    &lt;p&gt;The scope of the code of conduct extends to SmartOS, Triton, and Manta resources including mailing lists, chat, social media, and GitHub repositories.&lt;/p&gt;
    &lt;head rend="h3"&gt;Enforcement¶&lt;/head&gt;
    &lt;p&gt;Conduct violations involving the Triton community may be reported by contacting conduct@tritondatacenter.com instead of, or in addition to conduct@illumos.org as the case may be warranted.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to Use this Site¶&lt;/head&gt;
    &lt;p&gt;This documentation can provide you with a variety of resources for users at all levels.Â To get started, download SmartOS now, and be sure to review the Hardware Requirements. Once installed, refer to our Users Guide to help you learn your way around SmartOS.&lt;/p&gt;
    &lt;p&gt;When you have questions, refer to the SmartOS Community section for pointers to our IRC chat rooms and mailing lists.Â When you're ready to start improving and adding your own customizations to SmartOS please refer to our Developers Guide.&lt;/p&gt;
    &lt;p&gt;SmartOS is a community effort, as you explore and experiment with SmartOS please feel free to edit and contribute to this site to improve the documentation for other users in the community.&lt;/p&gt;
    &lt;head rend="h2"&gt;About Triton¶&lt;/head&gt;
    &lt;p&gt;SmartOS is a fundamental component of the Triton Data Center (Triton) product. Triton source and images are available for at no cost and powers several public and private clouds around the globe, namely the MNX Public Cloud.Â As you use SmartOS you will come across hooks that are used by Triton, such as file systems and services named "smartdc".&lt;/p&gt;
    &lt;p&gt;If you are interested in evaluating the full Triton Data Center product, please contact smartos@tritondatacenter.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46706947</guid><pubDate>Wed, 21 Jan 2026 15:23:18 +0000</pubDate></item><item><title>Bending Spoons laid off almost everybody at Vimeo yesterday</title><link>https://news.ycombinator.com/item?id=46707699</link><description>&lt;doc fingerprint="26211054f9a7256e"&gt;
  &lt;main&gt;
    &lt;p&gt;&amp;gt; Bending Spoons has a pattern of acquiring companies, then laying off staff and cutting features. For example, Bending Spoons acquired note-taking and task management app Evernote in 2022, after which the company laid off most of its U.S. and Chile staff and moved operations to Europe in 2023. Evernote then shut down the Linux and older legacy versions of the app, and then proceeded to place heavy restrictions on the app’s free tier in 2024.&lt;/p&gt;
    &lt;p&gt;&amp;gt; In another example, Bending Spoons acquired WeTransfer in July 2024 and then laid off 75% of its staff a few weeks after. A couple months later, WeTransfer began limiting free users to 10 transfers per month.&lt;/p&gt;
    &lt;p&gt;I don't understand this model. Such significant layoffs would indicate that there is no real appetite for expansion or growth.&lt;/p&gt;
    &lt;p&gt;Their goal might be be to acquire, dramatically cut costs, and then run the product for as long as they can at a profit before breaking it down and selling it off (or hope for a buyout by a bigger player.) But that wouldn't make sense — customers of a depreciating SaaS product surely churn after a 1-3 years, so they wouldn't make enough of a return from their existing customers to justify the investment...&lt;/p&gt;
    &lt;p&gt;Yeah this is what I think Bending Spoons does, mostly based on the Evernote situation.&lt;/p&gt;
    &lt;p&gt;Product has paying users and it's in a "complete" state. Cut costs to optimize profit for a bit and hope not everyone leaves.&lt;/p&gt;
    &lt;p&gt;In the case of Evernote, it's probably really hard to get 10 year users off of it at this point, so they can double subscriptions and they're locked in. My assumption is that there's a serious amount of people that go "eh" and just deal with the cost increase and stagnated features.&lt;/p&gt;
    &lt;p&gt;What's hard to understand? They switch the companies from growth (no matter the cost) to revenue extraction (even if it will eventually fade)&lt;/p&gt;
    &lt;p&gt;Minimum viable cost of keeping the lights on. And sometimes they even compromise a little, "let's spend a tiny bit more and see how much growth we can get from that"&lt;/p&gt;
    &lt;p&gt;&amp;gt; Their goal might be be to acquire, dramatically cut costs, and then run the product for as long as they can at a profit before breaking it down and selling it off&lt;/p&gt;
    &lt;p&gt;In the 80's people who did this were known as "corperate raiders". Nowadays it's just called business.&lt;/p&gt;
    &lt;p&gt;My best guess is that a part of it is replacing US devs with much cheaper Italian/European ones, earning ~a quarter than their US counterparts and working longer hours, as Bending Spoons has an extremely competitive hiring process&lt;/p&gt;
    &lt;p&gt;I imagine a lot of companies have contracts with Vimeo and switching costs are real. They'll likely stick with Vimeo if they manage to maintain their offering to the level it exists at today. In the long term I think it guarantees death but they will be able to extract plenty of money before that happens.&lt;/p&gt;
    &lt;p&gt;This is just my personal opinion, but if they didnt change the price of Evernote and never made any changes, I probably would remain a customer for a very very long time. There is a high switching cost for me to use any app to move all my docs, and notes.&lt;/p&gt;
    &lt;p&gt;I dont know if the same can be said for Vimeo, though&lt;/p&gt;
    &lt;p&gt;Look at the companies they're acquiring - it's 100% about getting user data and tertiary monetization, and they're making bank. They couldn't care less about what the companies they buy supposedly do.&lt;/p&gt;
    &lt;p&gt;They did the same thing with Komoot and other apps. I don't understand where the money comes from and how they are planning to keep this portfolio growing.&lt;/p&gt;
    &lt;p&gt;(For Komoot) Did they, though? I am aware of the layoffs, but after that they slightly redesigned the app, collected the poll for next year's requested features, the lifetime maps option is still there to buy etc. If not for HN, I wouldn't have noticed any change in the direction that it's going in.&lt;/p&gt;
    &lt;p&gt;I suspect that the VAST majority of users want their saas tools to do today what they did yesterday, and so stopping active development of new features is actually a positive - no sudden Liquid Ass is going to appear in a program in maintenance mode.&lt;/p&gt;
    &lt;p&gt;What I understand from listening to the management from various podcasts, it was a mix of shipping the most minimum impactful features with the leanest product team needed and then jacking up the price every year for the people that can't move away from these products.&lt;/p&gt;
    &lt;p&gt;The long tail of revenue is not only a substantial sum, but decays more steadily than growth. This is a low risk investment that still turns a profit.&lt;/p&gt;
    &lt;p&gt;It's also not their only investment or even necessarily their own money. Individual holding companies don't tell you much about the larger pool of money they come from.&lt;/p&gt;
    &lt;p&gt;Oh, it will - but they don't care. I'm sure they'll eek out 1.5b from their 1.3b acquisition and be happy as clams.&lt;/p&gt;
    &lt;p&gt;It certainly is depressing to look at what was built and what could be made of it but most of the folks with money lack the creativity or skill to actually build a lasting business. Just burn it down and rob it on the way out - such is the modern economy.&lt;/p&gt;
    &lt;p&gt;Elon Musk acquired Twitter and fired %80 of the employees and it was just fine.&lt;/p&gt;
    &lt;p&gt;I bet there's so many more people that can be let go from all tech industry. It's mature and product discovery is mostly locked behind advertisement so what's left is exploitation.&lt;/p&gt;
    &lt;p&gt;If you think about it, as long as you don't mingle much with the product that works it keeps working indefinitely. It's no different than running Excel or WhatsApp, especially when the servers are managed by 3rd party providers these days.&lt;/p&gt;
    &lt;p&gt;Probably they will also fire remaining 15 people and move everything to Italy.&lt;/p&gt;
    &lt;p&gt;Their strategy is to&lt;/p&gt;
    &lt;p&gt;- fire everyone,&lt;/p&gt;
    &lt;p&gt;- give product to very small but ambitious team of people&lt;/p&gt;
    &lt;p&gt;- cut free version of the product to minimum even if does not make a sense to have a free version such as 5 video upload per month etc (they are doing this just to avoid backlash from users and community)&lt;/p&gt;
    &lt;p&gt;- use every possible dark pattern exist to get every penny from the users&lt;/p&gt;
    &lt;p&gt;Well, some good news is that if you've ever wanted to build a new video streaming platform there are a bunch of companies that'd love to sign up.&lt;/p&gt;
    &lt;p&gt;I'm sure dropout et all will be able to continue with their same level of functionality in the short term but I can imagine the bills they'll be receiving will be escalating quickly.&lt;/p&gt;
    &lt;p&gt;Dropout's CEO has been pretty open about the company, and he described their early efforts as 'Brutal'&lt;/p&gt;
    &lt;p&gt;&amp;gt; No! We tried, but people don’t realize this. The first rendition of Dropout was built on Vimeo OTT’s API, but it was our own product. We employed something like eight sophisticated engineers at IAC to build our own product around it, and it was brutal. Which is to say, it’s just very hard to do very well. And these were great engineers.&lt;/p&gt;
    &lt;p&gt;Wonder what this means for vimeo/psalm, the static analyzer for PHP, which has recently seen some new life breathed into it after long neglect. Psalm has credible alternatives in PHPStan and now Mago, but it would still be a loss to see it go unmaintained again.&lt;/p&gt;
    &lt;p&gt;Our business uses Vimeo because we get a discounted rate on acami CDN via their bulk purchasing power. YouTube is free but that comes with a lot of headaches. For example not being able to hide the recommended videos at the end of a video, which annoyed our clients in the past when we did use YouTube. YouTube also needs to be public to be embeddable, which also created issues for us. However this announcement has me terrified and literally scrambling for a backup plan.&lt;/p&gt;
    &lt;p&gt;Vimeo you manage your brand and presentation. YouTube you have little control over where or how your video is presented. Vimeo also provides VOD for some large brands and media companies.&lt;/p&gt;
    &lt;p&gt;i have made https://codekeep.io for storing snippets, have similar features to evernote. all users will get free pro membership now. if you are thinking about moving , please consider codekeep too.&lt;/p&gt;
    &lt;p&gt;I just cancelled my account that I've had for about 10 years... maybe longer. I barely used it, but it's now &amp;gt;$100/year for my plan. I had maybe 15 videos uploaded that I would share occasionally.&lt;/p&gt;
    &lt;p&gt;I used to be an indie filmmaker and used it to host features when nothing else could. I’ve been paying since 2008. The price would go up but they were great so I let it be.&lt;/p&gt;
    &lt;p&gt;This comment from e98cuenc seems extremely prescient.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Everybody loves to hate BendingSpoon, but there is a lesson here. They consistently rewrite the code of their acquisitions with a tiny team, fire everybody and are able to maintain and improve the product. They basically skip everything but engineers, and they are kept at a minimum. Feedback from users is the products they take over 1) become more expensive, 2) they ship features waaaay faster. It looks like next generation private equity, and my guess is more houses will start copying them&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46707699</guid><pubDate>Wed, 21 Jan 2026 16:14:39 +0000</pubDate></item><item><title>JPEG XL Demo Page</title><link>https://tildeweb.nl/~michiel/jxl/</link><description>&lt;doc fingerprint="df8397f1150bd9b4"&gt;
  &lt;main&gt;
    &lt;p&gt;This page shows a JPEG XL image, if your browser can handle it! At this point in time (January 2026) this means only Safari will display the image, as far as I know.&lt;/p&gt;
    &lt;p&gt;The person in the image is Jon Sneyers, co-author of the JPEG XL spec and also creator of the "Free Lossless Image Format" that came before it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46708032</guid><pubDate>Wed, 21 Jan 2026 16:38:26 +0000</pubDate></item><item><title>European lawmakers suspend U.S. trade deal</title><link>https://www.cnbc.com/2026/01/21/european-lawmakers-suspend-us-trade-deal-amid-greenland-tariff-tensions.html</link><description>&lt;doc fingerprint="1fe338645f5dde07"&gt;
  &lt;main&gt;
    &lt;p&gt;European lawmakers on Wednesday suspended the approval of the trade deal that the EU and U.S. agreed in July.&lt;/p&gt;
    &lt;p&gt;In a statement on Wednesday, European Parliament member Bernd Lange, and INTA chair on EU-US trade relations, said the recent plans by President Donald Trump to impose tariffs of between 10% to 25% on European nations go against the terms of the trade pact.&lt;/p&gt;
    &lt;p&gt;Referring to Trump's address at the World Economic Forum in Davos on Wednesday, Lange said: "I guess he didn't revise his position. He wants to have Greenland as part of the United States as quick as possible."&lt;/p&gt;
    &lt;p&gt;In his speech, the president called for "immediate negotiations" on the acquisition of the Arctic territory.&lt;/p&gt;
    &lt;p&gt;Trump ruled out the use of military force in his speech, a commitment Lange described as a "small positive element."&lt;/p&gt;
    &lt;p&gt;However, Lange said the proposed 10% to 25% tariffs remain on the table, adding that, until the threat of them is over, "there will be no possibility of compromise."&lt;/p&gt;
    &lt;p&gt;"We will hold on the procedure... until there is clarity regarding Greenland and the threats," he said.&lt;/p&gt;
    &lt;p&gt;"There was a breaking of the Scotland deal by President Trump," Lange said, referring to the trade pact agreed by the EU and the U.S. at Trump's Turnberry golf resort last year.&lt;/p&gt;
    &lt;p&gt;Lange said Trump is "using tariffs as an instrument of political pressure" as a way to buy Greenland, and described the move as "an attack against the economic and territorial sovereignty of the European Union."&lt;/p&gt;
    &lt;p&gt;He added that the Committee on International Trade would on Monday discuss the use of the EU's Anti-Coercion Instrument (ACI) — a far-reaching measure variously described as a "trade bazooka" — which would allow the EU to substantially restrict U.S. companies' access to its single market, block them from tenders, reduce the flow of goods and capital, and curb foreign direct investment in the bloc.&lt;/p&gt;
    &lt;p&gt;"This was created exactly for such a case when a foreign country [uses] tariffs and investment for political and coercive pressure," Lange said of the ACI.&lt;/p&gt;
    &lt;p&gt;Earlier in the day, Bundesbank president Joachim Nagel told CNBC that he hoped Trump would reverse his stance, calling the transatlantic tensions a "very problematic situation."&lt;/p&gt;
    &lt;p&gt;Nagel, a governing council member of the European Central Bank, acknowledged that the tariff threat will likely have "some spillover" to monetary policy in the region.&lt;/p&gt;
    &lt;p&gt;Speaking to CNBC's Karen Tso at the World Economic Forum gathering in Davos Wednesday, Nagel said the tariff dispute could "maybe" be a game changer for monetary policy in the eurozone, which he said was still on "a good path."&lt;/p&gt;
    &lt;p&gt;"I still have the hope that we can find a solution, a joint understanding," Nagel added.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46708060</guid><pubDate>Wed, 21 Jan 2026 16:40:17 +0000</pubDate></item><item><title>PicoPCMCIA – a PCMCIA development board for retro-computing enthusiasts</title><link>https://www.yyzkevin.com/picopcmcia/</link><description>&lt;doc fingerprint="350bfe1d4bb7fc5c"&gt;
  &lt;main&gt;
    &lt;p&gt;This is a PCMCIA development board for retro-computing enthusiasts who want to experiment with audio, networking, and expansion on vintage laptops and mobile devices. While ISA users have enjoyed projects like PicoGUS and PicoMEM, PCMCIA users have long been limited to scarce legacy cards with narrow functionality — this board aims to change that. The project is fully open source, and while it is designed to encourage low-level experimentation and development, pre-built, community-provided firmware is available for users who want to test functionality without diving into the technical details. It is intended for hobbyist and development use and is not certified for production deployment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Device Compatibility&lt;/head&gt;
    &lt;p&gt;This is a Type II, 5V, 16-bit PC Card designed for use in compliant PCMCIA sockets and should work in most devices. While I have not yet encountered a device advertising PCMCIA support that was incompatible, support for every device cannot be guaranteed. Power consumption varies depending on enabled functions; support for low-power devices such as the HP 200LX is considered mandatory, and the card has been tested to remain within the 150 mA limit while using network functionality and storage emulation. On devices with very limited power budgets, simultaneous use of networking and audio may require external power.&lt;/p&gt;
    &lt;p&gt;A short list of devices that I actively test on:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;IBM PC110&lt;/item&gt;
      &lt;item&gt;HP 200LX&lt;/item&gt;
      &lt;item&gt;Amiga 1200&lt;/item&gt;
      &lt;item&gt;Apple Newton&lt;/item&gt;
      &lt;item&gt;HP Jornada 720&lt;/item&gt;
      &lt;item&gt;Compaq LTE Elite&lt;/item&gt;
      &lt;item&gt;IBM Thinkpad 235&lt;/item&gt;
      &lt;item&gt;IBM Thinkpad 240&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Features Overview&lt;/head&gt;
    &lt;p&gt;Built around the RP2350 and leveraging the ISA-like nature of the PCMCIA bus, this project benefits greatly from code interchangeability with other RP-based retro projects, most notably PicoGUS and PicoMEM. This shared foundation allows features and improvements to move quickly between platforms, expanding functionality over time.&lt;/p&gt;
    &lt;head rend="h1"&gt;Wi-fi / Bluetooth&lt;/head&gt;
    &lt;p&gt;The card has an onboard wireless module containing the Infineon CYW43439, same as found on the Raspberry Pi Pico W. This allows the card to attach to modern Wi-Fi networks (2.4GHz 802.11b/g/n WPA2). It can then emulate an NE2000 adapter and/or a dialup modem allowing the host computer to access the network as if it was wired, unaware it is wireless.&lt;/p&gt;
    &lt;p&gt;Essentially every platform containing PCMCIA will have existing drivers to recognize and utilize the card as a modem or ethernet adapter making this a near universal option for all devices and platforms including rare devices such as the Apple Newton.&lt;/p&gt;
    &lt;p&gt;It also has a Bluetooth which opens up a lot of possibilities for A2DP wireless audio streaming and wireless gamepads/mice. Software for these features Bluetooth features are still under development and is at a proof of concept stage.&lt;/p&gt;
    &lt;head rend="h1"&gt;Audio&lt;/head&gt;
    &lt;head rend="h2"&gt;Hardware&lt;/head&gt;
    &lt;p&gt;The card has an included Texas Instruments TLV320AIC3254 which calls itself a “Very Low-Power Stereo Audio CODEC with programmable miniDSP”. The main features of this device in our application are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DAC that is fed high quality audio from the RP2354 over i2s&lt;/item&gt;
      &lt;item&gt;Amplified stereo headphone amplifier&lt;/item&gt;
      &lt;item&gt;Line out feeding the host device internal speaker (where supported)&lt;/item&gt;
      &lt;item&gt;Line in from the onboard midi sythesizer (see below)&lt;/item&gt;
      &lt;item&gt;Line in from external i/o connector for mixing external audio&lt;/item&gt;
      &lt;item&gt;Controlled by the RP2350 via i2c (controlling volumes etc).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is combined with a DREAM SAM2695 “Low power single chip synthesizer with effects and built-in codec”, this is the same chip used on the Serdashop Dreamblaster S2. It is a great device for DOS gaming and other applications, its main features are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;64-voice polyphony (without effects)&lt;/item&gt;
      &lt;item&gt;38-voice polyphony + effects&lt;/item&gt;
      &lt;item&gt;CleanWave soundset&lt;/item&gt;
      &lt;item&gt;General MIDI compatible effects&lt;/item&gt;
      &lt;item&gt;4-band stereo equalizer&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;MPU-401&lt;/head&gt;
    &lt;p&gt;Emulation of intelligent mode MPU-401 is possible thanks to implementation done by PicoGUS base on SoftMPU/HardMPU. The midi output is driven to the internal SAM2695 as well as to an external Midi port. While using external Midi you are able to mute the internal SAM2695, or if you are not using any of the internal sound hardware you can power it down. Planning has been done with the external GPIO to support MIDI IN if ever implemented.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sound Blaster Emulation&lt;/head&gt;
    &lt;p&gt;Sound Blaster emulation on PCMCIA is particularly challenging, as most PCMCIA sockets and cards lack native DMA support. To address this, the PicoPCMCIA implements DMA emulation, similar in spirit to the approach used by the infamous IBM 3D Sound card, resulting in good compatibility with many real-mode and protected-mode games — including the obligatory Doom. The IBM card was essentially the only card to offer this functionality, it seems it may have been that way due to IBM patenting (expired) the concept of DMA emulation with PCMCIA.&lt;/p&gt;
    &lt;p&gt;The core Sound Blaster emulation developed for PicoPCMCIA has been shared with the PicoGUS project, where it is actively used and has greatly benefited from additional community-contributed improvements. Adlib/OPL emulation is borrowed from the PicoGUS implementation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gravis Ultrasound (GUS)&lt;/head&gt;
    &lt;p&gt;Thanks to the incredible work from the PicoGUS, it is now possible to have the worlds first PCMCIA Gravis Ultrasound! Currently this does not support DMA so only some games/demos work. The GUS is a little bit different with its use of TC, but it may be possible to apply the DMA emulation strategies from the SoundBlaster mode to the GUS.&lt;/p&gt;
    &lt;head rend="h2"&gt;CD-ROM Audio&lt;/head&gt;
    &lt;p&gt;The card implements an emulated Panasonic MKE CD-ROM which an be used for both data and audio. The audio at full quality is sent to the TI DSP over i2c and can be used simultaneously with all the other audio functions. This code was shared to the PicoGUS and is currently in use there and has been improved by the community.&lt;/p&gt;
    &lt;head rend="h1"&gt;Storage Emulation&lt;/head&gt;
    &lt;p&gt;While storage emulation is not a primary focus given the ready availability of solutions like CompactFlash, it is supported and continues to evolve. Current implementations include Panasonic MKE CD-ROM emulation as well as linear flash emulation, and ATA/ATAPI emulation should be possible in the future once the PicoIDE project becomes available and code can be shared. Disk images can be BIN/CUE, ISO and are stored on the MicroSD card.&lt;/p&gt;
    &lt;p&gt;There is also a special edge case for the HP 200LX, where the card can emulate an “Accurite Doubleslot” device, allowing an emulated flash card to coexist with networking or sound functionality. This is particularly important on systems with only a single PCMCIA slot, where storage availability is at a premium.&lt;/p&gt;
    &lt;head rend="h1"&gt;USB&lt;/head&gt;
    &lt;p&gt;The USB port for the RP2354 is made available on the external connector. It’s primary purpose is to be used for flashing the card with firmware, however as demonstrated on the PicoGUS and PicoMEM, it can use used for USB Gamdpads and USB Mice which are presented the the host system as legacy gamepad and serial mouse. It has also been demonstrated the latest update to the PicoGUS that accessing flash storage at a reasonable speed is possible via USB.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46708096</guid><pubDate>Wed, 21 Jan 2026 16:43:57 +0000</pubDate></item><item><title>Autonomous (YC F25, AI-native financial advisor at 0% advisory fees) is hiring</title><link>https://atg.science/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46708315</guid><pubDate>Wed, 21 Jan 2026 17:00:54 +0000</pubDate></item></channel></rss>