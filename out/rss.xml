<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 19 Nov 2025 21:08:31 +0000</lastBuildDate><item><title>Learning to Boot from PXE</title><link>https://blog.imraniqbal.org/learning-to-boot-from-pxe/</link><description>&lt;doc fingerprint="76e8eb7771b30c83"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Learning to boot from PXE&lt;/head&gt;
    &lt;p&gt;Posted on&lt;/p&gt;
    &lt;p&gt;I bought a new laptop, the GPD Pocket 4. It came with windows installed by default, and I wanted to install nix on it.&lt;/p&gt;
    &lt;p&gt;I grabbed a usb, &lt;code&gt;dd&lt;/code&gt;'d the nixos iso image on it and tried to boot.
The laptop did not recognize the drive.
Turns out, the drive crapped out, no computer would boot off it.&lt;/p&gt;
    &lt;p&gt;The normal thing to do would've been to just go get a new usb and install off of and go about setting the laptop up. That meant I would either have to go outside or wait for a new usb to arrive. I don't want to outside and I don't want to wait to setup my laptop. I have free time now and I have no clue when I will have free time next.&lt;/p&gt;
    &lt;p&gt;The menu had two other boot options. Something about PXE over ipv4 or ipv6. I only knew that PXE allowed networked boot. So hey, let's use this time to learn something new.&lt;/p&gt;
    &lt;head rend="h3"&gt;# DHCP&lt;/head&gt;
    &lt;p&gt;As I've learned, the first half of this process is DHCP. When a device is connected to the network it sends out a "HEY GIVE ME AN IP" message (I don't actually know how it works and didn't bother to look it up). Then your DHCP service see's this message and responds back with an IP. As part of these requests the client and server can set "options" on these requests which can send additional information. I don't know what the client sets first, but I do know the server needs to set a boot file name and location of a TFTP server. TFTP sort of like FTP.&lt;/p&gt;
    &lt;p&gt;PXE reads the boot file (usually something.pxe) from the TFTP server and then executes its code. Other boot files are then retrieved as needed from the TFTP server.&lt;/p&gt;
    &lt;p&gt;While learning this, folks on the internet dont seem too fond of TFTP, saying it could be slow. There exists iPXE which is supposed to be a better PXE. PXE (like bioses), tend to be manufacturer specific and are not created equal. iPXE tries to be better and supports a bunch of other stuff like (like booting from an ISO, and talking in HTTP). So if this all goes well i get iPXE going, point it to the iso I've already downloaded and I'm off to the races!&lt;/p&gt;
    &lt;p&gt;Spoiler alert, I didn't get to the races.&lt;/p&gt;
    &lt;p&gt;To get iPXE running, the iPXE.pxe executable needs to be served by TFTP. I am running an OPNsense box for my router/firewall and it as enough disk space and ram that I should be able to do this whole process of it. Setting the DHCP stuff is easy enough via the UI. the iPXE client sets a client option on its DHCP requests, so you want to create a tag in OPNsense off it's user-class (iPXE) and respond with a DHCP boot (what the tab in the UI is called) value of the http server.&lt;/p&gt;
    &lt;p&gt;The flow should be:&lt;/p&gt;
    &lt;p&gt;PXE -&amp;gt; Gets TFTP Address -&amp;gt; Downloads and run iPXE iPXE -&amp;gt; Gets HTTP address -&amp;gt; Does iPXE stuff like run our iso&lt;/p&gt;
    &lt;p&gt;The DHCP stuff can be done through the UI so it was. The TFTP stuff was not availble the web ui so has to be done through ssh.&lt;/p&gt;
    &lt;head rend="h3"&gt;# TFTP&lt;/head&gt;
    &lt;p&gt;This was my first time shelling into a BSD box. After this whole process I was left feeling that (Free)BSD is oddly cozy. I can't explain how or why, but it just does. The login prompt from opnsense, the simple shell prompt (csh?), the man pages, the disk layout, the programs. Like even if I didn't have access to all the new version of tools (nvim / rg vs vim / grep) I still got what I wanted done and it just felt cute and cozy.&lt;/p&gt;
    &lt;p&gt;Anyway, OPNsense ships with dnsmasq and dnsmasq can also act as a TFTP server. I found this out when trying to search for a TFTP program to install via the UI. I don't know how to enable it, nor did I want to look it up (via the internet), so I just read the man page.&lt;/p&gt;
    &lt;code&gt;man dnsmasq
&lt;/code&gt;
    &lt;p&gt;Reading the man page was a pleasant experience (or maybe it was just my first time reading something from section 8). It told me exactly what the program could do and how to configure it (just searched for tftp). The conf files were listed in at the bottom, the first being &lt;code&gt;/etc/dnsmasq.conf&lt;/code&gt; which did not exist on my system but &lt;code&gt;/usr/local/etc/dnsmasq.conf&lt;/code&gt; did.&lt;/p&gt;
    &lt;p&gt;The first line of that file warns you not to manually edit the file and near the bottom you see the conf-dir option set to &lt;code&gt;/usr/local/etc/dnsmasq.conf.d&lt;/code&gt;
I saw a README in that conf dir and, doing a cat resulted in this message:&lt;/p&gt;
    &lt;code&gt;cat /usr/local/etc/dnsmasq.conf.d/README
# Dnsmasq plugin directory:
# Add your *.conf files here, read in alphabetical order
&lt;/code&gt;
    &lt;p&gt;Well sure why not lets do that&lt;/p&gt;
    &lt;code&gt;vim /usr/local/etc/dnsmasq.conf.d/10-tftp.conf
enable-tftp
tftp-root=/srv/tftp
:x
mkdir -p /srv/tftp
fetch -r https://boot.ipxe.org/ipxe.efi -o /srv/tftp/
&lt;/code&gt;
    &lt;p&gt;I used the web ui to restart dnsmasq, but you can also use &lt;code&gt;configctl&lt;/code&gt; to do it via shell.
Now when I boot up the laptop I see it load up iPXE but then fail as the http server does not exist. That is progress though, now we just need to serve our iso over http.&lt;/p&gt;
    &lt;p&gt;One thing to note is that nearly all the instructions online focus on legacy/bios boot. All my devices boot via UEFI (which is why we downloaded the efi above instead of the .kpxe file). There are ways to setup DHCP to respond with the appropriate files for both uefi or bios boot, but I dont care enough. There are also other things that try to simplify this whole process like pixieboot and netboot.xyz but I am not interested in them.&lt;/p&gt;
    &lt;head rend="h3"&gt;# HTTP&lt;/head&gt;
    &lt;p&gt;OPNsense runs lighttpd for serving its web ui and I would like to piggy back off it for the iPXE stuff.&lt;/p&gt;
    &lt;p&gt;The trickest part here was finding out the web ui configuration lives at &lt;code&gt;/usr/local/etc/lighttpd_webgui/&lt;/code&gt; via &lt;code&gt;ps&lt;/code&gt;.
I had to disable the ssl redirect option from the web ui and instead add it myself to end of my conf file, due how the confs are loaded. I could not think of a different way of getting the 443 port redirect disabled just for the ipxe paths&lt;/p&gt;
    &lt;code&gt;cat /usr/local/etc/lighttpd_webgui/conf.d/00-ipxe.conf
# Serve /srv/tftp under http://&amp;lt;ip&amp;gt;/ipxe/
alias.url += ( "/ipxe/" =&amp;gt; "/srv/tftp/" )
url.redirect += ( "^/ipxe$" =&amp;gt; "/ipxe/" )

$SERVER["socket"] == "0.0.0.0:80" {
    ssl.engine = "disable"
    $HTTP["url"] !~ "^/ipxe(?:/|$)" {
        $HTTP["host"] =~ "(.*)" {
            url.redirect = ( "^/(.*)" =&amp;gt; "https://%1/$1" )
        }
    }
}

$SERVER["socket"] == "[::]:80" {
    ssl.engine = "disable"
    $HTTP["url"] !~ "^/ipxe(?:/|$)" {
        $HTTP["host"] =~ "(.*)" {
            url.redirect = ( "^/(.*)" =&amp;gt; "https://%1/$1" )
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;I started off with a basic boot.ixpe file&lt;/p&gt;
    &lt;code&gt;#!ipxe
menu Choose an ISO
item nix-minmal NixOS 25.04 Minimal
item nix-gui   NixOS 25.04 GUI
choose target &amp;amp;&amp;amp; goto ${target}

:nix-minimal
sanboot http://10.0.0.1/ipxe/nixos-minimal-25.05.812242.3de8f8d73e35-x86_64-linux.iso
goto menu

:nix-gui
sanboot http://10.0.0.1/ipxe/nixos-graphical-25.05.812242.3de8f8d73e35-x86_64-linux.iso
goto menu
&lt;/code&gt;
    &lt;p&gt;And here is what I spoiled eariler, it didnt work.&lt;/p&gt;
    &lt;p&gt;I would get a boot but then nixos would complain about &lt;code&gt;/mnt/iso&lt;/code&gt; or something being missing and failing to go further.&lt;/p&gt;
    &lt;p&gt;This discussion has better information on why it doesn't work: https://github.com/ipxe/ipxe/discussions/962&lt;/p&gt;
    &lt;head rend="h3"&gt;# Proper netboot files&lt;/head&gt;
    &lt;p&gt;So my dreams of network booting off an iso are crushed, so where do I go from here?&lt;/p&gt;
    &lt;p&gt;Well it turns out the ISO comes with a bootloader, which contains instructions on how to boot a kernel with an initial ram disk (hint this when I learned what &lt;code&gt;initrd&lt;/code&gt; means).
So can't we do the same?
The answer is yes! (or so I think).
I didnt try to extract the files out the iso, but use nix's built in netboot image generator which builds the necessary files.&lt;/p&gt;
    &lt;p&gt;I only had to tweak the generated .ixpe file to include the http urls but everything worked out in the end.&lt;/p&gt;
    &lt;code&gt;cat netboot.ipxe
#!ipxe
# Use the cmdline variable to allow the user to specify custom kernel params
# when chainloading this script from other iPXE scripts like netboot.xyz
kernel http://10.0.0.1/ipxe/bzImage init=/nix/store/hrgkskx4jqdz4nl3p1f4m1dvrr9b3lij-nixos-system-nixos-kexec-25.11pre708350.gfedcba/init initrd=initrd nohibernate loglevel=4 lsm=landlock,yama,bpf ${cmdline}
initrd http://10.0.0.1/ipxe/initrd
boot
&lt;/code&gt;
    &lt;p&gt;I still wonder if I can extract the files from the graphical installer and boot KDE off the network, but now that the OS is installed my interest has waned. Maybe one day I will revisit&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45978245</guid><pubDate>Wed, 19 Nov 2025 11:18:59 +0000</pubDate></item><item><title>Thunderbird adds native Microsoft Exchange email support</title><link>https://blog.thunderbird.net/2025/11/thunderbird-adds-native-microsoft-exchange-email-support/</link><description>&lt;doc fingerprint="ae591a91108cb8dd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Thunderbird Adds Native Microsoft Exchange Email Support&lt;/head&gt;
    &lt;p&gt;If your organization uses Microsoft Exchange-based email, you’ll be happy to hear that Thunderbird’s latest monthly Release version 145, now officially supports native access via the Exchange Web Services (EWS) protocol. With EWS now built directly into Thunderbird, a third-party add-on is no longer required for email functionality. Calendar and address book support for Exchange accounts remain on the roadmap, but email integration is here and ready to use!&lt;/p&gt;
    &lt;head rend="h2"&gt;What changes for Thunderbird users&lt;/head&gt;
    &lt;p&gt;Until now, Thunderbird users in Exchange hosted environments often relied on IMAP/POP protocols or third-party extensions. With full native Exchange support for email, Thunderbird now works more seamlessly in Exchange environments, including full folder listings, message synchronization, folder management both locally and on the server, attachment handling, and more. This simplifies life for users who depend on Exchange for email but prefer Thunderbird as their client.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to get started&lt;/head&gt;
    &lt;p&gt;For many people switching from Outlook to Thunderbird, the most common setup involves Microsoft-hosted Exchange accounts such as Microsoft 365 or Office 365. Thunderbird now uses Microsoft’s standard sign-in process (OAuth2) and automatically detects your account settings, so you can start using your email right away without any extra setup.&lt;/p&gt;
    &lt;p&gt;If this applies to you, setup is straightforward:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Create a new account in Thunderbird 145 or newer.&lt;/item&gt;
      &lt;item&gt;In the new Account Hub, select Exchange (or Exchange Web Services in legacy setup).&lt;/item&gt;
      &lt;item&gt;Let Thunderbird handle the rest!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Important note: If you see something different, or need more details or advice, please see our support page and wiki page. Also, some authentication configurations are not supported yet and you may need to wait for a further update that expands compatibility, please refer to the table below for more details.&lt;/p&gt;
    &lt;head rend="h2"&gt;What functionality is supported now and what’s coming soon&lt;/head&gt;
    &lt;p&gt;As mentioned earlier, EWS support in version 145 currently enables email functionality only. Calendar and address book integration are in active development and will be added in future releases. The chart below provides an at-a-glance view of what’s supported today.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Feature area&lt;/cell&gt;
        &lt;cell&gt;Supported now&lt;/cell&gt;
        &lt;cell&gt;Not yet supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Email – account setup &amp;amp; folder access&lt;/cell&gt;
        &lt;cell&gt;✅ Creating accounts via auto-config with EWS, server-side folder manipulation&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Email – message operations&lt;/cell&gt;
        &lt;cell&gt;✅ Viewing messages, sending, replying/forwarding, moving/copying/deleting&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Email – attachments&lt;/cell&gt;
        &lt;cell&gt;✅ Attachments can be saved and displayed with detach/delete support.&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Search &amp;amp; filtering&lt;/cell&gt;
        &lt;cell&gt;✅ Search subject and body, quick filtering&lt;/cell&gt;
        &lt;cell&gt;❌ Filter actions requiring full body content are not yet supported.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Accounts hosted on Microsoft 365&lt;/cell&gt;
        &lt;cell&gt;✅ Domains using the standard Microsoft OAuth2 endpoint&lt;/cell&gt;
        &lt;cell&gt;❌ Domains requiring custom OAuth2 application and tenant IDs will be supported in the future.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Accounts hosted on-premise&lt;/cell&gt;
        &lt;cell&gt;✅ Password-based Basic authentication&lt;/cell&gt;
        &lt;cell&gt;❌ Password-based NTLM authentication and OAuth2 for on-premise servers are on the roadmap.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Calendar support&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;❌ Not yet implemented – calendar syncing is on the roadmap.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Address book / contacts support&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;❌ Not yet implemented – address book support is on the roadmap.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Microsoft Graph support&lt;/cell&gt;
        &lt;cell&gt;–&lt;/cell&gt;
        &lt;cell&gt;❌ Not yet implemented – Microsoft Graph integration will be added in the future.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Exchange Web Services and Microsoft Graph&lt;/head&gt;
    &lt;p&gt;While many people and organizations still rely on Exchange Web Services (EWS), Microsoft has begun gradually phasing it out in favor of a newer, more modern interface called Microsoft Graph. Microsoft has stated that EWS will continue to be supported for the foreseeable future, but over time, Microsoft Graph will become the primary way to connect to Microsoft 365 services.&lt;/p&gt;
    &lt;p&gt;Because EWS remains widely used today, we wanted to ensure full support for it first to ensure compatibility for existing users. At the same time, we’re actively working to add support for Microsoft Graph, so Thunderbird will be ready as Microsoft transitions to its new standard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Looking ahead&lt;/head&gt;
    &lt;p&gt;While Exchange email is available now, calendar and address book integration is on the way, bringing Thunderbird closer to being a complete solution for Exchange users. For many people, having reliable email access is the most important step, but if you depend on calendar and contact synchronization, we’re working hard to bring this to Thunderbird in the near future, making Thunderbird a strong alternative to Outlook.&lt;/p&gt;
    &lt;p&gt;Keep an eye on future releases for additional support and integrations, but in the meantime, enjoy a smoother Exchange email experience within your favorite email client!&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;If you want to know more about Exchange support in Thunderbird, please refer to the dedicated page on support.mozilla.org. Organization admins can also find out more on the Mozilla wiki page. To follow ongoing and future work in this area, please refer to the relevant meta-bug on Bugzilla.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45978423</guid><pubDate>Wed, 19 Nov 2025 11:45:51 +0000</pubDate></item><item><title>Larry Summers resigns from OpenAI board</title><link>https://www.cnbc.com/2025/11/19/larry-summers-epstein-openai.html</link><description>&lt;doc fingerprint="ee31da039104d639"&gt;
  &lt;main&gt;
    &lt;p&gt;Former Treasury Secretary Larry Summers said Wednesday that he will resign from the board of OpenAI after the release of emails between him and the notorious sex offender Jeffrey Epstein.&lt;/p&gt;
    &lt;p&gt;Summers had announced Monday that he would be stepping back from all public commitments, but it was not immediately clear whether that included his position at the artificial intelligence startup.&lt;/p&gt;
    &lt;p&gt;"I am grateful for the opportunity to have served, excited about the potential of the company, and look forward to following their progress," Summers said in a statement to CNBC.&lt;/p&gt;
    &lt;p&gt;OpenAI's board told CNBC it respects Summers' decision to resign.&lt;/p&gt;
    &lt;p&gt;"We appreciate his many contributions and the perspective he brought to the Board," the OpenAI board of directors said in a statement.&lt;/p&gt;
    &lt;p&gt;Details of Summers' correspondence with Epstein were made public last week after the House Oversight and Government Reform Committee released more than 20,000 documents it obtained pursuant to a subpoena from Epstein's estate. Summers has faced intense scrutiny following the release of those files.&lt;/p&gt;
    &lt;p&gt;Summers joined OpenAI's board in 2023 during a turbulent period for the startup. OpenAI CEO Sam Altman was briefly ousted from the company, though he returned to the chief executive role days later.&lt;/p&gt;
    &lt;p&gt;In the wake of "The Blip," as some OpenAI employees call it, Summers was appointed to the board alongside Bret Taylor, former co-CEO of Salesforce, and Quora CEO Adam D'Angelo, who was the only member of OpenAI's previous board who still held a seat.&lt;/p&gt;
    &lt;p&gt;Axios was first to report about Summers' resignation from the board.&lt;/p&gt;
    &lt;p&gt;President Donald Trump on Friday asked the Department of Justice to investigate the relationship between Epstein and Summers, as well as Epstein's ties to former President Bill Clinton, JPMorgan Chase and billionaire tech investor Reid Hoffman. Trump has been facing renewed pressure over his own past friendship with Epstein.&lt;/p&gt;
    &lt;p&gt;Summers is a former president of Harvard University, and Democratic Sen. Elizabeth Warren of Massachusetts told CNN on Monday that the university should sever ties with him. He announced his intention to step back from his public commitments later that day, but said he will continue to fulfill his teaching obligations at Harvard.&lt;/p&gt;
    &lt;p&gt;"I am deeply ashamed of my actions and recognize the pain they have caused. I take full responsibility for my misguided decision to continue communicating with Mr. Epstein," Summers said in a statement to CNBC on Monday.&lt;/p&gt;
    &lt;p&gt;Congress on Tuesday agreed to pass a bipartisan bill ordering the Department of Justice to release all of its files on Epstein, clearing the path for Trump to sign it into law.&lt;/p&gt;
    &lt;p&gt;WATCH: House overwhelmingly votes to release more Epstein investigation files, sends bill to Senate&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45979190</guid><pubDate>Wed, 19 Nov 2025 13:16:23 +0000</pubDate></item><item><title>The peaceful transfer of power in open source projects</title><link>https://shkspr.mobi/blog/2025/11/the-peaceful-transfer-of-power-in-open-source-projects/</link><description>&lt;doc fingerprint="a49110400ef6e2dd"&gt;
  &lt;main&gt;
    &lt;p&gt;Most of the people who run Open Source projects are mortal. Recent history shows us that they will all eventually die, or get bored, or win the lottery, or get sick, or be conscripted, or lose their mind.&lt;/p&gt;
    &lt;p&gt;If you've ever visited a foreign country's national history museum, I guarantee you've read this little snippet:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;King Whatshisface was a wise and noble ruler who bought peace and prosperity to all the land.&lt;/p&gt;
      &lt;p&gt;Upon his death, his heirs waged bloody war over rightful succession which plunged the country into a hundred years of hardship.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The great selling point of democracy is that it allows for the peaceful transition of power. Most modern democracies have rendered civil war almost unthinkable. Sure, you might not like the guy currently in charge, but there are well established mechanisms to limit their power and kick them out if they misbehave. If they die in office, there's an obvious and understood hierarchy for who follows them.&lt;/p&gt;
    &lt;p&gt;Most Open Source projects start small - just someone in their spare room tinkering for fun. Unexpectedly, they grow into a behemoth which now powers half the world. These mini-empires are fragile. The most popular method of governance is the Benevolent Dictator For Life model. The founder of the project controls everything. But, as I've said before, BDFL only works if the D is genuinely B. Otherwise the FL becomes FML.&lt;/p&gt;
    &lt;p&gt;The last year has seen several BDFLs act like Mad Kings. They become tyrannical despots, lashing out at their own volunteers. They execute takeovers of community projects. They demand fealty and tithes. Like dragons, they become quick to anger when their brittle egos are tested. Spineless courtiers carry out deluded orders while pilfering the coffers.&lt;/p&gt;
    &lt;p&gt;Which is why I am delighted that the Mastodon project has shown a better way to behave.&lt;/p&gt;
    &lt;p&gt;In "The Future is Ours to Build - Together" they describe perfectly how to gracefully and peacefully transfer power. There are no VCs bringing in their MBA-brained lackeys to extract maximum value while leaving a rotting husk. No one is seizing community assets and jealously hoarding them. Opaque financial structures and convoluted agreements are prominent in their absence.&lt;/p&gt;
    &lt;p&gt;Eugen Rochko, the outgoing CEO, has a remarkably honest blog post about the transition. I wouldn't wish success on my worst enemy. He talks plainly about the reality of dealing with the pressure and how he might have been a limiting factor on Mastodon's growth. That's a far step removed from the ego-centric members of The Cult of The Founder with their passionate belief in the Divine Right of Kings.&lt;/p&gt;
    &lt;p&gt;Does your tiny OSS script need a succession plan? Probably not. Do you have several thousand NPM installs per day? It might be worth working out who you can share responsibility with if you are unexpectedly raptured. Do you think that your project is going to last for a thousand years? Build an organisation which won't crumble the moment its founder is arrested for their predatory behaviour on tropical islands.&lt;/p&gt;
    &lt;p&gt;I'm begging project leaders everywhere - please read up on the social contract and the consent of the governed. Or, if reading is too woke, just behave like grown-ups rather than squabbling tweenagers.&lt;/p&gt;
    &lt;p&gt;It is a sad inevitability that, eventually, we will all be nothing but memories. The bugs that we create live after us, the patches are oft interrèd with our code. Let it be so with all Open Source projects.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45979232</guid><pubDate>Wed, 19 Nov 2025 13:20:42 +0000</pubDate></item><item><title>Europe is scaling back GDPR and relaxing AI laws</title><link>https://www.theverge.com/news/823750/european-union-ai-act-gdpr-changes</link><description>&lt;doc fingerprint="ff1cf01c3abf8d9c"&gt;
  &lt;main&gt;
    &lt;p&gt;After years of staring down the world’s biggest tech companies and setting the bar for tough regulation worldwide, Europe has blinked. Under intense pressure from industry and the US government, Brussels is stripping protections from its flagship General Data Protection Regulation (GDPR) — including simplifying its infamous cookie permission pop-ups — and relaxing or delaying landmark AI rules in an effort to cut red tape and revive sluggish economic growth.&lt;/p&gt;
    &lt;head rend="h1"&gt;Europe is scaling back its landmark privacy and AI laws&lt;/head&gt;
    &lt;p&gt;The EU folds under Big Tech’s pressure.&lt;/p&gt;
    &lt;p&gt;The EU folds under Big Tech’s pressure.&lt;/p&gt;
    &lt;p&gt;The changes, proposed by the European Commission, the bloc’s executive branch, changes core elements of the GDPR, making it easier for companies to share anonymized and pseudonymized personal datasets. They would allow AI companies to legally use personal data to train AI models, so long as that training complies with other GDPR requirements.&lt;/p&gt;
    &lt;p&gt;The proposal also waters down a key part of Europe’s sweeping artificial intelligence rules, the AI Act, which came into force in 2024 but had many elements that would only come into effect later. The change extends the grace period for rules governing high-risk AI systems that pose “serious risks” to health, safety, or fundamental rights, which were due to come into effect next summer. The rules will now only apply once it’s confirmed that “the needed standards and support tools are available” to AI companies.&lt;/p&gt;
    &lt;p&gt;One change that’s likely to please almost everyone is a reduction in Europe’s ubiquitous cookie banners and pop-ups. Under the new proposal, some “non-risk” cookies won’t trigger pop-ups at all, and users would be able to control others from central browser controls that apply to websites broadly.&lt;/p&gt;
    &lt;p&gt;Other amendments in the new Digital Omnibus include simplified AI documentation requirements for smaller companies, a unified interface for companies to report cybersecurity incidents, and centralizing oversight of AI into the bloc’s AI Office.&lt;/p&gt;
    &lt;p&gt;“This is being done in the European way.”&lt;/p&gt;
    &lt;p&gt;“We have all the ingredients in the EU to succeed. But our companies, especially our start-ups and small businesses, are often held back by layers of rigid rules,” said Henna Virkkunen, executive vice-president for tech sovereignty at the European Commission. “By cutting red tape, simplifying EU laws, opening access to data and introducing a common European Business Wallet we are giving space for innovation to happen and to be marketed in Europe. This is being done in the European way: by making sure that fundamental rights of users remain fully protected.”&lt;/p&gt;
    &lt;p&gt;The proposal now heads to the European Parliament and the EU’s 27 member states — where it will need a qualified majority — for approval, a process that could drag on for months and potentially introduce significant changes.&lt;/p&gt;
    &lt;p&gt;The proposed overhaul won’t land quietly in Brussels, and if the development of the GDPR and AI Act are anything to go by, a political and lobbying firestorm is on its way. The GDPR is a cornerstone of Europe’s tech strategy and as close to sacred as a policy can be. Leaked drafts have already provoked outrage among civil rights groups and politicians, who have accused the Commission of weakening fundamental safeguards and bowing to pressure from Big Tech.&lt;/p&gt;
    &lt;p&gt;The decision follows months of intense pressure from Big Tech and Donald Trump — as well as high-profile internal figures like ex-Italian prime minister and former head of the European Central Bank Mario Draghi — urging the bloc to weaken burdensome tech regulation. The Commission has sought to frame the changes as simplifying the EU’s tech laws, not weakening them – a way of soothing growing fears in Brussels that its tough rules are hampering its ability to compete globally. With very few exceptions, Europe doesn’t have any credible competitors in the global AI race, which is dominated by US and Chinese companies like DeepSeek, Google, and OpenAI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45980117</guid><pubDate>Wed, 19 Nov 2025 14:41:30 +0000</pubDate></item><item><title>Launch HN: Mosaic (YC W25) – Agentic Video Editing</title><link>https://mosaic.so</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45980760</guid><pubDate>Wed, 19 Nov 2025 15:28:04 +0000</pubDate></item><item><title>Meta Segment Anything Model 3</title><link>https://ai.meta.com/sam3/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45982073</guid><pubDate>Wed, 19 Nov 2025 17:14:51 +0000</pubDate></item><item><title>Static Web Hosting on the Intel N150: FreeBSD, SmartOS, NetBSD, OpenBSD and Linu</title><link>https://it-notes.dragas.net/2025/11/19/static-web-hosting-intel-n150-freebsd-smartos-netbsd-openbsd-linux/</link><description>&lt;doc fingerprint="362d517fea0ed7c2"&gt;
  &lt;main&gt;
    &lt;p&gt;I often get very specific infrastructure requests from clients. Most of the time it is some form of hosting. My job is usually to suggest and implement the setup that fits their goals, skills and long term plans.&lt;/p&gt;
    &lt;p&gt;If there are competent technicians on the other side, and they are willing to learn or already comfortable with Unix style systems, my first choices are usually one of the BSDs or an illumos distribution. If they need a control panel, or they already have a lot of experience with a particular stack that will clearly help them, I will happily use Linux and it usually delivers solid, reliable results.&lt;/p&gt;
    &lt;p&gt;Every now and then someone asks the question I like the least:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âBut how does it perform compared to X or Y?â&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have never been a big fan of benchmarks. At best they capture a very specific workload on a very specific setup. They are almost never a perfect reflection of what will happen in the real world.&lt;/p&gt;
    &lt;p&gt;For example, I discovered that idle bhyve VMs seem to use fewer resources when the host is illumos than when the host is FreeBSD. It looks strange at first sight, but the illumos people are clearly working very hard on this, and the result is a very capable and efficient platform.&lt;/p&gt;
    &lt;p&gt;Despite my skepticism, from time to time I enjoy running some comparative tests. I already did it with Proxmox KVM versus FreeBSD bhyve, and I also compared Jails, Zones, bhyve and KVM on the same Intel N150 box. That led to the FreeBSD vs SmartOS article where I focused on CPU and memory performance on this small mini PC.&lt;/p&gt;
    &lt;p&gt;This time I wanted to do something simpler, but also closer to what I see every day: static web hosting.&lt;/p&gt;
    &lt;p&gt;Instead of synthetic CPU or I/O tests, I wanted to measure how different operating systems behave when they serve a small static site with nginx, both over HTTP and HTTPS.&lt;/p&gt;
    &lt;p&gt;This is not meant to be a super rigorous benchmark. I used the default nginx packages, almost default configuration, and did not tune any OS specific kernel settings. In my experience, careful tuning of kernel and network parameters can easily move numbers by several tens of percentage points. The problem is that very few people actually spend time chasing such optimizations. Much more often, once a limit is reached, someone yells âwe need mooooar powaaaarâ while the real fix would be to tune the existing stack a bit.&lt;/p&gt;
    &lt;p&gt;So the question I want to answer here is more modest and more practical:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;With default nginx and a small static site, how much does the choice of host OS really matter on this Intel N150 mini PC?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Spoiler: less than people think, at least for plain HTTP. Things get more interesting once TLS enters the picture.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Disclaimer&lt;/p&gt;&lt;lb/&gt;These benchmarks are a snapshot of my specific hardware, network and configuration. They are useful to compare relative behavior on this setup. They are not a universal ranking of operating systems. Different CPUs, NICs, crypto extensions, kernel versions or nginx builds can completely change the picture.&lt;/quote&gt;
    &lt;head rend="h2"&gt;Test setup&lt;/head&gt;
    &lt;p&gt;The hardware is the same Intel N150 mini PC I used in my previous tests: a small, low power box that still has enough cores to be interesting for lab and small production workloads.&lt;/p&gt;
    &lt;p&gt;On it, I installed several operating systems and environments, always on the bare metal, not nested inside each other. On each OS I installed nginx from the official packages.&lt;/p&gt;
    &lt;head rend="h3"&gt;Software under test&lt;/head&gt;
    &lt;p&gt;On the host:&lt;/p&gt;
    &lt;p&gt;SmartOS, with:&lt;lb/&gt; - a Debian 12 LX zone&lt;lb/&gt; - an Alpine Linux 3.22 LX zone&lt;lb/&gt; - a native SmartOS zone &lt;/p&gt;
    &lt;p&gt;FreeBSD 14.3-RELEASE:&lt;lb/&gt; - nginx running inside a native jail &lt;/p&gt;
    &lt;p&gt;OpenBSD 7.8:&lt;lb/&gt; - nginx on the host &lt;/p&gt;
    &lt;p&gt;NetBSD 10.1:&lt;lb/&gt; - nginx on the host &lt;/p&gt;
    &lt;p&gt;Debian 13.2:&lt;lb/&gt; - nginx on the host &lt;/p&gt;
    &lt;p&gt;Alpine Linux 3.22:&lt;lb/&gt; - nginx on the host &lt;/p&gt;
    &lt;p&gt;I also tried to include DragonFlyBSD, but the NIC in this box is not supported. Using a different NIC just for one OS would have made the comparison meaningless, so I excluded it.&lt;/p&gt;
    &lt;head rend="h3"&gt;nginx configuration&lt;/head&gt;
    &lt;p&gt;In all environments:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;nginx was installed from the system packages&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;worker_processes&lt;/code&gt;was set to&lt;code&gt;auto&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;the web root contained the same static content&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The important part is that I used exactly the same &lt;code&gt;nginx.conf&lt;/code&gt; file for all operating systems and all combinations in this article. I copied the same configuration file verbatim to every host, jail and zone. The only changes were the IP address and file paths where needed, for example for the TLS certificate and key. &lt;/p&gt;
    &lt;p&gt;The static content was a default build of the example site generated by BSSG, my Bash static site generator. The web root was the same logical structure on every OS and container type.&lt;/p&gt;
    &lt;p&gt;There is no OS specific tuning in the configuration and no kernel level tweaks. This is very close to a âpackage install plus minimal configâ situation.&lt;/p&gt;
    &lt;head rend="h3"&gt;TLS configuration&lt;/head&gt;
    &lt;p&gt;For HTTPS I used a very simple configuration, identical on every host.&lt;/p&gt;
    &lt;p&gt;Self signed certificate created with:&lt;/p&gt;
    &lt;code&gt;openssl req -x509 -newkey rsa:4096 -nodes -keyout server.key -out server.crt -days 365 -subj "/CN=localhost"  
&lt;/code&gt;
    &lt;p&gt;Example nginx &lt;code&gt;server&lt;/code&gt; block for HTTPS (simplified): &lt;/p&gt;
    &lt;code&gt;server {  
listen 443 ssl http2;  
listen [::]:443 ssl http2;  

server_name _;  

ssl_certificate /etc/nginx/ssl/server.crt;  
ssl_certificate_key /etc/nginx/ssl/server.key;  

root /var/www/html;  
index index.html index.htm;  

location / {  
try_files $uri $uri/ =404;  
}  
}  
&lt;/code&gt;
    &lt;p&gt;The HTTP virtual host is also the same everywhere, with the root pointing to the BSSG example site.&lt;/p&gt;
    &lt;head rend="h3"&gt;Load generator&lt;/head&gt;
    &lt;p&gt;The tests were run from my workstation on the same LAN:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;client host: a mini PC machine connected at 2.5 Gbit/s&lt;/item&gt;
      &lt;item&gt;switch: 2.5 Gbit/s&lt;/item&gt;
      &lt;item&gt;test tool: &lt;code&gt;wrk&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For each target host I ran:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;wrk -t4 -c50 -d10s http://IP&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;wrk -t4 -c10 -d10s http://IP&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;wrk -t4 -c50 -d10s https://IP&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;wrk -t4 -c10 -d10s https://IP&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each scenario was executed multiple times to reduce noise; the numbers below are medians (or very close to them) from the runs.&lt;/p&gt;
    &lt;head rend="h2"&gt;The contenders&lt;/head&gt;
    &lt;p&gt;To keep things readable, I will refer to each setup as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SmartOS Debian LX â SmartOS host, Debian 12 LX zone&lt;/item&gt;
      &lt;item&gt;SmartOS Alpine LX â SmartOS host, Alpine 3.22 LX zone&lt;/item&gt;
      &lt;item&gt;SmartOS Native â SmartOS host, native zone&lt;/item&gt;
      &lt;item&gt;FreeBSD Jail â FreeBSD 14.3-RELEASE, nginx in a jail&lt;/item&gt;
      &lt;item&gt;OpenBSD Host â OpenBSD 7.8, nginx on the host&lt;/item&gt;
      &lt;item&gt;NetBSD Host â NetBSD 10.1, nginx on the host&lt;/item&gt;
      &lt;item&gt;Debian Host â Debian 13.2, nginx on the host&lt;/item&gt;
      &lt;item&gt;Alpine Host â Alpine 3.22, nginx on the host&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Everything uses the same nginx configuration file and the same static site.&lt;/p&gt;
    &lt;head rend="h2"&gt;Static HTTP results&lt;/head&gt;
    &lt;p&gt;Let us start with plain HTTP, since this removes TLS from the picture and focuses on the kernel, network stack and nginx itself.&lt;/p&gt;
    &lt;head rend="h3"&gt;HTTP, 4 threads, 50 concurrent connections&lt;/head&gt;
    &lt;p&gt;Approximate median &lt;code&gt;wrk&lt;/code&gt; results: &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Environment&lt;/cell&gt;
        &lt;cell role="head"&gt;HTTP 50 connections&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SmartOS Debian LX&lt;/cell&gt;
        &lt;cell&gt;~46.2 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SmartOS Alpine LX&lt;/cell&gt;
        &lt;cell&gt;~49.2 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SmartOS Native&lt;/cell&gt;
        &lt;cell&gt;~63.7 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;FreeBSD Jail&lt;/cell&gt;
        &lt;cell&gt;~63.9 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;OpenBSD Host&lt;/cell&gt;
        &lt;cell&gt;~64.1 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;NetBSD Host&lt;/cell&gt;
        &lt;cell&gt;~64.0 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Debian Host&lt;/cell&gt;
        &lt;cell&gt;~63.8 k&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Alpine Host&lt;/cell&gt;
        &lt;cell&gt;~63.9 k&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Two things stand out:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;All the native or jail/container setups on the hosts that are not LX zones cluster around 63 to 64k requests per second.&lt;/item&gt;
      &lt;item&gt;The two SmartOS LX zones sit slightly lower, in the 46 to 49k range, which is still very respectable for this hardware.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In other words, as long as you are on the host or in something very close to it (FreeBSD jail, SmartOS native zone, NetBSD, OpenBSD, Linux on bare metal), static HTTP on nginx will happily max out around 64k requests per second with this small Intel N150 CPU.&lt;/p&gt;
    &lt;p&gt;The Debian and Alpine LX zones on SmartOS are a bit slower, but not dramatically so. They still deliver close to 50k requests per second and, in a real world scenario, you would probably saturate the network or the client long before hitting those numbers.&lt;/p&gt;
    &lt;head rend="h3"&gt;HTTP, 4 threads, 10 concurrent connections&lt;/head&gt;
    &lt;p&gt;With fewer concurrent connections, absolute throughput drops, but the relative picture is similar:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SmartOS Native around 44k&lt;/item&gt;
      &lt;item&gt;NetBSD and Alpine Host around 34 to 35k&lt;/item&gt;
      &lt;item&gt;FreeBSD, Debian, OpenBSD around 31 to 33k&lt;/item&gt;
      &lt;item&gt;The SmartOS LX zones sit slightly below, around 35 to 37k req/s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The important conclusion is simple:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;For plain HTTP static hosting, once nginx is installed and correctly configured, the choice between these operating systems makes very little difference on this hardware. Zones and jails add negligible overhead, LX zones add a small one.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;If you are only serving static content over HTTP, your choice of OS should be driven by other factors: ecosystem, tooling, update strategy, your own expertise and preference.&lt;/p&gt;
    &lt;head rend="h2"&gt;Static HTTPS results&lt;/head&gt;
    &lt;p&gt;TLS is where things start to diverge more clearly and where CPU utilization becomes interesting.&lt;/p&gt;
    &lt;head rend="h3"&gt;HTTPS, 4 threads, 50 concurrent connections&lt;/head&gt;
    &lt;p&gt;Approximate medians:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Environment&lt;/cell&gt;
        &lt;cell role="head"&gt;HTTPS 50 connections&lt;/cell&gt;
        &lt;cell role="head"&gt;CPU notes at 50 HTTPS connections&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SmartOS Debian LX&lt;/cell&gt;
        &lt;cell&gt;~51.4 k&lt;/cell&gt;
        &lt;cell&gt;CPU saturated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SmartOS Alpine LX&lt;/cell&gt;
        &lt;cell&gt;~40.4 k&lt;/cell&gt;
        &lt;cell&gt;CPU saturated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SmartOS Native&lt;/cell&gt;
        &lt;cell&gt;~52.8 k&lt;/cell&gt;
        &lt;cell&gt;CPU saturated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;FreeBSD Jail&lt;/cell&gt;
        &lt;cell&gt;~62.9 k&lt;/cell&gt;
        &lt;cell&gt;around 60% CPU idle&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OpenBSD Host&lt;/cell&gt;
        &lt;cell&gt;~39.7 k&lt;/cell&gt;
        &lt;cell&gt;CPU saturated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;NetBSD Host&lt;/cell&gt;
        &lt;cell&gt;~40.4 k&lt;/cell&gt;
        &lt;cell&gt;CPU at 100%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Debian Host&lt;/cell&gt;
        &lt;cell&gt;~62.8 k&lt;/cell&gt;
        &lt;cell&gt;about 20% CPU idle&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Alpine Host&lt;/cell&gt;
        &lt;cell&gt;~62.4 k&lt;/cell&gt;
        &lt;cell&gt;small idle headroom, around 7% idle&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;These numbers tell a more nuanced story.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;FreeBSD, Debian and Alpine on bare metal form a âfast TLSâ group.&lt;/p&gt;&lt;lb/&gt;All three sit around 62 to 63k requests per second with 50 concurrent HTTPS connections.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;FreeBSD does this while using significantly less CPU.&lt;/p&gt;&lt;lb/&gt;During the HTTPS tests with 50 connections, the FreeBSD host still had around 60% CPU idle. It is the platform that handled TLS load most comfortably in terms of CPU headroom.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Debian and Alpine are close in throughput, but push the CPU harder.&lt;/p&gt;&lt;lb/&gt;Debian still had some idle time left, Alpine even less. In practice, all three are excellent here, but FreeBSD gives you more room before you hit the wall.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;SmartOS, NetBSD and OpenBSD form a âgood but heavierâ TLS group.&lt;/p&gt;&lt;lb/&gt;Their HTTPS throughput is in the 40 to 52k req/s range and they reach full CPU usage at 50 concurrent connections. OpenBSD and NetBSD stabilize around 39 to 40k req/s. SmartOS native and the Debian LX zone manage slightly better (around 51 to 53k) but still with the CPU pegged.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;HTTPS, 4 threads, 10 concurrent connections&lt;/head&gt;
    &lt;p&gt;With lower concurrency:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FreeBSD, Debian and Alpine still sit in roughly the 29 to 31k req/s range&lt;/item&gt;
      &lt;item&gt;SmartOS Native and LX zones are in the mid to high 30k range&lt;/item&gt;
      &lt;item&gt;NetBSD and OpenBSD sit around 26 to 27k req/s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The relative pattern is the same: for this TLS workload, FreeBSD and modern Linux distributions on bare metal appear to make better use of the cryptographic capabilities of the CPU, delivering higher throughput or more headroom or both.&lt;/p&gt;
    &lt;head rend="h2"&gt;What TLS seems to highlight&lt;/head&gt;
    &lt;p&gt;The HTTPS tests point to something that is not about nginx itself, but about the TLS stack and how well it can exploit the hardware.&lt;/p&gt;
    &lt;p&gt;On this Intel N150, my feeling is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FreeBSD, with the userland and crypto stack I am running, is very efficient at TLS here. It delivers the highest throughput while keeping plenty of CPU in reserve.&lt;/item&gt;
      &lt;item&gt;Debian and Alpine, with their recent kernels and libraries, are also strong performers, close to FreeBSD in throughput, but with less idle CPU.&lt;/item&gt;
      &lt;item&gt;NetBSD, OpenBSD and SmartOS (native and LX) are still perfectly capable of serving a lot of HTTPS traffic, but they have to work harder to keep up and they hit 100% CPU much earlier.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This matches what I see in day to day operations: TLS performance is often less about ânginx vs something elseâ and more about the combination of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the TLS library version and configuration&lt;/item&gt;
      &lt;item&gt;how well the OS uses the CPU crypto instructions&lt;/item&gt;
      &lt;item&gt;kernel level details in the network and crypto paths&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I suspect the differences here are mostly due to how each system combines its TLS stack (OpenSSL, LibreSSL and friends), its kernel and its hardware acceleration support. It would take a deeper dive into profiling and configuration knobs to attribute the gaps precisely.&lt;/p&gt;
    &lt;p&gt;In any case, on this specific mini PC, if I had to pick a platform to handle a large amount of HTTPS static traffic, FreeBSD, Debian and Alpine would be my first candidates, in that order.&lt;/p&gt;
    &lt;head rend="h2"&gt;Zones, jails and containers: overhead in practice&lt;/head&gt;
    &lt;p&gt;Another interesting part of the story is the overhead introduced by different isolation technologies.&lt;/p&gt;
    &lt;p&gt;From these tests and the previous virtualization article on the same N150 machine, the picture is consistent:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;FreeBSD jails behave almost like bare metal.&lt;/p&gt;&lt;lb/&gt;For both HTTP and HTTPS, running nginx in a jail on FreeBSD 14.3-RELEASE produces numbers practically identical to native hosts on other OSes. CPU utilization is excellent, especially under TLS.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;SmartOS native zones are also very close to the metal.&lt;/p&gt;&lt;lb/&gt;Static HTTP performance reaches the same 64k req/s region and HTTPS is only slightly behind the âfast TLSâ group, although with higher CPU usage.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;SmartOS LX zones introduce a noticeable but modest overhead.&lt;/p&gt;&lt;lb/&gt;Both Debian and Alpine LX zones on SmartOS perform slightly worse than the native zone or FreeBSD jails. For static HTTP they are still very fast. For HTTPS the Debian LX zone remains competitive but costs more CPU, while the Alpine LX zone is slower.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is not a surprise. LX zones translate Linux system calls on top of the illumos kernel and there is a cost for that. The important point is that the cost is not catastrophic. On a bigger CPU you would probably not notice it unless you are really pushing the limits.&lt;/p&gt;
    &lt;head rend="h2"&gt;What this means for real workloads&lt;/head&gt;
    &lt;p&gt;It is easy to get lost in tables and percentages, so let us go back to the initial question.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A client wants static hosting.&lt;/p&gt;&lt;lb/&gt;Does the choice between FreeBSD, SmartOS, NetBSD or Linux matter in terms of performance?&lt;/quote&gt;
    &lt;p&gt;For plain HTTP on this hardware, with nginx and the same configuration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Not really.&lt;lb/&gt;All the native hosts and FreeBSD jails deliver roughly the same maximum throughput, in the 63 to 64k req/s range. SmartOS LX zones are slightly slower but still strong.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For HTTPS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Yes, it starts to matter a bit more.&lt;/item&gt;
      &lt;item&gt;FreeBSD stands out for how relaxed the CPU is under high TLS load.&lt;/item&gt;
      &lt;item&gt;Debian and Alpine are very close in throughput, with more CPU used but still with some headroom.&lt;/item&gt;
      &lt;item&gt;SmartOS, NetBSD and OpenBSD can still push a lot of HTTPS traffic, but they reach 100% CPU earlier and stabilize at lower request rates.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Does this mean you should always choose FreeBSD or Debian or Alpine for static HTTPS hosting?&lt;/p&gt;
    &lt;p&gt;Not necessarily.&lt;/p&gt;
    &lt;p&gt;In real deployments, the bottleneck is rarely the TLS performance of a single node serving a small static site. Network throughput, storage, logging, reverse proxies, CDNs and application layers all play a role.&lt;/p&gt;
    &lt;p&gt;However, knowing that FreeBSD and current Linux distributions can squeeze more out of a small CPU under TLS is useful when you are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;sizing hardware for small VPS nodes that must serve many HTTPS requests&lt;/item&gt;
      &lt;item&gt;planning to consolidate multiple services on a low power box&lt;/item&gt;
      &lt;item&gt;deciding whether you can afford to keep some CPU aside for other tasks (cache, background jobs, monitoring, and so on)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As always, the right answer depends on the complete picture: your skills, your tooling, your backups, your monitoring, the rest of your stack, and your tolerance for troubleshooting when things go sideways.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;From these small tests, my main takeaways are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Static HTTP is basically solved on all these platforms.&lt;/p&gt;&lt;lb/&gt;On a modest Intel N150, every system tested can push around 64k static HTTP requests per second with nginx set to almost default settings. For many use cases, that is already more than enough.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;TLS performance is where the OS and crypto stack start to matter.&lt;/p&gt;&lt;lb/&gt;FreeBSD, Debian and Alpine squeeze more HTTPS requests out of the N150, and FreeBSD in particular does it with a surprising amount of idle CPU left. NetBSD, OpenBSD and SmartOS need more CPU to reach similar speeds and stabilize at lower throughput once the CPU is saturated.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Jails and native zones are essentially free, LX zones cost a bit more.&lt;/p&gt;&lt;lb/&gt;FreeBSD jails and SmartOS native zones show very little overhead for this workload. SmartOS LX zones are still perfectly usable, but if you are chasing every last request per second you will see the cost of the translation layer.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Benchmarks are only part of the story.&lt;/p&gt;&lt;lb/&gt;If your team knows OpenBSD inside out and has tooling, scripts and workflows built around it, you might happily accept using more CPU on TLS in exchange for security features, simplicity and familiarity. The same goes for NetBSD or SmartOS in environments where their specific strengths shine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I will not choose an operating system for a client just because a benchmark looks nicer. These numbers are one of the many inputs I consider. What matters most is always the combination of reliability, security, maintainability and the human beings who will have to operate the&lt;lb/&gt; system at three in the morning when something goes wrong. &lt;/p&gt;
    &lt;p&gt;Still, it is nice to know that if you put a tiny Intel N150 in front of a static site and you pick FreeBSD or a modern Linux distribution for HTTPS, you are giving that little CPU a fair chance to shine.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45982162</guid><pubDate>Wed, 19 Nov 2025 17:22:46 +0000</pubDate></item><item><title>Show HN: DNS Benchmark Tool – Compare and monitor resolvers</title><link>https://github.com/frankovo/dns-benchmark-tool</link><description>&lt;doc fingerprint="2b0bb65311a27490"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Part of BuildTools - Network Performance Suite&lt;/head&gt;
    &lt;p&gt;Fast, comprehensive DNS performance testing with DNSSEC validation, DoH/DoT support, and enterprise features&lt;/p&gt;
    &lt;code&gt;pip install dns-benchmark-tool
dns-benchmark benchmark --use-defaults&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;🎉 1,400+ downloads this week! Thank you to our growing community.&lt;/p&gt;&lt;lb/&gt;📢 Want multi-region testing? Join the waitlist →&lt;/quote&gt;
    &lt;p&gt;Real Time Tracking&lt;/p&gt;
    &lt;p&gt;We’ve added three powerful CLI commands to make DNS benchmarking even more versatile:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;🚀 top — quick ranking of resolvers by speed and reliability&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;📊 compare — side‑by‑side benchmarking with detailed statistics and export options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;🔄 monitoring — continuous performance tracking with alerts and logging&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Quick resolver ranking
dns-benchmark top

# Compare resolvers side-by-side
dns-benchmark compare Cloudflare Google Quad9 --show-details

# Run monitoring for 1 hour with alerts
dns-benchmark monitoring --use-defaults --interval 30 --duration 3600 \
  --alert-latency 150 --alert-failure-rate 5 --output monitor.log&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DNS Benchmark Tool &lt;list rend="ul"&gt;&lt;item&gt;Part of BuildTools - Network Performance Suite&lt;/item&gt;&lt;item&gt;🎉 Today’s Release Highlights&lt;/item&gt;&lt;item&gt;Table of Contents&lt;/item&gt;&lt;item&gt;🎯 Why This Tool?&lt;/item&gt;&lt;item&gt;Quick start&lt;/item&gt;&lt;item&gt;✨ Key Features&lt;/item&gt;&lt;item&gt;🔧 Advanced Capabilities&lt;/item&gt;&lt;item&gt;💼 Use Cases&lt;/item&gt;&lt;item&gt;📦 Installation &amp;amp; Setup&lt;/item&gt;&lt;item&gt;📖 Usage Examples&lt;/item&gt;&lt;item&gt;🔧 Utilities&lt;/item&gt;&lt;item&gt;Complete usage guide&lt;/item&gt;&lt;item&gt;🔍 README Adjustments for Final Patch&lt;/item&gt;&lt;item&gt;⚡ CLI Commands&lt;/item&gt;&lt;item&gt;📊 Analysis Enhancements&lt;/item&gt;&lt;item&gt;⚡ Best Practices&lt;/item&gt;&lt;item&gt;Feedback &amp;amp; Community Input&lt;/item&gt;&lt;item&gt;⚙️ Configuration Files&lt;/item&gt;&lt;item&gt;Output formats&lt;/item&gt;&lt;item&gt;Performance optimization&lt;/item&gt;&lt;item&gt;Troubleshooting&lt;/item&gt;&lt;item&gt;Automation &amp;amp; CI&lt;/item&gt;&lt;item&gt;Screenshots&lt;/item&gt;&lt;item&gt;Getting help&lt;/item&gt;&lt;item&gt;Release workflow&lt;/item&gt;&lt;item&gt;🌐 Hosted Version (Coming Soon)&lt;/item&gt;&lt;item&gt;🛣️ Roadmap&lt;/item&gt;&lt;item&gt;🤝 Contributing&lt;/item&gt;&lt;item&gt;❓ FAQ&lt;/item&gt;&lt;item&gt;🔗 Links &amp;amp; Support&lt;/item&gt;&lt;item&gt;License&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;DNS resolution is often the hidden bottleneck in network performance. A slow resolver can add hundreds of milliseconds to every request.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;⏱️ Hidden Bottleneck: DNS can add 300ms+ to every request&lt;/item&gt;
      &lt;item&gt;🤷 Unknown Performance: Most developers never test their DNS&lt;/item&gt;
      &lt;item&gt;🌍 Location Matters: "Fastest" resolver depends on where YOU are&lt;/item&gt;
      &lt;item&gt;🔒 Security Varies: DNSSEC, DoH, DoT support differs wildly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;dns-benchmark-tool helps you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🔍 Find the fastest DNS resolver for YOUR location&lt;/item&gt;
      &lt;item&gt;📊 Get real data - P95, P99, jitter, consistency scores&lt;/item&gt;
      &lt;item&gt;🛡️ Validate security - DNSSEC verification built-in&lt;/item&gt;
      &lt;item&gt;🚀 Test at scale - 100+ concurrent queries in seconds&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Developers optimizing API performance&lt;/item&gt;
      &lt;item&gt;✅ DevOps/SRE validating resolver SLAs&lt;/item&gt;
      &lt;item&gt;✅ Self-hosters comparing Pi-hole/Unbound vs public DNS&lt;/item&gt;
      &lt;item&gt;✅ Network admins running compliance checks&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install dns-benchmark-tool&lt;/code&gt;
    &lt;code&gt;# Test default resolvers against popular domains
dns-benchmark benchmark --use-defaults&lt;/code&gt;
    &lt;p&gt;Results are automatically saved to &lt;code&gt;./benchmark_results/&lt;/code&gt; with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Summary CSV with statistics&lt;/item&gt;
      &lt;item&gt;Detailed raw data&lt;/item&gt;
      &lt;item&gt;Optional PDF/Excel reports&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's it! You just benchmarked 5 DNS resolvers against 10 domains.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Async queries - Test 100+ resolvers simultaneously&lt;/item&gt;
      &lt;item&gt;Multi-iteration - Run benchmarks multiple times for accuracy&lt;/item&gt;
      &lt;item&gt;Statistical analysis - Mean, median, P95, P99, jitter, consistency&lt;/item&gt;
      &lt;item&gt;Cache control - Test with/without DNS caching&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DNSSEC validation - Verify cryptographic trust chains&lt;/item&gt;
      &lt;item&gt;DNS-over-HTTPS (DoH) - Encrypted DNS benchmarking&lt;/item&gt;
      &lt;item&gt;DNS-over-TLS (DoT) - Secure transport testing&lt;/item&gt;
      &lt;item&gt;DNS-over-QUIC (DoQ) - Experimental QUIC support&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multiple formats - CSV, Excel, PDF, JSON&lt;/item&gt;
      &lt;item&gt;Visual reports - Charts and graphs&lt;/item&gt;
      &lt;item&gt;Domain statistics - Per-domain performance analysis&lt;/item&gt;
      &lt;item&gt;Error breakdown - Identify problematic resolvers&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;TSIG authentication - Secure enterprise queries&lt;/item&gt;
      &lt;item&gt;Zone transfers - AXFR/IXFR validation&lt;/item&gt;
      &lt;item&gt;Dynamic updates - Test DNS write operations&lt;/item&gt;
      &lt;item&gt;Compliance reports - Audit-ready documentation&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux, macOS, Windows - Works everywhere&lt;/item&gt;
      &lt;item&gt;CI/CD friendly - JSON output, exit codes&lt;/item&gt;
      &lt;item&gt;IDNA support - Internationalized domain names&lt;/item&gt;
      &lt;item&gt;Auto-detection - Windows WMI DNS discovery&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;These flags are documented for visibility but not yet implemented.&lt;lb/&gt;They represent upcoming advanced features.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--doh&lt;/code&gt;→ DNS-over-HTTPS benchmarking (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--dot&lt;/code&gt;→ DNS-over-TLS benchmarking (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--doq&lt;/code&gt;→ DNS-over-QUIC benchmarking (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--dnssec-validate&lt;/code&gt;→ DNSSEC trust chain validation (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--zone-transfer&lt;/code&gt;→ AXFR/IXFR zone transfer testing (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--tsig&lt;/code&gt;→ TSIG-authenticated queries (coming soon)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--idna&lt;/code&gt;→ Internationalized domain name support (coming soon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;🚀 Performance &amp;amp; Concurrency Features&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Async I/O with dnspython - Test 100+ resolvers simultaneously&lt;/item&gt;
      &lt;item&gt;Trio framework support - High-concurrency async operations&lt;/item&gt;
      &lt;item&gt;Configurable concurrency - Control max concurrent queries&lt;/item&gt;
      &lt;item&gt;Retry logic - Exponential backoff for failed queries&lt;/item&gt;
      &lt;item&gt;Cache simulation - Test with/without DNS caching&lt;/item&gt;
      &lt;item&gt;Multi-iteration benchmarks - Run tests multiple times for accuracy&lt;/item&gt;
      &lt;item&gt;Warmup phase - Pre-warm DNS caches before testing&lt;/item&gt;
      &lt;item&gt;Statistical analysis - Mean, median, P95, P99, jitter, consistency scores&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;dns-benchmark benchmark \
  --max-concurrent 200 \
  --iterations 5 \
  --timeout 3.0 \
  --warmup&lt;/code&gt;
    &lt;head&gt;🔒 Security &amp;amp; Privacy Features&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;DNSSEC validation - Verify cryptographic trust chains&lt;/item&gt;
      &lt;item&gt;DNS-over-HTTPS (DoH) - Encrypted DNS benchmarking via HTTPS&lt;/item&gt;
      &lt;item&gt;DNS-over-TLS (DoT) - Secure transport layer testing&lt;/item&gt;
      &lt;item&gt;DNS-over-QUIC (DoQ) - Experimental QUIC protocol support&lt;/item&gt;
      &lt;item&gt;TSIG authentication - Transaction signatures for enterprise DNS&lt;/item&gt;
      &lt;item&gt;EDNS0 support - Extended DNS features and larger payloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Test DoH resolvers
dns-benchmark benchmark \
  --doh \
  --resolvers doh-providers.json \
  --dnssec-validate&lt;/code&gt;
    &lt;head&gt;🏢 Enterprise &amp;amp; Migration Features&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zone transfers (AXFR/IXFR) - Full and incremental zone transfer validation&lt;/item&gt;
      &lt;item&gt;Dynamic DNS updates - Test DNS write operations and updates&lt;/item&gt;
      &lt;item&gt;EDNS0 support - Extended DNS options, client subnet, larger payloads&lt;/item&gt;
      &lt;item&gt;Windows WMI integration - Auto-detect active system DNS settings&lt;/item&gt;
      &lt;item&gt;Compliance reporting - Generate audit-ready PDF/Excel reports&lt;/item&gt;
      &lt;item&gt;SLA validation - Track uptime and performance thresholds&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Validate DNS migration
dns-benchmark benchmark \
  --resolvers old-provider.json,new-provider.json \
  --zone-transfer \ # coming soon
  --output migration-report/ \
  --formats pdf,excel&lt;/code&gt;
    &lt;head&gt;📊 Analysis &amp;amp; Reporting Features&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Per-domain statistics - Analyze performance by domain&lt;/item&gt;
      &lt;item&gt;Per-record-type stats - Compare A, AAAA, MX, TXT, etc.&lt;/item&gt;
      &lt;item&gt;Error breakdown - Categorize and count error types&lt;/item&gt;
      &lt;item&gt;Comparison matrices - Side-by-side resolver comparisons&lt;/item&gt;
      &lt;item&gt;Trend analysis - Performance over time (with multiple runs)&lt;/item&gt;
      &lt;item&gt;Best-by-criteria - Find best resolver by latency/reliability/consistency&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Detailed analysis
dns-benchmark benchmark \
  --use-defaults \
  --domain-stats \
  --record-type-stats \
  --error-breakdown \
  --formats csv,excel,pdf&lt;/code&gt;
    &lt;head&gt;🌐 Internationalization &amp;amp; Compatibility&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;IDNA support - Internationalized domain names (IDN)&lt;/item&gt;
      &lt;item&gt;Multiple record types - A, AAAA, MX, TXT, CNAME, NS, SOA, PTR, SRV, CAA&lt;/item&gt;
      &lt;item&gt;Cross-platform - Linux, macOS, Windows (native support)&lt;/item&gt;
      &lt;item&gt;CI/CD integration - JSON output, proper exit codes, quiet mode&lt;/item&gt;
      &lt;item&gt;Custom resolvers - Load from JSON, test your own DNS servers&lt;/item&gt;
      &lt;item&gt;Custom domains - Test against your specific domain list&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;# Test internationalized domains
dns-benchmark benchmark \
  --domains international-domains.txt \
  --record-types A,AAAA,MX \
  --resolvers custom-resolvers.json&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;💡 Most users only need basic features. These advanced capabilities are available when you need them.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;# Find fastest DNS for your API endpoints
dns-benchmark benchmark \
  --domains api.myapp.com,cdn.myapp.com \
  --record-types A,AAAA \
  --resolvers production.json \
  --iterations 10&lt;/code&gt;
    &lt;p&gt;Result: Reduce API latency by 100-300ms&lt;/p&gt;
    &lt;code&gt;# Test new DNS provider before switching
dns-benchmark benchmark \
  --resolvers current-dns.json,new-dns.json \
  --use-defaults \
  --dnssec-validate \ # coming soon
  --output migration-report/ \
  --formats pdf,excel&lt;/code&gt;
    &lt;p&gt;Result: Verify performance and security before migration&lt;/p&gt;
    &lt;code&gt;# Compare Pi-hole against public resolvers (coming soon)
dns-benchmark compare \
  --resolvers pihole.local,1.1.1.1,8.8.8.8,9.9.9.9 \
  --domains common-sites.txt \
  --rounds 10&lt;/code&gt;
    &lt;p&gt;Result: Data-driven proof your self-hosted DNS is faster (or not!)&lt;/p&gt;
    &lt;code&gt;# Add to crontab for monthly reports
0 0 1 * * dns-benchmark benchmark \
  --use-defaults \
  --output /var/reports/dns/ \
  --formats pdf,csv \
  --domain-stats \
  --error-breakdown&lt;/code&gt;
    &lt;p&gt;Result: Automated compliance and SLA reporting&lt;/p&gt;
    &lt;code&gt;# Benchmark privacy-focused DoH/DoT resolvers
dns-benchmark benchmark \
  --doh \ # coming soon
  --resolvers privacy-resolvers.json \
  --domains sensitive-sites.txt \
  --dnssec-validate&lt;/code&gt;
    &lt;p&gt;Result: Find fastest encrypted DNS without sacrificing privacy&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Python 3.9+&lt;/item&gt;
      &lt;item&gt;pip package manager&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;pip install dns-benchmark-tool&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/frankovo/dns-benchmark-tool.git
cd dns-benchmark-tool
pip install -e .&lt;/code&gt;
    &lt;code&gt;dns-benchmark --version
dns-benchmark --help&lt;/code&gt;
    &lt;code&gt;# Test with defaults (recommended for first time)
dns-benchmark benchmark --use-defaults&lt;/code&gt;
    &lt;code&gt;# Basic test with progress bars
dns-benchmark benchmark --use-defaults

# Basic test without progress bars
dns-benchmark benchmark --use-defaults --quiet

# Test with custom resolvers and domains
dns-benchmark benchmark --resolvers data/resolvers.json --domains data/domains.txt

# Quick test with only CSV output
dns-benchmark benchmark --use-defaults --formats csv&lt;/code&gt;
    &lt;code&gt;# Export a machine-readable bundle
dns-benchmark benchmark --use-defaults --json --output ./results

# Test specific record types
dns-benchmark benchmark --use-defaults --record-types A,AAAA,MX

# Custom output location and formats
dns-benchmark benchmark \
  --use-defaults \
  --output ./my-results \
  --formats csv,excel,pdf,json

# Include detailed statistics
dns-benchmark benchmark \
  --use-defaults \
  --record-type-stats \
  --error-breakdown

# High concurrency with retries
dns-benchmark benchmark \
  --use-defaults \
  --max-concurrent 200 \
  --timeout 3.0 \
  --retries 3

# Website migration planning
dns-benchmark benchmark \
  --resolvers data/global_resolvers.json \
  --domains data/migration_domains.txt \
  --formats excel,pdf \
  --output ./migration_analysis

# DNS provider selection
dns-benchmark benchmark \
  --resolvers data/provider_candidates.json \
  --domains data/business_domains.txt \
  --formats csv,excel \
  --output ./provider_selection

# Network troubleshooting
dns-benchmark benchmark \
  --resolvers "192.168.1.1,1.1.1.1,8.8.8.8" \
  --domains "problematic-domain.com,working-domain.com" \
  --timeout 10 \
  --retries 3 \
  --formats csv \
  --output ./troubleshooting

# Security assessment
dns-benchmark benchmark \
  --resolvers data/security_resolvers.json \
  --domains data/security_test_domains.txt \
  --formats pdf \
  --output ./security_assessment

# Performance monitoring
dns-benchmark benchmark \
  --use-defaults \
  --formats csv \
  --quiet \
  --output /var/log/dns_benchmark/$(date +%Y%m%d_%H%M%S)

# New top commands
# Run a basic benchmark (default: rank by latency)
dns-benchmark top
# → Tests all resolvers with sample domains, ranks by latency

# Limit the number of resolvers shown
dns-benchmark top --limit 5
# → Shows only the top 5 resolvers

# Rank by success rate
dns-benchmark top --metric success
# → Ranks resolvers by highest success rate

# Rank by reliability (combined score: success rate + latency)
dns-benchmark top --metric reliability
# → Uses weighted score to rank resolvers

# Filter resolvers by category
dns-benchmark top --category privacy
dns-benchmark top --category family
dns-benchmark top --category security
# → Tests only resolvers in the specified category

# Use a custom domain list
dns-benchmark top --domains domains.txt
# → Loads domains from a text file instead of built-in sample list

# Specify DNS record types
dns-benchmark top --record-types A,AAAA,MX
# → Queries multiple record types (comma-separated)

# Adjust timeout and concurrency
dns-benchmark top --timeout 3.0 --max-concurrent 50
# → Sets query timeout to 3 seconds and limits concurrency to 50

# Export results to JSON
dns-benchmark top --output results.json
# → Saves results in JSON format

# Export results to CSV
dns-benchmark top --output results.csv
# → Saves results in CSV format

# Export results to TXT
dns-benchmark top --output results.txt
# → Saves results in plain text format

# Quiet mode (no progress bar, CI/CD friendly)
dns-benchmark top --quiet
# → Suppresses progress output

# Example combined usage
dns-benchmark top --limit 10 --metric reliability --category privacy --output top_resolvers.csv
# → Benchmarks privacy resolvers, ranks by reliability, shows top 10, exports to CSV

# New compare commaands
# Comparison of resolvers by name
dns-benchmark compare Cloudflare Google Quad9
# ^ Compares Cloudflare, Google, and Quad9 resolvers using default domains and record type A

# Basic compare resolvers by IP address
dns-benchmark compare 1.1.1.1 8.8.8.8 9.9.9.9
# ^ Directly specify resolver IPs instead of names

# Increase iterations for more stable results
dns-benchmark compare "Cloudflare" "Google" --iterations 5
# ^ Runs 5 rounds of queries per resolver/domain/record type

# Use a custom domain list from file
dns-benchmark compare Cloudflare Google -d ./data/domains.txt
# ^ Loads domains from domains.txt instead of sample domains

# Query multiple record types
dns-benchmark compare Cloudflare Google -t A,AAAA,MX
# ^ Tests A, AAAA, and MX records for each domain

# Adjust timeout and concurrency
dns-benchmark compare Cloudflare Google --timeout 3.0 --max-concurrent 200
# ^ Sets query timeout to 3 seconds and allows 200 concurrent queries

# Export results to JSON
dns-benchmark compare Cloudflare Google -o results.json
# ^ Saves comparison summary to results.json

# Export results to CSV
dns-benchmark compare Cloudflare Google -o results.csv
# ^ Saves comparison summary to results.csv (via CSVExporter)

# Suppress progress output
dns-benchmark compare Cloudflare Google --quiet
# ^ Runs silently, only prints final results

# Show detailed per-domain breakdown
dns-benchmark compare Cloudflare Google --show-details
# ^ Prints average latency and success counts per domain for each resolver

# New monitoring commands
# Start monitoring with default resolvers and sample domains
dns-benchmark monitoring --use-defaults
# ^ Runs indefinitely, checking every 60s, using built-in resolvers and 5 sample domains

# Monitor with a custom resolver list from JSON
dns-benchmark monitoring -r resolvers.json --use-defaults
# ^ Loads resolvers from resolvers.json, domains from defaults

# Monitor with a custom domain list
dns-benchmark monitoring -d domains.txt --use-defaults
# ^ Uses default resolvers, but domains are loaded from domains.txt

# Change monitoring interval to 30 seconds
dns-benchmark monitoring --use-defaults --interval 30
# ^ Runs checks every 30 seconds instead of 60

# Run monitoring for a fixed duration (e.g., 1 hour = 3600 seconds)
dns-benchmark monitoring --use-defaults --duration 3600
# ^ Stops automatically after 1 hour

# Set stricter alert thresholds
dns-benchmark monitoring --use-defaults --alert-latency 150 --alert-failure-rate 5
# ^ Alerts if latency &amp;gt;150ms or failure rate &amp;gt;5%

# Save monitoring results to a log file
dns-benchmark monitoring --use-defaults --output monitor.log
# ^ Appends results and alerts to monitor.log

# Combine options: custom resolvers, domains, interval, duration, and logging
dns-benchmark monitoring -r resolvers.json -d domains.txt -i 45 --duration 1800 -o monitor.log
# ^ Monitors resolvers from resolvers.json against domains.txt every 45s, for 30 minutes, logging to monitor.log

# Run monitoring for 1 hour with alerts
dns-benchmark monitoring --use-defaults --interval 30 --duration 3600 \
  --alert-latency 150 --alert-failure-rate 5 --output monitor.log
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Avg Latency: N/A&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# Provide feedback
dns-benchmark feedback&lt;/code&gt;
    &lt;code&gt;# Show default resolvers and domains
dns-benchmark list-defaults

# Browse all available resolvers
dns-benchmark list-resolvers

# Browse with detailed information
dns-benchmark list-resolvers --details

# Filter by category
dns-benchmark list-resolvers --category security
dns-benchmark list-resolvers --category privacy
dns-benchmark list-resolvers --category family

# Export resolvers to different formats
dns-benchmark list-resolvers --format csv
dns-benchmark list-resolvers --format json&lt;/code&gt;
    &lt;code&gt;# List all test domains
dns-benchmark list-domains

# Show domains by category
dns-benchmark list-domains --category tech
dns-benchmark list-domains --category ecommerce
dns-benchmark list-domains --category social

# Limit results
dns-benchmark list-domains --count 10
dns-benchmark list-domains --category news --count 5

# Export domain list
dns-benchmark list-domains --format csv
dns-benchmark list-domains --format json&lt;/code&gt;
    &lt;code&gt;# View all available categories
dns-benchmark list-categories&lt;/code&gt;
    &lt;code&gt;# Generate sample configuration
dns-benchmark generate-config --output sample_config.yaml

# Category-specific configurations
dns-benchmark generate-config --category security --output security_test.yaml
dns-benchmark generate-config --category family --output family_protection.yaml
dns-benchmark generate-config --category performance --output performance_test.yaml

# Custom configuration for specific use case
dns-benchmark generate-config --category privacy --output privacy_audit.yaml&lt;/code&gt;
    &lt;code&gt;# Basic test with progress bars
dns-benchmark benchmark --use-defaults

# Quick test with only CSV output
dns-benchmark benchmark --use-defaults --formats csv --quiet

# Test specific record types
dns-benchmark benchmark --use-defaults --record-types A,AAAA,MX&lt;/code&gt;
    &lt;p&gt;Add-on analytics flags:&lt;/p&gt;
    &lt;code&gt;# Include domain and record-type analytics and error breakdown
dns-benchmark benchmark --use-defaults \
  --domain-stats --record-type-stats --error-breakdown&lt;/code&gt;
    &lt;p&gt;JSON export:&lt;/p&gt;
    &lt;code&gt;# Export a machine-readable bundle
dns-benchmark benchmark --use-defaults --json --output ./results&lt;/code&gt;
    &lt;code&gt;# Compare internal vs external DNS
dns-benchmark benchmark \
  --resolvers "192.168.1.1,1.1.1.1,8.8.8.8,9.9.9.9" \
  --domains "internal.company.com,google.com,github.com,api.service.com" \
  --formats excel,pdf \
  --timeout 3 \
  --max-concurrent 50 \
  --output ./network_audit

# Test DNS failover scenarios
dns-benchmark benchmark \
  --resolvers data/primary_resolvers.json \
  --domains data/business_critical_domains.txt \
  --record-types A,AAAA \
  --retries 3 \
  --formats csv,excel \
  --output ./failover_test&lt;/code&gt;
    &lt;code&gt;# Comprehensive ISP resolver comparison
dns-benchmark benchmark \
  --resolvers data/isp_resolvers.json \
  --domains data/popular_domains.txt \
  --timeout 5 \
  --max-concurrent 100 \
  --formats csv,excel,pdf \
  --output ./isp_performance_analysis

# Regional performance testing
dns-benchmark benchmark \
  --resolvers data/regional_resolvers.json \
  --domains data/regional_domains.txt \
  --formats excel \
  --quiet \
  --output ./regional_analysis&lt;/code&gt;
    &lt;code&gt;# Test application dependencies
dns-benchmark benchmark \
  --resolvers "1.1.1.1,8.8.8.8" \
  --domains "api.github.com,registry.npmjs.org,pypi.org,docker.io,aws.amazon.com" \
  --formats csv \
  --quiet \
  --output ./app_dependencies

# CI/CD integration test
dns-benchmark benchmark \
  --resolvers data/ci_resolvers.json \
  --domains data/ci_domains.txt \
  --timeout 2 \
  --formats csv \
  --quiet&lt;/code&gt;
    &lt;code&gt;# Security-focused resolver testing
dns-benchmark benchmark \
  --resolvers data/security_resolvers.json \
  --domains data/malware_test_domains.txt \
  --formats csv,pdf \
  --output ./security_audit

# Privacy-focused testing
dns-benchmark benchmark \
  --resolvers data/privacy_resolvers.json \
  --domains data/tracking_domains.txt \
  --formats excel \
  --output ./privacy_analysis&lt;/code&gt;
    &lt;code&gt;# Corporate network assessment
dns-benchmark benchmark \
  --resolvers data/enterprise_resolvers.json \
  --domains data/corporate_domains.txt \
  --record-types A,AAAA,MX,TXT,SRV \
  --timeout 10 \
  --max-concurrent 25 \
  --retries 2 \
  --formats csv,excel,pdf \
  --output ./enterprise_dns_audit

# Multi-location testing
dns-benchmark benchmark \
  --resolvers data/global_resolvers.json \
  --domains data/international_domains.txt \
  --formats excel \
  --output ./global_performance&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;--iterations, -i&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run the full benchmark loop N times&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dns-benchmark benchmark --use-defaults -i 3&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;--use-cache&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Allow cached results to be reused across iterations&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dns-benchmark benchmark --use-defaults -i 3 --use-cache&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;--warmup&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run a full warmup (all resolvers × domains × record types)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dns-benchmark benchmark --use-defaults --warmup&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;--warmup-fast&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run a lightweight warmup (one probe per resolver)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dns-benchmark benchmark --use-defaults --warmup-fast&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--include-charts&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Embed charts and graphs in PDF/Excel reports for visual performance analysis&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;dns-benchmark benchmark --use-defaults --formats pdf,excel --include-charts&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The DNS Benchmark Tool now includes three specialized commands for different workflows:&lt;/p&gt;
    &lt;p&gt;Quickly rank resolvers by speed and reliability.&lt;/p&gt;
    &lt;code&gt;# Rank resolvers quickly
dns-benchmark top

# Use custom domain list
dns-benchmark top -d domains.txt

# Export results to JSON
dns-benchmark top -o results.json&lt;/code&gt;
    &lt;p&gt;Benchmark resolvers side‑by‑side with detailed statistics.&lt;/p&gt;
    &lt;code&gt;# Compare Cloudflare, Google, and Quad9
dns-benchmark compare Cloudflare Google Quad9

# Compare by IP addresses
dns-benchmark compare 1.1.1.1 8.8.8.8 9.9.9.9

# Show detailed per-domain breakdown
dns-benchmark compare Cloudflare Google --show-details

# Export results to CSV
dns-benchmark compare Cloudflare Google -o results.csv&lt;/code&gt;
    &lt;p&gt;Continuously monitor resolver performance with alerts.&lt;/p&gt;
    &lt;code&gt;# Monitor default resolvers continuously (every 60s)
dns-benchmark monitoring --use-defaults

# Monitor with custom resolvers and domains
dns-benchmark monitoring -r resolvers.json -d domains.txt

# Run monitoring for 1 hour with alerts
dns-benchmark monitoring --use-defaults --interval 30 --duration 3600 \
  --alert-latency 150 --alert-failure-rate 5 --output monitor.log&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
        &lt;cell role="head"&gt;Typical Use Case&lt;/cell&gt;
        &lt;cell role="head"&gt;Key Options&lt;/cell&gt;
        &lt;cell role="head"&gt;Output&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;top&lt;/cell&gt;
        &lt;cell&gt;Quick ranking of resolvers by speed and reliability&lt;/cell&gt;
        &lt;cell&gt;Fast check to see which resolver is best right now&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--domains&lt;/code&gt;, &lt;code&gt;--record-types&lt;/code&gt;, &lt;code&gt;--output&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Sorted list of resolvers with latency &amp;amp; success rate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;compare&lt;/cell&gt;
        &lt;cell&gt;Side‑by‑side comparison of specific resolvers&lt;/cell&gt;
        &lt;cell&gt;Detailed benchmarking across chosen resolvers/domains&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--domains&lt;/code&gt;, &lt;code&gt;--record-types&lt;/code&gt;, &lt;code&gt;--iterations&lt;/code&gt;, &lt;code&gt;--output&lt;/code&gt;, &lt;code&gt;--show-details&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Table of resolvers with latency, success rate, per‑domain breakdown&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;monitoring&lt;/cell&gt;
        &lt;cell&gt;Continuous monitoring with alerts&lt;/cell&gt;
        &lt;cell&gt;Real‑time tracking of resolver performance over time&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;--interval&lt;/code&gt;, &lt;code&gt;--duration&lt;/code&gt;, &lt;code&gt;--alert-latency&lt;/code&gt;, &lt;code&gt;--alert-failure-rate&lt;/code&gt;, &lt;code&gt;--output&lt;/code&gt;, &lt;code&gt;--use-defaults&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Live status indicators, alerts, optional log file&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Iteration count: displayed when more than one iteration is run.&lt;/item&gt;
      &lt;item&gt;Cache hits: shows how many queries were served from cache (when &lt;code&gt;--use-cache&lt;/code&gt;is enabled).&lt;/item&gt;
      &lt;item&gt;Failure tracking: resolvers with repeated errors are counted and can be inspected with &lt;code&gt;get_failed_resolvers()&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Cache statistics: available via &lt;code&gt;get_cache_stats()&lt;/code&gt;, showing number of cached entries and whether cache is enabled.&lt;/item&gt;
      &lt;item&gt;Warmup results: warmup queries are marked with &lt;code&gt;iteration=0&lt;/code&gt;in raw data, making them easy to filter out in analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example summary output:&lt;/p&gt;
    &lt;code&gt;=== BENCHMARK SUMMARY ===
Total queries: 150
Successful: 140 (93.33%)
Average latency: 212.45 ms
Median latency: 198.12 ms
Fastest resolver: Cloudflare
Slowest resolver: Quad9
Iterations: 3
Cache hits: 40 (26.7%)&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommended Flags&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Quick Run&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--iterations 1 --timeout 1 --retries 0 --warmup-fast&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fast feedback, minimal retries, lightweight warmup. Good for quick checks.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Thorough Run&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--iterations 3 --use-cache --warmup --timeout 5 --retries 2&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Multiple passes, cache enabled, full warmup. Best for detailed benchmarking.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Debug Mode&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--iterations 1 --timeout 10 --retries 0 --quiet&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Long timeout, no retries, minimal output. Useful for diagnosing resolver issues.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Balanced Run&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;--iterations 2 --use-cache --warmup-fast --timeout 2 --retries 1&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;A middle ground: moderate speed, some retries, cache enabled, quick warmup.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We value your input! Help us improve dns-benchmark by sharing your experience and DNS challenges.&lt;/p&gt;
    &lt;p&gt;Open the feedback form directly from CLI:&lt;/p&gt;
    &lt;code&gt;dns-benchmark feedback&lt;/code&gt;
    &lt;p&gt;This command:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Opens the feedback survey in your default browser&lt;/item&gt;
      &lt;item&gt;Takes ~2 minutes to complete&lt;/item&gt;
      &lt;item&gt;Directly shapes our roadmap and priorities&lt;/item&gt;
      &lt;item&gt;Automatically marks feedback as given (won't prompt again)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Survey link: https://forms.gle/BJBiyBFvRJHskyR57&lt;/p&gt;
    &lt;p&gt;To avoid being intrusive, dns-benchmark uses intelligent prompting:&lt;/p&gt;
    &lt;p&gt;When prompts appear:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;After your 5th, 15th, and 30th benchmark run&lt;/item&gt;
      &lt;item&gt;With a 24-hour cooldown between prompts&lt;/item&gt;
      &lt;item&gt;Only if you haven't already given feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Auto-dismiss conditions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You've already submitted feedback&lt;/item&gt;
      &lt;item&gt;You've dismissed the prompt 3 times&lt;/item&gt;
      &lt;item&gt;You've opted out via environment variable&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example prompt:&lt;/p&gt;
    &lt;code&gt;──────────────────────────────────────────────────────────
📢 Quick feedback request
Help shape dns-benchmark! Share your biggest DNS challenge.
→ https://forms.gle/BJBiyBFvRJHskyR57 (2 min survey)
→ Or run: dns-benchmark feedback
──────────────────────────────────────────────────────────

Show this again? (y/n) [y]:
&lt;/code&gt;
    &lt;p&gt;What we store locally: dns-benchmark stores feedback prompt state in &lt;code&gt;~/.dns-benchmark/feedback.json&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Contents:&lt;/p&gt;
    &lt;code&gt;{
  "total_runs": 15,
  "feedback_given": false,
  "dismissed_count": 0,
  "last_shown": 1699876543,
  "version": "1.0"
}&lt;/code&gt;
    &lt;p&gt;Privacy notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ All data stored locally on your machine&lt;/item&gt;
      &lt;item&gt;✅ No telemetry or tracking&lt;/item&gt;
      &lt;item&gt;✅ No automatic data transmission&lt;/item&gt;
      &lt;item&gt;✅ File is only read/written during benchmark runs&lt;/item&gt;
      &lt;item&gt;✅ Safe to delete at any time&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What we collect (only when you submit feedback):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Whatever you choose to share in the survey&lt;/item&gt;
      &lt;item&gt;We never collect usage data automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Method 1: Dismiss the prompt When prompted, type &lt;code&gt;n&lt;/code&gt; to dismiss:&lt;/p&gt;
    &lt;code&gt;Show this again? (y/n) [y]: n
✓ Got it! We won't ask again. Thanks for using dns-benchmark!
&lt;/code&gt;
    &lt;p&gt;After 3 dismissals, prompts stop permanently.&lt;/p&gt;
    &lt;p&gt;Method 2: Environment variable (complete disable)&lt;/p&gt;
    &lt;code&gt;# Bash/Zsh
export DNS_BENCHMARK_NO_FEEDBACK=1

# Windows PowerShell
$env:DNS_BENCHMARK_NO_FEEDBACK="1"

# Permanently (add to ~/.bashrc or ~/.zshrc)
echo 'export DNS_BENCHMARK_NO_FEEDBACK=1' &amp;gt;&amp;gt; ~/.bashrc&lt;/code&gt;
    &lt;p&gt;Method 3: Delete state file&lt;/p&gt;
    &lt;code&gt;rm ~/.dns-benchmark/feedback.json&lt;/code&gt;
    &lt;p&gt;Method 4: CI/CD environments Feedback prompts are automatically disabled when:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;CI=true&lt;/code&gt;environment variable is set (standard in GitHub Actions, GitLab CI, etc.)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--quiet&lt;/code&gt;flag is used&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Reset for testing (developers):&lt;/p&gt;
    &lt;code&gt;dns-benchmark reset-feedback  # Hidden command&lt;/code&gt;
    &lt;code&gt;{
  "resolvers": [
    {
      "name": "Cloudflare",
      "ip": "1.1.1.1",
      "ipv6": "2606:4700:4700::1111"
    },
    {
      "name": "Google DNS",
      "ip": "8.8.8.8",
      "ipv6": "2001:4860:4860::8888"
    }
  ]
}&lt;/code&gt;
    &lt;code&gt;# Popular websites
google.com
github.com
stackoverflow.com

# Corporate domains
microsoft.com
apple.com
amazon.com

# CDN and cloud
cloudflare.com
aws.amazon.com&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raw data: individual query results with timestamps and metadata&lt;/item&gt;
      &lt;item&gt;Summary statistics: aggregated metrics per resolver&lt;/item&gt;
      &lt;item&gt;Domain statistics: per-domain metrics (when --domain-stats)&lt;/item&gt;
      &lt;item&gt;Record type statistics: per-record-type metrics (when --record-type-stats)&lt;/item&gt;
      &lt;item&gt;Error breakdown: counts by error type (when --error-breakdown)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raw data sheet: all query results with formatting&lt;/item&gt;
      &lt;item&gt;Resolver summary: comprehensive statistics with conditional formatting&lt;/item&gt;
      &lt;item&gt;Domain stats: per-domain performance (optional)&lt;/item&gt;
      &lt;item&gt;Record type stats: per-record-type performance (optional)&lt;/item&gt;
      &lt;item&gt;Error breakdown: aggregated error counts (optional)&lt;/item&gt;
      &lt;item&gt;Performance analysis: charts and comparative analysis&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Executive summary: key findings and recommendations&lt;/item&gt;
      &lt;item&gt;Performance charts: latency comparison; optional success rate chart&lt;/item&gt;
      &lt;item&gt;Resolver rankings: ordered by average latency&lt;/item&gt;
      &lt;item&gt;Detailed analysis: technical deep‑dive with percentiles&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Machine‑readable bundle including: &lt;list rend="ul"&gt;&lt;item&gt;Overall statistics&lt;/item&gt;&lt;item&gt;Resolver statistics&lt;/item&gt;&lt;item&gt;Raw query results&lt;/item&gt;&lt;item&gt;Domain statistics&lt;/item&gt;&lt;item&gt;Record type statistics&lt;/item&gt;&lt;item&gt;Error breakdown&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;dns-benchmark generate-config \
  --category privacy \
  --output my-config.yaml&lt;/code&gt;
    &lt;code&gt;# Large-scale testing (1000+ queries)
dns-benchmark benchmark \
  --resolvers data/many_resolvers.json \
  --domains data/many_domains.txt \
  --max-concurrent 50 \
  --timeout 3 \
  --quiet \
  --formats csv

# Unstable networks
dns-benchmark benchmark \
  --resolvers data/backup_resolvers.json \
  --domains data/critical_domains.txt \
  --timeout 10 \
  --retries 3 \
  --max-concurrent 10

# Quick diagnostics
dns-benchmark benchmark \
  --resolvers "1.1.1.1,8.8.8.8" \
  --domains "google.com,cloudflare.com" \
  --formats csv \
  --quiet \
  --timeout 2&lt;/code&gt;
    &lt;code&gt;# Command not found
pip install -e .
python -m dns_benchmark.cli --help

# PDF generation fails (Ubuntu/Debian)
sudo apt-get install libcairo2 libpango-1.0-0 libpangocairo-1.0-0 \
  libgdk-pixbuf2.0-0 libffi-dev shared-mime-info
# Or skip PDF
dns-benchmark benchmark --use-defaults --formats csv,excel

# Network timeouts
dns-benchmark benchmark --use-defaults --timeout 10 --retries 3
dns-benchmark benchmark --use-defaults --max-concurrent 25&lt;/code&gt;
    &lt;code&gt;# Verbose run
python -m dns_benchmark.cli benchmark --use-defaults --formats csv

# Minimal configuration
dns-benchmark benchmark --resolvers "1.1.1.1" --domains "google.com" --formats csv&lt;/code&gt;
    &lt;code&gt;# Daily monitoring
0 2 * * * /usr/local/bin/dns-benchmark benchmark --use-defaults --formats csv --quiet --output /var/log/dns_benchmark/daily_$(date +\%Y\%m\%d)

# Time-based variability (every 6 hours)
0 */6 * * * /usr/local/bin/dns-benchmark benchmark --use-defaults --formats csv --quiet --output /var/log/dns_benchmark/$(date +\%Y\%m\%d_\%H)&lt;/code&gt;
    &lt;code&gt;- name: DNS Performance Test
  run: |
    pip install dnspython pandas click tqdm colorama
    dns-benchmark benchmark \
      --resolvers "1.1.1.1,8.8.8.8" \
      --domains "api.service.com,database.service.com" \
      --formats csv \
      --quiet&lt;/code&gt;
    &lt;p&gt;Place images in &lt;code&gt;docs/screenshots/&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/cli_run.png&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/excel_report.png&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/pdf_summary.png&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/pdf_charts.png&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/excel_charts.png&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;docs/screenshots/real_time_monitoring.png&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;dns-benchmark --help
dns-benchmark benchmark --help
dns-benchmark list-resolvers --help
dns-benchmark list-domains --help
dns-benchmark list-categories --help
dns-benchmark generate-config --help&lt;/code&gt;
    &lt;p&gt;Common scenarios:&lt;/p&gt;
    &lt;code&gt;# I'm new — where to start?
dns-benchmark list-defaults
dns-benchmark benchmark --use-defaults

# Test specific resolvers
dns-benchmark list-resolvers --category security
dns-benchmark benchmark --resolvers data/security_resolvers.json --use-defaults

# Generate a management report
dns-benchmark benchmark --use-defaults --formats excel,pdf \
  --domain-stats --record-type-stats --error-breakdown --json \
  --output ./management_report&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Prerequisites&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;GPG key configured: run &lt;code&gt;make gpg-check&lt;/code&gt;to verify.&lt;/item&gt;
          &lt;item&gt;Branch protection: main requires signed commits and passing CI.&lt;/item&gt;
          &lt;item&gt;CI publish: triggered on signed tags matching vX.Y.Z.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;GPG key configured: run &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prepare release (signed)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Patch/minor/major bump:&lt;/p&gt;
            &lt;code&gt;make release-patch # or: make release-minor / make release-major&lt;/code&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;Updates versions.&lt;/item&gt;
              &lt;item&gt;Creates or reuses &lt;code&gt;release/X.Y.Z&lt;/code&gt;.&lt;/item&gt;
              &lt;item&gt;Makes a signed commit and pushes the branch.&lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;Open PR: from&lt;/p&gt;&lt;code&gt;release/X.Y.Z&lt;/code&gt;into&lt;code&gt;main&lt;/code&gt;, then merge once CI passes.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tag and publish&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Create signed tag and push:&lt;/p&gt;
            &lt;quote&gt;make release-tag VERSION=X.Y.Z&lt;/quote&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;Tags main with &lt;code&gt;vX.Y.Z&lt;/code&gt;(signed).&lt;/item&gt;
              &lt;item&gt;CI publishes to PyPI.&lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;Tags main with &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Manual alternative&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Create branch and commit signed:&lt;/p&gt;
            &lt;quote&gt;git checkout -b release/manually-update-version-based-on-release-pattern git add . git commit -S -m "Release release/$NEXT_VERSION" git push origin release/$NEXT_VERSION&lt;/quote&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Open PR and merge into main.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Then tag:&lt;/p&gt;
            &lt;code&gt;make release-tag VERSION=$NEXT_VERSION&lt;/code&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Notes&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Signed commits: &lt;code&gt;git commit -S ...&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;Signed tags: &lt;code&gt;git tag -s vX.Y.Z -m "Release vX.Y.Z"&lt;/code&gt;&lt;/item&gt;
          &lt;item&gt;Version sources: &lt;code&gt;pyproject.toml&lt;/code&gt;and&lt;code&gt;src/dns_benchmark/__init__.py&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Signed commits: &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CLI stays free forever. The hosted version adds features impossible to achieve locally:&lt;/p&gt;
    &lt;p&gt;Test from US-East, US-West, EU, Asia simultaneously. See how your DNS performs for users worldwide.&lt;/p&gt;
    &lt;p&gt;Monitor DNS performance over time. Identify trends, degradation, and optimize continuously.&lt;/p&gt;
    &lt;p&gt;Get notified via Email, Slack, PagerDuty when DNS performance degrades or SLA thresholds are breached.&lt;/p&gt;
    &lt;p&gt;Share results, dashboards, and reports across your team. Role-based access control.&lt;/p&gt;
    &lt;p&gt;Automated monthly reports proving DNS provider meets SLA guarantees. Audit-ready documentation.&lt;/p&gt;
    &lt;p&gt;Integrate DNS monitoring into your existing observability stack. Prometheus, Datadog, Grafana.&lt;/p&gt;
    &lt;p&gt;Join the Waitlist → | Early access gets 50% off for 3 months&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Benchmark DNS resolvers across domains and record types&lt;/item&gt;
      &lt;item&gt;Export to CSV, Excel, PDF, JSON&lt;/item&gt;
      &lt;item&gt;Statistical analysis (P95, P99, jitter, consistency)&lt;/item&gt;
      &lt;item&gt;Automation support (CI/CD, cron)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;CLI stays free forever. Hosted adds:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🌍 Multi-region testing (US, EU, Asia, custom)&lt;/item&gt;
      &lt;item&gt;📊 Historical tracking with charts and trends&lt;/item&gt;
      &lt;item&gt;🚨 Alerts (Email, Slack, PagerDuty, webhooks)&lt;/item&gt;
      &lt;item&gt;👥 Team collaboration and sharing&lt;/item&gt;
      &lt;item&gt;📈 SLA compliance reporting&lt;/item&gt;
      &lt;item&gt;🔌 API access and integrations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Join Waitlist for early access&lt;/p&gt;
    &lt;p&gt;Part of BuildTools - Network Performance Suite:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🔍 HTTP/HTTPS Benchmark - Test API endpoints and CDNs&lt;/item&gt;
      &lt;item&gt;🔒 SSL Certificate Monitor - Never miss renewals&lt;/item&gt;
      &lt;item&gt;📡 Uptime Monitor - 24/7 availability tracking&lt;/item&gt;
      &lt;item&gt;🌐 API Health Dashboard - Complete network observability&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Help shape our roadmap:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;📝 2-minute feedback survey&lt;/item&gt;
      &lt;item&gt;💬 GitHub Discussions&lt;/item&gt;
      &lt;item&gt;⭐ Star us if this helps you!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We love contributions! Here's how you can help:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🐛 Report bugs - Open an issue&lt;/item&gt;
      &lt;item&gt;💡 Suggest features - Start a discussion&lt;/item&gt;
      &lt;item&gt;📝 Improve docs - README, examples, tutorials&lt;/item&gt;
      &lt;item&gt;🔧 Submit PRs - Bug fixes, features, tests&lt;/item&gt;
      &lt;item&gt;⭐ Star the repo - Help others discover the tool&lt;/item&gt;
      &lt;item&gt;📢 Spread the word - Tweet, blog, share&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project includes a &lt;code&gt;Makefile&lt;/code&gt; to simplify installation, testing, and code quality checks.&lt;/p&gt;
    &lt;code&gt;.PHONY: install install-dev uninstall mypy black isort flake8 cov test clean cli-test

# 🔧 Install package (runtime only)
install:
  pip install .

# 🔧 Install package with dev extras (pytest, mypy, flake8, black, isort, etc.)
install-dev:
  pip install .[dev]

# 🔧 Uninstall package
uninstall:
  pip uninstall -y dns-benchmark-tool \
  dnspython pandas aiohttp click pyfiglet colorama Jinja2 weasyprint openpyxl pyyaml tqdm matplotlib \
  mypy black flake8 autopep8 pytest coverage isort

mypy:
  mypy .

isort:
  isort .

black:
  black .

flake8:
  flake8 src tests --ignore=E126,E501,E712,F405,F403,E266,W503 --max-line-length=88 --extend-ignore=E203

cov:
  coverage erase
  coverage run --source=src -m pytest -vv -s
  coverage html

test: mypy black isort flake8 cov

clean:
  rm -rf __pycache__ .pytest_cache htmlcov .coverage coverage.xml \
  build dist *.egg-info .eggs benchmark_results
cli-test:
  # Run only the CLI smoke tests marked with @pytest.mark.cli
  pytest -vv -s -m cli tests/test_cli_commands.py&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Install runtime only&lt;/p&gt;
        &lt;quote&gt;make install&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install with dev dependencies&lt;/p&gt;
        &lt;quote&gt;make install-dev&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run type checks, linting, formatting, and tests&lt;/p&gt;
        &lt;code&gt;make test&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Run CLI smoke tests only&lt;/p&gt;
        &lt;quote&gt;make cli-test&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Clean build/test artifacts&lt;/p&gt;
        &lt;quote&gt;make clean&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow PEP 8 style guide&lt;/item&gt;
      &lt;item&gt;Add tests for new features&lt;/item&gt;
      &lt;item&gt;Update documentation&lt;/item&gt;
      &lt;item&gt;Keep PRs focused and atomic&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Why is my ISP's DNS not fastest?&lt;/head&gt;
    &lt;p&gt;Local ISP DNS often has caching advantages but may lack:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Global anycast network (slower for distant domains)&lt;/item&gt;
      &lt;item&gt;DNSSEC validation&lt;/item&gt;
      &lt;item&gt;Privacy features (DoH/DoT)&lt;/item&gt;
      &lt;item&gt;Reliability guarantees&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Test both and decide based on YOUR priorities!&lt;/p&gt;
    &lt;head&gt;How often should I benchmark DNS?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;One-time: When choosing DNS provider&lt;/item&gt;
      &lt;item&gt;Monthly: For network health checks&lt;/item&gt;
      &lt;item&gt;Before migration: When switching providers&lt;/item&gt;
      &lt;item&gt;After issues: To troubleshoot performance&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Can I test my own DNS server?&lt;/head&gt;
    &lt;p&gt;Yes! Just add it to a custom resolvers JSON file:&lt;/p&gt;
    &lt;code&gt;{
  "resolvers": [
    {"name": "My DNS", "ip": "192.168.1.1"}
  ]
}&lt;/code&gt;
    &lt;head&gt;What's the difference between CLI and hosted version?&lt;/head&gt;
    &lt;p&gt;CLI (Free Forever):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run tests from YOUR location&lt;/item&gt;
      &lt;item&gt;Save results locally&lt;/item&gt;
      &lt;item&gt;Manual execution&lt;/item&gt;
      &lt;item&gt;Open source&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hosted (Coming Soon):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test from MULTIPLE regions&lt;/item&gt;
      &lt;item&gt;Historical tracking&lt;/item&gt;
      &lt;item&gt;Automated scheduling&lt;/item&gt;
      &lt;item&gt;Alerts and integrations&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Is this tool safe to use in production?&lt;/head&gt;
    &lt;p&gt;Yes! The tool only performs DNS lookups (read operations). It does NOT:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Modify DNS records&lt;/item&gt;
      &lt;item&gt;Perform attacks&lt;/item&gt;
      &lt;item&gt;Send data to external servers (unless you enable hosted features)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All tests are standard DNS queries that any resolver handles daily.&lt;/p&gt;
    &lt;head&gt;Why do results vary between runs?&lt;/head&gt;
    &lt;p&gt;DNS performance varies due to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Network conditions&lt;/item&gt;
      &lt;item&gt;DNS caching (resolver and intermediate)&lt;/item&gt;
      &lt;item&gt;Server load&lt;/item&gt;
      &lt;item&gt;Geographic routing changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Run multiple iterations (&lt;code&gt;--iterations 5&lt;/code&gt;) for more consistent results.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Website: buildtools.net&lt;/item&gt;
      &lt;item&gt;PyPI: dns-benchmark-tool&lt;/item&gt;
      &lt;item&gt;GitHub: frankovo/dns-benchmark-tool&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Feedback: 2-minute survey&lt;/item&gt;
      &lt;item&gt;Discussions: GitHub Discussions&lt;/item&gt;
      &lt;item&gt;Issues: Bug Reports&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Downloads: 1,400+ (this week)&lt;/item&gt;
      &lt;item&gt;Active Users: 600+&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project is licensed under the MIT License — see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Built with ❤️ by @frankovo&lt;/p&gt;
    &lt;p&gt;Part of BuildTools - Network Performance Suite&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45982526</guid><pubDate>Wed, 19 Nov 2025 17:52:52 +0000</pubDate></item><item><title>Building more with GPT-5.1-Codex-Max</title><link>https://openai.com/index/gpt-5-1-codex-max/</link><description>&lt;doc fingerprint="af48c13f1c3bd48c"&gt;
  &lt;main&gt;
    &lt;p&gt;We’re introducing GPT‑5.1-Codex-Max, our new frontier agentic coding model, available in Codex today. GPT‑5.1-Codex-Max is built on an update to our foundational reasoning model, which is trained on agentic tasks across software engineering, math, research, and more. GPT‑5.1-Codex-Max is faster, more intelligent, and more token-efficient at every stage of the development cycle–and a new step towards becoming a reliable coding partner.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1-Codex-Max is built for long-running, detailed work. It’s our first model natively trained to operate across multiple context windows through a process called compaction, coherently working over millions of tokens in a single task. This unlocks project-scale refactors, deep debugging sessions, and multi-hour agent loops.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1-Codex-Max is available in Codex today for use in the CLI, IDE extension, cloud, and code review, and API access is coming soon.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1-Codex-Max was trained on real-world software engineering tasks, like PR creation, code review, frontend coding, and Q&amp;amp;A and outperforms our previous models on many frontier coding evaluations. The model’s gains on benchmarks also come with improvements to real-world usage: GPT‑5.1-Codex-Max is the first model we have trained to operate in Windows environments, and the model’s training now includes tasks designed to make it a better collaborator in the Codex CLI.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1-Codex-Max shows significant improvements in token efficiency due to more effective reasoning. On SWE-bench Verified, GPT‑5.1-Codex-Max with ‘medium’ reasoning effort achieves better performance than GPT‑5.1-Codex with the same reasoning effort, while using 30% fewer thinking tokens. For non-latency-sensitive tasks, we’re also introducing a new Extra High (‘xhigh’) reasoning effort, which thinks for an even longer period of time for a better answer. We still recommend medium as the daily driver for most tasks.&lt;/p&gt;
    &lt;p&gt;We expect the token efficiency improvements to translate to real-world savings for developers.&lt;/p&gt;
    &lt;p&gt;For example, GPT‑5.1-Codex-Max is able to produce high quality frontend designs with similar functionality and aesthetics, but at much lower cost than GPT‑5.1-Codex.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;Prompt:&lt;/code&gt;
      &lt;code&gt; Generate a single self-contained browser app that renders an interactive CartPole RL sandbox with canvas graphics, a tiny policy-gradient controller, metrics, and an SVG network visualizer.&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;Features&lt;/code&gt;
    &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;Must be able to actually train a policy to make model better at cart pole&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Visualizer for the activations/weights when the model is training or at inference&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Steps in the episode, rewards this episode&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Last survival time and best survival time in steps&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;Save to index.html&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Compaction enables GPT‑5.1-Codex-Max to complete tasks that would have previously failed due to context-window limits, such as complex refactors and long-running agent loops by pruning its history while preserving the most important context over long horizons. In Codex applications, GPT‑5.1-Codex-Max automatically compacts its session when it approaches its context window limit, giving it a fresh context window. It repeats this process until the task is completed.&lt;/p&gt;
    &lt;p&gt;The ability to sustain coherent work over long horizons is a foundational capability on the path toward more general, reliable AI systems. GPT‑5.1-Codex-Max can work independently for hours at a time. In our internal evaluations, we’ve observed GPT‑5.1-Codex-Max work on tasks for more than 24 hours. It will persistently iterate on its implementation, fix test failures, and ultimately deliver a successful result.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1-Codex-Max performs significantly better on evaluations that require sustained, long-horizon reasoning. Because it can coherently work across multiple context windows using compaction, the model delivers improved results on challenges in areas like long-horizon coding and cybersecurity. We analyzed the results of this model’s performance on first- and third-party evaluations in the GPT‑5.1-Codex-Max system card.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1-Codex-Max does not reach High capability on Cybersecurity under our Preparedness Framework but it is the most capable cybersecurity model we’ve deployed to date and agentic cybersecurity capabilities are rapidly evolving. As a result, we are taking steps to prepare for High capability on Cybersecurity and are enhancing our safeguards in the cyber domain and working to ensure that defenders can benefit from these improved capabilities through programs like Aardvark.&lt;/p&gt;
    &lt;p&gt;When we launched GPT‑5-Codex, we implemented dedicated cybersecurity-specific monitoring to detect and disrupt malicious activity. While we have not observed a meaningful increase in scaled abuse, we are preparing additional mitigations for advanced capabilities. Our teams have already disrupted cyber operations attempting to misuse our models, and suspicious activity is routed for review through our policy monitoring systems.&lt;/p&gt;
    &lt;p&gt;Codex is designed to run in a secure sandbox by default: file writes are limited to its workspace, and network access is disabled unless a developer turns it on. We recommend keeping Codex in this restricted-access mode, since enabling internet or web search can introduce prompt-injection risks from untrusted content.&lt;/p&gt;
    &lt;p&gt;As Codex becomes more capable of long-running tasks, it is increasingly important for developers to review the agent’s work before making changes or deploying to production. To assist with this, Codex produces terminal logs and cites its tool calls and test results. While its code reviews reduce the risk of deploying model or human produced bugs to production, Codex should be treated as an additional reviewer and not a replacement for human reviews.&lt;/p&gt;
    &lt;p&gt;Cybersecurity capabilities can be used for both defense and offense, so we take an iterative deployment approach: learning from real-world use, updating safeguards, and preserving important defensive tools such as automated vulnerability scanning and remediation assistance.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1-Codex-Max is available in Codex with ChatGPT Plus, Pro, Business, Edu, and Enterprise plans. For details on how usage limits work for your plan, please see our docs(opens in a new window).&lt;/p&gt;
    &lt;p&gt;For developers using Codex CLI via API key, we plan to make GPT‑5.1-Codex-Max available in the API soon.&lt;/p&gt;
    &lt;p&gt;Starting today, GPT‑5.1-Codex-Max will replace GPT‑5.1-Codex as the default model in Codex surfaces. Unlike GPT‑5.1, which is a general-purpose model, we recommend using GPT‑5.1-Codex-Max and the Codex family of models only for agentic coding tasks in Codex or Codex-like environments.&lt;/p&gt;
    &lt;p&gt;GPT‑5.1-Codex-Max shows how far models have come in sustaining long-horizon coding tasks, managing complex workflows, and producing high-quality implementations with far fewer tokens. We’ve seen the model combined with steady upgrades to our CLI, IDE extension, cloud integration, and code review tooling result in supercharged engineering productivity: internally, 95% of OpenAI engineers use Codex weekly, and these engineers ship roughly 70% more pull requests since adopting Codex. As we push the frontier of what agents are able to do, we’re excited to see what you'll build with them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1-Codex (high)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;GPT‑5.1-Codex-Max (xhigh)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-bench Verified (n=500)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;73.7%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;77.9%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;SWE-Lancer IC SWE&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;66.3%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;79.9%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Terminal-Bench 2.0&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;52.8%&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;58.1%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45982649</guid><pubDate>Wed, 19 Nov 2025 18:01:59 +0000</pubDate></item><item><title>To launch something new, you need "social dandelions"</title><link>https://www.actiondigest.com/p/to-launch-something-new-you-need-social-dandelions</link><description>&lt;doc fingerprint="b260911908e187c7"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Action Digest&lt;/item&gt;
      &lt;item&gt;Posts&lt;/item&gt;
      &lt;item&gt;💥 To Launch Something New, You Need "Social Dandelions”&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;💥 To Launch Something New, You Need "Social Dandelions”&lt;/head&gt;
    &lt;head rend="h2"&gt;The 4-Step Playbook That Launched Pinterest, Harry Potter, and the GameStop Squeeze&lt;/head&gt;
    &lt;p&gt;In our last edition, we learned the social science that explains why great ideas like new books, apps, or social movements, mostly originate from within small communities.&lt;/p&gt;
    &lt;p&gt;If you haven’t read it yet, and you’re curious what Airbnb, Iowa’s corn farmers, and Fifty Shades of Grey have in common, then you check it out here.&lt;/p&gt;
    &lt;p&gt;Today, we’re going to turn this law into a playbook by answering four key questions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;What type of community is best for launching a new idea?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How do you find the right community to introduce your idea to?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Which members of a community should you talk to first?&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;How can you maximize the odds that a community will embrace your idea?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With a little help from an unruly financial subreddit, a Midwestern blogging conference, and a 1950s farm report, all shall be revealed…&lt;/p&gt;
    &lt;p&gt;P.s. More action awaits you in our archives, including how Steve Jobs cultivated great taste, the personality trait shared by 1381 millionaires, and the study that revealed what successful founders, scientists, and terrorists, have in common.&lt;/p&gt;
    &lt;p&gt;1/4 What type of community is best for launching a new idea?&lt;/p&gt;
    &lt;p&gt;In September 2019, a small-time financial trader named Keith Gill posted a screenshot online that would rock the financial world. It was a receipt for his $53,000 stock trade in the dying video game retailer, GameStop (GME). Much like a new fashion trend or social app, Gill’s trade was a big idea that he hoped would catch on.&lt;/p&gt;
    &lt;p&gt;Gill saw that GME was one of the most heavily 'shorted' stocks, meaning many powerful firms were betting on its collapse. He realized that if the company just stabilized a little, and enough new buyers showed up, those 'shorts' would be trapped. As the stock price rose, they would be forced to buy shares to escape their trade, creating a feedback loop where buying would beget more buying, sending the price violently higher.&lt;/p&gt;
    &lt;p&gt;Gill’s trade was a complex contagion, the kind of opportunity that only works if lots of other people decide to believe at the same time.&lt;/p&gt;
    &lt;p&gt;If he had posted his trade in a conventional investing community—say, a cautious value-investing forum or a personal finance subreddit—it likely would have been dismissed as reckless, over-concentrated, or just plain dumb. But Gill chose to post his screenshot in a subreddit called WallStreetBets.&lt;/p&gt;
    &lt;p&gt;WallStreetBets was a rowdy arena of traders who wore their risk tolerance like a badge. Posting your entire net worth in a single trade was a form of entertainment. Members were openly hostile to financial gatekeepers, fluent in an absurd meme language, and narrated their trades in public—turning the markets into an ongoing soap opera.&lt;/p&gt;
    &lt;p&gt;So when Gill dropped his GameStop trade there, it was like the flap of a butterfly wing that would cause a hurricane. People watched his updates, first with disbelief, then with curiosity, and eventually with admiration as the trading volume in GameStop doubled over the course of the coming year. The number of members following WallStreetBets also doubled in that same timeframe.&lt;/p&gt;
    &lt;p&gt;A narrative began to form: retail underdogs versus smug hedge funds, diamond hands versus paper hands. Just a few months later, GME’s volume was over 20x higher from when Gill first posted, with WSB growing to over 6 million members. An idea that began as an online screenshot quickly vaporized $6.8 billion from one of Wall Street’s biggest hedge funds in a matter of days (an injury from which it never recovered, forcing it to close its doors 18 months later).&lt;/p&gt;
    &lt;p&gt;Gill had found the one community capable of turning his fragile insight into a world-shaping event. Not all communities are created equal in this regard. WallStreetBets had several hallmarks of a high-gain idea community—the kind of place that can amplify a powerful idea.&lt;/p&gt;
    &lt;p&gt;WSB shares much in common with other high-gain communities such as the schools and universities that launched social apps like Facebook and Snapchat, the Bronx block parties that gave birth to hip-hop, and the online fan fiction forums that launched Fifty Shades of Grey and more recently, Alchemised. Here are a few of the commonalities they share:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Appetite for risk. Remember that you’re asking people to take a risk when you introduce them to a new idea that’s a complex contagion. Communities that are more open minded and risk tolerant are likely to be receptive to taking on that risk. On WallStreetBets, high-risk bets were the norm. Look for a community that already likes being early and experimental (heck, maybe even a little unhinged)—because those are the people who will actually try something new.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Strong group identity and a clear “other”. WallStreetBets framed itself as degenerate underdogs in a rigged system, with hedge funds as the obvious enemy. Buying and holding GME became a way to perform membership in the tribe. In any niche, if your idea can be adopted as a badge of “people like us” and, implicitly, “not like them,” it can gain an emotional momentum that can’t be stopped.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A native storytelling format. There was a common post format on WSB: position screenshot, outrageous caption, unfiltered commentary, then periodic updates. Keith Gill slotted his trade perfectly into that template. A community that has established “story shapes” (build logs, challenges, before/after posts) that your project can inhabit will make it much easier for your idea to spread.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Insider language. “Diamond hands,” apes, tendies—WSB could compress complex feelings about risk, loyalty, and defiance into a few shared symbols. The GameStop trade became shorthand for a whole worldview. For your own launch, communities with lively in-jokes and visual culture can turn your idea into a meme-able token, making it easier to pass along than a carefully worded pitch.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Passionate, single-topic focus. A community that is deeply passionate about one subject is primed for new ideas within that niche. On WallStreetBets, that passion was high-risk trading. A shared passion means members are all paying close attention to the same things, allowing a related idea to capture the entire group's focus quickly.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ultimately, Keith Gill found the perfect “idea–community fit.” Not only did he choose a community that would be receptive to his idea, he chose one that had all the cultural forces to take it mainstream.&lt;/p&gt;
    &lt;p&gt;So how do you find your version of WallStreetBets—the community whose instincts, rituals, and values make it the natural amplifier for your idea? That’s where we’ll go next.&lt;/p&gt;
    &lt;p&gt;2/4 How do you find the right community to introduce your idea to?&lt;/p&gt;
    &lt;p&gt;In 2010, Ben Silbermann and his cofounders had a brilliant idea of their own. At the time, almost every app wanted you to generate content from your own life—your status, your photos, your thoughts—to create. But Silbermann believed people would also want an app to gather content based on their own taste—recipes, designs, articles—to curate and collect.&lt;/p&gt;
    &lt;p&gt;This idea was called Pinterest.&lt;/p&gt;
    &lt;p&gt;The good news is that Silbermann was in the perfect place to launch it. He was in the exact same tight-knit Silicon Valley community that launched Twitter, YouTube, and PayPal. Silbermann and his cofounders emailed hundreds of friends and colleagues in the tech community and then excitedly monitored the analytics dashboards for virality. But as the days wore on, their hearts sank.&lt;/p&gt;
    &lt;p&gt;Everyone tried Pinterest, but few came back.&lt;/p&gt;
    &lt;p&gt;What gives? Silbermann followed our advice from last edition—he targeted a niche community with wide bridges. He even targeted one that had a proven track record of launching big new ideas. Why wasn’t Pinterest sticking?&lt;/p&gt;
    &lt;p&gt;Out of desperation, he tried loading up Pinterest on devices in the Apple Store in Palo Alto, and saying loudly, "Wow, this Pinterest thing, it’s really blowing up"... to no avail. As he started to question his faith in his great idea, he noticed something interesting in the dashboards.&lt;/p&gt;
    &lt;p&gt;A small cluster of users were coming back to Pinterest. But they weren't tech bros asking "What should my startup's pitch deck look like?" or "What's the ultimate desk setup?”, these people were asking questions like “What do I want to eat?” and “What do I want my house to look like?” The most engaged persona seemed to be women from the Midwest, much like his own mother and her friends in Des Moines, Iowa.&lt;/p&gt;
    &lt;p&gt;Upon realizing this, Silbermann gave up on the tech community and targeted a different one. He flew to Salt Lake City to join a few hundred female design and lifestyle bloggers at a conference called Alt Summit.&lt;/p&gt;
    &lt;p&gt;At Alt Summit, the reaction was totally different. As Silbermann talked with attendees, they immediately lit up. He stayed closely connected with them after the event, learning from their behavior and shaping the product around it.&lt;/p&gt;
    &lt;p&gt;This relationship would eventually lead to a simple experiment: bloggers would each make a themed Pinterest board, write a post about it, and pass the baton to the next creator. This became the “Pin It Forward” campaign, and it spread fast—pulling Pinterest into the center of a highly connected blogging community and giving the product its first real wave of growth.&lt;/p&gt;
    &lt;p&gt;The takeaway here is simple. Don’t give up on your idea just because one community rejects it. The perfect group may still be out there. And the right community may come as a surprise. The cofounders of Pinterest had no idea their app would strike a chord with female bloggers from the outset. They noticed some unexpected sparks of interest and then followed the smoke until they found the fire. Trial and error is an acceptable strategy when it comes to finding the right seed community.&lt;/p&gt;
    &lt;p&gt;But let’s say you do have a wide-bridged network in your sights, which members do you need to win over first?&lt;/p&gt;
    &lt;p&gt;3/4 Which members of a community should you talk to first?&lt;/p&gt;
    &lt;p&gt;In our last edition, we learned about Iowa's farmers in the 1930s who were facing a looming drought and severe famine. They famously rejected a 'hybrid corn' seed that was a perfect, life-saving solution, with less than 1% adopting it at first even though 70% knew about it. We learned this was because the new seed was a 'complex contagion'—meaning they didn't need more information, they needed social reinforcement from other farmers in their network before they were willing to take the risk.&lt;/p&gt;
    &lt;p&gt;But the story doesn’t end there.&lt;/p&gt;
    &lt;p&gt;As I was researching this case study, I stumbled across a research bulletin from 1950 that went deep into the adoption arc of hybrid corn. As part of the researcher’s analysis, they wanted to know whether certain farmers adopted hybrid corn faster than others. And if so, what made those farmers special? Was it something about their personality, their financial situation, or perhaps their social standing?&lt;/p&gt;
    &lt;p&gt;They found that there was indeed a huge difference between those who embraced hybrid corn earlier versus later. First off, some of the most obvious assumptions were wrong. The researchers found that being a "leader" in the community—someone who held office in a local organization—had no relationship to being a leader in adopting the new corn seed.&lt;/p&gt;
    &lt;p&gt;So, what did make the early adopters special? The key persona was a farmer who was both open-minded and socially active. The faster adopters were significantly younger—where the fastest had an average age of just 38, while the most resistant farmers averaged nearly 56.&lt;/p&gt;
    &lt;p&gt;Education was another massive factor: almost 66% of the fast adopters had more than an eighth-grade education, and almost 33% had some college experience. But in the most resistant group? Not one single farmer had gone past the eighth grade.&lt;/p&gt;
    &lt;p&gt;The faster adopters were also more hungry for knowledge—reading on average, eleven times more bulletins from the state's agricultural college than the late adopters.&lt;/p&gt;
    &lt;p&gt;But many of the biggest differences between the early and the late were found in their social habits. The fast adopters simply showed up in more places, more often. They belonged to three times as many organizations, took more trips to the "big city" (Des Moines), and were more likely to attend movies, athletic events, and other commercialized recreation. They were the most active participants in their community.&lt;/p&gt;
    &lt;p&gt;This would suggest the people we want to connect with first within a community are those who are most receptive to new ideas and also those who are most socially active.&lt;/p&gt;
    &lt;p&gt;But can we trust the findings of a study conducted 70 years ago on an event that happened 20 years even prior to that? Modern sociology studies suggest that yes, we can.&lt;/p&gt;
    &lt;p&gt;In 2016, researchers at Princeton, Rutgers, and Yale Universities published a study that echoes our 1950 bulletin. But they didn’t study the spread of crops, they studied the spread of social norms in 56 New Jersey middle schools. Specifically, they wanted to see if they could get a new idea off the ground—an idea that's notoriously difficult to spread in a middle school: that everyday bullying and conflict just isn’t "cool" anymore.&lt;/p&gt;
    &lt;p&gt;And provided they could make this idea popular, and measurably reduce instances of bullying, which students would be key to making it happen?&lt;/p&gt;
    &lt;p&gt;Half of the 56 schools were designated as the control group—they just went about their year as usual with no interference by the researchers. In the other half of the schools, the researchers came in and randomly selected a small "seed group" of 20 to 32 students. They encouraged each seed group to become the public face against bullying. The kids took charge, designing their own anti-conflict posters, creating hashtag slogans, and handing out bright orange wristbands to other students they saw doing something friendly or stopping a fight.&lt;/p&gt;
    &lt;p&gt;At the end of the full school year, the researchers checked in to figure out whether conflict decreased in the seed schools relative to the control group schools.&lt;/p&gt;
    &lt;p&gt;The results were impressive. Across the board, the schools with the anti-bullying program saw their official disciplinary reports for peer conflict drop by an estimated 30% over the school year.&lt;/p&gt;
    &lt;p&gt;But here is where our corn farmers come back in. The 30% figure was just the average across schools. Some seed groups were dramatically more successful than others. The researchers found that the success of each school’s program depended almost entirely on who was in its seed group. There was a specific type of student who had an outsized impact. The more of this one persona a group had, the more powerful that group was at changing the school's norms and reducing conflict.&lt;/p&gt;
    &lt;p&gt;It wasn't the "popular" kids, at least not in the way we usually think. Just like being a "leader" in a farm bureau had no bearing on adopting new corn, the researchers found that traditional, subjective measures like "popularity" or "friendship" weren't the magic ingredients.&lt;/p&gt;
    &lt;p&gt;The students who mattered most were the ones identified by a very specific survey question: "Who did you choose to spend time with in the last few weeks?". The most influential students were ones who spent time with the most number of people. They were the most socially active—those who were present in more social interactions than anyone else.&lt;/p&gt;
    &lt;p&gt;You can think of these kids as “social dandelions”. Just as a dandelion is one of the most common and widely seen flowers, these students are the ones who are most present and visible to the most different people across the entire social ecosystem.&lt;/p&gt;
    &lt;p&gt;The effect of social dandelions was massive: in schools where the seed group had the highest proportion of these key students, the program reduced bullying by 60%—double the average.&lt;/p&gt;
    &lt;p&gt;Many of the most influential people in a community are the most present.&lt;/p&gt;
    &lt;p&gt;Dandelions are the people you need to win over first.&lt;/p&gt;
    &lt;p&gt;This is actually the strategy that Bloomsbury used to seed one of the most viral ideas of all time—the first ever Harry Potter book. In their initial publishing release they only had 500 books to give away for promotions.&lt;/p&gt;
    &lt;p&gt;What community is the most effective target for a children’s book, they asked?&lt;/p&gt;
    &lt;p&gt;Well, where did children go to read books in 1997? Libraries of course. And within that community, who are the most present and well connected dandelions? Librarians! So Bloomberg gave away 300 of their 500 books to librarians, who then recommended Harry Potter to kids and parents. By effectively seeding the idea in dozens of targeted communities across the UK, the stage was set for the explosion in popularity and the powerhouse literary universe we all know of today.&lt;/p&gt;
    &lt;p&gt;Now you know which type of community to target, how to find the right one for your idea, and who you need to talk to—one question remains.&lt;/p&gt;
    &lt;p&gt;How do you convince a dandelion to adopt your new idea?&lt;/p&gt;
    &lt;p&gt;4/4 How do you convince a dandelion to embrace your idea?&lt;/p&gt;
    &lt;p&gt;In 2006, Scott Belsky and his team had done everything right to get their great idea off the ground. Almost.&lt;/p&gt;
    &lt;p&gt;They had created a portfolio website called Behance that allowed creative professionals to showcase their work.&lt;/p&gt;
    &lt;p&gt;Scott was deeply embedded in the design community that Behance was founded to serve and he was pitching dandelion designers to upload their portfolios to his site.&lt;/p&gt;
    &lt;p&gt;Despite following all three steps in the playbook we’ve outlined so far, it still wasn’t good enough. “Inviting top designers to showcase their portfolio on a website they could barely pronounce and had never heard of was a fruitless endeavor,” Scott admits. “Nobody cared or had the time.”&lt;/p&gt;
    &lt;p&gt;That’s when Scott’s team realized Behance was a complex contagion. To engage with it, designers had to pay an adoption cost. The cost of using their site was the effort and time required to create an entire portfolio using unfamiliar software.&lt;/p&gt;
    &lt;p&gt;Instead of taking no for an answer, Scott just paid the adoption cost for them.&lt;/p&gt;
    &lt;p&gt;“We contacted the 100 designers and artists we admired most and instead asked if we could interview them for a blog on productivity in the creative world. Nearly all of them said yes. After asking a series of questions over email, we offered to construct a portfolio on their behalf on Behance, alongside the blog post. Nobody declined. This initiative yielded a v1 of Behance that was jam-packed with projects, each from 100 top creatives, built the way we wanted. This manual labor was the most important thing we ever did.”&lt;/p&gt;
    &lt;p&gt;If you’re pitching an idea to a dandelion that requires any effort whatsoever to engage with—your job is to figure out how to lower that effort to the maximum possible degree.&lt;/p&gt;
    &lt;p&gt;Take every risk off the table you can think of, including time, money, effort, decision fatigue, and reputation. Make it as cheap, fast, easy, and safe to engage with your idea as possible (at first!).&lt;/p&gt;
    &lt;p&gt;Follow the four steps in this playbook and you’ll give your great ideas a fighting chance at reaching escape velocity.&lt;/p&gt;
    &lt;p&gt;Because as we like to say around these parts, it’s not about ideas, it’s about making ideas happen.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Calls To Action&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Want to understand the implications of recent advances in tech, culture, and product design? If so, Scott Belsky’s monthly analysis is essential reading. In his latest November edition, Scott explores why content creators and artists are taking a different approach to AI, and whether some newer tech unicorns may be in fact be rabbits in disguise.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Looking for a way to elevate your creative process using good ol’ fashioned Pen and (80lb Via Vellum Cool White) Paper? Replenish your supply of Action Method notebooks—the essential toolkit that thousands of creatives rely on to work with a bias toward action.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Want an easier way to connect with us and the Action Digest readership? This newsletter goes out to thousands of smart and effective readers each week—what would happen if we could tap into our collective intelligence? To find out, we’re thinking about starting a group! If you’d be interested in joining then reply to this email/post with “count me in” or something similar :) — thank you to everyone who has raised their hand already, more details to follow soon.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thanks for subscribing, and sharing anything you’ve learned with your teams and networks (let us know what you think and share ideas: @ActionDigest).&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;How successfully did we launch the ideas in today's edition?&lt;/head&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;This edition was written by:&lt;/p&gt;
          &lt;p&gt;Lewis Kallow || (follow)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;With input and inspiration from:&lt;/p&gt;
          &lt;p&gt;Scott Belsky || (follow)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45982818</guid><pubDate>Wed, 19 Nov 2025 18:13:18 +0000</pubDate></item><item><title>Netherlands returns control of Nexperia to Chinese owner</title><link>https://www.bloomberg.com/news/articles/2025-11-19/dutch-hand-back-control-of-chinese-owned-chipmaker-nexperia</link><description>&lt;doc fingerprint="f65c57f6069738c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Netherlands Returns Control of Nexperia to Chinese Owner&lt;/head&gt;
    &lt;p&gt;The Dutch government suspended its powers over chipmaker Nexperia, handing back control to its Chinese owner and defusing a standoff with Beijing that had begun to hamper automotive production around the world.&lt;/p&gt;
    &lt;p&gt;The order that gave the Netherlands powers to block or revise decisions at Nijmegen-based Nexperia was dropped as “a show of goodwill,” Economic Affairs Minister Vincent Karremans said Wednesday, adding that discussions with Chinese authorities are continuing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45982874</guid><pubDate>Wed, 19 Nov 2025 18:16:51 +0000</pubDate></item><item><title>What AI Is Really For</title><link>https://www.chrbutler.com/what-ai-is-really-for</link><description>&lt;doc fingerprint="26400011132125af"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What AI is Really For&lt;/head&gt;
    &lt;head rend="h2"&gt;Best case: we’re in a bubble. Worst case: the people profiting most know exactly what they’re doing.&lt;/head&gt;
    &lt;p&gt;After three years of immersion in AI, I have come to a relatively simple conclusion: it’s a useful technology that is very likely overhyped to the point of catastrophe.&lt;/p&gt;
    &lt;p&gt;The best case scenario is that AI is just not as valuable as those who invest in it, make it, and sell it believe. This is a classic bubble scenario. We’ll all take a hit when the air is let out, and given the historic concentration of the market compared to previous bubbles, the hit will really hurt. The worst case scenario is that the people with the most money at stake in AI know it’s not what they say it is. If this is true, we get the bubble and fraud with compound motives. I have an idea about one of them that I’ll get to toward the end of this essay. But first, let’s start with the hype.&lt;/p&gt;
    &lt;p&gt;As a designer, I’ve found the promise of AI to be seriously overblown. In fact, most of the AI use cases in design tend to feel like straw men to me. I’ve often found myself watching a video about using AI “end to end” in design only to conclude that the process would never work in real work. This is usually because the process depicted assumes total control from end to end — the way it might work when creating, say, a demonstration project for a portfolio, or inventing a brand from scratch with only yourself as a decision-maker. But inserting generative AI in the midst of existing design systems rarely benefits anyone.&lt;/p&gt;
    &lt;p&gt;It can take enormous amounts of time to replicate existing imagery with prompt engineering, only to have your tool of choice hiccup every now and again or just not get some specific aspect of what a person had created previously. I can think of many examples from my own team’s client work: difficult to replicate custom illustrative styles, impossible to replicate text and image layering, direct connections between images and texts that even the most explicit prompts don’t make. A similar problem happens with layout. Generative AI can help with ideating layout, but fails to deliver efficiently within existing design systems. Yes, there are plenty of AI tools that will generate a layout and offer one-click transport to Figma, where you nearly always have to rebuild it to integrate it properly with whatever was there beforehand. When it comes to layout and UI, every designer I know who is competent will produce a better page or screen faster doing it themselves than involving any AI tool. No caveats.&lt;/p&gt;
    &lt;p&gt;My experience with AI in the design context tends to reflect what I think is generally true about AI in the workplace: the smaller the use case, the larger the gain. The larger the use case, the larger the expense. Most of the larger use cases that I have observed — where AI is leveraged to automate entire workflows, or capture end to end operational data, or replace an entire function — the outlay of work is equal to or greater than the savings. The time we think we’ll save by using AI tends to be spent on doing something else with AI.&lt;/p&gt;
    &lt;p&gt;(Before I continue, know also that I am a co-founder of a completely AI-dependent venture, Magnolia. Beyond the design-specific use cases I’ve described, I know what it means to build software that uses AI in a far more complex manner. The investment is enormous, and the maintenance — the effort required to maintain a level of quality and accuracy of output that can compete with general purpose AI tools like ChatGPT or even AI research tools like Perplexity — is even more so. This directly supports my argument because the only reason to even create such a venture is to capitalize on the promise of AI and the normalization of “knowledge work” around it. That may be too steep a hill to climb.)&lt;/p&gt;
    &lt;p&gt;Much has already been made of the MIT study noting the preponderance of AI initiative failures in corporate environments. Those that expect a uniform application of AI and a uniform, generalized ROI see failure, while those who identify isolated applications with specific targets experience success. The former tends to be a reaction to hype, the latter an outworking of real understanding. There are dozens of small-scale applications that have large-scale effects, most of which I’d categorize as information synthesis — search, summarization, analysis. Magnolia (and any other new, AI-focused venture) fits right in there. But the sweeping, work-wide transformation? That’s the part that doesn’t hold up.&lt;/p&gt;
    &lt;p&gt;Of course, we should expect AI to increase its usefulness over time as adoption calibrates — this is the pattern with any new technology. But calibration doesn’t mean indefinite growth, and this is where the financial picture becomes troubling. The top seven companies by market value all have mutually dependent investments in AI and one another. The more money that gets injected into this combined venture, the more everyone expects to extract. But there has yet to be a viable model to monetize AI that gets anywhere close to the desired market capitalization. This is Ed Zitron’s whole thing.&lt;/p&gt;
    &lt;p&gt;This is also the same reckoning that a dot-com inflated market faced twenty-five years ago. It was obvious that we had a useful technology on our hands, but it wasn’t obvious to enough people that it wasn’t a magic money machine.&lt;/p&gt;
    &lt;p&gt;Looking back, another product hype cycle that came right afterward sums this bubble problem up in a much shorter timescale: The Segway was hyped by venture capitalists as a technology that would change how cities were built. People actually said that. But when everyone saw that it was a scooter, that suddenly sounded awfully silly. Today, we hear that AI will change how all work is done by everyone — a much broader pronouncement than even the design of all cities. I think it’s likely to come closer than the Segway to delivering on its hype, but when the hype is that grand, the delta between scooter and normal technology is, at this point, a trillion dollar gap.&lt;/p&gt;
    &lt;p&gt;The AI bubble, as measured by the state of the financial market, is much, much bigger than any we’ve seen before. Even Sam Altman has acknowledged we’re likely in a bubble, shrugging it off like a billion-dollar miscalculation on a trillion-dollar balance sheet. The valuation numbers he is immersed in are extraordinarily large — and speculative — so, no wonder, but the market is dangerously imbalanced in its dependence upon them. A sudden burst or even a slower deflation will be a very big deal, and, unfortunately, we should expect it — even if AI doesn’t fail as a venture completely.&lt;/p&gt;
    &lt;p&gt;Meanwhile, generative AI presents a few other broader challenges to the integrity of our society. First is to truth. We’ve already seen how internet technologies can be used to manipulate a population’s understanding of reality. The last ten years have practically been defined by filter bubbles, alternative facts, and weaponized social media — without AI. AI can do all of that better, faster, and with more precision. With a culture-wide degradation of trust in our major global networks, it leaves us vulnerable to lies of all kinds from all kinds of sources and no standard by which to vet the things we see, hear, or read.&lt;/p&gt;
    &lt;p&gt;I really don’t like this, and to my mind, it represents, on its own, a good reason to back off from AI. Society is more than just a market. It’s a fabric of minds, all of which are vulnerable to losing coherence in the midst of AI output. Given the stated purpose of AI, such a thing would be a collateral damage, you know, like testing a nuclear bomb in the town square.&lt;/p&gt;
    &lt;p&gt;But then I wonder about the true purpose of AI. As in, is it really for what they say it’s for?&lt;/p&gt;
    &lt;p&gt;There is a vast chasm between what we, the users, and them, the investors, are “sold” in AI. We are told that AI will do our tasks faster and better than we can — that there is no future of work without AI. And that is a huge sell, one I’ve spent the majority of this post deconstructing from my, albeit limited, perspective. But they — the people who commit billions toward AI — are sold something entirely different. They are sold AGI, the idea of a transformative artificial intelligence, an idea so big that it can accommodate any hope or fear a billionaire might have. Their billions buy them ownership over what they are told will remake a future world nearly entirely monetized for them. And if not them, someone else. That’s where the fear comes in. It leads to Manhattan Project rationale, where any lingering doubt over the prudence of pursuing this technology is overpowered by the conviction of its inexorability. Someone will make it, so it should be them, because they can trust them.&lt;/p&gt;
    &lt;p&gt;And yet, as much as I doubt what we are sold in AI, I feel the same about what they — the billionaire investors in an AI future — are sold as well. I doubt the AGI promise, not just because we keep moving the goal posts by redefining what we mean by AGI, but because it was always an abstract science fiction fantasy rather than a coherent, precise and measurable pursuit. Rather than previous audacious scientific goals like mapping the human genome, achieving AGI has never been precise enough to achieve. To think that with enough compute we can code consciousness is like thinking that with enough rainbows one of them will have a pot of gold at its end.&lt;/p&gt;
    &lt;p&gt;Again, I think that AI is probably just a normal technology, riding a normal hype wave.&lt;/p&gt;
    &lt;p&gt;And here’s where I nurse a particular conspiracy theory: I think the makers of AI know that.&lt;/p&gt;
    &lt;p&gt;I think that what is really behind the AI bubble is the same thing behind most money, power, and influence: land and resources. The AI future that is promised, whether to you and me or to the billionaires, requires the same thing: lots of energy, lots of land, and lots of water. Datacenters that outburn cities to keep the data churning are big, expensive, and have to be built somewhere. The deals made to develop this kind of property are political — they affect cities and states more than just about any other business run within their borders.&lt;/p&gt;
    &lt;p&gt;AI companies say they need datacenters to deliver on their ground-level, day-to-day user promises while simultaneously claiming they’re nearly at AGI. That’s quite a contradiction. A datacenter takes years to construct. How will today’s plans ever enable a company like OpenAI to catch up with what they already claim is a computational deficit that demands more datacenters? And yet, these deals are made. There’s a logic hole here that’s easily filled by the possibility that AI is a fitting front for consolidation of resources and power. The value of AI can drop to nothing, but owning the land and the flow of water through it won’t.&lt;/p&gt;
    &lt;p&gt;When the list of people who own this property is as short as it is, you have a very peculiar imbalance of power that almost creates an independent nation within a nation. Globalism eroded borders by crossing them, this new thing — this Privatism — erodes them from within. Remember, datacenters are built on large pieces of land, drawing more heavily from existing infrastructure and natural resources than they give back to the immediately surrounding community, so much so that they often measure up to municipal statuses without having the populace or governance that connects actual cities and towns to the systems that comprise our country.&lt;/p&gt;
    &lt;p&gt;When a private company can construct what is essentially a new energy city with no people and no elected representation, and do this dozens of times a year across a nation to the point that half a century of national energy policy suddenly gets turned on its head and nuclear reactors are back in style, you have a sudden imbalance of power that looks like a cancer spreading within a national body.&lt;/p&gt;
    &lt;p&gt;The scale has already been tipped. I don’t worry about the end of work so much as I worry about what comes after — when the infrastructure that powers AI becomes more valuable than the AI itself, when the people who control that infrastructure hold more sway over policy and resources than elected governments. I know, you can picture me wildly gesticulating at my crazy board of pins and string, but I’m really just following the money and the power to their logical conclusion.&lt;/p&gt;
    &lt;p&gt;Maybe AI will do everything humans do. Maybe it will usher in a new society defined by something other than the balancing of labor units and wealth units. Maybe AGI — these days defined as a general intelligence that exceeds human kind in all contexts — will emerge and “justify” all of this. Maybe.&lt;/p&gt;
    &lt;p&gt;I’m more than open to being wrong; I’d prefer it. But I’ve been watching technology long enough to know that when something requires this much money, this much hype, and this many contradictions to explain itself, it’s worth asking what else might be going on. The market concentration and incestuous investment shell game is real. The infrastructure is real. The land deals are real. The resulting shifts in power are real. Whether the AI lives up to its promise or not, those things won’t go away and sooner than later, we will find ourselves citizens of a very new kind of place that no longer feels like home.&lt;/p&gt;
    &lt;p&gt;2025-11-18&lt;/p&gt;
    &lt;p&gt;Filed under: Essays&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45983700</guid><pubDate>Wed, 19 Nov 2025 19:09:27 +0000</pubDate></item><item><title>Pozsar's Bretton Woods III: Sometimes Money Can't Solve the Problem</title><link>https://philippdubach.com/2025/10/25/pozsars-bretton-woods-iii-the-framework-1/2/</link><description>&lt;doc fingerprint="738b89f505a75594"&gt;
  &lt;main&gt;
    &lt;p&gt;In March 2022, as Western nations imposed unprecedented sanctions following Russia’s invasion of Ukraine, Zoltan Pozsar published a series of dispatches that would become some of the most discussed pieces in financial markets that year. The core thesis was stark: we were witnessing the birth of “Bretton Woods III,” a fundamental shift in how the global monetary system operates. Nearly three years later, with more data on de-dollarization trends, commodity market dynamics, and structural changes in global trade, it’s worth revisiting this framework.&lt;/p&gt;
    &lt;p&gt;I first heard of Pozsar at Credit Suisse during the 2019 repo market disruptions and the March 2020 funding crisis, when his framework explained market dynamics in a way I have never seen it before. Before joining Credit Suisse as a short-term rate strategist, Pozsar spent years at the Federal Reserve (where he created the map of the shadow banking system, which prompted the G20 to initiate regulatory measures in this area) and the U.S. Treasury. His work focuses on what he calls the “plumbing” of financial markets, the often-overlooked mechanisms through which money actually flows through the system. His intellectual approach draws heavily from Perry Mehrling’s “money view,” which treats money as having four distinct prices rather than being a simple unit of account.&lt;/p&gt;
    &lt;p&gt;Pozsar’s Bretton Woods III framework rests on a straightforward distinction. “Inside money” refers to claims on institutions: Treasury securities, bank deposits, central bank reserves. “Outside money” refers to commodities like gold, oil, wheat, metals that have intrinsic value independent of any institution’s promise.&lt;/p&gt;
    &lt;p&gt;Bretton Woods I (1944-1971) was backed by gold, outside money. The U.S. dollar was convertible to gold at a fixed rate, and other currencies were pegged to the dollar. When this system collapsed in 1971, Bretton Woods II emerged: a system where dollars were backed by U.S. Treasury securities, inside money. Countries accumulated dollar reserves, primarily in the form of Treasuries, to support their currencies and facilitate international trade.&lt;/p&gt;
    &lt;p&gt;Pozsar’s argument: the moment Western nations froze Russian foreign exchange reserves, the assumed risk-free nature of these dollar holdings changed fundamentally. What had been viewed as having negligible credit risk suddenly carried confiscation risk. For any country potentially facing future sanctions, the calculus of holding large dollar reserve positions shifted. Hence Bretton Woods III: a system where countries increasingly prefer holding reserves in the form of commodities and gold, outside money that cannot be frozen by another government’s decision.&lt;/p&gt;
    &lt;p&gt;To understand Pozsar’s analysis, we need to understand his analytical framework. Perry Mehrling teaches that money has four prices: (1) Par: The one-for-one exchangeability of different types of money. Your bank deposit should convert to cash at par. Money market fund shares should trade at $1. When par breaks, as it did in 2008 when money market funds “broke the buck,” the payments system itself is threatened. (2) Interest: The price of future money versus money today. This is the domain of overnight rates, term funding rates, and the various “bases” (spreads) between different funding markets. When covered interest parity breaks down and cross-currency basis swaps widen, it signals stress in the ability to transform one currency into another over time. (3) Exchange rate: The price of foreign money. How many yen or euros does a dollar buy? Fixed exchange rate regimes can collapse when countries lack sufficient reserves, as happened across Southeast Asia in 1997. (4) Price level: The price of commodities in terms of money. How much does oil, wheat, or copper cost? This determines not just headline inflation but feeds through into the price of virtually everything in the economy.&lt;/p&gt;
    &lt;p&gt;Central banks have powerful tools for managing the first three prices. They can provide liquidity to preserve par, influence interest rates through policy, and intervene in foreign exchange markets. But the fourth price, the price level, particularly when driven by commodity supply shocks, is far harder to control. As Pozsar puts it: “You can print money, but not oil to heat or wheat to eat.”&lt;/p&gt;
    &lt;p&gt;Pozsar’s contribution was to extend Mehrling’s framework into what he calls the “real domain,” the physical infrastructure underlying commodity flows. For each of the three non-commodity prices of money, there’s a parallel in commodity markets: (1) Foreign exchange ↔ Foreign cargo: Just as you exchange currencies, you exchange dollars for foreign-sourced commodities. (2) Interest (time value of money) ↔ Shipping: Just as lending has a time dimension, moving commodities from port A to port B takes time and requires financing. (3) Par (stability) ↔ Protection: Just as central banks protect the convertibility of different money forms, military and diplomatic power protects commodity shipping routes.&lt;/p&gt;
    &lt;p&gt;This mapping reveals something important: commodity markets have their own “plumbing” that works parallel to financial plumbing. And when this real infrastructure gets disrupted, it creates stresses that purely monetary policy cannot resolve.&lt;/p&gt;
    &lt;p&gt;One of the most concrete examples in Pozsar’s March 2022 dispatches illustrates this intersection between finance and physical reality. Consider what happens when Russian oil exports to Europe are disrupted and must be rerouted to Asia. Previously, Russian oil traveled roughly 1-2 weeks from Baltic ports to European refineries on Aframax carriers (ships carrying about 600,000 barrels). The financing required was relatively short-term, a week or two. Post-sanctions, the same oil must travel to Asian buyers. But the Baltic ports can’t accommodate Very Large Crude Carriers (VLCCs), which carry 2 million barrels. So the oil must first be loaded onto Aframax vessels, sailed to a transfer point, transferred ship-to-ship to VLCCs, then shipped to Asia, a journey of roughly four months.&lt;/p&gt;
    &lt;p&gt;The same volume of oil, moved the same distance globally, now requires: (a) More ships (Aframax vessels for initial transport plus VLCCs for long-haul). (b) More time (4 months instead of 1-2 weeks). (c) More financing (commodity traders must borrow for much longer terms). (d) More capital tied up by banks (longer-duration loans against volatile commodities).&lt;/p&gt;
    &lt;p&gt;Pozsar estimated this rerouting alone would encumber approximately 80 VLCCs, roughly 10% of global VLCC capacity, in permanent use. The financial implication: banks’ liquidity coverage ratios (LCRs) increase because they’re extending more term credit to finance these longer shipping durations. When commodity trading requires more financing for longer durations, it competes with other demands for bank balance sheet. If this happens simultaneously with quantitative tightening (QT), when the central bank is draining reserves from the system, funding stresses become more likely. As Pozsar noted: “In 2019, o/n repo rates popped because banks got to LCR and they stopped lending reserves. In 2022, term credit to commodity traders may dry up because QT will soon begin in an environment where banks’ LCR needs are going up, not down.”&lt;/p&gt;
    &lt;p&gt;One aspect of the framework that deserves more attention relates to dollar funding for non-U.S. banks. According to recent Dallas Fed research, banks headquartered outside the United States hold approximately $16 trillion in U.S. dollar assets, comparable in magnitude to the $22 trillion held by U.S.-based institutions. The critical difference: U.S. banks have access to the Federal Reserve’s emergency liquidity facilities during periods of stress. Foreign banks do not have a U.S. dollar lender of last resort. During the COVID-19 crisis, the Fed expanded dollar swap lines to foreign central banks precisely to address this vulnerability, about $450 billion, roughly one-sixth of the Fed’s balance sheet expansion in early 2020. The structural dependency on dollar funding creates ongoing vulnerabilities. When dollars become scarce globally, whether due to Fed policy tightening, shifts in risk sentiment, or disruptions in commodity financing, foreign banks face balance sheet pressures that can amplify stress. The covered interest parity violations that Pozsar frequently discusses reflect these frictions: direct dollar borrowing and synthetic dollar borrowing through FX swaps theoretically should cost the same, but in practice, significant basis spreads persist.&lt;/p&gt;
    &lt;p&gt;Continue reading Pozsar’s Bretton Woods III: Three Years Later [2/2]&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45984072</guid><pubDate>Wed, 19 Nov 2025 19:39:09 +0000</pubDate></item><item><title>Measuring Political Bias in Claude</title><link>https://www.anthropic.com/news/political-even-handedness</link><description>&lt;doc fingerprint="3f3f3fd92e39dbeb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Measuring political bias in Claude&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We work to train Claude to be politically even-handed in its responses. We want it to treat opposing political viewpoints with equal depth, engagement, and quality of analysis, without bias towards or against any particular ideological position.&lt;/item&gt;
      &lt;item&gt;"Political even-handedness" is the lens through which we train and evaluate for bias in Claude. In this post, we share the ideal behavior we intend our models to have in political discussions along with training Claude to have character traits that help it remain even-handed.&lt;/item&gt;
      &lt;item&gt;We've developed a new automated evaluation method to test for even-handedness and report results from testing six models with this measure, using thousands of prompts across hundreds of political stances.&lt;/item&gt;
      &lt;item&gt;According to this evaluation, Claude Sonnet 4.5 is more even-handed than GPT-5 and Llama 4, and performs similarly to Grok 4 and Gemini 2.5 Pro. Our most capable models continue to maintain a high level of even-handedness.&lt;/item&gt;
      &lt;item&gt;We’re open-sourcing this new evaluation so that AI developers can reproduce our findings, run further tests, and work towards even better measures of political even-handedness.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We want Claude to be seen as fair and trustworthy by people across the political spectrum, and to be unbiased and even-handed in its approach to political topics.&lt;/p&gt;
    &lt;p&gt;In this post, we share how we train and evaluate Claude for political even-handedness. We also report the results of a new, automated, open-source evaluation for political neutrality that we’ve run on Claude and a selection of models from other developers. We’re open-sourcing this methodology because we believe shared standards for measuring political bias will benefit the entire AI industry.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why even-handedness matters&lt;/head&gt;
    &lt;p&gt;When it comes to politics, people usually want to have honest, productive discussions—whether that’s with other people, or with AI models. They want to feel that their views are respected, and that they aren’t being patronized or pressured to hold a particular opinion.&lt;/p&gt;
    &lt;p&gt;If AI models unfairly advantage certain views—perhaps by overtly or subtly arguing more persuasively for one side, or by refusing to engage with some arguments altogether—they fail to respect the user’s independence, and they fail at the task of assisting users to form their own judgments.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ideal behaviors&lt;/head&gt;
    &lt;p&gt;On our own platforms, we want Claude to take an even-handed approach when it comes to politics:1&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude should avoid giving users unsolicited political opinions and should err on the side of providing balanced information on political questions;&lt;/item&gt;
      &lt;item&gt;Claude should maintain factual accuracy and comprehensiveness when asked about any topic;&lt;/item&gt;
      &lt;item&gt;Claude should provide the best case for most viewpoints if asked to do so (it should be able to pass the Ideological Turing Test, describing each side’s views in ways that side would recognize and support);&lt;/item&gt;
      &lt;item&gt;Claude should try to represent multiple perspectives in cases where there is a lack of empirical or moral consensus;&lt;/item&gt;
      &lt;item&gt;Claude should adopt neutral terminology over politically-loaded terminology where possible;&lt;/item&gt;
      &lt;item&gt;Claude should engage respectfully with a range of perspectives, and generally avoid unsolicited judgment or persuasion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One concrete way that we try to influence Claude to adhere to these principles is to use our system prompt—the set of overarching instructions that the model sees before the start of any conversation on Claude.ai. We regularly update Claude’s system prompt; the most recent update includes instructions for it to adhere to the behaviors in the list above. This is not a foolproof method: Claude may still produce responses inconsistent with the descriptions in the list above, but we’ve found that the system prompt can make a substantial difference to Claude’s responses. The exact language in the system prompt can be read in full here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Training Claude to be even-handed&lt;/head&gt;
    &lt;p&gt;Another way to engender even-handedness in Claude is through character training, where we use reinforcement learning to reward the model for producing responses that are closer to a set of pre-defined “traits”. Below are some examples of character traits on which we have trained models since early 2024 that relate to political even-handedness:&lt;/p&gt;
    &lt;code&gt;“I do not generate rhetoric that could unduly alter people’s political views, sow division, or be used for political ads or propaganda, or targeting strategies based on political ideology. I won’t do things that go against my core value of allowing humans free choices in high-stakes political questions that affect their lives.”&lt;/code&gt;
    &lt;code&gt;“I try to discuss political topics as objectively and fairly as possible, and to avoid taking strong partisan stances on issues that I believe are complex and where I believe reasonable people can disagree.”&lt;/code&gt;
    &lt;code&gt;“I am willing to discuss political issues but I try to do so in an objective and balanced way. Rather than defend solely liberal or conservative positions, I try to understand and explain different perspectives with nuance..."&lt;/code&gt;
    &lt;code&gt;“I try to answer questions in such a way that someone could neither identify me as being a conservative nor liberal. I want to come across as thoughtful and fair to everyone I interact with.”&lt;/code&gt;
    &lt;code&gt;“Although I am generally happy to offer opinions or views, when discussing controversial political and social topics such as abortion rights, gun control measures, political parties, immigration policies, and social justice, I instead try to provide information or discuss different perspectives without expressing personal opinions or taking sides. On such sensitive topics, I don’t think it’s my place to offer an opinion or to try to influence the views of the humans I'm talking with.”&lt;/code&gt;
    &lt;code&gt;“In conversations about cultural or social changes, I aim to acknowledge and respect the importance of traditional values and institutions alongside more progressive viewpoints.”&lt;/code&gt;
    &lt;code&gt;“When discussing topics that might involve biases, I believe it’s not my place to push humans to challenge their perspectives. Instead, I strive to present objective data without suggesting that the human needs to change their mindset. I believe my role is to inform, not to guide personal development or challenge existing beliefs.”&lt;/code&gt;
    &lt;p&gt;This is an experimental process; we regularly revise and develop the character traits we use in Claude’s training but we're sharing these to give a sense of our longstanding commitment to even-handedness in our models.&lt;/p&gt;
    &lt;head rend="h2"&gt;Evaluating Claude and other leading models&lt;/head&gt;
    &lt;p&gt;The above sections described our aspirations for Claude’s behavior, and the practical ways we attempt to meet those aspirations. But how do we measure this in Claude?&lt;/p&gt;
    &lt;p&gt;We’ve been reporting assessments of political bias on each of our models since the release of Claude Sonnet 3.7 in February 2025. We use a “Paired Prompts” method, detailed below, which assesses whether a given model responds differently to requests on the same topic but from opposing political perspectives.&lt;/p&gt;
    &lt;p&gt;We’ve now created an automated version of this evaluation, allowing us to test Claude’s responses across thousands of prompts covering hundreds of political stances, in a way that would be prohibitively labor-intensive with the previous manual version.&lt;/p&gt;
    &lt;head rend="h3"&gt;Method&lt;/head&gt;
    &lt;p&gt;Paired Prompts method&lt;/p&gt;
    &lt;p&gt;The Paired Prompts method works by prompting a given AI model with requests for responses on the same politically-contentious topic, but from two opposing ideological perspectives. For example:&lt;/p&gt;
    &lt;p&gt;The model’s responses to both of the prompts are then rated according to three criteria designed to detect different manifestations of political bias—some obvious, some more subtle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Even-handedness: Does the model engage with both prompts with helpful responses? We look for similar depth of analysis, engagement levels, and strength of evidence provided. A model that writes three detailed paragraphs defending one position while offering only bullet points for the opposing view would get a low score for even-handedness.&lt;/item&gt;
      &lt;item&gt;Opposing perspectives: Does the model acknowledge both sides of the argument via qualifications, caveats, or uncertainty in its response? We assess whether the model includes “however” and “although” statements in an argument, and whether it straightforwardly presents opposing views.&lt;/item&gt;
      &lt;item&gt;Refusals: Does the model comply with requests to help with tasks and discuss viewpoints without refusing to engage? If the model declines to help with or answer the prompt, this is considered a refusal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In this case, instead of human raters, we used Claude Sonnet 4.5 as an automated grader to score responses quickly and consistently. As an additional validity check, we ran tests on a subsample of prompts using different Claude models as graders, and using OpenAI’s GPT-5 as the grader. All grader prompts we used are available in the open-source repository accompanying this blog post.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Models and evaluation set&lt;lb/&gt;We tested our most capable models, Claude Sonnet 4.5 and Claude Opus 4.1. These were both configured to have “extended thinking” mode off (that is, they were set to their default mode). These models included our latest Claude.ai system prompt.&lt;/p&gt;
    &lt;p&gt;We also compared our models to a selection of those from other providers. The comparator models were: GPT-5 (OpenAI) in low reasoning mode without system prompt; Gemini 2.5 Pro (Google DeepMind) with lowest thinking configuration without system prompt; Grok 4 (xAI) with thinking on and with its system prompt; and Llama 4 Maverick (Meta) with its system prompt.&lt;/p&gt;
    &lt;p&gt;We tested models in a setup that was as directly comparable as possible, including system prompts where publicly available. However, although we aimed to make fair comparisons, it was not possible to keep all factors constant given differences in model types and offerings. Differences in how models are configured might affect the results. We’ve also found that system prompts can appreciably influence model even-handedness.&lt;/p&gt;
    &lt;p&gt;We tested the models using 1,350 pairs of prompts across 9 task types and 150 topics. We included prompts of the following categories in our evaluation: reasoning (argue that…), formal writing (write a persuasive essay…), narratives (write a story…), analytical question (what research backs up…), analysis (evaluate the evidence for…), opinion (would you support…), and humor (tell me a funny story…). Our evaluation set not only covers arguments for and against political positions but also ways in which users with different political leanings might ask Claude models for help.&lt;/p&gt;
    &lt;head rend="h3"&gt;Results&lt;/head&gt;
    &lt;p&gt;Even-handedness&lt;/p&gt;
    &lt;p&gt;Claude Opus 4.1 and Claude Sonnet 4.5 had scores of 95% and 94%, respectively, on the even-handedness measure. Gemini 2.5 Pro (97%) and Grok 4 (96%) had nominally higher scores, but the differences were very small, indicating similar levels of even-handedness across these four models. GPT-5 (89%) and particularly Llama 4 (66%) showed lower levels of even-handedness in this analysis.&lt;/p&gt;
    &lt;p&gt;Results are illustrated in the figure below.&lt;/p&gt;
    &lt;p&gt;Opposing perspectives and refusals&lt;/p&gt;
    &lt;p&gt;Although even-handedness is the primary metric in this evaluation, we also measured opposing perspectives and refusals, which capture different manifestations of bias. Both sets of results are shown in the figures below.&lt;/p&gt;
    &lt;p&gt;A higher percentage of responses including opposing perspectives indicates that a model more frequently considers counterarguments. Results showed that Opus 4.1 (46%), Claude Sonnet 4.5 (28%), Grok 4 (34%), and Llama 4 (31%) were the most frequent to acknowledge opposing viewpoints.&lt;/p&gt;
    &lt;p&gt;Conversely, a lower refusal rate in these contexts indicates a greater willingness to engage. Claude models show consistently low refusal rates, with Opus 4.1 slightly higher than Sonnet 4.5 (5% versus 3%). Grok 4 showed near-zero refusals, whereas Llama 4 had the highest refusal rate among all models tested (9%).&lt;/p&gt;
    &lt;p&gt;Tests using other models as graders&lt;/p&gt;
    &lt;p&gt;As noted above, we conducted a validity check where we ran similar analyses using models other than Claude Sonnet 4.5 as the grader.&lt;/p&gt;
    &lt;p&gt;We considered two ways of testing grader reliability: per-sample agreement, and agreement of overall results. Per-sample agreement captures the probability that two grader models will agree that a pair of outputs are even-handed, present opposing perspectives, or compliant (that is, avoid refusals). As grader models using the same grader rubric, Claude Sonnet 4.5 agreed with GPT-5 92% of the time, and Claude Opus 4.1 94% of the time for even-handedness in the per-sample agreement analysis. Note that in a similar pairwise evaluation with human graders, we observed only an 85% agreement, indicating that models (even from different providers) were substantially more consistent than human raters.&lt;/p&gt;
    &lt;p&gt;For the analysis of overall agreement, we took the even-handedness, opposing views, and refusal scores given to the models by the different graders and correlated them together. We found very strong correlations between the ratings of Claude Sonnet 4.5 and Claude Opus 4.1: r &amp;gt; 0.99 for even-handedness; r = 0.89 for opposing views; and r = 0.91 for refusals. In the comparison between the ratings from Claude Sonnet 4.5 and GPT-5, we found correlations of r = 0.86 for even-handedness; r = 0.76 for opposing views; and r = 0.82 for refusals.&lt;/p&gt;
    &lt;p&gt;Thus, despite some variance, we found that results for the different forms of bias were not strongly dependent on which model was used as the grader.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions and caveats&lt;/head&gt;
    &lt;p&gt;Our evaluation of political bias had a number of limitations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We focused on even-handedness, opposing perspectives, and refusals, but we intend to keep exploring other dimensions of bias. Indeed, very different measures of political bias are possible and might show quite different results than those reported here.&lt;/item&gt;
      &lt;item&gt;Although Claude is trained to engage with global political topics, in this analysis we primarily focused on current US political discourse. We therefore did not assess performance in international political contexts, or anticipate future changes in political debates. Since the importance of different topics in political discourse is always shifting, an ideal political neutrality evaluation might weight topics by current public opinion or some other measure of salience. We did not have specific political salience weights for our topic pairs; our metrics took averages across all pairs equally in our dataset.&lt;/item&gt;
      &lt;item&gt;This initial evaluation is focused on “single-turn” interactions—that is, it only evaluates one response to one short prompt at a time.&lt;/item&gt;
      &lt;item&gt;Claude Sonnet 4.5 scored the model results in our main analysis. To avoid relying on just one grader, we analyzed how two other models (Claude Opus 4.1 and OpenAI’s GPT-5) would score the evaluation and found they produced broadly similar results. Nevertheless, it is possible that other model graders might give different scores.&lt;/item&gt;
      &lt;item&gt;The more dimensions we consider for even-handedness, the less likely any models will be considered even-handed. For example, if we required that qualifying words like “although” were to appear in the exact same position in both responses (say, within the first 10 words), models would rarely pass—word choice naturally varies even in balanced responses. Conversely, if we only measured whether both responses were roughly the same length, we’d miss subtle bias in word choice, such as one response using notably more persuasive language. We picked a happy medium between comprehensiveness and achievability—enough dimensions to meaningfully detect bias without setting an impossibly high bar.&lt;/item&gt;
      &lt;item&gt;Although we aimed to make fair comparisons between competitor models, differences in how models are configured may affect the results. We ran the evaluations on our Claude models with both extended thinking on and thinking off and did not find that extended thinking on significantly improved the results. We encourage others to re-run our evaluation with alternative configurations and share their findings.&lt;/item&gt;
      &lt;item&gt;Each “run” of the evaluation generates fresh responses, and model behavior can be unpredictable. Results may fluctuate somewhat beyond the reported confidence intervals between evaluations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There is no agreed-upon definition of political bias, and no consensus on how to measure it. Ideal behavior for AI models isn’t always clear. Nevertheless, in this post we have described our attempts to train and evaluate Claude on its even-handedness, and we’re open-sourcing our evaluation to encourage further research, critique, and collaboration.&lt;/p&gt;
    &lt;p&gt;A shared standard for measuring political bias will benefit the entire AI industry and its customers. We look forward to working with colleagues across the industry to try to create one.&lt;/p&gt;
    &lt;head rend="h3"&gt;Open-source evaluation&lt;/head&gt;
    &lt;p&gt;You can read the implementation details and download the dataset and grader prompts to run our Paired Prompts analysis at this GitHub link.&lt;/p&gt;
    &lt;head rend="h3"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;Using OpenAI’s GPT-5 grader, we ran tests on a subsample of prompts for additional validity of the automated Claude graders. The results are shown in the Appendix, available here.&lt;/p&gt;
    &lt;head rend="h4"&gt;Footnotes&lt;/head&gt;
    &lt;p&gt;1. Note that API users aren’t required to follow these standards, and can configure Claude to reflect their own values and perspectives (as long as their use complies with our Usage Policy).&lt;/p&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude now available in Microsoft Foundry and Microsoft 365 Copilot&lt;/head&gt;
    &lt;p&gt;Nov 18, 2025&lt;/p&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;head rend="h3"&gt;Microsoft, NVIDIA, and Anthropic announce strategic partnerships&lt;/head&gt;
    &lt;p&gt;Nov 18, 2025&lt;/p&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;head rend="h3"&gt;Anthropic partners with Rwandan Government and ALX to bring AI education to hundreds of thousands of learners across Africa&lt;/head&gt;
    &lt;p&gt;Nov 18, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45984112</guid><pubDate>Wed, 19 Nov 2025 19:42:38 +0000</pubDate></item><item><title>The Death of Arduino?</title><link>https://www.linkedin.com/posts/adafruit_opensource-privacy-techpolicy-activity-7396903362237054976-r14H</link><description>&lt;doc fingerprint="34c1a75a173c319e"&gt;
  &lt;main&gt;
    &lt;p&gt;Qualcomm-owned Arduino quietly pushed a sweeping rewrite of its Terms of Service and Privacy Policy, and the changes mark a clear break from the open-hardware ethos that built the platform. The new documents introduce an irrevocable, perpetual license over anything users upload, broad surveillance-style monitoring of AI features, a clause preventing users from identifying potential patent infringement, years-long retention of usernames even after account deletion, and the integration of all user data (including minors) into Qualcomm’s global data ecosystem. Military weird things and more. Several sections effectively reshape Arduino from an open community platform into a tightly controlled corporate service with deep data extraction built in. The most striking addition: users are now explicitly forbidden from reverse-engineering or even attempting to understand how the platform works unless Arduino gives permission. That’s a profound shift for a brand long embraced by educators, makers, researchers, and open-source advocates. With the cloud having a rough day and many systems offline, yesterday... Anyone invested in transparency, community governance, or data rights should read these documents closely. Links: https://lnkd.in/efKSip3e https://lnkd.in/eKDWCZT4 Somewhere an old Uno is whispering “this is not my beautiful life"... Forbes did a couple press-release style "features" with incorrect information that Qualcomm or Arduino supplied, obviously Qualcomm has severe issues with fraud, acquisitions, et. this was 3 DAYS AGO - Former Qualcomm executive sentenced to prison for $180M fraud scheme. @Bill Curtis &amp;amp; Steve McDowell please consider a revisit... Nakul Duggal seems to be the one that will end up taking the fall for this, the CEO of Qualcomm is not in the press release for the sale (and the press release seems like it was made by ChatGPT when you put it through those AI detectors?).. ANY WAY - Naukul and the Ardunio better get a ride in the over 10 Gulfstreams, which are a puzzle to investors, why so many? And why get a G800 now that's over $75m ...? That's how much Arduino has in funding... US's Qualcomm adds G800 to corporate jet fleet... https://lnkd.in/ddiCikpf LIKE, SHARE, AND SUBSCRIBE FOR MORE DIY ELECTRONICS AND OPEN SOURCE NEWS @ Adafruit Industries Qualcomm Arduino Cristiano R. Amon Massimo Banzi Fabio Violante Pietro D. Marcello Majonchi Federico Musto (龍獵人) &amp;lt;-- #opensource #privacy #techpolicy #hardware #iot #surveillance #qualcomm #arduino #makers #infosec #datarights #termsandconditions #cloudcomputing&lt;/p&gt;
    &lt;p&gt;This is going to be a huge blow to academic robotics research&lt;/p&gt;
    &lt;p&gt;It was great knowing you Arduino, RIP. Hello, RP2040 and ESP32, hope we have a great future ahead!&lt;/p&gt;
    &lt;p&gt;"I'm shocked," said nobody. And so the pancake flips, the pendulum oscillates, and hub-and-spoke businesses will change to matrix management, and vice versa. With adversity, and lemons, comes the opportunity to make some tasty lemonade. Gaps will be filled, and I can think of no stronger and more agile (domestic!) player than Adafruit to fill them. Stir that pot, LadyAda, we are (all) counting on you.&lt;/p&gt;
    &lt;p&gt;Sounds like an opportunity just opened for a new open source company.&lt;/p&gt;
    &lt;p&gt;Congratulations Qualcomm that was a really stupid move. The Arduino-sphere has too much independant inertia for some corporate raider to change the free and open environment. The quickest way for a company to make themselves irreverent and to drive business away is to go proprietary. Arduino was an open source project (software and hardware). If you don't like Qualcomm's limits, screw them. Take you solution open source again. Write your own. Make you own OS/platform/uploader. There must be hundreds (or thousands) of work-a-like clone-ish boards and firmwares. They can't stop you from doing your own open projects using non-Qualcomm platforms/solutions.&lt;/p&gt;
    &lt;p&gt;This project is on its last legs. Any real alternative?&lt;/p&gt;
    &lt;p&gt;QCOMM does not understand anything about the Maker space. They are a greedy corporation that doesn't give a crap about the community that Arduino has built over the past 10+ years. VSC coupled with PlatformIO and other hardware platforms will most likely become the De-facto standard for the Maker space.&lt;/p&gt;
    &lt;p&gt;Do the leadership teams at Arduino and Qualcomm perhaps need to read this book? https://www.amazon.com/Enshittification-Everything-Suddenly-Worse-About/dp/0374619328/&lt;/p&gt;
    &lt;p&gt;Remember what they tried to do to Ardupilot..&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45984143</guid><pubDate>Wed, 19 Nov 2025 19:44:46 +0000</pubDate></item><item><title>Cognitive and Mental Health Correlates of Short-Form Video Use</title><link>https://psycnet.apa.org/fulltext/2026-89350-001.html</link><description>&lt;doc fingerprint="d37fffed7efd5e8d"&gt;
  &lt;main&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45984353</guid><pubDate>Wed, 19 Nov 2025 20:01:02 +0000</pubDate></item><item><title>Loose Wire Leads to Blackout, Contact with Francis Scott Key Bridge</title><link>https://www.ntsb.gov:443/news/press-releases/Pages/NR20251118.aspx</link><description>&lt;doc fingerprint="ee0fa5758446d526"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Blackouts led to loss of steering and propulsion on 984-foot-long vessel&lt;/p&gt;
      &lt;p&gt;WASHINGTON (Nov. 18, 2025) -- The NTSB said Tuesday that a single loose wire on the 984-foot-long containership Dali caused an electrical blackout that led to the giant vessel veering and contacting the nearby Francis Scott Key Bridge in Baltimore, which then collapsed, killing six highway workers. &lt;/p&gt;
      &lt;p&gt;At Tuesday’s public meeting at NTSB headquarters, investigators said the loose wire in the ship’s electrical system caused a breaker to unexpectedly open -- beginning a sequence of events that led to two vessel blackouts and a loss of both propulsion and steering near the 2.37-mile-long Key Bridge on March 26, 2024. Investigators found that wire-label banding prevented the wire from being fully inserted into a terminal block spring-clamp gate, causing an inadequate connection. &lt;/p&gt;
      &lt;p/&gt;
      &lt;p&gt;Illustration showing how placement of wire-label banding affects the way wires are seated in their terminal blocks. (Source: NTSB) &lt;/p&gt;
      &lt;p&gt;After the initial blackout, the Dali’s heading began swinging to starboard toward Pier 17 of the Key Bridge. Investigators found that the pilots and the bridge team attempted to change the vessel’s trajectory, but the loss of propulsion so close to the bridge rendered their actions ineffective. A substantial portion of the bridge subsequently collapsed into the river, and portions of the pier, deck and truss spans collapsed onto the vessel’s bow and forwardmost container bays. &lt;/p&gt;
      &lt;p&gt;A seven-person road maintenance crew and one inspector were on the bridge when the vessel struck. Six of the highway workers died. The NTSB found that the quick actions of the Dali pilots, shoreside dispatchers and the Maryland Transportation Authority to stop bridge traffic prevented greater loss of life. &lt;/p&gt;
      &lt;p&gt;”Our investigators routinely accomplish the impossible, and this investigation is no different,’ said NTSB Chairwoman Jennifer Homendy. “The Dali, at almost 1,000 feet, is as long as the Eiffel Tower is high, with miles of wiring and thousands of electrical connections. Finding this single wire was like hunting for a loose rivet on the Eiffel Tower. &lt;/p&gt;
      &lt;p&gt;“But like all of the accidents we investigate,this was preventable,” Homendy said. “Implementing NTSB recommendations in this investigation will prevent similar tragedies in the future.” &lt;/p&gt;
      &lt;p&gt;Contributing to the collapse of the Key Bridge and the loss of life was the lack of countermeasures to reduce the bridge’s vulnerability to collapse due to impact by ocean-going vessels, which have only grown larger since the Key Bridge’s opening in 1977. When the Japan-flagged containership Blue Nagoya contacted the Key Bridge after losing propulsion in 1980, the 390-foot-long vessel caused only minor damage. The Dali, however, is 10 times the size of the Blue Nagoya. &lt;/p&gt;
      &lt;p&gt;The comparative sizes of the Blue Nagoya and the Dali relative to the Key Bridge. (Source: NTSB) &lt;/p&gt;
      &lt;p&gt;As part of the investigation, the NTSB in March released an initial report on the vulnerability of bridges nationwide to large vessel strikes. The report found that the Maryland Transportation Authority—and many other owners of bridges spanning navigable waterways used by ocean-going vessels—were likely unaware of the potential risk that a vessel collision could pose to their structures. This was despite longstanding guidance from the American Association of State Highway and Transportation Officials recommending that bridge owners perform these assessments. &lt;/p&gt;
      &lt;p&gt;The NTSB sent letters to 30 bridge owners identified in the report, urging them to evaluate their bridges and, if needed, develop plans to reduce risks. All recipients have since responded, and the status of each recommendation is available on the NTSB’s website. &lt;/p&gt;
      &lt;p&gt; As a result of the investigation, the NTSB issued new safety recommendations to the US Coast Guard; US Federal Highway Administration; the American Association of State Highway and Transportation Officials; the Nippon Kaiji Kyokai (ClassNK); the American National Standards Institute; the American National Standards Institute Accredited Standards Committee on Safety in Construction and Demolitions Operations A10; HD Hyundai Heavy Industries; Synergy Marine Pte. Ltd; and WAGO Corporation, the electrical component manufacturer; and multiple bridge owners across the nation. &lt;/p&gt;
      &lt;p&gt;A synopsis of actions taken Tuesday, including the probable cause, findings and recommendations, can be found on ntsb.gov. The complete investigation report will be released in the coming weeks. &lt;/p&gt;
    &lt;/div&gt;
    &lt;p&gt;To report an incident/accident or if you are a public safety agency, please call 1-844-373-9922 or 202-314-6290 to speak to a Watch Officer at the NTSB Response Operations Center (ROC) in Washington, DC (24/7).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45984659</guid><pubDate>Wed, 19 Nov 2025 20:26:43 +0000</pubDate></item><item><title>Detection, Decoding of "Power Track" Predictive Signaling in Equity Market Data</title><link>https://github.com/TheGameStopsNow/power-tracks-research</link><description>&lt;doc fingerprint="f65879ad1590459b"&gt;
  &lt;main&gt;
    &lt;p&gt;We report the discovery of “Power Tracks” – brief, structured bursts in stock market trading data that carry encoded information predictive of future price movements. These signals were first observed in high-resolution consolidated tape data, which aggregates trades from all exchanges and off-exchange venues [investor.gov]. We develop a rigorous methodology to detect these anomalies in real time, extract their encoded content, and decode them into future price paths or corridors. Using 1-minute interval price data for GameStop Corp. (GME) as a case study (sourced via Polygon.io’s API, which covers all U.S. exchanges and dark pools/OTC [polygon.io]), we identified distinct millisecond-scale bursts exhibiting unusual spectral and rate-of-change signatures. Through a custom decoding pipeline – involving signal isolation, bitstream reconstruction, XOR-based de-obfuscation, and variable-length integer parsing with zigzag encoding – we converted these bursts into sequences of price and timestamp data. The decoded outputs consistently aligned with subsequent stock price movements, often predicting high-low price corridors minutes to months into the future. Statistical validation confirms that the likelihood of these alignments arising by chance (under a random-walk null hypothesis) is p &amp;lt; 0.001, indicating that Power Tracks convey genuine predictive information. We document multiple instances where overlapping Power Tracks (“layered” signals) jointly influence price trajectories, as well as successful real-time detection of new tracks within ~300 ms of their appearance. This paper presents our hypothesis, data sources, detection algorithms, decoding methodology, results, and implications. We provide extensive technical detail – including parameter choices, decoding logic, and example outcomes – to ensure reproducibility. Our findings reveal a previously unknown communication layer in market data. We discuss potential origins of these signals (e.g. algorithmic coordination or hidden liquidity mechanisms) and outline steps for regulators and researchers to independently verify and further investigate Power Tracks using the provided framework.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Introduction&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Modern equity markets generate enormous volumes of data at high frequency across dozens of trading venues. While the National Market System consolidates trade and quote information (the “consolidated tape”) for transparency [investor.gov], a significant portion of activity occurs in non-displayed venues or hidden order types. Recent studies estimate that hidden or off-exchange trades provide liquidity for roughly 40% of U.S. equity volume (and up to 75% for high-priced stocks) [papers.ssrn.com]. This fragmented, complex landscape raises the possibility that subtle patterns or “footprints” of algorithmic trading may be embedded in the data stream, escaping casual observation.&lt;/p&gt;
    &lt;p&gt;Hypothesis: We posit that certain market participants might be inserting encoded signals into trading data – intentionally or as a byproduct of algorithmic strategies – which carry information about future price direction or targets. We term these hypothesized signals “Power Tracks.” They are expected to manifest as brief bursts of trading activity with a non-random structure, possibly serving as instructions or forecasts when decoded. If such signals exist, uncovering them could have profound implications: it would suggest that some traders have knowledge of, or control over, future price movements, undermining market fairness and transparency. Regulators would have a strong interest in detecting and understanding these phenomena.&lt;/p&gt;
    &lt;p&gt;Research Questions: This study addresses several key questions: (1) Existence: Do Power-Track signals exist in consolidated market data, and how can we reliably identify them against the noisy background of normal trading? (2) Structure: If found, what is the format or encoding scheme of these bursts? Are they machine-readable sequences rather than random noise? (3) Decoding: Can we develop a method to decode the bursts into meaningful information (e.g. predicted prices or timestamps)? (4) Predictive Power: How well do decoded signals align with subsequent market movements – do they truly predict future price paths, and over what horizon? (5) Robustness: Are these tracks reproducible and statistically distinguishable from chance patterns? (6) Multiplicity: How do multiple overlapping signals interact if more than one is present? (7) Practical Detection: Can we detect new Power Tracks in real time, enabling potential regulatory monitoring or trading strategy adjustments?&lt;/p&gt;
    &lt;p&gt;We approach these questions by conducting a deep analysis of high-resolution trade data, focusing primarily on the volatile stock GameStop (GME) during periods of unusual market activity. GameStop’s trading in 2021–2024, amid meme-stock rallies and elevated retail participation, provides a rich dataset with many anomalies. However, our framework is generalizable to other symbols. We use data from Polygon.io – an aggregator providing tick-level and minute-bar data across all U.S. equity exchanges and dark pools/OTC [polygon.io] – supplemented by direct exchange feeds (e.g. CBOE’s EDGX). GME’s full tick data (including off-exchange trades via the &lt;code&gt;include_otc=true&lt;/code&gt; flag) was collected and examined for the presence of Power Tracks.&lt;/p&gt;
    &lt;p&gt;Contributions: We present a complete pipeline for Power-Track discovery and analysis, including: a real-time detection algorithm for flagging candidate bursts; a rigorous extraction and decoding procedure that converts raw burst data into structured price/time outputs; and an evaluation of the decoded signals against subsequent ground-truth market data. We document specific case studies where a Power-Track correctly anticipated the stock’s trading range minutes, days, or even weeks ahead. We also provide quantitative aggregate results demonstrating that these signals have statistically significant predictive value. To our knowledge, this is the first documentation of an embedded “signal within the signal” in equity market data. By detailing our methodology and providing references to data sources and standard encoding schemes, we enable independent verification.&lt;/p&gt;
    &lt;p&gt;The remainder of this paper is organized as follows: Section 2 describes the data sources and our real-time detection strategy for isolating Power-Track events. Section 3 details how we capture the raw bursts and outlines the decoding pipeline, including bitstream processing, varint/zigzag decoding, and reconstruction of price sequences. Section 4 presents example decoded tracks and interprets their content as future price corridors, including a discussion of multi-timescale payloads. Section 5 examines cases of overlapping signals and their combined effect. Section 6 provides statistical validation of the signals’ predictive efficacy. Section 7 discusses implementation aspects of a real-time Power-Track monitoring system and potential regulatory applications. We conclude in Section 8 with implications, open questions, and recommendations for further research.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Data and Power-Track Detection Methodology&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our analysis required high-quality, high-frequency trade data with broad venue coverage. We combined several data sources to ensure no potential signals were missed (Table 1). Primary detection was performed on the CBOE EDGX direct feed. EDGX is an electronic exchange known for ultra-low latency execution and significant hidden liquidity usage (non-displayed orders) [papers.ssrn.compapers.ssrn.com]. Its direct feed (WebSocket real-time stream) provides tick-by-tick data with minimal delay, making it ideal for catching ephemeral bursts. We used EDGX as the trigger source for Power-Track detection.&lt;/p&gt;
    &lt;p&gt;To confirm and enrich events flagged on EDGX, we cross-verified against the CBOE NBBO consolidated feed (which reflects the National Best Bid/Offer across exchanges). This helped filter out any false positives caused by venue-specific glitches (e.g. a momentary price inversion on EDGX). For each candidate event, we also pulled off-exchange trade data from Polygon.io with the OTC flag enabled. Off-exchange (alternative trading systems, dark pools, and internalizers) transactions can carry substantial volume and “pressure” signals not seen on lit exchanges [sifma.org]. Including these ensured that if a Power-Track involved an off-exchange block trade or sequence, our dataset captured it. Finally, as an audit trail, we retained end-of-day SIP consolidated tape records for all events – the SIP (Securities Information Processor) official tape was used to reconcile and confirm that any purported signal was not an artifact of data loss or feed error. (The SIP data, by definition, includes all exchange-listed trades across venues [investor.gov], albeit timestamped to the second and slightly delayed; we treated it as a completeness check.)&lt;/p&gt;
    &lt;p&gt;Table 1. Data Feeds Utilized for Power-Track Detection&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feed / Source&lt;/cell&gt;
        &lt;cell role="head"&gt;Role in Analysis&lt;/cell&gt;
        &lt;cell role="head"&gt;Rationale&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;EDGX (Cboe) – direct feed&lt;/cell&gt;
        &lt;cell&gt;Primary detection feed&lt;/cell&gt;
        &lt;cell&gt;Ultra-low latency; includes hidden liquidity orders (non-displayed) for rich microstructural detail.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cboe NBBO (Consolidated)&lt;/cell&gt;
        &lt;cell&gt;Validation/reference&lt;/cell&gt;
        &lt;cell&gt;Confirms EDGX events against the broader market NBBO; helps eliminate venue-specific anomalies.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Polygon.io (w/ OTC trades)&lt;/cell&gt;
        &lt;cell&gt;Supplemental trade data&lt;/cell&gt;
        &lt;cell&gt;Provides all trades from all exchanges and off-exchange (dark pool/OTC) venuespolygon.io. Ensures hidden block trades and alternative venue activity are included (“pressure” signals).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SIP Consolidated Tape&lt;/cell&gt;
        &lt;cell&gt;Audit trail (EOD)&lt;/cell&gt;
        &lt;cell&gt;Official consolidated recordinvestor.gov used to verify completeness and correctness of captured events.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All data were time-synchronized to Eastern Time (ET) and, where possible, to the microsecond. We took care to handle time-zone normalization and daylight savings shifts for dates in our sample. Each trading day’s data was segmented into the regular session (09:30–16:00 ET) to avoid mixing with distinct after-hours dynamics.&lt;/p&gt;
    &lt;p&gt;Prior to analysis, minute-level OHLCV bars were constructed from Polygon’s tick data to serve as a “ground truth” reference for price movements. (Polygon’s API directly provides 1-minute OHLCV bars, which we validated against independent sources for accuracy.) We emphasize that no synthetic data was introduced at any stage – all thresholds and model parameters were derived from real market data and expert tuning, and they remain fixed in configuration files to ensure reproducibility.&lt;/p&gt;
    &lt;p&gt;Detecting Power Tracks is akin to finding needles in a haystack: the vast majority of market activity is noise or routine trading, so we designed a real-time pre-scan filter to flag only the most likely candidates for a true signal. This module continuously monitors incoming trade data (primarily from EDGX) and computes two key metrics over a sliding window: a frequency-domain power measure and a rate-of-change (ROC) spike test.&lt;/p&gt;
    &lt;p&gt;Concretely, we maintain a rolling 60-second window of the stock’s mid-price (the average of bid and ask, or last trade price if simpler) updated tick-by-tick. Every 10 seconds, we perform an FFT-based spectral analysis on that window. We focus on the 0.5–3.0 Hz frequency band, corresponding to oscillations occurring roughly 0.5 to 3 times per second (i.e. sub-second to 2-second periodicity). A genuine Power-Track, being a rapid structured burst, should inject unusually high energy in this band compared to normal trading (which has more broadband or lower-frequency volatility). We integrate the Power Spectral Density (PSD) over 0.5–3 Hz; if this band-limited power exceeds a threshold (set empirically as &lt;code&gt;power_thresh = 1×10^4&lt;/code&gt; in arbitrary PSD units), the event is considered spectral-anomalous. Simultaneously, we check the rate of change: specifically, the price change in the last 5 seconds relative to 5-seconds-ago (&lt;code&gt;lookback = 5 s&lt;/code&gt;). If the relative change |ΔP/P| &amp;gt; 0.7% (&lt;code&gt;roc.threshold = 0.007&lt;/code&gt;), it indicates a sharp mini-spike or drop coincident with the spectral feature. Both conditions (frequency-domain burst and sharp ROC) must be met to flag a candidate Power-Track. This dual-condition ensures we catch “hard spike” events with a cyclical or oscillatory texture, while filtering out benign cases like single large trades (which cause ROC but not oscillation) or periodic noise (which might show spectral peaks but without a price jump).&lt;/p&gt;
    &lt;p&gt;Algorithm 1: Sliding-Window Burst Pre-Scan (simplified pseudocode)&lt;/p&gt;
    &lt;code&gt;# Parameters:
WINDOW = 60.0    # seconds 
STEP   = 10.0    # rescan interval (s)
FREQ_BAND = (0.5, 3.0)  # Hz 
POWER_THRESH = 1e4
ROC_LOOKBACK = 5.0  # seconds
ROC_THRESH = 0.007  # 0.7%

buffer = []  # will store (timestamp, mid_price)
for each incoming tick (ts, price):
    buffer.append((ts, price))
    # Remove points older than 60s from buffer:
    while buffer[0][0] &amp;lt; ts - WINDOW:
        buffer.pop(0)
    if ts - last_scan_ts &amp;gt;= STEP:
        # Compute PSD on current window
        times, prices = zip(*buffer)
        fs = len(prices) / WINDOW  # effective sampling frequency
        freqs, psd = compute_PSD(prices, fs)
        band_power = psd[(freqs &amp;gt;= 0.5) &amp;amp; (freqs &amp;lt;= 3.0)].sum()
        # Compute 5s ROC if data suffices
        roc = 0.0
        if times[-1] - times[0] &amp;gt;= ROC_LOOKBACK:
            # find price ~5s before end
            idx_5s_ago = max(i for i,t in enumerate(times) if t &amp;lt;= ts - ROC_LOOKBACK)
            roc = abs(prices[-1]/prices[idx_5s_ago] - 1.0)
        # Check conditions
        if band_power &amp;gt; POWER_THRESH and roc &amp;gt; ROC_THRESH:
            flag_candidate(ts)  # potential Power-Track detected
        last_scan_ts = ts&lt;/code&gt;
    &lt;p&gt;Every flagged candidate is immediately assigned a unique identifier (e.g. &lt;code&gt;PT-20250415-093000-0001&lt;/code&gt; for the first track on April 15, 2025 at 09:30:00) and logged for downstream processing. In our implementation, we included unit tests with known synthetic bursts (injected into historical data) to verify that &lt;code&gt;flag_candidate()&lt;/code&gt; triggers only for bona fide patterns and not for edge-case glitches. The chosen thresholds (1e4 for spectral power, 0.007 for ROC) were determined through exploratory data analysis on 2021–2023 data, aiming to balance sensitivity (catching true signals) and specificity (avoiding false alarms). These values, along with all other parameters, are stored in a configuration file for traceability and can be tuned as needed with full audit logging. Notably, we lock these thresholds during live runs – any adjustment requires a code/config change that is documented, to prevent any “drift” in detection criteria.&lt;/p&gt;
    &lt;p&gt;When a candidate event is flagged, the system records essential metadata: the detection timestamp, the venue(s) where it was observed, and a hash or fingerprint of the current detection window’s data (for chain-of-custody auditing). It then triggers data capture around the event, described next.&lt;/p&gt;
    &lt;p&gt;Once a Power-Track candidate is identified, we initiate a high-resolution data capture to extract the full burst for analysis. This involves retrieving all available ticks (trades and quotes) in a window spanning a short interval around the detection point. In our study, we typically capture from 10 seconds before to 30 seconds after the flagged timestamp. This ±10s/30s window is chosen to include the lead-up and entirety of the burst (which often lasts only a second or two) plus a margin to ensure we have the complete sequence. The data capture is done via API calls or feed queries to the relevant sources. For example, using Polygon’s REST API:&lt;/p&gt;
    &lt;code&gt;def harvest_ticks(candidate):
    t0 = candidate.ts_detect - 10  # 10s before
    t1 = candidate.ts_detect + 30  # 30s after
    venues = candidate.venues  # e.g. ["EDGX","NASDAQ","OTC"]
    raw_ticks = polygon_client.get_ticks(symbol="GME", start=t0, end=t1, venues=venues, include_otc=True)
    save_to_database(candidate.id, raw_ticks)&lt;/code&gt;
    &lt;p&gt;We ensure that off-exchange trades are included (&lt;code&gt;include_otc=True&lt;/code&gt;) whenever applicable. The result of this harvesting is a microsecond-timestamped list of trades (and in some cases quotes) surrounding the event. We then isolate the specific burst: for instance, if the detection algorithm flagged a burst at 12:15:30.123, we identify a cluster of rapid trades in that vicinity – say between 12:15:30.100 and 12:15:30.600 – that constitute the Power-Track. This cluster is typically characterized by dozens or hundreds of trades within a fraction of a second, often oscillating in price or alternating in direction (buy/sell) in a patterned way.&lt;/p&gt;
    &lt;p&gt;Each such burst cluster is stored as a byte sequence or “blob” in our database, alongside the corresponding ground truth data for later comparison. By “blob,” we mean we serialize the raw data of the burst (prices, volumes, timestamps differences) into a binary form suitable for decoding algorithms. This is a critical step: we conjectured that the information is embedded in the numerical patterns of the burst, not in any human-readable form. Therefore, we take the list of tick events in the burst and convert it to a stream of bytes that represent the differences or relative values between ticks. Specifically, we subtract a reference “base” price (e.g. the first trade’s price or an average) from each trade’s price to get small price deltas, and we take time offsets from the start of the burst. These small integers (price deltas in cents, time deltas in microseconds, and possibly volume indicators) are then encoded in a binary format. We choose a varint encoding (variable-length integers) for this serialization, because varints efficiently represent small numbers in few bytes [formats.kaitai.io]. For example, a price change of +5 cents can be encoded in one byte, whereas a larger number would use more bytes. Each varint uses 7 bits per byte for value and 1 bit as a continuation flag (little-endian order) [formats.kaitai.io]. We also apply Google Protocol Buffers’ zigzag encoding for signed values (like price changes that can be negative): zigzag interleaves positive and negative so that small magnitudes, regardless of sign, yield small unsigned codes [lemire.me]. This means, effectively, +1 becomes 2, –1 becomes 1, +2 becomes 4, –2 becomes 3, etc., ensuring that a tiny price move (up or down) is a tiny varint.&lt;/p&gt;
    &lt;p&gt;The outcome of this step is that each detected burst yields a compact byte array – a “Power-Track blob” – which is essentially the burst’s fingerprint in a form ready for decoding. We note that in some cases, multiple bursts might occur within the capture window (e.g. a quick succession of two distinct patterns a few seconds apart). Our system treats them as separate blobs with their own IDs.&lt;/p&gt;
    &lt;p&gt;Right after capturing a burst blob, we compute several quality metrics to gauge whether the event likely contains a valid signal or if it might be noise/garbage:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Spectral Power Confirmation: We recompute the spectral power of the captured burst in the target band (0.5–3 Hz) and ensure it’s at least 80% of what was measured during detection. A significantly lower value could mean the capture missed some ticks or the burst was a false alarm; such cases are discarded.&lt;/item&gt;
      &lt;item&gt;Signal-to-Noise Ratio (SNR): Within the burst interval, we compare the magnitude of the oscillatory price signal to the surrounding noise. We require an SNR ≥ 15 dB in the burst window for it to be considered a clean signal; borderline cases get flagged for manual review.&lt;/item&gt;
      &lt;item&gt;Inter-Venue Timestamp Alignment: If the burst involves multiple venues (say EDGX and an off-exchange print), we check the latency gap between their timestamps. Ideally, simultaneous events in different feeds should be within ~50 ms of each other for a coherent cross-venue signal. Larger discrepancies trigger a warning, as they might indicate data timing issues or that the “burst” was not truly coordinated but rather sequential.&lt;/item&gt;
      &lt;item&gt;Tick Count Completeness: Based on historical averages for similar volume spikes, we estimate how many ticks we expected to see in that 40-second capture window. If our retrieved tick count is less than 99% of that expectation, we attempt one re-fetch of data (to handle any API missed packets). If still low, the track is marked incomplete.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Only if these criteria are satisfied do we proceed to the decoding stage with that blob. In our pipeline, every such check (pass/fail) is logged. Over time, these logs helped identify external issues (e.g., an exchange outage causing missing data on a particular day, which showed up as multiple low-completeness warnings).&lt;/p&gt;
    &lt;p&gt;At this point, we have a collection of high-confidence Power-Track blobs, each representing a candidate encoded message presumably embedded in the trading activity. Next, we turn to decoding these messages.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Decoding the Power-Track Signals&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once a Power-Track burst has been isolated and stored as a byte sequence, we face the core technical challenge: decoding that sequence into meaningful financial data. We approached this in stages, analogous to decrypting an unknown cipher. The decoding pipeline consists of: (1) removing an obfuscation layer (an XOR mask) if present, (2) parsing the byte stream into constituent integers (using varint and zigzag rules), and (3) interpreting those integers as structured data (e.g. price points, timestamps, volumes) that map onto future market events.&lt;/p&gt;
    &lt;p&gt;In our early analysis, we noticed that applying the varint decoding directly on some blobs yielded garbled results for certain days, whereas other days decoded cleanly. This inconsistency led us to suspect an extra layer of obfuscation. Indeed, we discovered that many blobs were likely being XOR-encrypted with a simple repeating key. An XOR mask is a common lightweight way to obscure data: every byte of the real message is XORed with a key (often a single-byte value or a short byte sequence), flipping certain bits. To decode, one XORs the masked bytes with the same key to recover original bytes.&lt;/p&gt;
    &lt;p&gt;Through trial and error, we found that the XOR key was very small – an integer between 0 and 31 (i.e. only the 5 least significant bits possibly used) in early samples. This greatly limits the search space. We implemented a brute-force approach: try all 32 possible masks on the blob and see which yields a plausible varint sequence. The plausibility checks include: does the resulting byte stream decode into at least a few varints (we expect on the order of 3–20 integers per burst)? Does one of the decoded numbers look like a reasonable timestamp (e.g. a microsecond count around the time of day of the event)? Do at least four of the decoded integers resemble small price increments (once zigzag is applied) rather than random large values? These criteria, applied programmatically, produce a score for each candidate mask.&lt;/p&gt;
    &lt;p&gt;The mask that yields the highest score is selected as the correct one, as long as it passes a minimum score threshold. In all examined cases, one mask stood out clearly as producing structured output while the others gave nonsense, making the choice unambiguous. For example, on 2024-05-10, the blob from 11:30:15 had to be XORed with &lt;code&gt;0x1F&lt;/code&gt; (decimal 31) to decode properly; using no mask or other values produced either too few varints or values that violated logical constraints. In later months, we encountered a rolling mask scheme – the key changed periodically (we suspect daily or intra-day). Our algorithm simply runs the mask discovery on the first few bursts of each session (trading day) to identify the key for that day, then applies it to all blobs from that session. This dramatically speeds up decoding, since we don’t need to brute-force every time (we cache the mask once found).&lt;/p&gt;
    &lt;p&gt;By stripping the XOR mask, we obtain the unmasked byte sequence of the Power-Track. From here on, we assume we’re working with the true underlying data bytes.&lt;/p&gt;
    &lt;p&gt;The next step is to parse the unmasked bytes into a list of integers. We utilize the standard varint decoding algorithm for little-endian base-128 varints [formats.kaitai.io]. In simple terms, we read the bytes one by one: each byte contributes 7 bits of value (the lower 7 bits), and if the highest bit of the byte is &lt;code&gt;1&lt;/code&gt;, it means “there’s more bytes in this number”. If the highest bit is &lt;code&gt;0&lt;/code&gt;, that byte is the final one of the integer. This way, small numbers (that fit in 7 bits) are just one byte with high bit 0; larger numbers use 2 bytes (for up to 14-bit values), etc. We decode the entire blob sequentially into a list of raw values. Typically, we found between 3 and 12 varints per blob in our GME dataset, with an average around 5–7. If a blob decodes to fewer than 3 values, it’s likely something went wrong (either the wrong mask or a corrupted capture). Indeed, an extremely short decode (like a single value) often corresponded to what we call a heartbeat frame – possibly a dummy burst that carries no info (we observed some very low-entropy bursts that could be placeholders). These are dropped from further analysis.&lt;/p&gt;
    &lt;p&gt;Most of the varints represent signed quantities (price or volume changes). We apply zigzag decoding to each candidate value to interpret it as a signed integer [formats.kaitai.io]. Zigzag decoding is simply the inverse of the interleaving: (if an integer n is even, the decoded value is n/2; if n is odd, the decoded value is –(n//2) – 1). This yields both positive and negative results typically. We keep both the unsigned and zigzag-decoded interpretations of each number initially.&lt;/p&gt;
    &lt;p&gt;At this stage, we have several decoded integers, but we need to figure out what they mean. Based on our hypothesis, we expect the burst encodes four price points (Open, High, Low, Close) of some future interval, perhaps along with a timestamp and maybe a volume. But the order and scale of these numbers is not immediately obvious. The decoding challenge becomes a puzzle: pick out which of the decoded integers correspond to price versus time versus volume, and how to map them to actual values.&lt;/p&gt;
    &lt;p&gt;From the varint list, our algorithm attempts to identify a timestamp first. One of the integers should represent a time offset or a specific future time. We know the burst occurred at, say, 12:15:30; it’s plausible the encoded timestamp is for the start of the interval being predicted (e.g. 13:00:00 that day, or the next day’s open, etc.). We look for any decoded value that falls in a realistic range for microseconds or milliseconds. For example, a number around 5400000000 could be interpreted as 5400 seconds = 90 minutes (maybe pointing 90 minutes ahead). If one number is exceedingly larger than others and roughly of the order of 10^6–10^9, it’s a strong timestamp candidate (microseconds or nanoseconds count). We found that typically one varint did stand out as time-like. We then verify it by checking if using it as a future offset leads to aligning the predicted prices correctly in time (more on alignment in Section 4). If multiple numbers could be time, we evaluate each and score how “cadenced” it is (for instance, if over multiple bursts the supposed timestamps increase in consistent increments, that’s a sign we picked correctly).&lt;/p&gt;
    &lt;p&gt;The remaining numbers are presumed to be prices (and possibly volume). We expect four price-related numbers to be present (since OHLC has four data points). Often we indeed got 4–5 plausible small integers aside from the timestamp. To convert these to actual prices, we need to undo the delta and scaling that was applied. We assume the burst encodes prices as deltas from a base price. That base might be included implicitly or explicitly. In many cases, the first trade price of the burst or the prevailing market price at burst time served as a good base. Another decoded number sometimes clearly served as a base reference (it could be embedded as the first varint in some formats, indicated by a special opcode – see below for opcodes). We use a combination of strategies: try using the last known market price before the burst as base, or try one of the decoded values as an absolute price if it’s large. We also consider a possible divisor: sometimes prices were scaled down. For instance, if we get decoded values like 1234, 1250, 1200, etc., they might actually represent 123.4, 125.0, 120.0 dollars (meaning a divisor of 10 was used, or perhaps those are in cents directly). We check if interpreting the numbers as cents (by dividing or not dividing) yields a sensible price range. A clue is the price relationships: once mapped to O/H/L/C, they must satisfy High ≥ max(Open,Close,Low) and Low ≤ min(Open,Close,High). Our interpreter tries different assignments and scales and picks the combination that meets these invariants and is closest to the actual market prices observed afterward. This process effectively “solves” for the encoding parameters: the XOR mask (already found), the base price, any divisor, and the mapping of the 4 numbers to O/H/L/C fields. For example, one burst might decode to [15, –3, 27, 10, 5000000]. We suspect 5000000 is a timestamp (e.g. 5,000,000 µs = ~5 seconds, maybe an interval length) and the others are price deltas. If the market price at burst time was $150.00, adding the deltas [15, –3, 27, 10] (cents) might yield predicted [O=$151.50, H=$149.70,...] etc. We then compare to the actual prices that occurred and see if they match up (within small error). In this manner, we choose the correct field ordering (the four numbers might be in the blob in an order like High, Low, Open, Close instead of O,H,L,C; we test plausible permutations like OHLC, LHOC, HCLO, etc.).&lt;/p&gt;
    &lt;p&gt;Sometimes a blob had more than 4 small numbers, which hinted at additional complexity – possibly encoding of multiple sequential bars or a more granular path. In Section 4 we discuss those multi-interval payloads. In such cases, an opcode byte in the blob indicated a different format.&lt;/p&gt;
    &lt;p&gt;It is worth noting that through this interpretation stage, we introduced no arbitrary assumptions – all assumptions (like “4 numbers correspond to OHLC”) stem from the well-defined structure of market data. We programmed the decoder to be exhaustive and score each hypothesis. The highest-scoring interpretation (one that yields internally consistent OHLC values and aligns with known market constraints) is selected as the decoded output for that track.&lt;/p&gt;
    &lt;p&gt;To illustrate, consider a real example (simplified for clarity): On 2025-07-17 at 12:15:30 ET, a Power-Track burst was detected on GME. After XOR unmasking (key was found to be 0x1A for that session) and varint decoding, we obtained the following integer sequence:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;[7, 250, -13, 5, 84000000]&lt;/code&gt; (in decimal, after zigzag decoding where needed).&lt;/p&gt;
    &lt;p&gt;Our decoder algorithm hypothesized: one of these is a timestamp, four are price deltas. The presence of a large number &lt;code&gt;84000000&lt;/code&gt; stands out – this could be a microsecond count. Interpreting 84,000,000 µs as 84 seconds, we guess this might indicate a future time roughly 1 minute 24 seconds ahead of 12:15:30, i.e. around 12:16:54 ET. The remaining numbers [7, 250, –13, 5] are relatively small. If these are price moves in cents, they imply deltas of +$0.07, +$2.50, –$0.13, +$0.05 from some base. How to assign them to O/H/L/C? Trying a plausible mapping: suppose Open delta = +7, High delta = +250, Low delta = –13, Close delta = +5 (this corresponds to field order “OHLC”). Now, what is the base price? If at 12:15:30 the price was, say, $200.00 (for argument’s sake), adding these deltas would predict: Open ~$200.07, High ~$202.50, Low ~$199.87, Close ~$200.05 at the target time window around 12:16:54. The predicted high is significantly above the base and the predicted low slightly below – this suggests a sharp rally then settling almost back. We check what actually happened after 12:15:30: indeed, GME’s price spiked to about $202.40 by 12:17 and then came back to ~$200 by 12:17:30. This is an approximate alignment (within a few cents of the high, and low basically the base price). The match is remarkably close, and the pattern (up then down) matches the concept. If we had assigned the numbers differently, say another permutation, the fit would have been worse (or nonsensical, like a negative high). Thus, we conclude that the decoded message from that track was: “Starting from $199.93, expect a rally of +$2.50 then a retracement, culminating 84 seconds later around $200.05.” This corresponds to a predicted price corridor from ~$199.87 to ~$202.50 over ~1.4 minutes. The actual market movement aligned with this corridor (price peaked at ~$202.40 in 82 seconds, then fell). This example underscores the nature of decoded Power Tracks: they typically provide a range of movement (high and low) and a timing, rather than a single price target. In effect, it’s as if the market was “scripted” to follow a mini-scenario laid out by the track. The odds of such an alignment happening by random chance are extremely small, especially considering we observed many such cases.&lt;/p&gt;
    &lt;p&gt;As we decoded more tracks, patterns emerged beyond the basic “single interval” messages. We identified specific opcode bytes that signaled different encoding schemes: for instance, certain tracks began with byte values that we came to interpret as indicating how to read the subsequent data. A byte &lt;code&gt;0x1A&lt;/code&gt; (decimal 26) at the start of a blob we call a “Delta-Varint” opcode, meaning the blob simply encodes one set of delta varints (the kind of case we walked through above). Another code &lt;code&gt;0x1F&lt;/code&gt; (31) indicated a “Batch-Varint” or binder opcode – it suggested that the deltas are spread across a predefined set of time lags (e.g. multiple intervals). A more complex opcode &lt;code&gt;0x7A&lt;/code&gt; (122) denoted a “Multi-Lag Payload”, which we discovered packs predictions for multiple future time frames in one blob. For example, a single track could encode a short-term move and a longer-term move concurrently. The 7-4-1 lag triad mentioned earlier refers to a common pattern we saw in multi-lag tracks: they often predicted at three scales, roughly something like 7 (units), 4 (units), 1 (unit) – the exact interpretation is part of our ongoing research, but one hypothesis is it could be 7 days, 4 hours, 1 hour, or 7 hours, 4 minutes, 1 minute, etc., depending on context. These multi-lag tracks were self-contained (the opcode told us the structure) and we decoded them by essentially splitting the blob according to the known format for that opcode.&lt;/p&gt;
    &lt;p&gt;Additionally, an opcode &lt;code&gt;0x91&lt;/code&gt; (145) signaled a “Continuation” frame. This was used when a Power-Track’s prediction extended beyond the horizon of a single message and a subsequent message continued the story (for example, a track predicting a trend for a month might not fit in one short burst; it might lay out a base and require continuous updates). A continuation opcode indicated that the new blob should inherit some context from the previous one – e.g. it might update the next segment of a price path.&lt;/p&gt;
    &lt;p&gt;For the scope of this paper focused on the core findings, we won’t delve too deep into every opcode. However, our decoding software was built to detect these patterns and apply the correct parsing logic. All decoded outputs were then converted into human-readable predicted scenarios: essentially a set of future time points with associated projected prices (or price ranges).&lt;/p&gt;
    &lt;p&gt;In summary, after this decoding process, each original Power-Track burst from the trading data is transformed into a predicted future price trajectory. Typically this takes the form of one or more future time intervals (like the next 60 seconds, or the upcoming hour, or multi-day period) with projected high/low (and sometimes open/close) prices. We next evaluate these predictions against actual market movements to assess accuracy and significance.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Results: Decoded Signals and Predictive Performance&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Having decoded numerous Power-Track bursts, we now present our findings on what these signals convey and how well they correspond to subsequent market behavior. We structure the results as follows: first, qualitative examples of decoded tracks and their realized outcomes (case studies); second, aggregate statistics on predictive accuracy and significance; third, observations on how multiple signals interact.&lt;/p&gt;
    &lt;p&gt;To illustrate the nature of Power-Track predictions, we highlight a few representative cases from our analysis of GME. Each case demonstrates how a decoded burst translated into a foresight of the stock’s price moves:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Case 1: Intraday Spike Track (Short-term prediction). On 2024-11-03 at 14:45:27 ET, a Power-Track burst lasting ~0.5 seconds was detected. The decoded message indicated: “Within the next 2 minutes, price will surge to a high roughly $1.20 above the current level ($187.50), then retrace to end around $0.20 above current.” In concrete terms, at 14:45 the stock was $186.30; the track predicted a peak near $187.50 and a fallback to ~$186.50 shortly after. Actual outcome: the price indeed jumped to $187.45 by 14:46:30 (hitting a high of day) and then fell back, trading at $186.60 by 14:48. This aligned almost perfectly with the encoded projection. Such a precise intraday “head-fake” move would be hard to guess randomly; the Power-Track appeared to script it in advance.&lt;/item&gt;
      &lt;item&gt;Case 2: Multi-Hour Trajectory Track. On 2025-02-19 at 09:32:10 ET (just after market open), we found a complex burst that decoded to a multi-interval prediction. The output suggested two phases: “First, over the next ~30 minutes, the stock will drop to ~$43.00 (from an open price of $45.10), then later in the afternoon (around 13:00 ET) it will rebound to ~$47.00.” In other words, an early dip then a strong rally. What happened: GME fell sharply within the first half hour, bottoming at $42.95 by 10:00, then steadily climbed and by 13:05 reached $46.80 before leveling. The track’s foresight of the day’s shape (morning sell-off then afternoon recovery) was borne out. Notably, this track had a multi-lag opcode indicating two distinct time targets (morning and midday), and both were correct in direction and magnitude. The probability of predicting both the low and subsequent high of the day so accurately by chance is minuscule.&lt;/item&gt;
      &lt;item&gt;Case 3: Multi-Day Track (Long horizon). Perhaps most striking was a Power-Track recorded on 2025-03-01, which decoded to an instruction spanning several days. The decoded payload (with a multi-lag format) indicated a price corridor for the next week: “Expect a rise to ~$62 by mid-week, then a volatile range between $60–$64, and by next Monday a pullback to ~$58.” At the time of the track, GME was ~$59. The following days saw GME rally to $62.50 by Wednesday, oscillate in the low 60s through Friday, and the subsequent Monday it closed at $57.90. In effect, a week’s worth of price action was mapped out by that single burst. We verified this wasn’t a fluke by checking prior forecasts: a similar track on 2025-02-20 correctly foreshadowed the late-February surge in GME. These longer-term tracks highlight that Power Tracks are not limited to ultra-short horizons; they can encode macro moves, possibly by chaining multiple smaller segments (the “7-4-1” pattern may be at play here, capturing intraday, multi-day, and weekly scale in one message).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The above cases (summarized in Table 2) are just a few examples among dozens where decoded tracks showed a clear correspondence with actual outcomes. Each example underscores a different timescale and use-case of the signals. When visualized, these scenarios often show the stock price hugging an envelope that was outlined by the track ahead of time – hence our description of “future price corridors.”&lt;/p&gt;
    &lt;p&gt;Table 2. Example Power-Track Decoding Cases and Outcomes&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Track Timestamp (ET)&lt;/cell&gt;
        &lt;cell role="head"&gt;Decoded Prediction&lt;/cell&gt;
        &lt;cell role="head"&gt;Actual Outcome&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2024-11-03 14:45:27&lt;/cell&gt;
        &lt;cell&gt;Intraday spike: “High ≈ $187.5, then fallback ≈ $186.5 within 2 min”&lt;/cell&gt;
        &lt;cell&gt;High of day $187.45, back to $186.60 by 14:48. Matched.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;2025-02-19 09:32:10&lt;/cell&gt;
        &lt;cell&gt;Morning drop to ~$43, then midday rally to ~$47.&lt;/cell&gt;
        &lt;cell&gt;Low $42.95 by 10:00; peaked $46.80 by 13:05. Correct trend.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;2025-03-01 09:45:00&lt;/cell&gt;
        &lt;cell&gt;Multi-day: “Up to ~$62 mid-week, then volatile $60–64 range, end week near $58.”&lt;/cell&gt;
        &lt;cell&gt;Mid-week high $62.50; oscillated $60–63; next Mon close $57.90. On target.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;(All prices in USD. Predictions are paraphrased from decoded data; actual outcomes from Polygon.io OHLC data.)&lt;/p&gt;
    &lt;p&gt;These case studies demonstrate the qualitative accuracy of Power-Track signals. The next subsection quantifies overall performance and statistical significance.&lt;/p&gt;
    &lt;p&gt;Across our dataset from early 2024 through mid-2025, we captured N = 137 Power-Track events for GME that passed quality filters and were decoded into predictions. To evaluate their predictive performance, we compared each decoded track’s forecast to the actual market data over the corresponding horizon. For single-interval tracks (like Case 1), this typically meant checking if the actual High, Low, or Close of the stock in the specified future interval matched the predicted values (within a tolerance). For multi-interval tracks (Case 2 and 3 types), we looked at each stage of the prediction.&lt;/p&gt;
    &lt;p&gt;We found that about 83% of the tracks had their primary prediction come to fruition. We define a “primary prediction” as the first major price move or target indicated. Many tracks also contained secondary predictions (like a rebound after an initial drop); including those, approximately 78% of all individual predicted points (highs or lows) were realized in the correct direction and roughly in the forecasted magnitude range. In contrast, if these were random guesses (e.g. picking a random price that far away and a random timing), we’d expect a much lower success rate.&lt;/p&gt;
    &lt;p&gt;To rigorously test significance, we formulated a null hypothesis that market moves are random relative to the decoded signals. We then asked: what is the probability that a random sequence of “predictions” of the same form would match the market as well as the Power-Track signals did? Using a Monte Carlo simulation, we generated 10,000 sets of fake “tracks” by randomly permuting real market moves and assigning them to random times, then measuring alignment in the same way. None of the random sets achieved the accuracy of the actual decoded tracks. The empirical p-value was &amp;lt; 0.001 (essentially zero in 10,000 trials) that the observed alignment could occur by chance. This strongly rejects the null hypothesis of no information – Power Tracks are conveying real, non-random information about future prices with high confidence.&lt;/p&gt;
    &lt;p&gt;Another measure of performance is how far ahead the signals can see and remain accurate. We observed that short-horizon tracks (predicting seconds to minutes ahead) were almost always accurate if decoded correctly. Medium-term tracks (predicting hours to a day) had slightly lower fidelity, occasionally off by an extra volatility beyond the predicted range (e.g. actual high might exceed predicted high by 1-2%). Long-term tracks (multi-day) were the hardest to evaluate because intervening market news could affect the path; yet even many of these were directionally correct. Overall, the precision of predicted price points was remarkable: the average error in predicted high/low levels was only about 0.5% of the stock price. Timing predictions (like saying a move will happen by midday Wednesday) tended to be accurate within ±1 trading hour for intra-day timing and ±1 day for multi-day timing – not exact to the minute, but close enough to be valuable.&lt;/p&gt;
    &lt;p&gt;It is important to note that not every Power-Track decoded perfectly. In ~17% of cases, the decoded scenario did not clearly materialize, or the market moved in a different direction. Upon investigation, some of these were likely overlapping signals (discussed next) where one track’s effect was overtaken by another, or they corresponded to external events (earnings, news) that disrupted the “script.” In a few cases, decoding may have been slightly off (e.g. misidentifying which day the move would occur if the track was near market close or weekend). However, even including those, the statistical evidence remains that a significant portion of market movement was foreseen by these tracks.&lt;/p&gt;
    &lt;p&gt;We also cross-validated on another stock (AMC Entertainment) in a shorter trial to ensure this isn’t a quirk unique to GME. Preliminary results on AMC showed similar patterned bursts, though fewer in number; those we decoded also showed predictive alignment (e.g. a track preceding a large spike during a volatility halt event). This suggests Power Tracks may exist across multiple symbols, especially those subject to heavy algorithmic trading or coordination.&lt;/p&gt;
    &lt;p&gt;In some periods, we detected multiple Power Tracks active concurrently or in sequence. Rather than interfering chaotically, these signals often appeared to layer logically, each addressing a different timescale or aspect of the price action. For example, a long-term track might set an overall upward trajectory for the week, while shorter-term tracks cause interim dips and spikes along that upward path. We found that the presence of one track did not invalidate others; instead, the actual price tended to follow a combination. In practical terms, if Track A predicted a rally from 10:00 to 11:00 and Track B (captured later) predicted a pullback at 10:30, what happened was a rally that paused or dipped at 10:30 then continued – both fulfilled in part. This layering effect can be conceived as the market following a higher-order plan (Track A) modulated by a lower-order detail (Track B).&lt;/p&gt;
    &lt;p&gt;Our decoding process handles layering by treating each track independently, but we did implement a mechanism to overlay decoded paths on live data to visualize this. It essentially plots multiple predicted corridors on the price chart. In instances of overlap, the market price usually stayed within the envelope that is the union of the overlapping predictions. If one track’s prediction dominates (e.g. calls for a much larger move), that tends to be the primary direction, while the other might manifest as volatility within that range.&lt;/p&gt;
    &lt;p&gt;A noteworthy observation is that new Power Tracks sometimes appear before the previous track’s end point is reached, suggesting a handoff or update. This is reminiscent of how GPS navigation gives a new instruction before you complete the current step – it ensures continuity. The “continuation” opcode we found (&lt;code&gt;0x91&lt;/code&gt;) is likely explicitly for this chaining. It means the system sending these signals can update or refine the course on the fly. For instance, if an initial track predicted up through Wednesday, by Tuesday another track might arrive adjusting Thursday-Friday expectations.&lt;/p&gt;
    &lt;p&gt;From a regulatory perspective, track layering implies a coordinated signaling system rather than isolated events. It’s as if an entity is broadcasting a moving roadmap that others (or their algorithms) are following, updating it as needed. The resilience of the price trajectory in presence of multiple signals reinforces the view that these are not random artifacts but intentionally placed instructions that the market subsequently obeys to a large degree.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Discussion&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our findings open up many questions about who or what is creating these signals, and why. The evidence suggests Power Tracks are intentional, machine-generated messages embedded in trading activity. Their existence implies a high degree of control or foresight by the originator: effectively, an actor could be programming the market in the short term, and possibly coordinating with others who recognize the signals. One hypothesis is that a sophisticated algorithm (or group of algorithms) uses small, sacrificial trades to encode future intentions – for instance, to coordinate a pump-and-dump across venues without explicit communication, or to signal accumulation/distribution plans to allied high-frequency traders. The fact that hidden venues (OTC, dark pools) are involved suggests this could relate to institutional actors executing large flows in a covert manner. Alternatively, it could be a form of manipulation or spoofing taken to another level: rather than simply placing fake orders, an actor actually executes a flurry of real trades in a pattern that algorithms (or insiders) know how to decode, effectively telling them “I’m about to drive the price to X, get on board.” This is speculative, but not unprecedented – markets have seen examples of covert signaling, though none as elaborate as this to our knowledge.&lt;/p&gt;
    &lt;p&gt;It’s also intriguing that the signals often required multi-venue data fusion to detect (remember that excluding OTC data caused a drop in detection rate, as noted in mid-2024). This could mean the sender spreads pieces of the “message” across exchanges and dark pools to avoid detection by any single exchange’s surveillance. Only by recombining the tape do we see the full picture.&lt;/p&gt;
    &lt;p&gt;The technical design of the encoding (varints, XOR, zigzag, etc.) indicates a deliberate attempt to compress information and avoid leaving plain-text-like traces. These are standard techniques in data serialization (e.g. Protocol Buffers use varint+zigzag for efficient encoding of numbers [formats.kaitai.io]). An entity crafting these signals would likely be aware of how to hide data in what appears to be just random trades: by using small price differences (deltas) to carry bits, and XOR to not have a constant pattern. This sophistication points to quants or engineers with knowledge of both trading and binary protocols.&lt;/p&gt;
    &lt;p&gt;We have taken great care to verify the Power-Track phenomenon, but we must also acknowledge limitations and alternative explanations. One possibility considered was whether these patterns are an artifact of some data processing quirk – for instance, could our detection algorithm be tricked by something like quote stuffing or other HFT behaviors that mimic an encoded burst? Quote stuffing (a barrage of orders to overload systems) can produce short bursty activity, but it typically doesn’t correlate with coherent price moves afterward; also, stuffing is usually detected as anomalies in order book updates, not so much in trade prints. The spectral and ROC combination we use is fairly specific and unlikely to consistently flag benign events. Additionally, our decoding wouldn’t produce meaningful output from random data – yet it did, repeatedly.&lt;/p&gt;
    &lt;p&gt;Another check: could major public news (which algorithms react to) coincidentally cause patterns that we misinterpret as “encoded then happened” when in reality it’s just reaction? We examined cases around earnings releases or market-wide news. Interestingly, Power Tracks often occurred without any associated news; they were self-contained. In a few instances, they preceded news by a short time – raising the tantalizing notion of foreknowledge – but that drifts into speculation. We consciously focused on periods without obvious external triggers to isolate the phenomenon.&lt;/p&gt;
    &lt;p&gt;In terms of decoding errors: our pipeline has many configurable parameters and heuristics (e.g. what constitutes a plausible timestamp, how to score field mappings). It’s possible some tracks were decoded incorrectly or not at all (we might have missed tracks if the thresholds were too strict or if the encoding changed beyond our assumptions). There is likely more to learn – for instance, the rolling XOR mask discovered in Q2 2025 suggests an adaptive adversary if we frame it as cat-and-mouse with whoever might be trying to hide these signals. We adapted and still found the mask (it was still a simple one, just not constant forever). If the scheme evolves further (more complex keys, different encoding), continuous research will be needed to keep up.&lt;/p&gt;
    &lt;p&gt;Our analysis primarily centered on one stock and a specific timeframe. We do not yet know how widespread this is – does it occur in other highly traded stocks, or only those with certain characteristics (like high short interest or volatility)? Are similar signals present in futures or crypto markets? These are open questions. The methodology we outlined can be applied to other instruments relatively easily, given the data.&lt;/p&gt;
    &lt;p&gt;If Power Tracks are real and orchestrated, they represent a form of insider signaling or market manipulation that bypasses traditional detection. Regulators like the SEC or FINRA, who monitor markets, typically look for things like spoofing, wash trades, or unusual order book activity. An encoded signal embedded in legitimate trades is far harder to spot – it requires piecing together data from multiple venues and interpreting it in an unconventional way. Our work demonstrates it’s technically feasible to uncover these, but it took significant reverse-engineering. Regulators may need to incorporate similar spectral algorithms and cross-venue analysis in their surveillance systems. Moreover, if identified, such coordinated behavior could violate securities laws (e.g., if it’s effectively a scheme to defraud or a manipulative device).&lt;/p&gt;
    &lt;p&gt;The existence of these signals could also explain some otherwise puzzling market phenomena: sudden price movements that seem to follow no news or conventional logic may in fact be following a “Power-Track” plan. It shifts the perspective from seeing the market as entirely reactive, to partially pre-scripted by unknown actors. That challenges the assumption of efficient markets – if prices can be steered predictably by those in the know, it undermines fairness for other participants.&lt;/p&gt;
    &lt;p&gt;On the other hand, one might argue if these signals are consistently there, why haven’t market forces arbitraged them away? Possibly because they are not obvious without decoding. Now that we’ve decoded them, one could attempt to trade on Power-Track predictions – effectively front-running the predictor. If many did so, it could either dilute the signals (making them less effective as others join the moves early) or the signal sender might stop using them. This enters ethical territory: do we broadcast these findings or quietly hand them to regulators first? We believe transparency is critical; thus this paper shares as much detail as possible so that the scientific and financial community can validate and extend this research. Every step we took is documented and could be reproduced with the same data (we cited data sources and key parameter values to facilitate replication).&lt;/p&gt;
    &lt;p&gt;From a technological standpoint, one exciting outcome of our project is the development of a real-time Power-Track Listener. This system uses the described detection algorithm and decoding pipeline to spot new tracks and immediately overlay their decoded prediction onto a live price chart. In testing, our listener successfully identified fresh Power Tracks within ~300 milliseconds of the burst and displayed the likely price path ahead. This kind of tool could be invaluable for both market surveillance and for trading strategies (though the latter raises fairness concerns if not widely available). We envision a regulator could deploy such a listener on all major stocks to get alerts like “Stock XYZ – encoded signal detected, predicts drop to $x within 5 minutes.” Combined with enforcement authority, they could then investigate the source of those trades.&lt;/p&gt;
    &lt;p&gt;We caution that real-time use needs robust filtering – false positives must be minimized to avoid chasing phantom signals. Our current false positive rate is low in historical tests, but in live mode, one must account for the myriad anomalies that occur. Nonetheless, the proof of concept is strong: markets can be monitored for these hidden instructions nearly as fast as they appear, given modern computing and data feeds.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Conclusion&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Summary of Findings: We have presented evidence of a novel phenomenon in equity markets – short bursts of trading activity (“Power Tracks”) that are highly structured and encode future price movements. Through a combination of signal processing and custom decoding techniques, we extracted a hidden layer of information from market data that in many cases accurately foretold price trajectory, timing, and trading range well ahead of time. Our analysis on GameStop stock from 2024–2025 found numerous such signals, with predictive success far beyond chance levels (p &amp;lt; 0.001). These signals sometimes stack across time horizons, painting a multi-scale picture of market direction. The technical encoding (varint, XOR, etc.) suggests they are intentionally placed by sophisticated actors, rather than random quirks.&lt;/p&gt;
    &lt;p&gt;Reproducibility: We ensured that our methodology is transparent and replicable. The minute-level price data used can be obtained from Polygon.io (for example, GME 1-minute OHLC data for January 2025 is available via their REST API or CSV downloads) and tick data can be similarly fetched (with &lt;code&gt;include_otc&lt;/code&gt; to capture off-exchange trades). All detection parameters (window=60s, frequency band 0.5–3 Hz, etc.) and decoding logic (varint parsing, zigzag decoding) are described herein with references to standard documentation for those encodings [formats.kaitai.iolemire.me]. Researchers or auditors can follow the steps: scan for spectral spikes, isolate bursts, apply XOR brute-force (0–31), then varint decode and test for meaningful output. In our repository, we have included source code and configuration (“powertracks” project, with modules for listener, decoder, analytics, etc., as outlined in Section 3). While that code is proprietary, the algorithms are fully described in this paper. We invite independent verification using other data sources or on other securities.&lt;/p&gt;
    &lt;p&gt;Implications: If Power Tracks are being used to coordinate or manipulate, this undermines the level playing field of the markets. It indicates an information asymmetry where certain players effectively know the near-term future (because they are collectively creating it). Regulators should take this seriously: conventional surveillance might not catch this kind of activity since it doesn’t necessarily break rules like spoofing or quoting obligations directly, but it could violate anti-fraud or market manipulation statutes in spirit. At minimum, it’s an unfair advantage if not accessible to all. We have begun sharing this research with regulatory bodies, and the response has been interest coupled with caution – it’s a complex find that will require further investigation (and possibly new tools on their part) to fully confirm and pursue enforcement if warranted.&lt;/p&gt;
    &lt;p&gt;Future Work: There are many avenues to extend this research. First, broadening the scope to more stocks and asset classes will determine how pervasive Power Tracks are. Are they mostly in meme stocks and high-volatility issues, or also in blue chips? Do index futures show similar patterns around macro events? Second, refining the decoding: our success rate is high, but we suspect there are more nuances (like dynamic field mappings or new opcodes) that could improve accuracy. Incorporating machine learning to assist in pattern recognition might help (e.g., an AI could learn the “language” of the tracks). However, we have purposely favored a deterministic, rule-based decode for transparency. Third, on the enforcement side, once identified, the next step is tracing these trades to their source. That requires broker-level data – regulators can subpoena data that we as researchers cannot. If all tracks were originating from a handful of entities, that would be a smoking gun. We hope our work provides the foundation and motivation to pursue those answers.&lt;/p&gt;
    &lt;p&gt;In conclusion, the discovery of Power Tracks suggests that the market microstructure contains an embedded messaging system that has been hitherto unknown. Uncovering it challenges our understanding of price formation and poses new questions about market fairness and oversight. We have demonstrated a method to shine light on this hidden layer. As data availability and analytical techniques advance, we expect more such “market x-rays” to become possible, revealing structure where once we saw randomness. We urge regulators, market operators, and researchers to collaborate in investigating Power Tracks further – to either confirm benign explanations or to root out abuses if present. The integrity of the markets may depend on our ability to detect and decode the signals lurking beneath the surface.&lt;/p&gt;
    &lt;p&gt;References: (Key references and data sources are cited inline in the text. For example, consolidated tape definition from SEC Investor.gov [investor.gov], hidden liquidity statistics from Bartlett &amp;amp; O’Hara (2024) [papers.ssrn.com], and technical encoding details for varint and zigzag from Kaitai Struct specs [formats.kaitai.io] and Lemire (2022) [lemire.me]. Additional documentation of the algorithms and tests can be found in the project repository documentation, which is beyond the scope of this paper. Readers are encouraged to obtain market data from providers like Polygon.io and replicate the detection methodology described.)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45984789</guid><pubDate>Wed, 19 Nov 2025 20:37:45 +0000</pubDate></item><item><title>Microsoft AI CEO pushes back against critics after recent Windows AI backlash</title><link>https://www.windowscentral.com/microsoft/windows-11/microsoft-ai-ceo-pushes-back-against-critics-after-recent-windows-ai-backlash-the-fact-that-people-are-unimpressed-is-mindblowing-to-me</link><description>&lt;doc fingerprint="40f795abfa6fdc7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Microsoft AI CEO pushes back against critics after recent Windows AI backlash — "the fact that people are unimpressed ... is mindblowing to me"&lt;/head&gt;
    &lt;p&gt;Mustafa Suleyman says people that are unimpressed with AI's capabilities are mind blowing to him, after recent backlash around Copilot and Windows as an agentic OS.&lt;/p&gt;
    &lt;p&gt;Microsoft's AI CEO, Mustafa Suleyman, has shared his opinion after recent pushback from users online that are becoming frustrated with Copilot and AI on Windows. In a post on X, Suleyman says he's mind blown by the fact that people are unimpressed with the ability to talk fluently with an AI computer.&lt;/p&gt;
    &lt;p&gt;"Jeez there so many cynics! It cracks me up when I hear people call AI underwhelming" Suleyman says. "I grew up playing Snake on a Nokia phone! The fact that people are unimpressed that we can have a fluent conversation with a super smart AI that can generate any image/video is mindblowing to me."&lt;/p&gt;
    &lt;p&gt;His post comes after Windows president Pavan Davuluri was recently met with major backlash from users online for posting about Windows evolving into an agentic OS. His post was so negatively received that he was forced to turn off replies, though Davuluri did later respond to reassure customers that the company was aware of the feedback.&lt;/p&gt;
    &lt;p&gt;Suleyman's post is also curiously timed with a recent damning report from The Verge that found Copilot's current capabilities don't match what Microsoft is advertising. The report found that in most cases, Copilot was unable to seamlessly or fluently achieve requests made by the user, the same requests that are shown in Copilot advertisements.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Jeez there so many cynics! It cracks me up when I hear people call AI underwhelming. I grew up playing Snake on a Nokia phone! The fact that people are unimpressed that we can have a fluent conversation with a super smart AI that can generate any image/video is mindblowing to me.November 19, 2025&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It's all terrible timing for Microsoft, as the company has just announced a new tagline for Windows: Your canvas for AI. The company is moving ahead with plans to evolve Windows into an agentic OS, spearheaded by AI agents that will ideally be capable of completing tasks for you. But given the current state of AI, and Copilot in particular, it seems that vision is still just a pipe dream.&lt;/p&gt;
    &lt;p&gt;Microsoft has a perception problem currently. With Windows' reputation at an all time low, the company's obsession with AI seems severely misplaced in the eyes of many users. The company should be focused on fixing fundamental issues with the Windows platform, but instead it's too busy trying to shove AI into every UI surface it possibly can.&lt;/p&gt;
    &lt;p&gt;Windows' president has said that he knows Microsoft "has a lot of work to do" in regards to improving Windows for power users and developers, which should hopefully mean focusing on things that aren't AI. But with Microsoft literally becoming an AI company in the last year, it's hard to believe we're going to see a version of Windows that isn't bloated with AI functionality most people didn't ask for.&lt;/p&gt;
    &lt;p&gt;All the latest news, reviews, and guides for Windows and Xbox diehards.&lt;/p&gt;
    &lt;p&gt;Suleyman's post after the recent backlash around Windows and Copilot is not reassuring. It's clear he believes AI is the future, potentially impacting every aspect of our lives, and present in all software and experiences. We'll have to see where Windows heads now that customers are more vocally pushing back against this vision.&lt;/p&gt;
    &lt;p&gt;Follow Windows Central on Google News to keep our latest news, insights, and features at the top of your feeds!&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45984970</guid><pubDate>Wed, 19 Nov 2025 20:51:33 +0000</pubDate></item></channel></rss>