<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 15 Sep 2025 12:19:07 +0000</lastBuildDate><item><title>Human writers have always used the em dash</title><link>https://www.theringer.com/2025/08/20/pop-culture/em-dash-use-ai-artificial-intelligence-chatgpt-google-gemini</link><description>&lt;doc fingerprint="378152dd2e855f8d"&gt;
  &lt;main&gt;
    &lt;p&gt;I stand before you today with violence in my heart. I do not come in peace. I come to obliterate, disparage, and destroy. In this fallen world of ours, there exist certain ideas that must be annihilated before goodness can flourish. I am here to rain holy fire upon one of those ideas, and I am here to do so in the name of a punctuation mark.&lt;/p&gt;
    &lt;p&gt;The punctuation mark in question is the em dash. The idea—the terrible, mistaken idea—is that the use of em dashes in a piece of writing is a sign that the text was generated by AI. Some people have been saying this on, guess where, the internet. The implication is that human writers should avoid em dashes for fear of being mistaken for chatbots. No. Wrong. I am here to raze this implication to the very ground and salt the earth where it stood.&lt;/p&gt;
    &lt;p&gt;You're familiar, I hope, with the em dash? I speak of the elegant, elongated hyphen, the gentle friend and ally of all writers, used to set off a chunk of text within a sentence. It looks like this: —. Notice, please, how gracious it is, how welcoming, like an outstretched arm guiding you over a tricky step. It can be used in so many different ways. Perhaps you start a thought and—because thoughts don't always move in perfectly straight lines—find that you need to modify the thought with another thought before you finish it. Perhaps you reach the end of a thought and want to emphasize your conclusion—an em dash can do it. Perhaps you simply wish to mark a pause—a hitch, a breath—in the flow of your thinking. The em dash can be annoying when overused, sure. But when it's used well, it's as flexible and subtle as the turns of thought itself.&lt;/p&gt;
    &lt;p&gt;Yet for months, the em dash has been the victim of a relentless campaign of online persecution the likes of which no punctuation mark, not even the semicolon, has ever seen. Any chucklehead who disagrees with an online post can, if that post happens to include an em dash, dismiss it with the claim that it was written by a chatbot. It's gotten so bad that people on random subreddits are having to issue PSAs informing their fellow users that em dashes were not invented by ChatGPT. These people are heroes of our time. Normally I'm an enthusiastic advocate of AI-shaming, but I will not stand idly by while you make the em dash a pawn in your dirty game of bot-spotting.&lt;/p&gt;
    &lt;p&gt;As with many modern plagues, the theory that em dashes belong to the machines has a disputed origin. Some people say it began with a racist-seeming X post. Others say it started on the ChatGPT subreddit. Still others suggest the trend was noticed by people working on AI detection tools, or trying to eliminate bot-spawned slop from forums. Wherever it came from, the theory seems to have started going viral around this past February—objectively the dumbest month, a time when no one should try to have thoughts, much less broadcast them—and soon, em dash discourse was convulsing the internet.&lt;/p&gt;
    &lt;p&gt;There are two reasons why this discourse must be stopped: The first has to do with the way generative AI works; the second has to do with the fate of the human soul. Let's take them one by one.&lt;/p&gt;
    &lt;p&gt;First, generative AI. I haven't seen any hard evidence that chatbots, in practice, use more em dashes than anyone else. Anecdotally, people say it's hard to get them to stop using em dashes; you can order an AI agent to generate text without dashes, and it might still put dashes in the text it generates. But that only proves that AI chatbots suck, not that human writers use em dashes less frequently.&lt;/p&gt;
    &lt;p&gt;I can understand why lots of people seized on the idea of a simple AI detection filter. You recognize a vampire because it can't enter your house without an invitation; it would be handy, at a moment when language is suddenly being produced at enormous scale by unthinking machines, to have a similar rubric for text. But looking to em dashes to play this role is like claiming you can identify vampires because they wear black clothes. You're going to confuse a lot of humans for vampires, and you're going to trust a lot of vampires by mistake.&lt;/p&gt;
    &lt;p&gt;If generative AI does have a predilection for em dashes, though, the reason is simply that many human writers use em dashes. Your chatbot also uses commas, just as human writers do. A chatbot does not have a consciousness. It does not "know" how to write, in any meaningful sense. It doesn't have a style, because style requires thought, preference, and taste. A gen-AI chatbot is trained by scanning gargantuan amounts of text. Based on the patterns it detects in that text, it then assesses the probability that certain words and syntactic constructions will occur in proximity to one another. When you ask a chatbot to write an essay about the architecture of Oslo, it looks for material containing those words, then makes algorithmic guesses as to what other words should appear, and in what order. The reason AI bots generate false results so often—the reason they'll claim buildings are in Oslo that aren't actually in Oslo, or that don't even exist—is that these kinds of probability assessments are crude and imprecise, and include no reliable means of comparing the generated text to reality or truth.&lt;/p&gt;
    &lt;p&gt;So where did the text on which the bots were trained come from in the first place? Well, until bots started being trained on the output of other bots, all the material they were fed was the work of human writers, and much of it was the work of professionals. Meta, for instance, torrented more than 80 terabytes of copyrighted books. Just straight-up pirated hundreds of thousands, possibly millions, of books. And because em dashes are so useful to writers, books tend to include them. So the bots, programmed to copy the work their creators pirated, use them too.&lt;/p&gt;
    &lt;p&gt;In other words, it's not accurate to say that the use of em dashes in a text is a sign that the text is AI-generated. It's more accurate to say that the prevalence of em dashes in AI-generated text is a sign of how reliant the AI companies are on the human writers they want to replace.&lt;/p&gt;
    &lt;p&gt;Now, the second point. Let's talk about what the em dash represents at a moment when it seems as if culture—meaning, for example, the collision of social media, an increasingly authoritarian political discourse, the rapid decline of post-pandemic literacy, the decline of original art, and other related phenomena—has become a massive conspiracy to destroy our ability to think. Grammar and punctuation may seem like boring topics, but you need them if you want to keep that ability alive. Grammar is our system for organizing thoughts into language. Grammar is what lets us put thoughts in relation to one another. The more complex the grammatical structures, the more complex the work they can do with ideas, which is why children's books tend to utilize simpler language than philosophical treatises.&lt;/p&gt;
    &lt;p&gt;Before the 20th century, a great deal of formal writing instruction concentrated on how to make complex language more readable, elegant, and beautiful. Around a hundred years ago, though, writing instruction started to favor simple, direct sentences and pared-down grammatical structures in the interest of clarity and efficiency rather than complicated thought. Think about the sort of advice you'd get at a college writing lab, for instance, or the sort you'd find in Strunk and White's legendary writing manual, The Elements of Style. Don't use more words than necessary. Keep your sentences short. Use active verbs over passive verbs. Cut down on adverbs and chuck anything that looks ornate straight out the window.&lt;/p&gt;
    &lt;p&gt;And all this advice is great if you're trying to write an internet column, say, or dash off a quick work email. If you're dealing with more complex ideas, though, the bias of contemporary English toward grammatical minimalism can be somewhat limiting. Without all the shades and nuances of complex sentence forms, you encounter a sort of downward pressure to simplify your ideas—which, again, is often a good thing to do, but not always when you're trying to say something original about the multifarious and contradictory world in which you live.&lt;/p&gt;
    &lt;p&gt;And that's where the em dash comes to your rescue. This may be a bizarre thing to say, but I think of the em dash as the evolutionary heir of 18th- and 19th-century rhetoric, in the same way that birds are the heirs of dinosaurs. The em dash is a tool that lets writers expand their ideas without making them inaccessible. It's almost always easy to follow in use, and the rules that govern it aren't as ironclad as the rules that govern parentheses or semicolons; you're able to deploy the em dash in more discretionary ways. It lets you break up sentences into parts that rub up against each other, challenge each other, give each other new inflections, or change each other's meanings in ways that resemble the flow of human thought.&lt;/p&gt;
    &lt;p&gt;It would be a tragedy if writers stopped using em dashes out of fear of sounding like AI, because em dashes are one of the best tools writers have for not sounding robotic in the first place. Their very potential to be irritating is a sign of what makes them so beautiful: Of all the forms of punctuation, the em dash is the one that most rewards tact, judgment, and taste. It has the closest relationship to the way we experience thinking—rushing forward, suddenly swerving, forking into different branches that eventually come together again. If chatbots copy our use of it, they do so for the same reason we need to protect it. It's the most human punctuation there is.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45226150</guid><pubDate>Fri, 12 Sep 2025 20:06:44 +0000</pubDate></item><item><title>NASA's Guardian Tsunami Detection Tech Catches Wave in Real Time</title><link>https://www.jpl.nasa.gov/news/nasas-guardian-tsunami-detection-tech-catches-wave-in-real-time/</link><description>&lt;doc fingerprint="f724582e4fbe7c29"&gt;
  &lt;main&gt;
    &lt;p&gt;A recent tsunami triggered by a magnitude 8.8 earthquake off Russia’s Kamchatka Peninsula sent pressure waves to the upper layer of the atmosphere, NASA scientists have reported. While the tsunami did not wreak widespread damage, it was an early test for a detection system being developed at the agency’s Jet Propulsion Laboratory in Southern California.&lt;/p&gt;
    &lt;p&gt;Called GUARDIAN (GNSS Upper Atmospheric Real-time Disaster Information and Alert Network), the experimental technology “functioned to its full extent,” said Camille Martire, one of its developers at JPL. The system flagged distortions in the atmosphere and issued notifications to subscribed subject matter experts in as little as 20 minutes after the quake. It confirmed signs of the approaching tsunami about 30 to 40 minutes before waves made landfall in Hawaii and sites across the Pacific on July 29 (local time).&lt;/p&gt;
    &lt;p&gt;“Those extra minutes of knowing something is coming could make a real difference when it comes to warning communities in the path,” said JPL scientist Siddharth Krishnamoorthy.&lt;/p&gt;
    &lt;p&gt;Near-real-time outputs from GUARDIAN must be interpreted by experts trained to identify the signs of tsunamis. But already it’s one of the fastest monitoring tools of its kind: Within about 10 minutes of receiving data, it can produce a snapshot of a tsunami’s rumble reaching the upper atmosphere.&lt;/p&gt;
    &lt;p&gt;The goal of GUARDIAN is to augment existing early warning systems. A key question after a major undersea earthquake is whether a tsunami was generated. Today, forecasters use seismic data as a proxy to predict if and where a tsunami could occur, and they rely on sea-based instruments to confirm that a tsunami is passing by. Deep-ocean pressure sensors remain the gold standard when it comes to sizing up waves, but they are expensive and sparse in locations.&lt;/p&gt;
    &lt;p&gt;“NASA’s GUARDIAN can help fill the gaps,” said Christopher Moore, director of the National Oceanic and Atmospheric Administration Center for Tsunami Research. “It provides one more piece of information, one more valuable data point, that can help us determine, yes, we need to make the call to evacuate.”&lt;/p&gt;
    &lt;p&gt;Moore noted that GUARDIAN adds a unique perspective: It’s able to sense sea surface motion from high above Earth, globally and in near-real-time.&lt;/p&gt;
    &lt;p&gt;Bill Fry, chair of the United Nations technical working group responsible for tsunami early warning in the Pacific, said GUARDIAN is part of a technological “paradigm shift.” By directly observing ocean dynamics from space, “GUARDIAN is absolutely something that we in the early warning community are looking for to help underpin next generation forecasting.”&lt;/p&gt;
    &lt;head rend="h3"&gt;How GUARDIAN works&lt;/head&gt;
    &lt;p&gt;GUARDIAN takes advantage of tsunami physics. During a tsunami, many square miles of the ocean surface can rise and fall nearly in unison. This displaces a significant amount of air above it, sending low-frequency sound and gravity waves speeding upwards toward space. The waves interact with the charged particles of the upper atmosphere — the ionosphere — where they slightly distort the radio signals coming down to scientific ground stations of GPS and other positioning and timing satellites. These satellites are known collectively as the Global Navigation Satellite System (GNSS).&lt;/p&gt;
    &lt;p&gt;Get the JPL Newsletter&lt;/p&gt;
    &lt;p&gt;From Mars to the Milky Way—never miss a discovery! Delivered straight to your inbox.&lt;/p&gt;
    &lt;p&gt;While GNSS processing methods on Earth correct for such distortions, GUARDIAN uses them as clues.&lt;/p&gt;
    &lt;p&gt;The software scours a trove of data transmitted to more than 350 continuously operating GNSS ground stations around the world. It can potentially identify evidence of a tsunami up to about 745 miles (1,200 kilometers) from a given station. In ideal situations, vulnerable coastal communities near a GNSS station could know when a tsunami was heading their way and authorities would have as much as 1 hour and 20 minutes to evacuate the low-lying areas, thereby saving countless lives and property.&lt;/p&gt;
    &lt;p&gt;Key to this effort is the network of GNSS stations around the world supported by NASA’s Space Geodesy Project and Global GNSS Network, as well as JPL’s Global Differential GPS network that transmits the data in real time.&lt;/p&gt;
    &lt;p&gt;The Kamchatka event offered a timely case study for GUARDIAN. A day before the quake off Russia’s northeast coast, the team had deployed two new elements that were years in the making: an artificial intelligence to mine signals of interest and an accompanying prototype messaging system.&lt;/p&gt;
    &lt;p&gt;Both were put to the test when one of the strongest earthquakes ever recorded spawned a tsunami traveling hundreds of miles per hour across the Pacific Ocean. Having been trained to spot the kinds of atmospheric distortions caused by a tsunami, GUARDIAN flagged the signals for human review and notified subscribed subject matter experts.&lt;/p&gt;
    &lt;p&gt;Notably, tsunamis are most often caused by large undersea earthquakes, but not always. Volcanic eruptions, underwater landslides, and certain weather conditions in some geographic locations can all produce dangerous waves. An advantage of GUARDIAN is that it doesn’t require information on what caused a tsunami; rather, it can detect that one was generated and then can alert the authorities to help minimize the loss of life and property.&lt;/p&gt;
    &lt;p&gt;While there’s no silver bullet to stop a tsunami from making landfall, “GUARDIAN has real potential to help by providing open access to this data,” said Adrienne Moseley, co-director of the Joint Australian Tsunami Warning Centre. “Tsunamis don’t respect national boundaries. We need to be able to share data around the whole region to be able to make assessments about the threat for all exposed coastlines.”&lt;/p&gt;
    &lt;p&gt;To learn more about GUARDIAN, visit:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45226938</guid><pubDate>Fri, 12 Sep 2025 21:25:08 +0000</pubDate></item><item><title>A set of smooth, fzf-powered shell aliases&amp;functions for systemctl</title><link>https://silverrainz.me/blog/2025-09-systemd-fzf-aliases.html</link><description>&lt;doc fingerprint="1b6a18422f25c136"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;A set of smooth, fzf-powered shell aliases&amp;amp;functions for &lt;code&gt;systemctl&lt;/code&gt;#&lt;/head&gt;&lt;p&gt;If you've ever found yourself repeatedly typing long systemctl commands or struggling to remember exact service names, this post is for you.&lt;/p&gt;&lt;p&gt;A while ago I implemented a set of shell aliases and functions, and now I can manage my systemd services very smoothly:&lt;/p&gt;&lt;head rend="h2"&gt;Motivation: The Pain Point#&lt;/head&gt;&lt;p&gt;Let's acknowledge a universal sysadmin/developer experience: typing &lt;code&gt;systemctl&lt;/code&gt; over and over, managing long unit names.&lt;/p&gt;&lt;p&gt;Beside, the current shell completion implementation is is quite slow, especially on thin client (for example: Raspberry Pi). I am not familiar with the shell completion, so I am not tended to improve it.&lt;/p&gt;&lt;p&gt;⛺ joehillen/sysz is a TUI for systemd, which is a great inspiration that proves fzf is perfect for this job: Fuzzy completion is much more efficient than prefix completion. It is quite frustrating that:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;sysz is a TUI program; it is not a one-command action, you have to&lt;/p&gt;step-by-step. If you have to start and stop service frequently (for example, when debugging), you have to repeat these steps over and over&lt;/item&gt;&lt;item&gt;&lt;p&gt;sysz is unmaintained since 2022&lt;/p&gt;&lt;code&gt;:'(&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;So I decided to build a custom set of shell functions and aliases that supercharges &lt;code&gt;systemctl&lt;/code&gt; and &lt;code&gt;journalctl&lt;/code&gt; with fuzzy-finding magic for my personal usage.&lt;/p&gt;&lt;head rend="h2"&gt;The Vision: What I Wanted to Build#&lt;/head&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;Core Principle&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;Keep it in the shell. No new binaries, just Shell functions and aliases.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-2"&gt;Desired Features&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-3"&gt;Easy to type:&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;no need to type long command name and unit name&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-4"&gt;Easy to repeat:&lt;/item&gt;&lt;item rend="dd-4"&gt;&lt;p&gt;history operations can be easily performed&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-5"&gt;Easy to maintain:&lt;/item&gt;&lt;item rend="dd-5"&gt;&lt;p&gt;just like any other programming, keep it simple and avoid repetition.&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-6"&gt;Dual Support:&lt;/item&gt;&lt;item rend="dd-6"&gt;&lt;p&gt;seamlessly handle both&lt;/p&gt;&lt;code&gt;--system&lt;/code&gt;(sudo) and&lt;code&gt;--user&lt;/code&gt;units.&lt;/item&gt;&lt;item rend="dt-7"&gt;Error handling:&lt;/item&gt;&lt;item rend="dd-7"&gt;&lt;p&gt;print detailed information when operation failed&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-8"&gt;One for one:&lt;/item&gt;&lt;item rend="dd-8"&gt;&lt;p&gt;each operation corresponds to a command/alias, which is completion friendly constrast to subcommand&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Implementation: Breaking Down the Script#&lt;/head&gt;&lt;head rend="h3"&gt;1. The Basics: Aliases#&lt;/head&gt;&lt;p&gt;We can easily define some extremely short aliases for the long &lt;code&gt;systemctl&lt;/code&gt; and &lt;code&gt;journalctl&lt;/code&gt; commands:&lt;/p&gt;&lt;code&gt;alias s='sudo systemctl'
alias sj='journalctl'
alias u='systemctl --user'
alias uj='journalctl --user'
&lt;/code&gt;&lt;p&gt;Operating on &lt;code&gt;systemctl&lt;/code&gt; unit require root privilege, so a &lt;code&gt;sudo&lt;/code&gt; is required.&lt;/p&gt;&lt;head rend="h3"&gt;2. The Heart: Taming Systemd with FZF#&lt;/head&gt;&lt;p&gt;Using fzf to fuzzy complete can greatly improve the efficiency of inputting SystemD units.&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;&lt;code&gt;systemctl list-units | fzf&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;The output of&lt;/p&gt;&lt;code&gt;systemctl list-units&lt;/code&gt;looks like this:&lt;quote&gt;UNIT LOAD ACTIVE SUB DESCRIPTION ... ... ... ... ... -.mount loaded active mounted Root Mount boot.mount loaded active mounted /boot dev-hugepages.mount loaded active mounted Huge Pages File System dev-mqueue.mount loaded active mounted POSIX Message Queue File System proc-sys-fs-binfmt_misc.mount loaded active mounted Arbitrary Executable File Formats File System run-user-1000-doc.mount loaded active mounted /run/user/1000/doc ... ... ... ... ... Legend: LOAD → Reflects whether the unit definition was properly loaded. ACTIVE → The high-level unit activation state, i.e. generalization of SUB. SUB → The low-level unit activation state, values depend on unit type. 162 loaded units listed. Pass --all to see loaded but inactive units, too. To show all installed unit files use 'systemctl list-unit-files'.&lt;/quote&gt;&lt;p&gt;We can easily write a script like that:&lt;/p&gt;&lt;quote&gt;systemctl list-units --legend=false \ | fzf --accept-nth=1 \ --no-hscroll \ --preview="systemctl status {1}" \ --preview-window=down&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-2"&gt;&lt;code&gt;--legend=false&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;p&gt;can hide the trailing hints of outputs, but the column is also hidden&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-3"&gt;&lt;code&gt;--accept-nth=1&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;ask fzf only print the first column (aka the unit name) of the select row&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-4"&gt;&lt;code&gt;--preview="... {1}"&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-4"&gt;&lt;p&gt;the&lt;/p&gt;&lt;code&gt;{num}&lt;/code&gt;syntax means pass the numth colmun of highlighting row. We can therefore preview the service status in real-time&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item rend="dt-5"&gt;Merging &lt;code&gt;list-units&lt;/code&gt;and&lt;code&gt;list-unit-files&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-5"&gt;&lt;p&gt;As&lt;/p&gt;&lt;code&gt;list-units&lt;/code&gt;only list units currently in memory, we usually need to start from the unit that has not yet been loaded, so we also need to list all installed unit files via&lt;code&gt;list-unit-files&lt;/code&gt;:&lt;quote&gt;cat &amp;lt;(systemctl list-units --legend=false) \ &amp;lt;(systemctl list-unit-files --legend=false) \ | fzf --accept-nth=1 \ --no-hscroll \ --preview="systemctl status {1}" \ --preview-window=down&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-6"&gt;&lt;code&gt;&amp;lt;(systemctl ...)&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-6"&gt;&lt;p&gt;Use the Process Substitution Syntax to merge stdout from multiple&lt;/p&gt;&lt;code&gt;systemctl ...&lt;/code&gt;commands&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item rend="dt-7"&gt;Columnating&lt;/item&gt;&lt;item rend="dd-7"&gt;&lt;p&gt;The above script doesn't work well,&lt;/p&gt;&lt;code&gt;list-units&lt;/code&gt;and&lt;code&gt;list-unit-files&lt;/code&gt;have different output formats: the former one has 5 columns and the latter has 3, which will mess up fzf's UI:&lt;quote&gt;cat &amp;lt;(echo 'UNIT/FILE LOAD/STATE ACTIVE/PRESET SUB DESCRIPTION') \ &amp;lt;(systemctl list-units --legend=false) \ &amp;lt;(systemctl list-unit-files --legend=false) \ | column --table --table-columns-limit=5 \ | sed 's/●/ /' \ | grep . \ | fzf --header-lines=1 \ --accept-nth=1 \ --no-hscroll \ --preview="SYSTEMD_COLORS=1 systemctl status {1}" \ --preview-window=down&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-8"&gt;&lt;code&gt;echo 'UNIT/FILE ...'&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-8"&gt;&lt;p&gt;a hardcoded table header that can tell the user the meaning of the column&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-9"&gt;&lt;code&gt;column --table ...&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-9"&gt;&lt;p&gt;the&lt;/p&gt;&lt;code&gt;column&lt;/code&gt;command from util-linux can columnate the text and output as a table, set&lt;code&gt;--table-columns-limit&lt;/code&gt;to&lt;code&gt;5&lt;/code&gt;to prevent the "DESCRIPTION" column from being trimmed&lt;/item&gt;&lt;item rend="dt-10"&gt;&lt;code&gt;sed 's/●/ /'&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-10"&gt;&lt;p&gt;to strip the dot ("●") unit state which breaks the colmun&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-11"&gt;&lt;code&gt;grep .&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-11"&gt;&lt;p&gt;to strip the empty line&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-12"&gt;&lt;code&gt;SYSTEMD_COLORS=1&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-12"&gt;&lt;p&gt;force enabled colorful output&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item rend="dt-13"&gt;Reusable for &lt;code&gt;--user&lt;/code&gt;&lt;/item&gt;&lt;item rend="dd-13"&gt;&lt;p&gt;As we want to handle both&lt;/p&gt;&lt;code&gt;--system&lt;/code&gt;and&lt;code&gt;--user&lt;/code&gt;units, we can encapsulate the script to a function:&lt;quote&gt;# SystemD unit selector. _sysls() { WIDE=$1 [ -n "$2" ] &amp;amp;&amp;amp; STATE="--state=$2" cat \ &amp;lt;(echo 'UNIT/FILE LOAD/STATE ACTIVE/PRESET SUB DESCRIPTION') \ &amp;lt;(systemctl $WIDE list-units --quiet $STATE) \ &amp;lt;(systemctl $WIDE list-unit-files --quiet $STATE) \ | sed 's/●/ /' \ | grep . \ | column --table --table-columns-limit=5 \ | fzf --header-lines=1 \ --accept-nth=1 \ --no-hscroll \ --preview="SYSTEMD_COLORS=1 systemctl $WIDE status {1}" \ --preview-window=down } alias sls='_sysls --system' alias uls='_sysls --user'&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-14"&gt;&lt;code&gt;$1&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-14"&gt;&lt;p&gt;is&lt;/p&gt;&lt;code&gt;--system&lt;/code&gt;or&lt;code&gt;--user&lt;/code&gt;&lt;/item&gt;&lt;item rend="dt-15"&gt;&lt;code&gt;$2&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-15"&gt;&lt;p&gt;is service states, see also&lt;/p&gt;&lt;code&gt;systemctl list-units --state=help&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Then we can use&lt;/p&gt;&lt;code&gt;sls&lt;/code&gt;and&lt;code&gt;uls&lt;/code&gt;to get the full service name by fuzzy matching.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;3. The Complete Function#&lt;/head&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;Error handling&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;When performing&lt;/p&gt;&lt;code&gt;systemctl start xxx.service&lt;/code&gt;, if the service does not start successfully, it only tell you to run&lt;code&gt;journalctl -xeu&lt;/code&gt;to see the log:&lt;quote&gt;$ s start docker.service Job for docker.service failed because the control process exited with error code. See "systemctl status docker.service" and "journalctl -xeu docker.service" for details.&lt;/quote&gt;&lt;p&gt;In another situation, if a service immediately dies after launched, systemctl even tells you nothing:&lt;/p&gt;&lt;quote&gt;$ s start getty@foo $ echo $? 0 $ s status getty@foo × getty@foo.service - Getty on foo Loaded: loaded (/usr/lib/systemd/system/getty@.service; disabled; preset: enabled) Active: failed (Result: start-limit-hit) since Fri 2025-09-12 20:44:26 CST; 1s ago ...: ... Sep 12 20:44:26 x1c systemd[1]: ... Sep 12 20:44:26 x1c systemd[1]: Failed to start Getty on foo.&lt;/quote&gt;&lt;p&gt;To help users get detailed service status after launching a service, we can use the following pattern:&lt;/p&gt;&lt;quote&gt;s start foo.service &amp;amp;&amp;amp; s status $_ || sj -xeu $_&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-2"&gt;&lt;code&gt;A &amp;amp;&amp;amp; B || C&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;p&gt;if A success, performing B, else C&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-3"&gt;&lt;code&gt;$_&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;is the last argument of the previous command, in this case it is "foo.service"&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item rend="dt-4"&gt;Repeatable&lt;/item&gt;&lt;item rend="dd-4"&gt;&lt;p&gt;The key to efficient debugging is repeatability. After fuzzy-selecting and starting a service once, I should be able to simply press the ↑ arrow and Enter to run the exact same command again, without going through the fuzzy selection process every time:&lt;/p&gt;&lt;quote&gt;sstart () { CMD="s start $(sls static,disabled,failed) &amp;amp;&amp;amp; s status \$_ || sj -xeu \$_" eval $CMD [ -n "$BASH_VERSION" ] &amp;amp;&amp;amp; history -s $CMD [ -n "$ZSH_VERSION" ] &amp;amp;&amp;amp; print -s $CMD }&lt;/quote&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-5"&gt;&lt;code&gt;sls static,...&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-5"&gt;&lt;p&gt;pre-filtering services by states, services that need to be "start"-ed must not be in active state, filter by these states can reduce the number of outputs, accelerate the command to some extent 存疑&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-6"&gt;&lt;code&gt;\$_&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-6"&gt;&lt;p&gt;prevent the variable from being expanded before eval&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-7"&gt;&lt;code&gt;history -s&lt;/code&gt;and&lt;code&gt;print -s&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-7"&gt;&lt;p&gt;push the command to history, facilitating subsequent repetition&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;4. The Magic: Dynamic Function Generation#&lt;/head&gt;&lt;p&gt;After implementing &lt;code&gt;sstart&lt;/code&gt;, we also have to implement:&lt;/p&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;&lt;code&gt;sstop&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;for&lt;/p&gt;&lt;code&gt;systemctl stop&lt;/code&gt;&lt;/item&gt;&lt;item rend="dt-2"&gt;&lt;code&gt;sre&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;p&gt;for&lt;/p&gt;&lt;code&gt;systemctl restart&lt;/code&gt;&lt;/item&gt;&lt;item rend="dt-3"&gt;&lt;code&gt;ustart&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;for&lt;/p&gt;&lt;code&gt;systemctl --user start&lt;/code&gt;&lt;/item&gt;&lt;item rend="dt-4"&gt;&lt;code&gt;ustop&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-4"&gt;&lt;p&gt;for&lt;/p&gt;&lt;code&gt;systemctl --user stop&lt;/code&gt;&lt;/item&gt;&lt;item rend="dt-5"&gt;&lt;code&gt;ure&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-5"&gt;&lt;p&gt;for&lt;/p&gt;&lt;code&gt;systemctl --user restart&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Repeatedly implementing these functions is tedious and boring. Fortunately, we can dynamically generate them in a loop:&lt;/p&gt;&lt;p&gt;备注&lt;/p&gt;&lt;p&gt;This dynamic generation approach avoids repetitive code but adds some complexity. For clarity, you could instead explicitly define each function.&lt;/p&gt;&lt;code&gt;_SYS_ALIASES=(
    sstart sstop sre
    ustart ustop ure
)
_SYS_CMDS=(
    's start $(sls static,disabled,failed)'
    's stop $(sls running,failed)'
    's restart $(sls)'
    'u start $(uls static,disabled,failed)'
    'u stop $(uls running,failed)'
    'u restart $(uls)'
)

_sysexec() {
    for ((j=0; j &amp;lt; ${#_SYS_ALIASES[@]}; j++)); do
        if [ "$1" == "${_SYS_ALIASES[$j]}" ]; then
            cmd=$(eval echo "${_SYS_CMDS[$j]}") # expand service name
            wide=${cmd:0:1}
            cmd="$cmd &amp;amp;&amp;amp; ${wide} status \$_ || ${wide}j -xeu \$_"
            eval $cmd

            # Push to history.
            [ -n "$BASH_VERSION" ] &amp;amp;&amp;amp; history -s $cmd
            [ -n "$ZSH_VERSION" ] &amp;amp;&amp;amp; print -s $cmd
            return
        fi
    done
}

# Generate bash function/zsh widgets.
for i in ${_SYS_ALIASES[@]}; do
    source /dev/stdin &amp;lt;&amp;lt;EOF
$i() {
    _sysexec $i
}
EOF
done
&lt;/code&gt;&lt;list rend="dl"&gt;&lt;item rend="dt-1"&gt;&lt;code&gt;for ((j=0; j &amp;lt; ...; j++))&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-1"&gt;&lt;p&gt;is a bash and zsh compatible&lt;/p&gt;&lt;code&gt;for&lt;/code&gt;loop syntax&lt;/item&gt;&lt;item rend="dt-2"&gt;&lt;code&gt;_sysexec&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-2"&gt;&lt;p&gt;a wrapper for dynamically dispatching function&lt;/p&gt;&lt;/item&gt;&lt;item rend="dt-3"&gt;&lt;code&gt;source ...&lt;/code&gt;:&lt;/item&gt;&lt;item rend="dd-3"&gt;&lt;p&gt;a way for generating function dynamically&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Final Thoughts#&lt;/head&gt;&lt;p&gt;Now you can&lt;/p&gt;to start a service. If service is failed, the related logs are print automaticlly. You also can press ↑ to browse commands history to repeat the previous operation.&lt;p&gt;Using this script saved me a lot of unnecessary keystrokes, just an &lt;code&gt;s&lt;/code&gt; gives me more happiness than &lt;code&gt;systemctl&lt;/code&gt;. The fuzzy search algorithm of fzf is good enough that I can get the desired result in one go even with a casual keystroke. It also works well on my Raspberry Pi 3B.&lt;/p&gt;&lt;p&gt;Feel free to grab the script from my dotfiles repository and adapt it to your own workflow. I'd love to hear about your own systemd productivity tricks in the comments!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45228615</guid><pubDate>Sat, 13 Sep 2025 01:40:37 +0000</pubDate></item><item><title>Why We Spiral</title><link>https://behavioralscientist.org/why-we-spiral/</link><description>&lt;doc fingerprint="5d0a0a15efb30cdb"&gt;
  &lt;main&gt;
    &lt;p&gt;Say you’re a senior member of your team at work. You’re 12 minutes late to the weekly staff Zoom. Once you’ve “joined audio,” the first thing you hear is your old friend’s voice. “There you are! So glad you could fit us in.” You laugh and explain the disastrous traffic, difficult drop-off at your kids’ school, or whatever it was that messed up your morning. The moment passes and the conversation moves on. You turn to the job at hand, focused and ready to go.&lt;/p&gt;
    &lt;p&gt;But what if you’re a junior staffer, still feeling your way. Same thing happens: You’re 12 minutes late to the weekly staff Zoom. Once you’ve “joined audio,” the first thing you hear is the boss’s voice. “There you are! So glad you could fit us in.” A few colleagues chuckle. You consider making excuses—about traffic, drop-off, whatever it was—but the moment passes, and the conversation moves on.&lt;/p&gt;
    &lt;p&gt;Your mind doesn’t, though. It’s still ruminating. Was that snark in my boss’s voice? Were they talking about me before I logged on? Do I fit in here? Am I any good at this job? You might not be fully aware of these questions. Your mind works quickly on multiple tracks at the same time. And those questions are nasty; they threaten your sense of belonging, your worth, and your value, at least at work. So you try to push them away, to suppress them. But they’re still there. And once they’ve been triggered, it might feel like the evidence keeps pouring in.&lt;/p&gt;
    &lt;p&gt;Someone makes an inside joke in the chat. You don’t get it. I don’t belong here. Someone rolls their eyes while you’re talking. They don’t respect me. The boss ignores you for the rest of the meeting. No one sees me. Again, these thoughts may not be fully conscious. But there’s no mistaking the fact that your motivation to get back to work has waned by the time you log off. What was it you were supposed to look into?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Was that snark in my boss’s voice? Were they talking about me before I logged on? Do I fit in here? Am I any good at this job?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Next thing you know, you’re idly messing around online when a text comes in from the person who rolled their eyes. “You okay? You seemed out of it at the meeting.” You ignore it. But your mind doesn’t. It’s busy composing possible replies. The full spectrum from passive-aggressive to career imperiling. Eventually you pick up your phone. What will you text back?&lt;/p&gt;
    &lt;p&gt;This is how self-defeating spirals start and how they gather speed. Let’s break down the moving parts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A circumstance places a big question on the table—about identity, belonging, or adequacy: You’re new at work. You want to succeed and belong, but you wonder . . . That question looms, latent and inactive, but present.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A “bad” thing happens: Your boss is a little snarky.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;That question gets triggered: You read the room for answers, drawing negative inferences from ambiguous evidence. You’re distracted from the task at hand. Your pessimistic hypothesis becomes more entrenched.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You act on that pessimistic hypothesis, making matters worse.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Maybe you send that colleague a snarky text back. And what do you know: When you see them a few days later, they’re cold to you.&lt;/p&gt;
    &lt;p&gt;Now you aren’t talking. Maybe you flub that assignment your boss gave you, and they lose confidence in you. Fast-forward a year and you’re at a new job. Tensions are emerging with the new coworkers. Or are they? How will this story end? Do you have any control over it?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;When a core question is unsettled, it functions like a lens through which you see the world.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, you do. We all do. Negative spirals or feedback loops like these aren’t inevitable. In fact, there are small things we can do both for ourselves and for others to nip them in the bud—and prevent catastrophic outcomes months and years into the future. Better yet, there are ways we can launch positive spirals—dramatically increasing our chances of future happiness, success, and flourishing. The very same processes can either propel us upward or pull us down.&lt;/p&gt;
    &lt;p&gt;To understand how all this is possible, let’s get more precise about sequences like 1–4 above. There are three key concepts at play: “core questions” (number 1), meaning making or “construal” (numbers 2 and 3), and “calcification” (number 4). Think of these as “the three Cs” of spirals—whether positive or negative.&lt;/p&gt;
    &lt;p&gt;Core questions. There are the fundamental questions all of us face, at one time or another. For example: Who am I? Do I belong? Am I enough? I think of these questions as “defining” because they help define you and your life: your sense of self, what relationships you’ll have, and whether you’ll be able to do and be the things you aspire to. There might be long stretches when you don’t think about a given question much because it’s settled for you then. But at critical junctures specific questions flare up, unsettle and preoccupy you. Then they begin to shape what you see and how you act.&lt;/p&gt;
    &lt;p&gt;Construal. It’s natural to think that we have an unfiltered view of the world. That light hits your eyes and you just see what’s out there. But it’s more that we read the world, interpret it, drawing inferences based on what’s already in our heads. We pick up on themes that seem relevant or important to us, not noticing or screening out other details.&lt;/p&gt;
    &lt;p&gt;A friend once told me of an ingenious class demonstration that helped her begin to understand this process. A professor split the class in two and then spoke to the first half alone, telling them of his love for travel and a recent trip to Libya. Next, he spoke to the second half about shopping and how hard it was to find the right size shoe. Last, he brought the class together and said a single word. He asked the students to write it down. Students in the first group wrote, “Tripoli.” Those in the second wrote, “Triple E.”&lt;/p&gt;
    &lt;p&gt;Construal is like a kind of focus. As you look out at the social scene, what snaps to attention? If you’re anything like me, one of the most powerful guides is whatever could pose a risk to you, could threaten you. If you’re walking through a forest where a tiger is said to prowl, you might hear that tiger in every rustle of leaves, see it in every sway of reeds. But in the social world, we don’t all face the same threats. That’s why when you’re new at work and nervous about your place you might hear snark in your boss’s voice, but not if it’s your old friend.&lt;/p&gt;
    &lt;p&gt;When a core question is unsettled for a person, it functions like a lens through which you see the world. We seek answers that can help us resolve that question. Is it true? we ask. Are my doubts and fears well founded? Then, if a “bad” thing happens, it can seem like proof of your negative hypothesis. We aren’t neutral observers on the lookout for evidence one way or the other. We’re in the grip of confirmation bias, attuned to evidence that corroborates our preconceived theory, even if it’s the tiniest thing.&lt;/p&gt;
    &lt;p&gt;Calcification. Calcification happens when our negative thoughts and feelings get entrenched—often as a consequence of our own actions. You have a bad date and think, Am I unlovable? Will I be alone forever? Pretty soon your next date isn’t going well either. Rinse and repeat long enough, and you’re stuck in a romantic rut.&lt;/p&gt;
    &lt;p&gt;When you start to look, you can see spirals everywhere. You fail an important math test. You think you can’t succeed, and stop going to class. You feel sick from a treatment designed to help you overcome an illness. You think it means your illness is especially strong and resistant and so avoid treatment. You have a fight with your kids. You think you’re a “bad parent,” and then yell at them even more the next time. This is self-sabotage, and one step at a time it costs us our achievements, our health, our relationships, and our well-being.&lt;/p&gt;
    &lt;p&gt;Spiraling up&lt;/p&gt;
    &lt;p&gt;Yet if our struggles arise, in part, from the inferences we draw, we have an opportunity. In my work, my colleagues and I identify early moments where people could go one way or the other. By understanding the questions that come up at critical junctures, we can offer people better ways to think through challenges—ways that can help them spiral up, instead of down.&lt;/p&gt;
    &lt;p&gt;That’s what we call “wise” interventions: graceful ways to offer people good answers to the questions that define our lives. It sure can seem like magic that 21 minutes could improve marriage a year later; that a one-page letter could keep kids out of jail; that a string of postcards could cut suicide rates by half over two years; or that an hour-long reflection on belonging in the first year of college could improve life satisfaction and career success a decade later. But this—this is ordinary magic.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Negative spirals or feedback loops aren’t inevitable. There are things we can do both for ourselves and for others to nip them in the bud.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In my first year of college, I was biking back through campus one lovely fall day when I saw a large group of fellow students gathered enthusiastically around a truck from the California burger chain In-N-Out. Maybe they craved a taste of home. But in Michigan, where I was from, there are no In-N-Outs. I’d never heard of it. Feeling excluded from the burger party, I biked off in a huff to eat my lunch in the dining hall alone. I remember thinking, I’m not standing in line for a burger!&lt;/p&gt;
    &lt;p&gt;What was my problem?&lt;/p&gt;
    &lt;p&gt;As an 18-year-old, I certainly didn’t want to think of myself as feeling that I did not belong in college. And I definitely didn’t want to think that an In-N-Out truck could trigger that feeling. How ridiculous that would be. Who thinks they don’t belong because of a burger truck?&lt;/p&gt;
    &lt;p&gt;It was ridiculous. After my brother experienced a particularly mysterious romantic disaster, it’s something we christened a “tifbit”—tiny fact, big theory. Of course not knowing about In-N-Out didn’t mean I didn’t belong in college. But that’s the point. For looking back now, I know the truth is I was homesick. I felt so far from home and all the people I knew and loved. So I wondered, Will I make friends in California? Will I fit in? Seeing all those classmates crowded together, eager to get lunch from a place I’d never even heard of, just triggered those anxieties.&lt;/p&gt;
    &lt;p&gt;With wisdom and kindness and a little distance, we can laugh at ourselves in situations like these. But we should pay attention. For beneath every tifbit is a real question, and it’s almost always a reasonable one. Big responses to small experiences can help us see what lies beneath the surface. For a tifbit is never just a tiny fact. It’s a clue to the bigger questions that define our lives.&lt;/p&gt;
    &lt;p&gt;With a little prompt, I could have known that almost everyone feels homesick at first in college, that we’re all in some sense far from home, even the kids from California, that everyone was trying to find new communities. Maybe then I would have joined the line at the In-N-Out truck. I could have asked someone to tell me what In-N-Out was. Why do they love it? What is “animal style”?&lt;/p&gt;
    &lt;p&gt;I’m sure they would have been glad to share. I know I would have had a better lunch. And maybe I would have made a friend, too.&lt;/p&gt;
    &lt;p&gt;Excerpted from Ordinary Magic copyright © 2025 by Gregory M. Walton. Used by permission of Harmony Books, an imprint of Random House, a division of Penguin Random House LLC, New York. All rights reserved. No part of this excerpt may be reproduced or reprinted without permission in writing from the publisher.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45240146</guid><pubDate>Sun, 14 Sep 2025 14:46:58 +0000</pubDate></item><item><title>Writing an operating system kernel from scratch</title><link>https://popovicu.com/posts/writing-an-operating-system-kernel-from-scratch/</link><description>&lt;doc fingerprint="b60e921211a1b64c"&gt;
  &lt;main&gt;
    &lt;p&gt;I recently implemented a minimal proof of concept time-sharing operating system kernel on RISC-V. In this post, I’ll share the details of how this prototype works. The target audience is anyone looking to understand low-level system software, drivers, system calls, etc., and I hope this will be especially useful to students of system software and computer architecture.&lt;/p&gt;
    &lt;p&gt;This is a redo of an exercise I did for my undergraduate course in operating systems, and functionally it should resemble a typical operating systems project. However, this experiment focuses on modern tooling, as well as the modern architecture of RISC-V. RISC-V is an amazing technology that is easy to understand more quickly than other CPU architectures, while remaining a popular choice for many new systems, not just an educational architecture.&lt;/p&gt;
    &lt;p&gt;Finally, to do things differently here, I implemented this exercise in Zig, rather than traditional C. In addition to being an interesting experiment, I believe Zig makes this experiment much more easily reproducible on your machine, as it’s very easy to set up and does not require any installation (which could otherwise be slightly messy when cross-compiling to RISC-V).&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;GitHub repo&lt;/head&gt;
    &lt;p&gt;The final code for this experiment is on GitHub here. We’ll be referencing the code from it as we go.&lt;/p&gt;
    &lt;p&gt;GitHub should be the source of truth and may be slightly out of sync with the code below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Recommended reading&lt;/head&gt;
    &lt;p&gt;The basic fundamentals of computer engineering and specifically computer architecture are assumed. Specifically, knowledge of registers, how the CPU addresses memory, and interrupts is all necessary.&lt;/p&gt;
    &lt;p&gt;Before diving deep into this experiment, it’s recommended to also review the following background texts:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bare metal programming on RISC-V&lt;/item&gt;
      &lt;item&gt;RISC-V boot process with SBI&lt;/item&gt;
      &lt;item&gt;RISC-V interrupts with a timer example&lt;/item&gt;
      &lt;item&gt;Optional - Making a micro Linux distro - mainly for the brief philosophy on the kernel / user space split&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Unikernel&lt;/head&gt;
    &lt;p&gt;We’ll be developing a type of unikernel. Simply put, this setup links the application code directly with the OS kernel it depends on. Essentially, everything is bundled into a single binary executable, and the user code is loaded into memory alongside the kernel.&lt;/p&gt;
    &lt;p&gt;This bypasses the need to separately load the user code at runtime, which is a complex field in itself (involving linkers, loaders, etc.).&lt;/p&gt;
    &lt;head rend="h2"&gt;SBI layer&lt;/head&gt;
    &lt;p&gt;RISC-V supports a layered permissions model. The system boots into machine mode (M), which is completely bare-metal, and then supports a couple of other less privileged modes. Please check the background texts for more details; below is a quick summary:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;M-mode can do pretty much anything; it is fully bare-metal.&lt;/item&gt;
      &lt;item&gt;In the middle is S-mode, supervisor, which typically hosts the operating system kernel.&lt;/item&gt;
      &lt;item&gt;At the bottom is U-mode, user, where application code runs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Lower privilege levels can send requests to higher privilege levels.&lt;/p&gt;
    &lt;p&gt;We’ll assume that at the bottom of our software stack is an SBI layer, specifically OpenSBI. Please study this text for the necessary background, as we’ll use the SBI layer to manage console printing and control the timer hardware. While manual implementation is possible, I wanted to add more value to this text by demonstrating a more portable approach with OpenSBI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Goal for the kernel&lt;/head&gt;
    &lt;p&gt;We want to support a few key features for simplicity:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Statically define threads ahead of execution; i.e., dynamic thread creation is not supported. Additionally, for simplicity, threads are implemented as never-ending functions.&lt;/item&gt;
      &lt;item&gt;Threads operate in user mode and are able to send system calls to the kernel operating in S-mode.&lt;/item&gt;
      &lt;item&gt;Time is sliced and allocated among different threads. The system timer will be set to tick every couple of milliseconds, at which point a thread may be switched out.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Finally, development is targeted for a single-core machine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Virtualization and what exactly is a thread&lt;/head&gt;
    &lt;p&gt;Before implementing threads, we should decide what they really are. The concept of threads in a time-sharing environment enables multiple workloads to run on a single core (as noted above, we’re focusing on single-core machines), while the programming model for each thread remains largely the same as if it were the sole software on the machine. This is a loose definition, which we will refine.&lt;/p&gt;
    &lt;p&gt;To understand time-sharing, let’s briefly consider its contrast: cooperative scheduling/threading. In cooperative scheduling/threading, a thread voluntarily yields CPU time to another workload. Eventually, the expectation is that another thread will yield control back to the first.&lt;/p&gt;
    &lt;code&gt;function thread():
  operation_1();
  operation_2();
  YIELD();
  operation_3();
  YIELD();
  ...&lt;/code&gt;
    &lt;p&gt;To be clear, this isn’t an “outdated” technique, despite being older. In fact, it’s alive and well in many modern programming languages and their runtimes (often abstracted from programmers). One good example is Go, which uses Goroutines to run multiple workloads on top of one operating system thread. While programmers don’t necessarily add explicit yield operations, the compiler and runtime can inject them into the workload.&lt;/p&gt;
    &lt;p&gt;Now, it should be clearer what it means for the programming model to remain largely the same in a time-sharing context. The thread would naturally look like this:&lt;/p&gt;
    &lt;code&gt;function thread():
  operation_1();
  operation_2();
  operation_3();
  ...&lt;/code&gt;
    &lt;p&gt;There are simply no explicit yield operations; instead, the kernel utilizes timers and interrupts to seamlessly switch between threads on the same core. This is precisely what we’ll implement in this experiment.&lt;/p&gt;
    &lt;p&gt;When multiple workloads run on the same resource, and each retains the same programming model as if it were the only workload, we can say the resource is virtualized. In other words, if we’re running 5 threads on the same core, each thread “feels” like it has its own core, effectively running on 5 little cores instead of 1 big core. More formally, each thread retains its own view of the core’s architectural registers (in RISC-V, &lt;code&gt;x0-x31&lt;/code&gt; and some CSRs, more on this below) and… some memory! Let’s look deeper into that.&lt;/p&gt;
    &lt;head rend="h3"&gt;The stack and memory virtualization&lt;/head&gt;
    &lt;p&gt;To begin, a thread has its own stack for reasons we’ll analyze shortly. The rest of the memory is “shared” with other threads, but this requires further investigation.&lt;/p&gt;
    &lt;p&gt;It’s important to understand that hardware virtualization exists on a spectrum, rather than as a few rigid options. Here are some of the options for virtualization:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Threads: virtualizes architectural registers and stacks, but not much else; i.e., different threads can share data elsewhere in memory.&lt;/item&gt;
      &lt;item&gt;Process: more heavyweight than threads, memory is virtualized such that each process “feels” like it has a dedicated CPU core and its own memory untouchable by other processes; additionally, a process houses multiple threads.&lt;/item&gt;
      &lt;item&gt;Container: virtualizes even more - each container has its own filesystem and potentially its own set of network interfaces; containers share the same kernel and underlying hardware.&lt;/item&gt;
      &lt;item&gt;VM: virtualizes everything.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are many more shades in between, and each of these options likely has different subtypes. The point here is that all these approaches enable running different workloads with varying isolations, or more intuitively, different views of the machine and their environment.&lt;/p&gt;
    &lt;p&gt;Interestingly, if you examine the Linux kernel source code, you won’t find a construct explicitly called a container. What we popularly call containers isn’t a mechanism baked into the kernel, but rather a set of kernel mechanisms used together to form a specific view of the environment for our workload. For example, the &lt;code&gt;chroot&lt;/code&gt; mechanism restricts filesystem visibility, while &lt;code&gt;cgroups&lt;/code&gt; impose limits on workloads; together, these form what we call a container.&lt;/p&gt;
    &lt;p&gt;Furthermore, I believe (though don’t quote me on this) that the boundaries between threads and processes in Linux are somewhat blurred. To the best of my knowledge, both are implemented on top of tasks in the kernel, but when creating a task, the API allows different restrictions to be specified.&lt;/p&gt;
    &lt;p&gt;Ultimately, this is all to say that we’re always defining a workload with varying restrictions on what it can see and access. When and why to apply different restrictions is a topic for another day. Many questions arise when writing an application, ranging from the difficulty of an approach to its security.&lt;/p&gt;
    &lt;head rend="h3"&gt;Virtualizing a thread&lt;/head&gt;
    &lt;p&gt;In this experiment, we’ll implement minimal virtualization with very basic, time-sharing threads. Therefore, the goals are the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The programming model for a thread should remain mostly untouched. As long as a thread doesn’t interact with memory contents used by other threads, its programming model should remain consistent, powered by time-sharing.&lt;/item&gt;
      &lt;item&gt;A thread should have its own protected view of architectural registers, including some RISC-V CSRs.&lt;/item&gt;
      &lt;item&gt;A thread should be assigned its own stack.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It should be obvious why a thread needs its own view of the registers. If other threads could freely touch a thread’s registers, the thread wouldn’t be able to do any meaningful work. All (I believe) RISC-V instructions work with at least one register, so protecting a thread’s register view is essential.&lt;/p&gt;
    &lt;p&gt;Furthermore, assigning a private stack to a thread is necessary, though slightly less obvious. The answer is that different stacks are needed to manage different execution contexts. Namely, when a function is invoked, by convention, the stack is used to allocate function-private variables. Additionally, registers like &lt;code&gt;ra&lt;/code&gt; can be pushed to the stack to retain the correct return address from a function (in case another function is invoked within it). In short, there are various reasons, per RISC-V convention, why the stack is needed to maintain the execution context. The details of RISC-V calling conventions will not be described here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Interrupt context&lt;/head&gt;
    &lt;p&gt;It’s crucial to understand how interrupt code runs and what it should consist of, as this mechanism will be heavily exploited to achieve seamless time-sharing between threads. For a detailed, practical example, please check out this past text.&lt;/p&gt;
    &lt;p&gt;I’ll briefly include the assembly for the timer interrupt routine from that text:&lt;/p&gt;
    &lt;code&gt;s_mode_interrupt_handler:
        addi    sp,sp,-144
        sd      ra,136(sp)
        sd      t0,128(sp)
        sd      t1,120(sp)
        sd      t2,112(sp)
        sd      s0,104(sp)
        sd      a0,96(sp)
        sd      a1,88(sp)
        sd      a2,80(sp)
        sd      a3,72(sp)
        sd      a4,64(sp)
        sd      a5,56(sp)
        sd      a6,48(sp)
        sd      a7,40(sp)
        sd      t3,32(sp)
        sd      t4,24(sp)
        sd      t5,16(sp)
        sd      t6,8(sp)
        addi    s0,sp,144
        call    clear_timer_pending_bit
        call    set_timer_in_near_future
        li      a1,33
        lla     a0,.LC0
        call    debug_print
        nop
        ld      ra,136(sp)
        ld      t0,128(sp)
        ld      t1,120(sp)
        ld      t2,112(sp)
        ld      s0,104(sp)
        ld      a0,96(sp)
        ld      a1,88(sp)
        ld      a2,80(sp)
        ld      a3,72(sp)
        ld      a4,64(sp)
        ld      a5,56(sp)
        ld      a6,48(sp)
        ld      a7,40(sp)
        ld      t3,32(sp)
        ld      t4,24(sp)
        ld      t5,16(sp)
        ld      t6,8(sp)
        addi    sp,sp,144
        sret&lt;/code&gt;
    &lt;p&gt;This assembly was obtained by writing a C function tagged as an S-level interrupt in RISC-V. With this tag, the GCC compiler knew how to generate the prologue and epilogue of the interrupt routine. The prologue preserves architectural registers on the stack, and the epilogue recovers them (in addition to specifically returning from S-mode). All of this was generated by correctly tagging the C function’s invoking convention.&lt;/p&gt;
    &lt;p&gt;This somewhat resembles function calling, and that’s essentially what it is. Interrupts can be thought of (in a very simplified sense) as functions invoked by some system effect. Consequently, utilized registers must be carefully preserved on the stack and then restored at the routine’s exit; otherwise, asynchronous interrupts like timer interrupts would randomly corrupt architectural register values, completely blocking any practical software from running!&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation (high-level)&lt;/head&gt;
    &lt;p&gt;We’ll explore the implementation by first describing the high-level idea and then digging into the code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Leveraging the interrupt stack convention&lt;/head&gt;
    &lt;p&gt;Adding an interrupt is, in a way, already introducing a form of threading to your application code. In a system with a timer interrupt, the main application code runs, which can occasionally be interleaved with instances of timer interrupt invocations. The core jumps to this interrupt routine when the timer signals, and it carefully restores the architectural state before control flow returns to the “main thread”. There are two control flows running concurrently here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Main application code.&lt;/item&gt;
      &lt;item&gt;Repetitions of the interrupt routine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This interleaving of the timer interrupt can be leveraged to implement additional control flows, and the main idea is outlined below.&lt;/p&gt;
    &lt;p&gt;The core of the interrupt routine is sandwiched between the prologue and the epilogue. That’s where the interrupt is serviced before control returns to the main application thread by restoring registers from the stack.&lt;/p&gt;
    &lt;p&gt;However, why must we restore the registers from the same stack location? If our interrupt logic swaps the stack pointer to some other piece of memory, we’ll end up with a different set of architectural register values recovered, thus entering a whole different flow. In other words, we achieve a context switch, and this is precisely how it’s implemented in this experiment. We’ll see the code for it shortly.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kernel/user space separation&lt;/head&gt;
    &lt;p&gt;We can now delineate the kernel space and user space. With RISC-V, this naturally translates to kernel code running in supervisor (S) mode and user space code running in U-mode.&lt;/p&gt;
    &lt;p&gt;The machine boots into machine (M) mode, and since we want to leverage the SBI layer, we’ll allow OpenSBI to run there. Then, the kernel will perform some initial setup in S-mode before starting the U-mode execution of user space threads. Periodic timer interrupts will enable context switches, and the interrupt code will execute in S-mode. Finally, user threads will be able to make system calls to the kernel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementation (code)&lt;/head&gt;
    &lt;p&gt;Please refer to the GitHub repository for the full code; we will only cover core excerpts below.&lt;/p&gt;
    &lt;head rend="h3"&gt;Assembly startup&lt;/head&gt;
    &lt;p&gt;As usual, a short assembly snippet is needed to start our S-mode code and enter the “main program” in Zig. This is in &lt;code&gt;startup.S&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;...
done_bss:

    # Jump to Zig main
    call main
...&lt;/code&gt;
    &lt;p&gt;The rest of the assembly startup primarily involves cleaning up the BSS section and setting up the stack pointer for the initial kernel code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Main kernel file and I/O drivers&lt;/head&gt;
    &lt;p&gt;We’ll now examine &lt;code&gt;kernel.zig&lt;/code&gt;, which contains the &lt;code&gt;main&lt;/code&gt; function.&lt;/p&gt;
    &lt;p&gt;First, we probe the OpenSBI layer for console capabilities. We’ll only consider running on a relatively recent version of OpenSBI (from the last few years) that includes console capability. Otherwise, the kernel will halt and report an error.&lt;/p&gt;
    &lt;code&gt;export fn main() void {
    const initial_print_status = sbi.debug_print(BOOT_MSG);

    if (initial_print_status.sbi_error != 0) {
        // SBI debug console not available, fall back to direct UART
        const error_msg = "ERROR: OpenSBI debug console not available! You need the latest OpenSBI.\n";
        const fallback_msg = "Falling back to direct UART at 0x10000000...\n";

        uart.uart_write_string(error_msg);
        uart.uart_write_string(fallback_msg);
        uart.uart_write_string("Stopping... We rely on OpenSBI, cannot continue.\n");

        while (true) {
            asm volatile ("wfi");
        }

        unreachable;
    }&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;main&lt;/code&gt; is marked as &lt;code&gt;export&lt;/code&gt; to conform to the C ABI.&lt;/p&gt;
    &lt;p&gt;Here, we have a lightweight implementation of a couple of I/O drivers. As you can see, writing can occur in one of two ways: either we go through the SBI layer (&lt;code&gt;sbi.zig&lt;/code&gt;) or, if that fails, we use direct MMIO (&lt;code&gt;uart_mmio.zig&lt;/code&gt;). The SBI method should theoretically be more portable, as it delegates output management details to the M-level layer (essentially what we do with MMIO), freeing us from concerns about exact memory space addresses.&lt;/p&gt;
    &lt;p&gt;Let’s quickly look at &lt;code&gt;sbi.zig&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// Struct containing the return status of OpenSBI
pub const SbiRet = struct {
    sbi_error: isize,
    value: isize,
};

pub fn debug_print(message: []const u8) SbiRet {
    var err: isize = undefined;
    var val: isize = undefined;

    const msg_ptr = @intFromPtr(message.ptr);
    const msg_len = message.len;

    asm volatile (
        \\mv a0, %[len]
        \\mv a1, %[msg]
        \\li a2, 0
        \\li a6, 0x00
        \\li a7, 0x4442434E
        \\ecall
        \\mv %[err], a0
        \\mv %[val], a1
        : [err] "=r" (err),
          [val] "=r" (val),
        : [msg] "r" (msg_ptr),
          [len] "r" (msg_len),
        : .{ .x10 = true, .x11 = true, .x12 = true, .x16 = true, .x17 = true, .memory = true });

    return SbiRet{
        .sbi_error = err,
        .value = val,
    };
}&lt;/code&gt;
    &lt;p&gt;This is very straightforward; we’re simply performing the system call exactly as described in the OpenSBI documentation. Note that when I first wrote this code, I wasn’t fully familiar with Zig’s error handling capabilities, hence the somewhat non-idiomatic error handling.&lt;/p&gt;
    &lt;p&gt;However, this can be considered a first driver in this kernel, as it directly manages output to the device.&lt;/p&gt;
    &lt;p&gt;Next is &lt;code&gt;uart_mmio.zig&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;// UART MMIO address (standard for QEMU virt machine)
pub const UART_BASE: usize = 0x10000000;
pub const UART_TX: *volatile u8 = @ptrFromInt(UART_BASE);

// Direct UART write function (fallback when SBI is not available)
pub fn uart_write_string(message: []const u8) void {
    for (message) |byte| {
        UART_TX.* = byte;
    }
}&lt;/code&gt;
    &lt;p&gt;This is straightforward and self-explanatory.&lt;/p&gt;
    &lt;p&gt;Returning to &lt;code&gt;kernel.zig&lt;/code&gt; and the &lt;code&gt;main&lt;/code&gt; function, we create 3 user threads, each printing a slightly different message (the thread ID is the varying bit). At this point, the kernel setup is almost complete.&lt;/p&gt;
    &lt;p&gt;The final steps involve setting up and running the timer interrupt. Once that is done, kernel code will only run when the timer interrupts the system or when user space code requests a system call.&lt;/p&gt;
    &lt;code&gt;interrupts.setup_s_mode_interrupt(&amp;amp;s_mode_interrupt_handler);
_ = timer.set_timer_in_near_future();
timer.enable_s_mode_timer_interrupt();&lt;/code&gt;
    &lt;p&gt;We could request a context switch immediately, but for simplicity, we’ll wait until the timer activates and begins the actual work in the system.&lt;/p&gt;
    &lt;head rend="h3"&gt;S-mode handler and the context switch&lt;/head&gt;
    &lt;p&gt;While the Zig compiler could generate the adequate prologue and epilogue for our S-mode handler, we will do it manually. The reason is that we also want to capture some CSRs in the context that otherwise wouldn’t have been captured by the generated routine.&lt;/p&gt;
    &lt;p&gt;That’s why we use the &lt;code&gt;naked&lt;/code&gt; calling convention in Zig. This forces us to write the entire function in assembly, though a quick escape hatch to this limitation is to call a Zig function whenever Zig logic is needed.&lt;/p&gt;
    &lt;p&gt;I won’t copy paste the whole prologue and epilogue here because they are very similar to what was done in the previous C experiment with RISC-V interrupts. Instead, I’ll just focus on the bit that is different:&lt;/p&gt;
    &lt;code&gt;...
        // Save S-level CSRs (using x5 as a temporary register)
        \\csrr x5, sstatus
        \\sd x5, 240(sp)
        \\csrr x5, sepc
        \\sd x5, 248(sp)
        \\csrr x5, scause
        \\sd x5, 256(sp)
        \\csrr x5, stval
        \\sd x5, 264(sp)

        // Call handle_kernel
        \\mv a0, sp
        \\call handle_kernel
        \\mv sp, a0

        // Epilogue: Restore context
        // Restore S-level CSRs (using x5 as a temporary register)
        \\ld x5, 264(sp)
        \\csrw stval, x5
        \\ld x5, 256(sp)
        \\csrw scause, x5
        \\ld x5, 248(sp)
        \\csrw sepc, x5
        \\ld x5, 240(sp)
        \\csrw sstatus, x5
...&lt;/code&gt;
    &lt;p&gt;As you can see, a couple more registers were added to the prologue and epilogue in addition to the core architectural registers.&lt;/p&gt;
    &lt;p&gt;Next, within this prologue/epilogue sandwich, we invoke the &lt;code&gt;handle_kernel&lt;/code&gt; Zig function. This routes to the correct logic based on whether the interrupt source is a synchronous system call from user space or an asynchronous timer interrupt. The reason is that we land in the same S-level interrupt routine regardless of the interrupt source, and then we inspect the &lt;code&gt;scause&lt;/code&gt; CSR for details.&lt;/p&gt;
    &lt;p&gt;To successfully work with the &lt;code&gt;handle_kernel&lt;/code&gt; function, we need to be aware of the assembly-level calling conventions. This function takes a single integer parameter and returns a single integer parameter. Since the function signature is small, it works as simply as this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The sole function parameter is passed through the &lt;code&gt;a0&lt;/code&gt;architectural register.&lt;/item&gt;
      &lt;item&gt;The same register also holds the function’s result upon return.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is pretty easy. Let’s quickly look at the signature of this function:&lt;/p&gt;
    &lt;code&gt;export fn handle_kernel(current_stack: usize) usize {
...&lt;/code&gt;
    &lt;p&gt;It is slightly awkward but gets the job done. The input to this Zig logic is the stack top before invoking the Zig logic (which inevitably leads to some data added to the stack). The function’s output is where the stack top should be after the Zig logic is done. If it differs from the input, then we’re performing a context switch. If it’s the same, the same workload thread will continue running after the interrupt.&lt;/p&gt;
    &lt;p&gt;The rest of the logic is very simple. It inspects the interrupt source (system call from user space or timer interrupt) and performs accordingly.&lt;/p&gt;
    &lt;p&gt;In the case of a timer interrupt, a context switch is performed. The &lt;code&gt;schedule&lt;/code&gt; function from &lt;code&gt;scheduling.zig&lt;/code&gt; is invoked, and it potentially returns the other stack we should switch to:&lt;/p&gt;
    &lt;code&gt;const build_options = @import("build_options");
const sbi = @import("sbi");
const std = @import("std");
const thread = @import("thread");

pub fn schedule(current_stack: usize) usize {
    const maybe_current_thread = thread.getCurrentThread();

    if (maybe_current_thread) |current_thread| {
        current_thread.sp_save = current_stack;

        if (comptime build_options.enable_debug_logs) {
            _ = sbi.debug_print("[I] Enqueueing the current thread\n");
        }
        thread.enqueueReady(current_thread);
    } else {
        if (comptime build_options.enable_debug_logs) {
            _ = sbi.debug_print("[W] NO CURRENT THREAD AVAILABLE!\n");
        }
    }

    const maybe_new_thread = thread.dequeueReady();

    if (maybe_new_thread) |new_thread| {
        // TODO: software interrupt to yield to the user thread

        if (comptime build_options.enable_debug_logs) {
            _ = sbi.debug_print("Yielding to the new thread\n");
        }

        thread.setCurrentThread(new_thread);

        if (comptime build_options.enable_debug_logs) {
            var buffer: [256]u8 = undefined;
            const content = std.fmt.bufPrint(&amp;amp;buffer, "New thread ID: {d}, stack top: {x}\n", .{ new_thread.id, new_thread.sp_save }) catch {
                return 0; // Return bogus stack, should be more robust in reality
            };
            _ = sbi.debug_print(content);
        }

        return new_thread.sp_save;
    }

    _ = sbi.debug_print("NO NEW THREAD AVAILABLE!\n");

    while (true) {
        asm volatile ("wfi");
    }
    unreachable;
}&lt;/code&gt;
    &lt;p&gt;The code from the &lt;code&gt;thread&lt;/code&gt; module is very simple, serving as boilerplate for a basic queue that manages structs representing threads. I won’t copy it here, as it’s mostly AI-generated. It is important to note, however, that the stacks are statically allocated in memory, and the maximum number of running threads is hardcoded.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;thread&lt;/code&gt; module also includes logic for setting up a new thread. This is where data is pushed onto the stack before the thread even runs. If you wonder why, it’s because when returning from the S-level trap handler, we need something on the stack to indicate where to go. The initial data does precisely that. We can seed the initial register values here as desired. In fact, in this experiment, we demonstrate passing a single integer parameter to the thread function by seeding the &lt;code&gt;a0&lt;/code&gt; register value (per calling convention) on the stack, which the thread function can then use immediately.&lt;/p&gt;
    &lt;head rend="h3"&gt;The user space threads&lt;/head&gt;
    &lt;p&gt;As mentioned in the introduction, we’ll bundle the user space and kernel space code into a single binary blob to avoid dynamic loading, linking, and other complexities. Hence, our user space code consists of regular functions:&lt;/p&gt;
    &lt;code&gt;/// Example: Create a simple idle thread
pub fn createPrintingThread(thread_number: usize) !*Thread {
    const thread = allocThread() orelse return error.NoFreeThreads;

    // Idle thread just spins
    const print_fn = struct {
        fn print(thread_arg: usize) noreturn {
            while (true) {
                var buffer: [256]u8 = undefined;
                const content = std.fmt.bufPrint(&amp;amp;buffer, "Printing from thread ID: {d}\n", .{thread_arg}) catch {
                    continue;
                };

                syscall.debug_print(content);

                // Simulate a delay
                var i: u32 = 0;
                while (i &amp;lt; 300000000) : (i += 1) {
                    asm volatile ("" ::: .{ .memory = true }); // Memory barrier to prevent optimization
                }
            }
            unreachable;
        }
    }.print;

    initThread(thread, @intFromPtr(&amp;amp;print_fn), thread_number);
    return thread;
}&lt;/code&gt;
    &lt;p&gt;Additionally, as mentioned above, we pre-seeded the stack such that when &lt;code&gt;a0&lt;/code&gt; is recovered from the stack upon the first interrupt return for a given thread, the function argument will be picked up. That’s how the &lt;code&gt;print&lt;/code&gt; function accesses the &lt;code&gt;thread_arg&lt;/code&gt; value and uses it in its logic.&lt;/p&gt;
    &lt;p&gt;To demonstrate the user/kernel boundary, we have &lt;code&gt;syscall.debug_print(content);&lt;/code&gt;. This conceptually behaves more or less as &lt;code&gt;printf&lt;/code&gt; from &lt;code&gt;stdio.h&lt;/code&gt; in C. It performs prepares the arguments to the kernel and runs a system call with these arguments which should lead to some content getting printed on the output device. Here’s what the printing library looks like (from &lt;code&gt;syscall.zig&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;// User-level debug_print function
pub fn debug_print(message: []const u8) void {
    const msg_ptr = @intFromPtr(message.ptr);
    const msg_len = message.len;

    // Let's say syscall number 64
    // a7 = syscall number
    // a0 = message pointer
    // a1 = message length
    asm volatile (
        \\mv a0, %[msg]
        \\mv a1, %[len]
        \\li a7, 64
        \\ecall
        :
        : [msg] "r" (msg_ptr),
          [len] "r" (msg_len),
        : .{ .x10 = true, .x11 = true, .x17 = true, .memory = true });

    // Ignore return value for simplicity
}&lt;/code&gt;
    &lt;p&gt;System call 64 is served from the S-mode handler in &lt;code&gt;kernel.zig&lt;/code&gt;. This is self-explanatory, and we won’t go into further details here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Running the kernel&lt;/head&gt;
    &lt;p&gt;We will deploy the kernel on bare-metal, specifically on a virtual machine. In theory, this should also work on a real machine, provided an SBI layer is present when the kernel starts, and the linker script, I/O “drivers,” and other machine-specific constants are adapted.&lt;/p&gt;
    &lt;p&gt;To build, we simply run&lt;/p&gt;
    &lt;code&gt;zig build&lt;/code&gt;
    &lt;p&gt;To now run the kernel, we run:&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -nographic -bios /tmp/opensbi/build/platform/generic/firmware/fw_dynamic.bin -kernel zig-out/bin/kernel&lt;/code&gt;
    &lt;p&gt;Refer to the previous text on OpenSBI for details on building OpenSBI. It is strongly recommended to use a freshly built OpenSBI, as QEMU may use an outdated version if no &lt;code&gt;-bios&lt;/code&gt; flag is passed.&lt;/p&gt;
    &lt;p&gt;The output should begin with a big OpenSBI splash along with some OpenSBI data:&lt;/p&gt;
    &lt;code&gt;OpenSBI v1.7
   ____                    _____ ____ _____
  / __ \                  / ____|  _ \_   _|
 | |  | |_ __   ___ _ __ | (___ | |_) || |
 | |  | | '_ \ / _ \ '_ \ \___ \|  _ &amp;lt; | |
 | |__| | |_) |  __/ | | |____) | |_) || |_
  \____/| .__/ \___|_| |_|_____/|____/_____|
        | |
        |_|

Platform Name               : riscv-virtio,qemu
Platform Features           : medeleg
Platform HART Count         : 1
Platform IPI Device         : aclint-mswi
Platform Timer Device       : aclint-mtimer @ 10000000Hz
Platform Console Device     : uart8250
Platform HSM Device         : ---
Platform PMU Device         : ---
Platform Reboot Device      : syscon-reboot
Platform Shutdown Device    : syscon-poweroff
Platform Suspend Device     : ---
Platform CPPC Device        : ---
Firmware Base               : 0x80000000
Firmware Size               : 317 KB
Firmware RW Offset          : 0x40000
Firmware RW Size            : 61 KB
Firmware Heap Offset        : 0x46000
Firmware Heap Size          : 37 KB (total), 2 KB (reserved), 11 KB (used), 23 KB (free)
Firmware Scratch Size       : 4096 B (total), 400 B (used), 3696 B (free)
Runtime SBI Version         : 3.0
Standard SBI Extensions     : time,rfnc,ipi,base,hsm,srst,pmu,dbcn,fwft,legacy,dbtr,sse
Experimental SBI Extensions : none

Domain0 Name                : root
....&lt;/code&gt;
    &lt;p&gt;Following the OpenSBI splash, we’ll see the kernel output:&lt;/p&gt;
    &lt;code&gt;Booting the kernel...
Printing from thread ID: 0
Printing from thread ID: 0
Printing from thread ID: 0
Printing from thread ID: 1
Printing from thread ID: 1
Printing from thread ID: 1
Printing from thread ID: 2
Printing from thread ID: 2
Printing from thread ID: 2
Printing from thread ID: 0
Printing from thread ID: 0
Printing from thread ID: 1
Printing from thread ID: 1
Printing from thread ID: 2
Printing from thread ID: 2
Printing from thread ID: 0
Printing from thread ID: 0
Printing from thread ID: 0
Printing from thread ID: 1
Printing from thread ID: 1
Printing from thread ID: 1
Printing from thread ID: 2
Printing from thread ID: 2
Printing from thread ID: 2&lt;/code&gt;
    &lt;p&gt;The prints will continue running until QEMU is terminated.&lt;/p&gt;
    &lt;p&gt;If you want to build the kernel in an extremely verbose mode for debugging and experimentation, use the following command:&lt;/p&gt;
    &lt;code&gt;zig build -Ddebug-logs=true&lt;/code&gt;
    &lt;p&gt;After running the kernel with the same QEMU command, the output will appear as follows:&lt;/p&gt;
    &lt;code&gt;Booting the kernel...
DEBUG mode on
Interrupt source: Timer, Current stack: 87cffe70
[W] NO CURRENT THREAD AVAILABLE!
Yielding to the new thread
New thread ID: 0, stack top: 80203030
Interrupt source: Ecall from User mode, Current stack: 80202ec0
Printing from thread ID: 0
Interrupt source: Ecall from User mode, Current stack: 80202ec0
Printing from thread ID: 0
Interrupt source: Ecall from User mode, Current stack: 80202ec0
Printing from thread ID: 0
Interrupt source: Timer, Current stack: 80202ec0
[I] Enqueueing the current thread
Yielding to the new thread
New thread ID: 1, stack top: 80205030
Interrupt source: Ecall from User mode, Current stack: 80204ec0
Printing from thread ID: 1
Interrupt source: Ecall from User mode, Current stack: 80204ec0
Printing from thread ID: 1
Interrupt source: Ecall from User mode, Current stack: 80204ec0
Printing from thread ID: 1
Interrupt source: Timer, Current stack: 80204ec0
[I] Enqueueing the current thread
Yielding to the new thread
New thread ID: 2, stack top: 80207030
Interrupt source: Ecall from User mode, Current stack: 80206ec0
Printing from thread ID: 2
Interrupt source: Ecall from User mode, Current stack: 80206ec0
Printing from thread ID: 2
Interrupt source: Ecall from User mode, Current stack: 80206ec0
Printing from thread ID: 2
Interrupt source: Timer, Current stack: 80206ec0
...&lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Many educational OS kernels exist, but this experiment combines RISC-V, OpenSBI, and Zig, offering a fresh perspective compared to traditional C implementations.&lt;/p&gt;
    &lt;p&gt;The resulting code runs on a QEMU virtual machine, which can be easily set up, even by building QEMU from source.&lt;/p&gt;
    &lt;p&gt;To keep the explanation concise, error reporting was kept minimal. Should you modify the code and require debugging, sufficient clues are provided, despite some areas where the code is simplified (e.g., anonymous results after SBI print invocations like &lt;code&gt;_ = ...&lt;/code&gt;). Much of the code in this example was AI-generated by Claude to save time, and it should function as intended. While some parts of the code are simplified, such as stack space over-allocation, these do not detract from the experiment’s educational value.&lt;/p&gt;
    &lt;p&gt;Overall, this experiment serves as a starting point for studying operating systems, assuming a foundational understanding of computer engineering and computer architecture. It likely has plenty of flaws for a practical application, but for now, we’re just hacking here!&lt;/p&gt;
    &lt;p&gt;I hope this was a useful exploration.&lt;/p&gt;
    &lt;p&gt;Please consider following on Twitter/X and LinkedIn to stay updated.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45240682</guid><pubDate>Sun, 14 Sep 2025 15:44:44 +0000</pubDate></item><item><title>OCSP Service Has Reached End of Life</title><link>https://letsencrypt.org/2025/08/06/ocsp-service-has-reached-end-of-life</link><description>&lt;doc fingerprint="e337cfd427579385"&gt;
  &lt;main&gt;
    &lt;p&gt;Today we turned off our Online Certificate Status Protocol (OCSP) service, as announced in December of last year. We stopped including OCSP URLs in our certificates more than 90 days ago, so all Let’s Encrypt certificates that contained OCSP URLs have now expired. Going forward, we will publish revocation information exclusively via Certificate Revocation Lists (CRLs).&lt;/p&gt;
    &lt;p&gt;We ended support for OCSP primarily because it represents a considerable risk to privacy on the Internet. When someone visits a website using a browser or other software that checks for certificate revocation via OCSP, the Certificate Authority (CA) operating the OCSP responder immediately becomes aware of which website is being visited from that visitor’s particular IP address. Even when a CA intentionally does not retain this information, as is the case with Let’s Encrypt, it could accidentally be retained or CAs could be legally compelled to collect it. CRLs do not have this issue.&lt;/p&gt;
    &lt;p&gt;We are also taking this step because keeping our CA infrastructure as simple as possible is critical for the continuity of compliance, reliability, and efficiency at Let’s Encrypt. For every year that we have existed, operating OCSP services has taken up considerable resources that can soon be better spent on other aspects of our operations. Now that we support CRLs, our OCSP service has become unnecessary.&lt;/p&gt;
    &lt;p&gt;At the height of our OCSP service’s traffic earlier this year, we handled approximately 340 billion OCSP requests per month. That’s more than 140,000 requests per second handled by our CDN, with 15,000 requests per second handled by our origin. We’d like to thank Akamai for generously donating CDN services for OCSP to Let’s Encrypt for the past ten years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45242591</guid><pubDate>Sun, 14 Sep 2025 19:34:48 +0000</pubDate></item><item><title>AMD Turin PSP binaries analysis from open-source firmware perspective</title><link>https://blog.3mdeb.com/2025/2025-09-11-gigabyte-mz33-ar1-blob-analysis/</link><description>&lt;doc fingerprint="bc4ac671c73409c5"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;In the previous post, we showed coreboot running on Gigabyte MZ33-AR1 with Turin CPU, the current, newest family of AMD server processors. However, we faced various obstacles and problems. Despite AMD publishing a set of blobs required for the Turin system initialization, they turned out to be not enough to release the CPU from reset by PSP. We were forced to do a workaround by injecting coreboot into the vendor firmware image and flashing it back. The whole process is far from ideal; thus, it forced us to perform an analysis, where we demystify and explain the problems and solutions we came up with.&lt;/p&gt;
    &lt;head rend="h2"&gt;AMD PSP firmware structure&lt;/head&gt;
    &lt;p&gt;Nowadays, the x86 CPUs are not the first entities that begin code execution after pressing the power button. The design of the processors and silicon overall drifted towards adding many co-processors, which perform a specialized subset of actions and have a very specific role in the system. For example: Intel Management Engine (ME) on Intel platforms and AMD Platform Security Processor (PSP), also known as AMD Security Processor (ASP). These co-processors run their own firmware, which is usually stored in the same flash memory as the BIOS for an x86 CPU. Often these firmwares contain other firmwares for yet another co-processors or IP blocks. This is true for both Intel and AMD. We will not dive into Intel specifics, but if you are curious, just open an Intel firmware image in UEFITool and expand the Intel ME region. You will see how many various applications or firmwares reside there.&lt;/p&gt;
    &lt;p&gt;The situation is no different on an AMD system, although the separation of x86 BIOS and PSP firmware/blobs is not as clean as on Intel systems. AMD PSP does not have any separate flash region for its own use. Instead, the PSP blobs are packed into specific directory structures, which you can read a bit about here.&lt;/p&gt;
    &lt;p&gt;To understand how it is supposed to work on the Turin system, we have to go through each structure of the PSP firmware and analyze it, starting with Embedded Firmware Structure (EFS), through PSP directories up to the BIOS directories.&lt;/p&gt;
    &lt;head rend="h2"&gt;Embedded Firmware Structure&lt;/head&gt;
    &lt;p&gt;Embedded Firmware Structure is like a header that indicates the location of PSP and BIOS directories. It is used by PSP during power-on to locate the blobs and configure certain properties of the system, e.g., SPI interface speeds, eSPI bus configuration, etc. The tool responsible for creating EFS, PSP and BIOS directories in coreboot are amdfwtool. The coreboot build system uses this utility during the build process to stitch all blobs together into a bootable image.&lt;/p&gt;
    &lt;p&gt;There has been some activity around this tool recently, which has enhanced its debugging and analysis capabilities:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;util/amdfwtool/amdfwread: add initial parsing for EFW structure&lt;/item&gt;
      &lt;item&gt;util/amdfwtool/amdfwread: fix offset decision for PSP/BIOS directory lookup&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Seeing this opportunity, I have reviewed and tested the patches, even added more information to be dumped, and fixed parsing of the images for Turin processors, to serve the purpose of my analysis:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;util/amdfwtool: Extend parsing of embedded firmware structures and dirs&lt;/item&gt;
      &lt;item&gt;util/amdfwtool: Handle address mode properly for Turin&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With those improvements in place, I was able to dump information about coreboot images as well as vendor images. When something does not work, one of the best and easiest ways is to compare a faulty case with a known good reference. And so by dumping the information on both images, we could spot some major differences, which potentially could cause the image to be unbootable.&lt;/p&gt;
    &lt;p&gt;Since the outputs from amdfwtool are quite long, I have added them as a paste:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;./util/amdfwtool/amdfwread -d --ro-list build/coreboot.rom&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;./util/amdfwtool/amdfwread -d --ro-list MZ33-AR1_R11_F08/SPI_UPD/image.bin&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The second dump comes from the latest BIOS update package &lt;code&gt;R11_F08&lt;/code&gt; for the
Gigabyte MZ33-AR1 which can be downloaded from the vendor’s site.&lt;/p&gt;
    &lt;p&gt;Right now, we are interested in everything that comes before &lt;code&gt;Table: FW Offset Size&lt;/code&gt; line, since it represents the EFS. We can see that many fields are
different. Also, we have to remember that vendor image is a dual BIOS for
Genoa and Turin platforms. &lt;code&gt;amdfwread&lt;/code&gt; prints only the contents of the first
BIOS image (first 16MB). So the &lt;code&gt;image.bin&lt;/code&gt; has to be split into two 16MB
files, and the dump should be taken from the second file:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;./util/amdfwtool/amdfwread -d --ro-list xab&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now we can start comparing the output. Pointers to directories or firmwares that are either 00000000 or ffffffff can be omitted, since both should indicate an invalid pointer. Most notable differences are SPI speeds, eSPI config and Multi Gen EFS value.&lt;/p&gt;
    &lt;p&gt;Fixing SPI speeds is pretty much straightforward. The board code should supply the following Kconfig values to match vendor firmware:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Having a correct SPI speed may be crucial for the proper operation of the system components and flawless communication on the SPI bus interface.&lt;/p&gt;
    &lt;p&gt;The eSPI configuration fields are configuration values that the PSP will use to configure the eSPI bus before the reset vector. The eSPI bus is important because the Baseboard Management Controller is connected to it. With BMC, we can use the serial port to debug problems, so we have to match the configuration. Support for setting the eSPI configuration has been implemented in the patch that adds Turin support to amdfwtool. With these modifications in place, I could define the Kconfig values again in board code (only one value is enough, because the rest is 0xff - default):&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The only difference left to cover is the Multi Gen EFS value. The usage of this field is described in the AMD documentation only. For the purpose of the analysis and explaining its importance, let’s say this value is specific to the processor family, and PSP uses it to match whether the given EFS is appropriate for the given CPU. These values are fixed for a given CPU family, and for Turin it has to be 0xffffffe3.&lt;/p&gt;
    &lt;p&gt;Covering these differences was not enough to allow the CPU to be released from reset using the public blobs. So we have to proceed further with the analysis, that is, to the PSP and BIOS directories.&lt;/p&gt;
    &lt;head rend="h2"&gt;PSP and BIOS directories&lt;/head&gt;
    &lt;p&gt;In the coreboot paste, we can see that PSP and BIOS directories have the same attributes as in the vendor image, but there are fewer entries in them than in the vendor image. For comparison:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vendor BIOS: &lt;list rend="ul"&gt;&lt;item&gt;PSPL1: 53 entries&lt;/item&gt;&lt;item&gt;PSPL2: 48 entries&lt;/item&gt;&lt;item&gt;BIOSL1: 25 entries&lt;/item&gt;&lt;item&gt;BIOSL2: 34 entries&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;coreboot: &lt;list rend="ul"&gt;&lt;item&gt;PSPL1: 30 entries&lt;/item&gt;&lt;item&gt;PSPL2: 41 entries&lt;/item&gt;&lt;item&gt;BIOSL1: 17 entries&lt;/item&gt;&lt;item&gt;BIOSL2: 19 entries&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The difference is, of course, dictated by the number of blobs AMD published here. Also, a subset of entries is duplicated between level 1 (L1) and level 2 (L2) directories of the same type. Level 1 directory is typically considered a recovery, and the level 2 is the main directory. However, for some reason, Gigabyte puts more blobs into the level 1 PSP directory than the level 2. To proceed further, we had no other choice but to eliminate the differences by extracting the blobs from the vendor image and integrating the missing ones. Extracting the blobs is possible with PSPTool:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;We have to take the extra &lt;code&gt;-r 1&lt;/code&gt; parameter because we want to extract the
Turin blobs, which live in the second 16MB half of the image, due to the dual
BIOS nature of the firmware images for this platform. When the blobs are
extracted, they have to be put into
&lt;code&gt;coreboot/3rdparty/amd_firmwares/Firmwares/Turin/&lt;/code&gt; directory to replace the
public blobs. Also, the &lt;code&gt;coreboot/src/soc/amd/turin_poc/fw.cfg&lt;/code&gt; file had to be
modified to point to the missing files. On top of that, the &lt;code&gt;amdfwtool&lt;/code&gt; had to
be extended with the new blob types. It has also been done as part of the
patch with Turin support.
However, to obtain full information on how these missing blobs should be
included, I needed the subprogram and instance numbers. These numbers are used
by PSP to distinguish the same type of program/blob, but for a different CPU
variant. For example, we may have multiple SMU firmwares, but only one will be
loaded on a given processor family, based on the subprogram and instance
numbers.&lt;/p&gt;
    &lt;p&gt;However, I had no tooling to dump these numbers. So again, I had to implement something myself. Thankfully, the PSPTool was very close to what I needed, and I simply extended it to print the subprogram and instance numbers for each blob:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;psptool -E MZ33-AR1_R11_F08/SPI_UPD/image.bin&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Based on the dump from the vendor image, I have modified &lt;code&gt;coreboot/src/soc/amd/turin_poc/fw.cfg&lt;/code&gt; and &lt;code&gt;amdfwtool&lt;/code&gt; to stitch the
components extracted from the vendor image. Only then was I able to obtain a
bootable coreboot image (CPU has been released from reset by PSP). Now having
the known good reference and a working tool, I could go back to the public
blobs.&lt;/p&gt;
    &lt;p&gt;I haven’t yet published the patches with PSPTool modifications. They will follow very soon, so stay tuned.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running coreboot with public PSP blobs&lt;/head&gt;
    &lt;p&gt;Before proceeding with reverting to public PSP blobs, I have made one additional safety measure. I flashed back the vendor BIOS and enabled PSP verbose debug output. This can be done with the &lt;code&gt;ABL Console Out&lt;/code&gt; options in
the &lt;code&gt;SOC Miscellaneous Control&lt;/code&gt; described in the board
manual
(section 2-3-6). Booted the platform with debug options enabled, and on the
serial console port, I could see verbose debugging messages almost immediately
after pressing the power button. This debug output will help me quickly
determine if the public blobs are even consumed by PSP or not. If I don’t see
anything on the serial port, it will mean they are not consumed. The debug
switches are stored inside APCB blobs, so I dumped the BIOS image, extracted
the APCBs with PSPTool and copied them in place on the old ones in the board
code. First, I confirmed whether they are working with the blobs extracted
from the vendor image, to avoid any mistakes later. And fortunately, it also
gave me debug output with the coreboot image. Then I proceeded with replacing
the vendor image blobs with the public ones, stitched the image again, and
flashed it on the board. But nothing happened on the serial console,
unfortunately. This was very unexpected and left me at a loss. After many
hours of analysis and comparisons, what else might be wrong or different, I
noticed something strange in the PSPTool output made from the current coreboot
image:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;psptool -E build/coreboot.rom&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The most worrying is the first entry of PSPL1, which corresponds to AMD Root Key. This Root Key is the main key used to sign PSP blobs and derive other keys to sign other components:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;Here we can see that the AMD Root Key has an ID &lt;code&gt;9F9D&lt;/code&gt;. The very same key is
used to verify PSP FW bootloader and PSP FW recovery bootloader
(&lt;code&gt;veri-failed(9F9D)&lt;/code&gt;, not sure why the verification fails, but it could be one
of the reasons why it is not working). When I looked closer at the output of
PSPTool from the vendor image, the verification of the PSP FW recovery boot
loader passed with the AMD Root Key:&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;The second significant difference is the key ID. The key ID in the vendor image was different (&lt;code&gt;D05C&lt;/code&gt;)! At first, I suspected that Gigabyte could have
created their own key to sign PSP blobs and got it signed by the AMD Root Key.
But is that really possible?&lt;/p&gt;
    &lt;p&gt;To be 100% sure, I have attempted to build the coreboot image with the latest AMD PSP blobs available in the Turin PI package (available to AMD partners). This is the package with the silicon initialization source code and the set of PSP blobs required to build a bootable image. So I updated &lt;code&gt;coreboot/src/soc/amd/turin_poc/fw.cfg&lt;/code&gt; again for the blobs from the Turin PI
package and dumped the directories with PSPTool. To my surprise, the AMD Root
Key was the same as in Gigabyte vendor firmware (full paste
here):&lt;/p&gt;
    &lt;table&gt;
      &lt;row/&gt;
    &lt;/table&gt;
    &lt;p&gt;This means my hypothesis about the custom root key was incorrect. AMD simply published a different set of blobs, or blobs that are signed with pre-production key (that would make sense, since the PoC code was proven on the AMD CRB platform, which most likely uses a pre-production CPU). I have filed an issue on the repository requesting to update the blobs to the newest ones from the Turin PI package. A separate &lt;code&gt;fw.cfg&lt;/code&gt; file for the use with official Turin PI package blobs has been
prepared in this
patch,
for future use.&lt;/p&gt;
    &lt;p&gt;While using public blobs proved to be impossible for now, I still decided to prove that blobs from official sources are working properly and can eventually replaced the incorrect blobs from the repo. So I flashed the freshly produced image with blobs from the Turin PI package, and thankfully, the CPU was released from reset, and I saw debug messages from coreboot’s bootblock!&lt;/p&gt;
    &lt;p&gt;With these results, we have fulfilled the goals of the following milestones in the project:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Task 2. Public blobs integration &lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Milestone a. Analysis of vendor’s image&lt;/p&gt;&lt;p&gt;We have thoroughly analyzed the vendor image and eliminated any differences in the integrated PSP blobs that could have hindered the boot process with coreboot. We also learned about the inappropriate blobs published by AMD. The usage of publicly available blobs is currently impossible, until the correct blobs are published by AMD. However, when that happens, we are already prepared to build functional images. So building based on public components is possible but produces unbootable image. People with access to Turin PI package are able to build bootable image. Regardless of what will happen to the public blobs, 3mdeb will ship hardware and firmware with the correct blobs. The relevant patch adding support for the blob integration can be found here.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Milestone b. Update coreboot’s amdfwtool&lt;/p&gt;&lt;p&gt;Thanks to the extensive analysis of the vendor image, the amdfwtool is now successfully creating a bootable image for the Turin system. A patch has been uploaded and the “work in progress” state got removed, indicating the change is ready to review and functional.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;The journey of porting Gigabyte MZ33-AR1 seems to be still quite long. Lots of surprises probably still await us. Stay tuned for more blog posts where further porting efforts will be shown and explained.&lt;/p&gt;
    &lt;p&gt;Huge kudos to NLnet Foundation for sponsoring the project.&lt;/p&gt;
    &lt;p&gt;Unlock the full potential of your hardware and secure your firmware with the experts at 3mdeb! If you’re looking to boost your product’s performance and protect it from potential security threats, our team is here to help. Schedule a call with us or drop us an email at &lt;code&gt;contact&amp;lt;at&amp;gt;3mdeb&amp;lt;dot&amp;gt;com&lt;/code&gt; to start unlocking the
hidden benefits of your hardware. And if you want to stay up-to-date on all
things firmware security and optimization, be sure to sign up for our
newsletter:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45243439</guid><pubDate>Sun, 14 Sep 2025 21:35:42 +0000</pubDate></item><item><title>Betty Crocker broke recipes by shrinking boxes</title><link>https://www.cubbyathome.com/boxed-cake-mix-sizes-have-shrunk-80045058</link><description>&lt;doc fingerprint="800a557659315311"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The 70-Year-Old Beloved Boxed Mix Grandmas Won’t Be Buying Anymore&lt;/head&gt;
    &lt;p&gt;There’s a problem with boxed mixes, and it’s impacting grandmas’ most beloved recipes. At some point last year, shoppers noticed Betty Crocker cake mixes shrank (again), this time from 15.25 ounces down to 13.25 ounces. And while few people would ever argue for less cake, the 2-ounce decrease is weighing particularly heavy on grandmas.&lt;/p&gt;
    &lt;p&gt;Take my neighbor Judith (and grandma to two). Her chocolate crinkle cookies have been practically synonymous with our community potlucks for the past 15 years — and that’s only how long I’ve known her! It wasn’t until she suddenly stopped bringing them that I knew something was terribly wrong.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Grandmas Aren’t Buying Boxed Cake Mixes&lt;/head&gt;
    &lt;p&gt;Her cookie recipe — a box of Betty Crocker chocolate cake mix, two eggs, and ⅓ cup neutral oil — no longer works now that the box is a full 5 ounces smaller than its original 18.25-ounce size (a 27% decrease and textbook example of shrinkflation). What once yielded 24 consistently light, fluffy cookies now makes 20 goopy, lackluster blobs. The only thing that has changed? The box mix.&lt;/p&gt;
    &lt;p&gt;“It’s just so upsetting,” says Judith, whose cookie recipe was passed down by her mother. These “perfect little cookies” once made the rounds at bake sales, Christmas cookie exchanges, and birthdays. She now calls them “unusable.” She could buy an additional box to make up the difference, she acknowledges, “but out of principle, I just can’t.”&lt;/p&gt;
    &lt;p&gt;Judith isn’t the only one. She says that her other (grandma) friends are feeling the changes, too, and they’re not happy about it. Betty Crocker has empowered home cooks to make delicious desserts for over a century now. So it’s no surprise just how many cherished family recipes — involving that once familiar box of cake mix — have been passed down from generation to generation.&lt;/p&gt;
    &lt;p&gt;It’s not just cookies that are affected; beloved family dump cakes, crumbles, pancakes, and more all fall short because the cake mix is no longer the same.&lt;/p&gt;
    &lt;p&gt;To add to the frustration, one Redditor pointed out that the brand may have “tinkered with the amount of leaveners in the mix itself. … When [the cake] first comes out of the oven, it looks like a more substantial amount of cake but then shrinks as it cools down.” (We reached out to Betty Crocker to verify any ingredient changes and have yet to receive a response.)&lt;/p&gt;
    &lt;p&gt;Baking is indeed a science that needs precise measurements and consistency. Many home bakers, like Judith, find it disheartening to see decades-old cherished recipes forever changed by corporate decisions. I did, though, share our Chocolate Crinkle Cookie recipe with her so she can try to make new traditions. &lt;lb/&gt;This article originally published on The Kitchn. See it there: The 70-Year-Old Beloved Boxed Mix Grandmas Won’t Be Buying This Holiday Season&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45243635</guid><pubDate>Sun, 14 Sep 2025 21:54:07 +0000</pubDate></item><item><title>Trigger Crossbar</title><link>https://serd.es/2025/09/14/Trigger-crossbar.html</link><description>&lt;doc fingerprint="2e1b921501a716a4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Trigger crossbar&lt;/head&gt;
    &lt;p&gt;If you have a large, well-equipped electronics lab youâre going to have a lot of instrumentation with trigger input and output ports.&lt;/p&gt;
    &lt;p&gt;In my case all three oscilloscopes, the vector signal generator, and even my VNAs have trigger sync capability, and thereâs probably more things Iâm missing. And that doesnât even count the ThunderScope or the two Siglent AWGs I have on loan for ThunderScope R&amp;amp;D.&lt;/p&gt;
    &lt;p&gt;Very often, itâs handy to cascade these in order to enable complex multi-instrument setups (for example, having a scope trigger when an AWG creates a pulse of some sort, without burning a scope channel to look at the AWG output, or to have two scopes trigger simultaneously to capture more channels of data in a complex system).&lt;/p&gt;
    &lt;p&gt;Thereâs just one obvious problem: All of my equipment is rack mounted, thereâs a LOT of it, and thereâs already a ton of cable spaghetti in a fairly confined space. The last thing I want to be doing is reaching around behind the racks and crawling under the bench to untangle coax and route trigger signals from one instrument to another every time I want to set up a multi-instrument experiment.&lt;/p&gt;
    &lt;p&gt;The second, slightly less obvious, problem is that not all of these signals are compatible voltage levels. For example, the trigger output on my Teledyne LeCroy oscilloscopes is 1V into high-Z or 500 mV into a 50Î© load. The Siglent vector signal generator has a 5V TTL trigger input. So you canât just directly connect these without a level shifter or buffer.&lt;/p&gt;
    &lt;p&gt;What if there was a better way?&lt;/p&gt;
    &lt;head rend="h2"&gt;The concept&lt;/head&gt;
    &lt;p&gt;Pretty quickly I came up with a high level concept for what I wanted to build: a 1U device with an Ethernet SCPI interface plus a ton of coaxial trigger inputs and outputs, connected to a buffered FPGA-based switch fabric.&lt;/p&gt;
    &lt;p&gt;Some outputs would be buffered by external level shifters to enable interfacing with different voltage levels, while others would be directly connected to FPGA GPIOs for the lowest possible jitter but with a fixed voltage range. Inputs would be routed to comparators to allow arbitrary switching thresholds.&lt;/p&gt;
    &lt;p&gt;A handful of these channels would be bidirectional, with latching relays to swap between input and output modes. This is important because a few of my instruments, most notably the PicoScope and Siglent vector signal generator, use the same BNC as both trigger input and output.&lt;/p&gt;
    &lt;p&gt;The whole thing would be powered by 48V DC using my existing intermediate bus converter and controlled over IP via ngscopeclient, using the filter graph to create virtual connections between crossbar ports..&lt;/p&gt;
    &lt;head rend="h2"&gt;High level design&lt;/head&gt;
    &lt;p&gt;I selected the Xilinx XC7K70T-2FBG484C as the FPGA for a few reasons:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;I had a lot of 7 series parts in inventory, so no need to buy anything new.&lt;/item&gt;
      &lt;item&gt;Kintex-7 has high-performance (HP) I/O banks which have faster slew and lower jitter than the high-range (HR) I/Os found in Spartan/Artix parts. I didnât want to increase cross-trigger jitter more than necessary so this was important.&lt;/item&gt;
      &lt;item&gt;The 70T in FBG484 is the lowest cost part in the Kintex-7 line, we donât need a ton of stuff in the FPGA so no reason to go bigger.&lt;/item&gt;
      &lt;item&gt;The -2 speed has 10.3125 Gbps capable SERDES (weâll get to why this is important in a bit).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This then got paired with the STM32H735 MCU as the controller.&lt;/p&gt;
    &lt;p&gt;I started this project all the way back in October 2023, long before I realized how cursed the OCTOSPI on the H735 was. In fact, many of the things I discuss in that post were learned on the crossbar project, it just took me this long to get to the point of having time to write about the crossbar as a whole.&lt;/p&gt;
    &lt;p&gt;So (foreshadowing a bit), I made the mistake of connecting it via the OCTOSPI thinking that I wouldnât need the bandwidth of the FMC to control a few muxes and it would save pins. I also hadnât tried the FMC yet and thought (famous last words) that the OCTOSPI would be simpler and easier to set up. This ultimately turned out to be a massive annoyance and cost me a lot of time, but I did get it working in the end.&lt;/p&gt;
    &lt;p&gt;The final concept I came up with was logically a 12x12 crossbar: eight inputs, eight outputs, and four bidirectional ports.&lt;/p&gt;
    &lt;p&gt;The input ports were all 50Î© impedance, with a 6 dB (2:1) attenuator and ESD diode prior to the input termination. The input then entered a MAX40026 high-speed LVDS comparator, with the positive input fed by the trigger signal and the negative by a reference voltage generated by a DAC. This design provides runtime-variable thresholding and 5V tolerance while operating from a 3.3V supply.&lt;/p&gt;
    &lt;p&gt;Output channels 0-3 were driven directly by HP I/Os on the FPGA, providing a fixed 1.8V swing but the highest possible jitter performance. The remaining 8 outputs were driven by TI 74LVC1T45 level shifters. Each level shifter had its own independent VCCIO power domain supplied by an ISL24021 power op-amp buffering a reference voltage generated by one output from an 8-channel DAC, allowing runtime adjustment of VCCIO for each port. The buffered output then passes a final ESD diode before reaching the connector.&lt;/p&gt;
    &lt;p&gt;You can see the whole schematic (and firmware) on my GitHub.&lt;/p&gt;
    &lt;p&gt;The rest of the logic board was fairly straightforward: a KSZ9031 gigabit Ethernet PHY for management (routed to the FPGA in case I wanted to add any kind of hardware offload), a bunch of Murata DC-DC modules to generate all of the necessary supply rails from the 12V intermediate bus, a serial port for initial IP configuration and debug, an EEPROM to store the MAC address, and a connector supplying power and SPI to the front panel board.&lt;/p&gt;
    &lt;p&gt;The FPGA has four GTX SERDES lanes in a single quad. I hooked them all up (it always seems like a shame to not pin out transceivers to something):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Two lanes to front panel TX/RX differential SMA ports for use as a 2-lane BERT or serial pattern generator/receiver. Iâve wanted to build a proper BERT for a while and this was a good opportunity to play with it.&lt;/item&gt;
      &lt;item&gt;One lane to a back panel 10G SFP+ (because why not, 10GbE is always handy to have)&lt;/item&gt;
      &lt;item&gt;One lane TX to a front panel differential SMA port for use as a deskew reference&lt;/item&gt;
      &lt;item&gt;The RX of the split channel went to a comparator and single-ended coaxial input for a potential future CDR trigger feature.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unfortunately, having all of the transceivers in a single quad was a bit limiting due to the 7 series clocking architecture: they share the same QPLL, which has to be at 10.3125 Gbps for 10Gbase-R operation. While the CPLLs can be configured freely, they have a lower Fmax which meant that the BERT / CDR trigger channels cannot operate at arbitrary frequencies above 6 Gbps (most notably, 8 Gbps operation for PCIe gen3 mode is not available).&lt;/p&gt;
    &lt;p&gt;In theory it would be possible to reconfigure the QPLL for PCIe gen3 at the cost of temporarily disabling the SFP+ but current firmware/gateware doesnât support this. Using an UltraScale+ FPGA (which has a much higher CPLL frequency range, plus two QPLLs per quad) would also have provided a lot more clocking flexibility, but would also increase the cost since I didnât have a suitable FPGA on the shelf at the time and this board started out as a âjunk box buildâ from parts I mostly had on the shelf already.&lt;/p&gt;
    &lt;head rend="h2"&gt;The PCB&lt;/head&gt;
    &lt;p&gt;The board was fabricated at Multech on the 10-layer stackup Iâve used for my last few high-end designs: SGS GPPG SGS, with TU872SLK between the signal and reference layers and S1000-2M between the power and ground layers since thereâs no reason to use an expensive low-loss laminate on a power layer.&lt;/p&gt;
    &lt;p&gt;All of the trigger I/Os were placed in the northwest corner, with what ultimately turned out to be perhaps a bit too much packing density in retropect. The other rear-panel connectors were RJ45s for RS232 (Cisco pinout) and 1000baseT and the SFP+ for high speed I/O.&lt;/p&gt;
    &lt;p&gt;The 12V power input and I2C interface to the IBC were placed in the southwest corner, with the front panel SERDES ports along the south edge and the FPGA roughly centered in the board for easy access to everything. All of the high speed I/Os used SMPM connectors, my go-to for high speed high density these days because theyâre much smaller than SMA and have higher bandwidth. But this many of them was perhaps a bit excessive; if I were doing it again Iâd likely have used some kind of multi-lane board to board or board-to-cable connector and then broken out elsewhere.&lt;/p&gt;
    &lt;p&gt;The MCU was jammed into the 3 oâclock position just south of the SFP+; since it was mostly a âbrain on a stickâ hanging off the quad SPI link to the FPGA and not needing much IO of its own it could be placed almost anywhere.&lt;/p&gt;
    &lt;p&gt;I also provided a PMOD for debug GPIO, a couple of LEDs, and two 12V 4-pin fan connectors (only one of which ended up being used).&lt;/p&gt;
    &lt;p&gt;The final board size was 165 x 121 mm with 633 components, one of my larger designs to date. Mounting holes were 4-40, 5mm in from each corner plus one in the center of each of the long edges.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bringup woes&lt;/head&gt;
    &lt;p&gt;Once I assembled the board (most of a weekend worth of tweezering components) and started trying to write firmware and gateware, I ran into problems. Well, actually I found one even before finishing populating the board (the SFP+ was recessed too far, and the EMI fingers on the bottom were bumping into the PCB surface and tilting it up slightly). I was able to fix this by cutting the EMI fingers off.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power connection&lt;/head&gt;
    &lt;p&gt;Then I tried to actually turn it on, and nothing happened. The 48V IBC connects to the logic board via two connections - an 8-pin Molex Mini-Fit Jr carrying 12V power and ground, and a 5-pin Molex PicoBlade carrying a 3.3V standby power rail, an I2C management bus, and an enable line for the main 12V rail.&lt;/p&gt;
    &lt;p&gt;After a bit of probing, I discovered that I had wired the connector as if it were 1:1 pinout (i.e. pin 1 of IBC connector to pin 1 of logic board connector). But standard PicoBlade cables are wired straight through (a strip of parallel wires with one connector on each end, pin 1 wired to pin N-1).&lt;/p&gt;
    &lt;p&gt;I briefly considered reworking the IBC or logic board before realizing that bodging the cable was a much simpler solution. All I had to do was gently push in the latching pin on each crimp terminal to remove it from the plastic housing, then reinsert them in the correct order and add a bright orange stick-on warning label âmirrored pinoutâ so I didnât mix up this special cable with a standard-pinout one.&lt;/p&gt;
    &lt;p&gt;With that fixed, I was at least able to get power to the board. There was no magic smoke, always a good sign.&lt;/p&gt;
    &lt;head rend="h3"&gt;More power issues&lt;/head&gt;
    &lt;p&gt;I normally have the majority of power rails on my boards default to the off state, then turn them on one at a time under control of a supervisor MCU which functions as a PMIC (in this case a STM32L031). This is great for prototypes and small-run boards because it lets me change rail sequencing dynamically with a software patch, as well as automating bringup one rail at a time with instant panic-shutdown within milliseconds if a rail doesnât stabilize when and where it should. This minimizes the chance of hardware damage in case of solder defects or PCB design bugs on a hand soldered prototype.&lt;/p&gt;
    &lt;p&gt;Right out of the gate, things werenât too happy: 1V8 was reporting no-good on PGOOD. 3V0_N and GTX_1V8 werenât coming up, GTX_1V0 was unstable, and 1V2 was a dead short to ground.&lt;/p&gt;
    &lt;p&gt;Scoping the 1.8V rail gave a somewhat surprising result: the rail came up fine, stabilized at 1.7903V, but PGOOD never went high and after a few ms the supervisor assumed the rail was shorted (it wasnât monitoring the actual rail voltage with an ADC, just PGOOD reported by the regulator) and entered panic-shutdown mode to protect the board.&lt;/p&gt;
    &lt;p&gt;I decided to ignore the PGOOD fault since it looked like it would be a pain to rework due to the thermal mass of the board, and just patch the supervisor firmware to continue bringing up other rails a fixed delay after turning on 1V8. This isnât something I would do in a ârealâ system of course, but for a one-off it was a risk I was willing to take after having confirmed the rail wasnât in fact shorted (I had plenty of other fusing and protection mechanisms, the software timeouts were deliberately paranoid for bringup).&lt;/p&gt;
    &lt;p&gt;Most of the other rail issues turned out to be bad solder joints on the LGA Murata DC-DC modules I was usingâ¦ my solder paste print in this area was decidedly subpar (the board was larger than I was used to and flexed in the printing fixture due to insufficient back-side support while only fixturing it from the edges). In retrospect I should have just wiped the board off and re-printed but I had faith in my solder paste. A bit too much faith.&lt;/p&gt;
    &lt;p&gt;Anyway, I pulled and resoldered the 1V2 and GTX_1V8 DC-DC Modules, and added more capacitance to the 3V0_N regulator input which was causing instability during startup. I had ferrites on the output of GTX_1V0 and a few other rails as secondary filters but these were hurting stability so I removed them and replaced them with 0Râs.&lt;/p&gt;
    &lt;p&gt;At some point I noticed that decoupling capacitor C6, one of two 22 uF MLCCs as the input of the 3.3V DC-DC module, was not actually placed within the 12V power zone fill on layer 6. The 12V terminal on the capacitor was only connected by a very thin trace coming off the vias, which significantly reduced its effectiveness. This was easily bodged with a surface jumper connecting it to C3, the other 22 uF capacitor.&lt;/p&gt;
    &lt;head rend="h3"&gt;Comparator Vcm issues&lt;/head&gt;
    &lt;p&gt;With the power reasonably stable, the next step was to actually hook up some signal sources and bring up the I/O buffers.&lt;/p&gt;
    &lt;p&gt;They worked fine with large-swing inputs like 3.3V, but when I tried to use low-amplitude signals the LVDS outputs of the comparators werenât toggling.&lt;/p&gt;
    &lt;p&gt;After a while, I realized that I had missed the 1.5V minimum Vcm spec on the MAX40026 comparators when designing the input stage. And since I had a 2x attenuator on the input to provide 5V tolerance, this really came out to 3V at the input. Experimentally, if the DAC for setting the comparator threshold was set below about 800 mV, I stopped getting useful results out of the comparator. This was a major problem, since these inputs were key to the functionality of the device and my LeCroy scopes had 1V full-scale output on the trigger sync ports.&lt;/p&gt;
    &lt;p&gt;A switchable attenuator is the ârightâ solution here - with the 2x switched in it would perform well for ~3V to 5V inputs, and with no attenuation it would accept inputs from 3.3V down to ~1V logic levels. But retrofitting eight SPDT relays and drive circuitry around the attenuators, plus figuring out how to power and control them, seemed a bit beyond the scope of reasonable rework. I didnât want to respin the board and since this was mostly a project to scratch my own itch and not something I planned to mass produce, I wanted to avoid scrapping the board (now worth well over $2K between PCB and components, plus several days of hand assembly time).&lt;/p&gt;
    &lt;p&gt;After giving it some thought I decided that since I planned to have any given trigger input permanently connected to a specific instrument for the lifetime of the device, I didnât actually need a runtime-switchable attenuator. So I reworked most of the inputs to have a 0 dB passthrough instead of the original 6 dB attenuator (limiting them to 3.3V input levels), keeping attenuators only on the inputs that I planned to connect to instruments which had 5V trigger outputs. This seemed to work fine.&lt;/p&gt;
    &lt;head rend="h3"&gt;SPI/QSPI bus issues&lt;/head&gt;
    &lt;p&gt;As I started writing firmware, I couldnât get the main MCU (STM32H735) to talk to the supervisor (STM32L031) over their shared SPI bus. I pretty quickly discovered that this was due to a MOSI/MISO crossover that shouldnât have been there (STM32 SPI block wants them connected 1:1, changing pin directions based on host/device mode rather than having fixed in/out pins). This was easily reworked with a few trace cuts and a surface jumper right next to the MCU.&lt;/p&gt;
    &lt;p&gt;Around this time I also discovered how cursed the OCTOSPI was. I have a whole post about this so I wonât repeat all of the details here.&lt;/p&gt;
    &lt;p&gt;Finally, I started trying to bring up the front panel MCU and lost connectivity to the debugger as soon as I tried to send SPI traffic to it. This puzzled me to no end until I realized I was being hit by STM32L431 errata 2.2.6. Basically, the pin muxing logic is broken and if you try to use PB4 as anything but NJTRST, it doesnât actually disconnect the signal from the JTAG TAP and any logic low on the pin will reset the TAP.&lt;/p&gt;
    &lt;p&gt;I ended up working around this issue in two different ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First: Patch the firmware to keep PB4 alt mode as JTRST unless actively sending SPI data back to the main processor (which will cause a momentary debugger disconnection, but allow the debugger to reconnect as soon as the SPI burst ends).&lt;/item&gt;
      &lt;item&gt;Second: Switch from debugging over JTAG to SWD, which bypasses the issue entirely. This also frees up the JTDO/SWO pin for use as serial trace output, which I wasnât using at the time but have since began to take advantage of.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Single ended CDR trigger input not working&lt;/head&gt;
    &lt;p&gt;I never solved this one. There was a HMC675 comparator feeding a GTX lane that I had intended to use as a CDR trigger input, but I wasnât getting toggles on the LVDS/CML output no matter what I did. I had so many other issues on the board and the CDR trigger was mostly a stretch goal, so I just shelved it. Could be anything from a soldering issue to a bad pinout to something borked in the power supply / circuit design.&lt;/p&gt;
    &lt;p&gt;I can always add CDR trigger input functionality in gateware via the BERT lanes, it will just require a 100 ohm differential input (or terminating one of the inputs to provide a single ended input?) rather than using the 50 ohm single ended input I had originally planned.&lt;/p&gt;
    &lt;head rend="h3"&gt;FPGA flash pinout bug&lt;/head&gt;
    &lt;p&gt;At this point, I had all of the major issues fixed (so I thought) and I started writing firmware and gateware. I had things working pretty well, until I tried to burn a bitstream to FPGA flash so I could boot the board from a cold state without having to JTAG it.&lt;/p&gt;
    &lt;p&gt;And nothing happened.&lt;/p&gt;
    &lt;p&gt;So I dug deeper, and was very upset with what I found: I had hooked CS# of the QSPI flash to CSO_B (daisy chain configuration chip select output), not FCS_B (flash chip select). Iâm still not sure how I made this mistake and didnât catch it during design review: if this was my first 7 series FPGA design it would be somewhat understandable, but itâs not. Iâve done probably 10+ 7 series designs in the past, and always got this connection right. Until now.&lt;/p&gt;
    &lt;p&gt;The actual FLASH_CS_N signal connected to pin M22 of the FBG484 package, while it should have gone to L16.&lt;/p&gt;
    &lt;p&gt;Disconnecting it from M22 was trivial, there was a via in the M22 BGA land (at the outer perimeter of the BGA) going straight into a 33Î© series terminator footprint on the back side of the board. Desoldering the terminator trivially disconnected the incorrect connection.&lt;/p&gt;
    &lt;p&gt;Adding the new connection at L16? Less trivial. In this area, there were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;JTAG TMS and TCK in the routing channels immediately east/west of the L16 land on layer 3&lt;/item&gt;
      &lt;item&gt;Ground planes on layers 2, 4, 7, and 9&lt;/item&gt;
      &lt;item&gt;3.3V VCCO power plane on layer 5, connecting to a via in L17 immediately south of the L16 land&lt;/item&gt;
      &lt;item&gt;1.0V VCCINT power plane on layer 5, connecting to a via in L15 immediately north of the L16 land&lt;/item&gt;
      &lt;item&gt;1.8V VCCO/VCCAUX power plane on layer 6, but with no vias in the immediate area&lt;/item&gt;
      &lt;item&gt;4.7 Î¼F 0603 decoupling capacitor on layer 10 partially overlapping the L16 land&lt;/item&gt;
      &lt;item&gt;The remote sense line for VCCINT on layer 10&lt;/item&gt;
      &lt;item&gt;And of course the FPGA already soldered to layer 1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Given how far into the BGA footprint the missing connection was, trying to reach in from the edge wasnât viable.&lt;/p&gt;
    &lt;p&gt;This really left only three options:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Give up. Accept that SPI boot isnât going to be available, then bodge up some board with a little MCU that acts as a JTAG controller, hooks up to the FPGA JTAG, and squirts a bitstream into the FPGA on powerup. This would require no hardware modification at all (although Iâd have to design the MCU board) but would block FPGA JTAG access for debug and just be ugly. The lack of debug access alone was enough of a reason to reject this,&lt;/item&gt;
      &lt;item&gt;Remove the FPGA, drill out a via from the top, figure out how to plate/fill it so it wouldnât suck the solder down, reball the FPGA, put it back. I donât have great gear for BGA desoldering, had a bunch of heat sensitive non-reflowable components on the board at this point, and didnât feel like tweezering 484 solder balls or scrapping a several hundred dollar FPGA. So this was a non-starter as well.&lt;/item&gt;
      &lt;item&gt;Root canal approach: add the via from the back side. This really seemed like the only way forward.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So basically I had to drill a ~1.6mm deep flat-bottomed hole, exposing but not perforating the 35 Î¼m thick copper foil on L1 attached to the BGA land, and solder a jumper wire to it without shorting to any of the six power/ground plane layers in close proximity, damaging the VCCINT/VCCO vias 1mm centered north/south of the target, or cutting either of the layer 3 JTAG lines centered 500 Î¼m east/west of the target. No big deal /s.&lt;/p&gt;
    &lt;p&gt;I started out by removing two capacitors and a resistor in close proximity to the work area, that were getting in the way.&lt;/p&gt;
    &lt;p&gt;Then I fixtured the board on my Sherline 5400 mini mill and loaded up a 250 Î¼m carbide endmill. The mill is set up with a cheap Amscope stereo microscope (Iâm not going to risk my nice Leica getting hit by flying debris etc) and a 1/8â collet for mounting fine-pitch drills and endmills. At the time of this project I didnât have a proper gooseneck LED illuminator so I just used a random headlamp, although Iâve since fixed that.&lt;/p&gt;
    &lt;p&gt;At this point there was nothing left to do but start drilling.&lt;/p&gt;
    &lt;p&gt;The overall setup was very similar to a dual-beam SEM/FIB scaled up by a few orders of magnitude and rotated: milling column coming in from the top, and imaging column at a ~45 degree angle for in-process inspection. And using a vacuum system (aka a shopvac) for removing debris from the milled cavity. No fancy secondary ion mass spectrometer is needed for endpoint detection though, you can just see the swarf change from white fiberglass to shiny red copper when you hit a metal layer.&lt;/p&gt;
    &lt;p&gt;Just like with a dual-beam, this sort of work requires making an angled cut to allow the angled imaging path to see the work area of the mill. I decided to orient the board in the canonical CAD orientation and come in from the south side, parallel to the JTAG traces. As long as I kept the cavity centered and not more than around 750 Î¼m wide, there was no real danger of hitting the JTAG lines. The angled cut was more tricky as I had a VCCO via 1mm south of the work area that I needed to keep intact from L1 to L5 (but back-drilling it from L6 to L10 wouldnât hurt anything), so I had to pay careful attention to depth as I approached this area.&lt;/p&gt;
    &lt;p&gt;It worked perfectly. After about an hour of very careful machining, periodically stopping to unmount the board and look at it from different angles under the nice microscope, I saw the back side of the target BGA land coming into view, still under around 50 Î¼m of laminate but clearly visible after adding a drop of IPA (a little trick Iâve learned when doing rework: it soaks into the resin and fills the gaps between the glass strands, acting to match the refractive index better and making it more transparent with less scatter)&lt;/p&gt;
    &lt;p&gt;The only thing left to do was a final plunge cut to expose a 250 Î¼m circle of bare copper to solder to. In this view you can clearly see how the milled cavity gets smaller and smaller as the target area approaches. This is rotated 180 degrees from canonical (south in CAD view is up in this image). Note the VCCO via barrel terminating at L5, then L4 ground plane visible slightly below it. The L2 ground plane is barely visible at the 7 oâclock position of the final hole.&lt;/p&gt;
    &lt;p&gt;I did a final check for shorts and everything looked good. The only thing left to do was solder it up (not trivial, given that Iâm trying to hit a 250 Î¼m diameter target at the bottom of a 1.6mm deep hole).&lt;/p&gt;
    &lt;p&gt;After a few minutes of contorting myself trying to get a 100 Î¼m conical soldering tip and a 125 Î¼m bare copper wire positioned at the bottom of the hole while also angling the board enough that I could see under a microscope, I managed to get the wire securely attached.&lt;/p&gt;
    &lt;p&gt;Once that was done, I reconnected the VCCINT remote sense input, added a new series terminator to the CS# signal, put the removed decoupling capacitor back on its new slightly-smaller footprint, and tacked down all the wires with UV glue so they wouldnât move around.&lt;/p&gt;
    &lt;p&gt;After verifying the board was electrically functional and I could program the flash and boot from it, I went back and flooded the whole area with UV cured conformal coating to lock everything in place permanently.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chassis design and assembly problems&lt;/head&gt;
    &lt;p&gt;I went with a custom 1U enclosure from ProtoCase, designed using their free ProtoCase Designer software.&lt;/p&gt;
    &lt;p&gt;I have very mixed feelings about it: it has a lot of convenient templates and integrations with their standard pems and printing processes, and runs on Linux (a rarity in the proprietary CAD world). But itâs also a custom file format to lock you into their service, and if you import a couple of moderately complex STEP models of PCBs it slows to a crawl. Iâll probably keep using it for now, as the free tools donât seem to be up to the task for complex sheet metal work and I donât want to buy SolidWorks or something and set up a Windows VM for it.&lt;/p&gt;
    &lt;p&gt;My original plan had been to use their standard 1U folded sheet metal design, but I discovered too late, after boards were ordered, that it wasnât going to work due to the 5mm corner mounting holes being inside the keepout area for the rear bend. I went with an extruded aluminum design, which let me salvage the design but had several significant problems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The stock template has no rack ears (although itâs 1U high) and thereâs no way to add them within their software. I had to pay an hourly fee for one of their engineering technicians to create a new template with ears and extruded sides.&lt;/item&gt;
      &lt;item&gt;The top and bottom panels slide into the extrusions and are held in place by the front and back. This means you cannot take the top panel off, e.g. to access JTAG ports for debug, without also removing the front panel (wearing out the self-tapping screwsâ holes and putting shear forces on any attached cables)&lt;/item&gt;
      &lt;item&gt;The front panel is only attached at the edges, and can flex relative to the top/bottom panels by quite a bit.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Iâm definitely not going to use this case design again, although I made it work for a one-off. All of my future projects will do chassis-PCB codesign and not order boards until the enclosure is finished enough that Iâm confident everything will mate properly.&lt;/p&gt;
    &lt;p&gt;All of the front/rear panel coaxial ports connect to the logic board using custom sized SMA-to-SMPM semirigid cables. I went with semirigid to ensure the best possible repeatability of the trigger path delays, but it was likely overkill. It also made chassis assembly an absolute nightmare even with the custom 3D printed bending jigs I madeâ¦ Iâm never doing this again. Either flexible cables, hand-formable cables, or a much lower density design with semirigid that I have more space to route.&lt;/p&gt;
    &lt;p&gt;The biggest problem was that the SMAs bolted to the outside of the chassis, meaning that I had to pre-bend the cables, then thread the SMPM end in through the hole in the chassis, then try to snake the SMPM in through all of the previously installed cables. Lesson very painfully learned: I did get it fully assembled in the end, but I would never build a second unit like this.&lt;/p&gt;
    &lt;p&gt;I also had slightly oversized some of the cables thinking that this would be better than undersizing, but in many cases this led to extra slack that I had to remove by U-bends and such, adding to the tangle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Front panel&lt;/head&gt;
    &lt;p&gt;The front panel has a STM32L431 and an I2C I/O expander to drive two rows of LEDs, showing pulse-stretched trigger input and output state. Four additional pairs of LEDs show the direction of the bidirectional ports, and a small e-paper display shows system health and status information.&lt;/p&gt;
    &lt;p&gt;I may have gone slightly overkill with the stitching vias on the board, but Iâm so used to high speed designs that itâs a hard habit to break. I used an 0.5mm BGA package for the STM32 as a test to see if I could make it work on OSHPark, but probably would have just gone with the 48-QFN if this was a ârealâ product since there was absolutely no point in using a fine pitch BGA here.&lt;/p&gt;
    &lt;p&gt;This was my first time using e-paper and I initially had specced a white/black/red panel thinking I could use the red to denote error states or something, but quickly realized the error of my ways: the tricolor panels have a very very slow refresh rate and are really only suitable for signage, not user interface type stuff. Luckily Pervasive Displays made a pin- and mechanically-compatible B&amp;amp;W fast refresh display, so I swapped that in.&lt;/p&gt;
    &lt;p&gt;After spending a little while figuring out the not-great documentation for the panel controller and baking a few bitmap fonts into the STM32 firmware, I got it displaying some pretty system health stats: Ethernet link speed, IPv4/6 addresses, unit serial number, firmware timestamps for the FPGA and MCUs, input and output voltage/current/power for the IBC, temperatures at four points in the system, and fan speed.&lt;/p&gt;
    &lt;p&gt;I also swapped the original first-generation IBC shown in the build photos with my second-generation MYC0409 based version to improve power efficiency, at which point the hardware was essentially done.&lt;/p&gt;
    &lt;p&gt;The unit is now mounted on one of my 19â benchtop racks and cabled to most of my instrumentation (I have a few more cables to run still but the main oscilloscopes and such are connected). I left the top off because I still have some firmware tweaks to do, so I need to be able to get JTAG/SWD dongles in to debug it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Operation&lt;/head&gt;
    &lt;p&gt;The crossbar has two remoting interfaces: SSH and SCPI.&lt;/p&gt;
    &lt;head rend="h3"&gt;SSH&lt;/head&gt;
    &lt;p&gt;The SSH interface is mostly used for low level configuration and setup - managing keys for administrative access, forcing refreshes of the front panel LCD, displaying hardware sensor values, setting the NTP server IP address, etc.&lt;/p&gt;
    &lt;p&gt;Itâs also used for firmware updates via SFTP. This is a somewhat unconventional DFU flow but I quite liked it:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To flash the FPGA, SFTP a .bit file to /dev/fpga and it will be directly written to QSPI flash on the FPGA&lt;/item&gt;
      &lt;item&gt;To flash the front panel or logic board MCU, SFTP an ELF binary to /dev/mcu or /dev/frontpanel. The ELF will be parsed live by the SFTP server and any PT_LOAD program headers in the flash address range are then written to flash. This is implemented in a single-pass streaming flow which requires a specific construction of the ELF (ELF header then program header table then program headers in linear address order) however all sane ELF generators like the GNU linker produce binaries that follow this. If you try to flash with a pathological ELF hand crafted in a hex editor and brick things, thatâs on you :P&lt;/item&gt;
      &lt;item&gt;Supervisor and IBC MCUs are not field updateable because they were (at the time) STM32L031 based and lacked the flash for a bootloader. IBC is now STM32L431 based so adding a bootloader is possible, but I canât see any reason Iâd want to OTA the power supply so Iâll probably just JTAG it if I ever need to patch.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Code signing support could be added to this flow easily (basically just mark the image as non-bootable in some way until youâve checked the signature, then clear the flag once youâve verified it). For the near term though, I trust that anyone with a valid administrative SSH key on the device is an authorized admin and can flash arbitrary code. Since thatâs only me, itâs fine for now.&lt;/p&gt;
    &lt;p&gt;But since I have a curve25519 acceleration block in the FPGA already Iâll probably prototype a signing flow at some point just to have it available for future projects; I can always turn it off. The basic concept is to add an extra .signature section in the linker script that will be filled with 0x00 padding at link time, then a signing tool run post-link will hash the contents and headers of all data to be written to flash, sign with the curve25519 key, and overwrite .signature with that.&lt;/p&gt;
    &lt;p&gt;Signing the FPGA bitstream would be even simpler, just sign the entire .bit and append 32 bytes of signature to the end. The bitstream can be made unbootable by writing a dummy bitstream to the first flash sector containing an invalid CRC and a DESYNC command until the verification is done, then erasing this and writing the actual first sector bitstream content at the very end. This will require one flash erase block (typically 4 kB) of scratchpad buffer in the bootloader but not an entire bitstream worth of RAM, enabling a fly-by update flow at the cost of a second program/erase cycle on that one flash sector which is probably OK.&lt;/p&gt;
    &lt;head rend="h3"&gt;SCPI&lt;/head&gt;
    &lt;p&gt;The SCPI interface is the primary remote control interface for application layer access to the crossbar. It provides a standard SCPI-compliant *IDN? command a well as custom commands for controlling the actual crossbar matrix, setting input thresholds and output levels, and accessing the BERT.&lt;/p&gt;
    &lt;p&gt;Crossbar paths can be configured in ngscopeclient by drawing connections in the filter graph from source port to sink port.&lt;/p&gt;
    &lt;p&gt;The BERT works pretty much like any other BERT supported by ngscopeclient. You can configure TX/RX bit rate, inversion, PRBS pattern or custom arbitrary pattern, NRZ baud rates from 625 Mbps to 10.3125 Gbps, etc. TX-side swing and pre/postcursor equalizer taps are also easily controlled from the channel properties dialog.&lt;/p&gt;
    &lt;p&gt;Thereâs still a few things I want to tweak. RX side equalization is currently fixed until I figure out how to properly tune the 7 series GTX receiver via the DRP. I havenât implemented long-duration single point BER measurements, oversampling density plot mode, or offset sampling single-point scans.&lt;/p&gt;
    &lt;p&gt;The BERT inputs also contain an incomplete âCDR-based logic analyzerâ feature. Essentially the raw GTX output is fed through 8b/10b or 64b/66b decoders and into a pattern matching block; once the requested trigger event is seen the LA will trigger and capture about a megapoint of raw line coded serial bits into block RAM then output to ngscopeclient as a waveform.&lt;/p&gt;
    &lt;p&gt;Eventually I want to finish building out various pattern triggers as well as integrating the CDR block with the trigger crossbar proper, such that a CDR pattern match can trigger an oscilloscope or other instrument.&lt;/p&gt;
    &lt;p&gt;The current gateware also provides a fixed 10.3125 Gbps PRBS-31 on the front panel âsyncâ port although I will likely make the baud rate and polynomial configurable at some point (basically a third output-only BERT channel). The intended use here is a deskew reference signal for use with ngscopeclientâs multi instrument sync feature, allowing the cross-trigger delay between multiple instruments to be automatically calibrated out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;This was my first large, standalone, rackmountable, network connected project that Iâve taken to something resembling completion in a long time (although Iâm sure Iâll be continuing to poke at firmware for some time since the feature set isnât quite where I want it). I learned a lot of things not to do, ranging from PCB design to mounting hole positionining to the awful OCTOSPI.&lt;/p&gt;
    &lt;p&gt;But itâs a useful tool I work with in my lab on a regular basis, and proved out a lot of software and hardware building blocks and techniques that I plan to use in many of my future projects, such as the Ethernet switch.&lt;/p&gt;
    &lt;p&gt;Like this post? Drop me a comment on Mastodon&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45243740</guid><pubDate>Sun, 14 Sep 2025 22:08:13 +0000</pubDate></item><item><title>Grapevine canes can be converted into plastic-like material that will decompose</title><link>https://www.sdstate.edu/news/2025/08/can-grapevines-help-slow-plastic-waste-problem</link><description>&lt;doc fingerprint="f488c5b0bf730735"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Can grapevines help slow the plastic waste problem?&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;A new study from South Dakota State University reveals how grapevine canes can be converted into plastic-like material that is stronger than traditional plastic and will decompose in the environment in a relatively short amount of time.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The need for biodegradable packaging material has never been higher.&lt;/p&gt;
    &lt;p&gt;Currently, most packaging is "single use" and is made with plastic materials, derived from nonrenewable sources like crude oil that take hundreds of years to decompose in the environment. On top of this, only 9% of plastic is recycled. This has resulted in the formation of floating piles of plastic garbage in the ocean, called the "Great Pacific Garbage Patch."&lt;/p&gt;
    &lt;p&gt;But maybe even more concerning is the discovery of micro- and nano-plastics in the environment. Research has found that plastic breaks down into tiny particles, which are being ingested or inhaled by both humans and animals, and are found literally everywhere, including in the human body — according to recent research studies. Worse, little is known about the long-term health effects of microplastics.&lt;/p&gt;
    &lt;p&gt;Srinivas Janaswamy is an associate professor in South Dakota State University's Department of Dairy and Food Science. His research has focused on developing value-added products through biowaste and agricultural byproducts. One of the overarching goals of Janaswamy's research is to tackle the plastic waste crisis.&lt;/p&gt;
    &lt;p&gt;Perhaps the biggest contributor to plastic waste, at least in the United States, is plastic bags, the kind found at most retail stores. These bags, while sometimes recycled, are often only used once and can be found littered throughout the environment.&lt;/p&gt;
    &lt;p&gt;To address this problem, Janaswamy is working toward developing a plastic-like bag that will decompose in the environment.&lt;/p&gt;
    &lt;p&gt;"That is my dream," Janaswamy said.&lt;/p&gt;
    &lt;p&gt;The key ingredient to Janaswamy's work? Cellulose. This biopolymer is the most abundant organic substance on Earth and is found, primarily, in the cell walls of plants. Cellulose, thanks to strong hydrogen bonds and a chain of glucose molecules, gives plants structural strength and rigidity along with other biopolymers such as mannan, xylose, hemicellulose and lignin.&lt;/p&gt;
    &lt;p&gt;Humans have long used cellulose to create products. Cotton, the material used to make a majority of the world’s clothing, is primarily composed of cellulose. Wood is rich in cellulose as well.&lt;/p&gt;
    &lt;p&gt;In previous research, Janaswamy has extracted cellulose from agricultural products like avocado peels, soyhulls, alfalfa, switchgrass, spent coffee grounds, corncob and banana peels. He uses the extracted cellulose to develop films — materials that look and feel similar to traditional plastic wrapping.&lt;/p&gt;
    &lt;p&gt;"By extracting cellulose from agricultural products, value-added products can be created," Janaswamy said.&lt;/p&gt;
    &lt;p&gt;Each of Janaswamy's films has different characteristics and properties. Some are more transparent than others. Some are stronger. But thanks to a unique collaboration with a fellow SDSU faculty member, Janaswamy may have created his best value-added product yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Grapevine canes&lt;/head&gt;
    &lt;p&gt;Janaswamy had just finished presenting “Ag Biomass – A Holy Grail to Clean up the Plastic Mess” at SDSU's Celebration of Faculty Excellence when he was approached by Anne Fennell, a Distinguished Professor in the Department of Agronomy, Horticulture and Plant Science.&lt;/p&gt;
    &lt;p&gt;After listening to Janaswamy's presentation, Fennell became interested in the research and had an idea. A leading researcher in the study of grapevines, she knew that grapevine canes — the woody plant material that grapes grow on — were rich in cellulose. She also knew that grapevine canes were abundant and had limited use after harvest.&lt;/p&gt;
    &lt;p&gt;"Every year we prune the majority of yearly biomass off the vine," Fennell said. "The pruned canes are either mowed over, composted and reapplied to the soil, or burned in some areas. Research in Australia showed that prunings could be removed from the field in alternate years without effecting soil health. My thought was why not use this for value added films. Several of the materials that Janaswamy previously used had a high-water content, in contrast the winter pruning yields a cellulose-dense material with low water content, making them an abundant ideal material to work with."&lt;/p&gt;
    &lt;p&gt;Fennell's idea led to a collaboration, and soon Janaswamy was extracting cellulose — which looks almost like cotton — from the canes of grapevines. The resulting films were eye-opening.&lt;/p&gt;
    &lt;p&gt;According to a recent study published in the academic journal Sustainable Food Technology, Janaswamy's grapevine cane films are transparent and strong and biodegrade within 17 days in the soil — leaving behind no harmful residue.&lt;/p&gt;
    &lt;p&gt;"High transmittance in packaging films enhances product visibility, making them more attractive to consumers and facilitating easy quality inspection without the need for unsealing," Janaswamy said. "These films demonstrate outstanding potential for food packaging applications."&lt;/p&gt;
    &lt;p&gt;The grapevine canes were harvested from SDSU's research vineyard. The research team, which includes doctoral candidates Sandeep Paudel and Sumi Regmi, and Sajal Bhattarai, an SDSU graduate and a doctoral candidate at Purdue University, followed a published protocol in developing the films, which includes drying and grinding the canes and extracting the cellulosic residue. The residue was then solubilized and cast onto glass plates to create the films.&lt;/p&gt;
    &lt;p&gt;Testing revealed the grapevine cane-derived films were actually stronger than traditional plastic bags — in terms of tensile strength.&lt;/p&gt;
    &lt;p&gt;"Using underutilized grapevine prunings as a cellulose source for packaging films enhances waste management in the field and addresses the global issue of plastic pollution," Janaswamy said. "Developing eco-friendly films from grapevine cellulose represents a practical approach to sustainability, helping to conserve the environment and its resources and contributing to the circular bioeconomy."&lt;/p&gt;
    &lt;p&gt;The results of this work move Janaswamy one step closer to his dream of developing a bag made from a plastic-like material that will quickly decompose in the environment.&lt;/p&gt;
    &lt;p&gt;Funding for this research was provided by the U.S. Department of Agriculture's National Institute of Food and Agriculture and the National Science Foundation.&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;Republishing&lt;/p&gt;
    &lt;p&gt;You may republish SDSU News Center articles for free, online or in print. Questions? Contact us at sdsu.news@sdstate.edu or 605-688-6161.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45243803</guid><pubDate>Sun, 14 Sep 2025 22:15:33 +0000</pubDate></item><item><title>Titania Programming Language</title><link>https://github.com/gingerBill/titania</link><description>&lt;doc fingerprint="cc9bde28ba1db0de"&gt;
  &lt;main&gt;
    &lt;p&gt;Based on the Oberon-07 programming language designed by the late Niklaus Wirth.&lt;/p&gt;
    &lt;p&gt;This is designed to be a language to teach compiler development with.&lt;/p&gt;
    &lt;p&gt;Meaning behind the name:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Titania is the wife of Oberon (Fairy King) in Shakespeare's A Midsummer Night's Dream&lt;/item&gt;
      &lt;item&gt;https://en.wikipedia.org/wiki/Titania_(A_Midsummer_Night%27s_Dream)&lt;/item&gt;
      &lt;item&gt;This is just a codename, and probably not final for this teaching language&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;module = "module" ident ";" [import_list] decl_sequence
         ["begin" stmt_sequence] "end" [";"].

import_list = "import" import_decl {"," import_decl} ";".
decl_sequence = ["const" {const_decl ";"}]
                ["type"  {type_decl  ";"}]
                ["var"   {var_decl   ";"}]
                [{proc_decl          ";"}].

const_decl = ident "=" const_expr.
type_decl = ident "="" struct_type.
var_decl = ident_list ":" type.

proc_decl = "proc" ident [formal_parameters] ";" proc_body.
proc_body = decl_sequence ["begin" stmt_sequence] ["return" expr] "end".


const_expr = expr.
expr = simple_expr {relation simple_expr}.

simple_expr = ["+" | "-"] unary_expr {add_operator unary_expr}.
unary_expr = ["+" | "-"] term.
term = factor {mul_operator factor}.

factor = integer | real | string | nil | true | false | set |
         "(" expr ")" | "not" expr | designator.

element = expr [".." expr].

ident_list = ident {"," ident}.
qual_ident = [ident "."] ident.

struct_type = array_type | record_type | pointer_type | proc_type.
array_type = "["" const_expr {"," const_expr} "]" type.
record_type = "record" ["(" qual_ident ")"] [field_list_sequence] "end".
pointer_type = "^" type.
proc_type = "proc" formal_parameters.
field_list = ident_list ":" type.
formal_parmeters = "(" [fp_section {";" fp_section}] [";"] ")".
formal_type = "[" "]" qual_ident.

stmt_sequence = stmt {";" stmt} [";"].
stmt = [assignment | proc_call | if_stmt | case_stmt | while_stmt | repeat_stmt | for_stmt ].

assignment = designator ":=" expr

if_stmt = "if" expr "then" stmt_sequence
          {"elseif" expr "then" stmt_sequence}
          ["else" stmt_sequence]
          "end".

case_stmt = "case" expr "of" case {"|" case} "end".
case = [case_label_list ":" stmt_sequence].
case_list = label_range {"," label_range}.
label_range = label [".." label].
label = integer | string | qual_ident.

while_stmt = "while" expr "do" stmt_sequence
             {"elseif" expr "then" stmt_sequence}
             "end".
repeat_stmt = "repeat" stmt_sequence "until" expr.
for_stmt = "for" ident ":=" expr "to" expr ["by" const_expr] "do" stmt_sequence "end".


designator = qual_ident {selector}.
selector = "." ident | "[" expr_list "]" | "^" | "(" qual_ident ")".
expr_list = expr {"," expr}.


add_operator = "+" | "-" | "xor" | "or".
mul_operator = "*" | "/" | "%"   | "and".
relation     = "=" | "&amp;lt;&amp;gt;" | "&amp;lt;" | "&amp;lt;=" | "&amp;gt;" | "&amp;gt;=" | "in" | "is".
&lt;/code&gt;
    &lt;code&gt;and    else    import  of      then   while
begin  elseif  in      or      to     xor
by     end     is      proc    true
case   false   module  record  type
const  for     nil     repeat  until
do     if      not     return  var
&lt;/code&gt;
    &lt;code&gt;+    .   (   )   =  &amp;lt;&amp;gt;
-    ,   [   ]   &amp;lt;  &amp;lt;=
*    ;   {   }   &amp;gt;  &amp;gt;=
/    |   :=  :   ..
%    ^
&lt;/code&gt;
    &lt;p&gt;Note: These will be added to as the compiler develops&lt;/p&gt;
    &lt;code&gt;abs(x)            - absolute value of
lsh(x, y)         - logical shift left
ash(x, y)         - arithmetic shift right
ror(x, y)         - rotate right
chr(i)            - convert int to char
ord(c)            - convert char to int
inc(x)            - x := x + 1
inc(x, y)         - x := x + y
dec(x)            - x := x - 1
dec(x, y)         - x := x - y
incl(x, y)        - include y in set x
excl(x, y)        - exclude y in set x
odd(x)            - x % 2 = 0
floor(x)          - round-down for real
ceil(x)           - round-up   for real
assert(cond)      - assert when cond is false
new(ptr)          - allocate memory
delete(ptr)       - free memory
addr(x)           - address of addressable memory
size_of(x)        - size of the type of 'x'
align_of(x)       - alignment of the type of 'x'
copy(dst, src, n) - non-overlapping memory copying from `src` to `dst` of `n` bytes
print(...)        - variadic print without newline
println(...)      - variadic print with newline
len(x)            - length of an array 'x'
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45243925</guid><pubDate>Sun, 14 Sep 2025 22:29:49 +0000</pubDate></item><item><title>For Good First Issue – A repository of social impact and open source projects</title><link>https://forgoodfirstissue.github.com/</link><description>&lt;doc fingerprint="7b5002672bfb2689"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Committing to a better future&lt;/head&gt;
    &lt;p&gt;Lend your skills to an open source project focused on the Digital Public Goods (DPGs). From fighting climate change, to solving world hunger, your efforts will contribute to creating a better future for everyone. Together, we can drive positive and lasting contributions to the world, one commit at a time.&lt;lb/&gt;Explore a DPG repo below to get started.&lt;/p&gt;
    &lt;head rend="h2"&gt;Find a project&lt;/head&gt;
    &lt;p&gt;The participatory democracy framework. A generator and multiple gems made with Ruby on Rails&lt;/p&gt;
    &lt;p&gt;ODK Central is a server that is easy to use, very fast, and stuffed with features that make data collection easier. Contribute and make the world a better place! ✨🗄✨&lt;/p&gt;
    &lt;p&gt;📰 Diários oficiais brasileiros acessíveis a todos | 📰 Brazilian government gazettes, accessible to everyone.&lt;/p&gt;
    &lt;p&gt;Augmentative and Alternative Communication (AAC) system with text-to-speech for the browser&lt;/p&gt;
    &lt;p&gt;Documents added by volunteer contributors and historically imported from TOSBack.org. Maintenance is collaborative and volunteer-based.&lt;/p&gt;
    &lt;p&gt;OpenFn/Lightning ⚡️ is the newest version of the OpenFn DPG and provides a web UI to visually manage complex workflow automation projects.&lt;/p&gt;
    &lt;p&gt;A collection of tools for extracting FHIR resources and analytics services on top of that data.&lt;/p&gt;
    &lt;p&gt;Source code of the X-Road® data exchange layer software&lt;/p&gt;
    &lt;p&gt;Book repository for The Turing Way: a how to guide for reproducible, ethical and collaborative data science&lt;/p&gt;
    &lt;p&gt;Volunteer management system for nonprofit CASA, which serves foster youth in counties across America.&lt;/p&gt;
    &lt;p&gt;ODK Collect is an Android app for filling out forms. It's been used to collect billions of data points in challenging environments around the world. Contribute and make the world a better place! ✨📋✨&lt;/p&gt;
    &lt;p&gt;The CHT Core Framework makes it faster to build responsive, offline-first digital health apps that equip health workers to provide better care in their communities. It is a central resource of the Community Health Toolkit.&lt;/p&gt;
    &lt;p&gt;Mautic: Open Source Marketing Automation Software.&lt;/p&gt;
    &lt;p&gt;PolicyEngine's free web app for computing the impact of public policy.&lt;/p&gt;
    &lt;p&gt;Open source, Open standards based Decentralised Identity &amp;amp; Verifiable Credentials Platform&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45245313</guid><pubDate>Mon, 15 Sep 2025 02:02:39 +0000</pubDate></item><item><title>Which NPM package has the largest version number?</title><link>https://adamhl.dev/blog/largest-number-in-npm-package/</link><description>&lt;doc fingerprint="2b5d6a05af70ae19"&gt;
  &lt;main&gt;
    &lt;p&gt;I was recently working on a project that uses the AWS SDK for JavaScript. When updating the dependencies in said project, I noticed that the version of that dependency was &lt;code&gt;v3.888.0&lt;/code&gt;. Eight hundred eighty eight. That’s a big number as far as versions go.&lt;/p&gt;
    &lt;p&gt;That got me thinking: I wonder what package in the npm registry has the largest number in its version. It could be a major, minor, or patch version, and it doesn’t have to be the latest version of the package. In other words, out of the three numbers in &lt;code&gt;&amp;lt;major&amp;gt;.&amp;lt;minor&amp;gt;.&amp;lt;patch&amp;gt;&lt;/code&gt; for each version for each package, what is the largest number I can find?&lt;/p&gt;
    &lt;p&gt;TL;DR? Jump to the results to see the answer.&lt;/p&gt;
    &lt;head rend="h2"&gt;The npm API&lt;/head&gt;
    &lt;p&gt;Obviously npm has some kind of API, so it shouldn’t be too hard to get a list of all… 3,639,812 packages. Oh. That’s a lot of packages. Well, considering npm had 374 billion package downloads in the past month, I’m sure they wouldn’t mind me making a few million HTTP requests.&lt;/p&gt;
    &lt;p&gt;Doing a quick search for “npm api” leads me to a readme in the npm/registry repo on GitHub. There’s a &lt;code&gt;/-/all&lt;/code&gt; endpoint listed in the table of contents which seems promising. That section doesn’t actually exist in the readme, but maybe it still works?&lt;/p&gt;
    &lt;p&gt;Whelp, maybe npm packages have an ID and I can just start at 1 and count up? It looks like packages have an &lt;code&gt;_id&lt;/code&gt; field… never mind, the &lt;code&gt;_id&lt;/code&gt; field is the package name. Okay, let’s try to find something else.&lt;/p&gt;
    &lt;p&gt;A little more digging brings me to this GitHub discussion about the npm replication API. So npm replicates package info in CouchDB at &lt;code&gt;https://replicate.npmjs.com&lt;/code&gt;, and conveniently, they support the &lt;code&gt;_all_docs&lt;/code&gt; endpoint. Let’s give that a try:&lt;/p&gt;
    &lt;p&gt;Those are some interesting package names. Looks like this data is paginated and by default I get 1,000 packages at a time. When I write the final script, I can set the &lt;code&gt;limit&lt;/code&gt; query parameter to the max of 10,000 to make pagination a little less painful.&lt;/p&gt;
    &lt;p&gt;Fortunately, the CouchDB docs have a guide for pagination, and it looks like it’s as simple as using the &lt;code&gt;skip&lt;/code&gt; query parameter.&lt;/p&gt;
    &lt;p&gt;Never mind. According to the GitHub discussion linked above, &lt;code&gt;skip&lt;/code&gt; is no longer supported. The “Paging (Alternate Method)” section of the same page says that I can use &lt;code&gt;startkey_docid&lt;/code&gt; instead. If I grab the &lt;code&gt;id&lt;/code&gt; of the last row, I should be able to use that to return the next set of rows. Fun fact: The 1000th package (alphabetically) on npm is &lt;code&gt;03-webpack-number-test&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Nice. Also, another &lt;code&gt;3628102 - 3628088 = 14&lt;/code&gt; packages have been published in the ~15 minutes since I ran the last query.&lt;/p&gt;
    &lt;p&gt;Now, there’s one more piece of the puzzle to figure out. How do I get all the versions for a given package? Unfortunately, it doesn’t seem like I can get package version information along with the base info returned by &lt;code&gt;_all_docs&lt;/code&gt;. I have to separately fetch each package’s metadata from &lt;code&gt;https://registry.npmjs.org/&amp;lt;package_id&amp;gt;&lt;/code&gt;. Let’s see what good ol’ trusty &lt;code&gt;03-webpack-number-test&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;p&gt;Alright, I have everything I need. Now I just need to write a bash script that— just kidding. A wise programmer once said, “if your shell script is more than 10 lines, it shouldn’t be a shell script” (that was me, I said that). I like TypeScript, so let’s use that.&lt;/p&gt;
    &lt;p&gt;The biggest bottleneck is going to be waiting on the &lt;code&gt;GET&lt;/code&gt;s for each package’s metadata. My plan is this:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Grab all the package IDs from the replication API and save that data to a file (I don’t want to have to refetch everything if the something goes wrong later in the script)&lt;/item&gt;
      &lt;item&gt;Fetch package data in batches so we’re not just doing 1 HTTP request at a time&lt;/item&gt;
      &lt;item&gt;Save the package data to a file (again, hopefully I only have to fetch everything once)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once I have all the package data, I can answer the original question of “largest number in version” and look at a few other interesting things.&lt;/p&gt;
    &lt;p&gt;(A few hours and many iterations later…)&lt;/p&gt;
    &lt;p&gt;See the script section at the end if you want to see what it looks like.&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;Some stats:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Time to fetch all ~3.6 million package IDs: A few minutes&lt;/item&gt;
      &lt;item&gt;Time to fetch version data for each one of those packages: ~12 hours (yikes)&lt;/item&gt;
      &lt;item&gt;Packages fetched per second: ~84 packages/s&lt;/item&gt;
      &lt;item&gt;Size of &lt;code&gt;package-ids.json&lt;/code&gt;: ~78MB&lt;/item&gt;
      &lt;item&gt;Size of &lt;code&gt;package-data.json&lt;/code&gt;: ~886MB&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And the winner is… (not really) latentflip-test at version &lt;code&gt;1000000000000000000.1000000000000000000.1000000000000000000&lt;/code&gt;. And no, there haven’t actually been one quintillion major versions of this package published. Disappointing, I know.&lt;/p&gt;
    &lt;p&gt;Okay, I feel like that shouldn’t count. I think we can do better and find a “real” package that actually follows semantic versioning. I think a better question to ask is this:&lt;/p&gt;
    &lt;p&gt;For packages that follow semantic versioning, which package has the largest number from &lt;code&gt;&amp;lt;major&amp;gt;.&amp;lt;minor&amp;gt;.&amp;lt;patch&amp;gt;&lt;/code&gt; in any of its versions?&lt;/p&gt;
    &lt;p&gt;So, what does it mean to “follow semantic versioning”? Should we “disqualify” a package for skipping a version number? In this case, I think we’ll just say that a package has to have more versions published than the largest number we find for that package. For example, a package with a version of &lt;code&gt;1.888.0&lt;/code&gt; will have had at least 888 versions published if it actually followed semver.&lt;/p&gt;
    &lt;p&gt;Before we get to the real winner, here are the top 10 packages by total number of versions published:&lt;/p&gt;
    &lt;p&gt;Top 10 packages that (probably) follow semver by largest number in one of its versions:&lt;/p&gt;
    &lt;p&gt;So it seems like the winner is @mahdiarjangi/phetch-cli with &lt;code&gt;19494&lt;/code&gt;, right? Unfortunately, I’m not going to count that either. It only has so many versions because of a misconfigured GitHub action that published new versions in a loop.&lt;/p&gt;
    &lt;p&gt;I manually went down the above list, disqualifying any packages that had similar issues. I also checked that “new” versions actually differed from previous versions in terms of content. Overall, I looked for a package that was actually publishing new versions on purpose with some kind of change to the package content.&lt;/p&gt;
    &lt;p&gt;The real winner (#19 on the list) is: all-the-package-names with &lt;code&gt;2401&lt;/code&gt; from version &lt;code&gt;2.0.2401&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Well, that’s sort of disappointing, but also kind of funny. I don’t know what I was expecting to be honest. If you’re curious, you can see more results at the bottom of this post.&lt;/p&gt;
    &lt;p&gt;What you do with all of this extremely important and useful information is up to you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Script&lt;/head&gt;
    &lt;head rend="h2"&gt;More Results&lt;/head&gt;
    &lt;p&gt;This is from the script:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45245678</guid><pubDate>Mon, 15 Sep 2025 03:03:18 +0000</pubDate></item><item><title>Language Models Pack Billions of Concepts into 12k Dimensions</title><link>https://nickyoder.com/johnson-lindenstrauss/</link><description>&lt;doc fingerprint="150b027d080f93db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Beyond Orthogonality: How Language Models Pack Billions of Concepts into 12,000 Dimensions&lt;/head&gt;
    &lt;p&gt;In a recent 3Blue1Brown video series on transformer models, Grant Sanderson posed a fascinating question: How can a relatively modest embedding space of 12,288 dimensions (GPT-3) accommodate millions of distinct real-world concepts?&lt;/p&gt;
    &lt;p&gt;The answer lies at the intersection of high-dimensional geometry and a remarkable mathematical result known as the Johnson-Lindenstrauss lemma. While exploring this question, I discovered something unexpected that led to an interesting collaboration with Grant and a deeper understanding of vector space geometry.&lt;/p&gt;
    &lt;p&gt;The key insight begins with a simple observation: while an N-dimensional space can only hold N perfectly orthogonal vectors, relaxing this constraint to allow for "quasi-orthogonal" relationships (vectors at angles of, say, 85-95 degrees) dramatically increases the space's capacity. This property is crucial for understanding how language models can efficiently encode semantic meaning in relatively compact embedding spaces.&lt;/p&gt;
    &lt;p&gt;In Grant's video, he demonstrated this principle with an experiment attempting to fit 10,000 unit vectors into a 100-dimensional space while maintaining near-orthogonal relationships. The visualization suggested success, showing angles clustered between 89-91 degrees. However, when I implemented the code myself, I noticed something interesting about the optimization process.&lt;/p&gt;
    &lt;p&gt;The original loss function was elegantly simple:&lt;/p&gt;
    &lt;p&gt; loss = (dot_products.abs()).relu().sum()&lt;lb/&gt;While this loss function appears perfect for an unbounded ℝᴺ space, it encounters two subtle but critical issues when applied to vectors constrained to a high-dimensional unit sphere:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The Gradient Trap: The dot product between vectors is the cosine of the angle between them, and the gradient is the sine of this angle. This creates a perverse incentive structure: when vectors approach the desired 90-degree relationship, the gradient (sin(90°) = 1.0) strongly pushes toward improvement. However, when vectors drift far from the goal (near 0° or 180°), the gradient (sin(0°) ≈ 0) vanishes—effectively trapping these badly aligned vectors in their poor configuration.&lt;/item&gt;
      &lt;item&gt;The 99% Solution: The optimizer discovered a statistically favorable but geometrically perverse solution. For each vector, it would be properly orthogonal to 9,900 out of 9,999 other vectors while being nearly parallel to just 99. This configuration, while clearly not the intended outcome, actually represented a global minimum for the loss function—mathematically similar to taking 100 orthogonal basis vectors and replicating each one roughly 100 times.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This stable configuration was particularly insidious because it satisfied 99% of the constraints while being fundamentally different from the desired constellation of evenly spaced, quasi-orthogonal vectors. To address this, I modified the loss function to use an exponential penalty that increases aggressively as dot products grow:&lt;/p&gt;
    &lt;p&gt;loss = exp(20*dot_products.abs()**2).sum() (Full code here)&lt;/p&gt;
    &lt;p&gt;This change produced the desired behavior, though with a revealing result: the maximum achievable pairwise angle was around 76.5 degrees, not 89 degrees.&lt;/p&gt;
    &lt;p&gt;This discovery led me down a fascinating path exploring the fundamental limits of vector packing in high-dimensional spaces, and how these limits relate to the Johnson-Lindenstrauss lemma.&lt;/p&gt;
    &lt;p&gt;When I shared these findings with Grant, his response exemplified the collaborative spirit that makes the mathematics community so rewarding. He not only appreciated the technical correction but invited me to share these insights with the 3Blue1Brown audience. This article is that response, expanded to explore the broader implications of these geometric properties for machine learning and dimensionality reduction.&lt;/p&gt;
    &lt;head rend="h1"&gt;The Johnson-Lindenstrauss Lemma: A Geometric Guarantee&lt;/head&gt;
    &lt;p&gt;At its core, the Johnson-Lindenstrauss (JL) lemma makes a remarkable promise: you can project points from an arbitrarily high-dimensional space into a surprisingly low-dimensional space while preserving their relative distances with high probability. What makes this result particularly striking is that the required dimensionality of the low-dimensional space grows only logarithmically with the number of points you want to project.&lt;/p&gt;
    &lt;p&gt;Formally, the lemma states that for an error factor ε (between 0 and 1), and any set of N points in a high-dimensional space, there exists a projection into k dimensions where for any two points u and v in the original space, their projections f(u) and f(v) in the lower dimensional space satisfy:&lt;/p&gt;
    &lt;p&gt;(1 - ε)||u - v||² ≤ ||f(u) - f(v)||² ≤ (1 + ε)||u - v||²&lt;/p&gt;
    &lt;p&gt;The number of dimensions (k) required to guarantee these error bounds is given by:&lt;/p&gt;
    &lt;p&gt;k ≥ O(log(N)/ε²)&lt;/p&gt;
    &lt;p&gt;The "Big O" notation can be replaced with a concrete constant C:&lt;/p&gt;
    &lt;p&gt;k ≥ (C/ε²) * log(N)&lt;/p&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;k is the target dimension&lt;/item&gt;
      &lt;item&gt;N is the number of points&lt;/item&gt;
      &lt;item&gt;ε is the maximum allowed distortion&lt;/item&gt;
      &lt;item&gt;C is a constant that determines the probability of success&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While most practitioners use values between 4 and 8 as a conservative choice for random projections, the optimal value of C remains an open question. As we'll see in the experimental section, engineered projections can achieve much lower values of C, with profound implications for embedding space capacity.&lt;/p&gt;
    &lt;p&gt;The fascinating history of this result speaks to the interconnected nature of mathematical discovery. Johnson and Lindenstrauss weren't actually trying to solve a dimensionality reduction problem – they stumbled upon this property while working on extending Lipschitz functions in Banach spaces. Their 1984 paper turned out to be far more influential in computer science than in their original domain.&lt;/p&gt;
    &lt;head rend="h1"&gt;From Theory to Practice: Two Domains of Application&lt;/head&gt;
    &lt;p&gt;The JL lemma finds practical application in two distinct but equally important domains:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Dimensionality Reduction: Consider an e-commerce platform like Amazon, where each customer's preferences might be represented by a vector with millions of dimensions (one for each product). Direct computation with such vectors would be prohibitively expensive. The JL lemma tells us we can project this data into a much lower-dimensional space – perhaps just a thousand dimensions – while preserving the essential relationships between customers. This makes previously intractable computations feasible on a single GPU, enabling real-time customer relationship management and inventory planning.&lt;/item&gt;
      &lt;item&gt;Embedding Space Capacity: This application is more subtle but equally powerful. Rather than actively projecting vectors, we're interested in understanding how many distinct concepts can naturally coexist in a fixed-dimensional space. This is where our experiments provide valuable insight into the practical limits of embedding space capacity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let's consider what we mean by "concepts" in an embedding space. Language models don't deal with perfectly orthogonal relationships – real-world concepts exhibit varying degrees of similarity and difference. Consider these examples of words chosen at random:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"Archery" shares some semantic space with "precision" and "sport"&lt;/item&gt;
      &lt;item&gt;"Fire" overlaps with both "heat" and "passion"&lt;/item&gt;
      &lt;item&gt;"Gelatinous" relates to physical properties and food textures&lt;/item&gt;
      &lt;item&gt;"Southern-ness" encompasses culture, geography, and dialect&lt;/item&gt;
      &lt;item&gt;"Basketball" connects to both athletics and geometry&lt;/item&gt;
      &lt;item&gt;"Green" spans color perception and environmental consciousness&lt;/item&gt;
      &lt;item&gt;"Altruistic" links moral philosophy with behavioral patterns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beauty of high-dimensional spaces is that they can accommodate these nuanced, partial relationships while maintaining useful geometric properties for computation and inference.&lt;/p&gt;
    &lt;head rend="h1"&gt;Empirical Investigation of Embedding Capacity&lt;/head&gt;
    &lt;p&gt;When we move from random projections to engineered solutions, the theoretical bounds for C of the JL lemma become surprisingly conservative. While a Hadamard matrix transformation with random elements can reliably achieve a C value between 2.5 and 4 in a single pass, our GPU experiments suggest even more efficient arrangements are possible through optimization.&lt;/p&gt;
    &lt;p&gt;To explore these limits, I implemented a series of experiments projecting standard basis vectors into spaces of varying dimensionality. Using GPU acceleration, I tested combinations of N (number of vectors) up to 30,000 and k (embedding dimensions) up to 10,000, running each optimization for 50,000 iterations. The results reveal some fascinating patterns:&lt;/p&gt;
    &lt;p&gt;Several key observations emerge from this data:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The value of C initially rises with N, reaching a maximum around ~0.9 (notably always below 1.0)&lt;/item&gt;
      &lt;item&gt;After peaking, C begins a consistent downward trend&lt;/item&gt;
      &lt;item&gt;At high ratios of N to K, we observe C values trend below 0.2&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This behavior likely relates to an interesting property of high-dimensional geometry: as dimensionality increases, sphere packing becomes more efficient when the spheres are small relative to the unit sphere. This suggests that our observed upper bounds on C might still be conservative for very large numbers of concepts.&lt;/p&gt;
    &lt;head rend="h1"&gt;Practical Implications for Language Models&lt;/head&gt;
    &lt;p&gt;Let's consider three scenarios for the constant C:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;C = 4: A conservative choice for random projections with Hadamard matrices&lt;/item&gt;
      &lt;item&gt;C = 1: A likely upper bound for optimized or emergent embeddings&lt;/item&gt;
      &lt;item&gt;C = 0.2: A value suggested by our experiments for very large spaces&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The implications of these geometric properties are staggering. Let's consider a simple way to estimate how many quasi-orthogonal vectors can fit in a k-dimensional space. If we define F as the degrees of freedom from orthogonality (90° - desired angle), we can approximate the number of vectors as:&lt;/p&gt;
    &lt;p&gt;Vectors ≈ 10^(k * F² / 1500)&lt;/p&gt;
    &lt;p&gt;where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;k is the embedding dimension&lt;/item&gt;
      &lt;item&gt;F is the degrees of "freedom" from orthogonality (e.g., F = 3 for 87° angles)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Applying this to GPT-3's 12,288-dimensional embedding space reveals its extraordinary capacity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;At 89° (F = 1): approximately 10^8 vectors&lt;/item&gt;
      &lt;item&gt;At 88° (F = 2): approximately 10^32 vectors&lt;/item&gt;
      &lt;item&gt;At 87° (F = 3): approximately 10^73 vectors&lt;/item&gt;
      &lt;item&gt;At 85° (F = 5): more than 10^200 vectors&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To put this in perspective, even the conservative case of 86° angles provides capacity far exceeding the estimated number of atoms in the observable universe (~10^80). This helps explain how language models can maintain rich, nuanced relationships between millions of concepts while working in relatively modest embedding dimensions.&lt;/p&gt;
    &lt;head rend="h1"&gt;Practical Applications and Future Directions&lt;/head&gt;
    &lt;p&gt;The insights from this investigation have two major practical implications:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Efficient Dimensionality Reduction: The robustness of random projections, particularly when combined with Hadamard transformations (or BCH coding), provides a computationally efficient way to work with high-dimensional data. No complex optimization required – the mathematics of high-dimensional spaces does the heavy lifting for us.&lt;/item&gt;
      &lt;item&gt;Embedding Space Design: Understanding the true capacity of high-dimensional spaces helps explain how transformer models can maintain rich, nuanced representations of language in relatively compact embeddings. Concepts like "Canadian," "morose," "Hitchcockian," "handsome," "whimsical," and "Muppet-like" can all find their place in the geometry while preserving their subtle relationships to each other.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This research suggests that current embedding dimensions (1,000-20,000) provide more than adequate capacity for representing human knowledge and reasoning. The challenge lies not in the capacity of these spaces but in learning the optimal arrangement of concepts within them.&lt;/p&gt;
    &lt;p&gt;My code for Hadamard and optimized projections.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;What began as an investigation into a subtle optimization issue has led us to a deeper appreciation of high-dimensional geometry and its role in modern machine learning. The Johnson-Lindenstrauss lemma, discovered in a different context nearly four decades ago, continues to provide insight into the foundations of how we can represent meaning in mathematical spaces.&lt;/p&gt;
    &lt;p&gt;I want to express my sincere gratitude to Grant Sanderson and the 3Blue1Brown channel. His work consistently inspires deeper exploration of mathematical concepts, and his openness to collaboration exemplifies the best aspects of the mathematical community. The opportunity to contribute to this discussion has been both an honor and a genuine pleasure.&lt;/p&gt;
    &lt;p&gt;I would also like to thank Suman Dev for his help in optimizing the GPU code.&lt;/p&gt;
    &lt;p&gt;This was enormously fun to research and write.&lt;/p&gt;
    &lt;p&gt;Nick Yoder&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Further Reading&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Sphere Packings, Lattices and Groups by Conway and Sloane&lt;/item&gt;
      &lt;item&gt;Database-friendly random projections: Johnson-Lindenstrauss with binary coins by Achlioptas&lt;/item&gt;
      &lt;item&gt;Hadamard Matrices, Sequences, and Block Designs by Seberry and Yamada&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45245948</guid><pubDate>Mon, 15 Sep 2025 03:54:20 +0000</pubDate></item><item><title>A qualitative analysis of pig-butchering scams</title><link>https://arxiv.org/abs/2503.20821</link><description>&lt;doc fingerprint="7c9cd00988b1068b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Cryptography and Security&lt;/head&gt;&lt;p&gt; [Submitted on 25 Mar 2025 (v1), last revised 24 May 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:"Hello, is this Anna?": Unpacking the Lifecycle of Pig-Butchering Scams&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Pig-butchering scams have emerged as a complex form of fraud that combines elements of romance, investment fraud, and advanced social engineering tactics to systematically exploit victims. In this paper, we present the first qualitative analysis of pig-butchering scams, informed by in-depth semi-structured interviews with $N=26$ victims. We capture nuanced, first-hand accounts from victims, providing insight into the lifecycle of pig-butchering scams and the complex emotional and financial manipulation involved. We systematically analyze each phase of the scam, revealing that perpetrators employ tactics such as staged trust-building, fraudulent financial platforms, fabricated investment returns, and repeated high-pressure tactics, all designed to exploit victims' trust and financial resources over extended periods. Our findings reveal an organized scam lifecycle characterized by emotional manipulation, staged financial exploitation, and persistent re-engagement efforts that amplify victim losses. We also find complex psychological and financial impacts on victims, including heightened vulnerability to secondary scams. Finally, we propose actionable intervention points for social media and financial platforms to curb the prevalence of these scams and highlight the need for non-stigmatizing terminology to encourage victims to report and seek assistance.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Rajvardhan Oak [view email]&lt;p&gt;[v1] Tue, 25 Mar 2025 23:15:48 UTC (342 KB)&lt;/p&gt;&lt;p&gt;[v2] Sat, 24 May 2025 07:36:41 UTC (343 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45245962</guid><pubDate>Mon, 15 Sep 2025 03:58:23 +0000</pubDate></item><item><title>Omarchy on CachyOS</title><link>https://github.com/mroboff/omarchy-on-cachyos</link><description>&lt;doc fingerprint="a0d7e45f0deae3ea"&gt;
  &lt;main&gt;
    &lt;p&gt;This project provides an installation script for implementing DHH's Omarchy configuration on top of CachyOS. Omarchy is an 'opinionated' desktop setup, based on Hyprland, that emphasizes simplicity and productivity, while CachyOS offers a performance-optimized Arch Linux distribution.&lt;/p&gt;
    &lt;p&gt;This installation script does the following three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Clones Omarchy from its github repository&lt;/item&gt;
      &lt;item&gt;Makes adjustments to the Omarchy install scripts to support installation on CachyOS&lt;/item&gt;
      &lt;item&gt;Launches the installation of Omarchy on an already setup CachyOS system&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This script does not:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install CachyOS or any other Linux operating system&lt;/item&gt;
      &lt;item&gt;Partition, format, or encrypt hard disks&lt;/item&gt;
      &lt;item&gt;Install or configure a boot loader&lt;/item&gt;
      &lt;item&gt;Install graphics drivers&lt;/item&gt;
      &lt;item&gt;Install or configure a login display manager&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All of the above need to be done when you install CachyOS.&lt;/p&gt;
    &lt;p&gt;This script (and README.md) is intended primarily for the experienced Arch Linux user. The author of this README.md assumes the reader is comfortable using a shell/command line and is familiar with Arch specific terms such as AUR.&lt;/p&gt;
    &lt;p&gt;The philosophy behind this script is to produce a strong and stable blend of CachyOS and Omarchy that changes as little as possible between the two. This script does not add software or make configuration changes outside of what CachyOS or Omarchy provide as default, except when such software or configurations provided by CachyOS and Omarchy are in conflict. In these cases, the script will choose the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;AUR helper: CachyOS uses Paru by default while Omarchy uses Yay. This script opts for Yay and will install it if not already installed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shell: CachyOS uses the Fish shell by default while Omarchy uses Bash. This script will keep Fish as the default interactive shell.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;TLDR implementation: CachyOS installs Tealdeer by default, which is a TLDR implementation written in Rust. This script will preserve use of Tealdeer.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mise: Omarchy will setup Mise to run automatically via mise-activate. This script will supply the right mise-activate command for the fish shell.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Login System: As a distribution, Omarchy skips installation of a login display manager. Instead, Hyprland autostarts and password protection is provided upon boot by the LUKS full disk encryption service. This script, however, assumes a display manager is installed. (Note: this script does not install a display manager, but also does not configure Hyprland to start automatically if a display manager is not installed.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Full Disk Encryption: As a distribution, Omarchy automatically turns on full disk encryption via LUKS. This script, however, leaves this decision up to the user. CachyOS can be installed with or without full disk encryption, and this script will install Omarchy on either setup.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;IMPORTANT: This script does not install CachyOS. You must do that separately (and first.) This script is intended to be run on a fresh installation of CachyOS with the following configuration choices made: (Note, for information on installing CachyOS, please refer to https://www.cachyos.org.)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;File System: You must choose BTRFS as the file system and Snapper as the snapshot manager. This aligns with CachyOS's default recommendation for most systems, and is required for Omarchy to properly function.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Shell: You must choose Fish as the default shell for this installation script to work properly. (This is the default CachyOS shell choice.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Desktop Environment to Install: You can install a minimal system with no desktop environment or you can choose to install the CachyOS Hyprland Desktop Environment. If you have CachyOS install Hyprland, it will also install SDDM as the login display manager by default. Do not install GNOME or KDE.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Graphics Drivers for NVIDIA users: If you are using an NVIDIA GPU, install the recommended graphics driver via CachyOS. The script will turn off driver installation in Omarchy.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other configuration changes are up to you. Note, however, that this script has not been extensively tested on various CachyOS installations other than the author's own machine.&lt;/p&gt;
    &lt;code&gt;# Clone the repository
git clone https://github.com/mroboff/omarchy-on-cachyos.git

# Navigate to the project directory
cd omarchy-on-cachyos/bin

# Make the script executable
chmod +x install.sh

# Run the installation script
bin/install.sh&lt;/code&gt;
    &lt;p&gt;Note: Please review the script contents before running to understand what changes will be made to your system.&lt;/p&gt;
    &lt;p&gt;THIS SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.&lt;/p&gt;
    &lt;p&gt;Use this script at your own risk. Always backup your system and important data before running installation scripts.&lt;/p&gt;
    &lt;p&gt;We welcome contributions to improve this project! Here's how you can help:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the Repository: Click the "Fork" button on GitHub to create your own copy&lt;/item&gt;
      &lt;item&gt;Create a Feature Branch: &lt;code&gt;git checkout -b feature/your-feature-name&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Make Your Changes: Implement your improvements or fixes&lt;/item&gt;
      &lt;item&gt;Commit Your Changes: &lt;code&gt;git commit -m "Add descriptive commit message"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Push to Your Fork: &lt;code&gt;git push origin feature/your-feature-name&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Open a Pull Request: Submit a PR with a clear description of your changes&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Test your changes thoroughly on CachyOS before submitting&lt;/item&gt;
      &lt;item&gt;Follow existing code style and conventions&lt;/item&gt;
      &lt;item&gt;Update documentation if adding new features&lt;/item&gt;
      &lt;item&gt;Report bugs using GitHub Issues&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45246325</guid><pubDate>Mon, 15 Sep 2025 05:13:33 +0000</pubDate></item><item><title>Celestia – Real-time 3D visualization of space</title><link>https://celestiaproject.space/</link><description>&lt;doc fingerprint="5f8f763c0c2747ac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Making sure you're not a bot!&lt;/head&gt;
    &lt;p&gt;Loading...&lt;/p&gt;
    &lt;head&gt;Why am I seeing this?&lt;/head&gt;
    &lt;p&gt;You are seeing this because the administrator of this website has set up Anubis to protect the server against the scourge of AI companies aggressively scraping websites. This can and does cause downtime for the websites, which makes their resources inaccessible for everyone.&lt;/p&gt;
    &lt;p&gt;Anubis is a compromise. Anubis uses a Proof-of-Work scheme in the vein of Hashcash, a proposed proof-of-work scheme for reducing email spam. The idea is that at individual scales the additional load is ignorable, but at mass scraper levels it adds up and makes scraping much more expensive.&lt;/p&gt;
    &lt;p&gt;Ultimately, this is a hack whose real purpose is to give a "good enough" placeholder solution so that more time can be spent on fingerprinting and identifying headless browsers (EG: via how they do font rendering) so that the challenge proof of work page doesn't need to be presented to users that are much more likely to be legitimate.&lt;/p&gt;
    &lt;p&gt;Please note that Anubis requires the use of modern JavaScript features that plugins like JShelter will disable. Please disable JShelter or other such plugins for this domain.&lt;/p&gt;
    &lt;p&gt;This website is running Anubis version &lt;code&gt;1.21.3&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45246403</guid><pubDate>Mon, 15 Sep 2025 05:30:19 +0000</pubDate></item><item><title>Folks, we have the best π</title><link>https://lcamtuf.substack.com/p/folks-we-have-the-best</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45246953</guid><pubDate>Mon, 15 Sep 2025 07:10:03 +0000</pubDate></item><item><title>RustGPT: A pure-Rust transformer LLM built from scratch</title><link>https://github.com/tekaratzas/RustGPT</link><description>&lt;doc fingerprint="35db335f75314ff8"&gt;
  &lt;main&gt;
    &lt;head class="px-3 py-2"&gt;RustGPT-demo-zoon.mp4&lt;/head&gt;
    &lt;p&gt;A complete Large Language Model implementation in pure Rust with no external ML frameworks. Built from the ground up using only &lt;code&gt;ndarray&lt;/code&gt; for matrix operations.&lt;/p&gt;
    &lt;p&gt;This project demonstrates how to build a transformer-based language model from scratch in Rust, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pre-training on factual text completion&lt;/item&gt;
      &lt;item&gt;Instruction tuning for conversational AI&lt;/item&gt;
      &lt;item&gt;Interactive chat mode for testing&lt;/item&gt;
      &lt;item&gt;Full backpropagation with gradient clipping&lt;/item&gt;
      &lt;item&gt;Modular architecture with clean separation of concerns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Start with these two core files to understand the implementation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/main.rs&lt;/code&gt;- Training pipeline, data preparation, and interactive mode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;src/llm.rs&lt;/code&gt;- Core LLM implementation with forward/backward passes and training logic&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The model uses a transformer-based architecture with the following components:&lt;/p&gt;
    &lt;code&gt;Input Text → Tokenization → Embeddings → Transformer Blocks → Output Projection → Predictions
&lt;/code&gt;
    &lt;code&gt;src/
├── main.rs              # 🎯 Training pipeline and interactive mode
├── llm.rs               # 🧠 Core LLM implementation and training logic
├── lib.rs               # 📚 Library exports and constants
├── transformer.rs       # 🔄 Transformer block (attention + feed-forward)
├── self_attention.rs    # 👀 Multi-head self-attention mechanism  
├── feed_forward.rs      # ⚡ Position-wise feed-forward networks
├── embeddings.rs        # 📊 Token embedding layer
├── output_projection.rs # 🎰 Final linear layer for vocabulary predictions
├── vocab.rs            # 📝 Vocabulary management and tokenization
├── layer_norm.rs       # 🧮 Layer normalization
└── adam.rs             # 🏃 Adam optimizer implementation

tests/
├── llm_test.rs         # Tests for core LLM functionality
├── transformer_test.rs # Tests for transformer blocks
├── self_attention_test.rs # Tests for attention mechanisms
├── feed_forward_test.rs # Tests for feed-forward layers
├── embeddings_test.rs  # Tests for embedding layers
├── vocab_test.rs       # Tests for vocabulary handling
├── adam_test.rs        # Tests for optimizer
└── output_projection_test.rs # Tests for output layer
&lt;/code&gt;
    &lt;p&gt;The implementation includes two training phases:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Pre-training: Learns basic world knowledge from factual statements&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;"The sun rises in the east and sets in the west"&lt;/item&gt;
          &lt;item&gt;"Water flows downhill due to gravity"&lt;/item&gt;
          &lt;item&gt;"Mountains are tall and rocky formations"&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Instruction Tuning: Learns conversational patterns&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;"User: How do mountains form? Assistant: Mountains are formed through tectonic forces..."&lt;/item&gt;
          &lt;item&gt;Handles greetings, explanations, and follow-up questions&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Clone and run
git clone &amp;lt;your-repo&amp;gt;
cd llm
cargo run

# The model will:
# 1. Build vocabulary from training data
# 2. Pre-train on factual statements (100 epochs)  
# 3. Instruction-tune on conversational data (100 epochs)
# 4. Enter interactive mode for testing&lt;/code&gt;
    &lt;p&gt;After training, test the model interactively:&lt;/p&gt;
    &lt;code&gt;Enter prompt: How do mountains form?
Model output: Mountains are formed through tectonic forces or volcanism over long geological time periods

Enter prompt: What causes rain?
Model output: Rain is caused by water vapor in clouds condensing into droplets that become too heavy to remain airborne
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Vocabulary Size: Dynamic (built from training data)&lt;/item&gt;
      &lt;item&gt;Embedding Dimension: 128&lt;/item&gt;
      &lt;item&gt;Hidden Dimension: 256&lt;/item&gt;
      &lt;item&gt;Max Sequence Length: 80 tokens&lt;/item&gt;
      &lt;item&gt;Architecture: 3 Transformer blocks + embeddings + output projection&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Optimizer: Adam with gradient clipping&lt;/item&gt;
      &lt;item&gt;Pre-training LR: 0.0005 (100 epochs)&lt;/item&gt;
      &lt;item&gt;Instruction Tuning LR: 0.0001 (100 epochs)&lt;/item&gt;
      &lt;item&gt;Loss Function: Cross-entropy loss&lt;/item&gt;
      &lt;item&gt;Gradient Clipping: L2 norm capped at 5.0&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Custom tokenization with punctuation handling&lt;/item&gt;
      &lt;item&gt;Greedy decoding for text generation&lt;/item&gt;
      &lt;item&gt;Gradient clipping for training stability&lt;/item&gt;
      &lt;item&gt;Modular layer system with clean interfaces&lt;/item&gt;
      &lt;item&gt;Comprehensive test coverage for all components&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Run all tests
cargo test

# Test specific components
cargo test --test llm_test
cargo test --test transformer_test
cargo test --test self_attention_test

# Build optimized version
cargo build --release

# Run with verbose output
cargo test -- --nocapture&lt;/code&gt;
    &lt;p&gt;This implementation demonstrates key ML concepts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transformer architecture (attention, feed-forward, layer norm)&lt;/item&gt;
      &lt;item&gt;Backpropagation through neural networks&lt;/item&gt;
      &lt;item&gt;Language model training (pre-training + fine-tuning)&lt;/item&gt;
      &lt;item&gt;Tokenization and vocabulary management&lt;/item&gt;
      &lt;item&gt;Gradient-based optimization with Adam&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Perfect for understanding how modern LLMs work under the hood!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;ndarray&lt;/code&gt;- N-dimensional arrays for matrix operations&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;rand&lt;/code&gt;+&lt;code&gt;rand_distr&lt;/code&gt;- Random number generation for initialization&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No PyTorch, TensorFlow, or Candle - just pure Rust and linear algebra!&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! This project is perfect for learning and experimentation.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🏪 Model Persistence - Save/load trained parameters to disk (currently all in-memory)&lt;/item&gt;
      &lt;item&gt;⚡ Performance optimizations - SIMD, parallel training, memory efficiency&lt;/item&gt;
      &lt;item&gt;🎯 Better sampling - Beam search, top-k/top-p, temperature scaling&lt;/item&gt;
      &lt;item&gt;📊 Evaluation metrics - Perplexity, benchmarks, training visualizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Advanced architectures (multi-head attention, positional encoding, RoPE)&lt;/item&gt;
      &lt;item&gt;Training improvements (different optimizers, learning rate schedules, regularization)&lt;/item&gt;
      &lt;item&gt;Data handling (larger datasets, tokenizer improvements, streaming)&lt;/item&gt;
      &lt;item&gt;Model analysis (attention visualization, gradient analysis, interpretability)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the repository&lt;/item&gt;
      &lt;item&gt;Create a feature branch: &lt;code&gt;git checkout -b feature/model-persistence&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Make your changes and add tests&lt;/item&gt;
      &lt;item&gt;Run the test suite: &lt;code&gt;cargo test&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Submit a pull request with a clear description&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follow standard Rust conventions (&lt;code&gt;cargo fmt&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Add comprehensive tests for new features&lt;/item&gt;
      &lt;item&gt;Update documentation and README as needed&lt;/item&gt;
      &lt;item&gt;Keep the "from scratch" philosophy - avoid heavy ML dependencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🚀 Beginner: Model save/load, more training data, config files&lt;/item&gt;
      &lt;item&gt;🔥 Intermediate: Beam search, positional encodings, training checkpoints&lt;/item&gt;
      &lt;item&gt;⚡ Advanced: Multi-head attention, layer parallelization, custom optimizations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Questions? Open an issue or start a discussion!&lt;/p&gt;
    &lt;p&gt;No PyTorch, TensorFlow, or Candle - just pure Rust and linear algebra!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45247890</guid><pubDate>Mon, 15 Sep 2025 09:47:18 +0000</pubDate></item><item><title>The $10 Payment That Cost Me $43.95 – The Madness of SaaS Chargebacks</title><link>https://medium.com/@citizenblr/the-10-payment-that-cost-me-43-95-the-madness-of-saas-chargebacks-5c308d5a49cc</link><description>&lt;doc fingerprint="f641d18e10afc728"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The $10 Payment That Cost Me $43.95 — The Madness of SaaS Chargebacks&lt;/head&gt;
    &lt;p&gt;We run several SaaS products at Everhour, all billed through Stripe. Majority of the time everything works fine, but sometimes we get chargebacks. Even thought we do everything possible to prevent them.&lt;/p&gt;
    &lt;p&gt;We don’t ask for a credit card until the moment of subscription. A few days before each renewal, we send an email notifying the customer about the upcoming charge and giving them time to cancel if they’ve forgotten. After the charge, we send an invoice. Both the invoice and the bank statement clearly display the product name so the customer can easily identify us. Inside the product, we also provide a simple, self-service way to cancel the subscription without any questions asked.&lt;/p&gt;
    &lt;p&gt;Still, some people prefer to silently file a dispute rather than reach out and ask for a refund.&lt;/p&gt;
    &lt;p&gt;The worst part is that it doesn’t matter whether you win or lose a dispute — the very fact that it was filed still counts against your account. On top of that, you get hit with extra fees for both the dispute and the counter-dispute, which feels especially unfair when the payment itself is small.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;p&gt;Still, we always submit evidence.&lt;/p&gt;
    &lt;p&gt;To me, blindly accepting a dispute is a de facto admission of wrongdoing, and not fighting against the claims only makes my account look worse. Even if it costs money and time, I’d rather defend the company and show that we don’t agree with absurd claims. On top of that, chargebacks are extremely rare for us, so it’s not a huge waste of time.&lt;/p&gt;
    &lt;p&gt;That said, when a customer simply asks for a refund, we always meet them halfway. What I don’t understand is why some people can’t just reach out and request it — instead of going straight to a chargeback.&lt;/p&gt;
    &lt;p&gt;The problem is — it’s not a fair game. Banks almost always side with the cardholder, even when we provide clear logs and evidence. I seriously doubt anyone even reads what we submit.&lt;/p&gt;
    &lt;p&gt;Stripe doesn’t fight for you either, though they do promote now their paid tool for “automatic dispute handling.” But in my view, if no one cares even when you carefully craft every word of a manual response, then automation is pointless. And if their tool predicts a “dispute win likelihood 5/5” but you still lose the case, then clearly something isn’t working :)&lt;/p&gt;
    &lt;head rend="h3"&gt;Recent lost dispute story&lt;/head&gt;
    &lt;p&gt;A client canceled their subscription in the 3rd month of usage. A few days later they filed a chargeback, claiming: “The company continued charging me after I canceled.”&lt;/p&gt;
    &lt;p&gt;Facts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Charge was processed August 12 (regular billing cycle).&lt;/item&gt;
      &lt;item&gt;Subscription canceled August 18 (6 days later).&lt;/item&gt;
      &lt;item&gt;Dispute created August 19.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The claim is false.&lt;/p&gt;
    &lt;p&gt;All goes through Stripe. It’s not just our words without proof — everything is documented directly in Stripe.&lt;/p&gt;
    &lt;p&gt;We submitted everything: logs, screens, terms, full context. We still lost.&lt;/p&gt;
    &lt;p&gt;The bank’s response: “ THANK YOU FOR YOUR RESPONSE. THE MERCHANT S RESPONSE FAILS TO REMEDY THE DISPUTE. THE CARDHOLDER CANCELLED ON THE DATE PROVIDED ON OUR DISPUTE QUESTIONNAIRE. THE CARDHOLDER IS NOT REQUIRED TO PROVIDE PROOF OF CANCELLATION. THE MERCHANT S LIMITED CANCELLATION POLICY BEARS NO RELEVANCE IN A DISPUTE OF THIS NATURE. FULL CREDIT IS DUE ”&lt;/p&gt;
    &lt;p&gt;Translation: the customer doesn’t have to prove anything, and your terms don’t matter.&lt;/p&gt;
    &lt;p&gt;The only truly reliable way to win a dispute is if the customer withdraws it themselves. Once they provide proof of withdrawal, you can submit it to Stripe and the dispute gets resolved in your favor.&lt;/p&gt;
    &lt;head rend="h3"&gt;Another — but this time a won dispute story&lt;/head&gt;
    &lt;p&gt;One day we received a notice of a dispute. The reason: “Payment disputed because subscription cancelled”.&lt;/p&gt;
    &lt;p&gt;The strange part? They had been paying for our subscription for over a year, had a large team, were still actively using our product — and no one had ever canceled the subscription.&lt;/p&gt;
    &lt;p&gt;We reached out by email to ask what was going on. The customer replied that they had “never authorized the use of this specific card”, claimed we should have used a different one, and said they would continue disputing every future payment until we switch cards.&lt;/p&gt;
    &lt;p&gt;We explained that only they could update the card in their billing portal, it’s not our responsibility, especially since they had never asked for help with it. And once they updated it, all future payments would automatically be processed from the new card.&lt;/p&gt;
    &lt;p&gt;When nothing changed, we escalated the issue to the other admins on the account, copying the existing thread, and explained that unless the situation was resolved, we would have no choice but to pause the account — which would affect the entire team.&lt;/p&gt;
    &lt;p&gt;Another admin quickly replied, explaining that there had been some internal changes and he would now take over billing. We made him the new billing owner, he updated the payment method, and confirmed that they had no complaints with us and that the earlier charges had been reimbursed internally.&lt;/p&gt;
    &lt;p&gt;After that, the original cardholder confirmed that he had withdrawn the dispute. We asked for proof of the withdrawal, and he provided an official confirmation from the bank. We submitted it to Stripe and only then was the dispute marked as won.&lt;/p&gt;
    &lt;head rend="h3"&gt;So here’s my question to the community&lt;/head&gt;
    &lt;p&gt;What’s really going on here? Why do banks completely ignore the terms customers agreed to when they subscribed or in cases where they’re clearly making false claims? And why aren’t customers required to provide any proof at all?&lt;/p&gt;
    &lt;p&gt;What actually prevents someone from using a SaaS product, filing chargebacks every time they cancel their subscription, and essentially getting refunded for the last several months of usage?&lt;/p&gt;
    &lt;p&gt;Would love to hear your thoughts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45248446</guid><pubDate>Mon, 15 Sep 2025 11:30:46 +0000</pubDate></item></channel></rss>