<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 19 Sep 2025 12:19:23 +0000</lastBuildDate><item><title>Slack has raised our charges by $195k per year</title><link>https://skyfall.dev/posts/slack</link><description>&lt;doc fingerprint="325980567bbeeaf1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Slack is extorting us with a $195k/yr bill increase&lt;/head&gt;
    &lt;p&gt;An open letter, or something&lt;/p&gt;
    &lt;head rend="h2"&gt;The good ending&lt;/head&gt;
    &lt;p&gt;Turns out that this post went pretty viral on Hacker News and Twitter/X:&lt;/p&gt;
    &lt;p&gt;I’m very happy to announce that a couple hours ago, Slack’s CEO got in contact with us and offered to put things right (I can’t exactly say what it is, but it’s better than the plan we were on previously!) A massive thank you to everyone who helped spread awareness - it was incredibly heartwarming to see so many people support us and help get this sorted.&lt;/p&gt;
    &lt;p&gt;With that being said though, this ordeal has made us think more deeply about entrusting data with external SaaSes and ensuring that we own our data is definitely going to be a very big priority going forward. I’d encourage you to think the same way!&lt;/p&gt;
    &lt;p&gt;For nearly 11 years, Hack Club - a nonprofit that provides coding education and community to teenagers worldwide - has used Slack as the tool for communication. We weren’t freeloaders. A few years ago, when Slack transitioned us from their free nonprofit plan to a $5,000/year arrangement, we happily paid. It was reasonable, and we valued the service they provided to our community.&lt;/p&gt;
    &lt;p&gt;However, two days ago, Slack reached out to us and said that if we don’t agree to pay an extra $50k this week and $200k a year, they’ll deactivate our Slack workspace and delete all of our message history.&lt;/p&gt;
    &lt;p&gt;One could argue that Slack is free to stop providing us the nonprofit offer at any time, but in my opinion, a six month grace period is the bare minimum for a massive hike like this, if not more. Essentially, Salesforce (a $230 billion company) is strong-arming a small nonprofit for teens, by providing less than a week to pony up a pretty massive sum of money, or risk cutting off all our communications. That’s absurd.&lt;/p&gt;
    &lt;head rend="h2"&gt;The impact&lt;/head&gt;
    &lt;p&gt;The small amount of notice has also been catastrophic for the programs that we run. Dozens of our staff and volunteers are now scrambling to update systems, rebuild integrations and migrate years of institutional knowledge. The opportunity cost of this forced migration is simply staggering.&lt;/p&gt;
    &lt;p&gt;Anyway, we’re moving to Mattermost. This experience has taught us that owning your data is incredibly important, and if you’re a small business especially, then I’d advise you move away too.&lt;/p&gt;
    &lt;p&gt;This post was rushed out because, well, this has been a shock! If you’d like any additional details then feel free to send me an email.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45283887</guid><pubDate>Thu, 18 Sep 2025 01:37:11 +0000</pubDate></item><item><title>Nvidia buys $5B in Intel</title><link>https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal</link><description>&lt;doc fingerprint="27f94771cdaa5063"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nvidia and Intel announce jointly developed 'Intel x86 RTX SOCs' for PCs with Nvidia graphics, also custom Nvidia data center x86 processors — Nvidia buys $5 billion in Intel stock in seismic deal&lt;/head&gt;
    &lt;p&gt;Cats and Dogs, living together!&lt;/p&gt;
    &lt;p&gt;In a surprise announcement that finds two long-time rivals working together, Nvidia and Intel announced today that the companies will jointly develop multiple new generations of x86 products together — a seismic shift with profound implications for the entire world of technology. Before the news broke, Tom's Hardware spoke with Nvidia representatives to learn more details about the company’s plans.&lt;/p&gt;
    &lt;p&gt;The products include x86 Intel CPUs tightly fused with an Nvidia RTX graphics chiplet for the consumer gaming PC market, named the ‘Intel x86 RTX SOCs.’ Nvidia will also have Intel build custom x86 data center CPUs for its AI products for hyperscale and enterprise customers. Additionally, Nvidia will buy $5 billion in Intel common stock at $23.28 per share, representing a roughly 5% ownership stake in Intel. (Intel stock is now up 33% in premarket trading.)&lt;/p&gt;
    &lt;p&gt;The partnership between the two companies is in the very early stages, Nvidia told us, so the timeline for product releases, along with any product specifications, will be disclosed at a later, unspecified date. (Given the traditionally long lead-times for new processors, it is rational to expect these products will take at least a year, and likely longer, to come to market.)&lt;/p&gt;
    &lt;p&gt;Nvidia emphasized that the companies are committed to multi-generation roadmaps for the co-developed products, which represents a strong investment in the x86 ecosystem. But Nvidia representatives tell us it also remains fully committed to other announced product roadmaps and architectures, including the company's Arm-based GB10 Grace Blackwell processors for workstations and the Nvidia Grace CPUs for data centers, as well as the next-gen Vera CPUs. Nvidia says it also remains committed to products on its internal roadmaps that haven’t been publicly disclosed yet, indicating that the new roadmap with Intel will merely be additive to existing initiatives.&lt;/p&gt;
    &lt;p&gt;Nvidia hasn’t disclosed whether it will use Intel Foundry to produce any of these products yet. However, while Intel has used TSMC to manufacture some of its own recent products, its goal is to bring production of most high-performance products back into its own foundries.&lt;/p&gt;
    &lt;p&gt;Some products never left. For instance, Intel’s existing Granite Rapids data center processors use the ‘Intel 3’ node, and the upcoming Clearwater Forest Xeons will use Intel’s own 18A process node for compute. This suggests that at least some of the Nvidia-custom x86 silicon, particularly for the data center, could be fabbed on Intel nodes. Intel also uses TSMC to fabricate many of its client x86 processors, however, so we won’t know for sure until official announcements are made — particularly for the RTX GPU chiplet.&lt;/p&gt;
    &lt;p&gt;In either case, Nvidia has been mulling using Intel Foundry since 2022, has fabbed test chips there, and participates in the U.S. Defense Dept.'s RAMP-C project with Intel. The DoD project involves Nvidia already making chips on Intel's 18A process node, so it wouldn't be a total surprise.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;While the two companies have engaged in heated competition in some market segments, Intel and Nvidia have partnered for decades, ensuring interoperability between their hardware and software for products spanning both the client and data center markets. The PCIe interface has long been used to connect Intel CPUs and Nvidia GPUs. The new partnership will find tighter integration using the NVLink interface for CPU-to-GPU communication, which affords up to 14 times more bandwidth along with lower latency than PCIe, thus granting the new x86 products access to the highest performance possible when paired with GPUs. That's a strategic advantage. Let’s dive into the details we’ve learned so far.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intel x86 RTX SOCs for the PC gaming market&lt;/head&gt;
    &lt;p&gt;For the PC market, the Intel x86 RTX SoC chips will come with an x86 CPU chiplet tightly connected with an Nvidia RTX GPU chiplet via the NVLink interface. This type of processor will have both CPU and GPU units merged into one compact chip package that externally looks much like a standard CPU, rivaling AMD’s competing APU products.&lt;/p&gt;
    &lt;p&gt;Intel's new x86 RTX CPUs will compete directly with AMD's APUs. For AMD, that means it faces intensifying competition from a company with the leading market share in notebook CPUs (Intel ships ~79% of laptop chips worldwide) that's now armed with GPU tech from Nvidia, which ships 92% of the world's gaming GPUs.&lt;/p&gt;
    &lt;p&gt;This type of tight integration packs all the gaming prowess into one package without an external discrete GPU, providing power and footprint advantages. As such, we're told these chips will be heavily focused on thin-and-light gaming laptops and small form-factor PCs, much like today’s APUs from AMD. However, it’s possible the new Nvidia/Intel chips could come in multiple flavors and permeate further into the Intel stack over time.&lt;/p&gt;
    &lt;p&gt;Intel has worked on a similar type of chip before with AMD; there is at least one significant technical difference between these initiatives, however. Intel launched its Kaby Lake-G chip in 2017 with an Intel processor fused into the same package as an AMD Radeon GPU chiplet, much the same as the description of the new Nvidia/Intel chips. You can see an image of the Intel/AMD chip below.&lt;/p&gt;
    &lt;p&gt;This SoC had a CPU at one end connected via a PCIe connection to the separate AMD GPU chiplet, which is flanked by a small, dedicated memory package. This separate memory package was only usable by the GPU. The Nvidia/Intel products will have an RTX GPU chiplet connected to the CPU chiplet via the faster and more efficient NVLink interface, and we’re told it will have uniform memory access (UMA), meaning both the CPU and GPU will be able to access the same pool of memory. Given the particulars of Nvidia's NVLink Fusion architecture, we can expect the chips to communicate via a refined interface, but it is unlikely that it will leverage Nvidia's C2C (Chip-to-Chip) technology, an inter-die/inter-chip interconnect that's based on Arm protocols that aren't likely optimized for x86.&lt;/p&gt;
    &lt;p&gt;Intel notoriously axed the Kaby Lake-G products in 2019, and the existing systems were left without proper driver support for quite some time, in part because Intel was responsible for validating the drivers, and then finger-pointing ensued. We’re told that both Intel and Nvidia will be responsible for their respective drivers for the new models, with Nvidia naturally providing its own GPU drivers. However, Intel will build and sell the consumer processors.&lt;/p&gt;
    &lt;p&gt;We haven’t spoken with Intel yet, but the limited scope of this project means that Intel’s proprietary Xe graphics architecture will most assuredly live on as the primary integrated GPU (iGPU) for its mass-market products.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nvidia's first x86 data center CPUs&lt;/head&gt;
    &lt;p&gt;Intel will fabricate custom x86 data center CPUs for Nvidia, which Nvidia will then sell as its own products to enterprise and data center customers. However, the entirety and extent of the modifications are currently unknown. We know that Nvidia will employ its NVLink interface, which suggests that the chips could leverage Nvidia’s new NVLink Fusion technology for custom CPUs and accelerators, enabling faster and more efficient communication with Nvidia’s GPUs than is possible with the PCIe interface.&lt;/p&gt;
    &lt;p&gt;Intel has long offered custom Xeons to its customers, primarily hyperscalers, often with relatively minor tweaks to clock rates, cache capacities, and other specifications. In fact, these mostly slightly-modified custom Xeon models once comprised more than 50% of Intel’s Xeon shipments. Intel has endured several years of market share erosion due to AMD’s advances, most acutely in the hyperscale market. Therefore, it is unclear if the 50% number still holds true, as hyperscalers were the primary customers for custom models.&lt;/p&gt;
    &lt;p&gt;Intel has said that it will design completely custom x86 chips for customers as part of its IDM 2.0 strategy. However, aside from a recent announcement of custom AWS chips that sound like the slightly modified Xeons mentioned above, we haven’t heard of any large-scale uptake for significantly modified custom x86 processors. Intel announced a new custom chip design unit just two weeks ago, so it will be interesting to learn the extent of the customization for Nvidia’s x86 data center CPUs.&lt;/p&gt;
    &lt;p&gt;Nvidia already uses Intel’s Xeons in several of its systems, like the Nvidia DGX B300, but these systems still use the PCIe interface to communicate with the CPU. Intel’s new collaboration with Nvidia will obviously open up new opportunities, given the tighter integration with NVLink and all the advantages it brings with it.&lt;/p&gt;
    &lt;p&gt;The likelihood of AMD adopting NVLink Fusion is somewhere around zero, as the company is heavily invested in its own Infinity Fabric (XGMI) and Ultra Accelerator Link (UALink) initiatives, which aim to provide an open-standard interconnect to rival NVLink and democratize rack-scale interconnect technologies. Intel is also a member of UALink, which uses AMD’s Infinity Fabric protocol as the foundation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dollar and Cents, Geopolitics&lt;/head&gt;
    &lt;p&gt;Nvidia’s $5 billion purchase of Intel common stock will come at $23.28 a share, roughly 6% below the current market value, but several aspects of this investment remain unclear. Nvidia hasn’t stated whether it will have a seat on the board (which is unlikely) or how it will vote on matters requiring shareholder approval. It is also unclear if Intel will issue new stock (primary issuance) for Nvidia to purchase, as it did when the U.S. government recently became an Intel shareholder (that is likely). Naturally, the investment is subject to approval from regulators.&lt;/p&gt;
    &lt;p&gt;Nvidia’s buy-in comes on the heels of the U.S government buying $10 billion of newly-created Intel stock, granting the country a 9.9% ownership stake at $20.47 per share. The U.S. government won’t have a seat on the board and agreed to vote with Intel’s board on matters requiring shareholder approval “with limited exceptions.” Softbank has also recently purchased $2 billion worth of primary issuance of Intel stock at $23 per share.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Row 0 - Cell 0&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Total&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Share Price&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Stake in Intel&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Nvidia&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$5 Billion&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$23.28&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~5%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;U.S. Government&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$9 Billion&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$20.47&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~9.9%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Softbank&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$2 Billion&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$23&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Row 3 - Cell 3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The U.S. government says it invested in Intel with the goal of bolstering US technology, manufacturing, and national security, and the investments from the private sector also help solidify the struggling Intel. Altogether, these investments represent a significant cash influx for Intel as it attempts to maintain the heavy cap-ex investments required to compete with TSMC, all while struggling with a negative amount of free cash flow.&lt;/p&gt;
    &lt;p&gt;“AI is powering a new industrial revolution and reinventing every layer of the computing stack — from silicon to systems to software. At the heart of this reinvention is Nvidia’s CUDA architecture,” said Nvidia CEO Jensen Huang. “This historic collaboration tightly couples NVIDIA’s AI and accelerated computing stack with Intel’s CPUs and the vast x86 ecosystem—a fusion of two world-class platforms. Together, we will expand our ecosystems and lay the foundation for the next era of computing.”&lt;/p&gt;
    &lt;p&gt;“Intel’s x86 architecture has been foundational to modern computing for decades – and we are innovating across our portfolio to enable the workloads of the future,” said Intel CEO Lip-Bu Tan. “Intel’s leading data center and client computing platforms, combined with our process technology, manufacturing and advanced packaging capabilities, will complement Nvidia's AI and accelerated computing leadership to enable new breakthroughs for the industry. We appreciate the confidence Jensen and the Nvidia team have placed in us with their investment and look forward to the work ahead as we innovate for customers and grow our business.”&lt;/p&gt;
    &lt;p&gt;We’ll learn more details of the new partnership later today when Nvidia CEO Jensen Huang and Intel CEO Lip-Bu Tan hold a webcast press conference at 10 am PT. {EDIT: you can read our futher coverage of that press event here.}&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!&lt;/p&gt;
    &lt;p&gt;Paul Alcorn is the Editor-in-Chief for Tom's Hardware US. He also writes news and reviews on CPUs, storage, and enterprise hardware.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;bolweval&lt;/header&gt;WOW, didn't see that coming, but it makes good business sense, especially for Intel..Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;usertests&lt;/header&gt;Wow.Reply&lt;lb/&gt;So this is still kind of like an integrated dGPU, unlike the chiplet/tile-based CPUs with integrated graphics they're already making?&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;watzupken&lt;/header&gt;AMD must be sweating now. An Intel and Nvidia tag team is going to give AMD some tough competition.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;hotaru251&lt;/header&gt;Reply&lt;quote/&gt;pretty much.... basically it keeps intel afloat by riding on nvidia and nvidia gets to take its monopoly even further.watzupken said:AMD must be sweating now. An Intel and Nvidia tag team is going to give AMD some tough competition.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;usertests&lt;/header&gt;Reply&lt;quote/&gt;What products are they trying to compete with?watzupken said:AMD must be sweating now. An Intel and Nvidia tag team is going to give AMD some tough competition.&lt;lb/&gt;Intel's mobile graphics were already competitive with AMD's phoned-in 128-bit APUs, with Lunar Lake being around Strix Point for example. They don't need an Nvidia chiplet to compete.&lt;lb/&gt;Intel never cared to make a desktop APU from the aforementioned graphics.&lt;lb/&gt;So that leaves Strix Halo. And Apple products.&lt;lb/&gt;They also get to outsource drivers and software to Nvidia, and CUDA will work with these. That could be a bigger deal than any hardware match-up.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;rluker5&lt;/header&gt;Reply&lt;quote/&gt;I wonder how much this will help transitioning common CPU tasks to GPU like OneAPI was supposed to?usertests said:What products are they trying to compete with?&lt;lb/&gt;Intel's mobile graphics were already competitive with AMD's phoned-in 128-bit APUs, with Lunar Lake being around Strix Point for example. They don't need an Nvidia chiplet to compete.&lt;lb/&gt;Intel never cared to make a desktop APU from the aforementioned graphics.&lt;lb/&gt;So that leaves Strix Halo. And Apple products.&lt;lb/&gt;They also get to outsource drivers and software to Nvidia, and CUDA will work with these. That could be a bigger deal than any hardware match-up.&lt;quote/&gt;Isn't buying common shares is the opposite of dilution? It sounds like same amount of shares enriched with more money.dalek1234 said:So another share dilution, yet Intel stock price is going up.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;aldaia&lt;/header&gt;"Nvidia announced that it will buy $5 billion in Intel common stock"Reply&lt;lb/&gt;Wondering if this is the start of a takeover process.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;blppt&lt;/header&gt;Wait a minute, if they start moving in this direction, wouldn't that make Intel's growing GPU division superfluous or redundant?Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;tamalero&lt;/header&gt;So an outright overpowered monopoly trying to prop up a falling monopoly by setting up joint products.. what can go wrong?Reply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45288161</guid><pubDate>Thu, 18 Sep 2025 11:04:48 +0000</pubDate></item><item><title>KDE is now my favorite desktop</title><link>https://kokada.dev/blog/kde-is-now-my-favorite-desktop/</link><description>&lt;doc fingerprint="7416641b31042abb"&gt;
  &lt;main&gt;
    &lt;p&gt;From my last blog post, I am now using KDE as the desktop environment for my gaming rig. The reason is because I want a reasonably easy to use Linux desktop for when my wife needs to use the PC for something other than gaming, and this was the reason why my "traditional" Sway setup was a no-go.&lt;/p&gt;
    &lt;p&gt;But, after using KDE for a while I am starting to really appreciate how good it is. And no, this is not compared to other Linux desktops, but also with both Windows and macOS (that I need to use often, especially the later since my job gave me a MacBook Pro).&lt;/p&gt;
    &lt;p&gt;To start, KDE is surprisingly feature-complete. For example, the network applet gives lots of information that in other operational systems are either not available or difficult to access. It is easy to see in the screenshot below:&lt;/p&gt;
    &lt;p&gt;You can see things like channel, signal strength, frequency, MAC address, BSSID address (so the MAC address of the router). It even includes a handy button to share the Wi-Fi information via QR code, so you can easily setup a new mobile device like Android.&lt;/p&gt;
    &lt;p&gt;By the way, the crop and blur from that screenshot above? I made everything using the integrated screenshot tool. I didn't need to open an external application even once. It is also really smart, I need to redo this screenshot a few times and it kept the cropping to the exact area I was taking the screenshot before.&lt;/p&gt;
    &lt;p&gt;Another example, I wanted Steam to start automatically with the system, but it has the bad habit of putting its main window at the top. Really annoying since it sometimes ended up stealing up the focus. However KDE has this "Window Rules" feature inside "Window Management" settings where you can pretty much control whatever you want about application windows. Really useful tool.&lt;/p&gt;
    &lt;p&gt;KDE also has lots of really well integrated tools. For example, I am using some Flatpak applications and I can easily configure the permissions via System Settings. Or if I want hardware information like SMART status, I can just open Info Center. I can prevent the screen and computer to sleep at the click of a button (something that in both Windows and macOS I need to install a separate program). The list goes on, I keep getting surprised how many things that I used to need a third-party program that KDE just has available by default.&lt;/p&gt;
    &lt;p&gt;But not only KDE is fully featured, it is also fast. Now to be clear, this is a completely subjective analysis but I find KDE faster than Windows 11 in the same hardware, especially for things integrated in the system itself. For example, while opening Windows settings it can take a few seconds after a cold boot, the KDE's System Settings is pretty much instantaneous. Even compared with macOS in my MacBook Pro M2 Pro (that is of course comparing Apples and Bananas), KDE just feels snappier. I actually can't find much difference between KDE and my Sway setup to be honest, except maybe for the heavy use of animations (that can be disabled, but I ended up liking it after a while).&lt;/p&gt;
    &lt;p&gt;I will not say KDE is perfect though. At the first launch I got one issue where it started without the task bar because I connected this PC to both my monitor and TV, but the TV is used exclusively for gaming. However, KDE considered my TV the primary desktop and put the task bar only in that monitor, and even disabling the TV didn't add the task bar to my monitor. Easily fixed by manually adding a task bar, but an annoying problem (especially when you're not used to the desktop). There were also a few other minor issues that I don't remember right now.&lt;/p&gt;
    &lt;p&gt;After using KDE for about a week I can say that this is the first time that I really enjoy a desktop environment on Linux, after all those years. Props for the KDE developers for making the experience so good.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45288690</guid><pubDate>Thu, 18 Sep 2025 12:17:51 +0000</pubDate></item><item><title>Flipper Zero Geiger Counter</title><link>https://kasiin.top/blog/2025-08-04-flipper_zero_geiger_counter_module/</link><description>&lt;doc fingerprint="1c73dc206bab88f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Flipper Zero Geiger Counter&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;注意：所有模块均在第三方固件中测试，推荐使用:&lt;/p&gt;&lt;lb/&gt;unleashed固件，链接：[https://github.com/DarkFlippers/unleashed-firmware]&lt;lb/&gt;Momentum固件，链接：[https://github.com/Next-Flip/Momentum-Firmware]&lt;/quote&gt;
    &lt;head rend="h2"&gt;Compatible apps&lt;/head&gt;
    &lt;head rend="h2"&gt;Geiger counter&lt;/head&gt;
    &lt;p&gt;This app gives you a graph view with counts per second (instantaneous measure of the radioactivity) as CPS and per minute as CPM.&lt;/p&gt;
    &lt;p&gt;There is extra functionality to record, zoom, change units, etc.&lt;/p&gt;
    &lt;p&gt;(credits to nmrr)&lt;/p&gt;
    &lt;p&gt;CPS: counts per second (instantaneous measure of the radioactivity). CPS is alway displayed on the left corner.&lt;/p&gt;
    &lt;p&gt;CPM: counts per minute (the sum of CPS over a period of one minute). Other units of measurement can be chosen with Left/Right.&lt;/p&gt;
    &lt;p&gt;New CPS bar measure appears on the left every second.&lt;/p&gt;
    &lt;p&gt;A4 GPIO can be connected on A7 GPIO to test this application without using a geiger tube. A4 GPIO generates a signal with a frequency that varies every second.&lt;/p&gt;
    &lt;head rend="h3"&gt;Button assignments:&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;button&lt;/cell&gt;
        &lt;cell role="head"&gt;function&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ok [long press]&lt;/cell&gt;
        &lt;cell&gt;Clear the graph&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Left/Right [short press]&lt;/cell&gt;
        &lt;cell&gt;Choose unit on the right corner (cpm, μSv/h, mSv/y, Rad/h, mRad/h, uRad/h), cps on the left is always displayed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Up [long press]&lt;/cell&gt;
        &lt;cell&gt;Enable/disable recording, led of Flipper Zero is colored in red when recording&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Up/Down [short press]&lt;/cell&gt;
        &lt;cell&gt;Zoom/unzoom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Down [long press]&lt;/cell&gt;
        &lt;cell&gt;Display version of the application&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Back [long press]&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Use cases&lt;/head&gt;
    &lt;p&gt;Ambient radioactivity (descendants of radon gas are detected, not radon itself):&lt;/p&gt;
    &lt;p&gt;Measurement of a sample of uranium ore within a lead container:&lt;/p&gt;
    &lt;p&gt;Note: measures in Sv or Rad are not precise&lt;/p&gt;
    &lt;p&gt;Measurement of a sample of uranium ore (the most radioactive part):&lt;/p&gt;
    &lt;p&gt;Measurement of radium dial pointers:&lt;/p&gt;
    &lt;p&gt;All prior measurements in sequence (the scale of the graph is automatically adjusted):&lt;/p&gt;
    &lt;p&gt;Measurement of uranium orange pottery:&lt;/p&gt;
    &lt;p&gt;Measurement of americium-241 button from a smoke detector (descendants of americium or radioisotope impurities are detected, not americium itself):&lt;/p&gt;
    &lt;p&gt;A4 GPIO on A7 GPIO (to test this program without a geiger board):&lt;/p&gt;
    &lt;p&gt;Zoom levels (the third picture is the default zoom):&lt;/p&gt;
    &lt;p&gt;Version of the application (press down button during 1 sec to display version):&lt;/p&gt;
    &lt;head rend="h3"&gt;Recording function&lt;/head&gt;
    &lt;p&gt;Output CSV files are stored in the root directory of the SD card. Date and time are incorporated into the file name (example: geiger-2023-07-03--23-48-15.csv)&lt;/p&gt;
    &lt;p&gt;Data sample:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;epoch&lt;/cell&gt;
        &lt;cell role="head"&gt;cps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: J305 geiger tube is only sensible to beta and gamma rays. Alpha rays cannot be detected.&lt;/p&gt;
    &lt;p&gt;Usable radioactive sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;natural uranium (alpha, beta, gamma)&lt;/item&gt;
      &lt;item&gt;natural thorium (alpha, beta, gamma)&lt;/item&gt;
      &lt;item&gt;radium-226 (alpha, beta, gamma)&lt;/item&gt;
      &lt;item&gt;cobalt-60 (beta &amp;amp; gamma)&lt;/item&gt;
      &lt;item&gt;iodine-131 (beta &amp;amp; gamma)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not really usable radioactive sources (must be in contact with the geiger tube to be detected):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;americium-241 (alpha &amp;amp; low gamma, some strong beta/gamma rays are emitted during radioactive cascade or due to the presence of radioisotope impurities)&lt;/item&gt;
      &lt;item&gt;high purity metallic uranium/thorium (same as am241)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Totaly unusable radioactive sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;polonium-210 (pure alpha)&lt;/item&gt;
      &lt;item&gt;tritium (very low beta)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Atomic dice roller&lt;/head&gt;
    &lt;p&gt;With this app you can get a really random dice based on the geiger counter. Make your decisions and board games extra exciting.&lt;/p&gt;
    &lt;p&gt;(credits to nmrr)&lt;/p&gt;
    &lt;p&gt;This application generates true random numbers by hashing timestamps obtained when a tick is produced by the geiger counter (i.e. when a beta or gamma ray is detected). Timestamps have 32 bit resolution and are produced from a 64 MHz signal.&lt;/p&gt;
    &lt;p&gt;Two hash methods have been implemented:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CRC32: 8 ticks are needed to obtain a hash, for low activity sources&lt;/item&gt;
      &lt;item&gt;MD5: 32 ticks are needed to obtain a hash, for high activity sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dice rolls are produced by transforming a single hash into a number between 1 and 6. Out of scope values are ignored so the dice is really balanced. Modulo-based methods are ugly because they are usually unbalanced.&lt;/p&gt;
    &lt;p&gt;It's possible to roll the dice without using a radioactive isotope. Air contains radon gas that is radioactive. Geiger board can detect descendants of radon gas that emit strong beta or gamma rays.&lt;/p&gt;
    &lt;p&gt;In the left corner, counts per second (cps) indicates the activity. In the right corner, availiable dice rolls are indicated. 64 rolls can be stored.&lt;/p&gt;
    &lt;head rend="h3"&gt;Button assignments:&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;button&lt;/cell&gt;
        &lt;cell role="head"&gt;function&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ok [short short]&lt;/cell&gt;
        &lt;cell&gt;Roll the dice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Left [long press]&lt;/cell&gt;
        &lt;cell&gt;Set CRC32 as hash method&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Right [long press]&lt;/cell&gt;
        &lt;cell&gt;Set MD5 as hash method&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Up [long press]&lt;/cell&gt;
        &lt;cell&gt;Set 0-1 as output range (coin flipper)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Down [long press]&lt;/cell&gt;
        &lt;cell&gt;Set 1-6 as output range (dice roller)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Back [long press]&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Warning&lt;/head&gt;
    &lt;p&gt;These apps are for educational purposes only. Please use this code responsibly and only use these apps on your own equipment.&lt;lb/&gt; 本模块和软件只能用作教育和学习用途，一切使用责任请自负，请仅在您拥有的设备上使用&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45289453</guid><pubDate>Thu, 18 Sep 2025 13:28:59 +0000</pubDate></item><item><title>Grief gets an expiration date, just like us</title><link>https://bessstillman.substack.com/p/oh-fuck-youre-still-sad</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45290021</guid><pubDate>Thu, 18 Sep 2025 14:17:40 +0000</pubDate></item><item><title>Learn Your Way: Reimagining Textbooks with Generative AI</title><link>https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/</link><description>&lt;doc fingerprint="bd71201abef387ab"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Learn Your Way: Reimagining textbooks with generative AI&lt;/head&gt;
    &lt;p&gt;September 16, 2025&lt;/p&gt;
    &lt;p&gt;Gal Elidan, Research Scientist, and Yael Haramaty, Senior Product Manager, Google Research&lt;/p&gt;
    &lt;p&gt;New research into GenAI in education demonstrates a novel approach to reimagining textbooks that led to improved learning outcomes in a recent study. The research comes to life in our interactive experience, Learn Your Way, now available on Google Labs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick links&lt;/head&gt;
    &lt;p&gt;Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. The manual creation of textbooks demands significant human effort, and as a result they lack alternative perspectives, multiple formats and tailored variations that can make learning more effective and engaging. At Google, we’re exploring how we can use generative AI (GenAI) to automatically generate alternative representations or personalized examples, while preserving the integrity of the source material. What if students had the power to shape their own learning journey, exploring materials using various formats that fit their evolving needs? What if we could reimagine the textbook to be as unique as every learner?&lt;/p&gt;
    &lt;p&gt;Recent advances in GenAI are bringing this vision closer to reality. Today we are excited to introduce Learn Your Way, now on Google Labs, a research experiment that explores how GenAI can transform educational materials to create a more effective, engaging, learner-driven experience for every student. Here we outline the research and pedagogy underpinning Learn Your Way, with more details in the accompanying tech report. We also report early indicators of its impact: in our efficacy study, students using Learn Your Way scored 11 percentage points higher on retention tests than students using a standard digital reader.&lt;/p&gt;
    &lt;head rend="h2"&gt;Grounded in learning, built for the student&lt;/head&gt;
    &lt;p&gt;Our approach is built on two key pillars that work together to augment the learning experience: (1) generating various multimodal representations of the content, and (2) taking foundational steps toward personalization.&lt;/p&gt;
    &lt;p&gt;The seminal dual coding theory states that forging mental connections between different representations strengthens the underlying conceptual schema in our brain. Subsequent research indeed showed that when students actively engage with information in various formats, they build a more robust and complete mental model of the material. Inspired by this, our approach empowers students with the agency to choose and intermix multiple formats and modalities to best help them understand the material. In addition, personalization is increasingly becoming an aspirational standard in K-12 educational settings, and so our research reflects this. We aim to enhance the relatability and effectiveness of educational content by adapting it to student attributes. Moreover, we incorporate quizzing capabilities that enable us to further tailor the experience according to the learners’ real-time responses. Such personalization can be a powerful method for enhancing motivation and deepening learning.&lt;/p&gt;
    &lt;p&gt;Bringing this to life involves a layered technical approach using LearnLM, our best-in-class pedagogy-infused family of models, now integrated directly into Gemini 2.5 Pro. The first layer is a unique personalization pipeline that serves as the basis for the second layer of multiple content representations. Our starting point is a textbook PDF, although our approach could be used with other forms of source material.&lt;/p&gt;
    &lt;head rend="h3"&gt;The personalization pipeline&lt;/head&gt;
    &lt;p&gt;The Learn Your Way interface asks the learner to select their grade and interests (e.g., sports, music, food). The original source material is first re-leveled to the learner’s reported grade level, while maintaining the scope of its content. This is followed by the strategic replacement of generic examples with ones that are personalized to the learner’s reported interests. The resulting text serves as the basis for the generation of all the other representations, effectively propagating the personalization effect and setting up a pipeline for further personalization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Multiple representations of content&lt;/head&gt;
    &lt;p&gt;Following the source personalization, we generate multiple representations of the content. For some content representations, such as mind maps and timelines, Gemini’s broad capabilities are used directly. Other features such as narrated slides, require more elaborate pipelines that weave together multiple specialized AI agents and tools to achieve an effective pedagogical result. Finally, specialized tasks, such as generating effective educational visuals, proved too challenging even for state-of-the-art general-purpose image models. To overcome this, we fine-tuned a dedicated model specifically for generating educational illustrations. The combination of a powerful base model, multi-step agentic workflows, and fine-tuned components allows us to generate a wide range of high-quality multimodal representations for learning.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Learn Your Way experience&lt;/head&gt;
    &lt;p&gt;Our research comes to life in Learn Your Way. The interface brings together multiple, personalized representations of content including: (1) immersive text, (2) section-level quizzes, (3) slides &amp;amp; narration, (4) audio lessons, and (5) mind maps.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Immersive text: Breaks the content up into digestible sections that are augmented with generated images and embedded questions. Put together, these transform passive reading into an active multimodal experience that follows learning science principles.&lt;/item&gt;
      &lt;item&gt;Section-level quizzes: Promote active learning by allowing a user to interactively assess their learning, and uncover existing knowledge gaps.&lt;/item&gt;
      &lt;item&gt;Slides &amp;amp; narration: Offers presentations that span the entire source material and include engaging activities like fill-in-the-blanks, as well as a narrated version, mimicking a recorded lesson.&lt;/item&gt;
      &lt;item&gt;Audio lesson: Provides simulated conversations, coupled with visual aids, between an AI-powered teacher and a student that models how a real learner might engage with the material, including the expression of misconceptions, which are clarified by the teacher.&lt;/item&gt;
      &lt;item&gt;Mind map: Organizes the knowledge hierarchically and allows learners to zoom in and out from the big picture to the details.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The above representations give learners choice and are all adapted to their selected grade level and personal interests. Throughout the experience, the interactive quizzes provide dynamic feedback, guiding students to revisit specific content areas where they struggled. This marks our first steps towards true personalization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pedagogical evaluation&lt;/head&gt;
    &lt;p&gt;To evaluate Learn You Way's pedagogical performance, we transformed ten varied source materials from OpenStax (a provider of free educational textbooks) to three different personalization settings. The source materials covered various subjects from history to physics. Three pedagogical subject matter experts then evaluated the transformed materials using pedagogical criteria, such as accuracy, coverage, and the LearnLM learning science principles.&lt;/p&gt;
    &lt;p&gt;The results were highly positive, with an average expert rating of 0.85 or higher across all pedagogical criteria. See the tech report for more evaluation details.&lt;/p&gt;
    &lt;head rend="h2"&gt;Efficacy study&lt;/head&gt;
    &lt;p&gt;An AI-powered learning tool is only valuable if it both effectively improves learning outcomes and students want to use it. Learn Your Way now serves as a research platform for us to conduct studies with partners around the world to explore how AI-powered transformations and personalization affects outcomes, and to ensure that what we build is effective and locally relevant.&lt;/p&gt;
    &lt;p&gt;Recently, we conducted a randomized controlled study with 60 students from the Chicago area, ages 15–18 and with similar reading levels. Participants were given up to 40 minutes to learn about adolescent brain development from a textbook, and randomly assigned to learn using Learn Your Way or a traditional digital PDF reader.&lt;/p&gt;
    &lt;p&gt;We assessed students with a quiz immediately after the study session, and with a retention test 3–5 days later, using assessments designed by pedagogical experts to be a good measure of content comprehension. We also surveyed them about the learning experience, and to gain deeper insights beyond these quantitative metrics, each student participated in a 30-minute qualitative interview where they could share more nuanced feedback about their experience.&lt;/p&gt;
    &lt;p&gt;The results were compelling and statistically significant. Here are the highlights. See the tech report for more details.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Positive learning outcomes: The Learn Your Way group scored, on average, 9% higher on the immediate assessment following the study session.&lt;/item&gt;
      &lt;item&gt;Better long-term retention: Similarly, the Learn Your Way group scored 11% higher on the retention assessment 3-5 days later (78% vs. 67%).&lt;/item&gt;
      &lt;item&gt;Positive user sentiment: 100% of students who used Learn Your Way reported that they felt the tool made them more comfortable taking the assessment, compared to 70% in the digital reader control group. 93% said they would want to use Learn Your Way for future learning, compared to just 67% for the digital reader.&lt;/item&gt;
      &lt;item&gt;Valuable experience: Insights from the qualitative interviews revealed that students found great value in Learn Your Way.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Experience Learn Your Way&lt;/head&gt;
    &lt;p&gt;To give a concrete feel for the Learn Your Way interactive experience, today we are releasing example experiences on Google Labs, including:&lt;/p&gt;
    &lt;head rend="h2"&gt;The path forward&lt;/head&gt;
    &lt;p&gt;Our findings suggest that generative AI can be used to build learning experiences that are not only more effective but also more empowering. By evolving the static textbook into an interactive artifact and giving students greater agency over how they learn, we saw learning retention improve.&lt;/p&gt;
    &lt;p&gt;This work is just the beginning of our exploration. We envision many more ways to tailor content, moving towards systems that continuously adapt to each learner's unique needs and progress. As we take our next steps towards personalized education, we will continue to ground our research in pedagogical principles, measuring the impact of AI on learning efficacy, so that in the future every student might have access to a high-quality, engaging learning experience that is custom built for them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Acknowledgements&lt;/head&gt;
    &lt;p&gt;Shout out to our Google Research LearnLM team who have contributed to this work: Alicia Martín, Amir Globerson, Amy Wang, Anirudh Shekhawat, Anisha Choudhury, Anna Iurchenko, Avinatan Hassidim, Ayça Çakmakli, Ayelet Shasha Evron, Charlie Yang, Courtney Heldreth, Dana Oria, Diana Akrong, Hairong Mu, Ian Li, Ido Cohen, Komal Singh, Lev Borovoi, Lidan Hackmon, Lior Belinsky, Michael Fink, Preeti Singh, Rena Levitt, Shashank Agarwal, Shay Sharon, Sophie Allweis, Tracey Lee-Joe, Xiaohong Hao, Yael Gold-Zamir, Yishay Mor, and Yoav Bar Sinai. Special thanks to our executive champions: Niv Efron, Avinatan Hassidim, Yossi Matias and Ben Gomes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45292648</guid><pubDate>Thu, 18 Sep 2025 17:42:56 +0000</pubDate></item><item><title>This map is not upside down</title><link>https://www.maps.com/this-map-is-not-upside-down/</link><description>&lt;doc fingerprint="f00ca5471275de50"&gt;
  &lt;main&gt;
    &lt;p&gt;If you were to close your eyes and picture a map of the world, chances are you would imagine a conventional image: North America (including Greenland) and Europe at the top, Africa somewhere in the middle, and South America, Australia, and Antarctica at the bottom. But it does not have to be so, and it was not always so.&lt;/p&gt;
    &lt;p&gt;It’s not that our planet underwent change—it did, of course, but long before maps. Our conventions as map readers and makers have converged over time to a north-up default. What happens when that convention is challenged? Robert Simmon, a cartographer who previously developed maps for Planet Labs and NASA Earth Observatory, designed a map to address that question.&lt;/p&gt;
    &lt;p&gt;Simmon’s map includes countries, major lakes, oceans, gulfs, seas, roads, and cities. It also features inset maps depicting Earth’s biosphere, global land cover, and bathymetry. All of these are familiar to most map readers. Yet the map may seem disorienting to many. Simmon’s map is geographically correct, yet purposely—and literally—turns convention on its head. What was once familiar becomes alien, challenging readers to look at Earth anew. It also encourages us to think more deeply about such conventions: Why is north almost always at the top of maps? And must it always be that way? Simmon’s map reminds us that it doesn’t have to be.&lt;/p&gt;
    &lt;p&gt;Simmon is not the first to create a south-up map. Others have made these maps to challenge conventions and norms. And it was only rather recently that north-up maps became so commonplace. Centuries ago, cartographers drew maps with the top being south, east, or other orientations. These alternatives reflected the limited tools, knowledge, and practices of the time. The basic idea behind a compass (which most today recognize as a device that points north) was known to the Han and Tang dynasties of China more than 2,000 years ago. Early Chinese navigators used magnetized devices as a compass, but for them south was the dominant position from which bearings were derived.&lt;/p&gt;
    &lt;p&gt;Deciding to put south, or north, at the top of maps is a decision of consequence. Psychologically, we tend to view things nearer the top as ‘good’ and those lower as ‘bad.’ This can influence our interpretation of maps at both global and local scales. Still, the prominence of north-up maps did not arrive deliberately to elevate the status of some areas or their rulers. It is in part a consequence of the work of Ptolemey, who first labeled his maps with calculated lines of latitude and longitude. This made it easy for others to copy, extend, and derive new maps. Each map drawn in this way would adopt Ptolemy’s orientation.&lt;/p&gt;
    &lt;p&gt;Regardless of the reasons for a given orientation and the implications that follow, Simmon’s map reminds us to challenge tradition and consider its influence. As both a map and a philosophical prompt, this example hits the mark beautifully.&lt;/p&gt;
    &lt;head rend="h2"&gt;About This Map&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Title&lt;/item&gt;
      &lt;item rend="dd-1"&gt;The World, South Up&lt;/item&gt;
      &lt;item rend="dt-2"&gt;Creator&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Robert simmon&lt;/item&gt;
      &lt;item rend="dt-3"&gt;Data Sources&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Natural Earth, GEBCO&lt;/item&gt;
          &lt;item&gt;NASA/USGS MODIS Land Cover Classification&lt;/item&gt;
          &lt;item&gt;NOAA VIIRS NDVI&lt;/item&gt;
          &lt;item&gt;NOAA VIIRS Ocean Color&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This map was contributed through the Maps.com submission program. If you’d like your map to be featured, submit it for consideration.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Tags&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45292694</guid><pubDate>Thu, 18 Sep 2025 17:47:19 +0000</pubDate></item><item><title>U.S. already has the critical minerals it needs, according to new analysis</title><link>https://www.minesnewsroom.com/news/us-already-has-critical-minerals-it-needs-theyre-being-thrown-away-new-analysis-shows</link><description>&lt;doc fingerprint="e9780551b293659e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;U.S. already has the critical minerals it needs – but they're being thrown away, new analysis shows&lt;/head&gt;
    &lt;p&gt;In new Science article, Colorado School of Mines researchers call for more research, development and policy to increase critical mineral recovery&lt;/p&gt;
    &lt;p&gt;All the critical minerals the U.S. needs annually for energy, defense and technology applications are already being mined at existing U.S. facilities, according to a new analysis published today in the journal Science.&lt;/p&gt;
    &lt;p&gt;The catch? These minerals, such as cobalt, lithium, gallium and rare earth elements like neodymium and yttrium, are currently being discarded as tailings of other mineral streams like gold and zinc, said Elizabeth Holley, associate professor of mining engineering at Colorado School of Mines and lead author of the new paper.&lt;/p&gt;
    &lt;p&gt;"The challenge lies in recovery," Holley said. "It's like getting salt out of bread dough – we need to do a lot more research, development and policy to make the recovery of these critical minerals economically feasible."&lt;/p&gt;
    &lt;p&gt;To conduct the analysis, Holley and her team built a database of annual production from federally permitted metal mines in the U.S. They used a statistical resampling technique to pair these data with the geochemical concentrations of critical minerals in ores, recently compiled by the U.S. Geological Survey, Geoscience Australia and the Geologic Survey of Canada.&lt;/p&gt;
    &lt;p&gt;Using this approach, Holley’s team was able to estimate the quantities of critical minerals being mined and processed every year at U.S. metal mines but not being recovered. Instead, these valuable minerals are ending up as discarded tailings that must be stored and monitored to prevent environmental contamination.&lt;/p&gt;
    &lt;p&gt;“This is a brand-new view of ‘low hanging fruit’ – we show where each critical mineral exists and the sites at which even 1 percent recovery of a particular critical mineral could make a huge difference, in many cases dramatically reducing or even eliminating the need to import that mineral,” Holley said.&lt;/p&gt;
    &lt;p&gt;The analysis in Science looks at a total of 70 elements used in applications ranging from consumer electronics like cell phones to medical devices to satellites to renewable energy to fighter jets and shows that unrecovered byproducts from other U.S. mines could meet the demand for all but two – platinum and palladium.&lt;/p&gt;
    &lt;p&gt;Among the elements included in the analysis are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cobalt (Co): The lustrous bluish-gray metal, a key component in electric car batteries, is a byproduct of nickel and copper mining. Recovering less than 10 percent of the cobalt currently being mined and processed but not recovered would be more than enough to fuel the entire U.S. battery market.&lt;/item&gt;
      &lt;item&gt;Germanium (Ge): The brittle silvery-white semi-metal used for electronics and infrared optics, including sensors on missiles and defense satellites, is present in zinc and molybdenum mines. If the U.S. recovered less than 1 percent of the germanium currently mined and processed but not recovered from U.S. mines, it would not have to import any germanium to meet industry needs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The benefits of enhanced recovery are not only economic and geopolitical but also environmental, Holley said – recovering these critical minerals instead of sending them to tailings piles would reduce the environmental impact of mine waste and open more opportunities for reuse in construction and other industries.&lt;/p&gt;
    &lt;p&gt;“Now that we know which sites are low-hanging fruit, we need to conduct detailed analyses of the minerals in which these chemical elements reside and then test the technologies suitable for recovery of those elements from those specific minerals,” Holley said. “We also need policies that incentivize mine operators to incorporate additional processing infrastructure. Although these elements are needed, their market value may not be sufficient to motivate operators to invest in new equipment and processes without the right policies in place.”&lt;/p&gt;
    &lt;p&gt;Co-authors on the paper are Karlie Hadden, PhD candidate in geology; Dorit Hammerling, associate professor of applied mathematics and statistics; Rod Eggert, research professor of economics and business; Erik Spiller, research professor of mining engineering; and Priscilla Nelson, professor of mining engineering.&lt;/p&gt;
    &lt;p&gt;Read the full paper, "Byproduct recovery from US metal mines could reduce import reliance for critical minerals," on the Science website. To access the data and figures before the paper appears in print, contact Mines Media Relations Specialist Erich Kirshner at erich.kirshner@mines.edu.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45294058</guid><pubDate>Thu, 18 Sep 2025 19:41:50 +0000</pubDate></item><item><title>Apple: SSH and FileVault</title><link>https://keith.github.io/xcode-man-pages/apple_ssh_and_filevault.7.html</link><description>&lt;doc fingerprint="201543195f03b8c1"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;apple_ssh_and_filevault(7)&lt;/cell&gt;
        &lt;cell&gt;Miscellaneous Information Manual&lt;/cell&gt;
        &lt;cell&gt;apple_ssh_and_filevault(7)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;code&gt;apple_ssh_and_filevault&lt;/code&gt; —
    SSH and FileVault&lt;/p&gt;
    &lt;p&gt;When FileVault is enabled, the data volume is locked and unavailable during and after booting, until an account has been authenticated using a password. The macOS version of OpenSSH stores all of its configuration files, both system-wide and per-account, in the data volume. Therefore, the usually configured authentication methods and shell access are not available during this time. However, when Remote Login is enabled, it is possible to perform password authentication using SSH even in this situation. This can be used to unlock the data volume remotely over the network. However, it does not immediately permit an SSH session. Instead, once the data volume has been unlocked using this method, macOS will disconnect SSH briefly while it completes mounting the data volume and starting the remaining services dependent on it. Thereafter, SSH (and other enabled services) are fully available.&lt;/p&gt;
    &lt;p&gt;The capability to unlock the data volume over SSH appeared in macOS 26 Tahoe.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;1 July, 2025&lt;/cell&gt;
        &lt;cell&gt;Darwin&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45294440</guid><pubDate>Thu, 18 Sep 2025 20:15:45 +0000</pubDate></item><item><title>AI tools are making the world look weird</title><link>https://strat7.com/blogs/weird-in-weird-out/</link><description>&lt;doc fingerprint="7adbb9070d4782bc"&gt;
  &lt;main&gt;
    &lt;quote&gt;&lt;p&gt;The images arrive already complete; there is no communication between them and myself, no reciprocal exchange. As much as we like to say that the world is opening up to us, since we can see every part of it, we can also say that the world is closing itself off – in all its openness.&lt;/p&gt;Karl Ove Knausgård&lt;/quote&gt;
    &lt;p&gt;In academia and the media, AI is often described as mirroring human psychology with humanlike reasoning, human-level performance, human-like communication. In these comparisons, “humans” are treated as the benchmark.&lt;/p&gt;
    &lt;p&gt;In a provocative 2023 paper, researchers at Harvard University asked – which humans?&lt;/p&gt;
    &lt;p&gt;The diversity of human psychologies has been a hot topic since 2010, when researchers found that many accepted psychological “truths” were often confined to so-called “WEIRD people”: Western, Educated, Industrialised, Rich, Democratic. What feel like universal beliefs for people like me and no doubt many of the readers of this blog, e.g. that I am an automonous individual, are instead only true for a thin slice of humanity.&lt;/p&gt;
    &lt;p&gt;So when we say AI tools are “human-like”, what we mean is that AI is WEIRD.&lt;/p&gt;
    &lt;p&gt;In fact, this paper found that more than that, it thinks American. The greater the cultural distance between a country and the USA, the less accurate ChatGPT got at simulating peoples’ values. For countries like Libya and Pakistan, AI results are little better than a coin toss.&lt;/p&gt;
    &lt;p&gt;This paper showed this through an ingenious method – administering the World Values Survey (WVS) 1,000 times to ChatGPT and then comparing it to real data from other countries. The WVS measures everything from self-reported cultural values to moral principles, attitudes towards family, religion, poverty and so on.[1]&lt;lb/&gt;In this simple chart, they plotted two variables:&lt;/p&gt;
    &lt;p&gt;The eagle-eyed reader may note that ChatGPT responses are slightly more correlated to smaller Western countries such as New Zealand than the US. This likely reflects the USA’s greater cultural diversity, and the fact that ChatGPT was developed in California.&lt;/p&gt;
    &lt;p&gt;Marketers and researchers in non-WEIRD countries have often struggled for budget and bandwidth. AI tools’ poorer accuracy in their markets therefore introduce a double jeopardy: the non-WEIRD countries least likely to secure research budgets also have the worst accuracy from “off the shelf” AI tools.&lt;/p&gt;
    &lt;p&gt;These biases could show up throughout the research process, for example:&lt;/p&gt;
    &lt;p&gt;There is a real risk that increasing use of AI tools in international research will flatten out and devalue insights, as highly diverse peoples’ individual spoken and unspoken responses are fed into text-processing machines and emerge looking and sounding vaguely Californian. What looks like a living, breathing forest to us may end up being processed as just so much wood.&lt;/p&gt;
    &lt;p&gt;This doesn’t mean we should ignore AI tools when working cross-culturally. They’re simply too useful. Instead, I’d suggest that we need to invest in the cultural fitness of our thinking and processes. Just as you don’t have to lose physical stamina when you start driving a car, your projects do not have to atrophy cultural meaning when you introduce elements of automation.&lt;/p&gt;
    &lt;p&gt;As researchers we can deepen the cultural layer in our international work by:&lt;/p&gt;
    &lt;p&gt;And as AI product owners or users, it’s more a question of how we minimise the loss of cultural meaning. I remain to be convinced that AI moderation will achieve that for as long as the “moderator” is powered by an American LLM with deep-coded cultural biases. When using AI for analysis or to design research, however, there is likely to be marginal value in:&lt;/p&gt;
    &lt;p&gt;LLMs process information in a WEIRD way, are psychologically WEIRD, and assume the average human is too. At the same time, the LLM space is getting ever more concentrated with US companies dominating. There is a real risk that as researchers we could get pulled into ways of working and thinking that make the world feel smaller, and much less wondrous. We must build our cultural fitness to ensure that we can introduce automation without losing sight of the many ways of being human. &lt;lb/&gt;Here at STRAT7 (and for me personally), we want to explore further:&lt;/p&gt;
    &lt;p&gt;We’re going to conduct a few experiments of our own with LLMs from different continents – comment below if you have any thoughts on what to look out for.&lt;/p&gt;
    &lt;p&gt;[1] Separately, I recommend checking out the WVS website – it is old school but has some great data and insights.&lt;/p&gt;
    &lt;p&gt;[2] The researchers released an interactive tool to compare cultural distance across a range of dimensions, worth a play around&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45295794</guid><pubDate>Thu, 18 Sep 2025 22:27:17 +0000</pubDate></item><item><title>Want to piss off your IT department? Are the links not malicious looking enough?</title><link>https://phishyurl.com/</link><description>&lt;doc fingerprint="1ee9dc214d0432b1"&gt;
  &lt;main&gt;
    &lt;p&gt;This tool is guaranteed to help with that!&lt;/p&gt;
    &lt;p&gt;This is a tool that takes any link and makes it look malicious. It works on the idea of a redirect. Much like https://tinyurl.com/ for example. Where tinyurl makes an url shorter, this site makes it look malicious.&lt;/p&gt;
    &lt;p&gt;Place any link in the below input, press the button and get back a fishy(phishy, heh...get, it?) looking link. The fishy link doesn't actually do anything, it will just redirect you to the original link you provided.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45295898</guid><pubDate>Thu, 18 Sep 2025 22:40:06 +0000</pubDate></item><item><title>Llama-Factory: Unified, Efficient Fine-Tuning for 100 Open LLMs</title><link>https://github.com/hiyouga/LLaMA-Factory</link><description>&lt;doc fingerprint="bd11b258ad3a99da"&gt;
  &lt;main&gt;
    &lt;p&gt;👋 Join our WeChat, NPU, Lab4AI, LLaMA Factory Online user group.&lt;/p&gt;
    &lt;p&gt;[ English | 中文 ]&lt;/p&gt;
    &lt;p&gt;Fine-tuning a large language model can be easy as...&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;train_en.mp4&lt;/head&gt;
    &lt;p&gt;Choose your path:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Documentation (WIP): https://llamafactory.readthedocs.io/en/latest/&lt;/item&gt;
      &lt;item&gt;Documentation (AMD GPU): https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/llama_factory_llama3.html&lt;/item&gt;
      &lt;item&gt;Colab (free): https://colab.research.google.com/drive/1eRTPn37ltBbYsISy9Aw2NuI2Aq5CQrD9?usp=sharing&lt;/item&gt;
      &lt;item&gt;Local machine: Please refer to usage&lt;/item&gt;
      &lt;item&gt;PAI-DSW (free trial): https://gallery.pai-ml.com/#/preview/deepLearning/nlp/llama_factory&lt;/item&gt;
      &lt;item&gt;Alaya NeW (cloud GPU deal): https://docs.alayanew.com/docs/documents/useGuide/LLaMAFactory/mutiple/?utm_source=LLaMA-Factory&lt;/item&gt;
      &lt;item&gt;Official Course: https://www.lab4ai.cn/course/detail?id=7c13e60f6137474eb40f6fd3983c0f46&amp;amp;utm_source=LLaMA-Factory&lt;/item&gt;
      &lt;item&gt;LLaMA Factory Online: https://www.llamafactory.com.cn/?utm_source=LLaMA-Factory&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Except for the above links, all other websites are unauthorized third-party websites. Please carefully use them.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Features&lt;/item&gt;
      &lt;item&gt;Blogs&lt;/item&gt;
      &lt;item&gt;Changelog&lt;/item&gt;
      &lt;item&gt;Supported Models&lt;/item&gt;
      &lt;item&gt;Supported Training Approaches&lt;/item&gt;
      &lt;item&gt;Provided Datasets&lt;/item&gt;
      &lt;item&gt;Requirement&lt;/item&gt;
      &lt;item&gt;Getting Started&lt;/item&gt;
      &lt;item&gt;Projects using LLaMA Factory&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
      &lt;item&gt;Citation&lt;/item&gt;
      &lt;item&gt;Acknowledgement&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Various models: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.&lt;/item&gt;
      &lt;item&gt;Integrated methods: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.&lt;/item&gt;
      &lt;item&gt;Scalable resources: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.&lt;/item&gt;
      &lt;item&gt;Advanced algorithms: GaLore, BAdam, APOLLO, Adam-mini, Muon, OFT, DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.&lt;/item&gt;
      &lt;item&gt;Practical tricks: FlashAttention-2, Unsloth, Liger Kernel, RoPE scaling, NEFTune and rsLoRA.&lt;/item&gt;
      &lt;item&gt;Wide tasks: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.&lt;/item&gt;
      &lt;item&gt;Experiment monitors: LlamaBoard, TensorBoard, Wandb, MLflow, SwanLab, etc.&lt;/item&gt;
      &lt;item&gt;Faster inference: OpenAI-style API, Gradio UI and CLI with vLLM worker or SGLang worker.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Support Date&lt;/cell&gt;
        &lt;cell role="head"&gt;Model Name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Day 0&lt;/cell&gt;
        &lt;cell&gt;Qwen3 / Qwen2.5-VL / Gemma 3 / GLM-4.1V / InternLM 3 / MiniCPM-o-2.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Day 1&lt;/cell&gt;
        &lt;cell&gt;Llama 3 / GLM-4 / Mistral Small / PaliGemma2 / Llama 4&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;💡 Easy Dataset × LLaMA Factory: Enabling LLMs to Efficiently Learn Domain Knowledge (English)&lt;/item&gt;
      &lt;item&gt;Fine-tune a mental health LLM using LLaMA-Factory (Chinese)&lt;/item&gt;
      &lt;item&gt;Fine-tune GPT-OSS for Role-Playing using LLaMA-Factory (Chinese)&lt;/item&gt;
      &lt;item&gt;A One-Stop Code-Free Model Reinforcement Learning and Deployment Platform based on LLaMA-Factory and EasyR1 (Chinese)&lt;/item&gt;
      &lt;item&gt;How Apoidea Group enhances visual information extraction from banking documents with multimodal models using LLaMA-Factory on Amazon SageMaker HyperPod (English)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;All Blogs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fine-tune Llama3.1-70B for Medical Diagnosis using LLaMA-Factory (Chinese)&lt;/item&gt;
      &lt;item&gt;Fine-tune Qwen2.5-VL for Autonomous Driving using LLaMA-Factory (Chinese)&lt;/item&gt;
      &lt;item&gt;LLaMA Factory: Fine-tuning the DeepSeek-R1-Distill-Qwen-7B Model for News Classifier (Chinese)&lt;/item&gt;
      &lt;item&gt;A One-Stop Code-Free Model Fine-Tuning &amp;amp; Deployment Platform based on SageMaker and LLaMA-Factory (Chinese)&lt;/item&gt;
      &lt;item&gt;LLaMA Factory Multi-Modal Fine-Tuning Practice: Fine-Tuning Qwen2-VL for Personal Tourist Guide (Chinese)&lt;/item&gt;
      &lt;item&gt;LLaMA Factory: Fine-tuning Llama3 for Role-Playing (Chinese)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;[25/08/22] We supported OFT and OFTv2. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[25/08/20] We supported fine-tuning the Intern-S1-mini models. See PR #8976 to get started.&lt;/p&gt;
    &lt;p&gt;[25/08/06] We supported fine-tuning the GPT-OSS models. See PR #8826 to get started.&lt;/p&gt;
    &lt;head&gt;Full Changelog&lt;/head&gt;
    &lt;p&gt;[25/07/02] We supported fine-tuning the GLM-4.1V-9B-Thinking model.&lt;/p&gt;
    &lt;p&gt;[25/04/28] We supported fine-tuning the Qwen3 model family.&lt;/p&gt;
    &lt;p&gt;[25/04/21] We supported the Muon optimizer. See examples for usage. Thank @tianshijing's PR.&lt;/p&gt;
    &lt;p&gt;[25/04/16] We supported fine-tuning the InternVL3 model. See PR #7258 to get started.&lt;/p&gt;
    &lt;p&gt;[25/04/14] We supported fine-tuning the GLM-Z1 and Kimi-VL models.&lt;/p&gt;
    &lt;p&gt;[25/04/06] We supported fine-tuning the Llama 4 model. See PR #7611 to get started.&lt;/p&gt;
    &lt;p&gt;[25/03/31] We supported fine-tuning the Qwen2.5 Omni model. See PR #7537 to get started.&lt;/p&gt;
    &lt;p&gt;[25/03/15] We supported SGLang as inference backend. Try &lt;code&gt;infer_backend: sglang&lt;/code&gt; to accelerate inference.&lt;/p&gt;
    &lt;p&gt;[25/03/12] We supported fine-tuning the Gemma 3 model.&lt;/p&gt;
    &lt;p&gt;[25/02/24] Announcing EasyR1, an efficient, scalable and multi-modality RL training framework for efficient GRPO training.&lt;/p&gt;
    &lt;p&gt;[25/02/11] We supported saving the Ollama modelfile when exporting the model checkpoints. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[25/02/05] We supported fine-tuning the Qwen2-Audio and MiniCPM-o-2.6 on audio understanding tasks.&lt;/p&gt;
    &lt;p&gt;[25/01/31] We supported fine-tuning the DeepSeek-R1 and Qwen2.5-VL models.&lt;/p&gt;
    &lt;p&gt;[25/01/15] We supported APOLLO optimizer. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[25/01/14] We supported fine-tuning the MiniCPM-o-2.6 and MiniCPM-V-2.6 models. Thank @BUAADreamer's PR.&lt;/p&gt;
    &lt;p&gt;[25/01/14] We supported fine-tuning the InternLM 3 models. Thank @hhaAndroid's PR.&lt;/p&gt;
    &lt;p&gt;[25/01/10] We supported fine-tuning the Phi-4 model.&lt;/p&gt;
    &lt;p&gt;[24/12/21] We supported using SwanLab for experiment tracking and visualization. See this section for details.&lt;/p&gt;
    &lt;p&gt;[24/11/27] We supported fine-tuning the Skywork-o1 model and the OpenO1 dataset.&lt;/p&gt;
    &lt;p&gt;[24/10/09] We supported downloading pre-trained models and datasets from the Modelers Hub. See this tutorial for usage.&lt;/p&gt;
    &lt;p&gt;[24/09/19] We supported fine-tuning the Qwen2.5 models.&lt;/p&gt;
    &lt;p&gt;[24/08/30] We supported fine-tuning the Qwen2-VL models. Thank @simonJJJ's PR.&lt;/p&gt;
    &lt;p&gt;[24/08/27] We supported Liger Kernel. Try &lt;code&gt;enable_liger_kernel: true&lt;/code&gt; for efficient training.&lt;/p&gt;
    &lt;p&gt;[24/08/09] We supported Adam-mini optimizer. See examples for usage. Thank @relic-yuexi's PR.&lt;/p&gt;
    &lt;p&gt;[24/07/04] We supported contamination-free packed training. Use &lt;code&gt;neat_packing: true&lt;/code&gt; to activate it. Thank @chuan298's PR.&lt;/p&gt;
    &lt;p&gt;[24/06/16] We supported PiSSA algorithm. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/06/07] We supported fine-tuning the Qwen2 and GLM-4 models.&lt;/p&gt;
    &lt;p&gt;[24/05/26] We supported SimPO algorithm for preference learning. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/05/20] We supported fine-tuning the PaliGemma series models. Note that the PaliGemma models are pre-trained models, you need to fine-tune them with &lt;code&gt;paligemma&lt;/code&gt; template for chat completion.&lt;/p&gt;
    &lt;p&gt;[24/05/18] We supported KTO algorithm for preference learning. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/05/14] We supported training and inference on the Ascend NPU devices. Check installation section for details.&lt;/p&gt;
    &lt;p&gt;[24/04/26] We supported fine-tuning the LLaVA-1.5 multimodal LLMs. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/04/22] We provided a Colab notebook for fine-tuning the Llama-3 model on a free T4 GPU. Two Llama-3-derived models fine-tuned using LLaMA Factory are available at Hugging Face, check Llama3-8B-Chinese-Chat and Llama3-Chinese for details.&lt;/p&gt;
    &lt;p&gt;[24/04/21] We supported Mixture-of-Depths according to AstraMindAI's implementation. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/04/16] We supported BAdam optimizer. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/04/16] We supported unsloth's long-sequence training (Llama-2-7B-56k within 24GB). It achieves 117% speed and 50% memory compared with FlashAttention-2, more benchmarks can be found in this page.&lt;/p&gt;
    &lt;p&gt;[24/03/31] We supported ORPO. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/03/21] Our paper "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models" is available at arXiv!&lt;/p&gt;
    &lt;p&gt;[24/03/20] We supported FSDP+QLoRA that fine-tunes a 70B model on 2x24GB GPUs. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/03/13] We supported LoRA+. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/03/07] We supported GaLore optimizer. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/03/07] We integrated vLLM for faster and concurrent inference. Try &lt;code&gt;infer_backend: vllm&lt;/code&gt; to enjoy 270% inference speed.&lt;/p&gt;
    &lt;p&gt;[24/02/28] We supported weight-decomposed LoRA (DoRA). Try &lt;code&gt;use_dora: true&lt;/code&gt; to activate DoRA training.&lt;/p&gt;
    &lt;p&gt;[24/02/15] We supported block expansion proposed by LLaMA Pro. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[24/02/05] Qwen1.5 (Qwen2 beta version) series models are supported in LLaMA-Factory. Check this blog post for details.&lt;/p&gt;
    &lt;p&gt;[24/01/18] We supported agent tuning for most models, equipping model with tool using abilities by fine-tuning with &lt;code&gt;dataset: glaive_toolcall_en&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;[23/12/23] We supported unsloth's implementation to boost LoRA tuning for the LLaMA, Mistral and Yi models. Try &lt;code&gt;use_unsloth: true&lt;/code&gt; argument to activate unsloth patch. It achieves 170% speed in our benchmark, check this page for details.&lt;/p&gt;
    &lt;p&gt;[23/12/12] We supported fine-tuning the latest MoE model Mixtral 8x7B in our framework. See hardware requirement here.&lt;/p&gt;
    &lt;p&gt;[23/12/01] We supported downloading pre-trained models and datasets from the ModelScope Hub. See this tutorial for usage.&lt;/p&gt;
    &lt;p&gt;[23/10/21] We supported NEFTune trick for fine-tuning. Try &lt;code&gt;neftune_noise_alpha: 5&lt;/code&gt; argument to activate NEFTune.&lt;/p&gt;
    &lt;p&gt;[23/09/27] We supported &lt;code&gt;shift_attn: true&lt;/code&gt; argument to enable shift short attention.&lt;/p&gt;
    &lt;p&gt;[23/09/23] We integrated MMLU, C-Eval and CMMLU benchmarks in this repo. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[23/09/10] We supported FlashAttention-2. Try &lt;code&gt;flash_attn: fa2&lt;/code&gt; argument to enable FlashAttention-2 if you are using RTX4090, A100 or H100 GPUs.&lt;/p&gt;
    &lt;p&gt;[23/08/12] We supported RoPE scaling to extend the context length of the LLaMA models. Try &lt;code&gt;rope_scaling: linear&lt;/code&gt; argument in training and &lt;code&gt;rope_scaling: dynamic&lt;/code&gt; argument at inference to extrapolate the position embeddings.&lt;/p&gt;
    &lt;p&gt;[23/08/11] We supported DPO training for instruction-tuned models. See examples for usage.&lt;/p&gt;
    &lt;p&gt;[23/07/31] We supported dataset streaming. Try &lt;code&gt;streaming: true&lt;/code&gt; and &lt;code&gt;max_steps: 10000&lt;/code&gt; arguments to load your dataset in streaming mode.&lt;/p&gt;
    &lt;p&gt;[23/07/29] We released two instruction-tuned 13B models at Hugging Face. See these Hugging Face Repos (LLaMA-2 / Baichuan) for details.&lt;/p&gt;
    &lt;p&gt;[23/07/18] We developed an all-in-one Web UI for training, evaluation and inference. Try &lt;code&gt;train_web.py&lt;/code&gt; to fine-tune models in your Web browser. Thank @KanadeSiina and @codemayq for their efforts in the development.&lt;/p&gt;
    &lt;p&gt;[23/07/09] We released FastEdit ⚡🩹, an easy-to-use package for editing the factual knowledge of large language models efficiently. Please follow FastEdit if you are interested.&lt;/p&gt;
    &lt;p&gt;[23/06/29] We provided a reproducible example of training a chat model using instruction-following datasets, see Baichuan-7B-sft for details.&lt;/p&gt;
    &lt;p&gt;[23/06/22] We aligned the demo API with the OpenAI's format where you can insert the fine-tuned model in arbitrary ChatGPT-based applications.&lt;/p&gt;
    &lt;p&gt;[23/06/03] We supported quantized training and inference (aka QLoRA). See examples for usage.&lt;/p&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;If you cannot use the latest feature, please pull the latest code and install LLaMA-Factory again.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Model size&lt;/cell&gt;
        &lt;cell role="head"&gt;Template&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baichuan 2&lt;/cell&gt;
        &lt;cell&gt;7B/13B&lt;/cell&gt;
        &lt;cell&gt;baichuan2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;BLOOM/BLOOMZ&lt;/cell&gt;
        &lt;cell&gt;560M/1.1B/1.7B/3B/7.1B/176B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ChatGLM3&lt;/cell&gt;
        &lt;cell&gt;6B&lt;/cell&gt;
        &lt;cell&gt;chatglm3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Command R&lt;/cell&gt;
        &lt;cell&gt;35B/104B&lt;/cell&gt;
        &lt;cell&gt;cohere&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DeepSeek (Code/MoE)&lt;/cell&gt;
        &lt;cell&gt;7B/16B/67B/236B&lt;/cell&gt;
        &lt;cell&gt;deepseek&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DeepSeek 2.5/3&lt;/cell&gt;
        &lt;cell&gt;236B/671B&lt;/cell&gt;
        &lt;cell&gt;deepseek3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DeepSeek R1 (Distill)&lt;/cell&gt;
        &lt;cell&gt;1.5B/7B/8B/14B/32B/70B/671B&lt;/cell&gt;
        &lt;cell&gt;deepseekr1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Falcon&lt;/cell&gt;
        &lt;cell&gt;7B/11B/40B/180B&lt;/cell&gt;
        &lt;cell&gt;falcon&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Falcon-H1&lt;/cell&gt;
        &lt;cell&gt;0.5B/1.5B/3B/7B/34B&lt;/cell&gt;
        &lt;cell&gt;falcon_h1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gemma/Gemma 2/CodeGemma&lt;/cell&gt;
        &lt;cell&gt;2B/7B/9B/27B&lt;/cell&gt;
        &lt;cell&gt;gemma/gemma2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Gemma 3/Gemma 3n&lt;/cell&gt;
        &lt;cell&gt;270M/1B/4B/6B/8B/12B/27B&lt;/cell&gt;
        &lt;cell&gt;gemma3/gemma3n&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GLM-4/GLM-4-0414/GLM-Z1&lt;/cell&gt;
        &lt;cell&gt;9B/32B&lt;/cell&gt;
        &lt;cell&gt;glm4/glmz1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GLM-4.1V&lt;/cell&gt;
        &lt;cell&gt;9B&lt;/cell&gt;
        &lt;cell&gt;glm4v&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GLM-4.5/GLM-4.5V&lt;/cell&gt;
        &lt;cell&gt;106B/355B&lt;/cell&gt;
        &lt;cell&gt;glm4_moe/glm4v_moe&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPT-2&lt;/cell&gt;
        &lt;cell&gt;0.1B/0.4B/0.8B/1.5B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GPT-OSS&lt;/cell&gt;
        &lt;cell&gt;20B/120B&lt;/cell&gt;
        &lt;cell&gt;gpt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Granite 3.0-3.3&lt;/cell&gt;
        &lt;cell&gt;1B/2B/3B/8B&lt;/cell&gt;
        &lt;cell&gt;granite3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Granite 4&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;granite4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hunyuan&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;hunyuan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Index&lt;/cell&gt;
        &lt;cell&gt;1.9B&lt;/cell&gt;
        &lt;cell&gt;index&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;InternLM 2-3&lt;/cell&gt;
        &lt;cell&gt;7B/8B/20B&lt;/cell&gt;
        &lt;cell&gt;intern2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;InternVL 2.5-3.5&lt;/cell&gt;
        &lt;cell&gt;1B/2B/4B/8B/14B/30B/38B/78B/241B&lt;/cell&gt;
        &lt;cell&gt;intern_vl&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;InternLM/Intern-S1-mini&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;intern_s1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Kimi-VL&lt;/cell&gt;
        &lt;cell&gt;16B&lt;/cell&gt;
        &lt;cell&gt;kimi_vl&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Llama&lt;/cell&gt;
        &lt;cell&gt;7B/13B/33B/65B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Llama 2&lt;/cell&gt;
        &lt;cell&gt;7B/13B/70B&lt;/cell&gt;
        &lt;cell&gt;llama2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Llama 3-3.3&lt;/cell&gt;
        &lt;cell&gt;1B/3B/8B/70B&lt;/cell&gt;
        &lt;cell&gt;llama3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Llama 4&lt;/cell&gt;
        &lt;cell&gt;109B/402B&lt;/cell&gt;
        &lt;cell&gt;llama4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Llama 3.2 Vision&lt;/cell&gt;
        &lt;cell&gt;11B/90B&lt;/cell&gt;
        &lt;cell&gt;mllama&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LLaVA-1.5&lt;/cell&gt;
        &lt;cell&gt;7B/13B&lt;/cell&gt;
        &lt;cell&gt;llava&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LLaVA-NeXT&lt;/cell&gt;
        &lt;cell&gt;7B/8B/13B/34B/72B/110B&lt;/cell&gt;
        &lt;cell&gt;llava_next&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LLaVA-NeXT-Video&lt;/cell&gt;
        &lt;cell&gt;7B/34B&lt;/cell&gt;
        &lt;cell&gt;llava_next_video&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MiMo&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;mimo&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MiniCPM 1-4.1&lt;/cell&gt;
        &lt;cell&gt;0.5B/1B/2B/4B/8B&lt;/cell&gt;
        &lt;cell&gt;cpm/cpm3/cpm4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MiniCPM-o-2.6/MiniCPM-V-2.6&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;minicpm_o/minicpm_v&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ministral/Mistral-Nemo&lt;/cell&gt;
        &lt;cell&gt;8B/12B&lt;/cell&gt;
        &lt;cell&gt;ministral&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mistral/Mixtral&lt;/cell&gt;
        &lt;cell&gt;7B/8x7B/8x22B&lt;/cell&gt;
        &lt;cell&gt;mistral&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Mistral Small&lt;/cell&gt;
        &lt;cell&gt;24B&lt;/cell&gt;
        &lt;cell&gt;mistral_small&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;OLMo&lt;/cell&gt;
        &lt;cell&gt;1B/7B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PaliGemma/PaliGemma2&lt;/cell&gt;
        &lt;cell&gt;3B/10B/28B&lt;/cell&gt;
        &lt;cell&gt;paligemma&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Phi-1.5/Phi-2&lt;/cell&gt;
        &lt;cell&gt;1.3B/2.7B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Phi-3/Phi-3.5&lt;/cell&gt;
        &lt;cell&gt;4B/14B&lt;/cell&gt;
        &lt;cell&gt;phi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Phi-3-small&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;phi_small&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Phi-4&lt;/cell&gt;
        &lt;cell&gt;14B&lt;/cell&gt;
        &lt;cell&gt;phi4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Pixtral&lt;/cell&gt;
        &lt;cell&gt;12B&lt;/cell&gt;
        &lt;cell&gt;pixtral&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen (1-2.5) (Code/Math/MoE/QwQ)&lt;/cell&gt;
        &lt;cell&gt;0.5B/1.5B/3B/7B/14B/32B/72B/110B&lt;/cell&gt;
        &lt;cell&gt;qwen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen3 (MoE/Instruct/Thinking/Next)&lt;/cell&gt;
        &lt;cell&gt;0.6B/1.7B/4B/8B/14B/32B/80B/235B&lt;/cell&gt;
        &lt;cell&gt;qwen3/qwen3_nothink&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2-Audio&lt;/cell&gt;
        &lt;cell&gt;7B&lt;/cell&gt;
        &lt;cell&gt;qwen2_audio&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell&gt;3B/7B&lt;/cell&gt;
        &lt;cell&gt;qwen2_omni&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2-VL/Qwen2.5-VL/QVQ&lt;/cell&gt;
        &lt;cell&gt;2B/3B/7B/32B/72B&lt;/cell&gt;
        &lt;cell&gt;qwen2_vl&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Seed (OSS/Coder)&lt;/cell&gt;
        &lt;cell&gt;8B/36B&lt;/cell&gt;
        &lt;cell&gt;seed_oss/seed_coder&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Skywork o1&lt;/cell&gt;
        &lt;cell&gt;8B&lt;/cell&gt;
        &lt;cell&gt;skywork_o1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;StarCoder 2&lt;/cell&gt;
        &lt;cell&gt;3B/7B/15B&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TeleChat2&lt;/cell&gt;
        &lt;cell&gt;3B/7B/35B/115B&lt;/cell&gt;
        &lt;cell&gt;telechat2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;XVERSE&lt;/cell&gt;
        &lt;cell&gt;7B/13B/65B&lt;/cell&gt;
        &lt;cell&gt;xverse&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Yi/Yi-1.5 (Code)&lt;/cell&gt;
        &lt;cell&gt;1.5B/6B/9B/34B&lt;/cell&gt;
        &lt;cell&gt;yi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Yi-VL&lt;/cell&gt;
        &lt;cell&gt;6B/34B&lt;/cell&gt;
        &lt;cell&gt;yi_vl&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Yuan 2&lt;/cell&gt;
        &lt;cell&gt;2B/51B/102B&lt;/cell&gt;
        &lt;cell&gt;yuan&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;For the "base" models, the &lt;code&gt;template&lt;/code&gt; argument can be chosen from &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;alpaca&lt;/code&gt;, &lt;code&gt;vicuna&lt;/code&gt; etc. But make sure to use the corresponding template for the "instruct/chat" models.&lt;/p&gt;
    &lt;p&gt;Remember to use the SAME template in training and inference.&lt;/p&gt;
    &lt;p&gt;*: You should install the &lt;code&gt;transformers&lt;/code&gt; from main branch and use &lt;code&gt;DISABLE_VERSION_CHECK=1&lt;/code&gt; to skip version check.&lt;/p&gt;
    &lt;p&gt;**: You need to install a specific version of &lt;code&gt;transformers&lt;/code&gt; to use the corresponding model.&lt;/p&gt;
    &lt;p&gt;Please refer to constants.py for a full list of models we supported.&lt;/p&gt;
    &lt;p&gt;You also can add a custom chat template to template.py.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Approach&lt;/cell&gt;
        &lt;cell role="head"&gt;Full-tuning&lt;/cell&gt;
        &lt;cell role="head"&gt;Freeze-tuning&lt;/cell&gt;
        &lt;cell role="head"&gt;LoRA&lt;/cell&gt;
        &lt;cell role="head"&gt;QLoRA&lt;/cell&gt;
        &lt;cell role="head"&gt;OFT&lt;/cell&gt;
        &lt;cell role="head"&gt;QOFT&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Pre-Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Supervised Fine-Tuning&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reward Modeling&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;PPO Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;DPO Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;KTO Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ORPO Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;SimPO Training&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;The implementation details of PPO can be found in this blog.&lt;/p&gt;
    &lt;head&gt;Pre-training datasets&lt;/head&gt;
    &lt;head&gt;Supervised fine-tuning datasets&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Identity (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Stanford Alpaca (en)&lt;/item&gt;
      &lt;item&gt;Stanford Alpaca (zh)&lt;/item&gt;
      &lt;item&gt;Alpaca GPT4 (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Glaive Function Calling V2 (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;LIMA (en)&lt;/item&gt;
      &lt;item&gt;Guanaco Dataset (multilingual)&lt;/item&gt;
      &lt;item&gt;BELLE 2M (zh)&lt;/item&gt;
      &lt;item&gt;BELLE 1M (zh)&lt;/item&gt;
      &lt;item&gt;BELLE 0.5M (zh)&lt;/item&gt;
      &lt;item&gt;BELLE Dialogue 0.4M (zh)&lt;/item&gt;
      &lt;item&gt;BELLE School Math 0.25M (zh)&lt;/item&gt;
      &lt;item&gt;BELLE Multiturn Chat 0.8M (zh)&lt;/item&gt;
      &lt;item&gt;UltraChat (en)&lt;/item&gt;
      &lt;item&gt;OpenPlatypus (en)&lt;/item&gt;
      &lt;item&gt;CodeAlpaca 20k (en)&lt;/item&gt;
      &lt;item&gt;Alpaca CoT (multilingual)&lt;/item&gt;
      &lt;item&gt;OpenOrca (en)&lt;/item&gt;
      &lt;item&gt;SlimOrca (en)&lt;/item&gt;
      &lt;item&gt;MathInstruct (en)&lt;/item&gt;
      &lt;item&gt;Firefly 1.1M (zh)&lt;/item&gt;
      &lt;item&gt;Wiki QA (en)&lt;/item&gt;
      &lt;item&gt;Web QA (zh)&lt;/item&gt;
      &lt;item&gt;WebNovel (zh)&lt;/item&gt;
      &lt;item&gt;Nectar (en)&lt;/item&gt;
      &lt;item&gt;deepctrl (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Advertise Generating (zh)&lt;/item&gt;
      &lt;item&gt;ShareGPT Hyperfiltered (en)&lt;/item&gt;
      &lt;item&gt;ShareGPT4 (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;UltraChat 200k (en)&lt;/item&gt;
      &lt;item&gt;Infinity Instruct (zh)&lt;/item&gt;
      &lt;item&gt;AgentInstruct (en)&lt;/item&gt;
      &lt;item&gt;LMSYS Chat 1M (en)&lt;/item&gt;
      &lt;item&gt;Evol Instruct V2 (en)&lt;/item&gt;
      &lt;item&gt;Cosmopedia (en)&lt;/item&gt;
      &lt;item&gt;STEM (zh)&lt;/item&gt;
      &lt;item&gt;Ruozhiba (zh)&lt;/item&gt;
      &lt;item&gt;Neo-sft (zh)&lt;/item&gt;
      &lt;item&gt;Magpie-Pro-300K-Filtered (en)&lt;/item&gt;
      &lt;item&gt;Magpie-ultra-v0.1 (en)&lt;/item&gt;
      &lt;item&gt;WebInstructSub (en)&lt;/item&gt;
      &lt;item&gt;OpenO1-SFT (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Open-Thoughts (en)&lt;/item&gt;
      &lt;item&gt;Open-R1-Math (en)&lt;/item&gt;
      &lt;item&gt;Chinese-DeepSeek-R1-Distill (zh)&lt;/item&gt;
      &lt;item&gt;LLaVA mixed (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Pokemon-gpt4o-captions (en&amp;amp;zh)&lt;/item&gt;
      &lt;item&gt;Open Assistant (de)&lt;/item&gt;
      &lt;item&gt;Dolly 15k (de)&lt;/item&gt;
      &lt;item&gt;Alpaca GPT4 (de)&lt;/item&gt;
      &lt;item&gt;OpenSchnabeltier (de)&lt;/item&gt;
      &lt;item&gt;Evol Instruct (de)&lt;/item&gt;
      &lt;item&gt;Dolphin (de)&lt;/item&gt;
      &lt;item&gt;Booksum (de)&lt;/item&gt;
      &lt;item&gt;Airoboros (de)&lt;/item&gt;
      &lt;item&gt;Ultrachat (de)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head&gt;Preference datasets&lt;/head&gt;
    &lt;p&gt;Some datasets require confirmation before using them, so we recommend logging in with your Hugging Face account using these commands.&lt;/p&gt;
    &lt;code&gt;pip install --upgrade huggingface_hub
huggingface-cli login&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Mandatory&lt;/cell&gt;
        &lt;cell role="head"&gt;Minimum&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;python&lt;/cell&gt;
        &lt;cell&gt;3.9&lt;/cell&gt;
        &lt;cell&gt;3.10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;torch&lt;/cell&gt;
        &lt;cell&gt;2.0.0&lt;/cell&gt;
        &lt;cell&gt;2.6.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;torchvision&lt;/cell&gt;
        &lt;cell&gt;0.15.0&lt;/cell&gt;
        &lt;cell&gt;0.21.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;transformers&lt;/cell&gt;
        &lt;cell&gt;4.49.0&lt;/cell&gt;
        &lt;cell&gt;4.50.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;datasets&lt;/cell&gt;
        &lt;cell&gt;2.16.0&lt;/cell&gt;
        &lt;cell&gt;3.2.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;accelerate&lt;/cell&gt;
        &lt;cell&gt;0.34.0&lt;/cell&gt;
        &lt;cell&gt;1.2.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;peft&lt;/cell&gt;
        &lt;cell&gt;0.14.0&lt;/cell&gt;
        &lt;cell&gt;0.15.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;trl&lt;/cell&gt;
        &lt;cell&gt;0.8.6&lt;/cell&gt;
        &lt;cell&gt;0.9.6&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Optional&lt;/cell&gt;
        &lt;cell role="head"&gt;Minimum&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CUDA&lt;/cell&gt;
        &lt;cell&gt;11.6&lt;/cell&gt;
        &lt;cell&gt;12.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;deepspeed&lt;/cell&gt;
        &lt;cell&gt;0.10.0&lt;/cell&gt;
        &lt;cell&gt;0.16.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;bitsandbytes&lt;/cell&gt;
        &lt;cell&gt;0.39.0&lt;/cell&gt;
        &lt;cell&gt;0.43.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;vllm&lt;/cell&gt;
        &lt;cell&gt;0.4.3&lt;/cell&gt;
        &lt;cell&gt;0.8.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;flash-attn&lt;/cell&gt;
        &lt;cell&gt;2.5.6&lt;/cell&gt;
        &lt;cell&gt;2.7.2&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;* estimated&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Bits&lt;/cell&gt;
        &lt;cell role="head"&gt;7B&lt;/cell&gt;
        &lt;cell role="head"&gt;14B&lt;/cell&gt;
        &lt;cell role="head"&gt;30B&lt;/cell&gt;
        &lt;cell role="head"&gt;70B&lt;/cell&gt;
        &lt;cell role="head"&gt;&lt;code&gt;x&lt;/code&gt;B&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Full (&lt;code&gt;bf16&lt;/code&gt; or &lt;code&gt;fp16&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;32&lt;/cell&gt;
        &lt;cell&gt;120GB&lt;/cell&gt;
        &lt;cell&gt;240GB&lt;/cell&gt;
        &lt;cell&gt;600GB&lt;/cell&gt;
        &lt;cell&gt;1200GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;18x&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Full (&lt;code&gt;pure_bf16&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;60GB&lt;/cell&gt;
        &lt;cell&gt;120GB&lt;/cell&gt;
        &lt;cell&gt;300GB&lt;/cell&gt;
        &lt;cell&gt;600GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;8x&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Freeze/LoRA/GaLore/APOLLO/BAdam/OFT&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
        &lt;cell&gt;16GB&lt;/cell&gt;
        &lt;cell&gt;32GB&lt;/cell&gt;
        &lt;cell&gt;64GB&lt;/cell&gt;
        &lt;cell&gt;160GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;2x&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;QLoRA / QOFT&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;10GB&lt;/cell&gt;
        &lt;cell&gt;20GB&lt;/cell&gt;
        &lt;cell&gt;40GB&lt;/cell&gt;
        &lt;cell&gt;80GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;x&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;QLoRA / QOFT&lt;/cell&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;6GB&lt;/cell&gt;
        &lt;cell&gt;12GB&lt;/cell&gt;
        &lt;cell&gt;24GB&lt;/cell&gt;
        &lt;cell&gt;48GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;x/2&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;QLoRA / QOFT&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;4GB&lt;/cell&gt;
        &lt;cell&gt;8GB&lt;/cell&gt;
        &lt;cell&gt;16GB&lt;/cell&gt;
        &lt;cell&gt;24GB&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;x/4&lt;/code&gt;GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Important&lt;/p&gt;
    &lt;p&gt;Installation is mandatory.&lt;/p&gt;
    &lt;code&gt;git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]" --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Extra dependencies available: torch, torch-npu, metrics, deepspeed, liger-kernel, bitsandbytes, hqq, eetq, gptq, aqlm, vllm, sglang, galore, apollo, badam, adam-mini, qwen, minicpm_v, openmind, swanlab, dev&lt;/p&gt;
    &lt;code&gt;docker run -it --rm --gpus=all --ipc=host hiyouga/llamafactory:latest&lt;/code&gt;
    &lt;p&gt;This image is built on Ubuntu 22.04 (x86_64), CUDA 12.4, Python 3.11, PyTorch 2.6.0, and Flash-attn 2.7.4.&lt;/p&gt;
    &lt;p&gt;Find the pre-built images: https://hub.docker.com/r/hiyouga/llamafactory/tags&lt;/p&gt;
    &lt;p&gt;Please refer to build docker to build the image yourself.&lt;/p&gt;
    &lt;head&gt;Setting up a virtual environment with uv&lt;/head&gt;
    &lt;p&gt;Create an isolated Python environment with uv:&lt;/p&gt;
    &lt;code&gt;uv sync --extra torch --extra metrics --prerelease=allow&lt;/code&gt;
    &lt;p&gt;Run LLaMA-Factory in the isolated environment:&lt;/p&gt;
    &lt;code&gt;uv run --prerelease=allow llamafactory-cli train examples/train_lora/llama3_lora_pretrain.yaml&lt;/code&gt;
    &lt;head&gt;For Windows users&lt;/head&gt;
    &lt;p&gt;You need to manually install the GPU version of PyTorch on the Windows platform. Please refer to the official website and the following command to install PyTorch with CUDA support:&lt;/p&gt;
    &lt;code&gt;pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
python -c "import torch; print(torch.cuda.is_available())"&lt;/code&gt;
    &lt;p&gt;If you see &lt;code&gt;True&lt;/code&gt; then you have successfully installed PyTorch with CUDA support.&lt;/p&gt;
    &lt;p&gt;Try &lt;code&gt;dataloader_num_workers: 0&lt;/code&gt; if you encounter &lt;code&gt;Can't pickle local object&lt;/code&gt; error.&lt;/p&gt;
    &lt;p&gt;If you want to enable the quantized LoRA (QLoRA) on the Windows platform, you need to install a pre-built version of &lt;code&gt;bitsandbytes&lt;/code&gt; library, which supports CUDA 11.1 to 12.2, please select the appropriate release version based on your CUDA version.&lt;/p&gt;
    &lt;code&gt;pip install https://github.com/jllllll/bitsandbytes-windows-webui/releases/download/wheels/bitsandbytes-0.41.2.post2-py3-none-win_amd64.whl&lt;/code&gt;
    &lt;p&gt;To enable FlashAttention-2 on the Windows platform, please use the script from flash-attention-windows-wheel to compile and install it by yourself.&lt;/p&gt;
    &lt;head&gt;For Ascend NPU users&lt;/head&gt;
    &lt;p&gt;To install LLaMA Factory on Ascend NPU devices, please upgrade Python to version 3.10 or higher and specify extra dependencies: &lt;code&gt;pip install -e ".[torch-npu,metrics]"&lt;/code&gt;. Additionally, you need to install the Ascend CANN Toolkit and Kernels. Please follow the installation tutorial or use the following commands:&lt;/p&gt;
    &lt;code&gt;# replace the url according to your CANN version and devices
# install CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run
bash Ascend-cann-toolkit_8.0.0.alpha002_linux-"$(uname -i)".run --install

# install CANN Kernels
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/Milan-ASL/Milan-ASL%20V100R001C20SPC702/Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run
bash Ascend-cann-kernels-910b_8.0.0.alpha002_linux-"$(uname -i)".run --install

# set env variables
source /usr/local/Ascend/ascend-toolkit/set_env.sh&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Requirement&lt;/cell&gt;
        &lt;cell role="head"&gt;Minimum&lt;/cell&gt;
        &lt;cell role="head"&gt;Recommend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CANN&lt;/cell&gt;
        &lt;cell&gt;8.0.RC1&lt;/cell&gt;
        &lt;cell&gt;8.0.0.alpha002&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;torch&lt;/cell&gt;
        &lt;cell&gt;2.1.0&lt;/cell&gt;
        &lt;cell&gt;2.4.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;torch-npu&lt;/cell&gt;
        &lt;cell&gt;2.1.0&lt;/cell&gt;
        &lt;cell&gt;2.4.0.post2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;deepspeed&lt;/cell&gt;
        &lt;cell&gt;0.13.2&lt;/cell&gt;
        &lt;cell&gt;0.13.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;vllm-ascend&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;0.7.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Remember to use &lt;code&gt;ASCEND_RT_VISIBLE_DEVICES&lt;/code&gt; instead of &lt;code&gt;CUDA_VISIBLE_DEVICES&lt;/code&gt; to specify the device to use.&lt;/p&gt;
    &lt;p&gt;If you cannot infer model on NPU devices, try setting &lt;code&gt;do_sample: false&lt;/code&gt; in the configurations.&lt;/p&gt;
    &lt;p&gt;Download the pre-built Docker images: 32GB | 64GB&lt;/p&gt;
    &lt;head rend="h4"&gt;Install BitsAndBytes&lt;/head&gt;
    &lt;p&gt;To use QLoRA based on bitsandbytes on Ascend NPU, please follow these 3 steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Manually compile bitsandbytes: Refer to the installation documentation for the NPU version of bitsandbytes to complete the compilation and installation. The compilation requires a cmake version of at least 3.22.1 and a g++ version of at least 12.x.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Install bitsandbytes from source
# Clone bitsandbytes repo, Ascend NPU backend is currently enabled on multi-backend-refactor branch
git clone -b multi-backend-refactor https://github.com/bitsandbytes-foundation/bitsandbytes.git
cd bitsandbytes/

# Install dependencies
pip install -r requirements-dev.txt

# Install the dependencies for the compilation tools. Note that the commands for this step may vary depending on the operating system. The following are provided for reference
apt-get install -y build-essential cmake

# Compile &amp;amp; install  
cmake -DCOMPUTE_BACKEND=npu -S .
make
pip install .&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install transformers from the main branch.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone -b main https://github.com/huggingface/transformers.git
cd transformers
pip install .&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Set &lt;code&gt;double_quantization: false&lt;/code&gt;in the configuration. You can refer to the example.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please refer to data/README.md for checking the details about the format of dataset files. You can use datasets on HuggingFace / ModelScope / Modelers hub, load the dataset in local disk, or specify a path to s3/gcs cloud storage.&lt;/p&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;Please update &lt;code&gt;data/dataset_info.json&lt;/code&gt; to use your custom dataset.&lt;/p&gt;
    &lt;p&gt;You can also use Easy Dataset, DataFlow and GraphGen to create synthetic data for fine-tuning.&lt;/p&gt;
    &lt;p&gt;Use the following 3 commands to run LoRA fine-tuning, inference and merging of the Llama3-8B-Instruct model, respectively.&lt;/p&gt;
    &lt;code&gt;llamafactory-cli train examples/train_lora/llama3_lora_sft.yaml
llamafactory-cli chat examples/inference/llama3_lora_sft.yaml
llamafactory-cli export examples/merge_lora/llama3_lora_sft.yaml&lt;/code&gt;
    &lt;p&gt;See examples/README.md for advanced usage (including distributed training).&lt;/p&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Use &lt;code&gt;llamafactory-cli help&lt;/code&gt; to show help information.&lt;/p&gt;
    &lt;p&gt;Read FAQs first if you encounter any problems.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fine-Tuning with LLaMA Board GUI (powered by Gradio)&lt;/head&gt;
    &lt;code&gt;llamafactory-cli webui&lt;/code&gt;
    &lt;p&gt;Read our documentation.&lt;/p&gt;
    &lt;p&gt;For CUDA users:&lt;/p&gt;
    &lt;code&gt;cd docker/docker-cuda/
docker compose up -d
docker compose exec llamafactory bash&lt;/code&gt;
    &lt;p&gt;For Ascend NPU users:&lt;/p&gt;
    &lt;code&gt;cd docker/docker-npu/
docker compose up -d
docker compose exec llamafactory bash&lt;/code&gt;
    &lt;p&gt;For AMD ROCm users:&lt;/p&gt;
    &lt;code&gt;cd docker/docker-rocm/
docker compose up -d
docker compose exec llamafactory bash&lt;/code&gt;
    &lt;head&gt;Build without Docker Compose&lt;/head&gt;
    &lt;p&gt;For CUDA users:&lt;/p&gt;
    &lt;code&gt;docker build -f ./docker/docker-cuda/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host --gpus=all \
    -p 7860:7860 \
    -p 8000:8000 \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash&lt;/code&gt;
    &lt;p&gt;For Ascend NPU users:&lt;/p&gt;
    &lt;code&gt;docker build -f ./docker/docker-npu/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=torch-npu,metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -v /usr/local/dcmi:/usr/local/dcmi \
    -v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \
    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
    -v /etc/ascend_install.info:/etc/ascend_install.info \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/davinci0 \
    --device /dev/davinci_manager \
    --device /dev/devmm_svm \
    --device /dev/hisi_hdc \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash&lt;/code&gt;
    &lt;p&gt;For AMD ROCm users:&lt;/p&gt;
    &lt;code&gt;docker build -f ./docker/docker-rocm/Dockerfile \
    --build-arg PIP_INDEX=https://pypi.org/simple \
    --build-arg EXTRAS=metrics \
    -t llamafactory:latest .

docker run -dit --ipc=host \
    -p 7860:7860 \
    -p 8000:8000 \
    --device /dev/kfd \
    --device /dev/dri \
    --name llamafactory \
    llamafactory:latest

docker exec -it llamafactory bash&lt;/code&gt;
    &lt;head&gt;Use Docker volumes&lt;/head&gt;
    &lt;p&gt;You can uncomment &lt;code&gt;VOLUME [ "/root/.cache/huggingface", "/app/shared_data", "/app/output" ]&lt;/code&gt; in the Dockerfile to use data volumes.&lt;/p&gt;
    &lt;p&gt;When building the Docker image, use &lt;code&gt;-v ./hf_cache:/root/.cache/huggingface&lt;/code&gt; argument to mount the local directory to the container. The following data volumes are available.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;hf_cache&lt;/code&gt;: Utilize Hugging Face cache on the host machine.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;shared_data&lt;/code&gt;: The directionary to store datasets on the host machine.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;output&lt;/code&gt;: Set export dir to this location so that the merged result can be accessed directly on the host machine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;API_PORT=8000 llamafactory-cli api examples/inference/llama3.yaml infer_backend=vllm vllm_enforce_eager=true&lt;/code&gt;
    &lt;p&gt;If you have trouble with downloading models and datasets from Hugging Face, you can use ModelScope.&lt;/p&gt;
    &lt;code&gt;export USE_MODELSCOPE_HUB=1 # `set USE_MODELSCOPE_HUB=1` for Windows&lt;/code&gt;
    &lt;p&gt;Train the model by specifying a model ID of the ModelScope Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at ModelScope Hub, e.g., &lt;code&gt;LLM-Research/Meta-Llama-3-8B-Instruct&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You can also use Modelers Hub to download models and datasets.&lt;/p&gt;
    &lt;code&gt;export USE_OPENMIND_HUB=1 # `set USE_OPENMIND_HUB=1` for Windows&lt;/code&gt;
    &lt;p&gt;Train the model by specifying a model ID of the Modelers Hub as the &lt;code&gt;model_name_or_path&lt;/code&gt;. You can find a full list of model IDs at Modelers Hub, e.g., &lt;code&gt;TeleAI/TeleChat-7B-pt&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To use Weights &amp;amp; Biases for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt;
    &lt;code&gt;report_to: wandb
run_name: test_run # optional&lt;/code&gt;
    &lt;p&gt;Set &lt;code&gt;WANDB_API_KEY&lt;/code&gt; to your key when launching training tasks to log in with your W&amp;amp;B account.&lt;/p&gt;
    &lt;p&gt;To use SwanLab for logging experimental results, you need to add the following arguments to yaml files.&lt;/p&gt;
    &lt;code&gt;use_swanlab: true
swanlab_run_name: test_run # optional&lt;/code&gt;
    &lt;p&gt;When launching training tasks, you can log in to SwanLab in three ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add &lt;code&gt;swanlab_api_key=&amp;lt;your_api_key&amp;gt;&lt;/code&gt;to the yaml file, and set it to your API key.&lt;/item&gt;
      &lt;item&gt;Set the environment variable &lt;code&gt;SWANLAB_API_KEY&lt;/code&gt;to your API key.&lt;/item&gt;
      &lt;item&gt;Use the &lt;code&gt;swanlab login&lt;/code&gt;command to complete the login.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have a project that should be incorporated, please contact via email or create a pull request.&lt;/p&gt;
    &lt;head&gt;Click to show&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Wang et al. ESRL: Efficient Sampling-based Reinforcement Learning for Sequence Generation. 2023. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yu et al. Open, Closed, or Small Language Models for Text Classification? 2023. [arxiv]&lt;/item&gt;
      &lt;item&gt;Wang et al. UbiPhysio: Support Daily Functioning, Fitness, and Rehabilitation with Action Understanding and Feedback in Natural Language. 2023. [arxiv]&lt;/item&gt;
      &lt;item&gt;Luceri et al. Leveraging Large Language Models to Detect Influence Campaigns in Social Media. 2023. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. Alleviating Hallucinations of Large Language Models through Induced Hallucinations. 2023. [arxiv]&lt;/item&gt;
      &lt;item&gt;Wang et al. Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs. KDD 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Wang et al. CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning. ACL 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Choi et al. FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Lyu et al. KnowTuning: Knowledge-aware Fine-tuning for Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yang et al. LaCo: Large Language Model Pruning via Layer Collaps. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Bhardwaj et al. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yang et al. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yi et al. Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding. ACL 2024 Findings. [arxiv]&lt;/item&gt;
      &lt;item&gt;Cao et al. Head-wise Shareable Attention for Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Kim et al. Efficient and Effective Vocabulary Expansion Towards Multilingual Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yu et al. KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models. ACL 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Huang et al. Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Duan et al. Negating Negatives: Alignment without Human Positive Samples via Distributional Dispreference Optimization. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Xie and Schwertfeger. Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Wu et al. Large Language Models are Parallel Multilingual Learners. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. EDT: Improving Large Language Models' Generation by Entropy-based Dynamic Temperature Sampling. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Weller et al. FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Hongbin Na. CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering. COLING 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zan et al. CodeS: Natural Language to Code Repository via Multi-Layer Sketch. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Liu et al. Extensive Self-Contrast Enables Feedback-Free Language Model Alignment. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Luo et al. BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Du et al. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Ma et al. Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation. ICML 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Liu et al. Dynamic Generation of Personalities with Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Shang et al. How Far Have We Gone in Stripped Binary Code Understanding Using Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Huang et al. LLMTune: Accelerate Database Knob Tuning with Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Deng et al. Text-Tuple-Table: Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Acikgoz et al. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. Small Language Models Need Strong Verifiers to Self-Correct Reasoning. ACL 2024 Findings. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhou et al. FREB-TQA: A Fine-Grained Robustness Evaluation Benchmark for Table Question Answering. NAACL 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Xu et al. Large Language Models for Cyber Security: A Systematic Literature Review. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Dammu et al. "They are uncultured": Unveiling Covert Harms and Social Threats in LLM Generated Conversations. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yi et al. A safety realignment framework via subspace-oriented model fusion for large language models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Lou et al. SPO: Multi-Dimensional Preference Sequential Alignment With Implicit Reward Modeling. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. Getting More from Less: Large Language Models are Good Spontaneous Multilingual Learners. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhang et al. TS-Align: A Teacher-Student Collaborative Framework for Scalable Iterative Finetuning of Large Language Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zihong Chen. Sentence Segmentation and Sentence Punctuation Based on XunziALLM. 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Gao et al. The Best of Both Worlds: Toward an Honest and Helpful Large Language Model. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Wang and Song. MARS: Benchmarking the Metaphysical Reasoning Abilities of Language Models with a Multi-task Evaluation Dataset. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Hu et al. Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Ge et al. Time Sensitive Knowledge Editing through Efficient Finetuning. ACL 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Tan et al. Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Song et al. Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Gu et al. RWKV-CLIP: A Robust Vision-Language Representation Learner. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Chen et al. Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhu et al. Are Large Language Models Good Statisticians?. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Li et al. Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Ding et al. IntentionQA: A Benchmark for Evaluating Purchase Intention Comprehension Abilities of Language Models in E-commerce. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;He et al. COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Lin et al. FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Treutlein et al. Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Feng et al. SS-Bench: A Benchmark for Social Story Generation and Evaluation. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Feng et al. Self-Constructed Context Decompilation with Fined-grained Alignment Enhancement. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Liu et al. Large Language Models for Cuffless Blood Pressure Measurement From Wearable Biosignals. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Iyer et al. Exploring Very Low-Resource Translation with LLMs: The University of Edinburgh's Submission to AmericasNLP 2024 Translation Task. AmericasNLP 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Li et al. Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yang et al. Financial Knowledge Large Language Model. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Lin et al. DogeRM: Equipping Reward Models with Domain Knowledge through Model Merging. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Bako et al. Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Huang et al. RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Jiang et al. LLM-Collaboration on Automatic Science Journalism for the General Audience. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Inouye et al. Applied Auto-tuning on LoRA Hyperparameters. 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Qi et al. Research on Tibetan Tourism Viewpoints information generation system based on LLM. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Xu et al. Course-Correction: Safety Alignment Using Synthetic Preferences. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Sun et al. LAMBDA: A Large Model Based Data Agent. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zhu et al. CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Yu et al. Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Xie et al. The Power of Personalized Datasets: Advancing Chinese Composition Writing for Elementary School through Targeted Model Fine-Tuning. IALP 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Liu et al. Instruct-Code-Llama: Improving Capabilities of Language Model in Competition Level Code Generation by Online Judge Feedback. ICIC 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Wang et al. Cybernetic Sentinels: Unveiling the Impact of Safety Data Selection on Model Security in Supervised Fine-Tuning. ICIC 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Xia et al. Understanding the Performance and Estimating the Cost of LLM Fine-Tuning. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Zeng et al. Perceive, Reflect, and Plan: Designing LLM Agent for Goal-Directed City Navigation without Instructions. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Xia et al. Using Pre-trained Language Model for Accurate ESG Prediction. FinNLP 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Liang et al. I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm. 2024. [arxiv]&lt;/item&gt;
      &lt;item&gt;Bai et al. Aligning Large Language Model with Direct Multi-Preference Optimization for Recommendation. CIKM 2024. [paper]&lt;/item&gt;
      &lt;item&gt;Zhang et al. CPsyCoun: A Report-based Multi-turn Dialogue Reconstruction and Evaluation Framework for Chinese Psychological Counseling. ACL 2024. [paper]&lt;/item&gt;
      &lt;item&gt;StarWhisper: A large language model for Astronomy, based on ChatGLM2-6B and Qwen-14B.&lt;/item&gt;
      &lt;item&gt;DISC-LawLLM: A large language model specialized in Chinese legal domain, based on Baichuan-13B, is capable of retrieving and reasoning on legal knowledge.&lt;/item&gt;
      &lt;item&gt;Sunsimiao: A large language model specialized in Chinese medical domain, based on Baichuan-7B and ChatGLM-6B.&lt;/item&gt;
      &lt;item&gt;CareGPT: A series of large language models for Chinese medical domain, based on LLaMA2-7B and Baichuan-13B.&lt;/item&gt;
      &lt;item&gt;MachineMindset: A series of MBTI Personality large language models, capable of giving any LLM 16 different personality types based on different datasets and training methods.&lt;/item&gt;
      &lt;item&gt;Luminia-13B-v3: A large language model specialized in generate metadata for stable diffusion. [demo]&lt;/item&gt;
      &lt;item&gt;Chinese-LLaVA-Med: A multimodal large language model specialized in Chinese medical domain, based on LLaVA-1.5-7B.&lt;/item&gt;
      &lt;item&gt;AutoRE: A document-level relation extraction system based on large language models.&lt;/item&gt;
      &lt;item&gt;NVIDIA RTX AI Toolkit: SDKs for fine-tuning LLMs on Windows PC for NVIDIA RTX.&lt;/item&gt;
      &lt;item&gt;LazyLLM: An easy and lazy way for building multi-agent LLMs applications and supports model fine-tuning via LLaMA Factory.&lt;/item&gt;
      &lt;item&gt;RAG-Retrieval: A full pipeline for RAG retrieval model fine-tuning, inference, and distillation. [blog]&lt;/item&gt;
      &lt;item&gt;360-LLaMA-Factory: A modified library that supports long sequence SFT &amp;amp; DPO using ring attention.&lt;/item&gt;
      &lt;item&gt;Sky-T1: An o1-like model fine-tuned by NovaSky AI with very small cost.&lt;/item&gt;
      &lt;item&gt;WeClone: One-stop solution for creating your digital avatar from chat logs.&lt;/item&gt;
      &lt;item&gt;EmoLLM: A project about large language models (LLMs) and mental health.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This repository is licensed under the Apache-2.0 License.&lt;/p&gt;
    &lt;p&gt;Please follow the model licenses to use the corresponding model weights: Baichuan 2 / BLOOM / ChatGLM3 / Command R / DeepSeek / Falcon / Gemma / GLM-4 / GPT-2 / Granite / Index / InternLM / Llama / Llama 2 / Llama 3 / Llama 4 / MiniCPM / Mistral/Mixtral/Pixtral / OLMo / Phi-1.5/Phi-2 / Phi-3/Phi-4 / Qwen / Skywork / StarCoder 2 / TeleChat2 / XVERSE / Yi / Yi-1.5 / Yuan 2&lt;/p&gt;
    &lt;p&gt;If this work is helpful, please kindly cite as:&lt;/p&gt;
    &lt;code&gt;@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}&lt;/code&gt;
    &lt;p&gt;This repo benefits from PEFT, TRL, QLoRA and FastChat. Thanks for their wonderful works.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45296403</guid><pubDate>Thu, 18 Sep 2025 23:48:48 +0000</pubDate></item><item><title>David Lynch LA House</title><link>https://www.wallpaper.com/design-interiors/david-lynch-house-los-angeles-for-sale</link><description>&lt;doc fingerprint="3a0002d17f174be5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tour David Lynch's house as it hits the market&lt;/head&gt;
    &lt;p&gt;David Lynch's LA estate is for sale at $15m, and the listing pictures offer a glimpse into the late filmmaker's aesthetic and creative universe&lt;/p&gt;
    &lt;p&gt;David Lynch, the visionary American filmmaker behind Twin Peaks, Blue Velvet and Mulholland Dr, passed away this January, yet his creative universe endures in objects, spaces and ideas.&lt;/p&gt;
    &lt;p&gt;Among the most striking of these relics is his larger-than-life, meticulously designed Hollywood Hills home; a cinematic setting in its own right. Perched on a sweeping 2.3-acre hillside, David Lynch’s private compound, which is now listed for $15 million by Marc Silver of The Agency, unfolds like one of his own intricately plotted storylines. A showcase of Mid-Century modern architecture, the estate was conceived with the same care and cinematic precision that defined his work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Inside David Lynch's Los Angeles estate&lt;/head&gt;
    &lt;p&gt;The property, set across five contiguous parcels, reads like a storyboard in relief: three main residences and several ancillary structures stepping down the hillside, each capturing a different note in Lynch’s creative oeuvre.&lt;/p&gt;
    &lt;p&gt;The story behind this compound started in 1987, when he acquired the pink-hued Beverly Johnson House designed in the early 1960s by Lloyd Wright, son of Frank Lloyd Wright. The home, in fact, was recognised by Historic Places LA as an exemplary work of Mid-Century Modern residential design. Then in 1991, he commissioned Eric Lloyd Wright (Lloyd Wright’s son) to add a pool and pool house, extending the Wright imprint on his property with a new generation.&lt;/p&gt;
    &lt;p&gt;Across the years, Lynch kept expanding the plotline: in 1989, he purchased an adjoining two-bedroom Brutalist house; in 1995, a studio building; and later, more pieces of land, ultimately shaping a seven-structure sanctuary with 10 bedrooms and 11 bathrooms spread over roughly 11,000 square feet. The result was a creative campus perched above the city.&lt;/p&gt;
    &lt;p&gt;At the heart of the compound lies the architectural crescendo – the approximately 2,000 square feet home where light pours through generous windows and skylights to rake across organic textures and bold geometries. The facade’s cement chevrons catch the sun; inside, simple metalwork and natural woods are drenched in material honesty that often surfaced in Lynch’s films.&lt;/p&gt;
    &lt;p&gt;Two neighbouring addresses deepen the lore: 7029 Senalda served as the home of Asymmetrical Productions, while 7035 Senalda attained near-mythic status as both the Madison residence in the movie Lost Highway and Lynch’s own studio, complete with a library, screening room and editing suite – spaces where he refined major works, including Mulholland Drive.&lt;/p&gt;
    &lt;p&gt;Receive our daily digest of inspiration, escapism and design stories from around the world direct to your inbox.&lt;/p&gt;
    &lt;p&gt;Beyond the exemplary structures, Lynch left a personal handprint, collaborating on additional buildings: a sculptural two-storey guest house and a one-bedroom retreat finished in his favoured smooth grey plaster. Outdoors the terraces, courtyards and planted walkways offer a counterpoint to the intensity of production and everyday life.&lt;/p&gt;
    &lt;p&gt;As a listing note from The Agency suggests, this is a 'creative sanctuary and architectural landmark,' with provenance unlike any other in Los Angeles. For admirers of Lynch, it reads as both home and archive: a lived-in factory of ideas, meticulously composed and, at last, ready for its next act.&lt;/p&gt;
    &lt;p&gt;Aditi Sharma is a content specialist with 14 years of experience in the design and lifestyle space. She specialises in producing content that resonates with diverse audiences, bridging global trends with local stories, and translating complex ideas into engaging, accessible narratives.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Here’s what to order (and admire) at Carbone London&lt;p&gt;New York’s favourite, and buzziest, Italian restaurant arrives in the British capital, marking the brand’s first expansion into Europe&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Griffin Frazen on conceiving the cinematic runway sets for New York label Khaite: ‘If people feel moved we’ve succeeded’&lt;p&gt;The architectural designer – who helped conceive the sets for ‘The Brutalist’ – collaborates with his wife Catherine Holstein on the scenography for her Khaite runway shows, the latest of which took place in NYFW this past weekend&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; How to travel meaningfully in an increasingly generic world&lt;p&gt;Lauren Ho explores the need for resonance, not reach, in the way we choose to make journeys of discovery&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45296638</guid><pubDate>Fri, 19 Sep 2025 00:30:14 +0000</pubDate></item><item><title>Help Us Raise $200k to Free JavaScript from Oracle</title><link>https://deno.com/blog/javascript-tm-gofundme</link><description>&lt;doc fingerprint="e7214019db839d69"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Help Us Raise $200k to Free JavaScript from Oracle&lt;/head&gt;
    &lt;p&gt;After more than 27,000 people signed our open letter to Oracle about the “JavaScript” trademark, we filed a formal Cancellation Petition with the US Patent and Trademark Office. Ten months in, we’re finally reaching the crucial discovery phase.&lt;/p&gt;
    &lt;p&gt;Deno initiated this petition since we have legal standing as a JavaScript runtime, but it’s really on behalf of all developers. If we win, “JavaScript” becomes public domain – free for all developers, conferences, book authors, and companies to use without fear of trademark threats.&lt;/p&gt;
    &lt;p&gt;We’re asking for your support through our GoFundMe campaign so we can put forward the strongest case possible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why $200k?&lt;/head&gt;
    &lt;p&gt;Because federal litigation is expensive. Discovery is the most resource-intensive stage of litigation, where evidence is collected and arguments are built.&lt;/p&gt;
    &lt;p&gt;We don’t want to cut corners – we want to make the best case possible by funding:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Professional public surveys that carry legal weight in front of the USPTO, proving that “JavaScript” is universally recognized as the name of a language, not Oracle’s brand.&lt;/item&gt;
      &lt;item&gt;Expert witnesses from academia and industry to testify on JavaScript’s history, usage, and meaning.&lt;/item&gt;
      &lt;item&gt;Depositions and records from standards bodies, browser vendors, and industry leaders showing Oracle has no role in the language’s development.&lt;/item&gt;
      &lt;item&gt;Legal filings and responses to counter Oracle’s claims at every step.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If there are leftover funds, we’ll donate them to the OpenJS to continue defending civil liberties in the digital space. None of the funds will go to Deno.&lt;/p&gt;
    &lt;head rend="h3"&gt;Oracle officially denies “JavaScript” is generic&lt;/head&gt;
    &lt;p&gt;On August 6th, 2025, Oracle for the first time addressed the validity of the trademark. Their response to our petition denies that “JavaScript” is a generic term.&lt;/p&gt;
    &lt;p&gt;If you’re a web developer, it’s self-evident that Oracle has nothing to do with JavaScript. The trademark system was never meant to let companies squat on commonly-used names and rent-seek – it was designed to protect active brands in commerce. US law makes this distinction explicit.&lt;/p&gt;
    &lt;p&gt;We urge you to read our petition and open letter to understand our arguments.&lt;/p&gt;
    &lt;p&gt;If we don’t win discovery, Oracle locks in ownership of the word “JavaScript.” This is the decisive moment.&lt;/p&gt;
    &lt;p&gt;But this case is bigger than JavaScript. It’s about whether trademark law works as written, or whether billion-dollar corporations can ignore the rule that trademarks cannot be generic or abandoned. “JavaScript” is obviously both. If Oracle wins anyway, it undermines the integrity of the whole system.&lt;/p&gt;
    &lt;p&gt;Let’s make sure the law holds. Please donate. Please share and upvote this.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45297066</guid><pubDate>Fri, 19 Sep 2025 01:40:35 +0000</pubDate></item><item><title>Playing “Minecraft” without Minecraft (2024)</title><link>https://lenowo.org/viewtopic.php?t=5</link><description>&lt;doc fingerprint="fdfff65cc5636a85"&gt;
  &lt;main&gt;
    &lt;p&gt;Ever wanted to play the worlds second most popular videogame without actually playing it?&lt;lb/&gt; Well, I will guide you through it!&lt;lb/&gt; First of all, what do I mean by 'Playing "Minecraft" without Minecraft'?&lt;lb/&gt; To put it simple, Minecraft is a Videogame developed by Mojang. The game consists of two parts, a Client and a Server.&lt;lb/&gt; Over the years, the community has written many custom cleanroom implementations of both sides, which will allow us to play "Minecraft" without executing any Mojang code.&lt;lb/&gt; Requirements:&lt;lb/&gt; We will be using the Cuberite Server (C++), ViaProxy Protocol Translator (Java) and Minosoft Client (Kotlin).&lt;lb/&gt; In order to run all of these, you will need a 64 bit computer (ideally little endian, on BE you will need to use a JIT like box64 which decreases performance) with 4 or more processor cores at over 900 MHz. A minimum of 4 GB of RAM is recommended, with some trickery like zram you might get away with less but it will cause slowdown. Ideally youd want an OpenGL capable graphics card too.&lt;lb/&gt; Installing the Server:&lt;lb/&gt; Now, first of all, we need the backend of our game. The (at the time of writing this) most accurate implementation of a 1.12.2 Minecraft server is Cuberite. It implements survival mode pretty close to the original, even having a nether and end dimension. This allows us to actually beat the game which might be a cool challenge.&lt;lb/&gt; So, how do we install this now?&lt;lb/&gt; Its pretty straight forward. Go to the Cuberite Website and download the right version for your operating system and architecture. For some platforms you may have to compile from source but doing so is rather trivial due to Cuberite using pkgconfig.&lt;lb/&gt; Now, once you have your server binary, copy it into a folder and execute it. After a while, you will have a Minecraft-compatible server running on 127.0.0.1:25565. We will be using this later so keep it open.&lt;lb/&gt; Installing the Proxy:&lt;lb/&gt; This whole adventure has one small problem: Minecraft changed how items and blocks are named between 1.12.2 and 1.13. Minosoft does not have good support for versions below 1.14. We need a translator.&lt;lb/&gt; Luckily, the Minecraft community has us covered here too!&lt;lb/&gt; With a software called ViaProxy, anyone is able to connect any client to any server!&lt;lb/&gt; To run the proxy software, download the latest Jar file from the official GitHub releases page. Make sure to pick a version matching your Java build. As we will need a Java 17 JDK install to run the Kotlin program Minosoft later on, you should ideally go with a Java 17 compatible release as to not require a second Java install.&lt;lb/&gt; Now, run the Proxy Software and configure it as follows:&lt;lb/&gt; After setting it up, hit Start. It should show a green text at the bottom with the IP you will later have to enter into Minosoft.&lt;lb/&gt; Installing the Client&lt;lb/&gt; Now that we have both server and proxy running, its time to get started with the Client. The currently best option for this is Minosoft.&lt;lb/&gt; You can download Minosoft from the GitHub Actions builds. Make sure you pick the latest build with a green marking and scroll down all the way to artifacts.&lt;lb/&gt; Now that you have the file acquired, we are finally ready for...&lt;lb/&gt; Launching the Game:&lt;lb/&gt; Run the file using your Java 17 JDK install.&lt;lb/&gt; You should be greeted by this screen:&lt;lb/&gt; In order to connect to your proxied server, you will have to create an account. Click on "No account selected", then on "Add account". For the purposes of this guide, we will be naming the player "singleplayer".&lt;lb/&gt; Now, click on the right facing triangle (play button) icon and click on "Add server".&lt;lb/&gt; A new Window will pop up. Make sure to fill it in as follows:&lt;lb/&gt; (port may vary, double check with the green text in your ViaProxy window)&lt;lb/&gt; With all of this completed now, you can finally click on your new server profile and click on Connect.&lt;lb/&gt; Congratulations! You have made it!&lt;lb/&gt; Troubleshooting:&lt;lb/&gt; If you are running into issues connecting to the Cuberite server, it might be due to the build of Cuberite defaulting to Online mode login. In that case, stop the Cuberite process, open the settings.ini file in a text editor and change Authenticate=1 into Authenticate=0. Relaunch Cuberite and attempt to connect again.&lt;/p&gt;
    &lt;head rend="h2"&gt;Playing "Minecraft" without Minecraft (free minecraft-like/compatible game)&lt;/head&gt;
    &lt;head rend="h3"&gt;Playing "Minecraft" without Minecraft (free minecraft-like/compatible game)&lt;/head&gt;
    &lt;p&gt;~-~-~ MSD - Making your old devices useful again since 2022! ~-~-~&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt; Velimir_Dev&lt;/item&gt;
      &lt;item rend="dd-1"&gt;Posts: 1&lt;/item&gt;
      &lt;item rend="dd-2"&gt;Joined: 2025-03-24&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Re: Playing "Minecraft" without Minecraft (free minecraft-like/compatible game)&lt;/head&gt;
    &lt;p&gt;Ah, 1.12.2 , one of my favourite versions, so nostalgic&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45297258</guid><pubDate>Fri, 19 Sep 2025 02:13:05 +0000</pubDate></item><item><title>Gemini in Chrome</title><link>https://gemini.google/overview/gemini-in-chrome/</link><description>&lt;doc fingerprint="f852a98b3bb793b5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Meet Gemini in Chrome&lt;/head&gt;
    &lt;head rend="h2"&gt;AI assistance, right in your browser.&lt;/head&gt;
    &lt;head rend="h2"&gt;Intelligence that works with you, right where you are.&lt;/head&gt;
    &lt;p&gt;Get key takeaways, clarify concepts, and find answers based on the context of your open tabs.&lt;/p&gt;
    &lt;p&gt;Need to quickly grasp the essentials? Gemini delivers concise summaries of articles, pages, or threads directly in your browser, so you quickly grasp the main points.&lt;/p&gt;
    &lt;p&gt;Have a question about what you're reading? Ask Gemini. It uses the context of your open tabs to provide relevant answers and explanations, keeping you focused.&lt;/p&gt;
    &lt;p&gt;Go beyond simple explanations. When you’re tackling dense topics or new concepts, ask Gemini to not only clarify confusing parts but also help you actively engage with the material.&lt;/p&gt;
    &lt;p&gt;Researching products or weighing choices? Ask Gemini to pull key information, specs, or pros and cons from the page, helping you make informed choices with clarity and ease.&lt;/p&gt;
    &lt;p&gt;Want to brainstorm, organize thoughts, or dive deeper into a topic? Chat naturally with Gemini Live and get spoken answers, all within Chrome.&lt;/p&gt;
    &lt;p&gt;Just like on your computer, Gemini on mobile is there to answer questions about what you’re reading. On Android it works with anything on your screen—including Chrome. And coming soon to iOS, Gemini will be built right into the Chrome app.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your web, your control&lt;/head&gt;
    &lt;p&gt;Gemini in Chrome works with you, on your terms. It assists only when you ask, putting you in control.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready when you are&lt;/head&gt;
    &lt;p&gt;Gemini in Chrome activates only when you choose to use it via clicking on the Gemini icon or the keyboard shortcut you set up. It assists on your terms, stepping in only when you ask.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get help, your way&lt;/head&gt;
    &lt;p&gt;Get help your way with Gemini in Chrome. Talk or type your question naturally, and Gemini can use the page's content to help you quickly understand content or complete tedious tasks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Manage your activity easily&lt;/head&gt;
    &lt;p&gt;You can access your Gemini Apps Activity anytime to manage, delete and turn off your activity.&lt;/p&gt;
    &lt;head rend="h2"&gt;The web, reimagined.&lt;/head&gt;
    &lt;p&gt;With Gemini in Chrome, no tab switching is needed with AI assistance right in your browser, helping you quickly understand content or get tasks done using the context of your open tabs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;
    &lt;p&gt;With the Gemini in Chrome feature, you can get AI assistance from your browser to do things easily like get key takeaways, clarify concepts, find answers and more. To provide the most relevant responses, Gemini in Chrome uses the context of your open tabs.&lt;/p&gt;
    &lt;p&gt;Gemini in Chrome is part of the Chrome browser on desktop, and is different from visiting Gemini in any browser at gemini.google.com or starting a chat with the Gemini web app by typing @gemini in the address bar in Chrome. You can use the Gemini web app in other browsers (or the content area of Chrome), but you won’t be able to share page content or use Live mode like you can with Gemini in Chrome.&lt;/p&gt;
    &lt;p&gt;You can access Gemini in Chrome through the Gemini icon in the Chrome toolbar or via a keyboard shortcut that you set up on a Windows or Mac desktop.&lt;/p&gt;
    &lt;p&gt;You can also activate Gemini when using Chrome on Android, and other apps, by holding the power button. And starting soon, on iOS Gemini in Chrome will be built into the app, with access through the Chrome omnibox.&lt;/p&gt;
    &lt;p&gt;Gemini in Chrome is rolling out to all eligible Mac and Windows users in the US who have their Chrome language set to English. We look forward to bringing this feature to more people and additional languages soon.&lt;/p&gt;
    &lt;p&gt;Gemini in Chrome on iOS is coming soon to eligible iPhone users in the US who have their Chrome language set to English.&lt;/p&gt;
    &lt;p&gt;Check responses. Setup required. Compatibility and availability varies. 18+&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45297331</guid><pubDate>Fri, 19 Sep 2025 02:25:34 +0000</pubDate></item><item><title>Count Folke Bernadotte: Sweden's Servant of Peace (2010)</title><link>https://www.historytoday.com/archive/feature/count-folke-bernadotte-swedens-servant-peace</link><description>&lt;doc fingerprint="b2e99a41b323c3ba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Count Folke Bernadotte: Sweden’s Servant of Peace&lt;/head&gt;
    &lt;p&gt;Folke Bernadotte was a great humanitarian who navigated the perilous path between warring parties, a mission that was to cost him his life.&lt;/p&gt;
    &lt;p&gt;On 17 September 1948 Count Folke Bernadotte, the first of many United Nations mediators assigned the thankless task of attempting to find a peaceful solution to the seemingly intractable problem of hostility in the Middle East, was assassinated in Jerusalem. He had arrived in the city on a mission to establish a ceasefire between the Arabs and the newly proclaimed state of Israel. His killers were members of the extremist Jewish group Lehi – often called ‘the Stern Gang’ – who had already murdered several British officials and soldiers administering the mandated territory of Palestine before the birth of Israel.&lt;/p&gt;
    &lt;p&gt;The hopelessness of Bernadotte’s peacemaking efforts was underlined by the fact that his murder was the second attempt on his life that day, the first being carried out by the ‘other side’ in the conflict: a young Arab had shot at his car as he drove from the airport, the bullet passing harmlessly through the vehicle’s bumper. The Stern Gang’s ambush was typically more lethal and efficient. An apparently broken-down jeep was slewed across the road along which Bernadotte’s convoy was travelling, forcing it to halt. Three gunmen emerged, dressed in the army uniforms of the new Jewish state’s defence forces. Two shot out the tyres of Bernadotte’s car, while the third, later identified as Yehoshua Cohen (1922-86), poked a machine gun through the car window and opened fire, killing Bernadotte and his French military aide, André Serot. No one was ever brought to trial for the killing, though after Cohen’s death, two fellow members of the gang admitted responsibility. One of the chief organisers of the assassination was Yitzhak Shamir, who became Israel’s prime minister in 1984.&lt;/p&gt;
    &lt;p&gt;The death of Bernadotte at the hands of Jewish gunmen was a supreme irony given the Swedish statesman’s leading role in one of the most remarkable humanitarian achievements of the Second World War: the rescue of more than 17,000 people, including many Jews, from Nazi concentration camps in the last days of the war. This Scarlet Pimpernel operation, known as the ‘White Buses’, after the whitewashed Red Cross vehicles used to snatch the captives to safety in neutral Sweden, was carried out with the permission of SS overlord Heinrich Himmler. He was desperately seeking to build up credit with the Allies as a man of mercy and possible future leader of a post-Hitler Germany, rather than the mass-murdering master of terror that he was.&lt;/p&gt;
    &lt;p&gt;Born in 1895, Count Bernadotte was a member of Sweden’s royal family, the nephew of King Gustav V. He received a military education thought suitable for a descendant of the first Bernadotte to sit on Sweden’s throne – Napoleon’s Marshal Jean-Baptiste – and became active in Swedish voluntary organisations, especially the Red Cross, of which he became vice-chairman. As a neutral ‘Nordic’ nation with close historic ties to both Germany and the western Allies and a humanitarian reputation, Sweden was the focus throughout the war of peace ‘feelers’ put out by both sides. In 1943 Bernadotte organised his first humanitarian mission: exchanging disabled prisoners of war through the Swedish port of Gothenburg.&lt;/p&gt;
    &lt;p&gt;In March 1945, with the end of the war in Europe weeks away, Bernadotte led his first Red Cross rescue expedition into the Third Reich itself. The original convoy, staffed by Swedish army officers with Red Cross doctors and nurses, plucked some 8,000 Norwegian and Danish prisoners, both their countries having been occupied by the Germans, from Nazi hands. Gradually, Bernadotte expanded the scope of the White Buses. Later convoys rescued 6,000 Poles, two thirds of whom were Jews bound for the gas chambers, including a large contingent liberated from Ravensbrück, the concentration camp for women located near Berlin. Other camps visited by the White Buses included Dachau, Mauthausen, Theresienstadt and Flossenbürg. The convoys, painted bright white and emblazoned with huge red crosses to deter the Allied planes then ravaging Germany, made their perilous journeys bearing their often sick and starving human cargoes to freedom in Sweden.&lt;/p&gt;
    &lt;p&gt;Naturally, Himmler had not authorised this mass rescue from the kindness of his flinty heart. His aim was to use the prisoners as human bargaining chips with the Allied leaders Churchill and Roosevelt and, after the latter’s death, with the new US President Truman. In head-to-head negotiations, sometimes carried out by candlelight in tents in the grounds of Friedrichsruh Castle, the Swedish Red Cross’s HQ in Germany, Himmler attempted to use Bernadotte as an envoy to carry his ‘peace offer’ to the Allied leaders. In essence, he was proposing that Germany should surrender to the Western Allies – with the proviso that it should continue fighting a ‘defensive’ war against the Russians.&lt;/p&gt;
    &lt;p&gt;Bernadotte was in a dilemma. He knew that the Allies, publicly committed to their alliance with the Soviet Union and sworn to fighting Germany until she surrendered unconditionally, would reject such an offer out of hand – especially coming from Himmler. But Bernadotte also realised that if Himmler knew how he was regarded in the West the camp gates would slam shut, the White Buses would stop running and the inmates would be left to their fate. So Bernadotte strung Himmler along, promising to convey his olive branch to the West, while his White Buses continued their life-saving work. They did so up to and after the Allied liberation of the concentration camps, the German surrender and Himmler’s suicide following his capture by the British.&lt;/p&gt;
    &lt;p&gt;After Bernadotte’s murder, his reputation was attacked by Felix Kersten, Himmler’s personal masseur. Kersten had conceived a grudge against Bernadotte who, rightly regarding the masseur as a Nazi, had blocked his application for Swedish citizenship. Kersten’s revenge had been to forge a letter from Bernadotte to Himmler on his own typewriter in which the Swede allegedly informed the SS chief that Jews freed from the camps would be unwelcome in Sweden. Sadly, this forgery was swallowed by the British historian Hugh Trevor-Roper – not the last time the historian would be fooled by a Nazi fake – who used it in a 1953 magazine article to blacken Bernadotte’s reputation.&lt;/p&gt;
    &lt;p&gt;Now, a Swedish historian, Sune Persson of Gothenburg University, claims to have established the truth about Bernadotte and his heroic humanitarian efforts. Persson has tracked down survivors, comparing and evaluating their accounts of a chaotic and traumatic time. Sir Brian Urquhart, a Second World War intelligence veteran and subsequently UN under-secretary, describes Persson’s work, in his introduction to the English edition of the book, Escape from the Third Reich (Frontline Books), ‘as a major contribution to the history of the Second World War and particularly to the involvement of the Scandinavian countries’.&lt;/p&gt;
    &lt;p&gt;Persson’s investigations, Urquhart adds, have vindicated Bernadotte’s role in a great humanitarian enterprise and his conclusions, ‘backed up by years of tireless research and study’, will lay to rest the calumnies thrown at Bernadotte by those seeking to justify his murder. As a result, his own country, after years of silence, is at last honouring Bernadotte’s achievements. In the words of his surviving sons, Count Folke junior and Count Bertil Bernadotte:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‘If these criticisms had been made during his lifetime, it is certain that he himself would have explained the circumstances … but this opportunity was denied him by an assassin’s bullet.’&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45298489</guid><pubDate>Fri, 19 Sep 2025 06:21:04 +0000</pubDate></item><item><title>Ruby Central's Attack on RubyGems [pdf]</title><link>https://pup-e.com/goodbye-rubygems.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45299170</guid><pubDate>Fri, 19 Sep 2025 08:09:17 +0000</pubDate></item><item><title>Statistical Physics with R: Ising Model with Monte Carlo</title><link>https://github.com/msuzen/isingLenzMC</link><description>&lt;doc fingerprint="d9f7ce022b3b170f"&gt;
  &lt;main&gt;
    &lt;p&gt;Classical Ising Model is a land mark system in statistical physics. The model explains the physics of spin glasses and magnetic materials, and cooperative phenomenon in general, for example phase transitions and neural networks. This package provides utilities to simulate one dimensional Ising Model with Metropolis and Glauber Monte Carlo with single flip dynamics in periodic boundary conditions. Utility functions for exact solutions are provided.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Effective ergodicity in single-spin-flip dynamics&lt;lb/&gt;Mehmet Suezen, Phys. Rev. E 90, 032141&lt;lb/&gt;Dataset&lt;/item&gt;
      &lt;item&gt;Anomalous diffusion in convergence to effective ergodicity, Suezen, Mehmet, arXiv:1606.08693&lt;lb/&gt;Dataset&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45299625</guid><pubDate>Fri, 19 Sep 2025 09:19:04 +0000</pubDate></item><item><title>Dynamo AI (YC W22) Is Hiring a Senior Kubernetes Engineer</title><link>https://www.ycombinator.com/companies/dynamo-ai/jobs/fU1oC9q-senior-kubernetes-engineer</link><description>&lt;doc fingerprint="a770c0f34e35ef6a"&gt;
  &lt;main&gt;
    &lt;p&gt;Compliant-Ready AI for the Enterprise&lt;/p&gt;
    &lt;p&gt;Dynamo AI is building the future of secure, scalable AI systems. Our platform helps enterprises safely deploy powerful AI models in production, with reliability, control, and trust at the core. We’re a team of builders working at the intersection of machine learning, infrastructure, and security.&lt;/p&gt;
    &lt;p&gt;As a Senior Kubernetes Engineer, you’ll lead the full onboarding journey for our enterprise customers — from first engagement to successful production rollout. You will own the deployment of Dynamo AI clusters (Kubernetes-based) into customer environments and serve as the technical bridge between our product and the customer’s infrastructure.&lt;/p&gt;
    &lt;p&gt;This is a deeply hands-on and customer-facing role. You’ll work with Kubernetes, Helm, and cloud-native tools to deliver secure, scalable deployments of cutting-edge AI systems. You’ll partner with engineering, product, and leadership to bring customer feedback directly into our roadmap and shape how AI is adopted across industries. Serving this role, you will grow into an expert in building the most cutting-edge enterprise-level AI systems.&lt;/p&gt;
    &lt;p&gt;This role will work with the U.S. government clients, so it requires U.S. government security clearance or US citizenship. Our company policy also requires in-office presence in San Francisco or New York office for 2-3 days per week.&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;The enterprise platform for enabling private, secure, and regulation-compliant Gen AI models.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45300615</guid><pubDate>Fri, 19 Sep 2025 12:00:16 +0000</pubDate></item></channel></rss>