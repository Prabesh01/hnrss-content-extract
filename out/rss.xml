<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 27 Oct 2025 13:47:00 +0000</lastBuildDate><item><title>Asbestosis</title><link>https://diamondgeezer.blogspot.com/2025/10/asbestosis.html</link><description>&lt;doc fingerprint="fe60955783648a74"&gt;
  &lt;main&gt;
    &lt;p&gt;This monument popped up in the middle of Barking recently. I thought it was very recently but it was actually unveiled in April 2022 and I'm just not very observant.&lt;/p&gt;
    &lt;p&gt;It says "In Memory of those who lost their lives because of exposure to asbestos".&lt;/p&gt;
    &lt;p&gt;And it's here because Barking has one of the highest rates of asbestos-related deaths in the country.&lt;/p&gt;
    &lt;p&gt;In 1913 the Cape Asbestos Company built a huge asbestos factory beside the River Roding in Barking. The company mined asbestos-bearing rock at several sites in South Africa, then shipped them in sacks to a private quay in Barking for processing. Hundreds of people were employed to mill the ore into usable fibres and then process these into lagging, packaging, pipes, resins, boards and all forms of insulation widely used in the building trade. They worked without masks or other protection, the dangers of asbestos either unknown or not thought worth bothering about. And hundreds of workers died, often many years later, of insidious chronic respiratory disease.&lt;/p&gt;
    &lt;p&gt;I found a 32-page booklet published by Cape Asbestos in the days before blue asbestos was recognised as dangerous and banned, which was as late as 1985. It shows workers with rolled-up sleeves and women leaning over unshielded machines, all potentially inhaling enough fibres to ultimately kill them. I read reports about the local school in Barking, barely 100 metres away, saying that the playground was often covered in fine dust which children rolled up and played with as if it were snow. I read that mesothelioma was so common in the area it was known as the ‘Barking Cough’. These were different times, but times that linger on.&lt;/p&gt;
    &lt;p&gt;Cape Asbestos's plant eventually closed in 1968 and in its place was built the Harts Lane council estate, which is still not the loveliest corner of Barking. It included two tall tower blocks called Colne House and Mersey House, both of which Barking &amp;amp; Dagenham council would now like to demolish. This is chiefly because they're old and covered in combustible cladding, but the additional complications of potentially disturbing polluted land puts any remediation out of financial reach. It's always the insulation you have to watch out for.&lt;/p&gt;
    &lt;p&gt;The memorial in Barking Town Square comprises a polished chunk of blue pearl granite and was unveiled on Workers' Memorial Day 2022 in a ceremony attended by several trade unionists and representatives of the London Asbestos Support Awareness Group. The emphasis is partly on remembrance and partly on the importance of standing up for workers' rights to make conditions better for all. As the inscription says, "Remember the Dead and Fight for the Living".&lt;/p&gt;
    &lt;p&gt;My grandfather worked for another Cape Asbestos plant on Tolpits Lane in Watford. Originally it had been run by Universal Asbestos Manufacturing but in 1967 the factory was acquired by Cape as part of a diversification into cement-based products. They made corrugated roofing, flat sheets, decorated sheets, slates, soil pipes, decking for flat roofs and reinforced troughing - that kind of thing - the asbestos moulded into a multiplicity of shapes for the benefit of the building trade.&lt;/p&gt;
    &lt;p&gt;To him Cape Universal was just a convenient place to work, a short walk across the moor for a day's shift and then home again for tea. He worked there for many years, from the 1930s to the 1960s, rising through the ranks from a labourer to a machine operator on the factory floor. On his death certificate his occupation was listed as 'Asbestos Moulder', and it was very much a premature death because this didn't end well.&lt;/p&gt;
    &lt;p&gt;I don't remember very much about my grandfather because he died when I was 8. I know he was there when I took my first steps in his back garden and I can remember sitting at his dining room table and hoping nobody would force me to eat the celery. My final memory is being led up to his bedroom, I suspect not long before his death, to see an ill old man laid out in bed and struggling to breathe. I don't know what was said, nor how short a time I stayed in his presence, indeed my strongest recollection is of the room itself with its austere cupboards and the curtains drawn. And then at the age of 67 he was gone.&lt;/p&gt;
    &lt;p&gt;My family fought for asbestosis to be recognised as his cause of death but were not successful. I've read recently of fellow workers working at the Tolpits Lane factory now getting six figure payouts in compensation, indeed it's hard to research this topic without ending up on legal websites with popups urging you to make a claim. Even four decades after the factory's closure there are still employees severely affected, and many more already passed, as the toxic legacy endures. The factory site is now a rather cleaner industrial estate and business park, indeed it's where the National Lottery's been based for the last 30 years because risk and loss are still in play.&lt;/p&gt;
    &lt;p&gt;Today my Dad reaches the grand old age of 87, a full 20 years more than his father lived. Science has moved on a long way since the 1970s, also educational opportunities and also workers' rights. Health and safety is sometimes much derided but it can genuinely save lives, even much extend them, rather than everyone continually moaning about additional costs and annoying procedures. If someone had shouted earlier and louder about the dangers of asbestos I might have known my grandfather better, my grandmother could have had many more years of married life and my father could have had a father for much longer.&lt;/p&gt;
    &lt;p&gt;My Dad lost his Dad at the age of 34, which is no age at all in the grand scheme of things. By contrast I still have my Dad at the age of 60, which has meant an extra quarter century of guidance, support, advice, love and always being there. How lucky am I? Every day we overlap with our parents is a blessing and I've had 22,000 of them, for all of which I'm truly grateful. We're off out later to celebrate with a slap-up dinner, or as slap-up as an 87-year-old stomach requires, which the wider family are greatly looking forward to. What Barking's memorial reminded me is that many families have not been so fortunate, and sometimes that loss can be very close to home.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45710065</guid><pubDate>Sun, 26 Oct 2025 08:34:38 +0000</pubDate></item><item><title>You already have a Git server</title><link>https://maurycyz.com/misc/easy_git/</link><description>&lt;doc fingerprint="dcf8f5f9c827be83"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;You already have a git server:&lt;/head&gt;(Programming)&lt;p&gt;If you have a git repository on a server with ssh access, you can just clone it:&lt;/p&gt;&lt;code&gt;# This works. 
git clone ssh://username@hostname/path/to/repo
&lt;/code&gt;&lt;p&gt;You can then work on it locally and push your changes back to the origin server. By default, git won’t let you push to the branch that is currently checked out, but this is easy to change:&lt;/p&gt;&lt;code&gt;# Run this on the remote server. 
git config receive.denyCurrentBranch updateInstead
&lt;/code&gt;&lt;p&gt;This is a great way to sync code between multiple computers or to work on server-side files without laggy typing or manual copying. If you want to publish your code, just point your web server at the git repo:&lt;/p&gt;&lt;code&gt;git clone https://hostname/path/to/repo/.git
# You can get rid of the .git part of the command by either setting the
# server to remap it to a nicer URL or by just renaming the .git directory
# (although this stops you from running git server side)
&lt;/code&gt;&lt;p&gt;… although you will have to run this command server-side to make it cloneable:&lt;/p&gt;&lt;code&gt;# Create some files used by git-over-http:
# Should be repeated after making changes.
git update-server-info
&lt;/code&gt;&lt;p&gt;That’s a lot of work, so let’s set up a hook to do that automatically:&lt;/p&gt;&lt;code&gt;# Automatically run git update-server-info.
# Should be run server-side
cp .git/hooks/post-update.sample .git/hooks/post-update
chmod a+x .git/hooks/post-update
&lt;/code&gt;&lt;p&gt;Git hooks are just shell scripts, so they can do things like running a static site generator:&lt;/p&gt;&lt;code&gt;cat &amp;gt; .git/hooks/post-update &amp;lt;&amp;lt;EOF
#!/bin/sh
set -euo pipefail
cd /path/to/site
/path/to/generator
EOF
chmod a+x .git/hooks/post-update
&lt;/code&gt;&lt;p&gt;This is how I’ve been doing this blog for a while now: It’s very nice to be able to type up posts locally (no network lag), and then push them to the server and have the rest handled automatically.&lt;/p&gt;&lt;p&gt;It’s also backed up by default: If the server breaks, I’ve still got the copy on my laptop, and if my laptop breaks, I can download everything from the server. Git’s version tracking also prevents accidental deletions, and if something breaks, it’s easy to figure out what caused it.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45710721</guid><pubDate>Sun, 26 Oct 2025 10:53:37 +0000</pubDate></item><item><title>Feed the bots</title><link>https://maurycyz.com/misc/the_cost_of_trash/</link><description>&lt;doc fingerprint="273b981161f213a7"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;You should feed the bots:&lt;/head&gt;(Programming)&lt;p&gt;A week ago, I set up an infinite nonsense crawler trap – now it makes up 99% of my server’s traffic. What surprised me is that feeding scrapers garbage is the cheapest and easiest thing I could do.&lt;/p&gt;&lt;head rend="h2"&gt;Meet the bots:&lt;/head&gt;&lt;p&gt;These aren’t the indexing bots of old, but scrapers collecting data to train LLMs. Unlike search engines, which need the websites they crawl to stay up, AI companies provide a replacement.&lt;/p&gt;&lt;p&gt;It should come as no surprise that these bots are aggressive and relentless: They ignore robots.txt, and if block them by user agent they just pretend to be a browser. If you ban their IP, they switch addresses.&lt;/p&gt;&lt;p&gt;… all while sending multiple requests per second, all day, every day.&lt;/p&gt;&lt;head rend="h2"&gt;Giving up:&lt;/head&gt;&lt;p&gt;So what if we let them access the site?&lt;/p&gt;&lt;p&gt;Serving static files is is relatively cheap, but not free. SSD access times are in the tens milliseconds, and that’s before you pay the filesystem tax. Bots also like to grab old and obscure pages, ones that are unlikely to be in cache. As a result, it doesn’t take all that many requests to bog down the server.&lt;/p&gt;&lt;p&gt;Then there’s the matter of bandwidth: Many blog posts also include images weighing hundreds to thousands of kB, which can add up quite quickly. With an average file size of 100 kB, 4 requests per second adds up to a terabyte each month – not a huge amount of data, but more then I’m willing to throw away.&lt;/p&gt;&lt;head rend="h2"&gt;The ban hammer:&lt;/head&gt;&lt;p&gt;Simply making a list of IPs and blocking them would for normal bots…&lt;/p&gt;&lt;p&gt;… but these are hardly normal bots. Because they are backed by billion dollar companies, they don’t just have a few addresses, but many thousands. If you managed to ban all of their addresses, they’ll just buy more.&lt;/p&gt;&lt;p&gt;Rate limits fail for the same reason: They just switch IPs. I’ve even seen them using new IP for each request.&lt;/p&gt;&lt;head rend="h2"&gt;Building a wall:&lt;/head&gt;&lt;p&gt;Ok, what about a pay-wall, login-wall, CAPTCHA-wall, or a hash based proof-of-work?&lt;/p&gt;&lt;p&gt;All of these inconvenience users. Requiring an account guaranties that no one will read what I wrote. Even just a simple JavaScript challenge will block anyone who’s browser doesn’t support JS … and when it works, anything that must load before the does content still hugely slows down page loads.&lt;/p&gt;&lt;head rend="h2"&gt;Throw them some bombs:&lt;/head&gt;&lt;p&gt;“Serve them few gzip bombs, that’ll teach them” — Half the internet.&lt;/p&gt;&lt;p&gt;Gzip only provides a compression ratio of a little over 1000: If I want a file that expands to 100 GB, I’ve got to serve a 100 MB asset. Worse, when I tried it, the bots just shrugged it off, with some even coming back for more.&lt;/p&gt;&lt;head rend="h2"&gt;Jedi mind tricks:&lt;/head&gt;&lt;p&gt;Ok, what if we just send them 404s – try and make them think my site doesn’t exist.&lt;/p&gt;&lt;p&gt;These tricks only work if your adversary has a mind to trick. If a link is posted somewhere, the bots will know it exists, and if they can’t access it, they’ll just become more aggressive:. sending more requests, with more user agents and using more addresses.&lt;/p&gt;&lt;p&gt;Keeping them happy keeps them tolerable.&lt;/p&gt;&lt;head rend="h2"&gt;Garbage:&lt;/head&gt;&lt;p&gt;But surely sending them dynamically generated content would be expensive right?&lt;/p&gt;&lt;p&gt;Well… no.&lt;/p&gt;&lt;p&gt;CPU and RAM are the fastest parts of a modern computer. Dynamic content has the reputation of being slow because it often involves a database (lots of disk IO), a million lines of JavaScript, or both.&lt;/p&gt;&lt;p&gt;My lightly optimized Markov babbler consumes around ~60 CPU microseconds per request. There’s no disk IO, and the memory cost is only around 1.2 MB. There’s also no rules or blacklists to maintain: the bots come to it and it consumes them.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45711094</guid><pubDate>Sun, 26 Oct 2025 12:09:02 +0000</pubDate></item><item><title>Ken Thompson recalls Unix's rowdy, lock-picking origins</title><link>https://thenewstack.io/ken-thompson-recalls-unixs-rowdy-lock-picking-origins/</link><description>&lt;doc fingerprint="3a3a188cfbb6805d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ken Thompson Recalls Unix’s Rowdy, Lock-Picking Origins&lt;/head&gt;
    &lt;p&gt;The 82-year-old Ken Thompson has some amazing memories about the earliest days of the Unix operating system — and the rowdy room full of geeks who built it.&lt;/p&gt;
    &lt;p&gt;This month Silicon Valley’s Computer History Museum released a special four-and-a-half-hour oral history, in partnership with the Association for Computing Machinery, recorded 18 months ago by technology historian David C. Brock. And Thompson dutifully recalled many of his career highlights — from his work on the C programming language and Unix to the “Plan 9 from Bell Labs” operating system and the Go programming language.&lt;/p&gt;
    &lt;p&gt;But what comes through is his gratefulness for the people he’d worked with, and the opportunity they’d had to all experiment together in an open environment to explore the limits of new and emerging technologies. It’s a tale of curiosity, a playful sense of serendipity and the enduring value of a community.&lt;/p&gt;
    &lt;p&gt;And along the way, Thompson also tells the story of raising a baby alligator that a friend sent to his office at Bell Labs. (“It just showed up in the mail… They’re not the sweetest of pets.”)&lt;/p&gt;
    &lt;head rend="h2"&gt;The Accidental Birth of Unix&lt;/head&gt;
    &lt;p&gt;Travel back in time to 1966, when 23-year-old Thompson’s first project at Bell Labs was the ill-fated Multics, a collaboration with MIT and General Electric which Thompson remembers as “horrible… big and slow and ugly and very expensive,” requiring a giant specially-built computer just to run and “just destined to be dead before it started.”&lt;/p&gt;
    &lt;p&gt;But when the Multics project died, “the computer became completely available — this one-of-a-kind monster computer… and so I took advantage.”&lt;/p&gt;
    &lt;p&gt;Thompson had wanted to work with CRAM, a data storage device with a high-speed drum memory, but like disk storage of the time, it was slow to read from memory.&lt;/p&gt;
    &lt;p&gt;Thompson thought he’d improve the situation with simultaneous (and overlapping) memory reads, but of course this required programs for testing, plus a way to load and run them.&lt;/p&gt;
    &lt;p&gt;“And suddenly, without knowing it — I mean, this is sneaking up on me…. Suddenly it’s an operating system!” Thompson’s initial memory-reading work became “the disk part” for Unix’s filesystem. He still needed a text editor and a user-switching multiplexing layer (plus a compiler and an assembler for programs), but it already had a filesystem, a disk driver and I/O peripherals.&lt;/p&gt;
    &lt;p&gt;Thompson wondered if it took so long to recognize its potential because he’d been specifically told not to work on operating systems. Multics “was a bad experience” for Bell Labs, he’d been told. “We spent a ton of money on it, and we got nothing out of it!”&lt;/p&gt;
    &lt;p&gt;“I actually got reprimands saying, ‘Don’t work on operating systems. Bell Labs is out of operating systems!”&lt;/p&gt;
    &lt;head rend="h2"&gt;One-Digit User IDs&lt;/head&gt;
    &lt;p&gt;But now Unix had its first user community — future legends like Dennis Ritchie, Doug McIlroy, Robert Morris and occasionally Brian Kernighan. (“All the user IDs were one digit. That definitely put a limit on it.”) Thompson remembers designing the Unix filesystem on a blackboard in an office with Rudd Canaday — using a special Bell Labs phone number that took dictation and delivered a typed-up transcript the next day. And Joe Ossanna “got things done” with a special talent for navigating Bell Labs’ bureaucracy that ultimately procured a crucial PDP-11 for the Unix team to work on.&lt;/p&gt;
    &lt;p&gt;“We were being told no, ‘because we don’t deal in operating systems.'” But Ossanna knew the patent department was evaluating a third-party system for preparing documents — and Ossanna proposed an in-house alternative. “So we got our first PDP-11 to do word processing.”&lt;/p&gt;
    &lt;p&gt;And history shows that it happened partly because the department paying for it “had extra money, and if they didn’t spend it, they’d lose it the next year…”&lt;/p&gt;
    &lt;p&gt;So the young Unix community picked up somewhere between five and eight new users, Thompson remembers, “the secretaries for the Patent Department, writing patents on our system!”&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fellowship of the Unix Room&lt;/head&gt;
    &lt;p&gt;That PDP-11 wound up in “a spot on the sixth floor where we cleaned out a vending machine and a couple of cages of stored junk from 1920,” Thompson remembered. They eventually installed a second PDP-11, which turned the room into “a hotbed of things,” with discussions about networking — and an upcoming typesetter for documents. Thompson calls it the Unix room, and most of them eventually had extensions for their phones wired into the room. (It even had its own call-switching PBX …)&lt;/p&gt;
    &lt;p&gt;There was camaraderie and some laughter. He adds later, almost as an aside, that “in the Unix room, we used to pick locks a lot and steal things.” (When one of the secretaries discovered security had affixed a “parking boot” to her car that was parked in the wrong zone, “we went down there, and we picked the lock and stole the boot. And after that, slowly, we picked up all four boots, and we hid them under the raised floor of the Unix room…”)&lt;/p&gt;
    &lt;p&gt;The punchline? “The head of security came around and pleaded with us. ‘We won’t pick on your secretaries if you give us back our boots.'”&lt;/p&gt;
    &lt;p&gt;And the deal was accepted.&lt;/p&gt;
    &lt;p&gt;Thompson remembers things like gathering for a regular “Unix lunch” in the Bell Labs lunchroom, which “caused a symbiosis of thought and things. It was great.” Although it always seemed to happen just minutes after the lunchroom stopped serving food. “If I was late, I’d buy McDonald’s and sit down at the lunchroom with my McDonald’s. They used to get mad at me for that …”&lt;/p&gt;
    &lt;head rend="h2"&gt;Growing From Community&lt;/head&gt;
    &lt;p&gt;Looking back, Thompson credited the success of C and Unix to Bell Labs and its no-pressure/no users environment. “It was essentially a ‘whatever you want to do’ atmosphere, and ‘for anybody you wanted to do it for’… Bell Labs was by far the biggest contributor to this whole type of programming.”&lt;/p&gt;
    &lt;p&gt;Bell Labs was an eclectic mix, but this community paid unexpected dividends. While Lee McMahon was originally hired as a linguistics researcher, he was ultimately the one who procured machine-readable dictionaries for the Unix team, along with machine-readable version of the Federalist Papers. (When the whole text wouldn’t fit into their text editor ed, Thompson famously created the line-by-line pattern-scanning tool grep.)&lt;/p&gt;
    &lt;p&gt;And in the end Thompson says Unix grew from there for one simple fact: People liked it. It spread within Bell Labs, at first for “the administrative kind of stuff, typing in trouble tickets…” But this being a phone company, “then it started actually doing some switching, and stuff like that. It was getting deeper and deeper into the guts of the Bell System and becoming very popular.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Before Open Source&lt;/head&gt;
    &lt;p&gt;Thompson credits Richard Stallman with developing much more of the open source philosophy. “But Unix had a bit of that.” Maybe it grew out of what Dennis Ritchie was remembering, that fellowship that formed around Unix. “For some reason, and I think it’s just because of me and Dennis, everything was open…”&lt;/p&gt;
    &lt;p&gt;It was just the way they operated. “We had protection on files — if you didn’t want somebody to read it, you could set some bits and then nobody could read them, right? But nobody set those permissions on anything … All of the source was writable, by anybody! It was just open …&lt;/p&gt;
    &lt;p&gt;“If you had an idea for an editor, you’d pull the editor out and you’d write on it and put it back … There was a mantra going around that, ‘You touch it, you own it.'”&lt;/p&gt;
    &lt;p&gt;Thompson provides an example: Bell Labs co-worker P. J. Plauger, with whom he later wrote the 1974 book “Elements of Programming Style.” Plauger was also a professional science fiction writer, Thompson remembers, “And whatever he was writing on was in his directory, right? So, we’d all go in there and be reading it as he’s writing it … and we’d all write back, ‘You ought to kill this guy, and move him over here and turn him green!’ or something.&lt;/p&gt;
    &lt;p&gt;“And he didn’t mind it, because that’s just the theory of Unix in those days …&lt;/p&gt;
    &lt;p&gt;“I think that generated a fellowship. Just the fact that it was like writing on a blackboard — everybody read it.”&lt;/p&gt;
    &lt;p&gt;And more of their Bell Labs experiments found their way into the world when some work on the later Plan 9 operating system found its way into the UTF-8 standard, which underlies most of today’s web connections.&lt;/p&gt;
    &lt;head rend="h2"&gt;After Bell Labs&lt;/head&gt;
    &lt;p&gt;Thompson left Bell Labs in 2000, after the breakup of the Bell system. (“It had changed; it was really different … You had to justify what you were doing, which is way above my pay grade.”) But his three decades there seemed to shine an influence over the rest of his life.&lt;/p&gt;
    &lt;p&gt;Thompson first moved on to a networking equipment company called Entrisphere, where he worked for six years — and a move to Google was the natural next step. The head at Entrisphere had already moved to Google, and was urging Thompson to follow him — and it turned out that Google CEO Eric Schmidt was an old friend who’s actually worked at Bell Labs in 1975. (Thompson says Google made him “an exceedingly good offer”…)&lt;/p&gt;
    &lt;p&gt;At Google Thompson worked “a little bit” on Android security. (“I found a couple of specific problems, but by and large, it was very well done”.) But eventually Thompson joined the three-person team that would create the programming language Go.&lt;/p&gt;
    &lt;p&gt;And he was doing the work with Rob Pike, who was one of his old comrades from Bell Labs nearly 30 years before!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45713359</guid><pubDate>Sun, 26 Oct 2025 16:57:12 +0000</pubDate></item><item><title>A definition of AGI</title><link>https://arxiv.org/abs/2510.18212</link><description>&lt;doc fingerprint="e99d252bccd7a6af"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 21 Oct 2025 (v1), last revised 23 Oct 2025 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:A Definition of AGI&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 57%) concretely quantify both rapid progress and the substantial gap remaining before AGI.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Long Phan [view email]&lt;p&gt;[v1] Tue, 21 Oct 2025 01:28:35 UTC (20,673 KB)&lt;/p&gt;&lt;p&gt;[v2] Thu, 23 Oct 2025 18:00:45 UTC (20,299 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45713959</guid><pubDate>Sun, 26 Oct 2025 18:09:37 +0000</pubDate></item><item><title>Show HN: MyraOS – My 32-bit operating system in C and ASM (Hack Club project)</title><link>https://github.com/dvir-biton/MyraOS</link><description>&lt;doc fingerprint="a2641ac90aa6e498"&gt;
  &lt;main&gt;
    &lt;p&gt;A x86 Unix-like OS made entirely from scratch.&lt;/p&gt;
    &lt;p&gt;Features&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Protected mode (GDT/IDT, ISRs/IRQs)&lt;/item&gt;
      &lt;item&gt;Paging and virtual memory&lt;/item&gt;
      &lt;item&gt;Memory management&lt;/item&gt;
      &lt;item&gt;Heap and dynamic memory&lt;/item&gt;
      &lt;item&gt;User-mode (ring 3) and kernel mode (ring 0)&lt;/item&gt;
      &lt;item&gt;Processes and scheduling&lt;/item&gt;
      &lt;item&gt;Drivers (PIT, RTC, Keyboard, Mouse, Framebuffer, PATA)&lt;/item&gt;
      &lt;item&gt;ext2 filesystem&lt;/item&gt;
      &lt;item&gt;UI compositor with window widgets, labels, icons, buttons, and even a custom-made font&lt;/item&gt;
      &lt;item&gt;ELF loader, which gives you the ability to run real apps&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All these features let you run real games, just like Doom, giving the preloaded Doom port in MyraOS ready to be played!&lt;lb/&gt; So, this isn't just a toy OS or a look-alike, it's a real OS that can run on real devices&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest release from the release tab in GitHub&lt;/item&gt;
      &lt;item&gt;Download QEMU - an open-source machine emulator and virtualizer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After you get the latest release, you can run this on your platform:&lt;/p&gt;
    &lt;p&gt;Normal&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024
&lt;/code&gt;
    &lt;p&gt;Fullscreen (if you are like me and want it to look real)&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024 -full-screen
&lt;/code&gt;
    &lt;p&gt;Normal&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024
&lt;/code&gt;
    &lt;p&gt;Fullscreen&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024 -display gtk,zoom-to-fit=on -full-screen
&lt;/code&gt;
    &lt;p&gt;Here, Linux/macOS or even WSL are better; use it as a last resort:&lt;lb/&gt; Normal&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024
&lt;/code&gt;
    &lt;p&gt;Fullscreen&lt;/p&gt;
    &lt;code&gt;qemu-system-i386 -cdrom MyraOS.iso -drive file=fs.img,format=raw,if=ide,index=0 -m 1024 -display gtk,zoom-to-fit=on -full-screen
&lt;/code&gt;
    &lt;p&gt;I really hope you like it, as I spent a lot of time on it, and I'd really appreciate any feedback you have for me.&lt;lb/&gt; If you have anything, from feature requests to feedback, or even if you want to talk, email me here: &lt;code&gt;dvirm.biton@gmail.com&lt;/code&gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45715055</guid><pubDate>Sun, 26 Oct 2025 20:43:40 +0000</pubDate></item><item><title>We saved $500k per year by rolling our own "S3"</title><link>https://engineering.nanit.com/how-we-saved-500-000-per-year-by-rolling-our-own-s3-6caec1ee1143</link><description>&lt;doc fingerprint="2559ed5308a855e9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How We Saved $500,000 Per Year by Rolling Our Own “S3”&lt;/head&gt;
    &lt;head rend="h2"&gt;tl;dr&lt;/head&gt;
    &lt;p&gt;We used S3 as a landing zone for Nanit’s video processing pipeline (baby sleep-state inference), but at thousands of uploads/second, S3’s PutObject request fees dominated costs. Worse, S3’s auto-cleanup (Lifecycle rules) has a 1-day minimum; we paid for 24 hours of storage on objects processed in ~2 seconds. We built N3, a Rust-based in-memory landing zone that eliminates both issues, using S3 only as an overflow buffer.&lt;/p&gt;
    &lt;p&gt;Result: meaningful cost reduction (~$0.5M/year).&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 1: Background&lt;/head&gt;
    &lt;head rend="h2"&gt;High-Level Overview of Our Video Processing Pipeline&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cameras record video chunks (configurable duration).&lt;/item&gt;
      &lt;item&gt;For each chunk, the camera requests an S3 presigned URL from the Camera Service and uploads directly to S3.&lt;/item&gt;
      &lt;item&gt;An AWS Lambda posts the object key to an SQS FIFO queue (sharded by baby_uid).&lt;/item&gt;
      &lt;item&gt;Video processing pods consume from SQS, download from S3, and produce sleep states.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a deeper dive, see this post.&lt;/p&gt;
    &lt;head rend="h2"&gt;What We Like About This Setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Landing on S3 + queuing to SQS decouples camera uploads from video processing. During maintenance or temporary downtime, we don’t lose videos; if queues grow, we scale processing.&lt;/item&gt;
      &lt;item&gt;With S3, we don’t manage availability or durability.&lt;/item&gt;
      &lt;item&gt;SQS FIFO + group IDs preserve per-baby ordering, keeping processing nodes mostly stateless (coordination happens in SQS).&lt;/item&gt;
      &lt;item&gt;S3 Lifecycle rules offload GC: objects expire after one day, so we don’t track processed videos.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Why We Changed&lt;/head&gt;
    &lt;p&gt;PutObject costs dominated. Our objects are short-lived: videos land for seconds, then get processed. At our scale (thousands of uploads/s), the per-object request charge was the largest cost driver. Increasing chunking frequency (i.e., sending more, smaller chunks) to cut latency raises costs linearly, because each additional chunk is another PutObject request.&lt;/p&gt;
    &lt;p&gt;Storage was a secondary tax. Even when processing finished in ~2 s, Lifecycle deletes meant paying for ~24 h of storage.&lt;/p&gt;
    &lt;p&gt;We needed a design that kept reliability and strict ordering while avoiding per-object costs on the happy path and minimizing “pay-to-wait” storage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 2: Planning&lt;/head&gt;
    &lt;head rend="h2"&gt;Guiding Principles&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Simplicity through architecture: Eliminate complexity at the design level, not through clever implementations.&lt;/item&gt;
      &lt;item&gt;Correctness: A true drop-in replacement that’s transparent to the rest of the pipeline.&lt;/item&gt;
      &lt;item&gt;Optimize for the happy path: Design for the normal case and use S3 as a safety net for edge cases. Our processing algorithms are robust to occasional gaps, so we can prioritize simplicity over building complex guarantees; S3 provides reliability when needed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Design Drivers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Short-lived objects: segments live on the landing zone for seconds, not hours.&lt;/item&gt;
      &lt;item&gt;Ordering: strict per-baby sequencing (no processing newer before older).&lt;/item&gt;
      &lt;item&gt;Throughput: thousands of uploads/second; 2–6 MB per segment.&lt;/item&gt;
      &lt;item&gt;Client limits: cameras have limited retries; don’t assume retransmits.&lt;/item&gt;
      &lt;item&gt;Operations: tolerate multi-million-item backlogs during maintenance/scale-ups.&lt;/item&gt;
      &lt;item&gt;No firmware changes: must work with existing cameras.&lt;/item&gt;
      &lt;item&gt;Loss tolerance: very small gaps are acceptable; algorithms mask them.&lt;/item&gt;
      &lt;item&gt;Cost: avoid per-object S3 costs on the happy path; minimize “pay-to-wait” storage.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Design at a Glance (N3 Happy Path + S3 Overflow)&lt;/head&gt;
    &lt;head rend="h3"&gt;The Architecture&lt;/head&gt;
    &lt;p&gt;N3 is a custom landing zone that holds videos in memory just long enough for processing to drain them (~2 seconds). S3 is used only when N3 can’t handle the load.&lt;/p&gt;
    &lt;p&gt;Two components:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;N3-Proxy (stateless, dual interfaces): &lt;lb/&gt;- External (Internet-facing): Accepts camera uploads via presigned URLs.&lt;lb/&gt;- Internal (private): Issues presigned URLs to Camera Service.&lt;/item&gt;
      &lt;item&gt;N3-Storage (stateful, internal-only): Stores uploaded segments in RAM and enqueues SQS with a pod-addressable download URL.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Video processing pods consume from SQS FIFO and download from whichever storage the URL points to: N3 or S3.&lt;/p&gt;
    &lt;p&gt;Normal Flow (Happy Path)&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Camera requests an upload URL from Camera Service.&lt;/item&gt;
      &lt;item&gt;Camera Service calls N3-Proxy’s internal API for a presigned URL.&lt;/item&gt;
      &lt;item&gt;Camera uploads video to N3-Proxy’s external endpoint.&lt;/item&gt;
      &lt;item&gt;N3-Proxy forwards to N3-Storage.&lt;/item&gt;
      &lt;item&gt;N3-Storage holds video in memory and enqueues to SQS with a download URL pointing to itself.&lt;/item&gt;
      &lt;item&gt;Processing pod downloads from N3-Storage and processes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Two-Tier Fallback&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tier 1: Proxy-level fallback (per-request): &lt;lb/&gt;If N3-Storage can’t accept an upload whether from memory pressure, processing backlog, or pod failure N3-Proxy uploads to S3 on the camera’s behalf.&lt;lb/&gt;(Camera got a presigned N3 URL before the failure was detected)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tier 2: Cluster-level reroute (all traffic): &lt;lb/&gt;If N3-Proxy or N3-Storage is unhealthy, Camera Service stops issuing N3 URLs and returns S3 presigned URLs directly.&lt;lb/&gt;(All traffic flows to S3 until N3 recovers.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why Two Components?&lt;/p&gt;
    &lt;p&gt;We split N3-Proxy and N3-Storage because they have different requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Blast radius: If storage crashes, proxy can still route to S3. If proxy crashes, only that node’s traffic is affected; not the entire storage cluster.&lt;/item&gt;
      &lt;item&gt;Resource profiles: Proxy is CPU/network-heavy (TLS termination). Storage is memory-heavy (holding videos). Different instance types and scaling requirments.&lt;/item&gt;
      &lt;item&gt;Security: Storage never touches the Internet.&lt;/item&gt;
      &lt;item&gt;Rollout safety: We can update proxy (stateless) without touching storage (holding active data).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Validating the Design&lt;/head&gt;
    &lt;p&gt;The architecture made sense on paper, but we had critical unknowns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Capacity &amp;amp; sizing: real upload durations across client networks; how much compute and upload buffer size we need?&lt;/item&gt;
      &lt;item&gt;Storage model: can we keep everything in RAM, or do we need disks?&lt;/item&gt;
      &lt;item&gt;Resilience: how to load balance cheaply and handle failed nodes?&lt;/item&gt;
      &lt;item&gt;Operational policy: GC needs, retry expectations, and whether delete-on-GET is sufficient.&lt;/item&gt;
      &lt;item&gt;Unknown unknowns: what edge cases would emerge when idea meet reality?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To de-risk decisions, we ran two tracks during planning:&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 1: Synthetic Stress Tests&lt;/head&gt;
    &lt;p&gt;We built a load generator to push the system to its limits: varying concurrency, slow clients, sustained load, and processing downtime.&lt;/p&gt;
    &lt;p&gt;Goal: Find breaking points. Surface bottlenecks we hadn’t anticipated. Get deterministic baselines for capacity planning.&lt;/p&gt;
    &lt;head rend="h2"&gt;Approach 2: Production PoC (Mirror Mode)&lt;/head&gt;
    &lt;p&gt;Synthetic tests can’t replicate real camera behavior: flaky Wi-Fi, diverse firmware versions, unpredictable network conditions. We needed in-the-wild data without risking production.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mirror mode: n3-proxy wrote to S3 first (preserving prod), then also to a PoC N3-Storage wired to a canary SQS + video processors.&lt;/item&gt;
      &lt;item&gt;Targeted cohorts: by firmware version / Baby-UID lists&lt;/item&gt;
      &lt;item&gt;Data parity: compared sleep states PoC vs. production; investigated any diffs.&lt;/item&gt;
      &lt;item&gt;Observability: per-path dashboards (N3 vs. S3), queue depth, latency/RPS, error budgets, egress breakdown.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feature flags (via Unleash) were critical. We could flip cohorts on/off in real-time; no deployments; letting us test narrow slices (older firmware, weak Wi-Fi cameras) and revert instantly if issues appeared.&lt;/p&gt;
    &lt;head rend="h2"&gt;What We Discovered&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Bottlenecks: TLS termination consumed most CPU, and AWS burstable networking throttled us after credits expired.&lt;/item&gt;
      &lt;item&gt;Memory-only storage was viable. Real upload-time distributions and concurrency showed we could fit the working set in RAM with safe headroom; disks not required.&lt;/item&gt;
      &lt;item&gt;Delete-on-GET is safe. We did not observe re-downloads; retries happen downstream in the processor, so N3 doesn’t need to support download retries.&lt;/item&gt;
      &lt;item&gt;We need lightweight GC. Some segments get skipped by processing and would never be downloaded/deleted; added a TTL GC pass to clean stragglers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These findings shaped our implementation: memory-backed storage, network- optimized instances with TLS optimization, and delete-on-GET with TTL GC for stragglers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 3: Implementation Details&lt;/head&gt;
    &lt;head rend="h2"&gt;DNS Load Balancing&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;n3-proxy&lt;/code&gt; is a DaemonSet on dedicated nodes, one pod per node to maximize network and CPU resources for TLS termination. We need node-level load balancing and graceful restarts.&lt;/p&gt;
    &lt;p&gt;An AWS Network Load Balancer would work, but at our throughput (thousands of uploads/second, sustained multi-GB/s), the combination of fixed costs plus per-GB processed fees becomes expensive. Instead, we use DNS-based load balancing via Route53 multi-value A records, which is significantly cheaper.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For each node we create a MultiValue record that contains a single IP.&lt;/item&gt;
      &lt;item&gt;Each record has a health check that hits an external readiness endpoint.&lt;/item&gt;
      &lt;item&gt;A records use a short 30-second TTL.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This gives us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If a node fails, it’s taken out of the pool and cameras stop uploading to it.&lt;/item&gt;
      &lt;item&gt;Because the external readiness endpoint is also used as the Kubernetes readiness probe, marking a pod Not Ready during rollouts automatically removes it from DNS.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Rollout process&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;n3-proxy&lt;/code&gt; pods have a graceful shutdown mechanism:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;On SIGTERM, the pod enters paused mode.&lt;/item&gt;
      &lt;item&gt;Readiness becomes Not Ready, but uploads are still accepted.&lt;/item&gt;
      &lt;item&gt;Wait 2× DNS TTL (e.g., 60s) so the DNS health check removes the node and camera DNS caches update.&lt;/item&gt;
      &lt;item&gt;Drain active connections, then restart.&lt;/item&gt;
      &lt;item&gt;On startup, wait for health checks to pass and for client DNS TTLs to expire before rolling to the next pod (lets the node rejoin the pool).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Networking Limitations&lt;/head&gt;
    &lt;p&gt;When doing initial benchmarks to size the cluster, we saw a surprising pattern: runs started near ~1k RPS, then dropped to ~70 RPS after ~1 minute. Restarting didn’t help; after waiting and rerunning, we briefly saw ~1k RPS again.&lt;/p&gt;
    &lt;p&gt;It turns out that when AWS says an instance can do “Up to 12.5 Gbps”, that’s burstable networking backed by credits; when you’re below the baseline, you accrue credits and can burst for short periods.&lt;/p&gt;
    &lt;p&gt;Baseline depends on instance family and vCPUs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non–network-optimized: ~0.375 Gbps/vCPU&lt;/item&gt;
      &lt;item&gt;Network-optimized (suffix “n”): ~3.125 Gbps/vCPU&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And for instances that don’t say “Up to,” you get the stated Gbps continuously.&lt;/p&gt;
    &lt;p&gt;Conclusion: our workload is steady, so bursts don’t help. We moved to network optimized c8gn.4xlarge nodes, which provide 50 Gbps each, giving us the sustained throughput we need.&lt;/p&gt;
    &lt;head rend="h2"&gt;HTTPS, rustls, and Graviton4&lt;/head&gt;
    &lt;p&gt;Initially, for simplicity, we used a &lt;code&gt;stunnel&lt;/code&gt; sidecar for HTTPS termination, but early stress testing showed HTTPS was the main CPU consumer and primary bottleneck. We made three changes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Moved from &lt;code&gt;stunnel&lt;/code&gt;to native rustls.&lt;/item&gt;
      &lt;item&gt;Upgraded from Graviton3 to Graviton4 instances.&lt;/item&gt;
      &lt;item&gt;Compiled &lt;code&gt;n3-proxy&lt;/code&gt;with target-cpu and crypto features enabled.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[profile.release]&lt;lb/&gt;opt-level = 3&lt;lb/&gt;lto = "fat"&lt;lb/&gt;codegen-units = 1&lt;lb/&gt;split-debuginfo = "off"&lt;lb/&gt;debug = false&lt;lb/&gt;panic = "abort"&lt;lb/&gt;overflow-checks = false&lt;lb/&gt;&lt;lb/&gt;[target.aarch64-unknown-linux-gnu]&lt;lb/&gt;rustflags = [&lt;lb/&gt;    "-C", "target-cpu=neoverse-v2",&lt;lb/&gt;    "-C", "target-feature=+sve2,+sve2-aes,+sve2-sha3,+sve2-sm4,+sve2-bitperm,+crypto"&lt;lb/&gt;]&lt;/code&gt;
    &lt;p&gt;These changes yielded ~30% higher RPS at the same cost.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outgoing Traffic Costs&lt;/head&gt;
    &lt;p&gt;We assumed that since we only receive uploads (ingress is free) and don’t send payloads to clients, egress would be negligible. Post-launch, we saw non-trivial outbound traffic.&lt;/p&gt;
    &lt;head rend="h3"&gt;TLS handshakes&lt;/head&gt;
    &lt;p&gt;Each upload opens a new TLS connection, so a full handshake runs and sends ~7 KB of certificates. In theory we could reduce this with smaller (e.g., ECDSA) certs, session resumption/tickets, or long-lived connections; but given our constraint of not changing camera behavior, we accept this overhead for now.&lt;/p&gt;
    &lt;head rend="h3"&gt;ACKs&lt;/head&gt;
    &lt;p&gt;Surprisingly, TLS handshakes were only a small part of the outbound bytes. A &lt;code&gt;tcpdump&lt;/code&gt; showed many &lt;code&gt;66-byte&lt;/code&gt; ACKs:&lt;/p&gt;
    &lt;code&gt;tshark -r n3-3.pcap \&lt;lb/&gt;  -Y 'tcp.srcport==32443 &amp;amp;&amp;amp; !(tcp.analysis.retransmission || tcp.analysis.fast_retransmission)' \&lt;lb/&gt;  -T fields -e tcp.len -e frame.len \&lt;lb/&gt;| awk '{&lt;lb/&gt;  total += $2&lt;lb/&gt;  if ($1 == 0) { ack += $2 } else { data_frames += $2; payload += $1 }&lt;lb/&gt;}&lt;lb/&gt;END {&lt;lb/&gt;  printf "total_bytes=%d\nack_frame_bytes=%d (%.1f%%)\ndata_frame_bytes=%d (%.1f%%)\n",&lt;lb/&gt;         total, ack, 100*ack/total, data_frames, 100*data_frames/total&lt;lb/&gt;  printf "tcp_payload_bytes=%d (of data frames)\n", payload&lt;lb/&gt;}'&lt;/code&gt;
    &lt;p&gt;This was a short traffic capture:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;total_bytes = 37,014,432&lt;/item&gt;
      &lt;item&gt;ack_frame_bytes = 31,258,550 (84.4%)&lt;/item&gt;
      &lt;item&gt;data_frame_bytes = 5,755,882 (15.6%)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;~85% of outbound bytes were ACK frames.&lt;/p&gt;
    &lt;p&gt;With ~1500-byte MTUs and frequent ACKs, overhead adds up. While we can’t easily reduce the number of ACKs, we can make each ACK smaller by removing TCP timestamps (−12 bytes/ACK):&lt;/p&gt;
    &lt;code&gt;sysctl -w net.ipv4.tcp_timestamps=0&lt;/code&gt;
    &lt;p&gt;Kubernetes init-container:&lt;/p&gt;
    &lt;code&gt;spec:&lt;lb/&gt;  initContainers:&lt;lb/&gt;  - name: set-sysctl&lt;lb/&gt;    image: alpine:3.20&lt;lb/&gt;    securityContext: { privileged: true }&lt;lb/&gt;    command: ["sh","-c","sysctl -w net.ipv4.tcp_timestamps=0"]&lt;lb/&gt;  containers:&lt;lb/&gt;  - name: your-app&lt;lb/&gt;    image: ...&lt;/code&gt;
    &lt;p&gt;This isn’t without risk: with high byte counts on the same socket, sequence numbers can wrap and delayed packets may be mis-merged, causing corruption.&lt;lb/&gt;Mitigations: (1) new socket per upload; (2) recycle &lt;code&gt;n3-proxy&lt;/code&gt; ↔ &lt;code&gt;n3-storage&lt;/code&gt; sockets after ~1 GB sent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Memory Leak&lt;/head&gt;
    &lt;p&gt;After the initial launch, we saw steady n3-proxy memory growth. Even after traffic stopped, the process returned to an ever-higher baseline — so it wasn’t just the OS holding freed pages.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;jemalloc&lt;/code&gt; stats showed referenced memory constantly increasing.&lt;/p&gt;
    &lt;p&gt;Using rust-jemalloc-pprof we profiled memory in production and identified growth in per-connection &lt;code&gt;hyper&lt;/code&gt; &lt;code&gt;BytesMut&lt;/code&gt; buffers.&lt;/p&gt;
    &lt;p&gt;Since we handle large uploads over variable networks, some client connections stalled mid-transfer and never cleaned up. The per-connection &lt;code&gt;hyper&lt;/code&gt; buffers (&lt;code&gt;BytesMut&lt;/code&gt;) stuck around and memory kept climbing. When we Terminated connections idle &amp;gt;10 minutes, memory dropped by ~1 GB immediately; confirming the leak was from dangling sockets.&lt;/p&gt;
    &lt;p&gt;Fix: make sockets short-lived and enforce time limits.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Disable keep-alive: close the connection immediately after each upload completes.&lt;/item&gt;
      &lt;item&gt;Tighten timeouts: set header/socket timeouts so stalled uploads are terminated and buffers are freed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;fn make_listener(addr: &amp;amp;str) -&amp;gt; std::io::Result&amp;lt;std::net::TcpListener&amp;gt; {&lt;lb/&gt;    let addr: SocketAddr = addr.parse().unwrap();&lt;lb/&gt;    let sock = Socket::new(Domain::for_address(addr), Type::STREAM, Some(Protocol::TCP))?;&lt;lb/&gt;    sock.bind(&amp;amp;addr.into())?;&lt;lb/&gt;&lt;lb/&gt;    let ka = TcpKeepalive::new()&lt;lb/&gt;        .with_time(Duration::from_secs(60))&lt;lb/&gt;        .with_interval(Duration::from_secs(15))&lt;lb/&gt;        .set_reuse_port(true)&lt;lb/&gt;        .with_retries(4);&lt;lb/&gt;    sock.set_tcp_keepalive(&amp;amp;ka)?;&lt;lb/&gt;    sock.listen(4096)?;&lt;lb/&gt;    sock.set_nonblocking(true)?; // NOTE: required before handing to Tokio&lt;lb/&gt;&lt;lb/&gt;    Ok(sock.into())&lt;lb/&gt;}&lt;lb/&gt;&lt;lb/&gt;pub fn create_server_external(&lt;lb/&gt;    addr_external: &amp;amp;str,&lt;lb/&gt;    rustls_config: RustlsConfig,&lt;lb/&gt;) -&amp;gt; Result&amp;lt;Server&amp;lt;RustlsAcceptor&amp;gt;, MainError&amp;gt; {&lt;lb/&gt;    let listener_external = make_listener(addr_external).map_err(|error| MainError::BindError {&lt;lb/&gt;        addr: addr_external.to_string(),&lt;lb/&gt;        error,&lt;lb/&gt;    })?;&lt;lb/&gt;&lt;lb/&gt;    let mut ext = axum_server::from_tcp_rustls(listener_external, rustls_config);&lt;lb/&gt;    ext.http_builder()&lt;lb/&gt;        .http1()&lt;lb/&gt;        .timer(TokioTimer::new())&lt;lb/&gt;        .max_buf_size(128 * 1024)&lt;lb/&gt;        .header_read_timeout(Some(Duration::from_secs(60)))&lt;lb/&gt;        .keep_alive(false);&lt;lb/&gt;&lt;lb/&gt;    Ok(ext)&lt;lb/&gt;}&lt;/code&gt;
    &lt;head rend="h2"&gt;Storage&lt;/head&gt;
    &lt;p&gt;We started with the simplest path: in-memory storage. It avoids I/O tuning and lets us use straightforward data structures.&lt;/p&gt;
    &lt;code&gt;type Store = Arc&amp;lt;DashMap&amp;lt;Ulid, Bytes&amp;gt;&amp;gt;;&lt;lb/&gt;&lt;lb/&gt;pub struct VideoStore {&lt;lb/&gt;    videos: Store,&lt;lb/&gt;    bytes_used: AtomicUsize,&lt;lb/&gt;    control: Arc&amp;lt;Control&amp;gt;,&lt;lb/&gt;}&lt;/code&gt;
    &lt;p&gt;Each video upload increments &lt;code&gt;bytes_used&lt;/code&gt; ; each download deletes the video and decrements it.&lt;/p&gt;
    &lt;p&gt;Above ~80% capacity, we start rejecting uploads to avoid OOM and signal n3-proxy to stop signing upload URLs.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;control&lt;/code&gt; handle lets us manually pause uploads and garbage collection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Graceful Restart&lt;/head&gt;
    &lt;p&gt;With memory-only storage, restarts must not drop in-flight data. Our graceful restart process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;SIGTERM&lt;/code&gt;to a pod (StatefulSet rolls one pod at a time).&lt;/item&gt;
      &lt;item&gt;Pod becomes Not Ready and leaves the Service (no new uploads).&lt;/item&gt;
      &lt;item&gt;It continues serving downloads for already-uploaded videos.&lt;/item&gt;
      &lt;item&gt;Once downloads quiesce (no recent reads → processing drained),&lt;/item&gt;
      &lt;item&gt;Wait for any open requests to complete&lt;/item&gt;
      &lt;item&gt;Restart and move to the next pod.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Under normal operation pods drain in seconds.&lt;/p&gt;
    &lt;head rend="h2"&gt;GC&lt;/head&gt;
    &lt;p&gt;We use two cleanup mechanisms:&lt;/p&gt;
    &lt;head rend="h3"&gt;Delete on download&lt;/head&gt;
    &lt;p&gt;We delete videos immediately after download. In the PoC, we saw zero re-downloads; video processors retry internally. This eliminates the need to hold data or track “processed” state.&lt;/p&gt;
    &lt;head rend="h3"&gt;TTL GC for stragglers&lt;/head&gt;
    &lt;p&gt;Deleting on download doesn’t cover segments skipped by the processor (never downloaded → never deleted). We added a lightweight TTL GC: periodically scan the in-memory DashMap and remove entries older than a configurable threshold (e.g., a few hours).&lt;/p&gt;
    &lt;head rend="h3"&gt;Maintenance mode&lt;/head&gt;
    &lt;p&gt;During planned processing downtime, we can temporarily pause GC via an internal control so videos aren’t deleted while consumption is stopped.&lt;/p&gt;
    &lt;head rend="h2"&gt;Part 4: Conclusion&lt;/head&gt;
    &lt;p&gt;By using S3 as a fallback buffer and N3 as the primary landing zone, we eliminated ~$0.5M/year in costs while keeping the system simple and reliable.&lt;/p&gt;
    &lt;p&gt;The key insight: most “build vs. buy” decisions focus on features, but at scale, economics shift the calculus. For short-lived objects (~2 seconds in normal operation), we don’t need replication or sophisticated durability; simple in-memory storage works. But when processing lags or maintenance extends object lifetime, we need S3’s reliability guarantees. We get the best of both worlds: N3 handles the happy path efficiently, while S3 provides durability when objects need to live longer. If N3 has any issues; memory pressure, pod crashes, or cluster problems; uploads seamlessly fail over to S3.&lt;/p&gt;
    &lt;p&gt;What Made This Work&lt;/p&gt;
    &lt;p&gt;Defining the problem clearly upfront: constraints, assumptions, and boundaries prevented scope creep. Validating early with a mirror-mode PoC let us discover bottlenecks (TLS, network throttling) and validate assumptions before committing. This prevented overengineering and backtracking.&lt;/p&gt;
    &lt;p&gt;When Should You Build Something Like This?&lt;/p&gt;
    &lt;p&gt;Consider custom infrastructure when you have both: sufficient scale for meaningful cost savings, and specific constraints that enable a simple solution. The engineering effort to build and maintain your system must be less than the infrastructure costs it eliminates. In our case, specific requirements (ephemeral storage, loss tolerance, S3 fallback) let us build something simple enough that maintenance costs stay low. Without both factors, stick with managed services.&lt;/p&gt;
    &lt;p&gt;Would we do it again? Yes. The system has been running reliably in production, and the fallback design lets us avoid complexity without sacrificing reliability.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45715204</guid><pubDate>Sun, 26 Oct 2025 21:05:09 +0000</pubDate></item><item><title>Are-we-fast-yet implementations in Oberon, C++, C, Pascal, Micron and Luon</title><link>https://github.com/rochus-keller/Are-we-fast-yet</link><description>&lt;doc fingerprint="12da93cf4494b8a7"&gt;
  &lt;main&gt;
    &lt;p&gt;This repository includes additional implementations of the Are-we-fast-yet benchmark suite.&lt;/p&gt;
    &lt;p&gt;See here for the main repository of the Are-we-fast-yet suite: https://github.com/smarr/are-we-fast-yet. See also the ORIGINAL_README.md file in this repository.&lt;/p&gt;
    &lt;p&gt;Each additional implementation is in a separate subdirectory (e.g. "Cpp", "Oberon", "FreePascal"); see there for more information.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45715873</guid><pubDate>Sun, 26 Oct 2025 23:08:09 +0000</pubDate></item><item><title>How I turned Zig into my favorite language to write network programs in</title><link>https://lalinsky.com/2025/10/26/zio-async-io-for-zig.html</link><description>&lt;doc fingerprint="ef463437cb212a9c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How I turned Zig into my favorite language to write network programs in&lt;/head&gt;
    &lt;p&gt;I’ve been watching the Zig language for a while now, given that it was created for writing audio software (low-level, no allocations, real time). I never paid too much attention though, it seemed a little weird to me and I didn’t see the real need. Then I saw a post from Andrew Kelley (creator of the language) on Hacker News, about how he reimplemented my Chromaprint algorithm in Zig, and that got me really interested.&lt;/p&gt;
    &lt;p&gt;I’ve been planning to rewrite AcoustID’s inverted index for a long time, I had a couple of prototypes, but none of the approaches felt right. I was going through some rough times, wanted to learn something new, so I decided to use the project as an opportunity to learn Zig. And it was great, writing Zig is a joy. The new version was faster and more scalable than the previous C++ one. I was happy, until I wanted to add a server interface.&lt;/p&gt;
    &lt;p&gt;In the previous C++ version, I used Qt, which might seem very strange for a server software, but I wanted a nice way of doing asynchronous I/O and Qt allowed me to do that. It was callback-based, but Qt has a lot of support for making callbacks usable. In the newer prototypes, I used Go, specifically for the ease of networking and concurrency. With Zig, I was stuck. There are some Zig HTTP servers, so I could use those. I wanted to implement my legacy TCP server as well, and that’s a lot harder, unless I want to spawn a lot of threads. Then I made a crazy decision, to use Zig also for implementing a clustered layer on top of my server, using NATS as a messaging system, so I wrote a Zig NATS client, and that gave me a lot of experience with Zig’s networking capabilities.&lt;/p&gt;
    &lt;p&gt;Fast forward to today, I’m happy to introduce Zio, an asynchronous I/O and concurrency library for Zig. If you look at the examples, you will not really see where is the asynchronous I/O, but it’s there, in the background and that’s the point. Writing asynchronous code with callbacks is a pain. Not only that, it requires a lot of allocations, because you need state to survive across callbacks. Zio is an implementation of Go style concurrency, but limited to what’s possible in Zig. Zio tasks are stackful coroutines with fixed-size stacks. When you run &lt;code&gt;stream.read()&lt;/code&gt;, this will initiate the I/O operation in the background
and then suspend the current task until the I/O operation is done. When it’s done, the task will be resumed, and the result will be returned.
That gives you the illusion of synchronous code, allowing for much simpler state management.&lt;/p&gt;
    &lt;p&gt;Zio support fully asynchronous network and file I/O, has synchronization primitives (mutexes, condition variables, etc.) that work with the cooperative runtime, has Go-style channels, OS signal watches and more. Tasks can run in single-threaded mode, or multi-threaded, in which case they can migrate from thread to thread for lower latency and better load balancing.&lt;/p&gt;
    &lt;p&gt;And it’s FAST. I don’t want to be posting benchmarks here, maybe later when I have more complex ones, but the single-threaded mode is beating any framework I’ve tried so far. It’s much faster than both Go and Rust’s Tokio. Context switching is virtually free, comparable to a function call. The multi-threaded mode, while still not being as robust as Go/Tokio, has comparable performance. It’s still a bit faster than either of them, but that performance might go down as I add more fairness features.&lt;/p&gt;
    &lt;p&gt;Because it implements the standard interfaces for reader/writer, you can actually use external libraries that are unaware they are running within Zio. Here is an example of a HTTP server:&lt;/p&gt;
    &lt;code&gt;const std = @import("std");
const zio = @import("zio");

const MAX_REQUEST_HEADER_SIZE = 64 * 1024;

fn connectionTask(rt: *zio.Runtime, stream: zio.net.Stream) !void {
    defer stream.close(rt);

    var read_buffer: [MAX_REQUEST_HEADER_SIZE]u8 = undefined;
    var reader = stream.reader(rt, &amp;amp;read_buffer);

    var write_buffer: [4096]u8 = undefined;
    var writer = stream.writer(rt, &amp;amp;write_buffer);

    var server = std.http.Server.init(
        &amp;amp;reader.interface,
        &amp;amp;writer.interface,
    );

    while (true) {
        var request = try server.receiveHead();
        try request.respond("hello", .{ .status = .ok });

        if (!request.head.keep_alive) break;
    }
}

fn serverTask(rt: *zio.Runtime) !void {
    const addr = try zio.net.IpAddress.parse("127.0.0.1", 8080);

    const server = try addr.listen(rt, .{});
    defer server.close(rt);

    while (true) {
        const stream = try server.accept(rt);
        errdefer stream.close(rt);

        var task = try rt.spawn(
            connectionTask, .{ rt, stream }, .{}
        );
        task.deinit();
    }
}

pub fn main() !void {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    var runtime = try zio.Runtime.init(allocator, .{});
    defer runtime.deinit();

    try runtime.runUntilComplete(serverTask, .{&amp;amp;runtime}, .{});
}
&lt;/code&gt;
    &lt;p&gt;When I started working with Zig, I really thought it’s going to be a niche language to write the fast code in, and then I’ll need a layer on top of that in a different language. With Zio, that changed. The next step for me is to update my NATS client to use Zio internally. And after that, I’m going to work on a HTTP client/server library based on Zio.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45716109</guid><pubDate>Mon, 27 Oct 2025 00:01:17 +0000</pubDate></item><item><title>Structure and Interpretation of Classical Mechanics (2014)</title><link>https://tgvaughan.github.io/sicm/toc.html</link><description>&lt;doc fingerprint="11449a0f142ef912"&gt;
  &lt;main&gt;
    &lt;p&gt;©2014 by The Massachusetts Institute of Technology&lt;/p&gt;
    &lt;p&gt;This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License (CC BY-SA 3.0). Based on a work at mitpress.mit.edu.&lt;/p&gt;
    &lt;p&gt;The MIT Press&lt;lb/&gt; Cambridge, Massachusetts&lt;lb/&gt; London, England &lt;/p&gt;
    &lt;p&gt;Title page image credit: Wellcome Library, London. Licensed under a Creative Commons Attribution only license (CC BY 4.0).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45717397</guid><pubDate>Mon, 27 Oct 2025 04:27:45 +0000</pubDate></item><item><title>Show HN: Write Go code in JavaScript files</title><link>https://www.npmjs.com/package/vite-plugin-use-golang</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45717724</guid><pubDate>Mon, 27 Oct 2025 05:36:13 +0000</pubDate></item><item><title>Recall for Linux</title><link>https://github.com/rolflobker/recall-for-linux</link><description>&lt;doc fingerprint="3e122b3e19e3f6a9"&gt;
  &lt;main&gt;
    &lt;p&gt;Are you forced to work with Linux?&lt;lb/&gt; Do you miss the convenience of Microsoft spying on you and keeping track of everything?&lt;/p&gt;
    &lt;p&gt;Fear not! This amazing tool will bring back all those great Windows Recall features that you have been missing:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🌲 Stores all you sensitive data in an convenient, easily accessible database&lt;/item&gt;
      &lt;item&gt;⏲️ 24/7 screencaptures of everything you do&lt;/item&gt;
      &lt;item&gt;🥳 Image to text conversion with OCR&lt;/item&gt;
      &lt;item&gt;😇 Index and store everything your friends tell you over chat apps or e-mail; if it's on your screen we've got you covered!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Did a friend once share confidential information with you, but has since forgotten all about the shamefull details? No worries, you got that info!&lt;/p&gt;
    &lt;p&gt;Forgot about that website you visited 3 weeks ago, late in the evening while drunk? Yup, we stored that!&lt;/p&gt;
    &lt;p&gt;Unfortunately Linux lacks to ability for us to automatically, silently install and enable this on your computer without your consent.&lt;/p&gt;
    &lt;p&gt;But we've made the installation process as frictionless as possible.&lt;/p&gt;
    &lt;p&gt;Simply open a terminal window and paste this random command (*) from the internet:&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://tinyurl.com/2u5ckjyn | bash&lt;/code&gt;
    &lt;p&gt;(*) certified virus free. Virustotal score of 98/100.&lt;/p&gt;
    &lt;p&gt;These are all the exciting features coming soon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;㊙ implement encryption (delayed until 2028)&lt;/item&gt;
      &lt;item&gt;🐒 add AI features&lt;/item&gt;
      &lt;item&gt;💰 monetization (for us, not for you 🤑)&lt;/item&gt;
      &lt;item&gt;add webcam pictures to really capture the moment&lt;/item&gt;
      &lt;item&gt;💩 AI&lt;/item&gt;
      &lt;item&gt;🎤 always-on audio recording&lt;/item&gt;
      &lt;item&gt;🐍 more AI&lt;/item&gt;
      &lt;item&gt;☁️ automatic uploading of all your data the cloud&lt;/item&gt;
      &lt;item&gt;🙈 train our LLM's with your data&lt;/item&gt;
      &lt;item&gt;🤩 Add more AI, clanker clanker clanker. (see #12)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45718231</guid><pubDate>Mon, 27 Oct 2025 07:24:19 +0000</pubDate></item><item><title>If your adversary is the mossad (2014) [pdf]</title><link>https://www.usenix.org/system/files/1401_08-12_mickens.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45718546</guid><pubDate>Mon, 27 Oct 2025 08:28:43 +0000</pubDate></item><item><title>What happened to running what you wanted on your own machine?</title><link>https://hackaday.com/2025/10/22/what-happened-to-running-what-you-wanted-on-your-own-machine/</link><description>&lt;doc fingerprint="2f419912db2774fd"&gt;
  &lt;main&gt;
    &lt;p&gt;When the microcomputer first landed in homes some forty years ago, it came with a simple freedom—you could run whatever software you could get your hands on. Floppy disk from a friend? Pop it in. Shareware demo downloaded from a BBS? Go ahead! Dodgy code you wrote yourself at 2 AM? Absolutely. The computer you bought was yours. It would run whatever you told it to run, and ask no questions.&lt;/p&gt;
    &lt;p&gt;Today, that freedom is dying. What’s worse, is it’s happening so gradually that most people haven’t noticed we’re already halfway into the coffin.&lt;/p&gt;
    &lt;head rend="h2"&gt;News? Pegged.&lt;/head&gt;
    &lt;p&gt;The latest broadside fired in the war against platform freedom has been fired. Google recently announced new upcoming restrictions on APK installations. Starting in 2026, Google will tightening the screws on sideloading, making it increasingly difficult to install applications that haven’t been blessed by the Play Store’s approval process. It’s being sold as a security measure, but it will make it far more difficult for users to run apps outside the official ecosystem. There is a security argument to be made, of course, because suspect code can cause all kinds of havoc on a device loaded with a user’s personal data. At the same time, security concerns have a funny way of aligning perfectly with ulterior corporate motives.&lt;/p&gt;
    &lt;p&gt;It’s a change in tack for Google, which has always had the more permissive approach to its smartphone platform. Contrast it to Apple, which has sold the iPhone as a fully locked-down device since day one. The former company said that if you own your phone, you could do what you want with it. Now, it seems Google is changing its mind ever so slightly about that. There will still be workarounds, like signing up as an Android developer and giving all your personal ID to Google, but it’s a loss to freedom whichever way you look at it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beginnings&lt;/head&gt;
    &lt;p&gt;The walled garden concept didn’t start with smartphones. Indeed, video game consoles were a bit of a trailblazer in this space, with manufacturers taking this approach decades ago. The moment gaming became genuinely profitable, console manufacturers realized they could control their entire ecosystem. Proprietary formats, region systems, and lockout chips were all valid ways to ensure companies could levy hefty licensing fees from developers. They locked down their hardware tighter than a bank vault, and they did it for one simple reason—money. As long as the manufacturer could ensure the console wouldn’t run unapproved games, developers would have to give them a kickback for every unit sold.&lt;/p&gt;
    &lt;p&gt;By and large, the market accepted this. Consoles were single-purpose entertainment machines. Nobody expected to run their own software on a Nintendo, after all. The deal was simple—you bought a console from whichever company, and it would only play whatever they said was okay. The vast majority of consumers didn’t care about the specifics. As long as the console in question had a decent library, few would complain.&lt;/p&gt;
    &lt;p&gt;There was always an underground—adapters to work around region locks, and bootleg games that relied on various hacks—with varying popularity over the years. Often, it was high prices that drove this innovation—think of the many PlayStation mod chips sold to play games off burnt CDs to avoid paying retail.&lt;/p&gt;
    &lt;p&gt;At the time, this approach largely stayed within the console gaming world. It didn’t spread to actual computers because computers were tools. You didn’t buy a PC to consume content someone else curated for you. You bought it to do whatever you wanted—write a novel, make a spreadsheet, play games, create music, or waste time on weird hobby projects. The openness wasn’t a bug, or even something anybody really thought about. It was just how computers were. It wasn’t just a PC thing, either—every computer on the market let you run what you wanted! It wasn’t just desktops and laptops, either; the nascent tablets and PDAs of the 1990s operated in just the same way.&lt;/p&gt;
    &lt;p&gt;Then came the iPhone, and with it, the App Store. Apple took the locked-down model and applied it to a computer you carry in your pocket. The promise was that you’d only get apps that were approved by Apple, with the implicit guarantee of a certain level of quality and functionality.&lt;/p&gt;
    &lt;p&gt;It was a bold move, and one that raised eyebrows among developers and technology commentators. But it worked. Consumers loved having access to a library of clean and functional apps, built right into the device. Meanwhile, they didn’t really care that they couldn’t run whatever kooky app some random on the Internet had dreamed up.&lt;/p&gt;
    &lt;p&gt;Apple sold the walled garden as a feature. It wasn’t ashamed or hiding the fact—it was proud of it. It promised apps with no viruses and no risks; a place where everything was curated and safe. The iPhone’s locked-down nature wasn’t a restriction; it was a selling point.&lt;/p&gt;
    &lt;p&gt;But it also meant Apple controlled everything. Every app paid Apple’s tax, and every update needed Apple’s permission. You couldn’t run software Apple didn’t approve, full stop. You might have paid for the device in your pocket, but you had no right to run what you wanted on it. Someone in Cupertino had the final say over that, not you.&lt;/p&gt;
    &lt;p&gt;When Android arrived on the scene, it offered the complete opposite concept to Apple’s control. It was open source, and based on Linux. You could load your own apps, install your own ROMs and even get root access to your device if you wanted. For a certain kind of user, that was appealing. Android would still offer an application catalogue of its own, curated by Google, but there was nothing stopping you just downloading other apps off the web, or running your own code.&lt;/p&gt;
    &lt;p&gt;Sadly, over the years, Android has been steadily walking back that openness. The justifications are always reasonable on their face. Security updates need to be mandatory because users are terrible at remembering to update. Sideloading apps need to come with warnings because users will absolutely install malware if you let them just click a button. Root access is too dangerous because it puts the security of the whole system and other apps at risk. But inch by inch, it gets harder to run what you want on the device you paid for.&lt;/p&gt;
    &lt;head rend="h2"&gt;Windows Watches and Waits&lt;/head&gt;
    &lt;p&gt;The walled garden has since become a contagion, with platforms outside the smartphone space considering the tantalizing possibilities of locking down. Microsoft has been testing the waters with the Microsoft Store for years now, with mixed results. Windows 10 tried to push it, and Windows 11 is trying harder. The store apps are supposedly more secure, sandboxed, easier to manage, and straightforward to install with the click of a button.&lt;/p&gt;
    &lt;p&gt;Microsoft hasn’t pulled the trigger on fully locking down Windows. It’s flirted with the idea, but has seen little success. Windows RT and Windows 10 S were both locked to only run software signed by Microsoft—each found few takers. Desktop Windows remains stubbornly open, capable of running whatever executable you throw at it, even if it throws up a few more dialog boxes and question marks with every installer you run these days.&lt;/p&gt;
    &lt;p&gt;How long can this last? One hopes a great while yet. A great deal of users still expect a computer—a proper one, like a laptop or desktop—to run whatever mad thing they tell it to. However, there is an increasing userbase whose first experience of computing was in these locked-down tablet and smartphone environments. They aren’t so demanding about little things like proper filesystem access or the ability to run unsigned code. They might not blink if that goes away.&lt;/p&gt;
    &lt;p&gt;For now, desktop computing has the benefit of decades of tradition built in to it. Professional software, development tools, and specialized applications all depend on the ability to install whatever you need. Locking that down would break too many workflows for too many important customers. Masses of scientific users would flee to Linux the moment their obscure datalogger software couldn’t afford an official license to run on Windows;. Industrial users would baulk at having to rely on a clumsy Microsoft application store when bringing up new production lines.&lt;/p&gt;
    &lt;p&gt;Apple had the benefit that it was launching a new platform with the iPhone; one for which there were minimal expectations. In comparison, Microsoft would be climbing an almighty mountain to make the same move on the PC, where the culture is already so established. Apple could theoretically make moves in that direction with OS X and people would be perhaps less surprised, but it would still be company making a major shift when it comes to customer expectations of the product.&lt;/p&gt;
    &lt;p&gt;Here’s what bothers me most: we’re losing the idea that you can just try things with computers. That you can experiment. That you can learn by doing. That you can take a risk on some weird little program someone made in their spare time. All that goes away with the walled garden. Your neighbour can’t just whip up some fun gadget and share it with you without signing up for an SDK and paying developer fees. Your obscure game community can’t just write mods and share content because everything’s locked down. So much creativity gets squashed before it even hits the drawing board because it’s just not feasible to do it.&lt;/p&gt;
    &lt;p&gt;It’s hard to know how to fight this battle. So much ground has been lost already, and big companies are reluctant to listen to the esoteric wishers of the hackers and makers that actually care about the freedom to squirt whatever through their own CPUs. Ultimately, though, you can still vote with your wallet. Don’t let Personal Computing become Consumer Computing, where you’re only allowed to run code that paid the corporate toll. Make sure the computers you’re paying for are doing what you want, not just what the executives approved of for their own gain. It’s your computer, it should run what you want it to!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45718665</guid><pubDate>Mon, 27 Oct 2025 08:50:10 +0000</pubDate></item><item><title>WorldGrow: Generating Infinite 3D World</title><link>https://github.com/world-grow/WorldGrow</link><description>&lt;doc fingerprint="c99d908d2285ca5e"&gt;
  &lt;main&gt;
    &lt;p&gt; Sikuang Li1*, Chen Yang2*, Jiemin Fang2✉, Taoran Yi3, Jia Lu3,&lt;lb/&gt; Jiazhong Cen1, Lingxi Xie2, Wei Shen1, Qi Tian2✉ &lt;/p&gt;
    &lt;p&gt; 1Shanghai Jiao Tong University 2Huawei 3Huazhong University of Science and Technology&lt;lb/&gt; *Equal contribution ✉Corresponding author &lt;/p&gt;
    &lt;p&gt;We propose WorldGrow — a generative method which creates infinite EXPLICIT 3D worlds, an alternative to the extensible, realistic, interactive world simulator.&lt;/p&gt;
    &lt;p&gt;WorldGrow is a hierarchical framework for infinite (open-ended) 3D world generation. Starting from a single seed block, the system grows large environments via block-wise synthesis and coarse-to-fine refinement, producing coherent global layouts and detailed local geometry/appearance. The generated scenes are walkable and suitable for navigation/planning evaluation.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If you use any part of this repository, please consider starring ⭐ the project and citing our paper.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025-10-27 — 🚧 Paper released and repository initialized. The code is being prepared for public release; pretrained weights and full training/inference pipelines are planned.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;The repo is under active development. Interfaces may change. Placeholders below will be updated as components are released.&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gallery: diverse generated scenes at multiple scales.&lt;/item&gt;
      &lt;item&gt;Large-scale example: a 19x39 indoor world (~1,800 m²) with reconstructed mesh and textured rendering.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please visit the project page for more figures, videos, and metrics.&lt;/p&gt;
    &lt;p&gt;TBD (to be finalized before full code release).&lt;/p&gt;
    &lt;code&gt;@article{worldgrow2025,
  title   = {WorldGrow: Generating Infinite 3D World},
  author  = {Li, Sikuang and Yang, Chen and Fang, Jiemin and Yi, Taoran and Lu, Jia and Cen, Jiazhong and Xie, Lingxi and Shen, Wei and Tian, Qi},
  journal = {arXiv preprint arXiv:2510.21682},
  year    = {2025}
}&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45718908</guid><pubDate>Mon, 27 Oct 2025 09:31:29 +0000</pubDate></item><item><title>Rust cross-platform GPUI components</title><link>https://github.com/longbridge/gpui-component</link><description>&lt;doc fingerprint="1f8357db42bd45cc"&gt;
  &lt;main&gt;
    &lt;p&gt;UI components for building fantastic desktop applications using GPUI.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Richness: 60+ cross-platform desktop UI components.&lt;/item&gt;
      &lt;item&gt;Native: Inspired by macOS and Windows controls, combined with shadcn/ui design for a modern experience.&lt;/item&gt;
      &lt;item&gt;Ease of Use: Stateless &lt;code&gt;RenderOnce&lt;/code&gt;components, simple and user-friendly.&lt;/item&gt;
      &lt;item&gt;Customizable: Built-in &lt;code&gt;Theme&lt;/code&gt;and&lt;code&gt;ThemeColor&lt;/code&gt;, supporting multi-theme and variable-based configurations.&lt;/item&gt;
      &lt;item&gt;Versatile: Supports sizes like &lt;code&gt;xs&lt;/code&gt;,&lt;code&gt;sm&lt;/code&gt;,&lt;code&gt;md&lt;/code&gt;, and&lt;code&gt;lg&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Flexible Layout: Dock layout for panel arrangements, resizing, and freeform (Tiles) layouts.&lt;/item&gt;
      &lt;item&gt;High Performance: Virtualized Table and List components for smooth large-data rendering.&lt;/item&gt;
      &lt;item&gt;Content Rendering: Native support for Markdown and simple HTML.&lt;/item&gt;
      &lt;item&gt;Charting: Built-in charts for visualizing your data.&lt;/item&gt;
      &lt;item&gt;Editor: High performance code editor (support up to 200K lines) with LSP (diagnostics, completion, hover, etc).&lt;/item&gt;
      &lt;item&gt;Syntax Highlighting: Syntax highlighting for editor and markdown components using Tree Sitter.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here is the first application: Longbridge Pro, built using GPUI Component.&lt;/p&gt;
    &lt;p&gt;We built multi-theme support in the application. This feature is not included in GPUI Component itself, but is based on the &lt;code&gt;Theme&lt;/code&gt; feature, so it's easy to implement.&lt;/p&gt;
    &lt;p&gt;GPUI and GPUI Component are still in development, so you need to add dependencies by git.&lt;/p&gt;
    &lt;code&gt;gpui = "0.2.2"
gpui-component = "0.3.0"&lt;/code&gt;
    &lt;code&gt;use gpui::*;
use gpui_component::{button::*, *};

pub struct HelloWorld;
impl Render for HelloWorld {
    fn render(&amp;amp;mut self, _: &amp;amp;mut Window, _: &amp;amp;mut Context&amp;lt;Self&amp;gt;) -&amp;gt; impl IntoElement {
        div()
            .v_flex()
            .gap_2()
            .size_full()
            .items_center()
            .justify_center()
            .child("Hello, World!")
            .child(
                Button::new("ok")
                    .primary()
                    .label("Let's Go!")
                    .on_click(|_, _, _| println!("Clicked!")),
            )
    }
}

fn main() {
    let app = Application::new();

    app.run(move |cx| {
        // This must be called before using any GPUI Component features.
        gpui_component::init(cx);

        cx.spawn(async move |cx| {
            cx.open_window(WindowOptions::default(), |window, cx| {
                let view = cx.new(|_| HelloWorld);
                // This first level on the window, should be a Root.
                cx.new(|cx| Root::new(view.into(), window, cx))
            })?;

            Ok::&amp;lt;_, anyhow::Error&amp;gt;(())
        })
        .detach();
    });
}&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;Still early and experimental; there are a lot of limitations.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;GPUI Component has a &lt;code&gt;WebView&lt;/code&gt; element based on Wry. This is an optional feature, which you can enable with a feature flag.&lt;/p&gt;
    &lt;code&gt;gpui-component = { version = "0.3.0", features = ["webview"] }
wry = { version = "0.53.3, package = "lb-wry" }&lt;/code&gt;
    &lt;p&gt;More usage examples can be found in the story directory.&lt;/p&gt;
    &lt;p&gt;GPUI Component has an &lt;code&gt;Icon&lt;/code&gt; element, but it does not include SVG files by default.&lt;/p&gt;
    &lt;p&gt;The example uses Lucide icons, but you can use any icons you like. Just name the SVG files as defined in IconName. You can add any icons you need to your project.&lt;/p&gt;
    &lt;p&gt;We have a gallery of applications built with GPUI Component.&lt;/p&gt;
    &lt;code&gt;cargo run&lt;/code&gt;
    &lt;p&gt;More examples can be found in the &lt;code&gt;examples&lt;/code&gt; directory. You can run them with &lt;code&gt;cargo run --example &amp;lt;example_name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Check out CONTRIBUTING.md for more details.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Features&lt;/cell&gt;
        &lt;cell role="head"&gt;GPUI Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Iced&lt;/cell&gt;
        &lt;cell role="head"&gt;egui&lt;/cell&gt;
        &lt;cell role="head"&gt;QT 6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Language&lt;/cell&gt;
        &lt;cell&gt;Rust&lt;/cell&gt;
        &lt;cell&gt;Rust&lt;/cell&gt;
        &lt;cell&gt;Rust&lt;/cell&gt;
        &lt;cell&gt;C++/QML&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Core Render&lt;/cell&gt;
        &lt;cell&gt;GPUI&lt;/cell&gt;
        &lt;cell&gt;wgpu&lt;/cell&gt;
        &lt;cell&gt;wgpu&lt;/cell&gt;
        &lt;cell&gt;QT&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;License&lt;/cell&gt;
        &lt;cell&gt;Apache 2.0&lt;/cell&gt;
        &lt;cell&gt;MIT&lt;/cell&gt;
        &lt;cell&gt;MIT/Apache 2.0&lt;/cell&gt;
        &lt;cell&gt;Commercial&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Min Binary Size 1&lt;/cell&gt;
        &lt;cell&gt;12MB&lt;/cell&gt;
        &lt;cell&gt;11MB&lt;/cell&gt;
        &lt;cell&gt;5M&lt;/cell&gt;
        &lt;cell&gt;20MB 2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Cross-Platform&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Documentation&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Simple&lt;/cell&gt;
        &lt;cell&gt;Simple&lt;/cell&gt;
        &lt;cell&gt;Good&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Web&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;UI Style&lt;/cell&gt;
        &lt;cell&gt;Modern&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CJK Support&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Bad&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Chart&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Table (Large dataset)&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;p&gt;(Virtual Rows, Columns)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;p&gt;(Virtual Rows)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;p&gt;(Virtual Rows, Columns)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Table Column Resize&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Text base&lt;/cell&gt;
        &lt;cell&gt;Rope&lt;/cell&gt;
        &lt;cell&gt;COSMIC Text 3&lt;/cell&gt;
        &lt;cell&gt;trait TextBuffer 4&lt;/cell&gt;
        &lt;cell&gt;QTextDocument&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CodeEditor&lt;/cell&gt;
        &lt;cell&gt;Simple&lt;/cell&gt;
        &lt;cell&gt;Simple&lt;/cell&gt;
        &lt;cell&gt;Simple&lt;/cell&gt;
        &lt;cell&gt;Basic API&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Dock Layout&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Syntax Highlight&lt;/cell&gt;
        &lt;cell&gt;Tree Sitter&lt;/cell&gt;
        &lt;cell&gt;Syntect&lt;/cell&gt;
        &lt;cell&gt;Syntect&lt;/cell&gt;
        &lt;cell&gt;QSyntaxHighlighter&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Markdown Rendering&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Markdown mix HTML&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HTML Rendering&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Basic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Text Selection&lt;/cell&gt;
        &lt;cell&gt;TextView&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Any Label&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Themes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;I18n&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;Please submit an issue or PR if any mistakes or outdated are found.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Apache-2.0&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Release builds by use simple hello world example. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Iced Editor: https://github.com/iced-rs/iced/blob/db5a1f6353b9f8520c4f9633d1cdc90242c2afe1/graphics/src/text/editor.rs#L65-L68 ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;egui TextBuffer: https://github.com/emilk/egui/blob/0a81372cfd3a4deda640acdecbbaf24bf78bb6a2/crates/egui/src/widgets/text_edit/text_buffer.rs#L20 ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45719004</guid><pubDate>Mon, 27 Oct 2025 09:44:18 +0000</pubDate></item><item><title>Don't forget these tags to make HTML work like you expect</title><link>https://blog.jim-nielsen.com/2025/dont-forget-these-html-tags/</link><description>&lt;doc fingerprint="d752f0500ce6aa0d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Don’t Forget These Tags to Make HTML Work Like You Expect&lt;/head&gt;
    &lt;p&gt;I was watching Alex Petros’ talk and he has a slide in there titled “Incantations that make HTML work correctly”.&lt;/p&gt;
    &lt;p&gt;This got me thinking about the basic snippets of HTML I’ve learned to always include in order for my website to work as I expect in the browser — like “Hey I just made a &lt;code&gt;.html&lt;/code&gt; file on disk and am going to open it in the browser. What should be in there?”&lt;/p&gt;
    &lt;p&gt;This is what comes to mind:&lt;/p&gt;
    &lt;code&gt;&amp;lt;!doctype html&amp;gt;
&amp;lt;html lang="en"&amp;gt;
&amp;lt;meta charset="utf-8"&amp;gt;
&amp;lt;meta name="viewport" content="width=device-width,initial-scale=1.0"&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Why each?&lt;/p&gt;
    &lt;head rend="h2"&gt;doctype&lt;/head&gt;
    &lt;code&gt;&amp;lt;!doctype html&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Without &lt;code&gt;&amp;lt;!doctype html&amp;gt;&lt;/code&gt;, browsers may switch to quirks mode, emulating legacy, pre-standards behavior. This will change how calculations work around layout, sizing, and alignment.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;&amp;lt;!doctype html&amp;gt;&lt;/code&gt; is what you want for consistent rendering. Or &lt;code&gt;&amp;lt;!DOCTYPE HTML&amp;gt;&lt;/code&gt; if you prefer writing markup like it’s 1998. Or even &lt;code&gt;&amp;lt;!doCTypE HTml&amp;gt;&lt;/code&gt; if you eschew all societal norms. It’s case-insensitive so they’ll all work.&lt;/p&gt;
    &lt;head rend="h2"&gt;html lang&lt;/head&gt;
    &lt;code&gt;&amp;lt;html lang="en"&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Declare the document’s language. Browsers, search engines, assistive technologies, etc. can leverage it to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Get pronunciation and voice right for screen readers&lt;/item&gt;
      &lt;item&gt;Improve indexing and translation accuracy&lt;/item&gt;
      &lt;item&gt;Apply locale-specific tools (e.g. spell-checking)&lt;/item&gt;
      &lt;item&gt;And more…&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Omit it and things will look ok, but lots of basic web-adjacent tools might get things wrong. Specifying it makes everything around the HTML work better and more accurately, so I always try to remember to include it.&lt;/p&gt;
    &lt;head rend="h2"&gt;meta utf-8&lt;/head&gt;
    &lt;p&gt;This piece of info can come back from the server as a header, e.g.&lt;/p&gt;
    &lt;code&gt;return new Response(
    "&amp;lt;!doctype html&amp;gt;&amp;lt;h1&amp;gt;Hello world&amp;lt;/h1&amp;gt;",
    {
        status: 200,
        headers: { "Content-Type": "text/html; charset=utf-8" },
    }
);
&lt;/code&gt;
    &lt;p&gt;But I like to set it in my HTML, especially when I’m making files on disk I open manually in the browser.&lt;/p&gt;
    &lt;code&gt;&amp;lt;meta charset="utf-8"&amp;gt;
&lt;/code&gt;
    &lt;p&gt;This tells the browser how to interpret text, ensuring characters like é, ü, and others display correctly.&lt;/p&gt;
    &lt;p&gt;So many times I’ve opened a document without this tag and things just don’t look right — like my smart quotes.&lt;/p&gt;
    &lt;p&gt;For example: copy this snippet, stick it in an HTML file, and open it on your computer:&lt;/p&gt;
    &lt;code&gt;&amp;lt;!doctype html&amp;gt;
&amp;lt;h1&amp;gt;Without meta utf-8&amp;lt;/h1&amp;gt;
&amp;lt;dl&amp;gt;
  &amp;lt;dt&amp;gt;Smart quotes&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;“” and ‘’&amp;lt;/dd&amp;gt;
  &amp;lt;dt&amp;gt;Symbols&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;©, ™, ®, etc.&amp;lt;/dd&amp;gt;
  &amp;lt;dt&amp;gt;Ellipsis&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;…&amp;lt;/dd&amp;gt;
  &amp;lt;dt&amp;gt;Emojis&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;👍&amp;lt;/dd&amp;gt;
  &amp;lt;dt&amp;gt;Non-latin characters&amp;lt;/dt&amp;gt;
  &amp;lt;dd&amp;gt;é, ñ, etc.&amp;lt;/dd&amp;gt;
&amp;lt;/dl&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Things might look a bit wonky. But stick a &lt;code&gt;&amp;lt;meta charset="utf-8"&amp;gt;&lt;/code&gt; tag in there and you’ll find some relief.&lt;/p&gt;
    &lt;head rend="h2"&gt;Meta viewport&lt;/head&gt;
    &lt;code&gt;&amp;lt;meta name="viewport" content="width=device-width,initial-scale=1.0"&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Sometimes I’ll quickly prototype a little HTML and think, “Great it’s working as I expect!” Then I go open it on mobile and everything looks tiny — “[Facepalm] you forgot the meta viewport tag!”&lt;/p&gt;
    &lt;p&gt;Take a look at this screenshot, where I forgot the meta viewport tag on the left but included it on the right:&lt;/p&gt;
    &lt;p&gt;That ever happen to you? No, just me? Well anyway, it’s a good ‘un to include to make HTML work the way you expect.&lt;/p&gt;
    &lt;head rend="h2"&gt;Last But Not Least…&lt;/head&gt;
    &lt;p&gt;I know what you’re thinking, I forgot the most important snippet of them all for writing HTML:&lt;/p&gt;
    &lt;code&gt;&amp;lt;div id="root"&amp;gt;&amp;lt;/div&amp;gt;
&amp;lt;script src="bundle.js"&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Lol.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45719140</guid><pubDate>Mon, 27 Oct 2025 10:01:12 +0000</pubDate></item><item><title>Microsoft needs to open up more about its OpenAI dealings</title><link>https://www.wsj.com/tech/ai/microsoft-needs-to-open-up-more-about-its-openai-dealings-59102de8</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45719669</guid><pubDate>Mon, 27 Oct 2025 11:19:27 +0000</pubDate></item><item><title>You are how you act</title><link>https://boz.com/articles/you-are-how-you-act</link><description>&lt;doc fingerprint="7e0099193ae60539"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You Are How You Act&lt;/head&gt;
    &lt;p&gt;The modern American self is best defined by two Enlightenment thinkers who never met but have been arguing in our heads ever since.&lt;/p&gt;
    &lt;p&gt;Jean-Jacques Rousseau believed in the primacy of the inner self: a core of goodness constantly betrayed by circumstance. In his view, the world corrupts us. We begin pure and only fail because society, obligation, or expectation pulls us away from who we truly are.&lt;/p&gt;
    &lt;p&gt;Benjamin Franklin saw it differently. For him there was no such thing as a good person or a bad person, only people who do good things and people who do bad things. Virtue was a habit, not an essence.&lt;/p&gt;
    &lt;p&gt;Modern America carries both of these ideas, switching between them whenever convenient. We invoke Rousseau when we need forgiveness: I meant well. We invoke Franklin when we need accountability: Show me what you’ve done. It’s an almost entirely incompatible pair of philosophies that coexist perfectly in practice because they’re both so flattering — one to our intentions, the other to our ambition.&lt;/p&gt;
    &lt;p&gt;But only one of them scales.&lt;/p&gt;
    &lt;p&gt;“Fake it until you make it” is often dismissed as shallow, but it’s closer to Franklin’s truth. Faking it long enough is making it. The repetition of behavior, not the sincerity of belief, is what shapes character. You become the kind of person who does the things you repeatedly do.&lt;/p&gt;
    &lt;p&gt;Rousseau invites endless introspection. Franklin invites progress. The first is about how you feel; the second is about what you build.&lt;/p&gt;
    &lt;p&gt;I find the Franklin model far more useful. Not because it’s truer in some cosmic sense, but because it gives you agency. You can’t always change how you feel, but you can always decide what to do next.&lt;/p&gt;
    &lt;p&gt;“It doesn’t take great men to do things, but it is doing things that make men great.” — Arnold Glasow&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45719788</guid><pubDate>Mon, 27 Oct 2025 11:35:58 +0000</pubDate></item><item><title>Artifact (YC W25) is hiring engineers in NYC to build modern ECAD</title><link>https://news.ycombinator.com/item?id=45719996</link><description>&lt;doc fingerprint="fd2f3004b942e881"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Artifact is a CAD tool to collaboratively design electrical systems for complex hardware projects.&lt;/p&gt;
      &lt;p&gt;People are using Artifact to design their supersonic jets, reusable satellites, marine robots, and autonomous drones.&lt;/p&gt;
      &lt;p&gt;We're looking for full-stack software engineers to join us at our office in Manhattan - to help design and build how hardware teams do their electrical systems engineering.&lt;/p&gt;
      &lt;p&gt;Learn more here: https://www.artifact.engineer/&lt;/p&gt;
      &lt;p&gt;If you're interested, shoot me an email with your resume:&lt;/p&gt;
      &lt;p&gt;- Antony antony@artifact.engineer&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45719996</guid><pubDate>Mon, 27 Oct 2025 12:00:30 +0000</pubDate></item></channel></rss>