<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 02 Jan 2026 16:45:48 +0000</lastBuildDate><item><title>Can Bundler be as fast as uv?</title><link>https://tenderlovemaking.com/2025/12/29/can-bundler-be-as-fast-as-uv/</link><description>&lt;doc fingerprint="7685981e496335d9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Can Bundler Be as Fast as uv?&lt;/head&gt;Dec 29, 2025 @ 12:26 pm&lt;p&gt;At RailsWorld earlier this year, I got nerd sniped by someone. They asked “why can’t Bundler be as fast as uv?” Immediately my inner voice said “YA, WHY CAN’T IT BE AS FAST AS UV????”&lt;/p&gt;&lt;p&gt;My inner voice likes to shout at me, especially when someone asks a question so obvious I should have thought of it myself. Since then I’ve been thinking about and investigating this problem, going so far as to give a presentation at XO Ruby Portland about Bundler performance. I firmly believe the answer is “Bundler can be as fast as uv” (where “as fast” has a margin of error lol).&lt;/p&gt;&lt;p&gt;Fortunately, Andrew Nesbitt recently wrote a post called “How uv got so fast”, and I thought I would take this opportunity to review some of the highlights of the post and how techniques applied in uv can (or can’t) be applied to Bundler / RubyGems. I’d also like to discuss some of the existing bottlenecks in Bundler and what we can do to fix them.&lt;/p&gt;&lt;p&gt;If you haven’t read Andrew’s post, I highly recommend giving it a read. I’m going to quote some parts of the post and try to reframe them with RubyGems / Bundler in mind.&lt;/p&gt;&lt;head rend="h2"&gt;Rewrite in Rust?&lt;/head&gt;&lt;p&gt;Andrew opens the post talking about rewriting in Rust:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;uv installs packages faster than pip by an order of magnitude. The usual explanation is âitâs written in Rust.â Thatâs true, but it doesnât explain much. Plenty of tools are written in Rust without being notably fast. The interesting question is what design decisions made the difference.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is such a good quote. I’m going to address “rewrite in Rust” a bit later in the post. But suffice to say, I think if we eliminate bottlenecks in Bundler such that the only viable option for performance improvements is to “rewrite in Rust”, then I’ll call it a success. I think rewrites give developers the freedom to “think outside the box”, and try techniques they might not have tried. In the case of &lt;code&gt;uv&lt;/code&gt;, I think it gave the developers a good way to say “if we don’t have to worry about backwards compatibility, what could we achieve?”.&lt;/p&gt;&lt;p&gt;I suspect it would be possible to write a uv in Python (PyUv?) that approaches the speeds of uv, and in fact much of the blog post goes on to talk about performance improvements that aren’t related to Rust.&lt;/p&gt;&lt;head rend="h2"&gt;Installing code without eval’ing&lt;/head&gt;&lt;quote&gt;&lt;p&gt;pipâs slowness isnât a failure of implementation. For years, Python packaging required executing code to find out what a package needed.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I didn’t know this about Python packages, and it doesn’t really apply to Ruby Gems so I’m mostly going to skip this section.&lt;/p&gt;&lt;p&gt;Ruby Gems are tar files, and one of the files in the tar file is a YAML representation of the GemSpec. This YAML file declares all dependencies for the Gem, so RubyGems can know, without evaling anything, what dependencies it needs to install before it can install any particular Gem. Additionally, RubyGems.org provides an API for asking about dependency information, which is actually the normal way of getting dependency info (again, no &lt;code&gt;eval&lt;/code&gt; required).&lt;/p&gt;&lt;p&gt;There’s only one other thing from this section I’d like to quote:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;PEP 658 (2022) put package metadata directly in the Simple Repository API, so resolvers could fetch dependency information without downloading wheels at all.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Fortunately RubyGems.org already provides the same information about gems.&lt;/p&gt;&lt;p&gt;Reading through the number of PEPs required as well as the amount of time it took to get the standards in place was very eye opening for me. I can’t help but applaud folks in the Python community for doing this. It seems like a mountain of work, and they should really be proud of themselves.&lt;/p&gt;&lt;head rend="h2"&gt;What uv drops&lt;/head&gt;&lt;p&gt;I’m mostly going to skip this section except for one point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Ignoring requires-python upper bounds. When a package says it requires python&amp;lt;4.0, uv ignores the upper bound and only checks the lower. This reduces resolver backtracking dramatically since upper bounds are almost always wrong. Packages declare python&amp;lt;4.0 because they havenât tested on Python 4, not because theyâll actually break. The constraint is defensive, not predictive.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think this is very very interesting. I don’t know how much time Bundler spends on doing “required Ruby version” bounds checking, but it feels like if uv can do it, so can we.&lt;/p&gt;&lt;head rend="h2"&gt;Optimizations that donât need Rust&lt;/head&gt;&lt;p&gt;I really love that Andrew pointed out optimizations that could be made that don’t involve Rust. There are three points in this section that I want to pull out:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Parallel downloads. pip downloads packages one at a time. uv downloads many at once. Any language can do this.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is absolutely true, and is a place where Bundler could improve. Bundler currently has a problem when it comes to parallel downloads, and needs a small architectural change as a fix.&lt;/p&gt;&lt;p&gt;The first problem is that Bundler tightly couples installing a gem with downloading the gem. You can read the installation code here, but I’ll summarize the method in question below:&lt;/p&gt;&lt;code&gt;def install
  path = fetch_gem_if_not_cached
  Bundler::RubyGemsGemInstaller.install path, dest
end
&lt;/code&gt;&lt;p&gt;The problem with this method is that it inextricably links downloading the gem with installing it. This is a problem because we could be downloading gems while installing other gems, but we’re forced to wait because the installation method couples the two operations. Downloading gems can trivially be done in parallel since the &lt;code&gt;.gem&lt;/code&gt; files are just archives that can be fetched independently.&lt;/p&gt;&lt;p&gt;The second problem is the queuing system in the installation code. After gem resolution is complete, and Bundler knows what gems need to be installed, it queues them up for installation. You can find the queueing code here. The code takes some effort to understand. Basically it allows gems to be installed in parallel, but only gems that have already had their dependencies installed.&lt;/p&gt;&lt;p&gt;So for example, if you have a dependency tree like “gem &lt;code&gt;a&lt;/code&gt; depends on gem &lt;code&gt;b&lt;/code&gt; which depends on gem &lt;code&gt;c&lt;/code&gt;” (&lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt;), then no gems will be installed (or downloaded) in parallel.&lt;/p&gt;&lt;p&gt;To demonstrate this problem in an easy-to-understand way, I built a slow Gem server. It generates a dependency tree of &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; (&lt;code&gt;a&lt;/code&gt; depends on &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; depends on &lt;code&gt;c&lt;/code&gt;), then starts a Gem server.
The Gem server takes 3 seconds to return any Gem, so if we point Bundler at this Gem server and then profile Bundler, we can see the impact of the queueing system and download scheme.&lt;/p&gt;&lt;p&gt;In my test app, I have the following Gemfile:&lt;/p&gt;&lt;code&gt;source "http://localhost:9292"

gem "a"
&lt;/code&gt;&lt;p&gt;If we profile Bundle install with Vernier, we can see the following swim lanes in the marker chart:&lt;/p&gt;&lt;p&gt;The above chart is showing that we get no parallelism during installation. We spend 3 seconds downloading the &lt;code&gt;c&lt;/code&gt; gem, then we install it.
Then we spend 3 seconds downloading the &lt;code&gt;b&lt;/code&gt; gem, then we install it.
Finally we spend 3 seconds downloading the &lt;code&gt;a&lt;/code&gt; gem, and we install it.&lt;/p&gt;&lt;p&gt;Timing the &lt;code&gt;bundle install&lt;/code&gt; process shows we take over 9 seconds to install (3 seconds per gem):&lt;/p&gt;&lt;code&gt;&amp;gt; rm -rf x; rm -f Gemfile.lock; time GEM_PATH=(pwd)/x GEM_HOME=(pwd)/x bundle install
Fetching gem metadata from http://localhost:9292/...
Resolving dependencies...
Fetching c 1.0.0
Installing c 1.0.0
Fetching b 1.0.0
Installing b 1.0.0
Fetching a 1.0.0
Installing a 1.0.0
Bundle complete! 1 Gemfile dependency, 3 gems now installed.
Use `bundle info [gemname]` to see where a bundled gem is installed.

________________________________________________________
Executed in   11.80 secs      fish           external
   usr time  341.62 millis  231.00 micros  341.38 millis
   sys time  223.20 millis  712.00 micros  222.49 millis
&lt;/code&gt;&lt;p&gt;Contrast this with a Gemfile containing &lt;code&gt;d&lt;/code&gt;, &lt;code&gt;e&lt;/code&gt;, and &lt;code&gt;f&lt;/code&gt;, which have no dependencies, but still take 3 seconds to download:&lt;/p&gt;&lt;code&gt;source "http://localhost:9292"

gem "d"
gem "e"
gem "f"
&lt;/code&gt;&lt;p&gt;Timing &lt;code&gt;bundle install&lt;/code&gt; for the above Gemfile shows it takes about 4 seconds:&lt;/p&gt;&lt;code&gt;&amp;gt; rm -rf x; rm -f Gemfile.lock; time GEM_PATH=(pwd)/x GEM_HOME=(pwd)/x bundle install
Fetching gem metadata from http://localhost:9292/.
Resolving dependencies...
Fetching d 1.0.0
Fetching e 1.0.0
Fetching f 1.0.0
Installing e 1.0.0
Installing f 1.0.0
Installing d 1.0.0
Bundle complete! 3 Gemfile dependencies, 3 gems now installed.
Use `bundle info [gemname]` to see where a bundled gem is installed.

________________________________________________________
Executed in    4.14 secs      fish           external
   usr time  374.04 millis    0.38 millis  373.66 millis
   sys time  368.90 millis    1.09 millis  367.81 millis
&lt;/code&gt;&lt;p&gt;We were able to install the same number of gems in a fraction of the time. This is because Bundler is able to download siblings in the dependency tree in parallel, but unable to handle other relationships.&lt;/p&gt;&lt;p&gt;There is actually a good reason that Bundler insists dependencies are installed before the gems themselves: native extensions. When installing native extensions, the installation process must run Ruby code (the &lt;code&gt;extconf.rb&lt;/code&gt; file).
Since the &lt;code&gt;extconf.rb&lt;/code&gt; could require dependencies be installed in order to run, we must install dependencies first.
For example &lt;code&gt;nokogiri&lt;/code&gt; depends on &lt;code&gt;mini_portile2&lt;/code&gt;, but &lt;code&gt;mini_portile2&lt;/code&gt; is only used during the installation process, so it needs to be installed before &lt;code&gt;nokogiri&lt;/code&gt; can be compiled and installed.&lt;/p&gt;&lt;p&gt;However, if we were to decouple downloading from installation it would be possible for us to maintain the “dependencies are installed first” business requirement but speed up installation. In the &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case, we could have been downloading gems &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; at the same time as gem &lt;code&gt;c&lt;/code&gt; (or even while waiting on &lt;code&gt;c&lt;/code&gt; to be installed).&lt;/p&gt;&lt;p&gt;Additionally, pure Ruby gems don’t need to execute any code on installation. If we knew that we were installing a pure Ruby gem, it would be possible to relax the “dependencies are installed first” business requirement and get even more performance increases. The above &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case could install all three gems in parallel since none of them execute Ruby code during installation.&lt;/p&gt;&lt;p&gt;I would propose we split installation in to 4 discrete steps:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Download the gem&lt;/item&gt;&lt;item&gt;Unpack the gem&lt;/item&gt;&lt;item&gt;Compile the gem&lt;/item&gt;&lt;item&gt;Install the gem&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Downloading and unpacking can be done trivially in parallel. We should unpack the gem to a temporary folder so that if the process crashes or the machine loses power, the user isn’t stuck with a half-installed gem. After we unpack the gem, we can discover whether the gem is a native extension or not. If it’s not a native extension, we “install” the gem simply by moving the temporary folder to the “correct” location. This step could even be a “hard link” step as discussed in the next point.&lt;/p&gt;&lt;p&gt;If we discover that the gem is a native extension, then we can “pause” installation of that gem until its dependencies are installed, then resume (by compiling) at an appropriate time.&lt;/p&gt;&lt;p&gt;Side note: &lt;code&gt;gel&lt;/code&gt;, a Bundler alternative, works mostly in this manner today.
Here is a timing of the &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case from above:&lt;/p&gt;&lt;code&gt;&amp;gt; rm -f Gemfile.lock; time gel install
Fetching sources....
Resolving dependencies...
Writing lockfile to /Users/aaron/git/gemserver/app/Gemfile.lock
Installing c (1.0.0) 
Installing a (1.0.0)
Installing b (1.0.0)
Installed 3 gems  

________________________________________________________
Executed in    4.07 secs      fish           external
   usr time  289.22 millis    0.32 millis  288.91 millis
   sys time  347.04 millis    1.36 millis  345.68 millis
&lt;/code&gt;&lt;p&gt;Lets move on to the next point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Global cache with hardlinks. pip copies packages into each virtual environment. uv keeps one copy globally and uses hardlinks&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think this is a great idea, but I’d actually like to split the idea in two. First, RubyGems and Bundler should have a combined, global cache, full stop. I think that global cache should be in &lt;code&gt;$XDG_CACHE_HOME&lt;/code&gt;, and we should store &lt;code&gt;.gem&lt;/code&gt; files there when they are downloaded.&lt;/p&gt;&lt;p&gt;Currently, both Bundler and RubyGems will use a Ruby version specific cache folder. In other words, if you do &lt;code&gt;gem install rails&lt;/code&gt; on two different versions of Ruby, you get two copies of Rails and all its dependencies.&lt;/p&gt;&lt;p&gt;Interestingly, there is an open ticket to implement this, it just needs to be done.&lt;/p&gt;&lt;p&gt;The second point is hardlinking on installation. The idea here is that rather than unpacking the gem multiple times, once per Ruby version, we simply unpack once and then hard link per Ruby version. I like this idea, but I think it should be implemented after some technical debt is paid: namely implementing a global cache and unifying Bundler / RubyGems code paths.&lt;/p&gt;&lt;p&gt;On to the next point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;PubGrub resolver&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Actually Bundler already uses a Ruby implementation of the PubGrub resolver. You can see it here. Unfortunately, RubyGems still uses the molinillo resolver.&lt;/p&gt;&lt;p&gt;In other words you use a different resolver depending on whether you do &lt;code&gt;gem install&lt;/code&gt; or &lt;code&gt;bundle install&lt;/code&gt;.
I don’t really think this is a big deal since the vast majority of users will be doing &lt;code&gt;bundle install&lt;/code&gt; most of time.
However, I do think this discrepancy is some technical debt that should be addressed, and I think this should be addressed via unification of RubyGems and Bundler codebases (today they both live in the same repository, but the code isn’t necessarily combined).&lt;/p&gt;&lt;p&gt;Lets move on to the next section of Andrew’s post:&lt;/p&gt;&lt;head rend="h2"&gt;Where Rust actually matters&lt;/head&gt;&lt;p&gt;Andrew first mentions “Zero-copy deserialization”. This is of course an important technique, but I’m not 100% sure where we would utilize it in RubyGems / Bundler. I think that today we parse the YAML spec on installation, and that could be a target. But I also think we could install most gems without looking at the YAML gemspec at all.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Thread-level parallelism. Pythonâs GIL forces parallel work into separate processes, with IPC overhead and data copying.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is an interesting point. I’m not sure what work pip needed to do in separate processes. Installing a pure Ruby, Ruby Gem is mostly an IO bound task, with some ZLIB mixed in. Both of these things (IO and ZLIB processing) release Ruby’s GVL, so it’s possible for us to do things truly in parallel. I imagine this is similar for Python / pip, but I really have no idea.&lt;/p&gt;&lt;p&gt;Given the stated challenges with Python’s GIL, you might wonder whether Ruby’s GVL presents similar parallelism problems for Bundler. I don’t think so, and in fact I think Ruby’s GVL gets kind of a bad rap. It prevents us from running CPU bound Ruby code in parallel. Ractors address this, and Bundler could possibly leverage them in the future, but since installing Gems is mostly an IO bound task I’m not sure what the advantage would be (possibly the version solver, but I’m not sure what can be parallelized in there). The GVL does allow us to run IO bound work in parallel with CPU bound Ruby code. CPU bound native extensions are allowed to release the GVL, allowing Ruby code to run in parallel with the native extension’s CPU bound code.&lt;/p&gt;&lt;p&gt;In other words, Ruby’s GVL allows us to safely run work in parallel. That said, the GVL can work against us because releasing and acquiring the GVL takes time.&lt;/p&gt;&lt;p&gt;If you have a system call that is very fast, releasing and acquiring the GVL could end up being a large percentage of that call. For example, if you do &lt;code&gt;File.binwrite(file, buffer)&lt;/code&gt;, and the buffer is very small, you could encounter a situation where GVL book keeping is the majority of the time.
A bummer is that Ruby Gem packages usually contain lots of very small files, so this problem could be impacting us.
The good news is that this problem can be solved in Ruby itself, and indeed some work is being done on it today.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;No interpreter startup. Every time pip spawns a subprocess, it pays Pythonâs startup cost.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Obviously Ruby has this same problem. That said, we only start Ruby subprocesses when installing native extensions. I think native extensions make up the minority of gems installed, and even when installing a native extension, it isn’t Ruby startup that is the bottleneck. Usually the bottleneck is compilation / linking time (as we’ll see in the next post).&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Compact version representation. uv packs versions into u64 integers where possible, making comparison and hashing fast.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is a cool optimization, but I don’t think it’s actually Rust specific. Comparing integers is much faster than comparing version objects. The idea is that you take a version number, say &lt;code&gt;1.0.0&lt;/code&gt;, and then pack each part of the version in to a single integer.
For example, we could represent &lt;code&gt;1.0.0&lt;/code&gt; as &lt;code&gt;0x0001_0000_0000_0000&lt;/code&gt; and &lt;code&gt;1.1.0&lt;/code&gt; as &lt;code&gt;0x0001_0001_0000_0000&lt;/code&gt;, etc.&lt;/p&gt;&lt;p&gt;It should be possible to use this trick in Ruby and encode versions to integer immediates, which would unlock performance in the resolver. Rust has an advantage here - compiled native code comparing u64s will always be faster than Ruby, even with immediates. However, I would bet that with the YJIT or ZJIT in play, this gap could be closed enough that no end user would notice the difference between a Rust or Ruby implementation of Bundler.&lt;/p&gt;&lt;p&gt;I started refactoring the &lt;code&gt;Gem::Version&lt;/code&gt; object so that we might start doing this, but we ended up reverting it because of backwards compatibility (I am jealous of &lt;code&gt;uv&lt;/code&gt; in that regard).
I think the right way to do this is to refactor the solver entry point and ensure all version requirements are encoded as integer immediates before entering the solver.
We could keep the &lt;code&gt;Gem::Version&lt;/code&gt; API as “user facing” and design a more internal API that the solver uses.
I am very interested in reading the version encoding scheme in uv.
My intuition is that minor numbers tend to get larger than major numbers, so would minor numbers have more dedicated bits?
Would it even matter with 64 bits?&lt;/p&gt;&lt;head rend="h2"&gt;Wrapping this up&lt;/head&gt;&lt;p&gt;I’m going to quote Andrew’s last 2 paragraphs:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;uv is fast because of what it doesnât do, not because of what language itâs written in. The standards work of PEP 518, 517, 621, and 658 made fast package management possible. Dropping eggs, pip.conf, and permissive parsing made it achievable. Rust makes it a bit faster still.&lt;/p&gt;&lt;p&gt;pip could implement parallel downloads, global caching, and metadata-only resolution tomorrow. It doesnât, largely because backwards compatibility with fifteen years of edge cases takes precedence. But it means pip will always be slower than a tool that starts fresh with modern assumptions.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think these are very good points. The difference is that in RubyGems and Bundler, we already have the infrastructure in place for writing a “fast as uv” package manager. The difficult part is dealing with backwards compatibility, and navigating two legacy codebases. I think this is the real advantage the uv developers had. That said, I am very optimistic that we could “repair the plane mid-flight” so to speak, and have the best of both worlds: backwards compatibility and speed.&lt;/p&gt;&lt;p&gt;I mentioned at the top of the post I would address “rewrite it in Rust”, and I think Andrew’s own quote mostly does that for me. I think we could have 99% of the performance improvements while still maintaining a Ruby codebase. Of course if we rewrote it in Rust, you could squeeze an extra 1% out, but would it be worthwhile? I don’t think so.&lt;/p&gt;&lt;p&gt;I have a lot more to say about this topic, and I feel like this post is getting kind of long, so I’m going to end it here. Please look out for part 2, which I’m tentatively calling “What makes Bundler / RubyGems slow?” This post was very “can we make RubyGems / Bundler do what uv does?” (the answer is “yes”). In part 2 I want to get more hands-on by discussing how to profile Bundler and RubyGems, what specifically makes them slow in the real world, and what we can do about it.&lt;/p&gt;&lt;p&gt;I want to end this post by saying “thank you” to Andrew for writing such a great post about how uv got so fast.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46458302</guid><pubDate>Thu, 01 Jan 2026 21:37:10 +0000</pubDate></item><item><title>Why users cannot create Issues directly</title><link>https://github.com/ghostty-org/ghostty/issues/3558</link><description>&lt;doc fingerprint="d3525efe9c762c38"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 1.4k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Users are not allowed to create Issues directly in this repository - we ask that you create a Discussion first.&lt;/p&gt;
    &lt;p&gt;Unlike some other projects, Ghostty does not use the issue tracker for discussion or feature requests. Instead, we use GitHub discussions for that. Once a discussion reaches a point where a well-understood, actionable item is identified, it is moved to the issue tracker. This pattern makes it easier for maintainers or contributors to find issues to work on since every issue is ready to be worked on.&lt;/p&gt;
    &lt;p&gt;This approach is based on years of experience maintaining open source projects and observing that 80-90% of what users think are bugs are either misunderstandings, environmental problems, or configuration errors by the users themselves. For what's left, the majority are often feature requests (unimplemented features) and not bugs (malfunctioning features). Of the features requests, almost all are underspecified and require more guidance by a maintainer to be worked on.&lt;/p&gt;
    &lt;p&gt;Any Discussion which clearly identifies a problem in Ghostty and can be confirmed or reproduced will be converted to an Issue by a maintainer, so as a user finding a valid problem you don't do any extra work anyway. Thank you.&lt;/p&gt;
    &lt;p&gt;For more details, see our CONTRIBUTING.md.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460319</guid><pubDate>Fri, 02 Jan 2026 01:24:51 +0000</pubDate></item><item><title>Happy Public Domain Day 2026</title><link>https://publicdomainreview.org/blog/2026/01/public-domain-day-2026/</link><description>&lt;doc fingerprint="3081d05f80b58200"&gt;
  &lt;main&gt;
    &lt;p&gt;The calendar turns, and once again a lively procession of books, images, films, and music leaves copyright behind and steps into the ever-growing public domain! On this year's Public Domain Day (which falls each January 1st) we welcome, in lots of countries around the world, the words of Wallace Stevens, Thomas Mann, Hannah Arendt, and Albert Einstein, and in the US a bevy of brilliant books including William Faulkner’s As I Lay Dying, Langston Hughes’ Not Without Laughter, Agatha Christie’s The Murder at the Vicarage, and, in their original German, Robert Musil’s The Man Without Qualities and Hermann Hesse’s Narcissus and Goldmund.&lt;/p&gt;
    &lt;p&gt;Due to differing copyright laws around the world, there is no one single public domain, but there are three main types of copyright term for historical works which cover most cases. For these three systems, newly entering the public domain today are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;works by people who died in 1955, for countries with a copyright term of “life plus 70 years” (relevant in UK, most of the EU, and South America);&lt;/item&gt;
      &lt;item&gt;works by people who died in 1975, for countries with a term of “life plus 50 years” (relevant to most of Africa and Asia);&lt;/item&gt;
      &lt;item&gt;films and books (incl. artworks featured) published in 1929 (relevant solely to the United States).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some of you may have been following our advent-style countdown calendar which revealed day-by-day through December our highlights for these new public domain entrants. The last window was opened yesterday, and while such a format was fun for the slow reveal, for the sake of a good gorgeable list we’ve exploded the calendar out into a digestible array below. Enjoy!&lt;/p&gt;
    &lt;head rend="h2"&gt;Entering the public domain in the US&lt;/head&gt;
    &lt;head rend="h3"&gt;William Faulkner – As I Lay Dying&lt;/head&gt;
    &lt;p&gt;As I Lay Dying is a Southern Gothic novel by American author William Faulkner, consistently ranked among the best novels of the 20th century. The title is derived from William Marris’s 1925 translation of Homer’s Odyssey, referring to the similar themes of both works.&lt;lb/&gt;The novel traces the story of the death of Addie Bundren and her poor, rural family’s quest to honor her wish to be buried in her hometown of Jefferson, Mississippi, as well as the motives—noble or selfish—they show on the journey. It uses a stream-of-consciousness writing technique and varying chapter lengths, and is narrated by 15 different characters over 59 chapters.&lt;lb/&gt;Faulkner said that he wrote the novel from midnight to 4:00 a.m. over the course of six weeks and that he did not change a word of it. He spent the first eight hours of his twelve-hour shift at the University of Mississippi Power House shoveling coal or directing other works and the remaining four hours handwriting his manuscript on unlined onionskin paper. As I Lay Dying represents a progenitor of the Southern Renaissance, reflecting on being, existence, and other existential metaphysics of everyday life, and helped to solidify Faulkner’s reputation as a pioneer, like James Joyce and Virginia Woolf, of stream of consciousness. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Arthur Ransome - Swallows and Amazons&lt;/head&gt;
    &lt;p&gt;Swallows and Amazons is a children’s adventure novel by English author Arthur Ransome. It is the first book in the Swallows and Amazons series, followed by Swallowdale.&lt;lb/&gt;Set in the summer of 1929 in England’s Lake District, the book relates the outdoor adventures and play of two families of children. These involve sailing, camping, fishing, exploration and piracy. The Walker children (John, Susan, Titty and Roger) are staying at a farm near a lake in the Lake District of England, during the school holidays. They sail a borrowed dinghy named Swallow and meet the Blackett children (Nancy and Peggy), who sail a dinghy named Amazon. When the children meet, they agree to join forces against a common enemy – the Blacketts’ uncle Jim Turner whom they call “Captain Flint” (after the parrot in Treasure Island).&lt;lb/&gt;The book was inspired by a summer spent by Ransome teaching the children of his friends, the Altounyans, to sail. At the time, Ransome had been working as a journalist with the Manchester Guardian, but decided to become a full-time author rather than go abroad as a foreign correspondent. Three of the Altounyan children’s names are adopted directly for the Walker family. However, later in life Ransome tried to downplay the Altounyan connections, changing the initial dedication of Swallows and Amazons and writing a new foreword which gave other sources. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Nan Shepherd – The Weatherhouse&lt;/head&gt;
    &lt;p&gt;The Weatherhouse is the second novel by Anna “Nan” Shepherd, a Scottish modernist writer and poet. The novel concerns interactions between people in a small rural Scottish community. It belongs to the great line of Scottish fiction dealing with the complex interactions of small communities, and especially the community of women — a touching and hilarious network of mothers, daughters, spinsters and widows. It is also a striking meditation on the nature of truth, the power of human longing and the mystery of being.&lt;lb/&gt;Shepherd published three works of fiction. Her short non-fiction book The Living Mountain, inspired by her love for hillwalking, is the book for which she is best known and has been quoted as an influence by prominent nature writers. The landscape and weather of this area play a major role in her novels and provide a focus for her poetry.&lt;lb/&gt;Shepherd’s fiction brings out the sharp conflict between the demands of tradition and the pull of modernity, particularly in women’s lives. All three novels assign a major role to the landscape and weather in small northern Scottish communities they describe. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Langston Hughes – Not Without Laughter&lt;/head&gt;
    &lt;p&gt;Not Without Laughter* is the debut novel of Langston Hughes, the American writer, activist, and leader of the Harlem Renaissance.&lt;lb/&gt;The novel portrays African-American life in Kansas in the 1910s, focusing on the effects of class and religion on the community. In telling the story of Sandy Rogers, a young African American boy in small-town Kansas, and of his family—his mother, Annjee, a housekeeper for a wealthy white family; his irresponsible father, Jimboy, who plays the guitar and travels the country in search of employment; his strong-willed grandmother Hager, who clings to her faith; his Aunt Tempy, who marries a rich man; and his Aunt Harriet, who struggles to make it as a blues singer—Hughes gives the longings and lineaments of Black life in the early twentieth century an important place in the history of racially divided America.&lt;lb/&gt;Hughes said that *Not Without Laughter* is semi-autobiographical, and that a good portion of the characters and setting included in the novel are based on his memories of growing up in Lawrence, Kansas. A review in *The New York Times* said that the novel is “very slow, even tedious, reading in its early chapters, but once it gains its momentum it moves as swiftly as a jazz rhythm”. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Hermann Hesse – Narcissus and Goldmund&lt;/head&gt;
    &lt;p&gt;Narcissus and Goldmund (in German, Narziß und Goldmund), also published in English as Death and the Lover, is a novel written by the German-Swiss author Hermann Hesse. At its publication, it was considered Hesse’s literary triumph.&lt;lb/&gt;The novel is the story of a young man, Goldmund (German for “Gold mouth”), who wanders aimlessly throughout Medieval Germany after leaving a Catholic monastery school in search of what could be described as “the meaning of life”. With the help of Narcissus, a gifted young teacher, and following an epiphanic experience with a beautiful Gypsy woman, Goldmund leaves the monastery and embarks on a wandering existence. He has numerous love affairs, studies art, and encounters human existence at its ugliest when the Black Death devastates the region. Eventually, he is reunited with his friend Narcissus, now an abbot.&lt;lb/&gt;Like most of Hesse’s works, the main theme of this book is the wanderer’s struggle to find himself, as well as the Jungian union of polar opposites (Mysterium Coniunctionis). Goldmund represents nature and the “feminine conscious mind” (but also anima, a man’s unconscious), while Narcissus represents science and logic and God and the “masculine conscious mind” (but also animus, a woman’s unconscious).&lt;lb/&gt;A film adaptation, directed by the Austrian Oscar-winning director Stefan Ruzowitzky, was released in 2020. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;All Quiet on the Western Front (1930 film)&lt;/head&gt;
    &lt;p&gt;All Quiet on the Western Front is a 1930 American pre-Code epic anti-war film based on the 1929 novel of the same name by German novelist Erich Maria Remarque. Directed by Lewis Milestone, it stars Lew Ayres, Louis Wolheim, John Wray, Slim Summerville, and William Bakewell.&lt;lb/&gt;The movie follows a group of German students moved to enlist in the army as part of the new 2nd Company. Their romantic delusions are quickly shattered during their brief but rigorous training under the abusive Sergeant Himmelstoss. After being sent to the Western Front, their idealism is destroyed by the harsh realities of combat.&lt;lb/&gt;Considered a realistic and harrowing account of warfare in World War I, the film opened to wide acclaim in the United States and made the American Film Institute’s first 100 Years... 100 Movies list in 1997. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page&lt;/p&gt;
    &lt;head rend="h3"&gt;Evelyn Waugh – Vile Bodies&lt;/head&gt;
    &lt;p&gt;Vile Bodies is the second novel by Arthur Evelyn St. John Waugh, an English writer of novels, biographies, and travel books, and a prolific journalist and book reviewer. It satirises London’s post–First World War “bright young things” — a group of Bohemian young aristocrats and socialites in London — and the press coverage around them. Waugh originally considered the title Bright Young Things but changed it; the published title echoes a narrator’s remark on crowds and parties: “Those vile bodies”.&lt;lb/&gt;The novel follows a vivid assortment of characters, among them the struggling writer Adam Fenwick-Symes and the glamorous, aristocratic Nina Blount, who hunt fast and furiously for ever greater sensations and the hedonistic fulfillment of their desires. Waugh’s acidly funny satire reveals the darkness and vulnerability beneath the sparkling surface of the high life.&lt;lb/&gt;The book shifts in tone from light-hearted romp to bleak desolation (Waugh himself later attributed it to the breakdown of his first marriage halfway through the book’s composition). Critics have noted the novel’s fragmented scenes, jump-cuts, and telephone dialogue, often linking its method to cinema and to modernist effects. Some have defended the novel’s downbeat ending as a poetically just reversal of the conventions of comic romance.&lt;lb/&gt;David Bowie cited the novel as the primary influence in writing his song “Aladdin Sane”, and a film adaptation, written and directed by Stephen Fry, was released in 2003. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Margaret Ayer Barnes - Years of Grace&lt;/head&gt;
    &lt;p&gt;Years of Grace is the first book by the American playwright, novelist, and short-story writer Margaret Ayer Barnes. It won the Pulitzer Prize for the Novel in 1931.&lt;lb/&gt;The story, beginning in the 1890s and continuing into the 1930s, chronicles the life of Jane Ward Carver from her teens to age 54. This novel follows many of the same themes as Barnes’s other works. Centering on the social manners of upper middle class society, her female protagonists are often traditionalists, struggling to uphold conventional morality in the face of changing social climates. Barnes’s alma mater Bryn Mawr College, along with the characters of college presidents M. Carey Thomas and Marion Park, figure prominently in this work.&lt;lb/&gt;The New York Times commented that “this story of the death of an old order and the birth of a new one, of the perpetually renewed conflict between succeeding generations... holds the reader’s attention to the end.” Despite the success of Years of Grace, it is not Barnes’s best-known work; that honor belongs to Dishonored Lady, a play she co-wrote with Edward Sheldon, which was adapted twice into film. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read free ebook through Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Hellbound Train&lt;/head&gt;
    &lt;p&gt;Hell-Bound Train is a 1930 film written and directed by James and Eloyce Gist. A self-taught husband-and-wife team with a shared religious mission, they produced at least three silent films for African American church audiences, touring them across the United States. Shown alongside sermons, these works used cinema as a vehicle for evangelism. In Hell-Bound Train — which Eloyce is said to have rewritten, re-edited, and partly refilmed after James’s initial version — the viewer passes from carriage to carriage as the filmmakers stage various “Jazz Age” sins, including dancing, drinking, and gambling, all overseen by a mischievous devil conductor. Though Hell-Bound Train has gained some renewed attention via Kino Lorber’s Pioneers of African-American Cinema box set and a brief run on the Criterion Channel, this film — one of the few surviving silent works by an African American woman — is still often absent from retrospectives on early women filmmakers, perhaps because of its modest production values and overtly moralizing tone. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Watch on YouTube&lt;/p&gt;
    &lt;head rend="h3"&gt;Robert Musil – The Man Without Qualities&lt;/head&gt;
    &lt;p&gt;The Man Without Qualities (in German Der Mann ohne Eigenschaften) is an unfinished modernist novel in three volumes and various drafts, by the Austrian writer Robert Musil, published in parts from 1930 to 1943.&lt;lb/&gt;The novel is a “story of ideas”, which takes place in the time of the Austro-Hungarian monarchy’s last days. The plot often veers into allegorical digressions on a wide range of existential themes concerning humanity and feelings. It has a particular concern with the values of truth and opinion and how society organizes ideas about life and society. The book is well over a thousand pages long in its entirety, and no one single theme dominates.&lt;lb/&gt;The story takes place in 1913 in Vienna, the capital of Austria-Hungary, which Musil refers to by the playful term Kakanien. Part I, titled A Sort of Introduction, is an introduction to the protagonist, a mathematician named Ulrich whose ambivalence towards morals and indifference to life make him “a man without qualities”. In Part II, Pseudoreality Prevails, Ulrich joins preparations for a celebration in honor of 70 years of the Austrian Emperor Franz Joseph’s reign. Part III, entitled Into the Millennium (The Criminals), is about Ulrich’s sister Agathe. They experience a mystically incestuous stirring upon meeting after their father’s death.&lt;lb/&gt;Musil worked on the novel for more than twenty years: his detailed portrait of a decaying fin de siècle world has strong autobiographical features. Musil’s almost daily preoccupation with writing left his family in dire financial straits. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read German original on Project Gutenberg&lt;/p&gt;
    &lt;head rend="h3"&gt;T. S. Eliot – Ash Wednesday&lt;/head&gt;
    &lt;p&gt;Ash Wednesday is a long poem written by T. S. Eliot during his 1927 conversion to Anglicanism. Published in 1930, the poem deals with the struggle that ensues when one who has lacked faith in the past strives to move towards God.&lt;lb/&gt;Sometimes referred to as Eliot’s “conversion poem”, Ash Wednesday, with a base of Dante’s Purgatorio, is richly but ambiguously allusive and deals with the move from spiritual barrenness to hope for human salvation. The style is different from his poetry which predates his conversion. Ash Wednesday and the poems that followed had a more casual, melodic, and contemplative method.&lt;lb/&gt;The poem’s title comes from the Western Christian fast day that marks the beginning of Lent, forty days before Easter. It is a poem about the difficulty of religious belief, and concerned with personal salvation in an age of uncertainty. In it, Eliot’s poetic persona, one who has lacked faith in the past, has somehow found the courage, through spiritual exhaustion, to seek faith.&lt;lb/&gt;The initial reception of Ash Wednesday was largely positive, though many of the more secular literati found its groundwork of orthodox Christianity discomfiting. Edwin Muir maintained that “‘Ash Wednesday’ is one of the most moving poems he [Eliot] has written, and perhaps the most perfect”. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on English Verse and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Agatha Christie - The Murder at the Vicarage&lt;/head&gt;
    &lt;p&gt;The Murder at the Vicarage is a work of detective fiction by the British writer Agatha Christie. It is the first novel to feature the character of Miss Marple and her village of St Mary Mead (characters that had previously appeared in short stories).&lt;lb/&gt;The story is set in the quiet English village of St Mary Mead, where life is seemingly peaceful until Colonel Protheroe, the local magistrate and a widely disliked man, is found shot dead in the vicar’s study. The vicar, Leonard Clement, is the narrator of the story. Just before the murder, he had remarked that “anyone who murdered Colonel Protheroe would be doing the world a service” — a comment that comes back to haunt him.&lt;lb/&gt;Several suspects quickly emerge, as well as Miss Marple, who proves, though she appears at first as a nosy old spinster, to have unmatched observational skills and a deep understanding of human nature. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read free ebook through Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Franz Kafka - The Castle (english translation)&lt;/head&gt;
    &lt;p&gt;The Castle (in German, *Das Schloss*) is a 1926 novel by Franz Kafka. In it a protagonist known only as “K.” arrives in a village and struggles to gain access to the mysterious authorities who govern it from a castle supposedly owned by Count Westwest. Kafka died before he could finish the work, but suggested it would end with K. dying in the village, the castle notifying him on his death bed that his “legal claim to live in the village was not valid, yet, taking certain auxiliary circumstances into account, he was permitted to live and work there.” Dark and at times surreal, *The Castle* is often understood to be about alienation, unresponsive bureaucracy, the frustration of trying to conduct business with non-transparent, seemingly arbitrary controlling systems, and the futile pursuit of an unobtainable goal. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read free ebook through Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Sigmund Freud – Civilization and Its Discontents&lt;/head&gt;
    &lt;p&gt;Civilization and Its Discontents is a book by Sigmund Freud, the founder of psychoanalysis. It was written in 1929 and first published in German in 1930 as Das Unbehagen in der Kultur (“The Uneasiness in Civilization”).&lt;lb/&gt;Exploring what Freud saw as a clash between the desire for individuality and the expectations of society, the book is considered one of Freud’s most important and widely read works, and was described in 1989 by historian Peter Gay as one of the most influential and studied books in the field of modern psychology.&lt;lb/&gt;The book espouses a theory grounded in the notion that humans have certain characteristic instincts that are immutable. The primary tension originates from an individual attempting to find instinctive freedom, and civilization’s contrary demand for conformity and repression of instincts. Freud states that when any situation that is desired by the pleasure principle is prolonged, it creates a feeling of mild resentment as it clashes with the reality principle.&lt;lb/&gt;Primitive instincts—for example, the desire to kill and the insatiable craving for sexual gratification—are harmful to the collective wellbeing of a human community. The historical development of laws that prohibit violence, murder, rape, and adultery, he argued, is an inherent quality of civilization that gives rise to perpetual feelings of discontent among individuals. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Stella Benson - The Far-Away Bride&lt;/head&gt;
    &lt;p&gt;The Far-Away Bride is the most famous book by the English feminist, novelist, poet, and travel writer Stella Benson. It was published in the United States first in 1930 and as Tobit Transplanted in Britain in 1931. It won the Femina Vie Heureuse Prize for English writers in 1932.&lt;lb/&gt;The novel deals with a family of Russian émigrés in Manchuria. Its characters are the old, grumbling and tearfully sentimental Russian intellectual, Malinin; his disheveled, kind-hearted and unbearable wife, Anna; and Seryozha, their resourceful 19-year-old son. Spending their time in laziness, indulging in exaggerated Russian disorder and comical quarrels growing out of every trifle, they are incongruously happy. The humorous and adventurous action of the novel starts when Seryozha sets out, on foot, on a business trip to the Korean city of Seoul (where he must recover 200 yens); it is there that he finds his “far-away bride” — a charming and whimsical Russian girl who has already broken seven hearts and whose heart he finally conquers.&lt;lb/&gt;Benson described the novel as an “accurate modernization” of the Book of Tobit, a work of Second Temple Jewish literature dating to the 3rd or early 2nd century BC; The New York Times described The Far-Away Bride, rather, as a “spirited parody of it.” Benson’s novel, writes the reviewer, is “a truly felicitous comedy of the human personality”. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Vladimir Nabokov - The Defense&lt;/head&gt;
    &lt;p&gt;The Defense (in Russian, Zashchita Luzhinais) is the third novel written by Vladimir Nabokov after he had emigrated to Berlin. It appeared first under Nabokov’s pen name V. Sirin in the Russian émigré quarterly Sovremennye zapiski and was thereafter published by the émigré publishing house Slovo as The Luzhin Defense in Berlin.&lt;lb/&gt;The novel tells the story of Luzhin. As a young boy, unattractive, withdrawn, sullen, he takes up chess as a refuge from the anxiety of his everyday life. His talent is prodigious and he rises to the rank of grandmaster, but at a cost: in Luzhin’s obsessive mind, the game of chess gradually supplants the world of reality. His own world falls apart during a crucial championship match, when the intricate defense he has devised withers under his opponent’s unexpected and unpredictable lines of assault.&lt;lb/&gt;The character of Luzhin is based on Curt von Bardeleben, a chess master Nabokov knew personally, and Nabokov links the events in the central chapters to moves as encountered in chess problems. The book was adapted to film in 2000, as The Luzhin Defence. It was directed by Marleen Gorris, and starred John Turturro as Luzhin. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Dashiell Hammett – The Maltese Falcon&lt;/head&gt;
    &lt;p&gt;The Maltese Falcon is a detective novel by American writer Dashiell Hammett, originally serialized in the magazine Black Mask beginning with the September 1929 issue. The story is told entirely in external third-person narrative; there is no description whatsoever of any character’s thoughts or feelings, only what they say and do, and how they look. The novel has been adapted several times for the cinema and is considered part of the hardboiled genre, which Hammett played a major part in popularizing.&lt;lb/&gt;The novel follows Sam Spade, a private detective in San Francisco, in partnership with Miles Archer. The beautiful “Miss Wonderley” hires them to follow Floyd Thursby, who she claims has run off with her sister. Archer takes the first stint but is found shot dead that night. “Miss Wonderley” is soon revealed to be an acquisitive adventuress named Brigid O’Shaughnessy, who is involved in the search for a black statuette of unknown but substantial value. Red herrings abound.&lt;lb/&gt;Although Hammett himself worked for a time as a private detective for the Pinkerton Detective Agency in San Francisco (and used his given name, Samuel, for the story’s protagonist), Hammett asserted that “Spade has no original. He is a dream man in the sense that he is what most of the private detectives I worked with would like to have been, and, in their cockier moments, thought they approached.” (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Stanisław Ignacy Witkiewicz – Insatiability&lt;/head&gt;
    &lt;p&gt;Insatiability (in Polish Nienasycenie) is a speculative fiction novel by the Polish writer, dramatist, philosopher, painter and photographer, Stanisław Ignacy Witkiewicz (Witkacy). It is Witkiewicz’s third novel, considered by some to be his best.&lt;lb/&gt;Consisting of two parts — Przebudzenie (Awakening) and Obłęd (The Madness) — the novel takes place in the future, circa 2000. Following a battle, modeled after the Bolshevik revolution, Poland is overrun by the army of the last and final Mongol conquest. The nation becomes enslaved to the Chinese leader Murti Bing. His emissaries give everyone a special pill called DAVAMESK B 2 which takes away their abilities to think and to mentally resist. East and West become one, in faceless misery fueled by sexual instincts.&lt;lb/&gt;The book combines chaotic action with deep philosophical and political discussion, and predicts many of the events and political outcomes of the subsequent years, specifically, the invasion of Poland, the postwar foreign domination as well as the totalitarian mind control exerted, first by the Germans, and then by the Soviet Union on Polish life and art. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read more about Witkiewicz’s artworks in our essay “Documenting Drugs” by Juliette Bretan&lt;/p&gt;
    &lt;head rend="h2"&gt;Entering the public domain in countries with a ‘life plus 70 year’ copyright term&lt;/head&gt;
    &lt;head rend="h3"&gt;Albert Einstein&lt;/head&gt;
    &lt;p&gt;Albert Einstein was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory. His mass–energy equivalence formula E = mc^2, which arises from special relativity, has been called “the world’s most famous equation”. He received the 1921 Nobel Prize in Physics for “his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect”.&lt;lb/&gt;In 1905, sometimes described as his *annus mirabilis* (miracle year), he published four groundbreaking papers. In them, he outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity, and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole. In 1917, Einstein introduced the concepts of spontaneous emission and stimulated emission, the latter of which is the core mechanism behind the laser and maser, and which helped lay the groundwork for later developments in physics such as quantum electrodynamics and quantum optics. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Wikisource&lt;/p&gt;
    &lt;head rend="h3"&gt;Wallace Stevens&lt;/head&gt;
    &lt;p&gt;Wallace Stevens was an American modernist poet. He was born in Reading, Pennsylvania, educated at Harvard and then New York Law School, and spent most of his life working as an executive for an insurance company in Hartford, Connecticut.&lt;lb/&gt;Stevens’s first period begins with the publication of Harmonium (1923), followed by a slightly revised and amended second edition in 1930. It features, among other poems, “The Emperor of Ice-Cream”, “Sunday Morning”, “The Snow Man”, and “Thirteen Ways of Looking at a Blackbird”. His second period commenced with Ideas of Order (1933), included in Transport to Summer (1947). His third and final period began with the publication of The Auroras of Autumn (1950), followed by The Necessary Angel: Essays On Reality and the Imagination (1951).&lt;lb/&gt;Many of Stevens’s poems deal with the making of art and poetry in particular. His Collected Poems (1954) won the Pulitzer Prize for Poetry in 1955 and Stevens is a rare example of a poet whose main output came largely only as he approached 40 years of age. His first major publication (four poems from a sequence titled “Phases” in the November 1914 edition of Poetry) was written at age 35, although as an undergraduate at Harvard, Stevens had written poetry and exchanged sonnets with Santayana. Many of his canonical works were written well after he turned 50. According to the literary scholar Harold Bloom, no Western writer since Sophocles has had such a late flowering of artistic genius. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Charlie Parker&lt;/head&gt;
    &lt;p&gt;Charles Parker Jr. was an American jazz saxophonist, bandleader, and composer. Parker was a highly influential soloist and leading figure in the development of bebop, a form of jazz characterized by fast tempos, virtuosic technique, and advanced harmonies. He was a virtuoso and introduced revolutionary rhythmic and harmonic ideas into jazz, including rapid passing chords, new variants of altered chords, and chord substitutions. Parker primarily played the alto saxophone.&lt;lb/&gt;Parker was an icon for the hipster subculture and later the Beat Generation, personifying the jazz musician as an uncompromising artist and intellectual rather than just an entertainer.&lt;lb/&gt;His style of composition involved interpolation of original melodies over existing jazz forms and standards, a practice known as contrafact and still common in jazz today. Examples include “Ornithology” (which borrows the chord progression of jazz standard “How High the Moon” and is said to be co-written with trumpet player Little Benny Harris), and “Moose The Mooche”. The practice was not uncommon prior to bebop, but it became a signature of the movement as artists began to move away from arranging popular standards and toward composing their own material. Parker contributed greatly to the modern jazz solo, one in which triplets and pick-up notes were used in unorthodox ways to lead into chord tones.&lt;lb/&gt;Miles Davis once said, “You can tell the history of jazz in four words: Louis Armstrong. Charlie Parker.” (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Thomas Mann&lt;/head&gt;
    &lt;p&gt;Paul Thomas Mann was a German novelist, short story writer, social critic, philanthropist, essayist, and the 1929 Nobel Prize in Literature laureate. His highly symbolic and ironic epic novels and novellas are noted for their insight into the psychology of the artist and the intellectual. His analysis and critique of the European and German soul used modernized versions of German and Biblical stories, as well as the ideas of Johann Wolfgang von Goethe, Friedrich Nietzsche, and Arthur Schopenhauer.&lt;lb/&gt;Mann was a member of the hanseatic Mann family and portrayed his family and class in his first novel, Buddenbrooks (1901). Further major novels include The Magic Mountain (1924), the tetralogy Joseph and His Brothers (1933–1943), and Doctor Faustus (1947); he also wrote short stories and novellas, including Death in Venice (1912).&lt;lb/&gt;When Adolf Hitler came to power in 1933, Mann fled to Switzerland and when World War II broke out in 1939, he moved to the United States, then returned to Switzerland in 1952. Mann is one of the best-known exponents of the so-called Exilliteratur, German literature written in exile by those who opposed the Hitler regime. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Project Gutenberg&lt;/p&gt;
    &lt;head rend="h3"&gt;Pierre Teilhard de Chardin&lt;/head&gt;
    &lt;p&gt;Pierre Teilhard de Chardin was a French Jesuit, Catholic priest, scientist, paleontologist, philosopher, mystic, and teacher. He investigated the theory of evolution from a perspective influenced by Henri Bergson and Christian mysticism, writing multiple scientific and religious works on the subject, his most popular being The Phenomenon of Man, published posthumously in 1955. His mainstream scientific achievements include his palaeontological research in China, taking part in the discovery of the significant Peking Man fossils from the Zhoukoudian cave complex near Beijing. His more speculative ideas, sometimes criticized as pseudoscientific, have included a vitalist conception of the Omega Point. Along with Vladimir Vernadsky, he contributed to the development of the concept of the noosphere.&lt;lb/&gt;In 1962, the Holy Office issued a warning regarding Teilhard’s works, alleging ambiguities and doctrinal errors without specifying them. Some eminent Catholic figures, including Pope Benedict XVI and Pope Francis, have made positive comments on some of his ideas since. The response to his writings by scientists has been divided. His work was controversial to some scientists and religious leaders because Teilhard combined theology and metaphysics with science.&lt;lb/&gt;Teilhard served in World War I as a stretcher-bearer. He received several citations, and was awarded the Médaille militaire and the Legion of Honor, the highest French order of merit, both military and civil. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Roger Mais&lt;/head&gt;
    &lt;p&gt;Roger Mais was a Jamaican journalist, novelist, poet, and playwright. He was born to a middle-class family in Kingston, Jamaica. By 1951, he had won ten first prizes in West Indian literary competitions. His integral role in the development of political and cultural nationalism is evidenced in his being awarded the high honour of the Order of Jamaica in 1978.&lt;lb/&gt;He worked at various times as a photographer, insurance salesman, and journalist, launching his journalistic career as a contributor to the weekly newspaper Public Opinion from 1939 to 1952. Mais published more than a hundred short stories, most appearing in Public Opinion and Focus, with others collected in Face and Other Stories and And Most of All Man. He wrote more than thirty stage and radio plays, as well as three novels: The Hills Were Joyful Together (1953), Brother Man (1954), and Black Lightning (1955).&lt;lb/&gt;Mais’ topics most frequently were the social injustice and inequality suffered by black, poor Jamaicans. Accused of sedition for writing the article “Now We Know,” a 1944 denunciation of the British Empire, the Jamaican novelist was tried, convicted and imprisoned for six months. His political activism, anti-colonial writing, and imprisonment helped galvanize Jamaican nationalism. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Saadat Hasan Manto&lt;/head&gt;
    &lt;p&gt;Saadat Hasan Manto was a Pakistani writer, playwright and novelist from Punjab, who is regarded as the greatest short-story author in Urdu literature. He was active from 1933 during British rule till his death in 1955 after independence.&lt;lb/&gt;Writing mainly in Urdu, he produced 22 collections of short stories, a novel, five series of radio plays, three collections of essays, and two collections of personal sketches. He is best known for his stories about the partition of India, which he opposed, immediately following independence in 1947. Manto’s most notable work has been archived by Rekhta.&lt;lb/&gt;Manto was tried six times for alleged obscenity in his writings; thrice before 1947 in British India, and thrice after independence in 1947 in Pakistan, but was never convicted. He started his literary career translating the works of Victor Hugo, Oscar Wilde and Russian writers such as Chekhov and Gorky. His first story was “Tamasha”, based on the Jallianwala Bagh massacre at Amritsar. His final works, which grew from the social climate and his own financial struggles, reflected an innate sense of human impotency towards darkness and contained satire that verged on dark comedy, as seen in his last story, “Toba Tek Singh”. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h2"&gt;Entering the public domain in countries with a ‘life plus 50 year’ copyright term&lt;/head&gt;
    &lt;head rend="h3"&gt;Barbara Hepworth&lt;/head&gt;
    &lt;p&gt;Dame Jocelyn Barbara Hepworth was an English artist and sculptor. Along with artists such as Ben Nicholson and Naum Gabo, Hepworth was a leading figure in the colony of artists who resided in St Ives during the Second World War. Born in Wakefield, Yorkshire, Hepworth studied at Leeds School of Art and the Royal College of Art in the 1920s. She married the sculptor John Skeaping in 1925. In 1931 she fell in love with the painter Ben Nicholson, and in 1933 divorced Skeaping. At this time she was part of a circle of modern artists centred on Hampstead, London, and was one of the founders of the art movement Unit One. At the beginning of the Second World War Hepworth and Nicholson moved to St Ives, Cornwall, where she would remain for the rest of her life. Best known as a sculptor, Hepworth also produced drawings – including a series of sketches of operating rooms following the hospitalisation of her daughter in 1944 – and lithographs. She died in a fire at her studio in 1975. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Hannah Arendt&lt;/head&gt;
    &lt;p&gt;Hannah Arendt was a German and American historian and philosopher. She was one of the most influential political theorists of the twentieth century.&lt;lb/&gt;Her works cover a broad range of topics, but she is best known for those dealing with the nature of wealth, power, fame, and evil, as well as politics, direct democracy, authority, tradition, and totalitarianism. She is also remembered for the controversy surrounding the trial of Adolf Eichmann, for her attempt to explain how ordinary people become actors in totalitarian systems, which was considered by some an apologia, and for the phrase “the banality of evil”.&lt;lb/&gt;In 1933, Arendt was briefly imprisoned by the Gestapo for performing illegal research into antisemitism. On release, she fled Germany, settling in Paris. There she worked for Youth Aliyah, assisting young Jews to emigrate to the British Mandate of Palestine. When Germany invaded France she was detained as an alien, but she escaped and made her way to the United States in 1941. She became a writer and editor and worked for the Jewish Cultural Reconstruction, becoming an American citizen in 1950. With the publication of The Origins of Totalitarianism in 1951, her reputation as a thinker and writer was established, and a series of works followed. These included the books The Human Condition in 1958, as well as Eichmann in Jerusalem and On Revolution in 1963. She taught at many American universities while declining tenure-track appointments. She died suddenly of a heart attack in 1975, leaving her last work, The Life of the Mind, unfinished. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Walker Evans&lt;/head&gt;
    &lt;p&gt;Walker Evans was an American photographer and photojournalist best known for his work for the Resettlement Administration and the Farm Security Administration (FSA) documenting the effects of the Great Depression. Evans’ published his first photos at the age of 27. Much of Evans’ New Deal work uses the large format, 8 × 10-inch (200×250 mm) view camera. He said that his goal as a photographer was to make pictures that are “literate, authoritative, transcendent”.&lt;lb/&gt;Many of his works are in the permanent collections of museums and have been the subject of retrospectives at such institutions as the Metropolitan Museum of Art or the George Eastman Museum.&lt;lb/&gt;Born in St. Louis, Missouri, Evans took up photography in 1928 around the time he was living in Ossining, New York. The Great Depression years of 1935–36 were a period of remarkable productivity and accomplishment for Evans. In 1936, employed by the National Recovery Administration, he photographed three impoverished sharecropper families in Hale County, Alabama. The photographs became iconic and were praised for effectively capturing the negative effects of the Great Depression in the American South. Between 1940 and 1959, Evans was awarded three Guggenheim Fellowships in Photography to continue his work of making record photographs of contemporary American subjects. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Library of Congress&lt;/p&gt;
    &lt;head rend="h3"&gt;P. G. Wodehouse&lt;/head&gt;
    &lt;p&gt;Sir Pelham Grenville Wodehouse was an English writer and one of the most widely read humorists of the 20th century. His creations include the feather-brained Bertie Wooster and his sagacious valet, Jeeves; the immaculate and loquacious Psmith; Lord Emsworth and the Blandings Castle set; the Oldest Member, with stories about golf; and Mr. Mulliner, with tall tales on subjects ranging from bibulous bishops to megalomaniac movie moguls.&lt;lb/&gt;Born in Guildford, his early novels were mostly school stories, but he later switched to comic fiction. Most of Wodehouse’s fiction is set in his native United Kingdom, although he spent much of his life in the US and used New York and Hollywood as settings for some of his novels and short stories. Wodehouse was a prolific writer throughout his life, publishing more than ninety books, forty plays, two hundred short stories and other writings between 1902 and 1974. Early in his career Wodehouse would produce a novel in about three months, but he slowed in old age to around six months. He used a mixture of Edwardian slang, quotations from and allusions to numerous poets, and several literary techniques to produce a prose style that has been compared to comic poetry and musical comedy. Some critics of Wodehouse have considered his work flippant, but among his fans are former British prime ministers and many of his fellow writers. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Project Gutenberg&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460440</guid><pubDate>Fri, 02 Jan 2026 01:42:16 +0000</pubDate></item><item><title>Marmot – A distributed SQLite server with MySQL wire compatible interface</title><link>https://github.com/maxpert/marmot</link><description>&lt;doc fingerprint="7bd207d0da0bb8c"&gt;
  &lt;main&gt;
    &lt;p&gt;Marmot v2 is a leaderless, distributed SQLite replication system built on a gossip-based protocol with distributed transactions and eventual consistency.&lt;/p&gt;
    &lt;p&gt;Key Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leaderless Architecture: No single point of failure - any node can accept writes&lt;/item&gt;
      &lt;item&gt;MySQL Protocol Compatible: Connect with any MySQL client (DBeaver, MySQL Workbench, mysql CLI)&lt;/item&gt;
      &lt;item&gt;WordPress Compatible: Full MySQL function support for running distributed WordPress&lt;/item&gt;
      &lt;item&gt;Distributed Transactions: Percolator-style write intents with conflict detection&lt;/item&gt;
      &lt;item&gt;Multi-Database Support: Create and manage multiple databases per cluster&lt;/item&gt;
      &lt;item&gt;DDL Replication: Distributed schema changes with automatic idempotency and cluster-wide locking&lt;/item&gt;
      &lt;item&gt;Production-Ready SQL Parser: Powered by rqlite/sql AST parser for MySQL→SQLite transpilation&lt;/item&gt;
      &lt;item&gt;CDC-Based Replication: Row-level change data capture for consistent replication&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MySQL active-active requires careful setup of replication, conflict avoidance, and monitoring. Failover needs manual intervention. Split-brain scenarios demand operational expertise. This complexity doesn't scale to edge deployments.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero operational overhead: Automatic recovery from split-brain via eventual consistency + anti-entropy&lt;/item&gt;
      &lt;item&gt;No leader election: Any node accepts writes, no failover coordination needed&lt;/item&gt;
      &lt;item&gt;Direct SQLite access: Clients can read the local SQLite file directly for maximum performance&lt;/item&gt;
      &lt;item&gt;Tunable consistency: Choose ONE/QUORUM/ALL per your latency vs durability needs&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ecosystem compatibility - existing drivers, ORMs, GUI tools work out-of-box&lt;/item&gt;
      &lt;item&gt;Battle-tested wire protocol implementations&lt;/item&gt;
      &lt;item&gt;Run real applications like WordPress without modification&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot excels at read-heavy edge scenarios:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
        &lt;cell role="head"&gt;How Marmot Helps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Distributed WordPress&lt;/cell&gt;
        &lt;cell&gt;Multi-region WordPress with replicated database&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Lambda/Edge sidecars&lt;/cell&gt;
        &lt;cell&gt;Lightweight regional SQLite replicas, local reads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Edge vector databases&lt;/cell&gt;
        &lt;cell&gt;Distributed embeddings with local query&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Regional config servers&lt;/cell&gt;
        &lt;cell&gt;Fast local config reads, replicated writes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Product catalogs&lt;/cell&gt;
        &lt;cell&gt;Geo-distributed catalog data, eventual sync&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strong serializability required → CockroachDB, Spanner&lt;/item&gt;
      &lt;item&gt;Single-region high-throughput → PostgreSQL, MySQL directly&lt;/item&gt;
      &lt;item&gt;Large datasets (&amp;gt;100GB) → Sharded solutions&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Start a single-node cluster
./marmot-v2

# Connect with MySQL client
mysql -h localhost -P 3306 -u root

# Or use DBeaver, MySQL Workbench, etc.&lt;/code&gt;
    &lt;code&gt;# Test DDL and DML replication across a 2-node cluster
./scripts/test-ddl-replication.sh

# This script will:
# 1. Start a 2-node cluster
# 2. Create a table on node 1 and verify it replicates to node 2
# 3. Insert data on node 1 and verify it replicates to node 2
# 4. Update data on node 2 and verify it replicates to node 1
# 5. Delete data on node 1 and verify it replicates to node 2

# Manual cluster testing
./examples/start-seed.sh              # Start seed node (port 8081, mysql 3307)
./examples/join-cluster.sh 2 localhost:8081  # Join node 2 (port 8082, mysql 3308)
./examples/join-cluster.sh 3 localhost:8081  # Join node 3 (port 8083, mysql 3309)

# Connect to any node and run queries
mysql --protocol=TCP -h localhost -P 3307 -u root
mysql --protocol=TCP -h localhost -P 3308 -u root

# Cleanup
pkill -f marmot-v2&lt;/code&gt;
    &lt;p&gt;Marmot can run distributed WordPress with full database replication across nodes. Each WordPress instance connects to its local Marmot node, and all database changes replicate automatically.&lt;/p&gt;
    &lt;p&gt;Marmot implements MySQL functions required by WordPress:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Functions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Date/Time&lt;/cell&gt;
        &lt;cell&gt;NOW, CURDATE, DATE_FORMAT, UNIX_TIMESTAMP, DATEDIFF, YEAR, MONTH, DAY, etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;String&lt;/cell&gt;
        &lt;cell&gt;CONCAT_WS, SUBSTRING_INDEX, FIND_IN_SET, LPAD, RPAD, etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Math/Hash&lt;/cell&gt;
        &lt;cell&gt;RAND, MD5, SHA1, SHA2, POW, etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DML&lt;/cell&gt;
        &lt;cell&gt;ON DUPLICATE KEY UPDATE (transformed to SQLite ON CONFLICT)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;cd examples/wordpress-cluster
./run.sh up&lt;/code&gt;
    &lt;p&gt;This starts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3 Marmot nodes with QUORUM write consistency&lt;/item&gt;
      &lt;item&gt;3 WordPress instances each connected to its local Marmot node&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│ WordPress-1 │  │ WordPress-2 │  │ WordPress-3 │
│ :9101       │  │ :9102       │  │ :9103       │
└──────┬──────┘  └──────┬──────┘  └──────┬──────┘
       ▼                ▼                ▼
┌─────────────┐  ┌─────────────┐  ┌─────────────┐
│  Marmot-1   │◄─┤  Marmot-2   │◄─┤  Marmot-3   │
│ MySQL: 9191 │  │ MySQL: 9192 │  │ MySQL: 9193 │
└─────────────┘  └─────────────┘  └─────────────┘
       └──────────────┴──────────────┘
              QUORUM Replication
&lt;/code&gt;
    &lt;p&gt;Test it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open http://localhost:9101 - complete WordPress installation&lt;/item&gt;
      &lt;item&gt;Open http://localhost:9102 or http://localhost:9103&lt;/item&gt;
      &lt;item&gt;See your content replicated across all nodes!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Commands:&lt;/p&gt;
    &lt;code&gt;./run.sh status   # Check cluster health
./run.sh logs-m   # Marmot logs only
./run.sh logs-wp  # WordPress logs only
./run.sh down     # Stop cluster&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Media uploads: Use S3/NFS for shared media storage (files not replicated by Marmot)&lt;/item&gt;
      &lt;item&gt;Sessions: Use Redis or database sessions for sticky-session-free load balancing&lt;/item&gt;
      &lt;item&gt;Caching: Each node can use local object cache (Redis/Memcached per region)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot v2 uses a fundamentally different architecture from other SQLite replication solutions:&lt;/p&gt;
    &lt;p&gt;vs. rqlite/dqlite/LiteFS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;❌ They require a primary node for all writes&lt;/item&gt;
      &lt;item&gt;✅ Marmot allows writes on any node&lt;/item&gt;
      &lt;item&gt;❌ They use leader election (Raft)&lt;/item&gt;
      &lt;item&gt;✅ Marmot uses gossip protocol (no leader)&lt;/item&gt;
      &lt;item&gt;❌ They require proxy layer or page-level interception&lt;/item&gt;
      &lt;item&gt;✅ Marmot uses MySQL protocol for direct database access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How It Works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write Coordination: 2PC (Two-Phase Commit) with configurable consistency (ONE, QUORUM, ALL)&lt;/item&gt;
      &lt;item&gt;Conflict Resolution: Last-Write-Wins (LWW) with HLC timestamps&lt;/item&gt;
      &lt;item&gt;Cluster Membership: SWIM-style gossip with failure detection&lt;/item&gt;
      &lt;item&gt;Data Replication: Full database replication - all nodes receive all data&lt;/item&gt;
      &lt;item&gt;DDL Replication: Cluster-wide schema changes with automatic idempotency&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Aspect&lt;/cell&gt;
        &lt;cell role="head"&gt;Marmot&lt;/cell&gt;
        &lt;cell role="head"&gt;MySQL Active-Active&lt;/cell&gt;
        &lt;cell role="head"&gt;rqlite/dqlite&lt;/cell&gt;
        &lt;cell role="head"&gt;TiDB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Leader&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
        &lt;cell&gt;None (but complex)&lt;/cell&gt;
        &lt;cell&gt;Yes (Raft)&lt;/cell&gt;
        &lt;cell&gt;Yes (Raft)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Failover&lt;/cell&gt;
        &lt;cell&gt;Automatic&lt;/cell&gt;
        &lt;cell&gt;Manual intervention&lt;/cell&gt;
        &lt;cell&gt;Automatic&lt;/cell&gt;
        &lt;cell&gt;Automatic&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Split-brain recovery&lt;/cell&gt;
        &lt;cell&gt;Automatic (anti-entropy)&lt;/cell&gt;
        &lt;cell&gt;Manual&lt;/cell&gt;
        &lt;cell&gt;N/A (leader-based)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Consistency&lt;/cell&gt;
        &lt;cell&gt;Tunable (ONE/QUORUM/ALL)&lt;/cell&gt;
        &lt;cell&gt;Serializable&lt;/cell&gt;
        &lt;cell&gt;Strong&lt;/cell&gt;
        &lt;cell&gt;Strong&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Direct file read&lt;/cell&gt;
        &lt;cell&gt;✅ SQLite file&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;JS-safe AUTO_INCREMENT&lt;/cell&gt;
        &lt;cell&gt;✅ Compact mode (53-bit)&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;❌ 64-bit breaks JS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Edge-friendly&lt;/cell&gt;
        &lt;cell&gt;✅ Lightweight&lt;/cell&gt;
        &lt;cell&gt;❌ Heavy&lt;/cell&gt;
        &lt;cell&gt;❌ Heavy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Operational complexity&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Marmot v2 supports distributed DDL (Data Definition Language) replication without requiring master election:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Cluster-Wide Locking: Each DDL operation acquires a distributed lock per database (default: 30-second lease)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Prevents concurrent schema changes on the same database&lt;/item&gt;
          &lt;item&gt;Locks automatically expire if a node crashes&lt;/item&gt;
          &lt;item&gt;Different databases can have concurrent DDL operations&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automatic Idempotency: DDL statements are automatically rewritten for safe replay&lt;/p&gt;
        &lt;quote&gt;CREATE TABLE users (id INT) → CREATE TABLE IF NOT EXISTS users (id INT) DROP TABLE users → DROP TABLE IF EXISTS users&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Schema Version Tracking: Each database maintains a schema version counter&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Incremented on every DDL operation&lt;/item&gt;
          &lt;item&gt;Exchanged via gossip protocol for drift detection&lt;/item&gt;
          &lt;item&gt;Used by delta sync to validate transaction applicability&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Quorum-Based Replication: DDL replicates like DML through the same 2PC mechanism&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;No special master node needed&lt;/item&gt;
          &lt;item&gt;Works with existing consistency levels (QUORUM, ALL, etc.)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[ddl]
# DDL lock lease duration (seconds)
lock_lease_seconds = 30

# Automatically rewrite DDL for idempotency
enable_idempotent = true&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Do: Execute DDL from a single connection/node at a time&lt;/item&gt;
      &lt;item&gt;✅ Do: Use qualified table names (&lt;code&gt;mydb.users&lt;/code&gt;instead of&lt;code&gt;users&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Caution: ALTER TABLE is less idempotent - avoid replaying failed ALTER operations&lt;/item&gt;
      &lt;item&gt;❌ Don't: Run concurrent DDL on the same database from multiple nodes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot v2 uses Change Data Capture (CDC) for replication instead of SQL statement replay:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row-Level Capture: Instead of replicating SQL statements, Marmot captures the actual row data changes (INSERT/UPDATE/DELETE)&lt;/item&gt;
      &lt;item&gt;Binary Data Format: Row data is serialized as CDC messages with column values, ensuring consistent replication regardless of SQL dialect&lt;/item&gt;
      &lt;item&gt;Deterministic Application: Row data is applied directly to the target database, avoiding parsing ambiguities&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consistency: Same row data applied everywhere, no SQL parsing differences&lt;/item&gt;
      &lt;item&gt;Performance: Binary format is more efficient than SQL text&lt;/item&gt;
      &lt;item&gt;Reliability: No issues with SQL syntax variations between MySQL and SQLite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For UPDATE and DELETE operations, Marmot automatically extracts row keys:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses PRIMARY KEY columns when available&lt;/item&gt;
      &lt;item&gt;Falls back to ROWID for tables without explicit primary key&lt;/item&gt;
      &lt;item&gt;Handles composite primary keys correctly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot can publish CDC events to external messaging systems, enabling real-time data pipelines, analytics, and event-driven architectures. Events follow the Debezium specification for maximum compatibility with existing CDC tooling.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debezium-Compatible Format: Events conform to the Debezium event structure, compatible with Kafka Connect, Flink, Spark, and other CDC consumers&lt;/item&gt;
      &lt;item&gt;Multi-Sink Support: Publish to multiple destinations simultaneously (Kafka, NATS)&lt;/item&gt;
      &lt;item&gt;Glob-Based Filtering: Filter which tables and databases to publish&lt;/item&gt;
      &lt;item&gt;Automatic Retry: Exponential backoff with configurable limits&lt;/item&gt;
      &lt;item&gt;Persistent Cursors: Survives restarts without losing position&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[publisher]
enabled = true

[[publisher.sinks]]
name = "kafka-main"
type = "kafka"                    # "kafka" or "nats"
format = "debezium"               # Debezium-compatible JSON format
brokers = ["localhost:9092"]      # Kafka broker addresses
topic_prefix = "marmot.cdc"       # Topics: {prefix}.{database}.{table}
filter_tables = ["*"]             # Glob patterns (e.g., "users", "order_*")
filter_databases = ["*"]          # Glob patterns (e.g., "prod_*")
batch_size = 100                  # Events per poll cycle
poll_interval_ms = 10             # Polling interval

# NATS sink example
[[publisher.sinks]]
name = "nats-events"
type = "nats"
format = "debezium"
nats_url = "nats://localhost:4222"
topic_prefix = "marmot.cdc"
filter_tables = ["*"]
filter_databases = ["*"]&lt;/code&gt;
    &lt;p&gt;Events follow the Debezium envelope structure:&lt;/p&gt;
    &lt;code&gt;{
  "schema": { ... },
  "payload": {
    "before": null,
    "after": {"id": 1, "name": "alice", "email": "alice@example.com"},
    "source": {
      "version": "2.0.0",
      "connector": "marmot",
      "name": "marmot",
      "ts_ms": 1702500000000,
      "db": "myapp",
      "table": "users"
    },
    "op": "c",
    "ts_ms": 1702500000000
  }
}&lt;/code&gt;
    &lt;p&gt;Operation Types (per Debezium spec):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Operation&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;op&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;before&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;after&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;INSERT&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;c&lt;/code&gt; (create)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;null&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;row data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;UPDATE&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;u&lt;/code&gt; (update)&lt;/cell&gt;
        &lt;cell&gt;old row&lt;/cell&gt;
        &lt;cell&gt;new row&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DELETE&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;d&lt;/code&gt; (delete)&lt;/cell&gt;
        &lt;cell&gt;old row&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;null&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Topics follow the pattern: &lt;code&gt;{topic_prefix}.{database}.{table}&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;marmot.cdc.myapp.users&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;marmot.cdc.myapp.orders&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;marmot.cdc.analytics.events&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-Time Analytics: Stream changes to data warehouses (Snowflake, BigQuery, ClickHouse)&lt;/item&gt;
      &lt;item&gt;Event-Driven Microservices: Trigger actions on data changes&lt;/item&gt;
      &lt;item&gt;Cache Invalidation: Keep caches in sync with database changes&lt;/item&gt;
      &lt;item&gt;Audit Logging: Capture all changes for compliance&lt;/item&gt;
      &lt;item&gt;Search Indexing: Keep Elasticsearch/Algolia in sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more details, see the Integrations documentation.&lt;/p&gt;
    &lt;p&gt;Deploy Marmot as a lightweight regional replica alongside Lambda functions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Local SQLite reads (sub-ms latency)&lt;/item&gt;
      &lt;item&gt;Writes replicate to cluster&lt;/item&gt;
      &lt;item&gt;No cold-start database connections&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Scale reads globally with replica mode:&lt;/p&gt;
    &lt;code&gt;[replica]
enabled = true
master_address = "central-cluster:8080"
reconnect_interval_seconds = 5&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Follows master via streaming replication&lt;/item&gt;
      &lt;item&gt;Zero cluster participation overhead&lt;/item&gt;
      &lt;item&gt;Auto-reconnect on network issues&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deploy full cluster in central region&lt;/item&gt;
      &lt;item&gt;Deploy read replicas at edge locations&lt;/item&gt;
      &lt;item&gt;Application routes writes to central, reads to local replica&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot supports a wide range of MySQL/SQLite statements through its MySQL protocol server. The following table shows compatibility for different statement types:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Statement Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Support&lt;/cell&gt;
        &lt;cell role="head"&gt;Replication&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DML - Data Manipulation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;INSERT&lt;/code&gt; / &lt;code&gt;REPLACE&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Includes qualified table names (db.table)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;UPDATE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Includes qualified table names&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;DELETE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Includes qualified table names&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SELECT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Read operations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;LOAD DATA&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Bulk data loading&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DDL - Data Definition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ALTER TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;DROP TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;TRUNCATE TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;RENAME TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP INDEX&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP VIEW&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP TRIGGER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Database Management&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE DATABASE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;DROP DATABASE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ALTER DATABASE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SHOW DATABASES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Metadata query&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SHOW TABLES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Metadata query&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;USE database&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Session state&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Transaction Control&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;BEGIN&lt;/code&gt; / &lt;code&gt;START TRANSACTION&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Transaction boundary&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;COMMIT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Commits distributed transaction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ROLLBACK&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Aborts distributed transaction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SAVEPOINT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Nested transaction support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;LOCK TABLES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Requires distributed locking coordination&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;UNLOCK TABLES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Requires distributed locking coordination&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Session Configuration&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;SET&lt;/code&gt; statements&lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Session-local, not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;XA Transactions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;XA START/END/PREPARE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Marmot uses its own 2PC protocol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;XA COMMIT/ROLLBACK&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Not compatible with Marmot's model&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DCL - Data Control&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;GRANT&lt;/code&gt; / &lt;code&gt;REVOKE&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;User management not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP USER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;User management not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ALTER USER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;User management not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Administrative&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;OPTIMIZE TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Node-local administrative command&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;REPAIR TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Node-local administrative command&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Full: Fully supported and working&lt;/item&gt;
      &lt;item&gt;✅ Parsed: Statement is parsed and recognized&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Limited: Works but has limitations in distributed context&lt;/item&gt;
      &lt;item&gt;❌ No: Not supported or not replicated&lt;/item&gt;
      &lt;item&gt;N/A: Not applicable (read-only or session-local)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Schema Changes (DDL): DDL statements are fully replicated with cluster-wide locking and automatic idempotency. See the DDL Replication section for details.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;XA Transactions: Marmot has its own distributed transaction protocol based on 2PC. MySQL XA transactions are not compatible with Marmot's replication model.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;User Management (DCL): User and privilege management statements are local to each node. For production deployments, consider handling authentication at the application or proxy level.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Table Locking:&lt;/p&gt;&lt;code&gt;LOCK TABLES&lt;/code&gt;statements are recognized but not enforced across the cluster. Use application-level coordination for distributed locking needs.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Qualified Names: Marmot fully supports qualified table names (e.g.,&lt;/p&gt;&lt;code&gt;db.table&lt;/code&gt;) in DML and DDL operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot includes a MySQL-compatible protocol server, allowing you to connect using any MySQL client (DBeaver, MySQL Workbench, mysql CLI, etc.). The server supports:&lt;/p&gt;
    &lt;p&gt;Marmot provides full support for MySQL metadata queries, enabling GUI tools like DBeaver to browse databases, tables, and columns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SHOW Commands: &lt;code&gt;SHOW DATABASES&lt;/code&gt;,&lt;code&gt;SHOW TABLES&lt;/code&gt;,&lt;code&gt;SHOW COLUMNS FROM table&lt;/code&gt;,&lt;code&gt;SHOW CREATE TABLE&lt;/code&gt;,&lt;code&gt;SHOW INDEXES&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;INFORMATION_SCHEMA: Queries against &lt;code&gt;INFORMATION_SCHEMA.TABLES&lt;/code&gt;,&lt;code&gt;INFORMATION_SCHEMA.COLUMNS&lt;/code&gt;,&lt;code&gt;INFORMATION_SCHEMA.SCHEMATA&lt;/code&gt;, and&lt;code&gt;INFORMATION_SCHEMA.STATISTICS&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Type Conversion: Automatic SQLite-to-MySQL type mapping for compatibility&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These metadata queries are powered by the rqlite/sql AST parser, providing production-grade MySQL query compatibility.&lt;/p&gt;
    &lt;code&gt;# Using mysql CLI
mysql -h localhost -P 3306 -u root

# Connection string for applications
mysql://root@localhost:3306/marmot&lt;/code&gt;
    &lt;p&gt;Marmot handles various failure and recovery scenarios automatically:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Minority partition&lt;/cell&gt;
        &lt;cell&gt;Writes fail - cannot achieve quorum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Majority partition&lt;/cell&gt;
        &lt;cell&gt;Writes succeed - quorum achieved&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Partition heals&lt;/cell&gt;
        &lt;cell&gt;Delta sync + LWW merges divergent data&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;How it works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;During partition, only the majority side can commit writes (quorum enforcement)&lt;/item&gt;
      &lt;item&gt;When partition heals, nodes exchange transaction logs via &lt;code&gt;StreamChanges&lt;/code&gt;RPC&lt;/item&gt;
      &lt;item&gt;Conflicts resolved using Last-Writer-Wins (LWW) with HLC timestamps&lt;/item&gt;
      &lt;item&gt;Higher node ID breaks ties for simultaneous writes&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;Recovery Method&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Brief outage&lt;/cell&gt;
        &lt;cell&gt;Delta sync - replay missed transactions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extended outage&lt;/cell&gt;
        &lt;cell&gt;Snapshot transfer + delta sync&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;New node joining&lt;/cell&gt;
        &lt;cell&gt;Full snapshot from existing node&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Anti-Entropy Background Process:&lt;/p&gt;
    &lt;p&gt;Marmot v2 includes an automatic anti-entropy system that continuously monitors and repairs replication lag across the cluster:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lag Detection: Every 60 seconds (configurable), each node queries peers for their replication state&lt;/item&gt;
      &lt;item&gt;Smart Recovery Decision: &lt;list rend="ul"&gt;&lt;item&gt;Delta Sync if lag &amp;lt; 10,000 transactions AND &amp;lt; 1 hour: Streams missed transactions incrementally&lt;/item&gt;&lt;item&gt;Snapshot Transfer if lag exceeds thresholds: Full database file transfer for efficiency&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Gap Detection: Detects when transaction logs have been GC'd and automatically falls back to snapshot&lt;/item&gt;
      &lt;item&gt;Multi-Database Support: Tracks and syncs each database independently&lt;/item&gt;
      &lt;item&gt;GC Coordination: Garbage collection respects peer replication state - logs aren't deleted until all peers have applied them&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Delta Sync Process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lagging node queries &lt;code&gt;last_applied_txn_id&lt;/code&gt;for each peer/database&lt;/item&gt;
      &lt;item&gt;Requests transactions since that ID via &lt;code&gt;StreamChanges&lt;/code&gt;RPC&lt;/item&gt;
      &lt;item&gt;Gap Detection: Checks if first received txn_id has a large gap from requested ID &lt;list rend="ul"&gt;&lt;item&gt;If gap &amp;gt; delta_sync_threshold_txns, indicates missing (GC'd) transactions&lt;/item&gt;&lt;item&gt;Automatically falls back to snapshot transfer to prevent data loss&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Applies changes using LWW conflict resolution&lt;/item&gt;
      &lt;item&gt;Updates replication state tracking (per-database)&lt;/item&gt;
      &lt;item&gt;Progress logged every 100 transactions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GC Coordination with Anti-Entropy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transaction logs are retained with a two-tier policy: &lt;list rend="ul"&gt;&lt;item&gt;Min retention (2 hours): Must be &amp;gt;= delta sync threshold, respects peer lag&lt;/item&gt;&lt;item&gt;Max retention (24 hours): Force delete after this time to prevent unbounded growth&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Config validation enforces: &lt;code&gt;gc_min &amp;gt;= delta_threshold&lt;/code&gt;and&lt;code&gt;gc_max &amp;gt;= 2x delta_threshold&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Each database tracks replication progress per peer&lt;/item&gt;
      &lt;item&gt;GC queries minimum applied txn_id across all peers before cleanup&lt;/item&gt;
      &lt;item&gt;Gap detection prevents data loss if GC runs while nodes are offline&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Write Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;ONE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns after 1 node ACK (fast, less durable)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;QUORUM&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns after majority ACK (default, balanced)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;ALL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns after all nodes ACK (slow, most durable)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Conflict Resolution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All conflicts resolved via LWW using HLC timestamps&lt;/item&gt;
      &lt;item&gt;No data loss - later write always wins deterministically&lt;/item&gt;
      &lt;item&gt;Tie-breaker: higher node ID wins for equal timestamps&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Selective Table Watching: All tables in a database are replicated. Selective table replication is not supported.&lt;/item&gt;
      &lt;item&gt;WAL Mode Required: SQLite must use WAL mode for reliable multi-process changes.&lt;/item&gt;
      &lt;item&gt;Eventually Consistent: Rows may sync out of order. &lt;code&gt;SERIALIZABLE&lt;/code&gt;transaction assumptions may not hold across nodes.&lt;/item&gt;
      &lt;item&gt;Concurrent DDL: Avoid running concurrent DDL operations on the same database from multiple nodes (protected by cluster-wide lock with 30s lease).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Distributed databases need globally unique IDs, but traditional solutions cause problems:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Solution&lt;/cell&gt;
        &lt;cell role="head"&gt;Issue&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;UUID&lt;/cell&gt;
        &lt;cell&gt;128-bit, poor index performance, not sortable&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Snowflake/HLC 64-bit&lt;/cell&gt;
        &lt;cell&gt;Exceeds JavaScript's &lt;code&gt;Number.MAX_SAFE_INTEGER&lt;/code&gt; (2^53-1)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TiDB AUTO_INCREMENT&lt;/cell&gt;
        &lt;cell&gt;Returns 64-bit IDs that break JavaScript clients silently&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The JavaScript Problem:&lt;/p&gt;
    &lt;code&gt;// 64-bit ID from TiDB or other distributed DBs
const id = 7318624812345678901;
console.log(id);  // 7318624812345679000 - WRONG! Precision lost!

// JSON parsing also breaks
JSON.parse('{"id": 7318624812345678901}');  // {id: 7318624812345679000}&lt;/code&gt;
    &lt;p&gt;TiDB's answer? "Use strings." But that breaks ORMs, existing application code, and type safety.&lt;/p&gt;
    &lt;p&gt;Marmot offers two ID generation modes to solve this:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Bits&lt;/cell&gt;
        &lt;cell role="head"&gt;Range&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;extended&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;64-bit&lt;/cell&gt;
        &lt;cell&gt;Full HLC timestamp&lt;/cell&gt;
        &lt;cell&gt;New systems, non-JS clients&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;compact&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;53-bit&lt;/cell&gt;
        &lt;cell&gt;JS-safe integers&lt;/cell&gt;
        &lt;cell&gt;Legacy systems, JavaScript, REST APIs&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;[mysql]
auto_id_mode = "compact"  # Safe for JavaScript (default)
# auto_id_mode = "extended"  # Full 64-bit for new systems&lt;/code&gt;
    &lt;p&gt;Compact Mode Guarantees:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;IDs stay under &lt;code&gt;Number.MAX_SAFE_INTEGER&lt;/code&gt;(9,007,199,254,740,991)&lt;/item&gt;
      &lt;item&gt;Still globally unique across all nodes&lt;/item&gt;
      &lt;item&gt;Still monotonically increasing (per node)&lt;/item&gt;
      &lt;item&gt;No silent precision loss in JSON/JavaScript&lt;/item&gt;
      &lt;item&gt;Works with existing ORMs expecting integer IDs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With Marmot compact mode:&lt;/p&gt;
    &lt;code&gt;const id = 4503599627370496;
console.log(id);  // 4503599627370496 - Correct!
JSON.parse('{"id": 4503599627370496}');  // {id: 4503599627370496} - Correct!&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Note: Marmot automatically converts&lt;/p&gt;&lt;code&gt;INT AUTO_INCREMENT&lt;/code&gt;to&lt;code&gt;BIGINT&lt;/code&gt;to support distributed ID generation.&lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;DDL Transformation: When you create a table with&lt;/p&gt;&lt;code&gt;AUTO_INCREMENT&lt;/code&gt;:&lt;quote&gt;CREATE TABLE users (id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100)) -- Becomes internally: CREATE TABLE users (id BIGINT PRIMARY KEY, name TEXT)&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;DML ID Injection: When inserting with&lt;/p&gt;&lt;code&gt;0&lt;/code&gt;or&lt;code&gt;NULL&lt;/code&gt;for an auto-increment column:&lt;quote&gt;INSERT INTO users (id, name) VALUES (0, 'alice') -- Becomes internally (compact mode): INSERT INTO users (id, name) VALUES (4503599627370496, 'alice')&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explicit IDs Preserved: If you provide an explicit non-zero ID, it is used as-is.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Schema-Based Detection:&lt;/p&gt;
    &lt;p&gt;Marmot automatically detects auto-increment columns by querying SQLite schema directly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single-column &lt;code&gt;INTEGER PRIMARY KEY&lt;/code&gt;(SQLite rowid alias)&lt;/item&gt;
      &lt;item&gt;Single-column &lt;code&gt;BIGINT PRIMARY KEY&lt;/code&gt;(Marmot's transformed columns)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No registration required - columns are detected from schema at runtime, works across restarts, and works with existing databases.&lt;/p&gt;
    &lt;p&gt;Marmot v2 uses a TOML configuration file (default: &lt;code&gt;config.toml&lt;/code&gt;). All settings have sensible defaults.&lt;/p&gt;
    &lt;code&gt;node_id = 0  # 0 = auto-generate
data_dir = "./marmot-data"&lt;/code&gt;
    &lt;code&gt;[transaction]
heartbeat_timeout_seconds = 10  # Transaction timeout without heartbeat
conflict_window_seconds = 10    # Conflict resolution window
lock_wait_timeout_seconds = 50  # Lock wait timeout (MySQL: innodb_lock_wait_timeout)&lt;/code&gt;
    &lt;p&gt;Note: Transaction log garbage collection is managed by the replication configuration to coordinate with anti-entropy. See &lt;code&gt;replication.gc_min_retention_hours&lt;/code&gt; and &lt;code&gt;replication.gc_max_retention_hours&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[connection_pool]
pool_size = 4              # Number of SQLite connections
max_idle_time_seconds = 10 # Max idle time before closing
max_lifetime_seconds = 300 # Max connection lifetime (0 = unlimited)&lt;/code&gt;
    &lt;code&gt;[grpc_client]
keepalive_time_seconds = 10    # Keepalive ping interval
keepalive_timeout_seconds = 3  # Keepalive ping timeout
max_retries = 3                # Max retry attempts
retry_backoff_ms = 100         # Retry backoff duration&lt;/code&gt;
    &lt;code&gt;[coordinator]
prepare_timeout_ms = 2000 # Prepare phase timeout
commit_timeout_ms = 2000  # Commit phase timeout
abort_timeout_ms = 2000   # Abort phase timeout&lt;/code&gt;
    &lt;code&gt;[cluster]
grpc_bind_address = "0.0.0.0"
grpc_port = 8080
seed_nodes = []                # List of seed node addresses
cluster_secret = ""            # PSK for cluster authentication (see Security section)
gossip_interval_ms = 1000      # Gossip interval
gossip_fanout = 3              # Number of peers to gossip to
suspect_timeout_ms = 5000      # Suspect timeout
dead_timeout_ms = 10000        # Dead timeout&lt;/code&gt;
    &lt;p&gt;Marmot supports Pre-Shared Key (PSK) authentication for cluster communication. This is strongly recommended for production deployments.&lt;/p&gt;
    &lt;code&gt;[cluster]
# All nodes in the cluster must use the same secret
cluster_secret = "your-secret-key-here"&lt;/code&gt;
    &lt;p&gt;Environment Variable (Recommended):&lt;/p&gt;
    &lt;p&gt;For production, use the environment variable to avoid storing secrets in config files:&lt;/p&gt;
    &lt;code&gt;export MARMOT_CLUSTER_SECRET="your-secret-key-here"
./marmot&lt;/code&gt;
    &lt;p&gt;The environment variable takes precedence over the config file.&lt;/p&gt;
    &lt;p&gt;Generating a Secret:&lt;/p&gt;
    &lt;code&gt;# Generate a secure random secret
openssl rand -base64 32&lt;/code&gt;
    &lt;p&gt;Behavior:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;code&gt;cluster_secret&lt;/code&gt;is empty and&lt;code&gt;MARMOT_CLUSTER_SECRET&lt;/code&gt;is not set, authentication is disabled&lt;/item&gt;
      &lt;item&gt;A warning is logged at startup when authentication is disabled&lt;/item&gt;
      &lt;item&gt;All gRPC endpoints (gossip, replication, snapshots) are protected when authentication is enabled&lt;/item&gt;
      &lt;item&gt;Nodes with mismatched secrets will fail to communicate (connection rejected with "invalid cluster secret")&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot provides admin HTTP endpoints for managing cluster membership (requires &lt;code&gt;cluster_secret&lt;/code&gt; to be configured):&lt;/p&gt;
    &lt;p&gt;Node Lifecycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New/restarted nodes auto-join via gossip - no manual intervention needed&lt;/item&gt;
      &lt;item&gt;Nodes marked REMOVED via admin API cannot auto-rejoin - must be explicitly allowed&lt;/item&gt;
      &lt;item&gt;This prevents decommissioned nodes from accidentally rejoining the cluster&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# View cluster members and quorum info
curl -H "X-Marmot-Secret: your-secret" http://localhost:8080/admin/cluster/members

# Remove a node from the cluster (excludes from quorum, blocks auto-rejoin)
curl -X POST -H "X-Marmot-Secret: your-secret" http://localhost:8080/admin/cluster/remove/2

# Allow a removed node to rejoin (node must then restart to join)
curl -X POST -H "X-Marmot-Secret: your-secret" http://localhost:8080/admin/cluster/allow/2&lt;/code&gt;
    &lt;p&gt;See the Operations documentation for detailed usage and examples.&lt;/p&gt;
    &lt;p&gt;For read-only replicas that follow a master node without participating in the cluster:&lt;/p&gt;
    &lt;code&gt;[replica]
enabled = true                       # Enable read-only replica mode
master_address = "master:8080"       # Master node gRPC address
reconnect_interval_seconds = 5       # Reconnect delay on disconnect&lt;/code&gt;
    &lt;p&gt;Note: Replica mode is mutually exclusive with cluster mode. A replica receives all data via streaming replication but cannot accept writes.&lt;/p&gt;
    &lt;code&gt;[replication]
default_write_consistency = "QUORUM"      # Write consistency level: ONE, QUORUM, ALL
default_read_consistency = "LOCAL_ONE"    # Read consistency level
write_timeout_ms = 5000                   # Write operation timeout
read_timeout_ms = 2000                    # Read operation timeout

# Anti-Entropy: Background healing for eventual consistency
# - Detects and repairs divergence between replicas
# - Uses delta sync for small lags, snapshot for large lags
# - Includes gap detection to prevent incomplete data after GC
enable_anti_entropy = true                 # Enable automatic catch-up for lagging nodes
anti_entropy_interval_seconds = 60         # How often to check for lag (default: 60s)
delta_sync_threshold_transactions = 10000  # Delta sync if lag &amp;lt; 10K txns
delta_sync_threshold_seconds = 3600        # Snapshot if lag &amp;gt; 1 hour

# Garbage Collection: Reclaim disk space by deleting old transaction records
# - gc_min must be &amp;gt;= delta_sync_threshold (validated at startup)
# - gc_max should be &amp;gt;= 2x delta_sync_threshold (recommended)
# - Set gc_max = 0 for unlimited retention
gc_min_retention_hours = 2   # Keep at least 2 hours (&amp;gt;= 1 hour delta threshold)
gc_max_retention_hours = 24  # Force delete after 24 hours&lt;/code&gt;
    &lt;p&gt;Anti-Entropy Tuning:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small clusters (2-3 nodes): Use default settings (60s interval)&lt;/item&gt;
      &lt;item&gt;Large clusters (5+ nodes): Consider increasing interval to 120-180s to reduce network overhead&lt;/item&gt;
      &lt;item&gt;High write throughput: Increase &lt;code&gt;delta_sync_threshold_transactions&lt;/code&gt;to 50000+&lt;/item&gt;
      &lt;item&gt;Long-running clusters: Keep &lt;code&gt;gc_max_retention_hours&lt;/code&gt;at 24+ to handle extended outages&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GC Configuration Rules (Validated at Startup):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;gc_min_retention_hours&lt;/code&gt;must be &amp;gt;=&lt;code&gt;delta_sync_threshold_seconds&lt;/code&gt;(in hours)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gc_max_retention_hours&lt;/code&gt;should be &amp;gt;= 2x&lt;code&gt;delta_sync_threshold_seconds&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Violating these rules will cause startup failure with helpful error messages&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[query_pipeline]
transpiler_cache_size = 10000  # LRU cache for MySQL→SQLite transpilation
validator_pool_size = 8        # SQLite connection pool for validation&lt;/code&gt;
    &lt;code&gt;[mysql]
enabled = true
bind_address = "0.0.0.0"
port = 3306
max_connections = 1000
unix_socket = ""              # Unix socket path (empty = disabled)
unix_socket_perm = 0660       # Socket file permissions
auto_id_mode = "compact"      # "compact" (53-bit, JS-safe) or "extended" (64-bit)&lt;/code&gt;
    &lt;p&gt;Unix Socket Connection (lower latency than TCP):&lt;/p&gt;
    &lt;code&gt;mysql --socket=/tmp/marmot/mysql.sock -u root&lt;/code&gt;
    &lt;code&gt;[publisher]
enabled = false  # Enable CDC publishing to external systems

[[publisher.sinks]]
name = "kafka-main"              # Unique sink name
type = "kafka"                   # "kafka" or "nats"
format = "debezium"              # Debezium-compatible JSON (only option)
brokers = ["localhost:9092"]     # Kafka broker addresses
topic_prefix = "marmot.cdc"      # Topic pattern: {prefix}.{db}.{table}
filter_tables = ["*"]            # Glob patterns for table filtering
filter_databases = ["*"]         # Glob patterns for database filtering
batch_size = 100                 # Events to read per poll cycle
poll_interval_ms = 10            # Polling interval (default: 10ms)
retry_initial_ms = 100           # Initial retry delay on failure
retry_max_ms = 30000             # Max retry delay (30 seconds)
retry_multiplier = 2.0           # Exponential backoff multiplier&lt;/code&gt;
    &lt;p&gt;See the Integrations documentation for details on event format, Kafka/NATS configuration, and use cases.&lt;/p&gt;
    &lt;code&gt;[logging]
verbose = false          # Enable verbose logging
format = "console"       # Log format: console or json&lt;/code&gt;
    &lt;code&gt;[prometheus]
enabled = true  # Metrics served on gRPC port at /metrics endpoint&lt;/code&gt;
    &lt;p&gt;Accessing Metrics:&lt;/p&gt;
    &lt;code&gt;# Metrics are multiplexed with gRPC on the same port
curl http://localhost:8080/metrics

# Prometheus scrape config
scrape_configs:
  - job_name: 'marmot'
    static_configs:
      - targets: ['node1:8080', 'node2:8080', 'node3:8080']&lt;/code&gt;
    &lt;p&gt;See &lt;code&gt;config.toml&lt;/code&gt; for complete configuration reference with detailed comments.&lt;/p&gt;
    &lt;p&gt;Performance benchmarks on a local development machine (Apple M-series, 3-node cluster, single machine):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nodes&lt;/cell&gt;
        &lt;cell&gt;3 (ports 3307, 3308, 3309)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Threads&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;10 ops/transaction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Consistency&lt;/cell&gt;
        &lt;cell&gt;QUORUM&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;4,175 ops/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TX Throughput&lt;/cell&gt;
        &lt;cell&gt;417 tx/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Records Loaded&lt;/cell&gt;
        &lt;cell&gt;200,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Errors&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;3,370 ops/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TX Throughput&lt;/cell&gt;
        &lt;cell&gt;337 tx/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Duration&lt;/cell&gt;
        &lt;cell&gt;120 seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Total Operations&lt;/cell&gt;
        &lt;cell&gt;404,930&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Errors&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Retries&lt;/cell&gt;
        &lt;cell&gt;37 (0.09%)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Operation Distribution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;READ: 20%&lt;/item&gt;
      &lt;item&gt;UPDATE: 30%&lt;/item&gt;
      &lt;item&gt;INSERT: 35%&lt;/item&gt;
      &lt;item&gt;DELETE: 5%&lt;/item&gt;
      &lt;item&gt;UPSERT: 10%&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Percentile&lt;/cell&gt;
        &lt;cell role="head"&gt;Latency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;4.3ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P90&lt;/cell&gt;
        &lt;cell&gt;14.0ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P95&lt;/cell&gt;
        &lt;cell&gt;36.8ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;85.1ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All 3 nodes maintained identical row counts (346,684 rows) throughout the test, confirming consistent replication.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: These benchmarks are from a local development machine with all nodes on the same host. Production deployments across multiple machines will have different characteristics based on network latency. Expect P99 latencies of 50-200ms for cross-region QUORUM writes.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Marmot's SQLite files are standard WAL-mode databases, compatible with Litestream:&lt;/p&gt;
    &lt;code&gt;litestream replicate /path/to/marmot-data/*.db s3://bucket/backup&lt;/code&gt;
    &lt;p&gt;Enable CDC publisher to stream changes to Kafka/NATS, then archive to your preferred storage.&lt;/p&gt;
    &lt;p&gt;Since Marmot uses SQLite with WAL mode, you can safely snapshot the data directory during operation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460676</guid><pubDate>Fri, 02 Jan 2026 02:21:57 +0000</pubDate></item><item><title>FreeBSD: Home NAS, part 1 – configuring ZFS mirror (RAID1)</title><link>https://rtfm.co.ua/en/freebsd-home-nas-part-1-configuring-zfs-mirror-raid1/</link><description>&lt;doc fingerprint="1e35285ba76e8ee5"&gt;
  &lt;main&gt;
    &lt;p&gt;I have an idea to set up a home NAS on FreeBSD.&lt;/p&gt;
    &lt;p&gt;For this purpose, I bought a Lenovo ThinkCentre M720s SFF – it’s quiet, compact, and offers the possibility to install 2 SATA III SSDs plus a separate M.2 slot for an NVMe SSD.&lt;/p&gt;
    &lt;p&gt;What is planned:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;on NVMe SSD: UFS and FreeBSD&lt;/item&gt;
      &lt;item&gt;on SATA SSDs: ZFS with RAID1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While waiting for the drives to arrive, let’s test how it all works on a virtual machine.&lt;/p&gt;
    &lt;p&gt;We will be installing FreeBSD 14.3, although version 15 is already out, but it has some interesting changes that I’ll play with separately.&lt;/p&gt;
    &lt;p&gt;Of course, I could have gone with TrueNAS, which is based on FreeBSD – but I want “vanilla” FreeBSD to do everything manually.&lt;/p&gt;
    &lt;p&gt;All posts in this blog series:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;(current) FreeBSD: Home NAS, part 1 – configuring ZFS mirror (RAID1)&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 2 – introduction to Packet Filter (PF) firewall&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 3 – WireGuard VPN, Linux peer, and routing&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 4 – Local DNS with Unbound&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 5 – ZFS pool, datasets, snapshots, and ZFS monitoring&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 6 – Samba server and client connections&lt;/item&gt;
      &lt;item&gt;… to be continued&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contents&lt;/p&gt;
    &lt;head rend="h1"&gt;Installing FreeBSD via SSH&lt;/head&gt;
    &lt;p&gt;We will perform the installation over SSH using &lt;code&gt;bsdinstall&lt;/code&gt; – boot the system in LiveCD mode, enable SSH, and then proceed with the installation from a workstation laptop.&lt;/p&gt;
    &lt;p&gt;The virtual machine has three disks – mirroring the future ThinkCentre setup:&lt;/p&gt;
    &lt;p&gt;Select Live System:&lt;/p&gt;
    &lt;p&gt;Login as &lt;code&gt;root&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Bring up the network:&lt;/p&gt;
    &lt;quote&gt;# ifconfig em0 up # dhclient em0&lt;/quote&gt;
    &lt;head rend="h2"&gt;Configuring SSH on FreeBSD LiveCD&lt;/head&gt;
    &lt;p&gt;For SSH, we need to set a &lt;code&gt;root&lt;/code&gt; password and make changes to &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt;, but currently, this doesn’t work because the system is mounted as read-only:&lt;/p&gt;
    &lt;p&gt;Check the current partitions:&lt;/p&gt;
    &lt;p&gt;And apply a “dirty hack”:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;mount a new &lt;code&gt;tmpfs&lt;/code&gt;file system in RAM at&lt;code&gt;/mnt&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;copy the contents of &lt;code&gt;/etc&lt;/code&gt;from the LiveCD there&lt;/item&gt;
      &lt;item&gt;mount &lt;code&gt;tmpfs&lt;/code&gt;over&lt;code&gt;/etc&lt;/code&gt;(overlaying the read-only directory from the ISO)&lt;/item&gt;
      &lt;item&gt;copy the prepared files from &lt;code&gt;/mnt&lt;/code&gt;back into the new&lt;code&gt;/etc&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Execute:&lt;/p&gt;
    &lt;quote&gt;# mount -t tmpfs tmpfs /mnt # cp -a /etc/* /mnt/ # mount -t tmpfs tmpfs /etc # cp -a /mnt/* /etc/&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;mount&lt;/code&gt; syntax for &lt;code&gt;tmpfs&lt;/code&gt; is &lt;code&gt;mount -t &amp;lt;fstype&amp;gt; &amp;lt;source&amp;gt; &amp;lt;mountpoint&amp;gt;&lt;/code&gt;. Since the &lt;code&gt;source&lt;/code&gt; value is required, we specify &lt;code&gt;tmpfs&lt;/code&gt; again.&lt;/p&gt;
    &lt;p&gt;Now, set the password with &lt;code&gt;passwd&lt;/code&gt; and start &lt;code&gt;sshd&lt;/code&gt; using &lt;code&gt;onestart&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;# passwd # service sshd onestart&lt;/quote&gt;
    &lt;p&gt;However, SSH will still deny access because &lt;code&gt;root&lt;/code&gt; login is disabled by default:&lt;/p&gt;
    &lt;quote&gt;$ ssh [email protected] ([email protected]) Password for root@: ([email protected]) Password for root@: ([email protected]) Password for root@:&lt;/quote&gt;
    &lt;p&gt;Set &lt;code&gt;PermitRootLogin yes&lt;/code&gt; in &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt; and restart &lt;code&gt;sshd&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;# echo "PermitRootLogin yes" &amp;gt;&amp;gt; /etc/ssh/sshd_config # service sshd onerestart&lt;/quote&gt;
    &lt;p&gt;Now we can log in:&lt;/p&gt;
    &lt;quote&gt;$ ssh [email protected] ([email protected]) Password for root@: Last login: Sun Dec 7 12:19:25 2025 FreeBSD 14.3-RELEASE (GENERIC) releng/14.3-n271432-8c9ce319fef7 Welcome to FreeBSD! ... root@:~ #&lt;/quote&gt;
    &lt;head rend="h1"&gt;Installation with &lt;code&gt;bsdinstall&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Run &lt;code&gt;bsdinstall&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;# bsdinstall&lt;/quote&gt;
    &lt;p&gt;Select the components to add to the system – &lt;code&gt;ports&lt;/code&gt; is necessary, &lt;code&gt;src&lt;/code&gt; is optional but definitely worth it for a real NAS:&lt;/p&gt;
    &lt;head rend="h2"&gt;Disk partitioning&lt;/head&gt;
    &lt;p&gt;We’ll do a minimal disk partition, so select Manual:&lt;/p&gt;
    &lt;p&gt;We will install the system on &lt;code&gt;ada0&lt;/code&gt;, select it, and click Create:&lt;/p&gt;
    &lt;p&gt;Next, choose a partition scheme. It’s standard for 2025 – GPT:&lt;/p&gt;
    &lt;p&gt;Confirm the changes, and now we have a new partition table on the system drive &lt;code&gt;ada0&lt;/code&gt;:&lt;/p&gt;
    &lt;head rend="h3"&gt;The &lt;code&gt;freebsd-boot&lt;/code&gt; Partition&lt;/head&gt;
    &lt;p&gt;Now we need to create the partitions themselves.&lt;/p&gt;
    &lt;p&gt;Select &lt;code&gt;ada0&lt;/code&gt; again, click Create, and create a partition for &lt;code&gt;freebsd-boot&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is just for the virtual machine; on the actual ThinkCentre, we would use type &lt;code&gt;efi&lt;/code&gt; with a size of about 200-500 MB.&lt;/p&gt;
    &lt;p&gt;For now, set:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;freebsd-boot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Size: 512K&lt;/item&gt;
      &lt;item&gt;Mountpoint: empty&lt;/item&gt;
      &lt;item&gt;Label: empty&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Confirm and proceed to the next partition.&lt;/p&gt;
    &lt;head rend="h3"&gt;The &lt;code&gt;freebsd-swap&lt;/code&gt; Partition&lt;/head&gt;
    &lt;p&gt;Click Create again to add Swap.&lt;/p&gt;
    &lt;p&gt;Given that on the ThinkCentre we will have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8 – 16 GB RAM&lt;/item&gt;
      &lt;item&gt;no sleep/hibernate&lt;/item&gt;
      &lt;item&gt;UFS and ZFS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2 gigabytes will be enough.&lt;/p&gt;
    &lt;p&gt;Set:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;freebsd-swap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Size: 2GB&lt;/item&gt;
      &lt;item&gt;Mountpoint: empty&lt;/item&gt;
      &lt;item&gt;Label: empty&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Root Partition with UFS&lt;/head&gt;
    &lt;p&gt;The main system will be on UFS because it is very stable, doesn’t require much RAM, mounts quickly, is easy to recover, and lacks complex caching mechanisms (UPD: however, after getting to know ZFS and its capabilities better, I decided to use it for the system disk as well)&lt;/p&gt;
    &lt;p&gt;Set:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;freebsd-ufs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Size: 14GB&lt;/item&gt;
      &lt;item&gt;Mountpoint: /&lt;/item&gt;
      &lt;item&gt;Label: rootfs – just a name for us&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’ll configure the rest of the disks later; for now, select Finish and Commit:&lt;/p&gt;
    &lt;head rend="h2"&gt;Finishing Installation&lt;/head&gt;
    &lt;p&gt;Wait for the copying to complete:&lt;/p&gt;
    &lt;p&gt;Configure the network:&lt;/p&gt;
    &lt;p&gt;Select Timezone:&lt;/p&gt;
    &lt;p&gt;In System Configuration – select &lt;code&gt;sshd&lt;/code&gt;, no mouse, enable &lt;code&gt;ntpd&lt;/code&gt; and &lt;code&gt;powerd&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;System Hardening – considering this will be a home NAS, but I might open external access (even behind a firewall), it makes sense to tune the security a bit:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;read_msgbuf&lt;/code&gt;: allow&lt;code&gt;dmesg&lt;/code&gt;access for root only&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;proc_debug&lt;/code&gt;: allow&lt;code&gt;ptrace&lt;/code&gt;for root only&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;random_pid&lt;/code&gt;: randomize PID numbers&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clear_tmp&lt;/code&gt;: clear&lt;code&gt;/tmp&lt;/code&gt;on reboot&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;secure_console&lt;/code&gt;: require&lt;code&gt;root&lt;/code&gt;password for login from the physical console&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add a user:&lt;/p&gt;
    &lt;p&gt;Everything is ready – reboot the machine:&lt;/p&gt;
    &lt;head rend="h1"&gt;Creating a ZFS RAID&lt;/head&gt;
    &lt;p&gt;Log in as the regular user:&lt;/p&gt;
    &lt;quote&gt;$ ssh [email protected] ... FreeBSD 14.3-RELEASE (GENERIC) releng/14.3-n271432-8c9ce319fef7 Welcome to FreeBSD! ... setevoy@test-nas-1:~ $&lt;/quote&gt;
    &lt;p&gt;Install &lt;code&gt;vim&lt;/code&gt; 🙂&lt;/p&gt;
    &lt;quote&gt;# pkg install vim&lt;/quote&gt;
    &lt;p&gt;Check our disks.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;geom disk&lt;/code&gt; for physical device info, and &lt;code&gt;gpart show&lt;/code&gt; to see partitions on the disks.&lt;/p&gt;
    &lt;p&gt;Check disks – there are three:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # geom disk list Geom name: ada0 Providers: 1. Name: ada0 Mediasize: 17179869184 (16G) Sectorsize: 512 Mode: r2w2e3 descr: VBOX HARDDISK ident: VB262b53f7-adc5cd2c rotationrate: unknown fwsectors: 63 fwheads: 16 Geom name: ada1 Providers: 1. Name: ada1 Mediasize: 17179869184 (16G) Sectorsize: 512 Mode: r0w0e0 descr: VBOX HARDDISK ident: VB059f9d08-4b0e1f56 rotationrate: unknown fwsectors: 63 fwheads: 16 Geom name: ada2 Providers: 1. Name: ada2 Mediasize: 17179869184 (16G) Sectorsize: 512 Mode: r0w0e0 descr: VBOX HARDDISK ident: VB3941028c-3ea0d485 rotationrate: unknown fwsectors: 63 fwheads: 16&lt;/quote&gt;
    &lt;p&gt;And with &lt;code&gt;gpart&lt;/code&gt; – current &lt;code&gt;ada0&lt;/code&gt; where the system was installed:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show =&amp;gt; 40 33554352 ada0 GPT (16G) 40 1024 1 freebsd-boot (512K) 1064 4194304 2 freebsd-swap (2.0G) 4195368 29359024 3 freebsd-ufs (14G)&lt;/quote&gt;
    &lt;p&gt;Disks &lt;code&gt;ada1&lt;/code&gt; and &lt;code&gt;ada2&lt;/code&gt; will be used for ZFS and its mirror (RAID1).&lt;/p&gt;
    &lt;p&gt;If there was anything on them – wipe it:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart destroy -F ada1 gpart: arg0 'ada1': Invalid argument root@test-nas-1:/home/setevoy # gpart destroy -F ada2 gpart: arg0 'ada2': Invalid argument&lt;/quote&gt;
    &lt;p&gt;Since this is a VM and the disks are empty, “Invalid argument” is expected and fine.&lt;/p&gt;
    &lt;p&gt;Create GPT partition tables on &lt;code&gt;ada1&lt;/code&gt; and &lt;code&gt;ada2&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart create -s gpt ada1 ada1 created root@test-nas-1:/home/setevoy # gpart create -s gpt ada2 ada2 created&lt;/quote&gt;
    &lt;p&gt;Check:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show ada1 =&amp;gt; 40 33554352 ada1 GPT (16G) 40 33554352 - free - (16G)&lt;/quote&gt;
    &lt;p&gt;Create partitions for ZFS:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart add -t freebsd-zfs ada1 ada1p1 added root@test-nas-1:/home/setevoy # gpart add -t freebsd-zfs ada2 ada2p1 added&lt;/quote&gt;
    &lt;p&gt;Check again:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show ada1 =&amp;gt; 40 33554352 ada1 GPT (16G) 40 33554352 1 freebsd-zfs (16G)&lt;/quote&gt;
    &lt;head rend="h2"&gt;Creating a ZFS mirror with &lt;code&gt;zpool&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The “magic” of ZFS is that everything works “out of the box” – you don’t need a separate LVM and its groups, and you don’t need &lt;code&gt;mdadm&lt;/code&gt; for RAID.&lt;/p&gt;
    &lt;p&gt;For managing disks in ZFS, the main utility is &lt;code&gt;zpool&lt;/code&gt;, and for managing data (datasets, file systems, snapshots), it’s &lt;code&gt;zfs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To combine one or more disks into a single logical storage, ZFS uses a pool – the equivalent of a volume group in Linux LVM.&lt;/p&gt;
    &lt;p&gt;Create the pool:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zpool create tank mirror ada1p1 ada2p1&lt;/quote&gt;
    &lt;p&gt;Here, tank is the pool name, &lt;code&gt;mirror&lt;/code&gt; specifies that it will be RAID1, and we provide the list of partitions included in this pool.&lt;/p&gt;
    &lt;p&gt;Check:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zpool status pool: tank state: ONLINE config: NAME STATE READ WRITE CKSUM tank ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 ada1p1 ONLINE 0 0 0 ada2p1 ONLINE 0 0 0 errors: No known data errors&lt;/quote&gt;
    &lt;p&gt;ZFS immediately mounts this pool at &lt;code&gt;/tank&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # mount /dev/ada0p3 on / (ufs, local, soft-updates, journaled soft-updates) devfs on /dev (devfs) tank on /tank (zfs, local, nfsv4acls)&lt;/quote&gt;
    &lt;p&gt;Check partitions now:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show =&amp;gt; 40 33554352 ada0 GPT (16G) 40 1024 1 freebsd-boot (512K) 1064 4194304 2 freebsd-swap (2.0G) 4195368 29359024 3 freebsd-ufs (14G) =&amp;gt; 40 33554352 ada1 GPT (16G) 40 33554352 1 freebsd-zfs (16G) =&amp;gt; 40 33554352 ada2 GPT (16G) 40 33554352 1 freebsd-zfs (16G)&lt;/quote&gt;
    &lt;p&gt;If we want to change the mountpoint – execute &lt;code&gt;zfs set mountpoint&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zfs set mountpoint=/data tank&lt;/quote&gt;
    &lt;p&gt;And it immediately mounts to the new directory:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # mount /dev/ada0p3 on / (ufs, local, soft-updates, journaled soft-updates) devfs on /dev (devfs) tank on /data (zfs, local, nfsv4acls)&lt;/quote&gt;
    &lt;p&gt;Enable data compression – useful for a NAS, see Compression and Compressing ZFS File Systems.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;lz4&lt;/code&gt; is the current default option, let’s enable it:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zfs set compression=lz4 tank&lt;/quote&gt;
    &lt;p&gt;Since we installed the system on UFS, we need to add a few parameters to autostart for ZFS to work.&lt;/p&gt;
    &lt;p&gt;Configure the boot loader in &lt;code&gt;/boot/loader.conf&lt;/code&gt; to load kernel modules:&lt;/p&gt;
    &lt;quote&gt;zfs_load="YES"&lt;/quote&gt;
    &lt;p&gt;Or, to avoid manual editing, use &lt;code&gt;sysrc&lt;/code&gt; with the &lt;code&gt;-f&lt;/code&gt; flag:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # sysrc -f /boot/loader.conf zfs_load="YES"&lt;/quote&gt;
    &lt;p&gt;And add to &lt;code&gt;/etc/rc.conf&lt;/code&gt; to start the &lt;code&gt;zfsd&lt;/code&gt; daemon and mount the file systems:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # sysrc zfs_enable="YES" zfs_enable: NO -&amp;gt; YES&lt;/quote&gt;
    &lt;p&gt;Reboot and check:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zpool status pool: tank state: ONLINE config: NAME STATE READ WRITE CKSUM tank ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 ada1p1 ONLINE 0 0 0 ada2p1 ONLINE 0 0 0&lt;/quote&gt;
    &lt;p&gt;Everything is in place.&lt;/p&gt;
    &lt;p&gt;Now you can proceed with further tuning – configuring separate datasets, snapshots, etc.&lt;/p&gt;
    &lt;p&gt;For a Web UI, you could try Seafile or FileBrowser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46462108</guid><pubDate>Fri, 02 Jan 2026 06:48:32 +0000</pubDate></item><item><title>Standard Ebooks: Public Domain Day 2026 in Literature</title><link>https://standardebooks.org/blog/public-domain-day-2026</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46462702</guid><pubDate>Fri, 02 Jan 2026 08:40:41 +0000</pubDate></item><item><title>HPV vaccination reduces oncogenic HPV16/18 prevalence from 16% to &lt;1% in Denmark</title><link>https://www.eurosurveillance.org/content/10.2807/1560-7917.ES.2025.30.27.2400820</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46463315</guid><pubDate>Fri, 02 Jan 2026 10:10:46 +0000</pubDate></item><item><title>One Number I Trust: Plain-Text Accounting for a Multi-Currency Household</title><link>https://lalitm.com/post/one-number-i-trust/</link><description>&lt;doc fingerprint="2f08811d87a33d15"&gt;
  &lt;main&gt;
    &lt;p&gt;Two people. Eighteen accounts spanning checking, savings, credit cards, investments. Three currencies. Twenty minutes of work every week.&lt;/p&gt;
    &lt;p&gt;One net worth number I actually trust.&lt;/p&gt;
    &lt;p&gt;The payoff: A single, trustworthy net worth number growing over time.&lt;/p&gt;
    &lt;p&gt;No app did exactly what I needed, so I built my own personal finance system using plain-text accounting principles and a powerful Python library called Beancount. This post shows you how I handle imports, investments, multi-currency, and a two-person view.&lt;/p&gt;
    &lt;head rend="h2"&gt;How I got here&lt;/head&gt;
    &lt;p&gt;It all started during the 2021 tax season. I had blocked out an entire weekend and was juggling statements, trying to compute capital gains, stressing about getting the numbers mixed up. “This is chaos”, I thought. “There must be a way to simplify this with automation”. Being a software engineer, I did what felt natural and hacked together a bunch of scripts on top of a database.&lt;/p&gt;
    &lt;p&gt;Though it worked and I kept using it day-to-day, by the next tax season the cracks became obvious. The code was hard to debug, random transactions went missing, and worst of all, the balances the scripts computed didn’t match the balances on my statements. I tried to fix it but the more I tried, the more I felt lost about what the system was really doing. Eventually I just gave up.&lt;/p&gt;
    &lt;p&gt;Why did I fail so spectacularly? My entire approach was flawed from the start! I’d ignored centuries of accounting wisdom and repeated fundamental mistakes humanity solved long ago. So I learned from my mistakes and did the research. And over time I incrementally discovered double-entry bookkeeping, plain-text accounting and Beancount.&lt;/p&gt;
    &lt;p&gt;Fast forward to today, and I have a flexible, powerful, and private system, fully customized to how my brain works. Most transactions import automatically from PDF statements (counterintuitively, it’s often more reliable than CSV!). Tax time is a simple matter of checking always-fresh reports and copying numbers over. The weekly ritual is simple: download statements, categorize transactions in a web UI, run a bunch of scripts to regenerate, commit (I walk through this in more detail later).&lt;/p&gt;
    &lt;p&gt;However, I want to be realistic: building a system like this takes time and effort.1 You will need to learn some basic accounting concepts, be comfortable with Python, and consistently spend time every week keeping things up-to-date. If your finances are simple or you just want day-to-day budgeting, this is almost certainly overkill. Apps like YNAB or even the humble spreadsheet work great.&lt;/p&gt;
    &lt;p&gt;But if you want uncompromising control over how you look at your finances, read on.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 1: The Concepts&lt;/head&gt;
    &lt;head rend="h4"&gt;Double-entry bookkeeping&lt;/head&gt;
    &lt;p&gt;Suppose on a Saturday, I transfer money from my checking account to a savings account. The money leaves on the same day but doesn’t show up on the other side until Monday. So where was it for those two days?&lt;/p&gt;
    &lt;p&gt;In a “normal”2 personal finance system, the answer would be that it was just gone. That is, for those two days, there would be a drop in the total money in two accounts. But this is weird because in reality my “net worth” did not change, yet there’s no good way to represent this.&lt;/p&gt;
    &lt;p&gt;Or suppose I pay $90 for a dinner for me and two friends. They pay me back a week later. Again, in this case the money is “gone” for that week. And even worse, the full $90 would be categorized as a “restaurant expense” while each $30 my friends paid would be “income”. But this is wrong. My expense is just $30 and the money they give me should be matched against the $60 they owe me.&lt;/p&gt;
    &lt;p&gt;Both of these are fundamental problems with how so-called “single-entry bookkeeping” works: each account’s transactions and balance are tracked individually but without the context of the “whole”. In the case of the transfer, because we’re looking at each account in isolation, we lose the fact that even though the money has left one account, it’s really still part of the “pool of money” that belongs to us. Similarly, when our friends pay us back, we’re not tracking the fact that our friends owe us money when the original transaction happened and their payment later neutralizes the debt.&lt;/p&gt;
    &lt;p&gt;Double-entry bookkeeping is the solution to both these problems. Businesses have been using it for hundreds of years3 to run their accounts, and it has powerful yet elegant ways to solve these problems and many others too.&lt;/p&gt;
    &lt;p&gt;Let’s consider again the transfer. In double-entry bookkeeping, we would represent the initial move of money as:&lt;/p&gt;
    &lt;code&gt;Bank-Checking              -1000
Transfer-In-Flight         +1000
&lt;/code&gt;
    &lt;p&gt;And when it arrives:&lt;/p&gt;
    &lt;code&gt;Transfer-In-Flight         -1000
Savings-Account            +1000
&lt;/code&gt;
    &lt;p&gt;In both cases, we see the “golden rules” of double-entry bookkeeping:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Every transaction has at least two sides and the sum of all the sides is zero. -1000 + 1000 = 0. That is, transactions always “balance”.&lt;/item&gt;
      &lt;item&gt;Every side of a transaction is an account, whether it exists in the real world or not.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It should be clear that “Bank-Checking” and “Savings-Account” are labels for your checking and savings accounts, respectively. But what is “Transfer-In-Flight”?&lt;/p&gt;
    &lt;p&gt;Well, it’s also an account! It’s not an account you’ll find on your bank’s website, but within the double-entry system, it’s just as real. Concretely, accounts in double-entry are just labels for a “bucket of money”. So there’s no “category to put this transaction under”, no “expense tracking”, no special “transfer tag”. Everything is an account.&lt;/p&gt;
    &lt;p&gt;In this specific case, &lt;code&gt;Bank-Checking&lt;/code&gt;, &lt;code&gt;Savings-Account&lt;/code&gt;, and &lt;code&gt;Transfer-In-Flight&lt;/code&gt; are all a specific type of account: they are Asset accounts. Assets are stuff you own; these can be real accounts (bank account, savings, stocks, bonds) or conceptual accounts (money in transit between accounts).&lt;/p&gt;
    &lt;p&gt;Now let’s consider the dinner example. There are 4 sides to the transaction:&lt;/p&gt;
    &lt;code&gt;Credit-Card                  -90
Restaurant-Expense           +30
Owes-Me:Alice                +30
Owes-Me:Bob                  +30
&lt;/code&gt;
    &lt;p&gt;Again, -90 + 30 + 30 + 30 = 0. It balances. All of &lt;code&gt;Credit-Card&lt;/code&gt;, &lt;code&gt;Restaurant-Expense&lt;/code&gt;, &lt;code&gt;Owes-Me:Alice&lt;/code&gt;, and &lt;code&gt;Owes-Me:Bob&lt;/code&gt; are just accounts.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Credit-Card&lt;/code&gt; is a different type of account though: it’s a Liability. Liabilities are the opposite of assets: instead of stuff you own, they’re stuff you owe to someone else. So for example, loans, credit cards, and mortgages are all liabilities.&lt;/p&gt;
    &lt;p&gt;Why negative 90? The rule is always: negative means money flowed from this account and positive means it flowed into this account. The credit card company fronted you $90, so that money flowed from your credit card to fund the purchase.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Restaurant-Expense&lt;/code&gt; is yet another type of account, an Expense account. Expense accounts are money “leaving your world”. So any time you spend some money and you no longer have access to it, that’s an expense.&lt;/p&gt;
    &lt;p&gt;Finally, &lt;code&gt;Owes-Me:Alice&lt;/code&gt; and &lt;code&gt;Owes-Me:Bob&lt;/code&gt; are also Assets. Alice and Bob have promised to pay you back, and that promise has value, $30 each. It’s not cash in your pocket, but it’s money you have a claim on. In double-entry, anything with economic value you control is an asset, whether it’s a bank balance or an IOU.&lt;/p&gt;
    &lt;p&gt;Later, when Alice pays you back:&lt;/p&gt;
    &lt;code&gt;Bank-Checking               +30
Owes-Me:Alice               -30
&lt;/code&gt;
    &lt;p&gt;This is just money moving from the “virtual” &lt;code&gt;Owes-Me:Alice&lt;/code&gt; to the “real” &lt;code&gt;Bank-Checking&lt;/code&gt; account. Both of these are still assets; it’s just the type of asset that’s changing. So no money has “entered the system” at this point. You’re just settling the debt Alice owed you.&lt;/p&gt;
    &lt;p&gt;Let’s take one last example: a paycheck.&lt;/p&gt;
    &lt;code&gt;Bank-Checking              +3000
Salary                     -3000
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Bank-Checking&lt;/code&gt; is an Asset as we’ve learned. But &lt;code&gt;Salary&lt;/code&gt; is a new account type, an Income account. Just like Assets and Liabilities are opposites, so are Income and Expenses. Where Expenses are money leaving your world, Income is money entering it.&lt;/p&gt;
    &lt;p&gt;Money flowed from Salary (source, negative) to Bank-Checking (destination, positive). The sign feels backwards: “I received money, so why is Income negative?” Because the sign shows direction of flow: income is where the money came from, and your bank is where it went to.&lt;/p&gt;
    &lt;p&gt;This is the one part of double-entry that takes repetition.4 Don’t try to make it intuitive; just trust the invariant: if your transaction sums to zero, you’ve got the signs right. After a dozen transactions, the pattern becomes automatic.&lt;/p&gt;
    &lt;p&gt;These four types of accounts cover 99% of what you’ll do:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Assets: stuff you own (bank accounts, cash, investments, money owed to you)&lt;/item&gt;
      &lt;item&gt;Liabilities: stuff you owe (credit cards, loans)&lt;/item&gt;
      &lt;item&gt;Income: money entering your world (salary, interest, dividends)&lt;/item&gt;
      &lt;item&gt;Expenses: money leaving your world (groceries, rent, restaurants)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There’s a fifth type, Equity, which is a catch-all “this money doesn’t fit elsewhere” bucket.5 Suppose you start tracking an account that already has $1000 in it; that money came from somewhere but you don’t have a record of that. It can’t be income because you already had it, and the other types don’t fit. That’s a good sign it belongs in equity. The good news is that you rarely interact with Equity directly. Generally, the software handles it for you, but there are some exceptions that we’ll cover in Chapter 2.&lt;/p&gt;
    &lt;p&gt;There’s so much more that could be said about double-entry bookkeeping. For further reading, I particularly like the double entry explainer in the Beancount docs. It goes through some more examples and expands into a bunch of related topics.&lt;/p&gt;
    &lt;p&gt;But we now have the foundation which ensures that every transaction balances, every dollar is accounted for, and nothing slips through the cracks. But we still need a way to actually record and store these transactions.&lt;/p&gt;
    &lt;head rend="h4"&gt;Plain text accounting&lt;/head&gt;
    &lt;p&gt;One of the things I learned from writing my own finance system is that auditability is king. You need the ability to eyeball a transaction, ask yourself “does this look right,” and fix it if it doesn’t. And nothing beats being able to see and edit any transaction you’ve ever made in a text editor.&lt;/p&gt;
    &lt;p&gt;That’s one of the main things that drew me to the philosophy of Plain Text Accounting. This is a set of principles on using plain text files as the “immutable source of truth” of your finances and then building scripts and tools on top of them to process, analyze, and visualize them.&lt;/p&gt;
    &lt;p&gt;There are many other advantages to this I’ve come to appreciate over the years:&lt;/p&gt;
    &lt;p&gt;Everything is version controlled. You can store these transaction files in a git repo, which has powerful effects. You can look at diffs to see what changed on any day. You can &lt;code&gt;git blame&lt;/code&gt; any transaction to see when and why it was added. You can &lt;code&gt;git tag&lt;/code&gt; important states of the repo (e.g. when taxes were filed, when a new job was started, when a big refactoring happened). You can &lt;code&gt;git checkout&lt;/code&gt; any previous state to see e.g. “how did my repo look last year”.&lt;/p&gt;
    &lt;p&gt;It’s private so you never have to trust any third party with all your financial details. Everything can be stored in locations that you fully control.&lt;/p&gt;
    &lt;p&gt;There’s no lock-in. Because everything is just a plain text file, it’s trivially easy to change how you want things to be represented: you don’t have to deal with apps with broken or messy CSV exports making it difficult to take your data elsewhere.&lt;/p&gt;
    &lt;p&gt;It’s scriptable. If you want to refactor something, compute a new breakdown or even rewrite your system entirely, all you need is to write a script. Whether you write it yourself or prompt an LLM to do it for you, the text-based format makes automation trivial.&lt;/p&gt;
    &lt;p&gt;Plain text gives you the abstract idea of “storing transactions in text” but there’s still a bunch of questions. What’s the transaction syntax? How do you parse your files? How do you validate that everything balances, compute totals, and let you query the results? That’s where Beancount comes in.&lt;/p&gt;
    &lt;head rend="h4"&gt;Introducing Beancount!&lt;/head&gt;
    &lt;p&gt;Over the years, people have written many plain-text accounting tools which answer all the questions above. The main ones you’ll find which have gained a lot of popularity are Ledger, hledger, and Beancount. I’ve used each of them at some point in my plain text journey and all are solid choices. But I ended up on Beancount for a few reasons.&lt;/p&gt;
    &lt;p&gt;It’s a Python library, not just a command-line tool. I can write importers that parse my bank’s PDF statements, generate transactions programmatically, and build custom reports, all in a language I already know.&lt;/p&gt;
    &lt;p&gt;Strictness by default. Accounts must be declared before use, so typos get caught immediately. Transactions must balance and there are immediate error messages if they don’t. The tool catches mistakes early rather than letting them propagate.&lt;/p&gt;
    &lt;p&gt;Plugin and tool ecosystem. Beancount has a very rich set of libraries and tools which build on top of and integrate with it. Along with the core project, you get access to any and all of these projects you want to use. We’ll discuss this much more later.&lt;/p&gt;
    &lt;p&gt;Fava. The web UI which sits on top of the Beancount engine. It’s so good that people convert from other formats (using tools like ledger2beancount or gnucash2beancount) just to use it. Where Beancount gives you the reliable engine, Fava gives you the pretty yet powerful frontend:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reports: balance sheet, income statement, transaction journal&lt;/item&gt;
      &lt;item&gt;Query editor: SQL-like queries, exportable to CSV&lt;/item&gt;
      &lt;item&gt;Charts: spending breakdowns, net worth over time, holdings by currency&lt;/item&gt;
      &lt;item&gt;Error highlighting: problems highlighted immediately&lt;/item&gt;
      &lt;item&gt;Extensibility: plugins like fava-dashboards and fava-portfolio-returns&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With this, we now understand enough of the basic concepts for us to get started trying out Beancount!&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 2: Getting Started&lt;/head&gt;
    &lt;p&gt;The best way to learn plain-text accounting is to roll up your sleeves and try it out. So let’s pause the theory for a moment and get a real ledger running on your machine.&lt;/p&gt;
    &lt;p&gt;I’ve built a companion repository, LalitMaganti/beancount-blog-examples, which contains a cut-down version of the system I use day-to-day. The repo is organized into folders that match the chapters of this post (&lt;code&gt;chapter-2/&lt;/code&gt;, &lt;code&gt;chapter-3/&lt;/code&gt;, etc.), each building on the previous. It also includes a &lt;code&gt;demo/&lt;/code&gt; folder with 2+ years of synthetic history if you want to see the end result immediately.&lt;/p&gt;
    &lt;p&gt;To get started, clone the repository:&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/LalitMaganti/beancount-blog-examples.git
cd beancount-blog-examples

# Run the demo to see the end result
./scripts/quickstart.sh demo

# Or start Chapter 2 to follow the guide
./scripts/quickstart.sh chapter-2
&lt;/code&gt;
    &lt;p&gt;This will set up a Python environment, install dependencies, and open Fava at http://localhost:5000. Here’s what you should see.&lt;/p&gt;
    &lt;p&gt;Fava's Trial Balance view shows assets, income, and expenses in one place.&lt;/p&gt;
    &lt;p&gt;Sunburst charts make it easy to spot your biggest expense categories instantly.&lt;/p&gt;
    &lt;p&gt;Click around the different tabs. You’ll find that Fava gives you the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Balance sheet: the state of your accounts at the current point in time. Basically think of it as an aggregate view of all your finances. You can answer questions like “what’s my net worth now?”, “how much money do I owe across my credit cards?” or “how much have I gained from investments?”&lt;/item&gt;
      &lt;item&gt;Income statement: the sum of all the money flows into/out of your accounts. Think of it as a sum of all the income and expenses across time. You can answer questions like “how much did I earn from my job?”, “how much did I spend on Amazon?” or “did I spend more or less than last year?”&lt;/item&gt;
      &lt;item&gt;Transaction history: a flat list of all transactions you have in your journal. A way to see and search any transaction you’ve made across any accounts in your system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Query Console lets you run SQL-like queries against your financial data.&lt;/p&gt;
    &lt;p&gt;There’s much more to Fava’s features (queries, multi-currency, plugins) as we’ll see later on in the post.&lt;/p&gt;
    &lt;p&gt;You can also explore the text journal itself using a text editor. For example, here’s a grocery run and a payslip in the Beancount transaction format:&lt;/p&gt;
    &lt;code&gt;; chapter-2/src/transactions.beancount

; Beancount auto-fills the second amount when it can be inferred.
2024-01-15 * "Tesco" "Weekly groceries"
  Expenses:Groceries                    85.50 GBP
  Assets:Lalit:UK:HSBC:Current:GBP

2024-01-25 * "Google" "January salary"
  Assets:Lalit:UK:HSBC:Current:GBP    3200.00 GBP
  Income:Lalit:UK:Google:Salary
&lt;/code&gt;
    &lt;p&gt;Go through the transactions and get a feel for the format. It might seem alien at first but trust me when I say soon it’ll feel like the most natural thing in the world!&lt;/p&gt;
    &lt;p&gt;With a VS Code extension, you get syntax highlighting and auto-completion for your accounts.&lt;/p&gt;
    &lt;p&gt;Now that you have a working system, I want to share the hard-won insights that aren’t in the official docs. This post isn’t going to be a full Beancount tutorial. The official docs are excellent for that (Fava even has a demo that you can try without downloading anything!).&lt;/p&gt;
    &lt;p&gt;Instead, I want to focus on the architecture: the decisions I wish I’d made correctly from day one. I’m writing this as if I’m speaking to my past self.&lt;/p&gt;
    &lt;head rend="h4"&gt;Start with one account&lt;/head&gt;
    &lt;p&gt;You don’t need to track everything on day one. Pick one account. Your main checking account is a good start. Get comfortable with the flow. Import statements, categorize transactions, check that balances match. Once that feels solid, add another account. Then another.&lt;/p&gt;
    &lt;p&gt;I started with my HSBC current account. Now, I have my whole financial life inside the system and I trust it wholeheartedly. But this happened one account at a time. If I tried to do everything in one go, I would certainly have been overwhelmed and given up on the whole thing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Opening balances&lt;/head&gt;
    &lt;p&gt;Once you’ve picked your first account, you face an immediate problem: you may have opened that account years ago and there might already be thousands of transactions over that time. Trying to import them all in one go is another sure path to being overwhelmed and giving up.&lt;/p&gt;
    &lt;p&gt;Instead, a better idea is to pick a “starting date” at which you say “I will import everything from this day onwards”. But that poses its own problem: you already had money in that account, how do you tell Beancount it exists?&lt;/p&gt;
    &lt;p&gt;Well, Beancount has a &lt;code&gt;pad&lt;/code&gt; directive that creates the balancing entry automatically:&lt;/p&gt;
    &lt;code&gt;; chapter-2/src/balance.beancount
2024-01-01 open Equity:Opening-Balances
2024-01-01 pad Assets:Lalit:UK:HSBC:Current:GBP Equity:Opening-Balances
2024-01-02 balance Assets:Lalit:UK:HSBC:Current:GBP  1500.00 GBP
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;pad&lt;/code&gt; directive tells Beancount: “whatever amount is needed to make the balance assertion true, take it from &lt;code&gt;Equity:Opening-Balances&lt;/code&gt; and put it in this account”. This is one of the rare cases you actually have to think about equity accounts (though not much beyond blindly using &lt;code&gt;Equity:Opening-Balances&lt;/code&gt;!).&lt;/p&gt;
    &lt;head rend="h3"&gt;Structure That Scales&lt;/head&gt;
    &lt;p&gt;Adding a first account is easy and the second is straightforward, but adding a third, fourth, fifth… and you can easily find that things start becoming jumbled and messy. Just like code, putting a little bit of thought into the organization upfront goes a long way. This part covers the architectural decisions you’ll thank yourself later for.&lt;/p&gt;
    &lt;head rend="h4"&gt;Naming Asset and Liability accounts&lt;/head&gt;
    &lt;p&gt;The structure of asset and liability account names is very important, much more than I initially gave them credit for. It’s a good idea to keep as much information in them as possible. Here’s what I’ve settled on:&lt;/p&gt;
    &lt;code&gt;; chapter-2/src/accounts.beancount
2024-01-01 open Assets:Lalit:UK:HSBC:Current:GBP          GBP
2024-01-01 open Assets:Lalit:UK:Barclays:Current:GBP      GBP
2024-01-01 open Liabilities:Lalit:UK:AMEX:GBP             GBP
&lt;/code&gt;
    &lt;p&gt;The pattern is:&lt;/p&gt;
    &lt;code&gt;Type:Person:Region:Institution:Account:Currency
&lt;/code&gt;
    &lt;p&gt;Why this structure? Because it’s a lot easier to remove detail than add it in later! I initially started by not having the country, the currency or my name in the account. But over time, I wanted to understand:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How much money I have in the UK vs the US?&lt;/item&gt;
      &lt;item&gt;How much cash (i.e. not investments) is in a certain currency?&lt;/item&gt;
      &lt;item&gt;How much of our household wealth was in my wife’s accounts vs my own (discussed in more detail in Chapter 6)?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Having this information in the account name is great because Beancount’s SQL syntax makes it very easy to filter on account names. Want “all UK assets”? Filter on &lt;code&gt;:UK:&lt;/code&gt;. Want “all HSBC accounts”? Filter on &lt;code&gt;:HSBC:&lt;/code&gt;. Want “all GBP cash”? Filter on &lt;code&gt;:GBP&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h4"&gt;Powering up with Plugins&lt;/head&gt;
    &lt;p&gt;Once you have a structure, you want to ensure it stays clean. This is where Beancount’s Plugins come in.&lt;/p&gt;
    &lt;p&gt;Plugins are Python scripts that run when your ledger loads. They can validate data, modify entries, or even generate new transactions automatically. You load them in your journal file like this:&lt;/p&gt;
    &lt;code&gt;plugin "beancount.plugins.check_commodity"
&lt;/code&gt;
    &lt;p&gt;Remember the transfer-in-flight pattern from Chapter 1? Here’s how it looks in Beancount:&lt;/p&gt;
    &lt;code&gt;; chapter-2/src/transactions.beancount
; Money leaves on Saturday
2024-03-16 * "Transfer to Barclays"
  Assets:Lalit:UK:HSBC:Current:GBP      -1000.00 GBP
  Assets:Lalit:Transfers:Internal        1000.00 GBP

; Money arrives on Monday
2024-03-18 * "Transfer from HSBC"
  Assets:Lalit:Transfers:Internal       -1000.00 GBP
  Assets:Lalit:UK:Barclays:Current:GBP   1000.00 GBP
&lt;/code&gt;
    &lt;p&gt;Once both transactions are recorded, the transit account balance returns to zero, confirming the transfer is complete. If you record one leg of a transfer but forget the other, the transit account will simply show a non-zero balance.&lt;/p&gt;
    &lt;p&gt;Understanding where a non-zero balance in transfers is coming from is handled by my absolute favorite plugin, beancount_reds_plugins.zerosum. It’s responsible for matching both sides of a transaction and moving it to a separate account, meaning my &lt;code&gt;Transfers:Internal&lt;/code&gt; account only contains the actual “pending” transactions. Making this account empty is a surprisingly satisfying little “minigame” during my weekly imports (though it never lasts for long!).&lt;/p&gt;
    &lt;p&gt;There are also a couple more plugins that make handling closed accounts better:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;beancount.plugins.close_tree - When you close an account, automatically closes all child accounts too. Useful as you move your banking between institutions.&lt;/item&gt;
      &lt;item&gt;beancount_checkclosed.check_closed - Validates that closed accounts have zero balance and no transactions after the close date.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The boundary of your system&lt;/head&gt;
    &lt;p&gt;As you add accounts one by one, you’ll inevitably see money flowing to places you haven’t set up yet. Say you transfer £500 to a Natwest savings account you haven’t added to the system. Where does it go?&lt;/p&gt;
    &lt;p&gt;Specifically, use a named placeholder account in &lt;code&gt;Equity:Transfers&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;; chapter-2/src/transactions.beancount
2024-03-15 * "Transfer to savings (not yet tracked)"
  Assets:Lalit:UK:HSBC:Current:GBP     -500.00 GBP
  Equity:Transfers:Natwest-Savings      500.00 GBP
&lt;/code&gt;
    &lt;p&gt;This essentially says: “£500 went to Natwest Savings, which I’m not tracking yet”. Putting it in an equity account means it doesn’t pollute your balance sheet with incomplete information or your income statement with false expenses.&lt;/p&gt;
    &lt;p&gt;Note also the best practice of using a named equity account per untracked destination, not a generic bucket; this was a mistake I made when I did this initially. You’ll thank yourself when you import your Natwest savings account later, you can just do a search-replace to rename the accounts 6.&lt;/p&gt;
    &lt;head rend="h4"&gt;Organizing your repo&lt;/head&gt;
    &lt;p&gt;As you add these patterns (transit accounts, multiple institutions, liability accounts) your single &lt;code&gt;journal.beancount&lt;/code&gt; file will start to become unwieldy. Just like good software architecture, you want to organize upfront to be easy to maintain as the system continues to grow.&lt;/p&gt;
    &lt;p&gt;This is what the structure looks like with the concepts we have right now (it’ll get more complicated as we go deeper!):&lt;/p&gt;
    &lt;code&gt;chapter-2/
├── journal.beancount            # Main entry point, includes everything
├── src/
│   ├── accounts.beancount       # Account definitions
│   ├── transactions.beancount   # Primary transaction ledger
│   └── balance.beancount        # Balance assertions
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;src/&lt;/code&gt;is what you write and edit (your “code”)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;journal.beancount&lt;/code&gt;is the entry point that includes everything&lt;/item&gt;
      &lt;item&gt;Later, &lt;code&gt;data/&lt;/code&gt;will hold inputs from the outside world (raw statements: PDFs, OFX, CSVs)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This organization is a small change but can make a big difference in your subconscious feeling about the state of your finances!&lt;/p&gt;
    &lt;head rend="h4"&gt;Exit ramp&lt;/head&gt;
    &lt;p&gt;At this point, you have a solid foundation for tracking multiple accounts. If you stop here, you have a robust, auditable system for manual bookkeeping. You could continue adding transactions by hand indefinitely, and you’d still be miles ahead of any spreadsheet in terms of correctness and visibility.&lt;/p&gt;
    &lt;p&gt;But manual entry is a chore, and as your financial life grows, it becomes a bottleneck. In the next chapter, we’ll see how to automate the tedious part: getting transactions from your bank statements into your ledger without losing the control that plain-text accounting gives you.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 3: Automated Import&lt;/head&gt;
    &lt;head rend="h4"&gt;Automation is king, but harder than it looks&lt;/head&gt;
    &lt;p&gt;Inputting transactions by hand works for some people, but I don’t have the patience for it. Ever since I was young, I’ve always wanted to automate everything: it’s the reason why I became a software engineer in the first place!&lt;/p&gt;
    &lt;p&gt;But full automation is a dead end. Most banks don’t offer APIs, and scraping breaks constantly. 2FA flows change 7, websites get redesigned, sessions expire. I tried this route, and it wasn’t worth it. Even in the US where aggregators like Plaid exist, coverage is patchy.8 In the UK, it’s impossible.&lt;/p&gt;
    &lt;head rend="h4"&gt;The hierarchy of data sources&lt;/head&gt;
    &lt;p&gt;So what actually works? Well ideally your financial institution gives you something you can write a script against. But what that might be is non-obvious and counter-intuitive. Here’s the hierarchy I’ve landed on over time:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OFX is the gold standard. If your bank offers it, use it. The format is standardized, transactions have unique IDs, and deduplication is straightforward. Life is easy if you have good OFX.&lt;/item&gt;
      &lt;item&gt;CSV is the deceptive runner-up. It seems like the logical choice. Structured data, right? But in practice, bank CSVs are often afterthoughts. I’ve seen column formats change without notice, “CSVs” that are actually weird custom formats spread over multiple lines, and rows coalesced in ways that lose critical information (like cost basis).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So what do you do when OFX isn’t available and CSV isn’t trustworthy? You turn to an unlikely hero.&lt;/p&gt;
    &lt;head rend="h4"&gt;Why PDFs beat CSVs&lt;/head&gt;
    &lt;p&gt;It sounds backwards, but PDFs are often the most reliable data source available.&lt;/p&gt;
    &lt;p&gt;Banks have a strong incentive to get PDFs right. Customers actually read them. They’re legal documents that get printed and filed. If a bank messes up a PDF statement, they hear about it immediately. If they break a CSV export, it might go months without anyone noticing.&lt;/p&gt;
    &lt;p&gt;The key insight is that bank statement PDFs are almost always columnar. Of course, this relies on the PDF having a proper text layer; if your bank sends you scanned images, you’re out of luck (though I’ve yet to encounter one that does). When you convert them to text while preserving the layout, you get something that looks like this:&lt;/p&gt;
    &lt;code&gt;Date      Details                      Paid out     Paid in     Balance
15 Jan 24 TESCO STORES 1234            42.50                    1,457.50
16 Jan 24 TFL TRAVEL                    6.80                    1,450.70
25 Jan 24 GOOGLE SALARY                           3,200.00      4,650.70
&lt;/code&gt;
    &lt;p&gt;The columns are aligned by spaces, which means you can parse them as fixed-width data. The approach works in three steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Convert PDF to text: Run&lt;/p&gt;&lt;code&gt;pdftotext -layout statement.pdf statement.txt&lt;/code&gt;. The&lt;code&gt;-layout&lt;/code&gt;flag preserves the original column alignment.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the table boundaries: Bank statements have predictable markers. HSBC uses “BALANCE BROUGHT FORWARD” at the start and “BALANCE CARRIED FORWARD” at the end. You extract just the transaction rows between these markers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Parse with fixed-width columns: Pandas’&lt;/p&gt;&lt;code&gt;read_fwf&lt;/code&gt;function is designed exactly for this. You put this logic inside the&lt;code&gt;extract()&lt;/code&gt;method of your&lt;code&gt;beangulp&lt;/code&gt;importer class, where it converts the text into a DataFrame:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Inside your Importer class
df = pd.read_fwf(
    io.StringIO(text),
    colspecs=[(0, 10), (10, 40), (40, 52), (52, 64), (64, 80)],
    names=['Date', 'Details', 'Paid out', 'Paid in', 'Balance']
)
&lt;/code&gt;
    &lt;p&gt;The column positions come from inspecting the header row. In practice, I detect them dynamically by finding keywords like “Paid out” and “Paid in” in the header and using their character positions.&lt;/p&gt;
    &lt;p&gt;Once you have a DataFrame, generating Beancount transactions is straightforward. You write a small class that iterates through this DataFrame and maps each row to a Beancount &lt;code&gt;Transaction&lt;/code&gt; object, filling in the date, amount, and payee. See my HSBC importer for a working example.&lt;/p&gt;
    &lt;p&gt;For 95% of my banks, this approach works great. However, there is one bank where the text spacing becomes very strange and so I need to use something else. That’s when I turn to Tabula, a Java CLI that extracts data tables from PDFs, even very complex ones.&lt;/p&gt;
    &lt;p&gt;The main reason I don’t use it all the time is that it’s much slower. But it also succeeds in the cases where &lt;code&gt;pdftotext&lt;/code&gt; fails. I run it to get a structured JSON output, from which I create transactions (see my HSBC US credit card importer).&lt;/p&gt;
    &lt;p&gt;I’m sure some readers will have worries about the fragility of what I’ve described here. I can tell you from experience that in three years, neither my UK nor my US HSBC PDF importer has ever broken. Neither has my Schwab one, and my Aviva one has only needed a single change. So I can personally vouch that this approach works and works well.&lt;/p&gt;
    &lt;head rend="h4"&gt;From raw data to transactions&lt;/head&gt;
    &lt;p&gt;OK, so now you have statements. What do you do then? There are two pieces to the puzzle: parsing statements into transactions, and categorizing those transactions.&lt;/p&gt;
    &lt;p&gt;For parsing, Beancount has an official importer framework called beangulp. You write a Python class that knows how to read a particular file format (the skeleton importer in &lt;code&gt;chapter-3/&lt;/code&gt; already uses this API). Beangulp handles the mechanics: identifying which importer handles which file, extracting transactions, and deduplicating against existing entries.&lt;/p&gt;
    &lt;p&gt;But beangulp just extracts transactions without making any judgment on which account the transaction should be booked against. It doesn’t know that Tesco is groceries or that British Airways is an airline. I go to a new restaurant. How does the system know that is a restaurant?&lt;/p&gt;
    &lt;p&gt;This leads to a very important conclusion: we cannot fully automate importing transactions for bank accounts and credit cards. However, that doesn’t mean we have to enter things manually either. There’s a middle ground.&lt;/p&gt;
    &lt;p&gt;Enter beancount-import9 (Note: this is a standalone tool, distinct from Fava’s built-in import features). It’s a web UI that uses your beangulp importers and adds a categorization layer on top. You run it, it opens in your browser, and it presents pending transactions one by one for review. Think of it as a staging area where you approve or tweak before anything hits your ledger.&lt;/p&gt;
    &lt;p&gt;For each transaction, it shows the raw data from your statement alongside a suggested categorization. You can accept the suggestion, override it with a different account, or skip it entirely. Once you decide, it moves to the next one. The interface is simple, mostly keyboard-driven and optimized for speed.&lt;/p&gt;
    &lt;p&gt;The import UI allows you to manually categorize transactions, like this Tesco grocery run, while the system learns your preferences.&lt;/p&gt;
    &lt;p&gt;The categorization uses machine learning (old-school decision trees, running locally, no LLMs, no cloud). It learns from your previous choices: the first time you see “Tesco”, you pick “Expenses:Groceries”, and the second time it auto-suggests and you just hit Enter. After a few weeks, the system should know 90% of the types of transactions you make and it’s easy to correct the ones which it doesn’t.&lt;/p&gt;
    &lt;p&gt;Once you’re proficient, a month’s worth of transactions takes 5-10 minutes. Most are repeats and you’re just hitting Enter. You only pause on genuinely new merchants. This is the core of my weekly ritual, and why I’ve found categorization has not become a chore, even after doing it for years.&lt;/p&gt;
    &lt;head rend="h4"&gt;Exit ramp&lt;/head&gt;
    &lt;p&gt;With automated imports and semi-automated categorization, the “hard work” of bookkeeping is mostly solved. For many, this is the endgame: a perfect record of where every penny went, updated in minutes each week.&lt;/p&gt;
    &lt;p&gt;But your net worth isn’t just cash in a bank account. It’s also the stocks, bonds, and funds that grow (or shrink) over time. Tracking these requires a few more tools to handle cost basis, dividends, and market prices. We’ll tackle those in Chapter 4.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 4: Investments&lt;/head&gt;
    &lt;p&gt;Now for investments. It’s where things get more interesting, and I think there’s less material out there covering the nitty-gritty. Here are some lessons I’ve learned over the years.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Unified Mental Model&lt;/head&gt;
    &lt;p&gt;The most important thing to realize is that Beancount treats everything as a commodity.&lt;/p&gt;
    &lt;p&gt;A share of Apple (&lt;code&gt;AAPL&lt;/code&gt;) is a commodity. A US Dollar (&lt;code&gt;USD&lt;/code&gt;) is a commodity. A British Pound (&lt;code&gt;GBP&lt;/code&gt;) is a commodity. While Beancount can infer these on the fly, you’ll typically declare them explicitly in your journal (e.g., &lt;code&gt;2024-01-01 commodity AAPL&lt;/code&gt;).10&lt;/p&gt;
    &lt;p&gt;This explicit declaration is a small but critical architectural win: it prevents a simple typo from creating a phantom currency, and it provides the metadata that advanced reporting plugins (like those used to calculate your portfolio performance) rely on to work correctly.&lt;/p&gt;
    &lt;p&gt;This means you don’t “buy stocks with money”. You simply exchange one commodity for another. The syntax for buying shares is identical to the syntax for exchanging currency.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Account Structure&lt;/head&gt;
    &lt;p&gt;Just like with bank accounts, I break investments down by institution and security.&lt;/p&gt;
    &lt;code&gt;; chapter-4/src/accounts.beancount
2024-01-01 open Assets:Lalit:US:IB:Brokerage:USD          USD
2024-01-01 open Assets:Lalit:US:IB:Brokerage:AAPL         AAPL
2024-01-01 open Assets:Lalit:UK:Vanguard:ISA:GBP          GBP
2024-01-01 open Assets:Lalit:UK:Vanguard:ISA:VWRL         VWRL
&lt;/code&gt;
    &lt;p&gt;Cash in a brokerage is just another holding named by currency (&lt;code&gt;GBP&lt;/code&gt;, &lt;code&gt;USD&lt;/code&gt;), while stock holdings use their ticker (&lt;code&gt;AAPL&lt;/code&gt;, &lt;code&gt;VWRL&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;But you also need to track the flows generated by these assets: capital gains, dividends, commissions, and withholding taxes. I create specific accounts for each security:&lt;/p&gt;
    &lt;code&gt;2024-01-01 open Income:Lalit:US:IB:Brokerage:AAPL:Dividends        USD
2024-01-01 open Income:Lalit:US:IB:Brokerage:AAPL:Capital-Gains    USD
&lt;/code&gt;
    &lt;p&gt;Why so granular? It’s the same reason I name assets fully: aggregation up the tree is trivial; disaggregation after the fact is impossible. If you track all your dividends in a single &lt;code&gt;Income:Dividends&lt;/code&gt; account, it’s easy to know “how much dividends did I earn total?”. But if you want to know “what was my AAPL dividend yield this year?”, you’re out of luck. Track at the leaf (&lt;code&gt;Income:IB:AAPL:Dividends&lt;/code&gt;), and you can answer both questions.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Notation&lt;/head&gt;
    &lt;p&gt;With our accounts defined, we can now record the actual movement of assets. We use &lt;code&gt;{}&lt;/code&gt; to denote cost (what we paid per unit) and &lt;code&gt;@&lt;/code&gt; to denote price (what the unit is worth now).&lt;/p&gt;
    &lt;p&gt;Example 1: Buying Stock Exchanging 1850 USD for 10 shares of Apple.&lt;/p&gt;
    &lt;code&gt;2024-01-10 * "BUY AAPL"
  Assets:Lalit:US:IB:Brokerage:AAPL      10 AAPL {185.00 USD} @ 185.00 USD
  Assets:Lalit:US:IB:Brokerage:USD   -1850.00 USD
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;{185.00 USD}&lt;/code&gt; is the cost basis and the &lt;code&gt;@ 185.00 USD&lt;/code&gt; is the price. Beancount uses the cost basis to track lots and calculate capital gains.&lt;/p&gt;
    &lt;p&gt;Note: Cost basis rules depend heavily on where you are. In the US, you track cost basis of individual lots. In the UK, we have special “Section 104” pooling rules.11 I discuss this more in Chapter 5.&lt;/p&gt;
    &lt;p&gt;Example 2: Buying Currency Exchanging 950 GBP for 98,000 INR.&lt;/p&gt;
    &lt;code&gt;2024-03-02 * "Wise" "GBP to INR"
  Assets:Lalit:UK:Wise:INR            98000.00 INR @@ 950.00 GBP
  Assets:Lalit:UK:Wise:GBP             -950.00 GBP
&lt;/code&gt;
    &lt;p&gt;In the second example, &lt;code&gt;@@&lt;/code&gt; specifies the total cost rather than the per-unit cost.&lt;/p&gt;
    &lt;p&gt;Example 3: Selling Stock Selling 5 shares of Apple at $190 (bought at $185).&lt;/p&gt;
    &lt;code&gt;2024-01-30 * "SELL AAPL"
  Assets:Lalit:US:IB:Brokerage:AAPL      -5 AAPL {185.00 USD} @ 190.00 USD
  Assets:Lalit:US:IB:Brokerage:USD     950.00 USD
  Income:Lalit:US:IB:Brokerage:AAPL:Capital-Gains  -25.00 USD
&lt;/code&gt;
    &lt;p&gt;Here we specify the lot we’re selling (&lt;code&gt;{185.00 USD}&lt;/code&gt;) and the price we’re selling it at (&lt;code&gt;@ 190.00 USD&lt;/code&gt;). The difference is the capital gain (or loss).&lt;/p&gt;
    &lt;p&gt;But the principle is identical: &lt;code&gt;Assets:Wise:INR&lt;/code&gt; and &lt;code&gt;Assets:Brokerage:AAPL&lt;/code&gt; are just accounts holding commodities.&lt;/p&gt;
    &lt;head rend="h4"&gt;Automation&lt;/head&gt;
    &lt;p&gt;Investments are very different from normal accounts in that they can be fully automated. No categorization needed: a buy is a buy, a dividend is a dividend. You don’t have new merchants to worry about.&lt;/p&gt;
    &lt;p&gt;This means you can skip beancount-import’s web UI and run your beangulp importers directly with output going straight to the ledger. To help inspire you, I’ve open-sourced my personal collection of importers (IB, Vanguard, Schwab, and more) in the beancount-lalitm repo.&lt;/p&gt;
    &lt;head rend="h4"&gt;Handling Account Sprawl&lt;/head&gt;
    &lt;p&gt;However, one annoyance is that creating those granular accounts for every single stock (&lt;code&gt;...:AAPL:Dividends&lt;/code&gt;, &lt;code&gt;...:AAPL:Commissions&lt;/code&gt;, etc.) is tedious. To solve this, I wrote the ancillary_accounts plugin. Instead of manual account creation, you just add metadata to the main holding account:&lt;/p&gt;
    &lt;code&gt;2023-02-01 open Assets:Lalit:US:IB:Brokerage:BAC    BAC
  ancillary_commission_currency: "USD"
  ancillary_distribution_currency: "USD"
  ancillary_withholding_tax_currency: "USD"
  ancillary_capital_gains_currency: "USD"
&lt;/code&gt;
    &lt;p&gt;The plugin automatically generates the corresponding income and expense accounts for you.&lt;/p&gt;
    &lt;head rend="h4"&gt;Corporate Actions&lt;/head&gt;
    &lt;p&gt;I also use a plugin called stock_split to handle corporate actions. It retroactively adjusts historical transactions when a stock splits, keeping quantities and prices consistent with post-split values so your charts don’t show a sudden, fake drop in value.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Value of Things (Prices)&lt;/head&gt;
    &lt;p&gt;We have the quantities (10 AAPL, 98,000 INR), but to calculate a single “Net Worth” number, we need to know what they are worth in your home currency. This requires prices for both:&lt;/p&gt;
    &lt;code&gt;; chapter-4/src/prices.beancount
2024-01-10 price AAPL  185.50 USD  ; Stock price in USD
2024-01-10 price USD   0.79 GBP    ; Currency price in GBP
&lt;/code&gt;
    &lt;p&gt;I automate this using a daily CI job. A script fetches the latest stock prices and forex rates from AlphaVantage and commits them to &lt;code&gt;prices.beancount&lt;/code&gt;. You can find the script here.&lt;/p&gt;
    &lt;p&gt;In practice, I actually have three price files:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;prices.beancount&lt;/code&gt;- auto-fetched daily for as many securities as possible.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;prices-manual.beancount&lt;/code&gt;- for securities without automatic feeds (like some pension funds). I input these manually once a month.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;prices-delisted.beancount&lt;/code&gt;- historical prices for securities no longer trading. This saves me from making API calls which would fail anyway.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;The Payoff&lt;/head&gt;
    &lt;p&gt;With this data, Fava comes alive. To see the full potential of these reports, I’ve included a &lt;code&gt;demo/&lt;/code&gt; folder in the companion repo with 2+ years of history. Run &lt;code&gt;./scripts/quickstart.sh demo&lt;/code&gt; and you’ll see the payoff.&lt;/p&gt;
    &lt;p&gt;The Holdings page now shows your positions with their cost basis and current market value.&lt;/p&gt;
    &lt;p&gt;The Holdings report automatically calculates the market value of your assets using live price data.&lt;/p&gt;
    &lt;p&gt;In the demo environment, you can also see how plugins like fava-dashboards build custom visualizations. The plugin uses beanquery (Beancount’s SQL-like query language) to fetch data and renders interactive charts. It’s the best way to track long-term trends and asset allocation at a glance.&lt;/p&gt;
    &lt;p&gt;Custom dashboards (shown here using the demo data) allow you to track long-term trends and asset allocation at a glance.&lt;/p&gt;
    &lt;p&gt;And with fava-portfolio-returns, you can calculate your true Time-Weighted Return (TWR) and Internal Rate of Return (IRR) to see if you’re actually beating the market. It accounts for cash flows properly, so adding money mid-year doesn’t inflate your returns.&lt;/p&gt;
    &lt;p&gt;The portfolio returns plugin (using the demo data) calculates your actual investment performance, net of cash flows.&lt;/p&gt;
    &lt;head rend="h4"&gt;Exit ramp&lt;/head&gt;
    &lt;p&gt;You now have a system that tracks your entire financial world: from the coffee you bought this morning to the capital gains in your brokerage account. For most people, this is a complete solution.&lt;/p&gt;
    &lt;p&gt;However, as you collect more data, you’ll find you want to look at it in different ways. Maybe you want a simplified view for daily use and a detailed one for tax season. Or maybe you’re not the only person in your household. In the final chapters, we’ll see how to scale this system to handle multiple views and multiple people.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 5: Multiple Views&lt;/head&gt;
    &lt;p&gt;Day-to-day, I want a simple view of my finances. Take-home pay as a single number, investments without tax calculations cluttering the screen. But at tax time, I need detail. Every payslip line item and capital gains calculated the way HMRC wants them. Recording the same transaction twice would be maintenance hell. So instead, I extract multiple views from a single journal.&lt;/p&gt;
    &lt;p&gt;I think of these as “lenses” on the data. Some lenses aggregate: rolling up transactions into balances, summaries, or dashboards. Others transform: collapsing detail you don’t need day-to-day, or expanding it when you do. Both read from the same source files; nothing is duplicated.&lt;/p&gt;
    &lt;head rend="h4"&gt;Aggregated views&lt;/head&gt;
    &lt;p&gt;Fava is great for interactive exploration, but I also want textual snapshots I can version control. I have a script that generates daily summaries showing account balances, and a CI workflow that commits them automatically. In my setup this runs on a self-hosted Gitea instance on hardware I control, so the raw ledger never leaves machines I own. If you prefer, you can keep everything local-only or push to an encrypted remote; GitHub Actions works the same way if you’re comfortable with that trade-off. This gives me:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A record of how balances changed day to day&lt;/item&gt;
      &lt;item&gt;An immutable snapshot at tax time of what the system showed&lt;/item&gt;
      &lt;item&gt;Git as audit trail: “What was my net worth on March 15th 2023?” is answerable with &lt;code&gt;git checkout&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key script is archive.py. It uses the beanquery library to write SQL scripts over your journal and generate textual reports:&lt;/p&gt;
    &lt;code&gt;# Generate balance sheet in GBP
sql = '''
  SELECT account,
         round(sum(number(convert(value(position, '2024-12-31'), 'GBP', '2024-12-31'))), 2) as value
  FROM OPEN ON 2024-01-01 CLOSE ON 2024-12-31 CLEAR
  GROUP BY account
  HAVING round(sum(number), 2) != 0
  ORDER BY account;
'''
&lt;/code&gt;
    &lt;p&gt;I run this for each calendar year and tax year, generating files like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;networth.txt&lt;/code&gt;- Single-line net worth in each currency&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;balance-sheet.txt&lt;/code&gt;- Net worth breakdown by account&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;holdings.txt&lt;/code&gt;- Investment positions with cost basis and market value&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s what &lt;code&gt;networth.txt&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;     gbp            usd
-------------  -------------
 15978.42 GBP   19973.02 USD
&lt;/code&gt;
    &lt;p&gt;This is the concrete “one number I trust”: a single net-worth snapshot in my reporting currency, generated from the full ledger and price data.&lt;/p&gt;
    &lt;p&gt;And &lt;code&gt;holdings.txt&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;account                           units  curr  avg_cost  price  book_val  mkt_val
---------------------------------  -----  ----  --------  -----  --------  -------
Assets:Lalit:UK:HSBC:Current:GBP  4914.50  GBP      1.00   1.00   4914.50  4914.50
Assets:Lalit:UK:Vanguard:ISA:VWRL   20.00  VWRL    96.00  97.50   1920.00  1950.00
Assets:Lalit:US:IB:Brokerage:AAPL    5.00  AAPL   146.15 150.40    730.75   752.00
&lt;/code&gt;
    &lt;p&gt;My Gitea workflow is very simple too:&lt;/p&gt;
    &lt;code&gt;on:
  schedule:
    - cron: '00 7 * * *'  # Run daily at 7am

jobs:
  update:
    steps:
      - run: uv run scripts/archive.py outputs/ journal.beancount 2024-01-01 2024-12-31
      - run: git commit -am "Regen reports" &amp;amp;&amp;amp; git push
&lt;/code&gt;
    &lt;head rend="h4"&gt;Transformed views&lt;/head&gt;
    &lt;p&gt;Sometimes I want to change how transactions work fundamentally. This is a more advanced technique: while Aggregated views read data, Transformed views temporarily rewrite it in memory to simplify reality.&lt;/p&gt;
    &lt;p&gt;I have three transformed views, each for a different purpose:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Net - my daily driver. Collapses payslip details into a single take-home number.&lt;/item&gt;
      &lt;item&gt;Gross - breaks down payslip line items for tax time analysis.&lt;/item&gt;
      &lt;item&gt;CGT - a view that includes a “virtual currency” tracking capital gains the way my tax authority calculates them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The linchpin is the &lt;code&gt;rename_accounts&lt;/code&gt; plugin. It lets me keep one copy of all transactions and rename accounts on the fly to show or hide detail.&lt;/p&gt;
    &lt;p&gt;Gross vs net payslip&lt;/p&gt;
    &lt;p&gt;Let me start with the simpler example. In gross view, my payslip shows every line item:&lt;/p&gt;
    &lt;code&gt;2024-01-25 * "Google" "January salary"
  Assets:Lalit:UK:HSBC:Current:GBP           3500.00 GBP  ; Take-home pay
  Income:Lalit:UK:Google:Salary           -5000.00 GBP  ; Gross salary
  Expenses:Lalit:UK:Google:Income-Tax        1000.00 GBP  ; Tax withheld
  Expenses:Lalit:UK:Google:National-Insurance 400.00 GBP  ; NI contribution
  Expenses:Lalit:UK:Google:Pension            100.00 GBP  ; Pension contribution
&lt;/code&gt;
    &lt;p&gt;Useful for analyzing my tax situation. But day-to-day, I don’t care about the breakdown. In net view, the same transaction collapses:&lt;/p&gt;
    &lt;code&gt;; chapter-5/journal-net.beancount
include "journal.beancount"

plugin "beancount_reds_plugins.rename_accounts.rename_accounts" "{
  'Income:Lalit:UK:Google:Salary': 'Income:Lalit:UK:Google:Net-Income',
  'Income:Lalit:UK:Google:Bonus': 'Income:Lalit:UK:Google:Net-Income',
  'Expenses:Lalit:UK:Google:Income-Tax': 'Income:Lalit:UK:Google:Net-Income',
  'Expenses:Lalit:UK:Google:National-Insurance': 'Income:Lalit:UK:Google:Net-Income',
  'Expenses:Lalit:UK:Google:Pension': 'Income:Lalit:UK:Google:Net-Income',
}"
&lt;/code&gt;
    &lt;p&gt;Because Income is stored as a negative number and Expenses as positive numbers, merging them into one account mathematically subtracts the tax from the gross pay, leaving just the net amount. The difference should be obvious if I compare the Income Statements on Fava:&lt;/p&gt;
    &lt;p&gt;The Gross view is essential for tax season, but the detailed line items for taxes and insurance often dwarf your actual spending data.&lt;/p&gt;
    &lt;p&gt;The Net view collapses those details into a single take-home number, making your everyday expenses much easier to analyze.&lt;/p&gt;
    &lt;head rend="h4"&gt;Tracking capital gains for tax&lt;/head&gt;
    &lt;p&gt;We can use this same renaming technique to handle a much more complex beast: Capital Gains Tax.&lt;/p&gt;
    &lt;p&gt;Your broker reports one gain number, but your tax authority may calculate another. In the UK, where I live, we have specific rules like “Section 104 pooling” (averaging cost basis) and “bed-and-breakfasting” (wash sale rules).&lt;/p&gt;
    &lt;p&gt;To handle this, I use a virtual currency called &lt;code&gt;CGT-GBP&lt;/code&gt; that represents “pounds of gain HMRC cares about”. My plugin, &lt;code&gt;uk_cgt_lots&lt;/code&gt;, calculates this number and automatically appends a self-balancing pair of Equity postings to the original sale transaction:&lt;/p&gt;
    &lt;code&gt;2024-06-15 * "SELL AAPL"
  Assets:Lalit:US:IB:Brokerage:AAPL           -10 AAPL {150.00 USD} @ 175.00 USD
  Assets:Lalit:US:IB:Brokerage:USD            1750.00 USD
  Income:Lalit:US:IB:Brokerage:AAPL:Capital-Gains  -250.00 USD
  ; The following postings are generated by the uk_cgt_lots plugin:
  Equity:Taxable-Capital-Gains              195.00 CGT-GBP
  Equity:Taxable-Capital-Gains-Placeholder -195.00 CGT-GBP
&lt;/code&gt;
    &lt;p&gt;Since both legs are in &lt;code&gt;Equity&lt;/code&gt;, they remain invisible on my Income Statement in my daily “Net” view. In fact, I use &lt;code&gt;rename_accounts&lt;/code&gt; to collapse them into a single account so they net to zero:&lt;/p&gt;
    &lt;code&gt;; journal-net.beancount
plugin "beancount_reds_plugins.rename_accounts.rename_accounts" "{
  'Equity:Taxable-Capital-Gains-Placeholder' : 'Equity:Taxable-Capital-Gains',
}"
&lt;/code&gt;
    &lt;p&gt;But when I want to see my tax liability, I switch to the CGT View. This view renames the “Placeholder” to a visible Revenue account:&lt;/p&gt;
    &lt;code&gt;; journal-cgt.beancount
plugin "beancount_reds_plugins.rename_accounts.rename_accounts" "{
  'Equity:Taxable-Capital-Gains-Placeholder' : 'Revenues:Taxable-Capital-Gains',
}"
&lt;/code&gt;
    &lt;p&gt;Now, the -195.00 becomes Revenue, which shows up as profit on my tax report. The matching +195.00 remains in Equity. This allows me to have “Schrödinger’s Capital Gains”: they exist for the taxman, but not for my daily budget, all controlled by which view I load.&lt;/p&gt;
    &lt;p&gt;I don’t calculate the tax owed since that’s too complicated with allowances, rates, and bands; the system just tracks the gains. At tax time, I sum up the CGT-GBP balance and do the actual calculation on the tax form.&lt;/p&gt;
    &lt;head rend="h4"&gt;Exit ramp&lt;/head&gt;
    &lt;p&gt;By separating your “source of truth” from your “lenses,” you get a system that grows with you. You can add new plugins or virtual currencies to solve specific problems (like taxes) without ever touching the raw transactions you’ve already imported.&lt;/p&gt;
    &lt;p&gt;In the final chapter, we’ll see the ultimate application of this: combining two people’s financial lives into one unified view.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chapter 6: Two People, One Number&lt;/head&gt;
    &lt;p&gt;I got married at the start of the year, which brought a fundamental change to how I manage my finances. While many couples use joint accounts, we prefer to keep our individual accounts and perform occasional “normalization” transfers. However, we view our combined resources as shared household wealth.&lt;/p&gt;
    &lt;p&gt;This created a reporting paradox that I had to solve in Beancount.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Paradox&lt;/head&gt;
    &lt;p&gt;When I transfer £500 to my wife for my share of the bills, two things are true simultaneously:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The Individual Truth: From my perspective, £500 is “gone” (an expense). From her perspective, £500 has “arrived” (income).&lt;/item&gt;
      &lt;item&gt;The Household Truth: For the household, the net worth hasn’t changed. Money just moved from the left pocket to the right pocket.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In a traditional system, you usually have to pick one truth. In Beancount, we can have both.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Composable Architecture&lt;/head&gt;
    &lt;p&gt;To solve this, I treat the household as a composable system of three distinct entities: Me, Her, and Shared Definitions. We use Beancount’s &lt;code&gt;include&lt;/code&gt; feature to build the specific “lens” we need at any given moment:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lalit’s View = Shared Definitions + Lalit’s Transactions&lt;/item&gt;
      &lt;item&gt;Wife’s View = Shared Definitions + Wife’s Transactions&lt;/item&gt;
      &lt;item&gt;Household View = Shared Definitions + Lalit’s Transactions + Wife’s Transactions + Translation Logic&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, the household view literally just includes the other files (alongside the translation logic we’ll see in a moment):&lt;/p&gt;
    &lt;code&gt;; chapter-6/total/journal-net.beancount
include "../common/src/commodities.beancount"
include "../common/src/accounts.beancount"

include "../lalit/src/journal.beancount"
include "../wife/src/journal.beancount"

; ... Translation Logic follows
&lt;/code&gt;
    &lt;head rend="h4"&gt;Implementation: The Directory Structure&lt;/head&gt;
    &lt;p&gt;This architecture is reflected directly in the repository structure:&lt;/p&gt;
    &lt;code&gt;chapter-6/
├── common/                    # Shared configuration
│   └── src/
│       ├── accounts.beancount   # Shared expense accounts (e.g. Expenses:Groceries)
│       └── commodities.beancount
├── lalit/                     # My stuff
│   ├── src/                   # My ledger
│   └── data/                  # My statement PDFs/CSVs
├── wife/                      # Wife's stuff
│   ├── src/                   # Her ledger
│   └── data/                  # Her statement PDFs/CSVs
└── total/                     # Combined household view
    └── journal-net.beancount  # Entry point with Translation Logic
&lt;/code&gt;
    &lt;head rend="h4"&gt;The Rules of Engagement&lt;/head&gt;
    &lt;p&gt;For this to work without constant manual adjustment, we follow two simple rules:&lt;/p&gt;
    &lt;p&gt;Rule 1: Assets and Liabilities are Private. Bank accounts always include the person’s name in the path (e.g., &lt;code&gt;Assets:Lalit:HSBC&lt;/code&gt; or &lt;code&gt;Assets:Wife:HSBC&lt;/code&gt;). We never use a generic &lt;code&gt;Assets:Checking&lt;/code&gt; account. Legal ownership of the cash always matters.&lt;/p&gt;
    &lt;p&gt;Rule 2: Expenses are Public. Shared expenses like groceries or electricity use a generic name without a person prefix.&lt;/p&gt;
    &lt;code&gt;; chapter-6/common/src/accounts.beancount
2024-01-01 open Expenses:Groceries                   GBP
&lt;/code&gt;
    &lt;p&gt;When I buy groceries, I record it in my ledger using the shared account. We don’t track “who owes what” for individual grocery runs; we just track that the household spent the money. We accept that we lose the ability to split shared expenses by person, but the gain in simplicity is worth it.&lt;/p&gt;
    &lt;head rend="h4"&gt;The “Magic”: Solving the Transfer Paradox&lt;/head&gt;
    &lt;p&gt;Finally, we use the &lt;code&gt;rename_accounts&lt;/code&gt; plugin in the &lt;code&gt;total/&lt;/code&gt; folder to resolve the transfer paradox.&lt;/p&gt;
    &lt;p&gt;In my ledger, a transfer looks like a simple expense:&lt;/p&gt;
    &lt;code&gt;; chapter-6/lalit/src/transactions.beancount
2024-01-20 * "Transfer to Wife"
  Expenses:Lalit:Transfers:Wife       500.00 GBP
  Assets:Lalit:UK:HSBC:Current:GBP
&lt;/code&gt;
    &lt;p&gt;In her ledger, it looks like income:&lt;/p&gt;
    &lt;code&gt;; chapter-6/wife/src/transactions.beancount
2024-01-20 * "Transfer from Lalit"
  Assets:Wife:UK:HSBC:Current:GBP     500.00 GBP
  Income:Wife:Transfers:Lalit
&lt;/code&gt;
    &lt;p&gt;The “Translation Logic” in the combined view renames these into a shared transit account:&lt;/p&gt;
    &lt;code&gt;; chapter-6/total/journal-net.beancount
plugin "beancount_reds_plugins.rename_accounts.rename_accounts" "{
  'Expenses:Lalit:Transfers:Wife': 'Assets:Household:Transfers:Internal',
  'Income:Wife:Transfers:Lalit': 'Assets:Household:Transfers:Internal',
}"
&lt;/code&gt;
    &lt;p&gt;Now, when Fava loads the combined view, it sees £500 leave my account and enter &lt;code&gt;Assets:Household:Transfers:Internal&lt;/code&gt;, and then £500 leave that same account and enter her bank account. The transit account nets to zero, and our household net worth remains unchanged.&lt;/p&gt;
    &lt;head rend="h4"&gt;The Result&lt;/head&gt;
    &lt;p&gt;This setup gives us the best of both worlds. I can maintain my own financial autonomy and see my personal “runway,” while we can simultaneously monitor our combined progress toward shared goals.&lt;/p&gt;
    &lt;p&gt;The final result: a unified household view that tracks legal ownership without sacrificing the "One Number" net worth total.&lt;/p&gt;
    &lt;p&gt;That’s the full system (see chapter-6 for the complete multi-person structure). But how do I actually use it week to week?&lt;/p&gt;
    &lt;head rend="h2"&gt;The weekly ritual&lt;/head&gt;
    &lt;p&gt;Now here’s how I keep it current: the “20 minutes a week” I mentioned at the start:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Getting statements: During the week, banks email me saying a statement is available. Some attach PDFs directly; others require a login. Either way, I snooze the emails (I use inbox zero) until the weekend. For my wife’s accounts, I nudge her once a month and she drops stuff in a shared Drive folder.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Running imports: On the weekend, I work through my snoozed emails. Note that I’m not updating every single account every week. I only download statements for the 3-4 accounts that saw activity; long-term investments often just get a monthly or quarterly check-in. I move each file to the correct&lt;/p&gt;&lt;code&gt;data/&lt;/code&gt;subfolder for that institution (e.g.,&lt;code&gt;data/hsbc-uk-current/&lt;/code&gt;). My importer pipeline automatically runs&lt;code&gt;pdftotext&lt;/code&gt;to extract text so the parsers can read it.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Categorizing: I launch beancount-import, which opens a web UI in my browser:&lt;/p&gt;
        &lt;code&gt;python -m beancount_import.webserver --journal lalit/journal-net.beancount&lt;/code&gt;
        &lt;p&gt;As I mentioned before, most transactions auto-categorize and I’m just hitting Enter: I buy groceries from the same place, pay my hosting costs to the same provider etc. New merchants do need some manual work, but it’s a matter of typing a few characters and again pressing Enter.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Formatting and checking: I run&lt;/p&gt;&lt;code&gt;bean-format&lt;/code&gt;(a Beancount utility that normalizes indentation and aligns amounts) to keep my transactions file tidy. This makes git diffs cleaner. Then I open Fava for a quick sanity check:&lt;code&gt;fava lalit/journal-net.beancount&lt;/code&gt;&lt;p&gt;Are transfer accounts zeroed out? Do expenses look legit? How are investments doing? If it all checks out, commit and push.&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That’s it. I try to stick to doing this every week, but sometimes I’m on holiday or just have other commitments. In that case, it’s 40 minutes every two weeks. The system is forgiving; I’m never behind for too long.&lt;/p&gt;
    &lt;p&gt;I also have some automation helping me out: during the week, I have a CI workflow that runs daily, regenerates summaries, and commits them. Whenever I want, I can check the repo and see what the numbers look like. I particularly like this because I can easily see the before/after numbers in a single file, so I can spot check “does this make sense”. And of course at any time, I can also open Fava if I want to go a bit deeper.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;That 2021 tax disaster feels like a lifetime ago. What started as “there must be a better way” became a system I actually trust. One number, always current, completely under my control.&lt;/p&gt;
    &lt;p&gt;The specifics will evolve as life changes and tools improve, but three principles have held for years now. I expect them to hold for decades:&lt;/p&gt;
    &lt;p&gt;Double-entry everywhere. Every transaction balances. Money never appears from nowhere or vanishes into nothing. When something doesn’t add up, you know immediately.&lt;/p&gt;
    &lt;p&gt;Plain text as the source of truth. Your financial history lives in files you can read, diff, grep, and version control. No vendor lock-in, no opaque databases, no trusting a third party with your data.&lt;/p&gt;
    &lt;p&gt;Track at the leaf. Record transactions at the most granular level that makes sense. You can always aggregate up (&lt;code&gt;Income:Dividends&lt;/code&gt; from &lt;code&gt;Income:IB:AAPL:Dividends&lt;/code&gt;), but you can never disaggregate down. Capture the detail; collapse it later with views.&lt;/p&gt;
    &lt;p&gt;Finance systems are deeply personal. This post isn’t meant to say this is the system everyone should use, just what’s worked for me over several years.&lt;/p&gt;
    &lt;p&gt;Each chapter here could be its own post, so if you want me to go deeper on imports, investments, or the multi-person setup, let me know!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46463407</guid><pubDate>Fri, 02 Jan 2026 10:25:11 +0000</pubDate></item><item><title>A small collection of text-only websites</title><link>https://shkspr.mobi/blog/2025/12/a-small-collection-of-text-only-websites/</link><description>&lt;doc fingerprint="7e820a171fb48bd7"&gt;
  &lt;main&gt;
    &lt;p&gt;A couple of years ago, I started serving my blog posts as plain text. Add &lt;code&gt;.txt&lt;/code&gt; to the end of any URl and get a deliciously lo-fi, UTF-8, mono[chrome|space] alternative.&lt;/p&gt;
    &lt;p&gt;Here's this post in plain text - https://shkspr.mobi/blog/2025/12/a-small-collection-of-text-only-websites.txt&lt;/p&gt;
    &lt;p&gt;Obviously a webpage without links is like a fish without a bicycle, but the joy of the web is that there are no gatekeepers. People can try new concepts and, if enough people join in, it becomes normal. I'm not saying the plain-text is the best web experience. But it is an experience. Perfect if you like your browsing fast, simple, and readable. There are no cookie banners, pop-ups, permission prompts, autoplaying videos, or garish colour schemes.&lt;/p&gt;
    &lt;p&gt;I'm certainly not the first person to do this, so I thought it might be fun to gather a list of websites which you browse in text-only mode. If you know of any more - including your own site - please drop a comment in the box!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Terence Eden's blog - add &lt;code&gt;.txt&lt;/code&gt;to any URl.&lt;/item&gt;
      &lt;item&gt; Daring Fireball - add &lt;code&gt;.text&lt;/code&gt;to any URl.&lt;/item&gt;
      &lt;item&gt; Zach Flowers - replace &lt;code&gt;.html&lt;/code&gt;with&lt;code&gt;.txt&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt; Fabien Benetou's PIM - add &lt;code&gt;?action=source&lt;/code&gt;to any URl.&lt;/item&gt;
      &lt;item&gt; M0YNG - add &lt;code&gt;.txt&lt;/code&gt;to any URl.&lt;/item&gt;
      &lt;item&gt; Gwern - add &lt;code&gt;.md&lt;/code&gt;to any URl or send an HTTP Accept for Markdown.&lt;/item&gt;
      &lt;item&gt;Dan Q's textplain.blog - the entire blog is plain text!&lt;/item&gt;
      &lt;item&gt;Matt Hobbs - there is a feed of plaintext which allows you to read recent posts.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you'd like to add a site, please get in touch. The rules are simple - content which has the MIME type of &lt;code&gt;text/plain&lt;/code&gt;. No HTML, no multimedia, no RTF, no XML, no ANSI colour escape sequences.&lt;/p&gt;
    &lt;p&gt;Emoji are fine though; emoji are cool.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46463635</guid><pubDate>Fri, 02 Jan 2026 11:06:29 +0000</pubDate></item><item><title>10 years of personal finances in plain text files</title><link>https://sgoel.dev/posts/10-years-of-personal-finances-in-plain-text-files/</link><description>&lt;doc fingerprint="5e2f81978ba66636"&gt;
  &lt;main&gt;
    &lt;p&gt;January 2026 will mark 10 years since I started storing my personal finances in plain text files using Beancount. Since January 2016, I've taken out about 30-45 minutes every single month to download my monthly bank statements and import them into my Beancount ledger.&lt;/p&gt;
    &lt;p&gt;There's a lot to talk about here, but let's start with some fun numbers!&lt;/p&gt;
    &lt;head rend="h2"&gt;The 10 year old Beancount ledger&lt;/head&gt;
    &lt;p&gt;10 years of financial transactions is a lot of data! All in all, my ledger contains over 45,000 lines of Beancount entries spread across 16 plain text files. All of it is stored in a &lt;code&gt;finances&lt;/code&gt; directory (version controlled) on my laptop. Here's a snapshot:&lt;/p&gt;
    &lt;code&gt;❯ find . -name "*.beancount" | xargs wc -l
    4037 ./includes/2020.beancount
    3887 ./includes/2018.beancount
      27 ./includes/cash.beancount
    4398 ./includes/2021.beancount
    5531 ./includes/2019.beancount
    5267 ./includes/2022.beancount
    3287 ./includes/2017.beancount
    5506 ./includes/2024.beancount
    5606 ./includes/2023.beancount
    1454 ./includes/2016.beancount
    1089 ./includes/open/04-expenses.beancount
      66 ./includes/open/03-income.beancount
      11 ./includes/open/05-liabilities.beancount
      37 ./includes/open/02-assets.beancount
       1 ./includes/open/01-equity.beancount
    4807 ./main.beancount
   45011 total
&lt;/code&gt;
    &lt;p&gt;Running &lt;code&gt;bean-query&lt;/code&gt; on &lt;code&gt;main.beancount&lt;/code&gt; tells me I have about 10,000 transactions in
total, that in turn contain about 20,000 postings (in double-entry bookkeeping, one
transaction may have multiple postings).&lt;/p&gt;
    &lt;code&gt;❯ uv run bean-query main.beancount
Input file: "Goel"
Ready with 12466 directives (19743 postings in 9895 transactions).
beanquery&amp;gt;
&lt;/code&gt;
    &lt;p&gt;There are 1086 accounts in total.&lt;/p&gt;
    &lt;code&gt;beanquery&amp;gt; select count(*) from (select distinct(account));
coun
----
1086
&lt;/code&gt;
    &lt;p&gt;... which does not mean that there are 1086 bank accounts. Accounts in Beancount are virtual, and you can create as many as you like. Imagine one account for categorizing supermarket spending, one for tracking your income, one for your Netflix subscription, and so on.&lt;/p&gt;
    &lt;p&gt;Next, there are about 500 documents in the repository.&lt;/p&gt;
    &lt;code&gt;❯ find documents/ -name "*.pdf" | wc -l
     507
&lt;/code&gt;
    &lt;p&gt;Beancount lets you attach documents (e.g. receipts or invoices) to transactions, that makes bookkeeping very efficient. I love the fact that whenever I need to do my tax returns, I can just take a look at my Beancount ledger and find all the invoices right there next to the relevant transaction.&lt;/p&gt;
    &lt;p&gt;Lastly, in terms of postings, I started out with 715 in the year 2016, and the year 2023 was the busiest so far in terms of just the total postings count.&lt;/p&gt;
    &lt;code&gt;beanquery&amp;gt; select year(date), count(*) where year(date) &amp;lt; 2025 group by year(date);
year  coun
----  ----
2016   715
2017  1422
2018  1605
2019  2437
2020  1582
2021  2022
2022  2435
2023  2651
2024  2602
&lt;/code&gt;
    &lt;head rend="h2"&gt;The Monthly Ritual&lt;/head&gt;
    &lt;p&gt;I wrote earlier that every month I take about 30-45 minutes to import my financial transactions into Beancount. What does that workflow look like? I wrote another, much more detailed blog post about it a few years ago, but here's a gist.&lt;/p&gt;
    &lt;p&gt;It starts with me logging in to my bank account(s) to download my monthly statement(s) in CSV (CSV because it's much more predictable to parse compared to PDF). I then run these CSV files through what's called an "importer", that converts this CSV data into data structures that Beancount understands. I then append all those extracted entries into my current &lt;code&gt;.beancount&lt;/code&gt; file (which is the main file containing all my
financial transactions in plain text). I then go through each entry one by one and make
sure it's balanced (in double-entry bookkeeping, all the postings in a transaction must
sum to zero, and not all postings/transactions that an importer outputs are balanced).
Some of that balancing is manual and some of it is automated (e.g. the importer code can
look at the transaction's description and decide which account it should go into, and
balance automatically). This last part (balancing) is where most of those 30-45 minutes
go.&lt;/p&gt;
    &lt;p&gt;Whenever a new year starts, I move all the transactions from the past year into a &lt;code&gt;&amp;lt;year&amp;gt;.beancount&lt;/code&gt; file and add an &lt;code&gt;include &amp;lt;year.beancount&amp;gt;&lt;/code&gt; in the active
&lt;code&gt;main.beancount&lt;/code&gt;file, mostly to avoid the main file from becoming too long. Not that it
would be an issue for Beancount, but just for the sake of readability.&lt;/p&gt;
    &lt;p&gt;With such a workflow, all my financial transactions from the beginning of time are contained in a few plain text files in one directory on my laptop.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building Beancount Importers for German Banks&lt;/head&gt;
    &lt;p&gt;Beancount only provides the foundations for working with money, but it has no knowledge of what your bank statements look like. This is where the concept of an importer comes in. An importer is a (Python) class that takes in a bank statement of sorts (e.g. a CSV export of your transactions) and converts them into something that Beancount can work with.&lt;/p&gt;
    &lt;p&gt;I live in Germany and my bank accounts are with German banks, so I had to write a few importers for a few different banks, specifically beancount-dkb, beancount-ing, beancount-n26, and beancount-commerzbank. I closed out my Commerzbank account a while ago, so I don't maintain that integration anymore, but the first three libraries are actively maintained (and used)!&lt;/p&gt;
    &lt;head rend="h2"&gt;From User to Author&lt;/head&gt;
    &lt;p&gt;My start with Beancount was a bit bumpy. The documentation is very comprehensive but as a newcomer, I found it tricky to get a grasp on the overall workflow. It took me some trial and error to figure things out and have that "aha" moment.&lt;/p&gt;
    &lt;p&gt;I figured that if I found it tricky, maybe it's tricky for others as well. So I wrote a short book to help newcomers get up and running with Beancount easily. If you're interested, here's a link: https://personalfinancespython.com.&lt;/p&gt;
    &lt;p&gt;The feedback on the book has been super positive. It got mentioned on Beancount's external contributions page, and the reader reviews have all been very encouraging!&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thoughts&lt;/head&gt;
    &lt;p&gt;Having all my finances in a bunch of plain text files tracked in a git repository feels invaluable to me. And hitting the 10 years mark on that almost feels like a milestone.&lt;/p&gt;
    &lt;p&gt;Perhaps the nicest bit about all this is that this data is sitting on my own machine, not in some data center somewhere else. All of it is in plain text files that I can open up in my editor, and analyze using the tools that the Beancount ecosystem gives me. All of it will outlive any app or service, and that, I feel, is why plaintext accounting is so powerful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46463644</guid><pubDate>Fri, 02 Jan 2026 11:07:51 +0000</pubDate></item><item><title>Show HN: I built a clipboard tool to strip/keep specific formatting like Italics</title><link>https://custompaste.com</link><description>&lt;doc fingerprint="7410077e1d297e98"&gt;
  &lt;main&gt;
    &lt;p&gt;🎉 New Year Sale: Get Lifetime Access for just $14.50 (50% OFF) →&lt;/p&gt;
    &lt;p&gt;CustomPaste is a powerful Windows utility that automatically cleans, formats, and transforms your clipboard text as you paste. Create reusable 'recipes' and let `Ctrl+V` finally do the work you want it to.&lt;/p&gt;
    &lt;p&gt;We've all been there. You copy text from a website, a PDF, or an email, and what you get is a mess. Wrong fonts, awkward line breaks, and hidden formatting that breaks your workflow. You spend precious minutes on the mindless, repetitive task of cleaning it up.&lt;/p&gt;
    &lt;p&gt;How many hours of your life have you lost to this? There has to be a better way.&lt;/p&gt;
    &lt;p&gt;From simple text to complex data, CustomPaste gives you superpowers to make every paste perfect.&lt;/p&gt;
    &lt;p&gt;The #1 Frustration, Solved&lt;/p&gt;
    &lt;p&gt;Be a surgeon: keep essentials like bold and hyperlinks while enforcing your preferred font. Or, be the executioner: use the Force Plain Text option to nuke all formatting, no questions asked. You are always in control.&lt;/p&gt;
    &lt;p&gt;The perfect tool for one-off tasks. Use the Quick Menu,triggered from any app by hotkey or mouse, to apply any of your saved recipes on the fly. It's your full toolkit, on-demand, without ever needing to open the main dashboard.&lt;/p&gt;
    &lt;p&gt;Turn messy lists into organized data in one paste. Instantly purge duplicate lines to find unique entries, sort lists alphabetically, and enforce perfect capitalization with UPPERCASE, lowercase, Sentence Case, or Title Case conversions.&lt;/p&gt;
    &lt;p&gt;Go from chaotic copy to clean, usable text. Instantly fix AI-generated text by converting “smart quotes” and em-dashes, remove emojis, flatten extra blank lines, and fix all weird whitespace issues without you lifting a finger. Your text is clean before it even arrives.&lt;/p&gt;
    &lt;p&gt;Stop pasting broken web pages. CustomPaste intelligently handles complex formats. Create recipes to preserve or strip images and keep HTML tables structured. Finally, you can copy from a website without pasting a disaster.&lt;/p&gt;
    &lt;p&gt;CustomPaste is built to be invisible. It launches on startup and runs silently in the background, always ready but never in your way.&lt;/p&gt;
    &lt;p&gt;In the dashboard, simply check the boxes for the transformations you need. Think of it as creating the blueprint for your ideal paste.&lt;/p&gt;
    &lt;p&gt;No scripting or complex manuals required.&lt;/p&gt;
    &lt;p&gt;Click "Set as Active" to make your recipe the new default for `Ctrl+V`. Need to go back to normal? Just click "Deactivate." You are always in control.&lt;/p&gt;
    &lt;p&gt;The next time you press `Ctrl+V`, CustomPaste instantly applies your rules and pastes a perfectly clean result. It's so fast, it feels like magic.&lt;/p&gt;
    &lt;p&gt;Create rules for cleaning, formatting, and extraction, and watch Ctrl+V become the smartest key on your keyboard.&lt;/p&gt;
    &lt;p&gt;We believe clipboard tools should be private and lightweight. CustomPaste runs 100% locally on your Windows machine, using virtually no system resources.&lt;/p&gt;
    &lt;p&gt;Your text is never sent to our servers. All processing happens on your device.&lt;/p&gt;
    &lt;p&gt;We don't read, save, or sell anything you copy. We can't see it, period.&lt;/p&gt;
    &lt;p&gt;The app works perfectly, even without an internet connection.&lt;/p&gt;
    &lt;p&gt;CustomPaste is built for professionals who are tired of letting the clipboard slow them down.&lt;/p&gt;
    &lt;p&gt;Problem: Copying sources creates a mess of fonts and sizes.&lt;/p&gt;
    &lt;p&gt;Agitate: It's a tedious game of "hunt-the-font." One missed format makes the whole document look unprofessional.&lt;/p&gt;
    &lt;p&gt;Solve: Instantly standardize text while surgically preserving italics and bold.&lt;/p&gt;
    &lt;p&gt;Problem: You're pasting raw, jumbled data full of duplicates and junk.&lt;/p&gt;
    &lt;p&gt;Agitate: Your momentum is dead. That "quick insight" just became a 30-minute chore in Excel, killing your train of thought.&lt;/p&gt;
    &lt;p&gt;Solve: Paste and instantly remove duplicates, sort lists, and strip formatting.&lt;/p&gt;
    &lt;p&gt;Problem: Inspiration comes from sources with different formatting.&lt;/p&gt;
    &lt;p&gt;Agitate: Your creative flow evaporates. One pasted quote infects your document, forcing you to battle phantom styles instead of write.&lt;/p&gt;
    &lt;p&gt;Solve: Enforce font consistency, convert headlines to Title Case, and nuke emojis.&lt;/p&gt;
    &lt;p&gt;Problem: Copying code brings in “smart quotes” and weird whitespace.&lt;/p&gt;
    &lt;p&gt;Agitate: The build fails. It’s not your logic. You’re now losing 10 minutes to a SyntaxError from one invisible "smart quote."&lt;/p&gt;
    &lt;p&gt;Solve: Instantly clean snippets, fix smart quotes, and sort/deduplicate log files.&lt;/p&gt;
    &lt;p&gt;Costs less than 20 minutes of your billable time, but saves you hundreds of hours this year.&lt;/p&gt;
    &lt;p&gt;$0&lt;/p&gt;
    &lt;p&gt;Try it free for 100 pastes&lt;/p&gt;
    &lt;p&gt;$29&lt;/p&gt;
    &lt;p&gt;$14.50&lt;/p&gt;
    &lt;p&gt;One-time purchase. Own it forever.&lt;/p&gt;
    &lt;p&gt;30-Day Money-Back Guarantee&lt;/p&gt;
    &lt;p&gt;No. We can't.&lt;/p&gt;
    &lt;p&gt;CustomPaste is built to be 100% local and private. All text processing happens on your computer. Your clipboard data never leaves your machine.&lt;/p&gt;
    &lt;p&gt;We also offer a full offline activation method for high-security environments.&lt;/p&gt;
    &lt;p&gt;Your passwords, API keys, and private documents are yours alone. We built this for ourselves first, we wouldn't trust our own data with anything less.&lt;/p&gt;
    &lt;p&gt;Hey, I'm Joseph, the developer and one-person team behind CustomPaste. I built this tool for one simple reason: I was personally tired of fighting my clipboard. That "paste and fix" cycle you see on this page? That was my daily frustration while coding and researching.&lt;/p&gt;
    &lt;p&gt;CustomPaste is my solution. It's the tool I always wanted, and I use it every day. I'm committed to making it the best it can be, and I believe in software that's simple and honest. That's why CustomPaste is a one-time purchase. No subscriptions, no cloud accounts, and no data harvesting.&lt;/p&gt;
    &lt;p&gt;- Joseph M.&lt;/p&gt;
    &lt;p&gt;I'm still building! Get notified about major feature releases, special offers, and the (much-requested) macOS version.&lt;/p&gt;
    &lt;p&gt;Just the good stuff. No spam. Unsubscribe anytime.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46463992</guid><pubDate>Fri, 02 Jan 2026 12:07:00 +0000</pubDate></item><item><title>Assorted less(1) tips</title><link>https://blog.thechases.com/posts/assorted-less-tips/</link><description>&lt;doc fingerprint="3748813362a61f93"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Assorted less(1) tips&lt;/head&gt;
    &lt;p&gt; In a recent discussion on Reddit I shared a number of tips about the common utility &lt;code&gt;less(1)&lt;/code&gt;
that others found helpful
so I figured I'd aggregate some of those tips here.
&lt;/p&gt;
    &lt;head rend="h3"&gt;Operating on multiple files&lt;/head&gt;
    &lt;p&gt; While most folks invoke &lt;code&gt;less&lt;/code&gt;
at the tail of a pipeline like


you can directly provide one or more files to open

&lt;/p&gt;
    &lt;head rend="h3"&gt;Adding files after starting&lt;/head&gt;
    &lt;p&gt; When reading a document, sometimes you want to view another file, adding it to the file list. Perhaps while reading some C source code you want to also look over the corresponding header-file. You can add that header-file to the argument list with &lt;code&gt;:e file.h&lt;/code&gt;
&lt;/p&gt;
    &lt;head rend="h3"&gt;Navigating multiple files&lt;/head&gt;
    &lt;p&gt; You can navigate between multiple files using &lt;code&gt;:n&lt;/code&gt;
to go to the next file in the argument-list, and
&lt;code&gt;:p&lt;/code&gt;
for the previous file.
You can also use
&lt;code&gt;:x&lt;/code&gt;
to rewind to the first file in the argument-list
similar to how
&lt;code&gt;:rewind&lt;/code&gt;
behaves in
&lt;code&gt;vi&lt;/code&gt;/&lt;code&gt;vim&lt;/code&gt;.
&lt;/p&gt;
    &lt;head rend="h3"&gt;Removing files after starting&lt;/head&gt;
    &lt;p&gt; While I rarely feel the need to, if you have finished with a file and want to keep your argument list clean, you can use &lt;code&gt;:d&lt;/code&gt;
to delete the current file from the argument-list.
&lt;/p&gt;
    &lt;head rend="h2"&gt;Navigating&lt;/head&gt;
    &lt;head rend="h3"&gt;Jumping to a particular line-number&lt;/head&gt;
    &lt;p&gt; Use &lt;code&gt;Â«countÂ»G&lt;/code&gt;
to jump to a particular line-number.
So using
&lt;code&gt;3141G&lt;/code&gt;
will jump to line 3141.
It helps to display
line numbers.
&lt;/p&gt;
    &lt;head rend="h3"&gt;Jumping to a particular percentage-offset&lt;/head&gt;
    &lt;p&gt; Similarly, using &lt;code&gt;Â«countÂ»%&lt;/code&gt;
jumps to that percentage-offset of the file.
So if you want to go to Â¾
of the way through the file, you can type
&lt;code&gt;75%&lt;/code&gt;
to jump right there.
&lt;/p&gt;
    &lt;head rend="h3"&gt;Searching&lt;/head&gt;
    &lt;p&gt; While many folks know you can search forward with &lt;code&gt;/Â«patternÂ»&lt;/code&gt;
and some people know you can use
&lt;code&gt;?Â«patternÂ»&lt;/code&gt;
to search backwards,
or use
&lt;code&gt;n&lt;/code&gt;/&lt;code&gt;N&lt;/code&gt;
to search again for the next/previous match,
&lt;code&gt;less&lt;/code&gt;
provides modifiers you can specify
before the pattern to modify its behavior:
&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;
        &lt;code&gt;!&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-1"&gt;Find the next line that doesn't match the pattern&lt;/item&gt;
      &lt;item rend="dt-2"&gt;
        &lt;code&gt;*&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-2"&gt;search across multiple files, starting from the current location in the current file&lt;/item&gt;
      &lt;item rend="dt-3"&gt;
        &lt;code&gt;@&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-3"&gt;rewind to the first file and search from there&lt;/item&gt;
      &lt;item rend="dt-4"&gt;
        &lt;code&gt;@*&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-4"&gt;rewind to the first file and search from there across multiple files&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Thus you would use &lt;code&gt;/@*Â«patternÂ»&lt;/code&gt;
to search for "pattern"
starting with the first file.
&lt;/p&gt;
    &lt;head rend="h2"&gt;Filtering lines&lt;/head&gt;
    &lt;p&gt; Using &lt;code&gt;&amp;amp;&lt;/code&gt;
lets you specify a pattern
and filter the displayed lines
to only those matching the pattern,
much like an internal
&lt;code&gt;grep&lt;/code&gt;
command.
If you modify it with
&lt;code&gt;!&lt;/code&gt;,
so it will display only those lines that do
not match
the pattern, like
&lt;code&gt;&amp;amp;!Â«patternÂ»&lt;/code&gt;.
I find this particularly helpful for browsing log-files.
&lt;/p&gt;
    &lt;head rend="h2"&gt;Bookmarking&lt;/head&gt;
    &lt;p&gt; You can bookmark points in a file with &lt;code&gt;m&lt;/code&gt;
followed by a letter,
then jump back to that bookmark with
&lt;code&gt;'&lt;/code&gt;
followed by the same letter.
These apply globally across all open files,
so if you
&lt;code&gt;ma&lt;/code&gt;
in the third file,
then navigate away to other files,
using
&lt;code&gt;'a&lt;/code&gt;
will take you back to the marked location
in that third file.
I use marks most when reading man-pages,
dropping one mark at the
OPTIONS
section such as
&lt;code&gt;mo&lt;/code&gt;,
and another at the
EXAMPLES
section,
such as
&lt;code&gt;me&lt;/code&gt;,
then bounce back and forth between them with
&lt;code&gt;'o&lt;/code&gt;
and
&lt;code&gt;'e&lt;/code&gt;.
While you can use any of
the 26 lowercase or uppercase letters
(for a total of 52 marks),
I rarely use more than two or three
either in alphabetical order
("a", "b", "c"),
or assigning mnemonics like in the
&lt;code&gt;man&lt;/code&gt;-page example above.
&lt;/p&gt;
    &lt;head rend="h3"&gt;Bracket matching&lt;/head&gt;
    &lt;p&gt; If the first line on the screen contains a &lt;code&gt;(&lt;/code&gt;,
&lt;code&gt;[&lt;/code&gt;,
or
&lt;code&gt;{&lt;/code&gt;,
typing that character
will jump to the matching/closing character,
putting it on the bottom line of the screen.
Similarly, if a closing
&lt;code&gt;)&lt;/code&gt;,
&lt;code&gt;]&lt;/code&gt;,
or
&lt;code&gt;}&lt;/code&gt;,
character appears on the last line,
typing that closing character
will jump to the matching/opening character,
putting it at the top of the screen.
I find it a little disorienting
if they fall &lt;code&gt;less&lt;/code&gt; than a screen-height apart
because what feels like a forward motion
to find the next matching close-bracket
might actually result in shifting the screen
down
rather than
up
which feels backwards.
&lt;/p&gt;
    &lt;p&gt; While I don't use it much, you can also specify match-pairs using &lt;code&gt;alt+ctrl+f&lt;/code&gt;
or
&lt;code&gt;alt+ctrl+b&lt;/code&gt;
followed by the opening/closing pair of characters
such as
&lt;code&gt;alt+ctrl+f&amp;lt;&amp;gt;&lt;/code&gt;
to define a "&amp;lt;"â¦"&amp;gt;" pair
and jump between them
in a manner similar to the
&lt;code&gt;(&lt;/code&gt;/&lt;code&gt;)&lt;/code&gt;,
&lt;code&gt;[&lt;/code&gt;/&lt;code&gt;]&lt;/code&gt;,
and
&lt;code&gt;{&lt;/code&gt;/&lt;code&gt;)&lt;/code&gt;
motions.
&lt;/p&gt;
    &lt;head rend="h2"&gt;Toggling options without restarting&lt;/head&gt;
    &lt;p&gt; While the &lt;code&gt;man&lt;/code&gt;-page
documents many flags you can pass
on the command-line,
you can also toggle boolean options from inside
&lt;code&gt;less&lt;/code&gt;.
I find this particularly helpful
when I've fed the output of a long-running process to
&lt;code&gt;less&lt;/code&gt;
and don't want to re-run it
because it will take a long time.
Instead of quitting,
you can type a literal
&lt;code&gt;-&lt;/code&gt;
followed by the option you want to change.
I most commonly want to toggle word-wrap for long lines,
so instead of quitting and adding
&lt;code&gt;-S&lt;/code&gt;
at the end of my pipeline,
I can type
&lt;code&gt;-S&lt;/code&gt;
directly in
&lt;code&gt;less&lt;/code&gt;.
Options I commonly toggle:
&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;
        &lt;code&gt;-S&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-1"&gt;word-wrap (mnemonic "splitting long lines")&lt;/item&gt;
      &lt;item rend="dt-2"&gt;
        &lt;code&gt;-G&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-2"&gt;search-highlighting&lt;/item&gt;
      &lt;item rend="dt-3"&gt;&lt;code&gt;-i&lt;/code&gt;/&lt;code&gt;-I&lt;/code&gt;&lt;/item&gt;
      &lt;item rend="dd-3"&gt;smart-case/case-sensitivity for searches&lt;/item&gt;
      &lt;item rend="dt-4"&gt;
        &lt;code&gt;-R&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-4"&gt;ANSI-color escaping&lt;/item&gt;
      &lt;item rend="dt-5"&gt;&lt;code&gt;-N&lt;/code&gt;/&lt;code&gt;-n&lt;/code&gt;&lt;/item&gt;
      &lt;item rend="dd-5"&gt;show/hide line-numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Running external commands&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;!&lt;/code&gt;
lets you invoke an external command.
I don't do this often,
but occasionally I want some simple reference
like the current date
(&lt;code&gt;!date&lt;/code&gt;)
or to do some simple math
(&lt;code&gt;!bc&lt;/code&gt;).
&lt;/p&gt;
    &lt;head rend="h2"&gt;Default options with &lt;code&gt;$LESS&lt;/code&gt;
&lt;/head&gt;
    &lt;p&gt; You might find yourself regularly setting a common group of options so you can put those in your environment (usually in your shell startup file like &lt;code&gt;~/.bashrc&lt;/code&gt;)
like
&lt;code&gt;LESS="-RNe"&lt;/code&gt;
if you want to
show ANSI colors,
show line-numbers,
and exit automatically when you reach the end of the file.
&lt;/p&gt;
    &lt;head rend="h2"&gt;Other misc&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;less&lt;/code&gt;
has a few other corners that I've never really used,
but figured I'd document here:
&lt;/p&gt;
    &lt;head rend="h3"&gt;Tags&lt;/head&gt;
    &lt;p&gt; While I've used tags in &lt;code&gt;vi&lt;/code&gt;/&lt;code&gt;vim&lt;/code&gt;
to easily jump between definitions.
However, even though
&lt;code&gt;less&lt;/code&gt;
provides support for tags generated by
&lt;code&gt;ctags&lt;/code&gt;.
I've never found cause to use them.
&lt;/p&gt;
    &lt;head rend="h3"&gt;Editing the current document&lt;/head&gt;
    &lt;p&gt; The &lt;code&gt;v&lt;/code&gt;
command will open your
&lt;code&gt;$VISUAL&lt;/code&gt;
editor on the current document.
&lt;/p&gt;
    &lt;head rend="h3"&gt;"Log" output&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;less&lt;/code&gt;
lets you redirect the output
it has gathered from
stdin
to a file using the
&lt;code&gt;o&lt;/code&gt;
command
(or the
&lt;code&gt;O&lt;/code&gt;
command to overwrite an existing file).
This might come in handy because
&lt;code&gt;less&lt;/code&gt;
won't let you
edit stdin
in an external editor
but you can write it directly to a file.
&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46464120</guid><pubDate>Fri, 02 Jan 2026 12:29:23 +0000</pubDate></item><item><title>Show HN: Dealta – A game-theoretic decentralized trading protocol</title><link>https://github.com/orgs/Dealta-Foundation/repositories</link><description>&lt;doc fingerprint="eb7ad05e09566404"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46464133</guid><pubDate>Fri, 02 Jan 2026 12:31:03 +0000</pubDate></item><item><title>FracturedJson</title><link>https://github.com/j-brooke/FracturedJson/wiki</link><description>&lt;doc fingerprint="767075204fb04a98"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 8&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Home&lt;/head&gt;
    &lt;p&gt;FracturedJson is a family of utilities that format JSON data in a way that's easy for humans to read, but fairly compact. Arrays and objects are written on single lines, as long as they're neither too long nor too complex. When several such lines are similar in structure, they're written with fields aligned like a table. Long arrays are written with multiple items per line across multiple lines.&lt;/p&gt;
    &lt;p&gt;There are lots of settings available to control the output, but usually you can ignore most of them. FracturedJson produces nice-looking output from any set of JSON data automatically.&lt;/p&gt;
    &lt;p&gt;You can try it out with the browser formatter page. It's also available as a .NET library, a JavaScript/Typescript package, and a Visual Studio Code extension. See here for Python options.&lt;/p&gt;
    &lt;p&gt;Here's a sample of output using default settings. See the Options page for examples of what you different settings do.&lt;/p&gt;
    &lt;code&gt;{
    "BasicObject"   : {
        "ModuleId"   : "armor",
        "Name"       : "",
        "Locations"  : [
            [11,  2], [11,  3], [11,  4], [11,  5], [11,  6], [11,  7], [11,  8], [11,  9],
            [11, 10], [11, 11], [11, 12], [11, 13], [11, 14], [ 1, 14], [ 1, 13], [ 1, 12],
            [ 1, 11], [ 1, 10], [ 1,  9], [ 1,  8], [ 1,  7], [ 1,  6], [ 1,  5], [ 1,  4],
            [ 1,  3], [ 1,  2], [ 4,  2], [ 5,  2], [ 6,  2], [ 7,  2], [ 8,  2], [ 8,  3],
            [ 7,  3], [ 6,  3], [ 5,  3], [ 4,  3], [ 0,  4], [ 0,  5], [ 0,  6], [ 0,  7],
            [ 0,  8], [12,  8], [12,  7], [12,  6], [12,  5], [12,  4]
        ],
        "Orientation": "Fore",
        "Seed"       : 272691529
    },
    "SimilarArrays" : {
        "Katherine": ["blue",       "lightblue", "black"       ],
        "Logan"    : ["yellow",     "blue",      "black", "red"],
        "Erik"     : ["red",        "purple"                   ],
        "Jean"     : ["lightgreen", "yellow",    "black"       ]
    },
    "SimilarObjects": [
        { "type": "turret",    "hp": 400, "loc": {"x": 47, "y":  -4}, "flags": "S"   },
        { "type": "assassin",  "hp":  80, "loc": {"x": 12, "y":   6}, "flags": "Q"   },
        { "type": "berserker", "hp": 150, "loc": {"x":  0, "y":   0}                 },
        { "type": "pittrap",              "loc": {"x": 10, "y": -14}, "flags": "S,I" }
    ]
}&lt;/code&gt;
    &lt;p&gt;Optionally, comments can be preserved. Comments aren't allowed by the official JSON standard, but they're ubiquitous, so it's nice to have the option. FracturedJson tries to keep the comments together with whatever elements they seem to relate to.&lt;/p&gt;
    &lt;code&gt;{
    /*
     * Multi-line comments
     * are fun!
     */
    "NumbersWithHex": [
          254 /*00FE*/,  1450 /*5AA*/ ,     0 /*0000*/, 36000 /*8CA0*/,    10 /*000A*/,
          199 /*00C7*/, 15001 /*3A99*/,  6540 /*198C*/
    ],
    /* Elements are keen */
    "Elements"      : [
        { /*Carbon*/   "Symbol": "C",  "Number":  6, "Isotopes": [11, 12, 13, 14] },
        { /*Oxygen*/   "Symbol": "O",  "Number":  8, "Isotopes": [16, 18, 17    ] },
        { /*Hydrogen*/ "Symbol": "H",  "Number":  1, "Isotopes": [ 1,  2,  3    ] },
        { /*Iron*/     "Symbol": "Fe", "Number": 26, "Isotopes": [56, 54, 57, 58] }
        // Not a complete list...
    ],

    "Beatles Songs" : [
        "Taxman",          // George
        "Hey Jude",        // Paul
        "Act Naturally",   // Ringo
        "Ticket To Ride"   // John
    ]
}&lt;/code&gt;
    &lt;p&gt;Most JSON libraries give you a choice between two formatting options. Minified JSON is very efficient, but difficult for a person to read.&lt;/p&gt;
    &lt;code&gt;{"AttackPlans":[{"TeamId":1,"Spawns":[{"Time":0.0,"UnitType":"Grunt","SpawnPointIndex":0},{"Time":0.0,"UnitType":"Grunt","SpawnPointIndex":0},{"Time":0.0,"UnitType":"Grunt","SpawnPointIndex":0}]}],"DefensePlans":[{"TeamId":2,"Placements":[{"UnitType":"Archer","Position":[41,7]},{"UnitType":"Pikeman","Position":[40,7]},{"UnitType":"Barricade","Position":[39,7]}]}]}&lt;/code&gt;
    &lt;p&gt;Most beautified/indented JSON, on the other hand, is too spread out, often making it difficult to take in quickly or to scan for specific information.&lt;/p&gt;
    &lt;code&gt;{
    "AttackPlans": [
        {
            "TeamId": 1,
            "Spawns": [
                {
                    "Time": 0,
                    "UnitType": "Grunt",
                    "SpawnPointIndex": 0
                },
                {
                    "Time": 0,
                    "UnitType": "Grunt",
                    "SpawnPointIndex": 0
                },
                {
                    "Time": 0,
                    "UnitType": "Grunt",
                    "SpawnPointIndex": 0
                }
            ]
        }
    ],
    "DefensePlans": [
        {
            "TeamId": 2,
            "Placements": [
                {
                    "UnitType": "Archer",
                    "Position": [
                        41,
                        7
                    ]
                },
                {
                    "UnitType": "Pikeman",
                    "Position": [
                        40,
                        7
                    ]
                },
                {
                    "UnitType": "Barricade",
                    "Position": [
                        39,
                        7
                    ]
                }
            ]
        }
    ]
}&lt;/code&gt;
    &lt;p&gt;FracturedJson tries to format data like a person would. Containers are kept to single lines as long as they're not too complex and not too long. If several successive inline arrays or objects are similar enough, they will be formatted as a table.&lt;/p&gt;
    &lt;code&gt;{
    "AttackPlans" : [
        {
            "TeamId": 1,
            "Spawns": [
                {"Time": 0, "UnitType": "Grunt", "SpawnPointIndex": 0},
                {"Time": 0, "UnitType": "Grunt", "SpawnPointIndex": 0},
                {"Time": 0, "UnitType": "Grunt", "SpawnPointIndex": 0}
            ]
        }
    ],
    "DefensePlans": [
        {
            "TeamId"    : 2,
            "Placements": [
                { "UnitType": "Archer",    "Position": [41, 7] },
                { "UnitType": "Pikeman",   "Position": [40, 7] },
                { "UnitType": "Barricade", "Position": [39, 7] }
            ]
        }
    ]
}&lt;/code&gt;
    &lt;p&gt;FracturedJson uses four types of formatting: inlined, compact multiline array, table and expanded.&lt;/p&gt;
    &lt;p&gt;When possible, sections of the document are written inlined, as long as that doesn't make them too long or too complex (as determined by the settings). "Complexity" refers to how deeply nested an array or object's contents are.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;{ "UnitType": "Archer", "Position": [41, 7] }&lt;/code&gt;
    &lt;p&gt;Use the setting MaxInlineComplexity to control how much nesting is allowed on one line.&lt;/p&gt;
    &lt;p&gt;The next option, for arrays, is to write them with multiple items per line, across multiple lines.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;[
    [19,  2], [ 3,  8], [14,  0], [ 9,  9], [ 9,  9], [ 0,  3], [10,  1],
    [ 9,  1], [ 9,  2], [ 6, 13], [18,  5], [ 4, 11], [12,  2]
]&lt;/code&gt;
    &lt;p&gt;Use the setting MaxCompactArrayComplexity to control how deeply nested items can be when arranged this way, or use &lt;code&gt;-1&lt;/code&gt; to disable this feature.&lt;/p&gt;
    &lt;p&gt;If an array or object contains inlineable items of the same type, they can be formatted in a tabular format. With enough room, all fields at any depth are lined up (and reordered if necessary).&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;{
    "Rect" : { "position": {"x": -44, "y":  3.4         }, "color": [0, 255, 255] },
    "Point": { "position": {          "y": 22   , "z": 3}                         },
    "Oval" : { "position": {"x": 140, "y":  0.04        }, "color": "#7f3e96"     },
    "Plane": { "position": null,                           "color": [0, 64, 64]   }
}&lt;/code&gt;
    &lt;p&gt;If the table wouldn't fit with all of the elements and subelements aligned, the inner-most containers are collapsed while keeping the outer ones still aligned.&lt;/p&gt;
    &lt;code&gt;{
    "Rect" : { "position": {"x": -44, "y": 3.4},  "color": [0, 255, 255] },
    "Point": { "position": {"y": 22, "z": 3}                             },
    "Oval" : { "position": {"x": 140, "y": 0.04}, "color": "#7f3e96"     },
    "Plane": { "position": null,                  "color": [0, 64, 64]   }
}&lt;/code&gt;
    &lt;p&gt;Use the setting MaxTableRowComplexity to control how how deeply nested rows can be, or use &lt;code&gt;-1&lt;/code&gt; to disable this feature.  Use TableCommaPlacement to adjust where commas are placed.&lt;/p&gt;
    &lt;p&gt;If none of those options work for an element, it will be written across multiple lines, with child items indented and starting on its own line.&lt;/p&gt;
    &lt;p&gt;Example:&lt;/p&gt;
    &lt;code&gt;[
    {
        "type" : "turret",
        "hp"   : 400,
        "loc"  : {"x": 47, "y": -4},
        "flags": ["stationary"]
    },
    {
        "type" : "assassin",
        "hp"   : 80,
        "loc"  : {"x": 102, "y": 6},
        "flags": ["stealth"]
    },
    { "type": "berserker", "hp": 150, "loc": {"x": 0, "y": 0} },
    {
        "type" : "pittrap",
        "loc"  : {"x": 10, "y": -14},
        "flags": ["invulnerable", "stationary"]
    }
]&lt;/code&gt;
    &lt;p&gt;If you have any questions, comments, requests, advice, or whatever, feel free to check out the discussions area.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46464235</guid><pubDate>Fri, 02 Jan 2026 12:46:31 +0000</pubDate></item><item><title>39th Chaos Communication Congress Videos</title><link>https://media.ccc.de/b/congress/2025</link><description>&lt;doc fingerprint="8c511ca995f789c6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 42 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Ein Jahr Adenauer SRP+ und der Walter Lübcke Memorial Park &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 31 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; How Hackers Breached the Great Firewall of China &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 40 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Was alte Behördendomains verraten &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 56 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Genug mit dem Bürgergeld-Fetisch. Stürmt die Paläste! &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 36 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Terrestrial Navigation in the Baltic Sea Region &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 39 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Wie Algorithmen in Glücksspielprodukten sich Wirkweisen des… &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 52 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; A Deep Dive into WhatsApp 0-Click Exploits on iOS and… &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 38 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Vom D-Flip-Flop bis zum eigenen Betriebssystem &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 38 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; "AI", Cybernetics, and Fascism and how to Intervene &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 58 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; From Embodied AI Jailbreak to Remote Takeover of Humanoid… &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 58 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Aktuelle Informationen zu den Verfahren im Budapest-Komplex… &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 61 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Wie Cellebrite ins Ausländeramt kam &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 39 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Klimaupdate mit dem FragDenStaat Climate Helpdesk &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 34 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Taking a rotary dial phone to the mobile age &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 60 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Wie autoritäre Akteure KI-generierte Inhalte für Social… &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 60 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Über KI-Systeme im Kriegseinsatz in Gaza und warum… &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 60 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; How the EU's reform agenda threatens to erase a decade of… &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 39 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; – an empowering journey through the energy transition &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 38 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; How to construct an Open Pager System for c3 &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 38 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; The Hidden Politics of Token Languages in Generative AI &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 40 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Herausforderungen für dezentrale Netzwerke aus Sicht der… &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 40 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Insights from 43 Million Kilometers of European Cycling Data &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 41 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Qualcomm GPU Emulation and Fuzzing with LibAFL QEMU &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 58 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; InselChaos und Håck ma’s Castle plaudern aus dem Nähkästchen &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 39 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Was die Informationsfreiheit in Österreich bringt &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 39 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Public, difficult to access, and not always correct &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 55 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; The story how Kenyans fought back against intrusive digital… &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 41 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; DNA security systems based on molecular randomness &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 34 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; Vom Algorithmus zum Baumpilz im digitalen Metabolismus &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt; 40 min &lt;/p&gt;
        &lt;/div&gt;
        &lt;div&gt;
          &lt;head rend="h4"&gt; The fight against data retention and boundless access to… &lt;/head&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46464426</guid><pubDate>Fri, 02 Jan 2026 13:17:06 +0000</pubDate></item><item><title>Parental Controls Aren't for Parents</title><link>https://beasthacker.com/til/parental-controls-arent-for-parents.html</link><description>&lt;doc fingerprint="2d2910b749eaf77e"&gt;
  &lt;main&gt;
    &lt;p&gt;A few days ago, I found that a grown man had been texting my twelve-year-old son on his "kid-safe" Gabb phone. The man got my son's number through a children's book chat on an app called GroupMe. Thankfully my wife and I discovered the situation and intervened before anything bad happened; but still it was sickening to discover that on Christmas morning, while our family was unwrapping presents next to the tree, some creep had been texting my son: "What did you get? Send pictures."&lt;/p&gt;
    &lt;p&gt;How could we have let this happen? How could we be such careless parents?&lt;/p&gt;
    &lt;p&gt;But wait . . . hadn't we done what we were supposed to do? We bought the "kid-safe" phone. And we confirmed GroupMe was on the Gabb "approved apps" list, which, as I understand it, offers "no social media or high-risk options." We did the safe things, right?&lt;/p&gt;
    &lt;p&gt;Maybe not. Turns out Gabb's own blog appears to include GroupMe on a list of seven apps with dangerous chat features, describing it as an app that "opens the door to potential dangers." We were apparently supposed to find that warning ourselves, somewhere among Gabb's 572 blog posts:&lt;/p&gt;
    &lt;code&gt;$ curl -s \
  https://gabb.com/post-sitemap.xml \
  | grep -oE 'https://gabb\.com/blog/[^&amp;lt;]+' \
  | sort -u \
  | wc -l \
  | xargs -I{} echo "{} blog posts as of $(date '+%B %d, %Y')"

572 blog posts as of January 02, 2026&lt;/code&gt;
    &lt;p&gt;But if GroupMe "opens the door" to danger, why did Gabb put it on their "approved apps" list? When I revisited the site, I noticed a small message beneath GroupMe mentioning Communication with Strangers. I hovered over it with my mouse pointer, and a tooltip appeared: "Allows contact and communication with people the child may not know."&lt;/p&gt;
    &lt;p&gt;So it allows communication with strangers, but it's not "high-risk?" The approved list isn't looking so safe. The approved list is apparently a catalog of risks I'm supposed to decipher by filtering through 838 apps and hovering my mouse pointer around to see tooltips:&lt;/p&gt;
    &lt;code&gt;$ for cat in \
  existing_apps \
  unapproved_apps \
  unmet_criteria_apps \
  music_apps; do
  count=$(curl -s "https://gabb.com/app-guide/" \
    | grep -o "${cat} = \[.*\]" \
    | head -1 \
    | sed "s/${cat} = \[//" \
    | sed "s/\]//" \
    | tr ',' '\n' \
    | sed "s/'//g" \
    | sed 's/^ *//' \
    | sort -u \
    | wc -l \
    | tr -d ' ')
  echo "$count $cat"
done &amp;amp;&amp;amp; echo "...as of $(date '+%B %d, %Y')"

586 existing_apps
60 unapproved_apps
170 unmet_criteria_apps
22 music_apps
...as of January 02, 2026&lt;/code&gt;
    &lt;p&gt;Whatever the reason for this complexity, I don't feel in control.&lt;/p&gt;
    &lt;p&gt;And Gabb isn't alone in making me feel like this. It seems like many companies selling tech to families operate in the same way: market safety, deliver complexity, and leave parents to figure it out.&lt;/p&gt;
    &lt;p&gt;Take the Nintendo Switch my son unwrapped between those creepy texts. To set it up, I had to:&lt;/p&gt;
    &lt;p&gt;Only to discover that there's no clear option to block internet access, no clear way to disable downloads from the Nintendo eShop, and no easy way to make this thing function like an old-school Game Boy and just let a kid have fun with a game cartridge. But that's just nostalgia talking. Nobody wants that anymore. Apparently.&lt;/p&gt;
    &lt;p&gt;Because next comes Minecraft. Ah, Minecraft. The game every middle-schooler on earth apparently needs to survive. To let my son play with his friends:&lt;/p&gt;
    &lt;p&gt;Now, I did my best to configure these settings. I really did. But xbox.com alone includes twenty-nine confusingly overlapping settings related to chat, friends, and communication. Twenty-nine.&lt;/p&gt;
    &lt;p&gt;And when I finally—finally—tried to test online play, Minecraft told me I would need to loosen the parental controls (it did not say which) and create a Nintendo Switch Online account for my son.&lt;/p&gt;
    &lt;p&gt;Nintendo Switch Online (not really another account, mind you, but a membership) involves a recurring fee. It also unlocks access to the Nintendo eShop, which I cannot disable. I can set his eShop spending limit to zero, sure. But I can't block free downloads. So to let my son play online Minecraft with his friends, I have to open him up to an unrelated store full of content I can't possibly evaluate. That's the deal. Take it or leave it.&lt;/p&gt;
    &lt;p&gt;I assume some marketing person at Nintendo, probably sitting in a conference room in Kyoto, surrounded by whiteboards covered in arrows and cartoon stick figures, has this entire process mapped out as a "customer journey." And by Step 17, the journey is supposed to be over. You're supposed to be so beaten down, so utterly depleted of will, that you just cave. You sign up for Nintendo Online. You disable a bunch of parental controls you don't really understand. You let your kid play his damn game. You become the ideal customer.&lt;/p&gt;
    &lt;p&gt;But I didn't cave. Instead, somewhere on the threshold of Customer Journey Step 18, I found myself gripping the Switch with both hands and imagining, quite vividly, what it would feel like to lift the Switch up, and bring it down over my knee. I could almost hear the crack. Could almost see that OLED display splintering into a thousand pieces. The little Joy-Cons skittering across the floor. My son's face. My wife's face. The stunned silence.&lt;/p&gt;
    &lt;p&gt;I did not break the Switch.&lt;/p&gt;
    &lt;p&gt;What I did was announce, in a voice louder than necessary, that nobody was to ask me about anything Minecraft-related on the Nintendo Switch for a minimum of two weeks. My son could play Zelda: Breath of the Wild instead, which, thank you, developers, thank you from the bottom of my heart, doesn't appear to involve any mandatory online anything whatsoever.&lt;/p&gt;
    &lt;p&gt;Here's what I want: an off switch. A single setting that says "this child cannot go online, communicate with strangers, spend money, or download anything without my explicit permission." Instead I get a maze, complex enough that when something goes wrong, I'm at fault for a tooltip I didn't hover over, a blog post I didn't read, a submenu I didn't find. Maybe that's by design. Maybe it's neglect. I don't know.&lt;/p&gt;
    &lt;p&gt;What I know is this. My son just wants to play video games and talk to his friends. I just want to keep him safe. Somewhere between those two things, I'm supposed to become an expert in the convoluted parental control schemes of Gabb, Nintendo, Microsoft, and Xbox, while a stranger's Christmas morning texts sit in my son's phone history.&lt;/p&gt;
    &lt;p&gt;Parental controls shouldn't be this hard.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46464652</guid><pubDate>Fri, 02 Jan 2026 13:42:37 +0000</pubDate></item><item><title>The Netflix Simian Army (2011)</title><link>https://netflixtechblog.com/the-netflix-simian-army-16e57fbab116</link><description>&lt;doc fingerprint="7bb1d357b131954b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;IPv6 just turned 30 and still hasn’t taken over the world, but don't call it a failure&lt;/head&gt;
    &lt;head rend="h2"&gt;The world has passed it by in many ways, yet it remains relevant&lt;/head&gt;
    &lt;p&gt;Feature In the early 1990s, internetworking wonks realized the world was not many years away from running out of Internet Protocol version 4 (IPv4) addresses, the numbers needed to identify any device connected to the public internet. Noting booming interest in the internet, the internet community went looking for ways to avoid an IP address shortage that many feared would harm technology adoption and therefore the global economy.&lt;/p&gt;
    &lt;p&gt;A possible fix arrived in December 1995 in the form of RFC 1883, the first definition of IPv6, the planned successor to IPv4.&lt;/p&gt;
    &lt;p&gt;The most important change from IPv4 to IPv6 was moving from 32-bit to 128-bit addresses, a decision that increased the available pool of IP addresses from around 4.3 billion to over 340 undecillion – a 39-digit number. IPv6 was therefore thought to have future-proofed the internet, because nobody could imagine humanity would ever need more than a handful of undecillion IP addresses, never mind the entire range available under IPv6.&lt;/p&gt;
    &lt;p&gt;As billions of devices and people came online, first using PCs and then wielding smartphones, conventional wisdom assumed that network operators would move to IPv6 rather than persist with IPv4.&lt;/p&gt;
    &lt;p&gt;Yet according to data from Google, the Asia Pacific Network Information Center (APNIC), and Cloudflare, less than half of all netizens use IPv6 today.&lt;/p&gt;
    &lt;p&gt;To understand why, know that IPv6 also suggested other, rather modest, changes to the way networks operate.&lt;/p&gt;
    &lt;p&gt;"IPv6 was an extremely conservative protocol that changed as little as possible," APNIC chief scientist Geoff Huston told The Register. "It was a classic case of mis-design by committee."&lt;/p&gt;
    &lt;p&gt;And that notional committee made one more critical choice: IPv6 was not backward-compatible with IPv4, meaning users had to choose one or the other – or decide to run both in parallel.&lt;/p&gt;
    &lt;p&gt;For many, the decision of which protocol to use was easy because IPv6 didn't add features that represented major improvements.&lt;/p&gt;
    &lt;p&gt;"One big surprise to me was how few features went into IPv6 in the end, aside from the massive expansion of address space," said Bruce Davie, a veteran computer scientist recently honored with a lifetime achievement award by the Association for Computing Machinery's Special Interest Group on Data Communications, which lauded him for "fundamental contributions in networking systems through design, standardization, and commercialization of network protocols and systems."&lt;/p&gt;
    &lt;p&gt;Davie said many of the security, plug-and-play, and quality of service features that didn't make it into IPv6 were eventually implemented in IPv4, further reducing the incentive to adopt the new protocol. "Given the small amount of new functionality in v6, it's not so surprising that deployment has been a 30 year struggle," he said.&lt;/p&gt;
    &lt;p&gt;Another innovation that meant IPv6 made less sense was network address translation (NAT), which allows many devices to share a single public IPv4 address. NAT meant IPv4 network operators could connect thousands of devices with a single IP address, meaning their existing IP addresses became more useful.&lt;/p&gt;
    &lt;p&gt;"These solutions were relatively easy to deploy, aligned with existing expertise, and avoided large-scale infrastructure changes," said Alvaro Vives, manager of the learning and development team at RIPE NCC, the regional internet registry for 76 nations across Europe, the Middle East, and Central Asia.&lt;/p&gt;
    &lt;p&gt;Because NAT stalled IPv6 adoption, vendors didn't rally behind the new protocol.&lt;/p&gt;
    &lt;p&gt;"Migration costs, complexity, and training requirements remain high, while short-term ROI is low," Gartner distinguished VP analyst Andrew Lerner told The Register. "Performance parity across applications and devices is inconsistent, and some organizations even disable IPv6 for better performance. Lack of dual-stack support in legacy infrastructure is another barrier," he added.&lt;/p&gt;
    &lt;head rend="h3"&gt;A misunderstood protocol&lt;/head&gt;
    &lt;p&gt;While IPv6 didn't take off as expected, it's not fair to say it failed.&lt;/p&gt;
    &lt;p&gt;"IPv6 wasn't about turning IPv4 off, but about ensuring the internet could continue to grow without breaking," said John Curran, president and CEO of the American Registry for Internet Numbers (ARIN).&lt;/p&gt;
    &lt;p&gt;"In fact, IPv4's continued viability is largely because IPv6 absorbed that growth pressure elsewhere – particularly in mobile, broadband, and cloud environments," he added. "In that sense, IPv6 succeeded where it was needed most, and must be regarded as a success."&lt;/p&gt;
    &lt;p&gt;RIPE NCC's Alvaro Vives agrees. "What IPv6 got right was its long-term design," he told The Register. "It provides a vast address space that allows networks to be planned more simply and consistently. This has enabled innovation, from large mobile networks to the Internet of Things and advanced routing techniques such as Segment Routing over IPv6."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unofficial IETF draft calls for grant of five nonillion IPv6 addresses to ham radio operators&lt;/item&gt;
      &lt;item&gt;IETF Draft suggests making IPv6 standard on DNS resolvers - partly to destroy IPv4&lt;/item&gt;
      &lt;item&gt;China's IPv6 adoption takes a decent leap forward, especially on fixed networks&lt;/item&gt;
      &lt;item&gt;Asia reaches 50 percent IPv6 capability and leads the world in user numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Gartner's Lerner thinks the time has come for organizations to develop detailed IPv6 migration plans.&lt;/p&gt;
    &lt;p&gt;"Validate application compatibility, and ensure new infrastructure supports IPv6," he advised. "Pilot deployments and lab testing with DNS64/NAT64 are recommended. Over time, IPv6 adoption will accelerate as private IPv4 space depletes and cloud providers introduce pricing models that favor IPv6."&lt;/p&gt;
    &lt;p&gt;APNIC's Huston, however, thinks that IPv6 has become less relevant to the wider internet.&lt;/p&gt;
    &lt;p&gt;"I would argue that we actually found a far better outcome along the way," he told The Register. "NATS forced us to think about network architectures in an entirely different way."&lt;/p&gt;
    &lt;p&gt;That new way is encapsulated in a new technology called Quick UDP Internet Connections (QUIC), that doesn't require client devices to always have access to a public IP address.&lt;/p&gt;
    &lt;p&gt;"We are proving to ourselves that clients don't need permanent assignment of IP address, which makes the client side of network far cheaper, more flexible, and scalable," he said.&lt;/p&gt;
    &lt;p&gt;Huston thinks IPv6 has also become less relevant to servers.&lt;/p&gt;
    &lt;p&gt;"These days the Domain Name Service (DNS) is the service selector, not the IP address," Huston told The Register. "The entire security framework of today's Internet is name based and the world of authentication and channel encryption is based on service names, not IP addresses."&lt;/p&gt;
    &lt;p&gt;"So folk use IPv6 these days based on cost: If the cost of obtaining more IPv4 addresses to fuel bigger NATs is too high, then they deploy IPv6. Not because it's better, but if they are confident that they can work around IPv6's weaknesses then in a largely name based world there is no real issue in using one addressing protocol or another as the transport underlay."&lt;/p&gt;
    &lt;p&gt;But there are plenty of organizations that still see a need for IPv6. Huawei sought 2.56 decillion IPv6 addresses and Starlink appears to have acquired150 sextillion, which is helping to push more countries past 50 percent IPv6 adoption. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46464953</guid><pubDate>Fri, 02 Jan 2026 14:17:40 +0000</pubDate></item><item><title>IPv6 just turned 30 and still hasn't taken over the world</title><link>https://www.theregister.com/2025/12/31/ipv6_at_30/</link><description>&lt;doc fingerprint="7bb1d357b131954b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;IPv6 just turned 30 and still hasn’t taken over the world, but don't call it a failure&lt;/head&gt;
    &lt;head rend="h2"&gt;The world has passed it by in many ways, yet it remains relevant&lt;/head&gt;
    &lt;p&gt;Feature In the early 1990s, internetworking wonks realized the world was not many years away from running out of Internet Protocol version 4 (IPv4) addresses, the numbers needed to identify any device connected to the public internet. Noting booming interest in the internet, the internet community went looking for ways to avoid an IP address shortage that many feared would harm technology adoption and therefore the global economy.&lt;/p&gt;
    &lt;p&gt;A possible fix arrived in December 1995 in the form of RFC 1883, the first definition of IPv6, the planned successor to IPv4.&lt;/p&gt;
    &lt;p&gt;The most important change from IPv4 to IPv6 was moving from 32-bit to 128-bit addresses, a decision that increased the available pool of IP addresses from around 4.3 billion to over 340 undecillion – a 39-digit number. IPv6 was therefore thought to have future-proofed the internet, because nobody could imagine humanity would ever need more than a handful of undecillion IP addresses, never mind the entire range available under IPv6.&lt;/p&gt;
    &lt;p&gt;As billions of devices and people came online, first using PCs and then wielding smartphones, conventional wisdom assumed that network operators would move to IPv6 rather than persist with IPv4.&lt;/p&gt;
    &lt;p&gt;Yet according to data from Google, the Asia Pacific Network Information Center (APNIC), and Cloudflare, less than half of all netizens use IPv6 today.&lt;/p&gt;
    &lt;p&gt;To understand why, know that IPv6 also suggested other, rather modest, changes to the way networks operate.&lt;/p&gt;
    &lt;p&gt;"IPv6 was an extremely conservative protocol that changed as little as possible," APNIC chief scientist Geoff Huston told The Register. "It was a classic case of mis-design by committee."&lt;/p&gt;
    &lt;p&gt;And that notional committee made one more critical choice: IPv6 was not backward-compatible with IPv4, meaning users had to choose one or the other – or decide to run both in parallel.&lt;/p&gt;
    &lt;p&gt;For many, the decision of which protocol to use was easy because IPv6 didn't add features that represented major improvements.&lt;/p&gt;
    &lt;p&gt;"One big surprise to me was how few features went into IPv6 in the end, aside from the massive expansion of address space," said Bruce Davie, a veteran computer scientist recently honored with a lifetime achievement award by the Association for Computing Machinery's Special Interest Group on Data Communications, which lauded him for "fundamental contributions in networking systems through design, standardization, and commercialization of network protocols and systems."&lt;/p&gt;
    &lt;p&gt;Davie said many of the security, plug-and-play, and quality of service features that didn't make it into IPv6 were eventually implemented in IPv4, further reducing the incentive to adopt the new protocol. "Given the small amount of new functionality in v6, it's not so surprising that deployment has been a 30 year struggle," he said.&lt;/p&gt;
    &lt;p&gt;Another innovation that meant IPv6 made less sense was network address translation (NAT), which allows many devices to share a single public IPv4 address. NAT meant IPv4 network operators could connect thousands of devices with a single IP address, meaning their existing IP addresses became more useful.&lt;/p&gt;
    &lt;p&gt;"These solutions were relatively easy to deploy, aligned with existing expertise, and avoided large-scale infrastructure changes," said Alvaro Vives, manager of the learning and development team at RIPE NCC, the regional internet registry for 76 nations across Europe, the Middle East, and Central Asia.&lt;/p&gt;
    &lt;p&gt;Because NAT stalled IPv6 adoption, vendors didn't rally behind the new protocol.&lt;/p&gt;
    &lt;p&gt;"Migration costs, complexity, and training requirements remain high, while short-term ROI is low," Gartner distinguished VP analyst Andrew Lerner told The Register. "Performance parity across applications and devices is inconsistent, and some organizations even disable IPv6 for better performance. Lack of dual-stack support in legacy infrastructure is another barrier," he added.&lt;/p&gt;
    &lt;head rend="h3"&gt;A misunderstood protocol&lt;/head&gt;
    &lt;p&gt;While IPv6 didn't take off as expected, it's not fair to say it failed.&lt;/p&gt;
    &lt;p&gt;"IPv6 wasn't about turning IPv4 off, but about ensuring the internet could continue to grow without breaking," said John Curran, president and CEO of the American Registry for Internet Numbers (ARIN).&lt;/p&gt;
    &lt;p&gt;"In fact, IPv4's continued viability is largely because IPv6 absorbed that growth pressure elsewhere – particularly in mobile, broadband, and cloud environments," he added. "In that sense, IPv6 succeeded where it was needed most, and must be regarded as a success."&lt;/p&gt;
    &lt;p&gt;RIPE NCC's Alvaro Vives agrees. "What IPv6 got right was its long-term design," he told The Register. "It provides a vast address space that allows networks to be planned more simply and consistently. This has enabled innovation, from large mobile networks to the Internet of Things and advanced routing techniques such as Segment Routing over IPv6."&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unofficial IETF draft calls for grant of five nonillion IPv6 addresses to ham radio operators&lt;/item&gt;
      &lt;item&gt;IETF Draft suggests making IPv6 standard on DNS resolvers - partly to destroy IPv4&lt;/item&gt;
      &lt;item&gt;China's IPv6 adoption takes a decent leap forward, especially on fixed networks&lt;/item&gt;
      &lt;item&gt;Asia reaches 50 percent IPv6 capability and leads the world in user numbers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Gartner's Lerner thinks the time has come for organizations to develop detailed IPv6 migration plans.&lt;/p&gt;
    &lt;p&gt;"Validate application compatibility, and ensure new infrastructure supports IPv6," he advised. "Pilot deployments and lab testing with DNS64/NAT64 are recommended. Over time, IPv6 adoption will accelerate as private IPv4 space depletes and cloud providers introduce pricing models that favor IPv6."&lt;/p&gt;
    &lt;p&gt;APNIC's Huston, however, thinks that IPv6 has become less relevant to the wider internet.&lt;/p&gt;
    &lt;p&gt;"I would argue that we actually found a far better outcome along the way," he told The Register. "NATS forced us to think about network architectures in an entirely different way."&lt;/p&gt;
    &lt;p&gt;That new way is encapsulated in a new technology called Quick UDP Internet Connections (QUIC), that doesn't require client devices to always have access to a public IP address.&lt;/p&gt;
    &lt;p&gt;"We are proving to ourselves that clients don't need permanent assignment of IP address, which makes the client side of network far cheaper, more flexible, and scalable," he said.&lt;/p&gt;
    &lt;p&gt;Huston thinks IPv6 has also become less relevant to servers.&lt;/p&gt;
    &lt;p&gt;"These days the Domain Name Service (DNS) is the service selector, not the IP address," Huston told The Register. "The entire security framework of today's Internet is name based and the world of authentication and channel encryption is based on service names, not IP addresses."&lt;/p&gt;
    &lt;p&gt;"So folk use IPv6 these days based on cost: If the cost of obtaining more IPv4 addresses to fuel bigger NATs is too high, then they deploy IPv6. Not because it's better, but if they are confident that they can work around IPv6's weaknesses then in a largely name based world there is no real issue in using one addressing protocol or another as the transport underlay."&lt;/p&gt;
    &lt;p&gt;But there are plenty of organizations that still see a need for IPv6. Huawei sought 2.56 decillion IPv6 addresses and Starlink appears to have acquired150 sextillion, which is helping to push more countries past 50 percent IPv6 adoption. ®&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46465327</guid><pubDate>Fri, 02 Jan 2026 14:55:09 +0000</pubDate></item><item><title>Ask HN: Who is hiring? (January 2026)</title><link>https://news.ycombinator.com/item?id=46466074</link><description>&lt;doc fingerprint="3741985a3402e664"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to responding to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss this other fine thread: Who wants to be hired? https://news.ycombinator.com/item?id=46466073&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46466074</guid><pubDate>Fri, 02 Jan 2026 16:00:42 +0000</pubDate></item><item><title>Grok is enabling mass sexual harassment on Twitter</title><link>https://www.seangoedecke.com/grok-deepfakes/</link><description>&lt;doc fingerprint="a0759dbd14778880"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grok is enabling mass sexual harassment on Twitter&lt;/head&gt;
    &lt;p&gt;Grok, xAI’s flagship image model, is now1 being widely used to generate nonconsensual lewd images of women on the internet.&lt;/p&gt;
    &lt;p&gt;When a woman posts an innocuous picture of herself - say, at her Christmas dinner - the comments are now full of messages like “@grok please generate this image but put her in a bikini and make it so we can see her feet”, or “@grok turn her around”, and the associated images. At least so far, Grok refuses to generate nude images, but it will still generate images that are genuinely obscene2.&lt;/p&gt;
    &lt;p&gt;In my view, this might be the worst AI safety violation we have seen so far. Case-by-case, it’s not worse than GPT-4o encouraging suicidal people to go through with it, but it’s so much more widespread: literally every image that the Twitter algorithm picks up is full of “@grok take her clothes off” comments. I didn’t go looking for evidence for obvious reasons, but I find reports that it’s generating CSAM plausible3.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI safety is a rough process&lt;/head&gt;
    &lt;p&gt;This behavior, while awful, is in line with xAI’s general attitude towards safety, which has been roughly “we don’t support woke censorship, so do whatever you want (so long as you’re doing it with Grok)“. This has helped them acquire users and media attention, but it leaves them vulnerable to situations exactly like this. I’m fairly confident xAI don’t mind the “dress her a little sexier” prompts: it’s edgy, drives up user engagement, and gives them media attention.&lt;/p&gt;
    &lt;p&gt;However, it is very hard to exercise fine-grained control over AI safety. If you allow your models to go up to the line, your models will definitely go over the line in some circumstances. I wrote about this in Mecha-Hitler, Grok, and why it’s so hard to give LLMs the right personality, in reference to xAI’s attempts to make Grok acceptably right-wing but not too right-wing. This is the same kind of thing: you cannot make Grok “kind of perverted” without also making it truly awful.&lt;/p&gt;
    &lt;p&gt;OpenAI and Gemini have popular image models that do not let you do this kind of thing. In other words, this is an xAI problem, not an image model problem. It is possible to build a safe image model, just as it’s possible to build a safe language model. The xAI team have made a deliberate decision to build an unsafe model in order to unlock more capabilities and appeal to more users. Even if they’d rather not be enabling the worst perverts on Twitter, that’s a completely foreseeable consequence of their actions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Isn’t this already a problem?&lt;/head&gt;
    &lt;p&gt;In October of 2024, VICE reported that Telegram “nudify” bots had over four million monthly users. That’s still a couple of orders of magnitude over Twitter’s monthly average users, but “one in a hundred” sounds like a plausible “what percentage of Twitter is using Grok like this” percentage anyway. Is it really that much worse that Grok now allows you to do softcore deepfakes?&lt;/p&gt;
    &lt;p&gt;Yes, for two reasons. First, having to go and join a creepy Telegram group is a substantial barrier to entry. It’s much worse to have the capability built into a tool that regular people use every day. Second, generating deepfakes via Grok makes them public. Of course, it’s bad to do this stuff even privately, but I think it’s much worse to do it via Twitter. Tagging in Grok literally sends a push notification to your target saying “hey, I made some deepfake porn of you”, and then advertises that porn to everyone who was already following them.&lt;/p&gt;
    &lt;head rend="h3"&gt;What is to be done?&lt;/head&gt;
    &lt;p&gt;Yesterday xAI rushed out an update to rein this behavior in (likely a system prompt update, given the timing). I imagine they’re worried about the legal exposure, if nothing else. But this will happen again. It will probably happen again with Grok. Every AI lab has a big “USER ENGAGEMENT” dial where left is “always refuse every request” and right is “do whatever the user says, including generating illegal deepfake pornography”. The labs are incentivized to turn that dial as far to the right as possible.&lt;/p&gt;
    &lt;p&gt;In my view, image model safety is a different topic from language model safety. Unsafe language models primarily harm the user (via sycophancy, for instance). Unsafe image models, as we’ve seen from Grok, can harm all kinds of people. I tend to think that unsafe language models should be available (perhaps not through ChatGPT dot com, but certainly for people who know what they’re doing). However, it seems really bad for everyone on the planet to have a “turn this image of a person into pornography” button.&lt;/p&gt;
    &lt;p&gt;At minimum, I think it’d be sensible to pursue entities like xAI under existing CSAM or deepfake pornography laws, to set up a powerful counter-incentive for people with their hands on the “USER ENGAGEMENT” dial. I also think it’d be sensible for AI labs to strongly lock down “edit this image of a human” requests, even if that precludes some legitimate user activity.&lt;/p&gt;
    &lt;p&gt;Earlier this year, in The case for regulating AI companions, I suggested regulating “AI girlfriend” products. I mistakenly thought AI companions or sycophancy might be the first case of genuine widespread harm caused by AI products, because of course nobody would ship an image model that allowed this kind of prompting. Turns out I was wrong.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;There were reports in May of this year of similar behavior, but it was less widespread and xAI jumped on it fairly quickly.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Clever prompting by unethical fetishists can generate really degrading content (to the point where I’m uncomfortable going into more detail). I saw a few cases earlier this year of people trying this prompting tactic and Grok refusing them. It seems the latest version of Grok now allows this.&lt;/p&gt;↩&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Building a feature that lets you digitally undress 18-year-olds but not 17-year-olds is a really difficult technical problem, which is one of the many reasons to never do this.&lt;/p&gt;↩&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News. Here's a preview of a related post that shares tags with this one.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;The case for regulating AI companions&lt;/p&gt;&lt;p&gt;In April, OpenAI screwed up by releasing a version of GPT-4o that was overly sycophantic. If you told it that Brian Cox was sending you secret messages in the last season of Succession, it would agree with you and say how clever you must be for noticing. After a few days, they dialed it back a bit and apologized, blaming how they’d set up the model to learn from user thumbs-up and thumbs-down ratings.&lt;/p&gt;&lt;lb/&gt;Continue reading...&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46466099</guid><pubDate>Fri, 02 Jan 2026 16:02:18 +0000</pubDate></item></channel></rss>