<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 02 Oct 2025 22:08:51 +0000</lastBuildDate><item><title>How Israeli actions caused famine in Gaza, visualized</title><link>https://www.cnn.com/2025/10/02/middleeast/gaza-famine-causes-vis-intl</link><description>&lt;doc fingerprint="edaed098b2dab5fa"&gt;
  &lt;main&gt;
    &lt;p&gt;Israel’s nearly two-year war pushed parts of Gaza into “man-made” famine, according to a report published in August by a United Nations-backed initiative, deepening the Palestinians’ struggle for survival under relentless bombing, mass displacement and the spread of disease.&lt;/p&gt;
    &lt;p&gt;The report by the Integrated Food Security Phase Classification (IPC), a UN-backed expert panel that assesses global food insecurity and malnutrition, helped to fuel growing international outcry over Israel’s campaign in Gaza following the Hamas-led October 7, 2023, attacks – and was cited by some of countries that recently made moves towards formally recognizing a Palestinian state. The IPC forecast that by the end of September nearly a third of Gaza’s total population would face famine conditions, although it has not yet provided an update on that forecast.&lt;/p&gt;
    &lt;p&gt;In Gaza governorate alone – the largest by population of five in the Gaza Strip – more than half a million people were condemned to a cycle of “starvation, destitution and death,” the IPC added. The Israeli assault on Gaza City, which Israeli Prime Minister Benjamin Netanyahu says is targeting one of Hamas’ “remaining strongholds” has choked relief operations for starving Palestinians, according to rights workers.&lt;/p&gt;
    &lt;p&gt;Michael Fakhri, the UN’s special rapporteur on the right to food, accused Israel of using hunger “as a weapon against Palestinians,” in violation of international law.&lt;/p&gt;
    &lt;p&gt;“Israel is using food and aid as a weapon to humiliate, weaken, displace and kill Palestinians in Gaza,” Fakhri told CNN on August 28.&lt;/p&gt;
    &lt;p&gt;Israel rejected the IPC’s findings, with the Israeli agency that oversees the entry of aid into Gaza claiming the report was “false” and based on “partial, biased” data “originating from Hamas.” Netanyahu slammed the UN-backed report, in a statement from his office, adding that “Israel does not have a policy of starvation.”&lt;/p&gt;
    &lt;p&gt;Israel has since insisted that it has stepped up the entry of aid into Gaza. But aid agencies say that Israel’s intensification of the war, particularly around Gaza City, has compounded the misery faced by Palestinians. Here is a look, in five charts, at how the situation described by the IPC materialized.&lt;/p&gt;
    &lt;head rend="h2"&gt;Famine projected to spread to central, southern Gaza&lt;/head&gt;
    &lt;p&gt;The IPC projected that famine would spread to Deir Al-Balah, central Gaza and further south, in Khan Younis by the end of September, affecting nearly 641,000 people.&lt;/p&gt;
    &lt;p&gt;Up to June 2026, at least 132,000 children under the age of five are expected to suffer from acute malnutrition, including more than 41,000 severe cases of children at heightened risk of death, the IPC added.&lt;/p&gt;
    &lt;p&gt;Under the IPC – a five-phase indicator used to measure the severity of food insecurity – a famine can only be declared if three thresholds are met: at least 20% of households face extreme food shortages, the proportion of children assessed as acutely malnourished reaches a certain threshold, and at least two in every 10,000 people die each day from starvation, or from malnutrition and disease.&lt;/p&gt;
    &lt;p&gt;Israel accused the IPC of lowering the second threshold of acutely malnourished children for a famine declaration, which the IPC has denied.&lt;/p&gt;
    &lt;p&gt;Researchers use three methods for assessing child malnutrition – either a child’s height and weight, their BMI, or a child’s mid-upper arm circumference, known as MUAC. The IPC used the latter, a metric employed since 2019, to determine that at least 15% of children aged six to 59 months have a mid-upper arm circumference of less than 125mm or edema, the agency told CNN. The thresholds for famine classification are “standard and were not modified for Gaza,” the IPC told CNN, adding that the MUAC metric “is the measurement most frequently available and has strong correlation with mortality outcomes,” and was also used in famine classifications in Sudan and South Sudan this decade.&lt;/p&gt;
    &lt;p&gt;Human rights advocates say Israel’s destruction of health infrastructure and intensified hostilities have hampered efforts to document the full scope of famine in Gaza.&lt;/p&gt;
    &lt;p&gt;After more than 700 days of war, 455 Palestinians have died of malnutrition or starvation, including 151 children, the health ministry in Gaza reported on October 1. One hundred and seventy-seven of the total number have died of malnutrition or starvation since the IPC confirmed famine on August 15, it said.&lt;/p&gt;
    &lt;head rend="h2"&gt;How much UN aid is getting into Gaza?&lt;/head&gt;
    &lt;p&gt;Israel’s vast web of bureaucratic impediments, including delayed approvals, arduous border checks and the arbitrary rejection of items, throttles the amount of aid that makes it to the other side of the border and sends food costs soaring, the UN and aid agencies say.&lt;/p&gt;
    &lt;p&gt;After visiting the region in late August, US Senators Chris Van Hollen and Jeff Merkley, both Democrats, warned that Netanyahu’s government was “implementing a plan to ethnically cleanse Gaza of Palestinians” and accused Israel of using food “as a weapon of war.” Israel has denied the allegations.&lt;/p&gt;
    &lt;p&gt;“The findings from our trip lead to the inescapable conclusion that the Netanyahu government’s war in Gaza has gone far beyond the targeting of Hamas to imposing collective punishment on the Palestinians there, with the goal of making life for them unsustainable,” said the report, published on September 11. “That is why it restricts the delivery of humanitarian assistance.”&lt;/p&gt;
    &lt;p&gt;Israeli authorities have said trucks “remain uncollected” at the border with Gaza – accusing the UN of failing to coordinate the entry of vehicles into the strip.&lt;/p&gt;
    &lt;p&gt;But Sam Rose, the acting director of affairs for the UN agency for Palestinian refugees (UNRWA) in Gaza, says Israel – which has near-total jurisdiction over what goods enter and exit Gaza – has controlled “to the calorie” the volume, type and overall flow of food into the enclave. “The system is designed not to function smoothly,” he said.&lt;/p&gt;
    &lt;p&gt;Israeli authorities “know and analyze each truck that goes into Gaza, the weight and the calories,” a senior official with COGAT, the Israeli agency that controls the entry of aid into the enclave, said in September. According to a COGAT statement published in response to the IPC famine declaration, “analysis of contents of food aid trucks that entered the Gaza Strip reveal that 4,400 calories per person per day entered Gaza since the beginning of August.”&lt;/p&gt;
    &lt;p&gt;However, as of May, Palestinians were consuming just 1,400 calories per day – or “67 per cent of what a human body needs to survive,” at 2,300 calories, the UN reported in June.&lt;/p&gt;
    &lt;p&gt;Last October, Israel’s government banned UNRWA from operating in areas under its control, a prohibition that went into effect in January, having accused the agency of failing to stop Hamas’ alleged theft of aid. An internal US government review found no evidence of widespread theft by Hamas of US-funded humanitarian aid in Gaza.&lt;/p&gt;
    &lt;p&gt;When the trickle of relief does enter the strip, aid workers face intensified hostilities, damaged roads and limited fuel supplies – impeding internal distribution efforts, minimizing viable routes and blocking access to displaced Palestinians, said Rose.&lt;/p&gt;
    &lt;head rend="h2"&gt;What other ways are there to receive aid?&lt;/head&gt;
    &lt;p&gt;Israel says UN aid makes up only part of the relief that gets into Gaza. A senior COGAT official told a briefing in early September that 27% of the trucks entering Gaza are UN vehicles, claiming it was “a lie” that the UN had brought in 600 aid trucks a day before the war.&lt;/p&gt;
    &lt;p&gt;“There is no famine in Gaza. Period,” the official said, adding that “Israel and the IDF are trying to strengthen the humanitarian situation in Gaza with partners.”&lt;/p&gt;
    &lt;p&gt;In May, the US and Israeli-backed Gaza Humanitarian Foundation (GHF) established a program that now plans to operate up to five distribution sites in the enclave, all but one in southern Gaza – which rely on private military contractors and largely replaced 400 UN-led hubs.&lt;/p&gt;
    &lt;p&gt;Relief and health workers say these other methods of delivering food in Gaza, including the GHF sites and aid pallet drops from planes, are dehumanizing and inaccessible for many Palestinians, and expose them to injury or death.&lt;/p&gt;
    &lt;p&gt;At least 1,172 people were killed “near militarized supply sites” between May 27 and September 9, the UN said on September 10, with another 1,084 deaths along convoy supply routes. In August, UN experts called for the immediate closure of GHF-operated sites in Gaza and accused Israeli forces of opening “indiscriminate fire” on people seeking aid there. The advocates warned the hubs are “especially difficult” for women, children, people with disabilities and elderly Palestinians to access.&lt;/p&gt;
    &lt;p&gt;GHF has defended its work in Gaza and said earlier in September that it was the only organization in Gaza able to deliver food “at scale without interference.” The organization also said that it had “repeatedly sought collaboration with UN agencies and international NGOs to deliver aid side-by-side” but that the UN had “declined those offers.” The Israeli military has acknowledged firing warning shots toward crowds in some instances and denied responsibility for other casualties near aid hubs.&lt;/p&gt;
    &lt;p&gt;The US and Israel plan to set up 12 additional sites across the enclave, an Israeli official told CNN in August. However, there is no indication that the new sites have been established. In September, GHF said it had sought IDF permission to open sites in northern Gaza but that Israel had not granted the permission.&lt;/p&gt;
    &lt;p&gt;“With parents injured and their siblings starving, many teenagers and young adults are taking the risk,” Mohammed Khaleel, an American surgeon who was deployed to Gaza earlier this year, told CNN in August.&lt;/p&gt;
    &lt;p&gt;“We’ve even heard some people report that they will go and accept their fate. Dying from a gunshot may be preferable to dying from starvation,” he added.&lt;/p&gt;
    &lt;head rend="h2"&gt;Farmland is shrinking and becoming increasingly inaccessible&lt;/head&gt;
    &lt;p&gt;Israel’s two-year offensive in Gaza had left just 1.5% of cropland accessible and undamaged as of July 28, according to the UN – largely preventing Palestinians from cultivating produce.&lt;/p&gt;
    &lt;p&gt;That destruction, coupled with Israel’s fishing ban and intensified assault in the north, has further limited the sources of food available to hundreds of thousands of displaced Palestinians.&lt;/p&gt;
    &lt;p&gt;“It is not by chance that Israel has focused its starvation tactics in northern Gaza,” Fakhri, the UN special rapporteur, said. “They have announced their intent to push people from the north to the south of Gaza… Just as now, the focus of their starvation campaign on Gaza City correlates with their invasion plans.”&lt;/p&gt;
    &lt;p&gt;The military’s invasion of Gaza City will collapse an “already fragile” aid supply chain, warned Arif Husain, chief economist at the World Food Programme.&lt;/p&gt;
    &lt;p&gt;Relief agencies need a ceasefire, unimpeded humanitarian access, large-scale multi-sector aid, protection of civilians and infrastructure – and restoration of commercial and local food systems – to reverse famine in Gaza, said Husain.&lt;/p&gt;
    &lt;p&gt;“We are already at the brink. Another escalation – especially in Gaza City – could push the situation into unimaginable catastrophe,” he added. “It will not only result in more deaths but destroy any foundation for future recovery.”&lt;/p&gt;
    &lt;p&gt;CNN’s Ibrahim Dahman, Kareem Khadder and Eyad Kourdi contributed reporting.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45447699</guid><pubDate>Thu, 02 Oct 2025 09:23:50 +0000</pubDate></item><item><title>NL Judge: Meta must respect user's choice of recommendation system</title><link>https://www.bitsoffreedom.nl/2025/10/02/judge-in-the-bits-of-freedom-vs-meta-lawsuit-meta-must-respect-users-choice/</link><description>&lt;doc fingerprint="bc501a786403c13c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Judge in the Bits of Freedom vs. Meta lawsuit: Meta must respect users’ choice&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;02 oktober 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Today the judge issued a ruling in the summary proceedings brought by digital human rights organisation Bits of Freedom against Meta. The organisation demanded that Meta gives its users on apps such as Instagram and Facebook the option to select a feed that is not based on profiling.&lt;/p&gt;
    &lt;p&gt;Bits of Freedom sued Meta for a breach of the Digital Services Act (DSA). This European legislation is intended to give users more autonomy and control over the major online platforms. One of the core elements of the DSA is that users must have greater influence over the information they see.&lt;/p&gt;
    &lt;p&gt;For many people, and especially for young people, social media platforms are a major source of news and information. Therefore it is crucial that users themselves can decide which content appears on their feed. Without that freedom of choice, participation in the public debate is seriously hampered. That is problematic at any time, but especially so during election periods. In the Netherlands, national elections will be held at the end of this month.&lt;/p&gt;
    &lt;p&gt;The judge states that Meta is indeed acting in violation of the law. He says that “a non‑persistent choice option for a recommendation system runs counter to the purpose of the DSA, which is to give users genuine autonomy, freedom of choice, and control over how information is presented to them.” The judge also concludes that the way Meta has designed its platforms constitutes “a significant disruption of the autonomy of Facebook and Instagram users.” The judge orders Meta to adjust its apps so that the user’s choice is preserved, even when the user navigates to another section or restarts the app.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“We are pleased that the judge now makes clear that Meta must respect the user’s choice,” says Maartje Knaap, spokesperson for Bits of Freedom. “It is absolutely unacceptable that a handful of American tech billionaires determine how we see the world. That concentration of power poses a risk to our democracy. At the same time, it is regrettable that we need to go to court to ensure Meta complies with the law.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Meta has an interest in steering users toward a feed where it can show as many interest‑ and behavior‑based ads as possible. That is the core of Meta’s revenue model. Subtle design techniques push users toward that feed, while the non‑profiled feed is hidden behind a logo, making it hard to find. Users who do choose the alternative timeline also lose direct access to features such as Direct Messages. Moreover, when you open the app, it always starts with Meta’s feed, even if the user selected a different one before. Because of the judge’s ruling, Meta must change its behavior.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“This ruling shows that Meta is not untouchable,” continues Maartje Knaap. “But we are also realistic, this is just a drop in the ocean. There’s still a long way to go. We hope the decision will inspire individuals, civil society organisations, regulators and lawmakers worldwide around the world who are working to rein in Meta’s power. Together we can stand up to a company that has become overwhelmingly powerful.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You can find the ruling here (in Dutch).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45448326</guid><pubDate>Thu, 02 Oct 2025 11:32:19 +0000</pubDate></item><item><title>Red Hat confirms security incident after hackers breach GitLab instance</title><link>https://www.bleepingcomputer.com/news/security/red-hat-confirms-security-incident-after-hackers-claim-github-breach/</link><description>&lt;doc fingerprint="f046ce5b18c3474a"&gt;
  &lt;main&gt;
    &lt;p&gt;Correction: After publishing, Red Hat confirmed that it was a breach of one of its GitLab instances, and not GitHub. Title and story updated.&lt;/p&gt;
    &lt;p&gt;An extortion group calling itself the Crimson Collective claims to have stolen nearly 570GB of compressed data across 28,000 internal development respositories, with the company confirming it was a breach of one of its GitLab instances.&lt;/p&gt;
    &lt;p&gt;This data allegedly includes approximately 800 Customer Engagement Reports (CERs), which can contain sensitive information about a customer's network and platforms.&lt;/p&gt;
    &lt;p&gt;A CER is a consulting document prepared for clients that often contains infrastructure details, configuration data, authentication tokens, and other information that could be abused to breach customer networks.&lt;/p&gt;
    &lt;p&gt;Red Hat confirmed that it suffered a security incident related to its consulting business, but would not verify any of the attacker's claims regarding the stolen GitLab repositories and customer CERs.&lt;/p&gt;
    &lt;p&gt;"Red Hat is aware of reports regarding a security incident related to our consulting business and we have initiated necessary remediation steps," Red Hat told BleepingComputer.&lt;/p&gt;
    &lt;p&gt;"The security and integrity of our systems and the data entrusted to us are our highest priority. At this time, we have no reason to believe the security issue impacts any of our other Red Hat services or products and are highly confident in the integrity of our software supply chain."&lt;/p&gt;
    &lt;p&gt;After publishing our story, Red Hat confirmed that the security incident was a breach of its GitLab instance used solely for Red Hat Consulting on consulting engagements, and not GitHub.&lt;/p&gt;
    &lt;p&gt;While Red Hat did not respond to any further questions about the breach, the hackers told BleepingComputer that the intrusion occurred approximately two weeks ago.&lt;/p&gt;
    &lt;p&gt;They allegedly found authentication tokens, full database URIs, and other private information in Red Hat code and CERs, which they claimed to use to gain access to downstream customer infrastructure.&lt;/p&gt;
    &lt;p&gt;The hacking group also published a complete directory listing of the allegedly stolen GitLab repositories and a list of CERs from 2020 through 2025 on Telegram.&lt;/p&gt;
    &lt;p&gt;The directory listing of CERs include a wide range of sectors and well known organizations such as Bank of America, T-Mobile, AT&amp;amp;T, Fidelity, Kaiser, Mayo Clinic, Walmart, Costco, the U.S. Navy’s Naval Surface Warfare Center, Federal Aviation Administration, the House of Representatives, and many others.&lt;/p&gt;
    &lt;p&gt;If you have any information regarding this incident or any other undisclosed attacks, you can contact us confidentially via Signal at 646-961-3731 or at tips@bleepingcomputer.com.&lt;/p&gt;
    &lt;p&gt;The hackers stated that they attempted to contact Red Hat with an extortion demand but received no response other than a templated reply instructing them to submit a vulnerability report to their security team.&lt;/p&gt;
    &lt;p&gt;According to them, the created ticket was repeatedly assigned to additional people, including Red Hat's legal and security staff members.&lt;/p&gt;
    &lt;p&gt;BleepingComputer sent Red Hat additional questions, and we will update this story if we receive more information.&lt;/p&gt;
    &lt;p&gt;The same group also claimed responsibility for briefly defacing Nintendo’s topic page last week to include contact information and links to their Telegram channel&lt;/p&gt;
    &lt;p&gt;Update 10/2/25: Story updated with correction from Red Hat that it was a GitLab instance that was breached and not a GitHub account.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Security Validation Event of the Year: The Picus BAS Summit&lt;/head&gt;
    &lt;p&gt;Join the Breach and Attack Simulation Summit and experience the future of security validation. Hear from top experts and see how AI-powered BAS is transforming breach and attack simulation.&lt;/p&gt;
    &lt;p&gt;Don't miss the event that will shape the future of your security strategy&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45448772</guid><pubDate>Thu, 02 Oct 2025 12:28:27 +0000</pubDate></item><item><title>Potential issues in curl found using AI assisted tools</title><link>https://mastodon.social/@bagder/115241241075258997</link><description>&lt;doc fingerprint="f8eb8f2f2d953eed"&gt;
  &lt;main&gt;
    &lt;p&gt;To use the Mastodon web application, please enable JavaScript. Alternatively, try one of the native apps for Mastodon for your platform.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45449348</guid><pubDate>Thu, 02 Oct 2025 13:29:55 +0000</pubDate></item><item><title>N8n added native persistent storage with DataTables</title><link>https://community.n8n.io/t/data-tables-are-here/192256</link><description>&lt;doc fingerprint="bfbc187f418403ca"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Hey everyone &lt;/head&gt;
        &lt;head rend="h2"&gt;We’re super excited to share that starting with v1.113 we’re rolling out data tables (beta) to all plans. &lt;/head&gt;
        &lt;p&gt;Since the very beginning of n8n we’ve heard many of you mention the need for a proper table inside n8n to store data between workflow executions without needing to switch platforms or setting up credentials and now it’s finally here.&lt;/p&gt;
        &lt;p&gt;With data tables you can:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Save specific data from your workflow runs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Keep data around between multiple executions&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Avoid duplicate runs by tracking execution status&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Store reusable prompts for different workflows&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Collect evaluation data for your AI workflows&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Do lookups, merges, enhancements…&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;…and honestly, probably 100 other creative things we haven’t even thought of yet &lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
        &lt;p&gt; To make sure your instance stays performant, we’ve set a 50MB limit for everyone. If you’re self-hosting (and know what you’re doing), you can change that via the ENV variable &lt;code&gt;N8N_DATA_TABLES_MAX_SIZE_BYTES&lt;/code&gt;&lt;/p&gt;
        &lt;p&gt; Upgrade to 1.113, give data tables a spin, and let us know what you think! What’s missing? What would make it even more useful for you? We’re really curious to hear your ideas and thoughts! &lt;/p&gt;
        &lt;p&gt; Read more about the data tables in the docs here.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 38 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;liam
2&lt;/div&gt;
      &lt;p&gt; 7 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;dszp
3&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is absolutely awesome to see, I can’t wait to use these! It’s probably been my number one frustration that saving even a small amount of data between executions for all sorts of purposes requires either integrating PostgreSQL and dealing with schemas, using a third party database or API like Supabase (as handy as they are), or using variables that are powerful but are somewhat clumsy to instantiate and track since they only work in Code nodes and only save data for production executions, making testing hard. Hoping data-tables makes a ton of these things easier! Probably won’t run the new version until it’s in final release rather than pre-release, but this is awesome to see!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 4 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;bartv
5&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;This is really great - when I migrated from “the other platform” almost 4 years ago, I really felt the pain of not having a simple in-app data storage. I played around with Data tables this weekend and it’s just SUCH a good and fast experience! Kudos to our Product and Engineering teams &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Hey all,&lt;/p&gt;
        &lt;p&gt;IMPORTANT NOTE: There is an issue with very large SQLite databases that is causing instances to slow down. Out of an abundance of caution, we are unfortunately removing version 1.113.0 until we fix this issue. We hope to have this released again with a fix within the next couple of days.&lt;/p&gt;
        &lt;p&gt;Very sorry about this!&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 10 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;This is great!&lt;/p&gt;
        &lt;p&gt;It´d be cool for self hosting to be able to add a second DB, where n8n pulls the data from. So one could have performance without having to set up each time a postgres connection.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;bartv
10&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;Data tables is back on!&lt;/p&gt;
        &lt;p&gt;A patch was released earlier today. It has now been tested and we have high confidence. Please update to 1.113.1 (which is still in beta) to try this feature.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 4 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;TH1
11&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;is that Data Tables only available for the Cloud version? local host will not have Data Tables?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;liam
12&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;It’s on all plans (cloud and self hosted) starting on version 1.113.1 &lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 2 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;Sujit
13&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;I am unable to see the data tables in my local self hosted n8n. I’ve also updated the docker image to pick the latest one. What am I missing?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Does this mean we can share data between multiple workflows now? This would make splitting up complex workflows across multiple workflows so much easier.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;I believe this is still only available in the beta version?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;jabbson
16&lt;/div&gt;
      &lt;div&gt;
        &lt;p&gt;The fact that the “latest” is not “1.113.1”. The latest is “the latest stable”, where 1.113.1 is not that.&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 3 Likes &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Very happy to see this feature. I’ve been testing it out, and was commenting feedback on Reddit but someone in the Discord server said the forums is the best place to post this instead. Here’s my list(so far)&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;When going to the Data Tables tab, “Create Table” is not default on the upper right button, it’s defaulted to “Create Workflow” instead.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Cannot change the data type after a column is created.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Cannot set any of the column’s as primary or unique such as the ID column (To prevent duplicates)&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;For some odd reason, setting a column data type to “number” then pushing data from JSON array into the table, physically opening the table and looking at the rows, the numbers in the “number” data type column are not all together. For example “29683389” shows in the table as “29 683 389”. This isn’t a one off either, ALL rows exhibit the same behavior and ALL columns set as “numbers” too.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Table page can only show 50 rows per page. Which I understand is probably for performance reasons. However, there really needs to be a “search” function for the table to search for data.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Are there any limitations for creating tables?&lt;lb/&gt; or we can create multiples/unlimited (in 50Mb limit)?&lt;/p&gt;
      &lt;/div&gt;
      &lt;p&gt; 1 Like &lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;Love this list @compaholic, thanks so much for sharing it.&lt;lb/&gt; 1, 2 and 5 are all planned. For (4), I think that is just a highlighting to make it easier to read that it’s actually 29M. So the number should still be correct.&lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;p&gt;You can created unlimited ones within the storage limit &lt;/p&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450044</guid><pubDate>Thu, 02 Oct 2025 14:26:23 +0000</pubDate></item><item><title>Two Amazon delivery drones crash into crane in commercial area of Tolleson, AZ</title><link>https://www.abc15.com/news/region-west-valley/tolleson/two-amazon-delivery-drones-crash-into-crane-in-commercial-area-of-tolleson</link><description>&lt;doc fingerprint="554baf5711d3a7d2"&gt;
  &lt;main&gt;
    &lt;p&gt;TOLLESON, AZ — The Tolleson Police Department is investigating after two Amazon delivery drones crashed on Wednesday morning.&lt;/p&gt;
    &lt;p&gt;Officials say they are working an active investigation after the two drones crashed into a crane that was in a commercial area near 96th Avenue and Roosevelt Street.&lt;/p&gt;
    &lt;p&gt;It's unclear if anyone was injured during the incident.&lt;/p&gt;
    &lt;p&gt;ABC15 reached out to Amazon which provided the following statement: “We’re aware of an incident involving two Prime Air drones in Tolleson, Arizona. We’re currently working with the relevant authorities to investigate.”&lt;/p&gt;
    &lt;p&gt;This is a developing story and will be updated once new information becomes available.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45450449</guid><pubDate>Thu, 02 Oct 2025 14:52:49 +0000</pubDate></item><item><title>Signal Protocol and Post-Quantum Ratchets</title><link>https://signal.org/blog/spqr/</link><description>&lt;doc fingerprint="ac708592c921c484"&gt;
  &lt;main&gt;
    &lt;p&gt;We are excited to announce a significant advancement in the security of the Signal Protocol: the introduction of the Sparse Post Quantum Ratchet (SPQR). This new ratchet enhances the Signal Protocol’s resilience against future quantum computing threats while maintaining our existing security guarantees of forward secrecy and post-compromise security.&lt;/p&gt;
    &lt;p&gt;The Signal Protocol is a set of cryptographic specifications that provides end-to-end encryption for private communications exchanged daily by billions of people around the world. After its publication in 2013, the open source Signal Protocol was adopted not only by the Signal application but also by other major messaging products. Technical information on the Signal Protocol can be found in the specifications section of our docs site.&lt;/p&gt;
    &lt;p&gt;In a previous blog post, we announced the first step towards advancing quantum resistance for the Signal Protocol: an upgrade called PQXDH that incorporates quantum-resistent cryptographic secrets when chat sessions are established in order to protect against harvest-now-decrypt-later attacks that could allow current chat sessions to become compromised if a sufficiently powerful quantum computer is developed in the future. However, the Signal Protocol isn’t just about protecting cryptographic material and keys at the beginning of a new chat or phone call; it’s also designed to minimize damage and heal from compromise as that conversation continues.&lt;/p&gt;
    &lt;p&gt;We refer to these security goals as Forward Secrecy (FS) and Post-Compromise Security (PCS). FS and PCS can be considered mirrors of each other: FS protects past messages against future compromise, while PCS protects future messages from past compromise. Today, we are happy to announce the next step in advancing quantum resistance for the Signal Protocol: an additional regularly advancing post-quantum ratchet called the Sparse Post Quantum Ratchet, or SPQR. On its own, SPQR provides secure messaging that provably achieves these FS and PCS guarantees in a quantum safe manner. We mix the output of this new ratcheting protocol with Signal’s existing Double Ratchet, in a combination we refer to as the Triple Ratchet.&lt;/p&gt;
    &lt;p&gt;What does this mean for you as a Signal user? First, when it comes to your experience using the app, nothing changes. Second, because of how we’re rolling this out and mixing it in with our existing encryption, eventually all of your conversations will move to this new protocol without you needing to take any action. Third, and most importantly, this protects your communications both now and in the event that cryptographically relevant quantum computers eventually become a reality, and it allows us to maintain our existing security guarantees of forward secrecy and post-compromise security as we proactively prepare for that new world.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Current State of the Signal Protocol&lt;/head&gt;
    &lt;p&gt;The original Signal ratchet uses hash functions for FS and a set of elliptic-curve Diffie Hellman (ECDH) secret exchanges for PCS. The hash functions are quantum safe, but elliptic-curve cryptography is not. An example is in order: our favorite users, Alice and Bob, establish a long-term connection and chat over it regularly. During that session’s lifetime, Alice and Bob regularly agree on new ECDH secrets and use them to “ratchet” their session. Mean ol’ Mallory records the entire (encrypted) communication, and really wants to know what Alice and Bob are talking about.&lt;/p&gt;
    &lt;p&gt;The concept of a “ratchet” is crucial to our current non-quantum FS/PCS protection. In the physical world, a ratchet is a mechanism that allows a gear to rotate forward, but disallows rotation backwards. In the Signal Protocol, it takes on a similar role. When Alice and Bob “ratchet” their session, they replace the set of keys they were using prior with a new set based on both the older secrets and a new one they agree upon. Given access to those new secrets, though, there’s no (non-quantum) way to compute the older secrets. By being “one-way”, this ratcheting mechanism provides FS.&lt;/p&gt;
    &lt;p&gt;The ECDH mechanism allows Alice and Bob to generate new, small (32 bytes) data blobs and attach them to every message. Whenever each party receives a message from the other, they can locally (and relatively cheaply) use this data blob to agree on a new shared secret, then use that secret to ratchet their side of the protocol. Crucially, ECDH also allows Alice and Bob to both agree on the new secret without sending that secret itself over their session, and in fact without sending anything over the session that Mallory could use to determine it. This description of Diffie-Hellman key exchange provides more details on the concepts of such a key exchange, and this description of ECDH provides specific details on the variant used by the current Signal protocol.&lt;/p&gt;
    &lt;p&gt;Sometime midway through the lifetime of Alice and Bob’s session, Mallory successfully breaches the defences of both Alice and Bob, gaining access to all of the (current) secrets used for their session at the time of request. Alice and Bob should have the benefits of Forward Secrecy - they’ve ratcheted sometime recently before the compromise, so no messages earlier than their last ratchet are accessible to Mallory, since ratcheting isn’t reversible. They also retain the benefits of Post-Compromise Security. Their ratcheting after Mallory’s secret access agrees upon new keys that can’t be gleaned just from the captured data they pass between each other, re-securing the session.&lt;/p&gt;
    &lt;p&gt;Should Mallory have access to a quantum computer, though, things aren’t so simple. Because elliptic curve cryptography is not quantum resistant, it’s possible that Mallory could glean access to the secret that Alice and Bob agreed upon, just by looking at the communication between them. Given this, Alice and Bob’s session will never “heal”; Mallory’s access to their network traffic from this point forward will allow her to decrypt all future communications.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mixing In Quantum Security&lt;/head&gt;
    &lt;p&gt;In order to make our security guarantees stand up to quantum attacks, we need to mix in secrets generated from quantum secure algorithms. In PQXDH, we did this by performing an additional round of key agreement during the session-initiating handshake, then mixing the resulting shared secret into the initial secret material used to create Signal sessions. To handle FS and PCS, we need to do continuous key agreement, where over the lifetime of a session we keep generating new shared secrets and mixing those keys into our encryption keys.&lt;/p&gt;
    &lt;p&gt;Luckily there is a tool designed exactly for this purpose: the quantum-secure Key-Encapsulation Mechanism (KEM). KEMs share similar behavior to the Diffie-Hellman mechanisms we described earlier, where two clients provide each other with information, eventually deciding on a shared secret, without anyone who intercepts their communications being able to access that secret. However, there is one important distinction for KEMs - they require ordered, asymmetric messages to be passed between their clients. In ECDH, both clients send the other some public parameters, and both combine these parameters with their locally held secrets and come up with an identical shared secret. In the standardized ML-KEM key-encapsulation mechanism, though, the initiating client generates a pair of encapsulation key (EK) and decapsulation key (DK) (analogous to a public and private key respectively) and sends the EK. The receiving client receives it, generates a secret, and wraps it into a ciphertext (CT) with that key. The initiating client receives that CT and decapsulates with its previously generated DK. In the end, both clients have access to the new, shared secret, just through slightly different means.&lt;/p&gt;
    &lt;p&gt;Wanting to integrate this quantum-secure key sharing into Signal, we could take a simple, naive approach for each session. When Alice initiates a session with Bob,&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alice, with every message she sends, sends an EK&lt;/item&gt;
      &lt;item&gt;Bob, with every message he receives, generates a secret and a CT, and sends the CT back&lt;/item&gt;
      &lt;item&gt;Alice, on receiving a CT, extracts the secret with her DK and mixes it in&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This initially simple-looking approach, though, quickly runs into a number of issues we’ll need to address to make our protocol actually robust. First, encapsulation keys and CTs are large - over 1000 bytes each for ML-KEM 768, compared to the 32 bytes required for ECDH. Second, while this protocol works well when both clients are online, what happens when a client is offline? Or a message is dropped or reordered? Or Alice wants to send 10 messages before Bob wants to send one?&lt;/p&gt;
    &lt;p&gt;Some of these problems have well-understood solutions, but others have trade-offs that may shine in certain circumstances but fall short in others. Let’s dive in and come to some conclusions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who Wants What When&lt;/head&gt;
    &lt;p&gt;How does Alice decide what to send based on what Bob needs next, and vice versa? If Bob hasn’t received an EK yet, she shouldn’t send the next one. What does Bob send when he hasn’t yet received an EK from Alice, or when he has, but he’s already responded to it? This is a common problem when remote parties send messages to communicate, so there’s a good, well-understood solution: a state machine. Alice and Bob both keep track of “what state am I in”, and base their decisions on that. When sending or receiving a message, they might also change their state. For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alice wants to send a message, but she’s in a StartingA state, so she doesn’t have an EK. So, she generates an EK/DK pair, stores them locally, and transitions to the SendEK state&lt;/item&gt;
      &lt;item&gt;Alice wants to send a message and is in the SendEK state. She sends the EK along with the message&lt;/item&gt;
      &lt;item&gt;Alice wants to send another message, but she’s still in the SendEK state. So, she sends the EK with the new message as well&lt;/item&gt;
      &lt;item&gt;Bob receives the message with the EK. He generates a secret and uses the EK to create a CT. He transitions to the SendingCT state.&lt;/item&gt;
      &lt;item&gt;Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the message&lt;/item&gt;
      &lt;item&gt;Bob wants to send a message and he’s in the SendingCT state. He sends the CT along with the message&lt;/item&gt;
      &lt;item&gt;… etc …&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By crafting a set of states and transitions, both sides can coordinate what’s sent. Note, though, that even in this simple case, we see problems. For example, we’re sending our (large) EK and (large) CT multiple times.&lt;/p&gt;
    &lt;head rend="h2"&gt;Say (or Send) Less&lt;/head&gt;
    &lt;p&gt;We’ve already mentioned that the size of the data we’re sending has increased pretty drastically, from 32 bytes to over 1000 per message. But bandwidth is expensive, especially on consumer devices like client phones, that may be anywhere in the world and have extremely varied costs for sending bytes over the wire. So let’s discuss strategies for conserving that bandwidth.&lt;/p&gt;
    &lt;p&gt;First, the simplest approach - don’t send a new key with every message. Just, for example, send with every 50 messages, or once a week, or every 50 messages unless you haven’t sent a key in a week, or any other combination of options. All of these approaches tend to work pretty well in online cases, where both sides of a session are communicating in real-time with no message loss. But in cases where one side is offline or loss can occur, they can be problematic. Consider the case of “send a key if you haven’t sent one in a week”. If Bob has been offline for 2 weeks, what does Alice do when she wants to send a message? What happens if we can lose messages, and we lose the one in fifty that contains a new key? Or, what happens if there’s an attacker in the middle that wants to stop us from generating new secrets, and can look for messages that are 1000 bytes larger than the others and drop them, only allowing keyless messages through?&lt;/p&gt;
    &lt;p&gt;Another method is to chunk up a message. Want to send 1000 bytes? Send 10 chunks of 100 bytes each. Already sent 10 chunks? Resend the first chunk, then the second, etc. This smooths out the total number of bytes sent, keeping individual message sizes small and uniform. And often, loss of messages is handled. If chunk 1 was lost, just wait for it to be resent. But it runs into an issue with message loss - if chunk 99 was lost, the receiver has to wait for all of chunks 1-98 to be resent before it receives the chunk it missed. More importantly, if a malicious middleman wants to stop keys from being decided upon, they could always drop chunk 3, never allowing the full key to pass between the two parties.&lt;/p&gt;
    &lt;p&gt;We can get around all of these issues using a concept called erasure codes. Erasure codes work by breaking up a larger message into smaller chunks, then sending those along. Let’s consider our 1000 byte message being sent as 100 byte chunks again. After chunk #10 has been sent, the entirety of the original 1000 byte message has been sent along in cleartext. But rather than just send the first chunk over again, erasure codes build up a new chunk #11, and #12, etc. And they build them in such a way that, once the recipient receives any 10 chunks in any order, they’ll be able to reconstruct the original 1000 byte message.&lt;/p&gt;
    &lt;p&gt;When we put this concept of erasure code chunks together with our previous state machine, it gives us a way to send large blocks of data in small chunks, while handling messages that are dropped. Crucially, this includes messages dropped by a malicious middleman: since any N chunks can be used to recreate the original message, a bad actor would need to drop all messages after #N-1 to disallow the data to go through, forcing them into a complete (and highly noticeable) denial of service. Now, if Alice wants to send an EK to Bob, Alice will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transition from the StartingA state to the SendingEK state, by generating a new EK and chunking it&lt;/item&gt;
      &lt;item&gt;While in the SendingEK state, send a new chunk of the EK along with any messages she sends&lt;/item&gt;
      &lt;item&gt;When she receives confirmation that the recipient has received the EK (when she receives a chunk of CT), transition to the ReceivingCT state&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On Bob’s side, he will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Transition from the StartingB state to the ReceivingEK state when he receives its first EK chunk&lt;/item&gt;
      &lt;item&gt;Keep receiving EK chunks until he has enough to reconstruct the EK&lt;/item&gt;
      &lt;item&gt;At that point, reconstruct the EK, generate the CT, chunk the CT, and transition to the SendingCT state&lt;/item&gt;
      &lt;item&gt;From this point on, he will send a chunk of the CT with every message&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One interesting way of looking at this protocol so far is to consider the messages flowing from Alice to Bob as potential capacity for sending data associated with post-quantum ratcheting: each message that we send, we could also send additional data like a chunk of EK or of the CT. If we look at Bob’s side, above, we notice that sometimes he’s using that capacity (IE: in step 4 when he’s sending CT chunks) and sometimes he’s not (if he sends a message to Alice during step 2, he has no additional data to send). This capacity is pretty limited, so using more of it gives us the potential to speed up our protocol and agree on new secrets more frequently.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Meditation On How Faster Isn’t Always Better&lt;/head&gt;
    &lt;p&gt;We want to generate shared secrets, then use them to secure messages. So, does that mean that we want to generate shared secrets as fast as possible? Let’s introduce a new term: an epoch. Alice and Bob start their sessions in epoch 0, sending the EKs for epoch 1 (EK#1) and associated ciphertext (CT#1) to each other. Once that process completes, they have a new shared secret they use to enter epoch 1, after which all newly sent messages are protected by the new secret. Each time they generate a new shared secret, they use it to enter a new epoch. Surely, every time we enter a new epoch with a new shared secret, we protect messages before that secret (FS) and after that secret (PCS), so faster generation is better? It seems simple, but there’s an interesting complexity here that deserves attention.&lt;/p&gt;
    &lt;p&gt;First, let’s discuss how to do things faster. Right now, there’s a lot of capacity we’re not using: Bob sends nothing while Alice sends an EK, and Alice sends nothing while Bob sends a CT. Speeding this up isn’t actually that hard. Let’s change things so that Alice sends EK#1, and once Bob acknowledges its receipt, Alice immediately generates and sends EK#2. And once she notices Bob has received that, she generates and sends EK#3, etc. Whenever Alice sends a new message, she always has data to send along with it (new EK chunks), so she’s using its full capacity. Bob doesn’t always have a new CT to send, but he is receiving EKs as fast as Alice can send them, so he often has a new CT to send along.&lt;/p&gt;
    &lt;p&gt;But now let’s consider what happens when an attacker gains access to Alice. Let’s say that Alice has sent EK#1 and EK#2 to Bob, and she’s in the process of sending EK#3. Bob has acknowledged receipt of EK#1 and EK#2, but he’s still in the process of sending CT#1, since in this case Bob sends fewer messages to Alice than vice versa. Because Alice has already generated 3 EKs she hasn’t used, Alice needs to keep the associated DK#1, DK#2, and DK#3 around. So, if at this point someone maliciously gains control of Alice’s device, they gain access to both the secrets associated with the current epoch (here, epoch 0) and to the DKs necessary to reconstruct the secrets to other epochs (here, epochs 1, 2, and 3) using only the over-the-wire CT that Bob has yet to send. This is a big problem: by generating secrets early, we’ve actually made the in-progress epochs and any messages that will be sent within them less secure against this single-point-in-time breach.&lt;/p&gt;
    &lt;p&gt;To test this out, we at Signal built a number of different state machines, each sending different sets of data either in parallel or serially. We then ran these state machines in numerous simulations, varying things like the ratio of messages sent by Alice vs Bob, the amount of data loss or reordering, etc. And while running these simulations, we tracked what epochs’ secrets were exposed at any point in time, assuming an attacker were to breach either Alice’s or Bob’s secret store. The results showed that, in general, while simulations that handled multiple epochs’ secrets in parallel (IE: by sending EK#2 before receipt of CT#1) did generate new epochs more quickly, they actually made more messages vulnerable to a single breach.&lt;/p&gt;
    &lt;head rend="h2"&gt;But Let’s Still Be Efficient&lt;/head&gt;
    &lt;p&gt;This still leaves us with a problem, though: the capacity present in messages we send in either direction is still a precious resource, and we want to use it as efficiently as possible. And our simple approach of Alice’s “send EK, receive CT, repeat” and Bob’s “receive EK, send CT, repeat” leaves lots of time where Alice and Bob have nothing to send, should that capacity be available.&lt;/p&gt;
    &lt;p&gt;To improve our use of our sending capacity, we decided to take a harder look into the ML-KEM algorithm we’re using to share secrets, to see if there was room to improve. Let’s break things down more and share some actual specifics on the ML-KEM algorithm.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates an EK of 1184 bytes to send to Bob, and an associated DK&lt;/item&gt;
      &lt;item&gt;Bob receives the EK&lt;/item&gt;
      &lt;item&gt;Bob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice&lt;/item&gt;
      &lt;item&gt;Alice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secret&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Diving in further, we can break out step #3 into some sub-steps&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates an EK of 1184 bytes to send to Bob, and an associated DK&lt;/item&gt;
      &lt;item&gt;Bob receives the EK&lt;/item&gt;
      &lt;item&gt;Bob samples a new shared secret (32 bytes), which he encrypts with EK into a CT of 1088 bytes to send to Alice1&lt;list rend="ol"&gt;&lt;item&gt;Bob creates a new shared secret S and sampled randomness R by sampling entropy and combining it with a hash of EK&lt;/item&gt;&lt;item&gt;Bob hashes the EK into a Hash&lt;/item&gt;&lt;item&gt;Bob pulls 32 bytes of the EK, a Seed&lt;/item&gt;&lt;item&gt;Bob uses the Seed and R to generate the majority of the CT&lt;/item&gt;&lt;item&gt;Bob then uses S and EK to generate the last portion of the CT&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Alice receives the CT, uses the DK to decrypt it, and now also has access to the 32 byte shared secret&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Step 3.d, which generates 960 bytes of the 1088-byte CT, only needs 64 bytes of input: a Seed that’s 32 of EK’s bytes, and the hash of EK, which is an additional 32. If we combine these values and send them first, then most of EK and most of the CT can be sent in parallel from Alice to Bob and Bob to Alice respectively. Our more complicated but more efficient secret sharing now looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alice generates EK and DK. Alice extracts the 32-byte Seed from EK&lt;/item&gt;
      &lt;item&gt;Alice sends 64 bytes EK1 (Seed + Hash(EK)) to Bob. Bob sends nothing during this time.&lt;/item&gt;
      &lt;item&gt;Bob receives the Seed and Hash, and generates the first, largest part of the CT from them (CT1)&lt;/item&gt;
      &lt;item&gt;After this point, Alice sends EK2 (the rest of the EK minus the Seed), while Bob sends CT1&lt;/item&gt;
      &lt;item&gt;Bob eventually receives EK2, and uses it to generate the final portion of the CT (CT2)&lt;/item&gt;
      &lt;item&gt;Once Alice tells Bob that she has received all of CT1, Bob sends Alice CT2. Alice sends nothing during this time.&lt;/item&gt;
      &lt;item&gt;With both sides having all of the pieces of EK and the CT that they need, they extract their shared secret and increment their epoch&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are still places in this algorithm (specifically steps 2 and 6) where one side has nothing to send. But during those times, the other side has only a very small amount of information to send, so the duration of those steps is minimal compared to the rest of the process. Specifically, while the full EK is 37 chunks and the full CT is 34, the two pieces of the new protocol which must be sent without data being received (EK1 and CT2) are 2 and 4 chunks respectively, while the pieces that can be sent while also receiving (EK2 and CT1) are the bulk of the data, at 36 and 30 chunks respectively. Far more of our sending capacity is actually used with this approach.&lt;/p&gt;
    &lt;p&gt;Remember that all of this is just to perform a quantum-safe key exchange that gives us a secret we can mix into the bigger protocol. To help us organize our code, our security proofs, and our understanding better we treat this process as a standalone protocol that we call the ML-KEM Braid.&lt;/p&gt;
    &lt;p&gt;This work was greatly aided by the authors of the libcrux-ml-kem Rust library, who graciously exposed the APIs necessary to work with this incremental version of ML-KEM 768. With this approach completed, we’ve been able to really efficiently use the sending capacity of messages sent between two parties to share secrets as quickly as possible without exposing secrets from multiple epochs to potential attackers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mixing Things Up - The Triple Ratchet&lt;/head&gt;
    &lt;p&gt;There are plenty of details to add to make sure that we reached every corner - check those out in our online protocol documentation - but this basic idea lets us build secure messaging that has post-quantum FS and PCS without using up anyone’s data. We’re not done, though! Remember, at the beginning of this post we said we wanted post-quantum security without taking away our existing guarantees.&lt;/p&gt;
    &lt;p&gt;While today’s Double Ratchet may not be quantum safe, it provides a high level of security today and we believe it will continue to be strong well into the future. We aren’t going to take that away from our users. So what can we do?&lt;/p&gt;
    &lt;p&gt;Our answer ends up being really simple: we run both the Double Ratchet and the Sparse Post Quantum Ratchet alongside each other and mix their keys together, into what we’re calling the Triple Ratchet protocol. When you want to send a message you ask both the Double Ratchet and SPQR “What encryption key should I use for the next message?” and they will both give you a key (along with some other data you need to put in a message header). Instead of either key being used directly, both are passed into a Key Derivation Function - a special function that takes random-enough inputs and produces a secure cryptographic key that’s as long as you need. This gives you a new “mixed” key that has hybrid security. An attacker has to break both our elliptic curve and ML-KEM to even be able to distinguish this key from random bits. We use that mixed key to encrypt our message.&lt;/p&gt;
    &lt;p&gt;Receiving messages is just as easy. We take the message header - remember it has some extra data in it - and send it to the Double Ratchet and SPQR and ask them “What key should I use to decrypt a message with this header?” They both return their keys and you feed them both into that Key Derivation Function to get your decryption key. After that, everything proceeds just like it always has.&lt;/p&gt;
    &lt;head rend="h2"&gt;Heterogeneous Rollout&lt;/head&gt;
    &lt;p&gt;So we’ve got this new, snazzy protocol, and we want to roll it out to all of our users across all of their devices… but none of the devices currently support that protocol. We roll it out to Alice, and Alice tries to talk to Bob, but Alice speaks SPQR and Bob doesn’t. Or we roll it out to Bob, but Alice wants to talk to Bob and Alice doesn’t know the new protocol Bob wants to use. How do we make this work?&lt;/p&gt;
    &lt;p&gt;Let’s talk about the simplest option: allowing downgrades. Alice tries to establish a session with Bob using SPQR and sends a message over it. Bob fails to read the message and establish the session, because Bob hasn’t been upgraded yet. Bob sends Alice an error, so Alice has to try again. This sounds fine, but in practice it’s not tenable. Consider what happens if Alice and Bob aren’t online at the same time… Alice sends a message at 1am, then shuts down. Bob starts up at 3am, sends an error, then shuts down. Alice gets that error when she restarts at 5am, then resends. Bob starts up at 7am and finally gets the message he should have received at 3am, 4 hours behind schedule.&lt;/p&gt;
    &lt;p&gt;To handle this, we designed the SPQR protocol to allow itself to downgrade to not being used. When Alice sends her first message, she attaches the SPQR data she would need to start up negotiation of the protocol. Noticing that downgrades are allowed for this session, Alice doesn’t mix any SPQR key material into the message yet. Bob ignores that data, because it’s in a location he glosses over, but since there’s no mixed in keys yet, he can still decrypt the message. He sends a response that lacks SPQR data (since he doesn’t yet know how to fill it in), which Alice receives. Alice sees a message without SPQR data, and understands that Bob doesn’t speak SPQR yet. So, she downgrades to not using it for that session, and they happily talk without SPQR protection.&lt;/p&gt;
    &lt;p&gt;There’s some scary potential problems here… let’s work through them. First off, can a malicious middleman force a downgrade and disallow Alice and Bob from using SPQR, even if both of them are able to? We protect against that by having the SPQR data attached to the message be MAC’d by the message-wide authentication code - a middleman can’t remove it without altering the whole message in such a way that the other party sees it, even if that other party doesn’t speak SPQR. Second, could some error cause messages to accidentally downgrade sometime later in their lifecycle, due either to bugs in the code or malicious activity? Crucially, SPQR only allows a downgrade when it first receives a message from a remote party. So, Bob can only downgrade if he receives his first message from Alice and notices that she doesn’t support SPQR, and Alice will only downgrade if she receives her first reply from Bob and notices that he doesn’t. After that first back-and-forth, SPQR is locked in and used for the remainder of the session.&lt;/p&gt;
    &lt;p&gt;Finally, those familiar with Signal’s internal workings might note that Signal sessions last a really long time, potentially years. Can we ever say “every session is protected by SPQR”, given that SPQR is only added to new sessions as they’re being initiated? To accomplish this, Signal will eventually (once all clients support the new protocol) roll out a code change that enforces SPQR for all sessions, and that archives all sessions which don’t yet have that protection. After the full rollout of that future update, we’ll be able to confidently assert complete coverage of SPQR.&lt;/p&gt;
    &lt;p&gt;One nice benefit to setting up this “maybe downgrade if the other side doesn’t support things” approach is that it also sets us up for the future: the same mechanisms that allow us to choose between SPQR or no-SPQR are designed to also allow us to upgrade from SPQR to some far-future (as yet unimagined) SPQRv2.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making Sure We Get It Right&lt;/head&gt;
    &lt;p&gt;Complex protocols require extraordinary care. We have to ensure that the new protocol doesn’t lose any of the security guarantees the Double Ratchet gives us. We have to ensure that we actually get the post-quantum protection we’re aiming for. And even then, after we have full confidence in the protocol, we have to make sure that our implementation is correct and robust and stays that way as we maintain it. This is a tall order.&lt;/p&gt;
    &lt;p&gt;To make sure we got this right, we started by building the protocol on a firm foundation of fundamental research. We built on the years of research the academic community has put into secure messaging and we collaborated with researchers from PQShield, AIST, and NYU to explore what was possible with post-quantum secure messaging. In a paper at Eurocrypt 25 we introduced erasure code based chunking and proposed the high-level Triple Ratchet protocol, proving that it gave us the post-quantum security we wanted without taking away any of the security of the classic Double Ratchet. In a follow up paper at USENIX 25, we observed that there are many different ways to design a post-quantum ratchet protocol and we need to pick the one that protects user messages the best. We introduced and analyzed six different protocols and two stood out: one is essentially SPQR, the other is a protocol using a new KEM, called Katana, that we designed just for ratcheting. That second one is exciting, but we want to stick to standards to start!&lt;/p&gt;
    &lt;head rend="h2"&gt;Formal Verification From the Start&lt;/head&gt;
    &lt;p&gt;This research gave us the framework to think about protocol design and prove protocols are secure, but there is a big leap from an academic paper to code. Already when designing PQXDH - a much simpler protocol! - we found that formal verification was an important tool for getting the details right. With the Triple Ratchet we partnered with Cryspen and made formal verification part of the process from the beginning.&lt;/p&gt;
    &lt;p&gt;As we kept finding better protocol candidates - and we implemented around a dozen of them - we modeled them in ProVerif to prove that they had the security properties we needed. Rather than wrapping up a protocol design and performing formal verification as a last step we made it a core part of the design process. Now that the design is settled, this gives us machine verified proof that our protocol has the security properties we demand from it. We wrote our Rust code to closely match the ProVerif models, so it is easy to check that we’re modeling what we implement. In particular, ProVerif is very good at reasoning about state machines, which we’re already using, making the mapping from code to model much simpler.&lt;/p&gt;
    &lt;p&gt;We are taking formal verification further than that, though. We are using hax to translate our Rust implementation into F* on every CI run. Once the F* models are extracted, we prove that core parts of our highly optimized implementation are correct, that function pre-conditions and post-conditions cannot be violated, and that the entire crate is panic free. That last one is a big deal. It is great for usability, of course, because nobody wants their app to crash. But it also matters for correctness. We aggressively add assertions about things we believe must be true when the protocol is running correctly - and we crash the app if they are false. With hax and F*, we prove that those assertions will never fail.&lt;/p&gt;
    &lt;head rend="h2"&gt;Formal Verification Doesn’t Freeze Our Progress&lt;/head&gt;
    &lt;p&gt;Often when people think about formally verified protocol implementations, they imagine a one-time huge investment in verification that leaves you with a codebase frozen in time. This is not the case here. We re-run formal verification in our CI pipeline every time a developer pushes a change to GitHub. If the proofs fail then the build fails, and the developer needs to fix it. In our experience so far, this is usually as simple as adding a pre- or postcondition or returning an error when a value is out of bounds. For us, formal verification is a dynamic part of the development process and ensures that the quality is high on every merge.&lt;/p&gt;
    &lt;head rend="h2"&gt;TLDR&lt;/head&gt;
    &lt;p&gt;Signal is rolling out a new version of the Signal Protocol with the Triple Ratchet. It adds the Sparse Post-Quantum Ratchet, or SPQR, to the existing Double Ratchet to create a new Triple Ratchet which gives our users quantum-safe messaging without taking away any of our existing security promises. It’s being added in such a way that it can be rolled out without disruption. It’s relatively lightweight, not using much additional bandwidth for each message, to keep network costs low for our users. It’s resistant to meddling by malicious middlemen - to disrupt it, all messages after a certain time must be dropped, causing a noticeable denial of service for users. We’re rolling it out slowly and carefully now, but in such a way that we’ll eventually be able to say with confidence “every message sent by Signal is protected by this.” Its code has been formally verified, and will continue to be so even as future updates affect the protocol. It’s the combined effort of Signal employees and external researchers and contributors, and it’s only possible due to the continued work and diligence of the larger crypto community. And as a user of Signal, our biggest hope is that you never even notice or care. Except one day, when headlines scream “OMG, quantum computers are here”, you can look back on this blog post and say “oh, I guess I don’t have to care about that, because it’s already been handled”, as you sip your Nutri-Algae while your self-driving flying car wends its way through the floating tenements of Megapolis Prime.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Those that are interested can look at https://nvlpubs.nist.gov/nistpubs/fips/nist.fips.203.pdf and note that Algorithm 17 uses randomness plus the hash of EK to generate a shared secret and random value, then that random value is used in Algorithm 14 to create c1. The rest of ekPKE is only used by Algorithm 14 to generate c2. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451527</guid><pubDate>Thu, 02 Oct 2025 16:06:10 +0000</pubDate></item><item><title>Launch HN: Simplex (YC S24) – Browser automation platform for developers</title><link>https://www.simplex.sh/</link><description>&lt;doc fingerprint="f4968ef0cf13274b"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Browser automation&lt;lb/&gt;for developers&lt;/head&gt;&lt;p&gt;Simplex provides all the infrastructure needed for modern browser automation. &lt;lb/&gt; Remote browsers, steerable web agents, and more.&lt;/p&gt;&lt;head rend="h3"&gt;A team from world class institutions&lt;/head&gt;&lt;head rend="h2"&gt;Watch a live demo of Simplex automating&lt;lb/&gt; a real billing portal.&lt;/head&gt;&lt;head rend="h3"&gt;Live Session Stream&lt;/head&gt;&lt;p&gt;Demo Preview&lt;/p&gt;&lt;head rend="h3"&gt;Live Demo Logs&lt;/head&gt;&lt;p&gt;Agent logs will appear here when demo is running&lt;/p&gt;&lt;head rend="h2"&gt;Engineered from the ground up to work with legacy systems.&lt;/head&gt;&lt;p&gt;Reliably automate every legacy portal your customers use.&lt;/p&gt;&lt;head rend="h3"&gt;Billing Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to log into a billing portal and download the list of invoices for a specified customer.&lt;/p&gt;&lt;head rend="h3"&gt;Prior Authorization Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to fill out complex, branching-logic prior authorization forms on medical provider portals.&lt;/p&gt;&lt;head rend="h3"&gt;ERPs&lt;/head&gt;&lt;p&gt;Simplex has been used to automate data entry and download report PDFs across different ERPs.&lt;/p&gt;&lt;head rend="h3"&gt;Government Portals&lt;/head&gt;&lt;p&gt;Simplex has been used to search and extract structured information across public government portals.&lt;/p&gt;&lt;head rend="h3"&gt;TMS/WMS Software&lt;/head&gt;&lt;p&gt;Simplex has been used to log into a TMS portal, create and edit the information for a shipment, then dispatch the shipment.&lt;/p&gt;&lt;head rend="h3"&gt;... and more&lt;/head&gt;&lt;p&gt;with us to discuss your specific use case.&lt;/p&gt;&lt;table&gt;&lt;row span="6"&gt;&lt;cell role="head"&gt;Order ID&lt;/cell&gt;&lt;cell role="head"&gt;Customer&lt;/cell&gt;&lt;cell role="head"&gt;Status&lt;/cell&gt;&lt;cell role="head"&gt;Amount&lt;/cell&gt;&lt;cell role="head"&gt;Last Updated&lt;/cell&gt;&lt;cell role="head"&gt;Actions&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-001&lt;/cell&gt;&lt;cell&gt;John Smith&lt;/cell&gt;&lt;cell&gt;PENDING&lt;/cell&gt;&lt;cell&gt;$1,234.56&lt;/cell&gt;&lt;cell&gt;01/15/2024&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-002&lt;/cell&gt;&lt;cell&gt;Jane Doe&lt;/cell&gt;&lt;cell&gt;PROCESSING&lt;/cell&gt;&lt;cell&gt;$987.65&lt;/cell&gt;&lt;cell&gt;01/14/2024&lt;/cell&gt;&lt;/row&gt;&lt;row span="6"&gt;&lt;cell&gt;PO-2024-003&lt;/cell&gt;&lt;cell&gt;Bob Johnson&lt;/cell&gt;&lt;cell&gt;COMPLETED&lt;/cell&gt;&lt;cell&gt;$2,345.67&lt;/cell&gt;&lt;cell&gt;01/13/2024&lt;/cell&gt;&lt;/row&gt;&lt;row&gt;&lt;cell&gt;PO-2024-004&lt;/cell&gt;&lt;cell&gt;Alice Brown&lt;/cell&gt;&lt;cell&gt;ERROR&lt;/cell&gt;&lt;cell&gt;$876.54&lt;/cell&gt;&lt;cell&gt;01/12/2024&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;head rend="h2"&gt;Deploy reliably, scale easily.&lt;/head&gt;&lt;head rend="h3"&gt;Run consistent workflows&lt;/head&gt;&lt;p&gt;Simplex automatically caches agent actions. This increases reliability of runs and makes developing flows lightning fast.&lt;/p&gt;&lt;head rend="h3"&gt;Create realtime flows&lt;/head&gt;We've achieved realtime latency to handle complex workflows during phone calls.&lt;p&gt;if low latency flows are a priority for you.&lt;/p&gt;&lt;quote&gt;simplex.click(“New Order”)simplex.click(“Shipment Address”)simplex.type(“”)&lt;/quote&gt;&lt;head rend="h2"&gt;Simplex just works.&lt;/head&gt;&lt;head rend="h3"&gt;Production-ready&lt;/head&gt;&lt;p&gt;Eval harnesses to stress-test your workflows at scale and emulate production conditions.&lt;/p&gt;&lt;head rend="h3"&gt;Authentication Handling&lt;/head&gt;&lt;p&gt;Authentication SDK functions to handle 2FA, login data, and more on your customers' sites. See more here.&lt;/p&gt;&lt;head rend="h3"&gt;Scalable Headless Browsers&lt;/head&gt;&lt;p&gt;Headless browsers that can scale to 100s of concurrent sessions in seconds.&lt;/p&gt;&lt;head rend="h3"&gt;Stealth Mode&lt;/head&gt;&lt;p&gt;Automatic CAPTCHA solving, proxies, and anti-bot protections.&lt;/p&gt;&lt;head rend="h3"&gt;Controllable Workflows&lt;/head&gt;&lt;p&gt;Our web agents are constrained to only take the actions you tell it to.&lt;/p&gt;&lt;head rend="h3"&gt;Optimized Workflows&lt;/head&gt;&lt;p&gt;Automatically cache your workflows for fast and reliable execution in production.&lt;/p&gt;&lt;head rend="h3"&gt;Robust SDKs&lt;/head&gt;&lt;p&gt;Our SDKs are designed to be robust and easy to use. Available in both Python and TypeScript.&lt;/p&gt;&lt;head rend="h3"&gt;Detailed Logging and Replays&lt;/head&gt;&lt;p&gt;View a livestream and live logs of sessions as they happen. Share session replays and detailed agent logs with your team and customers.&lt;/p&gt;&lt;head rend="h2"&gt;Ready to get started?&lt;/head&gt;&lt;p&gt;Book a call with our team to discuss your use case and get started.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451547</guid><pubDate>Thu, 02 Oct 2025 16:07:23 +0000</pubDate></item><item><title>Playball – Watch MLB games from a terminal</title><link>https://github.com/paaatrick/playball</link><description>&lt;doc fingerprint="607031164bc1d92f"&gt;
  &lt;main&gt;
    &lt;p&gt;Watch MLB games from the comfort of your own terminal&lt;/p&gt;
    &lt;p&gt;MLB Gameday and MLB.tv are great, but sometimes you want to keep an eye on a game a bit more discreetly. &lt;code&gt;playball&lt;/code&gt; puts the game in a terminal window.&lt;/p&gt;
    &lt;p&gt;Just want to try it out?&lt;/p&gt;
    &lt;code&gt;$ npx playball
&lt;/code&gt;
    &lt;p&gt;Ready for the big leagues? Install the package globally&lt;/p&gt;
    &lt;code&gt;$ npm install -g playball
&lt;/code&gt;
    &lt;p&gt;Then run it&lt;/p&gt;
    &lt;code&gt;$ playball
&lt;/code&gt;
    &lt;code&gt;$ docker build -t playball .
$ docker run -it --rm --name playball playball:latest
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;q&lt;/cell&gt;
        &lt;cell&gt;quit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;c&lt;/cell&gt;
        &lt;cell&gt;go to schedule view&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;s&lt;/cell&gt;
        &lt;cell&gt;go to standings view&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;↓/j, ↑/k, ←/h, →/l&lt;/cell&gt;
        &lt;cell&gt;change highlighted game&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;enter&lt;/cell&gt;
        &lt;cell&gt;view highlighted game&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;p&lt;/cell&gt;
        &lt;cell&gt;show previous day's schedule/results&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;n&lt;/cell&gt;
        &lt;cell&gt;show next day's schedule&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;t&lt;/cell&gt;
        &lt;cell&gt;return to today's schedule&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;↓/j, ↑/k&lt;/cell&gt;
        &lt;cell&gt;scroll list of all plays&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Playball can be configured using the &lt;code&gt;config&lt;/code&gt; subcommand. To list the current configuration values run the subcommand with no additional arguments:&lt;/p&gt;
    &lt;code&gt;playball config&lt;/code&gt;
    &lt;p&gt;You should see output similar to:&lt;/p&gt;
    &lt;code&gt;color.ball = green
color.favorite-star = yellow
color.in-play-no-out = blue
color.in-play-out = white
color.in-play-runs-bg = white
color.in-play-runs-fg = black
color.on-base = yellow
color.other-event = white
color.out = red
color.strike = red
color.strike-out = red
color.walk = green
favorites = 
&lt;/code&gt;
    &lt;p&gt;To get the value of a single setting pass the key as an additional argument:&lt;/p&gt;
    &lt;code&gt;playball config color.strike&lt;/code&gt;
    &lt;p&gt;To change a setting pass the key and value as arguments:&lt;/p&gt;
    &lt;code&gt;playball config color.strike blue&lt;/code&gt;
    &lt;p&gt;To revert a setting to its default value provide the key and the &lt;code&gt;--unset&lt;/code&gt; flag:&lt;/p&gt;
    &lt;code&gt;playball config color.strike --unset&lt;/code&gt;
    &lt;p&gt;This table summarizes the available settings:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;key&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
        &lt;cell role="head"&gt;default&lt;/cell&gt;
        &lt;cell role="head"&gt;allowed values&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.ball&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing balls in top row of game view&lt;/cell&gt;
        &lt;cell&gt;green&lt;/cell&gt;
        &lt;cell&gt;One of the following: &lt;code&gt;black&lt;/code&gt;, &lt;code&gt;red&lt;/code&gt;, &lt;code&gt;green&lt;/code&gt;, &lt;code&gt;yellow&lt;/code&gt;, &lt;code&gt;blue&lt;/code&gt;, &lt;code&gt;magenta&lt;/code&gt;, &lt;code&gt;cyan&lt;/code&gt;, &lt;code&gt;white&lt;/code&gt;, &lt;code&gt;grey&lt;/code&gt;. Any of those colors may be prefixed by &lt;code&gt;bright-&lt;/code&gt; or &lt;code&gt;light-&lt;/code&gt; (for example &lt;code&gt;bright-green&lt;/code&gt;). The exact color used will depend on your terminal settings. The value &lt;code&gt;default&lt;/code&gt; may be used to specify the default text color for your terminal. Finally hex colors (e.g &lt;code&gt;#FFA500&lt;/code&gt;) can be specified. If your terminal does not support true color, the closest supported color may be used.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.favorite-star&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of star indiciating favorite team in schedule and standing views&lt;/cell&gt;
        &lt;cell&gt;yellow&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-no-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where ball was put in play and no out was made (single, double, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;blue&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where ball was put in play and an out was made (flyout, fielder's choice, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-runs-bg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Background color for score update in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.in-play-runs-fg&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Foreground color for score update in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;black&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.on-base&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of diamonds representing runners on base in top row of game view&lt;/cell&gt;
        &lt;cell&gt;yellow&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.other-event&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of other events (mound visit, injury delay, etc) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;white&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing outs in top row of game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.strike&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of dots representing strikes in top row of game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.strike-out&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where play ends on a strike (strike out) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;red&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;color.walk&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Color of result where play ends on a ball (walk, hit by pitch) in list of plays in game view&lt;/cell&gt;
        &lt;cell&gt;green&lt;/cell&gt;
        &lt;cell&gt;See above&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;favorites&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Teams to highlight in schedule and standings views&lt;/cell&gt;
        &lt;cell&gt;Any one of the following: &lt;code&gt;ATL&lt;/code&gt;, &lt;code&gt;AZ&lt;/code&gt;, &lt;code&gt;BAL&lt;/code&gt;, &lt;code&gt;BOS&lt;/code&gt;, &lt;code&gt;CHC&lt;/code&gt;, &lt;code&gt;CIN&lt;/code&gt;, &lt;code&gt;CLE&lt;/code&gt;, &lt;code&gt;COL&lt;/code&gt;, &lt;code&gt;CWS&lt;/code&gt;, &lt;code&gt;DET&lt;/code&gt;, &lt;code&gt;HOU&lt;/code&gt;, &lt;code&gt;KC&lt;/code&gt;, &lt;code&gt;LAA&lt;/code&gt;, &lt;code&gt;LAD&lt;/code&gt;, &lt;code&gt;MIA&lt;/code&gt;, &lt;code&gt;MIL&lt;/code&gt;, &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;NYM&lt;/code&gt;, &lt;code&gt;NYY&lt;/code&gt;, &lt;code&gt;OAK&lt;/code&gt;, &lt;code&gt;PHI&lt;/code&gt;, &lt;code&gt;PIT&lt;/code&gt;, &lt;code&gt;SD&lt;/code&gt;, &lt;code&gt;SEA&lt;/code&gt;, &lt;code&gt;SF&lt;/code&gt;, &lt;code&gt;STL&lt;/code&gt;, &lt;code&gt;TB&lt;/code&gt;, &lt;code&gt;TEX&lt;/code&gt;, &lt;code&gt;TOR&lt;/code&gt;, &lt;code&gt;WSH&lt;/code&gt;. Or a comma-separated list of multiple (e.g. &lt;code&gt;SEA,MIL&lt;/code&gt;)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;git clone https://github.com/paaatrick/playball.git
cd playball
npm install
npm start
&lt;/code&gt;
    &lt;p&gt;Contributions are welcome!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451577</guid><pubDate>Thu, 02 Oct 2025 16:09:15 +0000</pubDate></item><item><title>The Atlantic Quantum team is joining Google</title><link>https://blog.google/technology/research/scaling-quantum-computing-even-faster-with-atlantic-quantum/</link><description>&lt;doc fingerprint="e10f117325f6a4e6"&gt;
  &lt;main&gt;
    &lt;p&gt;Google Quantum AI was founded in 2012 and our mission today remains the same — build quantum computing for otherwise unsolvable problems. We’re making steady progress on our roadmap, including with our latest Willow chip.&lt;/p&gt;
    &lt;p&gt;Today, we’re excited to announce that the Atlantic Quantum team is joining Google. Atlantic Quantum is an MIT-founded startup that develops highly integrated quantum computing hardware. Its modular chip stack, which combines qubits and superconducting control electronics within the cold stage, will help Google Quantum AI more effectively scale our superconducting qubit hardware, and accelerate progress on our roadmap to a large error-corrected quantum computer and real-world applications.&lt;/p&gt;
    &lt;p&gt;We’re delighted for Atlantic Quantum to join us as Google continues to invest in the future of quantum computing and deliver its benefits to society. Learn more about our mission and follow our progress at quantumai.google.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45451961</guid><pubDate>Thu, 02 Oct 2025 16:36:53 +0000</pubDate></item><item><title>Why I chose Lua for this blog</title><link>https://andregarzia.com/2025/03/why-i-choose-lua-for-this-blog.html</link><description>&lt;doc fingerprint="7f443511072312dc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I chose Lua for this blog&lt;/head&gt;
    &lt;p&gt;This blog used to run using with a stack based on Racket using Pollen and lots of hacks on top of it. At some point I realised that my setup was working against me. The moving parts and workflow I created added too much friction to keep my blog active. That happened mostly because it was a static generator trying to behave as if it was dynamic website with an editing interface. That can be done really well â cue Grav CMS â but that was not the case for me.&lt;/p&gt;
    &lt;p&gt;Once I decided to rewrite this blog as a simpler system, I faced the dilema of what stack to choose. The obvious choice for me would be Javascript, it is the language I use more often and one that I am quite confortable with. Still, I don't think it is a wise choice for the kind of blog I want to maintain.&lt;/p&gt;
    &lt;p&gt;Talking to some friends recently, I noticed that many people I know that have implemented their own blogging systems face many challenges keeping them running over many years. Not because it is hard to keep software running, but because their stack of choice is moving faster than their codebase.&lt;/p&gt;
    &lt;p&gt;This problem is specially prevalent in the Javascript world. It is almost a crime that JS as understood by the browser is this beautiful language with extreme retrocompatibility, while JS as understood and used by the current tooling and workflows is this mess moving at lightspeed. Let me unpack that for a bit.&lt;/p&gt;
    &lt;p&gt;You can open a web page from 1995 on your browser of choice and it will just work because browser vendors try really hard to make sure they don't break the web.&lt;/p&gt;
    &lt;p&gt;Developers who built the whole ecosystem of NodeJS, NPM, and all those libraries and frameworks don't share the same ethos. They all make a big case of semantic versioning and thus being able to handle breaking changes, but they have breaking changes all the time. You'd be hardpressed to actually run some JS code from ten years ago based on NodeJS and NPM. There is a big chance that dependencies might be gone, broken, or it might be incompatible with the current NodeJS.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I know this sounds like FUD, and that for many many projects, maybe even most projects, that will not be the case. But I heard from many people that keeping their blogging systems up to date requires a lot more work than they would like to do and if they don't, then they're screwed.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That is also true about other languages even though many of them move at a slower speed. A friend recently complained about a blogging system he implemented that requires Ruby 2.0 and that keeping that running sucks.&lt;/p&gt;
    &lt;p&gt;I want a simpler blogging system; one that requires minimal changes over time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Now we talk about Lua&lt;/head&gt;
    &lt;p&gt;Lua is a wonderful and nimble language that is often misunderstood.&lt;/p&gt;
    &lt;p&gt;One characteristic that I love about it, is that is evolves very slowly. Lua 5.1 was introduced in 2006, Lua 5.4 which is the current version initial release was in 2020. Yes, there are point released in between, but you can see how much slower it moves when compared to JS.&lt;/p&gt;
    &lt;p&gt;The differences between Lua 5.1 and Lua 5.4 are minimal when compared with how much other languages changed in the same time period.&lt;/p&gt;
    &lt;p&gt;Lua only requires a C89 compiler to bootstrap itself. It is very easy to make Lua work and even easier to make it interface with something.&lt;/p&gt;
    &lt;p&gt;JS is a lot larger than Lua, there is more to understand and more to remember. My blog needs are very simple and Lua can handle them with ease.&lt;/p&gt;
    &lt;head rend="h2"&gt;How this blog works&lt;/head&gt;
    &lt;p&gt;This is an old-school blog. I uses cgi-bin â aka Comon Gateway Interface â scripts to run it. It is a dynamic website with a SQLite database holding its data. When you open a page, it fetches the data from a database and assembles a HTML to send to the browser using Mustache templates.&lt;/p&gt;
    &lt;p&gt;One process per request. Like the old days.&lt;/p&gt;
    &lt;p&gt;You might argue that if I went with NodeJS, I'd be able to serve more requests using fewer resources. That is true. I don't need to serve that many requests though. My peak access was a couple years ago with 50k visitors on a week, even my old Racket blog could handle that fine. The Lua one should handle it too; and if it breaks it breaks. I'm a flawed human being, my code can be flawed too, we're in this together, holding hands.&lt;/p&gt;
    &lt;p&gt;Your blog is your place to experiment and program how you want it. You can drop the JS fatigue, you can drop your fancy Haskell types, you can just do whatever you find fun and keep going (and that includes JS and Haskell if that's your thing. You do you).&lt;/p&gt;
    &lt;p&gt;Cause I'm using Lua, I don't have as many libraries and frameworks available to me as JS people have, but I still have quite a large collection via Luarocks. I try not to add many dependencies to my blog. At the moment there are about ten and that is mostly because Lua is a batteries-not-included language so you start from a minimal core and build things up to suit your needs.&lt;/p&gt;
    &lt;p&gt;For a lot of things I went with the questionable choice of implementing things myself. I got my own little CGI library. It is 200 lines long and does the bare minimum to make this blog work. I got my own little libraries for many things. Micropub and IndieAuth were all implemented by hand.&lt;/p&gt;
    &lt;p&gt;At the moment I'm &lt;del&gt;despairing&lt;/del&gt;&lt;del&gt;frustrated&lt;/del&gt; having a lot of fun implementing WebMentions. Doing the Microformats2 &lt;del&gt;exorcism&lt;/del&gt; extraction on my own is teaching me a lot of things.&lt;/p&gt;
    &lt;p&gt;What I want to say is that by choosing a small language that moves very slowly and very few dependencies, I can keep all of my blogging system in my head. I can make sure it will run without too much change for the next ten or twenty years.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Lua is a lego set, a toolkit, it adapts to you and your needs. I don't need to keep chasing the new shiny or the latest framework du jour. I can focus on making the features I want and actually understanding how they work.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Instead of installing a single dependency in another language and it pulling a hundred of other small dependencies all of which were transpiled into something the engine understands to the point that understanding how all the pieces work and fit together takes more time than to learn a new language, I decided to keep things simple.&lt;/p&gt;
    &lt;p&gt;I got 29 Luarocks installed here and that is for all my Lua projects in this machine. That is my blog, my game development, my own work scripts for my day job. Not even half of those are for my blog.&lt;/p&gt;
    &lt;p&gt;I often see wisdom in websites such as Hacker News and Lobsters around the idea of "choosing boring" because it is proven, safe, easier to maintain. I think that boring is not necessarily applicable to my case. I don't find Lua boring at all, but all that those blog posts talk about that kind of mindset are all applicable to my own choices here.&lt;/p&gt;
    &lt;p&gt;Next time you're building your own blogging software, consider for a bit for how long do you want to maintain it. I first started blogging on macOS 8 in 2001. I choose badly many times and in the end couldn't keep my content moving forward in time with me as softwares I used or created became impossible to run. The last two changes: from JS to Racket and from Racket to Lua have been a lot safer and I managed to carry all my content forward into increasingly simpler setups and workflows.&lt;/p&gt;
    &lt;p&gt;My blogging system is not becoming more complex over the years, it is becoming smaller, because with each change I select a stack that is more nimble and smaller than the one I had before. I don't think I can go smaller than Lua though.&lt;/p&gt;
    &lt;p&gt;By small I mean:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A language I can fully understand and keep on my head.&lt;/item&gt;
      &lt;item&gt;A language that I know how to build the engine and can do it if needed.&lt;/item&gt;
      &lt;item&gt;An engine that requires very few resources and is easy to interface with third-party libraries in native code.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I chose Lua because of all that, and I'm happy with it and hope this engine will see me through the next ten or so years.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45452261</guid><pubDate>Thu, 02 Oct 2025 16:58:55 +0000</pubDate></item><item><title>Liva AI (YC S25) Is Hiring</title><link>https://www.ycombinator.com/companies/liva-ai/jobs/6xM8JYU-founding-operations-lead</link><description>&lt;doc fingerprint="a3de624f74d97bb3"&gt;
  &lt;main&gt;
    &lt;p&gt;Scale AI for video and voice data.&lt;/p&gt;
    &lt;p&gt;The mission at Liva AI (YC S25) is to make AI feel truly human. AI voices and faces today still feel flat and generic, missing emotion, nuance, and identity. We’re fixing that by building the world’s richest library of human voice and video data, fueling the next generation of realistic AI.&lt;/p&gt;
    &lt;p&gt;We’re hiring an extremely organized and committed operator to take on a full-time role. You’ll manage complex projects and people with efficiency, solve problems in uncertain situations, and help us scale fast. Over time, you’ll also play a key role in building the most automated operations system of any data company, translating the workflows you run today into scalable processes and overseeing the internal systems we’re developing.&lt;/p&gt;
    &lt;p&gt;This is a founding role: your work will directly fuel the next generation of AI in a tangible way, while shaping the foundation of how Liva runs at scale.&lt;/p&gt;
    &lt;p&gt;ABOUT THE ROLE&lt;/p&gt;
    &lt;p&gt;What you’ll do:&lt;/p&gt;
    &lt;p&gt;WHAT WE’RE LOOKING FOR&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;p&gt;Nice to have:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; BENEFITS:&lt;/p&gt;
    &lt;p&gt;Liva's mission is to make AI look and sound truly human. The AI voices and faces today feel off, and lack the capability to reflect diverse people across different ethnicities, races, accents, and career professions. We’re fixing that by building the world’s richest library of human voice and video data, fueling the next generation of realistic AI.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45452299</guid><pubDate>Thu, 02 Oct 2025 17:01:16 +0000</pubDate></item><item><title>Email immutability matters more in a world with AI</title><link>https://www.fastmail.com/blog/not-written-with-ai/</link><description>&lt;doc fingerprint="da5219db73f0ae99"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This blog post was not written with AI&lt;/head&gt;
    &lt;head rend="h2"&gt;Post categories&lt;/head&gt;
    &lt;p&gt;CEO&lt;/p&gt;
    &lt;p&gt;It’s all the rage right now. Everyone is scrambling to put AI into their products. The uncanny valley is shrinking enough that it’s hard to see how much AI was used to write something.&lt;/p&gt;
    &lt;p&gt;This isn’t entirely new, auto-complete on my phone already suggests the most likely word when I’m typing. AI writing tools are an extension of this, but they’re also much more capable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your electronic memory&lt;/head&gt;
    &lt;p&gt;I stand by one of the most important truths about email. It’s not only the largest and most diverse social network, email is your electronic memory.&lt;/p&gt;
    &lt;p&gt;In the novel 1984, the “Ministry of Truth” has a whole massive department which rewrites history. In a world where there’s enough AI capability to process the entire web and rewrite every page to remove something, the cost of “changing history” is much reduced, so we can expect more of it.&lt;/p&gt;
    &lt;p&gt;This is where the immutability of email really shines. An email is your copy, and the sender can’t revise it later. This is frustrating when you’ve sent the wrong thing and have to send a separate correction later, but in the long term it’s insanely valuable.&lt;/p&gt;
    &lt;p&gt;It makes a huge difference to be able to go back and double-check your memory against an email you saw years ago and know that if they disagree, the email is correct. This is already not the case with web pages — they change, and it’s only becoming worse.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adapting to a changing world&lt;/head&gt;
    &lt;p&gt;My son is studying at University now, and he’s one of a few students in his class who refuses to use AI to write his assignments. As he said “what’s the point of paying to be here if I’m not going to build the knowledge and skills for myself, and come out knowing how to do the thing” (near enough… I didn’t write the exact words down in an email, so I’m going off my own fallible memory!) I am so proud of him for having that attitude.&lt;/p&gt;
    &lt;p&gt;I’m also pleased to see that Fastmail’s staff, and many of our customers, are wary of AI tools.&lt;/p&gt;
    &lt;p&gt;But they are that, tools. The world is changing, and we need to adapt and understand it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our service, your data&lt;/head&gt;
    &lt;p&gt;For our service, we want you to be able to do what you desire with your own email, calendars, and contacts. We will continue to build tools and integrations to make that easier.&lt;/p&gt;
    &lt;p&gt;You are welcome to operate your Fastmail account with AI tools, so long as that usage doesn’t otherwise breach our Terms of Service, or degrade the performance of our systems for other customers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our staff, your privacy&lt;/head&gt;
    &lt;p&gt;For our staff, we encourage understanding the tools that exist in the world, and how to use them safely. Our policy makes it clear that any use of tools, including tools with AI in them, must follow clear privacy-preserving principles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data Protection: All data protection, confidentiality, and privacy policies must be followed (our vendors for things like anti-abuse and support are moving towards using AI for translation, categorization, abuse detection – and we are ensuring that their policies continue to provide protection for our customers)&lt;/item&gt;
      &lt;item&gt;Accountability for work: Any AI generated writing or code must be reviewed and understood by a human being, and go through our regular second-set-of-eyes processes before being used&lt;/item&gt;
      &lt;item&gt;Bias awareness: Actively look for biases or hallucinations in AI output&lt;/item&gt;
      &lt;item&gt;Human authority: Always have a path for appeal to a human from any decision that is made by automated tools&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The future&lt;/head&gt;
    &lt;p&gt;Who knows what the future will bring, but we continue to be guided by the principles that we first publicly articulated in 2016 and have held even longer. The data is yours, and we will be good stewards and good internet citizens, helping enable you to use your data in the ways you choose.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453135</guid><pubDate>Thu, 02 Oct 2025 18:00:27 +0000</pubDate></item><item><title>Babel is why I keep blogging with Emacs</title><link>https://entropicthoughts.com/why-stick-to-emacs-blog</link><description>&lt;doc fingerprint="26d8bd04cf2550b8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why I Keep Blogging With Emacs&lt;/head&gt;
    &lt;p&gt;Every time I look at someone’s simple static site generation setup for their blog, I feel a pang of envy. I’m sure I could make a decent blogging engine in 2,000 lines of code, and it would be something I’d understand, be proud over, able to extend, and willing to share with others.&lt;/p&gt;
    &lt;p&gt; Instead, I write these articles in Org mode, and use mostly the standard Org publishing functions to export them to html. This is sometimes brittle, but most annoyingly, I don’t understand it. I have been asked for details on how my publishing flow works, but the truth is I have no idea what happens when I run the &lt;code&gt;org-publish-current-file&lt;/code&gt; command.
&lt;/p&gt;
    &lt;p&gt; I could find out by tracing the evaluation of the Lisp code that runs on export, but I won’t, because just the html exporting code (&lt;code&gt;ox-html.el&lt;/code&gt;) is 5,000
lines of complexity. The general exporting framework (&lt;code&gt;ox-publish.el&lt;/code&gt; and
&lt;code&gt;ox.el&lt;/code&gt;) is 8,000 lines. The framework depends on Org parsing code
(&lt;code&gt;org-element.el&lt;/code&gt;) which is at least another 9,000 lines. This is over 20,000
lines of complexity I’d need to contend with.
&lt;/p&gt;
    &lt;p&gt;It might seem like a no-brainer to just write that 2,000 line custom static generator and use that instead.&lt;/p&gt;
    &lt;p&gt;Except one thing: Babel.&lt;/p&gt;
    &lt;p&gt;Any lightweight markup format (like Markdown or ReStructuredText or whatever) allows for embedding code blocks, but Org, through Babel, can run that code on export, and then display the output in the published document, even when the output is a table or an image. It supports sessions that lets code reuse definitions from earlier code blocks. It allows for injecting variables from the markup into the code, and vice versa. As a bonus, Org doesn’t require a JavaScript syntax highlighter, because it generates inline styles in the source code.&lt;/p&gt;
    &lt;p&gt;It does this for a large number of languages, although I mainly use it with R for drawing plots. Being able to do this is incredibly convenient, because it makes it trivial to draft data, illustrations, and text at the same time, adjusting both until the article coheres. Having tried it, I cannot see myself living without it.&lt;/p&gt;
    &lt;p&gt;A simple 2,000 line blogging engine would be a fun weekend project. Mirroring the features of Babel I use would turn it into a multi-month endeavour for someone with limited time such as myself. Not going to happen, and I will continue to beat myself up for overcomplicating my publishing workflow.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453222</guid><pubDate>Thu, 02 Oct 2025 18:06:41 +0000</pubDate></item><item><title>The Answer (1954)</title><link>https://sfshortstories.com/?p=5983</link><description>&lt;doc fingerprint="73199b7271208fff"&gt;
  &lt;main&gt;
    &lt;p&gt;The Answer by Fredric Brown (Angels and Spaceships, 1954) opens with a scientist called Dwar Ev completing a connection and then moving towards a switch:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The switch that would connect, all at once, all of the monster computing machines of all the populated planets in the universe—ninety-six billion planets—into the supercircuit that would connect them all into one super-calculator, one cybernetics machine that would combine all the knowledge of all the galaxies. p. 36&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Ev then asks the super-computer if there is a God, and it replies (spoiler), “Yes, now there is a God”. Then, when Ev rushes towards the switch to turn the computer off, it zaps him with a lightning bolt.&lt;lb/&gt;This is one of these squibs (it is less than a page long) that you find (a) pretty neat when you are twelve, but (b) a not very good gimmick story when older. The real sense of wonder here lies in the idea of ninety-six billion inhabited and interconnected planets.&lt;lb/&gt;* (Mediocre). 250 words. Story link.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453299</guid><pubDate>Thu, 02 Oct 2025 18:13:24 +0000</pubDate></item><item><title>Gemini 3.0 Pro – early tests</title><link>https://twitter.com/chetaslua/status/1973694615518880236</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45453448</guid><pubDate>Thu, 02 Oct 2025 18:26:57 +0000</pubDate></item><item><title>Drone No-Fly Zone Imposed over Greater Chicago Area</title><link>https://www.twz.com/air/massive-drone-no-fly-zone-imposed-over-greater-chicago-area</link><description>&lt;doc fingerprint="706945d040859022"&gt;
  &lt;main&gt;
    &lt;p&gt;With a large-scale ICE operation now underway in the Chicago area, the Federal Aviation Administration (FAA) has enacted a uniquely massive 15-mile radius prohibition against drone flights. The FAA told us the temporary flight restriction (TFR) for drones in this area was requested by the Department of Homeland Security (DHS). The no-fly zone lasts through Oct. 12.&lt;/p&gt;
    &lt;p&gt;Under this restriction, only drones operated in support of national defense, homeland security, law enforcement, search and rescue and other emergency response efforts, or commercially used drones with a valid statement of work are allowed to fly. In addition, media organizations can apply for an approved special governmental interest airspace waiver. Any drones violating this restriction can be seized or destroyed, the TFR explains. It also extends about 15 miles into Lake Michigan, without any explanation.&lt;/p&gt;
    &lt;p&gt;There have been no reports that drones have created major problems for federal agents. However, having uncrewed aerial vehicles flying during an ongoing operation like the one taking place in the Chicago area clearly raises concerns about operational security as well as the safety of helicopters and other aviation assets flying in support of it. Meanwhile, despite the possibility of waivers for commercial and journalistic purposes, the restriction is also drawing the ire of commercial drone operators and sparking worries about civil liberties violations.&lt;/p&gt;
    &lt;p&gt;The move comes as the Trump administration has followed through on its vow to bring federal forces into the nation’s third-largest city. Hundreds of federal agents have poured into the region. On Tuesday, President Donald Trump suggested responding to protests in Chicago and elsewhere would be a good way to prepare troops for combat.&lt;/p&gt;
    &lt;p&gt;“…we should use some of these dangerous cities as training grounds for our military – National Guard – but military, because we’re going into Chicago very soon,” Trump told a room full of admirals and generals gathered at Marine Base Quantico.&lt;/p&gt;
    &lt;p&gt;In response to these actions, hundreds of people have taken to the streets in downtown Chicago. They are protesting Immigration and Customs Enforcement (ICE) immigration arrests and Trump’s promised federal troop deployment. One hundred National Guard troops are being deployed to Illinois to protect federal facilities.&lt;/p&gt;
    &lt;p&gt;Early on Tuesday, about 300 agents from various federal organizations, “using drones, helicopters, trucks and dozens of vehicles, conducted a middle-of-the-night raid on a rundown apartment building on the South Side of Chicago, leaving the building mostly empty of residents by morning and neighbors stunned,” The New York Times reported. Sources said the raid targeted the Tren de Aragua cartel, which the Trump administration has declared a narco terrorist organization.&lt;/p&gt;
    &lt;p&gt;Federal officials say they have made nearly 1,000 arrests for immigration violations in what has been dubbed Operation Midway Blitz, according to the DHS.&lt;/p&gt;
    &lt;p&gt;In addition, many of the protests have been aimed at a federal facility in suburban Broadview, located about 10 miles west of Chicago. The facility is being used to detain hundreds of people arrested on immigration violations. At least five people have been arrested amid clashes between protesters and agents in which chemical agents have been deployed to disperse crowds.&lt;/p&gt;
    &lt;p&gt;Issuing TFRs for emerging security concerns is not uncommon. However, the area this one covers is unusually large. TFRs are more commonly much more focused geographically.&lt;/p&gt;
    &lt;p&gt;For instance, a previous TFR was imposed over the Broadview facility. There is also one that is active over the federal facility in Portland, Oregon, which is a hotpoint for protests, that is one mile in radius.&lt;/p&gt;
    &lt;p&gt;Last year, for example, dozens of drone no-fly zones were created in the New Jersey area following thousands of reported mystery drone sightings, most of which proved to be unfounded. However, unlike the Chicago-area TFR, those were imposed on a localized level, mostly over power infrastructure sites. The vast majority only covered a one-mile radius of airspace. The TFR imposed over the Picatinny Arsenal was an outlier with a three-mile radius, a fraction of the area covered by the Chicago restrictions.&lt;/p&gt;
    &lt;p&gt;Not surprisingly, the local drone industry, which relies on flying the skies of Chicago to conduct business, is not happy with the restriction.&lt;/p&gt;
    &lt;p&gt;“The airspace closure affects Chicago’s substantial commercial drone industry, including real estate photographers, construction inspectors, and infrastructure surveyors who rely on drones for daily operations,” wrote Haye Kesteloo, Editor in Chief of two drone tech publications: DroneXL.co and EVXL.co. “Part 107 commercial pilots cannot work in the restricted airspace, while recreational pilots face the same grounding through mid-October.”&lt;/p&gt;
    &lt;p&gt;The restriction “represents one of the most expansive non-emergency TFRs affecting civilian drone operations in a major U.S. city, comparable to airspace closures during major events like the Super Bowl but lasting significantly longer,” he added.&lt;/p&gt;
    &lt;p&gt;“There’s zero legitimate security reason for this TFR,” Charles Black, a Chicago resident who writes software, complained on X.&lt;/p&gt;
    &lt;p&gt;Despite the ability of news organizations to apply for a waiver to fly drones, there are also concerns that the TFR is infringing on the Constitutional right of people to observe the actions taking place on the ground.&lt;/p&gt;
    &lt;p&gt;“The Chicago TFR is the exact scenario First Amendment advocates warned about: government using airspace restrictions to prevent documentation of controversial operations in public spaces,” Kesteloo, who is also a drone journalist, told us. “Combined with the 5th Circuit’s ruling that drone operation isn’t expressive conduct, we’re seeing the emergence of a legal framework where federal agencies can effectively control visual journalism by controlling airspace.”&lt;/p&gt;
    &lt;p&gt;We have asked DHS, ICE and Customs and Border Protection (CBP) for more details about why they sought this large airspace closure and will update this story with any pertinent information provided.&lt;/p&gt;
    &lt;p&gt;Contact the author: howard@thewarzone.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45454152</guid><pubDate>Thu, 02 Oct 2025 19:19:29 +0000</pubDate></item><item><title>Why most product planning is bad and what to do about it</title><link>https://blog.railway.com/p/product-planning-improvement</link><description>&lt;doc fingerprint="ad29913538a777e8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why most product planning is bad and what to do about it&lt;/head&gt;
    &lt;p&gt;TL;DR: We tried OKRs, they created more ceremony than clarity. Our solution: Problem Driven Development, a 4-day quarterly process focused on identifying problems (not solutions), prioritizing as a team, and committing publicly. It's kept us shipping at velocity even as we've scaled to 1.7M+ users.&lt;/p&gt;
    &lt;p&gt;For most of my friends and colleagues at mature software companies, there are usually three ways for an item of work to get put on the board to eventually be done.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;There is a giant ceremony that determines what gets done by a grab bag of metrics.&lt;/item&gt;
      &lt;item&gt;A deal gets blocked by a missing feature, and the engineering team scrambles jets to eliminate the blocker.&lt;/item&gt;
      &lt;item&gt;Founder feels like we have to build something.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thats not to say that every company is a disorganized mess or a bureaucratic hell scape but, I have never met any engineer who said: “Wow, I just love my company planning process.”&lt;/p&gt;
    &lt;p&gt;These words are seldom spoken in the english language.&lt;/p&gt;
    &lt;p&gt;Railway, was fast approaching 1. and 2. at the same time.&lt;/p&gt;
    &lt;p&gt;Despite us using excellent tools (shoutout Linear) planning is much as a cultural phenomena as well as an interesting engineering problem. From our perspective, we would finalize the features and the requirements what we would want to build once every 3 months and then at times get blindsided every now and then from a new business priority or an incident. Especially now serving 1.7M+ customers, we have to aggregate our taste, feedback, and opportunities and make a roadmap that will get us everything we ever wanted.&lt;/p&gt;
    &lt;p&gt;This was an issue.&lt;/p&gt;
    &lt;p&gt;Why write about something that you would read in Rand’s? It’s Q4 for those who celebrate, and we felt that reigniting the agile vs. waterfall armistice needed to be torn up. (Besides, we’re blogging like it’s 2005.)&lt;/p&gt;
    &lt;p&gt;…and we have spent the better part of 18 months to improve our planning so that we finally get to a process that is not bad, and if you’d like, you can steal it, so that you can deliver excellent products.&lt;/p&gt;
    &lt;p&gt;Mike Tyson asked about his fight plan against Evander Holyfield responded with; “Everyone has a plan until they get punched in the mouth.”&lt;/p&gt;
    &lt;p&gt;And that quote ever since would be used out of context to fight the implementation of Objectives, Key Results across the world. At Railway, we aren’t Anti-Planning, we are Anti-Bad-Planning and as such, we used to avoid a lot of it because we always felt that a bad process is massively negative to the status quo. However, we did get to the point where we needed to finally harness the collective attention spans of the company and work towards a goal.&lt;/p&gt;
    &lt;p&gt;Railway at the time was (and is) a vision led company. We fundamentally believe that most infrastructure boilerplate shouldn’t exist so that you can work on what matters. Back in ‘21, we went from a world where we could do our work in weeks, to work that required us to think in months. I implemented a somewhat lighter form of SAFE which splits work between “sprints”, which were short pieces of work, then initiatives which helped grouped “epics” which are long running work-streams to deliver a feature.&lt;/p&gt;
    &lt;p&gt;Despite us having a strong founder, we still wanted the ability to have employees bring projects that they would be excited to work on.&lt;/p&gt;
    &lt;p&gt;As with most young companies, we found quickly that new work would come in and unseat the old work and we would begin anew.&lt;/p&gt;
    &lt;p&gt;So then we turned to OKRs.&lt;/p&gt;
    &lt;p&gt;We thought that having some long term objectives will help us focus the company.&lt;/p&gt;
    &lt;p&gt;We implemented them faithfully, we all read the John Doer book. …and it worked… somewhat.&lt;/p&gt;
    &lt;p&gt;We can make this a whole blog post on OKRs. But, trying to hold back here from just ranting and to give you the relevant information.&lt;/p&gt;
    &lt;p&gt;OKRs work REALLY well when you have concrete goals and really simple ways to measure them. Its why they belong at their primary birthplace, the factory. Objectives work when you have something binary or “limited”, like a product existing… or not. Or a product meeting a benchmark… or not. It’s easy to rally a team around them and conquer the world.&lt;/p&gt;
    &lt;p&gt;Where OKRs start to falter, are when you need to use them to prioritize work to meet a “unlimited” objective. Like: “Increase conversion rate of landing page”&lt;/p&gt;
    &lt;p&gt;OKRheads will notice that’s a particularly weak objective, however, teaching an entire organization to all of a sudden be great goal setters, which is what OKRs require you to do made it difficult for engineers to 1) bring them and then 2) plan work around them. Which leads to the first big issue of OKRs- which is that it really depends on the human psychology of the team. Engineers straddle the line between concrete numbers such as uptime, and more creative endeavors such as figuring out how to get a feature implemented right. Whenever the work enters the creative realm, the wheels come off.&lt;/p&gt;
    &lt;p&gt;Which is why outside of the factory, OKRs are great for Sales. You set a number, you get alignment on hitting that number, and then you pick up the phone, email, or LinkedIn until you hit it. The psychology of an account executive matches the planning process.&lt;/p&gt;
    &lt;p&gt;Where OKRs work great, is for alignment. If you are able to set great goals and if you wanted to prescribe a bunch of work toward said goal. …and most startups reach for the High Output Management book (which I love) because there is the lack of alignment between different orgs at a startup. Which is to say, I don’t shame any company reaching for them, the same way newborns have the palmar grasp reflex. We yearn for the alignment.&lt;/p&gt;
    &lt;p&gt;The second big issue for OKRs is that they are, by design, inflexible, once you commit to them. If you spend a week or two planning for them for the year or quarter, and you are midway through realizing that the “KR” part is incorrect, thats a one way door.&lt;/p&gt;
    &lt;p&gt;So we felt pressure into making sure that the OKRs were indeed correct. Which is where you enter the issues of the performative aspects of planning. It felt “mature” for Railway to be having these discussions, even though… it wasn’t productive.&lt;/p&gt;
    &lt;p&gt;At Railway, we would spend a significant amount of time discussing what is a valid OKR, only for us to realize we were two days into the planning process and we’ve yet to decide what we should build for the OKR.&lt;/p&gt;
    &lt;p&gt;This is when Christian, our Head of Operations had an answer for us.&lt;/p&gt;
    &lt;p&gt;Instead of crowd sourcing the OKRs from the company and bubbling them up per function Jake and Christian would work on some top level guidance that would provide a macro view of our finances, priorities, and strategy that we have to keep in mind.&lt;/p&gt;
    &lt;p&gt;We then have a Hex dashboard with topline metrics from different sides of the product and business. From a GTM perspective, it’s engagement, signups, and revenue. From an Engineering perspective, it’s support tickets, uptime, and feature performance.&lt;/p&gt;
    &lt;p&gt;This data would give us a “theme”. (You may have seen them in our launch weeks.)&lt;/p&gt;
    &lt;p&gt;Whenever someone goes on Central Station and requests a feature, depending on how large the work is, that usually determines if that request becomes a “Project”.&lt;/p&gt;
    &lt;p&gt;Then around the start of 2024, Christian spun up a Notion DB, and punched in a simple template for each new entry.&lt;/p&gt;
    &lt;p&gt;Railway at this point and time was organized into three sections of the business. Product Engineering which delivers features and value to our customers via the UI and terminal. Platform which supports the product with the Infrastructure and the APIs to control that Infrastructure. Then Logistics, which is a synthetic team which includes the Support, Marketing, and Sales function working to be the voice of the customer.&lt;/p&gt;
    &lt;p&gt;We would then fill out a “Project Candidate” and then go to bat on trying to figure out if was a priority that tied to the “theme” that was presented.&lt;/p&gt;
    &lt;p&gt;If something is a P0: it’s an existential company risk, we MUST deliver it.&lt;/p&gt;
    &lt;p&gt;Then, a P1: something we need to deliver for this quarter&lt;/p&gt;
    &lt;p&gt;Lastly, P2: a nice to have.&lt;/p&gt;
    &lt;p&gt;This worked well until some cracks formed.&lt;/p&gt;
    &lt;p&gt;At the start of this new process, our team was small enough to list maybe 50 project candidates, discuss as a group and then be on with it but with 200. Spending all day on a call with your co-workers to discuss if something was worth it isn’t the best use of time.&lt;/p&gt;
    &lt;p&gt;Second, Project Candidates would sometimes have full on fleshed out RFC style sections where the author would have a solution ready to go. When a candidate got deprioritized, it’s understandable why someone would feel not great about their hard effort not being reciprocated.&lt;/p&gt;
    &lt;p&gt;Third, to deal with the larger amount of project candidates and additional sources of project candidates, we would set up additional meetings before the planning week that would eat into our engineering cycles. Not that we want to squeeze every engineer for what they are worth, but these negotiation calls would take longer than the main call sometimes. Worse even, was that we would completely reset the board quarter after quarter.&lt;/p&gt;
    &lt;p&gt;Lastly, which was starting to be a bigger issue, was that having Project Candidates being a bottom up process was great when the people who knew that work they needed to do can fit the whole work-stream in their head. But… we were about to embark on a multi-quarter effort that would span all parts of the business to deliver Railway Metal. This system was not great for uncovering planning gaps that required attention from a different team.&lt;/p&gt;
    &lt;p&gt;(With that said, we did ship some really good software thanks to this planning process.)&lt;/p&gt;
    &lt;p&gt;However, Jake, the team, aren’t ones to sit on their laurels so we refined it until we have our current process.&lt;/p&gt;
    &lt;p&gt;So we flipped the project system on it’s head.&lt;/p&gt;
    &lt;p&gt;Instead, we practice what I call: “Problem Driven Development”&lt;/p&gt;
    &lt;p&gt;Rather than spending a loaded amount of time picking feedback items from our feedback systems, and spending a loaded amount of time on trying to flesh out requirements. We collect problems on a continuous process that begins in earnest on the Friday before our planning week.&lt;/p&gt;
    &lt;p&gt;We stopped asking people to propose solutions and started asking them to articulate problems. No more half-baked RFCs that people felt attached to. No more solution-first thinking that locked us into approaches before we understood what we were solving. Just clear problem statements.&lt;/p&gt;
    &lt;p&gt;Here's what a problem looks like in our system:&lt;/p&gt;
    &lt;p&gt;Problem Title: "Users can't debug failed deployments without SSHing into containers"&lt;/p&gt;
    &lt;p&gt;Notice what's missing? Any mention of how we'd solve it. That comes later, after we've committed to solving the problem. Then we turned the two week ceremony into a lightning 4 day sprint.&lt;/p&gt;
    &lt;p&gt;After the problems are listed and filled out fully, each team on Day 1 enters problems on their own. If a problem isn't fleshed out, it's put in the Parking Lot and we kindly nudge the idea person to fill the template.&lt;/p&gt;
    &lt;p&gt;By the time we get into a room together, everyone's had time to process what's on the board.&lt;/p&gt;
    &lt;p&gt;Then on Day 2, the planning captain will have a closed session with the team (with spectators from other teams) to put a best guess priority from P0, P1, and P2. If we find that a problem is contentious or requires an external dependency, we mark it so the day after, we can discuss it.&lt;/p&gt;
    &lt;p&gt;By having each team prioritize independently first, we avoid the tragedy of the commons where everything becomes P0 because someone shouted loudest on the call. Platform can look at their problems and say "yes, API reliability is more important than API versioning right now" without having to negotiate with Product in real-time.&lt;/p&gt;
    &lt;p&gt;(We have spent countless hours talking about difficult questions on a call when we have been able to diffuse rough conversations ahead of time.)&lt;/p&gt;
    &lt;p&gt;On Day 3, we get to a 95 percent certainty on the priorities listed and tie break the dependencies. We also confirm that we have the capacity and staffing to deal with the problems listed on the board. If not, we add a problem for hiring for a specific role.&lt;/p&gt;
    &lt;p&gt;This is where cross-team dependencies surface naturally. When Platform marks "Multi-mount volumes" as P1 and Product marks "HA DBs" as P1, we can see that one blocks the other. We're not discovering this mid-quarter when someone's already three weeks into building the wrong thing.&lt;/p&gt;
    &lt;p&gt;The capacity check is crucial too. We look at people's current commitments, oncall rotations, and whether anyone's about to take parental leave. If we have eight P1 problems and capacity for five, we have an honest conversation about what drops to P2 or what we need to hire for.&lt;/p&gt;
    &lt;p&gt;Then lastly, Day 4, commitment. Everyone looks at the whole list, mention final objections if we should be working on something that we aren't— or vice versa. After we confirm priorities, we assign problems to people by giving them a DRI (Directly Responsible Individual).&lt;/p&gt;
    &lt;p&gt;Then, we look into each other's eyes, as much as anyone can on a video call, and commit.&lt;/p&gt;
    &lt;p&gt;After the starting gun is off, we then write RFCs on problems to see how we can best solve them which eventually become Linear tickets.&lt;/p&gt;
    &lt;p&gt;We're working on open-sourcing our Notion templates because honestly, this isn't rocket science. It's just:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cataloguing your problems, not solutions&lt;/item&gt;
      &lt;item&gt;Let teams prioritize independently before negotiating&lt;/item&gt;
      &lt;item&gt;Front-load the hard conversations&lt;/item&gt;
      &lt;item&gt;Commit publicly&lt;/item&gt;
      &lt;item&gt;Then, and only then, figure out how to solve it&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the real hard part was building a culture where people feel safe bringing up problems without having to defend their pet solutions.&lt;/p&gt;
    &lt;p&gt;Keep process to a minimum, focus on performance of shipping, not performative work.&lt;/p&gt;
    &lt;p&gt;And like any good product, our planning process will keep evolving. Maybe in six months we'll realize Problem Driven Development has its own cracks. Theres always plenty to do on planning and I think we generally know how it’ll need to evolve, but we try to take small steps from cycle to cycle. If that happens, we'll write another blog post about what we learned and what we changed.&lt;/p&gt;
    &lt;p&gt;Until then, we're shipping.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45454374</guid><pubDate>Thu, 02 Oct 2025 19:34:08 +0000</pubDate></item><item><title>Anti-aging breakthrough: Stem cells reverse signs of aging in monkeys</title><link>https://www.nad.com/news/anti-aging-breakthrough-stem-cells-reverse-signs-of-aging-in-monkeys</link><description>&lt;doc fingerprint="3648d18e38a2aa99"&gt;
  &lt;main&gt;
    &lt;p&gt;Key Points:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Super stem cells” improve the memory of monkeys while protecting against neurodegeneration.&lt;/item&gt;
      &lt;item&gt;The super stem cells prevent age-related bone loss while rejuvenating over 50% of the 61 tissues analyzed.&lt;/item&gt;
      &lt;item&gt;Treatment with stem cells reduces inflammation and senescent cells (cells that accumulate to promote aging).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While small in number, our adult stem cells play a crucial role in regenerating our lost or damaged tissues, rebuilding our body cell by cell. However, with age, our bodies become riddled with inflammation, hardly providing an environment capable of keeping our stem cells healthy. Eventually, our stem cells lose their regenerative capacity, contributing to degenerative aging.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fox, O, Three&lt;/head&gt;
    &lt;p&gt;Hydra are a genus of immortal beings that live forever in freshwater environments like lakes and ponds. What scientists have called “nothing but an active stem cell community,” Hydra can escape death by infinitely regenerating. Their stem cells can continuously proliferate and renew by producing FoxO, a protein they share with humans.&lt;/p&gt;
    &lt;p&gt;In humans, the FoxO protein, specifically the FoxO3 isoform, responds to cellular stress by binding to DNA and turning genes on and off. These genes are involved in numerous cellular processes that promote healthy aging and extended lifespan. This is why the FoxO3 protein’s corresponding gene, FOXO3, is considered a longevity gene.&lt;/p&gt;
    &lt;head rend="h2"&gt;Experimenting with SRCs&lt;/head&gt;
    &lt;p&gt;As FoxO3 assists cells in resisting stressful environments, such as inflamed tissue, Chinese Academy of Sciences researchers engineered human stem cells to have enhanced FoxO3 activity. As published in Cell, these senescence-resistant stem cells (SRCs) were designed to exhibit greater resistance to age-related stress. To test this, cynomolgus monkeys, also known as crab-eating macaques, were first stratified into four groups based on age:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A1: 3-5 years (approximately equivalent to 9-15 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A2: 10-12 years (approximately equivalent to 30-36 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A3: 16-19 years (approximately equivalent to 48-57 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A4: 19-23 years (approximately equivalent to 57-69 human years)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The oldest of the monkeys, the A4 group, were the focus of the study and were subdivided into three groups. One group was injected with saline (salt and water), another group with normal stem cells, and the third with SRCs. The aged monkeys were injected every two weeks for 44 weeks, approximately equivalent to the duration of three human years. On safety, there were no serious adverse events, such as immune system rejection or tumor growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;SRCs Improve Cognition&lt;/head&gt;
    &lt;p&gt;After 44 weeks of biweekly injections, a suite of biological indices was measured from the aged monkeys to assess whether SRCs slow down biological aging. One of these indices was memory retention. To assess memory, the researchers conducted a common experiment called the Wisconsin General Test Apparatus (WGTA).&lt;/p&gt;
    &lt;p&gt;For this experiment, each monkey was trained to retrieve food located outside of one of two identical boxes. During the test session, after each monkey was trained, food was placed next to one of the boxes to keep it hidden. Subsequently, a flap was placed in front of the monkey to block the boxes from view. Three seconds later, when the flap was reopened, each monkey had to remember which of the two boxes contained the food.&lt;/p&gt;
    &lt;p&gt;Remarkably, the monkeys injected with SRCs remembered the location of the food with higher accuracy than the monkeys injected with saline. Moreover, the monkeys injected with normal stem cells exhibited the same level of accuracy as the monkeys injected with saline. These findings suggest that SRCs, and not normal stem cells, improve the memory of aged monkeys.&lt;/p&gt;
    &lt;p&gt;Furthermore, MRI-based structural analysis showed that treatment with SRCs mitigated age-related brain shrinkage. MRI-based experiments also revealed that brain connectivity was restored to that of young (A1 group) monkeys. Namely, the structural connectivity between seven brain regions, including those important for working memory (prefrontal cortex), was rejuvenated with SRC treatment. Overall, these findings suggest that SRC injections improve memory by protecting against neurodegeneration.&lt;/p&gt;
    &lt;head rend="h2"&gt;SRCs Rejuvenate Many Organs and Tissues&lt;/head&gt;
    &lt;p&gt;In addition to the brain, the Academy researchers found that SRC treatment rejuvenated multiple organs and tissues. This is important because the rejuvenation of a given organ or tissue could lead to the reduced risk of its corresponding age-related chronic diseases. For example, the rejuvenating effects of SRCs on the brain could reduce the risk of neurodegenerative disorders like Alzheimer’s and Parkinson’s diseases.&lt;/p&gt;
    &lt;p&gt;One common age-related disease is osteoporosis, characterized by brittle and weak bones that make patients more prone to fractures and deadly falls. Using an X-ray imaging technique called micro-CT, the researchers found evidence for the reversal of age-related bone loss. Namely, while the aged monkeys treated with saline exhibited dental bone loss, the aged monkeys treated with SRCs had teeth more similar to those of young monkeys.&lt;/p&gt;
    &lt;p&gt;To conduct a body-wide assessment, the researchers measured the up- and down-regulation of genes from 10 systems and 61 tissues. Elevations and reductions in gene activation reflect the function (or dysfunction) of cells, tissues, and organ systems. With that said, SRC treatment was shown to rejuvenate over 50% of the tissues examined, with maximal rejuvenation achieved in areas like the hippocampus (the memory consolidation center of the brain), fallopian tubes, and colon. In contrast, the regular stem cells rejuvenated about 30% of the tissues examined.&lt;/p&gt;
    &lt;p&gt;Confirming some of the rejuvenating effects inferred by the gene experiments, the researchers also observed structural changes to various organs and tissues from aged monkeys treated with SRCs. For example, the vascularity of the lung and heart was improved while the thickening of the aorta was reduced. Neurons had longer projections and fewer proteins associated with Alzheimer’s disease (e.g., beta-amyloid and phosphorylated-tau), and the kidney and brain showed less mineralization [abnormal mineral (usually calcium) deposits].&lt;/p&gt;
    &lt;head rend="h2"&gt;SRCs Reduce Cellular Senescence and Inflammation&lt;/head&gt;
    &lt;p&gt;Modern scientists have begun to unravel the underlying causes of aging by identifying commonalities between age-related diseases at the cellular level. Two of the most prominent purported underlying causes of aging are chronic inflammation and senescent cells. With age, senescent cells accumulate throughout the body, promoting inflammation by secreting pro-inflammatory molecules.&lt;/p&gt;
    &lt;p&gt;Eliminating senescent cells, which can be achieved through genetic manipulation or compounds called senolytics, ameliorates age-related diseases and even extends the lifespan of model organisms. Now, the Academy researchers demonstrate that SRCs reduce senescent cells, measured using a blue dye called SA-β-Gal, in multiple organs, including the brain, heart, and lungs. Along those lines, SRC treatment also reduced markers of inflammation and other underlying causes of aging, like DNA damage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Stem Cells Make Sense&lt;/head&gt;
    &lt;p&gt;When it comes to combating degenerative aging, it makes sense that regenerative stem cells are a promising solution. In fact, one of the underlying causes of aging is stem cell exhaustion, whereby stem cells lose their regenerative capacity. While normal stem cells have anti-aging effects, as shown by the Chinese Academy of Sciences researchers, they are not protected against stressors like age-related inflammation. This explains why SRCs provide enhanced regenerative capacity (they withstand the harsh microenvironments induced by aging and cellular senescence).&lt;/p&gt;
    &lt;p&gt;As no serious safety concerns were raised during the study, it would seem that SRCs are well tolerated. However, the long-term effects of the SRC treatment will need further evaluation. The primary concern with injecting stem cells into the bloodstream is that they can trigger the spread of cancer almost anywhere in the body. Nevertheless, the SRCs possess tumor suppression properties, suggesting they may not induce tumor growth. If this ends up being true, we may soon see SRCs being tested in humans.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45454460</guid><pubDate>Thu, 02 Oct 2025 19:39:08 +0000</pubDate></item><item><title>Launching Solveit – an antidote to AI fatigue</title><link>https://www.answer.ai/posts/2025-10-01-solveit-full.html</link><description>&lt;doc fingerprint="29342a7f4be50e99"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Launching Solveit, the antidote to AI fatigue&lt;/head&gt;
    &lt;p&gt;It’s a strange time to be a programmer. It’s easier than ever to get started, but also easier than ever to let AI steer you into a situation where you’re overwhelmed by code you don’t understand. We’ve got an antidote that we’ve been using ourselves with 1000 preview users for the last year. It’s changed our lives at Answer.AI, and hundreds of our users say the same thing. Now we’re ready to share it with you. Signups are open, and will remain so until October 20th. Over five weeks, we’ll give you a taste of how our new approach and platform, “Solveit”, can be applied to everything from programming challenges, web development, and system administration to learning, writing, business, and more.&lt;/p&gt;
    &lt;p&gt;OK, let’s explain what on earth we’re talking about!…&lt;/p&gt;
    &lt;p&gt;At the end of last year, Jeremy Howard (co-founder of fast.ai, Answer.AI, Kaggle, Fastmail, creator of the first LLM…) and I ran a small trial course titled “How To Solve It With Code”. The response was so overwhelming that we had to close signups after just one day. 1000 keen beans joined us for a deep dive into our general approach to solving problems. The first few lessons were taught via the vehicle of the ‘Advent of Code’ programming challenges and run in a new, purpose-built tool called solveit. As the course progressed, we had lots of fun exploring web development, AI, business, writing and more. And the solveit tool became an extremely useful test-bed for ideas around AI-assisted coding, learning and exploration.&lt;/p&gt;
    &lt;p&gt;In the year since, we’ve continued to refine and expand both the process and the platform. We now basically live in the solveit platform. We do all our sysadmin work in it (Solveit itself is hosted on a new horizontally scalable multi-server platform we built and run entirely using Solveit!), host production apps in it (e.g all students in the course can use a Discord AI bot “Discord Buddy” that’s running inside a Solveit dialog!), develop most of our software in it, our legal team does contract drafting in it, we iterate on GUIs in it, and in fact we do the vast majority of our day to day work of all kinds in it.&lt;/p&gt;
    &lt;p&gt;From October 20th for five weeks, Jeremy and I will show you how to use the solveit approach, and give you full access to the platform that powers it (and you’ll have the option to continute to access the lessons and platform afterwards too). Also Eric Ries will join us for lessons about building startups that don’t just make money, but that stick to your vision for how you want to impact the world. You’ll be amongst the first people in the world to have the opportunity to read his new unreleased book.&lt;/p&gt;
    &lt;p&gt;But what IS “the solveit approach”? It isn’t some new AI thing, but actually is based on ideas that are at least 80 years old…&lt;/p&gt;
    &lt;head rend="h2"&gt;Inspiration from Polya&lt;/head&gt;
    &lt;p&gt;George Polya was a Hungarian mathematician who wrote the influential book “How to Solve It” in 1945. In it, he shares his philosophies on education (focus on active learning, heuristic thinking, and careful questioning to guide students towards discovering answers for themselves) and outlines a four-step problem-solving framework:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Understand the Problem: identify what you’re being asked to do; restate the problem&lt;/item&gt;
      &lt;item&gt;Devise a Plan: draw on similar problems; break down into manageable parts; consider working backward; simplify the problem&lt;/item&gt;
      &lt;item&gt;Carry Out the Plan: verify each step&lt;/item&gt;
      &lt;item&gt;Look Back and Reflect: consider alternatives; extract lessons learned&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;He was focused on mathematics, but as Jeremy and I realized, these ideas translate far beyond maths! It turns out that it actually works great for coding, writing, reading, learning…&lt;/p&gt;
    &lt;p&gt;Of course, you can often just have AI code and write for you. But should you?&lt;/p&gt;
    &lt;p&gt;In most cases, we argue the answer is “no”.&lt;/p&gt;
    &lt;p&gt;There’s a myriad of problems waiting for you if you go down that path: - If you didn’t know the foundations of how to do it before, you don’t now either. You’ve learned nothing - If you keep working this way, you build up more and more code you don’t understand, creating technical and understanding debt that will eventually become crippling - You won’t be building up a foundation to solve harder tasks that neither humans nor AI can one-shot. So you’re limiting yourself to only solving problems that everyone else can trivially solve too. This is not a recipe for personal or organizational success!&lt;/p&gt;
    &lt;p&gt;On the other hand, if you build a discipline of always working to improve your understanding and expertise, you’ll discover that something delightful and amazing happens. Each time you tackle a task, you’ll find it’s a little easier than the last one. These improvements in understanding and capability will multiply, and you’ll find that your own skills develop even faster than AI improves. You’ll focus on using AI to help you dramatically increase your own productivity and abilities, instead of focusing on helping the AI improve its productivity and abilities!&lt;/p&gt;
    &lt;head rend="h2"&gt;Application to Coding: iterative, exploratory coding in notebook-like environments.&lt;/head&gt;
    &lt;p&gt;Let’s consider a quick example of coding the solveit way (without even any AI yet). For 2024’s Advent of Code, Day 1’s solution involves comparing two lists, sorted by value (there’s a whole backstory involving elves, which you can read if you like). Let’s imagine we’ve considered the problem, and are now focused on a small sub-task: extracting the first (sorted) list. We start with the sample data provided:&lt;/p&gt;
    &lt;code&gt;= '3   4\n4   3\n2   5\n1   3\n3   9\n3   3' x &lt;/code&gt;
    &lt;p&gt;Our plan might be:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Split into a list of lines&lt;/item&gt;
      &lt;item&gt;Grab the first number from each line&lt;/item&gt;
      &lt;item&gt;Sort&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;After thinking through the plan, we begin working on individual steps. We aim to write no more than a few lines of code at a time, with each piece giving some useful output that you can use to verify that you’re on the right track:&lt;/p&gt;
    &lt;code&gt;= x.splitlines()
 lines 
 lines&amp;gt;&amp;gt;&amp;gt; ['3   4', '4   3', '2   5', '1   3', '3   9', '3   3']&lt;/code&gt;
    &lt;p&gt;Now we build up a list comprehension to get the first elements. We might start with &lt;code&gt;[o for o in lines]&lt;/code&gt; and then add bits one at a time, inspecting the output, building up to:&lt;/p&gt;
    &lt;code&gt;= [int(o.split()[0]) for o in lines]
 l1 
 l1&amp;gt;&amp;gt;&amp;gt; [3, 4, 2, 1, 3, 3]&lt;/code&gt;
    &lt;p&gt;Now sorting:&lt;/p&gt;
    &lt;code&gt;sorted(l1)
&amp;gt;&amp;gt;&amp;gt; [1, 2, 3, 3, 3, 4]&lt;/code&gt;
    &lt;p&gt;Now that we’ve run all the pieces individually, and checked that the outputs are what we’d expect, we can stack them together into a function:&lt;/p&gt;
    &lt;code&gt;def get_list(x):
= x.splitlines()
     lines = [int(o.split()[0]) for o in lines]
     l1 return sorted(l1)
     
 get_list(x)&amp;gt;&amp;gt;&amp;gt; [1, 2, 3, 3, 3, 4]&lt;/code&gt;
    &lt;p&gt;At this point, you’d reflect on the solution, think back to the larger plan, perhaps ask yourself if there are better ways you could do it. You may be thinking that this is far too much work for &lt;code&gt;sorted(int(line.split()[0]) for line in x.splitlines())&lt;/code&gt; – as your skill increases you can tailor the level of granularity, but the idea remains the same: working on small pieces of code, checking the outputs, only combining them into larger functions once you’ve tried them individually, and constantly reflecting back on the larger goal.&lt;/p&gt;
    &lt;p&gt;(We’ll come back to this shortly – but also consider for a moment how integrated AI can fit into the above process. Any time you don’t know how to do something, you can ask for help with just that one little step. Any time you don’t understand how something works, or why it doesn’t, you can have AI help you with that exact piece.)&lt;/p&gt;
    &lt;head rend="h2"&gt;The Power of Fast Feedback Loops&lt;/head&gt;
    &lt;p&gt;The superpower that this kind of live, iterative coding gives you is near-instant feedback loops. Instead of building your giant app, waiting for the code to upload, clicking through to a website and then checking a debug console for errors – you’re inspecting the output of a chunk of code and seeing if it matches what you expected. It’s still possible to make mistakes and miss edge cases, but it is a LOT easier to catch most mistakes early when you code in this way.&lt;/p&gt;
    &lt;p&gt;This idea of setting things up so that you get feedback as soon as possible pops up again and again. Our cofounder Eric Ries talks about this in his book ‘The Lean Startup’, where getting feedback from customers is valuable for quick iteration on product or business ideas. Kaggle pros talk about the importance of fast evals – if you can test an idea in 5 minutes, you can try a lot more ideas than you could if each experiment requires 12 hours of model training.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI: Dialog Engineering Keeps Context Useful&lt;/head&gt;
    &lt;p&gt;One issue with current chat-based models is that once they go off the rails, it’s hard to get back on track. The model is now modelling a language sequence that involves the AI making mistakes – and more mistakes are likely to follow! If you’ve used language models much, then you’ve no doubt experienced this problem many times.&lt;/p&gt;
    &lt;p&gt;There is an interesting mathematical reason that this occurs. The vast majority of language model training is entirely about getting a neural network to predict the next word in a sentence – they are auto-regressive. Although they are later fine-tuned to do more than this, they are still at their heart really wanting to predict the next word of a sentence. In the documents used for training, there are plenty of examples of poor-quality reasoning and mistakes.Therefore, once an AI sees some mistakes in a chat, the most likely next tokens are going to be mistakes as well. That means that every time you are correcting the AI, you are making it more likely for the AI to give bad responses in the future!&lt;/p&gt;
    &lt;p&gt;Because solveit dialogs are fluid and editable, it’s much easier to go back and edit/remove mistakes, dead ends, and unrelated explorations. You can even edit past AI responses, to steer it into the kinds of behaviour you’d prefer. Combine this with the ability to easily hide messages from the AI or to pin messages to keep them in context even as the dialog grows beyond the context window and starts to be truncated, and you have a recipe for continued AI helpfulness as time goes on. We’ve been talking about this as “dialog engineering” for a long time – and it really is key to having AI work sessions that improve as time goes on, rather than degrading.&lt;/p&gt;
    &lt;p&gt;Of course, this is all useful for humans too! The discipline of keeping things tidy, using (collapsible) headings to organise sections, writing notes on what you’re doing or aiming for, and even past questions+answers with the AI all make it a pleasure to pick back up old work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building an App for Collaboration not Replacement&lt;/head&gt;
    &lt;p&gt;One thing is still (intentionally) hard in solveit though, and that is getting the AI to actually write all of your code in a hands-off way. We’ve made various choices to gently push towards the human remaining in control. Things like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Solveit defaults to code inputs&lt;/item&gt;
      &lt;item&gt;AI outputs code in fenced blocks, but these are not added to your code or run until you choose to do so. There are shortcuts to add them, but this extra step encourages you to read + refactor before mindlessly running&lt;/item&gt;
      &lt;item&gt;In ‘Learning’ mode especially, the AI will gently guide you to writing small steps rather than providing a big chunk of code, unless you really specifically ask it to do so.&lt;/item&gt;
      &lt;item&gt;In ‘Learning’ mode, the AI ‘ghost text’ auto-complete suggestions don’t show unless you trigger them with a keyboard shortcut.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Even the choice to have the editor be fairly small and down at the bottom emphasizes that this is a REPL/dialog, optimised for building small, understandable pieces. It’s entirely possible to practice the solveit approach in other tools, but we’ve also found that a combination of these intentional choices and the extra affordances for dialog engineering rapidly feel indispensible.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learning Trajectory&lt;/head&gt;
    &lt;p&gt;This brings us back to a foundational piece of the solveit approach: a learning mindset. It’s great that we can ask AI to fill in the gaps of our knowledge, or to save some time with fiddly pieces like matplotlib plots or library-specific boilerplate. But when the AI suggests something you don’t know, it is important not to skip it and move on – otherwise that new piece will never be something you learn!&lt;/p&gt;
    &lt;p&gt;We try to build the discipline to stop and explore anytime something like this comes up. Fortunately, it’s really easy to do this – you can add new messages trying out whatever new thing the AI has shown you, asking how it works, getting demo code, and poking it until you’re satisfied. And then the evidence of that side-quest can be collapsed below a heading (for later ref) or deleted, leaving you back in the main flow but with a new piece of knowledge in your brain.&lt;/p&gt;
    &lt;p&gt;Like many programmers, I’ve had my share of existential worries given the rapid rise in AI’s coding ability. What if AI keeps getting better and better, to the point where there’s little point for the average person actually learning to master any of these skills? If you assume your coding skills stay static, and imagine the AI continuing to get better, you may feel kinda bleak. The thing is, skill doesn’t have to be static! And as both you and the AI you’re carefully using get better, you will learn faster and be able to accomplish more and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mastery Requires Deliberate Practice&lt;/head&gt;
    &lt;p&gt;This is all hard work. It’s like exercise, or practicing a musical instrument. And like any pursuit of mastery, I don’t know that it’s for everyone. But as we’ve seen from all of the students who invested their time into the first cohort, the effort is well worth it in the end. Just take a look at the project showcase featuring a few hundred (!) things our community has made.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sign up for Solveit&lt;/head&gt;
    &lt;p&gt;If you’re interested in joining us to learn how to use the Solveit approach yourself, head over to our site and sign up: solve.it.com, Signups are open until October 20th, but may close earlier if we fill up, so don’t wait too long!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45455719</guid><pubDate>Thu, 02 Oct 2025 21:21:49 +0000</pubDate></item></channel></rss>