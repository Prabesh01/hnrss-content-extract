<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 28 Aug 2025 14:40:54 +0000</lastBuildDate><item><title>Open Source is one person</title><link>https://opensourcesecurity.io/2025/08-oss-one-person/</link><description>&lt;doc fingerprint="c75b107dde3c9131"&gt;
  &lt;main&gt;
    &lt;p&gt;The Register recently published a story titled Putin on the code: DoD reportedly relies on utility written by Russian dev. They should be ashamed of this story. This poor open source developer is getting beat up now to score some internet points. It’s very upsetting.&lt;/p&gt;
    &lt;p&gt;But anyway, let’s look at some receipts.&lt;/p&gt;
    &lt;p&gt;If you’re not real smrt, it seems like pointing out an open source project is written by one person in a country you don’t like is a bad thing. It could be. But it also could be the software running THE WHOLE F*CKING PLANET is written by one person. In a country. But we have no idea which country. It’s not the same person mind you, but it’s one person.&lt;/p&gt;
    &lt;p&gt;Here’s the thing. Almost all open source is literally one person. What I mean by that is if you look at all the open source projects out there, and there are a lot, we see a pattern of one person no matter how we slice and dice the data.&lt;/p&gt;
    &lt;p&gt;So let’s start with the data. A project exists called ecosyste.ms that catalogs a lot of open source. Most of it I would guess, but not all. They currently have 11.8 million open source projects in their data. You would be right to think that is a big number. I’m told anything over 15 is a big number, but it probably depends how smart you are, or think you are.&lt;/p&gt;
    &lt;p&gt;So what do we mean by one person is open source. What I mean is if we look at all the projects that ecosyste.ms is tracking, how many have a single person maintaining that project? It’s about 7 million. This is also a big number. 7 million open source projects are one person. It’s actually bigger than that, because of the 11.8 million projects ecosyste.ms is tracking, we don’t know how many maintainers 4 million of the projects have. A bunch of those will be one person. Here’s what a graph of this looks like&lt;/p&gt;
    &lt;p&gt;I clipped the graph so it looks nicer. There are projects with hundreds of maintainers. Not a ton, but they exist.&lt;/p&gt;
    &lt;p&gt;Now, the clever people among us are thinking “but Josh, surely these 7 million projects are all things nobody uses, the important open source we all use has loads of maintainers!!!”&lt;/p&gt;
    &lt;p&gt;You would be right to think that. It’s the first thought I had back when I started to look at this data. It’s OK. You’re still in the denial stage. Hopefully you’ll reach anger by the end of this post.&lt;/p&gt;
    &lt;p&gt;So we’re going to use the NPM ecosystem to explain this. I use NPM because they have the richest data in ecosyste.ms to explain my point. I’ve done this same thing across multiple ecosystems and the graphs all look the same.&lt;/p&gt;
    &lt;p&gt;So, what does the NPM maintainer graph look like. Your first thought is probably “why is the left axis green?” It’s not an axis, it’s the single maintainer number. It’s that huge compared to literally all the other data. There are just that many single person NPM projects.&lt;/p&gt;
    &lt;p&gt;So now, let’s look at the number of maintainers for projects with over 1 million downloads this month.&lt;/p&gt;
    &lt;p&gt;This time the graph shows how many downloads projects with over 1 million downloads, and one maintainer or more than one maintainer. It was easier to show the data by creating these two buckets.&lt;/p&gt;
    &lt;p&gt;That’s almost a 50/50 split. Think about that. About half of the 13,000 most downloaded NPM packages are ONE PERSON. We can change the download number and the graph stays this shape. It’s not until I change downloads to 1 billion downloads that we see 1 package maintained by 1 person, and 9 packages maintained by more than 1.&lt;/p&gt;
    &lt;p&gt;This is open source. Open source is one person, even the popular stuff.&lt;/p&gt;
    &lt;p&gt;I will also add, a lot of people own more than one package. So while NPM has over 4 million single person projects, they have about 900,000 maintainers for those 4 million single person projects. This will be an important data point at the end.&lt;/p&gt;
    &lt;p&gt;So here’s the big conclusion. If you want to make a big deal about something, maybe it shouldn’t be what country a sole maintainer is from. Let’s face it, the Russians aren’t dumb enough to backdoor a package owned by a guy living in Russia. They’re going to do something like pretend to be from another country with a name like Jia Tan, not Boris D. Badguy. This isn’t a Rocky and Bullwinkle episode.&lt;/p&gt;
    &lt;p&gt;Anyway, back to the conclusion&lt;/p&gt;
    &lt;p&gt;Open source, the thing that drives the world, the thing Harvard says has an economic value of 8.8 trillion dollars (also a big number). Most of it is one person. And I can promise you not one of those single person projects have the proper amount of resources they need. If you want to talk about possible risks to your supply chain, a single maintainer that’s grossly underpaid and overworked. That’s the risk. The country they are from is irrelevant.&lt;/p&gt;
    &lt;p&gt;And now if we have news stories being written about how a single person maintainer is the bad guy? That’s not cool (this is where your denial is supposed to turn into anger).&lt;/p&gt;
    &lt;p&gt;So what can you do about this? How can you turn your newly denial-turned-anger into action? We don’t really know unfortunately. I discussed this in a podcast episode Hobbyist Maintainers with Thomas DePierre. Like many hard problems, there isn’t an easy solution. But I guarantee the solution isn’t hunting down and demonizing single maintainers.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45047460</guid></item><item><title>Bookmarks.txt is a concept of keeping URLs in plain text files</title><link>https://github.com/soulim/bookmarks.txt</link><description>&lt;doc fingerprint="9d8698724de1ca68"&gt;
  &lt;main&gt;
    &lt;p&gt;bookmarks.txt is a concept of keeping bookmarks in plain text files.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bookmarked URLs are stored in files named &lt;code&gt;bookmarks.txt&lt;/code&gt;. The format is described below.&lt;/item&gt;
      &lt;item&gt;A "global" bookmarks file is located in the home directory (&lt;code&gt;$HOME/bookmarks.txt&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;"Local" bookmarks files could exist in different directories as well.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;URLs are stored one per line and could be accompanied with optional titles. Titles are separated from URLs with one space character.&lt;/p&gt;
    &lt;code&gt;URL [title]
&lt;/code&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;a URL without any title&lt;/p&gt;
        &lt;code&gt;https://www.example.com ----------------------- ^ URL&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;a URL with a title&lt;/p&gt;
        &lt;code&gt;https://sul.im personal website -------------- ---------------- ^ ^ URL Optional title&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The plain text nature of bookmark files allows to use any program to manage URLs. The &lt;code&gt;bin/&lt;/code&gt; directory of this repository contains &lt;code&gt;bookmarks&lt;/code&gt; script that could be used to list existing and add new URLs. However nothing should stop you from building your own tools.&lt;/p&gt;
    &lt;p&gt;Use fzf to select a URL and open it in the default browser:&lt;/p&gt;
    &lt;code&gt;./bin/bookmarks | fzf | cut -d ' ' -f 1 | xargs open&lt;/code&gt;
    &lt;p&gt;Add a new URL:&lt;/p&gt;
    &lt;code&gt;./bin/bookmarks https://github.com/soulim/bookmarks.txt&lt;/code&gt;
    &lt;p&gt;This is how I use bookmarks.txt:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;$HOME/bookmarks.txt&lt;/code&gt;contains URLs useful in any context. These are "global" addresses.&lt;/item&gt;
      &lt;item&gt;Each project directory has "local" &lt;code&gt;bookmarks.txt&lt;/code&gt;files with URLs pointing to tools specific to each project (repositories, monitoring tools, dashboards, and so on).&lt;/item&gt;
      &lt;item&gt;A symbolic link &lt;code&gt;$HOME/bin/bookmarks&lt;/code&gt;point to&lt;code&gt;bin/bookmarks&lt;/code&gt;from this directory.&lt;/item&gt;
      &lt;item&gt;With help of fzf I have a nice menu with fuzzy search to select URLs and open them automatically.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;bookmarks.txt is open to code contributions for bug fixes only. As features might carry a long-term maintenance burden, they will not be accepted at this time. Please submit an issue if you have a feature you would like to request.&lt;/p&gt;
    &lt;p&gt;See LICENSE for license text.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45047572</guid></item><item><title>Certificates for Onion Services</title><link>https://onionservices.torproject.org/research/proposals/usability/certificates/</link><description>&lt;doc fingerprint="4f7da5790839b582"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Certificates for Onion Services¶&lt;/head&gt;
    &lt;head rend="h2"&gt;About¶&lt;/head&gt;
    &lt;p&gt;This document tracks existing procedures or proposals for integrating and validating TLS/HTTPS certificates for Onion Services.&lt;/p&gt;
    &lt;p&gt;While some depends on Certificate Authorities (CA) model, others rely on alternative certification and validation procedures that does not require built-in certificate chains in the client software or reliance on financial transactions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction¶&lt;/head&gt;
    &lt;p&gt;Whenever you browse the internet regularly, the connection between your computer and a service is usually encrypted, and the safety of this communication happens through the verification of a special type of certificate.&lt;/p&gt;
    &lt;p&gt;With Onion Services, the connection is peer-to-peer encrypted by default, which means that no additional certificates are needed.&lt;/p&gt;
    &lt;p&gt;But as the web and other internet technologies mature, certificates are starting to be a requirement in order to unleash functionalities, especially in web browsers, such as the faster connection protocol HTTP/2 and payment processing.&lt;/p&gt;
    &lt;p&gt;That's why it's important to improve the certificate ecosystem to fully support Onion Services.&lt;/p&gt;
    &lt;p&gt;This is a hard problem, and an ongoing effort, but there has been some important work done to solve this.&lt;/p&gt;
    &lt;p&gt;The most relevant one should bring automation to the process of issuing certificates for Onion Services, through an enhancement in a protocol called ACME.&lt;/p&gt;
    &lt;p&gt;The ACME for Onions proposal is composed of tools and also an Internet Draft, which hopefully will turn into an Internet Standard soon.&lt;/p&gt;
    &lt;p&gt;We are also looking into other, non-conflicting alternatives that can also be used for certification, so service operators can decide which one fits best their use case.&lt;/p&gt;
    &lt;p&gt;Improving the certificate functionality will put Onion Services in parity with the modern stack of web development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benefits¶&lt;/head&gt;
    &lt;p&gt;It may be argued that Onion Services connections are already self-authenticated -- since the public key and the URL are tied together and the connection is peer-to-peer encrypted --, and thus making the need for HTTPS pointless, or at most giving only an impression on users of additional security.&lt;/p&gt;
    &lt;p&gt;But having valid HTTPS connection in Onion Services could enable many other enhancements, such as:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Some browser features are available only with HTTPS, like Secure Contexts, Content Security Policy (CSP), Secure cookies, WebAuthn, WebRTC and PaymentRequest.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A user may be using a browser that isn't the Tor browser. For example on iOS there is only Safari, and in such cases the browser will not be aware of the different semantics of security for an onion site, and won't allow the use of secure browser features (such as secure cookies). This limits the kind of web apps people can develop on onionsites as many modern browser APIs mentioned above.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Allows for the usage of HTTP/2, since some browsers only support it if on HTTPS1. In the future, HTTP2 and HTTP3 may only work with TLS, and thus valid certificates.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It also opens up new opportunities such as payment processing, "as current PCI DSS requirements do not allow non-standard TLS"2 and may only work with certificates having some sort of validation3. Payments card networks require HTTPS for a payment to be taken. So if someone wants to do that over an onion site they would need a TLS certificate.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It could be argued that this is also security-in-depth by having yet another layer of encryption atop of other existing encryption layers. Even if the theoretical gain in terms of interception and tampering resistance is not relevant, it would still allow for service operators to split their encryption keys in different servers -- like one with the Onion Service keys and a backend having the TLS keys, thus making a compromise in one of the servers exposing only the cryptographic material of one of the communication layers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Tor daemon that hosts the onion site might not be the final computer in the chain. In larger organizations, deployment concerns may result in plain HTTP traveling across their network from the Tor daemon to the final web server. Having HTTPs protects those hops in the chain. This is something that distributed setups may need. The same could be said for a web browser using Tor SOCKS proxy somewhere else on the network.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Non-web based applications, such as IMAP/POP/SMTP etc. can benefit from certificates being valid.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There is simply too much software that isn't aware of onionsites, and trying to force HTTP-over-Onion to be as secure as HTTPS-over-TCP creates a compatibility mess of things which do and don't know about the semantics.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;There is value in exposing the existence of an onion site via CT Logs. If someone navigates to the plain web version of a site, and is presented with a certificate containing a Subject Alternative Name (SAN) for both the plain web and the onion site that provides a strong cryptographic guarantee that they are the same site. Effectively this would replace the Onion-Location header with something more authenticated4.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The following discussion is not yet conclusive, and the problem space may be hard to solve.&lt;/p&gt;
    &lt;head rend="h2"&gt;Overview¶&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Proposal&lt;/cell&gt;
        &lt;cell role="head"&gt;Certification&lt;/cell&gt;
        &lt;cell role="head"&gt;Validation&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Existing CA validation&lt;/cell&gt;
        &lt;cell&gt;CA/B Baseline Requirements for .onion&lt;/cell&gt;
        &lt;cell&gt;CA chain&lt;/cell&gt;
        &lt;cell&gt;Implemented, fully supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ACME for .onion&lt;/cell&gt;
        &lt;cell&gt;CA/B Baseline Requirements for .onion&lt;/cell&gt;
        &lt;cell&gt;CA chain&lt;/cell&gt;
        &lt;cell&gt;Standardized as RFC 9799, need adoption by CAs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Self-signed certificates&lt;/cell&gt;
        &lt;cell&gt;Self-signed certificate&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
        &lt;cell&gt;Depends on per-application support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Self-signed X.509 from .onion&lt;/cell&gt;
        &lt;cell&gt;Signed by a "CA" derived from the .onion private key&lt;/cell&gt;
        &lt;cell&gt;Check if cert is issued by the .onion private key&lt;/cell&gt;
        &lt;cell&gt;Proof-of-concept, no browser integration&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Same Origin Onion Certificates (SOOC)&lt;/cell&gt;
        &lt;cell&gt;Self-signed certs&lt;/cell&gt;
        &lt;cell&gt;Skip for .onion addresses when conditions match&lt;/cell&gt;
        &lt;cell&gt;Proposal (not yet submitted for specification)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DANE for .onion&lt;/cell&gt;
        &lt;cell&gt;Self-signed certs&lt;/cell&gt;
        &lt;cell&gt;DNSSEC&lt;/cell&gt;
        &lt;cell&gt;Concept, no proposal yet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Onion-only CAs&lt;/cell&gt;
        &lt;cell&gt;Checks SAN and an .onion signature in an extension&lt;/cell&gt;
        &lt;cell&gt;CA chain&lt;/cell&gt;
        &lt;cell&gt;Concept, no proposal yet&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Self-auth certs via PKCS#11 module&lt;/cell&gt;
        &lt;cell&gt;Checks .onion Ed25519 signature in the cert key&lt;/cell&gt;
        &lt;cell&gt;Module returns a custom CA chain&lt;/cell&gt;
        &lt;cell&gt;Design, prototype available&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Main pros and cons¶&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Proposal&lt;/cell&gt;
        &lt;cell role="head"&gt;Pros&lt;/cell&gt;
        &lt;cell role="head"&gt;Cons&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Existing CA validation&lt;/cell&gt;
        &lt;cell&gt;None (already implemented)&lt;/cell&gt;
        &lt;cell&gt;None (already implemented)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ACME for .onion&lt;/cell&gt;
        &lt;cell&gt;No need for client/lib implementation&lt;/cell&gt;
        &lt;cell&gt;Depends on a CA willing to implement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Self-signed X.509 for .onion&lt;/cell&gt;
        &lt;cell&gt;No CA-reliance for .onion, self-auth.&lt;/cell&gt;
        &lt;cell&gt;Very hard to maintain and standardize, currently Ed25519 is unsupported by major browsers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Same Origin Onion Certificates (SOOC)&lt;/cell&gt;
        &lt;cell&gt;No CA-reliance for .onion&lt;/cell&gt;
        &lt;cell&gt;Very hard to maintain and standardize&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DANE for .onion&lt;/cell&gt;
        &lt;cell&gt;No CA-reliance for any domain or .onion&lt;/cell&gt;
        &lt;cell&gt;Very hard to implement and maintain&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Onion-only CAs&lt;/cell&gt;
        &lt;cell&gt;Simplify CA-reliance&lt;/cell&gt;
        &lt;cell&gt;Needs to convince existing CAs or trusted parties to maintain a whole CA organization and infrastructure&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Self-auth certs via PKCS#11 module&lt;/cell&gt;
        &lt;cell&gt;PKCS#11 is well established, future proof, no CA-reliance&lt;/cell&gt;
        &lt;cell&gt;Each Operating System or application would need to configure it; built-in OpenSSL support still underway&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Main implementation characteristics¶&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Proposal&lt;/cell&gt;
        &lt;cell role="head"&gt;Implementation level&lt;/cell&gt;
        &lt;cell role="head"&gt;Additional requirements&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Existing CA validation&lt;/cell&gt;
        &lt;cell&gt;Procedure happens at the CA side&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ACME for .onion&lt;/cell&gt;
        &lt;cell&gt;Procedure happens at the CA side&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Self-signed X.509 for .onion&lt;/cell&gt;
        &lt;cell&gt;Client or TLS library&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Same Origin Onion Certificates (SOOC)&lt;/cell&gt;
        &lt;cell&gt;Client or TLS library&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DANE for .onion&lt;/cell&gt;
        &lt;cell&gt;Client&lt;/cell&gt;
        &lt;cell&gt;Portable DNSSEC library&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Onion-only CAs&lt;/cell&gt;
        &lt;cell&gt;Client or TLS (only needs CA installation)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Self-auth certs via PKCS#11 module&lt;/cell&gt;
        &lt;cell&gt;Library (PKCS#11 module)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Existing CA validation¶&lt;/head&gt;
    &lt;p&gt;The CA/Browser Forum, a consortium that produces guidelines for X.509 (TLS/HTTPS) certification, created validation rules for Onion Service v2 addresses (in 2015), later extended for Onion Services v3 (in 2020), standardizing the way Certificate Authorities can issue certificates for .onion addresses and supports wildcards5.&lt;/p&gt;
    &lt;p&gt;Only a few commercial providers currently provide this service6.&lt;/p&gt;
    &lt;p&gt;The Appendix B of the CA/B Baseline Requirements (current repository version) for the Issuance and Management of PubliclyâTrusted Certificates (since Version 1.7.4, released in 2021) establishes two validation methods to ensure that someone request the certificate really control a given .onion address:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;An "AgreedâUpon Change to Website" (manually or via ACME), where the service operator must include some secret, such as at the &lt;code&gt;/.well-known/pki-validation&lt;/code&gt;of the site.&lt;/item&gt;
      &lt;item&gt;TLS using ALPN.&lt;/item&gt;
      &lt;item&gt;Checking of a Certificate Signing Request (CSR) signed by the Onion Service private key and containing an specific cryptographic nonce (i.e, a shared secret to be used only once), like using the onion-csr tool.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that both methods does not require that operators disclose the location of the Onion Service, nor them need to have a regular site for the service using DNS. Validation can either happen by accessing directly the Onion Service or by using the service private key to sign a CSR.&lt;/p&gt;
    &lt;p&gt;But still commercial CAs (or financial institution) may still collect identifiable information during the purchase of the certificates.&lt;/p&gt;
    &lt;head rend="h2"&gt;ACME for .onion (CA-validated)¶&lt;/head&gt;
    &lt;p&gt;In general, getting certificates from CAs supporting the CA/B Baseline Requirements for .onion addresses is still a manual, or in the best-case scenario, semi-automated task.&lt;/p&gt;
    &lt;p&gt;The Automatic Certificate Management Environment (ACME) standard (RFC 8555) solves part of the automation problem, and with the arrival of RFC 9799 it also supports methods for validating Onion Services.&lt;/p&gt;
    &lt;p&gt;Having support for .onion address in the ACME standard is the first step for projects like Let's Encrypt to offer free certificates for Onion Services, without financial transactions.&lt;/p&gt;
    &lt;p&gt;Existing proposals to bring ACME for Onion Services are discussed below.&lt;/p&gt;
    &lt;head rend="h3"&gt;ACME for Onions¶&lt;/head&gt;
    &lt;p&gt;The "Automated Certificate Management Environment (ACME) Extensions for ".onion" Domain Names" (draft-misell-acme-onion) is the second known proposal to bring ACME for .onion addresses.&lt;/p&gt;
    &lt;p&gt;This is the proposal that lead to RFC 9799.&lt;/p&gt;
    &lt;p&gt;A detailed analysis on ACME for Onions is available in a special appendix.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;head rend="h3"&gt;ACME Onion Identifier Validation Extension¶&lt;/head&gt;
    &lt;p&gt;The "Automated Certificate Management Environment (ACME) Onion Identifier Validation Extension" internet draft (draft-suchan-acme-onion-00) was proposed on 2022-05 and is the first known proposal to bring ACME for .onion addresses.&lt;/p&gt;
    &lt;p&gt;As of 2023-06-07, this internet draft is in the expired state, being now supplanted by RFC 9799.&lt;/p&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Relevant mail threads&lt;/item&gt;
      &lt;item&gt;orangepizza/acme-onion-doc: docs about standardize handling onion address in acme context&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Self-signed certificates¶&lt;/head&gt;
    &lt;p&gt;This proposal basically consists in allowing the use of self-signed certificates with Onion Services:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;For web applications like the Tor Browser, this would consist in disabling self-signed certificate warnings when visiting .onion sites. As an alternative, there's also the Self-authenticating TLS Certificates for Onion Services using a PKCS#11 module discussed below and relying on PKCS#11 modules or Authority Information Access (AIA) extensions, which could handle self-signed certificates matching the Onion Service address without the need to merge this logic directly in the applications, as it would remain decoupled in a PKCS#11 module, thus being easier to maintain.&lt;/item&gt;
      &lt;item&gt;For other applications -- like the TorVPN and third-party software --, this would probably require patches or documentation instructing users to accept non-CA signed certificates when accessing Onion Services, which is very hard to provide and to maintain for a wide ranging of tools.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Supported key types&lt;/p&gt;
    &lt;p&gt;In this proposal, any key types supported by applications could be used.&lt;/p&gt;
    &lt;p&gt;In case of popular web browsers, the CA/B Baseline Requirements must be taken into account, which as of 2024-09 only allows for RSA or ECDSA keys.&lt;/p&gt;
    &lt;p&gt;It could also be possible to use self-signed certs using Ed25519, which is discussed below and currently not widely supported by browsers.&lt;/p&gt;
    &lt;p&gt;This proposal would not provide:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A self-authentication mechanism (since the certificate is self-signed). This have a huge weight since an important piece of security provided by HTTPS is not just end-to-end encryption but also authentication.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Supporting self-signed certificates with Onion Services has a huge gain, but also introduces an authentication complexity. That's why proper UI indicators and hints are needed:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;For the encryption state of the site (HTTP and various HTTPS situations).&lt;/item&gt;
      &lt;item&gt;For the authentication state of the site, telling how it was (not) authenticated.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are already sketches for different scenarios for how various user interface hints and indicators could exist for Tor Browser and other software maintained by Tor, as well as existing certificate proposals that can change the certificate landscape for Onion Services in the future, which could be adopted by operators instead of relying on self-signed certs.&lt;/p&gt;
    &lt;p&gt;But all these enhancements would still limit the practical application domain of this proposal, since it would be readily available only to a small set of applications like Tor Browser, except if by pursuing some standardization such as the SOOC proposal below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-signed X.509 from .onion (self-signed by the .onion address)¶&lt;/head&gt;
    &lt;p&gt;Another option for having HTTPS in Onion Services that may be available in the future is to use Onion Service key pair to self-validate an HTTPS certificate using Ed25519 directly:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Onion x509 is an example in how a CA self-signed by an .onion could be constructed.&lt;/item&gt;
      &lt;item&gt;There's also a ticket requesting to add support for self-signed HTTPS onion sites derived from onion service's ed25519 key in the Tor Browser.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For an overview of Ed25519, check How do Ed5519 keys work?. For details about how Tor implements Ed25519, check prop220 (and rend-spec-v3 for how it implements at the Onion Services level).&lt;/p&gt;
    &lt;p&gt;This proposal has the advantage to not rely on Certificate Authorities, but the disadvantage that needs additional logic both server and client side to make it work, since a CA would needed to be installed for every visited Onion Service using this scheme.&lt;/p&gt;
    &lt;head rend="h3"&gt;On using .onion keys for certification¶&lt;/head&gt;
    &lt;p&gt;It's important to note that the current (as of 2024-09) Onion Services v3 specification does not allow the Master Onion Service identity key to be used for purposes other than generating blinded signing keys (see Section 1.9 from the rend-spec-v3):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Master (hidden service) identity key -- A master signing key pair used as the identity for a hidden service. This key is long term and not used on its own to sign anything; it is only used to generate blinded signing keys as described in [KEYBLIND] and [SUBCRED]. The public key is encoded in the ".onion" address according to [NAMING]. KP_hs_id, KS_hs_id.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;On Ed25519 certificates support in browsers¶&lt;/head&gt;
    &lt;p&gt;Also, while many TLS libraries support the Ed25519 signing scheme used for certificates (like in OpenSSL since version 1.1.1), major web browsers still does not support it (as of 2022-12)7, probably because they're not supported8 by the current (as of 2024-09) CA/B Baseline Requirements:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;6.1.1.3 Subscriber Key Pair Generation&lt;/p&gt;
      &lt;p&gt;The CA SHALL reject a certificate request if one or more of the following conditions are met:&lt;/p&gt;
      &lt;item&gt;The Key Pair does not meet the requirements set forth in Section 6.1.5 and/or Section 6.1.6;&lt;/item&gt;
      &lt;p&gt;[...]&lt;/p&gt;
      &lt;p&gt;6.1.5 Key sizes&lt;/p&gt;
      &lt;p&gt;For RSA key pairs the CA SHALL:&lt;/p&gt;
      &lt;p&gt;â¢ Ensure that the modulus size, when encoded, is at least 2048 bits, and; â¢ Ensure that the modulus size, in bits, is evenly divisible by 8.&lt;/p&gt;
      &lt;p&gt;For ECDSA key pairs, the CA SHALL:&lt;/p&gt;
      &lt;p&gt;â¢ Ensure that the key represents a valid point on the NIST Pâ256, NIST Pâ384 or NIST Pâ521 elliptic curve.&lt;/p&gt;
      &lt;p&gt;No other algorithms or key sizes are permitted.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Implementing X.509 certs derived from the .onion key pair¶&lt;/head&gt;
    &lt;p&gt;In summary, implementing this proposal would require pushing at least two specification changes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A ballot with CA/B Forum about including Ed25519 support.&lt;/item&gt;
      &lt;item&gt;An update in the Onion Services v3 spec, allowing the Onion Service identity keys to either:&lt;list rend="ol"&gt;&lt;item&gt;Also act as Certificate Authority root keys for the service.&lt;/item&gt;&lt;item&gt;Derive long-term (1 year) blinded keys to be used as a Certificate Authority for the service, maybe using the same approach described by Appendix A (&lt;code&gt;[KEYBLIND]&lt;/code&gt;) from rend-spec-v3 but covering the needed use case of a long-term key, i.e, depending in a long-term nonce and not in&lt;code&gt;[TIME-PERIODS]&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It's also important to avoid using the Onion Service key directly as the HTTPS certificate. That would:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Expose the Onion Service secret key material to more software than it's needed, like a web server.&lt;/item&gt;
      &lt;item&gt;Make it very difficult to manage offline Onion Service master keys.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead, it's better to use the Onion Service key pair to act as a CA that then certifies a separate key pair to be used with HTTPS.&lt;/p&gt;
    &lt;p&gt;Similar to the self-signed certificate proposal, this approach would have limited adoption if only as small number of applications implement it -- such as the Tor Browser --, except if endorsed by many stakeholders in the form of a specification -- like the SOOC proposal discussed below.&lt;/p&gt;
    &lt;head rend="h2"&gt;Self-authenticating TLS Certificates for Onion Services using a PKCS#11 module¶&lt;/head&gt;
    &lt;p&gt;The Self-authenticating TLS Certificates for Onion Services using a PKCS#11 module proposal mentioned above, that relies on PKCS#11 modules or Authority Information Access (AIA) extensions, could also be used to work with a X.509 certificate directly derived from the .onion key pair.&lt;/p&gt;
    &lt;p&gt;But contrary to the previous proposal, it would not need to use Ed25519: it would support a signature scheme where an Ed25519 private key could sign an ECDSA key. This Ed25519 signature could either be created using the .onion private key itself or a fresh Ed25519 subkey, thus avoiding key reuse.&lt;/p&gt;
    &lt;p&gt;Advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Would reduce logic in the Tor Browser by a well-established API.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Does not need to use Ed25519 X.509 certificates: can work with ECDSA which are fully supported by major browsers according to the CA/B Baseline Requirements, and maybe could even work with RSA.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Seems future-proof as PKCS#11 modules are widely used.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No reliance on the CA-model (and hence has increased censorship resistance).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;No need to use CT Logs.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Could be used by other browsers as well (such as Brave).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Could be used with any software, library or Operating System with PKCS#11 support.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Disadvantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;In the short-to-mid term this would not be supported on OpenSSL (as of 2024-09, support PKCS#11 modules is still underway).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;System-wide support would depend on how each Operating System could support this custom module. So could be hard to add this to TorVPN. But anyway, TorVPN can't validate existing self-signed .onion certs either, as of 2024-09.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Operators currently using self-signed certs would need to migrate to new certificates.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;namecoin/pkcs11mod: Go library for creating pkcs11 modules:&lt;/item&gt;
      &lt;item&gt;Consistent PKCS #11 support in Red Hat Enterprise Linux 8.&lt;/item&gt;
      &lt;item&gt;OpenSSL support for PKCS#11:&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Same Origin Onion Certificates (SOOC)¶&lt;/head&gt;
    &lt;p&gt;The Same Origin Onion Certificates (SOOC) proposal aims to specify when "in very limited circumstances, we shall not care about signatures at all", allowing clients to disable self-signed certificate warnings when visiting .onion sites.&lt;/p&gt;
    &lt;p&gt;The main difference between the SOOC proposal and to simply start allowing self-signed certificates is that SOOC is aimed to be an IETF proposal that could gain momentum and hence have a greater chance to be adopt by many different vendors.&lt;/p&gt;
    &lt;p&gt;See the SOOC document for details.&lt;/p&gt;
    &lt;head rend="h2"&gt;DANE for .onion¶&lt;/head&gt;
    &lt;p&gt;Another option is to use DNS-based Authentication of Named Entities (DANE), with DNS records like this to associate an Onion Service address with a given HTTPS certificate's public key hash:&lt;/p&gt;
    &lt;code&gt;_443._tcp.testk4ae7qr6nhlgahvyicxy7nsrmfmhigtdophufo3vumisvop2gryd.onion. TSLA 3 1 1 AB9BEB9919729F3239AF08214C1EF6CCA52D2DBAE788BB5BE834C13911292ED9
&lt;/code&gt;
    &lt;p&gt;In order for that to work, logic should be implemented in the client software.&lt;/p&gt;
    &lt;p&gt;Drawbacks:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Service operators must update this record whenever a new certificate is issued.&lt;/item&gt;
      &lt;item&gt;Has some limits for wildcard certificates on specific ports.&lt;/item&gt;
      &lt;item&gt;DANE is not widely supported, especially by web browsers.&lt;/item&gt;
      &lt;item&gt;It would only work for service operators willing to publish the .onion address in the DNS.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Onion-only CAs¶&lt;/head&gt;
    &lt;p&gt;This proposal consists of:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Having .onion-only CAs with name constraints (only allowing issuance for .onion). Services available both via DNS-based and .onion domains will need to have two TLS certificates in order to use this approach -- one certificate for the DNS-based domain (as usual) and another only for the .onion address.&lt;/item&gt;
      &lt;item&gt;Certification procedure would be automated, so generated .onion addresses could easily have it's certificates issued by this special type of CA.&lt;/item&gt;
      &lt;item&gt;Certification would then happen by checking a signature in a CSR and comparing the Subject Alternative Name (SAN). Signature must be validated by the .onion address in the SAN.&lt;/item&gt;
      &lt;item&gt;So this type of CA would be mainly a basic notary that attests signatures and issues a corresponding certificate.&lt;/item&gt;
      &lt;item&gt;The name constraint for this type of CA ensures that it only issues certificates for .onion domains.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Security considerations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Suppose there's a malicious or bugged CA of this type that issues a certificate containing a SAN for &lt;code&gt;$address1.onion&lt;/code&gt;but:&lt;list rend="ol"&gt;&lt;item&gt;Without checking whether the CSR has a signature made by &lt;code&gt;$address1.onion&lt;/code&gt;'s private key.&lt;/item&gt;&lt;item&gt;Or if allowing that another, unrelated &lt;code&gt;$address2.onion&lt;/code&gt;actually signs this CSR.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Without checking whether the CSR has a signature made by &lt;/item&gt;
      &lt;item&gt;Even if that's the case, i.e, the CA wrongly issued a certificate for &lt;code&gt;$address1.onion&lt;/code&gt;that did not match the requirements, this certificate won't work in practice, since in a successful Onion Service TLS connection to&lt;code&gt;$address1.onion&lt;/code&gt;the following must happen:&lt;list rend="ol"&gt;&lt;item&gt;The underlying Tor Rendezvous connection should ensure that the client is connected to &lt;code&gt;$address1.onion&lt;/code&gt;(Onion Services connections are self-authenticated by the .onion address).&lt;/item&gt;&lt;item&gt;The Onion Service should then offer it's TLS certificate, which would not be the malicious one (except if the service is already compromised, but in that case the attacker would not need to forge an invalid certificate anyway...).&lt;/item&gt;&lt;item&gt;Then the client's TLS library tests whether the certificate chain can be verified and any SAN in the presented certificate matches &lt;code&gt;$address1.onion&lt;/code&gt;, among other checks (such as expiration).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;The underlying Tor Rendezvous connection should ensure that the client is connected to &lt;/item&gt;
      &lt;item&gt;That said, the work done by this special type of CA is only to expand the self-authentication property from the .onion address into a certificate. So the attack surface of this special type of CA may be inherently low.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Implementation considerations:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Since Ed25519 certificates probably won't be supported by major browsers/clients in the foreseeable future (see discussion above at the Self-signed X.509 for .onion section), issuance should probably follow the Appendix B of the CA/B Baseline Requirements.&lt;/item&gt;
      &lt;item&gt;The entire certification procedure could happen via Onion Services.&lt;/item&gt;
      &lt;item&gt;Actually the whole CA infrastructure (website, APIs, OCSP etc) could be interacted only via Onion Services, to reduce the attack surface and protect the service location.&lt;/item&gt;
      &lt;item&gt;Important to consider whether would be possible to organizations setup and maintain a Onion-only CA that's as most automated as possible, including root certificate packaging/distribution/rotation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Pros:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Easy to implement on the client side (just need to install the CA).&lt;/item&gt;
      &lt;item&gt;Easy to implement and maintain on Tor-native applications such as Tor Browser and the Tor VPN.&lt;/item&gt;
      &lt;item&gt;Possibly lowest attack surface than with regular CAs!&lt;/item&gt;
      &lt;item&gt;Largest certification expiration dates could be used (like one year).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cons:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Might not be easy to find CAs willing to do this, or to a new one to be formed for this purpose.&lt;/item&gt;
      &lt;item&gt;Might need a merge request to include this method in the CA/B Baseline Requirements, if wider acceptance is intended.&lt;/item&gt;
      &lt;item&gt;No guarantees that these special CAs would be installed among all clients and libraries.&lt;/item&gt;
      &lt;item&gt;Need additional security analysis.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Open questions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Need to check if Certificate Revocation Lists (CRLs) are needed, and how to handle it.&lt;/item&gt;
      &lt;item&gt;Need to figure out how OCSP and OCSP Stapling could happen. OSCP connection could be available behind an Onion Service.&lt;/item&gt;
      &lt;item&gt;Does sending certificates to CT Logs still makes sense for this special type of certification?&lt;/item&gt;
      &lt;item&gt;Needs built-in DoS/service abuse protection:&lt;list rend="ul"&gt;&lt;item&gt;An idea for that: implement a simple PoW by additionally requiring that service operators provide a proof-ownership of another .onion address made by an specific vanity address (like limited to 5 or 6 chars).&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;References:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Proposal for Bring Accessible TLS Supports to All Onion Services, where this idea is initially written and discussed with the possibility for some clients to have only this type of CA installed (but in that case it might not accept valid certificates issued by regular CAs, with advantages and disadvantages)&lt;/item&gt;
      &lt;item&gt;Proposal for automated onion service certificate issuance based on fully qualified onion service key signed certificate request, where this proposal is sent to the CA/B Forum.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Custom CAs¶&lt;/head&gt;
    &lt;p&gt;There are also discussions about how to properly manage custom Certificate Authorities, i.e, those not distributed in TLS libraries by default (such as the certificate store in a web browser):&lt;/p&gt;
    &lt;head rend="h2"&gt;Further references¶&lt;/head&gt;
    &lt;head rend="h3"&gt;Meeting notes¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;From the 2024 Lisbon Meeting:&lt;/item&gt;
      &lt;item&gt;From the 2017 Montreal Meeting:&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Blog posts¶&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Facebook, hidden services, and https certs | The Tor Project:&lt;list rend="ul"&gt;&lt;item&gt;Part four: what do we think about an https cert for a .onion address?&lt;/item&gt;&lt;item&gt;Part five: What remains to be done?&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Related issues¶&lt;/head&gt;
    &lt;head rend="h2"&gt;Notes¶&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;But not HTTP/3 yet, since it uses UDP not available via Tor (as of 2023-05). The HTTP/2 standard does not require encryption, but all major browsers require encryption for HTTP/2 and encryption for HTTP/3 is required by default. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See PCI-DSS v4.0 - Appendix G - Term "Strong Cryptography" - page 355, which points only to "industry tested and accepted algorithms". While we could argue that PCI-DSS v4.0 is not precise enough about which transmission protocols might be used, it may be the case that the encryption used by the Onion Services' Rendezvous v3 protocol is not (yet) part of an "industry standard" (needs someone to carefully review this claim and open a merge request to update this information). It also may be the case that PCI-DSS compliance may be hard to get for a system that employs only the Rendezvous v3 protocol to transmit cardholder data between an user and an Onion Service, without TLS atop of it. And users might not trust the connection if not over TLS, or if their browser does not show certificate information. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It's worth note that PCI-DSS does allow for the use of self-signed certificates under some special conditions that may exclude some of the proposals in this document (see PCI-DSS v4.0 - Requirement 4.2 - Applicability Notes - page 106). In practice, this is only applicable for internal links within an organization or for clients and libraries that have the custom Certificate Authorities' root keys on it's keystores and that matches the standard requirements. And users would hardly trust a self-signed certificate for doing online purchases as their browsers would show warning messages. Recommendation (see PCI-DSS v4.0 - Requirement 4.2 - Guidance - page 106) goes instead towards a certificate trusted by a Certificate Authority. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;However, the argument about revealing an onion site that would like to remain hidden, is a real one. For those services, it could be considered options such as a non-CT Logs issuing CA that may not be in the "valid" set, but operated by a friendly to Tor organization, which is added to Tor Browser as a valid certifier. The standards space may be moving towards requiring CT log submissions at some point, so this is something to keep an eye on. Another possibility would be to consider writing a standard for hashing onion site names in CT Logs, so they can be verified, but not revealed (such as what WhatsApp did in their Auditable Key Directory). Such a standard could take years to get to place of usefulness, and probably encounter resistance. Otherwise, the only option for such a service operator is to have a self-signed certificate, or none at all. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For a historical background on Domain Validation (DV) certs, check the CA/B forum thread DV issuance for next-generation onion services. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As of January 2023, there are only two CAs issuing certificates for .onion domains: DigiCert providing only Extended Validation (EV) certs and HARICA providing only Domain Validated (DV) certs. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They usually just support X25519, which is a key agreement scheme not to be confused with Ed25519. ↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;This is tracked on cabforum/servercert issue #451, which is about adding EdDSA (and hence Ed25519) support in the CA/B Baseline Requirements. Check also the related mail thread (Servercert-wg) Ed25519 certificates. For Let's Encrypt support, check letsencrypt/boulder issue 3649; Request For CertBot To Support The Signing of Ed25519 Certificates and Support Ed25519 and Ed448 forum threads. On IETF, EdDSA and Ed25519 are standardized on RFC 8032, and have Algorithm Identifiers for X.509 Public Key Infrastructure as part of RFC 8410. ↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45047897</guid></item><item><title>The Deletion of Docker.io/Bitnami</title><link>https://community.broadcom.com/tanzu/blogs/beltran-rueda-borrego/2025/08/18/how-to-prepare-for-the-bitnami-changes-coming-soon</link><description>&lt;doc fingerprint="ca8a1a59085dec0f"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h5"&gt;Update&lt;/head&gt;
      &lt;p&gt;After evaluating the impact and community feedback, the Bitnami team has postponed the deletion of the Bitnami public catalog (&lt;code&gt;docker.io/bitnami&lt;/code&gt;) until September 29th to give users more time to adapt to the upcoming changes.&lt;lb/&gt;To raise awareness before the registry deletion, we will run a series of brownouts over the coming weeks. During each brownout, a set of 10 container images from &lt;code&gt;docker.io/bitnami&lt;/code&gt; will be temporarily unavailable for 24 hours. The scheduled brownouts are:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item style="list-style-type: none"&gt;
          &lt;list rend="ul"&gt;
            &lt;item&gt;August 28, 08:00 UTC → August 29, 08:00 UTC&lt;/item&gt;
            &lt;item&gt;September 2, 08:00 UTC → September 3, 08:00 UTC&lt;/item&gt;
            &lt;item&gt;September 17, 08:00 UTC → September 18, 08:00 UTC&lt;/item&gt;
          &lt;/list&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;The list of affected applications will be published on the day of each brownout via our usual channels.&lt;lb/&gt;As previously announced, since August 28th, we have not published new Bitnami container images or Helm charts to Docker Hub in OCI format. The source code for containers and Helm charts remains available on GitHub under the Apache 2.0 license.&lt;/p&gt;
      &lt;head rend="h3"&gt;What's changing?&lt;/head&gt;
      &lt;p&gt;Starting August 28th, Bitnami will be archiving its OCI registry of charts and images to a new location, Bitnami Legacy, to make room for the new secure, hardened images that will eventually reside in the main Bitnami registry. Users who are currently pulling these images will need to update their pipelines, internal mirrors, and Kubernetes clusters to pull from a new location before that time. A couple of options users have: &lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item dir="ltr" aria-level="1"&gt;
          &lt;p&gt;Switch to Bitnami Secure Images &lt;/p&gt;
        &lt;/item&gt;
        &lt;item dir="ltr" aria-level="1"&gt;
          &lt;p&gt;Switch to the Bitnami Legacy Registry&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;To retain existing functionality and maintain continuity of systems relying on Bitnami, we recommend switching to Bitnami Secure Images. In addition to a less disruptive transition, BSI helps strengthen your security and compliance posture by adopting the higher-quality images offered as part of BSI. &lt;/p&gt;
      &lt;head rend="h3"&gt;Switching to Bitnami Secure Images (BSI)&lt;/head&gt;
      &lt;p&gt;While some BSI images will be free, they are only for use in development/testing purposes, and a commercial subscription is recommended for access to the entire catalog, as well as stable tags, long-term support versions, and more. &lt;/p&gt;
      &lt;p&gt;Though a BSI subscription provides customers with the entire Bitnami Debian-based image catalog (which will continue to receive updates), we recommend users upgrade and start using the hardened Photon Linux-based images instead. These are designed to be replacement images for any of the Debian images and work with the same Helm charts. &lt;/p&gt;
      &lt;p&gt;The Photon images provide many other benefits not previously available to users of Debian images, including: &lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item dir="ltr" aria-level="1"&gt;
          &lt;p&gt;Drastically reduced CVE count (e.g., 100+ CVEs to in some cases 0)&lt;/p&gt;
        &lt;/item&gt;
        &lt;item dir="ltr" aria-level="1"&gt;
          &lt;p&gt;VEX statements for easier triage, along with Known Exploitable Vulnerabilities (KEV) and EPSS scores&lt;/p&gt;
        &lt;/item&gt;
        &lt;item dir="ltr" aria-level="1"&gt;
          &lt;p&gt;A self-service UI/API with powerful reporting and metadata capabilities&lt;/p&gt;
        &lt;/item&gt;
        &lt;item dir="ltr" aria-level="1"&gt;
          &lt;p&gt;More advanced Helm charts are not available on Docker Hub, such as Bitnami’s “distroless charts” which offer an 83% smaller attack surface (by MB).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item dir="ltr" aria-level="1"&gt;
          &lt;p&gt;Support for customizing the images built by our secure SLSA 3 software factory &lt;/p&gt;
        &lt;/item&gt;
        &lt;item dir="ltr" aria-level="1"&gt;
          &lt;p&gt;Images and Helm charts are delivered to a private and secure OCI registry dedicated to each customer instead of relying on a public registry with rate limits like Docker Hub.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item dir="ltr" aria-level="1"&gt;
          &lt;p&gt;Access to over 90 VM Images in OVA format&lt;/p&gt;
        &lt;/item&gt;
        &lt;item dir="ltr" aria-level="1"&gt;
          &lt;p&gt;Enterprise support for packaging and installation issues&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Switching to Bitnami Legacy Registry&lt;/head&gt;
      &lt;p&gt;Another option for users of Bitnami today is to switch to the historic archive registry called Bitnami Legacy. This is unsupported software that is being made available, at users' own risk, while they make plans for alternatives. As such, this is a temporary solution, and we do not plan to keep this registry around for long. It will quickly begin to accumulate vulnerabilities that are not patched and atrophy as any software frozen in time does. If this is your choice, we strongly recommend copying the images you are using to your own registry; again, this should be considered a temporary solution. While we think there are many better options to make before the August 28th change, this is an option of last resort for those who need more time. &lt;/p&gt;
      &lt;head rend="h3"&gt;Why is it a good time to consider upgrading your security and compliance for open source? &lt;/head&gt;
      &lt;p&gt;So why do all the work now to change what’s maybe been working and update the type of open source images you use? We get it, no one likes change. But the reality is the landscape of open source is changing all around us. For example, from 2019-2023, the number of malicious packages discovered has risen to more than 245,000 according to Sonatype. That’s 2x all the previous years combined. The implication is that bad actors are finding increasing opportunities to exploit open source software that is running in every major software organization around the world. Meanwhile, with the growth of AI and MCP models, open source consumption is only going to increase. So the risk profile tides are quickly rising around us, and having a better boat to be prepared for the impacts of this change is the only responsible response. &lt;/p&gt;
      &lt;p&gt;In addition, the Cyber Resilience Act in the EU creates an impending obligation for many organizations doing business there to provide guarantees about the open source software they use in their organization. It could soon be a liability to use open source that doesn’t have the required documentation to prove it’s been sourced from a safe place and hasn’t been tampered with. &lt;/p&gt;
      &lt;p&gt;This is why the launch of Bitnami Secure Images is so timely. BSI is making it easier than ever for organizations to responsibly prepare for what the future of open source software in our modern world looks like. As with many things, what started out simple has become increasingly complex and requires more care to navigate. Furthermore, BSI has one of the lowest TCOs in the industry, enabling more organizations than ever before to afford cutting-edge supply chain security. BSI is effectively democratizing security and compliance for open source so that it doesn’t require million-dollar contracts from vendors with sky-high valuations. &lt;/p&gt;
      &lt;head rend="h3"&gt;What are Competitors trying to claim about Bitnami? &lt;/head&gt;
      &lt;p&gt;It’s also a great time for the competition to try and steer the narrative about Bitnami. Some have claimed that Bitnami is “pulling their free container images and Helm charts from public access”. However, if we look a little more closely at the changes Bitnami has announced, this statement is inaccurate. For one, Bitnami Helm charts continue to be an open source project, under Apache 2, freely available to the public on GitHub. &lt;/p&gt;
      &lt;p&gt;Second, what is actually changing is the built OCI artifacts. Essentially, Bitnami has been the Jenkins of the internet for many years, but this has become unsustainable. Operating a build pipeline and OCI registry for the general public is very expensive. Just ask those same competitors throwing shade at Bitnami why they have never offered to make their charts or images publicly available at the same scale Bitnami has?&lt;/p&gt;
      &lt;p&gt;So users can continue to freely access the Helm chart source (as well as the Debian images). However, in order to sustain and support the dedicated team of engineers who maintain and build new charts and images, a subscription will be required if an organization needs the images and charts built and hosted in an OCI registry for them. And to reiterate the above again, organizations that choose this path are simultaneously upgrading their security posture and improving their OSS strategy.&lt;/p&gt;
      &lt;head rend="h3"&gt;How do the changes on August 28th work?&lt;/head&gt;
      &lt;p&gt;The changes to the Bitnami repo are slated to begin on Aug 28th. They will not all be at once. Over a multi-week period, the images will be cleaned up from the registry to make room for the new ones. This is being done gradually to minimize the disruptions. However, we can’t be precise about which image will be removed at what time due to the complexity of the 84TB of OCI content that the engineering team will be dealing with. Therefore, it’s best to assume starting Aug 28th, every image used in a key business function should be addressed with an alternate registry. &lt;/p&gt;
      &lt;p&gt;We’re making room in the mainline Bitnami registry so we can populate it with the free tier of Bitnami Secure Images. These hardened Photon have the same names as the Debian images, so they can’t occupy the same registry. And we want new adopters of Bitnami to start with the secure images going forward, as we believe this is the future of open-source software on the internet. In this FAQ, you can find more information about all the details of the upcoming changes.&lt;/p&gt;
      &lt;p&gt;&lt;lb/&gt;#bitnami&lt;lb/&gt;#Security&lt;lb/&gt;#helm&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45048419</guid></item><item><title>Claude Code Checkpoints</title><link>https://claude-checkpoints.com/</link><description>&lt;doc fingerprint="3786eeec2ca7cebb"&gt;
  &lt;main&gt;
    &lt;p&gt;Continuously monitors your entire project for file changes. No setup required - just select your project folder and start coding.&lt;/p&gt;
    &lt;p&gt;Create instant snapshots of your project state before making risky changes. Each checkpoint captures all files and their contents.&lt;/p&gt;
    &lt;p&gt;See exactly what changed between checkpoints with our built-in diff viewer. Track additions, modifications, and deletions at a glance.&lt;/p&gt;
    &lt;p&gt;Instantly restore your project to any previous checkpoint. Perfect for experimenting with confidence or recovering from mistakes.&lt;/p&gt;
    &lt;p&gt;Seamlessly integrates with Claude Desktop through MCP protocol. Automatic checkpoints when tasks complete.&lt;/p&gt;
    &lt;p&gt;Every checkpoint includes a complete backup of all project files. Your work is always safe and recoverable.&lt;/p&gt;
    &lt;p&gt;Choose your project folder in the Checkpoints app&lt;/p&gt;
    &lt;p&gt;Work with Claude Code as usual - changes are tracked automatically&lt;/p&gt;
    &lt;p&gt;Checkpoints are created automatically when tasks complete&lt;/p&gt;
    &lt;p&gt;One click to restore any previous state if needed&lt;/p&gt;
    &lt;p&gt;Works automatically with Claude Desktop through Model Context Protocol&lt;/p&gt;
    &lt;p&gt;MCP server starts automatically on port 8765. Claude Desktop connects instantly when you open a project.&lt;/p&gt;
    &lt;p&gt;Every task start and completion is tracked. Checkpoints are created automatically at key moments.&lt;/p&gt;
    &lt;p&gt;Claude can list checkpoints, view diffs, and restore previous states through MCP commands.&lt;/p&gt;
    &lt;p&gt;Clean, intuitive checkpoint management&lt;/p&gt;
    &lt;p&gt;Visual comparison between checkpoints&lt;/p&gt;
    &lt;p&gt;Seamless Claude Desktop connection&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45050090</guid></item><item><title>Windows 11 Update KB5063878 Causing SSD Failures</title><link>https://old.reddit.com/r/msp/comments/1n1sgxx/windows_11_update_kb5063878_causing_ssd_failures/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45050192</guid></item><item><title>Are OpenAI and Anthropic Losing Money on Inference?</title><link>https://martinalderson.com/posts/are-openai-and-anthropic-really-losing-money-on-inference/</link><description>&lt;doc fingerprint="271216d2049eed14"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Are OpenAI and Anthropic Really Losing Money on Inference?&lt;/head&gt;
    &lt;p&gt;I keep hearing what a cash incinerator AI is, especially around inference. While it seems reasonable on the surface, I've often been wary of these kind of claims, so I decided to do some digging.&lt;/p&gt;
    &lt;p&gt;I haven't seen anyone really try to deconstruct the costs in running inference at scale and the economics really interest me.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is really napkin math. I don't have any experience at running frontier models at scale, but I do know a lot about the costs and economics of running very high throughput services on the cloud and, also, some of the absolutely crazy margins involved from the hyperscalers vs bare metal. Corrections are most welcome.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Some assumptions&lt;/head&gt;
    &lt;p&gt;I'm only going to look at raw compute costs. This is obviously a complete oversimplification, but given how useful the current models are - even assuming no improvements - I want to stress test the idea that everyone is losing so much money on inference that it is completely unsustainable.&lt;/p&gt;
    &lt;p&gt;I've taken the cost of a single H100 at $2/hour. This is actually more than the current retail rental on demand price, and I (hope) the large AI firms are able to get these for a fraction of this price.&lt;/p&gt;
    &lt;p&gt;Secondly, I'm going to use the architecture of DeepSeek R1 as the baseline, 671B total params with 37B active via mixture of experts. Given this gets somewhat similar performance to Claude Sonnet 4 and GPT5 I think it's a fair assumption to make.&lt;/p&gt;
    &lt;head rend="h2"&gt;Working Backwards: H100 Math From First Principles&lt;/head&gt;
    &lt;head rend="h3"&gt;Production Setup&lt;/head&gt;
    &lt;p&gt;Let's start with a realistic production setup. I'm assuming a cluster of 72 H100s at $2/hour each, giving us $144/hour in total costs.&lt;/p&gt;
    &lt;p&gt;For production latency requirements, I'm using a batch size of 32 concurrent requests per model instance, which is more realistic than the massive batches you might see in benchmarks. With tensor parallelism across 8 GPUs per model instance, we can run 9 model instances simultaneously across our 72 GPUs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Prefill Phase (Input Processing)&lt;/head&gt;
    &lt;p&gt;The H100 has about 3.35TB/s of HBM bandwidth per GPU, which becomes our limiting factor for most workloads. With 37B active parameters requiring 74GB in FP16 precision, we can push through approximately 3,350GB/s ÷ 74GB = 45 forward passes per second per instance.&lt;/p&gt;
    &lt;p&gt;Here's the key insight: each forward pass processes ALL tokens in ALL sequences simultaneously. With our batch of 32 sequences averaging 1,000 tokens each, that's 32,000 tokens processed per forward pass. This means each instance can handle 45 passes/s × 32k tokens = 1.44 million input tokens per second. Across our 9 instances, we're looking at 13 million input tokens per second, or 46.8 billion input tokens per hour.&lt;/p&gt;
    &lt;p&gt;In reality, with MoE you might need to load different expert combinations for different tokens in your batch, potentially reducing throughput by 2-3x if tokens route to diverse experts. However, in practice, routing patterns often show clustering around popular experts, and modern implementations use techniques like expert parallelism and capacity factors to maintain efficiency, so the actual impact is likely closer to a 30-50% reduction rather than worst-case scenarios.&lt;/p&gt;
    &lt;head rend="h4"&gt;Decode Phase (Output Generation)&lt;/head&gt;
    &lt;p&gt;Output generation tells a completely different story. Here we're generating tokens sequentially - one token per sequence per forward pass. So our 45 forward passes per second only produce 45 × 32 = 1,440 output tokens per second per instance. Across 9 instances, that's 12,960 output tokens per second, or 46.7 million output tokens per hour.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raw Cost Per Token&lt;/head&gt;
    &lt;p&gt;The asymmetry is stark: $144 ÷ 46,800M = $0.003 per million input tokens versus $144 ÷ 46.7M = $3.08 per million output tokens. That's a thousand-fold difference!&lt;/p&gt;
    &lt;head rend="h3"&gt;When Compute Becomes the Bottleneck&lt;/head&gt;
    &lt;p&gt;Our calculations assume memory bandwidth is the limiting factor, which holds true for typical workloads. But compute becomes the bottleneck in certain scenarios. With long context sequences, attention computation scales quadratically with sequence length. Very large batch sizes with more parallel attention heads can also shift you to being compute bound.&lt;/p&gt;
    &lt;p&gt;Once you hit 128k+ context lengths, the attention matrix becomes massive and you shift from memory-bound to compute-bound operation. This can increase costs by 2-10x for very long contexts.&lt;/p&gt;
    &lt;p&gt;This explains some interesting product decisions. Claude Code artificially limits context to 200k tokens - not just for performance, but to keep inference in the cheap memory-bound regime and avoid expensive compute-bound long-context scenarios. This is also why providers charge extra for 200k+ context windows - the economics fundamentally change.&lt;/p&gt;
    &lt;head rend="h2"&gt;Real-World User Economics&lt;/head&gt;
    &lt;p&gt;So to summarise, I suspect the following is the case based on trying to reverse engineer the costs (and again, keep in mind this is retail rental prices for H100s):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Input processing is essentially free (~$0.001 per million tokens)&lt;/item&gt;
      &lt;item&gt;Output generation has real costs (~$3 per million tokens)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These costs map to what DeepInfra charges for R1 hosting, with the exception there is a much higher markup on input tokens.&lt;/p&gt;
    &lt;head rend="h3"&gt;A. Consumer Plans&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;$20/month ChatGPT Pro user: Heavy daily usage but token-limited &lt;list rend="ul"&gt;&lt;item&gt;100k toks/day&lt;/item&gt;&lt;item&gt;Assuming 70% input/30% output: actual cost ~$3/month&lt;/item&gt;&lt;item&gt;5-6x markup for OpenAI&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is your typical power user who's using the model daily for writing, coding, and general queries. The economics here are solid.&lt;/p&gt;
    &lt;head rend="h3"&gt;B. Developer Usage&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude Code Max 5 user ($100/month): 2 hours/day heavy coding &lt;list rend="ul"&gt;&lt;item&gt;~2M input tokens, ~30k output tokens/day&lt;/item&gt;&lt;item&gt;Heavy input token usage (cheap parallel processing) + minimal output&lt;/item&gt;&lt;item&gt;Actual cost: ~$4.92/month → 20.3x markup&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Claude Code Max 10 user ($200/month): 6 hours/day very heavy usage &lt;list rend="ul"&gt;&lt;item&gt;~10M input tokens, ~100k output tokens/day&lt;/item&gt;&lt;item&gt;Huge number of input tokens but relatively few generated tokens&lt;/item&gt;&lt;item&gt;Actual cost: ~$16.89/month → 11.8x markup&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The developer use case is where the economics really shine. Coding agents like Claude Code naturally have a hugely asymmetric usage pattern - they input entire codebases, documentation, stack traces, multiple files, and extensive context (cheap input tokens) but only need relatively small outputs like code snippets or explanations. This plays perfectly into the cost structure where input is nearly free but output is expensive.&lt;/p&gt;
    &lt;head rend="h3"&gt;C. API Profit Margins&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Current API pricing: $3/15 per million tokens vs ~$0.01/3 actual costs&lt;/item&gt;
      &lt;item&gt;Margins: 80-95%+ gross margins&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The API business is essentially a money printer. The gross margins here are software-like, not infrastructure-like.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;We've made a lot of assumptions in this analysis, and some probably aren't right. But even if you assume we're off by a factor of 3, the economics still look highly profitable. The raw compute costs, even at retail H100 pricing, suggest that AI inference isn't the unsustainable money pit that many claim it to be.&lt;/p&gt;
    &lt;p&gt;The key insight that most people miss is just how dramatically cheaper input processing is compared to output generation. We're talking about a thousand-fold cost difference - input tokens at roughly $0.005 per million versus output tokens at $3+ per million.&lt;/p&gt;
    &lt;p&gt;This cost asymmetry explains why certain use cases are incredibly profitable while others might struggle. Heavy readers - applications that consume massive amounts of context but generate minimal output - operate in an almost free tier for compute costs. Conversational agents, coding assistants processing entire codebases, document analysis tools, and research applications all benefit enormously from this dynamic.&lt;/p&gt;
    &lt;p&gt;Video generation represents the complete opposite extreme of this cost structure. A video model might take a simple text prompt as input - maybe 50 tokens - but needs to generate millions of tokens representing each frame. The economics become brutal when you're generating massive outputs from minimal inputs, which explains why video generation remains so expensive and why these services either charge premium prices or limit usage heavily.&lt;/p&gt;
    &lt;p&gt;The "AI is unsustainably expensive" narrative may be serving incumbent interests more than reflecting economic reality. When established players emphasize massive costs and technical complexity, it discourages competition and investment in alternatives. But if our calculations are even remotely accurate, especially for input-heavy workloads, the barriers to profitable AI inference may be much lower than commonly believed.&lt;/p&gt;
    &lt;p&gt;Let's not hype the costs up so much that people overlook the raw economics. I feel everyone fell for this a decade or two ago with cloud computing costs from the hyperscalers and allowed them to become money printers. If we're not careful we'll end up with the same on AI inference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45050415</guid></item><item><title>Fossjobs: A job board for Free and Open Source jobs</title><link>https://www.fossjobs.net/</link><description>&lt;doc fingerprint="1ae69789721b2f9b"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Technology Assessor&lt;/head&gt;
    &lt;p&gt;2025-08-28 - at NLnet foundation in Amsterdam, Netherlands Full-time&lt;/p&gt;
    &lt;p&gt;This is a job board exclusively for paid free &amp;amp; open source jobs: We only list jobs at organizations that improve and involve FOSS or open hardware projects. Merely using open source as part of the job is not enough.&lt;/p&gt;
    &lt;p&gt;Listings are free. Submit jobs you find! You can also send us job links to submit [(at)] fossjobs [dot] net.&lt;/p&gt;
    &lt;p&gt;Mastodon • IRC • RSS Feeds • GitHub&lt;/p&gt;
    &lt;p&gt;2025-08-28 - at NLnet foundation in Amsterdam, Netherlands Full-time&lt;/p&gt;
    &lt;p&gt;2025-08-25 - at Wikimedia Deutschland e.V. in Germany Full-time&lt;/p&gt;
    &lt;p&gt;2025-07-30 - at Free Software Foundation in Boston, MA, USA, United States Full-time&lt;/p&gt;
    &lt;p&gt;2025-07-30 - at Free Software Foundation in United States Part-time&lt;/p&gt;
    &lt;p&gt;2025-07-17 - at NetKnights GmbH in Kassel, Germany Full-time&lt;/p&gt;
    &lt;p&gt;2025-07-03 - at SYSTOPIA GmbH in Bonn, Germany Full-time&lt;/p&gt;
    &lt;p&gt;2025-06-09 - at Ruth Cheesley, Mautic — Worldwide/Remote Full-time&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45050538</guid></item><item><title>Microbial metabolite repairs liver injury by restoring hepatic lipid metabolism</title><link>https://journals.asm.org/doi/10.1128/mbio.01718-25</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45050873</guid></item><item><title>Important machine learning equations</title><link>https://chizkidd.github.io//2025/05/30/machine-learning-key-math-eqns/</link><description>&lt;doc fingerprint="6c3c4b014c3bd6b5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Most Important Machine Learning Equations: A Comprehensive Guide&lt;/head&gt;&lt;head rend="h2"&gt;Motivation&lt;/head&gt;&lt;p&gt;Machine learning (ML) is a powerful field driven by mathematics. Whether you’re building models, optimizing algorithms, or simply trying to understand how ML works under the hood, mastering the core equations is essential. This blog post is designed to be your go-to resource, covering the most critical and “mind-breaking” ML equations—enough to grasp most of the core math behind ML. Each section includes theoretical insights, the equations themselves, and practical implementations in Python, so you can see the math in action.&lt;/p&gt;&lt;p&gt;This guide is for anyone with a basic background in math and programming who wants to deepen their understanding of ML and is inspired by this tweet from @goyal__pramod. Let’s dive into the equations that power this fascinating field!&lt;/p&gt;&lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;&lt;head rend="h2"&gt;Introduction&lt;/head&gt;&lt;p&gt;Mathematics is the language of machine learning. From probability to linear algebra, optimization to advanced generative models, equations define how ML algorithms learn from data and make predictions. This blog post compiles the most essential equations, explains their significance, and provides practical examples using Python libraries like NumPy, scikit-learn, TensorFlow, and PyTorch. Whether you’re a beginner or an experienced practitioner, this guide will equip you with the tools to understand and apply ML math effectively.&lt;/p&gt;&lt;head rend="h2"&gt;Probability and Information Theory&lt;/head&gt;&lt;p&gt;Probability and information theory provide the foundation for reasoning about uncertainty and measuring differences between distributions.&lt;/p&gt;&lt;head rend="h3"&gt;Bayes’ Theorem&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[P(A|B) = \frac{P(B|A) P(A)}{P(B)}\]&lt;p&gt;Explanation: Bayes’ Theorem describes how to update the probability of a hypothesis ($A$) given new evidence ($B$). It’s a cornerstone of probabilistic reasoning and is widely used in machine learning for tasks like classification and inference.&lt;/p&gt;&lt;p&gt;Practical Use: Applied in Naive Bayes classifiers, Bayesian networks, and Bayesian optimization.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;def bayes_theorem(p_d, p_t_given_d, p_t_given_not_d):
    """
    Calculate P(D|T+) using Bayes' Theorem.
    
    Parameters:
    p_d: P(D), probability of having the disease
    p_t_given_d: P(T+|D), probability of testing positive given disease
    p_t_given_not_d: P(T+|D'), probability of testing positive given no disease
    
    Returns:
    P(D|T+), probability of having the disease given a positive test
    """
    p_not_d = 1 - p_d
    p_t = p_t_given_d * p_d + p_t_given_not_d * p_not_d
    p_d_given_t = (p_t_given_d * p_d) / p_t
    return p_d_given_t

# Example usage
p_d = 0.01  # 1% of population has the disease
p_t_given_d = 0.99  # Test is 99% sensitive
p_t_given_not_d = 0.02  # Test has 2% false positive rate
result = bayes_theorem(p_d, p_t_given_d, p_t_given_not_d) 
print(f"P(D|T+) = {result:.4f}")  # Output: P(D|T+) = 0.3333 
&lt;/code&gt;&lt;head rend="h3"&gt;Entropy&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[H(X) = -\sum_{x \in X} P(x) \log P(x)\]&lt;p&gt;Explanation: Entropy measures the uncertainty or randomness in a probability distribution. It quantifies the amount of information required to describe the distribution and is fundamental in understanding concepts like information gain and decision trees.&lt;/p&gt;&lt;p&gt;Practical Use: Used in decision trees, information gain calculations, and as a basis for other information-theoretic measures.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

def entropy(p):
    """
    Calculate entropy of a probability distribution.
    
    Parameters:
    p: Probability distribution array
    
    Returns:
    Entropy value
    """
    return -np.sum(p * np.log(p, where=p &amp;gt; 0))

# Example usage
fair_coin = np.array([0.5, 0.5])  # fair coin has the same probability of heads and tails
print(f"Entropy of fair coin: {entropy(fair_coin)}")  # Output: 0.6931471805599453 

biased_coin = np.array([0.9, 0.1])  # biased coin has a higher probability of heads
print(f"Entropy of biased coin: {entropy(biased_coin)}")  # Output: 0.4698716731013394 
&lt;/code&gt;&lt;head rend="h3"&gt;Joint and Conditional Probability&lt;/head&gt;&lt;p&gt;Equations:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Joint Probability:&lt;/p&gt;\[P(A, B) = P(A|B) P(B) = P(B|A) P(A)\]&lt;/item&gt;&lt;item&gt;&lt;p&gt;Conditional Probability:&lt;/p&gt;\[P(A|B) = \frac{P(A, B)}{P(B)}\]&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Explanation: Joint probability describes the likelihood of two events occurring together, while conditional probability measures the probability of one event given another. These are the building blocks of Bayesian methods and probabilistic models.&lt;/p&gt;&lt;p&gt;Practical Use: Used in Naive Bayes classifiers and probabilistic graphical models.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;from sklearn.naive_bayes import GaussianNB
import numpy as np

X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
y = np.array([0, 0, 1, 1])
model = GaussianNB().fit(X, y)
print(model.predict([[2.5, 3.5]]))  # Output: [1]
&lt;/code&gt;&lt;head rend="h3"&gt;Kullback-Leibler Divergence (KLD)&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[D_{KL}(P \| Q) = \sum_{x \in \mathcal{X}} P(x) \log \left( \frac{P(x)}{Q(x)} \right)\]&lt;p&gt;Explanation: KLD measures how much one probability distribution $P$ diverges from another $Q$. It’s asymmetric and foundational in information theory and generative models.&lt;/p&gt;&lt;p&gt;Practical Use: Used in variational autoencoders (VAEs) and model evaluation.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

P = np.array([0.7, 0.3])
Q = np.array([0.5, 0.5])
kl_div = np.sum(P * np.log(P / Q))
print(f"KL Divergence: {kl_div}")  # Output: 0.08228287850505156
&lt;/code&gt;&lt;head rend="h3"&gt;Cross-Entropy&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[H(P, Q) = -\sum_{x \in \mathcal{X}} P(x) \log Q(x)\]&lt;p&gt;Explanation: Cross-entropy quantifies the difference between the true distribution $P$ and the predicted distribution $Q$. It’s a widely used loss function in classification.&lt;/p&gt;&lt;p&gt;Practical Use: Drives training in logistic regression and neural networks.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

y_true = np.array([1, 0, 1])
y_pred = np.array([0.9, 0.1, 0.8])
cross_entropy = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
print(f"Cross-Entropy: {cross_entropy}")  # Output: 0.164252033486018
&lt;/code&gt;&lt;head rend="h2"&gt;Linear Algebra&lt;/head&gt;&lt;p&gt;Linear algebra powers the transformations and structures in ML models.&lt;/p&gt;&lt;head rend="h3"&gt;Linear Transformation&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[y = Ax + b \quad \text{where } A \in \mathbb{R}^{m \times n}, x \in \mathbb{R}^n, y \in \mathbb{R}^m, b \in \mathbb{R}^m\]&lt;p&gt;Explanation: This equation represents a linear mapping of input $x$ to output $y$ via matrix $A$ and bias $b$. It’s the core operation in neural network layers.&lt;/p&gt;&lt;p&gt;Practical Use: Foundational for linear regression and neural networks.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

A = np.array([[2, 1], [1, 3]])
x = np.array([1, 2])
b = np.array([0, 1])
y = A @ x + b
print(y)  # Output: [4 7]
&lt;/code&gt;&lt;head rend="h3"&gt;Eigenvalues and Eigenvectors&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[Av = \lambda v \quad \text{where } \lambda \in \mathbb{R}, v \in \mathbb{R}^n, v \neq 0\]&lt;p&gt;Explanation: Eigenvalues $\lambda$ and eigenvectors $v$ describe how a matrix $A$ scales and rotates space, crucial for understanding data variance.&lt;/p&gt;&lt;p&gt;Practical Use: Used in Principal Component Analysis (PCA).&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

A = np.array([[4, 2], [1, 3]])
eigenvalues, eigenvectors = np.linalg.eig(A)
print(f"Eigenvalues: {eigenvalues}")
print(f"Eigenvectors:\n{eigenvectors}")
&lt;/code&gt;&lt;head rend="h3"&gt;Singular Value Decomposition (SVD)&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[A = U \Sigma V^T\]&lt;p&gt;Explanation: SVD breaks down a matrix $A$ into orthogonal matrices $U$ and $V$ and a diagonal matrix $\Sigma$ of singular values. It reveals the intrinsic structure of data.&lt;/p&gt;&lt;p&gt;Practical Use: Applied in dimensionality reduction and recommendation systems.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

A = np.array([[1, 2], [3, 4], [5, 6]])
U, S, Vt = np.linalg.svd(A)
print(f"U:\n{U}\nS: {S}\nVt:\n{Vt}")
&lt;/code&gt;&lt;head rend="h2"&gt;Optimization&lt;/head&gt;&lt;p&gt;Optimization is how ML models learn from data.&lt;/p&gt;&lt;head rend="h3"&gt;Gradient Descent&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[\theta_{t+1} = \theta_t - \eta \nabla_{\theta} L(\theta)\]&lt;p&gt;Explanation: Gradient descent updates parameters $\theta$ by moving opposite to the gradient of the loss function $L$, scaled by learning rate $\eta$.&lt;/p&gt;&lt;p&gt;Practical Use: The backbone of training most ML models.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

def gradient_descent(X, y, lr=0.01, epochs=1000):
    m, n = X.shape
    theta = np.zeros(n)
    for _ in range(epochs):
        gradient = (1/m) * X.T @ (X @ theta - y)
        theta -= lr * gradient
    return theta

X = np.array([[1, 1], [1, 2], [1, 3]])
y = np.array([1, 2, 3])
theta = gradient_descent(X, y)
print(theta)  # Output: ~[0., 1.]
&lt;/code&gt;&lt;head rend="h3"&gt;Backpropagation&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial a_j} \cdot \frac{\partial a_j}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_{ij}}\]&lt;p&gt;Explanation: Backpropagation applies the chain rule to compute gradients of the loss $L$ with respect to weights $w_{ij}$ in neural networks.&lt;/p&gt;&lt;p&gt;Practical Use: Enables efficient training of deep networks.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import torch
import torch.nn as nn

model = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())
loss_fn = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

X = torch.tensor([[0., 0.], [1., 1.]], dtype=torch.float32)
y = torch.tensor([[0.], [1.]], dtype=torch.float32)

optimizer.zero_grad()
output = model(X)
loss = loss_fn(output, y)
loss.backward()
optimizer.step()
print(f"Loss: {loss.item()}")
&lt;/code&gt;&lt;head rend="h2"&gt;Loss Functions&lt;/head&gt;&lt;p&gt;Loss functions measure model performance and guide optimization.&lt;/p&gt;&lt;head rend="h3"&gt;Mean Squared Error (MSE)&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[\text{MSE} = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\]&lt;p&gt;Explanation: MSE calculates the average squared difference between true $y_i$ and predicted $\hat{y}_i$ values, penalizing larger errors more heavily.&lt;/p&gt;&lt;p&gt;Practical Use: Common in regression tasks.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

y_true = np.array([1, 2, 3])
y_pred = np.array([1.1, 1.9, 3.2])
mse = np.mean((y_true - y_pred)**2)
print(f"MSE: {mse}")  # Output: 0.01
&lt;/code&gt;&lt;head rend="h3"&gt;Cross-Entropy Loss&lt;/head&gt;&lt;p&gt;(See Cross-Entropy above for details.)&lt;/p&gt;&lt;head rend="h2"&gt;Advanced ML Concepts&lt;/head&gt;&lt;p&gt;These equations power cutting-edge ML techniques.&lt;/p&gt;&lt;head rend="h3"&gt;Diffusion Process&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[x_t = \sqrt{\alpha_t} x_0 + \sqrt{1 - \alpha_t} \epsilon \quad \text{where} \quad \epsilon \sim \mathcal{N}(0, I)\]&lt;p&gt;Explanation: This describes a forward diffusion process where data $x_0$ is gradually noised over time $t$, a key idea in diffusion models.&lt;/p&gt;&lt;p&gt;Practical Use: Used in generative AI like image synthesis.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import torch

x_0 = torch.tensor([1.0])
alpha_t = 0.9
noise = torch.randn_like(x_0)
x_t = torch.sqrt(torch.tensor(alpha_t)) * x_0 + torch.sqrt(torch.tensor(1 - alpha_t)) * noise
print(f"x_t: {x_t}")
&lt;/code&gt;&lt;head rend="h3"&gt;Convolution Operation&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[(f * g)(t) = \int f(\tau) g(t - \tau) \, d\tau\]&lt;p&gt;Explanation: Convolution combines two functions by sliding one over the other, extracting features in data like images.&lt;/p&gt;&lt;p&gt;Practical Use: Core to convolutional neural networks (CNNs).&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import torch
import torch.nn as nn

conv = nn.Conv2d(1, 1, kernel_size=3)
image = torch.randn(1, 1, 28, 28)
output = conv(image)
print(output.shape)  # Output: torch.Size([1, 1, 26, 26])
&lt;/code&gt;&lt;head rend="h3"&gt;Softmax Function&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[\sigma(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}\]&lt;p&gt;Explanation: Softmax converts raw scores $z_i$ into probabilities, summing to 1, ideal for multi-class classification.&lt;/p&gt;&lt;p&gt;Practical Use: Used in neural network outputs.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import numpy as np

z = np.array([1.0, 2.0, 3.0])
softmax = np.exp(z) / np.sum(np.exp(z))
print(f"Softmax: {softmax}")  # Output: [0.09003057 0.24472847 0.66524096]
&lt;/code&gt;&lt;head rend="h3"&gt;Attention Mechanism&lt;/head&gt;&lt;p&gt;Equation:&lt;/p&gt;\[\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V\]&lt;p&gt;Explanation: Attention computes a weighted sum of values $V$ based on the similarity between queries $Q$ and keys $K$, scaled by $\sqrt{d_k}$.&lt;/p&gt;&lt;p&gt;Practical Use: Powers transformers in NLP and beyond.&lt;/p&gt;&lt;p&gt;Implementation:&lt;/p&gt;&lt;code&gt;import torch

def attention(Q, K, V):
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))
    attn = torch.softmax(scores, dim=-1)
    return torch.matmul(attn, V)

Q = torch.tensor([[1., 0.], [0., 1.]])
K = torch.tensor([[1., 1.], [1., 0.]])
V = torch.tensor([[0., 1.], [1., 0.]])
output = attention(Q, K, V)
print(output)
&lt;/code&gt;&lt;head rend="h2"&gt;Conclusion&lt;/head&gt;&lt;p&gt;This blog post has explored the most critical equations in machine learning, from foundational probability and linear algebra to advanced concepts like diffusion and attention. With theoretical explanations, practical implementations, and visualizations, you now have a comprehensive resource to understand and apply ML math. Point anyone asking about core ML math here—they’ll learn 95% of what they need in one place!&lt;/p&gt;&lt;head rend="h2"&gt;Further Reading&lt;/head&gt;&lt;list rend="ul"&gt;&lt;item&gt;Pattern Recognition and Machine Learning by Christopher Bishop&lt;/item&gt;&lt;item&gt;Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville&lt;/item&gt;&lt;item&gt;Stanford CS229: Machine Learning&lt;/item&gt;&lt;item&gt;PyTorch Tutorials&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45050931</guid></item><item><title>GAN Math (2020)</title><link>https://jaketae.github.io/study/gan-math/</link><description>&lt;doc fingerprint="2b939a13243d9bb9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Math Behind GANs&lt;/head&gt;&lt;p&gt;Generative Adversarial Networks refer to a family of generative models that seek to discover the underlying distribution behind a certain data generating process. This distribution is discovered through an adversarial competition between a generator and a discriminator. As we saw in an earlier introductory post on GANs, the two models are trained such that the discriminator strives to distinguish between generated and true examples, while the generator seeks to confuse the discriminator by producing data that are as realistic and compelling as possible.&lt;/p&gt;&lt;p&gt;In this post, we’ll take a deep dive into the math behind GANs. My primary source of reference is Generative Adversarial Nets by Ian Goodfellow, et al. It is in this paper that Goodfellow first outlined the concept of a GAN, which is why it only makes sense that we commence from the analysis of this paper. Let’s begin!&lt;/p&gt;&lt;head rend="h1"&gt;Motivating the Loss Function&lt;/head&gt;&lt;p&gt;GAN can be seen as an interplay between two different models: the generator and the discriminator. Therefore, each model will have its own loss function. In this section, let’s try to motivate an intuitive understanding of the loss function for each.&lt;/p&gt;&lt;head rend="h2"&gt;Notation&lt;/head&gt;&lt;p&gt;To minimize confusion, let’s define some notation that we will be using throughout this post.&lt;/p&gt;\[\begin{multline} \shoveleft x: \text{Real data} \\ \shoveleft z: \text{Latent vector} \\ \shoveleft G(z): \text{Fake data} \\ \shoveleft D(x): \text{Discriminator's evaluation of real data} \\ \shoveleft D(G(z)): \text{Discriminator's evaluation of fake data} \\ \shoveleft \text{Error}(a, b): \text{Error between } a \text{ and } b\\ \end{multline}\]&lt;head rend="h2"&gt;The Discriminator&lt;/head&gt;&lt;p&gt;The goal of the discriminator is to correctly label generated images as false and empirical data points as true. Therefore, we might consider the following to be the loss function of the discriminator:&lt;/p&gt;\[L_D = \text{Error}(D(x), 1) + \text{Error}(D(G(z)), 0) \tag{1}\]&lt;p&gt;Here, we are using a very generic, unspecific notation for $\text{Error}$ to refer to some function that tells us the distance or the difference between the two functional parameters. (If this reminded you of something like cross entropy or Kullback-Leibler divergence, you are definitely on the right track.)&lt;/p&gt;&lt;head rend="h2"&gt;The Generator&lt;/head&gt;&lt;p&gt;We can go ahead and do the same for the generator. The goal of the generator is to confuse the discriminator as much as possible such that it mislabels generated images as being true.&lt;/p&gt;\[L_G = \text{Error}(D(G(z)), 1) \tag{2}\]&lt;p&gt;The key here is to remember that a loss function is something that we wish to minimize. In the case of the generator, it should strive to minimize the difference between 1, the label for true data, and the discriminator’s evaluation of the generated fake data.&lt;/p&gt;&lt;head rend="h2"&gt;Binary Cross Entropy&lt;/head&gt;&lt;p&gt;A common loss function that is used in binary classification problems is binary cross entropy. As a quick review, let’s remind ourselves of what the formula for cross entropy looks like:&lt;/p&gt;\[H(p, q) = \mathbb{E}_{x \sim p(x)}[- \log q(x)] \tag{3}\]&lt;p&gt;In classification tasks, the random variable is discrete. Hence, the expectation can be expressed as a summation.&lt;/p&gt;\[H(p, q) = - \sum_{x \in \chi} p(x) \log q(x) \tag{4}\]&lt;p&gt;We can simplify this expression even further in the case of binary cross entropy, since there are only two labels: zero and one.&lt;/p&gt;\[H(y, \hat{y}) = - \sum y \log(\hat{y}) + (1 - y) \log(1 - \hat{y}) \tag{5}\]&lt;p&gt;This is the $\text{Error}$ function that we have been loosely using in the sections above. Binary cross entropy fulfills our objective in that it measures how different two distributions are in the context of binary classification of determining whether an input data point is true or false. Applying this to the loss functions in (1),&lt;/p&gt;\[L_D = - \sum_{x \in \chi, z \in \zeta} \log(D(x)) + \log(1 - D(G(z))) \tag{6}\]&lt;p&gt;We can do the same for (2):&lt;/p&gt;\[L_G = - \sum_{z \in \zeta} \log(D(G(z)) \tag{7}\]&lt;p&gt;Now we have two loss functions with which to train the generator and the discriminator! Note that, for the loss function of the generator, the loss is small if $D(G(z))$ is close to 1, since $\log(1) = 0$. This is exactly the sort of behavior we want from a loss function for the generator. It isn’t difficult to see the cogency of (6) with a similar approach.&lt;/p&gt;&lt;head rend="h2"&gt;Minor Caveats&lt;/head&gt;&lt;p&gt;The original paper by Goodfellow presents a slightly different version of the two loss functions derived above.&lt;/p&gt;\[\max_D \{ \log(D(x)) + \log(1-D(G(z))) \} \tag{8}\]&lt;p&gt;Essentially, the difference between (6) and (8) is the difference in sign, and whether we want to minimize or maximize a given quantity. In (6), we framed the function as a loss function to be minimized, whereas the original formulation presents it as a maximization problem, with the sign obviously flipped.&lt;/p&gt;&lt;p&gt;Then, Goodfellow proceeds by framing (8) as a min-max game, where the discriminator seeks to maximize the given quantity whereas the generator seeks to achieve the reverse. In other words,&lt;/p&gt;\[\min_G \max_D \{ \log(D(x)) + \log(1-D(G(z))) \} \tag{9}\]&lt;p&gt;The min-max formulation is a concise one-liner that intuitively demonstrates the adversarial nature of thecompetition between the generator and the discriminator. However, in practice, we define separate loss functions for the generator and the discriminator as we have done above. This is because the gradient of the function $y = \log x$ is steeper near $x = 0$ than that of the function $y = \log (1 - x)$, meaning that trying to maximize $\log(D(G(z)))$, or equivalently, minimizing $- \log(D(G(z)))$ is going to lead to quicker, more substantial improvements to the performance of the generator than trying to minimize $\log(1 - D(G(z)))$.&lt;/p&gt;&lt;head rend="h1"&gt;Model Optimization&lt;/head&gt;&lt;p&gt;Now that we have defined the loss functions for the generator and the discriminator, it’s time to leverage some math to solve the optimization problem, i.e. finding the parameters for the generator and the discriminator such that the loss functions are optimized. This corresponds to training the model in practical terms.&lt;/p&gt;&lt;head rend="h2"&gt;Training the Discriminator&lt;/head&gt;&lt;p&gt;When training a GAN, we typically train one model at a time. In other words, when training the discriminator, the generator is assumed as fixed. We saw this in action in the previous post on how to build a basic GAN.&lt;/p&gt;&lt;p&gt;Let’s return back to the min-max game. The quantity of interest can be defined as a function of $G$ and $D$. Let’s call this the value function:&lt;/p&gt;\[V(G, D) = \mathbb{E}_{x \sim p_{data}}[\log(D(x))] + \mathbb{E}_{z \sim p_z}[\log(1 - D(G(z)))] \tag{10}\]&lt;p&gt;In reality, we are more interested in the distribution modeled by the generator than $p_z$. Therefore, let’s create a new variable, $y = G(z)$, and use this substitution to rewrite the value function:&lt;/p&gt;\[\begin{align} V(G, D) &amp;amp;= \mathbb{E}_{x \sim p_{data}}[\log(D(x))] + \mathbb{E}_{y \sim p_g}[\log(1 - D(y))] \\ &amp;amp;= \int_{x \in \chi} p_{data}(x) \log(D(x)) + p_g(x) \log(1 - D(x)) \, dx \end{align} \tag{11}\]&lt;p&gt;The goal of the discriminator is to maximize this value function. Through a partial derivative of $V(G, D)$ with respect to $D(x)$, we see that the optimal discriminator, denoted as $D^*(x)$, occurs when&lt;/p&gt;\[\frac{p_{data}(x)}{D(x)} - \frac{p_g(x)}{1 - D(x)} = 0 \tag{12}\]&lt;p&gt;Rearranging (12), we get&lt;/p&gt;\[D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \tag{12}\]&lt;p&gt;And this is the condition for the optimal discriminator! Note that the formula makes intuitive sense: if some sample $x$ is highly genuine, we would expect $p_{data}(x)$ to be close to one and $p_g(x)$ to be converge to zero, in which case the optimal discriminator would assign 1 to that sample. On the other hand, for a generated sample $x = G(z)$, we expect the optimal discriminator to assign a label of zero, since $p_{data}(G(z))$ should be close to zero.&lt;/p&gt;&lt;head rend="h2"&gt;Training the Generator&lt;/head&gt;&lt;p&gt;To train the generator, we assume the discriminator to be fixed and proceed with the analysis of the value function. Let’s first plug in the result we found above, namely (12), into the value function to see what turns out.&lt;/p&gt;\[\begin{align} V(G, D^*) &amp;amp;= \mathbb{E}_{x \sim p_{data}}[\log(D^*(x))] + \mathbb{E}_{x \sim p_g}[\log(1 - D^*(x))] \\ &amp;amp;= \mathbb{E}_{x \sim p_{data}} \left[ \log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \right] + \mathbb{E}_{x \sim p_g} \left[ \log \frac{p_g(x)}{p_{data}(x) + p_g(x)} \right] \end{align} \tag{13}\]&lt;p&gt;To proceed from here, we need a little bit of inspiration. Little clever tricks like these are always a joy to look at.&lt;/p&gt;\[\begin{align} V(G, D^*) &amp;amp;= \mathbb{E}_{x \sim p_{data}} \left[ \log \frac{p_{data}(x)}{p_{data}(x) + p_g(x)} \right] + \mathbb{E}_{x \sim p_g} \left[ \log \frac{p_g(x)}{p_{data}(x) + p_g(x)} \right] \\ &amp;amp;= - \log 4 + \mathbb{E}_{x \sim p_{data}} \left[ \log p_{data}(x) - \log \frac{p_{data}(x) + p_g(x))}{2} \right] \\ &amp;amp; \quad+ \mathbb{E}_{x \sim p_g} \left[ \log p_g(x) - \log\frac{p_{data}(x) + p_g(x))}{2} \right] \end{align} \tag{14}\]&lt;p&gt;If you are confused, don’t worry, you aren’t the only one. Basically, what is happening is that we are exploiting the properties of logarithms to pull out a $- \log4$ that previously did not exist. In pulling out this number, we inevitably apply changes to the terms in the expectation, specifically by dividing the denominator by two.&lt;/p&gt;&lt;p&gt;Why was this necessary? The magic here is that we can now interpret the expectations as Kullback-Leibler divergence:&lt;/p&gt;\[V(G, D^*) = - \log 4 + D_{KL}\left(p_{data} \parallel \frac{p_{data} + p_g}{2} \right) + D_{KL}\left(p_g \parallel \frac{p_g + p_g}{2} \right) \tag{15}\]&lt;p&gt;And it is here that we reencounter the Jensen-Shannon divergence, which is defined as&lt;/p&gt;\[J(P,Q) = \frac{1}{2} \left( D(P \parallel R) + D(Q \parallel R) \right) \tag{16}\]&lt;p&gt;where $R = \frac12(P + Q)$. This means that the expression in (15) can be expressed as a JS divergence:&lt;/p&gt;\[V(G, D^*) = - \log 4 + 2 \cdot D_{JS}(p_{data} \parallel p_g) \tag{15}\]&lt;p&gt;The conclusion of this analysis is simple: the goal of training the generator, which is to minimize the value function $V(G, D)$, we want the JS divergence between the distribution of the data and the distribution of generated examples to be as small as possible. This conclusion certainly aligns with our intuition: we want the generator to be able to learn the underlying distribution of the data from sampled training examples. In other words, $p_g$ and $p_{data}$ should be as close to each other as possible. The optimal generator $G$ is thus one that which is able to mimic $p_{data}$ to model a compelling model distribution $p_g$.&lt;/p&gt;&lt;head rend="h1"&gt;Conclusion&lt;/head&gt;&lt;p&gt;In this post, we took a brief tour of the math behind general adversarial networks. Since the publication of Goodfellow’s work, more GAN models have been introduced and studied by different scholars, such as the Wasserstein GAN or CycleGAN to name just a few. The underlying mathematics for these models are obviously going to be different from what we have seen today, but this is a good starting point nonetheless.&lt;/p&gt;&lt;p&gt;I hope you enjoyed reading this post. In the next post, I plan to explore the concept of Fisher information and the Fisher matrix. It is going to be another math-heavy ride with gradients and Hessians, so keep you belts fastened!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45050958</guid></item><item><title>Prosper AI (YC S23) Is Hiring Founding Account Executives (NYC)</title><link>https://jobs.ashbyhq.com/prosper-ai/29684590-4cec-4af2-bb69-eb5c6d595fb8</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45051096</guid></item><item><title>Rendering a Game in Real-Time with AI</title><link>https://blog.jeffschomay.com/rendering-a-game-in-real-time-with-ai</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45051188</guid></item><item><title>Group Borrowing: Zero-Cost Memory Safety with Fewer Restrictions</title><link>https://verdagon.dev/blog/group-borrowing</link><description>&lt;doc fingerprint="aea9a886bddb8b18"&gt;
  &lt;main&gt;
    &lt;p&gt;If you've read my blog before, you know that memory safety is a huge unsolved problem, and that there's a vast unexplored space between the various memory safety models. The discerning eye can infer that we're starting to see the lines blur between these seemingly unrelated memory safety approaches.&lt;/p&gt;
    &lt;p&gt;This is ridiculously exciting to me, because today's popular memory-safe languages are very limited: they're fast, or they're flexible, but not both. Finding new blends is an incredibly challenging and worthy endeavor... one which has claimed the sanity of many explorers.&lt;/p&gt;
    &lt;p&gt;A few of us have been approaching the problem by starting with reference counting or garbage collection (or generational references!) and allowing functions to "borrow" those objects with much less--or zero--overhead. 0&lt;/p&gt;
    &lt;p&gt;In my biased 1 opinion, these approaches have some strong benefits. But they're not a panacea, and the world needs more approaches here.&lt;/p&gt;
    &lt;p&gt;And luckily, my good friend Nick Smith (from the Mojo community 2) has been exploring exactly that for the past few years.&lt;/p&gt;
    &lt;p&gt;I think he's found a way to add mutable aliasing directly into a borrow checker without building on a foundation of reference counting, garbage collection, or generational references. In other words, an approach for zero-overhead mutable aliasing, which is a big deal. 3&lt;/p&gt;
    &lt;p&gt;After reading his original explanation here, I knew that it should definitely be more widely known. He's graciously allowed me to take a shot at explaining it, so here we are!&lt;/p&gt;
    &lt;p&gt;I'll try to explain the approach as simply as possible, but if you have any questions, Nick can be found in the Mojo server (username nick.sm), or feel free to ask me in the r/vale subreddit or the Vale discord's #languages channel. And if you find this interesting, consider sponsoring Nick!&lt;/p&gt;
    &lt;p&gt;Also, this article is gloriously long and has a lot of context, so I'll let you know when to skip ahead.&lt;/p&gt;
    &lt;p&gt;For example, blending GC with arenas, blending RC with region borrowing, or Vale's approach of blending generational references with region borrowing, and so on.&lt;/p&gt;
    &lt;p&gt;My language Vale uses one of these approaches, so I'm of course biased to see its benefits more strongly than others!&lt;/p&gt;
    &lt;p&gt;Disclaimer: I work on the Mojo team at Modular! But I'll keep this post more about Nick's discovery in general, rather than how it would do in any specific language.&lt;/p&gt;
    &lt;p&gt;My long-time readers will recognize my cognitive dissonance here because I think such a thing is a myth. Nick's approach makes me question that, though. At the very least, we're much closer to achieving the myth, if he hasn't solved it completely already.&lt;/p&gt;
    &lt;p&gt;TL;DR: Nick's approach is based on single ownership, like C++ and Rust. Every value is "owned" by a containing object, array, stack frame, or global.&lt;/p&gt;
    &lt;p&gt;If you know how C++ and Rust work already, skip ahead!&lt;/p&gt;
    &lt;p&gt;If you don't know, or just like reading, I'll explain what single ownership is.&lt;/p&gt;
    &lt;p&gt;For example if we have this C++ program:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;vector&amp;gt;
struct Engine { int fuel; };
struct Spaceship { unique_ptr&amp;lt;Engine&amp;gt; engine; };
void foo(vector&amp;lt;Spaceship&amp;gt;* ships) { ... }
void main() {
    vector&amp;lt;Spaceship&amp;gt; ships;
    ...
    foo(&amp;amp;ships);
}&lt;/code&gt;
    &lt;p&gt;...we can say this:&lt;/p&gt;
    &lt;p&gt;If you've coded in C++ or Rust, you're probably familiar with this mindset.&lt;/p&gt;
    &lt;p&gt;If you've coded in C, you might think like this too, even though C doesn't explicitly track single ownership. If you trace an object's journey all the way from its malloc() call to its free() call, all of the variables/fields that the pointer passes through are dealing with the "owning pointer", so to speak. It's almost like how detectives track the "chain of custody" for evidence. In other words, who is responsible for it at any given moment.&lt;/p&gt;
    &lt;p&gt;Heck, even Java and C# programmers sometimes think in terms of single ownership. If you're supposed to call an object's "dispose"/"cleanup"/"destroy"/"unregister"/etc. method at some point, you can trace that object's journey all the way from new to that (conceptually destructive) method call, and those are the variables/fields that are handling its "owning reference", so to speak.&lt;/p&gt;
    &lt;p&gt;Single ownership, as explained so far, is the foundation for a lot of languages:&lt;/p&gt;
    &lt;p&gt;Nick's system is the main topic of this article, but for some context, and to know why Nick's system stands out, let's take a quick detour to recap how Rust's borrow checking works.&lt;/p&gt;
    &lt;p&gt;To truly appreciate Nick's approach, it's helpful to know the limitations of Rust's borrow checker.&lt;/p&gt;
    &lt;p&gt;TL;DR: Rust's borrow checker has the "aliasing xor mutable" rule which makes it conservative. This means it rejects a lot of valid programs and useful patterns 4 and it causes accidental complexity for some use cases. 5&lt;/p&gt;
    &lt;p&gt;If you're already familiar with Rust's limitations, skip ahead to Nick's approach!&lt;/p&gt;
    &lt;p&gt;If not, here's a very simplified explanation of Rust's borrow checking, and I'll overview the limitations in the next section.&lt;/p&gt;
    &lt;p&gt;I'll assume some knowledge of modern C++, but if you're primarily a C programmer, check out this post instead.&lt;/p&gt;
    &lt;p&gt;There are two kinds of references: readwrite, and readonly. These are often called "mutable" and "immutable" (or more accurately "unique" and "shared") but for now, think of them as readwrite and readonly.&lt;/p&gt;
    &lt;p&gt;There are a few ways to get a readwrite reference:&lt;/p&gt;
    &lt;p&gt;Using these is pretty restrictive. Because of that first rule:&lt;/p&gt;
    &lt;p&gt;Now, let's introduce "readonly" references. They operate by different rules:&lt;/p&gt;
    &lt;p&gt;Rust adds some quality-of-life improvements to make this a little easier. For example, you can get a bunch of immutable references directly from an owned object. It's actually not that bad if you're writing a program that inherently agrees with the rules, like compilers, games using ECS, stateless web servers, or generally anything that transforms input data to output data.&lt;/p&gt;
    &lt;p&gt;Like observers, intrusive data structures, back-references and graphs (like doubly-linked lists), delegates, etc.&lt;/p&gt;
    &lt;p&gt;Like mobile/web apps, games using EC, or stateful servers... generally, things that inherently require a lot of state.&lt;/p&gt;
    &lt;p&gt;One can't improve on a paradigm unless they know its limitations. So let's talk about borrow checking's limitations!&lt;/p&gt;
    &lt;p&gt;Because of those "inaccessible" rules, we can never have a readwrite reference and a readonly reference to an object at the same time. This restriction is known as "aliasability xor mutability".&lt;/p&gt;
    &lt;p&gt;In theory this doesn't sound like a problem, but in practice it means you can't implement a lot of useful patterns like observers, intrusive data structures, back-references, graphs (like doubly-linked lists), delegates, etc. and it causes accidental complexity for use cases like mobile/web apps, games using EC, or stateful servers... generally, things that inherently require a lot of state.&lt;/p&gt;
    &lt;p&gt;But borrow checking is generally worth it, because it means we get memory safety without run-time overhead.&lt;/p&gt;
    &lt;p&gt;Well, mostly.&lt;/p&gt;
    &lt;p&gt;Like I explain in this post, it's not really free; even if you avoid Rc/RefCell/etc., borrow checking can often incur hidden costs, like extra bounds checking or potentially expensive cloning and hashing.&lt;/p&gt;
    &lt;p&gt;The borrow checker has long been known to reject programs that are actually safe, causing you to add and change code to satisfy its constraints. When this happens, one might just shrug and say "the borrow checker is conservative," but in reality, the borrow checker is imposing accidental complexity.&lt;/p&gt;
    &lt;p&gt;And besides, we know that mutable aliasing doesn't conflict with zero-cost memory safety, as we learned from the Arrrlang thought experiment. The only question is... can we get the best of both worlds?&lt;/p&gt;
    &lt;p&gt;(Or skip ahead to Nick's approach if you understood the above!)&lt;/p&gt;
    &lt;p&gt;Here's an example (source):&lt;/p&gt;
    &lt;code&gt;struct Entity {
    hp: u64,
    energy: u64,
}
impl Entity { ... }
fn attack(a: &amp;amp;mut Entity, d: &amp;amp;mut Entity) { ... }
fn main() {
    let mut entities = vec![
        Entity { hp: 10, energy: 10 },
        Entity { hp: 12, energy: 7 }
    ];
    attack(&amp;amp;mut entities[0], &amp;amp;mut entities[1]);
}&lt;/code&gt;
    &lt;p&gt;Rust rejects this, giving this output:&lt;/p&gt;
    &lt;code&gt;error[E0499]: cannot borrow `entities` as mutable more than once at a time
  --&amp;gt; src/main.rs:35:35
   |
35 |     attack(&amp;amp;mut entities[0], &amp;amp;mut entities[1]);
   |     ------      --------          ^^^^^^^^ second mutable borrow occurs here
   |     |           |
   |     |           first mutable borrow occurs here
   |     first borrow later used by call
   |
   = help: use `.split_at_mut(position)` to obtain two mutable non-overlapping sub-slices&lt;/code&gt;
    &lt;p&gt;Alas, .split_at_mut isn't always great in practice (reasons vary) 6 and besides, we sometimes want to have two &amp;amp;mut referring to the same object.&lt;/p&gt;
    &lt;p&gt;The more universal workaround is to use IDs and a central collection, like this (source, uses slotmap):&lt;/p&gt;
    &lt;code&gt;fn attack(
    entities: &amp;amp;mut SlotMap&amp;lt;DefaultKey, Entity&amp;gt;,
    attacker_id: DefaultKey,
    defender_id: DefaultKey
) -&amp;gt; Result&amp;lt;(), String&amp;gt; {
    let a = entities
        .get(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    let d = entities
        .get(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;

    let a_energy_cost = a.calculate_attack_cost(d);
    let d_energy_cost = d.calculate_defend_cost(a);
    let damage = a.calculate_damage(d);

    let a_mut = entities
        .get_mut(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    a_mut.use_energy(a_energy_cost);

    let d_mut = entities
        .get_mut(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;
    d_mut.use_energy(d_energy_cost);
    d_mut.damage(damage);

    Ok(())
}&lt;/code&gt;
    &lt;p&gt;This is using the slotmap crate (similar to generational_arena), though you often see this pattern with HashMap instead (or one could also use raw indices into a Vec, though that risks use-after-release problems).&lt;/p&gt;
    &lt;p&gt;If you want it to be more efficient, you might be tempted to get two mutable references up-front:&lt;/p&gt;
    &lt;code&gt;fn attack(
    entities: &amp;amp;mut SlotMap&amp;lt;DefaultKey, Entity&amp;gt;,
    attacker_id: DefaultKey,
    defender_id: DefaultKey
) -&amp;gt; Result&amp;lt;(), String&amp;gt; {
    let a = entities
        .get_mut(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    let d = entities
        .get_mut(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;
    let a_energy_cost = a.calculate_attack_cost(d);
    let d_energy_cost = d.calculate_defend_cost(a);
    let damage = a.calculate_damage(d);
    a.use_energy(a_energy_cost);
    d.use_energy(d_energy_cost);
    d.damage(damage);
    Ok(())
}&lt;/code&gt;
    &lt;p&gt;But alas, rustc complains:&lt;/p&gt;
    &lt;code&gt;error[E0499]: cannot borrow `*entities` as mutable more than once at a time
  --&amp;gt; src/main.rs:34:13
   |
31 |     let a = entities
   |             -------- first mutable borrow occurs here
...
34 |     let d = entities
   |             ^^^^^^^^ second mutable borrow occurs here
...
37 |     let a_energy_cost = a.calculate_attack_cost(d);
   |                         - first borrow later used here&lt;/code&gt;
    &lt;p&gt;...because we're mutably borrowing entities twice: once in a's get_mut call, and once in d's get_mut call, and their usages overlap.&lt;/p&gt;
    &lt;p&gt;Or, said differently, it's worried that a and d might be pointing to the same Entity, thus violating aliasability-xor-mutability.&lt;/p&gt;
    &lt;p&gt;But why is a compiler telling me that an Entity can't attack itself? That's odd, because in this game, that's totally allowed. Even pokémon can hurt themselves in their confusion.&lt;/p&gt;
    &lt;p&gt;One might say, "because that's a memory safety risk!" But that's not necessarily true. From what I can tell, that code would be just fine, and not risk memory safety. And in fact, Nick's system handles it just fine.&lt;/p&gt;
    &lt;p&gt;So let's take a look at Nick's system!&lt;/p&gt;
    &lt;p&gt;For example, if you need N references instead of just 2, or they don't need to be / shouldn't be distinct, or you want to still hold a reference while also holding those N references, etc.&lt;/p&gt;
    &lt;p&gt;As I explain Nick's system, please keep in mind:&lt;/p&gt;
    &lt;p&gt;Our goal is to write something like the Rust attack function from the last section:&lt;/p&gt;
    &lt;code&gt;fn attack(
    entities: &amp;amp;mut SlotMap&amp;lt;DefaultKey, Entity&amp;gt;,
    attacker_id: DefaultKey,
    defender_id: DefaultKey
) -&amp;gt; Result&amp;lt;(), String&amp;gt; {
    let a = entities
        .get(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    let d = entities
        .get(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;

    let a_energy_cost = a.calculate_attack_cost(d);
    let d_energy_cost = d.calculate_defend_cost(a);
    let damage = a.calculate_damage(d);

    let a_mut = entities
        .get_mut(attacker_id)
        .ok_or_else(|| "Attacker not found in entities map".to_string())?;
    a_mut.use_energy(a_energy_cost);

    let d_mut = entities
        .get_mut(defender_id)
        .ok_or_else(|| "Defender not found in entities map".to_string())?;
    d_mut.use_energy(d_energy_cost);
    d_mut.damage(damage);

    Ok(())
}&lt;/code&gt;
    &lt;p&gt;But we're going to write it with memory-safe mutable aliasing, so it's simpler and shorter!&lt;/p&gt;
    &lt;p&gt;A sneak peek of what it would look like:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  a_power = a.calculate_attack_power()
  a_energy_cost = a.calculate_attack_cost(d)
  d_armor = d.calculate_defense()
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  d.damage(a_power - d_armor)&lt;/code&gt;
    &lt;p&gt;I'll explain Nick's system in four steps:&lt;/p&gt;
    &lt;p&gt;We'll start simple, and build up gradually.&lt;/p&gt;
    &lt;p&gt;I know this from experience. I regret naming Vale's regions "regions"!&lt;/p&gt;
    &lt;p&gt;Let's start by completely forgetting the difference between readonly and readwrite references. Let's say that all references are readwrite.&lt;/p&gt;
    &lt;p&gt;Now, take this simple Mojo program that has two readwrite aliases to the same list:&lt;/p&gt;
    &lt;code&gt;fn example():
    my_list = [1, 2, 3, 4]
    ref list_ref_a = my_list
    ref list_ref_b = my_list
    list_ref_a.append(5)
    list_ref_b.append(6)&lt;/code&gt;
    &lt;p&gt;Here's the equivalent Rust code:&lt;/p&gt;
    &lt;code&gt;fn example() {
    let mut my_list: Vec&amp;lt;i64&amp;gt; = vec![1, 2, 3, 4];
    let list_ref_a = &amp;amp;mut my_list;
    let list_ref_b = &amp;amp;mut my_list;
    list_ref_a.push(5);
    list_ref_b.push(6);
}&lt;/code&gt;
    &lt;p&gt;The Rust compiler rejects it because we're violating aliasability-xor-mutability, specifically in that we have two active readwrite references:&lt;/p&gt;
    &lt;code&gt;error[E0499]: cannot borrow `my_list` as mutable more than once at a time
 --&amp;gt; src/lib.rs:4:22
  |
3 |   let list_ref_a = &amp;amp;mut my_list;
  |                   ------------ first mutable borrow occurs here
4 |   let list_ref_b = &amp;amp;mut my_list;
  |                   ^^^^^^^^^^^^ second mutable borrow occurs here
5 |
6 |   list_ref_a.push(5);
  |   ---------- first borrow later used here&lt;/code&gt;
    &lt;p&gt;But... we humans can easily conclude this is safe. After the evaluation of list_ref_a.push(5), my_list is still there, and it's still in a valid state. So there is no risk of memory errors when evaluating the second call to push.&lt;/p&gt;
    &lt;p&gt;In any language, when we hand a function a non-owning reference to an object, that function can't destroy the object, 8 nor change its type. The same is true here.&lt;/p&gt;
    &lt;p&gt;Therefore, the caller should be able to have (and keep using) other references to that object, and it's totally fine.&lt;/p&gt;
    &lt;p&gt;Nick's approach handles this by thinking about "a reference to an object" as different from "a reference to its contents". We can have multiple references to an object, but references into an object's contents will require some special logic.&lt;/p&gt;
    &lt;p&gt;I'll explain that more in the next section.&lt;/p&gt;
    &lt;p&gt;If a language supports temporarily destroying a live object's field, like Mojo, Nick's model supports that as well. It tracks that "some object in this group is partially destroyed" and temporarily disables other potential references to that object while that's true.&lt;/p&gt;
    &lt;p&gt;So how do we handle a caller's references to the contents of the object? What kind of special logic does that require?&lt;/p&gt;
    &lt;p&gt;In the below example, the compiler should reject print(element_ref) because append might have modified the List.&lt;/p&gt;
    &lt;code&gt;fn example():
    my_list = [1, 2, 3, 4]
    ref list_ref = my_list
    ref el_ref = my_list[0]
    list_ref.append(5)
    print(el_ref)&lt;/code&gt;
    &lt;p&gt;It would be amazing if a memory safety approach knew that the previous example was fine and this one isn't.&lt;/p&gt;
    &lt;p&gt;In other words, the approach should know that when we hand append a reference to List, it shouldn't invalidate the other reference list_ref, but it should invalidate any references to its contents (like el_ref).&lt;/p&gt;
    &lt;p&gt;I like how Nick put it in his proposal:&lt;/p&gt;
    &lt;p&gt;If I had to boil it down to one sentence, I'd say: When you might have used a reference to mutate an object, don't invalidate any other references to the object, but do invalidate any references to its contents.&lt;/p&gt;
    &lt;p&gt;Following this general rule, a lot of programs are revealed to be safe.&lt;/p&gt;
    &lt;p&gt;And this isn't that crazy; if you've used C++ a lot, this likely agrees with your intuition.&lt;/p&gt;
    &lt;p&gt;Note that we'll relax this rule later, and replace it with a more accurate one. But for now, it's a useful stepping stone.&lt;/p&gt;
    &lt;p&gt;Above, I gave a sneak peek at an attack function.&lt;/p&gt;
    &lt;p&gt;Let's look at it again:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  d.damage(damage)&lt;/code&gt;
    &lt;p&gt;For now:&lt;/p&gt;
    &lt;p&gt;(I'll explain both of those points more later.)&lt;/p&gt;
    &lt;p&gt;Note how this function isn't holding any references to Entitys' contents... only to whole Entitys.&lt;/p&gt;
    &lt;p&gt;All these methods don't delete any Entitys, so this attack function is completely memory safe. In fact, even though the use_energy and damage methods modify Entitys, every line in attack is still memory-safe. 10&lt;/p&gt;
    &lt;p&gt;Let's look at this alternate example now to see it catching an actual memory safety risk.&lt;/p&gt;
    &lt;p&gt;Entity looks like this now:&lt;/p&gt;
    &lt;code&gt;struct Entity:
    var hp: Int
    var rings: ArrayList[Ring]
    ...&lt;/code&gt;
    &lt;p&gt;attack now holds a reference to an Entity's contents, like so:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  ref ring_ref = d.rings[0] # Ref to contents

  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  ...&lt;/code&gt;
    &lt;p&gt;The compiler views the program like this:&lt;/p&gt;
    &lt;p&gt;The compiler knows that:&lt;/p&gt;
    &lt;p&gt;Now let's see what happens when we modify d with a call to damage and then try to use that ring_ref:&lt;/p&gt;
    &lt;code&gt;  ref ring_ref = d.rings[0] # Ref to contents

  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)

  d.damage(damage)
  print(ring_ref) # Invalid, should show error&lt;/code&gt;
    &lt;p&gt;The compiler shows an error, because one of the functions (like damage) might have deleted that first ring, so the compiler should invalidate any references to the contents of all Entitys in the group.&lt;/p&gt;
    &lt;p&gt;We're really just following the rule from before: When you might have used a reference to mutate an object, don't invalidate any other references to the object, but do invalidate any references to its contents.&lt;/p&gt;
    &lt;p&gt;More precisely, these methods are only able to access the entities in group r by going through the variables a and d. In other words, there are no "back channels" for gaining access to the entities. This is important for memory safety and also for optimizations' correctness.&lt;/p&gt;
    &lt;p&gt;I'd like to remind everyone that this is all theoretical. Let me know if you have any improvements or comments on the approach!&lt;/p&gt;
    &lt;p&gt;That's a useful rule, and it can get us pretty far. But let's make it even more specific, so we can prove more programs memory-safe.&lt;/p&gt;
    &lt;p&gt;For example, look at this snippet:&lt;/p&gt;
    &lt;code&gt;  ref hp_ref = d.hp # Ref to contents

  damage = a.calculate_damage(d)
  a_energy_cost = a.calculate_attack_cost(d)
  d_energy_cost = d.calculate_defend_cost(a)
  a.use_energy(a_energy_cost)
  d.use_energy(d_energy_cost)
  d.damage(damage)

  print(hp_ref) # Valid!&lt;/code&gt;
    &lt;p&gt;The previous (invalid) program had a ring_ref referring to an element in a ring array.&lt;/p&gt;
    &lt;p&gt;This new (correct) program has an hp_ref that's pointing to a mere integer instead.&lt;/p&gt;
    &lt;p&gt;This is actually safe, and the compiler should correctly accept this. After all, since none of these methods can delete an Entity, then they can't delete its contained hp integer.&lt;/p&gt;
    &lt;p&gt;Good news, Nick's approach takes that into account!&lt;/p&gt;
    &lt;p&gt;But wait, how? Wouldn't that violate our rule? We might have used a reference (damage may have used d) to mutate an object (the Entity that d is pointing to). So why didn't we invalidate all references to the Entity's contents, like that hp_ref?&lt;/p&gt;
    &lt;p&gt;So, at long last, let's relax our rule, and replace it with something more precise.&lt;/p&gt;
    &lt;p&gt;Old rule: When you might have used a reference to mutate an object, don't invalidate any other references to the object's group, but do invalidate any references to its contents.&lt;/p&gt;
    &lt;p&gt;Better rule: When you might have used a reference to mutate an object, don't invalidate any other references to the object's group, but do invalidate any references to anything in its contents that might have been destroyed.&lt;/p&gt;
    &lt;p&gt;Or, to have more precise terms:&lt;/p&gt;
    &lt;p&gt;Even better rule: When you might have used a reference to mutate an object, don't invalidate any other references to the object's group, but do invalidate any references to its "child groups".&lt;/p&gt;
    &lt;p&gt;So what's a "child group", and how is it different from the "contents" from the old rule?&lt;/p&gt;
    &lt;p&gt;If Entity was defined like this:&lt;/p&gt;
    &lt;code&gt;struct Entity:
    var hp: Int
    var rings: ArrayList[Ring]
    var armor: Box[IArmor]            # An owning pointer to heap (C++ "unique_ptr")
    var hand: Variant[Shield, Sword]  # A tagged union (Rust "enum")

struct Ring:
    var power: int

struct Shield:
    var durability: int

struct Sword:
    var sharpness: int

struct SteelArmor:
    var hardness: int&lt;/code&gt;
    &lt;p&gt;Then these things would be part of an Entity's group:&lt;/p&gt;
    &lt;p&gt;However, these would be in Entity's child groups:&lt;/p&gt;
    &lt;p&gt;For example, if we had this code:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):

  ref hp_ref = d.hp
  ref rings_list_ref = d.rings
  ref ring_ref = d.rings[rand() % len(d.rings)]
  ref armor_ref = d.armor[]  # Dereferences armor pointer

  match ref d.hand:
    case Shield as ref s:
      ...&lt;/code&gt;
    &lt;p&gt;Then these are the groups the compiler knows about:&lt;/p&gt;
    &lt;p&gt;Some observations:&lt;/p&gt;
    &lt;p&gt;As a user, you can use this rule-of-thumb: any element of a Variant or a collection (List, String, Dict, etc) or Box will be in a child group.&lt;/p&gt;
    &lt;p&gt;That all sounds abstract, so I'll state it in more familiar terms: if an object (even indirectly) owns something that could be independently destroyed, it must be in a child group.&lt;/p&gt;
    &lt;p&gt;Now, let's see what happens to the groups when we add a damage call in. Remember: Entity.damage mutates the entity, so it has the potential to destroy the rings, armor, shields and/or swords that the entity is holding:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):

  ref hp_ref = d.hp                              # Group r
  ref rings_list_ref = d.rings                   # Group r
  ref ring_ref = d.rings[rand() % len(d.rings)]  # Group r.rings.items[*]
  ref armor_ref = d.armor[]                      # Group r.armor[]

  match ref d.hand:
    case Shield as ref s:                        # Group r.hand.Shield
      ...
      d.damage(10)  # Invalidates refs to r's child groups
                    # Group r.rings.items[*] is invalidated
                    # Group r.armor[] is invalidated
                    # Group r.hand.Shield is invalidated

      print(hp_ref)               # Okay
      print(len(rings_list_ref))  # Okay
      print(ring_ref.power)       # Error, used invalidated group
      print(s.durability)         # Error, used invalidated group
      print(armor_ref)            # Error, used invalidated group&lt;/code&gt;
    &lt;p&gt;Let's look at it piece-by-piece.&lt;/p&gt;
    &lt;p&gt;An owning pointer to heap, unique_ptr in C++ speak.&lt;/p&gt;
    &lt;p&gt;A tagged union, "enum" in Rust speak.&lt;/p&gt;
    &lt;p&gt;This doesn't have an "(owns)" arrow because in Mojo (which Nick's proposal was for), a Variant is a tagged union, which holds its data inside itself, rather than pointing to its data on the heap.&lt;/p&gt;
    &lt;p&gt;The hp: Int isn't in a Variant or a collection, so it's pointing into the r group (not a child group), so the compiler can let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;Or using our more familiar terms: the integer can't be independently destroyed before or after the Entity (its memory is inside the Entity after all), so it's not in a child group, so the compiler can let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;Now consider ring_ref which points to an item in d.rings.&lt;/p&gt;
    &lt;code&gt;  ref ring_ref = d.rings[rand() % len(d.rings)]  # Group r.rings.items[*]
  ...
      ...
      d.damage(10)  # Invalidates refs to r's child groups
                    # Group r.rings.items[*] is invalidated
      ...
      print(ring_ref.power)  # Error, used invalidated group&lt;/code&gt;
    &lt;p&gt;That ring is in a collection (the d.rings ArrayList), so it's in a child group r.rings.items[*], so the compiler shouldn't let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;Or using our more familiar terms: the Ring could be independently destroyed (such as via a remove or append call on the ArrayList), so it's in a child group, so the compiler shouldn't let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;So, as you can see, hp is in the Entity's group, but a Ring is in a child group.&lt;/p&gt;
    &lt;p&gt;Let's do a harder example. Consider the rings_list_ref that points to the whole d.rings list, rather than an individual Ring.&lt;/p&gt;
    &lt;code&gt;  ref rings_list_ref = d.rings  # Group r
  ...
      ...
      d.damage(10)  # Invalidates refs to r's child groups
      ...
      print(len(rings_list_ref))  # Okay&lt;/code&gt;
    &lt;p&gt;That rings_list_ref is actually pointing at group r, not a child group, because the rings ArrayList isn't in a collection (it is the collection). It's in group r (not a child group), which wasn't invalidated, so the compiler can let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;Or using our more familiar terms: the List itself can't be independently destroyed before or after the Entity (its memory is inside the Entity after all), so it's not in a child group, so the compiler can let us use our reference after the damage method.&lt;/p&gt;
    &lt;p&gt;That means rings_list_ref is still valid, and we can use it in that print call!&lt;/p&gt;
    &lt;p&gt;Consider s, which points into the hand variant's Shield value.&lt;/p&gt;
    &lt;code&gt;  match ref d.hand:
    case Shield as ref s:  # Group r.hand.Shield
      ...
      d.damage(10)  # Invalidates refs to r's child groups
                    # Group r.hand.Shield is invalidated
      ...
      print(s.durability)  # Error, used invalidated group&lt;/code&gt;
    &lt;p&gt;damage could have replaced that Shield with a Sword, thus destroying the Shield.&lt;/p&gt;
    &lt;p&gt;Because of that risk, the compiler invalidates all of group r's child groups, and catches that print(s.durability) is invalid.&lt;/p&gt;
    &lt;p&gt;To summarize all the above:&lt;/p&gt;
    &lt;p&gt;If any of this doesn't make sense, please help us out by coming to the Vale discord and asking questions! I want to make this explanation as clear as possible, so more people understand it.&lt;/p&gt;
    &lt;p&gt;So we know what a child group is, but how does one make a group? Where do they come from?&lt;/p&gt;
    &lt;p&gt;Local variables! Each local variable has its own group. 14&lt;/p&gt;
    &lt;p&gt;Let's look at main:&lt;/p&gt;
    &lt;code&gt;fn main():
    entities = List(Entity(10, 10), Entity(12, 7))
    attack(entities[0], entities[1])&lt;/code&gt;
    &lt;p&gt;The local variable entities introduces a group, containing only itself. As we've just discussed, this group contains several child groups (that are not created by local variables). When we invoke attack, we're passing the child group that represents the elements of the entities list.&lt;/p&gt;
    &lt;p&gt;Additionally, groups can be combined to form other groups. This would also work:&lt;/p&gt;
    &lt;code&gt;fn main():
    entity_a = Entity(10, 10)
    entity_b = Entity(12, 7)
    attack(entity_a, entity_b)&lt;/code&gt;
    &lt;p&gt;This time, when we invoke attack, we're passing a group that represents the "union" of the two local variables.&lt;/p&gt;
    &lt;p&gt;So, to summarize where groups come from:&lt;/p&gt;
    &lt;p&gt;What about heap allocations? For example, if we had a var x = Box[Entity](10, 10). In this case, the local variable x has a group. The Entity it's pointing to is a child group.&lt;/p&gt;
    &lt;p&gt;There's a restriction I haven't yet mentioned: all items in a group must be mutually isolated, in other words, they can't indirectly own each other, and one can't have references into the other. In other words, in the above example, an Entity cannot contain a reference to another Entity.&lt;/p&gt;
    &lt;p&gt;With this restriction, we know that e.g. d.damage(42) can't possibly delete some other Entity, for example a. More generally, we know that if a function takes in a bunch of references into a group, it can't use those to delete any items in the group.&lt;/p&gt;
    &lt;p&gt;I won't go too deeply into this, but if you want an example of why this is needed, try mentally implementing an AVL tree with the system. AVL tree nodes have ownership of other nodes, so any function that has the ability to modify a node suddenly has the ability to destroy a node, and if nodes can be destroyed, we can't know if references to them are still valid. That would be bad. So instead, we have the mutual-isolation rule.&lt;/p&gt;
    &lt;p&gt;Here's a smaller version of one of the above snippets.&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  ref contents_ref = a.armor_pieces[0] # Ref to contents

  d.damage(3)

  print(contents_ref) # Invalid&lt;/code&gt;
    &lt;p&gt;At long last, we can talk about the [mut r: group Entity]! These are group annotations. They help the compiler know that two references might be referring to the same thing. Note that the call site doesn't explicitly have to supply a group for r, the compiler will infer it.&lt;/p&gt;
    &lt;p&gt;The use of the group r in the signature of attack informs the compiler that even though d.damage(3) is modifying d, this may change the value of a, and therefore we need to invalidate any references that exist to child groups of a.&lt;/p&gt;
    &lt;p&gt;Stated more accurately, d.damage(3) is modifying group r, so it invalidates all references that point into r's child groups (like contents_ref).&lt;/p&gt;
    &lt;p&gt;These group annotations also help at the call site, like in this example:&lt;/p&gt;
    &lt;code&gt;fn main():
    entities = List(Entity(10, 10), Entity(12, 7))
    attack(
        entities[rand() % len(entities)],
        entities[rand() % len(entities)])&lt;/code&gt;
    &lt;p&gt;Specifically, this invocation of attack is valid, because attack has been declared in such a way that the arguments are allowed to alias. This information is explicit in the function signature (in attack), so it is visible to both the programmer and the compiler.&lt;/p&gt;
    &lt;p&gt;Let's see a more complex example, and introduce a new concept called a path which helps the compiler reason about memory safety when calling functions.&lt;/p&gt;
    &lt;p&gt;Here's our main function again:&lt;/p&gt;
    &lt;code&gt;fn main():
    entities = List(Entity(10, 10), Entity(12, 7))
    attack(
        entities[rand() % len(entities)],
        entities[rand() % len(entities)])&lt;/code&gt;
    &lt;p&gt;And here's something similar to our attack from before, but with a new call to a new power_up_ring function:&lt;/p&gt;
    &lt;code&gt;fn attack[mut r: group Entity](
    ref[r] a: Entity,
    ref[r] d: Entity):
  ref armor_ref = a.armor # Ref to a's armor

  # Modifies a.rings' contents
  power_up_ring(a, a.rings[0])

  # Valid, compiler knows we only modified a.rings' contents
  armor_ref.hardness += 2&lt;/code&gt;
    &lt;p&gt;As the comments say, power_up_ring is modifying one of a's rings, and it doesn't invalidate our armor_ref.&lt;/p&gt;
    &lt;p&gt;To see how that's possible, let's see power_up_ring (note I'm taking some liberties with the syntax, a much shorter version is in a section below):&lt;/p&gt;
    &lt;code&gt;# Wielder Entity's energy will power up the ring.
# Changes the ring, but does not change the wielder Entity.
fn power_up_ring[e: group Entity, mut rr: group Ring = e.rings*](
    ref[e] entity: Entity,
    ref[rr] a_ring: Ring
):
    a_ring.power += entity.energy / 4&lt;/code&gt;
    &lt;p&gt;Let's unpack that fn line:&lt;/p&gt;
    &lt;p&gt;With this, the caller (attack) has enough information to know exactly what was modified. 15&lt;/p&gt;
    &lt;p&gt;Specifically, attack knows that Entitys' .rings elements may have changed. Therefore, after the call to power_up_ring, attack should invalidate any references pointing into Entitys' .rings elements, but not invalidate anything else. Therefore, it should not invalidate that armor_ref.&lt;/p&gt;
    &lt;p&gt;Inside the function, we see a a_ring.power += entity.energy / 4. Note how it's:&lt;/p&gt;
    &lt;p&gt;The latter is also why we have mut in mut rr: group Ring; the compiler requires a function put mut on any group it might be modifying.&lt;/p&gt;
    &lt;p&gt;This is also something that distinguishes this approach from Rust's. Partial borrows can do some of that, but generally you can't have a &amp;amp;Entity while also having an &amp;amp;mut Item pointing to one of the Entity's items.&lt;/p&gt;
    &lt;p&gt;Well not exactly. Technically, only the .power field is being modified, but power_up_ring is saying that anything inside Ring might have changed.&lt;/p&gt;
    &lt;p&gt;I want to really emphasize something from the last section:&lt;/p&gt;
    &lt;p&gt;mut rr: group Ring = e.rings*&lt;/p&gt;
    &lt;p&gt;This is the key that makes this entire approach work across function calls. Whenever there's a callsite, like attack's call to power_up_ring(a, a.rings[0]), it can assemble a full picture of whether that call is valid, and how it affects the code around it.&lt;/p&gt;
    &lt;p&gt;When compiling attack, the compiler thinks this:&lt;/p&gt;
    &lt;p&gt;This path is how the caller knows what the callee might have modified. That's the vital information that helps it know exactly what other references it might need to invalidate.&lt;/p&gt;
    &lt;p&gt;If you thought that syntax was verbose:&lt;/p&gt;
    &lt;code&gt;fn power_up_ring[e: group Entity, mut rr: group Ring = e.rings*](
    ref[e] entity: Entity,
    ref[rr] a_ring: Ring
):
    a_ring.power += entity.energy / 4&lt;/code&gt;
    &lt;p&gt;...that's my fault. I wanted to show what's really going on under the hood.&lt;/p&gt;
    &lt;p&gt;Nick actually has some better syntax in mind:&lt;/p&gt;
    &lt;code&gt;fn power_up_ring(
   entity: Entity,
   mut ref [entity.rings*] a_ring: Ring
):
    a_ring.power += entity.energy / 4&lt;/code&gt;
    &lt;p&gt;Way simpler!&lt;/p&gt;
    &lt;p&gt;With that, you now know all the pieces to Nick's approach. Summarizing:&lt;/p&gt;
    &lt;p&gt;References to object vs its contents: there's a distinction between an object and its contents. We can have as many references to an object as we'd like. Mutations to the contents will invalidate references that point into the contents, but don't have to invalidate any references to the object itself.&lt;/p&gt;
    &lt;p&gt;Child groups let us think a little more precisely about what mutations will invalidate what references to what contents.&lt;/p&gt;
    &lt;p&gt;Group annotations on the function give the compiler enough information at the callsite to know which references in the caller to invalidate.&lt;/p&gt;
    &lt;p&gt;When I was learning about the approach, I was kind of surprised that it had no unique references. They seemed inevitable. 16 In his proposal, Nick even mentions this example:&lt;/p&gt;
    &lt;code&gt;fn foo[mut r: group String](names: List[ref[r] String]):
    p1 = names[0]
    p2 = names[1]
    p1[] = p2[]     # Error: cannot copy p2[]; it might be uninitialized.&lt;/code&gt;
    &lt;p&gt;The final line of the function first destroys p1's pointee (implicitly, just before assigning it a new value), and then copies data from p2's pointee. (By the way, postfix [] is Mojo-speak for dereference, so p1[] is like C's *p1)&lt;/p&gt;
    &lt;p&gt;The challenge here, as he explains, is that p1 and p2 might be pointing to the same object. If so, one or both of these objects might end up with uninitialized data.&lt;/p&gt;
    &lt;p&gt;His solution mentions using escape hatches in this case, like this:&lt;/p&gt;
    &lt;code&gt;fn swap[T: Movable, mut r: group T](ref[r] x: T, ref[r] y: T):
    if __address_of(x) == __address_of(y):
        return
        
    # Now that we know the pointers don't alias, we can use unsafe
    # operations to swap the targets. The exact code isn't important.
    unsafe_x = UnsafePointer.address_of(x)
    unsafe_y = UnsafePointer.address_of(y)
    
    # ...use unsafe_x and unsafe_y here to swap the contents...&lt;/code&gt;
    &lt;p&gt;...but this can theoretically be built into the language, like this:&lt;/p&gt;
    &lt;code&gt;fn swap[T: Movable, mut r: group T](ref[r] x: T, ref[r] y: T):
    if not distinct(x, y):
        return
    
    # ...use x and y...&lt;/code&gt;
    &lt;p&gt;At first, I saw this and thought, "Aha! distinct hints to the compiler that these are unique references!"&lt;/p&gt;
    &lt;p&gt;But... maybe not. Instead of thinking of these as unique references, you could think of this as "splitting" group r into two temporary distinct groups.&lt;/p&gt;
    &lt;p&gt;Throughout the entire proposal, I was expecting the next section to talk about how we inevitably add unique references back in. And as I was thinking ahead, I kept on adding unique references in, in my tentative understanding of his model. This is the problem with being accustomed to conventional borrow checking... it makes it harder to think of any other approach.&lt;/p&gt;
    &lt;p&gt;Luckily, Nick consistently tried to understand what operations can cause pointers to dangle, and impose as few restrictions as possible while ensuring that dangling pointers are always invalidated. With that in mind, the AxM constraint never arose. It's the same mindset I used to come up with Vale's generational references + regions blend. It must be like art: design constraints lead to inspiration!&lt;/p&gt;
    &lt;p&gt;Group Borrowing could be much better than borrow checking.&lt;/p&gt;
    &lt;p&gt;Though, it might also result in programs that are architecturally similar to borrow checking.&lt;/p&gt;
    &lt;p&gt;It might be faster than borrow checking in some cases.&lt;/p&gt;
    &lt;p&gt;But it might be slower in some cases. Not having unique references means it could be challenging for the compiler to compile references to faster noalias 17 pointers. Nick showed me this article to highlight the possible speed differences, and we discussed a few promising options. Perhaps a compiler could:&lt;/p&gt;
    &lt;p&gt;And this model might have downsides:&lt;/p&gt;
    &lt;p&gt;So, will this be revolutionary? Perhaps! Or maybe it'll be just a surface-level improvement on borrow checking in practice. Or, it could be the key that unlocks borrowing and makes it more palatable to the mainstream.&lt;/p&gt;
    &lt;p&gt;noalias is an annotation given to LLVM to tell it that no other pointer will be observing the pointed-at data while the pointer is in scope. It helps the compiler skip some loads and stores.&lt;/p&gt;
    &lt;p&gt;Where does the idea go from here? Not sure!&lt;/p&gt;
    &lt;p&gt;This idea is still new, and could evolve in a lot of different directions.&lt;/p&gt;
    &lt;p&gt;In the grimoire, I hinted about a hypothetical blend of reference counting and borrowing that we don't yet know how to make. I mention that one possible path to it will be to combine various memory safety techniques together. This could be one of them.&lt;/p&gt;
    &lt;p&gt;So regardless of how well this model does on its own, it could be an amazing starting point for hybrid memory safety models. I wouldn't be surprised if one of you reads this, reads the grimoire, and discovers a clever way to blend this with existing mechanisms and techniques. Let me know if you do, and I can write an article like this for you too!&lt;/p&gt;
    &lt;p&gt;By this I mean, you can accomplish anything with extends, if you turn the base class into an interface and a struct (like Dart does), and your "subclass" would instead implements the interface, contain the struct in a field, and forward any calls from that interface into that struct.&lt;/p&gt;
    &lt;p&gt;This would have to be opt-in of course. Non-aliasability is a good default, because it allows the compiler to perform optimizations (e.g. keep values in registers for longer) that can actually have a dramatic impact on performance.&lt;/p&gt;
    &lt;p&gt;Once you understand it, the concept is pretty simple in hindsight.&lt;/p&gt;
    &lt;p&gt;Of course, it pains me to say that "it's simple", because it makes it seem like it was easy to discover. I know from personal experience just how hard it is to come up with something like this... it takes a lot of thinking, trial and error, and bumping into dead ends. 20&lt;/p&gt;
    &lt;p&gt;And we must remember that Nick's model is a draft, and is still being iterated upon. As with any new model, there will be holes, and there will likely be fixes. Vale's region borrowing design fell apart and was fixed a few times yet is still standing, and Nick's model feels even cleaner than regions, so I have hope.&lt;/p&gt;
    &lt;p&gt;If there's one big thing to take away from this post, it's that we aren't done yet. There is more to find out there!&lt;/p&gt;
    &lt;p&gt;That's all! I hope you enjoyed this post. If you have any questions for Nick, he hangs out in the Mojo server (username nick.sm), or feel free to ask questions in the r/vale subreddit or Vale discord server.&lt;/p&gt;
    &lt;p&gt;And most importantly, if you enjoy this kind of exploration, sponsor Nick!&lt;/p&gt;
    &lt;p&gt;Cheers,&lt;/p&gt;
    &lt;p&gt;- Evan Ovadia&lt;/p&gt;
    &lt;p&gt;Designing region borrowing for generational references took me years. And before that, I was almost broken by 32 iterations of a (now abandoned) Vale feature called "hybrid-generational memory". Near the end there, I was so burned out on the highs and lows of breaking and repairing and improving that feature, that I almost gave up on language design entirely.&lt;/p&gt;
    &lt;p&gt;Nick told me he's gone through a similarly grueling experience trying to nail down a design for his "groups". I'm glad he stuck with it!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45051345</guid></item><item><title>That boolean should probably be something else</title><link>https://ntietz.com/blog/that-boolean-should-probably-be-something-else/</link><description>&lt;doc fingerprint="12497b025b44397a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;That boolean should probably be something else&lt;/head&gt;
    &lt;p&gt;Monday, June 30, 2025&lt;/p&gt;
    &lt;p&gt;One of the first types we learn about is the boolean. It's pretty natural to use, because boolean logic underpins much of modern computing. And yet, it's one of the types we should probably be using a lot less of. In almost every single instance when you use a boolean, it should be something else.&lt;/p&gt;
    &lt;p&gt;The trick is figuring out what "something else" is. Doing this is worth the effort. It tells you a lot about your system, and it will improve your design (even if you end up using a boolean).&lt;/p&gt;
    &lt;p&gt;There are a few possible types that come up often, hiding as booleans. Let's take a look at each of these, as well as the case where using a boolean does make sense. This isn't exhaustiveâ[1]there are surely other types that can make sense, too.&lt;/p&gt;
    &lt;head rend="h1"&gt;Datetimes&lt;/head&gt;
    &lt;p&gt;A lot of boolean data is representing a temporal event having happened. For example, websites often have you confirm your email. This may be stored as a boolean column, &lt;code&gt;is_confirmed&lt;/code&gt;, in the database.
It makes a lot of sense.&lt;/p&gt;
    &lt;p&gt;But, you're throwing away data: when the confirmation happened. You can instead store when the user confirmed their email in a nullable column. You can still get the same information by checking whether the column is null. But you also get richer data for other purposes.&lt;/p&gt;
    &lt;p&gt;Maybe you find out down the road that there was a bug in your confirmation process. You can use these timestamps to check which users would be affected by that, based on when their confirmation was stored.&lt;/p&gt;
    &lt;p&gt;This is the one I've seen discussed the most of all these. We run into it with almost every database we design, after all. You can detect it by asking if an action has to occur for the boolean to change values, and if values can only change one time. If you have both of these, then it really looks like it is a datetime being transformed into a boolean. Store the datetime!&lt;/p&gt;
    &lt;head rend="h1"&gt;Enums&lt;/head&gt;
    &lt;p&gt;Much of the remaining boolean data indicates either what type something is, or its status. Is a user an admin or not? Check the &lt;code&gt;is_admin&lt;/code&gt; column!
Did that job fail?
Check the &lt;code&gt;failed&lt;/code&gt; column!
Is the user allowed to take this action?
Return a boolean for that, yes or no!
These usually make more sense as an enum.&lt;/p&gt;
    &lt;p&gt;Consider the admin case: this is really a user role, and you should have an enum for it. If it's a boolean, you're going to eventually need more columns, and you'll keep adding on other statuses. Oh, we had users and admins, but now we also need guest users and we need super-admins. With an enum, you can add those easily.&lt;/p&gt;
    &lt;code&gt;enum UserRole {
  User,
  Admin,
  Guest,
  SuperAdmin,
}
&lt;/code&gt;
    &lt;p&gt;And then you can usually use your tooling to make sure that all the new cases are covered in your code. With a boolean, you have to add more booleans, and then you have to make sure you find all the places where the old booleans were used and make sure they handle these new cases, too. Enums help you avoid these bugs.&lt;/p&gt;
    &lt;p&gt;Job status is one that's pretty clearly an enum as well. If you use booleans, you'll have &lt;code&gt;is_failed&lt;/code&gt;, &lt;code&gt;is_started&lt;/code&gt;, &lt;code&gt;is_queued&lt;/code&gt;, and on and on.
Or you could just have one single field, &lt;code&gt;status&lt;/code&gt;, which is an enum with the various statuses.
(Note, though, that you probably do want timestamp fields for each of these eventsâbut you're still best having the status stored explicitly as well.)
This begins to resemble a state machine once you store the status, and it means that you can make much cleaner code and analyze things along state transition lines.&lt;/p&gt;
    &lt;p&gt;And it's not just for storing in a database, either. If you're checking a user's permissions, you often return a boolean for that.&lt;/p&gt;
    &lt;code&gt;fn check_permissions(user: User) -&amp;gt; bool {
  false // no one is allowed to do anything i guess
}
&lt;/code&gt;
    &lt;p&gt;In this case, &lt;code&gt;true&lt;/code&gt; means the user can do it and &lt;code&gt;false&lt;/code&gt; means they can't.
Usually. I think.
But you can really start to have doubts here, and with any boolean, because the application logic meaning of the value cannot be inferred from the type.&lt;/p&gt;
    &lt;p&gt;Instead, this can be represented as an enum, even when there are just two choices.&lt;/p&gt;
    &lt;code&gt;enum PermissionCheck {
  Allowed,
  NotPermitted(reason: String),
}
&lt;/code&gt;
    &lt;p&gt;As a bonus, though, if you use an enum? You can end up with richer information, like returning a reason for a permission check failing. And you are safe for future expansions of the enum, just like with roles.&lt;/p&gt;
    &lt;p&gt;You can detect when something should be an enum a proliferation of booleans which are mutually exclusive or depend on one another. You'll see multiple columns which are all changed at the same time. Or you'll see a boolean which is returned and used for a long time. It's important to use enums here to keep your program maintainable and understandable.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conditionals&lt;/head&gt;
    &lt;p&gt;But when should we use a boolean? I've mainly run into one case where it makes sense: when you're (temporarily) storing the result of a conditional expression for evaluation. This is in some ways an optimization, either for the computer (reuse a variable[2]) or for the programmer (make it more comprehensible by giving a name to a big conditional) by storing an intermediate value.&lt;/p&gt;
    &lt;p&gt;Here's a contrived example where using a boolean as an intermediate value.&lt;/p&gt;
    &lt;code&gt;fn calculate_user_data(user: User, records: RecordStore) {
  // this would be some nice long conditional,
  // but I don't have one. So variables it is!
  let user_can_do_this: bool = (a &amp;amp;&amp;amp; b) &amp;amp;&amp;amp; (c || !d);

  if user_can_do_this &amp;amp;&amp;amp; records.ready() {
    // do the thing
  } else if user_can_do_this &amp;amp;&amp;amp; records.in_progress() {
    // do another thing
  } else {
    // and something else!
  }
}
&lt;/code&gt;
    &lt;p&gt;But even here in this contrived example, some enums would make more sense. I'd keep the boolean, probably, simply to give a name to what we're calculating. But the rest of it should be a &lt;code&gt;match&lt;/code&gt; on an enum!&lt;/p&gt;
    &lt;p&gt;Sure, not every boolean should go away. There's probably no single rule in software design that is always true. But, we should be paying a lot more attention to booleans.&lt;/p&gt;
    &lt;p&gt;They're sneaky. They feel like they make sense for our data, but they make sense for our logic. The data is usually something different underneath. By storing a boolean as our data, we're coupling that data tightly to our application logic.&lt;/p&gt;
    &lt;p&gt;Instead, we should remain critical and ask what data the boolean depends on, and should we maybe store that instead? It comes easier with practice. Really, all good design does. A little thinking up front saves you a lot of time in the long run.&lt;/p&gt;
    &lt;p&gt;Please share this post, and subscribe to the newsletter or RSS feed. You can email my personal email with any comments or questions.&lt;/p&gt;
    &lt;p&gt;If you're looking to grow more effective as a software engineer, please consider my coaching services.&lt;/p&gt;
    &lt;p&gt; Want to become a better programmer? Join the Recurse Center! &lt;lb/&gt; Want to hire great programmers? Hire via Recurse Center! &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45051361</guid></item><item><title>GPUPrefixSums – state of the art GPU prefix sum algorithms</title><link>https://github.com/b0nes164/GPUPrefixSums</link><description>&lt;doc fingerprint="531d823dc7305afd"&gt;
  &lt;main&gt;
    &lt;p&gt;GPUPrefixSums aims to bring state-of-the-art GPU prefix sum techniques from CUDA and make them available in portable compute shaders. In addition to this, it contributes "Decoupled Fallback," a novel fallback technique for Chained Scan with Decoupled Lookback that should allow devices without forward thread progress guarantees to perform the scan without crashing. The D3D12 implementation includes an extensive survey of GPU prefix sums, ranging from the warp to the device level; all included algorithms utilize wave/warp/subgroup (referred to as "wave" hereon) level parallelism but are completely agnostic of wave size. As a measure of the quality of the code, GPUPrefixSums has also been implemented in CUDA and benchmarked against Nvidia's CUB library. Although GPUPrefixSums aims to be portable to any wave size supported by HLSL, [4, 128], due to hardware limitations, it has only been tested on wave sizes 4, 16, 32, and 64. You have been warned!&lt;/p&gt;
    &lt;p&gt;If you are interested in prefix sums for their use in radix sorting, check out GPUPrefixSum's sibling repository GPUSorting!&lt;/p&gt;
    &lt;p&gt;In Decoupled Fallback, a threadblock will spin for a set amount of cycles while waiting for the reduction of a preceding partition tile. If the maximum spin count is exceeded, the threadblock is free to perform a fallback operation. Multiple thread blocks are allowed to perform fallbacks on the same deadlocking tile, but through use of atomic compare and swap, only one thread block ends up broadcasting its reduction in device memory. Although this means potentially performing redundant calculations, the upside is that fallback performance is no longer limited by the latency of signal propagation between thread blocks.&lt;/p&gt;
    &lt;p&gt;As of writing this 9/22/2024, Decoupled Fallback shows promising results on Apple M GPU's. However the version included here are out of date, with the most up-to-date development occuring in Vello.&lt;/p&gt;
    &lt;p&gt;A prefix sum, also called a scan, is a running total of a sequence of numbers at the n-th element. If the prefix sum is inclusive the n-th element is included in that total, if it is exclusive, the n-th element is not included. The prefix sum is one of the most important algorithmic primitives in parallel computing, underpinning everything from sorting, to compression, to graph traversal.&lt;/p&gt;
    &lt;p&gt;Headless implementation in D3D12, includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reduce then Scan&lt;/item&gt;
      &lt;item&gt;Chained Scan with Decoupled Lookback&lt;/item&gt;
      &lt;item&gt;Chained Scan with Decoupled Lookback Decoupled Fallback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Visual Studio 2019 or greater&lt;/item&gt;
      &lt;item&gt;Windows SDK 10.0.20348.0 or greater&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The repository folder contains a Visual Studio 2019 project and solution file. Upon building the solution, NuGet will download and link the following external dependencies:&lt;/p&gt;
    &lt;p&gt;See the repository wiki for information on running tests.&lt;/p&gt;
    &lt;p&gt;GPUPrefixSumsCUDA includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reduce then Scan&lt;/item&gt;
      &lt;item&gt;Chained Scan with Decoupled Lookback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The purpose of this implementation is to benchmark the algorithms and demystify their implementation in the CUDA environment. It is not intended for production or use; instead, a proper implementation can be found in the CUB library.&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Visual Studio 2019 or greater&lt;/item&gt;
      &lt;item&gt;Windows SDK 10.0.20348.0 or greater&lt;/item&gt;
      &lt;item&gt;CUDA Toolkit 12.3.2&lt;/item&gt;
      &lt;item&gt;Nvidia Graphics Card with Compute Capability 7.x or greater.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The repository folder contains a Visual Studio 2019 project and solution file; there are no external dependencies besides the CUDA toolkit. The use of sync primitives necessitates Compute Capability 7.x or greater. See the repository wiki for information on running tests.&lt;/p&gt;
    &lt;p&gt;Released as a Unity package includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Reduce then Scan&lt;/item&gt;
      &lt;item&gt;Chained Scan with Decoupled Lookback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unity 2021.3.35f1 or greater&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Within the Unity package manager, add a package from git URL and enter:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;https://github.com/b0nes164/GPUPrefixSums.git?path=/GPUPrefixSumsUnity&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;See the repository wiki for information on running tests.&lt;/p&gt;
    &lt;p&gt;Barebones implementation--no vectorization, no wave intrinsics--to be used as a testbed.&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;wgpu 22.0&lt;/item&gt;
      &lt;item&gt;pollster 0.3&lt;/item&gt;
      &lt;item&gt;bytemuck 1.16.3&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Duane Merrill and Michael Garland. “Single-pass Parallel Prefix Scan with De-coupled Lookback”. In: 2016. url: https://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back&lt;/p&gt;
    &lt;p&gt;Grimshaw, Andrew S. and Duane Merrill. “Parallel Scan for Stream Architectures.” (2012). url: https://libraopen.lib.virginia.edu/downloads/6t053g00z&lt;/p&gt;
    &lt;p&gt;Matt Pettineo. GPU Memory Pools in D3D12. Jul. 2022. url: https://therealmjp.github.io/posts/gpu-memory-pool/&lt;/p&gt;
    &lt;p&gt;Ralph Levien. Prefix sum on portable compute shaders. Nov. 2021. url: https://raphlinus.github.io/gpu/2021/11/17/prefix-sum-portable.html&lt;/p&gt;
    &lt;p&gt;Tyler Sorensen, Hugues Evrard, and Alastair F. Donaldson. “GPU Schedulers: How Fair Is Fair Enoughl”. In: 29th International Conference on Concurrency Theory (CONCUR 2018). Ed. by Sven Schewe and Lijun Zhang. Vol. 118. Leibniz International Proceedings in Informatics (LIPIcs). Dagstuhl, Germany: Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik, 2018, 23:1–23:17. isbn: 978-3-95977-087-3. doi: 10.4230/LIPIcs.CONCUR.2018.23. url: http://drops.dagstuhl.de/opus/volltexte/2018/9561.&lt;/p&gt;
    &lt;p&gt;Vasily Volkov. “Understanding Latency Hiding on GPUs”. PhD thesis. EECS Department, University of California, Berkeley, Aug. 2016. url: http://www2.eecs.berkeley.edu/Pubs/TechRpts/2016/EECS-2016-143.html&lt;/p&gt;
    &lt;p&gt;Zhe Jia et al. Dissecting the NVidia Turing T4 GPU via Microbenchmarking. 2019. arXiv: 1903.07486. url: https://arxiv.org/abs/1903.07486&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45051542</guid></item><item><title>The cost of transparency: Living with schizoaffective disorder in tech</title><link>https://kennethreitz.org/essays/2025-08-27-the_cost_of_transparency</link><description>&lt;doc fingerprint="f6c210151f4255be"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;August 2025&lt;/head&gt;
    &lt;p&gt;"We celebrate mental health awareness until someone actually needs mental health support."&lt;/p&gt;
    &lt;p&gt;In The Inclusion Illusion, I explored how tech companies perform diversity while quietly eliminating employees who actually need accommodations. What I didn't share was the personal cost of that analysis—how living openly with schizoaffective disorder has systematically excluded me from the very communities I helped build.&lt;/p&gt;
    &lt;p&gt;Over the past few years, I've worked for at least twenty companies, cycling through positions as organizations discovered my mental health history and found creative ways to make me unwelcomeThis isn't job-hopping by choice—it's a survival pattern forced by systematic exclusion. Each departure followed the same script: initial technical success, mental health disclosure or visibility, growing discomfort, elegant elimination.. The pattern is consistent: initial enthusiasm for my technical contributions, followed by discomfort when my condition becomes visible, and finally the inevitable "restructuring" or "cultural fit" conversations that push me toward the exit.&lt;/p&gt;
    &lt;p&gt;This isn't paranoia—it's pattern recognition honed by lived experience.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Mental Health Care Paradox&lt;/head&gt;
    &lt;p&gt;The discrimination starts in healthcare itself. When I seek treatment for schizoaffective disorder, providers often approach me with a mixture of fear and condescension that would be immediately recognized as unacceptable if directed at any other patient population.&lt;/p&gt;
    &lt;p&gt;I've had therapists decline to continue working with me without clear explanations, leaving me to wonder whether my openness about my condition or my technical background created discomfort they couldn't navigate. I've had doctors become visibly afraid of me after witnessing me during an acute episode, their fear palpable in subsequent interactions. During inpatient treatment, I've been accused of not taking prescribed medication when I was compliant, as if the persistence of symptoms could only be explained by patient deception rather than the complex reality of treatment-resistant episodes.&lt;/p&gt;
    &lt;p&gt;The message is clear: people with schizoaffective disorder are expected to be passive victims of their condition, not active participants in their own care. When we demonstrate competence, insight, or agency, we become threatening rather than inspiring.&lt;/p&gt;
    &lt;head rend="h2"&gt;Professional Exclusion&lt;/head&gt;
    &lt;p&gt;In professional settings, the discrimination is more subtle but equally systematic. Companies tout their mental health benefits and neurodiversity initiatives right up until they encounter someone whose mental health needs actual accommodation.&lt;/p&gt;
    &lt;p&gt;I've been excluded from team meetings after disclosing my condition, with managers explaining they wanted to "reduce stress" for meThis "benevolent" exclusion is particularly insidious because it's framed as care while actually removing you from critical decision-making and visibility opportunities that affect career advancement.. I've had colleagues avoid working with me after learning about my diagnosis, their discomfort palpable in every interaction. I've been passed over for promotions with vague explanations about "communication style" that clearly referenced my openness about mental health challenges"Communication style," "cultural fit," and "leadership presence" have become euphemisms for disability discrimination in performance reviews, providing legal cover for eliminating employees with mental health conditions..&lt;/p&gt;
    &lt;p&gt;The twenty-plus companies I've cycled through represent a pattern of organizations that genuinely believe they're progressive and inclusive. They sponsor diversity conferences, implement mental health awareness training, and celebrate neurodiversity—until they encounter the reality of what supporting someone with a severe mental illness actually requires.&lt;/p&gt;
    &lt;p&gt;Each departure follows a similar script: initial excitement about my technical contributions, growing discomfort as my condition becomes visible during routine workplace stress, and finally the careful choreography of making me unwelcome without explicitly mentioning my mental health. It's discrimination by a thousand paper cuts, each individually deniable but collectively devastating.&lt;/p&gt;
    &lt;head rend="h2"&gt;Open Source Ostracism&lt;/head&gt;
    &lt;p&gt;The Python community's response to my mental health advocacy has been particularly painful. This is a community I helped build, one that benefits from tools I created, yet it has systematically excluded me as I've become more open about living with schizoaffective disorderRequests alone has over 20 million downloads daily and powers much of the modern web, yet the community that benefits from this contribution has made it clear that mental health disclosure makes you too uncomfortable to include in leadership or speaking opportunities..&lt;/p&gt;
    &lt;p&gt;I've watched as former collaborators distance themselves, conference invitations dry up, and contributions get scrutinized with a level of suspicion not applied to other developers. The community celebrates the abstract idea of mental health awareness while making it clear that actual mental health disclosure makes you a liability.&lt;/p&gt;
    &lt;p&gt;A perfect example: the Python documentary releases tomorrow, featuring interviews with Python community leaders and contributors. Despite creating one of the most foundational Python libraries in history, I wasn't even contacted about participating. No email, no mention, complete invisibility. The omission speaks volumes about how mental health disclosure transforms you from community asset to community liabilityTo be clear, the documentary appears focused on Python's origin story rather than popular libraries, so I'm not upset about the exclusion itself. But the complete lack of outreach—not even a courtesy email—fits the broader pattern of invisibility I've experienced since my mental health disclosure..&lt;/p&gt;
    &lt;p&gt;The message from open source leadership is unmistakable: you can advocate for mental health in general terms, but if you make your own struggles visible, you become too uncomfortable to include. The very transparency that helped normalize mental health discussions in tech has made me persona non grata in spaces I helped create.&lt;/p&gt;
    &lt;head rend="h2"&gt;Personal Relationship Casualties&lt;/head&gt;
    &lt;p&gt;Perhaps most devastating is how mental health disclosure affects personal relationships. Friends, romantic partners, and social connections often initially express support for mental health awareness, right up until they encounter the reality of schizoaffective disorder.&lt;/p&gt;
    &lt;p&gt;I've lost friendships when people learned my diagnosis, with former friends suddenly finding excuses to avoid contact. I've had romantic relationships end when partners realized that dating someone with schizoaffective disorder meant occasional difficult conversations about symptoms, medication, and hospitalization risks. I've been excluded from social gatherings where my presence might make others "uncomfortable."&lt;/p&gt;
    &lt;p&gt;But there are exceptions that prove the rule—people like Sarah, whose response to learning about my condition was to educate herself, offer genuine support, and treat me as a complete person rather than a diagnosis. As she puts it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Your condition really can't be learned about. It's so abstract it must be experienced."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This wisdom—recognizing the limits of external understanding while still offering genuine support—represents how people should respond to mental health disclosure. These relationships are rare enough to be remarkable, which itself illustrates the problem.&lt;/p&gt;
    &lt;p&gt;The isolation is profound. The very condition that most requires social support becomes the barrier to accessing it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Stakes: Life and Death Statistics&lt;/head&gt;
    &lt;p&gt;The discrimination I'm documenting isn't just about hurt feelings or career setbacks—it has life-and-death consequences for people with schizoaffective disorder:&lt;/p&gt;
    &lt;p&gt;Life Expectancy: People with schizoaffective disorder live 8-17.5 years less than the general population, with women facing even greater mortality risk than menA 2011 study found average life expectancy of 64.1-69.4 years for people with schizoaffective disorder, compared to normal life expectancy. The gap is primarily attributed to cardiovascular disease, diabetes, respiratory problems, and suicide risk..&lt;/p&gt;
    &lt;p&gt;Homelessness: Among homeless populations, schizoaffective disorder affects 11-28% of individuals—a rate roughly 15-40 times higher than in the general populationRecent studies show only 5% of homeless individuals are diagnosed with schizoaffective disorder at admission, but 28% receive this diagnosis by discharge, suggesting massive underdiagnosis in this vulnerable population.. People with the condition experience longer psychiatric hospitalizations and more frequent healthcare crises.&lt;/p&gt;
    &lt;p&gt;Healthcare System Failure: The homeless population with schizoaffective disorder suffers from "diagnostic variability" and frequent emergency room visits, often cycling through multiple diagnoses without achieving clinical remission or therapeutic goalsThe diagnostic instability reflects both the complexity of the condition and systemic failures in providing consistent care to people experiencing homelessness and severe mental illness..&lt;/p&gt;
    &lt;p&gt;These aren't abstract statistics—they represent the human cost of the systematic exclusion I've experienced. When healthcare discriminates, when employers eliminate us, when communities ostracize us, people with schizoaffective disorder literally die younger and more often become homeless.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fear Behind the Discrimination&lt;/head&gt;
    &lt;p&gt;What drives this systematic exclusion is fear—fear of the unknown, fear of mental illness, and particularly fear of schizoaffective disorder's association with psychosis. People can intellectually support mental health awareness while being viscerally uncomfortable with conditions that challenge their assumptions about consciousness, reality, and social functioning.&lt;/p&gt;
    &lt;p&gt;Schizoaffective disorder carries stigma that depression and anxiety don't. People can relate to feeling sad or worried; they cannot relate to experiencing reality differently. The moment someone learns you've had psychotic episodes, you become fundamentally Other in their perception—no longer fully human, no longer fully trustworthy, no longer safe to include.&lt;/p&gt;
    &lt;p&gt;This fear is reinforced by media portrayals, social stereotypes, and the medical model's emphasis on pathology over person. People see the diagnosis before they see the individual, the condition before the contributions.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Cost of Silence vs. The Cost of Transparency&lt;/head&gt;
    &lt;p&gt;I face an impossible choice: remain silent about my condition and pass as neurotypical, or live authentically and face systematic exclusion. Neither option is sustainable.&lt;/p&gt;
    &lt;p&gt;Silence means constantly monitoring my behavior for signs that might reveal my condition, avoiding discussions of mental health that might trigger suspicion, and living with the constant anxiety that discovery will lead to rejection. It means being unable to advocate for accommodations I need, being forced to suffer in isolation, and perpetuating the very stigma that makes others afraid to seek help.&lt;/p&gt;
    &lt;p&gt;Transparency means facing the discrimination I've documented—professional exclusion, social isolation, and the constant emotional labor of educating people about a condition they'd rather not understandThe emotional labor is exhausting: constantly explaining that psychosis doesn't make you dangerous, that medication doesn't make you less competent, that accommodation needs don't make you unreliable. You become a one-person education campaign while trying to do your actual job.. It means accepting that my openness will cost me opportunities, relationships, and belonging in communities I helped create.&lt;/p&gt;
    &lt;p&gt;But transparency also means modeling that people with schizoaffective disorder can be productive, insightful, and valuable community members. It means challenging the assumptions that drive discrimination. It means refusing to disappear into the shame that society expects from people with severe mental illness.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Needs to Change&lt;/head&gt;
    &lt;p&gt;The pattern of discrimination I've experienced isn't unique—it's systematic. People with schizoaffective disorder face exclusion across all sectors of society, from healthcare to employment to personal relationships. The problem isn't individual prejudice but structural stigma that makes our very existence uncomfortable for others.&lt;/p&gt;
    &lt;p&gt;Real inclusion means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Healthcare providers who approach patients with schizoaffective disorder as partners in care rather than subjects to be managed. It means recognizing that insight into one's condition is a strength, not a manipulation, and that people with lived experience often understand their needs better than external observers.&lt;/item&gt;
      &lt;item&gt;Professional environments that provide actual accommodation rather than performative awareness. This means flexible work arrangements during symptom management, understanding that medication changes affect performance temporarily, and recognizing that mental health disclosure should be met with support rather than suspicion.&lt;/item&gt;
      &lt;item&gt;Communities that welcome the full spectrum of neurodiversity, including conditions that make people uncomfortable. The Python community, and tech culture generally, must move beyond celebrating abstract diversity toward actually including people whose neurodifference challenges social norms.&lt;/item&gt;
      &lt;item&gt;Personal relationships based on understanding rather than fear. This requires education about schizoaffective disorder, recognition that mental illness doesn't define personality or worth, and commitment to inclusion even when it requires emotional labor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Moving Forward&lt;/head&gt;
    &lt;p&gt;I'm done apologizing for living openly with schizoaffective disorder. The discrimination I've faced isn't my fault—it's a reflection of society's failure to move beyond tokenistic awareness toward genuine inclusion.&lt;/p&gt;
    &lt;p&gt;My condition doesn't make me less competent, less valuable, or less worthy of belonging. The tools I've created serve millions of developers; the insights I've shared have helped normalize mental health discussions in tech; the transparency I've modeled has encouraged others to seek help rather than suffer in silenceI regularly receive messages from developers who say my openness about mental health gave them permission to seek treatment, disclose their own conditions, or simply feel less alone. The personal cost of transparency has created collective benefit for others facing similar struggles..&lt;/p&gt;
    &lt;p&gt;The twenty companies that couldn't handle my openness about schizoaffective disorder represent their failure, not mine. The professional opportunities lost to stigma reflect society's limitations, not my worth. The relationships that couldn't survive mental health disclosure weren't relationships worth preserving.&lt;/p&gt;
    &lt;p&gt;I will continue living authentically, advocating openly, and refusing to disappear into the shame that society expects from people with severe mental illness. The cost of transparency has been high, but the cost of silence—both personal and societal—is higher.&lt;/p&gt;
    &lt;p&gt;People with schizoaffective disorder deserve better than systematic exclusion dressed up as concern. We deserve healthcare that treats us as partners, workplaces that provide actual accommodation, communities that welcome our contributions, and relationships based on understanding rather than fear.&lt;/p&gt;
    &lt;p&gt;Until that changes, I'll keep paying the cost of transparency—because the alternative is accepting a world where people like me are expected to remain invisible, untreated, and alone.&lt;/p&gt;
    &lt;p&gt;That's a cost too high to bear.&lt;/p&gt;
    &lt;p&gt;If you're living with schizoaffective disorder and facing discrimination, you're not alone. The patterns I've described are systematic, not personal failures. Your worth isn't determined by others' comfort with your condition.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45051584</guid></item><item><title>The sisters "paradox" – counter-intuitive probability</title><link>https://blog.engora.com/2025/08/the-sisters-paradox-counter-intuitive.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45051798</guid></item><item><title>US to put economic data on 9 blockchains</title><link>https://www.bloomberg.com/news/articles/2025-08-28/us-puts-gdp-data-on-the-blockchain-in-trump-crypto-push</link><description>&lt;doc fingerprint="effd5ba1ec3d2e73"&gt;
  &lt;main&gt;
    &lt;p&gt;Regulation&lt;/p&gt;
    &lt;head rend="h1"&gt;US Puts GDP Data on the Blockchain in Trump Crypto Push&lt;/head&gt;
    &lt;p&gt;The US government on Thursday began distributing gross domestic product data on public blockchains, marking the latest Trump administration endorsement of the crypto industry.&lt;/p&gt;
    &lt;p&gt;The move, as described by Commerce Department officials, will create another avenue — not a replacement — for publishing the economic data.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45052242</guid></item><item><title>Google Debuts Device-Bound Session Credentials Against Session Hijacking</title><link>https://www.feistyduck.com/newsletter/issue_128_google_debuts_device_bound_session_credentials_against_session_hijacking</link><description>&lt;doc fingerprint="263b8c218ca221f4"&gt;
  &lt;main&gt;
    &lt;p&gt;28 August 2025&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Feisty Duck’s Cryptography &amp;amp; Security Newsletter is a periodic dispatch bringing you commentary and news surrounding cryptography, security, privacy, SSL/TLS, and PKI. It's designed to keep you informed about the latest developments in this space. Enjoyed every month by more than 50,000 subscribers. Written by Ivan Ristić.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;HTTP cookies were never intended for session management, but that’s what we ended up with. Modern-day applications may have shifted to other methods, such as JSON Web Tokens (JWTs), but the traditional session management approach is based around the concept of a session identifier, which is a temporary authentication (bearer) token that gives access to the application session state. This is not insecure in itself, but it’s problematic when session identifiers are transported using cookies, which were never designed for security. These weaknesses gave rise to session hijacking attacks.&lt;/p&gt;
    &lt;p&gt;Over the years, many attempts have been made to make HTTP cookies secure, with features such as the Secure flag, then the HttpOnly flag, another feature called Name Prefixes, and then the Same-Site concept. And those are in addition to a series of changes to how browsers handle cookies. All of this helped, but it didn’t solve the problem.&lt;/p&gt;
    &lt;p&gt;My own research into this situation—too many years ago—led me to look into the possibility of binding HTTP sessions to TLS sessions. The idea was simple: You attach the observed TLS session identifier to the HTTP session on the first request, then you check that it remains the same on subsequent requests. The thinking was that the TLS session was going to stay the same for the original user, but be something else for the attackers. I was very pleased to discover that it worked in all browsers—except in Internet Explorer, which limited the duration of TLS sessions to only five minutes. And that was that.&lt;/p&gt;
    &lt;p&gt;In the early days of session hijacking, the main attack vector was lack of encryption. Anyone with access to the local network traffic could observe session identifiers in plaintext—for example, by using Wireshark. In fact, there was once (in 2010) a famous Firefox extension called Firesheep that automated the entire attack and made it trivial for anyone to execute.&lt;/p&gt;
    &lt;p&gt;Eventually, HTTPS became ubiquitous, and cookies became sufficiently secure to make session hijacking a thing of the past. However, as our security started to improve, the criminal networks looked for other options. For many years, password compromise was the preferred exploitation option, but the rise of two-factor authentication made that venue less and less feasible. The attackers then pivoted to cookie-theft malware.&lt;/p&gt;
    &lt;p&gt;An infostealer, as this type of malware is also called, is conceptually simple: It tricks the victim into running a piece of software that looks around their hard disks for anything of value; in the past, this included passwords, and now it also includes session tokens. Passwords may be worthless to access accounts protected with two-factor authentication, but session tokens can be used immediately.&lt;/p&gt;
    &lt;p&gt;For more than a decade, plenty of people at Google tried to design a channel binding mechanism to improve HTTP session security in such a way that session hijacking would become impossible. You may have heard the same concept called channel ID or token binding. For one reason or another, they couldn’t get it to work—until, perhaps, now.&lt;/p&gt;
    &lt;p&gt;The first we heard about Device-Bound Session Credentials (DBSC) was in a blog post from April 2024, when Google announced that it was experimenting with something new. Fast-forward to last month, when Google announced a beta of the new protections in Google Workspace for users running Chrome on Windows. In the UI, there’s just a checkbox that you need to tick.&lt;/p&gt;
    &lt;p&gt;Underneath, DBSC is based on public-key cryptography. For every session, a pair of keys is created and stored securely on the device, in a way that makes them difficult to recover. On Windows, Chrome will use Trusted Platform Module (TPM), which is available in about 60 percent of installations. The session-specific private key can be used periodically to prove that the access is attempted from the same device. With this, session identifiers become useless on any other device.&lt;/p&gt;
    &lt;p&gt;Sounds great! Now we just have to wait to see if the other browser vendors like the idea. If they do and they adopt DBSC, then session hijacking will finally become a thing of the past. It’d be nice to know that some problems are never coming back. The DBSC standard lives as a W3C standard.&lt;/p&gt;
    &lt;p&gt;This subscription is just for the newsletter; we won't send you anything else.&lt;/p&gt;
    &lt;p&gt;The 34th Usenix Security Symposium has been held in Seattle, and the materials are now available. There is a ton of information to process; the full proceedings are in a 1.2 GB PDF file. Here’s my list of interesting papers related to cryptography, without commentary and in the order I discovered them:&lt;/p&gt;
    &lt;p&gt;We have a great many news stories this month. Short news, long list. Happy reading!&lt;/p&gt;
    &lt;p&gt;Designed by Ivan Ristić, the author of SSL Labs, Bulletproof TLS and PKI, and Hardenize, our course covers everything you need to know to deploy secure servers and encrypted web applications.&lt;/p&gt;
    &lt;p&gt;Remote and trainer-led, with small classes and a choice of timezones.&lt;/p&gt;
    &lt;p&gt;Join over 2,000 students who have benefited from more than a decade of deep TLS and PKI expertise.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45052407</guid></item></channel></rss>