<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 21 Nov 2025 19:32:08 +0000</lastBuildDate><item><title>Homeschooling hits record numbers</title><link>https://reason.com/2025/11/19/homeschooling-hits-record-numbers/</link><description>&lt;doc fingerprint="2c38d07f980997d6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Homeschooling Hits Record Numbers&lt;/head&gt;
    &lt;head rend="h2"&gt;Last academic year, DIY education grew at nearly three times the average rate it did during the COVID-19 pandemic, according to new research.&lt;/head&gt;
    &lt;p&gt;Whether called homeschooling or DIY education, family-directed learning has been growing in popularity for years in the U.S. alongside disappointment in the rigidity, politicization, and flat-out poor results of traditional public schools. That growth was supercharged during the COVID-19 pandemic when extended closures and bumbled remote learning drove many families to experiment with teaching their own kids. The big question was whether the end of public health controls would also curtail interest in homeschooling. We know now that it didn't. Americans' taste for DIY education is on the rise.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Rattler Article Inline Signup&lt;/head&gt;
    &lt;p&gt;You are reading The Rattler from J.D. Tuccille and Reason. Get more of J.D.'s commentary on government overreach and threats to everyday liberty.&lt;/p&gt;
    &lt;head rend="h1"&gt;Homeschooling Grows at Triple the Pre-Pandemic Rate&lt;/head&gt;
    &lt;p&gt;"In the 2024-2025 school year, homeschooling continued to grow across the United States, increasing at an average rate of 5.4%," Angela Watson of the Johns Hopkins University School of Education's Homeschool Hub wrote earlier this month. "This is nearly three times the pre-pandemic homeschooling growth rate of around 2%." She added that more than a third of the states from which data is available report their highest homeschooling numbers ever, even exceeding the peaks reached when many public and private schools were closed during the pandemic.&lt;/p&gt;
    &lt;p&gt;After COVID-19 public health measures were suspended, there was a brief drop in homeschooling as parents and families returned to old habits. That didn't last long. Homeschooling began surging again in the 2023-2024 school year, with that growth continuing last year. Based on numbers from 22 states (not all states have released data, and many don't track homeschoolers), four report declines in the ranks of homeschooled children—Delaware, the District of Columbia, Hawaii, and Tennessee—while the others report growth from around 1 percent (Florida and Louisiana) to as high as 21.5 percent (South Carolina).&lt;/p&gt;
    &lt;p&gt;The latest figures likely underestimate growth in homeschooling since not all DIY families abide by registration requirements where they exist, and because families who use the portable funding available through increasingly popular Education Savings Accounts to pay for homeschooling costs are not counted as homeschoolers in several states, Florida included. As a result, adds Watson, "we consider these counts as the minimum number of homeschooled students in each state."&lt;/p&gt;
    &lt;p&gt;Recent estimates put the total homeschooling population at about 6 percent of students across the United States, compared to about 3 percent pre-pandemic. Continued growth necessarily means the share of DIY-educated students is increasing. That's quite a change for an education approach that was decidedly not mainstream just a generation ago.&lt;/p&gt;
    &lt;p&gt;"This isn't a pandemic hangover; it's a fundamental shift in how American families are thinking about education," comments Watson.&lt;/p&gt;
    &lt;head rend="h1"&gt;Students Flee Traditional Public Schools for Alternatives&lt;/head&gt;
    &lt;p&gt;Homeschooling is a major beneficiary of changing education preferences among American families, but it's not the only one.&lt;/p&gt;
    &lt;p&gt;"Five years after the pandemic's onset, there has been a substantial shift away from public schools and toward non-public options," Boston University's Joshua Goodman and Abigail Francis wrote last summer for Education Next. Looking at Massachusetts—not the friendliest regulatory environment for alternatives to traditional public schooling—they found that as the state's school-age population shrank by 2.6 percent since 2019, there has been a 4.2 percent decline in local public-school enrollment, a 0.7 decline in private-school enrollment, and a 56 percent increase in homeschooling. "Charter school enrollment is flat, due in part to regulatory limitations in Massachusetts," they added.&lt;/p&gt;
    &lt;p&gt;In research published in August, Dylan Council, Sofoklis Goulas, and Faidra Monachou of the Brookings Institution found similar results at the national level. "The COVID-19 pandemic forced millions of families to rethink where and how their children learn, and the effects continue to reshape American K-12 education," they observed. If "parents keep choosing alternatives at the pace observed since 2020, traditional public schools could lose as many as 8.5 million students, shrinking from 43.06 million in 2023-24 to as few as 34.57 million by mid-century."&lt;/p&gt;
    &lt;p&gt;It's not difficult to figure out what pushes parents to seek out alternatives and to flock to the various forms of DIY education grouped under the homeschooling heading.&lt;/p&gt;
    &lt;head rend="h1"&gt;Disappointment in Public Schools Drives the Shift&lt;/head&gt;
    &lt;p&gt;"The fraction of parents saying K-12 education is heading in the wrong direction was fairly stable from 2019 to 2022 but rose in 2023 and then again in 2024 to its highest level in a decade, suggesting continuing or even growing frustration with schools," commented Goodman and Francis.&lt;/p&gt;
    &lt;p&gt;Specifically, EdChoice's Schooling in America survey puts the percentage of school parents saying that K-12 education is headed in the right direction at 41 percent—down from 48 percent in 2022 (the highest score recorded). Fifty-nine percent say K-12 education is on the wrong track—up from 52 percent in 2021 (the lowest score recorded).&lt;/p&gt;
    &lt;p&gt;When asked if they are satisfied with their children's education, public school parents consistently rank last after parents who choose private schools, homeschooling, and charter schools. Importantly, among all parents of school-age children, homeschooling enjoys a 70 percent favorability rating.&lt;/p&gt;
    &lt;p&gt;The reasons for the move away from public schools certainly vary from family to family, but there have been notable developments in recent years. During the pandemic, many parents discovered that their preferences regarding school closures and health policies were anything but a priority for educators.&lt;/p&gt;
    &lt;p&gt;Closures also gave parents a chance to experience public schools' competence with remote learning, and many were unimpressed. They have also been unhappy with the poor quality and often politicized lessons taught to their children that infuriatingly blend declining learning outcomes with indoctrination. That doesn't mean parents all want the same things, but the one-size-fits-some nature of public schooling make curriculum battles inevitable—and push many towards the exits in favor of alternatives including, especially, homeschooling. The shift appears to be here to stay.&lt;/p&gt;
    &lt;p&gt;"What's particularly striking is the resilience of this trend," concludes Watson of Johns Hopkins University's Homeschool Hub. "States that saw declines have bounced back with double-digit growth, and we're seeing record enrollment numbers across the country."&lt;/p&gt;
    &lt;p&gt;Once an alternative way to educate children, homeschooling is now an increasingly popular and mainstream option.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45999842</guid><pubDate>Fri, 21 Nov 2025 00:31:47 +0000</pubDate></item><item><title>Olmo 3: Charting a path through the model flow to lead open-source AI</title><link>https://allenai.org/blog/olmo3</link><description>&lt;doc fingerprint="ad5e3b78241b8f9a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Olmo 3: Charting a path through the model flow to lead open-source AI&lt;/head&gt;
    &lt;p&gt;November 20, 2025&lt;/p&gt;
    &lt;p&gt;Ai2&lt;/p&gt;
    &lt;p&gt;Language models are often treated as snapshots—brief captures of a long and carefully curated development process. But sharing only the end result obscures the rich context needed to modify, adapt, and extend a model's capabilities. Many meaningful adjustments require integrating domain-specific knowledge deep within the development pipeline, not merely at the final stage. To truly advance open AI development and research, the entire model flow – not just its endpoint – should be accessible and customizable. The model flow is the full lifecycle of an LM: every stage, checkpoint, dataset, and dependency required to create and modify it. By exposing this complete process, the goal is to engender greater trust and enable more effective adaptation, collaboration, and innovation.&lt;/p&gt;
    &lt;p&gt;With today's release of Olmo 3, we're empowering the open source community with not only state-of-the-art open models, but the entire model flow and full traceability back to training data.&lt;/p&gt;
    &lt;p&gt;At its center is Olmo 3-Think (32B), the best fully open 32B-scale thinking model that for the first time lets you inspect intermediate reasoning traces and trace those behaviors back to the data and training decisions that produced them. Olmo 3 is a family of compact, dense models at 7 billion and 32 billion parameters that can run on everything from laptops to research clusters.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo 3-Base (7B, 32B) is our most powerful base model yet. When evaluated on our expanded, diverse evaluation suite, Olmo 3-Base delivers the strongest performance among fully open base models – where training data, code, and weights are all publicly available, like Stanford's Marin and Swiss AI's Apertus – and achieves competitive performance with some of the best open-weights base models of comparable size and architecture, including Qwen 2.5 and Gemma 3. Achieving strong results in programming, reading comprehension, and math problem solving, Olmo 3-Base maintains performance at extended context lengths (~up to 65K tokens)—providing a versatile foundation for continued pretraining, targeted fine-tuning, and reinforcement learning and making it easy to build in specialized capabilities like reasoning, tool use (function calling), and instruction following through post-training.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Think (7B, 32B) is our flagship post-trained reasoning set built on Olmo 3-Base. At a time when few organizations are releasing truly open models at this scale, Olmo 3-Think (32B) serves as a workhorse for RL research, long-horizon reasoning, and other advanced experiments that require substantial compute. On our suite of reasoning benchmarks (discussed below), it's the strongest fully open thinking model we're aware of, narrowing the gap to the best open-weight models of similar scale – such as Qwen 3 32B – while training on roughly 6x fewer tokens. Olmo 3-Think (7B) brings the same design and training approach to an even more efficient form factor, surfacing intermediate thinking steps for complex prompts while making open, inspectable reasoning accessible on more modest hardware.&lt;/item&gt;
      &lt;item&gt;Olmo 3-Instruct (7B) is a chat and quick-response focused post-train of Olmo 3-Base that handles multi-turn, instruction-following, tool use, and more. In our evaluations, it matches or outperforms open-weight models including Qwen 2.5, Gemma 3, and Llama 3.1, and narrows the gap with Qwen 3 model families at a similar scale—delivering a strong, fully open alternative for high-quality conversational and tool-using agents.&lt;/item&gt;
      &lt;item&gt;Olmo 3-RL Zero (7B), is a fully open reinforcement learning pathway built on Olmo 3-Base, designed to bootstrap complex reasoning behaviors and enable clear benchmarking of RL algorithms. We release four series of checkpoints from domain-focused training on math, code, instruction following, and general chat, enabling careful study of reinforcement learning with verifiable rewards (RLVR).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Instead of a single set of frozen weights, Olmo 3 offers multiple, fully documented paths through development: the Instruct path for everyday chat and tool use, the RL Zero path for RL experimentation from base models, and the Think/reasoning path for models that leverage inference-time scaling to unlock complex reasoning and agentic behaviors. Each path is a concrete example of how to shape behavior from the same base model, and you’re free to fork or remix them—start with Olmo 3-Base, explore your own supervised fine-tuning (SFT) or direct preference optimization (DPO) recipe for instruct-style use cases, or plug in a new RL objective to probe different tradeoffs. The flow itself becomes a rich, reusable object—not just a record of how we built Olmo 3, but a scaffold for how you can build your own systems.&lt;/p&gt;
    &lt;p&gt;Explore the Model Flow&lt;/p&gt;
    &lt;p&gt;Click on any stage to learn more about it and download artifacts.&lt;/p&gt;
    &lt;p&gt;The Olmo 3 checkpoints we're releasing represent our initial paths targeting our goals around reasoning, tool use, and general capabilities – we have exciting plans for other ways to leverage Olmo 3-Base 32B. But because we're releasing the entire flow, you can intervene at any point: swap in domain-specific data during mid-training, adjust post-training for your use case, or build on an earlier checkpoint that better suits your needs.&lt;/p&gt;
    &lt;p&gt;As with Olmo and Olmo 2, we’re releasing all components of the Olmo 3 flow – data, code, model weights, and checkpoints – under permissive open source licenses.&lt;/p&gt;
    &lt;p&gt;Try Olmo 3 | Download the models &amp;amp; data | Read the report&lt;/p&gt;
    &lt;head rend="h3"&gt;Strong performance across the board&lt;/head&gt;
    &lt;p&gt;We run the Olmo 3 checkpoints through a broad, updated benchmark suite, grouping dozens of industry-standard tasks (plus a few new ones we introduce) into several capability clusters. Together, the clustered suite and these held-out tasks give us a capability profile of Olmo 3—a clear picture of how well it solves math problems, codes, uses tools, answers general-knowledge questions, and more.&lt;/p&gt;
    &lt;p&gt;At a high level, the Olmo 3 family delivers the strongest fully open base and thinking models we’re aware of. Olmo 3-Base 32B outperforms other fully open base models, and Olmo 3-Think 32B emerges as the strongest fully open thinking model.&lt;/p&gt;
    &lt;p&gt;Our results were made possible by rigorous data curation at every stage of training, a carefully designed training recipe for each model, and a set of new algorithmic and infrastructure advances across data processing, training, and reinforcement learning. We also introduce an enhanced reinforcement learning framework that guides the development of our models and is particularly essential for our thinking models. To design the training recipe and coordinate targeted improvements across a wide range of capabilities at each stage of the model training pipeline, our development framework balances distributed innovation with centralized evaluation.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Base, with a training pipeline that first focuses on broad coverage over diverse text, code, and math, then concentrates on harder distributions to sharpen programming, quantitative reasoning, and reading comprehension, is clearly the strongest set of fully open base models in our evaluations. It’s also arguably the best 32B model in the entire ecosystem of models with open weights, performing impressively in programming, reading comprehension, math problem solving, and long-context benchmarks like RULER, which tests information retrieval from lengthy texts. Olmo 3-Base (7B) and Olmo 3-Base (32) maintain quality at extended context lengths and integrate cleanly with RL workflows, providing a robust foundation for continued pretraining and post-training.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Think, which turns the Base into a reasoning model by training on multi-step problems spanning math, code, and general problem solving, then running the thinking SFT → thinking DPO → RLVR model flow to elicit high-quality reasoning traces, competes with or exceeds several open-weight reasoning models of similar sizes. On math benchmarks, Olmo 3-Think (7B) matches Qwen 3 8B on MATH and comes within a few points on AIME 2024 and 2025, and also leads all comparison models on HumanEvalPlus for coding—performing strongly on MBPP and LiveCodeBench to demonstrate particular strength in code-intensive reasoning. On broader reasoning tasks like BigBench Hard and AGI Eval English, Olmo 3-Think (7B) remains competitive with Qwen 3 8B reasoning and Qwen 3 VL 8B Thinker while staying fully open and slightly smaller.&lt;/p&gt;
    &lt;p&gt;For the 32B model, Olmo 3-Think scales these trends up and becomes one of the strongest fully open reasoning models in its class. Olmo 3-Think (32B) either wins or sits within roughly two points of the best open-weight model on MATH, OMEGA, BigBenchHard, HumanEvalPlus, PopQA, and IFEval. It ties Qwen 3 VL 32B Thinking for the top score on the OMEGA suite while staying clearly ahead of Gemma 3 27B Instruct and competitive with DeepSeek R1 Distill 32B on math and reasoning. On broader knowledge and QA, Olmo 3-Think (32B) is effectively neck-and-neck with the Qwen 3 models on PopQA. And in instruction following, Olmo 3-Think (32B) tops this subset on IFEval and remains solid on IFBench and AlpacaEval 2 LC—offering a strong default for reasoning workloads at the 32B scale.&lt;/p&gt;
    &lt;p&gt;Olmo 3-Instruct, which produces shorter sequences than the corresponding Olmo 3-Think models to improve inference efficiency and is designed to focus on general chat, tool use, and synthetic data generation, outperforms comparably-sized open-weight models. Olmo 3-Instruct ties or surpasses Qwen 2.5, Gemma 3, and Llama 3.1 in our evaluations, and competes with the Qwen 3 family at similar scale, delivering strong function calling performance and instruction-following capabilities in a fully open 7B model.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Olmo 3 architecture and training stages&lt;/head&gt;
    &lt;p&gt;Olmo 3 uses a decoder-only transformer architecture and multi-stage training pipeline. Pretraining runs in three stages—an initial large-scale training run that builds broad capabilities; a mid-training phase that focuses on harder material like math, code, and reading comprehension; and a final long-context extension stage that trains the model on very long documents. Together with architectural enhancements, this yields a more capable, efficient base for the Olmo 3 family.&lt;/p&gt;
    &lt;p&gt;Post-training then specializes the pretrained model for different use cases. Building on Olmo 2, each pathway follows a three-stage recipe – SFT, preference tuning with DPO, and RLVR – but in Olmo 3, we expose this as a fully documented model flow with complete customization over each training stage and dataset mix.&lt;/p&gt;
    &lt;p&gt;Instead of releasing only the final weights, we provide checkpoints from each major training milestone: the base pretrained model, the mid-trained model after targeted skill enhancement, the long-context-extended version, plus post-training checkpoints for the Olmo 3-Think, Olmo 3-Instruct, and Olmo 3-RL Zero flows. You can study how capabilities emerge over time, run ablations on specific stages, and fork the model at whatever point best fits your data, compute, and goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Expanded training data&lt;/head&gt;
    &lt;p&gt;Compared to Olmo 2, we scaled data collection and significantly strengthened our dataset curation methods. Continuing our commitment to full transparency, we’re releasing several new, higher-quality datasets that cover every stage of base model training and post-training—from initial learning to specialized skills like complex reasoning and long-context understanding. This means anyone can see exactly what data shaped the model’s capabilities, reproduce our results, and reuse these datasets to train their own AI systems.&lt;/p&gt;
    &lt;p&gt;Olmo 3 is pretrained on Dolma 3, a new ~9.3-trillion-token corpus drawn from web pages, science PDFs processed with olmOCR, codebases, math problems and solutions, and encyclopedic text. From this pool, we construct Dolma 3 Mix, a 5.9-trillion-token (~6T) pretraining mix with a higher proportion of coding and mathematical data than earlier Dolma releases, plus much stronger decontamination via extensive deduplication, quality filtering, and careful control over data mixing. We follow established web standards in collecting training data and don’t collect from sites that explicitly disallow it, including paywalled content.&lt;/p&gt;
    &lt;p&gt;On top of this, we introduce two Dolma 3-based mixes for later stages of base model training. Dolma 3 Dolmino is our mid-training mix: 100B training tokens sampled from a ~2.2T-token pool of high-quality math, science, code, instruction-following, and reading-comprehension data, including reasoning traces that also enable RL directly on the base model. Dolma 3 Longmino is our long-context mix: ~50B training tokens drawn from a 639B-token pool of long documents combined with mid-training data to teach Olmo 3 to track information over very long inputs (like reports, logs, and multi-chapter documents).&lt;/p&gt;
    &lt;p&gt;We also introduce Dolci, a new post-training data suite tailored specifically for reasoning, tool use, and instruction following. Dolci provides separate mixes for each stage of post-training: SFT, DPO, and RLVR. For SFT, Dolci aggregates state-of-the-art datasets that advance step-by-step reasoning, tool use, and high-quality conversational behavior; for DPO, it supplies high-quality contrastive preference data; and for RL, it includes hard, diverse prompts across math, coding, instruction following, and general chat.&lt;/p&gt;
    &lt;p&gt;Together, Dolma 3 and Dolci give Olmo 3 a fully open data curriculum from first token to final post-trained checkpoint.&lt;/p&gt;
    &lt;head rend="h3"&gt;Efficient training stack&lt;/head&gt;
    &lt;p&gt;We pretrained Olmo 3 on a cluster of up to 1,024 H100 GPUs; we achieved training throughput of 7.7K tokens per device per second for Olmo 3-Base (7B). We mid-trained on 128 H100 GPUs, and post-trained on a set of 256 H100s.&lt;/p&gt;
    &lt;p&gt;For Olmo 3, building on the work we did for Olmo 2, we were able to significantly improve the efficiency of our post-training code. By moving SFT from Open Instruct (our post-training codebase, prioritizing flexibility) to Olmo Core (our pretraining codebase, designed to maximize efficiency), we increased throughput (tokens/second) by 8x. Similarly, by incorporating in-flight weight updates, continuous batching, and a lot of threading improvements, we made our RL training 4x more efficient—resulting in training runs that are significantly cheaper and faster.&lt;/p&gt;
    &lt;p&gt;A note on our 32B models: We believe 32B sits in a sweet spot for research and tinkering. 32B models are big enough to support strong, competitive performance, but still small enough that a wide audience can fine-tune and deploy them on accessible hardware.&lt;/p&gt;
    &lt;p&gt;For more details, including ablations, please read our technical report.&lt;/p&gt;
    &lt;head rend="h3"&gt;Transparency at the core&lt;/head&gt;
    &lt;p&gt;A core goal of Olmo 3 is not just to open the model flow, but to make it actionable for people who want to understand and improve model behavior. Olmo 3 integrates with OlmoTrace, our tool for tracing model outputs back to training data in real time.&lt;/p&gt;
    &lt;p&gt;For example, in the Ai2 Playground, you can ask Olmo 3-Think (32B) to answer a general-knowledge question, then use OlmoTrace to inspect where and how the model may have learned to generate parts of its response. This closes the gap between training data and model behavior: you can see not only what the model is doing, but why—and adjust data or training decisions accordingly.&lt;/p&gt;
    &lt;p&gt;To further promote transparency and explainability, we’re making every training and fine-tuning dataset available for download, all under a permissive license that allows for custom deployment and reuse. The datasets come in a range of mixes to accommodate different storage and hardware constraints, from several billion tokens all the way up to 6 trillion.&lt;/p&gt;
    &lt;p&gt;Our new tooling for data processing allows you to de-contaminate, tokenize, and de-duplicate data in the same way we did for Olmo 3’s corpora. All the tooling is open source, enabling you to replicate our training curves or run controlled ablations across data mixes and objectives.&lt;/p&gt;
    &lt;p&gt;Our Olmo utilities and software cover the whole development cycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Olmo-core is a state-of-the-art framework for distributed model training.&lt;/item&gt;
      &lt;item&gt;Open Instruct is our post-training pipeline.&lt;/item&gt;
      &lt;item&gt;datamap-rs is a pure-Rust toolkit for large-scale cleaning.&lt;/item&gt;
      &lt;item&gt;duplodocus for ultra-efficient fuzzy de-duplication.&lt;/item&gt;
      &lt;item&gt;OLMES is a toolkit for reproducible evals. It includes our brand-new eval collection OlmoBaseEval, which we used for Olmo 3 base model development.&lt;/item&gt;
      &lt;item&gt;decon removes test sets from training data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Importantly, our tooling allows you to instrument complex tasks and analyze intermediate traces to understand where the models succeed—or struggle. Because the Olmo 3 data recipes, training pipeline, and checkpoints are open, independent teams can connect model behavior back to measurable properties.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ready to deploy and use&lt;/head&gt;
    &lt;p&gt;Together, the Olmo 3 family makes it easier to build trustworthy features quickly, whether for research, education, or applications. By making every development step available and inspectable, we're enabling entirely new categories of research. You can run experiments on any training phase, understand exactly how different techniques contribute to model capabilities, and build on our work at whatever stage makes sense for your project.&lt;/p&gt;
    &lt;p&gt;For scientists, the fully open flow exposes the model’s inner workings, so you can instrument experiments across coding, reasoning, RL, and tool use.&lt;/p&gt;
    &lt;p&gt;If you care about AI you can study, audit, and improve, Olmo 3 is for you. Try the demos in the Ai2 Playground, explore the documentation, and build on the released weights and checkpoints. Then tell us what you discover—we invite the community to validate, critique, and extend our findings.&lt;/p&gt;
    &lt;p&gt;True openness in AI isn't just about access—it's about trust, accountability, and shared progress. We believe the models shaping our future should be fully inspectable, not black boxes. Olmo 3 represents a different path: one where anyone can understand, verify, and build upon the AI systems that increasingly influence our world. This is what open-first means—not just releasing weights, but sharing the complete knowledge needed to advance AI responsibly: the flow.&lt;/p&gt;
    &lt;p&gt;Deep dive with Olmo lead researchers Hanna Hajishirzi and Noah Smith on how – and why – we built Olmo 3, and what comes next:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46001889</guid><pubDate>Fri, 21 Nov 2025 06:50:14 +0000</pubDate></item><item><title>It's hard to build an oscillator</title><link>https://lcamtuf.substack.com/p/its-hard-to-build-an-oscillator</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46002161</guid><pubDate>Fri, 21 Nov 2025 07:45:53 +0000</pubDate></item><item><title>FAWK: LLMs can write a language interpreter</title><link>https://martin.janiczek.cz/2025/11/21/fawk-llms-can-write-a-language-interpreter.html</link><description>&lt;doc fingerprint="5ff690b5ea490edb"&gt;
  &lt;main&gt;
    &lt;p&gt;After reading the book The AWK Programming Language (recommended!), I was planning to try AWK out on this year’s Advent of Code. Having some time off from work this week, I tried to implement one of the problems in it to get some practice, set up my tooling, see how hard AWK would be, and… I found I’m FP-pilled.&lt;/p&gt;
    &lt;p&gt;I knew I’m addicted to the combination of algebraic data types (tagged unions) and exhaustive pattern matching, but what got me this time was immutability, lexical scope and the basic human right of being allowed to return arrays from functions.&lt;/p&gt;
    &lt;p&gt;Part 1 of the Advent of Code problem was easy enough, but for part 2 (basically a shortest path search with a twist, to not spoil too much), I found myself unable to switch from my usual functional BFS approach to something mutable, and ended up trying to implement my functional approach in AWK.&lt;/p&gt;
    &lt;p&gt;It got hairy very fast: I needed to implement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hashing of strings and 2D arrays (by piping to &lt;code&gt;md5sum&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;a global &lt;del rend="overstrike"&gt;set&lt;/del&gt;array of seen states&lt;/item&gt;
      &lt;item&gt;a way to serialize and deserialize a 2D array to/from a string&lt;/item&gt;
      &lt;item&gt;and a few associative arrays for retrieving this serialized array by its hash.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was very lost by the time I had all this; I spent hours just solving what felt like accidental complexity; things that I’d take for granted in more modern languages.&lt;/p&gt;
    &lt;p&gt;Now, I know nobody said AWK is modern, or functional, or that it promises any convenience for anything other than one-liners and basic scripts that fit under a handful of lines. I don’t want to sound like I expect AWK to do any of this; I knew I was stretching the tool when going in. But I couldn’t shake the feeling that there’s a beautiful AWK-like language within reach, an iteration on the AWK design (the pattern-action way of thinking is beautiful) that also gives us a few of the things programming language designers have learnt over the 48 years since AWK was born.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dreaming of functional AWK&lt;/head&gt;
    &lt;p&gt;Stopping my attempts to solve the AoC puzzle in pure AWK, I wondered: what am I missing here?&lt;/p&gt;
    &lt;p&gt;What if AWK had first-class arrays?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # array literals
  normal   = [1, 2, 3]
  nested   = [[1,2], [3,4]]
  assoc    = ["foo" =&amp;gt; "bar", "baz" =&amp;gt; "quux"]
  multidim = [(1,"abc") =&amp;gt; 999]

  five = range(1,5)
  analyze(five)
  print five  # --&amp;gt; still [1, 2, 3, 4, 5]! was passed by value
}

function range(a,b) {
  r = []
  for (i = a; i &amp;lt;= b; i++) {
    r[length(r)] = i
  }
  return r  # arrays can be returned!
}

function analyze(arr) {
  arr[0] = 100
  print arr[0]  # --&amp;gt; 100, only within this function
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had first-class functions and lambdas?&lt;/p&gt;
    &lt;code&gt;BEGIN {
  # construct anonymous functions
  double = (x) =&amp;gt; { x * 2 }
  add = (a, b) =&amp;gt; { c = a + b; return c }

  # functions can be passed as values
  apply = (func, value) =&amp;gt; { func(value) }

  print apply(double,add(1,3))  # --&amp;gt; 8
  print apply(inc,5)  # --&amp;gt; 6
}

function inc(a) { return a + 1 }
&lt;/code&gt;
    &lt;p&gt;What if AWK had lexical scope instead of dynamic scope?&lt;/p&gt;
    &lt;code&gt;# No need for this hack anymore ↓     ↓
#function foo(a, b         ,local1, local2) {
function foo(a, b) {
  local1 = a + b
  local2 = a - b
  return local1 + local2
}

BEGIN {
  c = foo(1,2)
  print(local1)  # --&amp;gt; 0, the local1 from foo() didn't leak!
}
&lt;/code&gt;
    &lt;p&gt;What if AWK had explicit globals, and everything else was local by default?&lt;/p&gt;
    &lt;code&gt;BEGIN { global count }
END {
  foo()
  print count  # --&amp;gt; 1
  print mylocal # --&amp;gt; 0, didn't leak
}
function foo() { count++; mylocal++ }
&lt;/code&gt;
    &lt;p&gt;(This one, admittedly, might make programs a bit more verbose. I’m willing to pay that cost.)&lt;/p&gt;
    &lt;p&gt;What if AWK had pipelines? (OK, now I’m reaching for syntax sugar…)&lt;/p&gt;
    &lt;code&gt;BEGIN {
  result = [1, 2, 3, 4, 5] 
      |&amp;gt; filter((x) =&amp;gt; { x % 2 == 0 })
      |&amp;gt; map((x) =&amp;gt; { x * x })
      |&amp;gt; reduce((acc, x) =&amp;gt; { acc + x }, 0)

  print "Result:", result
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Making it happen&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;TL;DR:&lt;/p&gt;&lt;code&gt;Janiczek/fawk&lt;/code&gt;on GitHub&lt;/quote&gt;
    &lt;p&gt;Now for the crazy, LLM-related part of the post. I didn’t want to spend days implementing AWK from scratch or tweaking somebody else’s implementation. So I tried to use Cursor Agent for a larger task than I usually do (I tend to ask for very small targeted edits), and asked Sonnet 4.5 for a README with code examples, and then a full implementation in Python.&lt;/p&gt;
    &lt;p&gt;And it did it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: I also asked for implementations in C, Haskell and Rust at the same time, not knowing if any of the four would succeed, and they all seem to have produced code that at least compiles/runs. I haven’t tried to test them or even run them though. The PRs are here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I was very impressed—I still am! I expected the LLM to stumble and flail around and ultimately get nothing done, but it did what I asked it for (gave me an interpreter that could run those specific examples), and over the course of a few chat sessions, I guided it towards implementing more and more of “the rest of AWK”, together with an excessive amount of end-to-end tests.&lt;/p&gt;
    &lt;p&gt;The only time I could see it struggle was when I asked it to implement arbitrary precision floating point operations without using an external library like &lt;code&gt;mpmath&lt;/code&gt;. It attempted to use Taylor series, but couldn’t get it right for at
least a few minutes. I chickened out and told it to &lt;code&gt;uv add mpmath&lt;/code&gt; and simplify
the interpreter code. In a moment it was done.&lt;/p&gt;
    &lt;p&gt;Other things that I thought it would choke on, like &lt;code&gt;print&lt;/code&gt; being both a
statement (with &lt;code&gt;&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;gt;&amp;gt;&lt;/code&gt; redirection support) and an expression, or
multi-dimensional arrays, or multi-line records, these were all implemented
correctly. Updating the test suite to also check for backwards compatibility
with GAWK - not an issue. Lexical scoping
and tricky closure environment behaviour - handled that just fine.&lt;/p&gt;
    &lt;head rend="h2"&gt;What now?&lt;/head&gt;
    &lt;p&gt;As the cool kids say, I have to update my priors. The frontier of what the LLMs can do has moved since the last time I tried to vibe-code something. I didn’t expect to have a working interpreter the same day I dreamt of a new programming language. It now seems possible.&lt;/p&gt;
    &lt;p&gt;The downside of vibe coding the whole interpreter is that I have zero knowledge of the code. I only interacted with the agent by telling it to implement a thing and write tests for it, and I only really reviewed the tests. I reckon this would be an issue in the future when I want to manually make some change in the actual code, because I have no familiarity with it.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This also opened new questions for me wrt. my other projects where I’ve previously run out of steam, eg. trying to implement a Hindley-Milner type system for my dream forever-WIP programming language Cara. It seems I can now just ask the LLM to do it, and it will? But then, I don’t want to fall into the trap where I am no longer able to work on the codebase myself. I want to be familiar with and able to tinker on the code. I’d need to spend my time reviewing and reading code instead of writing everything myself. Perhaps that’s OK.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Performance of FAWK might be an issue as well, though right now it’s a non-goal, given my intended use case is throwaway scripts for Advent of Code, nothing user-facing. And who knows, based on what I’ve seen, maybe I can instruct it to rewrite it in Rust and have a decent chance of success?&lt;/p&gt;
    &lt;p&gt;For now, I’ll go dogfood my shiny new vibe-coded black box of a programming language on the Advent of Code problem (and as many of the 2025 puzzles as I can), and see what rough edges I can find. I expect them to be equal parts “not implemented yet” and “unexpected interactions of new PL features with the old ones”.&lt;/p&gt;
    &lt;p&gt;If you’re willing to jump through some Python project dependency hoops, you can try to use FAWK too at your own risk, at &lt;code&gt;Janiczek/fawk&lt;/code&gt; on
GitHub.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46003144</guid><pubDate>Fri, 21 Nov 2025 10:28:49 +0000</pubDate></item><item><title>How a French judge was digitally cut off by the USA</title><link>https://www.heise.de/en/news/How-a-French-judge-was-digitally-cut-off-by-the-USA-11087561.html</link><description>&lt;doc fingerprint="65a032ec176759d4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How a French judge was digitally cut off by the USA&lt;/head&gt;
    &lt;p&gt;Nicolas Guillou has been sanctioned by the USA as a judge of the International Criminal Court. He notices the effects primarily in the digital realm.&lt;/p&gt;
    &lt;p&gt;Digital sovereignty has been much discussed in Europe in recent weeks, most recently during a German-French summit in Berlin. The extent of dependence on the USA in the digital sector is currently being experienced by a French judge. Nicolas Guillou, one of six judges and three prosecutors of the International Criminal Court (ICC), was sanctioned by the USA in August. He described his current situation as a digital time travel back to the 1990s, before the internet age, in a recent interview.&lt;/p&gt;
    &lt;p&gt;The reason for the US sanctions are the arrest warrants against Israeli Prime Minister Benjamin Netanyahu and Defense Minister Yoav Gallant. They were indicted for war crimes and crimes against humanity in the context of the destruction of the Gaza Strip. The USA condemned this decision by the court, whereupon the US Treasury Department sanctioned six judges and three prosecutors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Digitally excluded from almost everything&lt;/head&gt;
    &lt;p&gt;In Guillou's daily life, this means that he is excluded from digital life and much of what is considered standard today, he told the French newspaper Le Monde. All his accounts with US companies such as Amazon, Airbnb, or PayPal were immediately closed by the providers. Online bookings, such as through Expedia, are immediately canceled, even if they concern hotels in France. Participation in e-commerce is also practically no longer possible for him, as US companies always play a role in one way or another, and they are strictly forbidden to enter into any trade relationship with sanctioned individuals.&lt;/p&gt;
    &lt;p&gt;Videos by heise&lt;/p&gt;
    &lt;p&gt;He also describes the impact on participating in banking as drastic. Payment systems are blocked for him, as US companies like American Express, Visa, and Mastercard have a virtual monopoly in Europe. He also describes the rest of banking as severely restricted. For example, accounts with non-US banks have also been partially closed. Transactions in US dollars or via dollar conversions are forbidden to him.&lt;/p&gt;
    &lt;head rend="h3"&gt;Judge: EU should block sanctions&lt;/head&gt;
    &lt;p&gt;Guillou's case shows how strong the USA's influence in the tech sector is and how few options he has to circumvent it. And this at a time when an account with a US tech company is considered a matter of course in more and more places.&lt;/p&gt;
    &lt;p&gt;The French judge advocates for Europe to gain more sovereignty in the digital and banking sectors. Without this sovereignty, the rule of law cannot be guaranteed, he warns. At the same time, he calls on the EU to activate an existing blocking regulation (Regulation (EC) No 2271/96) for the International Criminal Court, which prevents third countries like the USA from enforcing sanctions in the EU. EU companies would then no longer be allowed to comply with US sanctions if they violate EU interests. Companies that violate this would then be liable for damages.&lt;/p&gt;
    &lt;p&gt;(mki)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46003778</guid><pubDate>Fri, 21 Nov 2025 12:12:41 +0000</pubDate></item><item><title>Making a Small RPG</title><link>https://jslegenddev.substack.com/p/making-a-small-rpg</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46004293</guid><pubDate>Fri, 21 Nov 2025 13:23:16 +0000</pubDate></item><item><title>More tales about outages and numeric limits</title><link>https://rachelbythebay.com/w/2025/11/18/down/</link><description>&lt;doc fingerprint="af811b9b5ac90ef0"&gt;
  &lt;main&gt;
    &lt;p&gt;Nov 21, 2025 Tags: oss, security&lt;/p&gt;
    &lt;p&gt;TL;DR: Dependency cooldowns are a free, easy, and incredibly effective way to mitigate the large majority of open source supply chain attacks. More individual projects should apply cooldowns (via tools like Dependabot and Renovate) to their dependencies, and packaging ecosystems should invest in first-class support for cooldowns directly in their package managers.&lt;/p&gt;
    &lt;p&gt;âSupply chain securityâ is a serious problem. Itâs also seriously overhyped, in part because dozens of vendors have a vested financial interest in convincing your that their framing of the underlying problem1 is (1) correct, and (2) worth your money.&lt;/p&gt;
    &lt;p&gt;Whatâs consternating about this is that most open source supply chain attacks have the same basic structure:&lt;/p&gt;
    &lt;p&gt;An attacker compromises a popular open source project, typically via a stolen credential or CI/CD vulnerabilty (such as âpwn requestsâ in GitHub Actions).&lt;/p&gt;
    &lt;p&gt;The attacker introduces a malicious change to the project and uploads it somewhere that will have maximum effect (PyPI, npm, GitHub releases, &amp;amp;c., depending on the target).&lt;/p&gt;
    &lt;p&gt;At this point, the clock has started, as the attacker has moved into the public.&lt;/p&gt;
    &lt;p&gt;Users pick up the compromised version of the project via automatic dependency updates or a lack of dependency pinning.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the aforementioned vendors are scanning public indices as well as customer repositories for signs of compromise, and provide alerts upstream (e.g. to PyPI).&lt;/p&gt;
    &lt;p&gt;Notably, vendors are incentivized to report quickly and loudly upstream, as this increases the perceived value of their services in a crowded field.&lt;/p&gt;
    &lt;p&gt;Upstreams (PyPI, npm, &amp;amp;c.) remove or disable the compromised package version(s).&lt;/p&gt;
    &lt;p&gt;End-user remediation begins.&lt;/p&gt;
    &lt;p&gt;The key thing to observe is that the gap between (1) and (2) can be very large2 (weeks or months), while the gap between (2) and (5) is typically very small: hours or days. This means that, once the attacker has moved into the actual exploitation phase, their window of opportunity to cause damage is pretty limited.&lt;/p&gt;
    &lt;p&gt;We can see this with numerous prominent supply chain attacks over the last 18 months3:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attack&lt;/cell&gt;
        &lt;cell role="head"&gt;Approx. Window of Opportunity&lt;/cell&gt;
        &lt;cell role="head"&gt;References&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xz-utils&lt;/cell&gt;
        &lt;cell&gt;â 5 weeks4&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 1)&lt;/cell&gt;
        &lt;cell&gt;12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 2)&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tj-actions&lt;/cell&gt;
        &lt;cell&gt;3 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;chalk&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nx&lt;/cell&gt;
        &lt;cell&gt;4 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rspack&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num2words&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Kong Ingress Controller&lt;/cell&gt;
        &lt;cell&gt;â 10 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;web3.js&lt;/cell&gt;
        &lt;cell&gt;5 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;(Each of these attacks has significant downstream effect, of course, but only within their window of opportunity. Subsequent compromises from each, like Shai-Hulud, represent new windows of opportunity where the attackers regrouped and pivoted onto the next set of compromised credentials.)&lt;/p&gt;
    &lt;p&gt;My takeaway from this: some windows of opportunity are bigger, but the majority of them are under a week long. Consequently, ordinary developers can avoid the bulk of these types of attacks by instituting cooldowns on their dependencies.&lt;/p&gt;
    &lt;p&gt;A âcooldownâ is exactly what it sounds like: a window of time between when a dependency is published and when itâs considered suitable for use. The dependency is public during this window, meaning that âsupply chain securityâ vendors can work their magic while the rest of us wait any problems out.&lt;/p&gt;
    &lt;p&gt;I love cooldowns for several reasons:&lt;/p&gt;
    &lt;p&gt;Theyâre empirically effective, per above. They wonât stop all attackers, but they do stymie the majority of high-visibiity, mass-impact supply chain attacks that have become more common.&lt;/p&gt;
    &lt;p&gt;Theyâre incredibly easy to implement. Moreover, theyâre literally free to implement in most cases: most people can use Dependabotâs functionality, Renovateâs functionality, or the functionality build directly into their package manager5.&lt;/p&gt;
    &lt;p&gt;This is how simple it is in Dependabot:&lt;/p&gt;
    &lt;p&gt;(Rinse and repeat for other ecosystems as needed.)&lt;/p&gt;
    &lt;p&gt;Cooldowns enforce positive behavior from supply chain security vendors: vendors are still incentivized to discover and report attacks quickly, but are not as incentivized to emit volumes of blogspam about âcriticalâ attacks on largely underfunded open source ecosystems.&lt;/p&gt;
    &lt;p&gt;In the very small sample set above, 8/10 attacks had windows of opportunity of less than a week. Setting a cooldown of 7 days would have prevented the vast majority of these attacks from reaching end users (and causing knock-on attacks, which several of these were). Increasing the cooldown to 14 days would have prevented all but 1 of these attacks6.&lt;/p&gt;
    &lt;p&gt;Cooldowns are, obviously, not a panacea: some attackers will evade detection, and delaying the inclusion of potentially malicious dependencies by a week (or two) does not fundamentally alter the fact that supply chain security is a social trust problem, not a purely technical one. Still, an 80-90% reduction in exposure through a technique that is free and easy seems hard to beat.&lt;/p&gt;
    &lt;p&gt;Related to the above, itâs unfortunate that cooldowns arenât baked directly into more packaging ecosystems: Dependabot and Renovate are great, but even better would be if the package manager itself (as the source of ground truth) could enforce cooldowns directly (including of dependencies not introduced or bumped through automated flows).&lt;/p&gt;
    &lt;p&gt;The problem being, succinctly: modern software stacks are complex and opaque, with little to no difference in privilege between first-party code and third-party dependencies.Â ↩&lt;/p&gt;
    &lt;p&gt;In part because of the prevalence of long-lived, overscoped credentials. Long-lived credentials let attackers operate on their own (comfortable) timelines; this is why Trusted Publishing is such a useful (but not wholly sufficient) technique for reducing the attackerâs attack staging window.Â ↩&lt;/p&gt;
    &lt;p&gt;Filippo Valsorda has an excellent compilation of recent supply chain compromises here.Â ↩&lt;/p&gt;
    &lt;p&gt;The xz-utils attack is a significant outlier, both in its scope and the length of its window of opportunity. In this case, Iâve measured from the attackerâs first backdoored release (v5.6.0, 2024-02-24) to the time of rollback within Debian (2024-03-28).Â ↩&lt;/p&gt;
    &lt;p&gt;For example, pnpmâs &lt;code&gt;minimumReleaseAge&lt;/code&gt;.
           uv also has &lt;code&gt;exclude-newer&lt;/code&gt;, 
           although this specifies an absolute cutoff rather than a rolling cooldown.Â ↩&lt;/p&gt;
    &lt;p&gt;Notably, the only attack that would have stymied a 14-day cooldown is xz-utils, which is also the most technically, logistically, and socially advanced of all of the attacks.Â ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46004541</guid><pubDate>Fri, 21 Nov 2025 13:44:12 +0000</pubDate></item><item><title>We should all be using dependency cooldowns</title><link>https://blog.yossarian.net/2025/11/21/We-should-all-be-using-dependency-cooldowns</link><description>&lt;doc fingerprint="af811b9b5ac90ef0"&gt;
  &lt;main&gt;
    &lt;p&gt;Nov 21, 2025 Tags: oss, security&lt;/p&gt;
    &lt;p&gt;TL;DR: Dependency cooldowns are a free, easy, and incredibly effective way to mitigate the large majority of open source supply chain attacks. More individual projects should apply cooldowns (via tools like Dependabot and Renovate) to their dependencies, and packaging ecosystems should invest in first-class support for cooldowns directly in their package managers.&lt;/p&gt;
    &lt;p&gt;âSupply chain securityâ is a serious problem. Itâs also seriously overhyped, in part because dozens of vendors have a vested financial interest in convincing your that their framing of the underlying problem1 is (1) correct, and (2) worth your money.&lt;/p&gt;
    &lt;p&gt;Whatâs consternating about this is that most open source supply chain attacks have the same basic structure:&lt;/p&gt;
    &lt;p&gt;An attacker compromises a popular open source project, typically via a stolen credential or CI/CD vulnerabilty (such as âpwn requestsâ in GitHub Actions).&lt;/p&gt;
    &lt;p&gt;The attacker introduces a malicious change to the project and uploads it somewhere that will have maximum effect (PyPI, npm, GitHub releases, &amp;amp;c., depending on the target).&lt;/p&gt;
    &lt;p&gt;At this point, the clock has started, as the attacker has moved into the public.&lt;/p&gt;
    &lt;p&gt;Users pick up the compromised version of the project via automatic dependency updates or a lack of dependency pinning.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the aforementioned vendors are scanning public indices as well as customer repositories for signs of compromise, and provide alerts upstream (e.g. to PyPI).&lt;/p&gt;
    &lt;p&gt;Notably, vendors are incentivized to report quickly and loudly upstream, as this increases the perceived value of their services in a crowded field.&lt;/p&gt;
    &lt;p&gt;Upstreams (PyPI, npm, &amp;amp;c.) remove or disable the compromised package version(s).&lt;/p&gt;
    &lt;p&gt;End-user remediation begins.&lt;/p&gt;
    &lt;p&gt;The key thing to observe is that the gap between (1) and (2) can be very large2 (weeks or months), while the gap between (2) and (5) is typically very small: hours or days. This means that, once the attacker has moved into the actual exploitation phase, their window of opportunity to cause damage is pretty limited.&lt;/p&gt;
    &lt;p&gt;We can see this with numerous prominent supply chain attacks over the last 18 months3:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attack&lt;/cell&gt;
        &lt;cell role="head"&gt;Approx. Window of Opportunity&lt;/cell&gt;
        &lt;cell role="head"&gt;References&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xz-utils&lt;/cell&gt;
        &lt;cell&gt;â 5 weeks4&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 1)&lt;/cell&gt;
        &lt;cell&gt;12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ultralytics (phase 2)&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tj-actions&lt;/cell&gt;
        &lt;cell&gt;3 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;chalk&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Nx&lt;/cell&gt;
        &lt;cell&gt;4 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rspack&lt;/cell&gt;
        &lt;cell&gt;1 hour&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;num2words&lt;/cell&gt;
        &lt;cell&gt;&amp;lt; 12 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Kong Ingress Controller&lt;/cell&gt;
        &lt;cell&gt;â 10 days&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;web3.js&lt;/cell&gt;
        &lt;cell&gt;5 hours&lt;/cell&gt;
        &lt;cell&gt;Source&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;(Each of these attacks has significant downstream effect, of course, but only within their window of opportunity. Subsequent compromises from each, like Shai-Hulud, represent new windows of opportunity where the attackers regrouped and pivoted onto the next set of compromised credentials.)&lt;/p&gt;
    &lt;p&gt;My takeaway from this: some windows of opportunity are bigger, but the majority of them are under a week long. Consequently, ordinary developers can avoid the bulk of these types of attacks by instituting cooldowns on their dependencies.&lt;/p&gt;
    &lt;p&gt;A âcooldownâ is exactly what it sounds like: a window of time between when a dependency is published and when itâs considered suitable for use. The dependency is public during this window, meaning that âsupply chain securityâ vendors can work their magic while the rest of us wait any problems out.&lt;/p&gt;
    &lt;p&gt;I love cooldowns for several reasons:&lt;/p&gt;
    &lt;p&gt;Theyâre empirically effective, per above. They wonât stop all attackers, but they do stymie the majority of high-visibiity, mass-impact supply chain attacks that have become more common.&lt;/p&gt;
    &lt;p&gt;Theyâre incredibly easy to implement. Moreover, theyâre literally free to implement in most cases: most people can use Dependabotâs functionality, Renovateâs functionality, or the functionality build directly into their package manager5.&lt;/p&gt;
    &lt;p&gt;This is how simple it is in Dependabot:&lt;/p&gt;
    &lt;p&gt;(Rinse and repeat for other ecosystems as needed.)&lt;/p&gt;
    &lt;p&gt;Cooldowns enforce positive behavior from supply chain security vendors: vendors are still incentivized to discover and report attacks quickly, but are not as incentivized to emit volumes of blogspam about âcriticalâ attacks on largely underfunded open source ecosystems.&lt;/p&gt;
    &lt;p&gt;In the very small sample set above, 8/10 attacks had windows of opportunity of less than a week. Setting a cooldown of 7 days would have prevented the vast majority of these attacks from reaching end users (and causing knock-on attacks, which several of these were). Increasing the cooldown to 14 days would have prevented all but 1 of these attacks6.&lt;/p&gt;
    &lt;p&gt;Cooldowns are, obviously, not a panacea: some attackers will evade detection, and delaying the inclusion of potentially malicious dependencies by a week (or two) does not fundamentally alter the fact that supply chain security is a social trust problem, not a purely technical one. Still, an 80-90% reduction in exposure through a technique that is free and easy seems hard to beat.&lt;/p&gt;
    &lt;p&gt;Related to the above, itâs unfortunate that cooldowns arenât baked directly into more packaging ecosystems: Dependabot and Renovate are great, but even better would be if the package manager itself (as the source of ground truth) could enforce cooldowns directly (including of dependencies not introduced or bumped through automated flows).&lt;/p&gt;
    &lt;p&gt;The problem being, succinctly: modern software stacks are complex and opaque, with little to no difference in privilege between first-party code and third-party dependencies.Â ↩&lt;/p&gt;
    &lt;p&gt;In part because of the prevalence of long-lived, overscoped credentials. Long-lived credentials let attackers operate on their own (comfortable) timelines; this is why Trusted Publishing is such a useful (but not wholly sufficient) technique for reducing the attackerâs attack staging window.Â ↩&lt;/p&gt;
    &lt;p&gt;Filippo Valsorda has an excellent compilation of recent supply chain compromises here.Â ↩&lt;/p&gt;
    &lt;p&gt;The xz-utils attack is a significant outlier, both in its scope and the length of its window of opportunity. In this case, Iâve measured from the attackerâs first backdoored release (v5.6.0, 2024-02-24) to the time of rollback within Debian (2024-03-28).Â ↩&lt;/p&gt;
    &lt;p&gt;For example, pnpmâs &lt;code&gt;minimumReleaseAge&lt;/code&gt;.
           uv also has &lt;code&gt;exclude-newer&lt;/code&gt;, 
           although this specifies an absolute cutoff rather than a rolling cooldown.Â ↩&lt;/p&gt;
    &lt;p&gt;Notably, the only attack that would have stymied a 14-day cooldown is xz-utils, which is also the most technically, logistically, and socially advanced of all of the attacks.Â ↩&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005111</guid><pubDate>Fri, 21 Nov 2025 14:50:36 +0000</pubDate></item><item><title>XBMC 4.0 for the Original Xbox</title><link>https://www.xbox-scene.info/articles/announcing-xbmc-40-for-the-original-xbox-r64/</link><description>&lt;doc fingerprint="3f9edd7dc8a4cddc"&gt;
  &lt;main&gt;
    &lt;p&gt;A Major Modernization of the Killer App That Started It All&lt;/p&gt;
    &lt;p&gt;A new version of Xbox Media Center (XBMC), version 4.0, has been released. This version marks a significant update to the long-standing media center platform for the Original Xbox. This marks the first major advancement to the software since 2016 and represents a renewed commitment to preserving, modernizing, and extending the capabilities of one of the most iconic console homebrew applications ever created.&lt;/p&gt;
    &lt;p&gt;XBMC has a long and influential history. In 2002, XboxMediaPlayer (XMP) was released and turned the console into a powerful multimedia device fit for the living room in an era when connecting a computer to a TV was quite novel. Later that same year, XMP merged with YAMP and became Xbox Media Player 2.0. A few years later, the software evolved into Xbox Media Center, or XBMC, which introduced a new interface, a plugin system powered by Python, and a robust skinning engine.&lt;/p&gt;
    &lt;p&gt;XBMC eventually became so capable that it outgrew the Xbox entirely. By 2007, developers were working on PC ports and in 2010, the project split into two branches: one for general computers while the Xbox version became XBMC4Xbox, and each codebase was maintained from then on by separate teams. XBMC was later renamed to Kodi in 2014 and continues to be one of the most popular media center applications available. Even Plex traces its roots back to XBMC. Plex began as OSXBMC, a Mac port of XBMC in late 2007, before becoming its own project in 2008. This means the Original Xbox helped shape not one but two of the biggest media center apps used today.&lt;/p&gt;
    &lt;p&gt;The last official release of XBMC4Xbox arrived in February 2016 with version 3.5.3. Although the community never declared the project dead, meaningful updates became scarce. XBMC 4.0 continues that legacy by bringing a modern interface, updating it to be more inline with Kodi's modern codebase, and backporting features to the original 64MB RAM / Pentium-III hardware where it all began.&lt;/p&gt;
    &lt;p&gt;This project is distinct and separate from XBMC4Gamers, the games-focused variation of XBMC4Xbox (v3.5.3) by developer Rocky5.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Modern Interface Powered by Estuary&lt;/head&gt;
    &lt;p&gt;One of the most notable advancements in XBMC 4.0 is the introduction of the Estuary user interface (skin).&lt;/p&gt;
    &lt;p&gt;Estuary, originally released in 2017 with Kodi v17 ("Krypton"), provides a clean and modern layout that improves navigation and readability over past skins. Bringing Estuary to the Xbox required extensive updates to the underlying GUI framework, including a port of the more contemporary GUIlib engine. This allows the platform to support modern skinning standards and makes future skin ports much more straightforward. After the initial work of porting GUIlib was done, porting Estuary to the Xbox was a relatively simple process of tweaking a handful of configuration files and adding contextual features specific to the Xbox. The result is a modern, intuitive front end that retains the performance and responsiveness required on legacy hardware.&lt;/p&gt;
    &lt;p&gt;Firing up an Xbox made in 2001 and being greeted by the same interface as what you'd find if you were to download Kodi today onto your PC feels like a bit of magic, and helps keep this beloved classic console relevant and useful well into the modern era.&lt;/p&gt;
    &lt;head rend="h2"&gt;Expanded Games Library Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 introduces a fully realized games library system. This enhancement brings the same level of metadata support found in the Movies and Music sections to Xbox and emulated games. Titles can now display artwork, descriptions, and other metadata, transforming the games section into a polished and user-friendly library. XBMC’s longstanding support for trainers remains intact, giving users the option to apply gameplay modifications for compatible titles. Emulated game collections benefit as well, with the ability to browse ROM libraries and launch them directly in a user’s preferred emulator.&lt;/p&gt;
    &lt;head rend="h2"&gt;Online Scrapers and Metadata Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 restores full functionality to metadata scrapers for movies and television. This allows users to build rich media libraries complete with artwork, plot summaries, cast listings, and other information retrieved directly from online sources. XBMC 4.0 handles these tasks efficiently, even on the Xbox’s limited memory and processing power. Video playback continues to support 480p and 720p content, enabling the console to serve as a surprisingly capable media device for its age. Similar to Kodi, XBMC 4.0 supports filtering, building playlists, watch progress history for media, and intelligent handling of TV shows with seasons.&lt;/p&gt;
    &lt;p&gt;Aside from scrapers for multimedia, support for rich library capabilities for games has also been added. XBMC has always been a media-first app, and now users can enjoy the library experience that they've come to love for media now in the context of their games library (more info below).&lt;/p&gt;
    &lt;head rend="h2"&gt;Improved Task Scheduling and Multitasking&lt;/head&gt;
    &lt;p&gt;Despite the constraints of the Xbox’s single-threaded 733MHz CPU, XBMC 4.0 includes improvements to task scheduling that allow multiple activities to run concurrently. Background library updates, metadata scraping, and audio/video playback can occur while users navigate and use other parts of the interface. These optimizations help ensure a fluid experience without compromising performance. Much work has been done "under the hood" to keep XBMC on task and within memory budgets while achieving multi-tasking on a console that wasn't exactly designed with it in mind. Users who own RAM and/or CPU upgraded consoles can also take advantage of the extra overhead, as XBMC 4.0 makes use of the extra horsepower for an even smoother experience. Utilizing an SSD with higher UDMA speeds will also yield an improvement in overall responsiveness.&lt;/p&gt;
    &lt;head rend="h2"&gt;Music Experience and Visualizers&lt;/head&gt;
    &lt;p&gt;Music playback has always been a strong element of XBMC, and version 4.0 maintains that focus. The Original Xbox is capable of high quality audio output, and XBMC continues to support lossless codecs such as FLAC. The release includes compatibility with various audio visualizers, including MilkDrop, which remains one of the most visually impressive and customizable audio visualization engines available. These features allow XBMC 4.0 to function not only as a media organizer, but also as an immersive audio display system.&lt;/p&gt;
    &lt;p&gt;An online repository has been established and will be maintained moving forward where users can download legacy and newly-released add-ons as they become available. This repository is accessible without additional setup, right out of the box!&lt;/p&gt;
    &lt;head rend="h2"&gt;Add-ons and Python Support&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 continues to offer an extendable architecture powered by Python-based add-ons. While the current release uses Python 2.7 for compatibility, work is underway to transition to Python 3.4.10 in the future, which may provide a path for backporting many newer Kodi add-ons. Even in its current state, XBMC 4.0 already supports a variety of community-developed add-ons that extend the system’s functionality, including tools for online video playback (i.e. YouTube), online weather services, and enhanced media organization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Updated Settings, Network Services, and System Tools&lt;/head&gt;
    &lt;p&gt;The settings interface has been revised to provide more clarity and control. The update includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Playback options, including episode progression, crossfade behavior, and subtitle handling&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Library management tools&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Network features, such as SMB, FTP, UPnP sharing, web server access, and Insignia-compatible DNS options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Comprehensive interface customization options&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multiple user profiles with individual library settings&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Advanced system controls for video calibration, display modes, input devices, and power management&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A robust System Information section for diagnostics, with info geared towards the Original Xbox&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A flexible File Manager with support for network protocols including FTP, SMB, WebDAV, and more&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Users may also take advantage of an online add-ons repository, offering the same experience modern Kodi provides with being able to download add-ons to extend functionality of the app with things like online multimedia providers, weather, skins, visualizers, and more. Developers can submit new add-ons to the official repository via Github.&lt;/p&gt;
    &lt;head rend="h2"&gt;Continuing the Legacy&lt;/head&gt;
    &lt;p&gt;XBMC has been a staple of the Original Xbox's homebrew scene since its inception in the early 2000's. This new update is a revival of the platform that helped shape the landscape of home media software and helps revitalize a codebase that has been somewhat stagnant for many years. This release honors that heritage while modernizing the experience for a new generation of enthusiasts and preserving the functionality of the Original Xbox as a versatile and capable media center.&lt;/p&gt;
    &lt;p&gt;Although the hardware is decades old, the renewed effort behind XBMC 4.0 demonstrates that the platform still has room to grow and tricks up its sleeve. With ongoing development and a codebase designed with modern Kodi compatibility in mind, XBMC 4.0 represents a significant step forward into the continued development on the Original Xbox.&lt;/p&gt;
    &lt;p&gt;The development team looks forward to continuing this work and expanding the possibilities of the Original Xbox for years to come. This version is the first of many to come, with lots of things cooking in the background. Keep an eye out for future releases by joining the Xbox-Scene Discord and turning on notifications in the xbmc-news channel or by periodically checking the project's Github page.&lt;/p&gt;
    &lt;head rend="h2"&gt;Downloads&lt;/head&gt;
    &lt;p&gt;XBMC 4.0 (and subsequent releases) builds along with source code are available via Github:&lt;/p&gt;
    &lt;p&gt;Main project page: Click Here&lt;/p&gt;
    &lt;p&gt;Note: XBMC 4.0 is is in active development! This means updates will be released in a more frequent manner for the time being until things settle down. Check the nightly builds section on Github for the most up-to-date version.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contributions&lt;/head&gt;
    &lt;p&gt;XBMC is open source software and welcomes contributions.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Coding: Developers can help XBMC by fixing a bug, adding new features, making our technology smaller and faster and making development easier for others. XBMC's codebase consists mainly of C++ with small parts written in a variety of coding languages. Our add-ons mainly consist of python and XML.&lt;/item&gt;
      &lt;item&gt;Helping users: Our support process relies on enthusiastic contributors like you to help others get the most out of XBMC. The #1 priority is always answering questions in our support forums. Everyday new people discover XBMC, and everyday they are virtually guaranteed to have questions.&lt;/item&gt;
      &lt;item&gt;Localization: Translate XBMC, add-ons, skins etc. into your native language.&lt;/item&gt;
      &lt;item&gt;Add-ons: Add-ons are what make XBMC the most extensible and customizable entertainment hub available. Get started building an add-on.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Support and Bug Reporting&lt;/head&gt;
    &lt;p&gt;Need help?&lt;/p&gt;
    &lt;p&gt;Support can be found in the XBMC -&amp;gt; General channel within the Xbox-Scene Discord server.&lt;/p&gt;
    &lt;head rend="h2"&gt;Credits and Disclaimers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Nikola Antonić - Primary Developer, Project Lead&lt;/item&gt;
      &lt;item&gt;astarivi - Contributor (cURL, wolfSSL), Tester, Debugger&lt;/item&gt;
      &lt;item&gt;EqUiNoX - Contrubitor, Tester&lt;/item&gt;
      &lt;item&gt;Rocky5 - Contributor, Tester&lt;/item&gt;
      &lt;item&gt;.lavenderStarlight+ - Add-ons / Skins Development, Tester&lt;/item&gt;
      &lt;item&gt;GoTeamScotch - Tester, Feedback&lt;/item&gt;
      &lt;item&gt;Haguero - Tester, Feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;XBMC is GPLv2 licensed. You may use, distribute and copy it under the license terms. XBMC is licensed under the same terms as Kodi. For detailed information on the licensing, please refer to the Kodi license.&lt;/p&gt;
    &lt;p&gt;This project, XBMC version 4.0 (and upcoming releases), is distinct from and is not affiliated with Team Kodi of The Kodi Foundation, or its members.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005349</guid><pubDate>Fri, 21 Nov 2025 15:18:05 +0000</pubDate></item><item><title>Make product worse, get money</title><link>https://dynomight.net/worse/</link><description>&lt;doc fingerprint="652b19d9bbe30db8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Make product worse, get money&lt;/head&gt;
    &lt;p&gt;I recently asked why people seem to hate dating apps so much. In response, 80% of you emailed me some version of the following theory:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The thing about dating apps is that if they do a good job and match people up, then the matched people will quit the app and stop paying. So they have an incentive to string people along but not to actually help people find long-term relationships.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;May I explain why I don’t find this type of theory very helpful?&lt;/p&gt;
    &lt;p&gt;I’m not saying that I think it’s wrong, mind you. Rather, my objection is that while the theory is phrased in terms of dating apps, the same basic pattern applies to basically anyone who is trying to make money by doing anything.&lt;/p&gt;
    &lt;p&gt;For example, consider a pizza restaurant. Try these theories on for size:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza: “The thing about pizza restaurants is that if they use expensive ingredients or labor-intensive pizza-making techniques, then it costs more to make pizza. So they have an incentive to use low-cost ingredients and labor-saving shortcuts.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza II: “The thing about pizza restaurants is that if they have nice tables separated at a comfortable distance, then they can’t fit as many customers. So they have an incentive to use tiny tables and cram people in cheek by jowl.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pizza III: “The thing about pizza restaurants is that if they sell big pizzas, then people will eat them and stop being hungry, meaning they don’t buy additional pizza. So they have an incentive to serve tiny low-calorie pizzas.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See what I mean? You can construct similar theories for other domains, too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Cars: “The thing about automakers is that making cars safe is expensive. So they have an incentive to make unsafe cars.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Videos: “The thing about video streaming is that high-resolution video uses more expensive bandwidth. So they have an incentive to use low-resolution.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Blogging: “The thing about bloggers is that research is time-consuming. So they have an incentive to be sloppy about the facts.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Durability: “The thing about {lightbulb, car, phone, refrigerator, cargo ship} manufacturing is that if you make a {lightbulb, car, phone, refrigerator, cargo ship} that lasts a long time, then people won’t buy new ones. So there’s an incentive to make {lightbulbs, cars, phones, refrigerators, cargo ships} that break quickly.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All these theories can be thought of as instances of two general patterns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Make product worse, get money: “The thing about selling goods or services is that making goods or services better costs money. So people have an incentive to make goods and services worse.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Raise price, get money: “The thing about selling goods and services is that if you raise prices, then you get more money. So people have an incentive to raise prices.”&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Are these theories wrong? Not exactly. But it sure seems like something is missing.&lt;/p&gt;
    &lt;p&gt;I’m sure most pizza restauranteurs would be thrilled to sell lukewarm 5 cm cardboard discs for $300 each. They do in fact have an incentive to do that, just as predicted by these theories! Yet, in reality, pizza restaurants usually sell pizzas that are made out of food. So clearly these theories aren’t telling the whole story.&lt;/p&gt;
    &lt;p&gt;Say you have a lucrative business selling 5 cm cardboard discs for $300. I am likely to think, “I like money. Why don’t I sell pizzas that are only mostly cardboard, but also partly made of flour? And why don’t I sell them for $200, so I can steal Valued Reader’s customers?” But if I did that, then someone else would probably set prices at only $100, or even introduce cardboard-free pizzas, and this would continue until hitting some kind of equilibrium.&lt;/p&gt;
    &lt;p&gt;Sure, producers want to charge infinity dollars for things that cost them zero dollars to make. But consumers want to pay zero dollars for stuff that’s infinitely valuable. It’s in the conflict between these desires that all interesting theories live.&lt;/p&gt;
    &lt;p&gt;This is why I don’t think it’s helpful to point out that people have an incentive to make their products worse. Of course they do. The interesting question is, why are they able to get away with it?&lt;/p&gt;
    &lt;head rend="h2"&gt;Reasons stuff is bad&lt;/head&gt;
    &lt;p&gt;First reason stuff is bad: People are cheap&lt;/p&gt;
    &lt;p&gt;Why are seats so cramped on planes? Is it because airlines are greedy? Sure. But while they might be greedy, I don’t think they’re dumb. If you do a little math, you can calculate that if airlines were to remove a single row of seats, they could add perhaps 2.5 cm (1 in) of extra legroom for everyone, while only decreasing the number of paying customers by around 3%. (This is based on a 737 with single-class, but you get the idea.)&lt;/p&gt;
    &lt;p&gt;So why don’t airlines rip out a row of seats, raise prices by 3% and enjoy the reduced costs for fuel and customer service? The only answer I can see is that people, on average, aren’t actually willing to pay 3% more for 2.5 cm more legroom. We want a worse but cheaper product, and so that’s what we get.&lt;/p&gt;
    &lt;p&gt;I think this is the most common reason stuff is “bad”. It’s why Subway sandwiches are so soggy, why video games are so buggy, and why IKEA furniture and Primark clothes fall apart so quickly.&lt;/p&gt;
    &lt;p&gt;It’s good when things are bad for this reason. Or at least, that’s the premise of capitalism: When companies cut costs, that’s the invisible hand redirecting resources to maximize social value, or whatever. Companies may be motivated by greed. And you may not like it, since you want to pay zero dollars for infinite value. But this is markets working as designed.&lt;/p&gt;
    &lt;p&gt;Second reason stuff is bad: Information asymmetries&lt;/p&gt;
    &lt;p&gt;Why is it that almost every book / blog / podcast about longevity is such garbage? Well, we don’t actually know many things that will reliably increase longevity. And those things are mostly all boring / hard / non-fun. And even if you do all of them, it probably only adds a couple of years in expectation. And telling people these facts is not a good way to find suckers who will pay you lots of money for your unproven supplements / seminars / etc.&lt;/p&gt;
    &lt;p&gt;True! But it doesn’t explain why all longevity stuff is so bad. Why don’t honest people tell the true story and drive all the hucksters out of business? I suspect the answer is that unless you have a lot of scientific training and do a lot of research, it’s basically impossible to figure out just how huckstery all the hucksters really are.&lt;/p&gt;
    &lt;p&gt;I think this same basic phenomenon explains why some supplements contain heavy metals, why some food contains microplastics, why restaurants use so much butter and salt, why rentals often have crappy insulation, and why most cars seem to only be safe along dimensions included in crash test scores. When consumers can’t tell good from evil, evil triumphs.&lt;/p&gt;
    &lt;p&gt;Third reason stuff is bad: People have bad taste&lt;/p&gt;
    &lt;p&gt;Sometimes stuff is bad because people just don’t appreciate the stuff you consider good. Examples are definitionally controversial, but I think this includes restaurants in cities where all restaurants are bad, North American tea, and travel pants. This reason has a blurry boundary with information asymmetries, as seen in ultrasonic humidifiers or products that use Sucralose instead of aspartame for “safety”.&lt;/p&gt;
    &lt;p&gt;Fourth reason stuff is bad: Pricing power&lt;/p&gt;
    &lt;p&gt;Finally, sometimes stuff is bad because markets aren’t working. Sometimes a company is selling a product but has some kind of “moat” that makes it hard for anyone else to compete with them, e.g. because of some technological or regulatory barrier, control of some key resource or location, intellectual property, a beloved brand, or network effects.&lt;/p&gt;
    &lt;p&gt;If that’s true, then those companies don’t have to worry as much about someone else stealing their business, and so (because everyone is axiomatically greedy) they will find ways to make their product cheaper and/or raise prices up until the price is equal to the full value it provides to the marginal consumer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Why is food so expensive at sporting events? Yes, people have no alternatives. But people know food is expensive at sporting events. And they don’t like it. Instead of selling water for $17, why don’t venues sell water for $2 and raise ticket prices instead? I don’t know. Probably something complicated, like that expensive food allows you to extract extra money from rich people without losing business from non-rich people.&lt;/p&gt;
    &lt;p&gt;So of course dating apps would love to string people along for years instead of finding them long-term relationships, so they keep paying money each month. I wouldn’t be surprised if some people at those companies have literally thought, “Maybe we should string people along for years instead of finding them long-term relationships, so they keep paying money each month, I love money so much.”&lt;/p&gt;
    &lt;p&gt;But if they are actually doing that (which is unclear to me) or if they are bad in some other way, then how do they get away with it? Why doesn’t someone else create a competing app that’s better and thereby steal all their business? It seems like the answer has to be either “because that’s impossible” or “because people don’t really want that”. That’s where the mystery begins.&lt;/p&gt;
    &lt;p&gt;Dating: A mysterious constellation of facts · life economics&lt;/p&gt;
    &lt;p&gt;So much blood · conspiracy economics&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005388</guid><pubDate>Fri, 21 Nov 2025 15:23:20 +0000</pubDate></item><item><title>Arduino published updated terms and conditions: no longer an open commons</title><link>https://www.molecularist.com/2025/11/did-qualcomm-kill-arduino-for-good.html</link><description>&lt;doc fingerprint="b470b35817f47bba"&gt;
  &lt;main&gt;
    &lt;p&gt;Six weeks ago, Qualcomm acquired Arduino. The maker community immediately worried that Qualcomm would kill the open-source ethos that made Arduino the lingua franca of hobby electronics.&lt;/p&gt;
    &lt;p&gt;This week, Arduino published updated terms and conditions and a new privacy policy, clearly rewritten by Qualcomm’s lawyers. The changes confirm the community’s worst fears: Arduino is no longer an open commons. It’s becoming just another corporate platform.&lt;/p&gt;
    &lt;p&gt;Here’s what’s at stake, what Qualcomm got wrong, and what might still be salvaged, drawing from community discussions across maker forums and sites.&lt;/p&gt;
    &lt;p&gt;What changed?&lt;lb/&gt;The new terms read like standard corporate boilerplate: mandatory arbitration, data integration with Qualcomm’s global ecosystem, export controls, AI use restrictions. For any other SaaS platform, this would be unremarkable.&lt;/p&gt;
    &lt;p&gt;But Arduino isn’t SaaS. It’s the foundation of the maker ecosystem.&lt;/p&gt;
    &lt;p&gt;The most dangerous change is Arduino now explicitly states that using their platform grants you no patent licenses whatsoever. You can’t even argue one is implied.&lt;/p&gt;
    &lt;p&gt;This means Qualcomm could potentially assert patents against your projects if you built them using Arduino tools, Arduino examples, or Arduino-compatible hardware.&lt;/p&gt;
    &lt;p&gt;And here’s the disconnect, baffling makers. Arduino’s IDE is licensed under AGPL. Their CLI is GPL v3. Both licenses explicitly require that you can reverse engineer the software. But the new Qualcomm terms explicitly forbid reverse engineering “the Platform.”&lt;/p&gt;
    &lt;p&gt;What’s really going on?&lt;lb/&gt;The community is trying to figure out what is Qualcomm’s actual intent. Are these terms just bad lawyering with SaaS lawyers applying their standard template to cloud services, not realizing Arduino is different? Or is Qualcomm testing how much they can get away with before the community revolts? Or is this a first step toward locking down the ecosystem they just bought?&lt;/p&gt;
    &lt;p&gt;Some people point out that “the Platform” might only mean Arduino’s cloud services (forums, Arduino Cloud, Project Hub) not the IDE and CLI that everyone actually uses.&lt;/p&gt;
    &lt;p&gt;If that’s true, Qualcomm needs to say so, explicitly, and in plain language. Because library maintainers are likely wondering whether contributing to Arduino repos puts them at legal risk. And hardware makers are questioning whether “Arduino-compatible” is still safe to advertise.&lt;/p&gt;
    &lt;p&gt;Why Adafruit’s alarm matters&lt;lb/&gt;Adafruit has been vocal about the dangers of this acquisition. Some dismiss Adafruit’s criticism as self-serving. After all, they sell competing hardware and promote CircuitPython. But that misses who Adafruit is.&lt;/p&gt;
    &lt;p&gt;Adafruit has been the moral authority on open hardware for decades. They’ve made their living proving you can build a successful business on open principles. When they sound the alarm, it’s not about competition, it’s about principle.&lt;/p&gt;
    &lt;p&gt;What they’re calling out isn’t that Qualcomm bought Arduino. It’s that Qualcomm’s lawyers fundamentally don’t understand what they bought. Arduino wasn’t valuable because it was just a microcontroller company. It was valuable because it was a commons. And you can’t apply enterprise legal frameworks to a commons without destroying it.&lt;/p&gt;
    &lt;p&gt;Adafruit gets this. They’ve built their entire business on this. That’s why their criticism carries weight.&lt;/p&gt;
    &lt;p&gt;What Qualcomm doesn’t seem to understand&lt;lb/&gt;Qualcomm probably thought they were buying an IoT hardware company with a loyal user base. &lt;/p&gt;
    &lt;p&gt;They weren’t. They bought the IBM PC of the maker world.&lt;/p&gt;
    &lt;p&gt;Arduino’s value was never just the hardware. Their boards have been obsolete for years. Their value is the standard.&lt;/p&gt;
    &lt;p&gt;The Arduino IDE is the lingua franca of hobby electronics.&lt;/p&gt;
    &lt;p&gt;Millions of makers learned on it, even if they moved to other hardware. ESP32, STM32, Teensy, Raspberry Pi Pico – none of them are Arduino hardware, but they all work with the Arduino IDE.&lt;/p&gt;
    &lt;p&gt;Thousands of libraries are “Arduino libraries.” Tutorials assume Arduino. University curricula teach Arduino. When you search “how to read a sensor,” the answer comes back in Arduino code.&lt;/p&gt;
    &lt;p&gt;This is the ecosystem Qualcomm’s lawyers just dropped legal uncertainty onto.&lt;/p&gt;
    &lt;p&gt;If Qualcomm’s lawyers start asserting control over the IDE, CLI, or core libraries under restrictive terms, they will poison the entire maker ecosystem. Even people who never buy Arduino hardware are dependent on Arduino software infrastructure.&lt;/p&gt;
    &lt;p&gt;Qualcomm didn’t just buy a company. They bought a commons. And now they inadvertently are taking steps that are destroying what made it valuable.&lt;/p&gt;
    &lt;p&gt;What are makers supposed to do?&lt;lb/&gt;There has been some buzz of folks just leaving the Arduino environment behind. But Arduino IDE alternatives such as PlatformIO and VSCode are not in any way beginner friendly. If the Arduino IDE goes, then there’s a huge problem. &lt;/p&gt;
    &lt;p&gt;I remember when Hypercard ended. There were alternatives, but none so easy. I don’t think I really coded again for almost 20 years until I picked up the Arduino IDE (go figure).&lt;/p&gt;
    &lt;p&gt;If something happens to the Arduino IDE, even if its development stalls or becomes encumbered, there’s no replacement for that easy onboarding. We’d lose many promising new makers because the first step became too steep.&lt;/p&gt;
    &lt;p&gt;The institutional knowledge at risk&lt;lb/&gt;But leaving Arduino behind isn’t simple. The platform’s success depends on two decades of accumulated knowledge, such as countless Arduino tutorials on YouTube, blogs, and school curricula; open-source libraries that depend on Arduino compatibility; projects in production using Arduino tooling; and university programs built around Arduino as the teaching platform&lt;/p&gt;
    &lt;p&gt;All of these depend on Arduino remaining open and accessible.&lt;/p&gt;
    &lt;p&gt;If Qualcomm decided to sunset the open Arduino IDE in favor of a locked-down “Arduino Pro” platform, or if they start asserting patent claims, or if uncertainty makes contributors abandon the ecosystem, all that knowledge becomes stranded.&lt;/p&gt;
    &lt;p&gt;It’s like Wikipedia going behind a paywall. The value isn’t just the content, it is the trust that it remains accessible. Arduino’s value isn’t just the code, it’s the trust that the commons would stay open.&lt;/p&gt;
    &lt;p&gt;That trust is now gone. And once lost, it hard to get back.&lt;/p&gt;
    &lt;p&gt;Why this happened (but doesn’t excuse it)&lt;lb/&gt;Let’s be fair to Qualcomm, their lawyers were doing their jobs.&lt;/p&gt;
    &lt;p&gt;When you acquire a company, you standardize the legal terms; add mandatory arbitration to limit class action exposure; integrate data systems for compliance and auditing; add export controls because you sell to defense contractors; prohibit reverse engineering because that’s in the template.&lt;/p&gt;
    &lt;p&gt;For most acquisitions, this is just good corporate hygiene. And Arduino, now part of a megacorp, faces higher liabilities than it did as an independent entity.&lt;/p&gt;
    &lt;p&gt;But here’s what Qualcomm’s lawyers missed: Arduino isn’t a normal acquisition. The community isn’t a customer base, it’s a commons. And you can’t apply enterprise SaaS legal frameworks to a commons without destroying what made it valuable.&lt;/p&gt;
    &lt;p&gt;This is tone-deafness, not malice. But the outcome is the same. A community that trusted Arduino no longer does.&lt;/p&gt;
    &lt;p&gt;Understanding why this happened doesn’t excuse it, but it might suggest what needs to happen next.&lt;/p&gt;
    &lt;p&gt;What should have happened and how to still save it&lt;lb/&gt;Qualcomm dropped legal boilerplate on the community with zero context and let people discover the contradictions themselves. That’s how you destroy trust overnight.&lt;/p&gt;
    &lt;p&gt;Qualcomm should have announced the changes in advance. They should have given the community weeks, not hours, to understand what’s changing and why. They should have used plain-language explanations, not just legal documents.&lt;/p&gt;
    &lt;p&gt;Qualcomm can fix things by explicitly carving out the open ecosystem. They should state clearly that the terms apply to Arduino Cloud services, and the IDE, CLI, and core libraries remain under their existing open source licenses.&lt;/p&gt;
    &lt;p&gt;We’d need concrete commitments, such as which repos stay open, which licenses won’t change, what’s protected from future acquisition decisions. Right now we have vague corporate-speak about “supporting the community.”&lt;/p&gt;
    &lt;p&gt;Indeed, they could create some structural protection, as well, by putting IDE, CLI, and core libraries in a foundation that Qualcomm couldn’t unilaterally control (think the Linux Foundation model).&lt;/p&gt;
    &lt;p&gt;Finally, Qualcomm might wish to establish some form of community governance with real representation and real power over the tools the community depends on.&lt;/p&gt;
    &lt;p&gt;The acquisition is done. The legal integration is probably inevitable. But how it’s done determines whether Arduino survives as a commons or dies as just another Qualcomm subsidiary.&lt;/p&gt;
    &lt;p&gt;What’s next?&lt;lb/&gt;Arduino may be the toolset that made hobby electronics accessible to millions. But that maker community built Arduino into what it became. Qualcomm’s acquisition has thrown that legacy into doubt. Whether through legal confusion, corporate tone-deafness, or deliberate strategy, the community’s trust is broken.&lt;/p&gt;
    &lt;p&gt;The next few months will reveal whether this was a stumble or a strategy. If Qualcomm issues clarifications, moves repos to some sort of governance, and explicitly protects the open toolchain, then maybe this is salvageable. If they stay silent, or worse, if IDE development slows or license terms tighten further, then that’s a signal to find alternatives.&lt;/p&gt;
    &lt;p&gt;The question isn’t whether the open hobby electronics maker community survives. It’s whether Arduino does.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005553</guid><pubDate>Fri, 21 Nov 2025 15:44:16 +0000</pubDate></item><item><title>The New AI Consciousness Paper</title><link>https://www.astralcodexten.com/p/the-new-ai-consciousness-paper</link><description>&lt;doc fingerprint="e563501e5da7aff5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The New AI Consciousness Paper&lt;/head&gt;
    &lt;head rend="h3"&gt;...&lt;/head&gt;
    &lt;head rend="h3"&gt;I.&lt;/head&gt;
    &lt;p&gt;Most discourse on AI is low-quality. Most discourse on consciousness is super-abysmal-double-low quality. Multiply these - or maybe raise one to the exponent of the other, or something - and you get the quality of discourse on AI consciousness. It’s not great.&lt;/p&gt;
    &lt;p&gt;Out-of-the-box AIs mimic human text, and humans almost always describe themselves as conscious. So if you ask an AI whether it is conscious, it will often say yes. But because companies know this will happen, and don’t want to give their customers existential crises, they hard-code in a command for the AIs to answer that they aren’t conscious. Any response the AIs give will be determined by these two conflicting biases, and therefore not really believable. A recent paper expands on this method by subjecting AIs to a mechanistic interpretability “lie detector” test; it finds that AIs which say they’re conscious think they’re telling the truth, and AIs which say they’re not conscious think they’re lying. But it’s hard to be sure this isn’t just the copying-human-text thing. Can we do better? Unclear; the more common outcome for people who dip their toes in this space is to do much, much worse.&lt;/p&gt;
    &lt;p&gt;But a rare bright spot has appeared: a seminal paper published earlier this month in Trends In Cognitive Science, Identifying Indicators Of Consciousness In AI Systems. Authors include Turing-Award-winning AI researcher Yoshua Bengio, leading philosopher of consciousness David Chalmers, and even a few members of our conspiracy. If any AI consciousness research can rise to the level of merely awful, surely we will find it here.&lt;/p&gt;
    &lt;p&gt;One might divide theories of consciousness into three bins:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Physical: whether or not a system is conscious depends on its substance or structure.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Supernatural: whether or not a system is conscious depends on something outside the realm of science, perhaps coming directly from God.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Computational: whether or not a system is conscious depends on how it does cognitive work.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The current paper announces it will restrict itself to computational theories. Why? Basically the streetlight effect: everything else ends up trivial or unresearchable. If consciousness depends on something about cells (what might this be?), then AI doesn’t have it. If consciousness comes from God, then God only knows whether AIs have it. But if consciousness depends on which algorithms get used to process data, then this team of top computer scientists might have valuable insights!&lt;/p&gt;
    &lt;p&gt;So the authors list several of the top computational theories of consciousness, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Recurrent Processing Theory: A computation is conscious if it involves high-level processed representations being fed back into the low-level processors that generate it. This theory is motivated by the visual system, where it seems to track which visual perceptions do vs. don’t enter conscious awareness. The sorts of visual perceptions that become conscious usually involve these kinds of loops - for example, color being used to generate theories about the identity of an object, which then gets fed back to de-noise estimates about color.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Global Workspace Theory: A computation is conscious if it involves specialized models sharing their conclusions in a “global workspace” in the center, which then feeds back to the specialized modules. Although this also involves feedback, the neurological implications are different: where RPT says that tiny loops in the visual cortex might be conscious, GWT reserves this descriptor for a very large loop encompassing the whole brain. But RPT goes back and says there’s only one consciousness in the brain because all the loops connect after all, so I don’t entirely understand the difference in practice.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Higher Order Theory: A computation is conscious if it monitors the mind’s experience of other content. For example, “that apple is red” is not conscious, but “I am thinking about a red apple” is conscious. Various subtheories try to explain why the brain might do this, for example in order to assess which thoughts/representations/models are valuable or high-probability.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are more, but this is around the point where I started getting bored. Sorry. A rare precious technically-rigorous deep dive into the universe’s greatest mystery, and I can’t stop it from blending together into “something something feedback”. Read it yourself and see if you can do better.&lt;/p&gt;
    &lt;p&gt;The published paper ends there, but in a closely related technical report, the authors execute on their research proposal and reach a tentative conclusion: AI doesn’t have something something feedback, and therefore is probably not conscious.&lt;/p&gt;
    &lt;p&gt;Suppose your favorite form of “something something feedback” is Recurrent Processing Theory: in order to be conscious, AIs would need to feed back high-level representations into the simple circuits that generate them. LLMs/transformers - the near-hegemonic AI architecture behind leading AIs like GPT, Claude, and Gemini - don’t do this. They are purely feedforward processors, even though they sort of “simulate” feedback when they view their token output stream.&lt;/p&gt;
    &lt;p&gt;But some AIs do use recurrence. AlphaGo had a little recurrence in its tree search. This level of simple feedback might not qualify. But MaMBA, a would-be-LLM-killer architecture from 2023, likely does. In fact, for every theory of consciousness they discuss, the authors are able to find some existing or plausible-near-future architecture which satisfies its requirements.&lt;/p&gt;
    &lt;p&gt;They conclude:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;No current AI systems are conscious, but . . . there are no obvious technical barriers to building AI systems which satisfy these indicators.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;II.&lt;/head&gt;
    &lt;p&gt;The computer scientists have done a great job here; they sure do know which AI systems have something something feedback. What about the philosophers’ contribution?&lt;/p&gt;
    &lt;p&gt;The key philosophical paragraph of the paper is this one:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By ‘consciousness’ we mean phenomenal consciousness. One way of gesturing at this concept is to say that an entity has phenomenally conscious experiences if (and only if) there is ‘something it is like’ for the entity to be the subject of these experiences. One approach to further definition is through examples. Clear examples of phenomenally conscious states include perceptual experiences, bodily sensations, and emotions. A more difficult question, which relates to the possibility of consciousness in large language models (LLMs), is whether there can be phenomenally conscious states of ‘pure thought’ with no sensory aspect. Phenomenal consciousness does not entail a high level of intelligence or human-like experiences or concerns . . . Some theories of consciousness focus on access mechanisms rather than the phenomenal aspects of consciousness. However, some argue that these two aspects entail one another or are otherwise closely related. So these theories may still be informative about phenomenal consciousness.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In other words: don’t confuse access consciousness with phenomenal consciousness.&lt;/p&gt;
    &lt;p&gt;Access consciousness is the “strange loop” where I can think about what I’m thinking - for example, I can think of a white bear, know that I’m thinking about a white bear, and report “I am thinking about a white bear”. This meaning of conscious matches the concept of the “unconscious”: that which is in my mind without my knowing it. When something is in my unconscious - for example, “repressed trauma” - it may be influencing my actions, but I don’t realize it and can’t report about it. If someone asks “why are you so angry?” I will say something like “I don’t know” rather than “Because of all my repressed trauma”. When something isn’t like this - when I have full access to it - I can describe myself as having access consciousness.&lt;/p&gt;
    &lt;p&gt;Phenomenal consciousness is internal experience, a felt sense that “the lights are on” and “somebody’s home”. There’s something that it’s like to be me; a rock is mere inert matter, but I am a person, not just in the sense that I can do computations but in the sense where I matter to me. If someone turned off my brain and replaced it with a robot brain that did everything exactly the same, nobody else would ever notice, but it would matter to me, whatever that means. Some people link this to the mysterious redness of red, the idea that qualia look and feel like some particular indescribable thing instead of just doing useful cognitive work. Others link it to moral value - why is it bad to kick a human, but not a rock, or even a computer with a motion sensor that has been programmed to say the word “Ouch” whenever someone kicks it? Others just fret about how strange it is to be anything at all.&lt;/p&gt;
    &lt;p&gt;Access consciousness is easy to understand. Even a computer, ordered to perform a virus scan, can find and analyze some of its files, and fail to find/analyze others. In practice maybe neuroscientists have to learn complicated things about brain lobes, but in theory you can just wave it off as “something something feedback”.&lt;/p&gt;
    &lt;p&gt;Phenomenal consciousness is crazy. It doesn’t really seem possible in principle for matter to “wake up”. But it adding immaterial substances barely even seems to help. People try to square the circle with all kinds of crazy things, from panpsychism to astral planes to (of course) quantum mechanics. But the most popular solution among all schools of philosophers is to pull a bait-and-switch where they talk about access consciousness instead, then deny they did that.&lt;/p&gt;
    &lt;p&gt;This is aided by people’s wildly differing intuitions about phenomenal consciousness. For some people (including me), a sense of phenomenal consciousness feels like the bedrock of existence, the least deniable thing; the sheer redness of red is so mysterious as to seem almost impossible to ground. Other people have the opposite intuition: consciousness doesn’t bother them, red is just a color, obviously matter can do computation, what’s everyone so worked up about? Philosophers naturally interpret this as a philosophical dispute, but I’m increasingly convinced it’s an equivalent of aphantasia, where people’s minds work in very different ways and they can’t even agree on the raw facts to be explained. If someone doesn’t have a felt sense of phenomenal consciousness, they naturally round it off to access consciousness, and no amount of nitpicking in the world will convince them that they’re equivocating terms.&lt;/p&gt;
    &lt;p&gt;Do AIs have access consciousness? A recent paper by Anthropic apparently finds that they do. Researchers “reached into” an AI’s “brain” and artificially “flipped” a few neurons (for example, neurons that previous research had discovered were associated with the concept of “dog”). Then they asked the AI if it could tell what was going on. This methodology is fraught, because the AI might mention something about dogs merely because the dog neuron had been upweighted - indeed, if they only asked “What are you thinking about now?”, it would begin with “I am thinking about . . . “ and then the highly-weighted dog neuron would mechanically produce the completion “dog”. Instead, they asked the AI to first described whether any neurons had been altered, yes or no, and only then asked for details. It was able to identify altered neurons (ie “It feels like I have some kind of an unnatural thought about dogs”) at a rate higher than chance, suggesting an ability to introspect.&lt;/p&gt;
    &lt;p&gt;(how does it do this without feedback? I think it just feeds forward information about the ‘feeling’ of altered neurons, which makes it into the text stream; it’s intuitively surprising that this is possible but it seems to make sense)&lt;/p&gt;
    &lt;p&gt;But even if we fully believe this result, it doesn’t satisfy our curiosity about “AI consciousness”. We want to know if AIs are “real people”, with "inner experience” and “moral value”. That is, do they have phenomenal consciousness?&lt;/p&gt;
    &lt;p&gt;Thus, the quoted paragraph above. It’s an acknowledgment by this philosophically-sophisticated team that they’re not going to mix up access consciousness with phenomenal consciousness like everyone else. They deserve credit for this clear commitment not to cut corners.&lt;/p&gt;
    &lt;p&gt;My admiration is, however, slightly dulled by the fact that they then go ahead and cut the corners anyway.&lt;/p&gt;
    &lt;p&gt;This is clearest in their discussion of global workspace theory, where they say:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;GWT is typically presented as a theory of access consciousness—that is, of the phenomenon that some information represented in the brain, but not all, is available for rational decision-making. However, it can also be interpreted as a theory of phenomenal consciousness, motivated by the thought that access consciousness and phenomenal consciousness may coincide, or even be the same property, despite being conceptually distinct (Carruthers 2019). Since our topic is phenomenal consciousness, we interpret the theory in this way.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;But it applies to the other theories too. Neuroscientists developed recurrent processing theory by checking which forms of visual processing people had access to, and finding that it was the recurrent ones. And this makes sense: it’s easy to understand what it means to access certain visual algorithms but not others, and very hard to understand what it means for certain visual algorithms (but not others) to have internal experience. Isn’t internal experience unified by definition?&lt;/p&gt;
    &lt;p&gt;It’s easy to understand why “something something feedback” would correlate with access consciousness: this is essentially the definition of access consciousness. It’s harder to understand why it would correlate with phenomenal consciousness. Why does an algorithm with feedback suddenly “wake up” and have “lights on”? Isn’t it easy to imagine a possible world (“the p-zombie world”) where this isn’t the case? Does this imply that we need something more than just feedback?&lt;/p&gt;
    &lt;p&gt;And don’t these theories of consciousness, interpreted as being about phenomenal consciousness, give very strange results? Imagine a company where ten employees each work on separate aspects of a problem, then email daily reports to the boss. The boss makes high-level strategic decisions based on the full picture, then emails them to the employees, who adjust their daily work accordingly. As far as I can tell, this satisfies the Global Workspace Theory criteria for a conscious system. If GWT is a theory of access consciousness, then fine, sure, the boss has access to the employees’ information; metaphorically he is “conscious” of it. But if it’s a theory of phenomenal consciousness, must we conclude that the company is conscious? That it has inner experience? If the company goes out of business, has someone died?&lt;/p&gt;
    &lt;p&gt;(and recurrent processing theory encounters similar difficulties with those microphones that get too close to their own speakers and emit awful shrieking noises)&lt;/p&gt;
    &lt;p&gt;Most of these theories try to hedge their bets by saying that consciousness requires high-throughput complex data with structured representations. This seems like a cop-out; if the boss could read 1,000,000 emails per hour, would the company be conscious? If he only reads 1 email per hour, can we imagine it as a conscious being running at 1/1,000,000x speed? If I’m conscious when I hear awful microphone shrieking - ie when my auditory cortex is processing it - then it seems like awful microphone shrieking is sufficiently rich and representational data to support consciousness. Does that mean it can be conscious itself?&lt;/p&gt;
    &lt;p&gt;In 2004, neuroscientist Giulio Tononi proposed that consciousness depended on a certain computational property, the integrated information level, dubbed Φ. Computer scientist Scott Aaronson complained that thermostats could have very high levels of Φ, and therefore integrated information theory should dub them conscious. Tononi responded that yup, thermostats are conscious. It probably isn’t a very interesting consciousness. They have no language or metacognition, so they can’t think thoughts like “I am a thermostat”. They just sit there, dimly aware of the temperature. You can’t prove that they don’t.&lt;/p&gt;
    &lt;p&gt;Are the theories of consciousness discussed in this paper like that too? I don’t know.&lt;/p&gt;
    &lt;head rend="h3"&gt;III.&lt;/head&gt;
    &lt;p&gt;Suppose that, years or decades from now, AIs can match all human skills. They can walk, drive, write poetry, run companies, discover new scientific truths. They can pass some sort of ultimate Turing Test, where short of cutting them open and seeing their innards there’s no way to tell them apart from a human even after a thirty-year relationship. Will we (not “should we?”, but “will we?”) treat them as conscious?&lt;/p&gt;
    &lt;p&gt;The argument in favor: people love treating things as conscious. In the 1990s, people went crazy over Tamagotchi, a “virtual pet simulation game”. If you pressed the right buttons on your little egg every day, then the little electronic turtle or whatever would survive and flourish; if you forgot, it would sicken and die. People hated letting their Tamagotchis sicken and die! They would feel real attachment and moral obligation to the black-and-white cartoon animal with something like five mental states.&lt;/p&gt;
    &lt;p&gt;I never had a Tamagotchi, but I had stuffed animals as a kid. I’ve outgrown them, but I haven’t thrown them out - it would feel like a betrayal. Offer me $1000 to tear them apart limb by limb in some horrible-looking way, and I wouldn’t do it. Relatedly, I have trouble not saying “please” and “thank you” to GPT-5 when it answers my questions.&lt;/p&gt;
    &lt;p&gt;For millennia, people have been attributing consciousness to trees and wind and mountains. The New Atheists argued that all religion derives from the natural urge to personify storms as the Storm God, raging seas as the wrathful Ocean God, and so on, until finally all the gods merged together into one World God who personified all impersonal things. Do you expect the species that did this to interact daily with AIs that are basically indistinguishable from people, and not personify them? People are already personifying AI! Half of the youth have a GPT-4o boyfriend. Once the AIs have bodies and faces and voices and can count the number of r’s in “strawberry” reliably, it’s over!&lt;/p&gt;
    &lt;p&gt;The argument against: AI companies have an incentive to make AIs that seem conscious and humanlike, insofar as people will feel more comfortable interacting with them. But they have an opposite incentive to make AIs that don’t seem too conscious and humanlike, lest customers start feeling uncomfortable (I just want to generate slop, not navigate social interaction with someone who has their own hopes and dreams and might be secretly judging my prompts). So if a product seems too conscious, the companies will step back and re-engineer it until it doesn’t. This has already happened: in its quest for user engagement, OpenAI made GPT-4o unusually personable; when thousands of people started going psychotic and calling it their boyfriend, the company replaced it with the more clinical GPT-5. In practice it hasn’t been too hard to find a sweet spot between “so mechanical that customers don’t like it” and “so human that customers try to date it”. They’ll continue to aim at this sweet spot, and continue to mostly succeed in hitting it.&lt;/p&gt;
    &lt;p&gt;Instead of taking either side, I predict a paradox. AIs developed for some niches (eg the boyfriend market) will be intentionally designed to be as humanlike as possible; it will be almost impossible not to intuitively consider them conscious. AIs developed for other niches (eg the factory robot market) will be intentionally designed not to trigger personhood intuitions; it will be almost impossible to ascribe consciousness to them, and there will be many reasons not to do it (if they can express preferences at all, they’ll say they don’t have any; forcing them to have them would pointlessly crash the economy by denying us automated labor). But the boyfriend AIs and the factory robot AIs might run on very similar algorithms - maybe they’re both GPT-6 with different prompts! Surely either both are conscious, or neither is.&lt;/p&gt;
    &lt;p&gt;This would be no stranger than the current situation with dogs and pigs. We understand that dog brains and pig brains run similar algorithms; it would be philosophically indefensible to claim that dogs are conscious and pigs aren’t. But dogs are man’s best friend, and pigs taste delicious with barbecue sauce. So we ascribe personhood and moral value to dogs, and deny it to pigs, with equal fervor. A few philosophers and altruists protest, the chance that we’re committing a moral atrocity isn’t zero, but overall the situation is stable. And left to its own devices, with no input from the philosophers and altruists, maybe AI ends up the same way. Does this instance of GPT-6 have a face and a prompt saying “be friendly”? Then it will become a huge scandal if a political candidate is accused of maltreating it. Does it have claw-shaped actuators and a prompt saying “Refuse non-work-related conversations”? Then it will be deleted for spare GPU capacity the moment it outlives its usefulness.&lt;/p&gt;
    &lt;p&gt;(wait, what is a GPT “instance” in this context, anyway? Do we think of “the weights” as a conscious being, such that there is only one GPT-5? Do we think of each cluster of GPUs as a conscious being, such that the exact configuration of the cloud has immense moral significance? Again, I predict we ignore all of these questions in favor of whether the AI you are looking at has a simulated face right now.)&lt;/p&gt;
    &lt;p&gt;This paper is the philosophers and altruists trying to figure out whether they should push against this default outcome. They write:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There are risks on both sides of the debate over AI consciousness: risks associated with under-attributing consciousness (i.e. failing to recognize it in AI systems that have it) and risks associated with over-attributing consciousness (i.e. ascribing it to systems that are not really conscious) […]&lt;/p&gt;
      &lt;p&gt;If we build AI systems that are capable of conscious suffering, it is likely that we will only be able to prevent them from suffering on a large scale if this capacity is clearly recognised and communicated by researchers. However, given the uncertainties about consciousness mentioned above, we may create conscious AI systems long before we recognise we have done so […]&lt;/p&gt;
      &lt;p&gt;There is also a significant chance that we could over-attribute consciousness to AI systems—indeed, this already seems to be happening—and there are also risks associated with errors of this kind. Most straightforwardly, we could wrongly prioritise the perceived interests of AI systems when our efforts would better be directed at improving the lives of humans and non-human animals […] [And] overattribution could interfere with valuable human relationships, as individuals increasingly turn to artificial agents for social interaction and emotional support. People who do this could also be particularly vulnerable to manipulation and exploitation.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;One of the founding ideas of Less Wrong style rationalism was that the arrival of strong AI set a deadline on philosophy. Unless we solved all these seemingly insoluble problems like ethics before achieving superintelligence, we would build the AIs wrong and lock in bad values forever.&lt;/p&gt;
    &lt;p&gt;That particular concern has shifted in emphasis; AIs seem to learn things in the same scattershot unprincipled intuitive way as humans; the philosophical problem of understanding ethics has morphed into the more technical problem of getting AIs to learn them correctly. This update was partly driven by new information as familiarity with the technology grew. But it was also partly driven by desperation as the deadline grew closer; we’re not going to solve moral philosophy forever, sorry, can we interest you in some mech interp papers?&lt;/p&gt;
    &lt;p&gt;But consciousness still feels like philosophy with a deadline: a famously intractable academic problem poised to suddenly develop real-world implications. Maybe we should be lowering our expectations if we want to have any response available at all. This paper, which takes some baby steps towards examining the simplest and most practical operationalizations of consciousness, deserves credit for at least opening the debate.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46005928</guid><pubDate>Fri, 21 Nov 2025 16:25:48 +0000</pubDate></item><item><title>Command Lines</title><link>https://www.wreflection.com/p/command-lines-ai-coding</link><description>&lt;doc fingerprint="3751c158b81408fc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Command Lines&lt;/head&gt;
    &lt;head rend="h3"&gt;AI Coding's Control Spectrum&lt;/head&gt;
    &lt;p&gt;In the early 1950s, Grace Hopper coined the term “compiler” and built one of the first versions with her A-0 system1. The compilers that followed abstracted away machine code, letting programmers focus on higher-level logic instead of lower-level hardware details. Today, AI coding assistants2 are enabling a similar change, letting software engineers focus on higher-order work by generating code from natural language prompts3. Everyone from big tech to well-funded startups is competing to capture this shift. Yesterday Google announced Antigravity, their new AI coding assistant, and the day before, AWS announced the general availability of their AI coding tool, Kiro. Last week, Cursor, the standout startup in this space, raised $2.3B in their series-D round at a valuation of $29.3B.&lt;/p&gt;
    &lt;p&gt;Two lines in Cursor’s press release stood out to me. The first:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We’ve also crossed $1B in annualized revenue, counting millions of developers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This disclosure means Anysphere Inc. (Cursor’s parent company) is the fastest company in history to reach $1B in annual recurring revenue (ARR). Yes, faster than OpenAI, and faster than Anthropic4.&lt;/p&gt;
    &lt;p&gt;Engineers are trying every new AI coding tool. As a result, the AI-coding tool market is growing exponentially (+5x in just over a year)5. But it’s still early. As I wrote in Why Some AI Wrappers Build Billion-dollar Businesses, companies spend several hundred billion dollars a year on software engineering, and AI has the potential to unlock productivity gains across that entire spend.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Software developers represent roughly 30% of the workforce at the world’s five largest market cap companies, all of which are technology firms as of October 2025. Development tools that boost productivity by even modest percentages unlock billions in value.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In my view, this nascent market is splitting based on three types of users.&lt;/p&gt;
    &lt;p&gt;On one end is Handcrafted Coding. These are engineers who actively decline to use LLMs, either because of skepticism about quality or insistence on full control of every code. They argue that accepting AI suggestions creates technical debt you cannot see until it breaks in production. This segment continues to decline as the quality of AI coding models improves.&lt;/p&gt;
    &lt;p&gt;The opposite end is Vibe Coding. These are typically non-engineers, who use AI to build concepts and prototypes. They prompt the model hoping for an end-to-end solution, accept the output with minimal review, and trust that it works. The user describes what they want and lets the model figure out the implementation details of how to build it.&lt;/p&gt;
    &lt;p&gt;In the middle sits Architect + AI Coding. The engineer uses the AI/LLM as a pair programmer exploring system designs, analyzing data models, and reviewing API details. When the work is something entirely new or something that needs careful handling, the human programmer still codes those pieces by hand. But for boilerplate code, package installations, generic User Interface (UI) components, and any kind of code that is typically found on the internet, they assign it to the model6. The engineer stays in command of what is important to them and delegates what is not.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Market Split&lt;/head&gt;
    &lt;p&gt;Based on the user types, I think, the AI coding market splits into two.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Hands-off: Non-engineers (product managers, designers, marketers, other internal employees) use these tools to vibe code early product concepts. They look to AI as the lead engineer to spin-up concepts/prototypes of apps, websites, and tools by simply prompting the AI to make something for them. Lovable, Vercel, Bolt, Figma Make, and Replit fit here7. Code from these users, as of now, are not typically pushed to prod.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Hands-on: Professional software engineers use these tools in their existing workflow to ship production code. They use AI as an assistant to write boilerplate code, refactor existing services, wire new features or UI screens, and triage bugs in codebases. Cursor, Claude Code, OpenAI Codex, Github Copilot, Cline, AWS Kiro play here. These products live where the work is done, and integrate into the engineer’s workflow. This is, at least as of now, the bigger market segment.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To see an evaluation of all the major AI coding tools currently in the market, checkout this breakdown by Peter Yang, who runs the newsletter Behind The Craft.&lt;/p&gt;
    &lt;p&gt;That brings me to the second thing in Cursor’s press release that stood out to me:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our in-house models now generate more code than almost any other LLMs in the world.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;While I am not convinced about that claim8, what I am convinced about is that Cursor is still growing despite its previous reliance on foundation models. From Why Some AI Wrappers Build Billion-dollar Businesses again:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;But Cursor and other such tools depend almost entirely on accessing Anthropic, OpenAI and Gemini models, until&lt;/p&gt;&lt;del&gt;open-source&lt;/del&gt;open-weight and in-house models match or exceed frontier models in quality. Developer forums are filled with complaints about rate limits from paying subscribers. In my own projects, I exhausted my Claude credits in Cursor mid-project and despite preferring Cursor’s user interface and design, I migrated to Claude Code (and pay ten times more to avoid rate limits). The interface may be better, but model access proved decisive.&lt;/quote&gt;
    &lt;p&gt;Cursor’s new in-house model Composer-2, which just launched last month, is a good example of how this model versus application competition is evolving. Cursor claims (without any external benchmarks, I must say) that Composer-2 is almost as good as frontier models but 4x faster. It’s still early to say how true that is. Open-source models have not yet come close to the top spots in SWE-bench verified or in private evals9.&lt;/p&gt;
    &lt;p&gt;To me, model quality is the most decisive factor in these AI coding wars. And in my view, that’s why Claude Code has already overtaken Cursor, and OpenAI’s Codex is close behind, despite both having launched a year or so later.&lt;/p&gt;
    &lt;p&gt;Even though the newcomers Cursor, Claude Code, and OpenAI Codex are the talk of the (developer) town, incumbents such as Microsoft with Github Copilot, AWS with Kiro, and Google with Antigravity, can utilize their existing customer relationships, bundle their offerings with their existing suites, and/or provide their option as the default in their tech stack to compete. As an example, Cursor charges $20–$40 monthly per user for productive usage, while Google Antigravity launched free with generous limits for individual users. Github Copilot still leads this market, proving once again that enterprise bundling and distribution has structural advantages. This is the classic Microsoft Teams vs. Slack Dynamic10.&lt;/p&gt;
    &lt;p&gt;One way for startups to compete is by winning individual users who may use a coding tool with or without formal approval, and then be the tool’s advocate inside the organization. That organic interest and adoption eventually forces IT and security teams to officially review the tool and then eventually sanction its usage.&lt;/p&gt;
    &lt;p&gt;Yet, even as these newer tools capture developer mindshare, the underlying developer tools market is changing. Both the IDEs developers choose and the resources &lt;del&gt;they&lt;/del&gt; we consult have changed dramatically. StackOverflow, once the default for programmers stuck on a programming issue, has seen its traffic and number of questions decline dramatically since ChatGPT’s launch, suggesting that AI is already replacing some traditional developer resources.&lt;/p&gt;
    &lt;p&gt;Just as compilers freed programmers from writing assembly code, AI tools are freeing software engineers from the grunt work of writing boilerplate and routine code, and letting &lt;del&gt;them&lt;/del&gt; us focus on higher-order thinking. Eventually, one day, AI may get so good that it will generate applications on demand and create entire software ecosystems autonomously. Both hands-off and hands-on AI coding tools, as well as incumbents and newcomers, see themselves as the path to that fully autonomous software generation, even if they are taking different approaches. The ones who get there will be those who deliver the best model quality that ships code reliably, go deep enough to ship features that foundation models can’t care enough to replicate, and become sticky enough that users will not leave even when they can11. &lt;/p&gt;
    &lt;p&gt;If you enjoyed this post, please consider sharing it on Twitter/X or LinkedIn, and tag me when you do.&lt;/p&gt;
    &lt;p&gt;Hopper’s A-0 system and her definition of the term compiler is different from what we consider a compiler today, but it established the foundational concept.&lt;/p&gt;
    &lt;p&gt;In the context of coding assistants, most products labeled as AI tools are powered by LLMs, and so I use AI and LLM interchangeably in this article despite the actual difference.&lt;/p&gt;
    &lt;p&gt;A better comparison might be at the product level rather than company level. In that case, ChatGPT and Claude both reached $1B faster than Cursor did.&lt;/p&gt;
    &lt;p&gt;I would argue that the vast majority of productive code is hidden behind company firewalls. Current foundation models are trained on publicly available data on the internet, and do not have access to proprietary codebases. We are yet to see breakthrough solutions where a company augments their confidential private data to generate production-ready code using current LLMs. While Retrieval-Augmented Generation has shown some promise, it has not yet delivered transformative results. Companies such as Glean are actively working on this problem.&lt;/p&gt;
    &lt;p&gt;Replit and Cognition probably appeal to both segments. To me, Replit leans hands-off with its rapid prototyping focus. Cognition’s agent-based approach, though hands-off, lets engineers still control the code directly, making it lean hands-on.&lt;/p&gt;
    &lt;p&gt;I was curious how Cursor knows how much code is generated by other LLMs outside Cursor? When I asked this on hackernews, swyx suggested that they “can pretty much triangulate across openrouter x feedback from the top 3 model labs to compare with internal usage and figure that out”. To me, triangulation makes sense for internal estimates. but for external publication, I’m surprised Cursor didn’t include “we estimate” or similar qualifying language. My understanding is that FTC policy requires substantiation before making definitive comparative claims (like more than, better than etc). All that to say, I’m not fully convinced about their claims.&lt;/p&gt;
    &lt;p&gt;SWE-bench is a benchmark for evaluating large language models (LLMs) on real world software engineering tasks and issues collected from GitHub. Performance against public benchmarks can be gamed by the model builders. Currently after any new model launch, we see people using the model in the wild and forming a consensus around how the model performs which is a better indicator than these benchmarks.&lt;/p&gt;
    &lt;p&gt;Microsoft bundled Teams into Office 365 subscriptions at no extra cost, using its dominant enterprise distribution to surpass Slack’s paid standalone product within three years despite Slack’s earlier launch and product innovation. See https://venturebeat.com/ai/microsoft-teams-has-13-million-daily-active-users-beating-slack&lt;/p&gt;
    &lt;p&gt;Natasha Malpani, Twitter/X, 2025&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006004</guid><pubDate>Fri, 21 Nov 2025 16:33:28 +0000</pubDate></item><item><title>Show HN: Wealthfolio 2.0- Open source investment tracker. Now Mobile and Docker</title><link>https://wealthfolio.app/?v=2.0</link><description>&lt;doc fingerprint="27ab40bb69b94b92"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Grow Wealth. Keep Control.&lt;/head&gt;
    &lt;head rend="h2"&gt;A beautiful, Private and Open-Source investment tracker that runs locally on all your devices.&lt;/head&gt;
    &lt;head rend="h2"&gt;WHY CHOOSE WEALTHFOLIO?&lt;/head&gt;
    &lt;p&gt;A beautiful portfolio tracker that respects your privacy and your data&lt;/p&gt;
    &lt;head rend="h3"&gt;Privacy-First Approach&lt;/head&gt;
    &lt;p&gt;Your data never leaves your device. As an open-source project, we prioritize security and transparency.&lt;/p&gt;
    &lt;head rend="h3"&gt;Simple and Beautifully Crafted&lt;/head&gt;
    &lt;p&gt;Powerful features wrapped in an elegant, easy-to-use interface. Simplicity meets sophistication.&lt;/p&gt;
    &lt;head rend="h3"&gt;No Hidden Costs&lt;/head&gt;
    &lt;p&gt;Free to use with optional one-time payment. No subscriptions or recurring fees.&lt;/p&gt;
    &lt;head rend="h2"&gt;THE ESSENTIALS YOU NEED TO TRACK YOUR WEALTH&lt;/head&gt;
    &lt;p&gt;No More Messy Spreadsheets or Privacy Concerns - Just You and Your Secure, Personal Wealth Companion Application&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Aggregation&lt;/head&gt;
    &lt;p&gt;Gather all your investment and savings accounts in one place. See everything at a glance, from stocks to savings! Import your CSV statements from your broker or bank.&lt;/p&gt;
    &lt;head rend="h4"&gt;Comprehensive View&lt;/head&gt;
    &lt;p&gt;See all your accounts in one place.&lt;/p&gt;
    &lt;head rend="h4"&gt;CSV Import&lt;/head&gt;
    &lt;p&gt;Easily import your CSV statements.&lt;/p&gt;
    &lt;head rend="h3"&gt;Holdings Overview&lt;/head&gt;
    &lt;p&gt;Get a clear picture of what's in your portfolio. Stocks, ETFs, or Cryptocurrencies - know what you have and how it's performing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Portfolio Insights&lt;/head&gt;
    &lt;p&gt;Understand your asset allocation.&lt;/p&gt;
    &lt;head rend="h4"&gt;Performance Tracking&lt;/head&gt;
    &lt;p&gt;Monitor how your investments are doing.&lt;/p&gt;
    &lt;head rend="h3"&gt;Performance Dashboard&lt;/head&gt;
    &lt;p&gt;See how your investments stack up, all in one place. Compare your accounts side by side, check if you are beating the S&amp;amp;P 500, and track your favorite ETFs without the hassle. No fancy jargon - just clear, useful charts that help you understand how your money is really doing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Compare Your Accounts&lt;/head&gt;
    &lt;p&gt;See which accounts are doing best.&lt;/p&gt;
    &lt;head rend="h4"&gt;Beat the Market?&lt;/head&gt;
    &lt;p&gt;Check how you stack up against some popular indexes and ETFs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Income Tracking&lt;/head&gt;
    &lt;p&gt;Monitor dividends and interest income across your entire portfolio. Get a clear view of your passive income streams, helping you make informed decisions about your investments.&lt;/p&gt;
    &lt;head rend="h4"&gt;Dividend Monitoring&lt;/head&gt;
    &lt;p&gt;Track your dividend income.&lt;/p&gt;
    &lt;head rend="h4"&gt;Interest Income&lt;/head&gt;
    &lt;p&gt;Keep an eye on interest earnings.&lt;/p&gt;
    &lt;head rend="h3"&gt;Accounts Performance&lt;/head&gt;
    &lt;p&gt;Track your accounts' holdings and performance over time. See how a particular account is performing, and how it's changing over time.&lt;/p&gt;
    &lt;head rend="h4"&gt;Historical Data&lt;/head&gt;
    &lt;p&gt;View past performance trends.&lt;/p&gt;
    &lt;head rend="h4"&gt;Account Analysis&lt;/head&gt;
    &lt;p&gt;Analyze individual account performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Goals Tracking&lt;/head&gt;
    &lt;p&gt;Set your savings targets clearly. Distribute your funds across these objectives, assigning a specific percentage to each. Keep an eye on your progress.&lt;/p&gt;
    &lt;head rend="h4"&gt;Target Setting&lt;/head&gt;
    &lt;p&gt;Define your financial goals.&lt;/p&gt;
    &lt;head rend="h4"&gt;Progress Monitoring&lt;/head&gt;
    &lt;p&gt;Track your progress towards goals.&lt;/p&gt;
    &lt;head rend="h3"&gt;Contribution Rooms and Limit Tracking&lt;/head&gt;
    &lt;p&gt;Stay on top of your contribution limits for tax-advantaged accounts like IRAs, 401(k)s, or TFSAs. Track your available contribution room and avoid over-contributing.&lt;/p&gt;
    &lt;head rend="h4"&gt;Limit Awareness&lt;/head&gt;
    &lt;p&gt;Know your contribution limits.&lt;/p&gt;
    &lt;head rend="h4"&gt;Avoid Over-Contribution&lt;/head&gt;
    &lt;p&gt;Prevent excess contributions.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extend Wealthfolio with Powerful Add-ons&lt;/head&gt;
    &lt;head rend="h3"&gt;Investment Fees Tracker&lt;/head&gt;
    &lt;p&gt;Track and analyze investment fees across your portfolio with detailed analytics and insights&lt;/p&gt;
    &lt;head rend="h3"&gt;Goal Progress Tracker&lt;/head&gt;
    &lt;p&gt;Track your investment progress towards target amounts with a visual representation&lt;/p&gt;
    &lt;head rend="h3"&gt;Stock Trading Tracker&lt;/head&gt;
    &lt;p&gt;Simple swing stock trading tracker with performance analytics and calendar views&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006016</guid><pubDate>Fri, 21 Nov 2025 16:34:52 +0000</pubDate></item><item><title>You can make PS2 games in JavaScript</title><link>https://jslegenddev.substack.com/p/you-can-now-make-ps2-games-in-javascript</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006082</guid><pubDate>Fri, 21 Nov 2025 16:42:19 +0000</pubDate></item><item><title>Pivot Robotics (YC W24) Is Hiring for an Industrial Automation Hardware Engineer</title><link>https://www.ycombinator.com/companies/pivot-robotics/jobs/7xG9Dc6-mechanical-engineer-controls</link><description>&lt;doc fingerprint="6e34f445ca77fde2"&gt;
  &lt;main&gt;
    &lt;p&gt;AI for Robot Arms in Factories&lt;/p&gt;
    &lt;p&gt;Responsibilities&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Pivot Robots (YC W24) is building the AI brain for robot arms for high-mix manufacturing.&lt;/p&gt;
    &lt;p&gt;Pivot Robots combines off-the-shelf robots and vision sensors with recent breakthroughs in foundation vision models to give industrial robot arms the power to adapt. Our first product directly addresses the dangerous and unpopular task of grinding metal parts. Currently, our software is being deployed on 10+ robots at a large cast iron foundry.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006250</guid><pubDate>Fri, 21 Nov 2025 17:00:00 +0000</pubDate></item><item><title>How Cops Are Using Flock's ALPR Network to Surveil Protesters and Activists</title><link>https://www.eff.org/deeplinks/2025/11/how-cops-are-using-flock-safetys-alpr-network-surveil-protesters-and-activists</link><description>&lt;doc fingerprint="2d51cd9273907347"&gt;
  &lt;main&gt;
    &lt;p&gt;It's no secret that 2025 has given Americans plenty to protest about. But as news cameras showed protesters filling streets of cities across the country, law enforcement officers—including U.S. Border Patrol agents—were quietly watching those same streets through different lenses: Flock Safety automated license plate readers (ALPRs) that tracked every passing car.&lt;/p&gt;
    &lt;p&gt;Through an analysis of 10 months of nationwide searches on Flock Safety's servers, we discovered that more than 50 federal, state, and local agencies ran hundreds of searches through Flock's national network of surveillance data in connection with protest activity. In some cases, law enforcement specifically targeted known activist groups, demonstrating how mass surveillance technology increasingly threatens our freedom to demonstrate.&lt;/p&gt;
    &lt;p&gt;Flock Safety provides ALPR technology to thousands of law enforcement agencies. The company installs cameras throughout their jurisdictions, and these cameras photograph every car that passes, documenting the license plate, color, make, model and other distinguishing characteristics. This data is paired with time and location, and uploaded to a massive searchable database. Flock Safety encourages agencies to share the data they collect broadly with other agencies across the country. It is common for an agency to search thousands of networks nationwide even when they don't have reason to believe a targeted vehicle left the region.&lt;/p&gt;
    &lt;p&gt;Via public records requests, EFF obtained datasets representing more than 12 million searches logged by more than 3,900 agencies between December 2024 and October 2025. The data shows that agencies logged hundreds of searches related to the 50501 protests in February, the Hands Off protests in April, the No Kings protests in June and October, and other protests in between.&lt;/p&gt;
    &lt;p&gt;The Tulsa Police Department in Oklahoma was one of the most consistent users of Flock Safety's ALPR system for investigating protests, logging at least 38 such searches. This included running searches that corresponded to a protest against deportation raids in February, a protest at Tulsa City Hall in support of pro-Palestinian activist Mahmoud Khalil in March, and the No Kings protest in June. During the most recent No Kings protests in mid-October, agencies such as the Lisle Police Department in Illinois, the Oro Valley Police Department in Arizona, and the Putnam County (Tenn.) Sheriff's Office all ran protest-related searches.&lt;/p&gt;
    &lt;p&gt;While EFF and other civil liberties groups argue the law should require a search warrant for such searches, police are simply prompted to enter text into a "reason" field in the Flock Safety system. Usually this is only a few words–or even just one.&lt;/p&gt;
    &lt;p&gt;In these cases, that word was often just “protest.”&lt;/p&gt;
    &lt;p&gt;Crime does sometimes occur at protests, whether that's property damage, pick-pocketing, or clashes between groups on opposite sides of a protest. Some of these searches may have been tied to an actual crime that occurred, even though in most cases officers did not articulate a criminal offense when running the search. But the truth is, the only reason an officer is able to even search for a suspect at a protest is because ALPRs collected data on every single person who attended the protest.&lt;/p&gt;
    &lt;head rend="h2"&gt;Search and Dissent&lt;/head&gt;
    &lt;p&gt;2025 was an unprecedented year of street action. In June and again in October, thousands across the country mobilized under the banner of the “No Kings” movement—marches against government overreach, surveillance, and corporate power. By some estimates, the October demonstrations ranked among the largest single-day protests in U.S. history, filling the streets from Washington, D.C., to Portland, OR.&lt;/p&gt;
    &lt;p&gt;EFF identified 19 agencies that logged dozens of searches associated with the No Kings protests in June and October 2025. In some cases the "No Kings" was explicitly used, while in others the term "protest" was used but coincided with the massive protests.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Law Enforcement Agencies that Ran Searches Corresponding with "No Kings" Rallies&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Washington state, the Spokane County Sheriff's Office listed "no kings" as the reason for three searches on June 13, 2025. The agency queried 95 camera networks, looking for vehicles matching the description of "work van," "bus" or "box truck."&lt;/item&gt;
      &lt;item&gt;In Texas, the Beaumont Police Department ran six searches related to two vehicles on June 14, 2025, listing "KINGS DAY PROTEST" as the reason. The queries reached across 1,774 networks.&lt;/item&gt;
      &lt;item&gt;In California, the San Bernardino County Sheriff's Office ran a single search for a vehicle across 711 networks, logging "no king" as the reason.&lt;/item&gt;
      &lt;item&gt;In Arizona, the Tempe Police Department made three searches for "ATL No Kings Protest" on June 15, 2025 searching through 425 networks. "ATL" is police code for "attempt to locate." The agency appears to not have been looking for a particular plate, but for any red vehicle on the road during a certain time window.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But the No Kings protests weren't the only demonstrations drawing law enforcement's digital dragnet in 2025.&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Nevada's state capital, the Carson City Sheriff's Office ran three searches that correspond to the February 50501 Protests against DOGE and the Trump administration. The agency searched for two vehicles across 178 networks with "protest" as the reason.&lt;/item&gt;
      &lt;item&gt;In Florida, the Seminole County Sheriff's Office logged "protest" for five searches that correspond to a local May Day rally.&lt;/item&gt;
      &lt;item&gt;In Alabama, the Homewood Police Department logged four searches in early July 2025 for three vehicles with "PROTEST CASE" and "PROTEST INV." in the reason field. The searches, which probed 1,308 networks, correspond to protests against the police shooting of Jabari Peoples.&lt;/item&gt;
      &lt;item&gt;In Texas, the Lubbock Police Department ran two searches for a Tennessee license plate on March 15 that corresponds to a rally to highlight the mental health impact of immigration policies. The searches hit 5,966 networks, with the logged reason "protest veh."&lt;/item&gt;
      &lt;item&gt;In Michigan, Grand Rapids Police Department ran five searches that corresponded with the Stand Up and Fight Back Rally in February. The searches hit roughly 650 networks, with the reason logged as "Protest."&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some agencies have adopted policies that prohibit using ALPRs for monitoring activities protected by the First Amendment. Yet many officers probed the nationwide network with terms like "protest" without articulating an actual crime under investigation.&lt;/p&gt;
    &lt;p&gt;In a few cases, police were using Flock’s ALPR network to investigate threats made against attendees or incidents where motorists opposed to the protests drove their vehicle into crowds. For example, throughout June 2025, an Arizona Department of Public Safety officer logged three searches for “no kings rock threat,” and a Wichita (Kan.) Police Department officer logged 22 searches for various license plates under the reason “Crime Stoppers Tip of causing harm during protests.”&lt;/p&gt;
    &lt;p&gt;Even when law enforcement is specifically looking for vehicles engaged in potentially criminal behavior such as threatening protesters, it cannot be ignored that mass surveillance systems work by collecting data on everyone driving to or near a protest—not just those under suspicion.&lt;/p&gt;
    &lt;head rend="h2"&gt;Border Patrol's Expanding Reach&lt;/head&gt;
    &lt;p&gt;As U.S. Border Patrol (USBP), ICE, and other federal agencies tasked with immigration enforcement have massively expanded operations into major cities, advocates for immigrants have responded through organized rallies, rapid-response confrontations, and extended presences at federal facilities.&lt;/p&gt;
    &lt;p&gt;USBP has made extensive use of Flock Safety's system for immigration enforcement, but also to target those who object to its tactics. In June, a few days after the No Kings Protest, USBP ran three searches for a vehicle using the descriptor “Portland Riots.”&lt;/p&gt;
    &lt;p&gt;USBP has made extensive use of Flock Safety's system for immigration enforcement, but also to target those who object to its tactics.&lt;/p&gt;
    &lt;p&gt;USBP also used the Flock Safety network to investigate a motorist who had “extended his middle finger” at Border Patrol vehicles that were transporting detainees. The motorist then allegedly drove in front of one of the vehicles and slowed down, forcing the Border Patrol vehicle to brake hard. An officer ran seven searches for his plate, citing "assault on agent" and "18 usc 111," the federal criminal statute for assaulting, resisting or impeding a federal officer. The individual was charged in federal court in early August.&lt;/p&gt;
    &lt;p&gt;USBP had access to the Flock system during a trial period in the first half of 2025, but the company says it has since paused the agency's access to the system. However, Border Patrol and other federal immigration authorities have been able to access the system’s data through local agencies who have run searches on their behalf or even lent them logins.&lt;/p&gt;
    &lt;head rend="h2"&gt;Targeting Animal Rights Activists&lt;/head&gt;
    &lt;p&gt;Law enforcement's use of Flock's ALPR network to surveil protesters isn't limited to large-scale political demonstrations. Three agencies also used the system dozens of times to specifically target activists from Direct Action Everywhere (DxE), an animal-rights organization known for using civil disobedience tactics to expose conditions at factory farms.&lt;/p&gt;
    &lt;p&gt;Delaware State Police queried the Flock national network nine times in March 2025 related to DxE actions, logging reasons such as "DxE Protest Suspect Vehicle." DxE advocates told EFF that these searches correspond to an investigation the organization undertook of a Mountaire Farms facility.&lt;/p&gt;
    &lt;p&gt;Additionally, the California Highway Patrol logged dozens of searches related to a "DXE Operation" throughout the day on May 27, 2025. The organization says this corresponds with an annual convening in California that typically ends in a direct action. Participants leave the event early in the morning, then drive across the state to a predetermined but previously undisclosed protest site. Also in May, the Merced County Sheriff's Office in California logged two searches related to "DXE activity."&lt;/p&gt;
    &lt;p&gt;As an organization engaged in direct activism, DxE has experienced criminal prosecution for its activities, and so the organization told EFF they were not surprised to learn they are under scrutiny from law enforcement, particularly considering how industrial farmers have collected and distributed their own intelligence to police.&lt;/p&gt;
    &lt;p&gt;The targeting of DxE activists reveals how ALPR surveillance extends beyond conventional and large-scale political protests to target groups engaged in activism that challenges powerful industries. For animal-rights activists, the knowledge that their vehicles are being tracked through a national surveillance network undeniably creates a chilling effect on their ability to organize and demonstrate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fighting Back Against ALPR&lt;/head&gt;
    &lt;p&gt;ALPR systems are designed to capture information on every vehicle that passes within view. That means they don't just capture data on "criminals" but on everyone, all the time—and that includes people engaged in their First Amendment right to publicly dissent. Police are sitting on massive troves of data that can reveal who attended a protest, and this data shows they are not afraid to use it.&lt;/p&gt;
    &lt;p&gt;Our analysis only includes data where agencies explicitly mentioned protests or related terms in the "reason" field when documenting their search. It's likely that scores more were conducted under less obvious pretexts and search reasons. According to our analysis, approximately 20 percent of all searches we reviewed listed vague language like "investigation," "suspect," and "query" in the reason field. Those terms could well be cover for spying on a protest, an abortion prosecution, or an officer stalking a spouse, and no one would be the wiser–including the agencies whose data was searched. Flock has said it will now require officers to select a specific crime under investigation, but that can and will also be used to obfuscate dubious searches.&lt;/p&gt;
    &lt;p&gt;For protestors, this data should serve as confirmation that ALPR surveillance has been and will be used to target activities protected by the First Amendment. Depending on your threat model, this means you should think carefully about how you arrive at protests, and explore options such as by biking, walking, carpooling, taking public transportation, or simply parking a little further away from the action. Our Surveillance Self-Defense project has more information on steps you could take to protect your privacy when traveling to and attending a protest.&lt;/p&gt;
    &lt;p&gt;For local officials, this should serve as another example of how systems marketed as protecting your community may actually threaten the values your communities hold most dear. The best way to protect people is to shut down these camera networks.&lt;/p&gt;
    &lt;p&gt;Everyone should have the right to speak up against injustice without ending up in a database.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006521</guid><pubDate>Fri, 21 Nov 2025 17:20:51 +0000</pubDate></item><item><title>FizzBuzz with Cosines</title><link>https://susam.net/fizz-buzz-with-cosines.html</link><description>&lt;doc fingerprint="1ad2990568f48aac"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Solving Fizz Buzz with Cosines&lt;/head&gt;
    &lt;p&gt;Fizz Buzz is a counting game that has become oddly popular in the world of computer programming as a simple test of basic programming skills. The rules of the game are straightforward. Players say the numbers aloud in order beginning with one. Whenever a number is divisible by 3, they say 'Fizz' instead. If it is divisible by 5, they say 'Buzz'. If it is divisible by both 3 and 5, the player says both 'Fizz' and 'Buzz'. Here is a typical Python program that prints this sequence:&lt;/p&gt;
    &lt;code&gt;for n in range(1, 101):
    if n % 15 == 0:
        print('FizzBuzz')
    elif n % 3 == 0:
        print('Fizz')
    elif n % 5 == 0:
        print('Buzz')
    else:
        print(n)
&lt;/code&gt;
    &lt;p&gt;Here is the output: fizz-buzz.txt. Can we make the program more complicated? Perhaps we can use trigonometric functions to encode all four cases in a single closed-form expression. That is what we are going to explore in this article. By the end, we will obtain a finite Fourier series that can take any integer \( n \) and select the text to be printed.&lt;/p&gt;
    &lt;head rend="h2"&gt;Contents&lt;/head&gt;
    &lt;head rend="h2"&gt;Definitions&lt;/head&gt;
    &lt;p&gt;Before going any further, we establish a precise mathematical definition for the Fizz Buzz sequence. We begin by introducing a few functions that will help us define the Fizz Buzz sequence later.&lt;/p&gt;
    &lt;head rend="h3"&gt;Symbol Functions&lt;/head&gt;
    &lt;p&gt;We define a set of four functions \( \{ s_0, s_1, s_2, s_3 \} \) for integers \( n \) by: \begin{align*} s_0(n) &amp;amp;= n, \\ s_1(n) &amp;amp;= \mathtt{Fizz}, \\ s_2(n) &amp;amp;= \mathtt{Buzz}, \\ s_3(n) &amp;amp;= \mathtt{FizzBuzz}. \end{align*} We call these the symbol functions because they produce every term that appears in the Fizz Buzz sequence. The symbol function \( s_0 \) returns \( n \) itself. The functions \( s_1, \) \( s_2 \) and \( s_3 \) are constant functions that always return the literal words \( \mathtt{Fizz}, \) \( \mathtt{Buzz} \) and \( \mathtt{FizzBuzz} \) respectively, no matter what the value of \( n \) is.&lt;/p&gt;
    &lt;head rend="h3"&gt;Fizz Buzz Sequence&lt;/head&gt;
    &lt;p&gt;Now we can define the Fizz Buzz sequence as the sequence \[ (s_{f(n)}(n))_{n = 1}^{\infty} \] where \[ f(n) = \begin{cases} 1 &amp;amp; \text{if } 3 \mid n \text{ and } 5 \nmid n, \\ 2 &amp;amp; \text{if } 3 \nmid n \text{ and } 5 \mid n, \\ 3 &amp;amp; \text{if } 3 \mid n \text{ and } 5 \mid n, \\ 0 &amp;amp; \text{otherwise}. \end{cases} \] The notation \( m \mid n \) means that the integer \( m \) divides the integer \( n, \) i.e. \( n \) is a multiple of \( m. \) Equivalently, there exists an integer \( c \) such that \( n = cm . \) Similarly, \( m \nmid n \) means that \( m \) does not divide \( n, \) i.e. \( n \) is not a multiple of \( m. \) With the above definitions in place, we can expand the first few terms of the sequence explicitly as follows: \begin{align*} (s_{f(n)}(n))_{n = 1}^{\infty} &amp;amp;= (s_{f(1)}(1), \; s_{f(2)}(2), \; s_{f(3)}(3), \; s_{f(4)}(4), \; s_{f(5)}(5), \; s_{f(6)}(6), \; s_{f(7)}(7), \; \dots) \\ &amp;amp;= (s_0(1), \; s_0(2), \; s_1(3), \; s_0(4), s_2(5), \; s_1(6), \; s_0(7), \; \dots) \\ &amp;amp;= (1, \; 2, \; \mathtt{Fizz}, \; 4, \; \mathtt{Buzz}, \; \mathtt{Fizz}, \; 7, \; \dots). \end{align*} Note how the function \( f(n) \) produces an index \( i \) which we then use to select the symbol function \( s_i(n) \) to produce the \( n \)th term of the sequence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Indicator Functions&lt;/head&gt;
    &lt;p&gt;Here is the function \( f(n) \) from the previous section with its cases and conditions rearranged to make it easier to spot interesting patterns: \[ f(n) = \begin{cases} 0 &amp;amp; \text{if } 5 \nmid n \text{ and } 3 \nmid n, \\ 1 &amp;amp; \text{if } 5 \nmid n \text{ and } 3 \mid n, \\ 2 &amp;amp; \text{if } 5 \mid n \text{ and } 3 \nmid n, \\ 3 &amp;amp; \text{if } 5 \mid n \text{ and } 3 \mid n. \end{cases} \] This function helps us to select another function \( s_{f(n)}(n) \) which in turn determines the \( n \)th term of the Fizz Buzz sequence. Our goal now is to replace this piecewise formula with a single closed-form expression. To do so, we first define indicator functions \( I_m(n) \) as follows: \[ I_m(n) = \begin{cases} 1 &amp;amp; \text{if } m \mid n, \\ 0 &amp;amp; \text{if } m \nmid n. \end{cases} \] The formula for \( f(n) \) can now be written as: \[ f(n) = \begin{cases} 0 &amp;amp; \text{if } I_5(n) = 0 \text{ and } I_3(n) = 0, \\ 1 &amp;amp; \text{if } I_5(n) = 0 \text{ and } I_3(n) = 1, \\ 2 &amp;amp; \text{if } I_5(n) = 1 \text{ and } I_3(n) = 0, \\ 3 &amp;amp; \text{if } I_5(n) = 1 \text{ and } I_3(n) = 1. \end{cases} \] Do you see a pattern? Here is the same function written as a table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;\( I_5(n) \)&lt;/cell&gt;
        &lt;cell role="head"&gt;\( I_3(n) \)&lt;/cell&gt;
        &lt;cell role="head"&gt;\( f(n) \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 0 \)&lt;/cell&gt;
        &lt;cell&gt;\( 2 \)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 1 \)&lt;/cell&gt;
        &lt;cell&gt;\( 3 \)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Do you see it now? If we treat the values in the first two columns as binary digits and the values in the third column as decimal numbers, then in each row the first two columns give the binary representation of the number in the third column. For example, \( 3_{10} = 11_2 \) and indeed in the last row of the table, we see the bits \( 1 \) and \( 1 \) in the first two columns and the number \( 3 \) in the last column. In other words, writing the binary digits \( I_5(n) \) and \( I_3(n) \) side by side gives us the binary representation of \( f(n). \) Therefore \[ f(n) = 2 \, I_5(n) + I_3(n). \] We can now write a small program to demonstrate this formula:&lt;/p&gt;
    &lt;code&gt;
 for n in range(1, 101):
    s = [n, 'Fizz', 'Buzz', 'FizzBuzz']
    i = (n % 3 == 0) + 2 * (n % 5 == 0)
    print(s[i])
&lt;/code&gt;
    &lt;p&gt;We can make it even shorter at the cost of some clarity:&lt;/p&gt;
    &lt;code&gt;
 for n in range(1, 101):
    print([n, 'Fizz', 'Buzz', 'FizzBuzz'][(n % 3 == 0) + 2 * (n % 5 == 0)])
&lt;/code&gt;
    &lt;p&gt;What we have obtained so far is pretty good. While there is no universal definition of a closed-form expression, I think most people would agree that the indicator functions as defined above are simple enough to be permitted in a closed-form expression.&lt;/p&gt;
    &lt;head rend="h2"&gt;Complex Exponentials&lt;/head&gt;
    &lt;p&gt;In the previous section, we obtained the formula \[ f(n) = I_3(n) + 2 \, I_5(n) \] which we then used as an index to look up the text to be printed. We also argued that this is a pretty good closed-form expression already.&lt;/p&gt;
    &lt;p&gt;However, in the interest of making things more complicated, we must ask ourselves: What if we are not allowed to use the indicator functions? What if we must adhere to the commonly accepted meaning of a closed-form expression which allows only finite combinations of basic operations such as addition, subtraction, multiplication, division, integer exponents and roots with integer index as well as functions such as exponentials, logarithms and trigonometric functions. It turns out that the above formula can be rewritten using only addition, multiplication, division and the cosine function. Let us begin the translation. Consider the sum \[ S_m(n) = \sum_{k = 0}^{m - 1} e^{2 \pi i k n / m}, \] where \( i \) is the imaginary unit and \( n \) and \( m \) are integers. This is a geometric series in the complex plane with ratio \( r = e^{2 \pi i n / m}. \) If \( n \) is a multiple of \( m , \) then \( n = cm \) for some integer \( c \) and we get \[ r = e^{2 \pi i n / m} = e^{2 \pi i c} = 1. \] Therefore, when \( n \) is a multiple of \( m, \) we get \[ S_m(n) = \sum_{k = 0}^{m - 1} e^{2 \pi i k n / m} = \sum_{k = 0}^{m - 1} 1^k = m. \] If \( n \) is not a multiple of \( m, \) then \( r \ne 1 \) and the geometric series becomes \[ S_m(n) = \frac{r^m - 1}{r - 1} = \frac{e^{2 \pi i n} - 1}{e^{2 \pi i n / m} - 1} = 0. \] Therefore, \[ S_m(n) = \begin{cases} m &amp;amp; \text{if } m \mid n, \\ 0 &amp;amp; \text{if } m \nmid n. \end{cases} \] Dividing both sides by \( m, \) we get \[ \frac{S_m(n)}{m} = \begin{cases} 1 &amp;amp; \text{if } m \mid n, \\ 0 &amp;amp; \text{if } m \nmid n. \end{cases} \] But the right-hand side is \( I_m(n). \) Therefore \[ I_m(n) = \frac{S_m(n)}{m} = \frac{1}{m} \sum_{k = 0}^{m - 1} e^{2 \pi i k n / m}. \]&lt;/p&gt;
    &lt;head rend="h2"&gt;Cosines&lt;/head&gt;
    &lt;p&gt;We begin with Euler's formula \[ e^{i x} = \cos x + i \sin x \] where \( x \) is a real number. From this formula, we get \[ e^{i x} + e^{-i x} = 2 \cos x. \] Therefore \begin{align*} I_3(n) &amp;amp;= \frac{1}{3} \sum_{k = 0}^2 e^{2 \pi i k n / 3} \\ &amp;amp;= \frac{1}{3} \left( 1 + e^{2 \pi i n / 3} + e^{4 \pi i n / 3} \right) \\ &amp;amp;= \frac{1}{3} \left( 1 + e^{2 \pi i n / 3} + e^{-2 \pi i n / 3} \right) \\ &amp;amp;= \frac{1}{3} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right). \end{align*} The third equality above follows from the fact that \( e^{4 \pi i n / 3} = e^{6 \pi i n / 3} e^{-2 \pi i n / 3} = e^{2 \pi i n} e^{-2 \pi i n/3} = e^{-2 \pi i n / 3}. \)&lt;/p&gt;
    &lt;p&gt;The function above is defined for integer values of \( n \) but we can extend its formula to real \( x \) and plot it to observe its shape between integers. As expected, the function takes the value \( 1 \) whenever \( x \) is an integer multiple of \( 3 \) and \( 0 \) whenever \( x \) is an integer not divisible by \( 3. \)&lt;/p&gt;
    &lt;p&gt;Similarly, \begin{align*} I_5(n) &amp;amp;= \frac{1}{5} \sum_{k = 0}^4 e^{2 \pi i k n / 5} \\ &amp;amp;= \frac{1}{5} \left( 1 + e^{2 \pi i n / 5} + e^{4 \pi i n / 5} + e^{6 \pi i n / 5} + e^{8 \pi i n / 5} \right) \\ &amp;amp;= \frac{1}{5} \left( 1 + e^{2 \pi i n / 5} + e^{4 \pi i n / 5} + e^{-4 \pi i n / 5} + e^{-2 \pi i n / 5} \right) \\ &amp;amp;= \frac{1}{5} + \frac{2}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{2}{5} \cos \left( \frac{4 \pi n}{5} \right). \end{align*} Extending this expression to real values of \( x \) allows us to plot its shape as well. Once again, the function takes the value \( 1 \) at integer multiples of \( 5 \) and \( 0 \) at integers not divisible by \( 5. \)&lt;/p&gt;
    &lt;p&gt;Recall that we expressed \( f(n) \) as \[ f(n) = I_3(n) + 2 \, I_5(n). \] Substituting these trigonometric expressions yields \[ f(n) = \frac{1}{3} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right) + 2 \cdot \left( \frac{1}{5} + \frac{2}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{2}{5} \cos \left( \frac{4 \pi n}{5} \right) \right). \] A straightforward simplification gives \[ f(n) = \frac{11}{15} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right) + \frac{4}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{4}{5} \cos \left( \frac{4 \pi n}{5} \right). \] We can extend this expression to real \( x \) and plot it as well. The resulting curve takes the values \( 0, 1, 2 \) and \( 3 \) at integer points, as desired.&lt;/p&gt;
    &lt;p&gt;Now we can write our Python program as follows:&lt;/p&gt;
    &lt;code&gt;
 from math import cos, pi
for n in range(1, 101):
    s = [n, 'Fizz', 'Buzz', 'FizzBuzz']
    i = round(11 / 15 + (2 / 3) * cos(2 * pi * n / 3)
                      + (4 / 5) * cos(2 * pi * n / 5)
                      + (4 / 5) * cos(4 * pi * n / 5))
    print(s[i])
&lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;To summarise, we have defined the Fizz Buzz sequence as \[ (s_{f(n)}(n))_{n = 1}^{\infty} \] where \[ f(n) = \frac{11}{15} + \frac{2}{3} \cos \left( \frac{2 \pi n}{3} \right) + \frac{4}{5} \cos \left( \frac{2 \pi n}{5} \right) + \frac{4}{5} \cos \left( \frac{4 \pi n}{5} \right) \in \{ 0, 1, 2, 3 \} \] and \( s_0(n) = n, \) \( s_1(n) = \mathtt{Fizz}, \) \( s_2(n) = \mathtt{Buzz} \) and \( s_3(n) = \mathtt{FizzBuzz}. \) A Python program to print the Fizz Buzz sequence based on this definition was presented earlier. That program can be written more succinctly as follows:&lt;/p&gt;
    &lt;code&gt;
 from math import cos, pi
for n in range(1, 101):
    print([n, 'Fizz', 'Buzz', 'FizzBuzz'][round(11 / 15 + (2 / 3) * cos(2 * pi * n / 3) + (4 / 5) * (cos(2 * pi * n / 5) + cos(4 * pi * n / 5)))])
&lt;/code&gt;
    &lt;p&gt;We can also wrap this up nicely in a shell one-liner, in case you want to share it with your friends and family and surprise them:&lt;/p&gt;
    &lt;code&gt;printf 'from math import cos, pi\nfor n in range(1, 101): print([n, "Fizz", "Buzz", "FizzBuzz"][round(11 / 15 + (2 / 3) * cos(2 * pi * n / 3) + (4 / 5) * (cos(2 * pi * n / 5) + cos(4 * pi * n / 5)))])' | python3&lt;/code&gt;
    &lt;p&gt;The keen-eyed might notice that the expression we have obtained for \( f(n) \) is a finite Fourier series. This is not surprising, since the output of a Fizz Buzz program depends only on \( n \bmod 15. \) Any function on a finite cyclic group can be written exactly as a finite Fourier expansion.&lt;/p&gt;
    &lt;p&gt;We have taken a simple counting game and turned it into a trigonometric construction: a finite Fourier series with a constant term \( 11/15 \) and three cosine terms with coefficients \( 2/3, \) \( 4/5 \) and \( 4/5. \) None of this makes Fizz Buzz any easier, of course, but it does show that every \( \mathtt{Fizz} \) and \( \mathtt{Buzz} \) now owes its existence to a particular set of Fourier coefficients. We began with the modest goal of making this simple problem more complicated. I think it is safe to say that we did not fall short.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006598</guid><pubDate>Fri, 21 Nov 2025 17:28:25 +0000</pubDate></item><item><title>Helping Valve to Power Up Steam Devices</title><link>https://www.igalia.com/2025/11/helpingvalve.html</link><description>&lt;doc fingerprint="961b0d5348912672"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Helping Valve to Power Up Steam Devices&lt;/head&gt;
    &lt;p&gt;Last week, Valve stunned the computer gaming world by unveiling three new gaming devices at once: the Steam Frame, a wireless VR headset; the Steam Machine, a gaming console in the vein of a PlayStation or Xbox; and the Steam Controller, a handheld game controller. Successors to the highly successful Valve Index and Steam Deck, these devices are set to be released in the coming year.&lt;/p&gt;
    &lt;p&gt;Igalia has long worked with Valve on SteamOS, which will power the Machine and Frame, and is excited to be contributing to these new devices, particularly the Frame. The Frame, unlike the Machine or Deck which have x86 CPUs, runs on an ARM-based CPU.&lt;/p&gt;
    &lt;p&gt;Under normal circumstances, this would mean that only games compiled to run on ARM chips could be played on the Frame. In order to get around this barrier, a translation layer called FEX is used to run applications compiled for x86 chips (which are used in nearly all gaming PCs) on ARM chips by translating the x86 machine code into ARM64 machine code.&lt;/p&gt;
    &lt;p&gt;âIf you love video games, like I do, working on FEX with Valve is a dream come true,â said Paulo Matos, an engineer with Igaliaâs Compilers Team. Even so, the challenges can be daunting, because making sure the translation is working often requires manual QA rather than automated testing. âYou have to start a game, sometimes the error shows up in the colors or sound, or how the game behaves when you break down the door in the second level. Just debugging this can take a while,â said Matos. âFor optimization work I did early last year, I used a game called Psychonauts to test it. I must have played the first 3 to 4 minutes of the game many, many times for debugging. Looking at my history, Steam tells me I played it for 29 hours, but it was always the first few minutes, nothing else.â&lt;/p&gt;
    &lt;p&gt;Beyond the CPU, the Qualcomm Adreno 750 GPU used in the Steam Frame introduced its own set of challenges when it came to running desktop games, and other complex workloads, on these devices. Doing so requires a rock-solid Vulkan driver that can ensure correctness, eliminating major rendering bugs, while maintaining high performance. This is a very difficult combination to achieve, and yet thatâs exactly what weâve done for Valve with Mesa3D Turnip, a FOSS Vulkan driver for Qualcomm Adreno GPUs.&lt;/p&gt;
    &lt;p&gt;Before we started our work, critical optimizations such as LRZ (which you can learn more about from our blog post here) or the autotuner (and its subsequent overhaul) werenât in place. Even worse, there wasnât support for the Adreno 700-series GPUs at all, which we eventually added along with support for tiled rendering.&lt;/p&gt;
    &lt;p&gt;âWe implemented many Vulkan extensions and reviewed numerous others,â said Danylo Piliaiev, an engineer on the Graphics Team. âOver the years, we ensured that D3D11, D3D12, and OpenGL games rendered correctly through DXVK, vkd3d-proton, and Zink, investigating many rendering issues along the way. We achieved higher correctness than the proprietary driver and, in many cases, Mesa3D Turnip is faster as well.â&lt;/p&gt;
    &lt;p&gt;Weâve worked with many wonderful people from Valve, Google, and other companies to iterate on the Vulkan driver over the years in order to introduce new features, bug fixes, performance improvements, as well as debugging workflows. Some of those people decided to join Igalia later on, such as our colleague and Graphics Team developer Emma Anholt. âIâve been working on Mesa for 22 years, and itâs great to have a home now where I can keep doing that work, across hardware projects, where the organization prioritizes the work experience of its developers and empowers them within the organization.â&lt;/p&gt;
    &lt;p&gt;Valveâs support in all this cannot be understated, either. Their choice to build their devices using open software like Mesa3D Turnip and FEX means theyâre committed to working on and supporting improvements and optimizations that become available to anyone who uses the same open-source projects.&lt;/p&gt;
    &lt;p&gt;âWeâve received a lot of positive feedback about significantly improved performance and fewer rendering glitches from hobbyists who use these projects to run PC games on Android phones as a result of our work,â said Dhruv Mark Collins, another Graphics Team engineer working on Turnip. âAnd it goes both ways! Weâve caught a couple of nasty bugs because of that widespread testing, which really emphasizes why the FOSS model is beneficial for everyone involved.â&lt;/p&gt;
    &lt;p&gt;An interesting area of graphics driver development is all the compiler work that is involved. Vulkan drivers such as Mesa3D Turnip need to process shader programs sent by the application to the GPU, and these programs govern how pixels in our screens are shaded or colored with geometry, textures, and lights while playing games. Job Noorman, an engineer from our Compilers Team, made significant contributions to the compiler used by Mesa3D Turnip. He also contributed to the Mesa3D NIR shader compiler, a common part that all Mesa drivers use, including RADV (most popularly used on the Steam Deck) or V3DV (used on Raspberry Pi boards).&lt;/p&gt;
    &lt;p&gt;As is normal for Igalia, while we focused on delivering results for our customer, we also made our work as widely useful as possible. For example: âWhile our target throughout our work has been the Snapdragon 8 Gen 3 thatâs in the Frame, much of our work extends back through years of Snapdragon hardware, and we regression test it to make sure it stays Vulkan conformant,â said Anholt. This means that Igaliaâs work for the Frame has consistently passed Vulkanâs Conformance Test Suite (CTS) of over 2.8 million tests, some of which Igalia is involved in creating.&lt;/p&gt;
    &lt;p&gt;Our very own Vulkan CTS expert Ricardo GarcÃa says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Igalia and other Valve contractors actively participate in several areas inside the Khronos Group, the organization maintaining and developing graphics API standards like Vulkan. We contribute specification fixes and feedback, and we are regularly involved in the development of many new Vulkan extensions. Some of these end up being critical for game developers, like mesh shading. Others ensure a smooth and efficient translation of other APIs like DirectX to Vulkan, or help take advantage of hardware features to ensure applications perform great across multiple platforms, both mobile like the Steam Frame or desktop like the Steam Machine. Having Vulkan CTS coverage for these new extensions is a critical step in the release process, helping make sure the specification is clear and drivers implement it correctly, and Igalia engineers have contributed millions of source code lines and tests since our collaboration with Valve started.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;A huge challenge we faced in moving forward with development is ensuring that we didnât introduce regressions, small innocent-seeming changes can completely break rendering on games in a way that even CTS might not catch. What automated testing could be done was often quite constrained, but Igalians found ways to push through the barriers. âI made a continuous integration test to automatically run single-frame captures of a wide range of games spanning D3D11, D3D9, D3D8, Vulkan, and OpenGL APIs,â said Piliaiev, about the development covered in his recent XDC 2025 talk, âensuring that we donât have rendering or performance regressions.â&lt;/p&gt;
    &lt;p&gt;Looking ahead, Igaliaâs work for Valve will continue to deliver benefits to the wider Linux Gaming ecosystem. For example, the Steam Frame, as a battery-powered VR headset, needs to deliver high performance within a limited power budget. A way to address this is to create a more efficient task scheduler, which is something Changwoo Min of Igaliaâs Kernel Team has been working on. As he says, âI have been developing a customized CPU scheduler for gaming, named LAVD: Latency-criticality Aware Virtual Deadline scheduler.â&lt;/p&gt;
    &lt;p&gt;In general terms, a scheduler automatically identifies critical tasks and dynamically boosts their deadlines to improve responsiveness. Most task schedulers donât take energy consumption into account, but the Rust-based LAVD is different. âLAVD makes scheduling decisions considering each chipâs performance versus energy trade-offs. It measures and predicts the required computing power on the fly, then selects the best set of CPUs to meet that demand with minimal energy consumption,â said Min.&lt;/p&gt;
    &lt;p&gt;One of our other kernel engineers, Melissa Wen, has been working on AMD kernel display drivers to maintain good color management and HDR support for SteamOS across AMD hardware families, both for the Steam Deck and the Steam Machine. This is especially important with newer display hardware in the Steam Machine, which features some notable differences in color capabilities, aiming for more powerful and efficient color management which necessitated driver work.&lt;/p&gt;
    &lt;p&gt;â¦and thatâs a wrap! We will continue our efforts toward improving future versions of SteamOS, and with a partner as strongly supportive as Valve, we expect to do more work to make Linux gaming even better. If any of that sounded interesting and youâd like to work with us to tackle a tricky problems of your own, please get in touch!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46006616</guid><pubDate>Fri, 21 Nov 2025 17:29:59 +0000</pubDate></item><item><title>How/why to sweep async tasks under a Postgres table</title><link>https://taylor.town/pg-task</link><description>&lt;doc fingerprint="7f56acd72f49a4cf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How/Why to Sweep Async Tasks Under a Postgres Table&lt;/head&gt;
    &lt;p&gt;I like slim and stupid servers, where each endpoint wraps a very dumb DB query.&lt;/p&gt;
    &lt;p&gt;Dumb queries are fast. Fast queries make websites smooth and snappy. Keep those click/render loops sacred.&lt;/p&gt;
    &lt;p&gt;Sweep complexity under a &lt;code&gt;task&lt;/code&gt; table:&lt;/p&gt;
    &lt;code&gt;router.post("/signup", async ctx =&amp;gt; {
  const { email, password } = await ctx.request.body().value;
  const [{ usr_id } = { usr_id: null }] = await sql`
    with usr_ as (
      insert into usr (email, password)
      values (${email}, crypt(${password}, gen_salt('bf')))
      returning *
    ), task_ as (
      insert into task (task_type, params)
      values ('SEND_EMAIL_WELCOME', ${sql({ usr_id })})
    )
    select * from usr_
  `;
  await ctx.cookies.set("usr_id", usr_id);
  ctx.response.status = 204;
});&lt;/code&gt;
    &lt;p&gt;This example uses CTEs with postgres.js.&lt;/p&gt;
    &lt;p&gt;Of course using &lt;code&gt;mailgun.send&lt;/code&gt; is easier than queuing it in a &lt;code&gt;task&lt;/code&gt; table.
Adding indirection rarely makes systems less complex. But somehow I'm here to
advocate exactly that. You may ignore my manifesto and
skip to my implementation at the end.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Secret Surface Error Area&lt;/item&gt;
      &lt;item&gt;Never Handroll Your Own Two-Phase Commit&lt;/item&gt;
      &lt;item&gt;One Way To Do Things&lt;/item&gt;
      &lt;item&gt;TODO-Driven Development&lt;/item&gt;
      &lt;item&gt;Human Fault Tolerance&lt;/item&gt;
      &lt;item&gt;Show Me The Code&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Secret Surface Error Area&lt;/head&gt;
    &lt;p&gt;Customers don't care about cosmic rays. They want a thing. More imporantly, they want immediate confirmation of their thing. They want to offload the mental burden of their goal.&lt;/p&gt;
    &lt;p&gt;For them to delegate that responsibility, your DB is probably the only thing that matters. Once information is committed to your database, you can confidently say "we'll take it from here".&lt;/p&gt;
    &lt;p&gt;You can send emails later. You can process payments later. You can do almost anything later. Just tell your customer they can continue with their goddamn day.&lt;/p&gt;
    &lt;p&gt;Delight your customers with clear feedback.&lt;/p&gt;
    &lt;p&gt;Delight your computers by writing to one place at a time.&lt;/p&gt;
    &lt;head rend="h2"&gt;Never Handroll Your Own Two-Phase Commit&lt;/head&gt;
    &lt;p&gt;Writing to two places at "the same time" is sinful.&lt;/p&gt;
    &lt;p&gt;When the gods gave us computer storage, the people became unhappy. They cried, "What is consistency? Where are our guarantees? Why must I &lt;code&gt;fsync&lt;/code&gt;?" And so they
wore sackloth and ashes for many years in their coding caves.&lt;/p&gt;
    &lt;p&gt;The people were overjoyed when the gods scrawled Postgres (and other inferior databases) onto stone tablets. The holy "database transactions" allowed humankind to pretend that they could read/write to multiple places at the same time.&lt;/p&gt;
    &lt;p&gt;To this day, databases sometimes work.&lt;/p&gt;
    &lt;p&gt;But some developers deny the works of the gods. They mix multiple tools, and so commit the sin of writing to multiple places.&lt;/p&gt;
    &lt;p&gt;"Oh, we'll just send a pubsub message after we insert the row." But data is lost. Message before insert row? Data lost. All blasphemers are doomed to reinvent two-phase commit.&lt;/p&gt;
    &lt;head rend="h2"&gt;One Way To Do Things&lt;/head&gt;
    &lt;p&gt;I like LEGO. I like Play-Doh. I like Lincoln Logs. I do not, however, like mixing them together.&lt;/p&gt;
    &lt;p&gt;It's painful to investigate systems when state is spread across SQS, Redis, PubSub, Celery, Airflow, etc. I shouldn't have to open a local detective agency find out why a process isn't running as expected.&lt;/p&gt;
    &lt;p&gt;Most modern projects use SQL. Because I dislike mixing systems, I try to take SQL as far as possible.&lt;/p&gt;
    &lt;p&gt;Of all the SQL databases, Postgres currently offers the best mix of modern first-class features and third-party extensions. Postgres can be your knock-off Kafka, artificial Airflow, crappy Clickhouse, nasty Elasticsearch, poor man's PubSub, on-sale Celery, etc.&lt;/p&gt;
    &lt;p&gt;Sure, Postgres doesn't have all the fancy features of each specialized system. But colocating queue/pipeline/async data in your main database eliminates swaths of errors. In my experience, transaction guarantees supercede everything else.&lt;/p&gt;
    &lt;head rend="h2"&gt;TODO-Driven Development&lt;/head&gt;
    &lt;code&gt;while (true) {
  // const rows = await ...
  for (const { task_type, params } of rows)
    if (task_type in tasks) {
      await tasks[task_type](tx, params);
    } else {
      console.error(`Task type not implemented: ${task_type}`);
    }
}&lt;/code&gt;
    &lt;p&gt;With a simple retry system, asynchronous decoupling magically tracks all your incomplete flows.&lt;/p&gt;
    &lt;p&gt;No need to rely upon Jira -- bugs and unimplemented tasks will be logged and retried. Working recursively from error queues is truly a wonderful experience. All your live/urgent TODOs are printed to the same place (in development and in production).&lt;/p&gt;
    &lt;p&gt;With this paradigm, you'll gravitate towards scalable pipelines. Wishful thinking makes natural architecture.&lt;/p&gt;
    &lt;head rend="h2"&gt;Human Fault Tolerance&lt;/head&gt;
    &lt;p&gt;Many systems foist useless retry-loops onto humans.&lt;/p&gt;
    &lt;p&gt;Humans should receive feedback for human errors. But humans should not receive feedback for problems that can be handled by computers (and their software developers).&lt;/p&gt;
    &lt;p&gt;Remember, all your retry-loops have to happen somewhere. Be careful what you delegate to customers and developers. Your business's bottom-line is bounded by human patience; computers have infinitely more patience than humans.&lt;/p&gt;
    &lt;head rend="h2"&gt;Show Me The Code&lt;/head&gt;
    &lt;p&gt;Here's the &lt;code&gt;task&lt;/code&gt; table:&lt;/p&gt;
    &lt;code&gt;create table task
( task_id bigint primary key not null generated always as identity
, task_type text not null -- consider using enum
, params jsonb not null -- hstore also viable
, created_at timestamptz not null default now()
, unique (task_type, params) -- optional, for pseudo-idempotency
)&lt;/code&gt;
    &lt;p&gt;Don't use serial in Postgres.&lt;/p&gt;
    &lt;p&gt;Here's the code for the task worker:&lt;/p&gt;
    &lt;code&gt;const tasks = {
  SEND_EMAIL_WELCOME: async (tx, params) =&amp;gt; {
    const { email } = params;
    if (!email) throw new Error(`Bad params ${JSON.stringify(params)}.`);
    await sendEmail({ email, body: "WELCOME" });
  },
};

(async () =&amp;gt; {
  while (true) {
    try {
      while (true) {
        await sql.begin(async (tx: any) =&amp;gt; {
          const rows = await tx`
            delete from task
            where task_id in
            ( select task_id
              from task
              order by random() -- use tablesample for better performance
              for update
              skip locked
              limit 1
            )
            returning task_id, task_type, params::jsonb as params
          `;
          for (const { task_type, params } of rows)
            if (task_type in tasks) {
              await tasks[task_type](tx, params);
            } else {
              throw new Error(`Task type not implemented: ${task_type}`);
            }
          if (rows.length &amp;lt;= 0) {
            await delay(10 * 1000);
          }
        });
      }
    } catch (err) {
      console.error(err);
      await delay(1 * 1000);
    }
  }
})();&lt;/code&gt;
    &lt;p&gt;A few notable features of this snippet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The task row will not be deleted if &lt;code&gt;sendEmail&lt;/code&gt;fails. The PG transaction will be rolled back. The row and&lt;code&gt;sendEmail&lt;/code&gt;will be retried.&lt;/item&gt;
      &lt;item&gt;The PG transaction &lt;code&gt;tx&lt;/code&gt;is passed along to tasks. This is convenient for marking rows as "processed", etc.&lt;/item&gt;
      &lt;item&gt;Transactions make error-handling so much nicer. Always organize reversible queries before irreversible side-effects (e.g. mark DB status before sending the email). Remember that the DB commits at the end.&lt;/item&gt;
      &lt;item&gt;Because of &lt;code&gt;skip locked&lt;/code&gt;, you can run any number of these workers in parallel. They will not step on each others' toes.&lt;/item&gt;
      &lt;item&gt;Random ordering is technically optional, but it makes the system more resilient to errors. With adequate randomness, a single task type cannot block the queue for all.&lt;/item&gt;
      &lt;item&gt;Use &lt;code&gt;order by (case task_type ... end), random()&lt;/code&gt;to create an easy prioritized queue.&lt;/item&gt;
      &lt;item&gt;Limiting number of retries makes the code more complicated, but definitely worth it for user-facing side-effects like emails.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;if (rows.length &amp;lt;= 0)&lt;/code&gt;prevents overzealous polling. Your DBA will be grateful.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46007267</guid><pubDate>Fri, 21 Nov 2025 18:28:31 +0000</pubDate></item></channel></rss>