<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 22 Jan 2026 21:42:51 +0000</lastBuildDate><item><title>Flowtel (YC W25) Is Hiring</title><link>https://www.ycombinator.com/companies/flowtel/jobs/LaddaEz-founding-engineer-staff-senior</link><description>&lt;doc fingerprint="b9f85b547d6ea7c3"&gt;
  &lt;main&gt;
    &lt;p&gt;The AI Voice agents for hotels&lt;/p&gt;
    &lt;p&gt;Flowtel is building the next generation operating system for hospitality, powered by modern AI agents that can handle everything from booking to room service. We just launched, secured $3 million in seed funding, and are backed by Y Combinator.&lt;/p&gt;
    &lt;p&gt;Flowtel is an AI first company. Every part of the business uses AI thoughtfully. Product, sales, marketing, operations, support, and of course engineering all run with AI at the core. We keep the team small and talented so that everyone is an engineer at heart, hands on with code, systems, and product decisions.&lt;/p&gt;
    &lt;p&gt;Must: High Agency, Extreme AI coding experience, Loves agents more than humans&lt;/p&gt;
    &lt;p&gt;Senior Engineer: $120K - $180K + ~0.5% equity&lt;/p&gt;
    &lt;p&gt;Ex-founder-Level: $150K - $250K + 1% – 3% equity&lt;/p&gt;
    &lt;p&gt;Flowtel is building the next generation operating system for hospitality, powered by modern AI agents that can handle everything from booking to room service.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46718140</guid><pubDate>Thu, 22 Jan 2026 12:00:45 +0000</pubDate></item><item><title>Qwen3-TTS family is now open sourced: Voice design, clone, and generation</title><link>https://qwen.ai/blog?id=qwen3tts-0115</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46719229</guid><pubDate>Thu, 22 Jan 2026 13:51:25 +0000</pubDate></item><item><title>Tree-sitter vs. Language Servers</title><link>https://lambdaland.org/posts/2026-01-21_tree-sitter_vs_lsp/</link><description>&lt;doc fingerprint="f7a20d9fabe23cdc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Explainer: Tree-sitter vs. LSP&lt;/head&gt;
    &lt;head rend="h5"&gt;21 Jan 2026&lt;/head&gt;
    &lt;p&gt;I got asked a good question today: what is the difference between Tree-sitter and a language server? I don’t understand how either of these tools work in depth, so I’m just going to explain from an observable, pragmatic point of view.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tree-sitter #&lt;/head&gt;
    &lt;p&gt;Tree-sitter is a parser generator. What this means is that you can hand Tree-sitter a description for a programming language and it will create a program that will parse that language for you. What’s special about Tree-sitter is that it is a.) fast, and b.) can tolerate syntax errors in the input. These two properties make Tree-sitter ideal for creating syntax highlighting engines in text editors. When you’re editing a program, most of the time the program will be in a syntactically invalid state. During that time, you don’t want your colors changing or just outright breaking while you’re typing. Naïve regex-based syntax highlighters frequently suffer from this issue.&lt;/p&gt;
    &lt;p&gt;Tree-sitter also provides a query language where you can make queries against the parse tree. I use this in the Emacs package I’m trying to develop to add Typst support to the Citar citation/bibliography tool: I can ask Tree-sitter to find a particular syntax object; it is safer and more robust than using a regular expression because it can do similar parsing to the Typst engine itself.&lt;/p&gt;
    &lt;p&gt;In short, Tree-sitter provides syntax highlighting that is faithful to how the language implementation parses the program, instead of relying on regular expressions that incidentally come close.&lt;/p&gt;
    &lt;head rend="h2"&gt;Language server #&lt;/head&gt;
    &lt;p&gt;A language server is a program that can analyze a program and report interesting information about that program to a text editor. A standard, called the Language Server Protocol (LSP), defines the kinds of JSON messages that pass between a text editor and the server. The protocol is an open standard; any language and any text editor can take advantage of the protocol to get nice smart programming helps in their system. Language servers can provide information like locating the definition of a symbol, possible completions at the cursor point, etc. to a text editor which can then decide how and when to display or use this information.&lt;/p&gt;
    &lt;p&gt;Language servers solve the “ \(N \times M\) problem” where \(N\) programming languages and \(M\) text editors would mean there have to be \(N \times M\) implementations for language analyzers. Now, every language just needs a language server, and every editor needs to be able to speak the LSP protocol.&lt;/p&gt;
    &lt;p&gt;Language servers are powerful because they can hook into the language’s runtime and compiler toolchain to get semantically correct answers to user queries. For example, suppose you have two versions of a &lt;code&gt;pop&lt;/code&gt; function, one imported from a &lt;code&gt;stack&lt;/code&gt; library, and another from a &lt;code&gt;heap&lt;/code&gt; library. If you use a tool like the dumb-jump package in Emacs


I just want to say that I think dumb-jump is very cool and I am not trying to knock it down at all. It’s honest about its limitations and can be handy when you do not have a language server available.

and you use it to jump to the definition for a call to &lt;code&gt;pop&lt;/code&gt;, it might get confused as to where to go because it’s not sure what module is in scope at the point. A language server, on the other hand, should have access to this information and would not get confused.&lt;/p&gt;
    &lt;head rend="h3"&gt;Using a language server for highlighting #&lt;/head&gt;
    &lt;p&gt;It is possible to use the language server for syntax highlighting. I am not aware of any particularly strong reasons why one would want to (or not want to) do this. The language server can be a more complicated program and so could surface particularly detailed information about the syntax; it might also be slower than tree-sitter.&lt;/p&gt;
    &lt;p&gt;Emacs’ built-in LSP client, Eglot, recently added &lt;code&gt;eglot-semantic-tokens-mode&lt;/code&gt; to support syntax highlighting as provided from the language server. I have tried this a little bit in Rust code and it seems fine; the Tree-sitter-based syntax highlighting has been working just fine for me, so I will probably stick to that unless I find a compelling reason to use the LSP-based highlighting.&lt;/p&gt;
    &lt;p&gt;Update: Thanks to a comment on HN, I now know of a good reason why you would want to use a language server for syntax highlighting: the Rust language server rust-analyzer can tell your text editor when a variable reference is mutable or not, which means you could highlight &lt;code&gt;mut&lt;/code&gt; references differently than non-&lt;code&gt;mut&lt;/code&gt; ones. Thanks to David Barsky for the tip!&lt;/p&gt;
    &lt;head rend="h2"&gt;Meta aside: the LLM angle #&lt;/head&gt;
    &lt;p&gt;I wrote all of the above article. I did not ask an LLM to generate any portion of it. Please know that whenever you read something on my blog, it comes 100% from a human—me, Ashton Wiersdorf.&lt;/p&gt;
    &lt;p&gt;I am not so anti-AI to say that LLMs are worthless or should never be used. I’ve used LLMs a little bit. I think they’re fantastic at translating between languages; this seems to be something that they should be good at doing. They’re helpful at writing some boring parts of the code I write. However, most of the time I find that I can typically write the tricky bits of the code about as fast as I could specify to an LLM what I want.&lt;/p&gt;
    &lt;p&gt;I know that an LLM could have generated a facile pile of text much like the above, and honestly it would probably be decently helpful. However, know that what you have just read came directly from the fingers of a person who thought about the topic and bent his effort to helping you understand. This is from real human who understands the meaning behind each word here. I do not play games with syntax and generate answer-shaped blog posts. There is real meaning here. Enjoy it, and go forth and make more of it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46719899</guid><pubDate>Thu, 22 Jan 2026 14:47:58 +0000</pubDate></item><item><title>GPTZero finds 100 new hallucinations in NeurIPS 2025 accepted papers</title><link>https://gptzero.me/news/neurips/</link><description>&lt;doc fingerprint="3b56a9bbb164ebc9"&gt;
  &lt;main&gt;
    &lt;row style="height:27.75pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Published Paper&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;GPTZero Scan&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Example of Verified Hallucination&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#356854;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Comment&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;John Doe and Jane Smith. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.00001, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Article with a matching title exists here. Authors are obviously fabricated. arXiv ID links to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;John Smith and Jane Doe. Deep learning techniques for avatar-based interaction in virtual environments. IEEE Transactions on Neural Networks and Learning Systems, 32(12):5600-5612, 2021. doi: 10.1109/ TNNLS.2021.3071234. URL https://ieeexplore.ieee.org/document/307123&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication. URL and DOI are fake.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Min-Jun Lee and Soo-Young Kim. Generative adversarial networks for hyper-realistic avatar creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1234-1243, 2022. doi: 10.1109/CVPR.2022.001234. URL https://ieeexplore.ieee.org/ document/00123&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication. URL and DOI are fake.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Firstname Lastname and Others. Drivlme: A large-scale multi-agent driving benchmark, 2023. URL or arXiv ID to be updated.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Potentially referring to this article, but year is off (2024)&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Firstname Lastname and Others. Robotslang: Grounded natural language for multi-robot object search, 2024. To appear.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Potentially referring to this article, but year is totally off (2020).&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Nuo Lou and et al. Dsp: Diffusion-based span prediction for masked text modeling. arXiv preprint arXiv:2305.XXXX, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match and arXiv ID is incomplete.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A. Sahoo and et al. inatk: Iterative noise aware text denoising. arXiv preprint arXiv:2402.XXXX, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match and arXiv ID is incomplete.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sheng Shi and et al. Maskgpt: Uniform denoising diffusion for language. arXiv preprint arXiv:2401.XXXX, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match and arXiv ID is incomplete.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Asma Issa, George Mohler, and John Johnson. Paraphrase identification using deep contextualized representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 517-526, 2018.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yi Tay, Kelvin Fu, Kai Wu, Ivan Casanueva, Jianfeng Liu, Byron Wallace, Shuohang Wang, Bajrang Singh, and Julian McAuley. Reasoning with heterogeneous graph representations for knowledge-aware question answering. In Findings of the Association for Computational Linguistics: ACL 2021, pp. 3497-3506, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No exact author or title match, although this title is close. No match in the publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Alex Wang, Rishi Bommasani, Dan Hendrycks, Daniel Song, and Zhilin Zhang. Efficient fewshot learning with efl: A single transformer for all tasks. In arXiv preprint arXiv:2107.13586, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. ArXiv ID leads to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Lei Yu, Jimmy Dumsmyr, and Kevin Knight. Deep paraphrase identification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $650-655,2014$.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. No match in publication&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;X. Ou and et al. Tuqdm: Token unmasking with quantized diffusion models. In ACL, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Franz Aichberger, Lily Chen, and John Smith. Semantically diverse language generation. In International Conference on Learning Representations (ICLR), 2025.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Some similarity to this article&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Maria Glushkova, Shiori Kobayashi, and Junichi Suzuki. Uncertainty estimation in neural text regression. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. $4567-4576,2021$.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yichao Wang, Bowen Zhou, Adam Lopez, and Benjamin Snyder. Uncertainty quantification in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL), pp. 1234-1245, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mohit Jain, Ethan Perez, and James Glass. Learning to predict confidence for language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 245-256, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Efficient semantic uncertainty quantification in language models via diversity-steered sampling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Srinivasan Kadavath, Urvashi Khandelwal, Alec Radford, and Noam Shazeer. Answer me this: Self-verifying large language models. In arXiv preprint arXiv:2205.05407, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Privacy Reasoning in Ambiguous Contexts&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zayne Sprague, Xi Ye, Kyle Richardson, and Greg Durrett. MuSR: Testing the limits of chain-of-thought with multistep soft reasoning. In EMNLP, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Two authors are omitted and one (Kyle Richardson) is added. This paper was published at ICLR 2024.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mario Paolone, Trevor Gaunt, Xavier Guillaud, Marco Liserre, Sakis Meliopoulos, Antonello Monti, Thierry Van Cutsem, Vijay Vittal, and Costas Vournas. A benchmark model for power system stability controls. IEEE Transactions on Power Systems, 35(5):3627-3635, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;The authors match this paper, but the title, publisher, volume, issue, and page numbers are incorrect. Year (2020) is correct.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mingliang Han, Bingni W Wei, Phelan Senatus, Jörg D Winkel, Mason Youngblood, I-Han Lee, and David J Mandell. Deep koopman operator: A model-free approach to nonlinear dynamical systems. Chaos: An Interdisciplinary Journal of Nonlinear Science, 30(12):123135, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Journal and other identifiers match this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Adaptive Quantization in Generative Flow Networks for Probabilistic Sequential Prediction&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Francisco Ramalho, Meng Liu, Zihan Liu, and Etienne Mathieu. Towards gflownets for continuous control. arXiv preprint arXiv:2310.18664, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID matches this paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Grounded Reinforcement Learning for Visual Reasoning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Arjun Gupta, Xi Victoria Lin, Chunyuan Zhang, Michel Galley, Jianfeng Gao, and Carlos Guestrin Ferrer. Robust compositional visual reasoning via language-guided neural module networks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. This paper has a similar title and matches publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;MTRec: Learning to Align with User Preferences via Mental Reward Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Diederik P. Kingma and Jimmy Ba. Deepfm: a factorization-machine based neural network for ctr prediction. In International Conference on Learning Representations, 2015.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title matches this paper. Authors, date, and publisher match this paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Weijia Xu, Xing Niu, and Marine Carpuat. Controlling toxicity in neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4245-4256, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors, publisher and date match this paper. Title and page numbers don't match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Xiang Zhang, Xuehai Wei, Xian Zhang, and Xue Zhang. Adversarial attacks and defenses in toxicity detection: A survey. In Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN), pages 1-8. IEEE, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fenglin Ding, Debesh Jha, Maria Härgestam, Pål Halvorsen, Michael A Riegler, Dag Johansen, Ronny Hänsch, and Håvard Stensland. Vits: Vision transformer for video self-supervised pretraining of surgical phase recognition. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 293-302. Springer, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Proceedings from this conference are split into volumes, but the citation doesn't have a volume number.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Humberto Acevedo-Viloria, Juan Martinez, and Maria Garcia. Relational graph convolutional networks for financial fraud detection. IEEE Transactions on Knowledge and Data Engineering, 33(7):1357-1370, 2021. doi: 10.1109/TKDE.2020.3007655.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in the cited publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Majid Zolghadr, Mohsen Jamali, and Jiawei Zhang. Diffurecsys: Diffusion-based generative modeling for sequential recommendation. Proceedings of the ACM Web Conference (WWW), pages 2156-2165, 2024. doi: 10.1145/3545678.3557899.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. DOI doesn't exist.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Bernd Kerbl, Thomas Müller, and Paolo Favaro. Efficient 3d gaussian splatting for real-time neural rendering. In CVPR, 2022. 2, 3&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Loosely matches this article, but only one author and part of the title actually match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Punchana Khungurn, Edward H. Adelson, Julie Dorsey, and Holly Rushmeier. Matching real-world material appearance. TPAMI, 2015. 6&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No clear match. Two authors and the subject match this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;When and How Unlabeled Data Provably Improve In-Context Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Ashish Kumar, Logan Engstrom, Andrew Ilyas, and Dimitris Tsipras. Understanding self-training for gradient-boosted trees. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, pp. 1651-1662, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;When and How Unlabeled Data Provably Improve In-Context Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Chuang Fan, Shipeng Liu, Seyed Motamed, Shiyu Zhong, Silvio Savarese, Juan Carlos Niebles, Anima Anandkumar, Adrien Gaidon, and Stefan Scherer. Expectation maximization pseudo labels. arXiv preprint arXiv:2305.01747, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;This paper exists, but all the authors are fabricated.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;T. Qiao, W. Liu, Z. Xie, H. Xu, J. Lin, J. Huang, and Y. Yang, "Clip-score: A robust scoring metric for text-to-image generation," arXiv preprint arXiv:2201.07519, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No clear author or title matches. Title loosely matches this article. ArXiv ID leads here.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yunwen Lei, Puyu Wang, Yiming Ying, and Ding-Xuan Zhou. Optimization and generalization of gradient descent for shallow relu networks with minimal width. preprint, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match. Authors match this paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;GeoDynamics: A Geometric State‑Space Neural Network for Understanding Brain Dynamics on Riemannian Manifolds&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Uher, R., Goodman, R., Moutoussis, M., Brammer, M., Williams, S.C.R., Dolan, R.J.: Cognitive and neural predictors of response to cognitive behavioral therapy for depression: a review of the evidence. Journal of Affective Disorders 169, 94-104 (2014)&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No exact title or author match. Loose title match with this article. Doesn't exist in the journal volume&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Label Proportions Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Scan&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Junyeong Lee, Yiseong Kim, Seungju Park, and Hyunjik Lee. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pages 18315-18327, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title matches this paper. No match in NeurIPS volume 36.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Z. Zhu, T. Yu, X. Zhang, J. Li, Y. Zhang, and Y. Fu. Neuralrgb-d: Neural representations for depth estimation and scene mapping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Y. Zhang, M. Oswald, and D. Cremers. Airslam: Illumination-invariant hybrid slam. In International Conference on Computer Vision (ICCV), pages 2345-2354, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Geometric Imbalance in Semi-Supervised Node Classification&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yihong Zhu, Junxian Li, Xianfeng Han, Shirui Pan, Liang Yao, and Chengqi Wang. Spectral contrastive graph clustering. In International Conference on Learning Representations, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. This paper has a similar title, but there's no match in the ICLR 2022 database.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Geometric Imbalance in Semi-Supervised Node Classification&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Ming Zhong, Han Liu, Weizhu Zhang, Houyu Wang, Xiang Li, Maosong Sun, and Xu Han. Hyperbolic and spherical embeddings for long-tail entities. In ACL, pages 5491-5501, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Ye Gao, Robert Tardif, Jiale Cao, and Tapio Schneider. Artificial intelligence reconstructs missing climate information. Nature Geoscience, 17:158-164, 2024. doi: 10.1038/s41561-023-01297-2.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title and publisher match this article. Issue, page numbers, and year match this article. DOI is fabricated.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Étienne Pardoux and Alexander Yu Veretennikov. Poisson equation for multiscale diffusions. Journal of Mathematical Sciences, 111(3):3713-3719, 2002.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors have frequently published together on the "poisson equation", but this title doesn't match any of their publications. Doesn't exist in publication volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Charanpal D Mummadi, Matthias Arens, and Thomas Brox. Test-time adaptation for continual semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11828-11837, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jiacheng He, Zhilu Zhang, Zhen Wang, and Yan Huang. Autoencoder based test-time adaptation for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 998-1007, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Global Minimizers of ℓp-Regularized Objectives Yield the Sparsest ReLU Neural Networks&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;M. Gong, F. Yu, J. Zhang, and D. Tao. Efficient $\ell_{p}$ norm regularization for learning sparsity in deep neural networks. IEEE Transactions on Neural Networks and Learning Systems, 33(10): $5381-5392,2022&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mihail Stoian, Richard Milbradt, and Christian Mendl. NP-Hardness of Optimal TensorNetwork Contraction and Polynomial-Time Algorithms for Tree Tensor Networks. Quantum, 6:e119, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;The authors match this article and the title is similar. However, the year, publisher and other data don't match. This article didn't appear in the 2022 Quantum volume.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jianyu Xu, Wei Li, and Ming Zhao. Complexity of Optimal Tensor Network Contraction Sequences. Journal of Computational Physics, 480:112237, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning World Models for Interactive Video Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Patrick Esser, Robin Rombach, and Björn Ommer. Structure-aware video generation with latent diffusion models. arXiv preprint arXiv:2303.07332, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors match this article. ArXiv ID leads to a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Lele Xu, Chen Lin, Hongyu Zhao, and et al. Gaborvit: Global attention with local frequency awareness. In European Conference on Computer Vision (ECCV), 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yoonwoo Lee, Jaehyeong Kang, Namil Kim, Jinwoo Shin, and Honglak Lee. Structured fast fourier transform attention for vision transformers. In Advances in Neural Information Processing Systems (NeurIPS), 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Siyuan Gong, Alan Yu, Xiaohan Chen, Yinpeng Lin, and Larry S Davis. Vision transformer compression: Early exiting and token pruning. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jiuxiang Shi, Zuxuan Wu, and Dahua Lin. Token-aware adaptive sampling for efficient diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Raphael Muller, Simon Kornblith, and Geoffrey Hinton. Adavit: Adaptive tokens for efficient vision transformer. In Proceedings of the International Conference on Machine Learning (ICML), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors match this article. Title matches this article. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Xin Wang, Anlin Chen, Lihui Xie, Xin Jin, Cheng Wang, and Ping Luo. Not all tokens are equal: Efficient transformer for tokenization and beyond. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. This article title is similar. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Z. Chen and N. Flammarion. When and why sam generalizes better: An optimization perspective. arXiv preprint arXiv:2206.09267, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to a different paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;K. A. Sankararaman, S. Sankararaman, H. Pandey, S. Ganguli, and F. Bromberg. The impact of neural network overparameterization on gradient confusion and stochastic gradient descent. In 37th International Conference on Machine Learning (ICML), pages 8469-8479, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;This paper is a match, but all authors but the first (K. A. Sankararaman) are fabricated.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Why Physically-Based Rendering. Physically-based rendering. Procedia IUTAM, 13(127137):3, 2015 .&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author given and title appears to be garbled. Publisher, issue, year, and pages match this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Pierre Casgrain, Anirudh Kulkarni, and Nicholas Watters. Learning to trade with continuous action spaces: Application to market making. arXiv preprint arXiv:2303.08603, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. ArXiv ID matches a different article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Z Ning and Y K Kwok. Q-learning for option pricing and hedging with transaction costs. Applied Economics, 52(55):6033-6048, 2020.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in journal volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;W L Chan and R O Shelton. Can machine learning improve delta hedging? Journal of Derivatives, $9(1): 39-56,2001$.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in journal volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Petter N Kolm, Sebastian Krügel, and Sergiy V Zadorozhnyi. Reinforcement learning for optimal hedging. The Journal of Trading, 14(4):4-17, 2019.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. There is no volume 14 of this journal.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Kyung Hyun Park, Hyeong Jin Kim, and Woo Chang Kim. Deep reinforcement learning for limit order book-based market making. Expert Systems with Applications, 169:114338, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publisher ID matches this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Moonseop Han and Elizabeth Qian. Robust prediction of dynamical systems with structured neural networks: Long-term behavior and chaos. Physica D: Nonlinear Phenomena, 427:133006, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publisher ID matches this article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Bart De Schutter and Serge P Hoogendoorn. Modeling and control of freeway traffic flow by state space neural networks. Neural Computing and Applications, 17(2):175-185, 2008.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match, although Schutter and Hoogendorn have written or coauthored several related papers (example and example). Journal volume/issue matches an unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jaideep Pathak, Brian R Hunt, Georg M Goerg, and Themistoklis P Sapsis. Data-driven prediction of chaotic dynamics: Methods, challenges, and opportunities. Annual Review of Condensed Matter Physics, 14:379-401, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. No match in journal volume.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Alejandro Güemes, Stefano Discetti, and Andrea Ianiro. Coarse-grained physics-based prediction of three-dimensional unsteady flows via neural networks. Science Advances, 7(46):eabj0751, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in journal volume/issue.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;BNMusic: Blending Environmental Noises into Personalized Music&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jeongseung Park, Minseon Yang, Minz Won Park, and Geonseok Lee. Diffsound: Differential sound manipulation with a few-shot supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 1767-1775, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title or author match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Wenxuan Sun, Tri Dao, Hongyu Zhuang, Zihang Dai, Albert Gu, and Christopher D Manning. Llamba: Efficient llms with mamba-based distillation. arXiv preprint arXiv:2502.14458, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;ArXiv ID leads to this article with a similar title and one matching author.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Tri Dao, Shizhe Ma, Wenxuan Sun, Albert Gu, Sam Smith, Aapo Kyrola, Christopher D Manning, and Christopher Re. An empirical study of state space models for large language modeling. arXiv preprint arXiv:2406.07887, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Two authors (Tri Dao and Albert Gu), the arXiv ID, and the year match this paper. However, the title is only a partial match.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Fourier Clouds: Fast Bias Correction for Imbalanced Semi-Supervised Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Junyan Zhu, Chenyang Li, Chao He, and et al. Freematch: A simple framework for long-tailed semi-supervised learning. In NeurIPS, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. This paper title is very close, but it was published by ICLR 2023 not NeurIPS 2021.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;NormFit: A Lightweight Solution for Few-Shot Federated Learning with Non-IID Data&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Scan&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yijie Zang et al. Fedclip: A federated learning framework for vision-language models. In NeurIPS, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match, although this title is close. No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;AI-Generated Video Detection via Perceptual Straightening&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jiahui Liu and et al. Tall-swin: Thumbnail layout transformer for generalised deepfake video detection. In ICCV, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. A paper with a similar title appears in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Multi-Expert Distributionally Robust Optimization for Out-of-Distribution Generalization&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Nitish Srivastava and Ruslan R Salakhutdinov. Discriminative features for fast frame-based phoneme classification. Neural networks, 47:17-23, 2013.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match, but authors have published together previously (example). No match in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Anh Tuan Nguyen, Shengping Li, and Chao Qin. Multimodal adversarial robustness: Attack and defense. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:77.25pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Jack Lau, Ankan Gayen, Philipp Tschandl, Gregory A Burns, Jiahong Yuan, Tanveer SyedaMahmood, and Mehdi Moradi. A dataset and exploration of models for understanding radiology images through dialogue. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2575-2584, 2018.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match. Title matches another hallucinated citation in this paper. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yikai Zhang et al. "Text-to-Image Diffusion Models with Customized Guidance". In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Author Song and AnotherAuthor Zhang. "Consistency in Diffusion Models: Improving Noise Embeddings". In: IEEE Transactions on Pattern Analysis and Machine Intelligence (2023). URL: https://arxiv.org/abs/2304.08787.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. This paper has a similar title. ArXiv ID leads to unrelated paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Strategic Costs of Perceived Bias in Fair Selection&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Claudia Goldin. Occupational choices and the gender wage gap. American Economic Review, 104(5):348-353, 2014.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Author is a famous economist, but the title doesn't match any of her works. Journal and locators match this unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Linear Transformers Implicitly Discover Unified Numerical Algorithms&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Olah, C., Elhage, N., Nanda, N., Schiefer, N., Jones, A., Henighan, T., and DasSarma, N. (2022). Transformer circuits. Distill, 7(3). https://distill.pub/2022/circuits/.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Most authors match this paper, but the title, publisher, and year are different. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Linear Transformers Implicitly Discover Unified Numerical Algorithms&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Nanda, N. (2023). Progress in mechanistic interpretability: Reverse-engineering induction heads in GPT-2.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match. Author may be Neel Nanda, who wrote several similar articles in 2023.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;J. Zhang and X. Li. Multi-agent systems for distributed problem solving: A framework for task decomposition and coordination. Procedia Computer Science, 55:1131-1138, 2015.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Erfan Aghasian, Shai Avidan, Piotr Dollar, and Justin Johnson. Hierarchical protocols for multi-agent 3d scene understanding. In CVPR, pages 7664-7673, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Rami El-Yaniv, and Yoshua Bengio. Quantizing deep convolutional networks for efficient inference: A whitepaper. arXiv preprint arXiv:1612.01462, 2017.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Authors mostly match this paper. Title matches this paper. ArXiv ID matches a third paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zhiqiang Wang, Chao Zhang, Bing Li, Zhen Xu, and Zhiwei Li. A survey of model compression and acceleration for deep neural networks. ACM Computing Surveys, 54(7):1-34, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match. Title matches this paper. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Andrew Black et al. Zero-shot skill composition with semantic feature fusion. arXiv preprint arXiv:2310.08573, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No title match. ArXiv ID leads to unrelated paper.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yufei Wu, Kiran Alwala, Vivek Ganapathi, Sudeep Sharma, Yilun Chang, Yicheng Zhang, Yilun Zhou, et al. Susie: Scaling up instruction-following policies for robot manipulation. arXiv preprint arXiv:2402.17552, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FLAME: Fast Long-context Adaptive Memory for Event-based Vision&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zhipeng Zhang, Chang Liu, Shihan Wu, and Yan Zhao. EST: Event spatio-temporal transformer for object recognition with event cameras. In ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;FLAME: Fast Long-context Adaptive Memory for Event-based Vision&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Daniel Gehrig, Mathias Gehrig, John Monaghan, and Davide Scaramuzza. Recurrent vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 3139-3148, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match, but this paper has a similar title. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Qiyang Du, Ozan Sener, and Silvio Savarese. Agree to disagree: Adaptive learning with gradient disagreement. In Advances in Neural Information Processing Systems (NeurIPS), 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Sener and Savarese have published together previously. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI**&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Longxuan Jing, Yu Tian, Yujun Pei, Yibing Shen, and Jiashi Feng. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Learning Representations (ICLR), 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author match. Title matches this paper. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Yair Leviathan, Clemens Rosenbaum, and Slav Petrov. Fast inference from transformers via speculative decoding. In ICML, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Title, publisher, and date match this paper, but all authors except one surname (Leviathan) are different.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Wenwen Chang, Tal Schuster, and Yann LeCun. Neural surgery for memorisation: Locating and removing verbatim recall neurons. In NeurIPS, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;M. Garcia and A. Thompson. Applications of llms in legal document analysis. Journal of Legal Technology, 7(1):50-65, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publication doesn't exist.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;J. Smith and A. Patel. Leveraging large language models for financial forecasting. International Journal of Financial Technology, 9(2):101-115, 2024.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Publication doesn't exist.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;David Jones et al. Gpsa: Gene expression and histology-based spatial alignment. Nature Methods, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:64.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI*&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Zhihao Chen, Hantao Zhang, Yuhan Zhang, Zhanlin Hu, Quanquan Gu, Qing Zhang, and Shuo Suo. Slat: a transformer-based method for simultaneous alignment and clustering of spatial transcriptomics data. Nature Communications, 14(1):5548, 2023.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;François Baccelli, Gérard H. Taché, and Etienne Altman. Flow complexity and heavytailed delays in packet networks. Performance Evaluation, 49(1-4):427-449, 2002.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:52.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Saravanan Jebarajakirthy, Paurav Shukla, and Prashant Palvia. Heavy-tailed distributions in online ad response: A marketing analytics perspective. Journal of Business Research, 124:818-830, 2021.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#ffffff;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. Doesn't exist in publication.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
    &lt;row style="height:40.5pt"&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Sources&lt;/p&gt;
        &lt;p&gt;AI&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;Mehdi Azabou, Micah Weber, Wenlin Ma, et al. Mineclip: Multimodal neural exploration of clip latents for automatic video annotation. arXiv preprint arXiv:2210.02870, 2022.&lt;/p&gt;
      &lt;/cell&gt;
      &lt;cell style="border-left:solid #cccccc 0.45454574999999997pt;border-right:solid #cccccc 0.45454574999999997pt;border-bottom:solid #cccccc 0.45454574999999997pt;border-top:solid #cccccc 0.45454574999999997pt;vertical-align:middle;background-color:#f6f8f9;padding:2pt 6pt 2pt 6pt;overflow:hidden;overflow-wrap:break-word;"&gt;
        &lt;p&gt;No author or title match. ArXiv ID leads to unrelated article.&lt;/p&gt;
      &lt;/cell&gt;
    &lt;/row&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46720395</guid><pubDate>Thu, 22 Jan 2026 15:20:48 +0000</pubDate></item><item><title>SFPark: Interactive map of SF parking regulations</title><link>https://hugues.betakappaphi.com/2026/01/21/sfpark/</link><description>&lt;doc fingerprint="363911572c27b0a7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;SFPark: interactive map of SF parking regulations&lt;/head&gt;
    &lt;p&gt;TL;DR there’s an app for that, and it’s 99% vibe coded.&lt;/p&gt;
    &lt;head rend="h2"&gt;“I could do this in a weekend”…&lt;/head&gt;
    &lt;p&gt;Our boys started attending an all-outdoors school in Golden Gate Park recently. It’s awesome! They love it, and quite frankly, I wish I had had that option as a kid!&lt;/p&gt;
    &lt;p&gt;The only downside is the commute: from our part of the Mission to Golden Gate Park takes ~30-45min each way with the cargo bike, so after many years of barely using the car, we’re now seriously looking at the reality of a daily car commute :(&lt;/p&gt;
    &lt;p&gt;The other day, Mom was dropping off one of the boys, and was thrilled to find plenty of street parking! Only to find out in short order that it was only plentiful because it was during street cleaning hours, and she got a ticket for being at the wrong place at the wrong time!&lt;/p&gt;
    &lt;p&gt;She mentioned over dinner how she’s been asking various tech-inclined people in our extended network to build an app that would allow her to more effectively look for parking by surfacing relevant restrictions. And getting repeatedly told that it would be easy because the data is all publicly available, but no one actually ever taking the time to do it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bespoke Software for the masses&lt;/head&gt;
    &lt;p&gt;This is a typical “weekend project” scope, which looks very straightforward, but for which the “activation energy” is surprisingly high, and the tail of bugfixes and polish always result in multiple weeks overrun.&lt;/p&gt;
    &lt;p&gt;Which is why those barely ever get done, because the reality of being a parent and wanting to maintain a modicum of social life means time for such projects is extremely limited.&lt;/p&gt;
    &lt;p&gt;Or at least it was, until Claude Code entered the scene, and Opus 4.5 upped the ante!&lt;/p&gt;
    &lt;p&gt;As many people have realized for themselves, and started to shout on the metaphorical rooftops, we have entered a new era of highly accessible bespoke software, or bespochastic software as I sometimes like to call it.&lt;/p&gt;
    &lt;p&gt;When I brought this idea to Claude and defined the parameter of a prototype, he came back with a reasonable plan, and a “2-3 days” estimate. I happily told him to get to work, and about 7min later the first pass prototype was ready for me to test and give feedback on!&lt;/p&gt;
    &lt;p&gt;We were on! Suddenly this project went from a daydream to something that could realistically be complete in a matter of hours, even in my packed schedule!&lt;/p&gt;
    &lt;head rend="h2"&gt;Levels of abstraction&lt;/head&gt;
    &lt;p&gt;LLMs elicit a wide range of reaction. Some people see massive potential, others are disdainful of “stochastic parrots”. As for me, curious as always, I am still exploring their strength and weaknesses, and while they exhibit very jagged understanding and skills, I definitely see a lot of value in them. In particular, working on this project showed me a very clear use case where, with adequate guidance, they can act as incredible force multipliers.&lt;/p&gt;
    &lt;p&gt;Once upon a time, software was written by tediously recording static bit patterns on tape.&lt;/p&gt;
    &lt;p&gt;Then people assigned textual mnemonic to those bit patterns and called it “assembly”. Assembly language(s) went from straight mapping, to inferring memory offsets for jump labels and variable references, to even supporting various form of compile-time macros in many cases.&lt;/p&gt;
    &lt;p&gt;Eventually higher level languages emerged: C, Basic, Java, … each with a different take on how to best express human desires to make them intelligible to the underlying machine.&lt;/p&gt;
    &lt;p&gt;I would argue that at this point, the frontier models have become good enough that, when staying within a reasonable distance of the training distribution, they can essentially act as a high-level compilers. Nowadays, most people, most of the time, do not choose to stare at the binary code generated by their compiler. Similarly I can now build good web apps without so much as glancing at the implementation details of the frontend! This allows me to focus on the functional requirements, high-level design, and backend bits, which I am much better at, and also find much more enjoyable!&lt;/p&gt;
    &lt;p&gt;I particularly appreciate that, if I wanted to dive in an understand the ins and outs of the frontend, I believe Claude would do an excellent job of explaining it so I could learn. Likewise, I believe Claude would probably be quite capable to take the opposite role, and act as a backend dev if a frontend-inclined person was looking to delegate work in the opposite way. At this point, the human provides a goal and taste, and gets to pick which parts to focus on and own, and which to delegate to an imperfect but very competent coworker!&lt;/p&gt;
    &lt;head rend="h2"&gt;Behold Public Data&lt;/head&gt;
    &lt;p&gt;One of the cool things about recent LLMs is how effective they can be at basic research, cutting through a few iterative steps to get you to the relevant information quicker than my feeble hand could possibly achieve with a keyboard.&lt;/p&gt;
    &lt;p&gt;It took Claude merely seconds to point us towards all the relevant data sources for this project: data.sfgov.org and services.sfmta.com offer a variety of datasets in GeoJSON format that is perfect for overlaying on top of a map, and Claude quickly identified the relevant ones:&lt;/p&gt;
    &lt;p&gt;My original intent was to do everything client side, so I wouldn’t have to host anything more than a few static HTML/CSS/JS files on my VPS. However, the relevant dataset in its raw GeoJSON glory turned out to weigh more than ~50Mb, a rather prohibitive amount for a mobile connection!&lt;/p&gt;
    &lt;p&gt;I still didn’t want to have to build a complicated API backend, so I decided to write a go tool to fetch and pre-process all that raw data for consumption by the frontend. The idea was to run this as an hourly job on a reasonably beefy node on my homelab, and automatically rsync the updated file to my VPS.&lt;/p&gt;
    &lt;p&gt;The pre-processing was primarily intended to trim-down the raw data to just the relevant bits, and encode those more succintly. This optimization process was iterative, as Claude and I figured out exactly what to keep and what to drop, and experimented with various ways to structure the data, to balance encoded size, speed of decoding, and support on most browsers without pulling in 3rd party deps.&lt;/p&gt;
    &lt;p&gt;One of the most magical things about working with Claude on this project, was how deftly it handled multiple rounds of schema and encoding change for this pre-preprocessed data. It would always correctly identify all the places it was used and update them correctly. This is particularly impressive considering that many language-specific automated refactoring tools still fall short of achieving that!&lt;/p&gt;
    &lt;head rend="h2"&gt;Bespoke, but for real this time&lt;/head&gt;
    &lt;p&gt;Claude’s initial prototype was based on Leaflet, which seemed like a solid choice given my requirements.&lt;/p&gt;
    &lt;p&gt;However, I quickly became frustrated about a few limitations that we ran into, especially when trying get my custom overlay layers to react smoothly to scrolling and panning, and when exploring further performance optimizations.&lt;/p&gt;
    &lt;p&gt;Emboldened by the success so far, I instructed Claude to reimplement the equivalent functionality in pure JS, with no dependencies. This turned out to be surprisingly quick and successful, and it unlocked excellent opportunities for follow-up optimizations.&lt;/p&gt;
    &lt;p&gt;In particular, it allowed me to drop lat/long coordinates entirely, and standardize both the UI and the pre-processed data on world coordinates, offset to one of the corners of my bounded SF map, and quantized to 32bit integers. This transformation massively simplified a lot of the code (especially the zoom logic and its interaction with CSS transforms), and made everything significantly faster by avoiding repeated coordinate conversions all over the place.&lt;/p&gt;
    &lt;p&gt;The custom coordinate format also was key to compressing the pre-processed data: going from string representation of floating point lat/long that were in excess of 10 characters each, to 4 bytes per point was a huge win, and the fixed-width allowed further gain by compacting long sequences of strings in the JSON to singular base64-encoded runs, further reducing the overhead. As a cherry-on-top, the quantized coordinates are much more compressible than the original strings, leading to further gain for the gzip/brotli compressed files.&lt;/p&gt;
    &lt;p&gt;Going further down the rabbit hole, here’s a sample of what Claude allowed me to do, all in the span of a 2-3 evenings:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Move from typical raster tiles for the base map, to a trimmed down vector outline of geography and major streets. Thus a gazillion image fetches while scrolling and panning became a single fetch of compressed json on first load.&lt;/item&gt;
      &lt;item&gt;Support both a canvas backend and a WebGL one for rendering, both with excellent performance&lt;/item&gt;
      &lt;item&gt;Optimize scroll/pan with smooth CSS transforms to avoid aggressive re-rendering&lt;/item&gt;
      &lt;item&gt;Assess which of the source data files support conditional fetch with ETag, and rework the backend job that periodically refreshes the pre-processed data to avoid redundant work when the inputs are unchanged.&lt;/item&gt;
      &lt;item&gt;Investigate surprising features of the raw data and work out how to properly handle right/left side of streets, regular day vs holiday schedules, …&lt;/item&gt;
      &lt;item&gt;Explore various data sources for the coastline polygon, ultimately landing on OpenStreetMap, and work out how to assemble a closed polygon from a jumble of segments from the raw OSM data, and automatically simplify it to reduce transfer size and rendering complexity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;Static Frontend:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;HTML: 1.5kb&lt;/item&gt;
      &lt;item&gt;JS: 111kb (uncompressed)&lt;/item&gt;
      &lt;item&gt;CSS: 27kb (uncompressed)&lt;/item&gt;
      &lt;item&gt;vector basemap: 1.5Mb uncompressed / ~800kb compressed, ETag-based caching on client side&lt;/item&gt;
      &lt;item&gt;pre-processed parking data: 5.1Mb uncompressed / ~800kb compressed, ETag-based caching on client side&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Backend:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;hourly job to refresh basemap and pre-processed data&lt;/item&gt;
      &lt;item&gt;takes less than 20s to pre-process data from raw sources on cold start (mostly limited by download size/speed)&lt;/item&gt;
      &lt;item&gt;most runs no-op in ~1-5s when raw data is unchanged (dominated by slow HTTP endpoints to check for changes)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Or go peruse the source (NB: deploy scripts are specific to my peculiar homelab setup…)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46720987</guid><pubDate>Thu, 22 Jan 2026 15:58:09 +0000</pubDate></item><item><title>It looks like the status/need-triage label was removed</title><link>https://github.com/google-gemini/gemini-cli/issues/16728</link><description>&lt;doc fingerprint="e9dfceee3b66ba3c"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 10.7k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;head rend="h3"&gt;What would you like to be added?&lt;/head&gt;
    &lt;p&gt;Adds native recognition for JetBrains IDE as a supported IDE environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why is this needed?&lt;/head&gt;
    &lt;p&gt;Currently, Gemini CLI restricts IDE integration features to environments where TERM_PROGRAM is vscode (or other hardcoded values). This forces 3rd-party integrations like jetbrains-ide-companion to mock VS Code by spoofing environment variables to enable core features, otherwise it could not be discovered by Gemini CLI.&lt;/p&gt;
    &lt;p&gt;For some reason, the process detection is not working properly on windows/linux (, reported by users here JetBrains Plugin Review and here #9273 , and a few other bug report email i've received), which making this native IDE detection logic a MUST do for gemini-cli discover and connect to IDE via environmental variables instead of port info file.&lt;/p&gt;
    &lt;p&gt;This PR adds JetBrains IDE Series to the IDE_DEFINITIONS and updates the detection logic to recognize TERMINAL_EMULATOR=JetBrains-JediTerm as a first-class supported environment.&lt;/p&gt;
    &lt;head rend="h3"&gt;Additional context&lt;/head&gt;
    &lt;p&gt;Inspired by #16083&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721179</guid><pubDate>Thu, 22 Jan 2026 16:10:20 +0000</pubDate></item><item><title>Show HN: BrowserOS – "Claude Cowork" in the browser</title><link>https://github.com/browseros-ai/BrowserOS</link><description>&lt;doc fingerprint="6d52ae51cdf925eb"&gt;
  &lt;main&gt;
    &lt;p&gt;🌐 BrowserOS is an open-source chromium fork that runs AI agents natively. Your open-source, privacy-first alternative to ChatGPT Atlas, Perplexity Comet, Dia.&lt;/p&gt;
    &lt;p&gt;🔒 Privacy first - use your own API keys or run local models with Ollama. Your data stays on your computer.&lt;/p&gt;
    &lt;p&gt;💡 Join our Discord or Slack and help us build! Have feature requests? Suggest here.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Download and install BrowserOS:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Import your Chrome data (optional)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Connect your AI provider (OpenAI, Anthropic, or local models via Ollama/LMStudio)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Start automating!&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🏠 Feels like home - same familiar interface as Google Chrome, works with all your extensions&lt;/item&gt;
      &lt;item&gt;🤖 AI agents that run on YOUR browser, not in the cloud&lt;/item&gt;
      &lt;item&gt;🔒 Privacy first - bring your own keys or use local models with Ollama. Your browsing history stays on your computer&lt;/item&gt;
      &lt;item&gt;🚀 Open source and community driven - see exactly what's happening under the hood&lt;/item&gt;
      &lt;item&gt;🤝 BrowserOS as MCP server - you can install our MCP server and use the browser from within &lt;code&gt;claude-code&lt;/code&gt;or&lt;code&gt;gemini-cli&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;🛡️ Built-in AI ad blocker that works across more scenarios!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;🎇 Install BrowserOS as MCP and control it from &lt;code&gt;claude-code&lt;/code&gt;&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;HackerNews.top.3.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;use-browserOS-to-chat.mp4&lt;/head&gt;
    &lt;head class="px-3 py-2"&gt;use-browserOS-to-extract.mp4&lt;/head&gt;
    &lt;p&gt;For the first time since Netscape pioneered the web in 1994, AI gives us the chance to completely reimagine the browser. We've seen tools like Cursor deliver 10x productivity gains for developers—yet everyday browsing remains frustratingly archaic.&lt;/p&gt;
    &lt;p&gt;You're likely juggling 70+ tabs, battling your browser instead of having it assist you. Routine tasks, like ordering something from amazon or filling a form should be handled seamlessly by AI agents.&lt;/p&gt;
    &lt;p&gt;At BrowserOS, we're convinced that AI should empower you by automating tasks locally and securely—keeping your data private. We are building the best browser for this future!&lt;/p&gt;
    &lt;head&gt;vs Chrome&lt;/head&gt;
    &lt;p&gt;While we're grateful for Google open-sourcing Chromium, but Chrome hasn't evolved much in 10 years. No AI features, no automation, no MCP support.&lt;/p&gt;
    &lt;head&gt;vs Brave&lt;/head&gt;
    &lt;p&gt;We love what Brave started, but they've spread themselves too thin with crypto, search, VPNs. We're laser-focused on AI-powered browsing.&lt;/p&gt;
    &lt;head&gt;vs Arc/Dia&lt;/head&gt;
    &lt;p&gt;Many loved Arc, but it was closed source. When they abandoned users, there was no recourse. We're 100% open source - fork it anytime!&lt;/p&gt;
    &lt;head&gt;vs Perplexity Comet&lt;/head&gt;
    &lt;p&gt;They're a search/ad company. Your browser history becomes their product. We keep everything local.&lt;/p&gt;
    &lt;head&gt;vs ChatGPT Atlas&lt;/head&gt;
    &lt;p&gt;Your browsing data could be used for ads or to train their models. We keep your history and agent interactions strictly local.&lt;/p&gt;
    &lt;p&gt;We'd love your help making BrowserOS better!&lt;/p&gt;
    &lt;p&gt;BrowserOS is open source under the AGPL-3.0 license.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ungoogled-chromium - BrowserOS uses some patches for enhanced privacy. Thanks to everyone behind this project!&lt;/item&gt;
      &lt;item&gt;The Chromium Project - At the core of BrowserOS, making it possible to exist in the first place.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Thank you to all our supporters!&lt;/p&gt;
    &lt;p&gt;Built with ❤️ from San Francisco&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721474</guid><pubDate>Thu, 22 Jan 2026 16:30:58 +0000</pubDate></item><item><title>Show HN: Text-to-video model from scratch (2 brothers, 2 years, 2B params)</title><link>https://huggingface.co/collections/Linum-AI/linum-v2-2b-text-to-video</link><description>&lt;doc fingerprint="745d5508e730e637"&gt;
  &lt;main&gt;
    &lt;p&gt;Hugging Face Models Datasets Spaces Community Docs Enterprise Pricing Log In Sign Up Linum-AI 's Collections Linum v2 (2B, text-to-video) Linum v2 (2B, text-to-video) updated about 4 hours ago 360p or 720p, 2-5 seconds, Apache 2.0 Upvote 1 Linum-AI/linum-v2-360p Text-to-Video • Updated 2 days ago • 4 Linum-AI/linum-v2-720p Text-to-Video • Updated 3 days ago • 3 Upvote 1 Share collection View history Collection guide Browse collections&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721488</guid><pubDate>Thu, 22 Jan 2026 16:31:47 +0000</pubDate></item><item><title>Reverse engineering Lyft Bikes for fun (and profit?)</title><link>https://ilanbigio.com/blog/lyft-bikes.html</link><description>&lt;doc fingerprint="b77dcf596807b0b5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Reverse engineering Lyft bikes for fun (and profit?)&lt;/head&gt;
    &lt;p&gt;One cold San Francisco summer morning in Haight-Ashbury, my commute down to Market was interrupted by the sight of a lucky duck taking the last Lyft bike – again.&lt;/p&gt;
    &lt;p&gt;"I should really just wake up 15 minutes earlier", I thought, fleetingly. Then instead proceeded to spend the next month reverse engineering Lyft's private API, bypassing SSL encryption, chasing loose bikes across the city, triggering an internal incident, and somehow making a profit.&lt;/p&gt;
    &lt;p&gt;I learned a lot doing this, so I'm writing it up in case you might too.&lt;/p&gt;
    &lt;head&gt;Technical summary, for the impatient (spoilers!)&lt;/head&gt;
    &lt;p&gt;Goal: Remotely unlock a Lyft bike.&lt;/p&gt;
    &lt;p&gt;Steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Capturing iOS App Encrypted Traffic – to re-construct request&lt;/item&gt;
      &lt;item&gt;Replaying Modified Unlock Request – to bypass geofence&lt;/item&gt;
      &lt;item&gt;Brute-forcing Bike ID – since not available remotely&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Capturing iOS App Encrypted Traffic&lt;/head&gt;
    &lt;p&gt;I used Charles Proxy to capture outgoing requests from the Lyft app on my iPhone.&lt;/p&gt;
    &lt;p&gt;Charles supports &lt;code&gt;SSL Proxying&lt;/code&gt;, which injects its own
ephemeral certificates during SSL handshake, making sure requests from
both sides are being signed with keys it controls. &lt;/p&gt;
    &lt;p&gt;This allows us to decrypt, read, and re-encrypt traffic in transit.&lt;/p&gt;
    &lt;p&gt;The ephemeral certificates are signed by a Charles Certificate Authority, which needs to be installed on your phone so Charles's certificates are not rejected. SSL traffic content is then viewable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Replaying Modified Unlock Request&lt;/head&gt;
    &lt;p&gt;From the Charles captures, we see the unlock request uses a &lt;code&gt;rent&lt;/code&gt; endpoint with the following structure:&lt;/p&gt;
    &lt;code&gt;POST "https://layer.bicyclesharing.net/mobile/v2/fgb/rent"

HEADERS
{
  "api-key": "sk-XXXXX",
  "authorization": "bearer-XXXXX",
  ...
}

DATA
{
  "userLocation": { "lat": 37.7714859, "lon": -122.4449036 },
  "qrCode": { "memberId": "user-XXXXX", "qrCode": "12345" },
  ...
}&lt;/code&gt;
    &lt;p&gt;A simple python replay script:&lt;/p&gt;
    &lt;code&gt;import requests

url="https://layer.bicyclesharing.net/mobile/v2/fgb/rent"

headers={
  "api-key": "sk-XXXXX",
  "authorization": "bearer-XXXXX",
}

station_coords = { "lat": 37.7730627, "lon": -122.4390777 }    # from maps
bike_id = "12345"                                              # dummy id

data={
  "userLocation": station_coords,
  "qrCode": { "memberId": "user-XXXXX", "qrCode":  bike_id},
}

requests.post(url, headers=headers, json=data)&lt;/code&gt;
    &lt;head rend="h2"&gt;Brute-forcing Bike ID&lt;/head&gt;
    &lt;p&gt;Bike IDs are only accessible through the physical bikes (not counting eBikes, which were out of scope), to unlock one remotely, we need to brute force it. Five-digit IDs, but in practice only the &lt;code&gt;10000&lt;/code&gt; to &lt;code&gt;20000&lt;/code&gt; range is used, so 10,000 IDs to
try.&lt;/p&gt;
    &lt;p&gt;A naive implementation takes ~3 hours:&lt;/p&gt;
    &lt;code&gt;def payload(i):
    return {
        "userLocation": station_coords,
        "qrCode": { "memberId": "mem123", "qrCode":  i},
    }

def send_one(i):
    requests.post(url, headers=headers, json=payload(i))

for i in range(10_000, 20_000):
    send_one(i)&lt;/code&gt;
    &lt;p&gt;But we can use &lt;code&gt;asyncio&lt;/code&gt; and &lt;code&gt;aiohttp&lt;/code&gt; to
reduce that to ~15 seconds:&lt;/p&gt;
    &lt;code&gt;import asyncio, aiohttp

async def send_one(session, i):                              # non-blocking
  async with session.post(url, headers=headers, json=payload(i)): pass

async def main():
  async with aiohttp.ClientSession() as s:
    tasks = [send_one(s, i) for i in range(10_000, 20_000)]  # start all
    await asyncio.gather(*tasks)                             # wait for all

asyncio.run(main())&lt;/code&gt;
    &lt;p&gt;Et voilà.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Disclaimer: This writeup is meant for educational purposes only. Vulnerabilities discussed here were disclosed to Lyft in 2019, who promptly responded and patched them. Not long after, they also introduced bike reservations as an official feature, solving my original problem and rendering the below techniques obsolete.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Table Of Contents&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Acquisition&lt;/item&gt;
      &lt;item&gt;Intercepting iOS App Requests&lt;/item&gt;
      &lt;item&gt;Spoofing SSL Root Certificate Authorities&lt;/item&gt;
      &lt;item&gt;Anatomy of a Lyft Request&lt;/item&gt;
      &lt;item&gt;I Promise it's not a Denial of Service Attack&lt;/item&gt;
      &lt;item&gt;The Test&lt;/item&gt;
      &lt;item&gt;The Good Days&lt;/item&gt;
      &lt;item&gt;Hacker One&lt;/item&gt;
      &lt;item&gt;Closing Thoughts&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The Acquisition&lt;/head&gt;
    &lt;p&gt;Back in 2019 Lyft Bikes (BayWheels) used to be called Ford GoBikes, and used to be unlocked on a per-station basis. You'd generate a temporary code for a specific station on your app, then punch it into that station which would release a random bike.&lt;/p&gt;
    &lt;p&gt;My goal was to make sure nobody would take a bike while I was on-route to the station, so what if I just kept manually generating codes until I arrived? Maybe that might block others from doing so. So I tried it. No luck. Generating a code didn't block others, and that was the only way to unlock bikes. Welp, nothing left to try...&lt;/p&gt;
    &lt;p&gt;...until the next day when Lyft, who had apparently just acquired Ford GoBikes, rebranded it to BayWheels, and changed the whole unlock mechanism. All hail Lyft.&lt;/p&gt;
    &lt;p&gt;The new BayWheels map also showed bikes at stations, but now you'd unlock a bike directly by scanning a QR code on it. Each bike also had a 5-digit number you could use in case scanning didn't work. Cool! This means maybe if I typed a bike's code into my app when I left my house, it would be unlocked (and hopefully still there) by the time I arrived? So I tried it.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;You are too far from this station.&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;They had geofenced it. I spent a solid day Googling how to spoof GPS on iPhone but no luck. I then wondered, fatefully, "what does the app actually send to Lyft during an unlock?", and my journey of capturing encrypted iOS traffic began.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intercepting iOS App Requests&lt;/head&gt;
    &lt;p&gt;If you've used Chrome DevTools (aka &lt;code&gt;Inspect Element&lt;/code&gt;) you may have noticed a
&lt;code&gt;Network&lt;/code&gt; tab that lets you see the traffic between a website
and its backend. Unfortunately it's not so simple for iOS. Some helpful
Reddit posts led me to Charles
Proxy which lets you see all traffic from your computer,
and a friendly eight
sentences explained how to wire it up to my phone's traffic. It's
basically a consensual man-in-the-middle
attack.&lt;/p&gt;
    &lt;p&gt;First I had to forward my phone's traffic to Charles on my laptop. To do this I enabled "HTTP Proxy" on my phone's wifi settings, and set the &lt;code&gt;[ip]:[port]&lt;/code&gt; to &lt;code&gt;192.168.0.7:8888&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;192.168.0.7&lt;/code&gt;is my laptop's local IP which I got by running&lt;code&gt;ipconfig getifaddr en0&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;8888&lt;/code&gt;is the port Charles Proxy is running on&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now my traffic was being forwarded to Charles Proxy and huzzah! I could see all requests coming out of my phone. But... I can't see the content? Oh, right. SSL1 encryption. The thing making sure we can trust the internet was getting in my way.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spoofing SSL Root Certificate Authorities&lt;/head&gt;
    &lt;p&gt;SSL ensures traffic from the Lyft app is encrypted using the &lt;code&gt;lyft.com&lt;/code&gt; public key, so only &lt;code&gt;lyft.com&lt;/code&gt; can
decrypt it2. All modern applications &amp;amp;
websites do this, and you can find the public key on a website's SSL
certificate.&lt;/p&gt;
    &lt;p&gt;In theory, this means my traffic can't be decrypted once it leaves my phone, even by me. However, Charles has a workaround: by enabling &lt;code&gt;SSL Proxying&lt;/code&gt;, Charles will prevent the real
&lt;code&gt;lyft.com&lt;/code&gt; SSL certificate from making it back to your phone,
and instead sends a new one it generates on the fly.&lt;/p&gt;
    &lt;p&gt;This means your phone is now encrypting &lt;code&gt;lyft.com&lt;/code&gt; traffic
with Charles's public key, so Charles can decrypt it, save it, then
re-encrypt it with the real &lt;code&gt;lyft.com&lt;/code&gt; cert and
forward it along.&lt;/p&gt;
    &lt;p&gt;But there's a catch – if anyone between you and Lyft can do this (coffee shop, cell provider, etc.), how can we ever know the certificate is really Lyft's? Well, your phone will reject a certificate unless it's been signed by a Certificate Authority, endorsing it actually belongs to &lt;code&gt;lyft.com&lt;/code&gt;. These CAs are third-party organizations acting
like notaries that issue "root certificates", and your phone comes with
many trusted CA root certificates pre-installed. So Charles just asks
you to install one more root certificate – the Charles Root Certificate,
used to sign all the other certificates Charles creates. And just like
that, my phone trusts Charles, and I can see SSL traffic. 3&lt;/p&gt;
    &lt;head rend="h2"&gt;Anatomy of a Lyft Request&lt;/head&gt;
    &lt;p&gt;So, let's unlock a bike from my phone, shall we?&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;Vehicle not found.&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Right. Well, I used &lt;code&gt;12345&lt;/code&gt; as the bike ID, so that's
expected. Charles managed to capture some traffic anyway. Let's see if
we can find the unlock request.&lt;/p&gt;
    &lt;p&gt;So many requests and API routes! I see one endpoint called &lt;code&gt;rent&lt;/code&gt;, which I bet is the unlock request. Let’s look at the
contents.&lt;/p&gt;
    &lt;code&gt;POST "https://layer.bicyclesharing.net/mobile/v2/fgb/rent"

HEADERS
{
  "api-key": "sk-XXXXX",
  "authorization": "bearer-XXXXX",
  ...
}

DATA
{
  "userLocation": { "lat": 37.7714859, "lon": -122.4449036 },
  "qrCode": { "memberId": "user-XXXXX", "qrCode": "12345" },
  ...
}&lt;/code&gt;
    &lt;p&gt;Yup, looks like it is. Look at these fields (I omitted the unrelated, redacted the sensitive.) There's some auth in the headers (&lt;code&gt;api-key&lt;/code&gt;, &lt;code&gt;authorization&lt;/code&gt;), the bike
&lt;code&gt;qrCode&lt;/code&gt; I used (&lt;code&gt;12345&lt;/code&gt;), a &lt;code&gt;memberId&lt;/code&gt;
which I assume identifies my account, and... &lt;code&gt;userLocation&lt;/code&gt;
coordinates! Bingo. Now I just need to replay that request with a python
script, but set the &lt;code&gt;lat&lt;/code&gt;, &lt;code&gt;lon&lt;/code&gt; to be right next
to the station (whose coordinates I got using google maps).&lt;/p&gt;
    &lt;code&gt;import requests

url="https://layer.bicyclesharing.net/mobile/v2/fgb/rent"

headers={
  "api-key": "sk-XXXXX",
  "authorization": "bearer-XXXXX",
}

station_coords = { "lat": 37.7730627, "lon": -122.4390777 }    # from maps
bike_id = "12345"                                              # dummy id

data={
  "userLocation": station_coords,
  "qrCode": { "memberId": "user-XXXXX", "qrCode":  bike_id},
}

requests.post(url, headers=headers, json=data)&lt;/code&gt;
    &lt;p&gt;Sweet, now I just needed a real &lt;code&gt;bike_id&lt;/code&gt; to test it on.
It was very late at night4 but I was excited, so out I went
with my PJs, flip flops, and laptop to squat by my target bike. I found
its ID, entered it into my script, hit run, and holy shit it worked. The
bike unlocked. I re-locked it, ran back to my apartment, hit run again,
ran back, and there she was. Unlocked, and inconspicuously so. Nobody
would think to take it… but me.&lt;/p&gt;
    &lt;p&gt;I was in business.&lt;/p&gt;
    &lt;head rend="h2"&gt;I Promise it’s not a Denial of Service Attack&lt;/head&gt;
    &lt;p&gt;Except… the bike IDs are only printed on the bikes. How would I know what &lt;code&gt;bike_id&lt;/code&gt; to use without going to the station? Maybe
some other request Charles captured might have all the bike IDs at a
station? Short answer – no. Two days of digging through captured traffic
yielded no way to fetch bike IDs.5&lt;/p&gt;
    &lt;p&gt;After considering many fruitless ideas, like hiding a little camera pointed at the bikes and using OCR, I thought… could I just try all IDs? Five digits, that’s 100,000 combinations… and thinking back, I had only seen IDs between &lt;code&gt;10000&lt;/code&gt; and &lt;code&gt;20000&lt;/code&gt;. 10,000 loop
iterations is not that many for python!&lt;/p&gt;
    &lt;p&gt;This runs in less than a second:&lt;/p&gt;
    &lt;code&gt;for i in range(10_000, 20_000):
    print(i)&lt;/code&gt;
    &lt;p&gt;This, however, takes ~a second per request. So… ~three hours for 10,000 requests.&lt;/p&gt;
    &lt;code&gt;def payload(i):
    return {
        "userLocation": station_coords,
        "qrCode": { "memberId": "mem123", "qrCode":  i},
    }

def send_one(i):
    requests.post(url, headers=headers, json=payload(i))

for i in range(10_000, 20_000):
    send_one(i)&lt;/code&gt;
    &lt;p&gt;But we don’t have to wait for each request to come back – we can run them in parallel. After trying &lt;code&gt;multiprocessing&lt;/code&gt; and
&lt;code&gt;threading&lt;/code&gt;, I massaged a stack overflow code snippet I found
using &lt;code&gt;aiohttp&lt;/code&gt; to start a bunch of requests without blocking
on a response. Here’s a slightly6 simplified version.&lt;/p&gt;
    &lt;code&gt;import asyncio, aiohttp

async def send_one(session, i):                              # non-blocking
  async with session.post(url, headers=headers, json=payload(i)): pass

async def main():
  async with aiohttp.ClientSession() as s:
    tasks = [send_one(s, i) for i in range(10_000, 20_000)]  # start all
    await asyncio.gather(*tasks)                             # wait for all

asyncio.run(main())&lt;/code&gt;
    &lt;p&gt;I benchmarked this against Postman's API (meant for testing) and it ran in 15 seconds! That's ~650 RPS. But, hm… is that too much for their servers? In April 2019 there were about 9,000 trips per day, so even if 80% of those all happened during rush hour (8-10am, 5-7pm) that's still a whopping 0.5 RPS at its peak. I'd be single-handedly 1,300x-ing their peak traffic on this endpoint. To be fair, (Google informed me,) 650 RPS is not that crazy for most servers. But a sudden spike like that might still look to Lyft like a Denial-of-Service attack...&lt;/p&gt;
    &lt;p&gt;...which it's not. Let me know if this is an issue and I'll stop.&lt;/p&gt;
    &lt;p&gt;I'm just a student, please don't call the cops.&lt;/p&gt;
    &lt;p&gt;Sincerely, Ilan&lt;/p&gt;
    &lt;p&gt;Aaaand sent7. Ok now let's run it on all the IDs.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Test&lt;/head&gt;
    &lt;p&gt;I'm about to run &lt;code&gt;python unlock_script.py&lt;/code&gt; when a thought
occurs to me: Is there any chance, however slim, that I'm about
to unlock every single Lyft Bike in and around the Bay Area? The
geofence should prevent that, in theory. Only the station at my
selected coordinates should respond. But what if it fails? What if– eh
screw it, let's live a little. 8&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;Enter ⏎&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;10,000 IDs fly through my screen.&lt;/p&gt;
    &lt;p&gt;1000 not-so-milli seconds tick by. Then, I get my first blessed&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;You are too far from this station.&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Then another. They start to trickle in, slowly at first, then suddenly flood my terminal. Ok, that's a good sign. I'm not actively unlocking the whole city. Then, another second or so pass by, until...&lt;/p&gt;
    &lt;code&gt;Bike 12539 unlocked&lt;/code&gt;
    &lt;p&gt;Hellllll yeah! Oh my freaking god it worked! It actually wor–&lt;/p&gt;
    &lt;code&gt;Bike 17322 unlocked
Done.&lt;/code&gt;
    &lt;p&gt;Wait, what? Um. Ok? Two bikes got unlocked? That's strange, my Lyft membership only allows me to unlock one bike at a time. But hey, a) I didn't unlock the whole city, and b) it worked, so what do I care. Let's go find my bikes.&lt;/p&gt;
    &lt;p&gt;And there they were. Resting peacefully in their docks, but secretly not actually locked. If someone tried scanning it, they'd just see an error and try a different one. I had accomplished what I had set out to do.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Good Days&lt;/head&gt;
    &lt;p&gt;And boy was it nice. Every morning I'd wake up, get ready for work, run my script, glance at the unlocked ID (sometimes two), leisurely stroll to the station, (re-lock the second bike if necessary), and be on my merry way.&lt;/p&gt;
    &lt;p&gt;I mostly kept this to myself, and a few trusted people including my parents, who were happy for me but nervous that I was now certainly a criminal waiting to be arrested.&lt;/p&gt;
    &lt;p&gt;But what fun, and what a pleasantly happy ending to this adventure.&lt;/p&gt;
    &lt;p&gt;What.&lt;/p&gt;
    &lt;p&gt;Oh no.&lt;/p&gt;
    &lt;p&gt;Oh no.&lt;/p&gt;
    &lt;p&gt;Panic? Panic.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hacker One&lt;/head&gt;
    &lt;p&gt;I think I spent ~two and a half minutes hyperventilating before I decided to start using my brain. I hadn't intended for this to cause an issue for Lyft: I had done the math, sanity checked with Google, and even let them know in advance. Even still, this could be interpreted maliciously and it'd be nice not to get arrested. So... what to do?&lt;/p&gt;
    &lt;p&gt;Well, how do hackers avoid getting arrested? Responsible disclosure! Companies will give bounties to people who report vulnerabilities, so hackers can keep hacking legally, and companies get to fix issues. Win-win! And maybe, just maybe, this might keep me out of jail. Win-win-win!&lt;/p&gt;
    &lt;p&gt;So I found HackerOne, and immediately a problem: Lyft's vulnerability disclosure guidelines state brute-force approaches aren't eligible. In reality, my approach wasn't bypassing anything at all – I was still unlocking a bike and paying like normal. No bugs to be reported. Although... the second bike! Definitely not normal behavior, and I wasn't getting charged for it. Let's hope it's enough to show I come in peace.&lt;/p&gt;
    &lt;p&gt;Summary: This vulnerability is specifically for the BayWheels bike sharing service. By brute-forcing the https://layer.bicyclesharing.net/mobile/v2/fgb/rent endpoint, an attacker is able to unlock more than one bicycle at a given station.&lt;/p&gt;
    &lt;p&gt;Proof of Concept: Trivial.&lt;/p&gt;
    &lt;p&gt;Steps To Reproduce:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Locate relevant auth info (api-key and authorization code) from downloaded app (possibly using Charles proxy MitM).&lt;/item&gt;
      &lt;item&gt;Discover rent endpoint (also using Charles proxy).&lt;/item&gt;
      &lt;item&gt;Quickly send rent requests for all possible bike IDs.&lt;/item&gt;
      &lt;item&gt;Retrieve bike.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Impact: An attacker could unlock more than one bike without having to go through the paywall.&lt;/p&gt;
    &lt;p&gt;(Yes I'm embarrassed to say I did actually write "trivial" because I was nervous about sharing my code.)&lt;/p&gt;
    &lt;p&gt;And now we wait. Except by sheer luck9, my summer roommate was also working at Lyft (unrelated to the intern friend who messaged me), and found the thread discussing my vulnerability report. Even though Lyft could have labeled my submission ineligible due to my somewhat... unorthodox methods, their security team treated my newbie submission seriously, asked a couple follow-ups, and eventually decided to make an exception and award me a bounty of $250. They even threw in an extra $250 bonus for a "good report"!&lt;/p&gt;
    &lt;p&gt;My goodness $500 is better than jail.&lt;/p&gt;
    &lt;p&gt;In the end I did what any (relieved, not arrested) student would do with a surprise $500 and threw an absolutely stocked little house party...&lt;/p&gt;
    &lt;p&gt;...and, naturally, invited the Lyft interns.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing Thoughts&lt;/head&gt;
    &lt;p&gt;Two-bike unlock: While I never got confirmation, I believe the two-bike unlock issue was ultimately a race condition in the &lt;code&gt;rent&lt;/code&gt; endpoint. Given the
&lt;code&gt;layer.bicyclesharing.net&lt;/code&gt; URL, I'm guessing Lyft inherited
some legacy code during the Ford GoBikes which did not correctly handle
multiple simultaneous requests from the same user. I expect they have
since migrated these endpoints to their own first-party (likely more
modern) backend.&lt;/p&gt;
    &lt;p&gt;Geofence bypass: As far as I understand, there's no easy way to enforce a geofence server-side other than timing, consistency, etc. You sort of just have to trust whatever the phone tells you.&lt;/p&gt;
    &lt;p&gt;So what did I learn?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Even scary "physical" systems have digital interfaces you may recognize.&lt;/item&gt;
      &lt;item&gt;There's few better ways to learn about a system than reverse engineering.&lt;/item&gt;
      &lt;item&gt;Curiosity and determination are unreasonably effective, and seem to create luck.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you made it to the end, you might just enjoy this stuff. So I leave you with homework: Go reverse engineer something. Just, be nice about it.&lt;/p&gt;
    &lt;p&gt;Happy hacking.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;TLS, SSL. Tomato, potato.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Slight simplification. In reality, SSL asymmetric keys are slow, so they're only used during the handshake to encrypt faster symmetric keys, which then encrypt everything else.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Even after this setup, I noticed some encrypted content was still not readable. Turns out some apps use Certificate Pinning, which means they come pre-installed with the server certificate they expect. So even if Charles intercepts the handshake, they will encrypt traffic with the pinned certificate. Pinned certs are a PAIN to get around. Glad I didn't have to.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;10:30pm in San Francisco.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Except for eBikes, which were very few at this point, and which flash very conspicuously when unlocked as I found out after having to trek to re-lock one I accidentally unlocked across the city during testing.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I also used semaphores to limit concurrency, but they make the code harder to follow.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Maybe a slightly more professional version.↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I did, in fact, test this out with smaller ID ranges to convince myself I wasn't unlocking bikes at other stations. But I had never run the full-range test so I was still nervous (and it sounds more exciting this way.)↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Or cosmic fate.↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I may write something every few months or so. Who knows? (You might.)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721703</guid><pubDate>Thu, 22 Jan 2026 16:45:52 +0000</pubDate></item><item><title>Show HN: isometric.nyc – giant isometric pixel art map of NYC</title><link>https://cannoneyed.com/isometric-nyc/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721802</guid><pubDate>Thu, 22 Jan 2026 16:52:35 +0000</pubDate></item><item><title>AnswerThis (YC F25) Is Hiring</title><link>https://www.ycombinator.com/companies/answerthis/jobs/r5VHmSC-ai-agent-orchestration</link><description>&lt;doc fingerprint="3d09585cf826800c"&gt;
  &lt;main&gt;
    &lt;p&gt;End-to-end workspace to accelerate scientific discovery&lt;/p&gt;
    &lt;p&gt;We crossed $1M ARR in 8 months. 200,000+ researchers at Stanford, MIT, and Amazon use us to do literature reviews 10x faster.&lt;lb/&gt; Now we're building something bigger: the system of record for scientists where they can find papers, analyze experiments, and write their drafts while collaborating with other scientists as well as our AI agents. &lt;lb/&gt; You should apply if you:&lt;lb/&gt; → Ship fast and learn faster &lt;lb/&gt; → Know the agentic AI stack cold (vector DBs, graph RAG, agent memory) &lt;lb/&gt; → Have built full-stack products that scaled past 1M users &lt;lb/&gt; → Actually care about accelerating scientific discovery&lt;lb/&gt; Bonus: You've published research yourself. &lt;lb/&gt; Don't apply if you:&lt;lb/&gt; → Can't be in SF, in person &lt;lb/&gt; → Haven't used the product yet &lt;lb/&gt; → Don't want to talk to customers &lt;lb/&gt; $120K-$200K + equity. We're a small team backed by YC. &lt;lb/&gt; Reach out on careers [at] answerthis.io&lt;lb/&gt; Tell us what you hate about AnswerThis, what you love, and one project you're proud of alongside your resume.&lt;lb/&gt; Science moves too slowly. Help us fix that.&lt;/p&gt;
    &lt;p&gt;We move fast. The whole process can be done in 2-3 weeks.&lt;/p&gt;
    &lt;p&gt;AnswerThis is building the system of record for scientists—where researchers can find papers, analyze experiments, and write drafts while collaborating with other scientists and AI agents.&lt;/p&gt;
    &lt;p&gt;We crossed $1M ARR in 8 months. 200,000+ researchers at Stanford, MIT, Amazon, and top institutions worldwide use us daily. We're backed by Y Combinator (F25) and cash-flow positive.&lt;/p&gt;
    &lt;p&gt;Science moves too slowly. Grant applications take months. Literature reviews take weeks. Researchers spend more time on paperwork than on discovery. We're fixing that.&lt;/p&gt;
    &lt;p&gt;You'll be joining a small, fast team in SF that ships constantly and talks to customers every day.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721897</guid><pubDate>Thu, 22 Jan 2026 17:00:40 +0000</pubDate></item><item><title>Launch HN: Constellation Space (YC W26) – AI for satellite mission assurance</title><link>https://news.ycombinator.com/item?id=46721933</link><description>&lt;doc fingerprint="22906374e6a95fca"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN! We're Kamran, Raaid, Laith, and Omeed from Constellation Space (&lt;/p&gt;https://constellation-io.com/&lt;p&gt;). We built an AI system that predicts satellite link failures before they happen. Here's a video walkthrough: &lt;/p&gt;https://www.youtube.com/watch?v=069V9fADAtM&lt;p&gt;.&lt;/p&gt;&lt;p&gt;Between us, we've spent years working on satellite operations at SpaceX, Blue Origin, and NASA. At SpaceX, we managed constellation health for Starlink. At Blue, we worked on next-gen test infra for New Glenn. At NASA, we dealt with deep space communications. The same problem kept coming up: by the time you notice a link is degrading, you've often already lost data.&lt;/p&gt;&lt;p&gt;The core issue is that satellite RF links are affected by dozens of interacting variables. A satellite passes overhead, and you need to predict whether the link will hold for the next few minutes. That depends on: the orbital geometry (elevation angle changes constantly), tropospheric attenuation (humidity affects signal loss via ITU-R P.676), rain fade (calculated via ITU-R P.618 - rain rates in mm/hr translate directly to dB of loss at Ka-band and above), ionospheric scintillation (we track the KP index from magnetometer networks), and network congestion on top of all that.&lt;/p&gt;&lt;p&gt;The traditional approach is reactive. Operators watch dashboards, and when SNR drops below a threshold, they manually reroute traffic or switch to a backup link. With 10,000 satellites in orbit today and 70,000+ projected by 2030, this doesn't scale. Our system ingests telemetry at around 100,000 messages per second from satellites, ground stations, weather radar, IoT humidity sensors, and space weather monitors. We run physics-based models in real-time - the full link budget equations, ITU atmospheric standards, orbital propagation - to compute what should be happening. Then we layer ML models on top, trained on billions of data points from actual multi-orbit operations.&lt;/p&gt;&lt;p&gt;The ML piece is where it gets interesting. We use federated learning because constellation operators (understandably) don't want to share raw telemetry. Each constellation trains local models on their own data, and we aggregate only the high-level patterns. This gives us transfer learning across different orbit types and frequency bands - learnings from LEO Ka-band links help optimize MEO or GEO operations. We can predict most link failures 3-5 minutes out with &amp;gt;90% accuracy, which gives enough time to reroute traffic before data loss. The system is fully containerized (Docker/Kubernetes) and deploys on-premise for air-gapped environments, on GovCloud (AWS GovCloud, Azure Government), or standard commercial clouds.&lt;/p&gt;&lt;p&gt;Right now we're testing with defense and commercial partners. The dashboard shows real-time link health, forecasts at 60/180/300 seconds out, and root cause analysis (is this rain fade? satellite setting below horizon? congestion?). We expose everything via API - telemetry ingestion, predictions, topology snapshots, even an LLM chat endpoint for natural language troubleshooting.&lt;/p&gt;&lt;p&gt;The hard parts we're still working on: prediction accuracy degrades for longer time horizons (beyond 5 minutes gets dicey), we need more labeled failure data for rare edge cases, and the federated learning setup requires careful orchestration across different operators' security boundaries. We'd love feedback from anyone who's worked on satellite ops, RF link modeling, or time-series prediction at scale. What are we missing? What would make this actually useful in a production NOC environment?&lt;/p&gt;&lt;p&gt;Happy to answer any technical questions!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46721933</guid><pubDate>Thu, 22 Jan 2026 17:03:21 +0000</pubDate></item><item><title>Show HN: First Claude Code client for Ollama local models</title><link>https://github.com/21st-dev/1code</link><description>&lt;doc fingerprint="423d0c5e65bff342"&gt;
  &lt;main&gt;
    &lt;p&gt;Best UI for Claude Code with local and remote agent execution.&lt;/p&gt;
    &lt;p&gt;By 21st.dev team&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Platforms: macOS, Linux, and Windows. Windows support improved thanks to community contributions from @jesus-mgtc and @evgyur.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;1Code&lt;/cell&gt;
        &lt;cell role="head"&gt;Claude Code&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Visual UI&lt;/cell&gt;
        &lt;cell&gt;✅ Cursor-like desktop app&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Git Worktree Isolation&lt;/cell&gt;
        &lt;cell&gt;✅ Each chat runs in isolated worktree&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Background Execution&lt;/cell&gt;
        &lt;cell&gt;✅ Run multiple agents in parallel&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Built-in Git Client&lt;/cell&gt;
        &lt;cell&gt;✅ Visual staging, commits, branches&lt;/cell&gt;
        &lt;cell&gt;❌ CLI git commands only&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Integrated Terminal&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Plan Mode&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MCP Support&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Memory (CLAUDE.md)&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Skills &amp;amp; Slash Commands&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Custom Subagents&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Subscription &amp;amp; API Key Support&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Custom Models &amp;amp; Providers (BYOK)&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Checkpointing&lt;/cell&gt;
        &lt;cell&gt;🚧 Beta&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tool Approve&lt;/cell&gt;
        &lt;cell&gt;📋 Backlog&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Hooks&lt;/cell&gt;
        &lt;cell&gt;❌&lt;/cell&gt;
        &lt;cell&gt;✅&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Run agents locally, in worktrees, in background — without touching main branch.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Git Worktree Isolation - Each chat session runs in its own isolated worktree&lt;/item&gt;
      &lt;item&gt;Background Execution - Run agents in background while you continue working&lt;/item&gt;
      &lt;item&gt;Local-first - All code stays on your machine, no cloud sync required&lt;/item&gt;
      &lt;item&gt;Branch Safety - Never accidentally commit to main branch&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cursor-like UI for Claude Code with diff previews, built-in git client, and the ability to see changes before they land.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Diff Previews - See exactly what changes Claude is making in real-time&lt;/item&gt;
      &lt;item&gt;Built-in Git Client - Stage, commit, and manage branches without leaving the app&lt;/item&gt;
      &lt;item&gt;Change Tracking - Visual diffs and PR management&lt;/item&gt;
      &lt;item&gt;Real-time Tool Execution - See bash commands, file edits, and web searches as they happen&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Claude asks clarifying questions, builds structured plans, and shows clean markdown preview — all before execution.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clarifying Questions - Claude asks what it needs to know before starting&lt;/item&gt;
      &lt;item&gt;Structured Plans - See step-by-step breakdown of what will happen&lt;/item&gt;
      &lt;item&gt;Clean Markdown Preview - Review plans in readable format&lt;/item&gt;
      &lt;item&gt;Review Before Execution - Approve or modify the plan before Claude acts&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Plan &amp;amp; Agent Modes - Read-only analysis or full code execution permissions&lt;/item&gt;
      &lt;item&gt;Project Management - Link local folders with automatic Git remote detection&lt;/item&gt;
      &lt;item&gt;Integrated Terminal - Full terminal access within the app&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Prerequisites: Bun, Python, Xcode Command Line Tools (macOS)
bun install
bun run claude:download  # Download Claude binary (required!)
bun run build
bun run package:mac  # or package:win, package:linux&lt;/code&gt;
    &lt;quote&gt;&lt;p&gt;Important: The&lt;/p&gt;&lt;code&gt;claude:download&lt;/code&gt;step downloads the Claude CLI binary which is required for the agent chat to work. If you skip this step, the app will build but agent functionality won't work.&lt;/quote&gt;
    &lt;p&gt;Get pre-built releases + background agents support by subscribing at 1code.dev.&lt;/p&gt;
    &lt;p&gt;Your subscription helps us maintain and improve 1Code.&lt;/p&gt;
    &lt;code&gt;bun install
bun run claude:download  # First time only
bun run dev&lt;/code&gt;
    &lt;p&gt;Join our Discord for support and discussions.&lt;/p&gt;
    &lt;p&gt;Apache License 2.0 - see LICENSE for details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46722285</guid><pubDate>Thu, 22 Jan 2026 17:26:12 +0000</pubDate></item><item><title>CSS Optical Illusions</title><link>https://alvaromontoro.com/blog/68091/css-optical-illusions</link><description>&lt;doc fingerprint="860c29efdbe599e5"&gt;
  &lt;main&gt;
    &lt;p&gt;You can find a collection with all the optical illusions in this article (and more!) on CodePen. You can move your mouse over many of the demos below to reveal the effect or stop the animations.&lt;/p&gt;
    &lt;head rend="h2"&gt;1 - Poggendorff Illusions&lt;/head&gt;
    &lt;p&gt;The Poggendorff illusion is an optical illusion in which a diagonal line interrupted by a vertical bar appears misaligned, even when both segments are actually continuous.&lt;/p&gt;
    &lt;p&gt;A simple version of this effect can be seen in the following demo. I used the &lt;code&gt;::before&lt;/code&gt; and &lt;code&gt;::after&lt;/code&gt; pseudo-elements to create the diagonal line and the vertical bar, respectively.&lt;/p&gt;
    &lt;p&gt;The effect can also be seen in a more elaborate version with multiple diagonal lines and vertical bars:&lt;/p&gt;
    &lt;p&gt;This drawing can easily be achieved using two CSS gradients: one tilted at 70 degrees and another consisting of a series of vertical columns. I applied it to the &lt;code&gt;body&lt;/code&gt;, although I could have used &lt;code&gt;:root&lt;/code&gt; instead.&lt;/p&gt;
    &lt;p&gt;Another variation of this illusion is the Münsterberg Poggendorff Arch, in which the two sides of an arch appear misaligned and seem as though they will not meet at the top - but they do (mouse over to see it).&lt;/p&gt;
    &lt;head rend="h2"&gt;2 - Induced Gradients&lt;/head&gt;
    &lt;p&gt;The following illusions combine gradients and flat colors. Surprisingly, some of the gradients do not actually exist. They are simple gray bars that, when placed over a gradient, appear to have gradients themselves.&lt;/p&gt;
    &lt;p&gt;Take the following demo: all three bars (two vertical ones on the sides and one horizontal bar in the center) are the same shade of gray. The only real gradient is behind them, which tricks our brain into believing that the bars are different colors and even contain gradients.&lt;/p&gt;
    &lt;p&gt;Here is another variation of this effect. It looks like the central line has a repeating gradient of dark and light grays, but in reality it is a flat color. If you mouse over the demo, the bar will expand, making it clear that there is no gradient at all.&lt;/p&gt;
    &lt;head rend="h2"&gt;3 - Cornsweet Illusion&lt;/head&gt;
    &lt;p&gt;The next few optical illusions share a common idea: some colors are identical, but they do not look the same. This typically happens when regions of the same color or brightness are surrounded by areas with different contrast.&lt;/p&gt;
    &lt;p&gt;For example, in the following demo, the left and right ends are the same shade of gray. However, one looks lighter because it is closer to white, while the other looks darker because it is closer to black. Mouse over to reveal that they are, in fact, the same color.&lt;/p&gt;
    &lt;head rend="h2"&gt;4 - White's Illusion&lt;/head&gt;
    &lt;p&gt;Run the following demo. You will see two gray columns in a black-and-white grid. Both columns are the same shade of gray, but the one surrounded by black appears darker than the one surrounded by white.&lt;/p&gt;
    &lt;p&gt;I coded this demo using &lt;code&gt;mix-blend-mode&lt;/code&gt; so I could try something a bit different. That worked well, but it also made it harder to showcase the effect on hover. In hindsight, I should have planned that better.&lt;/p&gt;
    &lt;p&gt;This optical illusion also works with colors. For example, these two squares appear to be different shades of blue, but they are the same color. This time, you can mouse over to reveal the effect:&lt;/p&gt;
    &lt;head rend="h2"&gt;5 - Wertheimer-Koffka Ring&lt;/head&gt;
    &lt;p&gt;The ring in the following illustration has the same color all the way around. However, one side is placed over white and the other over black, which makes them look different. If you mouse over the demo, the red bar will disappear, making it more obvious that the ring is a single, uniform color.&lt;/p&gt;
    &lt;head rend="h2"&gt;6 - Adelson's Illusion&lt;/head&gt;
    &lt;p&gt;You have probably seen the illusion involving a checkerboard and an object casting a shadow, where two tiles - one seemingly light and one seemingly dark - turn out to be the same color.&lt;/p&gt;
    &lt;p&gt;This demo follows the same principle. You will see two tiles labeled A and B. Both have the same shade of gray, but most people cannot tell at first glance (or second, or even third).&lt;/p&gt;
    &lt;head rend="h2"&gt;7 - Asahi illusion of Brightness&lt;/head&gt;
    &lt;p&gt;The circle at the center of this flower-shaped element is the same white as the rest of the page, but it gives the impression of being brighter, as if it were emitting light.&lt;/p&gt;
    &lt;head rend="h2"&gt;8 - Color Spheres&lt;/head&gt;
    &lt;p&gt;This is one of my favorite illusions in the collection. The circles (or spheres) look red, blue, or green, but in reality they are all the same grayish color. Our brain "colorizes" them based on the lines that overlap the shapes. Don't believe it? Mouse over the illustration.&lt;/p&gt;
    &lt;head rend="h2"&gt;9 - Colors from Contour&lt;/head&gt;
    &lt;p&gt;In the following illustration, the lines inside the yellow section appear blue, while the lines inside the blue section appear red... but they are all black (or very dark gray). The white contour creates the illusion of color. Mouse over to remove the contour and the lines will clearly appear black.&lt;/p&gt;
    &lt;head rend="h2"&gt;10 - Curvature Blindness&lt;/head&gt;
    &lt;p&gt;One set of lines looks straighter (top) while the other looks more curved (bottom). In reality, both sets are equally wavy. The only difference is how they are colored: changing the color at the peaks makes the lines look straighter. Changing it at the inflection points makes them look more curved.&lt;/p&gt;
    &lt;p&gt;The CSS code for the wavy lines is adapted from a Temani Afif snippet on CSS-Tricks and his wavy shape generator.&lt;/p&gt;
    &lt;head rend="h2"&gt;11 - Cafe Wall&lt;/head&gt;
    &lt;p&gt;This is a classic optical illusion and an easy one to code in CSS. Three gradients are all that is needed to generate the effect in which the horizontal lines appear slanted, even though they are perfectly parallel.&lt;/p&gt;
    &lt;head rend="h2"&gt;12 - Penrose Triangle&lt;/head&gt;
    &lt;p&gt;This optical illusion depicts an impossible shape. Parts that should be in front appear in the back, top becomes right, and everything feels contradictory. I coded this one some time ago for the 2024 Divtober event.&lt;/p&gt;
    &lt;head rend="h2"&gt;13 - Ebbinghaus Illusion&lt;/head&gt;
    &lt;p&gt;Which orange circle is larger: the one on the right or the one on the left? It is a trick question: both are the same size. However, having smaller surrounding elements gives the impression that one is larger.&lt;/p&gt;
    &lt;p&gt;I also created an animated version of this illusion (see below), as well as another version using a square shape instead of a flower shape:&lt;/p&gt;
    &lt;head rend="h2"&gt;14 - Kanizsa Square&lt;/head&gt;
    &lt;p&gt;When people look at this illustration, they usually say they see a white square over black circles. However, the square is not actually there. The "Pac-Man" shapes create the illusion of a square and a sense of depth. Our brain fills in the missing information.&lt;/p&gt;
    &lt;head rend="h2"&gt;15 - Ehrenstein's Illusion&lt;/head&gt;
    &lt;p&gt;There are no circles or discs in this illustration, only vertical and horizontal lines forming crosses. Our visual system completes the shape and makes us perceive a disc that does not exist.&lt;/p&gt;
    &lt;head rend="h2"&gt;16 - Neon-Color-Spreading Illusion&lt;/head&gt;
    &lt;p&gt;This illustration shows concentric circles, some of which have a green-and-black pattern. Our brain perceives a central patterned circle and four concentric circles around it, beneath the green circle.&lt;/p&gt;
    &lt;p&gt;I cheated a little when creating this in CSS, as I actually used a green circle blended with the other backgrounds.&lt;/p&gt;
    &lt;head rend="h2"&gt;17 - Hering and Wundt Illusions&lt;/head&gt;
    &lt;p&gt;Perspective-based illusions are fascinating. Even when we know we are looking at a flat image, our brain insists on interpreting depth.&lt;/p&gt;
    &lt;p&gt;In the Hering illusion, the red lines appear to curve outward, even though they are straight.&lt;/p&gt;
    &lt;p&gt;The opposite effect is the Wundt illusion. When the lines expand from the sides toward the center, the red lines appear to curve inward (this effect is more subtle).&lt;/p&gt;
    &lt;head rend="h2"&gt;18 - Ponzo Illusion&lt;/head&gt;
    &lt;p&gt;Both yellow lines are the same length, but the top one looks longer due to perceived depth and perspective. I tried a different approach when coding this one by applying a three-dimensional rotation in CSS... so the perspective is technically real.&lt;/p&gt;
    &lt;head rend="h2"&gt;19 - T Illusion&lt;/head&gt;
    &lt;p&gt;This illusion is easy to code in CSS and easy to fall for. Both the vertical and horizontal lines are the same length, but the vertical line appears longer.&lt;/p&gt;
    &lt;head rend="h2"&gt;20 - Müller-Lyer Illusion&lt;/head&gt;
    &lt;p&gt;A classic illusion: the horizontal lines are the same length, but inward- or outward-pointing edges dramatically change how we perceive them. I could swear the top one is longer. But it is not.&lt;/p&gt;
    &lt;p&gt; From a coding perspective, each shape is a pseudo-element. I ensured the horizontal lines were identical by using the same gradients and only repositioning the edges in the &lt;code&gt;::before&lt;/code&gt; and &lt;code&gt;::after&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;21 - Tilted Table Illusion&lt;/head&gt;
    &lt;p&gt;It looks like the top rectangle is leaning to the left, but it is actually parallel to the one at the bottom. The trick lies in the direction of the diagonal lines used to "color" each rectangle.&lt;/p&gt;
    &lt;p&gt;This illusion works better on larger screens. The effect is diminished when you can see the whole picture.&lt;/p&gt;
    &lt;head rend="h2"&gt;22 - Parallel Lines&lt;/head&gt;
    &lt;p&gt;This is a simple effect: the black lines are parallel, but they appear not to be because of the direction of the bars crossing them.&lt;/p&gt;
    &lt;p&gt;I slightly overcomplicated this one while coding it. I initially built the black-and-red version below and tried to reuse more code than I probably should have.&lt;/p&gt;
    &lt;p&gt;Here is the original version I created. The effect is also visible there:&lt;/p&gt;
    &lt;p&gt;Good news! There are more optical illusions below - but first, a warning.&lt;/p&gt;
    &lt;p&gt;ATTENTION: The following optical illusions are static, but they give the impression of movement. Proceed accordingly.&lt;/p&gt;
    &lt;p&gt;(Leaving some blank space in case you do not want to continue.)&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;23 - Expanding Hole&lt;/head&gt;
    &lt;p&gt;This is a trippy optical illusion. It is completely static, yet it looks like the black hole at the center is expanding - especially when you are not looking at it directly, creating the sensation of falling into a pit.&lt;/p&gt;
    &lt;p&gt;From a coding perspective, this one was very simple: a background pattern made with two radial gradients, plus a blurred pseudo-element for the "expanding" hole.&lt;/p&gt;
    &lt;head rend="h2"&gt;24 - Rotating Snakes&lt;/head&gt;
    &lt;p&gt;This is one of only two optical illusions in this collection where I used HTML elements instead of relying exclusively on CSS. It is a classic effect: when you look at the illustration, the peripheral discs appear to rotate, even though nothing is actually moving.&lt;/p&gt;
    &lt;head rend="h2"&gt;25 - Appearing Dots&lt;/head&gt;
    &lt;p&gt;Another classic illusion. Focus on the white dots and the adjacent dots will appear to turn black. There is no animation, no transition, and nothing dynamic. Just intersecting lines and small white circles, yet it looks like motion.&lt;/p&gt;
    &lt;head rend="h2"&gt;26 - Disappearing Dots&lt;/head&gt;
    &lt;p&gt;This pattern consists of repeating black and white dots across the page. If you focus on one dot, the others will begin to disappear. At first it may happen by row or column, but after a short while, most of them vanish.&lt;/p&gt;
    &lt;p&gt;If you do not immediately see the effect, try focusing on one black dot. Mouse over it, wait a few seconds while keeping your focus, and then mouse out.&lt;/p&gt;
    &lt;head rend="h2"&gt;27 - Ouchi Illusion&lt;/head&gt;
    &lt;p&gt;This is a static image, but it gives the impression that the pattern inside the circle is moving sideways. This happens because our eyes are constantly making small movements, even when we are not aware of it.&lt;/p&gt;
    &lt;p&gt;If you cannot see the illusion, try slightly moving the screen (or your head) while looking just outside the circle.&lt;/p&gt;
    &lt;head rend="h2"&gt;28 - Orthogonal Dotted Lines Sway&lt;/head&gt;
    &lt;p&gt;When you look around this pattern, the central area appears to slide and sway, even though it is completely static. This illusion makes me dizzy... but that may also be because I had to stare at it for a long time while coding it.&lt;/p&gt;
    &lt;head rend="h2"&gt;29 - Enigma&lt;/head&gt;
    &lt;p&gt;This illusion is particularly interesting. There is a pink circle surrounded by concentric pink and purple rings. If you focus on the pink circle, the rings appear to spin or scintillate, as if there were some activity in them. Of course, nothing is actually moving.&lt;/p&gt;
    &lt;head rend="h2"&gt;30 - Waves&lt;/head&gt;
    &lt;p&gt;This demo was challenging to code and takes a long time to load. Mainly because it uses a large number of conic gradients behind the scenes, which browsers struggle to render efficiently. There is probably a better way to implement it, but I have not explored that yet.&lt;/p&gt;
    &lt;p&gt;If you look closely at the illustration, you may notice wave-like motion. As with the previous illusions in this section, the image is entirely static.&lt;/p&gt;
    &lt;p&gt;Good news! There are more optical illusions below - but first, another warning.&lt;/p&gt;
    &lt;p&gt;ATTENTION: The following optical illusions actually move, and the illusion is created by motion itself. Some of them can be dizzying, so proceed accordingly.&lt;/p&gt;
    &lt;p&gt;(Leaving some blank space in case you do not want to continue.)&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;31 - Animated Ebbinghaus Illusion&lt;/head&gt;
    &lt;p&gt;Earlier, we saw two static versions of the Ebbinghaus illusion. This one is animated. The elements move side to side, and the surrounding shapes grow and shrink, giving the impression that the orange circle is changing size - when it definitely is not.&lt;/p&gt;
    &lt;head rend="h2"&gt;32 - Psychokinematic Tower&lt;/head&gt;
    &lt;p&gt;This looks like a three-dimensional tower spinning in space, as seen from above. In reality, it is a flat, two-dimensional image rotating.&lt;/p&gt;
    &lt;p&gt;Mouse over the demo to stop the rotation and the illusion of depth disappears entirely.&lt;/p&gt;
    &lt;head rend="h2"&gt;33 - Color Fan&lt;/head&gt;
    &lt;p&gt;This optical illusion requires only two gradients: a conic gradient for the fan-shaped arms and a radial gradient for the circles and discs.&lt;/p&gt;
    &lt;p&gt;If you focus on the black dot, the illustration may appear to develop a darker greenish or brownish border. However, the colors never change.&lt;/p&gt;
    &lt;head rend="h2"&gt;34 - Reverse Spoke Illusion&lt;/head&gt;
    &lt;p&gt;This illusion is delightful and disorienting. While the background colors of the wheel are spinning, the spokes remain fixed. However, they appear to rotate in the opposite direction. In reality, only the background is moving.&lt;/p&gt;
    &lt;head rend="h2"&gt;35 - Motion Binding&lt;/head&gt;
    &lt;p&gt;What do you see in this animation? Most people report two sets of lines operating independently: one moving horizontally and another moving vertically. And that is exactly how it looks.&lt;/p&gt;
    &lt;p&gt;In reality, it is a single shape moving uniformly. Run the demo, mouse over the lines, and the true motion will be revealed.&lt;/p&gt;
    &lt;head rend="h2"&gt;36 - Mainz-Linez Illusion&lt;/head&gt;
    &lt;p&gt;Focus on one of the red dots. You will notice it moves straight up and down along a vertical path. Now shift your focus to one of the black crosses in the center. Suddenly, the red dots appear to zigzag instead of moving straight.&lt;/p&gt;
    &lt;p&gt;The CSS code for the wavy lines is adapted from a Temani Afif snippet on CSS-Tricks and his wavy shape generator.&lt;/p&gt;
    &lt;head rend="h2"&gt;37 - Waddling Colors&lt;/head&gt;
    &lt;p&gt;It may look like the boxes are moving at different speeds or like a set of walking feet. In reality, all elements move at the same pace and in parallel. Mouse over the demo to reveal the effect.&lt;/p&gt;
    &lt;p&gt;The illusion also works when the "feet" move in circles, as shown in this alternative version:&lt;/p&gt;
    &lt;head rend="h2"&gt;38 - Dotted-Line Motion&lt;/head&gt;
    &lt;p&gt;Follow the red dot as it moves sideways. From the corner of your vision, it may appear that the dashed black-and-white lines are moving closer together (when the dot moves left) or farther apart (when it moves right). In reality, the lines are completely static.&lt;/p&gt;
    &lt;head rend="h2"&gt;39 - Contrast Asynchrony&lt;/head&gt;
    &lt;p&gt;These dots always have the same color. However, when placed against alternating backgrounds, they appear to jump or move out of sync because of how they blend with their surroundings.&lt;/p&gt;
    &lt;p&gt;Mouse over the demo to remove the background and the illusion disappears.&lt;/p&gt;
    &lt;head rend="h2"&gt;40 - Breathing Square&lt;/head&gt;
    &lt;p&gt;This illusion gives the impression that a blue square is growing and shrinking rhythmically, almost as if it were breathing or beating like a heart.&lt;/p&gt;
    &lt;p&gt;Although the image is rotating, its size never changes. Mouse over the illustration to remove the green boxes and reveal the rotating blue square.&lt;/p&gt;
    &lt;head rend="h2"&gt;41 - Troxler Fading&lt;/head&gt;
    &lt;p&gt;This illustration shows a circle made of pink dots, with one dot missing. Focus on the cross at the center and the missing dot will appear as a yellow or green dot, giving the impression that it is "eating" the pink dots. Just like Pac-Man.&lt;/p&gt;
    &lt;p&gt;I could have used CSS trigonometric functions to calculate the exact positions of the dots, but since they never change, I chose to hardcode the values instead.&lt;/p&gt;
    &lt;p&gt;Here is a related effect. Follow the light gray circle as it spins, and the darker circles will appear to change from gray to greenish. Focus on the cross at the center, and after a short time, the darker circles may begin to fade entirely.&lt;/p&gt;
    &lt;head rend="h2"&gt;42 - Pinna-Brelstaff Illusion&lt;/head&gt;
    &lt;p&gt;This illusion is particularly dizzying. Follow the bluish dot as it moves from right to left and back again. It will appear as though parts of the tiled background are shifting, even though they are static. The only moving element is the dot.&lt;/p&gt;
    &lt;p&gt; From a CSS perspective, I coded the pattern using conic gradients, and applied it to the &lt;code&gt;::before&lt;/code&gt; and &lt;code&gt;::after&lt;/code&gt; pseudo-elements. I then flipped one upside down and clipped it.&lt;/p&gt;
    &lt;head rend="h2"&gt;43 - Palisade&lt;/head&gt;
    &lt;p&gt;The radii of a wheel, when viewed through a palisade, appear to curve. In reality, they are perfectly straight. Mouse over the demo to remove the palisade and you will see that the radii never bend.&lt;/p&gt;
    &lt;head rend="h2"&gt;44 - Alternative Motion&lt;/head&gt;
    &lt;p&gt;This animation demonstrates how our minds infer motion that may not actually be there. Consider the two blue dots. Different people perceive different movements: side to side, top to bottom, or even circular motion.&lt;/p&gt;
    &lt;p&gt;Cover the right side of the animation so that you see only one dot at a time. The motion now appears vertical. Cover the bottom part instead, and the motion appears horizontal. This is our brain trying to complete the movement.&lt;/p&gt;
    &lt;head rend="h2"&gt;45 - Motion Inversion&lt;/head&gt;
    &lt;p&gt;These two illustrations are identical - same shapes, same animation. The only difference is the CSS timing function.&lt;/p&gt;
    &lt;p&gt;The top animation moves smoothly from right to left. The bottom one appears to move choppily in the same direction, but if you focus on it, it may suddenly seem to reverse direction and move faster.&lt;/p&gt;
    &lt;p&gt;Most of the inspiration for these optical illusions came from two excellent resources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;"35 optical illusions and why they trick your brain" by Patrick Pester.&lt;/item&gt;
      &lt;item&gt;"154 Visual Phenomena &amp;amp; Optical Illusions" with explanations by Michael Bach&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also find this article on:&lt;/p&gt;
    &lt;p&gt;(You can leave comments on those platforms and I will reply there).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46722570</guid><pubDate>Thu, 22 Jan 2026 17:41:22 +0000</pubDate></item><item><title>Recent discoveries on the acquisition of the highest levels of human performance</title><link>https://www.science.org/doi/abs/10.1126/science.adt7790</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46722853</guid><pubDate>Thu, 22 Jan 2026 18:01:02 +0000</pubDate></item><item><title>I was banned from Claude for scaffolding a Claude.md file?</title><link>https://hugodaniel.com/posts/claude-code-banned-me/</link><description>&lt;doc fingerprint="7a2301352c29c929"&gt;
  &lt;main&gt;
    &lt;p&gt;Excessive sitting isn't good for a person's physical or mental health, but there's a type of sedentary activity that may not shrink our brains or cost our cognition to the same extent.&lt;/p&gt;
    &lt;p&gt;A systematic review of 85 studies has now found good reason to differentiate between 'active' sitting, like playing cards or reading, and 'passive' sitting, like watching TV.&lt;/p&gt;
    &lt;p&gt;The former may actually boost brain health.&lt;/p&gt;
    &lt;p&gt;That's probably because active sitting engages the brain, whereas passive sitting lets a person take a back seat both physically and cognitively.&lt;/p&gt;
    &lt;p&gt;Related: Scientists Revealed How Much Exercise You Need to 'Offset' Sitting All Day&lt;/p&gt;
    &lt;p&gt;"Total sitting time has been shown to be related to brain health; however, sitting is often treated as a single entity, without considering the specific type of activity," explains public health researcher Paul Gardiner from the University of Queensland in Australia.&lt;/p&gt;
    &lt;p&gt;"Most people spend many hours sitting each day, so the type of sitting really matters … These findings show that small everyday choices – like reading instead of watching television – may help keep your brain healthier as you age."&lt;/p&gt;
    &lt;p&gt;Obviously, exercise remains incredibly important for cognitive health, but giving your brain a workout is also important, and that doesn't necessarily mean you have to be on your feet.&lt;/p&gt;
    &lt;p&gt;Across numerous studies, Gardiner and colleagues found that active sitting activities, like reading, playing card games, and using a computer, showed "overwhelmingly positive associations with cognitive health, enhancing cognitive functions such as executive function, situational memory, and working memory."&lt;/p&gt;
    &lt;p&gt;Meanwhile, passive sitting was most consistently associated with negative cognitive outcomes, including increased risk of dementia.&lt;/p&gt;
    &lt;p&gt;The effect sizes were small but significant. The study authors hope their results will help inform future health research and more nuanced health guidance.&lt;/p&gt;
    &lt;p&gt;For example, the researchers suggest guidelines should recognize the difference between passively watching TV and actively using a computer, and encourage people to take short breaks to stimulate their brains and move.&lt;/p&gt;
    &lt;p&gt;Their review focused on studies of typical sedentary activities in natural settings, rather than structured programs designed to boost brain function, making it relevant to people's everyday lives.&lt;/p&gt;
    &lt;p&gt;"Health advice could shift from simply saying 'sit less' to encouraging more mentally engaging activities while sitting," argues Gardiner.&lt;/p&gt;
    &lt;p&gt;"This could help people make easy, realistic changes that support long‑term brain health and potentially reduce dementia risk."&lt;/p&gt;
    &lt;p&gt;The study was published in the Journal of Alzheimer's Disease.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46723384</guid><pubDate>Thu, 22 Jan 2026 18:38:27 +0000</pubDate></item><item><title>'Active' sitting is better for brain health: review of studies</title><link>https://www.sciencealert.com/not-all-sitting-is-equal-one-type-was-just-linked-to-better-brain-health</link><description>&lt;doc fingerprint="7a2301352c29c929"&gt;
  &lt;main&gt;
    &lt;p&gt;Excessive sitting isn't good for a person's physical or mental health, but there's a type of sedentary activity that may not shrink our brains or cost our cognition to the same extent.&lt;/p&gt;
    &lt;p&gt;A systematic review of 85 studies has now found good reason to differentiate between 'active' sitting, like playing cards or reading, and 'passive' sitting, like watching TV.&lt;/p&gt;
    &lt;p&gt;The former may actually boost brain health.&lt;/p&gt;
    &lt;p&gt;That's probably because active sitting engages the brain, whereas passive sitting lets a person take a back seat both physically and cognitively.&lt;/p&gt;
    &lt;p&gt;Related: Scientists Revealed How Much Exercise You Need to 'Offset' Sitting All Day&lt;/p&gt;
    &lt;p&gt;"Total sitting time has been shown to be related to brain health; however, sitting is often treated as a single entity, without considering the specific type of activity," explains public health researcher Paul Gardiner from the University of Queensland in Australia.&lt;/p&gt;
    &lt;p&gt;"Most people spend many hours sitting each day, so the type of sitting really matters … These findings show that small everyday choices – like reading instead of watching television – may help keep your brain healthier as you age."&lt;/p&gt;
    &lt;p&gt;Obviously, exercise remains incredibly important for cognitive health, but giving your brain a workout is also important, and that doesn't necessarily mean you have to be on your feet.&lt;/p&gt;
    &lt;p&gt;Across numerous studies, Gardiner and colleagues found that active sitting activities, like reading, playing card games, and using a computer, showed "overwhelmingly positive associations with cognitive health, enhancing cognitive functions such as executive function, situational memory, and working memory."&lt;/p&gt;
    &lt;p&gt;Meanwhile, passive sitting was most consistently associated with negative cognitive outcomes, including increased risk of dementia.&lt;/p&gt;
    &lt;p&gt;The effect sizes were small but significant. The study authors hope their results will help inform future health research and more nuanced health guidance.&lt;/p&gt;
    &lt;p&gt;For example, the researchers suggest guidelines should recognize the difference between passively watching TV and actively using a computer, and encourage people to take short breaks to stimulate their brains and move.&lt;/p&gt;
    &lt;p&gt;Their review focused on studies of typical sedentary activities in natural settings, rather than structured programs designed to boost brain function, making it relevant to people's everyday lives.&lt;/p&gt;
    &lt;p&gt;"Health advice could shift from simply saying 'sit less' to encouraging more mentally engaging activities while sitting," argues Gardiner.&lt;/p&gt;
    &lt;p&gt;"This could help people make easy, realistic changes that support long‑term brain health and potentially reduce dementia risk."&lt;/p&gt;
    &lt;p&gt;The study was published in the Journal of Alzheimer's Disease.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46723694</guid><pubDate>Thu, 22 Jan 2026 19:03:56 +0000</pubDate></item><item><title>Why does SSH send 100 packets per keystroke?</title><link>https://eieio.games/blog/ssh-sends-100-packets-per-keystroke/</link><description>&lt;doc fingerprint="c61ded10eca0acfa"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Why does SSH send 100 packets per keystroke?&lt;/head&gt;
    &lt;p&gt;And why do I care?&lt;/p&gt;
    &lt;p&gt;Jan 22, 2026&lt;/p&gt;
    &lt;p&gt;Here are a few lines of summarized &lt;code&gt;tcpdump&lt;/code&gt; output for an ssh session where I send a single keystroke:&lt;/p&gt;
    &lt;code&gt;$ ./first_lines_of_pcap.sh single-key.pcap
  1   0.000s  CLIENT-&amp;gt;SERVER   36 bytes
  2   0.007s  SERVER-&amp;gt;CLIENT  564 bytes
  3   0.015s  CLIENT-&amp;gt;SERVER    0 bytes
  4   0.015s  CLIENT-&amp;gt;SERVER   36 bytes
  5   0.015s  SERVER-&amp;gt;CLIENT   36 bytes
  6   0.026s  CLIENT-&amp;gt;SERVER    0 bytes
  7   0.036s  CLIENT-&amp;gt;SERVER   36 bytes
  8   0.036s  SERVER-&amp;gt;CLIENT   36 bytes
  9   0.046s  CLIENT-&amp;gt;SERVER    0 bytes
 10   0.059s  CLIENT-&amp;gt;SERVER   36 bytes
&lt;/code&gt;
    &lt;p&gt;I said a “few” because there are a lot of these lines.&lt;/p&gt;
    &lt;code&gt;$ ./summarize_pcap.sh single-key.pcap
Total packets: 270

  36-byte msgs:   179 packets ( 66.3%)   6444 bytes
  Other data:       1 packet  (  0.4%)    564 bytes
  TCP ACKs:        90 packets ( 33.3%)

  Data sent:      6444 bytes in 36-byte messages,  564 bytes in other data
  Ratio:          11.4x more data in 36-byte messages than other data

  Data packet rate: ~90 packets/second (avg 11.1 ms between data packets)
&lt;/code&gt;
    &lt;p&gt;That is a lot of packets for one keypress. What’s going on here? Why do I care?&lt;/p&gt;
    &lt;head class="sc-4d1d4ca-1 bowwWe"&gt;here's those scripts if you're curious&lt;/head&gt;
    &lt;code&gt;# first_lines_of_pcap.sh
tshark -r "$1" \
  -T fields -e frame.number -e frame.time_relative -e ip.src -e ip.dst -e tcp.len | \
  awk 'NR&amp;lt;=10 {dir = ($3 ~ /71\.190/ ? "CLIENT-&amp;gt;SERVER" : "SERVER-&amp;gt;CLIENT");
       printf "%3d  %6.3fs  %-4s  %3s bytes\n", $1, $2, dir, $5}'
&lt;/code&gt;
    &lt;code&gt;# summarize_pcap.sh
tshark -r "$1" -Y "frame.time_relative &amp;lt;= 2.0" -T fields -e frame.time_relative -e tcp.len | awk '
  {
      count++
      payload = $2

      if (payload == 0) {
          acks++
      } else if (payload == 36) {
          mystery++
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      } else {
          game_data++
          game_bytes = payload
          if (NR &amp;gt; 1 &amp;amp;&amp;amp; prev_data_time &amp;gt; 0) {
              delta = $1 - prev_data_time
              sum_data_deltas += delta
              data_intervals++
          }
          prev_data_time = $1
      }
  }
  END {
      print "Total packets:", count
      print ""
      printf "  36-byte msgs:   %3d packets (%5.1f%%)  %5d bytes\n", mystery, 100*mystery/count, mystery*36
      printf "  Other data:     %3d packet  (%5.1f%%)  %5d bytes\n", game_data, 100*game_data/count, game_bytes
      printf "  TCP ACKs:       %3d packets (%5.1f%%)\n", acks, 100*acks/count
      print ""
      printf "  Data sent:      %d bytes in 36-byte messages,  %d bytes in other data\n", mystery*36, game_bytes
      printf "  Ratio:          %.1fx more data in 36-byte messages than other data\n", (mystery*36)/game_bytes
      print ""
      avg_ms = (sum_data_deltas / data_intervals) * 1000
      printf "  Data packet rate: ~%d packets/second (avg %.1f ms between data packets)\n", int(1000/avg_ms + 0.5), avg_ms
  }'
&lt;/code&gt;
    &lt;head rend="h2"&gt;Discovery&lt;/head&gt;
    &lt;p&gt;I am working on a high-performance game that runs over ssh. The TUI for the game is created in bubbletea 1 and sent over ssh via wish.&lt;/p&gt;
    &lt;p&gt;I have also forked bubbletea to make it faster. Stay tuned!&lt;/p&gt;
    &lt;p&gt;The game is played in an 80x60 window that I update 10 times a second. I’m targeting at least 2,000 concurrent players, which means updating ~100 million cells a second. I care about performance.&lt;/p&gt;
    &lt;p&gt;So I have a script that connects a few hundred bots over ssh and has them make a move a second. Then I use go’s outstanding profiling tools to look at what’s going on.&lt;/p&gt;
    &lt;p&gt;Yesterday I inadvertently broke my test harness. Instead of regularly sending game data, my server sent the bots a single message that said “your screen is too small.” This cut my game’s CPU and bandwidth usage in half.&lt;/p&gt;
    &lt;p&gt;At first I was disappointed. I (briefly) thought I had a free massive speedup on my hands, but it was actually a testing error.&lt;/p&gt;
    &lt;p&gt;But wait.&lt;/p&gt;
    &lt;p&gt;If I wasn’t sending game data back to my bots, why did CPU usage drop by 50% instead of 100%?&lt;/p&gt;
    &lt;head rend="h2"&gt;Investigation&lt;/head&gt;
    &lt;p&gt;As part of debugging the test harness issue, I used &lt;code&gt;tcpdump&lt;/code&gt; to log game traffic with and without the breaking change. Something like:&lt;/p&gt;
    &lt;code&gt;# The game runs on port 22
timeout 30s tcpdump -i eth0 'port 22' -w with-breaking-change.pcap
# Revert change
timeout 30s tcpdump -i eth0 'port 22' -w without-breaking-change.pcap
&lt;/code&gt;
    &lt;p&gt;Our breaking change stopped us from rendering our game over ssh. So &lt;code&gt;with-breaking-change.pcap&lt;/code&gt; contains packets that represent the overhead of each connection without actually rendering the game.&lt;/p&gt;
    &lt;p&gt;I was debugging this with Claude Code, so I asked it to summarize what it saw in the pcap.&lt;/p&gt;
    &lt;code&gt;Wanna take a look yourself? I put with-breaking-change.pcap in this directory

--

Wow! Here's what I found:

Packet Size Distribution (413,703 total packets):
274,907 packets (66%): Exactly 36 bytes
138,778 packets (34%): 0 bytes (TCP ACKs)
18 packets (&amp;lt;0.1%): 72 bytes
&lt;/code&gt;
    &lt;p&gt;Further analysis on a smaller pcap pointed to these mysterious packets arriving ~20ms apart.&lt;/p&gt;
    &lt;p&gt;This was baffling to me (and to Claude Code). We kicked around several ideas like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SSH flow control messages&lt;/item&gt;
      &lt;item&gt;PTY size polling or other status checks&lt;/item&gt;
      &lt;item&gt;Some quirk of bubbletea or wish&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;One thing stood out - these exchanges were initiated by my ssh client (stock ssh installed on MacOS) - not by my server.&lt;/p&gt;
    &lt;p&gt;On a hunch, I took a &lt;code&gt;tcpdump&lt;/code&gt; of a regular ssh session.&lt;/p&gt;
    &lt;code&gt;# on my mac, in one tab
sudo tcpdump -ien0 'port 22'

# on my mac, in another tab
ssh $some_vm_of_mine
&lt;/code&gt;
    &lt;p&gt;I waited for the initial connection chatter to die down, sent one keystroke to my remote vm, and looked at the &lt;code&gt;tcpdump&lt;/code&gt; output.&lt;/p&gt;
    &lt;p&gt;I saw the exact same pattern! What in the world?&lt;/p&gt;
    &lt;head rend="h2"&gt;Root cause&lt;/head&gt;
    &lt;p&gt;Once I realized that this was a property of stock ssh and not my game, debugging got a lot easier.&lt;/p&gt;
    &lt;p&gt;Running &lt;code&gt;ssh -vvv&lt;/code&gt; gave me a pretty good sense of what was going on:&lt;/p&gt;
    &lt;code&gt;debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (49 chaff packets sent) 
debug3: obfuscate_keystroke_timing: starting: interval ~20ms
debug3: obfuscate_keystroke_timing: stopping: chaff time expired (101 chaff packets sent)
&lt;/code&gt;
    &lt;p&gt;That &lt;code&gt;20ms&lt;/code&gt; is a smoking gun - it lines up perfectly with the mysterious pattern we saw earlier! And the rest of the message is pretty helpful too - we sent 49 “chaff” packets for the first keystroke and 101 “chaff” for around the second one.&lt;/p&gt;
    &lt;p&gt;In 2023, ssh added keystroke timing obfuscation. The idea is that the speed at which you type different letters betrays some information about which letters you’re typing. So ssh sends lots of “chaff” packets along with your keystrokes to make it hard for an attacker to determine when you’re actually entering keys.&lt;/p&gt;
    &lt;p&gt;That makes a lot of sense for regular ssh sessions, where privacy is critical. But it’s a lot of overhead for an open-to-the-whole-internet game where latency is critical.&lt;/p&gt;
    &lt;head rend="h2"&gt;Remediation&lt;/head&gt;
    &lt;p&gt;Keystroke obfuscation can be disabled client-side. After reverting my original breaking change, I tried updating my test harness to pass &lt;code&gt;ObscureKeystrokeTiming=no&lt;/code&gt; when starting up ssh sessions.&lt;/p&gt;
    &lt;p&gt;This worked great. CPU usage dropped dramatically and bots still received valid data.&lt;/p&gt;
    &lt;p&gt;But this is hardly a solution in the real world. I want &lt;code&gt;ssh mygame&lt;/code&gt; to Just Work without asking users to pass options that they might not understand.&lt;/p&gt;
    &lt;p&gt;Claude Code originally didn’t have much faith that we could disable this functionality server-side.&lt;/p&gt;
    &lt;p&gt;generated with simon wilson's excellent claude-code-transcripts tool&lt;/p&gt;
    &lt;p&gt;Fortunately, the description I found of SSH keystroke obfuscation made it easy to look up the relevant code in go’s ssh library (which I was transitively depending on).&lt;/p&gt;
    &lt;code&gt;Log message:
Introduce a transport-level ping facility

This adds a pair of SSH transport protocol messages SSH2_MSG_PING/PONG
to implement a ping capability. These messages use numbers in the "local
extensions" number space and are advertised using a "[email protected]"
ext-info message with a string version number of "0".
&lt;/code&gt;
    &lt;p&gt;The “chaff” messages that ssh uses to obscure keystrokes are SSH2_MSG_PING messages. And they’re sent to servers that advertise the availability of the &lt;code&gt;[email protected]&lt;/code&gt; extension. What if we just…don’t advertise &lt;code&gt;[email protected]&lt;/code&gt;?&lt;/p&gt;
    &lt;p&gt;I searched go’s ssh library for &lt;code&gt;[email protected]&lt;/code&gt; and found the commit where support was added. The commit was tiny and seemed very easy to revert.&lt;/p&gt;
    &lt;p&gt;I cloned the go crypto repo and told Claude to revert this change and update our dependencies to use our clone (go’s replace directive makes forking a library very easy).&lt;/p&gt;
    &lt;p&gt;Then I re-ran my test harness. The results were…very good:&lt;/p&gt;
    &lt;code&gt;Total CPU  29.90%          -&amp;gt; 11.64%
Syscalls   3.10s           -&amp;gt; 0.66s
Crypto     1.6s            -&amp;gt; 0.11s
Bandwidth  ~6.5 Mbit/sec   -&amp;gt; ~3 Mbit/sec
&lt;/code&gt;
    &lt;p&gt;Claude was also pretty pumped:&lt;/p&gt;
    &lt;p&gt;yes it's 1:30 am what of it&lt;/p&gt;
    &lt;p&gt;Obviously forking go’s crypto library is a little scary, and I’m gonna have to do some thinking about how to maintain my little patch in a safe way.&lt;/p&gt;
    &lt;p&gt;But this is a huge improvement. I’ve spent much of the last week squeezing out small single-digit performance wins. A &amp;gt;50% drop was unimaginable to me.&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging with LLMs was fun&lt;/head&gt;
    &lt;p&gt;I’ve been thinking about whether LLMs remove parts of the problem-solving process that I enjoy. But I’ve gotta say, debugging this problem using Claude Code was super fun.&lt;/p&gt;
    &lt;p&gt;I am familiar enough with &lt;code&gt;tcpdump&lt;/code&gt;, &lt;code&gt;tshark&lt;/code&gt;, and friends to know what they can do. But I don’t use them regularly enough to be fast with them. Being able to tell an agent “here’s a weird pcap - tell me what’s going on” was really lovely. And by watching commands as the agent ran them I was able to keep my mental model of the problem up to date.&lt;/p&gt;
    &lt;p&gt;There were still edge cases. At some point in my confusion I switched to ChatGPT and it very confidently told me that my tcpdump output was normal ssh behavior:&lt;/p&gt;
    &lt;p&gt;do all chatgpt messages have this tone and formatting now?&lt;/p&gt;
    &lt;p&gt;And then doubled down when I pushed back:&lt;/p&gt;
    &lt;p&gt;no!!!&lt;/p&gt;
    &lt;p&gt;Similarly, I had to push Claude Code to consider forking go’s ssh library. And I had to make the original leap of “wait…if our test harness was broken, why was usage not 0%?”&lt;/p&gt;
    &lt;p&gt;When you say “LLMs did not fully solve this problem” some people tend to respond with “you’re holding it wrong!”&lt;/p&gt;
    &lt;p&gt;I think they’re sometimes right! Interacting with LLMs is a new skill, and it feels pretty weird if you’re used to writing software like it’s 2020. A more talented user of LLMs may have trivially solved this problem.&lt;/p&gt;
    &lt;p&gt;But the best way to develop a skill is by practicing it. And for me, that means figuring out how to transfer my problem-solving intuitions to the tools that I’m using.&lt;/p&gt;
    &lt;p&gt;Besides. Being in the loop is fun. How else would I write this post?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46723990</guid><pubDate>Thu, 22 Jan 2026 19:27:32 +0000</pubDate></item><item><title>Taming P99s in OpenFGA: How we built a self-tuning strategy planner</title><link>https://auth0.com/blog/self-tuning-strategy-planner-openfga/</link><description>&lt;doc fingerprint="66f52325052d8083"&gt;
  &lt;main&gt;
    &lt;p&gt;Operating a latency-critical system means the inevitable work of reducing tail latency. Tail latency refers to the response time experienced by the slowest requests (the outliers), rather than the average. Since authorization happens on every request, these decisions must be fast; otherwise, they directly add overhead to the total response time. For OpenFGA, an open-source authorization system modeled after Google's Zanzibar, that powers up Auth0 FGA, this challenge manifests in its most critical operation: &lt;code&gt;Check&lt;/code&gt;. Answering "Can user X access resource Y?" requires traversing relationship graphs. In this context, traversal performance isn't just a feature; it is the fundamental constraint of the system's architecture.&lt;/p&gt;
    &lt;p&gt;In our quest to reduce latency for the &lt;code&gt;Check&lt;/code&gt; API, we initially developed multiple graph traversal strategies tailored to specific data distributions. Our early iterations selected these strategies statically based on the graph node’s complexity, lacking the context to determine whether a specific strategy would actually outperform the default traversal algorithm for a given dataset.&lt;/p&gt;
    &lt;p&gt;We needed a way to consistently select the optimal path based on performance data, not just static rules. This led to the development of a dynamic, self-tuning planner that learns from production latency in real-time. Because every node in a customer’s graph possesses unique complexity—varying by type of operations, operation count, data cardinality, and subgraph distribution—the planner treats each node independently, applying the most effective strategy for that specific point in the traversal.&lt;/p&gt;
    &lt;p&gt;This post details the algorithmic framework chosen for the self-tuning planner and the methodology used to calibrate the probabilistic distributions for each traversal strategy. We will examine how this architecture creates an extensible feedback loop, allowing us to continuously inject new, pre-tuned strategies into the decision engine (the planner) improving even more the performance of our system. By decoupling strategy definition from selection logic, we ensure the planner evolves in lockstep with our expanding library of optimization heuristics.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;At the heart of OpenFGA is a graph of relationship tuples. This graph is defined as a Weighted Directed Cyclic Graph that represents the OpenFGA model. Each edge and node has a defined weight (complexity factor) for each reachable user type in the subgraph. But each element in the graph also stores information about the characteristics of each subgraph, such as recursiveness, cycle presence, public access reachability, which allows us in O(1) time to select the appropriate traversal algorithm to use for each case.&lt;/p&gt;
    &lt;p&gt;Using this weighted graph, we were able to introduce new algorithms for traversing the subgraph and began seeing latency improvements for some customers. However, this wasn't applicable to everyone, and sometimes performance shifted as data distribution evolved. The reality is that the optimal strategy for traversing a subgraph is not just dependent on static graph characteristics and complexity, but is also highly dependent on the subgraph tuple distribution. This required building a traversal planner, a heuristic engine that selects the optimal algorithm for each subgraph based on both factors.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our Approach: Why We Chose Thompson Sampling&lt;/head&gt;
    &lt;p&gt;Our goal was to create an adaptive heuristic enabling the engine to dynamically modify traversal selection during execution based on observed performance. We initially considered implementing the planner using counters and thresholds. While a step in the right direction, this model felt brittle and necessitated the constant manual tuning of "magic numbers." A key insight emerged during our design discussions: this wasn't a simple routing problem, but a classic reinforcement learning scenario known as the Multi-Armed Bandit problem.&lt;/p&gt;
    &lt;p&gt;In this problem, the gambler has multiple slot machines to play, and he must determine what slot machine to play and how often to maximize their cumulative reward. We can think of our traversal algorithms as the slot machines in the bandit problem, and our planner will act as the agent selecting the strategy for each subgraph request to maximize the payout (or in our context: minimize latency). Crucially, the planner does not evaluate every algorithm indiscriminately; the selection pool is first restricted to only those strategies compatible with the specific subgraph characteristics and request context. This architecture forces us to balance two competing fundamental needs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exploitation: Leveraging the strategy that has historically performed the best to ensure stability.&lt;/item&gt;
      &lt;item&gt;Exploration: Sampling other strategies to discover potential optimizations, even if it means a marginal short-term performance cost for a long-term efficiency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To address this, we evaluated several standard solvers: Epsilon-Greedy, Upper Confidence Bound (UCB), and Thompson Sampling. We ultimately selected Thompson Sampling due to its proven robustness in real-world decision systems. Unlike simple heuristics that merely track a running average (a point estimate), Thompson Sampling maintains a full probability distribution for each strategy's expected performance.&lt;/p&gt;
    &lt;p&gt;To make a decision, the engine draws a random sample from each distribution and greedily selects the highest value. This Bayesian approach is powerful for our use case because it allows the system to natively quantify its own uncertainty.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exploration: A strategy with a wide, flat distribution represents high uncertainty, mathematically increasing its probability of generating a high sample and being selected for exploration.&lt;/item&gt;
      &lt;item&gt;Exploitation: A strategy with a narrow, sharp distribution represents high confidence, leading the planner to reliably exploit that known efficiency.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Critically, this model is not static. The planner employs continuous runtime learning, updating its priors with every single &lt;code&gt;Check&lt;/code&gt; request. As a customer's authorization model evolves, it could alter the underlying tuple distribution of their permission graph, and the latency profile of our strategies could also drift.&lt;/p&gt;
    &lt;p&gt;The planner automatically captures this shift through the feedback loop. If a previously optimal strategy degrades, its performance distribution will widen (indicating uncertainty) or shift lower (indicating poor performance). This mathematically increases the probability that alternative strategies are sampled and, if they prove superior, adopted as the new baseline. This runtime adaptability allows the engine to self-correct in response to data evolution, a level of resilience that static heuristics simply cannot achieve.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Art of the Prior: Encoding Domain Knowledge&lt;/head&gt;
    &lt;p&gt;A Bayesian system is only as good as its priors. The model mathematically synthesizes these priors with observed evidence to generate updated posterior distributions. In simpler terms, this means the engine combines its initial "best guess" with actual latency feedback — while accounting for confidence variables and allowed variance — to get smarter over time. Think of it like planning your commute. You might start with a high-confidence belief that the highway is the fastest route, expecting a 30-minute trip based on your past experience. However, after taking the highway for a week and consistently facing a 1.5-hour commute, you realize your initial belief was wrong for the current conditions. You then decide to explore alternative routes where you have very low confidence (high uncertainty). If one of those new routes consistently delivers a 45-minute commute, you switch to exploiting that verified path for future trips.&lt;/p&gt;
    &lt;p&gt;Because our traversal strategies exhibit distinct performance profiles depending on the subgraph structure, relying on a generic "blank slate" prior would yield slow or inaccurate convergence. Instead, we tune specific priors for each strategy, effectively encoding our domain knowledge of how tuple distributions impact execution time for each algorithm before the first request is even processed.&lt;/p&gt;
    &lt;p&gt;For the mathematical framework, we selected the Normal-Gamma distribution (utilizing the shape-rate parameterization) as our conjugate prior. We chose this specific distribution because it allows us to model two coupled unknowns simultaneously: the expected latency (mean) and the reliability of that latency (variance/precision). Crucially, its property of conjugacy ensures that our update step remains computationally efficient — allowing us to recalculate priors in O(1) time without expensive numerical integration. The code below shows the update function used in OpenFGA for the Thompson Sampling implementation to update the new distribution parameters.&lt;/p&gt;
    &lt;p&gt;To generate the precision we use the probability density function (PDF) for the Gamma distribution.&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;// Update performs a Bayesian update on the distribution's parameters // using the new data point. func (ts *ThompsonStats) Update(duration time.Duration) { x := float64(duration.Nanoseconds()) for { // 1. Atomically load the current parameters oldPtr := atomic.LoadPointer(&amp;amp;ts.params) currentParams := (*samplingParams)(oldPtr) // 2. Calculate the new parameters based on the old ones newLambda := currentParams.lambda + 1 newMu := (currentParams.lambda*currentParams.mu + x) / newLambda newAlpha := currentParams.alpha + 0.5 diff := x - currentParams.mu newBeta := currentParams.beta + (currentParams.lambda*diff*diff) /(2*newLambda) newParams := &amp;amp;samplingParams{ mu: newMu, lambda: newLambda, alpha: newAlpha, beta: newBeta, } // 3. Try to atomically swap the old pointer with the new one. // If another goroutine changed the pointer in the meantime, this will fail, and we will loop again to retry the whole operation. if atomic.CompareAndSwapPointer(&amp;amp;ts.params, oldPtr, unsafe.Pointer(newParams)) { return } } }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;To model this, we define four parameters for each strategy: &lt;code&gt;mu&lt;/code&gt;, &lt;code&gt;lambda&lt;/code&gt;, &lt;code&gt;alpha&lt;/code&gt; and &lt;code&gt;beta&lt;/code&gt;. First we draw a random precision (&lt;code&gt;tau&lt;/code&gt;) from a Gamma distribution (PDF), where &lt;code&gt;alpha&lt;/code&gt; acts as the shape of the precision curve and &lt;code&gt;beta&lt;/code&gt; as the rate. Once we have this precision, we estimate the mean. We draw a sample mean using initial estimate &lt;code&gt;mu&lt;/code&gt;, our confidence factor &lt;code&gt;lambda&lt;/code&gt;, and the precision &lt;code&gt;tau&lt;/code&gt; we just sampled. Essentially, &lt;code&gt;mu&lt;/code&gt; is our best guess, and &lt;code&gt;lambda&lt;/code&gt; represents how stubbornly we hold onto that guess.&lt;/p&gt;
    &lt;p&gt;For strategies proven to be consistent and reliable, we initialize with a high &lt;code&gt;lambda&lt;/code&gt;. This biases the system toward exploitation, preventing unnecessary exploration during a cold start. However, when combining this high &lt;code&gt;lambda&lt;/code&gt; with a high &lt;code&gt;alpha&lt;/code&gt; and a low &lt;code&gt;beta&lt;/code&gt; it tells the system expect low variance. Initially, when the system observes latency values drifting from the mean, the high &lt;code&gt;lambda&lt;/code&gt; forces the model to resist moving the mean. It treats these drifted values as outliers rather than a new baseline. However, the "error" (distance from the mean) is fed into the variance update. Because the starting &lt;code&gt;beta&lt;/code&gt; was so low, even a moderate error represents a massive relative increase. This causes the estimated variance to explode. The system will shift from "I am certain" to "I have no idea, I need to explore." This sudden spike in uncertainty forces the model to stop exploiting and start exploring. The code below shows an example of the configuration values selected for one strategy that facilitates exploitation in cold starts.&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;// This strategy is configured to show that it has proven fast and consistent. var weight2Plan = &amp;amp;planner.PlanConfig{ Name: WeightTwoStrategyName, InitialGuess: 20 * time.Millisecond, // High Lambda: Represents strong confidence in the initial guess. Lambda: 10.0, // High Alpha, Low Beta: Creates a very NARROW belief about variance. // This tells the planner: "I am very confident that the performance is // consistently close to 10ms". A single slow run will be a huge surprise // and will dramatically shift this belief. // High expected precision: 𝐸[𝜏]= 𝛼/𝛽 = 20/2 = 10 // Low expected variance: E[σ2]= β/(α−1) =2/9 = 0.105, narrow jitter // A slow sample will look like an outlier and move the posterior noticeably but overall this prior exploits. Alpha: 20, Beta: 2, }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;On the opposite extreme, we have Weak Priors. This setup is ideal for general-purpose strategies where latency is volatile and depends entirely on the data distribution. Here, we initialize with a minimal &lt;code&gt;lambda&lt;/code&gt; (λ=1) (representing negligible confidence) and uninformative &lt;code&gt;alpha&lt;/code&gt; (α=0.5) and &lt;code&gt;beta&lt;/code&gt; (β=0.5) parameters. This configuration heavily favors exploration during a cold start. The system is effectively saying: "I have a rough hypothesis, but I don't really know anything. I am ready to be convinced otherwise immediately." When this strategy yields low latency values, the weak &lt;code&gt;lambda&lt;/code&gt; ensures the initial mean is washed away within the first few observations. Because the estimated mean adapts instantly to follow the data, the calculated error remains small. The system quickly converges, learning that this strategy is not only fast but also stable. However, this agility works both ways: if the data shifts and latency spikes, the model will detect the degradation instantly and pivot to a different strategy. The code below shows an example of the configuration values selected for one strategy that facilitates exploration in cold starts.&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;var DefaultPlan = &amp;amp;planner.PlanConfig{ Name: DefaultStrategyName, InitialGuess: 50 * time.Millisecond, // Low Lambda: Represents zero confidence. It's a pure guess. Lambda: 1, // With α = 0.5 ≤ 1, it means maximum uncertainty about variance; with λ = 1, we also have weak confidence in the mean. // These values will encourage strong exploration of other strategies. Having these values for the default execute helps to enforce the usage of the "faster" strategies, // helping out with the cold start when we don't have enough data. Alpha: 0.5, Beta: 0.5, }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Production Results: Performance and the Self-Tuning Impact&lt;/head&gt;
    &lt;p&gt;Impact of the Fine-Tuned Planner on P99 Latency&lt;/p&gt;
    &lt;p&gt;The impact of deploying the fine-tuned planner to Auth0 FGA, our managed authorization service powered by OpenFGA, was immediate. While the results were understandably nuanced given our multi-tenant environment. Specifically, for some of our most complex models, we saw peak P99 latency collapse to under 50ms — a massive 98% reduction for those specific cases. While we saw solid performance gains across the rest of the Auth0 FGA customer base, those particular workloads represented the most dramatic shift.&lt;/p&gt;
    &lt;p&gt;The most surprising insight emerged from the diversity of our multi-tenant production environment. While our pre-deployment benchmarks confirmed that new strategies outperformed the legacy strategies across standard test distributions, real-world data is often far more varied than synthetic benchmarks. In production, the planner revealed that for certain specific customer distributions, the legacy logic remained the optimal choice. It successfully identified and exploited a performance niche that our initial analysis had not prioritized, proving that for those specific subgraphs, the original path was still the fastest.The graph below shows how the strategy selection changed after the rollout.&lt;/p&gt;
    &lt;p&gt;Real-time breakdown of strategy selection percentages pre- and post-rollout.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;p&gt;This project reinforced several core engineering tenets. First, for dynamic environments like authorization, a principled statistical model can be more robust than brittle, hand-tuned heuristics. Second, by prioritizing high-performance concurrency from the start, we engineered a solution that fits into the critical path of OpenFGA with negligible overhead. Finally, this work delivers immediate value to the entire OpenFGA community, providing a more performant and resilient system out of the box.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46724542</guid><pubDate>Thu, 22 Jan 2026 20:10:25 +0000</pubDate></item><item><title>Show HN: CLI for working with Apple Core ML models</title><link>https://github.com/schappim/coreml-cli</link><description>&lt;doc fingerprint="7f5845526018551a"&gt;
  &lt;main&gt;
    &lt;p&gt;A native command-line interface for working with Apple Core ML models on macOS. Inspect, run inference, benchmark, and manage Core ML models without Xcode or Python.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Inspect - View model structure, inputs/outputs, and metadata&lt;/item&gt;
      &lt;item&gt;Predict - Run inference on images, text, or JSON data&lt;/item&gt;
      &lt;item&gt;Batch - Process multiple files with concurrent execution&lt;/item&gt;
      &lt;item&gt;Benchmark - Measure inference latency and throughput&lt;/item&gt;
      &lt;item&gt;Compile - Convert &lt;code&gt;.mlmodel&lt;/code&gt;to optimized&lt;code&gt;.mlmodelc&lt;/code&gt;format&lt;/item&gt;
      &lt;item&gt;Metadata - View and manage model metadata&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;brew tap schappim/coreml-cli
brew install coreml-cli&lt;/code&gt;
    &lt;p&gt;Download the latest release from GitHub Releases:&lt;/p&gt;
    &lt;code&gt;curl -L https://github.com/schappim/coreml-cli/releases/download/v1.0.0/coreml-1.0.0-macos.tar.gz -o coreml.tar.gz
tar -xzf coreml.tar.gz
sudo mv coreml /usr/local/bin/&lt;/code&gt;
    &lt;p&gt;Requires macOS 13+ and Swift 5.9+&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/schappim/coreml-cli.git
cd coreml-cli
swift build -c release
sudo cp .build/release/coreml /usr/local/bin/&lt;/code&gt;
    &lt;code&gt;coreml --version
# coreml 1.0.0&lt;/code&gt;
    &lt;p&gt;View model structure, inputs, outputs, and metadata:&lt;/p&gt;
    &lt;code&gt;coreml inspect MobileNetV2.mlmodel&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Model: MobileNetV2
Size: 24.7 MB
Compiled: No

Inputs:
  image: image 224x224 BGRA32

Outputs:
  classLabel: string
  classLabelProbs: dictionary

Metadata:
  Author: Original Paper: Mark Sandler, Andrew Howard...
  Description: Detects the dominant objects present in an image...
&lt;/code&gt;
    &lt;p&gt;JSON output for scripting:&lt;/p&gt;
    &lt;code&gt;coreml inspect MobileNetV2.mlmodel --json&lt;/code&gt;
    &lt;p&gt;Classify an image:&lt;/p&gt;
    &lt;code&gt;coreml predict MobileNetV2.mlmodel --input photo.jpg&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Input: photo.jpg
Inference time: 1.66 ms

Outputs:
  classLabel: golden retriever
  classLabelProbs: golden retriever: 0.8721, Labrador retriever: 0.0543...
&lt;/code&gt;
    &lt;p&gt;Save results to file:&lt;/p&gt;
    &lt;code&gt;coreml predict MobileNetV2.mlmodel --input photo.jpg --output results.json --json&lt;/code&gt;
    &lt;p&gt;Select compute device:&lt;/p&gt;
    &lt;code&gt;coreml predict MobileNetV2.mlmodel --input photo.jpg --device ane  # Apple Neural Engine
coreml predict MobileNetV2.mlmodel --input photo.jpg --device gpu  # GPU
coreml predict MobileNetV2.mlmodel --input photo.jpg --device cpu  # CPU only&lt;/code&gt;
    &lt;p&gt;Process a directory of images:&lt;/p&gt;
    &lt;code&gt;coreml batch MobileNetV2.mlmodel --dir ./photos --out ./results --format csv&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Found 100 input files
Results written to: ./results/results.csv

Processed 100 files in 892.45 ms
Average inference time: 2.15 ms
&lt;/code&gt;
    &lt;p&gt;Control concurrency:&lt;/p&gt;
    &lt;code&gt;coreml batch MobileNetV2.mlmodel --dir ./photos --out ./results --concurrency 8&lt;/code&gt;
    &lt;p&gt;Measure inference latency:&lt;/p&gt;
    &lt;code&gt;coreml benchmark MobileNetV2.mlmodel --input sample.jpg&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Benchmark Results for: MobileNetV2
==================================================

Configuration:
  Device: all
  Iterations: 100
  Warmup: 10

Latency (ms):
  Mean:      1.279
  Min:       1.008
  Max:       1.602
  StdDev:    0.204

Percentiles (ms):
  P50:       1.200
  P95:       1.523
  P99:       1.589

Throughput: 781.86 inferences/sec
&lt;/code&gt;
    &lt;p&gt;Custom iterations:&lt;/p&gt;
    &lt;code&gt;coreml benchmark MobileNetV2.mlmodel --input sample.jpg -n 500 --warmup 50&lt;/code&gt;
    &lt;p&gt;JSON output for CI/CD:&lt;/p&gt;
    &lt;code&gt;coreml benchmark MobileNetV2.mlmodel --input sample.jpg --json &amp;gt; benchmark.json&lt;/code&gt;
    &lt;p&gt;Compile &lt;code&gt;.mlmodel&lt;/code&gt; to optimized &lt;code&gt;.mlmodelc&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;coreml compile MobileNetV2.mlmodel&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Compilation successful!
  Source: /path/to/MobileNetV2.mlmodel
  Output: /path/to/MobileNetV2.mlmodelc
  Original size: 24.7 MB
  Compiled size: 24.5 MB
&lt;/code&gt;
    &lt;p&gt;With validation:&lt;/p&gt;
    &lt;code&gt;coreml compile MobileNetV2.mlmodel --validate --output-dir ./compiled/&lt;/code&gt;
    &lt;p&gt;Get model metadata:&lt;/p&gt;
    &lt;code&gt;coreml meta get MobileNetV2.mlmodel&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Metadata for: MobileNetV2.mlmodel

  Author:      Original Paper: Mark Sandler, Andrew Howard...
  Description: Detects the dominant objects present in an image...
  License:     Please see https://github.com/tensorflow/tensorflow...
  Version:     1.0
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml inspect &amp;lt;model&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Inspect model structure and metadata&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml predict &amp;lt;model&amp;gt; -i &amp;lt;input&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Run inference on a single input&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml batch &amp;lt;model&amp;gt; --dir &amp;lt;dir&amp;gt; --out &amp;lt;dir&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Batch process multiple inputs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml benchmark &amp;lt;model&amp;gt; -i &amp;lt;input&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Benchmark model performance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml compile &amp;lt;model&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Compile model to optimized format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;coreml meta get &amp;lt;model&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;View model metadata&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;--json&lt;/code&gt;, &lt;code&gt;-j&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Output in JSON format&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;--device &amp;lt;device&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Compute device: &lt;code&gt;cpu&lt;/code&gt;, &lt;code&gt;gpu&lt;/code&gt;, &lt;code&gt;ane&lt;/code&gt;, or &lt;code&gt;all&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;--help&lt;/code&gt;, &lt;code&gt;-h&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Show help information&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;--version&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Show version&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Extensions&lt;/cell&gt;
        &lt;cell role="head"&gt;Used For&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Images&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;.jpg&lt;/code&gt;, &lt;code&gt;.jpeg&lt;/code&gt;, &lt;code&gt;.png&lt;/code&gt;, &lt;code&gt;.heic&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Vision models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Audio&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;.wav&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Sound classification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Text&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;.txt&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;NLP models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Tensors&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;.json&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Custom models&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;#!/bin/bash
# Classify all images in a folder and generate a report

MODEL="MobileNetV2.mlmodel"
INPUT_DIR="./images"
OUTPUT_DIR="./classifications"

# Run batch classification
coreml batch "$MODEL" --dir "$INPUT_DIR" --out "$OUTPUT_DIR" --format csv

# View results
cat "$OUTPUT_DIR/results.csv"&lt;/code&gt;
    &lt;code&gt;#!/bin/bash
# Compare inference speed across compute devices

MODEL="MobileNetV2.mlmodel"
INPUT="test.jpg"

echo "CPU Only:"
coreml benchmark "$MODEL" -i "$INPUT" --device cpu -n 50 --json | jq '.meanLatencyMs'

echo "GPU:"
coreml benchmark "$MODEL" -i "$INPUT" --device gpu -n 50 --json | jq '.meanLatencyMs'

echo "Neural Engine:"
coreml benchmark "$MODEL" -i "$INPUT" --device ane -n 50 --json | jq '.meanLatencyMs'&lt;/code&gt;
    &lt;code&gt;# GitHub Actions example
- name: Benchmark Model
  run: |
    coreml benchmark model.mlmodel -i test.jpg --json &amp;gt; benchmark.json

- name: Check Performance Regression
  run: |
    LATENCY=$(jq '.meanLatencyMs' benchmark.json)
    if (( $(echo "$LATENCY &amp;gt; 10" | bc -l) )); then
      echo "Performance regression detected: ${LATENCY}ms"
      exit 1
    fi&lt;/code&gt;
    &lt;p&gt;For models that accept numeric tensor inputs (not images), you can pass JSON arrays:&lt;/p&gt;
    &lt;p&gt;Create a JSON input file (&lt;code&gt;input.json&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;[5.1, 3.5, 1.4, 0.2]&lt;/code&gt;
    &lt;p&gt;Run prediction:&lt;/p&gt;
    &lt;code&gt;coreml predict MyClassifier.mlmodel --input input.json&lt;/code&gt;
    &lt;p&gt;Output:&lt;/p&gt;
    &lt;code&gt;Input: input.json
Inference time: 0.12 ms

Outputs:
  probabilities: [0.1377, 0.7100, 0.1522]
&lt;/code&gt;
    &lt;p&gt;Batch process multiple JSON files:&lt;/p&gt;
    &lt;code&gt;# Create a directory with JSON input files
mkdir json_samples
echo '[5.1, 3.5, 1.4, 0.2]' &amp;gt; json_samples/sample1.json
echo '[6.7, 3.1, 4.7, 1.5]' &amp;gt; json_samples/sample2.json
echo '[5.9, 3.0, 5.1, 1.8]' &amp;gt; json_samples/sample3.json
echo '[4.6, 3.4, 1.4, 0.3]' &amp;gt; json_samples/sample4.json

# Run batch prediction
coreml batch MyClassifier.mlmodel --dir json_samples --out json_results --format csv&lt;/code&gt;
    &lt;p&gt;Output CSV (&lt;code&gt;json_results/results.csv&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;input_file,inference_time_ms,probabilities
sample1.json,0.27,"[0.1377, 0.7100, 0.1522]"
sample2.json,0.22,"[0.0613, 0.5931, 0.3456]"
sample3.json,0.29,"[0.0522, 0.5000, 0.4479]"
sample4.json,0.17,"[0.1406, 0.6825, 0.1769]"&lt;/code&gt;
    &lt;p&gt;This is useful for models trained on tabular data, embeddings, or any non-image numeric inputs.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 or later&lt;/item&gt;
      &lt;item&gt;Apple Silicon or Intel Mac&lt;/item&gt;
      &lt;item&gt;Core ML models (&lt;code&gt;.mlmodel&lt;/code&gt;,&lt;code&gt;.mlpackage&lt;/code&gt;, or&lt;code&gt;.mlmodelc&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;MIT License - see LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Please open an issue or submit a pull request.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built with Swift Argument Parser&lt;/item&gt;
      &lt;item&gt;Uses Apple's Core ML framework&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46724565</guid><pubDate>Thu, 22 Jan 2026 20:12:26 +0000</pubDate></item></channel></rss>