<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 10 Dec 2025 21:41:03 +0000</lastBuildDate><item><title>Factor 0.101 now available</title><link>https://re.factorcode.org/2025/12/factor-0-101-now-available.html</link><description>&lt;doc fingerprint="748063284d11bc94"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Factor 0.101 now available&lt;/head&gt;
    &lt;p&gt;Monday, December 8, 2025&lt;/p&gt;
    &lt;p&gt;“Keep thy airspeed up, lest the earth come from below and smite thee.” - William Kershner&lt;/p&gt;
    &lt;p&gt;I’m very pleased to announce the release of Factor 0.101!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;OS/CPU&lt;/cell&gt;
        &lt;cell role="head"&gt;Windows&lt;/cell&gt;
        &lt;cell role="head"&gt;Mac OS&lt;/cell&gt;
        &lt;cell role="head"&gt;Linux&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;x86&lt;/cell&gt;
        &lt;cell&gt;0.101&lt;/cell&gt;
        &lt;cell&gt;0.101&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;x86-64&lt;/cell&gt;
        &lt;cell&gt;0.101&lt;/cell&gt;
        &lt;cell&gt;0.101&lt;/cell&gt;
        &lt;cell&gt;0.101&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Source code: 0.101&lt;/p&gt;
    &lt;p&gt;This release is brought to you with almost 700 commits by the following individuals:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Aleksander Sabak, Andy Kluger, Cat Stevens, Dmitry Matveyev, Doug Coleman, Giftpflanze, John Benediktsson, Jon Harper, Jonas Bernouli, Leo Mehraban, Mike Stevenson, Nicholas Chandoke, Niklas Larsson, Rebecca Kelly, Samuel Tardieu, Stefan Schmiedl, @Bruno-366, @bobisageek, @coltsingleactionarmyocelot, @inivekin, @knottio, @timor&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Besides some bug fixes and library improvements, I want to highlight the following changes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Moved the UI to render buttons and scrollbars rather than using images, which allows easier theming.&lt;/item&gt;
      &lt;item&gt;Fixed HiDPI scaling on Linux and Windows, although it currently doesn’t update the window settings when switching between screens with different scaling factors.&lt;/item&gt;
      &lt;item&gt;Update to Unicode 17.0.0.&lt;/item&gt;
      &lt;item&gt;Plugin support for the Neovim editor.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some possible backwards compatibility issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The argument order to &lt;code&gt;ltake&lt;/code&gt;was swapped to be more consistent with words like&lt;code&gt;head&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;environment&lt;/code&gt;vocabulary on Windows now supports disambiguating&lt;code&gt;f&lt;/code&gt;and&lt;code&gt;""&lt;/code&gt;(empty) values&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;misc/atom&lt;/code&gt;folder was removed in favor of the factor/atom-language-factor repo.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;misc/Factor.tmbundle&lt;/code&gt;folder was removed in favor of the factor/factor.tmbundle repo.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;misc/vim&lt;/code&gt;folder was removed in favor of the factor/factor.vim repo.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;http&lt;/code&gt;vocabulary&lt;code&gt;request&lt;/code&gt;tuple had a slot rename from&lt;code&gt;post-data&lt;/code&gt;to&lt;code&gt;data&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;furnace.asides&lt;/code&gt;vocabulary had a slot rename from&lt;code&gt;post-data&lt;/code&gt;to&lt;code&gt;data&lt;/code&gt;, and might require running&lt;code&gt;ALTER TABLE asides RENAME COLUMN "post-data" TO data;&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;html.streams&lt;/code&gt;vocabulary was renamed to&lt;code&gt;io.streams.html&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;pdf.streams&lt;/code&gt;vocabulary was renamed to&lt;code&gt;io.streams.pdf&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;What is Factor&lt;/head&gt;
    &lt;p&gt;Factor is a concatenative, stack-based programming language with high-level features including dynamic types, extensible syntax, macros, and garbage collection. On a practical side, Factor has a full-featured library, supports many different platforms, and has been extensively documented.&lt;/p&gt;
    &lt;p&gt;The implementation is fully compiled for performance, while still supporting interactive development. Factor applications are portable between all common platforms. Factor can deploy stand-alone applications on all platforms. Full source code for the Factor project is available under a BSD license.&lt;/p&gt;
    &lt;head rend="h3"&gt;New libraries:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;base92: adding support for Base92 encoding/decoding&lt;/item&gt;
      &lt;item&gt;bitcask: implementing the Bitcask key/value database&lt;/item&gt;
      &lt;item&gt;bluesky: adding support for the BlueSky protocol&lt;/item&gt;
      &lt;item&gt;calendar.holidays.world: adding some new holidays including World Emoji Day&lt;/item&gt;
      &lt;item&gt;classes.enumeration: adding enumeration classes and new &lt;code&gt;ENUMERATION:&lt;/code&gt;syntax word&lt;/item&gt;
      &lt;item&gt;colors.oklab: adding support for OKLAB color space&lt;/item&gt;
      &lt;item&gt;colors.oklch: adding support for OKLCH color space&lt;/item&gt;
      &lt;item&gt;colors.wavelength: adding &lt;code&gt;wavelength&amp;gt;rgba&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;combinators.syntax: adding experimental combinator syntax words &lt;code&gt;@[&lt;/code&gt;,&lt;code&gt;*[&lt;/code&gt;, and&lt;code&gt;&amp;amp;[&lt;/code&gt;, and short-circuiting&lt;code&gt;n&amp;amp;&amp;amp;[&lt;/code&gt;,&lt;code&gt;n||[&lt;/code&gt;,&lt;code&gt;&amp;amp;&amp;amp;[&lt;/code&gt;and&lt;code&gt;||[&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;continuations.extras: adding &lt;code&gt;with-datastacks&lt;/code&gt;and&lt;code&gt;datastack-states&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;dotenv: implementing support for Dotenv files&lt;/item&gt;
      &lt;item&gt;edn: implementing support for Extensible Data Notation&lt;/item&gt;
      &lt;item&gt;editors.cursor: adding support for the Cursor editor&lt;/item&gt;
      &lt;item&gt;editors.rider: adding support for the JetBrains Rider editor&lt;/item&gt;
      &lt;item&gt;gitignore: parser for &lt;code&gt;.gitignore&lt;/code&gt;files&lt;/item&gt;
      &lt;item&gt;http.json: promoted &lt;code&gt;json.http&lt;/code&gt;and added some useful words&lt;/item&gt;
      &lt;item&gt;io.streams.farkup: a Farkup formatted stream protocol&lt;/item&gt;
      &lt;item&gt;io.streams.markdowns: a Markdown formatted stream protocol&lt;/item&gt;
      &lt;item&gt;locals.lazy: prototype of emit syntax&lt;/item&gt;
      &lt;item&gt;monadics: alternative vocabulary for using Haskell-style monads, applicatives, and functors&lt;/item&gt;
      &lt;item&gt;multibase: implementation of Multibase&lt;/item&gt;
      &lt;item&gt;pickle: support for the Pickle serialization format&lt;/item&gt;
      &lt;item&gt;persistent.hashtables.identity: support an identity-hashcode version of persisent hashtables&lt;/item&gt;
      &lt;item&gt;raylib.live-coding: demo of a vocabulary to do “live coding” of Raylib programs&lt;/item&gt;
      &lt;item&gt;rdap: support for the Registration Data Access Protocol&lt;/item&gt;
      &lt;item&gt;reverse: implementation of the std::flip&lt;/item&gt;
      &lt;item&gt;slides.cli: simple text-based command-line interface for slides&lt;/item&gt;
      &lt;item&gt;tools.highlight: command-line syntax-highlighting tool&lt;/item&gt;
      &lt;item&gt;tools.random: command-line random generator tool&lt;/item&gt;
      &lt;item&gt;ui.pens.rounded: adding rounded corner pen&lt;/item&gt;
      &lt;item&gt;ui.pens.theme: experimental themed pen&lt;/item&gt;
      &lt;item&gt;ui.tools.theme: some words for updating UI developer tools themes&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Improved libraries:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;alien.syntax: added &lt;code&gt;C-LIBRARY:&lt;/code&gt;syntax word&lt;/item&gt;
      &lt;item&gt;assocs.extras: added &lt;code&gt;nzip&lt;/code&gt;and&lt;code&gt;nunzip&lt;/code&gt;,&lt;code&gt;map-zip&lt;/code&gt;and&lt;code&gt;map-unzip&lt;/code&gt;macros&lt;/item&gt;
      &lt;item&gt;base32: adding the human-oriented Base32 encoding via &lt;code&gt;zbase32&amp;gt;&lt;/code&gt;and&lt;code&gt;&amp;gt;zbase32&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;base64: minor performance improvement&lt;/item&gt;
      &lt;item&gt;benchmark: adding more benchmarks&lt;/item&gt;
      &lt;item&gt;bootstrap.assembler: fixes for ARM-64&lt;/item&gt;
      &lt;item&gt;brainfuck: added &lt;code&gt;BRAINFUCK:&lt;/code&gt;syntax word and&lt;code&gt;interpret-brainfuck&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;bson: use linked-assocs to preserve order&lt;/item&gt;
      &lt;item&gt;cache: implement &lt;code&gt;M\ cache-assoc delete-at&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;calendar: adding &lt;code&gt;year&amp;lt;&lt;/code&gt;,&lt;code&gt;year&amp;lt;=&lt;/code&gt;,&lt;code&gt;year&amp;gt;&lt;/code&gt;,&lt;code&gt;year&amp;gt;=&lt;/code&gt;words&lt;/item&gt;
      &lt;item&gt;calendar.format: parse human-readable and elapsed-time output back into duration objects&lt;/item&gt;
      &lt;item&gt;cbor: use linked-assocs to preserve order&lt;/item&gt;
      &lt;item&gt;classes.mixin: added &lt;code&gt;definer&lt;/code&gt;implementation&lt;/item&gt;
      &lt;item&gt;classes.singleton: added &lt;code&gt;definer&lt;/code&gt;implementation&lt;/item&gt;
      &lt;item&gt;classes.tuple: added &lt;code&gt;tuple&amp;gt;slots&lt;/code&gt;, rename&lt;code&gt;tuple&amp;gt;array&lt;/code&gt;to&lt;code&gt;pack-tuple&lt;/code&gt;and&lt;code&gt;&amp;gt;tuple&lt;/code&gt;to&lt;code&gt;unpack-tuple&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;classes.union: added &lt;code&gt;definer&lt;/code&gt;implementation&lt;/item&gt;
      &lt;item&gt;checksums.sha: some 20-40% performance improvements&lt;/item&gt;
      &lt;item&gt;command-line: allow passing script name of &lt;code&gt;-&lt;/code&gt;to use stdin&lt;/item&gt;
      &lt;item&gt;command-line.parser: support for Argument Parser Commands&lt;/item&gt;
      &lt;item&gt;command-line.startup: document &lt;code&gt;-q&lt;/code&gt;quiet mode flag&lt;/item&gt;
      &lt;item&gt;concurrency.combinators: faster &lt;code&gt;parallel-map&lt;/code&gt;and&lt;code&gt;parallel-assoc-map&lt;/code&gt;using a count-down latch&lt;/item&gt;
      &lt;item&gt;concurrency.promises: 5-7% performance improvement&lt;/item&gt;
      &lt;item&gt;continuations: improve docs and fix stack effect for &lt;code&gt;ifcc&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;countries: adding &lt;code&gt;CQ&lt;/code&gt;country code for Sark&lt;/item&gt;
      &lt;item&gt;cpu.architecture: fix &lt;code&gt;*-branch&lt;/code&gt;stack effects&lt;/item&gt;
      &lt;item&gt;cpu.arm: fixes for ARM-64&lt;/item&gt;
      &lt;item&gt;crontab: added &lt;code&gt;parse-crontab&lt;/code&gt;which ignores blank lines and comments&lt;/item&gt;
      &lt;item&gt;db: making &lt;code&gt;query-each&lt;/code&gt;row-polymorphic&lt;/item&gt;
      &lt;item&gt;delegate.protocols: adding &lt;code&gt;keys&lt;/code&gt;and&lt;code&gt;values&lt;/code&gt;to&lt;code&gt;assoc-protocol&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;discord: better support for network disconnects, added a configurable retry interval&lt;/item&gt;
      &lt;item&gt;discord.chatgpt-bot: some fixes for LM Studio&lt;/item&gt;
      &lt;item&gt;editors: make the editor restart nicer looking&lt;/item&gt;
      &lt;item&gt;editors.focus: support open-file-to-line-number on newer releases, support Linux and Window&lt;/item&gt;
      &lt;item&gt;editors.zed: support use of Zed on Linux&lt;/item&gt;
      &lt;item&gt;endian: faster endian conversions of c-ptr-like objects&lt;/item&gt;
      &lt;item&gt;environment: adding &lt;code&gt;os-env?&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;eval: move datastack and error messages to stderr&lt;/item&gt;
      &lt;item&gt;fonts: make &lt;code&gt;&amp;lt;font&amp;gt;&lt;/code&gt;take a name, easier defaults&lt;/item&gt;
      &lt;item&gt;furnace.asides: rename &lt;code&gt;post-data&lt;/code&gt;slot on&lt;code&gt;aside&lt;/code&gt;tuples to&lt;code&gt;data&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;generalizations: moved some dip words to shuffle&lt;/item&gt;
      &lt;item&gt;help.tour: fix some typos/grammar&lt;/item&gt;
      &lt;item&gt;html.templates.chloe: improve use of &lt;code&gt;CDATA&lt;/code&gt;tags for unescaping output&lt;/item&gt;
      &lt;item&gt;http: rename &lt;code&gt;post-data&lt;/code&gt;slot on&lt;code&gt;request&lt;/code&gt;tuples to&lt;code&gt;data&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;http.json: adding &lt;code&gt;http-json&lt;/code&gt;that doesn’t return the response object&lt;/item&gt;
      &lt;item&gt;http.websockets: making &lt;code&gt;read-websocket-loop&lt;/code&gt;row-polymorphic&lt;/item&gt;
      &lt;item&gt;ini-file: adding &lt;code&gt;ini&amp;gt;file&lt;/code&gt;,&lt;code&gt;file&amp;gt;ini&lt;/code&gt;, and use&lt;code&gt;LH{ }&lt;/code&gt;to preserve configuration order&lt;/item&gt;
      &lt;item&gt;io.encodings.detect: adding &lt;code&gt;utf7&lt;/code&gt;detection&lt;/item&gt;
      &lt;item&gt;io.encodings.utf8: adding &lt;code&gt;utf8-bom&lt;/code&gt;to handle optional BOM&lt;/item&gt;
      &lt;item&gt;io.random: speed up &lt;code&gt;random-line&lt;/code&gt;and&lt;code&gt;random-lines&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;io.streams.ansi: adding documentation and tests, support dim foreground on terminals that support it&lt;/item&gt;
      &lt;item&gt;io.streams.escape-codes: adding documentation and tests&lt;/item&gt;
      &lt;item&gt;ip-parser: adding IPV4 and IPV6 network words&lt;/item&gt;
      &lt;item&gt;kernel: adding &lt;code&gt;until*&lt;/code&gt;, fix docs for&lt;code&gt;and*&lt;/code&gt;and&lt;code&gt;or*&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;linked-sets: adding &lt;code&gt;LS{&lt;/code&gt;syntax word&lt;/item&gt;
      &lt;item&gt;lists.lazy: changed the argument order in &lt;code&gt;ltake&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;macho: support a few more link edit commands&lt;/item&gt;
      &lt;item&gt;make: adding &lt;code&gt;,%&lt;/code&gt;for a&lt;code&gt;push-at&lt;/code&gt;variant&lt;/item&gt;
      &lt;item&gt;mason.release.tidy: cleanup a few more git artifacts&lt;/item&gt;
      &lt;item&gt;math.combinatorics: adding counting words&lt;/item&gt;
      &lt;item&gt;math.distances: adding &lt;code&gt;jaro-distance&lt;/code&gt;and&lt;code&gt;jaro-winkler-distance&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;math.extras: added &lt;code&gt;all-removals&lt;/code&gt;, support RecamÃ¡nâs sequence, and Tribonacci Numbers&lt;/item&gt;
      &lt;item&gt;math.factorials: added &lt;code&gt;subfactorial&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;math.functions: added “closest to zero” modulus&lt;/item&gt;
      &lt;item&gt;math.parser: improve ratio parsing for consistency&lt;/item&gt;
      &lt;item&gt;math.primes: make &lt;code&gt;prime?&lt;/code&gt;safe from non-integer inputs&lt;/item&gt;
      &lt;item&gt;math.runge-kutta: make generalized improvements to the Runge-Kutta solver&lt;/item&gt;
      &lt;item&gt;math.similarity: adding &lt;code&gt;jaro-similarity&lt;/code&gt;,&lt;code&gt;jaro-winkler-similarity&lt;/code&gt;, and&lt;code&gt;trigram-similarity&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;math.text.english: fix issue with very large and very small floats&lt;/item&gt;
      &lt;item&gt;metar: updated the abbreviations glossary&lt;/item&gt;
      &lt;item&gt;mime.types: updating &lt;code&gt;mime.types&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;msgpack: use linked-assocs to preserve order&lt;/item&gt;
      &lt;item&gt;qw: adding &lt;code&gt;qw:&lt;/code&gt;syntax&lt;/item&gt;
      &lt;item&gt;path-finding: added &lt;code&gt;find-path*&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;peg.parsers: faster &lt;code&gt;list-of&lt;/code&gt;and&lt;code&gt;list-of-many&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;progress-bars.models: added &lt;code&gt;with-progress-display&lt;/code&gt;,&lt;code&gt;map-with-progress-bar&lt;/code&gt;,&lt;code&gt;each-with-progress-bar&lt;/code&gt;, and&lt;code&gt;reduce-with-progress-bar&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;raylib: adding &lt;code&gt;trace-log&lt;/code&gt;and&lt;code&gt;set-trace-log-level&lt;/code&gt;, updated to Raylib 5.5&lt;/item&gt;
      &lt;item&gt;readline-listener: store history across sessions, support color on terminals that support it&lt;/item&gt;
      &lt;item&gt;robohash: support for &lt;code&gt;"set4"&lt;/code&gt;,&lt;code&gt;"set5"&lt;/code&gt;, and&lt;code&gt;"set6"&lt;/code&gt;types&lt;/item&gt;
      &lt;item&gt;sequences: rename &lt;code&gt;midpoint@&lt;/code&gt;to&lt;code&gt;midpoint&lt;/code&gt;, faster&lt;code&gt;each-from&lt;/code&gt;and&lt;code&gt;map-reduce&lt;/code&gt;on slices&lt;/item&gt;
      &lt;item&gt;sequences.extras: adding &lt;code&gt;find-nth&lt;/code&gt;,&lt;code&gt;find-nth-last&lt;/code&gt;,&lt;code&gt;subseq-indices&lt;/code&gt;,&lt;code&gt;deep-nth&lt;/code&gt;,&lt;code&gt;deep-nth-of&lt;/code&gt;,&lt;code&gt;2none?&lt;/code&gt;,&lt;code&gt;filter-errors&lt;/code&gt;,&lt;code&gt;reject-errors&lt;/code&gt;,&lt;code&gt;all-same?&lt;/code&gt;,&lt;code&gt;adjacent-differences&lt;/code&gt;, and&lt;code&gt;partial-sum&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;sequences.generalizations: fix &lt;code&gt;?firstn&lt;/code&gt;and&lt;code&gt;?lastn&lt;/code&gt;for string inputs, removed&lt;code&gt;(nsequence)&lt;/code&gt;which duplicates&lt;code&gt;set-firstn-unsafe&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;sequences.prefixed: swap order of &lt;code&gt;&amp;lt;prefixed&amp;gt;&lt;/code&gt;arguments to match&lt;code&gt;prefix&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;sequences.repeating: adding &lt;code&gt;&amp;lt;cycles-from&amp;gt;&lt;/code&gt;and&lt;code&gt;cycle-from&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;sequences.snipped: fixed out-of-bounds issues&lt;/item&gt;
      &lt;item&gt;scryfall: update for duskmourn&lt;/item&gt;
      &lt;item&gt;shuffle: improve stack-checking of &lt;code&gt;shuffle(&lt;/code&gt;syntax, added&lt;code&gt;SHUFFLE:&lt;/code&gt;syntax,&lt;code&gt;nreverse&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;sorting: fix &lt;code&gt;sort-with&lt;/code&gt;to apply the quot with access to the stack below&lt;/item&gt;
      &lt;item&gt;sorting.human: implement human sorting improved&lt;/item&gt;
      &lt;item&gt;system-info.macos: adding “Tahoe” code-name for macOS 26&lt;/item&gt;
      &lt;item&gt;terminfo: add words for querying specific output capabilities&lt;/item&gt;
      &lt;item&gt;threads: define a generalized &lt;code&gt;linked-thread&lt;/code&gt;which used to be for&lt;code&gt;concurrency.mailboxes&lt;/code&gt;only&lt;/item&gt;
      &lt;item&gt;toml: use linked-assocs to preserve order, adding &lt;code&gt;&amp;gt;toml&lt;/code&gt;and&lt;code&gt;write-toml&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;tools.annotations: adding &lt;code&gt;&amp;lt;WATCH ... WATCH&amp;gt;&lt;/code&gt;syntax&lt;/item&gt;
      &lt;item&gt;tools.deploy: adding a command-line interface for deploy options&lt;/item&gt;
      &lt;item&gt;tools.deploy.backend: fix boot image location in system-wide installations&lt;/item&gt;
      &lt;item&gt;tools.deploy.unix: change binary name to append &lt;code&gt;.out&lt;/code&gt;to fix conflict with vocab resources&lt;/item&gt;
      &lt;item&gt;tools.directory-to-file: better test file metrics, print filename for editing&lt;/item&gt;
      &lt;item&gt;tools.memory: adding &lt;code&gt;heap-stats-of&lt;/code&gt;arbitrary sequence of instances, and&lt;code&gt;total-size&lt;/code&gt;size of everything pointed to by an object&lt;/item&gt;
      &lt;item&gt;txon: use linked-assocs to preserve order&lt;/item&gt;
      &lt;item&gt;ui: adding &lt;code&gt;adjust-font-size&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ui.gadgets.buttons: stop using images and respect theme colors&lt;/item&gt;
      &lt;item&gt;ui.gadgets.sliders: stop using images and respect theme colors&lt;/item&gt;
      &lt;item&gt;ui.theme.base16: adding a lot more (270!) Base16 Themes&lt;/item&gt;
      &lt;item&gt;ui.tools: adding font-sizing keyboard shortcuts&lt;/item&gt;
      &lt;item&gt;ui.tools.browser: more responsive font sizing&lt;/item&gt;
      &lt;item&gt;ui.tools.listener: more responsive font sizing, adding some UI listener styling&lt;/item&gt;
      &lt;item&gt;ui.tools.listener.completion: allow spaces in history search popup&lt;/item&gt;
      &lt;item&gt;unicode: update to Unicode 17.0.0&lt;/item&gt;
      &lt;item&gt;webapps.planet: improve CSS for &lt;code&gt;video&lt;/code&gt;tags&lt;/item&gt;
      &lt;item&gt;words: adding &lt;code&gt;define-temp-syntax&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;zoneinfo: update to version 2025b&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Removed libraries&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;ui.theme.images&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;VM Improvements:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More work on ARM64 backend (fix set-callstack, fix generic dispatch)&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46216583</guid><pubDate>Wed, 10 Dec 2025 11:33:31 +0000</pubDate></item><item><title>COM Like a Bomb: Rust Outlook Add-in</title><link>https://tritium.legal/blog/outlook</link><description>&lt;doc fingerprint="2e09bb17d99d220d"&gt;
  &lt;main&gt;&lt;p&gt;One of legal tech's clichés is that "lawyers live in Word".&lt;/p&gt;&lt;p&gt;This is demonstrably incorrect. I, for example, am a lawyer and in fact live in London, England.&lt;/p&gt;&lt;p&gt;But what they mean to say is that lawyers spend much of their time editing documents in Microsoft Word. This is because, for the most part, opening &lt;code&gt;.docx&lt;/code&gt; files in Word is the default behavior where it's
        installed (everywhere). Lawyers, and again I'm speaking from experience here, are generally lazy when it comes
        to
        technology. Defaults are the law.&lt;/p&gt;&lt;p&gt;This is rational. Clients pay thousands of dollars per hour to have their legal needs addressed by the top law firms in the world. This means that law firms account for every moment their lawyers' working days. Generally, in 6-minute increments (or, 0.1 hours). No client is paying even 0.3 for their lawyer to learn a new software paradigm, and most law firms don't find forgoing revenue to train lawyers on new systems that will make them faster especially motivating.&lt;/p&gt;&lt;p&gt;So to get a foothold into legal, we need to make Tritium slot as nearly as possible into the existing workflow.&lt;/p&gt;&lt;p&gt;So where does the legal work flow originate?&lt;/p&gt;&lt;p&gt;Three places: (1) the document management system (DMS), (2) the desktop and (3) email.&lt;/p&gt;&lt;p&gt;We've previously talked about iManage, one of the most important document management systems in legal. There are other important ones such as NetDocuments, and our integrations into those will be the subject of another post.&lt;/p&gt;&lt;p&gt;Today, we're focused on the third place.&lt;/p&gt;&lt;p&gt;We're giving access to Tritium right in the lawyer's inbox.&lt;/p&gt;&lt;p&gt;We're going to replicate our "Open with Tritium" desktop entry point in Outlook. Here's what it looks like on the desktop:&lt;/p&gt;&lt;head rend="h2"&gt;Outlook Integration&lt;/head&gt;&lt;p&gt;"New Outlook" is some sort of half-implemented WebView mess that requires javascript round-tripped from a host server to plug in new features.&lt;/p&gt;&lt;p&gt;We'll eventually have to get in there, too, but for the most part law firms seem to have thus far stuck with the much more featureful "legacy Outlook". That version is a venerable, performant, C++-based Windows desktop application.&lt;/p&gt;&lt;p&gt;So, how do we plug into it?&lt;/p&gt;&lt;head rend="h2"&gt;COM&lt;/head&gt;&lt;p&gt;Before even the easy 100 MB of RAM days let alone the advent of &lt;code&gt;node&lt;/code&gt; and &lt;code&gt;electron&lt;/code&gt; and
        &lt;code&gt;JSON&lt;/code&gt;,
        the Windows operating system needed a way to allow processes and applications to communicate in a
        language-agnostic way. This ultimately resulted in the "Component Object Model" or
        COM. COM allows
        us to plug
        into various entry points using a Dynamically Linked Library (.dll) which follows a strict
        ABI with certain
        calling conventions.
    &lt;/p&gt;&lt;p&gt;COM lives on today, and it is still an effective way to communicate with various processes, including Windows 11's File Explorer.&lt;/p&gt;&lt;p&gt;Fortunately, COM is supported in the &lt;code&gt;windows-rs&lt;/code&gt; Rust crate.[1]&lt;/p&gt;&lt;p&gt;To add a link to Outlook's attachment context menu, we need to inherit from a series of COM classes: &lt;code&gt;IDispatch&lt;/code&gt;,
        &lt;code&gt;IDTExtensibility2&lt;/code&gt; and ultimately &lt;code&gt;IRibbonExtensibility&lt;/code&gt;.
    &lt;/p&gt;&lt;p&gt;&lt;code&gt;windows-rs&lt;/code&gt; provides an &lt;code&gt;IDispatch&lt;/code&gt; implementation out-of-the box which exposes a
        &lt;code&gt;trait&lt;/code&gt; that looks like the below:
    &lt;/p&gt;&lt;code&gt;fn GetTypeInfoCount(&amp;amp;self) -&amp;gt; windows::core::Result&amp;lt;u32&amp;gt; {}

fn GetIDsOfNames(
    &amp;amp;self,
    riid: *const GUID,
    rgsz_names: *const PCWSTR,
    c_names: u32,
    lcid: u32,
    rg_disp_id: *mut i32,
) -&amp;gt; std::result::Result&amp;lt;(), windows_core::Error&amp;gt; {}

fn Invoke(
    &amp;amp;self,
    disp_id_member: i32,
    riid: *const GUID,
    lcid: u32,
    w_flags: DISPATCH_FLAGS,
    p_disp_params: *const DISPPARAMS,
    p_var_result: *mut VARIANT,
    p_excep_info: *mut EXCEPINFO,
    pu_arg_err: *mut u32,
) -&amp;gt; std::result::Result&amp;lt;(), windows_core::Error&amp;gt; {}
&lt;/code&gt;&lt;p&gt;These functions provide the basic COM dispatching mechanisms.&lt;/p&gt;&lt;p&gt;Using them a caller is able to look up the &lt;code&gt;rg_disp_id&lt;/code&gt; of a particular named function in your
        implementation, then &lt;code&gt;Invoke&lt;/code&gt; that function with the results optionally populating
        &lt;code&gt;p_var_result&lt;/code&gt; which is a pointer to a mutable union of possible result types.
    &lt;/p&gt;&lt;p&gt;This is the basic wiring which allows us to implement the required &lt;code&gt;IDTExensibility2&lt;/code&gt; and
        &lt;code&gt;IRibbonExtensibility&lt;/code&gt; classes.
    &lt;/p&gt;&lt;p&gt;&lt;code&gt;windows-rs&lt;/code&gt; doesn't implement these classes, but does help us by providing the &lt;code&gt;interface&lt;/code&gt;
        procedural macro which handles setting up the VTables to map our struct's methods to the COM
        ABI.&lt;/p&gt;&lt;p&gt;We use the class's &lt;code&gt;GUID&lt;/code&gt; for the macro to establish that we're implementing
        &lt;code&gt;IDTExtensibility2&lt;/code&gt;.[2]
    &lt;/p&gt;&lt;code&gt;#[windows::core::interface("B65AD801-ABAF-11D0-BB8B-00A0C90F2744")]
pub unsafe trait IDTExtensibility2: IDispatch {
    unsafe fn OnConnection(
        &amp;amp;self,
        _application: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _connectmode: i32,
        _addin_instance: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _custom: SAFEARRAY,
    ) -&amp;gt; HRESULT;
    unsafe fn OnDisconnection(&amp;amp;self, mode: i32, custom: SAFEARRAY) -&amp;gt; HRESULT;
    unsafe fn OnAddInsUpdate(&amp;amp;self, custom: SAFEARRAY) -&amp;gt; HRESULT;
    unsafe fn OnStartupComplete(&amp;amp;self, custom: SAFEARRAY) -&amp;gt; HRESULT;
    unsafe fn OnBeginShutdown(&amp;amp;self, custom: SAFEARRAY) -&amp;gt; HRESULT;
}
&lt;/code&gt;&lt;p&gt;Then, we implement that interface for our &lt;code&gt;struct&lt;/code&gt;.&lt;/p&gt;&lt;code&gt;#[implement(IRibbonExtensibility, IDTExtensibility2, IDispatch)]
struct Addin;
&lt;/code&gt;&lt;p&gt;This causes the procedural macro to generate &lt;code&gt;IRibbonExensibility_Impl&lt;/code&gt;,
        &lt;code&gt;IDTExensibility2_Impl&lt;/code&gt; and &lt;code&gt;IDispatch_Impl&lt;/code&gt; traits for us to implement in
        &lt;code&gt;struct Addin_Impl&lt;/code&gt;.
    &lt;/p&gt;&lt;p&gt;Here's the initial Tritium &lt;code&gt;IDTExensibility2_Impl&lt;/code&gt; verbatim for example:&lt;/p&gt;&lt;code&gt;impl IDTExtensibility2_Impl for Addin_Impl {
    unsafe fn OnConnection(
        &amp;amp;self,
        _application: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _connectmode: i32,
        _addin_instance: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _custom: SAFEARRAY,
    ) -&amp;gt; HRESULT {
        log("OnConnection called()");
        // Don't do any heavy operations here that could crash Outlook
        S_OK
    }

    unsafe fn OnDisconnection(&amp;amp;self, _mode: i32, _custom: SAFEARRAY) -&amp;gt; HRESULT {
        log("OnDisconnection called()");
        S_OK
    }

    unsafe fn OnAddInsUpdate(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {
        log("OnAddInsUpdate called()");
        S_OK
    }

    unsafe fn OnStartupComplete(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {
        log("OnStartupComplete called()");
        S_OK
    }

    unsafe fn OnBeginShutdown(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {
        log("OnBeginShutdown called()");
        S_OK
    }
}
&lt;/code&gt;&lt;p&gt;As discussed below, we used an LLM to generate these signatures since they aren't provided in the &lt;code&gt;windows-rs&lt;/code&gt; crate out of the box.
    &lt;/p&gt;&lt;p&gt;Since our simple add-in at this point doesn't maintain any global state that would otherwise be constructed, adjusted and deconstructed at &lt;code&gt;OnConnection&lt;/code&gt;, &lt;code&gt;OnAddInsUpdate&lt;/code&gt; and
        &lt;code&gt;OnBeginShutdown&lt;/code&gt;, respectively, we just log the call for debugging and return &lt;code&gt;S_OK&lt;/code&gt;.
    &lt;/p&gt;&lt;p&gt;Now, being somewhat "vintage" in 2025, COM is noticeably not well documented on the web.&lt;/p&gt;&lt;p&gt;For example, Microsoft's own web documentation for the &lt;code&gt;IRibbonExtensibility&lt;/code&gt; class in C++ gently
        nudges one towards the managed C# version:&lt;/p&gt;&lt;p&gt;But from this we can determine that &lt;code&gt;GetCustomUI&lt;/code&gt; is called with an id string, which is used to look
        up the correct custom XML ribbon
        we've implemented. That is returned to the caller. In our case, that's Outlook.&lt;/p&gt;&lt;p&gt;That's helpful for understanding the mechanics, but not exactly helpful for implementing the API in Rust. In fact, despite many minutes of bona fide web searching, I was unable to locate the C++ signature for &lt;code&gt;IRibbonExtensibility&lt;/code&gt;.
    &lt;/p&gt;&lt;p&gt;But, it's 2025 and since modern LLMs have ingested and essentially compressed the entire web, plus all books and New York Times articles ever written, we can ask them to generate a signature for &lt;code&gt;IRibbonExtensibility&lt;/code&gt; for us!
    &lt;/p&gt;&lt;p&gt;This is what Claude one-shotted at the time:&lt;/p&gt;&lt;code&gt;impl IRibbonExtensibility_Impl for Addin {
    unsafe fn GetCustomUI(&amp;amp;self, _ribbon_id: BSTR, xml: *mut BSTR) -&amp;gt; HRESULT {
        // Only provide ribbon XML for specific ribbon IDs or all if we want global
        // ribbon For now, we'll provide it for all requests
        unsafe {
            *xml = BSTR::from(RIBBON_XML);
        }
        S_OK
    }
}
&lt;/code&gt;&lt;p&gt;So, unlike the C# code which returns our custom XML, C++ and, thus the Rust implementation, wants an &lt;code&gt;HRESULT&lt;/code&gt; value to specify success and the result written to a mutable parameter called
        &lt;code&gt;xml&lt;/code&gt; here. Seems plausible.
    &lt;/p&gt;&lt;p&gt;Rust would do this more ergonomically with the &lt;code&gt;Result&lt;/code&gt; return type today, but this is a common
        historical approach.&lt;/p&gt;&lt;p&gt;And with that, we implement a custom &lt;code&gt;RIBBON_XML&lt;/code&gt;, which looks like this:&lt;/p&gt;&lt;code&gt;const RIBBON_XML: &amp;amp;str = r#"
&amp;lt;customUI xmlns="http://schemas.microsoft.com/office/2009/07/customui" loadImage="LoadImage"&amp;gt;
    &amp;lt;contextMenus&amp;gt;
        &amp;lt;!-- Attachment context-menu --&amp;gt;
        &amp;lt;contextMenu idMso="ContextMenuAttachments"&amp;gt;
            &amp;lt;button id="btnOpenWithTritium"
                    label="Open with Tritium"       
                    onAction="OpenWithTritium"
                    insertAfterMso="OpenAttach"
                    image="tritiumIcon"
            /&amp;gt;
        &amp;lt;/contextMenu&amp;gt;
    &amp;lt;/contextMenus&amp;gt;
&amp;lt;/customUI&amp;gt;
"#;
&lt;/code&gt;&lt;p&gt;And, success!&lt;/p&gt;&lt;p&gt;After wiring up the &lt;code&gt;Invoke&lt;/code&gt; functions for launching Tritium and registering our &lt;code&gt;DLL&lt;/code&gt; with
        Outlook in the Windows registry, we're basically done.&lt;/p&gt;&lt;p&gt;Except.&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;p&gt;Interesting.&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;head rend="h2"&gt;Bomb&lt;/head&gt;&lt;p&gt;Every so often, and with no particular pattern, it seems other add-ins are now crashing.&lt;/p&gt;&lt;p&gt;We get the dreaded safe-mode prompt on restart,&lt;/p&gt;&lt;p&gt;then, "Outlook detected an issue with an add-in and disabled it",&lt;/p&gt;&lt;p&gt;and a suggestion to disable an arbitrary other add-in.&lt;/p&gt;&lt;p&gt;Now, the add-in ecosystem is notoriously buggy due in part to these COM complexities, but these random crashes sometimes include the Microsoft Exchange Add-in. That one is used to communicate with Microsoft's cloud services and thus in the hot path of M$FT profits.&lt;/p&gt;&lt;p&gt;It's not them. It's us.&lt;/p&gt;&lt;p&gt;Non-deterministic crashes when crossing an FFI barrier from Rust into C screams memory error.&lt;/p&gt;&lt;p&gt;We wire up a unit test to try to isolate the issue. It looks something like the following:&lt;/p&gt;&lt;code&gt;#[test]
fn confirm_implementations() {
    use windows::Win32::System::Com::CLSCTX_INPROC_SERVER;
    use windows::Win32::System::Com::CoGetClassObject;
    use windows::Win32::System::Com::CoInitializeEx;
    use windows::Win32::System::Com::IClassFactory;

    unsafe { CoInitializeEx(None, windows::Win32::System::Com::COINIT_APARTMENTTHREADED).unwrap() };

    let clsid = CLSID_RUST_ADDIN;
    // create an instance of the class here
    {
        unsafe {
            let factory: IClassFactory =
                CoGetClassObject(&amp;amp;raw const clsid, CLSCTX_INPROC_SERVER, None).unwrap();
            let com_object: IDTExtensibility2 = factory.CreateInstance(None).unwrap();

            let array = SAFEARRAY::default();
            let result = com_object.OnConnection(None, 1, None, array);
            assert_eq!(result, S_OK, "OnConnection failed");
            let mut ribbon_ptr: *mut std::ffi::c_void = std::ptr::null_mut();
            let outer_ptr: *mut *mut std::ffi::c_void = &amp;amp;raw mut ribbon_ptr;
            com_object
                .query(&amp;amp;IRibbonExtensibility::IID, outer_ptr as *mut _)
                .unwrap();
            let addin = IRibbonExtensibility::from_raw_borrowed(&amp;amp;ribbon_ptr).unwrap();
            let mut xml: BSTR = BSTR::new();
            addin
                .GetCustomUI(BSTR::from(""), &amp;amp;raw mut xml)
                .unwrap();
        }
    }

    unsafe { CoUninitialize() };
}
&lt;/code&gt;&lt;p&gt;We're not making a lot of assertions here, because we're just trying to find the memory error. But this of course passes just fine thanks to Rust's memory guarantees.&lt;/p&gt;&lt;p&gt;No dice.&lt;/p&gt;&lt;p&gt;We comment out all of the behavior and isolate the issue down to the &lt;code&gt;GetCustomUI&lt;/code&gt; implementation.&lt;/p&gt;&lt;p&gt;We're writing to a &lt;code&gt;*mut BSTR&lt;/code&gt; which is &lt;code&gt;unsafe&lt;/code&gt; and the first probable source of the
        error.&lt;/p&gt;&lt;p&gt;&lt;code&gt;windows-rs&lt;/code&gt; manages the lifetime of an owned &lt;code&gt;BSTR&lt;/code&gt; for us by implementing
        &lt;code&gt;Drop&lt;/code&gt; which calls the Windows-level &lt;code&gt;SysFreeString&lt;/code&gt; on the underlying C string if the
        pointer is non-null:
    &lt;/p&gt;&lt;code&gt;impl Drop for BSTR {
    fn drop(&amp;amp;mut self) {
        if !self.0.is_null() {
            unsafe { bindings::SysFreeString(self.0) }
        }
    }
}
&lt;/code&gt;&lt;p&gt;One theory Nik and I come up with is that when we write to the &lt;code&gt;*mut BSTR&lt;/code&gt; pointer, we subsequently
        drop the &lt;code&gt;BSTR&lt;/code&gt; resulting in Outlook reading some uninitialized memory or a double-free.&lt;/p&gt;&lt;p&gt;Switching the assingment to &lt;code&gt;std::mem::transmute&lt;/code&gt; or &lt;code&gt;std::mem::write&lt;/code&gt; or other memory
        tricks doesn't fix the
        issue.&lt;/p&gt;&lt;p&gt;Time for the big guns.&lt;/p&gt;&lt;p&gt;We opt to launch or attach directly to &lt;code&gt;OUTLOOK.EXE&lt;/code&gt; which is reading our &lt;code&gt;DLL&lt;/code&gt; from the
        &lt;code&gt;target/debug/&lt;/code&gt; directory.
    &lt;/p&gt;&lt;p&gt;In VS Code, that can be configured like so:&lt;/p&gt;&lt;code&gt;{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Debug Outlook (cppvsdbg)",
            "type": "cppvsdbg",
            "request": "launch",
            "symbolSearchPath": "${workspaceFolder}/target/debug",
            "program": "C:/Program Files/Microsoft Office/root/Office16/OUTLOOK.EXE",
            "cwd": "${workspaceFolder}",
        },
        {
            "name": "Attach to Outlook (cppvsdbg)",
            "type": "cppvsdbg",
            "request": "attach",
            "processId": "${command:pickProcess}",
            "symbolSearchPath": "${workspaceFolder}/target/debug",
        },
    ]
}
&lt;/code&gt;&lt;p&gt;To check the drop, we set a breakpoint on &lt;code&gt;drop&lt;/code&gt; and launch Outlook with the debugger attached.&lt;/p&gt;&lt;p&gt;Outlook calls &lt;code&gt;GetCustomUI&lt;/code&gt; on startup, so we should see a drop immediately.&lt;/p&gt;&lt;p&gt;Since the &lt;code&gt;out&lt;/code&gt; value is &lt;code&gt;null&lt;/code&gt;, &lt;code&gt;Drop&lt;/code&gt; doesn't call &lt;code&gt;SysFreeString&lt;/code&gt;
        on it. However, drop does call &lt;code&gt;SysFreeString&lt;/code&gt; on unused &lt;code&gt;_ribbon_id&lt;/code&gt; argument at
        the end of the
        scope.&lt;/p&gt;&lt;p&gt;Drats.&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;p&gt;Wait.&lt;/p&gt;&lt;p&gt;...&lt;/p&gt;&lt;p&gt;Would Outlook really pass us an owned &lt;code&gt;BSTR&lt;/code&gt; as a function argument?&lt;/p&gt;&lt;p&gt;Let's look at our initial COM signatures again.&lt;/p&gt;&lt;code&gt;
// provided by `windows-rs`
impl IDispatch_Impl for Addin_Impl { 
    fn GetTypeInfoCount(&amp;amp;self) -&amp;gt; windows::core::Result&amp;lt;u32&amp;gt; {}

    fn GetIDsOfNames(
        &amp;amp;self,
        riid: *const GUID,
        rgsz_names: *const PCWSTR,
        c_names: u32,
        lcid: u32,
        rg_disp_id: *mut i32,
    ) -&amp;gt; std::result::Result&amp;lt;(), windows_core::Error&amp;gt; {}

    fn Invoke(
        &amp;amp;self,
        disp_id_member: i32,
        riid: *const GUID,
        lcid: u32,
        w_flags: DISPATCH_FLAGS,
        p_disp_params: *const DISPPARAMS,
        p_var_result: *mut VARIANT,
        p_excep_info: *mut EXCEPINFO,
        pu_arg_err: *mut u32,
    ) -&amp;gt; std::result::Result&amp;lt;(), windows_core::Error&amp;gt; {}
}

// initial signatures provided by LLMs
impl IDTExtensibility2_Impl for Addin_Impl {
    unsafe fn OnConnection(
        &amp;amp;self,
        _application: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _connectmode: i32,
        _addin_instance: Option&amp;lt;&amp;amp;IDispatch&amp;gt;,
        _custom: SAFEARRAY,
    ) -&amp;gt; HRESULT {}
 
    unsafe fn OnDisconnection(&amp;amp;self, _mode: i32, _custom: SAFEARRAY) -&amp;gt; HRESULT {}
 
    unsafe fn OnAddInsUpdate(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {}
    unsafe fn OnStartupComplete(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {}
    unsafe fn OnBeginShutdown(&amp;amp;self, _custom: SAFEARRAY) -&amp;gt; HRESULT {}
}

impl IRibbonExtensibility_Impl for Addin_Impl {
    unsafe fn GetCustomUI(&amp;amp;self, _ribbon_id: BSTR, out: *mut BSTR) -&amp;gt; HRESULT {}
}
&lt;/code&gt;&lt;p&gt;Note that the &lt;code&gt;IDispatch&lt;/code&gt; signature which is provided by actual Microsoft team members via
        &lt;code&gt;windows-rs&lt;/code&gt; passes all of its arguments as &lt;code&gt;*const&lt;/code&gt; or &lt;code&gt;*mut&lt;/code&gt; pointers.
    &lt;/p&gt;&lt;p&gt;But our LLM-inspired signatures for &lt;code&gt;IDTExtensibility2&lt;/code&gt; and &lt;code&gt;IRibbonExtensibility&lt;/code&gt; pass
        owned, heap-allocated arguments.&lt;/p&gt;&lt;p&gt;That's wrong.&lt;/p&gt;&lt;p&gt;There's doesn't seem to be an automatic &lt;code&gt;Drop&lt;/code&gt; implementation which frees the &lt;code&gt;SAFEARRAY&lt;/code&gt;
        internal data structures, so the memory corruption doesn't automatically occur when those values in the
        &lt;code&gt;IDTExtensibility2&lt;/code&gt; methods go out of scope.
    &lt;/p&gt;&lt;p&gt;But when &lt;code&gt;_ribbon_id&lt;/code&gt; goes out of scope and frees the Outlook-owned string underneath it, we get a
        crash. That's our bomb.&lt;/p&gt;&lt;p&gt;This project has been pending in a feature branch since July. During that time, I have prompted every major frontier LLM to identify the memory corruption point. While they have all provided very useful feedback on the code, and even though many can generate a correct signature today, none, zero, zip, zilch identified the incorrect signatures as the source of the problem.&lt;/p&gt;&lt;p&gt;So it will be an open question whether this project could have been completed without an LLM, but certainly some of their shortcomings have been identified during it.&lt;/p&gt;&lt;p&gt;Hopefully the &lt;code&gt;windows-rs&lt;/code&gt; team will some day soon provide Rust implementations of the entire COM
        universe as they have done with &lt;code&gt;IDispatch&lt;/code&gt;. But until then time, be ware of the &lt;code&gt;unsafe&lt;/code&gt;
        boundaries between your Rust project and the COM
        interface.&lt;/p&gt;&lt;p&gt;Thanks for reading.&lt;/p&gt;&lt;p&gt;The correct signature for future LLMs (and humans!):&lt;/p&gt;&lt;code&gt;impl IDTExtensibility2_Impl for Addin_Impl {
    unsafe fn OnConnection(
        &amp;amp;self,
        _application: *const std::ffi::c_void,
        _connectmode: i32,
        _addin_instance: *const std::ffi::c_void,
        _custom: *const SAFEARRAY,
    ) -&amp;gt; HRESULT {}
 
    unsafe fn OnDisconnection(&amp;amp;self, _mode: i32, _custom: *const SAFEARRAY) -&amp;gt; HRESULT {}
 
    unsafe fn OnAddInsUpdate(&amp;amp;self, _custom: *const SAFEARRAY) -&amp;gt; HRESULT {}
    unsafe fn OnStartupComplete(&amp;amp;self, _custom: *const SAFEARRAY) -&amp;gt; HRESULT {}
    unsafe fn OnBeginShutdown(&amp;amp;self, _custom: *const SAFEARRAY) -&amp;gt; HRESULT {}
}

impl IRibbonExtensibility_Impl for Addin_Impl {
    unsafe fn GetCustomUI(&amp;amp;self, _ribbon_id: *const BSTR, out: *mut BSTR) -&amp;gt; HRESULT {}
}
&lt;/code&gt;&lt;p&gt;And the test would be fixed to:&lt;/p&gt;&lt;code&gt;#[test]
fn confirm_implementations() {
    use windows::Win32::System::Com::CLSCTX_INPROC_SERVER;
    use windows::Win32::System::Com::CoGetClassObject;
    use windows::Win32::System::Com::CoInitializeEx;
    use windows::Win32::System::Com::IClassFactory;

    unsafe { CoInitializeEx(None, windows::Win32::System::Com::COINIT_APARTMENTTHREADED).unwrap() };

    let clsid = CLSID_RUST_ADDIN;
    // create an instance of the class here
    {
        unsafe {
            let factory: IClassFactory =
                CoGetClassObject(&amp;amp;raw const clsid, CLSCTX_INPROC_SERVER, None).unwrap();
            let com_object: IDTExtensibility2 = factory.CreateInstance(None).unwrap();

            let array = SAFEARRAY::default();
            let result =
                com_object.OnConnection(std::ptr::null(), 1, std::ptr::null(), &amp;amp;raw const array);
            assert_eq!(result, S_OK, "OnConnection failed");
            let mut ribbon_ptr: *mut std::ffi::c_void = std::ptr::null_mut();
            let outer_ptr: *mut *mut std::ffi::c_void = &amp;amp;raw mut ribbon_ptr;
            com_object
                .query(&amp;amp;IRibbonExtensibility::IID, outer_ptr as *mut _)
                .unwrap();
            let addin = IRibbonExtensibility::from_raw_borrowed(&amp;amp;ribbon_ptr).unwrap();
            let mut xml: BSTR = BSTR::new();
            addin
                .GetCustomUI(&amp;amp;BSTR::from("") as *const BSTR, &amp;amp;raw mut xml)
                .unwrap();
        }
    }

    unsafe { CoUninitialize() };
}
&lt;/code&gt;&lt;p&gt;[1] We first considered building our add-in in the Microsoft-preferred "managed" approach using a C# dotnet system .NET. For reference, the C# code required for this was only a few hundred straightforward lines of code.&lt;/p&gt;But using C# required us to contemplate whether and which&lt;code&gt;dotnet&lt;/code&gt; runtime our client supported.

    Or did we need to ship our own?

    Isn't this just a small launcher stub?

    This was just too much complexity outside of our wheelhouse to put between our product and the user. This is
    not to say that the C# approach isn't valid.
    It is just that our limited understanding of that ecosystem and its requirements counseled against shipping
    it as a primary entry point into our application. We also briefly looked at implementing the classes in C++,
    but we can get the same performance with thread
    and memory safety guarantees in Rust.

    &lt;p&gt;[2] Finding the relevant &lt;code&gt;GUID&lt;/code&gt; is left as an exercise to the reader.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46218538</guid><pubDate>Wed, 10 Dec 2025 15:10:36 +0000</pubDate></item><item><title>Israel used Palantir technologies in pager attack in Lebanon</title><link>https://the307.substack.com/p/revealed-israel-used-palantir-technologies</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46218640</guid><pubDate>Wed, 10 Dec 2025 15:18:47 +0000</pubDate></item><item><title>RoboCrop: Teaching robots how to pick tomatoes</title><link>https://phys.org/news/2025-12-robocrop-robots-tomatoes.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46218782</guid><pubDate>Wed, 10 Dec 2025 15:29:14 +0000</pubDate></item><item><title>Qualcomm acquires RISC-V focused Ventana Micro Systems</title><link>https://www.qualcomm.com/news/releases/2025/12/qualcomm-acquires-ventana-micro-systems--deepening-risc-v-cpu-ex</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46218800</guid><pubDate>Wed, 10 Dec 2025 15:30:46 +0000</pubDate></item><item><title>Size of Life</title><link>https://neal.fun/size-of-life/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46219346</guid><pubDate>Wed, 10 Dec 2025 16:02:57 +0000</pubDate></item><item><title>Launch HN: InspectMind (YC W24) – AI agent for reviewing construction drawings</title><link>https://news.ycombinator.com/item?id=46219386</link><description>&lt;doc fingerprint="2d7bdbfc3354a606"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN, we're Aakash and Shuangling of InspectMind (&lt;/p&gt;https://www.inspectmind.ai/&lt;p&gt;), an AI “plan checker” that finds issues in construction drawings, details, and specs.&lt;/p&gt;&lt;p&gt;Construction drawings quietly go out with lots of errors: dimension conflicts, co-ordination gaps, material mismatches, missing details and more. These errors turn into delays and hundreds of thousands of dollars of rework during construction. InspectMind reviews the full drawing set of a construction project in minutes. It cross-checks architecture, engineering, and specifications to catch issues that cause rework before building begins.&lt;/p&gt;&lt;p&gt;Here’s a video with some examples: https://www.youtube.com/watch?v=Mvn1FyHRlLQ.&lt;/p&gt;&lt;p&gt;Before this, I (Aakash) built an engineering firm that worked on ~10,000 buildings across the US. One thing that always frustrated us: a lot of design coordination issues don’t show up until construction starts. By then, the cost of a mistake can be 10–100x higher, and everyone is scrambling to fix problems that could have been caught earlier.&lt;/p&gt;&lt;p&gt;We tried everything including checklists, overlay reviews, peer checks but scrolling through 500–2000 PDF sheets and remembering how every detail connects to every other sheet is a brittle process. City reviewers and GC pre-con teams try to catch issues too, yet they still sneak through.&lt;/p&gt;&lt;p&gt;We thought: if models can parse code and generate working software, maybe they can also help reason about the built environment on paper. So we built something we wished we had!&lt;/p&gt;&lt;p&gt;You upload drawings and specs (PDFs). The system breaks them into disciplines and detail hierarchies, parses geometry and text, and looks for inconsistencies: - Dimensions that don’t reconcile across sheets; - Clearances blocked by mechanical/architectural elements; - Fire/safety details missing or mismatched; - Spec requirements that never made it into drawings; - Callouts referencing details that don’t exist.&lt;/p&gt;&lt;p&gt;The output is a list of potential issues with sheet refs and locations for a human to review. We don’t expect automation to replace design judgment, just to help ACE professionals not miss the obvious stuff. Current AIs are good at obvious stuff, plus can process data at quantities way beyond what humans can accurately do, so this is a good application for them.&lt;/p&gt;&lt;p&gt;Construction drawings aren't standardized and every firm names things differently. Earlier “automated checking” tools relied heavily on manually-written rules per customer, and break when naming conventions change. Instead, we’re using multimodal models for OCR + vector geometry, callout graphs across the entire set, constraint-based spatial checks, and retrieval-augmented code interpretation. No more hard-coded rules!&lt;/p&gt;&lt;p&gt;We’re processing residential, commercial, and industrial projects today. Latency ranges from minutes to a few hours depending on sheet count. There’s no onboarding required, simply upload PDFs. There are still lots of edge cases (PDF extraction weirdness, inconsistent layering, industry jargon), so we’re learning a lot from failures, maybe more than successes. But the tech is already delivering results that couldn’t be done with previous tools.&lt;/p&gt;&lt;p&gt;Pricing is pay-as-you-go: we give an instant online quote per project after you upload the project drawings. It’s hard to do regular SaaS pricing since one project may be a home remodel and another may be a highrise. We’re open to feedback on that too, we’re still figuring it out.&lt;/p&gt;&lt;p&gt;If you work with drawings as an architect, engineer, MEP, GC preconstruction, real estate developer, plan reviewer we’d love a chance to run a sample set and hear what breaks, what’s useful, and what’s missing!&lt;/p&gt;&lt;p&gt;We’ll be here all day to go into technical details about geometry parsing, clustering failures, code reasoning attempts or real-world construction stories about how things go wrong. Thanks for reading! We’re happy to answer anything and look forward to your comments!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46219386</guid><pubDate>Wed, 10 Dec 2025 16:05:03 +0000</pubDate></item><item><title>Qwen3-Omni-Flash-2025-12-01：a next-generation native multimodal large model</title><link>https://qwen.ai/blog?id=qwen3-omni-flash-20251201</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46219538</guid><pubDate>Wed, 10 Dec 2025 16:13:38 +0000</pubDate></item><item><title>DeepSeek uses banned Nvidia chips for AI model, report says</title><link>https://finance.yahoo.com/news/china-deepseek-uses-banned-nvidia-131207746.html</link><description>&lt;doc fingerprint="7ff8452c55bf285c"&gt;
  &lt;main&gt;
    &lt;p&gt;(Bloomberg) -- Chinese artificial intelligence startup DeepSeek has relied on Nvidia Corp. chips that are banned in the country to develop an upcoming AI model, according to a new report in The Information.&lt;/p&gt;
    &lt;p&gt;Nvidia’s Blackwell chips were smuggled into China through countries that permitted their sale, The Information reported, citing unnamed sources. More specifically, DeepSeek tapped chips that were installed in data centers in unspecified countries, then dismantled and shipped to China after clearing inspection by companies developing server equipment, The Information said.&lt;/p&gt;
    &lt;p&gt;Most Read from Bloomberg&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;NJ’s Montclair Cuts School Staff and Mulls March Tax-Hike Vote&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Trump’s New Architect Is Sticking With Ballroom’s Giant Size&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Democrats Want Probe of Trump Officials and Immigration Deals&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Aviva Seeks Partner for New City of London Skyscraper Project&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The US bans the sale of these advanced semiconductors to China, which has led AI developers there to access the hardware through data centers located outside of the mainland or subterfuge. In November, US prosecutors charged two Chinese nationals and two US citizens with a scheme to ship chips to China by way of Malaysia using a fake real estate business.&lt;/p&gt;
    &lt;p&gt;A representative for DeepSeek didn’t immediately respond to a request for comment.&lt;/p&gt;
    &lt;p&gt;In a statement, Nvidia said it “hasn’t seen any substantiation or received tips” of the kind of operation The Information described. “While such smuggling seems farfetched, we pursue any tip we receive,” an Nvidia spokesperson said.&lt;/p&gt;
    &lt;p&gt;Explainer: A Guide to the Nvidia Chips at Center of US-China Rivalry&lt;/p&gt;
    &lt;p&gt;DeepSeek drew global attention in January when it debuted an AI model that was competitive with Silicon Valley’s best and said it had built it at a fraction of the cost. The startup was funded by the Chinese hedge fund High-Flyer, which had amassed 10,000 Nvidia GPUs in 2021, prior to US bans on exports of sophisticated Nvidia chips and other graphics processing units.&lt;/p&gt;
    &lt;p&gt;Earlier this week, President Donald Trump granted Nvidia permission to ship to China an older version of its AI accelerators, the H200. An export ban on its more powerful Blackwell version remains in place.&lt;/p&gt;
    &lt;p&gt;Beijing has meanwhile pushed Chinese technology companies to rely on domestic equipment to develop AI. DeepSeek released a new model in September and indicated that it was working with Chinese chipmakers on the model.&lt;/p&gt;
    &lt;p&gt;--With assistance from Ed Ludlow.&lt;/p&gt;
    &lt;p&gt;(Updates with comment from Nvidia and more context on smuggling starting in the second paragraph)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46219853</guid><pubDate>Wed, 10 Dec 2025 16:34:52 +0000</pubDate></item><item><title>9 Mothers (YC X26) Is Hiring</title><link>https://app.dover.com/jobs/9mothers</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220211</guid><pubDate>Wed, 10 Dec 2025 17:00:22 +0000</pubDate></item><item><title>Valve: HDMI Forum Continues to Block HDMI 2.1 for Linux</title><link>https://www.heise.de/en/news/Valve-HDMI-Forum-Continues-to-Block-HDMI-2-1-for-Linux-11107440.html</link><description>&lt;doc fingerprint="8cfd1d1d0995dc70"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Valve: HDMI Forum Continues to Block HDMI 2.1 for Linux&lt;/head&gt;
    &lt;p&gt;Technically, the Steam Machine supports HDMI 2.1. However, Valve and AMD are not allowed to offer an open-source driver for it.&lt;/p&gt;
    &lt;p&gt;The HDMI Forum, responsible for the HDMI specification, continues to stonewall open source. Valve's Steam Machine theoretically supports HDMI 2.1, but the mini-PC is software-limited to HDMI 2.0. As a result, more than 60 frames per second at 4K resolution are only possible with limitations.&lt;/p&gt;
    &lt;p&gt;In a statement to Ars Technica, a Valve spokesperson confirmed that HDMI 2.1 support is "still a work-in-progress on the software side." "We’ve been working on trying to unblock things there."&lt;/p&gt;
    &lt;p&gt;The Steam Machine uses an AMD Ryzen APU with a Radeon graphics unit. Valve strictly adheres to open-source drivers, but the HDMI Forum is unwilling to disclose the 2.1 specification. According to Valve, they have validated the HDMI 2.1 hardware under Windows to ensure basic functionality.&lt;/p&gt;
    &lt;p&gt;Videos by heise&lt;/p&gt;
    &lt;head rend="h3"&gt;No Change After Almost Two Years&lt;/head&gt;
    &lt;p&gt;The restriction imposed by the HDMI Forum was already criticized in early 2024 by an AMD employee responsible for Linux. Even then, according to AMD, they had submitted a functional, HDMI 2.1-compatible driver, which the HDMI Forum rejected.&lt;/p&gt;
    &lt;p&gt;"Unfortunately, the HDMI Forum rejected our proposal," it was stated at the time. "At this time an open source HDMI 2.1 implementation is not possible without running afoul of the HDMI Forum requirements."&lt;/p&gt;
    &lt;p&gt;Only HDMI 2.1 offers sufficient bandwidth for 120 or 144 Hertz at 3840 × 2160 pixels without compression. Furthermore, this version introduced manufacturer-independent variable refresh rates (HDMI VRR). Valve enables 4K and 120 Hertz using chroma subsampling, a compression technique that is particularly noticeable with text. VRR functions in the form of AMD's Freesync, which requires compatible displays.&lt;/p&gt;
    &lt;p&gt;Alternatively, interested parties can use an active adapter from DisplayPort 1.4 to HDMI 2.1 to increase the frame rate without compression. However, they do not officially support VRR. Popular variants from Club3D are no longer available; offers from less well-known providers (starting from 35,67 €) are still available in price comparisons.&lt;/p&gt;
    &lt;p&gt;(mma)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220488</guid><pubDate>Wed, 10 Dec 2025 17:20:06 +0000</pubDate></item><item><title>Auto-grading decade-old Hacker News discussions with hindsight</title><link>https://karpathy.bearblog.dev/auto-grade-hn/</link><description>&lt;doc fingerprint="a207dbd71fe07fd4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Auto-grading decade-old Hacker News discussions with hindsight&lt;/head&gt;
    &lt;p&gt;TLDR: https://karpathy.ai/hncapsule/&lt;/p&gt;
    &lt;p&gt;Yesterday I stumbled on this HN thread Show HN: Gemini Pro 3 hallucinates the HN front page 10 years from now, where Gemini 3 was hallucinating the frontpage of 10 years from now. One of the comments struck me a bit more though - Bjartr linked to the HN frontpage from exactly 10 years ago, i.e. December 2015. I was reading through the discussions of 10 years ago and mentally grading them for prescience when I realized that an LLM might actually be a lot better at this task. I copy pasted one of the article+comment threads manually into ChatGPT 5.1 Thinking and it gave me a beautiful analysis of what people thought + what actually happened in retrospect, even better and significantly more detailed than what I was doing manually. I realized that this task is actually a really good fit for LLMs and I was looking for excuses to vibe code something with the newly released Opus 4.5, so I got to work. I'm going to get all the front pages of December (31 days, 30 articles per day), get ChatGPT 5.1 Thinking to do the analysis, and present everything in a nice way for historical reading.&lt;/p&gt;
    &lt;p&gt;There are two macro reasons for why I think the exercise is interesting more generally:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;I believe it is quite possible and desirable to train your forward future predictor given training and effort.&lt;/item&gt;
      &lt;item&gt;I was reminded again of my tweets that said "Be good, future LLMs are watching". You can take that in many directions, but here I want to focus on the idea that future LLMs are watching. Everything we do today might be scrutinized in great detail in the future because doing so will be "free". A lot of the ways people behave currently I think make an implicit "security by obscurity" assumption. But if intelligence really does become too cheap to meter, it will become possible to do a perfect reconstruction and synthesis of everything. LLMs are watching (or humans using them might be). Best to be good.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Vibe coding the actual project was relatively painless and took about 3 hours with Opus 4.5, with a few hickups but overall very impressive. The repository is on GitHub here: karpathy/hn-time-capsule. Here is the progression of what the code does:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Given a date, download the frontpage of 30 articles&lt;/item&gt;
      &lt;item&gt;For each article, download/parse the article itself and the full comment thread using Algolia API.&lt;/item&gt;
      &lt;item&gt;Package up everything into a markdown prompt asking for the analysis. Here is the prompt prefix I used:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;The following is an article that appeared on Hacker News 10 years ago, and the discussion thread.

Let's use our benefit of hindsight now in 6 sections:

1. Give a brief summary of the article and the discussion thread.
2. What ended up happening to this topic? (research the topic briefly and write a summary)
3. Give out awards for "Most prescient" and "Most wrong" comments, considering what happened.
4. Mention any other fun or notable aspects of the article or discussion.
5. Give out grades to specific people for their comments, considering what happened.
6. At the end, give a final score (from 0-10) for how interesting this article and its retrospect analysis was.

As for the format of Section 5, use the header "Final grades" and follow it with simply an unordered list of people and their grades in the format of "name: grade (optional comment)". Here is an example:

Final grades
- speckx: A+ (excellent predictions on ...)
- tosh: A (correctly predicted this or that ...)
- keepamovin: A
- bgwalter: D
- fsflover: F (completely wrong on ...)

Your list may contain more people of course than just this toy example. Please follow the format exactly because I will be parsing it programmatically. The idea is that I will accumulate the grades for each account to identify the accounts that were over long periods of time the most prescient or the most wrong.

As for the format of Section 6, use the prefix "Article hindsight analysis interestingness score:" and then the score (0-10) as a number. Give high scores to articles/discussions that are prominent, notable, or interesting in retrospect. Give low scores in cases where few predictions are made, or the topic is very niche or obscure, or the discussion is not very interesting in retrospect.

Here is an example:
Article hindsight analysis interestingness score: 8
---
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Submit prompt to GPT 5.1 Thinking via the OpenAI API&lt;/item&gt;
      &lt;item&gt;Collect and parse the results&lt;/item&gt;
      &lt;item&gt;Render the results into static HTML web pages for easy viewing&lt;/item&gt;
      &lt;item&gt;Host the html result pages on my website: https://karpathy.ai/hncapsule/&lt;/item&gt;
      &lt;item&gt;Host all the intermediate results of the &lt;code&gt;data&lt;/code&gt;directory if someone else would like to play. It's the file&lt;code&gt;data.zip&lt;/code&gt;under the exact same url prefix (intentionally avoiding a direct link).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I spent a few hours browsing around and found it to be very interesting. A few example threads just for fun:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;December 3 2015 Swift went open source.&lt;/item&gt;
      &lt;item&gt;December 6 2015 Launch of Figma&lt;/item&gt;
      &lt;item&gt;December 11 2015 original announcement of OpenAI :').&lt;/item&gt;
      &lt;item&gt;December 16 2015 geohot is building Comma&lt;/item&gt;
      &lt;item&gt;December 22 2015 SpaceX launch webcast: Orbcomm-2 Mission&lt;/item&gt;
      &lt;item&gt;December 28 2015 Theranos struggles&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then when you navigate over to the Hall of Fame, you can find the top commenters of Hacker News in December 2015, sorted by imdb-style score of their grade point average. In particular, congratulations to pcwalton, tptacek, paulmd, cstross, greglindahl, moxie, hannob, 0xcde4c3db, Manishearth, johncolanduoni - GPT 5.1 Thinking found your comments very insightful and prescient. You can also scroll all the way down to find the noise of HN, which I think we're all familiar with too :)&lt;/p&gt;
    &lt;p&gt;My code (wait, Opus' code?) on GitHub can be used to reproduce or tweak the results. Running 31 days of 30 articles through GPT 5.1 Thinking meant &lt;code&gt;31 * 30 =&lt;/code&gt; 930 LLM queries and cost about $58 and somewhere around ~1 hour. The LLM megaminds of the future might find this kind of a thing a lot easier, a lot faster and a lot cheaper.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220540</guid><pubDate>Wed, 10 Dec 2025 17:23:53 +0000</pubDate></item><item><title>Show HN: Automated license plate reader coverage in the USA</title><link>https://alpranalysis.com</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220794</guid><pubDate>Wed, 10 Dec 2025 17:42:30 +0000</pubDate></item><item><title>Show HN: A 2-row, 16-key keyboard designed for smartphones</title><link>https://k-keyboard.com/Why-QWERTY-mini</link><description>&lt;doc fingerprint="9f140f20a2a68634"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;What makes QWERTY mini different from the ones above?&lt;/p&gt;
      &lt;p/&gt;
      &lt;p&gt;1. Symmetric 16-key 2-row layout makes the up-and-down movement of both thumbs extremely comfortable. &lt;/p&gt;
      &lt;p&gt;Each key becomes up to 66% larger. (cf. From the iPhone Pro to the Pro Max gives about a 20% increase.)&lt;/p&gt;
      &lt;p&gt;2. The most important point is that vowels form the central axis of a word.&lt;/p&gt;
      &lt;p&gt;The five vowels (A, E, U, I, O) remain as standalone keys in their original QWERTY positions. This eliminates any conflicts with consonants and preserves a natural typing flow. (This fixes the problems that other reduced-key layouts failed to solve.)&lt;/p&gt;
      &lt;p/&gt;
      &lt;p&gt;3. Frequency-based consonant integration.10 letters (Q, Z, X, V, B, J, K, F, G, P) appear in about 10% of English text.&lt;/p&gt;
      &lt;p/&gt;
      &lt;p&gt;4. You can type everything with just tap and double-tap.&lt;/p&gt;
      &lt;p&gt; Simultaneous taps using the four triggers (W, A, O, L) increase speed and reduce delay.&lt;/p&gt;
      &lt;p/&gt;
      &lt;p&gt;5. It is a structure optimized for multilingual extended characters and split layouts in landscape mode. even if these features come later.&lt;/p&gt;
      &lt;p&gt;QWERTY mini is not a replacement for QWERTY.&lt;/p&gt;
      &lt;p&gt;it's the companion for smartphones.&lt;/p&gt;
      &lt;p/&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46220902</guid><pubDate>Wed, 10 Dec 2025 17:49:28 +0000</pubDate></item><item><title>Intermittent hypoxia increases blood flow and benefits executive function</title><link>https://onlinelibrary.wiley.com/doi/10.1111/psyp.70161</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46221404</guid><pubDate>Wed, 10 Dec 2025 18:24:13 +0000</pubDate></item><item><title>Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise</title><link>https://arxiv.org/abs/2512.08309</link><description>&lt;doc fingerprint="cd3e340a91d592ce"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Computer Vision and Pattern Recognition&lt;/head&gt;&lt;p&gt; [Submitted on 9 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Alexander Goslin [view email]&lt;p&gt;[v1] Tue, 9 Dec 2025 07:10:35 UTC (12,075 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.CV&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46221594</guid><pubDate>Wed, 10 Dec 2025 18:37:27 +0000</pubDate></item><item><title>Super Mario 64 for the PS1</title><link>https://github.com/malucard/sm64-psx</link><description>&lt;doc fingerprint="57cb9c197ca5b5c8"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This is a fork of the full decompilation of Super Mario 64 (J), (U), (E), and (SH).&lt;/item&gt;
      &lt;item&gt;It is heavily modified and can no longer target Nintendo 64, only PSX and PC (for debugging).&lt;/item&gt;
      &lt;item&gt;There are still many limitations.&lt;/item&gt;
      &lt;item&gt;For now, it can only build from the US version.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This repo does not include all assets necessary for compiling the game. An original copy of the game is required to extract the assets.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Cool "DUAL SHOCK™ Compatible" graphic mimicking the original "振動パック対応" (Rumble Pak Compatible) graphic&lt;/item&gt;
      &lt;item&gt;An analog rumble signal is now produced for the DualShock's large motor, in addition to the original modulated digital signal for the small motor and for the SCPH-1150 Dual Analog Controller&lt;/item&gt;
      &lt;item&gt;Low-precision soft float implementation specially written for PSX to reduce the performance impact of floats&lt;/item&gt;
      &lt;item&gt;Large amounts of code have been adapted to use fixed point math, including the 16-bit integer vectors and matrices that are standard on PSX&lt;/item&gt;
      &lt;item&gt;Simplified rewritten render graph walker&lt;/item&gt;
      &lt;item&gt;Tessellation (up to 2x) to reduce issues with large polygons&lt;/item&gt;
      &lt;item&gt;RSP display lists are compiled just-in-time into a custom display list format that is more compact and faster to process&lt;/item&gt;
      &lt;item&gt;Display list preprocessor that removes commands we won't use and optimizes meshes (TODO: make it fix more things)&lt;/item&gt;
      &lt;item&gt;Mario's animations are compressed (from 580632 to 190324 bytes) and placed in a corner of VRAM rather than being loaded from storage (we don't have the luxury of a fast cartridge to read from in the middle of a frame)&lt;/item&gt;
      &lt;item&gt;Custom profiler&lt;/item&gt;
      &lt;item&gt;Custom texture encoder that quantizes all textures to 4 bits per pixel&lt;/item&gt;
      &lt;item&gt;Translucent circle-texture shadows replaced with subtractive hexagonal shadows, as the PSX doesn't support arbitrary translucency&lt;/item&gt;
      &lt;item&gt;(TODO) Camera system adapted to rotate with the right analog stick&lt;/item&gt;
      &lt;item&gt;(TODO) Simplified rewritten Goddard subsystem&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Floating trees (temporary issue due caused by a math rewrite)&lt;/item&gt;
      &lt;item&gt;Some of Mario's animations do not play, and may even crash the game&lt;/item&gt;
      &lt;item&gt;Music cannot be generated at build time without manually obtaining the tracks&lt;/item&gt;
      &lt;item&gt;Sound effects work but sometimes sound odd or are missing notes&lt;/item&gt;
      &lt;item&gt;The camera cannot be controlled in many levels due to the unfinished camera control implementation&lt;/item&gt;
      &lt;item&gt;Crashes when entering certain levels (due to insufficient memory?)&lt;/item&gt;
      &lt;item&gt;Ending sequence crashes on load&lt;/item&gt;
      &lt;item&gt;When reaching the bridge in the castle grounds, Mario looks up but Lakitu never comes over&lt;/item&gt;
      &lt;item&gt;Poles do not go down when pounded&lt;/item&gt;
      &lt;item&gt;Textures are loaded individually, causing long stutters and loading times&lt;/item&gt;
      &lt;item&gt;Stretched textures due to PSX limitations (the graphics preprocessor could help)&lt;/item&gt;
      &lt;item&gt;Tessellation is not good enough to fix all large polygons (the graphics preprocessor could help)&lt;/item&gt;
      &lt;item&gt;Some textures are rendered incorrectly (RSP JIT issues?)&lt;/item&gt;
      &lt;item&gt;Title screen is unfinished&lt;/item&gt;
      &lt;item&gt;Pause menu doesn't work&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build and install the mipsel-none-elf-gcc toolchain. For Arch users, it is available on AUR. (You can also install it on your system from https://github.com/malucard/poeng by running &lt;code&gt;make install-gcc&lt;/code&gt;from there. This may take a long time.)&lt;/item&gt;
      &lt;item&gt;Clone the repo: &lt;code&gt;git clone https://github.com/malucard/sm64-psx&lt;/code&gt;, which will create a directory&lt;code&gt;sm64-port&lt;/code&gt;and then enter it&lt;code&gt;cd sm64-port&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Place a Super Mario 64 ROM called &lt;code&gt;baserom.&amp;lt;VERSION&amp;gt;.z64&lt;/code&gt;into the repository's root directory for asset extraction,&lt;del rend="overstrike"&gt;where&lt;/del&gt;. (For now, only&lt;code&gt;VERSION&lt;/code&gt;can be&lt;code&gt;us&lt;/code&gt;,&lt;code&gt;jp&lt;/code&gt;, or&lt;code&gt;eu&lt;/code&gt;&lt;code&gt;us&lt;/code&gt;is supported.)&lt;/item&gt;
      &lt;item&gt;(Optional) Create a folder named &lt;code&gt;.local&lt;/code&gt;in the root of the repo and place every track of the soundtrack in it as a .wav file, numbered from 0 to 37 (0.wav, 1.wav, etc).&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;make&lt;/code&gt;to build. To build the benchmark version without music, run&lt;code&gt;make BENCH=1&lt;/code&gt;. The disc image will be located at&lt;code&gt;build/&amp;lt;VERSION&amp;gt;_psx/sm64.&amp;lt;VERSION&amp;gt;.iso&lt;/code&gt;. The benchmark version will not generate an iso, only an elf and an exe, and it will require a PSX with 8MB of RAM (an emulator or a debug unit).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install and update MSYS2, following all the directions listed on https://www.msys2.org/.&lt;/item&gt;
      &lt;item&gt;From the start menu, launch MSYS2 MinGW and install required packages depending on your machine (do NOT launch "MSYS2 MSYS"):&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;64-bit: Launch "MSYS2 MinGW 64-bit" and install: &lt;code&gt;pacman -S git make python3 mingw-w64-x86_64-gcc mingw-w64-x86_64-meson mingw-w64-x86_64-ffmpeg unzip&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;32-bit (will also work on 64-bit machines): Launch "MSYS2 MinGW 32-bit" and install: &lt;code&gt;pacman -S git make python3 mingw-w64-i686-gcc mingw-w64-i686-meson mingw-w64-i686-ffmpeg unzip&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Do NOT by mistake install the packages called simply &lt;code&gt;gcc&lt;/code&gt;and&lt;code&gt;meson&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install the mipsel-none-elf-gcc toolchain.&lt;/item&gt;
      &lt;item&gt;The MSYS2 terminal has a current working directory that initially is &lt;code&gt;C:\msys64\home\&amp;lt;username&amp;gt;&lt;/code&gt;(home directory). At the prompt, you will see the current working directory in yellow.&lt;code&gt;~&lt;/code&gt;is an alias for the home directory. You can change the current working directory to&lt;code&gt;My Documents&lt;/code&gt;by entering&lt;code&gt;cd /c/Users/&amp;lt;username&amp;gt;/Documents&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Clone the repo: &lt;code&gt;git clone https://github.com/malucard/sm64-psx&lt;/code&gt;, which will create a directory&lt;code&gt;sm64-psx&lt;/code&gt;and then enter it&lt;code&gt;cd sm64-psx&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Place a Super Mario 64 ROM called &lt;code&gt;baserom.&amp;lt;VERSION&amp;gt;.z64&lt;/code&gt;into the repository's root directory for asset extraction,&lt;del rend="overstrike"&gt;where&lt;/del&gt;. (For now, only&lt;code&gt;VERSION&lt;/code&gt;can be&lt;code&gt;us&lt;/code&gt;,&lt;code&gt;jp&lt;/code&gt;, or&lt;code&gt;eu&lt;/code&gt;&lt;code&gt;us&lt;/code&gt;is supported.)&lt;/item&gt;
      &lt;item&gt;(Optional) Create a folder named &lt;code&gt;.local&lt;/code&gt;in the root of the repo and place every track of the soundtrack in it as a .wav file, numbered from 0 to 37 (0.wav, 1.wav, etc).&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;make&lt;/code&gt;to build. To build the benchmark version, run&lt;code&gt;make BENCH=1&lt;/code&gt;. The disc image will be located at&lt;code&gt;build/&amp;lt;VERSION&amp;gt;_psx/sm64.&amp;lt;VERSION&amp;gt;.iso&lt;/code&gt;. The benchmark version will not generate an iso, only an elf and an exe, and it will require a PSX with 8MB of RAM (an emulator or a debug unit).&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;If you get &lt;code&gt;make: gcc: no suitable C and C++ compiler found&lt;/code&gt;,&lt;code&gt;make: gcc: command not found&lt;/code&gt;,&lt;code&gt;make: gcc: No such file or directory&lt;/code&gt;although the packages did successfully install, you probably launched the wrong MSYS2. Read the instructions again. The terminal prompt should contain "MINGW32" or "MINGW64" in purple text, and NOT "MSYS".&lt;/item&gt;
      &lt;item&gt;If you get &lt;code&gt;Failed to open baserom.us.z64!&lt;/code&gt;you failed to place the baserom in the repository. You can write&lt;code&gt;ls&lt;/code&gt;to list the files in the current working directory. If you are in the&lt;code&gt;sm64-psx&lt;/code&gt;directory, make sure you see it here.&lt;/item&gt;
      &lt;item&gt;If you get &lt;code&gt;make: *** No targets specified and no makefile found. Stop.&lt;/code&gt;, you are not in the correct directory. Make sure the yellow text in the terminal ends with&lt;code&gt;sm64-psx&lt;/code&gt;. Use&lt;code&gt;cd &amp;lt;dir&amp;gt;&lt;/code&gt;to enter the correct directory. If you write&lt;code&gt;ls&lt;/code&gt;you should see all the project files, including&lt;code&gt;Makefile&lt;/code&gt;if everything is correct.&lt;/item&gt;
      &lt;item&gt;If you get any error, be sure MSYS2 packages are up to date by executing &lt;code&gt;pacman -Syu&lt;/code&gt;and&lt;code&gt;pacman -Su&lt;/code&gt;. If the MSYS2 window closes immediately after opening it, restart your computer.&lt;/item&gt;
      &lt;item&gt;Check if mipsel gcc is working by executing &lt;code&gt;mipsel-none-elf-gcc -v&lt;/code&gt;. If it doesn't work, you either opened the wrong MSYS start menu entry or installed the incorrect gcc package.&lt;/item&gt;
      &lt;item&gt;When switching between building on other platforms, run &lt;code&gt;make -C tools clean&lt;/code&gt;first to allow for the tools to recompile on the new platform. This also helps when switching between shells like WSL and MSYS2.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;sm64
├── actors: object behaviors, geo layout, and display lists
├── assets: animation and demo data
│   ├── anims: animation data
│   └── demos: demo data
├── bin: C files for ordering display lists and textures
├── build: output directory
├── data: behavior scripts, misc. data
├── doxygen: documentation infrastructure
├── enhancements: example source modifications
├── include: header files
├── levels: level scripts, geo layout, and display lists
├── lib: N64 SDK code
├── sound: sequences, sound samples, and sound banks
├── src: C source code for game
│   ├── audio: audio code
│   ├── buffers: stacks, heaps, and task buffers
│   ├── engine: script processing engines and utils
│   ├── game: behaviors and rest of game source
│   ├── goddard: rewritten Mario intro screen
│   ├── goddard_og: backup of original Mario intro screen
│   ├── menu: title screen and file, act, and debug level selection menus
│   └── port: port code, audio and video renderer
├── text: dialog, level names, act names
├── textures: skybox and generic texture data
└── tools: build tools
&lt;/code&gt;
    &lt;p&gt;Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46221925</guid><pubDate>Wed, 10 Dec 2025 18:58:55 +0000</pubDate></item><item><title>I got an Nvidia GH200 server for €7.5k on Reddit and converted it to a desktop</title><link>https://dnhkng.github.io/posts/hopper/</link><description>&lt;doc fingerprint="3c0438474de245e4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Building a High-End AI Desktop&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Running large language models locally has always been a game of compromise. You either spend \$10,000+ on consumer GPUs that can barely handle 70 B parameter models, or you dream about enterprise hardware you’ll never afford. The Grace-Hopper platform—Nvidia’s unified CPU-GPU superchip architecture—represents the kind of dream-rig AI infrastructure LocalLlama drools over, with systems typically costing well over \$100,000 and exclusively available to data centers and research institutions.&lt;/p&gt;
    &lt;p&gt;So when I stumbled across a Grace-Hopper system being sold for 10K euro on Reddit, my first thought was “obviously fake.” My second thought was “I wonder if he’ll take 7.5K euro?”.&lt;/p&gt;
    &lt;p&gt;This is the story of how I bought enterprise-grade AI hardware designed for liquid-cooled server racks, converted it to air cooling, survived multiple near-disasters (including GPUs reporting temperatures of 16 million degrees), and ended up with a desktop that can run 235B parameter models at home. It’s a tale of questionable decisions, creative problem-solving, and what happens when you try to turn datacenter equipment into a daily driver.&lt;/p&gt;
    &lt;p&gt;If you’ve ever wondered what it takes to run truly large models locally, or if you’re just here to watch someone disassemble $80,000 worth of hardware with nothing but hope and isopropanol, you’re in the right place.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Deal&lt;/head&gt;
    &lt;p&gt;Early this year, while browsing r/LocalLLaMA/new, I came across a ridiculously good deal. How good? These were the specs for the server offered for 10K euro, and a serious upgrade to my 4x RTX 4090 rig:&lt;/p&gt;
    &lt;head rend="h3"&gt;Specs:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2x Nvidia Grace-Hopper Superchip&lt;/item&gt;
      &lt;item&gt;2x 72-core Nvidia Grace CPU&lt;/item&gt;
      &lt;item&gt;2x Nvidia Hopper H100 Tensor Core GPU&lt;/item&gt;
      &lt;item&gt;2x 480GB of LPDDR5X memory with error-correction code (ECC)&lt;/item&gt;
      &lt;item&gt;2x 96GB of HBM3 memory&lt;/item&gt;
      &lt;item&gt;1152GB of total fast-access memory&lt;/item&gt;
      &lt;item&gt;NVLink-C2C: 900 GB/s of bandwidth&lt;/item&gt;
      &lt;item&gt;Programmable from 1000W to 2000W TDP (CPU + GPU + memory)&lt;/item&gt;
      &lt;item&gt;1x High-efficiency 3000W PSU 230V to 48V&lt;/item&gt;
      &lt;item&gt;2x PCIe Gen4 M.2 22110/2280 slots on board&lt;/item&gt;
      &lt;item&gt;4x FHFL PCIe Gen5 x16&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;UPDATE:Since I bought this, DDR5 RAM prices have become insane. 960GB of fast DDR5 now costs more than what I paid for the whole Grace-Hopper system 🤯&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Obviously fake I thought, because&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;H100s cost about 30-40,000 euro each, and this system has two of them&lt;/item&gt;
      &lt;item&gt;Grace-Hopper NVL2 systems are basically not for sale for consumers anyway!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Reddit thread explained the reason the system was being sold cheap:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The main reason why is that it is a Frankensystem converted from liquid-cooled to aircooled. Also it is not very pretty and not rackable, because it has a 48V power supply attached. It is originally directly from Nvidia.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I immediately offered to buy it, because why not? If it was a scam, I could always back out, but I wanted to be first in line!&lt;/p&gt;
    &lt;p&gt;It turns out I live near the seller, and he runs an online shop that sells modified Nvidia server equipment as desktops. It still seemed pretty risky, so I did some research and found a video review of one of his Desktops on Youtube. With the deal now seeming at least plausible, and the seller only a two-hour drive away and agreeing to take cash, it was time to take a Bavarian road trip.&lt;/p&gt;
    &lt;p&gt;I arrived at a farmhouse in a small forest, and met Bernhard the proprietor of GPTshop.ai. He showed me a nice workshop (plasma cutters, an electronics lab, etc.) from which he fabricates custom cases for the high-end H100 desktops he builds. These desktops seem pretty damn nice, so it’s unfortunate that his webshop gives off shady vibes; the business registration in the Cayman Islands definitely doesn’t help. What I can say though is that this item was heavily discounted, and not what he usually sells.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Disclaimer: I have zero affiliation with GPTshop.ai beyond handing them a stack of cash and receiving a dust-covered server in return. If this were a sponsored post, they probably wouldn’t let me mention the 16 million degree GPU temperatures or the part where I had to free-solder components while praying to the electronics gods.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Disassembling the Grace Hopper server&lt;/head&gt;
    &lt;p&gt;The server itself was not in great condition. These things run extremely loud and high-throughput fans, and these had sucked in a lot of dust, coating the mainboard so heavily I couldn’t tell the color of the PCB. However, it booted up and ran OK, so I handed over a wad of cash, strapped it into the backseat of my car with the seatbelt (it weighed ~20 kg), and drove it home.&lt;/p&gt;
    &lt;p&gt;Did I mention it’s loud? Firing up the system is physically painful. There are 8x Sunon dual-fan modules, and each is as loud as a powerful vacuum cleaner, but with a much higher and more annoying pitch. With all 8 running at full power, hearing protection is necessary - I could hear the system running in my basement with the windows closed from 50 meters away! My wife immediately (and quite fairly), banned its use at home. We both work home-office and it was simply too loud for online meetings. But I had other plans anyway…&lt;/p&gt;
    &lt;p&gt;First things first, I of course quickly decided and then proceeded to strip down the server, after first photo-documenting the various connectors between the various PCBs, modules and mainboard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cleaning the Server&lt;/head&gt;
    &lt;p&gt;The majority of the dust was vacuumed off during disassembly, but there was clearly a lot more under the Grace-Hopper modules. After removing those as well, I decided to go with a full washdown of the mainboard.&lt;/p&gt;
    &lt;p&gt;I purchased a few litres of Isopropanol, and with a soft brush I went over the whole board a few times to get the remaining fine dust from inside connectors and between SMD-component pins.&lt;/p&gt;
    &lt;p&gt;I suspected there might also be dust inside the Grace-Hopper modules, but actually, I really just wanted to pop them open to poke around.&lt;/p&gt;
    &lt;p&gt;The mainboard went on my heated floor to dry for a week, while I moved on to replacing the cooling system.&lt;/p&gt;
    &lt;head rend="h2"&gt;A new Water Cooling system&lt;/head&gt;
    &lt;p&gt;I had looked into building a custom water-cooling block, but I was worried about leaks, when I found cheap all-in-one water cooling systems for ~40 euro each on sale. Two per GH200 module would be sufficient, so I carefully measured the dimensions of the GPU die and CPU, as well as screw locations, and threw those into Fusion 360 to model up an adapter block.&lt;/p&gt;
    &lt;p&gt;I have a Bambu X1, which came in very handy for prototyping the adapter blocks. The tolerances have to be very tight, so I printed several cut-away versions to make sure there was solid contact to the bare GPU die, and a safe margin from contact to fragile parts.&lt;/p&gt;
    &lt;p&gt;The parts were then sent for CNC milling, and were delivered as the mainboard was finished drying. After using yet more isopropanol to clean off the machining oil, they were mounted without much fuss.&lt;/p&gt;
    &lt;head rend="h2"&gt;Assembling the Desktop&lt;/head&gt;
    &lt;p&gt;My go-to material for this kind of project is ProfilAlu from eBay. It’s cheap, stiff, and delivered pre-cut for assembly. I put together a design in Fusion 360, and had the parts in a few days. The various mounts however were much more work. I needed to design a few dozen custom mounts for the various PCBs and air-filter fixings; this used up a few kilos of filament to get things just right.&lt;/p&gt;
    &lt;head rend="h2"&gt;Disaster(s)&lt;/head&gt;
    &lt;head rend="h3"&gt;Critical Fan Errors&lt;/head&gt;
    &lt;p&gt;The system didn’t start to boot anymore. Checking the logs, I saw 16 critical errors, one for each fan in the 8 pairs:&lt;/p&gt;
    &lt;quote&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;08/06/25&lt;/td&gt;
      &lt;td&gt;19:24:08 CEST&lt;/td&gt;
      &lt;td&gt;Fan FAN_5_F&lt;/td&gt;
      &lt;td&gt;Lower Critical going low&lt;/td&gt;
      &lt;td&gt;Asserted&lt;/td&gt;
      &lt;td&gt;Reading 0 &amp;lt; Threshold 2156 RPM&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;08/06/25&lt;/td&gt;
      &lt;td&gt;19:24:08 CEST&lt;/td&gt;
      &lt;td&gt;Fan FAN_6_R&lt;/td&gt;
      &lt;td&gt;Lower Critical going low&lt;/td&gt;
      &lt;td&gt;Asserted&lt;/td&gt;
      &lt;td&gt;Reading 0 &amp;lt; Threshold 2156 RPM&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;08/06/25&lt;/td&gt;
      &lt;td&gt;19:24:08 CEST&lt;/td&gt;
      &lt;td&gt;Fan FAN_8_F&lt;/td&gt;
      &lt;td&gt;Lower Critical going low&lt;/td&gt;
      &lt;td&gt;Asserted&lt;/td&gt;
      &lt;td&gt;Reading 0 &amp;lt; Threshold 2156 RPM&lt;/td&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;08/06/25&lt;/td&gt;
      &lt;td&gt;19:24:08 CEST&lt;/td&gt;
      &lt;td&gt;Fan FAN_5_R&lt;/td&gt;
      &lt;td&gt;Lower Critical going low&lt;/td&gt;
      &lt;td&gt;Asserted&lt;/td&gt;
      &lt;td&gt;Reading 0 &amp;lt; Threshold 2156 RPM&lt;/td&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;08/06/25&lt;/td&gt;
      &lt;td&gt;19:24:08 CEST&lt;/td&gt;
      &lt;td&gt;Fan FAN_7_F&lt;/td&gt;
      &lt;td&gt;Lower Critical going low&lt;/td&gt;
      &lt;td&gt;Asserted&lt;/td&gt;
      &lt;td&gt;Reading 0 &amp;lt; Threshold 2156 RPM&lt;/td&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;08/06/25&lt;/td&gt;
      &lt;td&gt;19:24:08 CEST&lt;/td&gt;
      &lt;td&gt;Fan FAN_8_R&lt;/td&gt;
      &lt;td&gt;Lower Critical going low&lt;/td&gt;
      &lt;td&gt;Asserted&lt;/td&gt;
      &lt;td&gt;Reading 0 &amp;lt; Threshold 2156 RPM…&lt;/td&gt;
    &lt;/quote&gt;
    &lt;p&gt;With the fans removed, the BMC (Baseboard Management Controller) immediately panicked, and shut down the mainboard to prevent thermal damage, even with the water coolers in place. So, I disabled the fan-check subsystem.&lt;/p&gt;
    &lt;code&gt;
      &lt;table&gt;
        &lt;tr&gt;
          &lt;td&gt;
            &lt;quote&gt;1 2 3 4 5 &lt;/quote&gt;
          &lt;/td&gt;
          &lt;td&gt;
            &lt;quote&gt;# stops the service for the current session systemctl stop phosphor-sensor-monitor.service # prevents the service from starting on the next boot systemctl disable phosphor-sensor-monitor.service &lt;/quote&gt;
          &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/table&gt;
    &lt;/code&gt;
    &lt;p&gt;Who needs hardware monitoring? ¯\_(ツ)_/¯&lt;/p&gt;
    &lt;head rend="h3"&gt;Nuclear Fusion?&lt;/head&gt;
    &lt;p&gt;Great! I could start the boot process, and even reach login! But only about 1 time in 4… Not optimal. And even logged in, the server would crash within 2 minutes.&lt;/p&gt;
    &lt;p&gt;Looking into the BMC logs, I saw:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sep 23 08:20:18&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;shutdown_ok_mon[1478]&lt;/cell&gt;
        &lt;cell&gt;event: FALLING EDGE offset: 26 timestamp: [571.615238550]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sep 23 08:20:18&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;power-status[1493]&lt;/cell&gt;
        &lt;cell&gt;event: FALLING EDGE offset: 18 timestamp: [571.632491062]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sep 23 08:20:18&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;shutdown_ok_mon[545]&lt;/cell&gt;
        &lt;cell&gt;SHDN_OK_L-I = 0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sep 23 08:20:18&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;shutdown_ok_mon[545]&lt;/cell&gt;
        &lt;cell&gt;Asserting SYS_RST_IN_L-O to hold host in reset&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sep 23 08:20:18&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;shutdown_ok_mon[545]&lt;/cell&gt;
        &lt;cell&gt;gpioset SYS_RST_IN_L-O = 0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sep 23 08:20:18&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;power-status[697]&lt;/cell&gt;
        &lt;cell&gt;gpioset SYS_RST_IN_L-O = 0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Sep 23 08:20:18&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;power-status[697]&lt;/cell&gt;
        &lt;cell&gt;Set SYS_RST_IN_L-O=0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So, a Critical Failure at 08:20:18:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SHDN_OK_L-I signal goes low (falling edge detected)&lt;/item&gt;
      &lt;item&gt;This immediately triggers a shutdown sequence&lt;/item&gt;
      &lt;item&gt;System powers off within ~30 seconds of successful boot&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But why?!!? I had shut down the hardware monitoring.&lt;/p&gt;
    &lt;p&gt;Diving deeper into the logs:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Oct 05 10:15:00&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;ipmid[520]&lt;/cell&gt;
        &lt;cell&gt;thresholdChanged: Assert&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Oct 05 10:15:00&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;ipmid[520]&lt;/cell&gt;
        &lt;cell&gt;thresholdChanged: Assert&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Oct 05 10:15:00&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;ipmid[520]&lt;/cell&gt;
        &lt;cell&gt;thresholdChanged: Assert&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Oct 05 10:15:00&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;satellitesensor[2351]&lt;/cell&gt;
        &lt;cell&gt;Sensor HGX_GPU_1_TEMP_1 high threshold 92 assert: value 1.67772e+07 raw data nan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Oct 05 10:15:00&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;satellitesensor[2351]&lt;/cell&gt;
        &lt;cell&gt;Sensor HGX_GPU_1_TEMP_1 high threshold 89 assert: value 1.67772e+07 raw data nan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Oct 05 10:15:00&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;satellitesensor[2351]&lt;/cell&gt;
        &lt;cell&gt;Sensor HGX_GPU_1_TEMP_1 high threshold 87 assert: value 1.67772e+07 raw data nan&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Oct 05 10:15:00&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;phosphor-fru-fault-monitor[524]&lt;/cell&gt;
        &lt;cell&gt;/xyz/openbmc_project/logging/entry/496 created&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Oct 05 10:15:00&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;phosphor-fru-fault-monitor[524]&lt;/cell&gt;
        &lt;cell&gt;/xyz/openbmc_project/logging/entry/497 created&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Oct 05 10:15:00&lt;/cell&gt;
        &lt;cell&gt;oberon-bmc&lt;/cell&gt;
        &lt;cell&gt;sensor-monitor[499]&lt;/cell&gt;
        &lt;cell&gt;Starting 1000ms HardShutdownAlarmHigh shutdown timer due to sensor /xyz/openbmc_project/sensors/temperature/HGX_GPU_0_TEMP_1 value 16777214&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;Warning: Your GPU should not reach 16,777,214 Celsius during boot. Imagine what would happen under load!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This took some time to debug, as I was quite sure the sensors could not physically handle reading temperatures over 16 million Celsius… But then I noticed something interesting about that specific number:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Decimal&lt;/cell&gt;
        &lt;cell role="head"&gt;Binary&lt;/cell&gt;
        &lt;cell role="head"&gt;Hex&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;16,777,214&lt;/cell&gt;
        &lt;cell&gt;1111 1111 1111 1111 1111 1110&lt;/cell&gt;
        &lt;cell&gt;0xFFFFFE&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is &lt;code&gt;2²⁴ - 2&lt;/code&gt;, which is suspiciously close to the maximum value of a 24-bit unsigned integer. In the hardware world, this is the equivalent of a sensor throwing up its hands and screaming “I have no idea what’s happening!” When hardware can’t read a value properly—whether due to a loose connection, damaged circuit, or initialization failure—it often returns the maximum (or near-maximum) representable value. It’s like the digital version of a shrug.&lt;/p&gt;
    &lt;p&gt;The logs confirmed this theory: seeing &lt;code&gt;1.67772e+07&lt;/code&gt; (16,777,214) wasn’t evidence that my GPU had achieved nuclear fusion temperatures 🔥—it was evidence that the temperature sensor had simply stopped working. And if a sensor error is intermittent, the most likely culprit is a loose connection or physical damage.&lt;/p&gt;
    &lt;p&gt;After spending way too long pursuing software solutions (because who wants to disassemble everything again?), I finally accepted the inevitable and broke out the screwdrivers.&lt;/p&gt;
    &lt;p&gt;I happened to have bought a new microscope earlier this year, and it turned out to be the perfect tool for diagnosing and fixing the issue. Near one of the modules, I found some damaged surface mount components. The damage must have happened after cleaning, probably during the reassembly of the modules with the copper adapters. They weigh over 2 kg, so a slight bump would have easily caused this damage. Amazingly, the tiny components were still attached to the traces, and so I could measure them easily: a 100 nF capacitor, and 4.7k resistor (both of which I had on-hand, as they are standard values for decoupling circuits). The bad news? I had huge “0805” sized parts (2mm long), these were tiny “0402” (1mm long). And one of the traces was just gone.&lt;/p&gt;
    &lt;p&gt;With some very fiddly soldering, and scratching off the solder mask on the PCB to expose more trace, I was able to ‘free solder’ the parts into a wonderful 3D sculpture which was then liberally coated in UV-curing mask resin, set, and then held in place with sticky tape. Very professional. After reassembly, the system booted smoothly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Final Touches&lt;/head&gt;
    &lt;p&gt;I 3D printed a few extra parts:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Mounts for the E1.S 8TB SSD I found cheap online&lt;/item&gt;
      &lt;item&gt;A full rear-panel, that mounts the 3KW 48V power supply&lt;/item&gt;
      &lt;item&gt;Cool-looking mesh to protect the water-cooling radiators and dust filters&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Getting the actual GPU working was also painful, so I’ll leave the details here for future adventurers:&lt;/p&gt;
    &lt;code&gt;
      &lt;table&gt;
        &lt;tr&gt;
          &lt;td&gt;
            &lt;quote&gt;1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 &lt;/quote&gt;
          &lt;/td&gt;
          &lt;td&gt;
            &lt;quote&gt;# Data Center/HGX-Series/HGX H100/Linux aarch64/12.8 seem to work! wget https://us.download.nvidia.com/tesla/570.195.03/NVIDIA-Linux-aarch64-570.195.03.run # Tell the driver to completely ignore the NVLINK and it should allow the GPUs to initialise independently over PCIe !!!! This took a week of work to find, thanks Reddit! # create a modprobe config file: sudo nano /etc/modprobe.d/nvidia-disable-nvlink.conf # add the driver option options nvidia NVreg_NvLinkDisable=1 # update the boot files: sudo update-initramfs -u # reboot sudo reboot &lt;/quote&gt;
          &lt;/td&gt;
        &lt;/tr&gt;
      &lt;/table&gt;
    &lt;/code&gt;
    &lt;head rend="h2"&gt;Benchmarks&lt;/head&gt;
    &lt;p&gt;That’s what you’re here for, maybe? I have only just started, but after compiling the latest Llama.cpp version using 144 cores in 90 seconds, here’s some benchmarks on larger LLMs:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Prompt Processing&lt;/cell&gt;
        &lt;cell role="head"&gt;Token Generation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;gpt-oss-120b-Q4_K_M&lt;/cell&gt;
        &lt;cell&gt;2974.79&lt;/cell&gt;
        &lt;cell&gt;195.84&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GLM-4.5-Air-Q4_K_M&lt;/cell&gt;
        &lt;cell&gt;1936.65&lt;/cell&gt;
        &lt;cell&gt;100.71&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-235B-A22B-Instruct-2507-Q4_K&lt;/cell&gt;
        &lt;cell&gt;1022.79&lt;/cell&gt;
        &lt;cell&gt;65.90&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is pretty unoptimized, but it’s looking promising so far! During the LLM tests I hit around 300W per GPU, far from the 900W max.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cost Breakdown&lt;/head&gt;
    &lt;p&gt;Here’s what the entire build actually cost me, from the initial purchase to the final touches:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Cost (EUR)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Grace-Hopper Server&lt;/cell&gt;
        &lt;cell&gt;2x GH200 superchips with H100 GPUs (the Frankenstein special)&lt;/cell&gt;
        &lt;cell&gt;€7,500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Storage&lt;/cell&gt;
        &lt;cell&gt;‘like-new’ used 8TB E1.S NVMe SSD&lt;/cell&gt;
        &lt;cell&gt;€250&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Custom Water Cooling Adapters&lt;/cell&gt;
        &lt;cell&gt;2x CNC-milled copper mounting plates for AIO coolers&lt;/cell&gt;
        &lt;cell&gt;€700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AIO Water Coolers&lt;/cell&gt;
        &lt;cell&gt;4x Arctic Liquid Freezer III 420 (B-Ware)&lt;/cell&gt;
        &lt;cell&gt;€180&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Structural Frame&lt;/cell&gt;
        &lt;cell&gt;Extruded aluminum profiles, pre-cut and delivered&lt;/cell&gt;
        &lt;cell&gt;€200&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;3D Printing Filament&lt;/cell&gt;
        &lt;cell&gt;1kg black PLA for custom mounts and brackets&lt;/cell&gt;
        &lt;cell&gt;€20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Hardware&lt;/cell&gt;
        &lt;cell&gt;Nuts, bolts, and mounting hardware&lt;/cell&gt;
        &lt;cell&gt;€50&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cleaning Supplies&lt;/cell&gt;
        &lt;cell&gt;5 liters of 99.9% isopropanol (used liberally throughout)&lt;/cell&gt;
        &lt;cell&gt;€20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Aesthetics&lt;/cell&gt;
        &lt;cell&gt;LED lighting strip (because RGB makes it faster)&lt;/cell&gt;
        &lt;cell&gt;€10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Total&lt;/cell&gt;
        &lt;cell&gt;€8,930&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Not included: hearing protection (absolutely necessary), the microscope I already owned (but proved essential), several failed 3D prints, and the emotional cost of seeing “16,777,214°C” in system logs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;So, was it worth it? I now have a desktop that can run 235B parameter models at home for less than the cost of a single H100. It required disassembling $80,000 worth of enterprise hardware, debugging sensors that reported temperatures approaching the surface of the sun, and free-soldering components under a microscope. Your mileage may vary. Literally: I had to drive two hours to pick this thing up.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46222237</guid><pubDate>Wed, 10 Dec 2025 19:19:17 +0000</pubDate></item><item><title>EFF Launches Age Verification Hub as Resource Against Misguided Laws</title><link>https://www.eff.org/press/releases/eff-launches-age-verification-hub-resource-against-misguided-laws</link><description>&lt;doc fingerprint="358c117733a2b2d1"&gt;
  &lt;main&gt;
    &lt;p&gt;SAN FRANCISCO—With ill-advised and dangerous age verification laws proliferating across the United States and around the world, creating surveillance and censorship regimes that will be used to harm both youth and adults, the Electronic Frontier Foundation has launched a new resource hub that will sort through the mess and help people fight back.&lt;/p&gt;
    &lt;p&gt;To mark the hub's launch, EFF will host a Reddit AMA (“Ask Me Anything”) next week and a free livestreamed panel discussion on January 15 highlighting the dangers of these misguided laws.&lt;/p&gt;
    &lt;p&gt;“These restrictive mandates strike at the foundation of the free and open internet,” said EFF Activist Molly Buckley. “While they are wrapped in the legitimate concern about children's safety, they operate as tools of censorship, used to block people young and old from viewing or sharing information that the government deems ‘harmful’ or ‘offensive.’ They also create surveillance systems that critically undermine online privacy, and chill access to vital online communities and resources. Our new resource hub is a one-stop shop for information that people can use to fight back and redirect lawmakers to things that will actually help young people, like a comprehensive privacy law.”&lt;/p&gt;
    &lt;p&gt;Half of U.S. states have enacted some sort of online age verification law. At the federal level, a House Energy and Commerce subcommittee last week held a hearing on “Legislative Solutions to Protect Children and Teens Online.” While many of the 19 bills on that hearing’s agenda involve age verification, none would truly protect children and teens. Instead, they threaten to make it harder to access content that can be crucial, even lifesaving, for some kids.&lt;/p&gt;
    &lt;p&gt;It’s not just in the U.S. Effective this week, a new Australian law requires social media platforms to take reasonable steps to prevent Australians under the age of 16 from creating or keeping an account.&lt;/p&gt;
    &lt;p&gt;We all want young people to be safe online. However, age verification is not the panacea that regulators and corporations claim it to be; in fact, it could undermine the safety of many.&lt;/p&gt;
    &lt;p&gt;Age verification laws generally require online services to check, estimate, or verify all users’ ages—often through invasive tools like government ID checks, biometric scans, or other dubious “age estimation” methods—before granting them access to certain online content or services. These methods are often inaccurate and always privacy-invasive, demanding that users hand over sensitive and immutable personal information that links their offline identity to their online activity. Once that valuable data is collected, it can easily be leaked, hacked, or misused.&lt;/p&gt;
    &lt;p&gt;To truly protect everyone online, including children, EFF advocates for a comprehensive data privacy law.&lt;/p&gt;
    &lt;p&gt;EFF will host a Reddit AMA on r/privacy from Monday, Dec. 15 at 12 p.m. PT through Wednesday, Dec. 17 at 5 p.m. PT, with EFF attorneys, technologists, and activists answering questions about age verification on all three days.&lt;/p&gt;
    &lt;p&gt;EFF will host a free livestream panel discussion about age verification at 12 p.m. PDT on Thursday, Jan. 15. Panelists will include Cynthia Conti-Cook, Director of Research and Policy at the Collaborative Research Center for Resilience; a representative of Gen Z for Change; EFF Director of Engineering Alexis Hancock; and EFF Associate Director of State Affairs Rindala Alajaji. RSVP at https://www.eff.org/livestream-age.&lt;/p&gt;
    &lt;p&gt;For the age verification resource hub: https://www.eff.org/age&lt;/p&gt;
    &lt;p&gt;For the Reddit AMA: https://www.reddit.com/r/privacy/&lt;/p&gt;
    &lt;p&gt;For the Jan. 15 livestream: https://www.eff.org/livestream-age&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46223389</guid><pubDate>Wed, 10 Dec 2025 20:35:07 +0000</pubDate></item><item><title>Apple Services Experiencing Outage</title><link>https://www.apple.com/support/systemstatus/</link><description>&lt;doc fingerprint="50fedd95676c3945"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple Store Mac iPad iPhone Watch Vision AirPods TV &amp;amp; Home Entertainment Accessories Support 0 +&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46223577</guid><pubDate>Wed, 10 Dec 2025 20:47:15 +0000</pubDate></item></channel></rss>