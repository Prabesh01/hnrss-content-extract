<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 25 Jan 2026 17:09:08 +0000</lastBuildDate><item><title>Two Weeks Until Tapeout</title><link>https://essenceia.github.io/projects/two_weeks_until_tapeout/</link><description>&lt;doc fingerprint="37b09448082b2888"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Living under rocks#&lt;/head&gt;
    &lt;p&gt;As anyone that hasn’t been living under a rock might have heard, AI accelerators are the coolest kids in town these days. And although I have never been part of the “in” crowd, this time at least, I get the appeal.&lt;/p&gt;
    &lt;p&gt;So when the opportunity arose to join an experimental shuttle using global foundries 180nm for FREE I jumped onto the opportunity and designed my own JTAG!&lt;/p&gt;
    &lt;p&gt;…&lt;/p&gt;
    &lt;p&gt;I’m sorry? Is this not what you were expecting?&lt;/p&gt;
    &lt;p&gt;Frankly, I would love to tell you a great story about how I went into this wanting to design a free and open source silicon proven AI accelerator that the community could freely extend and re-use in their own projects. But in truth this project started out first as me wanting to design some far less sexy, in-silicon debug infrastructure and only later came to include the systolic matrix matrix multiplication accelerator … to serve as the design under test.&lt;/p&gt;
    &lt;p&gt;No wonder I was never one of the cool kids.&lt;/p&gt;
    &lt;p&gt;Also, I’m designing everything from scratch and have only two weeks left, welcome to the crunch.&lt;/p&gt;
    &lt;head rend="h3"&gt;Experimental shuttle#&lt;/head&gt;
    &lt;p&gt;Once again this tapeout was done as part of a Tiny Tapeout shuttle, but this time, it went out as not part of a public but an experimental shuttle.&lt;/p&gt;
    &lt;p&gt;These experimental shuttles are used as testing grounds for new nodes and flows in order to iron out issues before they are opened to the public.&lt;/p&gt;
    &lt;p&gt;Participation in these experimental shuttles is commonly reserved to contributors having previously submitted to Tiny Tapeout.&lt;/p&gt;
    &lt;p&gt;This limitation was set in place in order to help select for veteran designers as these experimental tapeout typically have less stable tooling and the final chip doesn’t feature the same level of inter design isolation as in a public tapeout.&lt;/p&gt;
    &lt;p&gt;Contributions to these shuttles are done with the understanding that the resulting chip might not be functional for some reason outside of the designers control.&lt;/p&gt;
    &lt;p&gt;Given these limitations, the Tiny Tapeout program is generously making submissions to these shuttles free of charge.&lt;/p&gt;
    &lt;p&gt;In practice, this makes area effectively free, explaining the higher occurrence of absolutely massive designs being submitted.&lt;/p&gt;
    &lt;p&gt;If the final chip is deemed sufficiently functional, the resulting ASICs along with the dev board will be available for purchase at the Tiny Tapeout store.&lt;/p&gt;
    &lt;p&gt;Given you need to have taped out a chip with Tiny Tapeout before to be eligible, I became eligible 14 days before the experimental shuttle submission deadline.&lt;/p&gt;
    &lt;head rend="h2"&gt;Combo#&lt;/head&gt;
    &lt;p&gt;Before we start, let us acknowledge what I am attempting to do, alone.&lt;lb/&gt;If this were any normal corporate setting, it would be the deadline equivalent to a one way ticket straight to the ever-lengthening queue outside the gates of Hell.&lt;lb/&gt;Do not try this at work!&lt;/p&gt;
    &lt;p&gt;Now that safety precautions are out of the way:&lt;/p&gt;
    &lt;p&gt;Welcome to a tale of two designs.&lt;/p&gt;
    &lt;p&gt;The first, is the systolic array, typically found at the heart of any AI inference accelerator, its function is to perform matrix-matrix multiplications.&lt;/p&gt;
    &lt;p&gt;The other is our silicon debug infrastructure, namely the all ubiquitous JTAG TAP component. Its goal is to provide something to latch onto when my ASIC comes back as an expensive brick and I am frantically trying to figure out where I fucked up. Given I wish to trust it not to be broken, having something that is proven early is very important since my masterplan is to slap it on all my future tapeouts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Project Roadmap#&lt;/head&gt;
    &lt;p&gt;Alright, I have to admit something: &lt;del&gt;I embellished reality in order to make myself look good&lt;/del&gt; I lied. Although initially I had 2 weeks to do this tapeout, because I spent the first 4 days reeling from the after effects of my previous tapeout (aka: sleeping, going outside and talking to another human being) and “deciding on a technical direction” which is corporate speech for describing a mixture between “procrastinating” and “figuring out what I could build without sacrificing too much of my remaining sanity”, I now had 10 days left. You’re welcome.&lt;/p&gt;
    &lt;p&gt;Given my self-imposed dire straits of a timeline, if I wanted to have any hope whatsoever of meeting the tapeout deadline I needed a battle plan: here is the roadmap.&lt;/p&gt;
    &lt;quote&gt;--- config: logLevel: 'debug' theme: 'default' gitGraph: showBranches: true mainBranchName: 'architecture' mainBranchOrder: 6 --- gitGraph TB: commit id: "idea " commit id: "figured it out " branch design order: 4 checkout design commit id: 'basic RTL ' branch simulation order: 5 checkout simulation commit id: 'this RTL is broken ' checkout design commit id: 'starts to work ' branch implementation order: 3 checkout implementation commit id: 'RIP timing ' commit id: 'RIP area ' checkout design commit id: 'looking good ' branch emulation order: 2 checkout emulation commit id: 'bitstream acquired ' branch firmware commit id: 'firmware bringup ' checkout emulation merge firmware checkout design merge emulation merge simulation checkout implementation merge design commit id: 'tapeout! '&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;architecture: what I need to build and how components would interface with one another.&lt;/item&gt;
      &lt;item&gt;design: where most of the RTL design takes place.&lt;/item&gt;
      &lt;item&gt;simulation: I write tests for simulating and validating that my design is behaving correctly and without any identifiable bugs. I used the classic Cocotb wrapper around iverilog for this.&lt;/item&gt;
      &lt;item&gt;FPGA emulation: when the design has mostly taken shape is when emulation takes place. This is the step where I port the design to an FPGA.&lt;/item&gt;
      &lt;item&gt;firmware: Alongside the emulation I bring up the firmware to interface with my design. This allows me to validate there are no issues when interfacing between the MCU and the ASIC.&lt;/item&gt;
      &lt;item&gt;implementation: this is when the ASIC flow is run and is the longest running task. It starts being run when the RTL design starts becoming functional to identify and fix timing and area utilization issues. And once all the verification is finished, it is run one final time to generate the final ASIC GDSII (manufacturing files).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The most astute readers might have noticed that this grand strategy is pretty much the same roadmap I always use.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;If it aint broken dont fix it. ~ a wise man&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;Flow saves the day#&lt;/head&gt;
    &lt;p&gt;So, why am I doing this to myself? Well, self delusion is a powerful force, and it was telling me this timeline would be possible if I leveraged my previous experience with the Tiny Tapeout/Librelane/OpenROAD flows, my existing personal linting/simulation/fpga/firmware flows, my existing code bases, and my own &lt;del&gt;ingrained knowledge of the dark arts&lt;/del&gt; experience.&lt;/p&gt;
    &lt;p&gt;But, let us not delude ourselves, the saving grace of this terrible idea was really just how great the Tiny Tapeout/Librelane/OpenROAD ASIC flow is.&lt;/p&gt;
    &lt;p&gt;The following section assumes readers are already familiar with the Open Source Silicon ecosystem.&lt;/p&gt;
    &lt;p&gt;For those not already familiar with Tiny Tapeout, Librelane and OpenROAD, you can find a short description of these in my previous hashing accelerator ASIC article, where I introduce some of the great tools that the open source silicon ecosystem has created.&lt;/p&gt;
    &lt;p&gt;The OpenROAD project was conceived with a no-human-in-the-loop (NHIL) target, and the goal of enabling 24-hour-or-less design turnaround times.&lt;/p&gt;
    &lt;p&gt;Librelane, the master coordinator of the flow itself, brings together OpenROAD, Yoysy, ABC, Magic, and many more amazing open source tools, building on top of this philosophy. Creating a process that takes you from your verilog and a few configurations all the way to the tapeout ready artifacts, in an extremely streamlined and fast fashion, requiring minimal human intervention.&lt;/p&gt;
    &lt;p&gt;Tiny Tapeout then completes the loop, running your testbenches on top of the entire implementation, and then allowing you to automatically upload your GDSII for integration into the shuttle chip.&lt;/p&gt;
    &lt;p&gt;This deeply resonates with my personal beliefs that faster iteration times are central to higher quality and more efficient design :&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Since I believe a low iteration time is paramount to project velocity and getting big things done, I also want to automatize all of the Vivado flow from taking the rtl to the SVF generation.&lt;/p&gt;&lt;lb/&gt;~ Previous article: Alibaba cloud FPGA: the 200$ Kintex UltraScale+&lt;/quote&gt;
    &lt;p&gt;Thus, not only was I building on top of a legacy of efficient design flows, but I had previously &lt;del&gt;spent&lt;/del&gt; invested a lot of time streamlining my tasks, especially repetitive ones.&lt;/p&gt;
    &lt;p&gt;A classic example would be my FPGA build flow used for emulation. It only requires a single command to create the Vivado project, read all the rtl and constraint files, synthesize, optionally add an ILA core and connect any wires marked for debug to it, implement, and then flash the bitstream :&lt;/p&gt;
    &lt;code&gt;make fpga_prog debug=1  
&lt;/code&gt;
    &lt;p&gt;Essentially, the bottleneck to making this design, from scratch in 10 days, wasn’t going to be the tools, but the squishy human between the chair and the keyboard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design#&lt;/head&gt;
    &lt;p&gt;Without further ado, let’s talk design !&lt;/p&gt;
    &lt;head rend="h2"&gt;Systolic Array Design#&lt;/head&gt;
    &lt;p&gt;The goal of this systolic array is to perform a 2×2 matrix-matrix multiply on 8-bit integer numbers.&lt;/p&gt;
    &lt;p&gt;Without going too much into detail on why systolic arrays are the recurring stars at the heart of modern AI inference accelerators, their main strength is achieving a high ratio of compute to everything else. And since memory operations are the most expensive family of operations by far, a hh ratio of compute to memory operations.&lt;/p&gt;
    &lt;p&gt;Data is recirculated directly within the array and reused across multiple consecutive operations rather than being repeatedly fetched/written from/to memory. This matters because memory accesses are expensive. SRAM accesses cost time and significant power, while DRAM accesses cost eternities of time and egregious amounts of power. Compute operations, even 64 bit floating point multiplications, are by comparison, cheap.&lt;/p&gt;
    &lt;p&gt;Thus, the larger the systolic array, the deeper the chain of compute, the better this compute to memory ratio becomes.&lt;/p&gt;
    &lt;head rend="h4"&gt;Energy ratio#&lt;/head&gt;
    &lt;p&gt;As an illustrative example of this evolution of compute ratios, let us compare the power cost of 64 bit floating point multiply-add (MAC) operations in a hypothetical systolic array designed on a 45nm node running at 0.9V.&lt;/p&gt;
    &lt;p&gt;Compared with the energy expenditure needed to access this data using 256 bit wide reads to the 16nm DRAM. DRAM access costs will include the 10mm of wire, interface and access costs.&lt;/p&gt;
    &lt;p&gt;In this example, we will very generously assume the weights are stored in place and do not need to be updated, so will assume only the input data matrix needs to be read from DRAM.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Operation&lt;/cell&gt;
        &lt;cell role="head"&gt;45 nm&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;16-bit integer multiply&lt;/cell&gt;
        &lt;cell&gt;2 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;64-bit floating-point multiply-add&lt;/cell&gt;
        &lt;cell&gt;50 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;64-bit access to 8-Kbyte SRAM&lt;/cell&gt;
        &lt;cell&gt;14 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;256-bit access to 1-Mbyte SRAM&lt;/cell&gt;
        &lt;cell&gt;566 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;256-bit 10 mm wire&lt;/cell&gt;
        &lt;cell&gt;310 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;256-bit DRAM interface&lt;/cell&gt;
        &lt;cell&gt;5,120 pJ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;256-bit DRAM access&lt;/cell&gt;
        &lt;cell&gt;2,048 pJ&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;source: ENERGY PROPORTIONAL MEMORY SYSTEMS&lt;/p&gt;
    &lt;p&gt;Even though I purposefully chose 64 bit floats, the most energy intensive arithmetic operation, we still required a 64x64 systolic array before the cost of compute started exceeding the cost of the initial DRAM value reads.&lt;/p&gt;
    &lt;p&gt;aka: For those not living in 2026, we have uncovered a new clue to the mystery of where all the low-power DRAM chips have suddenly vanished to!&lt;/p&gt;
    &lt;head rend="h4"&gt;Scaling#&lt;/head&gt;
    &lt;p&gt;Another great feature of systolic arrays is how regular they are and how well their design scales. So, although today I am designing a small 2x2 array, without much re-work this can be scaled up to a larger 256x256 array.&lt;/p&gt;
    &lt;p&gt;Some of you might be wondering: if area is free and systolic arrays scale so well, why am I limiting myself to only making a 2x2 array ?&lt;/p&gt;
    &lt;p&gt;Well, because I am a good neighbor and any additional tile of area I use is potentially depriving someone else from having the area available to submit their project.&lt;/p&gt;
    &lt;p&gt;Since the initial motivation for this project was to design some proven in silicon debug infrastructure, a 2x2 array is sufficient for my needs.&lt;/p&gt;
    &lt;head rend="h3"&gt;Constraints#&lt;/head&gt;
    &lt;p&gt;Since this ASIC is once again taping out as part of a Tiny Tapeout shuttle, it has to grapple with the similar limitations as my previous hashing accelerator, namely: the eternal I/O bandwidth limitation!&lt;/p&gt;
    &lt;p&gt;This limitations comes in two flavors:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Pin count: eight input pins, eight output pins, and eight configurable I/O pins, limiting my parallel buses in and out of the accelerator.&lt;/item&gt;
      &lt;item&gt;Maximum operating frequency: unlike with the well-trodden path that was the public Skywater 130nm shuttle tapeout, for this experimental shuttle the maximum switching frequency of these pins hasn’t been characterized yet. And since I haven’t yet figured out how to do this characterization using a simulator myself, I’ve hand-wavingly assumed that both the input and output directions have a sustainable switching frequency of 50 MHz, and have sized the path timings around that assumption.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally, I once again have the constraint of not having any SRAM. Though unlike with the Skywater 130 tapeout, the GlobalFoundries 180nm PDK does include a proven SRAM macro!&lt;/p&gt;
    &lt;p&gt;Hooray! Human progress is unstoppable !&lt;/p&gt;
    &lt;p&gt;This time, the limitation was actually imposed by the experimental nature of this shuttle.&lt;/p&gt;
    &lt;p&gt;Firstly, the flow didn’t initially support these macros out of the box. More specifically in the way they were laid out, which led to a slew of DRC failures that needed to be fixed.&lt;/p&gt;
    &lt;p&gt;Secondly, this experimental shuttle didn’t provide individual project power gating. As such, integrating this SRAM macro was commonly deemed not the best idea, and the community collectively agreed to wait for a future shuttle, which would include per-project macro power gating, before including it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design Details#&lt;/head&gt;
    &lt;p&gt;This system is broken into two parts :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the compute units&lt;/item&gt;
      &lt;item&gt;the main array controller&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Compute units#&lt;/head&gt;
    &lt;p&gt;Since this is a 2×2 systolic array, there are four compute units. Each unit takes in an 8 bit signed integer and performs a multiply, addition, and a clamping operation, producing 8 bit signed integers.&lt;/p&gt;
    &lt;head rend="h4"&gt;Multiplication#&lt;/head&gt;
    &lt;p&gt;The multiply is done using a custom (from scratch) implementation of a Booth Radix-4 multiplier with Wallace trees.&lt;/p&gt;
    &lt;p&gt;This multiplier architecture strikes a good balance between area, power and performance, and can be regarded as the multiplier equivalent of what “Rental White™"[1] is to interior design.&lt;/p&gt;
    &lt;p&gt;Meaning it is a solid, well rounded from a PPA’s perspective, multiplier option, that without being anything novel or groundbreaking, is probably good enough for your use case (and given my +2ns of worst negative slack, and comfortable area occupancy, it was plenty good enough for mine).&lt;/p&gt;
    &lt;p&gt;Another advantage of booth radix-4 is that, since we are performing a signed multiplication, we can optimize out a level in the Wallace tree we are using for the partial product additions given we only have 4 partial products, unlike the 5 needed for unsigned operations.&lt;/p&gt;
    &lt;p&gt;The original article plan included an in-depth explanation of the booth radix-4 multiplication implementation and optimization, resulting in a very interesting but also very dry multi-page explanation covered with boolean algebra. Resulting in its canning.&lt;/p&gt;
    &lt;p&gt;If you are looking for a good explanation of this multiplier and its optimization see chapter 11 of ‘CMOS VLSI Design: A Circuits and Systems Perspective’.&lt;/p&gt;
    &lt;head rend="h4"&gt;Clamping#&lt;/head&gt;
    &lt;p&gt;The clamping operation occurs after both the multiply and addition operations. At which point the data is now 17 bit wide, and since we want to prevent our data size from exploding, the clamping operation is needed to clamp the outgoing data back down to eight bits.&lt;/p&gt;
    &lt;p&gt;$$ clamp_{i8}(x) = \begin{cases} \phantom{-}127,\,\text{if}\,x &amp;gt; 127, \\ \phantom{-12}x,\,\text{if}\,x \in [-128,127], \\ -128,\,\text{if}\,x &amp;lt; -128 \end{cases} $$&lt;/p&gt;
    &lt;head rend="h4"&gt;In place weight storage#&lt;/head&gt;
    &lt;p&gt;Given that the weights have high temporal and spatial locality, meaning the same set of weights can be reused over multiple unique input data matrices, in order to save on bandwidth, the choice was made to store an 8 bit weight in place inside of each unit.&lt;/p&gt;
    &lt;p&gt;A separate control sequence allows the user to load a new set of weights inside each of the units. The weight packet is sent over four consecutive cycles using the same input interface as the data matrix.&lt;/p&gt;
    &lt;head rend="h3"&gt;Array controller#&lt;/head&gt;
    &lt;p&gt;Given the way in which the matrix matrix multiplication is performed using a systolic array, the input data matrix needs to be shaped and fed to the array in a staggered manner.&lt;/p&gt;
    &lt;p&gt;Understandably readers might not instinctively get what I mean by “the data needs to be shaped”.&lt;/p&gt;
    &lt;p&gt;In order to help better illustrate this point, I would like to bring your attention to a great animation to show how data flows through the array.&lt;/p&gt;
    &lt;p&gt;An added bonus is that this animation is also of a 2×2 systolic array, such that it shares the same data with the same arrival constraints as my accelerator.&lt;/p&gt;
    &lt;p&gt;This animation is not a carbon copy of my accelerator. Rather, readers should use this animation as a tool to better understand how data flows in a systolic array and how it results in the computation of a matrix-matrix multiplication. For the sake of completeness, I would like to point out the major differences with my implementation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;this animation uses floating-point numbers, I am using 8-bit integers&lt;/item&gt;
      &lt;item&gt;there is no clamping step&lt;/item&gt;
      &lt;item&gt;the timings are not entirely identical. In my accelerator, many more operations occur on the same cycle. Granted, this was probably done in an effort to help make the animation more legible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In addition to helping shape the input data to the systolic array, this controller also helps coordinate data transfers around my I/O limitations.&lt;/p&gt;
    &lt;p&gt;Given the parallel data bus allows only 8 bits of input data to arrive per cycle, it is used to control the input buffers in order to accumulate enough data to create the next wave.&lt;/p&gt;
    &lt;p&gt;Similarly on the output side, when the accelerator produces two 8-bit results per cycle, the controller stores the results in an output buffer in order to only stream out 8 bits per cycle.&lt;/p&gt;
    &lt;head rend="h3"&gt;Validation#&lt;/head&gt;
    &lt;p&gt;Validation was an extremely important step for this systolic array, particularly for validating my custom implementation of the Booth Radix-4 multiplication.&lt;/p&gt;
    &lt;p&gt;Once again, I used Cocotb as the abstracting layer allowing me to interface with multiple different simulators. Namely, icarus verilog for my standard verification and CVC for the post implementation timing annotated netlist.&lt;lb/&gt;This validation followed my standard approach of providing a set of input data matrices and weights over the standard accelerators interfaces and comparing the produced results to the expected values.&lt;/p&gt;
    &lt;p&gt;Given the nature of the problem, I made extensive use of randomization in order to get better testing coverage and attempt to hit corner cases that would not have been revealed using a directed approach. This randomization randomized both the values of the inputs weight and data, and also the timings with which this incoming data was fed over the parallel bus to the accelerator.&lt;/p&gt;
    &lt;head rend="h3"&gt;Firmware#&lt;/head&gt;
    &lt;p&gt;The firmware used to interface with this accelerator was designed to run on the RP2040 Raspberry Pi silicon and used the PIO hardware block to drive the parallel port.&lt;/p&gt;
    &lt;p&gt;I followed a similar approach as with my hashing accelerator, co-designing with the PIO imposed limitation when designing the parallel bus protocol.&lt;/p&gt;
    &lt;p&gt;Although this was designed for an RP2040, I later learned that the Tiny Tapeout boards would be evolving to a new version of the chip. These new PCBs would use a different pinout layout for communicating with the ASIC chips.&lt;/p&gt;
    &lt;p&gt;Although this will necessitate to re-adapt parts of the firmware for the new dev boards, this shouldn’t cause any major incompatibilities.&lt;/p&gt;
    &lt;head rend="h2"&gt;JTAG TAP Design#&lt;/head&gt;
    &lt;p&gt;The second major component of this design and admittedly the actual principle piece of this ASIC is the JTAG TAP.&lt;/p&gt;
    &lt;p&gt;As stated earlier when my ASIC comes back as a glorified paperweight I will suddenly have a strong and immediate need for some kind of hardware block providing some in-silicon observability.&lt;/p&gt;
    &lt;p&gt;This is absolutely crucial, as it isn’t a question of if but when one of my ASIC will have hardware issues.&lt;lb/&gt;And, when that day comes, not having a view of the internal behavior will only make it that much more painful to identify the root issue in order to fix it for the upcoming generation.&lt;/p&gt;
    &lt;p&gt;As such, this TAP is actually a part of my larger efforts to help produce a set of DFT (Design for Test) proven IPs/tools that I can integrate into all of my future ASICs. As such, I have quite an incentive to have these designs proven early.&lt;/p&gt;
    &lt;p&gt;JTAG was chosen as it is a very common debug protocol. Not only is it well-defined but has good off-the-shelf support, both on the hardware (debug probes) and software side.&lt;/p&gt;
    &lt;head rend="h3"&gt;JTAG Implementation#&lt;/head&gt;
    &lt;p&gt;In addition to implementing all of the basic JTAG features in order to be compliant with the protocol, I additionally added custom instructions, extending it to fit my personal requirements.&lt;/p&gt;
    &lt;p&gt;Required instructions :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;EXTEST&lt;/code&gt;, opcode&lt;code&gt;0x0&lt;/code&gt;, boundary scan operation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;IDCODE&lt;/code&gt;, opcode&lt;code&gt;0x1&lt;/code&gt;, read JTAG tap identifiers, which allows the hardware to advertise itself on the JTAG scan chain&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SAMPLE_PRELOAD&lt;/code&gt;, opcode&lt;code&gt;0x2&lt;/code&gt;, boundary scan operation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BYPASS&lt;/code&gt;, opcode&lt;code&gt;0x7&lt;/code&gt;, set the TAP in bypass mode&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Custom instructions :&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;USER_REG&lt;/code&gt;, opcode&lt;code&gt;0x3&lt;/code&gt;, custom instruction used to probe the internal registers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The systolic array is quite a deep structure in the sense that the data is directly reused as it is recirculated within the array.&lt;/p&gt;
    &lt;p&gt;As such, it is very easy to lose track of what the internal state is, having only the input and output observable to the user. Although this might stay manageable with a 2 x 2 array. As the structure grows larger, this will start to become more and more of an issue.&lt;/p&gt;
    &lt;p&gt;In order to help address this future pain point, I added the custom &lt;code&gt;USER_REG&lt;/code&gt; JTAG
instruction to read the current state of a target compute units internal registers.&lt;/p&gt;
    &lt;p&gt;Link to the datasheet of the &lt;code&gt;USER_REG&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;head rend="h3"&gt;Design#&lt;/head&gt;
    &lt;p&gt;The JTAG TAP design itself is quite straightforward as JTAG was conceived as a hardware first protocol, and this shows in its implementation. The design clearly flows from the JTAG specification to the RTL (unlike you BLAKE2 I am looking at you!). As such I do not think it is worthwhile to discuss it in detail.&lt;/p&gt;
    &lt;p&gt;What makes this design more interesting is how the JTAG and the systolic array live in two different clock domains. Making this accelerator have not one but two separate clock trees.&lt;/p&gt;
    &lt;p&gt;On the altar of this noble cause, I sacrificed one of my precious data input pins to serve as the JTAG clock input (TCK).&lt;/p&gt;
    &lt;p&gt;The SDC script I used to generate these two clock trees drew heavy inspiration from the official LibreLane clock generation SDC, but was re-adapted from my contorted used case:&lt;/p&gt;
    &lt;code&gt;# modified librelane base.sdc to support 2 clocks

# custom env variable
set ::env(JTAG_CLOCK_PERIOD) 500


if { [info exists ::env(CLOCK_PORT)] } {
    set port_count [llength $::env(CLOCK_PORT)]
    puts "/gc[INFO] Found ${port_count} clocks : $::env(CLOCK_PORT)%"
    if { $port_count == "0" } {
        puts "/gc[ERROR] No CLOCK_PORT found."
        error
    }

    # set both clock ports
    set ::clock_port [lindex $::env(CLOCK_PORT) 0]
    set ::jtag_clock_port [lindex $::env(CLOCK_PORT) 1]
}


set port_args [get_ports $clock_port]
set jtag_port_args [get_ports $jtag_clock_port]

puts "/gc[INFO] Using clock $clock_port… with args $port_args"
puts "/gc[INFO] Using jtag clock $jtag_clock_port… with args $jtag_port_args"


create_clock {*}$port_args -name $clock_port -period $::env(CLOCK_PERIOD)
create_clock {*}$jtag_port_args -name $jtag_clock_port -period $::env(JTAG_CLOCK_PERIOD)

set input_delay_value [expr $::env(CLOCK_PERIOD) * $::env(IO_DELAY_CONSTRAINT) / 100]
set output_delay_value [expr $::env(CLOCK_PERIOD) * $::env(IO_DELAY_CONSTRAINT) / 100]
puts "/gc[INFP] for clk $clock_port :"
puts "/gc[INFO] Setting output delay to: $output_delay_value"
puts "/gc[INFO] Setting input delay to: $input_delay_value"


# keep the same io delay constraints for jtag 
set jtag_input_delay_value [expr $::env(JTAG_CLOCK_PERIOD) * $::env(IO_DELAY_CONSTRAINT) / 100]
set jtag_output_delay_value [expr $::env(JTAG_CLOCK_PERIOD) * $::env(IO_DELAY_CONSTRAINT) / 100]
puts "/gc[INFP] for clk $jtag_clock_port :"
puts "/gc[INFO] Setting output delay to: $jtag_output_delay_value"
puts "/gc[INFO] Setting input delay to: $jtag_input_delay_value"


set_max_fanout $::env(MAX_FANOUT_CONSTRAINT) [current_design]
if { [info exists ::env(MAX_TRANSITION_CONSTRAINT)] } {
    set_max_transition $::env(MAX_TRANSITION_CONSTRAINT) [current_design]
}
if { [info exists ::env(MAX_CAPACITANCE_CONSTRAINT)] } {
    set_max_capacitance $::env(MAX_CAPACITANCE_CONSTRAINT) [current_design]
} 

# clk
set clk_input [get_port $clock_port]
set clk_indx [lsearch [all_inputs] $clk_input]
set all_inputs_wo_clk [lreplace [all_inputs] $clk_indx $clk_indx ""]

# jtag clk
set jtag_clk_input [get_port $jtag_clock_port]
set jtag_clk_indx [lsearch [all_inputs] $jtag_clk_input]
set jtag_all_inputs_wo_clk [lreplace [all_inputs] $jtag_clk_indx $jtag_clk_indx ""]

# rst
set all_inputs_wo_clk_rst $all_inputs_wo_clk

# jtag has no trst so there is no need to define another rst path 

# correct resetn
set clocks [get_clocks $clock_port]

set_input_delay $input_delay_value -clock $clocks $all_inputs_wo_clk_rst
set_output_delay $output_delay_value -clock $clocks [all_outputs]

if { ![info exists ::env(SYNTH_CLK_DRIVING_CELL)] } {
    set ::env(SYNTH_CLK_DRIVING_CELL) $::env(SYNTH_DRIVING_CELL)
}

set_driving_cell /gc
    -lib_cell [lindex [split $::env(SYNTH_DRIVING_CELL) "/"] 0] /gc
    -pin [lindex [split $::env(SYNTH_DRIVING_CELL) "/"] 1] /gc
    $all_inputs_wo_clk_rst

set_driving_cell /gc
    -lib_cell [lindex [split $::env(SYNTH_CLK_DRIVING_CELL) "/"] 0] /gc
    -pin [lindex [split $::env(SYNTH_CLK_DRIVING_CELL) "/"] 1] /gc
    $clk_input

set_driving_cell /gc
    -lib_cell [lindex [split $::env(SYNTH_CLK_DRIVING_CELL) "/"] 0] /gc
    -pin [lindex [split $::env(SYNTH_CLK_DRIVING_CELL) "/"] 1] /gc
    $jtag_clk_input

set cap_load [expr $::env(OUTPUT_CAP_LOAD) / 1000.0]
puts "/gc[INFO] Setting load to: $cap_load"
set_load $cap_load [all_outputs]

puts "/gc[INFO] Setting clock uncertainty to: $::env(CLOCK_UNCERTAINTY_CONSTRAINT)"
set_clock_uncertainty $::env(CLOCK_UNCERTAINTY_CONSTRAINT) $clocks

puts "/gc[INFO] Setting clock transition to: $::env(CLOCK_TRANSITION_CONSTRAINT)"
set_clock_transition $::env(CLOCK_TRANSITION_CONSTRAINT) $clocks

puts "/gc[INFO] Setting timing derate to: $::env(TIME_DERATING_CONSTRAINT)%"
set_timing_derate -early [expr 1-[expr $::env(TIME_DERATING_CONSTRAINT) / 100]]
set_timing_derate -late [expr 1+[expr $::env(TIME_DERATING_CONSTRAINT) / 100]]

if { [info exists ::env(OPENLANE_SDC_IDEAL_CLOCKS)] &amp;amp;&amp;amp; $::env(OPENLANE_SDC_IDEAL_CLOCKS) } {
    unset_propagated_clock [all_clocks]
} else {
    set_propagated_clock [all_clocks]
}


set_clock_groups -asynchronous -group $clock_port -group $jtag_clock_port
&lt;/code&gt;
    &lt;p&gt;Because the official JTAG spec lives behind the impregnable IEEE paywall, a castle in which I am not permitted to set foot as a result of not having paid its lord my dues, the verification of the JTAG TAP was actually quite interesting.&lt;/p&gt;
    &lt;p&gt;Since JTAG is such a common protocol, its easy to find free resources online to build a good mental model of the internal FSM and TAP structure. My initial implementation and test bench was derived from this best effort understanding.&lt;lb/&gt;As such emulation actually became a critical step for validating this JTAG TAP.&lt;/p&gt;
    &lt;p&gt;Recall how earlier I mentioned that JTAG was a well-supported protocol with good off-the-shelf support on the software side?&lt;lb/&gt;Did I mention OpenOCD has great support for JTAG?&lt;/p&gt;
    &lt;p&gt;If I wasn’t going to get access to the official spec on how JTAG is expected to behave, plan B was to let my implementation be guided by how OpenOCD expected JTAG to behave.&lt;/p&gt;
    &lt;p&gt;To the purists clutching their pearls flabbergasted by the idea of not implementing per the spec: I’m not sorry. But if you do have the official spec here is my email.&lt;/p&gt;
    &lt;p&gt;In practice, whenever OpenOCD flagged my JTAG behavior as problematic, I assumed that my implementation was at fault. And trust me, there were issues. I called this&lt;/p&gt;
    &lt;p&gt;$$Designed\, By\, Support\,^{TM}$$&lt;/p&gt;
    &lt;p&gt;Then came the matter of supporting my custom TAP’s unholy instructions.&lt;/p&gt;
    &lt;p&gt;Luckily for me, OpenOCD allows you to finetune its behavior, through custom TCL scripts (did I mention I have PTSD and now love TCL?).&lt;/p&gt;
    &lt;p&gt;Thanks to already having gone through the pain of learning to create custom OpenOCD scripts during my Alibaba accelerator salvage project, this was a breeze.&lt;/p&gt;
    &lt;p&gt;These scripts allowed me to bring up my custom TAP and add support for my own godforsaken instructions.&lt;/p&gt;
    &lt;p&gt;The script can be found here, and below is &lt;del&gt;proof of my crime&lt;/del&gt; a log of me interfacing with my design during emulation:&lt;/p&gt;
    &lt;code&gt;Open On-Chip Debugger 0.12.0+dev-02171-g11dc2a288 (2025-11-23-19:25)  
Licensed under GNU GPL v2  
For bug reports, read  
	http://openocd.org/doc/doxygen/bugs.html  
Info : J-Link V10 compiled Jan 30 2023 11:28:07  
Info : Hardware version: 10.10  
Info : VTarget /gc= 3.380 V  
Info : clock speed 2000 kHz  
Info : JTAG tap: tpu.tap tap/device found: 0x1beef0d7 (mfg: 0x06b (Transwitch), part: 0xbeef, ver: 0x1)  
&lt;/code&gt;
    &lt;p&gt;Readers having reached enlightenment in their level of familiarity with the JTAG protocol might notice that the chip actually advertises itself as an Nvidia accelerator (mfg &lt;code&gt;0x06b&lt;/code&gt;.)&lt;/p&gt;
    &lt;p&gt;It’s good to have dreams.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion#&lt;/head&gt;
    &lt;p&gt;And somehow, against all odds, I made it! I met this absurd self-imposed deadline and the chip is now in fabrication!&lt;/p&gt;
    &lt;p&gt;This project was a battle against time. But making it fit in the end is something I’m honestly very proud of.&lt;/p&gt;
    &lt;p&gt;One of the big difficulties was to not get side-tracked. Even if it was frustrating, I had to commit to only adding the minimal set of features needed to get the project rolling. And trust me, the temptation to add, just that one extra little teeny-tiny feature, either to the systolic array or the debug system, was excruciating.&lt;/p&gt;
    &lt;p&gt;The good news is that, with this upcoming tapeout (v2 has already started), I can finally lash out !! My grand strategy is for the next project to be an improvement of both concepts.&lt;/p&gt;
    &lt;p&gt;Firstly, I’d like to finally have a rematch with an old adversary of mine: Floating Point arithmetics! Less for the TOPS-&amp;gt;FLOPS bragging rights, but rather to finally pierce its deep mysteries through an optimized hardware implementation of my own.&lt;/p&gt;
    &lt;p&gt;Secondly, I will continue extending my debug infrastructure: adding scan chains and an ATPG flow not only to help identify silicon manufacturing issues, but also as a convenient way to extract the current state of all flops for usage debugging.&lt;/p&gt;
    &lt;p&gt;Yet, this is but another step towards my greater long term objective: to tape out my own chip, not as part of Tiny Tapeout, nor as part of a shuttle chip, but entirely on my own.&lt;/p&gt;
    &lt;p&gt;There are many minutiea of the ASIC’s design that for now (and for good reason) abstracted away by Tiny Tapeout. So, my objective is to use these shuttle programs as a harness while I master the missing skills.&lt;/p&gt;
    &lt;p&gt;I would like to finish with a thanks to a company, which has not been sponsoring me in any way, is totally unaware of my existence, but who, never the less, never fail to provide the level of reward desperately needed after an all nighter spent trying to wrap up my design:&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes#&lt;/head&gt;
    &lt;p&gt;[1] - All uses and attributions of the designation ‘rental white’™ are the sole property of John and are used hereby with acknowledgment of his proprietary rights.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46749671</guid><pubDate>Sun, 25 Jan 2026 01:25:37 +0000</pubDate></item><item><title>Challenges and Research Directions for Large Language Model Inference Hardware</title><link>https://arxiv.org/abs/2601.05047</link><description>&lt;doc fingerprint="bc9e340b55f380cd"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Hardware Architecture&lt;/head&gt;&lt;p&gt; [Submitted on 8 Jan 2026 (v1), last revised 14 Jan 2026 (this version, v2)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Challenges and Research Directions for Large Language Model Inference Hardware&lt;/head&gt;View PDF&lt;quote&gt;Abstract:Large Language Model (LLM) inference is hard. The autoregressive Decode phase of the underlying Transformer model makes LLM inference fundamentally different from training. Exacerbated by recent AI trends, the primary challenges are memory and interconnect rather than compute. To address these challenges, we highlight four architecture research opportunities: High Bandwidth Flash for 10X memory capacity with HBM-like bandwidth; Processing-Near-Memory and 3D memory-logic stacking for high memory bandwidth; and low-latency interconnect to speedup communication. While our focus is datacenter AI, we also review their applicability for mobile devices.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Xiaoyu Ma [view email]&lt;p&gt;[v1] Thu, 8 Jan 2026 15:52:11 UTC (832 KB)&lt;/p&gt;&lt;p&gt;[v2] Wed, 14 Jan 2026 20:37:46 UTC (983 KB)&lt;/p&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AR&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46750214</guid><pubDate>Sun, 25 Jan 2026 02:48:36 +0000</pubDate></item><item><title>Hands-On with Two Apple Network Server Prototype ROMs</title><link>http://oldvcr.blogspot.com/2026/01/hands-on-with-two-apple-network-server.html</link><description>&lt;doc fingerprint="a60898770926b1d7"&gt;
  &lt;main&gt;&lt;p&gt;Here's why I need to do inventory more often.&lt;/p&gt;This is an Apple prototype ROM I am ashamed to admit I found in my own box of junk from various Apple Network Server parts someone at Apple Austin sent me in 2003. The 1996 Apple Network Server is one of Apple's more noteworthy white elephants and, to date, the last non-Macintosh computer (iOS devices notwithstanding) to come from Cupertino. Best known for being about the size of a generous dorm fridge and officially only running AIX 4.1, IBM's proprietary Unix for Power ISA, its complicated history is a microcosm of some of Apple's strangest days during the mid-1990s. At $10,000+ a pop (in 2026 dollars over $20,700), not counting the AIX license, they sold poorly and were among the first products on the chopping block when Steve Jobs returned in 1997.&lt;p&gt;stockholm, my own Apple Network Server 500, was a castoff I got in 1998 — practically new — when the University bookstore's vendor wouldn't support the hardware and it got surplused. It was the first Unix server I ever owned personally, over the years I ended up installing nearly every available upgrade, and it ran Floodgap.com just about nonstop until I replaced it with a POWER6 in 2012 (for which it still functions as an emergency reserve). Plus, as the University was still running RS/6000 systems back then, I had ready access to tons of AIX software which the ANS ran flawlessly. It remains one of the jewels of my collection.&lt;/p&gt;&lt;p&gt;So when the mythical ANS MacOS ROM finally surfaced, I was very interested. There had always been interest in getting the ANS to run MacOS back in the day (I remember wasting an afternoon trying with a Mac OS 8 CD) and it was a poorly-kept secret that at various points in its development it could, given its hardware basis as a heavily modified Power Macintosh 9500. Apple itself perceived this interest, even demonstrating it with Mac OS prior to its release, and leading then-CTO Ellen Hancock to later announce that the ANS would get ROM upgrades to allow it to run both regular Mac OS and, in a shock to the industry, Windows NT. This would have made the ANS the first and only Apple machine ever sold to support it.&lt;/p&gt;&lt;p&gt;Well, guess what. This is that pre-production ROM Apple originally used to demonstrate Mac OS, and another individual has stepped up with the NT ROMs which are also now in my possession. However, at that time it wasn't clear what the prototype ROM stick was — just a whole bunch of flash chips on a Power Mac ROM DIMM which my Apple contacts tell me was used to develop many other machines at the time — and there was no way I was sticking it into my beloved production 500. But we have a solution for that. Network Servers came in three sizes: the rackmount ANS 300 ("Deep Dish") which was never released except for a small number of prototypes, the baseline ANS 500 ("Shiner LE"), and the highest tier ANS 700 ("Shiner HE") which added more drive bays and redundant, hot-swappable power supplies.&lt;/p&gt;&lt;p&gt;Which brings us to this machine.&lt;/p&gt;Meet holmstock, my Network Server 700, and the second ANS in my collection (the third is my non-functional Shiner ESB prototype). This was a ship of Theseus that my friend CB and I assembled out of two partially working but rather thrashed 700s we got for "come and get them" in August 2003. It served as stockholm's body double for a number of years until stockholm was retired and holmstock went into cold storage as a holding bay for spare parts. This makes it the perfect system to try a dodgy ROM in.&lt;p&gt;I'll give you a spoiler now: it turns out the NT ROM isn't enough to install Windows NT by itself, even though it has some interesting attributes. Sadly this was not unexpected. But the pre-production ROM does work to boot Mac OS, albeit with apparent bugs and an injection of extra hardware. Let's get the 700 running again (call it a Refurb Weekend) and show the process.&lt;/p&gt;The 700 weighs around 85 pounds unloaded and is exactly like trying to cram a refrigerator into the backseat of your car (in this case my Honda Civic Si). While it does have wheels on the bottom, even the good ones don't have a great turning radius (and these aren't good), and getting it in and out of the car unavoidably means having to pick it up. Lift with your knees, not with your back.&lt;head rend="h3"&gt;Preparing the 700 for testing&lt;/head&gt;&lt;p&gt;This section is basically a cloaked Refurb Weekend, but even if you're familiar with ANS guts, I'm going to point out a few specific things relevant to ROM support as we go along. We want this machine as ship-shape as we can get it so that accurate observations can be made for posterity!&lt;/p&gt;&lt;p&gt;I would also like to thank my wife who chose to politely ignore the new noisy beast hulking in the living room for a few days.&lt;/p&gt;Continuing in the fridge motif, the 500 and 700 have a front keylock controlling a sliding door, along with a unique 4-line LCD which displays boot information and can be used as an output device in AIX and other operating systems. Unlike my very minimally yellowed 500 which has spent most of its life in quiet smoke-free server rooms, this one seemed to have gotten a bit more sun. Fortunately most of the chassis is painted metal which is also where most of the weight comes from. The keylock position on power-up is noted by the firmware; the leftmost is the service setting, the middle is a normal boot, and the rightmost (locked) position puts the machine into a power failsafe mode. The sliding door covers seven front drive bays, normally one with a CD-ROM, one with some sort of tape drive (typically a DAT/DDS drive, but a few have 8mm tape instead, both the same drives as sold for the Workgroup Server 95 and 9150), and the rest various hard drives which can be either independent or connected into an optional RAID. The 700 can take two more drives in a rear bracket. Although I have the RAID card, I never ended up installing it since a single drive was more than sufficient for what I was using it for. As most of the drive trays and both drive brackets had been removed from the two donor 700s used to assemble holmstock, I ended up just keeping a CD-ROM and two trays, and used the other open space for storage.&lt;p&gt;At the top are the NMI, reset and power buttons, plus a standard Mac floppy drive.&lt;/p&gt;&lt;p&gt;It is worth noting here that the internal bays are all serviced by two Symbios Logic 53C825A controllers, providing two Fast Wide SCSI busses running at 20MB/s. Unlike the typical Power Mac MESH (10MB/s) controller, the ANS internal SCSI controllers are unique to the ANS and appear in no other Apple product. Remember this for later. A second external SCSI bus is available on the rear, using the same (slower 5MB/s) CURIO SCSI/Ethernet/serial combo chip as other contemporary Power Macs and implementing an NCR 53C94.&lt;/p&gt;The rear (with the monitor power cable photobombing the shot) is much less yellowed. Ports are here for audio in and out (standard AWACS), ADB, two beige Mac MiniDIN-8 serial ports, VGA (oddly but happily a conventional HDI-15, not Apple's traditional DA-15), AAUI 10Mbit Ethernet (any AAUI Mac dongle will work), and the external SCSI bus DB-25. Six PCI slots are available. A second keylock secures the logic board which is on a slide-out drawer accessed with the two handles. Both rear panels have their own fans which are hot-swappable as well. Apple included a monitor dongle in the box.&lt;p&gt;It is also worth noting here that the onboard video is a Cirrus Logic 54M30, also unique to the ANS, and likewise also used in no other Apple product. We'll be coming back to this point too.&lt;/p&gt;Parenthetically, here are the keylocks (new replacements in my part box). They are wafer-lock keys of the same type used in the Quadra 950, Apple Workgroup Server 95 and Workgroup Server 9150. As sold Network Servers came with three keys, one front, one back and one spare, but they are all interchangeable. These keys have a small three-digit code engraved into the metal identifying the lock they are designed to fit. I also got out a lot of parts from storage just in case they were needed, some of which were in the 700 and some of which were separate. Besides my two boxes of tricks, I also pulled out a spare logic board, five boxes of RAM upgrade kits (these are only 16MB each, though, so this isn't as much memory as you'd think), a 200MHz CPU upgrade kit, several more loose CPUs I also have, and a RAID card just for fun.&lt;p&gt;I dimly recalled the machine may not have been working right when I committed it to storage, but we'll proceed as if it had been, starting with a visual inspection of the electronics.&lt;/p&gt;The keylock on the logic board drawer (shown here with the rear panel off so you can see how it operates) has just two positions. In the horizontal locked position, the board is connected to power and a metal tab prevents the drawer from coming out. In the vertical unlocked position, the board is disconnected and the tab is moved away from the chassis so the drawer can be pulled free. We turn the rear key, grab the handles and pull the board drawer out. This is the logic board (the spare in the bag). It has a broadly similar layout to other six-slot Power Macs and has many of the same chips, including a Grand Central (labeled I/O CNTRL, near the Cirrus Logic video ASIC), CURIO (labeled SCSI/ENET) and two Bandits (labeled as PCI BRIDGEs). However, it only has eight RAM DIMM slots instead of the 9500's twelve, and most of the system connections are consolidated into a single card edge at the top and a large power connector at the bottom. There are separate slots for the ROM DIMM, the CPU daughtercard and the L2 cache. Headers handle both internal SCSI busses, the mainboard fan and the rear keylock. A small red CUDA reset button is at the top left. Installed, the board sits in front of the mainboard fan which is primarily used to cool the CPU daughtercard. This daughtercard rides in plastic rails that serve as alignment guides and structural support. Tabs and a couple mounting screws hold the logic board in place in the drawer. The tabs, card rails and much of the drawer itself are unfortunately made from Amelioplastic, but this drawer is thick and not normally exposed to the exterior, and it mercifully remains in good physical condition. Note that when the drawer is open, the board is completely ungrounded, so only handle it with antistatic precautions.&lt;p&gt;I never store machines with their PRAM batteries installed (especially since my Shiner ESB prototype had been ruined by the previous owner doing so, during which time it leaked and corroded the logic board), but in this particular case since we will be messing with the system it is easier to reset the logic board if we never install the battery at all. With the machine unplugged, the battery out and the rear key unlocked (horizontal), the board will be completely depowered and will reset in about three minutes or so.&lt;/p&gt;The CPU card is much larger than the ones used in most other PCI Power Macs and was intended to accommodate a dual-processor SMP option which was never sold, though again some prototypes have escaped (I would love to get one). Unfortunately this means that Power Mac CPU cards can't upgrade an ANS and the highest-speed option is the 200MHz 604e card shown here, but any ANS CPU card will work in any ANS, so stockholm also has a 200MHz card. Bus speed and CPU speed are related: the 132MHz (base 500) and 176MHz 604 cards run the bus at 44MHz, but the 150MHz 604 (base 700) and 200MHz 604e cards run the bus at 50MHz.&lt;p&gt;At the top is the 700's standard 1MB L2 cache (the 500 came with 512K). These are allegedly regular Power Mac caches, and a Network Server 1MB cache should work in other Power Macs, but the 500 kernel-panicked with a Sonnet L2 cache upgrade and I eventually had to chase down a 1MB card pulled from another 700.&lt;/p&gt;Behind that is the ROM stick and the centrepiece of this article. They are not always labeled — one of my spares isn't — but when they are, the standard production ROM is part 341-0833. It is a regular 4MB ROM like other Old World Macs. We're going to test this machine with that before we go installing the others. To get a test report will require a minimum amount of RAM. The ANS uses the same 168-pin DIMMs as other Power Macs and can accept up to 512MB (anything greater is not supported by the memory controller), but uniquely needs 60ns parity RAM for highest performance. If any DIMM is not parity, then the system ROM disables parity for all DIMMs and sets the timing to 70ns, even if the RAM is faster. This is a non-trivial hit, especially at the fastest 50MHz bus speed, so you really want parity if you can get it. Here I'm using parity FPM, which was sold standard in the units (all units came with at least 32MB in two 16MB DIMMs) and in upgrade kits (16MB in two 8MB DIMMs), all manufactured by IBM as OEM under contract and sold at typically exorbitant Apple prices. Later on 64MB and 128MB parity DIMMs became available and stockholm has a full 512MB from eight 64MB parity sticks. RAM need not be installed in pairs, though this is preferred as the ANS supports interleaving. While EDO RAM should "just work" (treated as FPM), I've never tried parity EDO in an ANS. We'll put in two IBM 16MB parity FPM DIMMs to equal the base 32MB. With the drawer closed and the rear key locked, we plug in the server (no drives attached yet), turn the front key to service, and then press the front power button to get ... a mostly blank front LCD instead of startup messages.&lt;p&gt;Having worked with these beasts for decades, this appearance — a backlit LCD with a mostly blank or dark block display — almost certainly indicates a problem with the processor card, because enough of the logic board is working to power on the front panel but the CPU isn't running. Typically this is because the processor wormed itself out of the board and needs to be reseated, but you can also get something like this if the card went bad, and less commonly if the ROM stick isn't installed correctly.&lt;/p&gt;However (moving the monitor cord out of the way), we have a problem: we can't get the drawer to open wide enough to pull out and reseat the CPU card. We'll have to take the drawer off. As usual, removing the drawer is relatively easy (it's getting it back on that's the trick). Two plastic latches on the underside of the drawer, fortunately still also in good nick, slip into two gaps in the metal slide rails. Supporting the drawer with your other hand so it doesn't fall off, push each latch in and push back the rail to disengage it. The drawer then lifts off and can be put aside, preferably onto an antistatic mat. Here's the inside, where the logic board connects. The powerplane connector is at the bottom. The board at the top is the right half of the mezzanine (codenamed "HENDY"), with the slot for the logic board's card edge and a connector for the front panel. The mezz is a "horseshoe" that straddles both sides, better shown here with the top off. The other side has connectors for the NMI and reset buttons, floppy drive and SCSI busses. Those bus connectors come from the SCSI backplane on the other side, here with that panel off (which can now be removed because the drawer is out). Both the front (and in the 700, the rear) drive connectors hook up here. I'd forgotten I'd disconnected bus 1 when I stored it, so I later reconnected the cable to J11 before closing this back up. If you don't do this, besides drives not working, you may get spurious errors warning that the drive fan failed or is not present (see later on). The problem with the sliding rails turned out to be two-fold, first some stuck broken pieces of plastic which I removed, and second whatever lubricant Apple had used which over the decades had desiccated into gluey, copper-coloured gunk. I cleaned off most of the ick and then used WD-40 white lithium (not regular WD-40) on the rails and worked it back and forth into the bearings. If it's good enough for your garage door opener, it's good enough for your unusual Apple server. After about ten minutes of spraying and sliding, both rails now move smoothly and reach their maximum extents. I was very careful to wipe off any excess so there wouldn't be a mess later. Now to remount the drawer. This is not well-explained in the official Apple service manual, so I'll be more explicit here. On each slide are two small metal hooks. If you don't see the hooks, pull the slides forward until you do. On each slide, one of the hooks goes into a metal notch on the two metal rails mounted on the back of the drawer. On the top slide, the bottom hook engages; on the bottom slide, the top one does. Once you've done that, then while using one hand to support the drawer, pull each slide forward until it engages with each of the black latches (it will click into position). Now we can pull the drawer all the way out, pull out the 200MHz card and try to reseat it using the card guides. You shouldn't need to force it in, though it does need a bit of grunt to ensure both rows of contacts get into the slot.&lt;p&gt;Closing the drawer likewise doesn't require force per se, but the rear keylock will not turn unless you have the board fully engaged with the mezz and powerplane. There are thumbscrews but they don't really make much difference for this. Sometimes you have to slam it in a couple times, making sure the thumbscrews are completely loosened and out so that they don't get in the way. When the logic board is properly engaged and the drawer is fully closed, it should be easy to turn the rear key.&lt;/p&gt;Unfortunately reseating the processor card didn't fix it, so the next step is to try a different one. I'm saving the other 200MHz card as a spare for stockholm, but we have several 150MHz cards, so I selected one of those. And it starts up! We have messages on the LCD showing the 150MHz 604 (with 50MHz bus), 32MB of parity RAM and 1MB of L2 cache were all properly detected. The reported ROM version of 1.1.22 is consistent with production ROMs as shipped. If you connect to Port 2 on the rear at 9600bps during POST, you may see additional messages. Since the front key is in the service position, it goes into a service boot, first trying the CD (looking for a bootloader) and then looking for the file diags on a floppy disk. We have provided the machine with neither, and nothing else is available, so the server drops to an Open Firmware prompt.&lt;p&gt;Open Firmware is the boot environment for all Power Macs starting with the first beige PCI systems. Originating at Sun as OpenBoot, Open Firmware provides an interactive Forth interpreter, which is used for interpreting cross-platform FCode (bytecode) in device ROMs but can also be used for development, and makes available a built-in means of managing and storing settings for installed hardware. In Macs of this generation it was generally invisible to the user except if specifically enabled or requested — remember this for later as well — and the Apple Network Server was the earliest Power Mac (well, derived, anyway) system where Open Firmware was explicitly user-facing. Open Firmware survives today largely in the form of OpenBIOS.&lt;/p&gt;The diags file in question could be theorically any XCOFF binary, but it's specifically looking for this, the Network Server Diagnostic Utility. This came on a floppy disk in the ANS accessory pack. We'll use the NSDU to check the rest of our configuration. We could reboot the server, but we can just start it from the Open Firmware prompt directly with boot fd:diags. You can also see some of the system's current Open Firmware environment variables; we'll have much more to say about those when we finally get to experimenting with the ROMs. Sorry about the screen photographs but the default refresh rate does not agree with my VGA capture box. The NSDU is also a single XCOFF binary. When it starts up it prints a summary of installed hardware and the results from initial POST. It has detected all RAM is parity, detected the CPU speed and internal L1, detected the external L2, detected both power supplies, and correctly shows installed RAM, no PCI cards, and most of the sensors. The only one that's wrong is the Drive Fan reads "Off" but that's because I hadn't remembered to reconnect the disconnected SCSI bus cable to the backplane. We'll now run the complete system test (option 3). The tests scroll up on the screen, here showing the two internal SCSI controllers and the LCD. The video chip also gets exercised for conformance with various test displays. In the end, we have a clean bill of health, both on the screen ... ... and on the LCD. There's one more thing left to do to certify operation: a test boot of AIX from the CD. ANS AIX, codenamed Harpoon, is specific to the Apple Network Server — you can't use a regular AIX CD, and installing Base Operating System packages from such a CD is likely to corrupt your install (don't ask me how I know this). Most systems shipped with this CD in the accessory pack, version 4.1.4.1. 4.1.2 was used on preproduction servers but I've never seen it myself. Apple later issued version 4.1.5, which fixed many bugs and is strongly recommended. Booting from the CD. The LCD is live during an AIX boot, showing the march of AIX bootloader codes. They are the same codes as most IBM servers of this era. Finally, the AIX kernel comes up, asking to define the system console. This proves our hardware (and CD-ROM) both work and that its native AIX can start, which means any weird behaviour after this point is more likely than not due to what we're testing.&lt;p&gt;We're finally ready to begin. Let's enumerate the currently known Network Server ROMs. In these pre-Open Firmware 3 ROMs, the ROM version and the Open Firmware version are the same. For comparison purposes, PCI Power Macs of this era were typically 1.0.5.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Pre-production ROMs. Currently one version is known, 1.1.20.1. These were used to boot Mac OS and AIX (and possibly another operating system I'll mention), but the internal video and SCSI controllers are not supported in Mac OS. This was the version that turned out to be on my flash DIMM.&lt;/item&gt;&lt;item&gt;Production ROMs. Currently one version is known, 1.1.22. These only boot AIX, though systems with these ROMs can also boot NetBSD and certain Linux distributions. I won't talk further about that in this article, but if I were to use a non-AIX operating system on a production ANS, it would almost certainly be NetBSD even though it doesn't currently support internal video or the on-board Ethernet.&lt;/item&gt;&lt;item&gt;Prototype Mac OS ROMs. Currently one version is known, 2.0. These contain ROM drivers for the internal video and SCSI controllers, and are the only known ROMs to fully support all internal devices in Mac OS. This is not currently in my possession — though I'd love to get one! — but at least one person has created replica ROMs from a dump graciously made available by their owner, and then used them to successfully boot their own machine.&lt;/item&gt;&lt;item&gt;Prototype Windows NT ROMs. These ROMs also appear to be required for multiprocessor support. Currently three versions are known, 2.26b6, 2.26b8 (not dumped, referred to on the LinuxPPC-ANS list) and 2.26NT, with relatively small changes between them.&lt;/item&gt;&lt;/list&gt;&lt;quote&gt;disk2:aix Device isn't there! can't OPEN: /bandit/53c825@11/sd@2,0:aixOpenFirmware1.1.22 To continue booting from the default boot device type: BOOT&amp;lt;return&amp;gt; ok 0 &amp;gt; dev / ok 0 &amp;gt; ls 004308E0: /PowerPC,604@0 00430B90: /l2-cache@0,0 004313F0: /chosen@0 00431520: /memory@0 00431668: /openprom@0 00431728: /AAPL,ROM@FFC00000 00431968: /options@0 00431E40: /aliases@0 00432080: /packages@0 00432108: /deblocker@0,0 004329A8: /disk-label@0,0 00432F18: /obp-tftp@0,0 00435B28: /mac-files@0,0 00436410: /mac-parts@0,0 00436D30: /aix-boot@0,0 00437488: /fat-files@0,0 00438DF0: /iso-9660-files@0,0 004398E0: /xcoff-loader@0,0 0043A410: /terminal-emulator@0,0 0043A4A8: /bandit@F2000000 0043B500: /53c825@11 0043DDE0: /sd@0,0 0043EA48: /st@0,0 0043F8A8: /53c825@12 00442188: /sd@0,0 00442DF0: /st@0,0 00444288: /gc@10 004446C0: /53c94@10000 00446460: /sd@0,0 00447248: /st@0,0 004480C8: /mace@11000 00449248: /escc@13000 004493A0: /ch-a@13020 00449AD8: /ch-b@13000 0044A210: /awacs@14000 0044A2F8: /swim3@15000 0044BB88: /via-cuda@16000 0044D088: /adb@0,0 0044D178: /keyboard@0,0 0044D950: /mouse@1,0 0044DA00: /pram@0,0 0044DAB0: /rtc@0,0 0044DFE0: /power-mgt@0,0 0044E1B8: /nvram@1D000 00462BC8: /lcd@1C000 00450780: /pci106b,1@B 00450958: /54m30@F 0044E7D0: /bandit@F4000000 00462350: /pci106b,1@B 0044FF28: /hammerhead@F8000000 ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;We'll do the first and last of these in the remainder of this article. Since the Bible says the first shall be last and the last first, let's begin with the final known ANS ROM, 2.26NT.&lt;/p&gt;&lt;head rend="h3"&gt;2.26NT Windows NT ROMs&lt;/head&gt;Hancock's late 1996 announcement that the Apple Network Server would optionally run Windows NT caught many industry observers by surprise. Although NT 3.x and 4.x were designed to be architecture-independent and ran on processors as diverse as MIPS and DEC Alpha as well as 32-bit x86, the PowerPC build had been limited to low-volume IBM hardware and never officially ran on Power Macs. Still, it was clear to Apple that NT would be very important in the industry and felt supporting it would broaden the appeal of the server line — or at least soften the impact of its sticker price. Importantly, NT support would not have to wait for Apple's then-expected CHRP Power Macs: reworked ROM support could enable the ANS to boot it "now." (In the end, Jobs eventually scuttled the CHRP initiative to starve the Mac clones; the upcoming New World Macs were ultimately an incompatible blend of CHRP and the earlier PReP standard instead.) When Jobs talked Gil Amelio into canning the ANS as well, the ROM initiative naturally went out the window with it. However, while the existing 2.0 Mac OS ROMs are only known on an unmarked development flash stick similar to mine, these final 2.26NT ROMs appear almost production-ready with fully printed labels, suggesting they had reached a very late stage of development. (The "ESB" tag indicates a prototype designation — consistent with Shiner, the ANS' beer-themed codename during development, ESB stands for "Extra Special Bitter.") These ROMs were kindly sent to me by a former Apple employee at the Elk Grove, CA campus. Sadly this person no longer has the 700 they were running in, but attests to the fact NT did run and apparently even ran well, adding, "I’m pretty certain that the NT ROM was the Apple business systems team trying to find a way to keep their product from being canceled completely. Motorola had just shipped their PowerStack NT machines a few months previously and they were garbage compared to the ANS when it came to field service and expandability." (So true!)&lt;p&gt;The NT ROM DIMM simply replaces the production ROM DIMM in the slot. We'll power it up with the front key set to service just in case.&lt;/p&gt;On the LCD, not only is the version displayed, but as mentioned this is also one of the ROMs that checks for a second CPU (if we had one of the prototype dual-CPU cards, that is — contact me, I'm interested if you've got one to get rid of!).&lt;p&gt;Our first order of business is to immediately dump these ROMs for posterity (they are posted on the group thread at Tinker Different). This can be done without a chip reader by having Open Firmware itself dump the contents in hex over one of the serial ports, and then post-processing the resulting output.&lt;/p&gt;We start by switching the console to a serial port using setenv input-device ttya:57600 and setenv output-device ttya:57600 (ttya is port 2 on the back) followed by reset-all to commit the settings. Then, on a connected terminal program at 57600bps capturing the output (I did something like picocom -b57600 /dev/cu.usbserial-10 | tee out), you can either enter&lt;quote&gt;h# ffc00000 h# 00400000 dump&lt;/quote&gt;&lt;p&gt;which dumps the contents with the addresses, or if you don't need those, you can try something faster but a little more complicated like (suggested by @joevt)&lt;/p&gt;&lt;quote&gt;0 ffc00000 do i 3f and 0= if cr then i l@ 8 u.r 4 +loop cr&lt;/quote&gt;&lt;p&gt;which emits 64 bytes per line. The ANS ROM is already visible in the default memory map, so it can be dumped immediately.&lt;/p&gt;This process is not very quick, but when it finishes you would take the transcript and turn the hex strings back into binary (Perl's pack function is perfect for this), which if properly captured would yield a file exactly 4,194,304 bytes long. Something like this should work on the 64-bytes-per-line output:&lt;quote&gt;#!/usr/bin/perl select(STDOUT); $|++; while(&amp;lt;&amp;gt;) { chomp; chomp; next unless (length == 128); print STDOUT pack("H*", $_); }&lt;/quote&gt;&lt;p&gt;which the Perl golfers will probably have turned into a handful of indecipherable bytes in the comments shortly. After the process is complete, setenv input-device kbd, setenv output-device screen and reset-all will move the console back to the ADB keyboard and VGA port.&lt;/p&gt;&lt;p&gt;There are a number of interesting things about this ROM, though most of it (about the first 3MB) is still identical to the 9500's.&lt;/p&gt;The default boot device remains disk2:aix, but there are apparently NT-specific words in this version of Open Firmware like nt-gen-configs, nt-gen-config-vars, init-nt-vars, maybe-create-nt-part, etc. Their Forth code looks like this:&lt;quote&gt;ok 0 &amp;gt; see nt-gen-configs defer nt-gen-configs : (nt-gen-configs maybe-read-nt-part get-first-str begin while/if _cfgval _cfgvallen encode-string _cfgname count set-option get-next-str repeat ; ok 0 &amp;gt; see nt-gen-config-vars defer nt-gen-config-vars : (nt-gen-config-vars maybe-read-nt-part get-first-str begin while/if _cfgname count _configname pack drop ['] string-var gen-config-var get-next-str repeat ; ok 0 &amp;gt; see maybe-read-nt-part : maybe-read-nt-part init-nt-vars osnv-good? if read-part else nvram-buffer nv-buffer-size erase then ; ok 0 &amp;gt; see init-nt-vars : init-nt-vars nvram-buffer 0= if /osnv dup to nv-buffer-size alloc-mem to nvram-buffer nvram-buffer nv-buffer-size erase nvram-size alloc-mem to _cfgval nvram-size to _cfgval-size _cfgval _cfgval-size erase then ;&lt;/quote&gt;&lt;p&gt;From this you can get the general notion that these allocate a block of NVRAM for NT-specific configuration variables. There are also words for direct mouse support.&lt;/p&gt;&lt;p&gt;If we list out packages, we see other interesting things.&lt;/p&gt;&lt;quote&gt;ok 0 &amp;gt; dev / ok 0 &amp;gt; ls FF8362C0: /PowerPC,604@0 FF836570: /l2-cache@0,0 FF836DA8: /chosen@0 FF836ED8: /memory@0 FF837020: /openprom@0 FF8370E0: /AAPL,ROM@FFC00000 FF8373A0: /options@0 FF837878: /aliases@0 FF837AF0: /packages@0 FF837B78: /deblocker@0,0 FF8383E0: /disk-label@0,0 FF838988: /obp-tftp@0,0 FF83BFA0: /mac-files@0,0 FF83C7A0: /mac-parts@0,0 FF83D078: /aix-boot@0,0 FF83D808: /fat-files@0,0 FF83F608: /iso-9660-files@0,0 FF840390: /xcoff-loader@0,0 FF840DB8: /pe-loader@0,0 FF8416A0: /terminal-emulator@0,0 FF841738: /bandit@F2000000 [...]&lt;/quote&gt;&lt;p&gt;Yes, there is a pe-loader package — as in Portable Executable, the format first introduced in Windows NT 3.1 to replace the old 16-bit New Executable .exe, and today the standard executable format for all modern versions of Windows. Here are some pieces of that:&lt;/p&gt;&lt;quote&gt;ok 0 &amp;gt; see boot : boot "boot " boot|load init-program go ; ok 0 &amp;gt; see boot|load : boot|load _reboot-command pack drop set-diag-mode ['] (init-program) to ^-7DA998 carret word count (load) ; ok 0 &amp;gt; see init-program defer init-program : (init-program) 0 to ^-7DB118 loadaddr "\ " comp 0= if "evaluating Forth source" type loadaddr loadsize evaluate loadaddr loadmapsize do-unmap true to ^-7DB118 else loadaddr 2c@-be F108 = if "evaluating FCode" type loadaddr 1 byte-load loadaddr loadmapsize do-unmap true to ^-7DB118 else loadaddr 2c@-be 1DF = if "loading XCOFF" type 0 0 "xcoff-loader" $open-package "init-program" 2 pick $call-method close-package else loadaddr 2c@-be F001 = if "Loading PE/COFF" type cr 0 0 "pe-loader" $open-package "init-program" 2 pick $call-method close-package else "unrecognized Client Program format" type then then then then ; ok 0 &amp;gt; dev /packages/pe-loader ok 0 &amp;gt; words init-program close open map-space header-size new-load-adr stack-size scthdr.size &amp;gt;pes.rawptr &amp;gt;pes.size_raw &amp;gt;pes.rva &amp;gt;pes.virt_size &amp;gt;pes.name opthdr.size &amp;gt;peo.no_dir &amp;gt;peo.loader_flags &amp;gt;peo.heap_com_size &amp;gt;peo.heap_res_size &amp;gt;peo.stack_com_size &amp;gt;peo.stack_res_size &amp;gt;peo.head_size &amp;gt;peo.image_size &amp;gt;peo.file_algn &amp;gt;peo.scns_algn &amp;gt;peo.image_base &amp;gt;peo.sndata &amp;gt;peo.sntext &amp;gt;peo.entry &amp;gt;peo.bsize &amp;gt;peo.dsize &amp;gt;peo.tsize &amp;gt;peo.magic filehdr.size &amp;gt;pe.nscns &amp;gt;pe.machine ok 0 &amp;gt; see init-program : init-program real? little? 0= or real_base 700000 u&amp;lt; or "load-base" eval 700000 u&amp;lt; or if "false" "real-mode?" $setenv "true" "little-endian?" $setenv @startvec &amp;gt;ramsize @ h#100000 - dup (u.) "real-base" $setenv h#100000 - (u.) "load-base" $setenv cr "RESETing to change Configuration!" type cr force-reboot then loadaddr filehdr.size + &amp;gt;peo.image_base @ dup to new-load-adr "image_base " type u. cr loadaddr filehdr.size + &amp;gt;peo.head_size @ to header-size new-load-adr stack-size - loadsize h#fff + h#-1000 and stack-size + map-space new-load-adr stack-size - stack-size 0 fill loadaddr header-size + new-load-adr loadsize header-size - move new-load-adr loadsize header-size - bounds do i ^dcbf i ^icbi 14 +loop loadaddr loadsize do-unmap 0 4000 map-space install-interrupt-vectors ci-regs h#100 h#deadbeef filll new-load-adr stack-size - FF00 + spsv reg! new-load-adr sasv reg! new-load-adr srr0sv reg! ['] cientry argsv reg! 0 crsv reg! msr@ 17FFF and srr1sv reg! state-valid on ?state-valid ; ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;Your eyes deceive you not: when configured to boot NT, this ROM runs the machine little-endian — which at the time would have been a first for a Power Mac as well, though this is the only way that Windows NT on PowerPC ever ran. 32-bit PowerPC has little-endian support through a little-endian bit in the machine state register or by setting a flag on memory pages in the MMU (which is how Virtual PC ran) or at the instruction level with byteswapping, but to this point all official Power Mac payloads had run big.&lt;/p&gt;That means this ROM may be able to run PowerPC Portable Executables directly, so I got out my OEM Windows NT 4.0 kit to see. I ran those words just in case they made a difference and then tried to do a naïve boot directly from the Windows NT 4 CD. This looks something like boot disk0:,\ppc\setupldr (don't forget the colon and the comma). And, well, it can indeed load it and has a sensible image base address — but immediately crashes with a CLAIM failed, suggesting it couldn't map memory for the executable image, even though 32MB of RAM should have been more than enough to start Windows NT Setup. You can see from init_program above that it provides computed values for Open Firmware load-base and real-base, so I imagine they were tailored specifically to boot NT (and NT Setup), but nevertheless I couldn't get past this point.&lt;p&gt;[In the comments, Andrei Warkentin asked if it could boot the veneer from the CD. It parses ...&lt;/p&gt;... but it does not run either.]&lt;p&gt;To be sure, we almost certainly don't have all the pieces together for a successful NT boot yet. One thing I could find no trace of in the ROM was ARC. We talked about the rise and fall of ARC in our SGI Indigo2 refurb weekend, but even though IBM, Sun, HP, Intel and Apple were never members of the Advanced Computing Environment consortium, Microsoft was. As a consequence virtually any machine capable of booting Windows NT would have some means of system specification through ARC (this particular historical vestige persisted until Windows Vista). On DEC Alphas, this was implemented in firmware, which is why you need the right firmware to boot it; for the IBM Power Series workstations and laptops, the ARC console was on floppy disk. It is highly likely the ANS also had an ARC console of its own, and since it doesn't appear to be in the ROM, there must have been a floppy or CD that provided it which we don't have.&lt;/p&gt;&lt;p&gt;Additionally, Windows NT relies on a hardware abstraction layer (HAL) which operates between the physical hardware and the rest of the operating system. The HAL is even more lower-level than device drivers, implementing functions like allowing device drivers to access ports in a more standardized fashion, abstracting away interrupt management, and unifying firmware interfaces and DMA. There are HAL DLLs on the 4.0 CD for various IBM (Types 6015, 6020, 6030, and 6070), FirePower (Powerized MX and ES) and Motorola (PowerStack 2 and Big Bend) PowerPC systems, but none for any Power Mac. The HAL necessarily gets loaded early in the setup process, often from another floppy, and you won't be able to successfully bring up Windows NT without it. Although there are apocryphal references to "halbandit" out there and this name is likely a reference to the ANS HAL, we don't have it either. (While it should be possible to get the Windows NT for Power Mac port running on the ANS, per the maintainer its current HAL relies on Mac OS support, so it wouldn't actually be using this ROM.)&lt;/p&gt;&lt;p&gt;Do you have any of these pieces? Post in the comments, or if you'd prefer to be anonymous, drop me an E-mail at ckaiser at floodgap dawt com.&lt;/p&gt;&lt;p&gt;Even without Jobs' looming axe, NT on the ANS was probably ill-starred anyway no matter how well it ran. The unique persistence of Windows NT on the DEC Alpha was a side-effect of primary architect Dave Cutler strongly basing NT on DEC VMS, an aspect hardly lost on DEC's legal team, to the point where various filenames and directory structures in the NT codebase even directly matched those in VMS. To avoid a lawsuit Microsoft paid off DEC, helped promote VMS, and committed to continued support for NT on Alpha, which remained until the beta phase of Windows 2000. This situation was absolutely not the case with PowerPC: IBM was so irked with Microsoft over OS/2 and NT's adoption of an expanded Windows API instead that its support for RISC NT was never more than half-hearted. Likewise, the only MIPS hardware that ran NT were DECstations — quickly cancelled by DEC in favour of Alpha — and directly from MIPS, the Magnum R4000 — also cancelled to avoid competition with Silicon Graphics' IRIX hardware when SGI bought them out. At that point, and already not favourably predisposed to Microsoft's initiative, IBM didn't see any value in continuing to support Windows NT on PowerPC and Amelio's Apple definitely didn't have the resources to do so themselves.&lt;/p&gt;&lt;head rend="h3"&gt;1.1.20.1 preproduction ROMs&lt;/head&gt;&lt;p&gt;Let's rewind a bit here and talk about booting Mac OS on the ANS, given that's how all this got started in the first place. The stock 1.1.22 ROM blocks booting it at the Open Firmware level:&lt;/p&gt;&lt;quote&gt;ok 0 &amp;gt; dev /AAPL,ROM ok 0 &amp;gt; words load open ok 0 &amp;gt; see open : open "MacOS is not supported. " type false ; ok 0 &amp;gt; see load : load real_base 400000 &amp;lt;&amp;gt; virt_base -800000 &amp;lt;&amp;gt; or real? or little? or if 10 base ! "FFFFFFFF" "real-base" $setenv "FFFFFFFF" "virt-base" $setenv "false" "real-mode?" $setenv "false" "little-endian?" $setenv "boot /AAPL,ROM" !set-restart cr "RESETing to change Configuration!" type cr reset-all then ; ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;If you try anyway with boot /AAPL,ROM, it won't work.&lt;/p&gt;You can force it by patching out those Forth words, but even though it will try to start, it will immediately crash and return you to the Open Firmware prompt.&lt;p&gt;Still, repeated reports back in the day swore they could do it. A couple people tried using 9500 ROMs, noting they would get a picture on an IMS Twin Turbo video card, though there was disagreement on whether it could actually boot anything and the different Bandit mapping almost certainly assured this wouldn't get off the ground. A few other people had intermittently acquired remaindered ANS systems from Apple that did indeed boot MacOS (retrospectively they very likely had 2.0 ROMs in them). More interesting, however, were reports that the Network Servers had previously booted Mac OS during development.&lt;/p&gt;One of these early ROMs ended up sitting in a box in my closet for about 20 years. Apple Austin (the address on the box is no longer an Apple building) was the last stand of the Network Server, where a number of systems remained serving content as late as 2005. Per an Apple employee on the LinuxPPC-ANS list in March 2003, "Our team here at Apple decommissioned over 40 Shiners early last year. They used to be the backbone of the Apple Support site [that is, the former www.info.apple.com] serving all the software downloads, all the images for the support site and performing much of the heavy lifting behind the scenes that made our website the highest rated support site in the industry." About twenty of them were sold to list members — I was a starving medical student at the time and couldn't afford either the cash or the space — but I did make a deal to pick up some of the spare parts. I got two 10Mbit Ethernet cards and some 68-pin SCSI interconnects, and also some RAM. I didn't look too closely at what was in the box otherwise. I am told the servers that did not sell were crushed. :(&lt;p&gt;It wasn't until I was looking through my box for a spare ROM to see if I could get it converted to 2.0 that I found this ROM stick in the bottom of the box. It was not labeled and if I hadn't seen a picture of the 2.0 ROM, I probably wouldn't have recognized it for what it was.&lt;/p&gt;This was how the 2.0 ROM looked in the Apple employee's Deep Dish that booted OS 9. Apple used flashable DIMMs exactly like this for Power Mac development generally; the form factor will fit in any beige Power Mac. (We don't know how to flash these yet but I know people are working on it.) Still, the fact it came from the Network Server afterlife meant it probably wasn't any ordinary DIMM, so now let's give it a spin. It comes right up ... and it's a pre-production ROM! This is currently the earliest known ROM available for the Network Server. I have no idea how it got in that box; I didn't request a spare ROM DIMM from them, but it was down at the bottom with the network cards and the other pieces that I did order. I immediately dumped this one also to compare. Our Apple employee with the 2.0 ROMs also had a 1.1.20.1 set, and the hashes match his dump, so this is the same.&lt;quote&gt;disk2:aix Device isn't there! can't OPEN: /bandit/53c825@11/sd@2,0:aixOpenFirmware1.1.20 To continue booting the MacOS type: BYE&amp;lt;return&amp;gt; To continue booting from the default boot device type: BOOT&amp;lt;return&amp;gt; ok 0 &amp;gt; dev / ok 0 &amp;gt; ls FF830648: /PowerPC,604@0 FF8308F8: /l2-cache@0,0 FF831158: /chosen@0 FF831288: /memory@0 FF8313D0: /openprom@0 FF831490: /AAPL,ROM@FFC00000 FF8316F0: /options@0 FF831BD0: /aliases@0 FF831E10: /packages@0 FF831E98: /deblocker@0,0 FF832738: /disk-label@0,0 FF832CA8: /obp-tftp@0,0 FF8358B8: /mac-files@0,0 FF8361A0: /mac-parts@0,0 FF836AC0: /aix-boot@0,0 FF837218: /fat-files@0,0 FF838B80: /iso-9660-files@0,0 FF839670: /xcoff-loader@0,0 FF83A1A0: /terminal-emulator@0,0 FF83A238: /bandit@F2000000 FF83B290: /53c825@11 FF83DB70: /sd@0,0 FF83E7D8: /st@0,0 FF83F638: /53c825@12 FF841F18: /sd@0,0 FF842B80: /st@0,0 FF844018: /gc@10 FF844450: /53c94@10000 FF8461F0: /sd@0,0 FF846FD8: /st@0,0 FF847E58: /mace@11000 FF848FD8: /escc@13000 FF849130: /ch-a@13020 FF849868: /ch-b@13000 FF849FA0: /awacs@14000 FF84A088: /swim3@15000 FF84B918: /via-cuda@16000 FF84CE18: /adb@0,0 FF84CF08: /keyboard@0,0 FF84D6E0: /mouse@1,0 FF84D790: /pram@0,0 FF84D840: /rtc@0,0 FF84DD70: /power-mgt@0,0 FF84DF48: /nvram@1D000 FF8628D8: /lcd@1C000 FF850490: /pci106b,1@B FF850668: /54m30@F FF84E560: /bandit@F4000000 FF862060: /pci106b,1@B FF84FCA8: /hammerhead@F8000000 ok&lt;/quote&gt;&lt;p&gt;This ROM specifically advertises it can boot Mac OS, and there is no block in Open Firmware.&lt;/p&gt;&lt;quote&gt;0 &amp;gt; dev /AAPL,ROM ok 0 &amp;gt; words load open ok 0 &amp;gt; see open : open true ; ok 0 &amp;gt; see load : load real_base 400000 &amp;lt;&amp;gt; virt_base -800000 &amp;lt;&amp;gt; or real? or little? or if 10 base ! "FFFFFFFF" "real-base" $setenv "FFFFFFFF" "virt-base" $setenv "false" "real-mode?" $setenv "false" "little-endian?" $setenv "boot /AAPL,ROM" !set-restart cr "RESETing to change Configuration!" type cr reset-all then ?cr "MacOS is currently unsupported, use at your own risk." type &amp;lt;bye&amp;gt; ; ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;However, if you enter bye as directed with a CD in the internal CD-ROM, the screen will go blank and nothing will happen.&lt;/p&gt;&lt;p&gt;The clue comes from those who claimed they got the system partially running with 9500 ROMs: the 9500 has no on-board video and always came from Apple with a video card, so they added a video card. With that, they got a picture on the video card. No Mac OS support for the internal Fast Wide SCSI nor the Cirrus Logic video is implemented in this ROM, and as we mentioned earlier, having never been used in any prior Apple product, the operating system proper doesn't know what they are either. In fact, the Cirrus Logic video is gimped even in AIX — the ANS Hardware Developer Notes say that the video controller provides "only a little-endian window into the packed-pixel frame buffer, hence Big Endian [sic] operating systems are limited to 8 bits per pixel unless low-level transformation routines are written."&lt;/p&gt;&lt;p&gt;For a server that's probably good enough. For a really powerful under-the-desk workstation, that stinks. Let's add a video card.&lt;/p&gt;I chose an IMS Twin Turbo 128MA, nearly the pinnacle of 2D classic Mac performance, and one of the BTO options Apple offered for the 9500. I also put as much high-capacity parity RAM in it as I could get my hands on. The biggest parity FPM DIMMs I have in stock were 64MB. You may need to examine your RAM sticks carefully to make sure you aren't actually putting in non-parity (the stick in the bottom picture is not parity). These two got me 128MB to start. Initially I could only scrape together 192MB of parity RAM from what I had left and the 16MB upgrade kits, so I started with that.&lt;p&gt;For a test boot, I decided to try the external DB-25 BlueSCSI dongle I had left over from when we experimented with Novell NetWare on the Power Macintosh 6100, for two reasons: it already had a bootable image of 7.6 on it I was using for another project, and it also has an image of Cyberpunk, Apple's codename for the very alpha port of NetWare to the Power Mac originally intended for Shiner systems. Recall that this Forth word exists in every known ANS ROM, even the late 2.26 NT ROM, with the notable exception of the 2.0 MacOS ROMs:&lt;/p&gt;&lt;quote&gt;0 &amp;gt; see setenv-netware : setenv-netware "false" "real-mode?" $setenv "ttya:19200" "input-device" $setenv "ttya:19200" "output-device" $setenv ?esb if "scsi-int/sd@2:0" else "scsi-int/sd@3:0" then "boot-device" $setenv ; ok&lt;/quote&gt;&lt;p&gt;I wasn't sure if this version of Cyberpunk, intended for Piltdown Man machines (i.e., the 6100 and allies), would start on it but if any ROM could, I felt sure these beta ROMs had a decent chance. I set the Open Firmware input-device back to the default kbd and the output-device back to the default screen and brought it back up again.&lt;/p&gt;Notice that it will still try to boot AIX as default — you would need to change the boot device to /AAPL,ROM to autoboot Mac OS, and this will be lost if the board NVRAM gets reset.&lt;p&gt;At this point, we plug the monitor into the Twin Turbo card and blindly type bye. Yes, you can set the Open Firmware output-device directly to the video card — something like /bandit@F2000000/IMS,tt128mbA@D would work for slot 1 — but this isn't necessary to boot ...&lt;/p&gt;... because the Toolbox ROM will automatically use the card anyway and we get our long awaited Happy Mac. This is analogous to the situation on a real 9500 where Open Firmware 1.0.5 isn't on the console; by default it's on the serial ports. Another big heaping bowl of foreshadowing for you to keep in mind. I left the Cyberpunk image on SCSI ID 0 to see what it would do, though I was pretty sure it would fail, and it did. This image has System 7.1.2 on it and no PCI Power Mac officially supported anything earlier than 7.5.2. But, rearranging the IDs so that the 7.6 image was on ID 0 and the Cyberpunk image was in ID 1, 7.6 will boot! Let's switch to proper screenshots. Unsurprisingly, 7.6's relatively underpowered System Profiler identifies the system as Gestalt ID 67, which matches the 9500, 9515, 9600 and the WGS 9650, but gives us little more detail than that. For a deeper dive we'll fire up TattleTech which was already on this disk image. TattleTech reports the same Gestalt ID. Cursorily scanning the Gestalt ID list, they all look pretty similar to a Mac of that generation booting 7.6. There is little hint here that this computer is anything other than a 9500. On the other hand, the PCI slot layout is a little different. Like the 9500 and 9600, the ANS has two Bandits (there is even space in the memory map for a third, which remains unimplemented) and thus two PCI busses, but the 9500/9600 assign three slots each to each Bandit (Grand Central handling non-PCI devices is on the first). In the ANS, the first Bandit also carries the internal SCSI and internal video as well as Grand Central, so it only handles two slots, with slot 3 going to the second Bandit. This rearrangement manifests here in TattleTech showing just two slots on the first bus. The ROM checksum also doesn't match. 9500 ROMs contain an Apple checksum of either $96CD923D or $9630C68B (the 9600 might also have $960E4BE9 or $960FC647), but this ROM checksums as $962F6C13. The same checksum appears in the 1.1.22 production ROM, which still contains a substantial portion of the 9500 v2 ROM even though it definitely won't boot Mac OS. This likely represents held-over code that simply no one bothered to remove. We can also see that the two internal SCSI busses are detected, even if they aren't bootable with this ROM, and they are properly probed as a 53C825. The 53C94 used for the external SCSI which we are running from likewise appears. Finally, the built-in AAUI Ethernet is detected as well (MACE, via Grand Central). I point this out specifically because ... ... it doesn't seem to work. While both AAUI dongles I tried showed working LEDs and activity on the network, 7.6 refused to enable the port. This did work in AIX at one point when I used it to sub for stockholm while investigating a hardware fault, but now it won't netboot either from Open Firmware. I'm concluding the MACE embedded in CURIO works but the PHY it connects to must have crapped out in storage. Since we have the Cyberpunk image up, I tried running the PDMLoader just to see. Recall from our NetWare on Power Macintosh article that the PDMLoader is, at least on NuBus Power Macs, what starts the NWstart kernel and enters NetWare. Among other things it provides a fake Open Firmware environment to allow those Macs to resemble a Shiner ESB unit for demonstration purposes, which was the intended target hardware. Early Shiners reportedly could boot it directly. Unsurprisingly, the PDMLoader checks that it was started on a supported Mac and (based on the Gestalt ID) finds our franken-ANS wanting.&lt;p&gt;If we look back at our definition for setenv-netware, however, we can see NetWare was expected to run from a so-called "partition zero" loader. This is like it sounds: a runnable binary occupies partition zero of a bootable disk, usually XCOFF, and is loaded as blocks into memory by Open Firmware and executed. Unfortunately, the Installer we used for Cyberpunk didn't support creating this, and it wouldn't have been necessary for a NuBus Power Mac anyway which doesn't boot like that. As it's a regular XCOFF binary otherwise, I tried putting NWstart onto a plain physical ISO 9660 CD and fed that to 1.1.20.1, but ...&lt;/p&gt;&lt;quote&gt;disk2:aix Device isn't there! can't OPEN: /bandit/53c825@11/sd@2,0:aixOpenFirmware1.1.20 To continue booting the MacOS type: BYE&amp;lt;return&amp;gt; To continue booting from the default boot device type: BOOT&amp;lt;return&amp;gt; ok 0 &amp;gt; boot disk0:,\NWSTART. disk0:,\NWSTART. loading XCOFF tsize=2A14A1 dsize=90028 bsize=10E17C entry=843EC SECTIONS: .pad 00000000 00000000 00000E14 000001EC .text 00000000 00000000 002A14A1 00001000 .pad 00000000 00000000 00000B5F 002A24A1 .data 00000000 00000000 00090028 002A3000 .bss 00090028 00090028 0010E17C 00000000 .pad 00000000 00000000 00000FD8 00333028 .loader 00000000 00000000 0003BC04 00334000 loading .text, done.. loading .dataCLAIM failed ok&lt;/quote&gt;&lt;p&gt;... while the ROM can read the file from disc and it will load, it halts with the same memory claim error I got trying it on the 500 with production ROMs, even after fiddling with the load and real base values to accommodate a large kernel. It's possible this kernel won't run outside of the PDMLoader environment and the Shiners used a different one, but that's not on the CD I have. Oh well.&lt;/p&gt;Since the on-board Ethernet was shot, I decided to see if I could get it working with one of the Ethernet cards I ordered from Apple Austin way back when. This is a 10Mbit "Apple Ethernet PCI" card but not the same as the more typical one found in regular Power Macs — this particular card (820-0765-A, 630-1798, MM4709Z/A) is specific to the Apple Network Server. It has 10baseT, 10base2 and AAUI ports and is based on the DEC 21041 "Tulip" NIC, and is also distinct from the ANS 10/100 card (M3906Z/A). I installed the card in slot 6 so it would be on the other Bandit. Rummaging through the box with the Ethernet cards in it, I also found some more 16MB sticks and bumped the parity RAM to 224MB at the same time.&lt;p&gt;Unfortunately Mac OS 7.6 doesn't see the card; it isn't even offered as a choice. This seemed like a good time to try installing Mac OS 9, first because it might have updated drivers, and second because I wanted to see if 9.1 would work in any event. I ended up copying the 7.6 screenshots to the main server with LocalTalk PhoneNET and a really long telephone cable, which my wife graciously chose to ignore temporarily as well.&lt;/p&gt;Incidentally, a shout-out to my trusty Power Macintosh 7300 that batch-converts these and other PICT screenshots to PNG using Graphic Converter. To start clean, I powered off the box, pulled the plug and turned the rear key for a full reset. While I waited for that to finish, I set up a new microSD card with an empty hard disk image and copied in an ISO of Mac OS 9.1. With power restored and the BlueSCSI reconnected, the CD image booted up — a gratifying sign that Mac OS 9 was going to work just fine — and I formatted the virtual hard disk in Drive Setup. Even though it was over the slow 5MB/sec external SCSI, the installation went surprisingly quickly, likely because the emulated "CD" it was installing from was so fast. When it finished, I restarted the ANS ... and got a black screen on both the video card and the onboard VGA, even though I could see activity on the BlueSCSI and heard alert sounds. The ANS also properly responded to me pressing RESET and RETURN to cleanly shut it down, just like a Mac should. I reset the board again and it rebooted normally with bye from the blind console. We're going to come back to this really soon, because now I was starting to doubt the logic board despite all our testing earlier. 9.1 System Profiler again identifies it with Gestalt ID 67. Everything shows up here that we expect to, including the CPU, clock speed, RAM size and L2 cache. We also see our Twin Turbo and Apple Ethernet PCI cards. And, to my profound pleasure, it shows up (as "Ethernet slot SLOT.&amp;gt;4" [sic], even though I put it in slot 6, because it's slot 4 to the second Bandit) and can be selected. We are now able to mount our usual assisting Netatalk server over the Ethernet, which replaces one long cable with another long cable, but it's all in the name of science! I did try the MACE Ethernet one more time here, and 9.1 doesn't throw an error, but it still doesn't work. As a transfer test I pulled Gauge Pro off the server. It transferred completely and quickly, so I ran it to see what it thought about the hardware, and it didn't seem to find anything unusual.&lt;p&gt;So, about those reboots. At this point I shut down the machine and found the same thing happened when I tried to start it up again: a black screen, rectified by another complete board reset, but the Mac OS still seemed to boot headless and regardless. After the third such attempt, and out of ideas, I decided to foul the boot completely and see what was going on over the serial port. This can be done by letting it boot in regular mode, then for the next boot ensure the floppy drive is empty and turn the key to service, which will forget the boot setting from beforehand and try to start diagnostics. Lo and behold ...&lt;/p&gt;&lt;quote&gt;fd:diags NO DISK can't OPEN: /bandit/gc/swim3:diagsOpenFirmware1.1.20 To continue booting the MacOS type: BYE&amp;lt;return&amp;gt; To continue booting from the default boot device type: BOOT&amp;lt;return&amp;gt; ok 0 &amp;gt; printenv security-#badlogins 1 security-password security-mode none little-endian? false false real-mode? false false auto-boot? true true diag-switch? false false fcode-debug? false false oem-banner? false false oem-logo? false false use-nvramrc? true false f-segment? false true real-base -1 -1 real-size 100000 100000 virt-base -1 -1 virt-size 100000 100000 load-base 4000 4000 pci-probe-list -1 -1 screen-#columns 64 64 screen-#rows 28 28 selftest-#megs 0 0 boot-device /AAPL,ROM disk2:aix boot-file diag-device fd:diags cd fd:diags /AAPL,ROM diag-file input-device ttya kbd output-device ttya screen oem-banner oem-logo z 2C + 8CC '&amp;amp; 8 + BRpatchyn then ;;l-method else $call-parent then ; boot-command boot boot ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;... the serial port was active. Instead of kbd and screen (or the TT video card directly), I could see the input and output devices had been set to ttya. I didn't do that — Mac OS did that. Its fingerprints can be found in the apparently nonsense line of text between oem-logo and boot-command, which is in fact an NVRAMRC expected to run at startup to wallpaper firmware bugs.&lt;/p&gt;&lt;p&gt;Now it made sense what was going on. Mac OS thought this was a real 9500, and patched its Open Firmware variables accordingly. The default settings for the Open Firmware 1.0.5 console point to the serial port, but on a real 9500 where Open Firmware wasn't intended as a user-facing interface, the ROM would simply ignore them and continue the boot with the video card and ADB HIDs. Not so on the ANS, where Open Firmware is meant to be interacted with directly: it actually obeys these settings! While Mac OS still brought ADB up regardless, neither the video card nor the onboard video would be enabled, and so the screen would stay black. (NetBSD/macppc explains a related phenomenon.)&lt;/p&gt;&lt;p&gt;However, even after I reset the input-device and output-device to kbd and screen, I still got no display. But from a cold board reset we wouldn't have an NVRAMRC either, so I also added setenv use-nvramrc? false, and now we reboot successfully! The PRAM settings persisted as well.&lt;/p&gt;&lt;p&gt;This means our logic board is likely not at fault, but I do consider this some sort of bug, especially because I don't want to have to constantly rescue it from a serial port just to restart the operating system. Fortunately there's a tool out there we can repurpose to get around the problem.&lt;/p&gt;Paul Mackerras, now working at IBM down under and well-known to us in the OpenPOWER community, years earlier had written a control panel utility called Boot Variables. This CDEV very simply gives you a graphical Mac OS interface to what's stored in Open Firmware. To get this back up I would have had to fix the Mac OS patches, so you can see that the new (tainted) settings are written on startup, not shutdown. This is good news because if we undo the damage beforehand, we'll shutdown and/or reboot normally.&lt;p&gt;Boot Variables lets you save the current contents or load them from a file. If we save the current contents, we can see the NVRAMRC is rather lengthy (extracting the text from the binary dump Boot Variables generates):&lt;/p&gt;&lt;quote&gt;boot: '&amp;amp; get-token drop ; : &amp;gt;&amp;amp; dup @ 6 &amp;lt;&amp;lt; 6 &amp;gt;&amp;gt;a -4 and + ; : &amp;amp; na+ &amp;gt;&amp;amp; ; 6ED '&amp;amp; execute 0 value mi : mmr " map-range" mi if my-self $call-method else $call-parent then ; 89B '&amp;amp; ' mmr BRpatch : mcm -1 to mi $call-method 0 to mi ; 8CB '&amp;amp; 1E na+ ' mcm BLpatch : maa -1 to mi 1D swap ; 8C9 '&amp;amp; 5 na+ ' maa BLpatch 8C9 '&amp;amp; 134 + ' 1 BLpatch 8CD '&amp;amp; 184 + dup 14 + &amp;gt;&amp;amp; BRpatch 8C6 '&amp;amp; 7C + ' u&amp;lt; BLpatch 0 value yn : y yn 0= if dup @ to yn then ; 8CB '&amp;amp; ' y BRpatch ' y 28 + 8CB '&amp;amp; 8 + BRpatch : z yn ?dup if over ! 0 to yn then ; 8CC '&amp;amp; ' z BRpatch ' z 2C + 8CC '&amp;amp; 8 + BRpatch&lt;/quote&gt;&lt;p&gt;This does a lot of low-level patching, and while it's not exactly clear what part the ANS doesn't like, the script is also rather unnecessary since it boots fine without it.&lt;/p&gt;Boot Variables can also write and restart the machine in one step with your new settings. In fact, if you open a Boot Variables dump with the Option key down, it will load those settings and reboot immediately with them, so we can just reboot that way — not exactly an ideal solution, but it works. Since the source code is available for Boot Variables, I'm tempted to write a Shutdown Items version that will do these steps automagically without prompting. In the meantime you can download it from the NetBSD archives, since it has obvious utility for NetBSD/macppc.&lt;p&gt;Because these steps are a bit of a pain, I suspected (and still do) that the version of Mac OS Apple exhibited during the ANS beta test was likely patched to work around the problem. That's yet to show up, though, if it even exists.&lt;/p&gt;&lt;p&gt;The former Apple employee who got me the 2.26NT ROM also mentioned he'd gotten Rhapsody running on one of their orphaned 700s. This would have had obvious political overtones within Apple at the time, and his boss told him not to tell anybody. Interestingly, the 2.26 ROMs do have strings in them claiming they can boot MacOS:&lt;/p&gt;&lt;quote&gt;% strings rom1122.bin | grep -i macos [...] driver,AAPL,MacOS,PowerPC MacOS is not supported. % strings rom226b6.bin | grep -i macos driver,AAPL,MacOS,PowerPC [...] MacOS is not supported. +MacOS is unsupported, use at your own risk. :MacOS requires PCI video card and external SCSI boot disk. % strings rom226nt.bin | grep -i macos driver,AAPL,MacOS,PowerPC [...] MacOS is not supported. +MacOS is unsupported, use at your own risk. :MacOS requires PCI video card and external SCSI boot disk.&lt;/quote&gt;&lt;p&gt;Despite running the system little when (trying to) boot NT, the 2.26NT ROM is of course perfectly capable of running big, and indeed must in order to boot AIX. Those strings appear to be false flags, though, because like the production 1.1.22 ROMs it too is blocked from booting Mac OS at the Open Firmware level:&lt;/p&gt;&lt;quote&gt;disk2:aix can't OPEN: /bandit/53c825@11/sd@2,0:aixOpenFirmware2.26 To continue booting from the default boot device type: BOOT&amp;lt;return&amp;gt; ok 0 &amp;gt; dev /AAPL,ROM ok 0 &amp;gt; words load open ok 0 &amp;gt; see open : open "MacOS is not supported. " type false ; ok 0 &amp;gt; see load : load real_base 400000 &amp;lt;&amp;gt; virt_base -800000 &amp;lt;&amp;gt; or real? or little? or if 10 base ! "FFFFFFFF" "real-base" $setenv "FFFFFFFF" "virt-base" $setenv "false" "real-mode?" $setenv "false" "little-endian?" $setenv "boot /AAPL,ROM" !set-restart cr "RESETing to change Configuration!" type cr reset-all then ?cr "MacOS is unsupported, use at your own risk." type ?cr "MacOS requires PCI video card and external SCSI boot disk." type &amp;lt;bye&amp;gt; ; ok 0 &amp;gt; boot /AAPL,ROM /AAPL,ROM MacOS is not supported. can't OPEN: /AAPL,ROM ok 0 &amp;gt;&lt;/quote&gt;&lt;p&gt;... and it will also hang if you patch out the words anyway.&lt;/p&gt;&lt;p&gt;No matter whatever hacking I tried, it would not go past this point either. It is noteworthy, however, that it claims it would boot with a PCI video card and external disk — it does not — which is exactly our successful configuration for 1.1.20.1. Given these limitations, it seems most likely that our Apple employee did this on a 2.0 ROM system (i.e., the "real" ANS Mac OS ROM), but let's see if the pre-production ROMs can pull off the same trick. Currently I run Mac OS X Server v1.2 (i.e., Rhapsody 5.5) on a WallStreet PowerBook G3, probably the best laptop for doing so, but all versions have been reported to run on beige PCI Power Macs including the 9500. However, my previous experience with Rhapsody was that it rebooted multiple times during the install, and I was concerned this would be a problem with our rickety restart situation. So ... let's have the Wally install it to the BlueSCSI for the 700, and then see if the 700 will boot it. The Wally is also technically unsupported, but you can get around that in the Installer, and the installation created is universal. The installation process ran a lot more slowly than Mac OS 9's, even with the Mac OS X Server v1.2 CD images on the BlueSCSI. When it completed, I took the finished hard disk and the installer CD disk image back to the 700. The 700 booted the CD just fine — it's just Mac OS 9, after all — but its Startup Disk control panel didn't see the Rhapsody disk. I rebooted from the Mac OS 9.1 hard disk image but with the Rhapsody install also present on SCSI ID 1. While both Drive Setup and SCSIProbe saw it, neither mounted it (not even forcibly with SCSIProbe), and Startup Disk still failed to see it.&lt;/p&gt;&lt;p&gt;Apple made a tool to deal with this and other related startup situations called System Disk. Distinct from the built-in Startup Disk CDEV, this is a utility application that lets you pick your boot volume and as a nice side effect can be used to edit Open Firmware variables too. It comes as a self-mounting disk image.&lt;/p&gt;System Disk is not supported on some systems and we should not be surprised it is not supported on this one either. That said, it alone is able to see the Rhapsody volume and can tell us what we need to know. It has the boot and output devices completely wrong — scsi-int would be the internal SCSI, not the external, and /chaos/control references built-in graphics in models like the Power Mac 7300 and 8600 — and this version of Open Firmware lacks the words O or bootr, but we can see where it expects to load the Mach kernel from (partition 8) using its own "partition zero" bootloader. This information is enough to come up with a command line to try booting it manually, but after all that I couldn't get it to start; it gives the same CLAIM failure message that's doomed our other attempts. Since I wasn't able to get it any further, it doesn't seem like trying real OS X out would go anywhere either. They may simply not work with this ROM.&lt;p&gt;Overall, however, the machine boots OS 9.1 well enough as long as you deal with the reboot-and-shutdown situation. It's a bit overkill to do this entirely over the external SCSI but at least doing it with flash media is far faster than a regular hard disk or CD-ROM, and as far as size goes I suppose it's no worse than using an SGI Crimson to browse your filesystem. If this is all you have to boot Mac OS on the ANS, and you really want to boot Mac OS on the ANS instead of indulging in the jackbooted bliss of AIX, it's perfectly cromulent.&lt;/p&gt;&lt;head rend="h3"&gt;The current situation&lt;/head&gt;&lt;p&gt;The pre-production ROMs work. Still, I'm hoping to get a 2.0 ROM in the near future and working with someone on doing just that. Even so, if you're an Apple employee with one of these ANS ROMs you need to get rid of, let's talk! The 2.0 ROM should solve our remaining issues with Mac OS 9, probably enable us to boot Rhapsody, and possibly even get early versions of Mac OS X working on the Apple Network Server too.&lt;/p&gt;&lt;p&gt;Similarly, if you know anything about "halbandit" or can provide the HAL or ARC console for the ANS' spin of Windows NT, that would be great! And anyone with knowledge of how Cyberpunk/NetWare was supposed to boot on Shiner ...&lt;/p&gt;&lt;p&gt;If you'd prefer not to post in the comments or wish to remain publicly anonymous, you can contact me at ckaiser at floodgap dawt com.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751825</guid><pubDate>Sun, 25 Jan 2026 08:06:53 +0000</pubDate></item><item><title>Introduction to PostgreSQL Indexes</title><link>https://dlt.github.io/blog/posts/introduction-to-postgresql-indexes/</link><description>&lt;doc fingerprint="791c02166dd40ba4"&gt;
  &lt;main&gt;
    &lt;p&gt;20 minutes&lt;/p&gt;
    &lt;head rend="h1"&gt;Introduction to PostgreSQL Indexes&lt;/head&gt;
    &lt;head rend="h2"&gt;Who’s this for&lt;/head&gt;
    &lt;p&gt;This text is for developers that have an intuitive knowledge of what database indexes are, but don’t necessarily know how they work internaly, what are the tradeoffs associated with indexes, what are the types of indexes provided by postgres and how you can use some of its more advanced options to make them more optimized for your use case.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basics&lt;/head&gt;
    &lt;p&gt;Indexes are special database objects primarily designed to increase the speed of data access, by allowing the database to read less data from the disk. They can also be used to enforce constraints like primary keys, unique keys and exclusion. Indexes are important for performance but do not speedup a query unless the query matches the columns and data types in the index. Also, as a very rough rule of thumb, an index will only help if less than 15-20% of the table will be returned in the query, otherwise the query planner, a part of postgres used to determine how the query is going to be executed, might prefer a sequential scan. In fact, reality is much more complex than this rule of thumb. The query planner uses statistics and predefined costs associated with each type of scan to do its job, but we’re only going approach the query planner behavior tangentially in this article. So, if your query returns a large percentage of the table, consider refactoring it, using summary tables or other techniques before throwing an index at the problem. With that in mind, let’s give a closer look at how Postgres stores your data in the disk and how indexes help to speedup querying this data.&lt;/p&gt;
    &lt;p&gt;There are six types of indexes available in the default postgres installation and more types available through extensions. Typically, they work by associating a key value with a data location in one or more rows of the table containing that key. Each line is identified by a TID, or tuple id.&lt;/p&gt;
    &lt;head rend="h3"&gt;How data is stored in disk&lt;/head&gt;
    &lt;p&gt;To understand indexes, it is important to first understand how postgres stores table data on disk. Every table in postgres has one or more corresponding files on disk, depending on its size. This set of files is called a heap and it is divided into 8kb pagesh. All table rows, internally referred to as “tuples”, are saved in these files and do not have a specific order. The index is a tree structure that links the indexes columns to the row locators, also known as ctid, in the heap. We’ll zoom into the index internals later.&lt;/p&gt;
    &lt;p&gt;To see the heap files we can use a few postgres internal tables to see where they’re located in the disk. First, we can enter psql and use &lt;code&gt;show data_directory&lt;/code&gt; to show the directory Postgres uses to store databases physical files.&lt;/p&gt;
    &lt;code&gt; show data_directory;

         data_directory          
---------------------------------
 /opt/homebrew/var/postgresql@16&lt;/code&gt;
    &lt;p&gt;Now we can use the internal &lt;code&gt;pg_class&lt;/code&gt; to find the file where the heap table is stored:&lt;/p&gt;
    &lt;code&gt;create table foo (id int, name text);


select oid, datname
from pg_database
where datname = 'my_database';                                                                                

  oid  |         datname        
-------+-------------------------
 71122 | my_database
(1 row)&lt;/code&gt;
    &lt;code&gt;select relfilenode from pg_class where relname = 'foo';                                                                                                  
 relfilenode
-------------
       71123&lt;/code&gt;
    &lt;p&gt;Finally, we can check the file on disk by running this command in the shell (ls $PGDATA/base/&amp;lt;database_oid&amp;gt;/&amp;lt;table_oid&amp;gt;):&lt;/p&gt;
    &lt;code&gt;ls -lrt /opt/homebrew/var/postgresql@16/base/71122/71123
-rw-------  1 dlt  admin  0 16 Aug 14:20 /opt/homebrew/var/postgresql@16/base/71122/71123&lt;/code&gt;
    &lt;p&gt;The file has size 0 because we haven’t done any INSERTs in this table yet.&lt;/p&gt;
    &lt;p&gt;Let’s add a couple of rows to our table:&lt;/p&gt;
    &lt;code&gt;insert into foo (id, name) values (1, 'Ronaldo');
INSERT 0 1
insert into foo (id, name) values (2, 'Romario');
INSERT 0 1&lt;/code&gt;
    &lt;p&gt;We can add the &lt;code&gt;ctid&lt;/code&gt; field to the query to retrieve the ctid of each line. The ctid is an internal field that has the address of the line in the heap. Think of it as a pointer to the row location in the heap. It consists of a tuple in the format (m, n) where m is the block id and n is the tuple offset. “ctid” stands for “current tuple id”. Here you can note that the row with id one is stored in the page 0, offset 1.&lt;/p&gt;
    &lt;code&gt;select ctid, * from foo;
 ctid  | id |  name   
-------+----+---------
 (0,1) |  1 | Ronaldo
 (0,2) |  2 | Romario
(2 rows)&lt;/code&gt;
    &lt;head rend="h3"&gt;How indexes speedup access to data&lt;/head&gt;
    &lt;p&gt;Let’s add more players to the table so that the total rows is one million:&lt;/p&gt;
    &lt;code&gt;insert into foo (id, name);
select generate_series(3, 1000000), 'Player ' || generate_series(3, 1000000);&lt;/code&gt;
    &lt;p&gt;After adding more rows to the table its corresponding file is 30MB. Internally, it is divided into 8kb pages.&lt;/p&gt;
    &lt;code&gt;ls -lrtah /opt/homebrew/var/postgresql@16/base/71122/71123
-rw-------  1 dlt  admin    30M 16 Aug 16:32 /opt/homebrew/var/postgresql@16/base/71122/71133&lt;/code&gt;
    &lt;p&gt;When we query a table without an index, Postgres reads all tuples in every page and apply a filter. For example, let’s analyze the command below that searches for rows whose &lt;code&gt;name&lt;/code&gt; column value is equal to “Ronaldo” and show how the database performed this search. We use the explain command with the options &lt;code&gt;(analyse, buffers)&lt;/code&gt;. &lt;code&gt;analyse&lt;/code&gt; will actually execute the query instead of just using cost estimates, and the &lt;code&gt;buffers&lt;/code&gt; option shows how much IO work was done.&lt;/p&gt;
    &lt;code&gt; explain (analyze, buffers) select * from foo where name = 'Ronaldo';
                                                     QUERY PLAN
---------------------------------------------------------------------------------------------------------------------
 Gather  (cost=1000.00..12577.43 rows=1 width=18) (actual time=0.307..264.991 rows=1 loops=1)
   Workers Planned: 2
   Workers Launched: 2
   Buffers: shared hit=97 read=6272
   -&amp;gt;  Parallel Seq Scan on foo  (cost=0.00..11577.33 rows=1 width=18) (actual time=169.520..256.639 rows=0 loops=3)
         Filter: (name = 'Ronaldo'::text)
         Rows Removed by Filter: 333333
         Buffers: shared hit=97 read=6272
 Planning Time: 0.143 ms
 Execution Time: 265.021 ms&lt;/code&gt;
    &lt;p&gt;Note the in output the line starting with " -&amp;gt; Parallel Seq scan on foo". This line denotes that the database performed a sequential search and read all the rows in the table. The execution time for this query was 265.021ms. Also note the line that says “Buffers: shared hit=97 read=6272”. This mean that we needed to read 97 pages from memory, and 6272 pages from disk.&lt;/p&gt;
    &lt;p&gt;Now let’s add an index on the name column and see how the same query performs. We’re using the command &lt;code&gt;create index concurrently&lt;/code&gt; because we don’t want to block the table for writes.&lt;/p&gt;
    &lt;code&gt;create index concurrently on foo(name);
CREATE INDEX

explain (analyze, buffers) select * from foo where name = 'Ronaldo';
                                                    QUERY PLAN
-------------------------------------------------------------------------------------------------------------------
 Index Scan using foo_name_idx on foo  (cost=0.42..8.44 rows=1 width=18) (actual time=0.047..0.049 rows=1 loops=1)
   Index Cond: (name = 'Ronaldo'::text)
   Buffers: shared hit=4
 Planning Time: 0.129 ms
 Execution Time: 0.077 ms
(5 rows)&lt;/code&gt;
    &lt;p&gt;Here we see that the index was used and that in this case the execution time was reduced from 264.21 to 0.074 milliseconds, and the database only needed to read 4 pages! The reduction in execution time happens because, now, instead of reading all the rows in the table, the database uses the index. The index is a tree structure mapping the value “Ronaldo” to the ctid(s) of the rows that have this value in the &lt;code&gt;name&lt;/code&gt; column (in our example we only have one such row). The ctid is then used to quickly locate these rows on the heap.&lt;/p&gt;
    &lt;p&gt;If we use &lt;code&gt;\di+&lt;/code&gt; to show the indexes in our database we can see that the index we’ve created occupies &lt;code&gt;30MB&lt;/code&gt;, roughly the same size as the &lt;code&gt;foo&lt;/code&gt; table.&lt;/p&gt;
    &lt;code&gt;\di+

                                         List of relations
 Schema |     Name     | Type  | Owner | Table | Persistence | Access method | Size  | Description
--------+--------------+-------+-------+-------+-------------+---------------+-------+-------------
 public | foo_name_idx | index | dlt   | foo   | permanent   | btree         | 30 MB |
(1 row)&lt;/code&gt;
    &lt;head rend="h2"&gt;Costs associated with indexes&lt;/head&gt;
    &lt;p&gt;It is important to highlight that the extra speed brought by indices is associated with several costs that must be considered when deciding where and how to apply them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disk Space&lt;/head&gt;
    &lt;p&gt;Indexes are stored in a separate area of the heap and take up additional disk space. The more indexes a table has, the greater the amount of disk space required to store them. This incurs in additional storage costs for your database and for backups, increased replication traffic, and increased backup and failover recovery times. Bear in mind that its not uncommon for btree indexes to be larger than the table itself. Learning about partial indexes, and multicolumn indexes, as well as about other more space efficient index types such as BRIN can be helpful.&lt;/p&gt;
    &lt;head rend="h3"&gt;Write operations&lt;/head&gt;
    &lt;p&gt;Also, there is a maintenance cost in writing operations such as UPDATE, INSERT and DELETE, if a field that is part of an index is modified, the corresponding index needs to be updated, which can add significant overhead to the writing process.&lt;/p&gt;
    &lt;head rend="h3"&gt;Query planner&lt;/head&gt;
    &lt;p&gt;The query planner (also known as query optimizer) is the component responsible for determining the best execution strategy for a query. With more indexes available, the query planner has more options to consider, which can increase the time needed to plan the query, especially in systems with many complex queries or where there are many indexes available.&lt;/p&gt;
    &lt;head rend="h3"&gt;Memory usage&lt;/head&gt;
    &lt;p&gt;PostgreSQL maintains a portion of frequently accessed data and index pages in memory in its shared buffers. When an index is used, the relevant index pages are loaded into shared buffers to speed up access. The more indexes you have and the more they are used, the more shared buffer memory is necessary. Since shared buffers are limited and are also used for caching data pages, filling the shared buffers with indexes can lead to less efficient caching of table data. It’s also good to keep in mind that the whole indexed column is copied in every node of the btree, since there’s a limit in node size capacity, the larger the indexed column the deeper the tree will be.&lt;/p&gt;
    &lt;p&gt;Another aspect of memory usage is that PostgreSQL uses work memory when it executes queries that involves sorting or complex index scans (involving multi-column or covering indexes). Larger indexes require more memory for these operations. Also, indexes require memory to store some metadata about their structure, column names and statistics in the system catalog cache. And finally indexes require memory for maintainance operations like vacuuming and reindexing operations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Types of Indexes&lt;/head&gt;
    &lt;head rend="h3"&gt;Btree&lt;/head&gt;
    &lt;p&gt;The B-Tree is a very powerful data structure, present not only in Postgres but in almost every database management system, since it is a very good general purpose index. It was invented by Rudolf Bayer and Edward M.McCreight while working at Boeing. Nobody really knows if the “B” in B-tree stands for Bayer, Boeing, balanced or better, and it doesn’t really matter. What really matters is that it enables us to search elements in the tree in O(log n) time. If you’re not familiar with Big-O notation, all you need to know is that is is really fast - you only need to make 20 comparisons in order to find an element in a set with 1 million items. Moreover, it can maintain O(log n) time complexity for data sets that are larger than the RAM available on a computer. This means that disks can be used to extend RAM, thanks to the btree efficient prevention of disk page accesses to find the desired data. In PostgreSQL the btree is the most common type of index and its the default, it’s also used to support system and TOAST indexes. Even an empty database has hundreds of btree indexes. It is the only index type that can be used for primary and unique key constraints.&lt;/p&gt;
    &lt;p&gt;In contrast with a binary tree, the BTree is a balanced tree and all of its leave nodes have the same distance from the root. The root nodes and inner nodes have pointers to lower levels, and the leaf nodes have the keys and pointers to the heap. Postgres btrees also have pointers to the left and right nodes for easier forward and backward scanning. Nodes can have multiple keys and these keys are sorted so that it’s easy to walk in ordered directions and to perform ORDER BY and JOIN operations. The values are only stored in the leaf nodes, this makes the tree more compact and facilitates a full traversal of the objects in a tree with just a linear pass through all the leaf nodes. This is just a simplified description of PostgreSQL Btree indexes, if you want to get into the low level details, I suggest you to read the README and the paper that inspired them. Below there’s a simplified illustration of a Postgres Btree.&lt;/p&gt;
    &lt;head rend="h4"&gt;Using multiple indexes&lt;/head&gt;
    &lt;p&gt;Postgres can use multiple indexes to handle cases that cannot be handled by single index scans, by forming &lt;code&gt;AND&lt;/code&gt; and &lt;code&gt;OR&lt;/code&gt; conditions across several index scans with the support of bitmaps. The bitmaps are ANDed or ORed together as needed by the query and finally the table rows are visited and returned. Let’s say we have a query like this:&lt;/p&gt;
    &lt;code&gt;select * from users where age = 30 and login_count = 100;&lt;/code&gt;
    &lt;p&gt;If the &lt;code&gt;age&lt;/code&gt; and &lt;code&gt;login_count&lt;/code&gt; columns are indexed, postgres scans index &lt;code&gt;age&lt;/code&gt; for all pages with &lt;code&gt;age=30&lt;/code&gt; and makes a bitmap where the pages that might contain rows with &lt;code&gt;age=30&lt;/code&gt; are true. In a similar way, it builds a bitmap using the &lt;code&gt;login_count&lt;/code&gt; index. It then ANDs the two bitmaps to form a third bitmap, and performs a table scan, only reading the pages that might contain candidate values, and only adding the rows where &lt;code&gt;age=30 and login_count=100&lt;/code&gt; to the result set.&lt;/p&gt;
    &lt;head rend="h4"&gt;Multi-column indexes&lt;/head&gt;
    &lt;p&gt;Multi-column indexes are an alternative for using multiple indexes. They’re generaly going to be smaller and faster than using multiple indexes, but they’ll also be less flexible. That’s because the order of the columns matter, because the database can search for a subset of the indexed columns, as long as they are the leftmost columns. For example, if you have an index on column &lt;code&gt;a&lt;/code&gt; and another index on column &lt;code&gt;b&lt;/code&gt;, these indexes will serve all the of queries below:&lt;/p&gt;
    &lt;code&gt;select * from my_table where a = 42 and b = 420;

select * from my_table where a = 43;

select * from my_table where b = 99;&lt;/code&gt;
    &lt;p&gt;On the other hand, only the first two queries would use an index if you created a multi-column index on (a, b) with a command like &lt;code&gt;create index on my_table(a, b)&lt;/code&gt;; So, when building multi-column indexes choose the order of the columns well so that your index can be used by the most queries possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Partial indexes&lt;/head&gt;
    &lt;p&gt;Partial indexes allow you to use a conditional expression to control what subset of rows will be indexed, this can bring you many benefits:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;your index can be smaller and more likely fit in RAM.&lt;/item&gt;
      &lt;item&gt;your index is shallower, so lookups are quicker&lt;/item&gt;
      &lt;item&gt;less overhead for index/update/delete (but can also mean more overhead if the column you’re using to filter rows in/out of the index is updated very frequently triggering constant index maintenance)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They’re mostly useful in situations where you don’t care about some rows, or when you’re indexing on a column where the proportion of one value is much greater than others. I’ll give two examples below.&lt;/p&gt;
    &lt;head rend="h5"&gt;When you don’t care about some rows&lt;/head&gt;
    &lt;p&gt;Let’s say you have a rules table where the rows can be marked as enabled/disabled, the vast majority of the rows are disabled and in your queries you only care about enabled rows. In this case, you would have a partial index, filtering out the disable rows like this:&lt;/p&gt;
    &lt;code&gt;create index on rules(status) where status = 'enabled';&lt;/code&gt;
    &lt;head rend="h5"&gt;When the distribution of values is skewed&lt;/head&gt;
    &lt;p&gt;Now imagine you’re building a todo application and the status column value can be either &lt;code&gt;TODO&lt;/code&gt;, &lt;code&gt;DOING&lt;/code&gt;, and &lt;code&gt;DONE&lt;/code&gt;. Suppose you have 1M rows and this is the current distribution of rows in each status:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Rows&lt;/cell&gt;
        &lt;cell role="head"&gt;Status&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TODO&lt;/cell&gt;
        &lt;cell&gt;90%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DOING&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DONE&lt;/cell&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Since postgres keeps statistics about the distribution of values in your table columns and knows that the vast majority of the rows are in the &lt;code&gt;TODO&lt;/code&gt; status, it would choose to do a sequential scan on the &lt;code&gt;tasks&lt;/code&gt; table when you have &lt;code&gt;status='TODO'&lt;/code&gt; in the &lt;code&gt;WHERE&lt;/code&gt; clause of your query, even if you have an index on status, leaving most part of the index unused and wasting space. In this case, a partial scan such as the one below is recommended:&lt;/p&gt;
    &lt;code&gt;create index on tasks(status) where status &amp;lt;&amp;gt; 'TODO';&lt;/code&gt;
    &lt;head rend="h4"&gt;Covering indexes&lt;/head&gt;
    &lt;p&gt;If you have a query that selects only columns in an index, Postgres has all information needed by the query in the index and doesn’t need to fetch pages from the heap to return the result. This optimization is called &lt;code&gt;index-only scan&lt;/code&gt;. To understand how it works, consider the following scenario:&lt;/p&gt;
    &lt;code&gt;create table bar (a int, b int, c int);
create index abc_idx on bar(a, b);

/* query 1 */
select a, b from bar;

/* query 2 */
select a, b, c from bar;&lt;/code&gt;
    &lt;p&gt;In the first query, postgres can do an index-only scan and avoid fetching data from the heap because the values &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; are present in the index. In the second query, since &lt;code&gt;c&lt;/code&gt; isn’t in the index, posgres needs to follow the reference to the heap to fetch its value. In the first query we allowed postgres do to an index-only scan with the help of a multi-column index, but we could also achieve the same result by using a covering index. The syntax for creating a covering index looks like this:&lt;/p&gt;
    &lt;code&gt;create index abc_cov_idx on bar(a, b) including c;&lt;/code&gt;
    &lt;p&gt;This is more space efficient than creating a multi-column index on (a, b, c), because c will only be inserted at the leaf nodes of the btree. Also, we might want to use a covering index in cases where we want an unique index and &lt;code&gt;c&lt;/code&gt; would “break” the uniqueness of the index.&lt;/p&gt;
    &lt;head rend="h4"&gt;Expression indexes&lt;/head&gt;
    &lt;p&gt;Expression indexes to index the result of an expression or function, rather than just the raw column values. This can be extremely useful when you frequently query based on a transformed version of your data. It is necessary if you use a function as part of a where clause as in the example below:&lt;/p&gt;
    &lt;code&gt;CREATE TABLE customers (
    id SERIAL PRIMARY KEY,
    name TEXT
);

CREATE INDEX idx_name ON customers(name);
SELECT * FROM customers WHERE LOWER(name) = 'john doe';&lt;/code&gt;
    &lt;p&gt;In this example above, Postgres won’t use the index because it was was built against the &lt;code&gt;name&lt;/code&gt; column. In order to make it work, the index key has to call the &lt;code&gt;lower&lt;/code&gt; function just like it’s used in the where clase. To fix it, do:&lt;/p&gt;
    &lt;p&gt;Now, when you run a query like this:&lt;/p&gt;
    &lt;code&gt;CREATE INDEX idx_lower_name ON customers (lower(name));&lt;/code&gt;
    &lt;p&gt;Now PostgreSQL can use the expression index to efficiently find the matching rows.&lt;/p&gt;
    &lt;p&gt;Expression indexes can be created using various types of expressions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Built-in functions: Like &lt;code&gt;lower()&lt;/code&gt;,&lt;code&gt;upper()&lt;/code&gt;, etc.&lt;/item&gt;
      &lt;item&gt;User-defined functions: As long as they are immutable.&lt;/item&gt;
      &lt;item&gt;String concatenations: Like &lt;code&gt;first_name || ' ' || last_name&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Hash&lt;/head&gt;
    &lt;p&gt;The hash index differs from B-Tree in strucutre, it is much more alike a hashmap data structure present in most programming languages (e.g. dict in Python, array in php, HashMap in java, etc). Instead of adding the full column value to the index, a 32bit hash code is derived from it and added to the hash. This makes hash indexes much smaller than btrees when indexing longer data such as UUIDs, URLs, etc. Any data type can be indexed with the help of postgres hashing functions. If you type &lt;code&gt;\df hash*&lt;/code&gt; and press TAB in psql, you’ll see that there are more then 50 hash related functions. Although it gracefully handles hash conflicts, it works better for even distribution of hash values and is most suited to unique or mostly unique data. Under the correct conditions it will not only be smaller than btree indexes, but also it will be faster for reads when compared with btress. Here’s what the official docs says about it:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“In a B-tree index, searches must descend through the tree until the leaf page is found. In tables with millions of rows, this descent can increase access time to data. The equivalent of a leaf page in a hash index is referred to as a bucket page. In contrast, a hash index allows accessing the bucket pages directly, thereby potentially reducing index access time in larger tables. This reduction in “logical I/O” becomes even more pronounced on indexes/data larger than shared_buffers/RAM.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;As for its limitations, it only supports equality operations and isn’t going to be helpful if you need to order by the indexed field. It also doesn’t support multi-column indexes and checking for uniqueness. For a in-depth analysis of how hash indexes fare in relation to btree, check Evgeniy Demin’s blog post on the subject.&lt;/p&gt;
    &lt;head rend="h3"&gt;BRIN&lt;/head&gt;
    &lt;p&gt;BRIN stands for Block Range Index and its name tells a lot about how it is implemented. Nodes in BRIN indexes store the minimum and maximum values of a range of values present in the page referred by the index. This makes the index more compact and cache friendly, but restricts the use cases for it. If you have a very large in a work load that is heavy on writes and low on deletes and updates. You can think of a BRIN index as an optimizer for sequential scans of large amounts of data in very large databases, and is a good optimization to try before partitioning a table. For a BRIN index to work well, the index key should be a column that strongly correlates to the location of the row in the heap.Some good use cases for BRIN are append-only tables and tables storing time series data.&lt;/p&gt;
    &lt;p&gt;BRIN won’t work well for tables where the rows are updated constantly, due to the nature of MVCC that duplicates rows and stores them in a different part of the heap. This tuple duplication and moving affect the correlation negatively and reduces the effectiveness of the index. Using extensions such as pg_repack or pg_squeeze isn’t recommended for tables that use BRIN indexes, since they change the internal data layour fo the table and mess up the correlation. Also, this index is lossy in the sense that the index leaf nodes point to pages taht might contain a value within a particular range. For this reason a BRIN is more helpful if you need to return large subset of data, and a btree would be more read performant for queries that only return one or few rows. You can make the index more or less lossy by adjusting the &lt;code&gt;page_per_range&lt;/code&gt; configuration, the trade off will be index size.&lt;/p&gt;
    &lt;head rend="h3"&gt;GIN&lt;/head&gt;
    &lt;p&gt;Generalized inverted index is appropriate for when you want to search for an item in composite data, such as finding a word in a blob of text, an item in an array or an object in a JSON. The GIN is generalized in the sense that it doesn’t need to know how it will acelerate the search for some item. Instead, there’s a set of custom strategies specific for each data type. Please note that in order to index an JSON value it needs to be stored in a JSONB column. Similarly, if you’re indexing text it’s better to store it as (or convert it to) tsvector or use the pg_trgm extension.&lt;/p&gt;
    &lt;head rend="h3"&gt;GiST &amp;amp; SP-GiST&lt;/head&gt;
    &lt;p&gt;The Generalized Search Tree and the Space-Partitioned Generalized Search Tree are tree structures that can be use as a base template to implement indexes for specific data types. You can think of them as framework for building indexes. The GiST is a balanced tree and the SP-GiST allow for the development of non-balanced data structures. They are useful for indexing points and geometric types, inet, ranges and text vectors. You can find an extensive list of the built-in strategies shipped with postgres in the official documentation. If you need an index to enable full-text search in your application, you’ll have to choose between GIN and GiST. Roughly speaking, GIN is faster for lookups but it’s bigger and has greater building and maintainance costs. So the right index type for you will depend on your application requirements.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Understanding and effectively using indexes is crucial for optimizing database performance in PostgreSQL. While indexes can greatly speed up query execution and improve overall efficiency, it’s important to be mindful of their impact on write operations and storage. By carefully selecting the appropriate types of indexes based on your specific use cases you can ensure that your PostgreSQL database remains both fast and efficient. I hope this article taught you at least one or two things you didn’t know about Postgres indexes, and that you’re better equiped to deal with different scenarios involving databases from now on.&lt;/p&gt;
    &lt;p&gt;4119 Words&lt;/p&gt;
    &lt;p&gt;2024-09-11 08:07&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751826</guid><pubDate>Sun, 25 Jan 2026 08:07:03 +0000</pubDate></item><item><title>Deutsche Telekom is throttling the internet</title><link>https://netzbremse.de/en/</link><description>&lt;doc fingerprint="66133f28ed9a317f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Deutsche Telekom is throttling the internet. Let's do something about it!&lt;/head&gt;
    &lt;p&gt;If you are a customer of Deutsche Telekom and some websites just won't load, then we might have the solution to your problem!&lt;/p&gt;
    &lt;head rend="h2"&gt;Short Explanation!&lt;/head&gt;
    &lt;head rend="h2"&gt;What is this about?&lt;/head&gt;
    &lt;p&gt;Epicenter.works, the Society for Civil Rights, the Federation of German Consumer Organizations, and Stanford Professor Barbara van Schewick are filing an official complaint with the Federal Network Agency against Deutsche Telekom’s unfair business practices.&lt;/p&gt;
    &lt;p&gt;Deutsche Telekom is creating artificial bottlenecks at access points to its network. Financially strong services that pay Telekom get through quickly and work perfectly. Services that cannot afford this are slowed down and often load slowly or not at all.&lt;/p&gt;
    &lt;p&gt;This means Telekom decides which services we can use without issues, violating net neutrality. We are filing a complaint with the Federal Network Agency to stop this unfair practice together!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testimonials&lt;/head&gt;
    &lt;head rend="h2"&gt;How can you help the project?&lt;/head&gt;
    &lt;p&gt;Are you a Deutsche Telekom customer and want to help? Get in touch with usâevery experience counts! Maybe you even have networking expertise and measurement data that could be relevant? Whether with or without measurements, weâd love to hear from you at netzbremse@epicenter.works.&lt;/p&gt;
    &lt;p&gt;Do you have experience with interconnection agreements with Deutsche Telekom and want to talk to us confidentially? Contact us via email or one of the following channels:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Signal: +43 670 404 98 89&lt;/item&gt;
      &lt;item&gt;Threema (ID: BXJMX4R5)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Donate now for a free internet!Donate now for a free internet!&lt;/head&gt;
    &lt;head rend="h2"&gt;Talk at Chaos Communication Congress 38C3&lt;/head&gt;
    &lt;head rend="h2"&gt;Media Coverage&lt;/head&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, bewusst EngpÃ¤sse im Internet zu schaffen, um zusÃ¤tzlich Geld fÃ¼r schnellere ZugÃ¤nge zu kassieren. Sie sehen darin eine Verletzung von EU-Recht. Die Telekom widerspricht. Von Markus Reher.&lt;/p&gt;
    &lt;p&gt;Ein BÃ¼ndnis aus mehreren Organisationen hat bei der Bundesnetzagentur eine Beschwerde wegen angeblicher Verletzungen von NetzneutralitÃ¤tspflichten eingereicht.&lt;/p&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, mit schlechtem Netz zusÃ¤tzliches Geld zu machen.&lt;/p&gt;
    &lt;p&gt;Ein BÃ¼ndnis mehrerer Organisationen wirft der Telekom vor, kÃ¼nstliche EngpÃ¤sse im Netz zu schaffen und damit Geld zu verdienen. Der Konzern weist die VorwÃ¼rfe zurÃ¼ck und holt zur Gegenkritik aus.&lt;/p&gt;
    &lt;p&gt;VerbraucherschÃ¼tzer werfen der Telekom vor, mit schlechtem Netz zusÃ¤tzliches Geld zu machen.&lt;/p&gt;
    &lt;p&gt;Die Telekom drosselt das Netz", beklagen die Macher einer neuen Kampagne. Das verstoÃe gegen die NetzneutralitÃ¤t. Eine offizielle Beschwerde soll bald folgen.&lt;/p&gt;
    &lt;p&gt;“Die Telekom drosselt das Netz”, beklagen die Macher einer neuen Kampagne. Das verstoÃe gegen die NetzneutralitÃ¤t. Eine offizielle Beschwerde soll bald folgen.&lt;/p&gt;
    &lt;p&gt;Die DeutÂsche Telekom muss endlich die Peering-KapaÂzitÃ¤ten zu anderen Internet-Knoten erhÃ¶hen.&lt;/p&gt;
    &lt;p&gt;VerbrauÂcherÂschÃ¼tzer wollen bei der BundesÂnetzÂagentur Beschwerde gegen Peering-Probleme im Telekom-Netz einreiÂchen.&lt;/p&gt;
    &lt;p&gt;Nicht nur als KrisenlÃ¶sung sucht das Deutsche Forschungsnetz den direkten Anschluss zur Deutschen Telekom. Die wollte sich aber zuerst auf nichts einlassen.&lt;/p&gt;
    &lt;p&gt;Schon seit mindestens Mai 2015 gibt es sie, eine Option fÃ¼r Hetzner-Kunden, die ihre Server fÃ¼r Kunden der Telekom zwischen 19 und 22 Uhr besser erreichbar machen wollen.&lt;/p&gt;
    &lt;p&gt;Die Telekom sieht sich mit schweren VorwÃ¼rfen konfrontiert, nach denen sie absichtlich gegen die NetzneutralitÃ¤t verstoÃen soll. Ãberzeugt davon ist nicht nur die Verbraucherzentrale.&lt;/p&gt;
    &lt;p&gt;Die Verbraucherzentrale sucht Betroffene, die Probleme im Netz der Telekom haben. Der Vorwurf: Verletzung der NetzneutralitÃ¤t.&lt;/p&gt;
    &lt;p&gt;Verbraucher und Organisationen wehren sich gegen Netzdrosselung von Telekom. Erste Beschwerden gehen ein.&lt;/p&gt;
    &lt;p&gt;Der Verbraucherschutz schlÃ¤gt Alarm: Die Deutsche Telekom bevorzugt bei der Internet-Geschwindigkeit offenbar Dienste und Webseiten, die fÃ¼r mehr Tempo zahlen, wÃ¤hrend sie andere drosselt. User sollen das nun bestÃ¤tigen.&lt;/p&gt;
    &lt;p&gt;Macht die Telekom Teile des Internets absichtlich langsam? Dieser Vorwurf hat es in sich und wird vom Verband der Verbraucherzentralen erhoben â ein Einblick in das Prinzip des Peerings und die Frage, wie die Telekom hier seit Jahren fÃ¼r Frust sorgt.&lt;/p&gt;
    &lt;p&gt;Macht die Telekom ihr Netz absichtlich langsamer? Der Verbraucherschutz wirft dem Unternehmen vor, die NetzneutralitÃ¤t absichtlich zu verletzen. Die Telekom verlange von Anbietern Zahlungen fÃ¼r bevorzugten Datentransfer und bremse andere Dienste aus.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751899</guid><pubDate>Sun, 25 Jan 2026 08:22:17 +0000</pubDate></item><item><title>Sony Data Discman</title><link>https://huguesjohnson.com/random/sony-ebook/</link><description>&lt;doc fingerprint="bb65194d5c9f2bbb"&gt;
  &lt;main&gt;
    &lt;p&gt;Introduction&lt;/p&gt;
    &lt;p&gt;Back in 1992 I worked at an Electronics Boutique that was an outlet location for the company. We sold regular merchandise but also had an outlet section for clearance stuff aggregated from other stores. This thoroughly confused customers who expected every item in the store to be discounted. The job involved a lot of explaining "no I'm sorry this game that literally launched today is not on clearance". Working retail is a great way to lose faith in the collective intelligence of our species.&lt;/p&gt;
    &lt;p&gt;One day we received several Sony Data Discman Electronic Book Player DD-1EX players that we were supposed to clear out. The original sticker price was $500 but they were marked down to roughly 1% of that. Largely out of curiosity I picked one up along with whatever software we had for it (also at a massive discount). I can't say I've used it for more than an hour. It's a very nice device that serves no practical or entertainment function whatsoever.&lt;/p&gt;
    &lt;p&gt;Using old catalogs as a reference, these were originally listed in the spring 1992 catalog. Here it is:&lt;/p&gt;
    &lt;p&gt;These did not appear in the summer 1992 catalog just a couple months later. Since I started working there during the 1992 holiday season the timeline works out. They must have hit the shelves in early 1992, not sold, then been marked down every month until they were rounded-up and shipped to our location.&lt;/p&gt;
    &lt;p&gt;Let's take a peek at it..&lt;/p&gt;
    &lt;p&gt;Gallery&lt;/p&gt;
    &lt;p&gt;Disclaimer: I am terrible at taking pictures.&lt;/p&gt;
    &lt;p&gt;Sony didn't nickel-and-dime consumers on accessories here. The package came with: the reader (duh), AC adapter, rechargeable battery, and another battery pack that holds AAs. Years later they refused to include an AC adapter in the PlayStation Classic.&lt;/p&gt;
    &lt;p&gt;The reader itself is fairly nice looking. It feels like a miniature laptop. It's a tad on the heavy side but also feels extremely durable. Looking at all the buttons and size of the screen makes me think this had a lot of potential beyond just electronic books. However, it lacks any mechanism to save data. In the early 90s it's not like SD-RAM cards were available. Miniature hard drive? Forget it. It has 90% of what it needs to be a PDA but the technology just wasn't there to get the last 10% in.&lt;/p&gt;
    &lt;p&gt;There's a QWERTY keyboard because all of the books are searchable. The directional pad is there to navigate through menus. Looking at it again just makes me irritated that I don't have any games for this (of course I doubt any were made). This would make a cool little text adventure player.&lt;/p&gt;
    &lt;p&gt;The electronic books are mini CDs in a caddy. I guess that means we can rip them (more on this soon).&lt;/p&gt;
    &lt;p&gt;Bad Screenshots&lt;/p&gt;
    &lt;p&gt;I picked up every electronic book we had in stock. The player has an output jack than can be connected to anything with an A/V input (well, just the "V" part is needed). These screenshots are from the A/V out.&lt;/p&gt;
    &lt;p&gt;The splash screen reminding you that this is for private use only. I guess I'm technically violating that, whatever.&lt;/p&gt;
    &lt;p&gt;Although I don't know the exact date this electronic book reader was produced, the bundled encyclopedia gives some hints. It still lists U.S.S.R. as a country so it had to be authored prior to Christmas day 1991.&lt;/p&gt;
    &lt;p&gt;Early 90s software developer salary in the career guide:&lt;/p&gt;
    &lt;p&gt;Thinking of traveling the world? Well, this handy translator is all you need. Someone once told me that if you ever got lost in a strange foreign country you should claim to be a Swedish citizen. Something about Sweden having an embassy in every country and nobody holding a grudge against them. I couldn't find a translation for "I'm a Swedish citizen please don't turn me over to the secret police" in this guide.&lt;/p&gt;
    &lt;p&gt;The least useful book (to me at least) is the crossword dictionary. You can search for word endings or a list of complete words but that's it.&lt;/p&gt;
    &lt;p&gt;The wellness encyclopedia is the perfect gift for a hypochondriac.&lt;/p&gt;
    &lt;p&gt;Since I won't pay more than $3.99 for a bottle of wine I found this guide relatively useless.&lt;/p&gt;
    &lt;p&gt;Ripping the CDs&lt;/p&gt;
    &lt;p&gt;If you rip the CDs you'll find that some of them contain an emulator for the Discman. Here's the wine guide main screen:&lt;/p&gt;
    &lt;p&gt;It seems like the books are fully functional in this emulator:&lt;/p&gt;
    &lt;p&gt;The career guide also comes with an emulator. You can use it to look for jobs that didn't exist in 1990 I guess:&lt;/p&gt;
    &lt;p&gt;Some CDs, like the encyclopedia, don't have the emulator bundled. However, if you copy the data files around it's trivial to launch it in the emulator bundled on the other CDs:&lt;/p&gt;
    &lt;p&gt;iso Downloads&lt;/p&gt;
    &lt;p&gt;Grab these before I receive a takedown notice.. I mean, these were completely obsolete before Wikipedia existed and are even worse after. I doubt that will stop anyone though. One of the reasons I deleted my YouTube videos was takedown notices from Sony over the intro to Dragon's Lair. Some rapper who sold &amp;lt;100 albums, but is apparently signed with Sony, sampled the intro of Dragon's Lair. In Sony's mind that means they own all rights to it. I don't know how you rap over the Dragon's Lair intro and I don't care to learn. EA also sent me a takedown notice over a Dragon's Lair video, a game they neither wrote nor own the rights to. As far as I can tell they at some point were the distributor for an early iOS version of Dragon's Lair that is no longer available. So to make a long story short, I'm sure these will be offline soon. When that happens I'll try moving them to archive.org since they are somehow able to get away with posting anything.&lt;/p&gt;
    &lt;p&gt;Career encyclopedia (SROM30_VERSION1)&lt;/p&gt;
    &lt;p&gt;Crossword dictionary (SROM13V1_07D)&lt;/p&gt;
    &lt;p&gt;Wellness encyclopedia (SROM25_V1_0)&lt;/p&gt;
    &lt;p&gt;World translator (SROM29_VERSION1)&lt;/p&gt;
    &lt;p&gt;Usual disclaimer that you are downloading isos with executable files from a total rando's site. I am 100% not responsible for any awful thing that happens if you download these and run the files on them.&lt;/p&gt;
    &lt;p&gt;Related&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46751906</guid><pubDate>Sun, 25 Jan 2026 08:23:51 +0000</pubDate></item><item><title>A flawed paper in Management Science has been cited more than 6,000 times</title><link>https://statmodeling.stat.columbia.edu/2026/01/22/aking/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752151</guid><pubDate>Sun, 25 Jan 2026 09:04:30 +0000</pubDate></item><item><title>Jurassic Park - Tablet device on Nedry's desk? (2012)</title><link>https://www.therpf.com/forums/threads/jurassic-park-tablet-device-on-nedrys-desk.169883/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752261</guid><pubDate>Sun, 25 Jan 2026 09:22:17 +0000</pubDate></item><item><title>Bridging the Gap Between PLECS and SPICE</title><link>https://erickschulz.dev/posts/plecs-spice/</link><description>&lt;doc fingerprint="351921528b158f82"&gt;
  &lt;main&gt;
    &lt;p&gt;Three years ago, we set out to bring SPICE simulation into PLECS. PLECS Spice is finally here.&lt;/p&gt;
    &lt;p&gt;PLECS Spice brings SPICE device-level simulation directly into PLECS. Available with PLECS 5.0, both system-level and device-level analysis can be performed within a single tool, eliminating the need to maintain duplicate models across separate softwares.&lt;/p&gt;
    &lt;head rend="h2"&gt;Separate Tools, Duplicate Work&lt;/head&gt;
    &lt;p&gt;Power electronics design has long faced a fundamental trade-off: system-level simulation tools deliver the speed and robustness needed for controller development and overall system analysis, but sacrifice the device-level detail necessary to validate component selection before procurement.&lt;/p&gt;
    &lt;p&gt;For over 20 years, Plexim has promoted a top-down design philosophy, enabling engineers to model complete power electronic systems using ideal switches and behavioral components. By avoiding the computational burden of simulating detailed switching transients, PLECS enables rapid validation of system-level requirements like efficiency, control performance and thermal behavior.&lt;/p&gt;
    &lt;p&gt;Conversely, traditional SPICE simulators embody an inherently bottom-up approach. They excel at validating device-level requirements through detailed semiconductor models, capturing switching losses, voltage overshoots and parasitic effects with high fidelity. This comes at a cost: system-level integration becomes computationally prohibitive.&lt;/p&gt;
    &lt;p&gt;This divide has forced engineers into parallel workflows using separate software platforms with different modeling approaches and incompatible component libraries. Moving from a system-level PLECS model to SPICE for device validation requires recreating the model, an error-prone and time-consuming process.&lt;/p&gt;
    &lt;head rend="h2"&gt;PLECS Spice&lt;/head&gt;
    &lt;p&gt;To solve this problem, Plexim has developed PLECS Spice, an extension that brings SPICE device-level simulation capabilities directly into PLECS. PLECS Spice can simulate hybrid systems containing both standard PLECS and SPICE circuits. This allows a schematic to be progressively refined by replacing the ideal switches in a circuit of interest, such as the power stage, with detailed SPICE netlists. Controls and other subsystems can remain unchanged. Because the entire workflow stays within PLECS, engineers can easily toggle between ideal and detailed configurations to compare results. This creates a true top-down workflow where device-level detail is added selectively, only where needed. With PLECS Spice, there is no longer a need to build the same model twice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Under the Hood&lt;/head&gt;
    &lt;p&gt;The PLECS Spice extension adds four key ingredients that transform PLECS into a fully-featured hybrid simulation platform that can simulate standard PLECS and SPICE models together.&lt;/p&gt;
    &lt;head rend="h3"&gt;Netlist Parser&lt;/head&gt;
    &lt;p&gt;SPICE models are typically distributed as netlists. Simply put, these are text files that describe a circuit topology, component interconnections and parameter values. A key capability of PLECS Spice is its parser’s support for multiple netlist dialects. Different SPICE implementations use distinct syntax conventions, making netlists from various vendors incompatible. The PLECS Spice parser handles these variations automatically, enabling engineers to integrate models provided by different semiconductor manufacturers directly into their schematics. Little to no manual conversion or syntax adaptation is needed, regardless of the dialect.&lt;/p&gt;
    &lt;head rend="h3"&gt;Compact Models&lt;/head&gt;
    &lt;p&gt;Netlists provided by manufacturers often rely on well-established semiconductor device models. These compact models combine physics-based modeling with empirical corrections to capture fundamental electrical behavior while maintaining reasonable complexity. PLECS Spice includes optimized implementations of compact models such as diodes, MOSFETs, BJTs, and switches. Each model defines a set of parameters that can be tuned to match the electrical response of specific physical devices. In PLECS Spice, classical compact models have been improved to guarantee continuity of key physical quantities, enhancing numerical stability. By tightly integrating these models into the solver, PLECS Spice achieves both computational efficiency and robust convergence even in the presence of highly nonlinear semiconductor characteristics.&lt;/p&gt;
    &lt;head rend="h3"&gt;Modified Nodal Analysis&lt;/head&gt;
    &lt;p&gt;Standard PLECS uses piecewise state-space equations to simulate electrical models. This approach is computationally efficient for circuits with mostly linear components but struggles with the strong nonlinearities present in detailed semiconductor models. To handle these nonlinearities, SPICE uses Modified Nodal Analysis (MNA), a formulation that produces differential algebraic equations (DAEs).&lt;/p&gt;
    &lt;p&gt;MNA constructs the circuit equations by applying Kirchhoff’s current law at each node and substituting component branch equations. Energy storage elements introduce differential equations, while the network topology and sources introduce algebraic constraints. The result is a coupled system where nodal voltages, source currents, and energy storage currents must satisfy both differential and algebraic equations simultaneously. This integrated treatment of constraints and dynamics is what makes MNA particularly robust for nonlinear semiconductor models.&lt;/p&gt;
    &lt;head rend="h3"&gt;Mixed-Formulation Solver&lt;/head&gt;
    &lt;p&gt;PLECS Spice employs third-order implicit Runge-Kutta methods augmented with circuit-tailored convergence helpers to solve the DAEs produced by MNA. These one-step methods have a crucial advantage for mixed-signal schematics that contain both SPICE and standard PLECS electrical circuits: they are inherently self-starting. In other words, they do not rely on information from previous time steps. When events such as topology changes or zero-crossings occur, the solver must compute the next time step using only the current state. This self-starting property makes one-step methods particularly well-suited for hybrid systems with frequent discontinuities.&lt;/p&gt;
    &lt;p&gt;The solver can simulate complex systems that combine standard PLECS and SPICE models in a single schematic. The only rule is that when an electrical circuit contains a netlist, it must be solved using MNA, and therefore all its components must be compatible with SPICE. But other electrical circuits can remain in the standard PLECS formulation. Circuits of different types connect through the control domain using sources and meters. This enables a powerful top-down workflow: engineers can refine specific circuits of interest by converting them to SPICE netlists while keeping other subsystems and controls unchanged in standard PLECS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Application Example&lt;/head&gt;
    &lt;p&gt;Mixed-signal simulation is particularly valuable when control strategies and device physics must be considered together. The soft switching operation of a Dual Active Bridge (DAB) converter, whose analysis requires taking into consideration both controls and circuit design aspects, serves as a perfect case study for the workflow enabled by PLECS Spice.&lt;/p&gt;
    &lt;p&gt;A DAB is a bidirectional DC-DC topology comprising identical primary and secondary bridges (typically full bridges) separated by a high-frequency transformer and an energy transfer inductance (representing leakage plus external inductance). It is widely employed in high-power, high-density applications requiring bidirectional power flow between two galvanically isolated sides, such as EV chargers and energy storage systems.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Soft Switching Challenge&lt;/head&gt;
    &lt;p&gt;Magnetic components are often the primary limitation to increasing power density. Their size can be reduced by increasing switching frequency. State-of-the-art designs have reached the hundreds of kHz range. However, at these frequencies, switching losses represent a significant part of the overall converter losses. Without careful design, the volume advantage of a smaller transformer could be negated by the increased size needed of the cooling system.&lt;/p&gt;
    &lt;p&gt;To resolve this dilemma, soft switching offers a compelling solution. Given the high switching frequencies, MOSFETs are the standard choice for modern DABs. However, their dominant loss mechanism stems from the charge stored in the parasitic output capacitance (). When the device blocks voltage, this capacitance stores the energy&lt;/p&gt;
    &lt;p&gt;which depends on the drain-source voltage. When a MOSFET is turned on, the stored charge must be evacuated. In hard switching, the closing channel effectively shorts the capacitance, dissipating the stored energy as heat within the semiconductor. At high frequencies, this thermal penalty becomes unsustainable.&lt;/p&gt;
    &lt;p&gt;Here, the DAB offers a distinct advantage. In a full-bridge topology, each leg contains a top and bottom switch that operate complementarily: when one conducts, the other blocks. In practice, a short interval called dead time is introduced between turning off one switch and turning on its complement. Its primary role is to prevent a short circuit across the DC link, but a DAB can also exploit this interval of time for soft switching. During dead time, the inductor current continues to flow. With both switches off, the only path available is through the parasitic capacitances. This discharges the output capacitance of the incoming MOSFET (the switch about to turn on), causing its drain-source voltage to fall. If the dead time is sufficient, reaches zero before the gate signal arrives. The soft switching challenge lies in properly tuning this interval.&lt;/p&gt;
    &lt;p&gt;To achieve such Zero Voltage Switching (ZVS), the relationship between the device output capacitance, the resonant path and the gate drive, must be carefully adjusted. Crucially, these hardware choices cannot be made in isolation. They must inform the control design. This is because robust ZVS depends on several dynamic factors: the converter’s operating point (voltage and power), the gate driving scheme (specifically the dead times) and the degrees of freedom utilized by the modulation strategy.&lt;/p&gt;
    &lt;p&gt;Standard PLECS simulations allow for precise tuning of the operating point within the ZVS region, represented by the blue area in the ZVS range figure. However, because ideal switches are inherently hard switching, they do not simulate the transients required for a detailed analysis of ZVS. The effects of using two different dead times are compared in the figures above. In both tests, the low side gate signal turns off a conducting MOSFET. After the dead time, the complementary MOSFET is turned on by the high side gate signal. In the first experiment, a dead time of 15 ns is used between the two events. In the second, it is set to 50 ns. Yet, the resulting voltage and current waveforms show no visible response to this parameter change.&lt;/p&gt;
    &lt;p&gt;In this case, the ideal model fails to indicate whether the timing achieves soft switching or leads to hard switching transients. The reason is that ideal switches lack parasitic capacitances. Without , the antiparallel diode of the complementary switch conducts immediately, making the simulated waveforms insensitive to the dead time.&lt;/p&gt;
    &lt;head rend="h3"&gt;Validating ZVS with Device-Level Detail&lt;/head&gt;
    &lt;p&gt;PLECS Spice enables precisely the analysis that ideal models cannot provide. Using configurable subsystems, engineers can add a detailed SPICE configuration alongside the ideal model, allowing them to toggle between fast system-level analysis and high-fidelity device validation without modifying the circuit topology or control logic.&lt;/p&gt;
    &lt;p&gt;The ideal switch configuration shown in the first figure consists of a standard PLECS MOSFET with antiparallel diode, driven directly by a control signal. The detailed configuration in the second figure provides a device-level description. It uses manufacturer-provided MOSFET and diode netlists that capture parasitic capacitances and charge dynamics. The control signal is converted to a gate-source voltage through a controlled voltage source. Separate on and off gate resistances ( and ) control switching speed. Engineers can switch between one configuration to the other between two simulations, while the control logic, operating point and all other subsystems remain unchanged.&lt;/p&gt;
    &lt;p&gt;With detailed device models in place, the impact of dead time on ZVS becomes immediately visible. The figure below shows the switching transient with a 15 ns dead time. The drain-source voltage remains high when the gate signal is applied, and only begins falling as channel current rises. This overlap between voltage and current is the signature of hard switching: the channel conducts before the output capacitance fully discharges, dissipating the stored energy as heat in the semiconductor. The insufficient dead time prevents the resonant discharge mechanism from completing.&lt;/p&gt;
    &lt;p&gt;By contrast, the second figure demonstrates successful ZVS with a 50 ns dead time. Here, completes its resonant transition to zero before the gate signal arrives. The channel opens with zero voltage across it, eliminating capacitive turn-on losses. Channel current then begins to flow, carrying the inductor current through the device. This extended dead time provides a sufficient interval for the inductor current to transfer energy from the MOSFET’s output capacitance, fulfilling the conditions for soft switching.&lt;/p&gt;
    &lt;p&gt;This example demonstrates how PLECS Spice enables validation of ZVS by bringing together three tightly coupled aspects within a single model. The control strategy establishes the operating point and determines the available inductor current for resonant transitions. Gate drive timing sets the window for capacitor discharge. The device physics, captured in the SPICE netlist, determines how quickly that discharge occurs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;PLECS Spice marks a significant step towards a unified power electronics design workflow. By integrating SPICE simulation directly into the PLECS environment, engineers no longer need to choose between system-level insights and device-level accuracy. The ability to seamlessly transition between ideal and detailed models within a single schematic eliminates the redundant and error-prone process of rebuilding circuits in separate tools. This empowers engineers to adopt a true top-down design philosophy, starting with a system-level view and progressively adding detail where it matters most. As power electronic systems grow in complexity, this unified approach will be crucial for accelerating innovation and reducing time-to-market.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752841</guid><pubDate>Sun, 25 Jan 2026 10:44:08 +0000</pubDate></item><item><title>150k lines of vibe coded Elixir: The Good, the Bad and the Ugly</title><link>https://getboothiq.com/blog/150k-lines-vibe-coded-elixir-good-bad-ugly</link><description>&lt;doc fingerprint="f72c54daabbd7c64"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;150,000 Lines of Vibe Coded Elixir: The Good, The Bad, and The Ugly&lt;/head&gt;
    &lt;p&gt;TL;DR:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Good: AI is great at Elixir. It gets better as your codebase grows.&lt;/item&gt;
      &lt;item&gt;Bad: It defaults to defensive, imperative code. You need to be strict about what good Elixir looks like.&lt;/item&gt;
      &lt;item&gt;Ugly: It can’t debug concurrent test failures. It doesn’t understand that each test runs in an isolated transaction, or that processes have independent lifecycles. It spirals until you step in.&lt;/item&gt;
      &lt;item&gt;Bottom Line: Even with the drawbacks, the productivity gains are off the charts. I expect it will only get better.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;BoothIQ is a universal badge scanner for trade shows. AI writes 100% of our code. We have 150,000 lines of vibe coded Elixir running in production. Here’s what worked and what didn’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Good&lt;/head&gt;
    &lt;head rend="h3"&gt;Elixir is Small: It Gets It Right the First Time&lt;/head&gt;
    &lt;p&gt;Elixir is a small language. Few operators. Small standard library. Only so many ways to control flow. It hasn’t been around for decades. It hasn’t piled up paradigms like .NET or Java, where functional and OOP fight for space.&lt;/p&gt;
    &lt;p&gt;This matters. AI is bad at decisions. If you want your agent to succeed, have it make fewer decisions. With Elixir, Claude doesn’t need to pick between OOP and functional. It doesn’t need to navigate old syntax next to new patterns. There’s one way to skin the cat. Claude finds it.&lt;/p&gt;
    &lt;p&gt;This matters more if you’re adding AI to an existing codebase. In languages where paradigms came and went—often with whatever developer pushed them—Claude tries to match the existing code. The existing code is inconsistent. So Claude is inconsistent.&lt;/p&gt;
    &lt;head rend="h3"&gt;Elixir is Terse: Longer Sessions, Fewer Compactions&lt;/head&gt;
    &lt;p&gt;Small and terse are related but different. Small means few concepts. Terse means fewer tokens to express the same thing. Go is small but not terse—few concepts, but verbose syntax and explicit error handling everywhere. Elixir is both. We got lucky.&lt;/p&gt;
    &lt;p&gt;Context windows are a real constraint. Elixir uses fewer tokens than most languages. No braces. No semicolons. No verbose boilerplate. I can stay in a working session longer. More iterations. Fewer compactions—those moments when the AI summarizes and forgets earlier context. More context in memory.&lt;/p&gt;
    &lt;p&gt;When I built the React Native version of our app, I hit compactions constantly. JavaScript is small-ish, but it’s not terse. It burns tokens to do what Elixir does with fewer.&lt;/p&gt;
    &lt;p&gt;I also see more compactions when working on heavy HTML and Tailwind in LiveView. Adding, updating, or editing large sections of markup at once. HTML and HEEx templates are token-heavy. But even then, it’s less painful than JavaScript-heavy work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Tidewave: Longer Unassisted Runs&lt;/head&gt;
    &lt;p&gt;Tidewave supercharges Elixir-specific context. It lets the agent read logs from the running app—debug, info, error, warning—so you don’t copy/paste logs around. It can query the dev database, see Ecto schemas, and view package documentation. Fewer hallucinations. Longer unassisted runs. The agent can check and validate its own assumptions without human intervention.&lt;/p&gt;
    &lt;head rend="h3"&gt;Immutability: Fewer Decisions, Less Code&lt;/head&gt;
    &lt;p&gt;If a variable gets mutated by a function call, AI now has three problems instead of one. The actual feature you want implemented. Whether to work around the mutation or update other call sites to stop mutating. And the mutated data itself—what is it, what was it, what will it be, what can it be?&lt;/p&gt;
    &lt;p&gt;AI ponders all of this and contorts itself into an overly defensive mess. It writes nonsense validation checks and if-statements on mutated data. Defensive code that wouldn’t exist in an immutable language.&lt;/p&gt;
    &lt;p&gt;In Elixir, the data is what it is. It’s not going to change. Fewer decisions. Less code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Frontend: Higher Quality, Less Time&lt;/head&gt;
    &lt;p&gt;I prompt high-level changes—“give the top section more padding”—and Claude does it faster than I could. It’s especially good at modifying or moving large chunks of page structure. Mobile-first views? Easy. Way faster than me, and it’s a better designer than me too.&lt;/p&gt;
    &lt;p&gt;The quality floor has gone way up. You can’t hide behind “I’m not a designer” anymore.&lt;/p&gt;
    &lt;head rend="h3"&gt;Git Worktrees: Build Multiple Features in Parallel&lt;/head&gt;
    &lt;p&gt;I use three git worktrees, so I can work on up to three features at any given time. Typically a main feature, a slightly less important one, and the third reserved for quick fixes, low priority stuff, or quick experiments.&lt;/p&gt;
    &lt;p&gt;Three is about the limit. Any more and context switching between features becomes the bottleneck.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bad&lt;/head&gt;
    &lt;head rend="h3"&gt;AI Can’t Organize: Architecture Is Still On You&lt;/head&gt;
    &lt;p&gt;AI is exceptional at churning out lines of code. It’s significantly less exceptional at deciding where those lines should go. It defaults to creating new files everywhere. It repeats code it’s already written. It introduces inconsistencies.&lt;/p&gt;
    &lt;p&gt;This is the “mess” people describe in vibe code projects as they grow. You still need a human making structural decisions.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trained on Imperative: It Writes Defensive Code&lt;/head&gt;
    &lt;p&gt; AI trained mostly on imperative code. Ruby, Python, JavaScript, C#. Elixir looks like Ruby. So Claude writes Ruby-style Elixir—&lt;code&gt;if/then/else&lt;/code&gt; chains, defensive nil-checking, early returns that don’t make sense in a functional context.&lt;/p&gt;
    &lt;p&gt;Elixir wants you to be assertive. Pattern match on what you expect. Let it crash if something’s wrong. The process restarts in a good state. This is foreign to most code Claude trained on.&lt;/p&gt;
    &lt;p&gt;This gets better as the codebase grows. Claude sees more assertive patterns. It starts to infer the style. But it still defaults to defensive. I still correct it regularly. Be strict about what good Elixir looks like.&lt;/p&gt;
    &lt;head rend="h3"&gt;Git Operations: Keep It Out of Context&lt;/head&gt;
    &lt;p&gt;Every git operation takes context window space. Checking status. Writing commit messages. Describing PRs. That space could go to actual work. Git context goes stale fast—a commit message from 20 minutes ago is worthless after three more changes.&lt;/p&gt;
    &lt;p&gt;When I’m babysitting a feature, I commit manually. Every point I’m happy with. It’s fast. It’s cheap version control. It doesn’t burn context.&lt;/p&gt;
    &lt;p&gt;Claude Code has “checkpoints” now. Internal version control that protects vibe coders without explicit commits. That’s better than AI managing git directly.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Ugly&lt;/head&gt;
    &lt;head rend="h3"&gt;OTP and Async: It Chases Ghosts&lt;/head&gt;
    &lt;p&gt;Claude is useless for debugging OTP, Task, or async issues. It doesn’t understand how processes, the actor model, and GenServers work together. When it tries to introspect the running system, it feeds itself bad data. It gets very lost.&lt;/p&gt;
    &lt;p&gt;It can course correct when you point out where it went wrong. But on its own, it chases ghosts.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ecto Sandbox: It Chases Red Herrings&lt;/head&gt;
    &lt;p&gt;In Elixir tests, each test runs in a database transaction that rolls back at the end. Tests run async without hitting each other. No test data persists.&lt;/p&gt;
    &lt;p&gt;Claude doesn’t understand this. It uses Tidewave’s dev DB connection and thinks it’s looking at the test DB—which is always empty. A test fails. Claude queries the database. Finds nothing. Thinks there’s a data problem.&lt;/p&gt;
    &lt;p&gt;I’ve watched Claude try to seed the test database so a test will pass. That’s clearly wrong.&lt;/p&gt;
    &lt;p&gt;Other times, two tests insert or query the same schema. Claude doesn’t understand transaction isolation—tests can’t see each other’s data. It confuses itself and recommends disabling async tests altogether. Manageable once you watch for it. But ugly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bottom Line&lt;/head&gt;
    &lt;p&gt;AI writing all the code has been a massive win. The friction exists, but it’s manageable and doesn’t interfere much with day-to-day work. By far the most important thing: have a consistent, coherent codebase architecture. Without it, you’ll quickly end up with spaghetti code.&lt;/p&gt;
    &lt;p&gt;The goal for this year: automate myself out of a job. That means giving Claude more control over the entire software development lifecycle—from a simple problem statement to a fully tested, working PR that only needs a quick glance before it’s merged and deployed.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46752907</guid><pubDate>Sun, 25 Jan 2026 10:54:29 +0000</pubDate></item><item><title>Show HN: TUI for managing XDG default applications</title><link>https://github.com/mitjafelicijan/xdgctl</link><description>&lt;doc fingerprint="fe9cba7b7ba6b9a3"&gt;
  &lt;main&gt;
    &lt;p&gt;&lt;code&gt;xdgctl&lt;/code&gt; is a TUI for managing XDG default applications. View and set defaults for file categories without using &lt;code&gt;xdg-mime&lt;/code&gt; directly.&lt;/p&gt;
    &lt;p&gt;Built with C using GLib/GIO and termbox2.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;xdgctl.mp4&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browse by category (Browsers, Text Editors, etc.)&lt;/item&gt;
      &lt;item&gt;Current default marked with &lt;code&gt;*&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Key&lt;/cell&gt;
        &lt;cell role="head"&gt;Action&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Up/Down&lt;/cell&gt;
        &lt;cell&gt;Navigate through categories or applications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Right/Tab&lt;/cell&gt;
        &lt;cell&gt;Switch from category list to application list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Arrow Left&lt;/cell&gt;
        &lt;cell&gt;Switch back to category list&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Enter&lt;/cell&gt;
        &lt;cell&gt;Set selected application as default for current category&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Esc / q&lt;/cell&gt;
        &lt;cell&gt;Quit the application&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To build &lt;code&gt;xdgctl&lt;/code&gt;, you need the following development libraries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;glib-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gio-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gio-unix-2.0&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clang&lt;/code&gt;or&lt;code&gt;gcc&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# On Void Linux
sudo xbps-install glibc-devel&lt;/code&gt;
    &lt;code&gt;git clone https://github.com/mitjafelicijan/xdgctl.git
cd xdgctl

# Build
make
sudo make install

# Using prefix
sudo make PREFIX=/usr/local install
make PREFIX=~/.local install&lt;/code&gt;
    &lt;p&gt;If you manually add new applications to your &lt;code&gt;~/.local/share/applications&lt;/code&gt; directory, you might need to run &lt;code&gt;update-desktop-database&lt;/code&gt; again.&lt;/p&gt;
    &lt;code&gt;ls /usr/share/applications
ls ~/.local/share/applications&lt;/code&gt;
    &lt;code&gt;xdg-mime query default text/plain
xdg-mime query default text/html
xdg-mime query default x-scheme-handler/http
xdg-mime query default x-scheme-handler/https
xdg-mime query default inode/directory&lt;/code&gt;
    &lt;code&gt;xdg-mime default brave.desktop x-scheme-handler/http
xdg-mime default brave.desktop x-scheme-handler/https&lt;/code&gt;
    &lt;code&gt;# ~/.local/share/applications/brave.desktop
[Desktop Entry]
Exec=/home/m/Applications/brave
Type=Application
Categories=Applications
Name=Brave Browser
MimeType=text/html;text/xml;application/xhtml+xml;x-scheme-handler/http;x-scheme-handler/https;&lt;/code&gt;
    &lt;code&gt;update-desktop-database ~/.local/share/applications
less ~/.config/mimeapps.list
less /usr/share/applications/mimeapps.list&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753078</guid><pubDate>Sun, 25 Jan 2026 11:19:04 +0000</pubDate></item><item><title>Show HN: Bonsplit – Tabs and splits for native macOS apps</title><link>https://bonsplit.alasdairmonk.com</link><description>&lt;doc fingerprint="8d0973528a96f1e8"&gt;
  &lt;main&gt;
    &lt;p&gt;Bonsplit is a custom tab bar and layout split library for macOS apps. Enjoy out of the box 120fps animations, drag-and-drop reordering, SwiftUI support &amp;amp; keyboard navigation.&lt;/p&gt;
    &lt;quote&gt;.package(url: "https://github.com/almonk/bonsplit.git", from: "1.0.0")&lt;/quote&gt;
    &lt;p&gt;### Features&lt;/p&gt;
    &lt;p&gt;Create tabs with optional icons and dirty indicators. Target specific panes or use the focused pane.&lt;/p&gt;
    &lt;quote&gt;let tabId = controller.createTab(title: "Document.swift",icon: "swift",isDirty: false,inPane: paneId)&lt;/quote&gt;
    &lt;p&gt;Create tabs with optional icons and dirty indicators. Target specific panes or use the focused pane.&lt;/p&gt;
    &lt;quote&gt;let tabId = controller.createTab(title: "Document.swift",icon: "swift",isDirty: false,inPane: paneId)&lt;/quote&gt;
    &lt;p&gt;Split any pane horizontally or vertically. New panes are empty by default, giving you full control.&lt;/p&gt;
    &lt;quote&gt;// Split focused pane horizontallylet newPaneId = controller.splitPane(orientation: .horizontal)// Split with a tab already in the new panecontroller.splitPane(orientation: .vertical,withTab: Tab(title: "New", icon: "doc"))&lt;/quote&gt;
    &lt;p&gt;Split any pane horizontally or vertically. New panes are empty by default, giving you full control.&lt;/p&gt;
    &lt;quote&gt;// Split focused pane horizontallylet newPaneId = controller.splitPane(orientation: .horizontal)// Split with a tab already in the new panecontroller.splitPane(orientation: .vertical,withTab: Tab(title: "New", icon: "doc"))&lt;/quote&gt;
    &lt;p&gt;Update tab properties at any time. Changes animate smoothly.&lt;/p&gt;
    &lt;quote&gt;// Mark document as modifiedcontroller.updateTab(tabId, isDirty: true)// Rename tabcontroller.updateTab(tabId, title: "NewName.swift")// Change iconcontroller.updateTab(tabId, icon: "doc.text")&lt;/quote&gt;
    &lt;p&gt;Update tab properties at any time. Changes animate smoothly.&lt;/p&gt;
    &lt;quote&gt;// Mark document as modifiedcontroller.updateTab(tabId, isDirty: true)// Rename tabcontroller.updateTab(tabId, title: "NewName.swift")// Change iconcontroller.updateTab(tabId, icon: "doc.text")&lt;/quote&gt;
    &lt;p&gt;Programmatically navigate between panes using directional navigation.&lt;/p&gt;
    &lt;quote&gt;// Move focus between panescontroller.navigateFocus(direction: .left)controller.navigateFocus(direction: .right)controller.navigateFocus(direction: .up)controller.navigateFocus(direction: .down)// Or focus a specific panecontroller.focusPane(paneId)&lt;/quote&gt;
    &lt;p&gt;Programmatically navigate between panes using directional navigation.&lt;/p&gt;
    &lt;quote&gt;// Move focus between panescontroller.navigateFocus(direction: .left)controller.navigateFocus(direction: .right)controller.navigateFocus(direction: .up)controller.navigateFocus(direction: .down)// Or focus a specific panecontroller.focusPane(paneId)&lt;/quote&gt;
    &lt;p&gt;### Read this, agents...&lt;/p&gt;
    &lt;p&gt;Complete reference for all Bonsplit classes, methods, and configuration options.&lt;/p&gt;
    &lt;p&gt;The main controller for managing tabs and panes. Create an instance and pass it to BonsplitView.&lt;/p&gt;
    &lt;p&gt;Implement this protocol to receive callbacks about tab bar events. All methods have default implementations and are optional.&lt;/p&gt;
    &lt;p&gt;Configure behavior and appearance. Pass to BonsplitController on initialization.&lt;/p&gt;
    &lt;code&gt;allowSplits&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable split buttons and drag-to-split&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCloseTabs&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Show close buttons on tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCloseLastPane&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Allow closing the last remaining pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;false&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowTabReordering&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable drag-to-reorder tabs within a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;allowCrossPaneTabMove&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable moving tabs between panes via drag&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;autoCloseEmptyPanes&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Automatically close panes when their last tab is closed&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;contentViewLifecycle&lt;/code&gt;
    &lt;code&gt;ContentViewLifecycle&lt;/code&gt;
    &lt;p&gt;How tab content views are managed when switching tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;.recreateOnSwitch&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;newTabPosition&lt;/code&gt;
    &lt;code&gt;NewTabPosition&lt;/code&gt;
    &lt;p&gt;Where new tabs are inserted in the tab list&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;.current&lt;/code&gt;&lt;/p&gt;
    &lt;quote&gt;let config = BonsplitConfiguration(allowSplits: true,allowCloseTabs: true,allowCloseLastPane: false,autoCloseEmptyPanes: true,contentViewLifecycle: .keepAllAlive,newTabPosition: .current)let controller = BonsplitController(configuration: config)&lt;/quote&gt;
    &lt;p&gt;Controls how tab content views are managed when switching between tabs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
        &lt;cell role="head"&gt;State&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;.recreateOnSwitch&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
        &lt;cell&gt;Simple content&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.keepAllAlive&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Higher&lt;/cell&gt;
        &lt;cell&gt;Full&lt;/cell&gt;
        &lt;cell&gt;Complex views, forms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Controls where new tabs are inserted in the tab list.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Mode&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;.current&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Insert after currently focused tab, or at end if none&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;.end&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Always insert at the end of the tab list&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;tabBarHeight&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Height of the tab bar&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;33&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabMinWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum width of a tab&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;140&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabMaxWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Maximum width of a tab&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;220&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;tabSpacing&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Spacing between tabs&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;0&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;minimumPaneWidth&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum width of a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;100&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;minimumPaneHeight&lt;/code&gt;
    &lt;code&gt;CGFloat&lt;/code&gt;
    &lt;p&gt;Minimum height of a pane&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;100&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;showSplitButtons&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Show split buttons in the tab bar&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;animationDuration&lt;/code&gt;
    &lt;code&gt;Double&lt;/code&gt;
    &lt;p&gt;Duration of animations in seconds&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;0.15&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;enableAnimations&lt;/code&gt;
    &lt;code&gt;Bool&lt;/code&gt;
    &lt;p&gt;Enable or disable all animations&lt;/p&gt;
    &lt;p&gt;Default: &lt;code&gt;true&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;.default&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Default configuration with all features enabled&lt;/p&gt;
    &lt;code&gt;.singlePane&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Single pane mode with splits disabled&lt;/p&gt;
    &lt;code&gt;.readOnly&lt;/code&gt;
    &lt;code&gt;BonsplitConfiguration&lt;/code&gt;
    &lt;p&gt;Read-only mode with all modifications disabled&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753301</guid><pubDate>Sun, 25 Jan 2026 11:56:42 +0000</pubDate></item><item><title>Nango (YC W23, Dev Infrastructure) Is Hiring Remotely</title><link>https://jobs.ashbyhq.com/Nango</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753336</guid><pubDate>Sun, 25 Jan 2026 12:02:01 +0000</pubDate></item><item><title>Doom has been ported to an earbud</title><link>https://doombuds.com</link><description>&lt;doc fingerprint="a6567235022113ff"&gt;
  &lt;main&gt;
    &lt;p&gt;It's almost your turn, get ready!&lt;/p&gt;
    &lt;p&gt;Player queue&lt;/p&gt;
    &lt;p&gt;Your position&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Players queued&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Wait time&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;You know the 1993 classic DOOM? I made it run on an earbud, then I connected it to the internet and made it possible for visitors like you to sit in a queue for hours play the game remotely!.&lt;/p&gt;
    &lt;p&gt;Yeah but it won't just run on any old earbud, this only works with the Pinebuds Pro, the only earbuds with open source firmware.&lt;/p&gt;
    &lt;p&gt;You sure can! There are two relevant repos:&lt;/p&gt;
    &lt;p&gt;This was a necessary optimisation to avoid paying outgoing bandwidth fees, once you're 5th in the queue, the twitch player will switch to a low-latency MJPEG stream.&lt;/p&gt;
    &lt;p&gt;shhhh don't look don't look it's ok just join the queue&lt;/p&gt;
    &lt;p&gt;Let's switch to a more readable font first.&lt;/p&gt;
    &lt;p&gt; I'll put out an article / video diving deeper into this later, but here are a few bits of info:&lt;lb/&gt; This project is made up of four parts: &lt;/p&gt;
    &lt;p&gt;The firmware pushes up against a few hardware limitations:&lt;/p&gt;
    &lt;p&gt; Earbuds don't have displays, so the only way to transfer data to/from them is either via bluetooth, or the UART contact pads.&lt;lb/&gt; Bluetooth is pretty slow, you'd be lucky to get a consistent 1mbps connection, UART is easily the better option.&lt;lb/&gt; DOOM's framebuffer is (width * height) bytes, 320 * 200 = 96kB. (doom's internal framebuffer is 8-bit not 24-bit)&lt;lb/&gt; The UART connection provides us with 2.4mbps of usable bandwidth. 2,400,000 / 8 / 96,000 gives us... 3 frames per second.&lt;lb/&gt; Clearly we need to compress the video stream. Modern video codecs like h264 consume way too much CPU and RAM.&lt;lb/&gt; The only feasible approach is sending the video as an MJPEG stream. MJPEG is a stream of JPEG images shown one after the other.&lt;lb/&gt; I found an excellent JPEG encoder for embedded devices here, thanks Larry!&lt;lb/&gt; A conservative estimate for the average HIGH quality JPEG frame is around 13.5KB, but most scenes (without enemies) are around 11kb.&lt;lb/&gt; Theoretical maximum FPS:&lt;lb/&gt; - Optimistic: `2,400,000 / (11,000 * 8)` = 27.3 FPS&lt;lb/&gt; - Conservative: `2,400,000 / (13,500 * 8)` = 22.2 FPS &lt;/p&gt;
    &lt;p&gt; The stock open source firmware has the CPU set to 100mhz, so I cranked that up to 300mhz and disabled low power mode.&lt;lb/&gt; The Cortex-M4F running at 300mhz is actually more than enough for DOOM, however it struggles with JPEG encoding.&lt;lb/&gt; This is why it maxes out at ~18fps, I don't think there's much else I can do to speed it up. &lt;/p&gt;
    &lt;p&gt; By default, we only have access to 768KB of RAM, after disabling the co-processor it gets bumped up to the advertised 992KB.&lt;lb/&gt; DOOM requires 4MB of RAM, though there are plenty of optimisations that can reduce this amount.&lt;lb/&gt; Pre-generating lookup tables, making variables const, reading const variables from flash, disabling DOOM's caching system, removing unneeded variables. It all adds up! &lt;/p&gt;
    &lt;p&gt; The shareware DOOM 1 wad (assets file) is 4.2MB and the earbuds can only store 4MB of data.&lt;lb/&gt; Thankfully, fragglet, a well-known doom modder, has already solved this issue for me.&lt;lb/&gt; Squashware is his trimmed-down DOOM 1 wad that is only 1.7MB in size.&lt;lb/&gt; With this wad file, everything comfortably fits in flash. &lt;/p&gt;
    &lt;p&gt;I thought you'd never ask! (please hire me)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753484</guid><pubDate>Sun, 25 Jan 2026 12:22:12 +0000</pubDate></item><item><title>Alarm overload is undermining safety at sea as crews face thousands of alerts</title><link>https://www.lr.org/en/knowledge/press-room/press-listing/press-release/2026/alarm-overload-is-undermining-safety-at-sea-as-new-research-shows-crews-face-tens-of-thousands-of-daily-alerts/</link><description>&lt;doc fingerprint="3452211c23d1e7ad"&gt;
  &lt;main&gt;
    &lt;p&gt;New research from Lloyd’s Register (LR) has revealed that excessive and nuisance shipboard alarm systems are routinely overwhelming crews and, in many cases, actively undermining safety at sea.&lt;/p&gt;
    &lt;p&gt;The findings, published today in Effective Alarm Management in the Maritime Industry are based on data collected from 11 operational vessels, spanning over 2,000 days and more than 40 million alarm-related events.&lt;/p&gt;
    &lt;p&gt;The study shows that many ships generate thousands of alarms every day, many of which provide little or no operational value. The result is widespread alarm fatigue, disrupted rest periods and a growing erosion of trust in systems that are intended to protect both crews and assets.&lt;/p&gt;
    &lt;p&gt;The research applied recognised industrial best practice, including IEC 62682 and EEMUA 191, to maritime operations for the first time at this scale. It found that fewer than half of the vessels studied met the recommended benchmark of fewer than 30 alarms per hour, while on ships with unattended machinery spaces alarms disrupted 63% of rest periods. In some cases, cruise ships experienced up to 2,600 alarms per day, with peak rates reaching 4,691 alarms in just ten minutes.&lt;/p&gt;
    &lt;p&gt;Crews, overwhelmed by the volume of alerts, are forced to silence alarms without acknowledgement or physically bypass alarm circuits, normalising unsafe practices and eroding trust in critical safety systems.&lt;/p&gt;
    &lt;p&gt;Effective Alarm Management in the Maritime Industry: Insights from 40 million vessel alarms builds on LR’s Effective Alarm Management in the Maritime Industry report (released in September 2024) by moving beyond diagnosis to demonstrate what can be achieved in practice. A pilot project on an operational cruise ship reduced total alarm numbers by almost 50 per cent over a six-month period, without new technology or major system redesign. Improvements were delivered through traditional marine engineering interventions, including correcting valve installations, replacing faulty sensors and tuning existing systems.&lt;/p&gt;
    &lt;p&gt;LR’s analysis also demonstrates that addressing the 10 most frequent alarms could reduce overall loads by nearly 40 per cent.&lt;/p&gt;
    &lt;p&gt;The report calls for greater adoption of objective alarm performance assessment, stronger consideration of human factors in system design and operation throughout the vessel lifecycle, and regulatory frameworks that support consistent, enforceable standards.&lt;/p&gt;
    &lt;p&gt;Duncan Duffy, LR’s Global Head of Technology, said: “Our research found that alarm systems, when poorly managed, have themselves become a safety risk. Without decisive industry action, alarm fatigue will continue to undermine situational awareness and increase the likelihood of serious incidents.&lt;/p&gt;
    &lt;p&gt;“If the maritime industry is serious about safety, it must commit to continuous performance measurement, objective evaluation, and a human-centred approach to alarm system design. Only then can alarm systems fulfil their intended purpose—supporting crews, safeguarding lives, and ensuring safer voyages for all.”&lt;/p&gt;
    &lt;p&gt;The research is part of LR’s Digital Transformation Research programme, specifically designed to provide in-depth analysis of key opportunities and challenges for maritime digitalisation.&lt;/p&gt;
    &lt;p&gt;For more information and to download the full report, visit the link below:&lt;lb/&gt;LR Alarm Management &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753611</guid><pubDate>Sun, 25 Jan 2026 12:40:00 +0000</pubDate></item><item><title>Web-based image editor modeled after Deluxe Paint</title><link>https://github.com/steffest/DPaint-js</link><description>&lt;doc fingerprint="56223e185d452d87"&gt;
  &lt;main&gt;
    &lt;p&gt;Webbased image editor modeled after the legendary Deluxe Paint with a focus on retro Amiga file formats. Next to modern image formats, DPaint.js can read and write Amiga icon files and IFF ILBM images.&lt;/p&gt;
    &lt;p&gt;Online version available at https://www.stef.be/dpaint/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fully Featured image editor with a.o. &lt;list rend="ul"&gt;&lt;item&gt;Layers&lt;/item&gt;&lt;item&gt;Selections&lt;/item&gt;&lt;item&gt;Masking&lt;/item&gt;&lt;item&gt;Transformation tools&lt;/item&gt;&lt;item&gt;Effects and filters&lt;/item&gt;&lt;item&gt;Multiple undo/redo&lt;/item&gt;&lt;item&gt;Copy/Paste from any other image program or image source&lt;/item&gt;&lt;item&gt;Customizable dither tools&lt;/item&gt;&lt;item&gt;Color Cycling&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Heavy focus on colour reduction with fine-grained dithering options&lt;/item&gt;
      &lt;item&gt;Amiga focus &lt;list rend="ul"&gt;&lt;item&gt;Read/write/convert Amiga icon files (all formats)&lt;/item&gt;&lt;item&gt;Reads IFF ILBM images (all formats including HAM and 24-bit)&lt;/item&gt;&lt;item&gt;Writes IFF ILBM images (up to 256 colors)&lt;/item&gt;&lt;item&gt;Read and write directly from Amiga Disk Files (ADF)&lt;/item&gt;&lt;item&gt;Embedded Amiga Emulator to preview your work in the real Deluxe Paint.&lt;/item&gt;&lt;item&gt;Limit the palette to 12 bit for Amiga OCS/ECS mode, or 9 bit for Atari ST mode.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Deluxe Paint Legacy &lt;list rend="ul"&gt;&lt;item&gt;Supports PBM files as used by the PC version of Deluxe Paint (Thanks to Michael Smith)&lt;/item&gt;&lt;item&gt;Supports Deluxe Paint Atari ST compression modes (Thanks to Nicolas Ramz)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It runs in your browser, works on any system and works fine on touch-screen devices like iPads.&lt;lb/&gt; It is written in 100% plain JavaScript and has no dependencies.&lt;lb/&gt; It's 100% free, no ads, no tracking, no accounts, no nothing.&lt;lb/&gt; All processing is done in your browser, no data is sent to any server.&lt;/p&gt;
    &lt;p&gt;The only part that is not included in this repository is the Amiga Emulator Files. (The emulator is based on the Scripted Amiga Emulator)&lt;/p&gt;
    &lt;p&gt;DPaint.js doesn't need building.&lt;lb/&gt; It also has zero dependencies so there's no need to install anything.&lt;lb/&gt; DPaint.js is written using ES6 modules and runs out of the box in modern browsers.&lt;lb/&gt; Just serve "index.html" from a webserver and you're good to go.&lt;/p&gt;
    &lt;p&gt;There's an optional build step to create a compact version of DPaint.js if you like.&lt;lb/&gt; I'm using Parcel.js for this.&lt;lb/&gt; For convenience, I've included a "package.json" file.&lt;lb/&gt; open a terminal and run &lt;code&gt;npm install&lt;/code&gt; to install Parcel.js and its dependencies.
Then run &lt;code&gt;npm run build&lt;/code&gt; to create a compact version of DPaint.js in the "dist" folder.&lt;/p&gt;
    &lt;p&gt;Documentation can be found at https://www.stef.be/dpaint/docs/&lt;/p&gt;
    &lt;p&gt;Dpaint.js is a web application, not an app that you install on your computer. That being said: DPaint.js has no online dependencies and runs fine offline if you want. One caveat: you have to serve the index.html file from a webserver, not just open it in your browser.&lt;lb/&gt; A quick way to do this is - for example - using the Spark app.&lt;lb/&gt; Download the binary for your platform, drop the Spark executable in the folder where you downloaded the Dpaint.js source files and run it. If you then point your browser to http://localhost:8080/ it should work.&lt;/p&gt;
    &lt;p&gt;If you are using Chrome, you can also "install" dpaint.js as app.&lt;lb/&gt; It will then show up your Chrome apps and work offline.&lt;/p&gt;
    &lt;p&gt;Current version is still alpha.&lt;lb/&gt; I'm sure there are bugs and missing features.&lt;lb/&gt; Bug reports and pull requests are welcome.&lt;/p&gt;
    &lt;p&gt;Planned for the next release, already in the works:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Color Cycling&lt;/del&gt;(done)&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Animation support (GIf and Amiga ANIM files)&lt;/del&gt;(done)&lt;/item&gt;
      &lt;item&gt;&lt;del rend="overstrike"&gt;Shading/transparency tools that stay within the palette.&lt;/del&gt;(done)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Planned for a future release if there's a need for it.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Support for non-square pixel modes such as HiRes and Interlaced&lt;/item&gt;
      &lt;item&gt;PSD import and export&lt;/item&gt;
      &lt;item&gt;SpriteSheet support&lt;/item&gt;
      &lt;item&gt;Write HAM,SHAM and Dynamic HiRes images&lt;/item&gt;
      &lt;item&gt;Commodore 64 graphics modes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Please note that the Brave browser is using "farbling" that introduces random image noise in certain conditions. They claim this is to protect your privacy. Although I totally understand the sentiment, In my opinion a browser should not actively alter the content of a webpage or intentionally break functionality.&lt;lb/&gt; But hey, who am I to speak, it's a free world. Just be aware that if you are using Brave, you will run into issues, so please "lower your shields" for this app in Brave or use another browser.&lt;/p&gt;
    &lt;p&gt;Dpaint.js supports Color-Cycling - a long lost art of "animating" a static image by only rotating some colors in the palette. See an example here:&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;The_Vision_cycle.mp4&lt;/head&gt;
    &lt;p&gt;Open the layered source file of the above image directly in Dpaint.js&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46753708</guid><pubDate>Sun, 25 Jan 2026 12:54:53 +0000</pubDate></item><item><title>Wine-Staging 11.1 Adds Patches for Enabling Recent Photoshop Versions on Linux</title><link>https://www.phoronix.com/news/Wine-Staging-11.1</link><description>&lt;doc fingerprint="de54f53ba99c238b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Wine-Staging 11.1 Adds Patches For Enabling Recent Adobe Photoshop Versions On Linux&lt;/head&gt;
    &lt;p&gt; Following yesterday's release of Wine 11.1 for kicking off the new post-11.0 development cycle, Wine-Staging 11.1 is now available for this experimental/testing version of Wine that present is around 254 patches over the upstream Wine state. &lt;lb/&gt;Besides re-basing those 250+ patches to the latest Wine Git state, the latest VKD3D Git code is also pulled into Wine-Staging 11.1. There is some new feature work with Wine-Staging... Landing the new patches for enabling the recent Adobe Photoshop versions to successfully install and run under Wine on Linux.&lt;lb/&gt;As covered last week, the latest Adobe Photoshop installer is working and the app running with some new patches against the Wine code for its MSXML3 and MSHTML code. Those patches were cited in Bug 47015 for upstream Wine around the Photoshop Creative Cloud 2019 screen hitting MSXML3 errors.&lt;lb/&gt;Upstream Wine Git hasn't yet taken those patches but now they are in Wine-Staging 11.1 for further testing by the community.&lt;lb/&gt;Hopefully the testing will go well and those patches will get picked up by an upstream Wine 11.x bi-weekly development release in the near future.&lt;lb/&gt;Besides those MSHTML/MSXML3 work to benefit Adobe software, there aren't any other new patches in Wine-Staging 11.1. Those looking for new binaries to test it out can find them via WineHQ.org.&lt;/p&gt;
    &lt;p&gt;Besides re-basing those 250+ patches to the latest Wine Git state, the latest VKD3D Git code is also pulled into Wine-Staging 11.1. There is some new feature work with Wine-Staging... Landing the new patches for enabling the recent Adobe Photoshop versions to successfully install and run under Wine on Linux.&lt;/p&gt;
    &lt;p&gt;As covered last week, the latest Adobe Photoshop installer is working and the app running with some new patches against the Wine code for its MSXML3 and MSHTML code. Those patches were cited in Bug 47015 for upstream Wine around the Photoshop Creative Cloud 2019 screen hitting MSXML3 errors.&lt;/p&gt;
    &lt;p&gt;Upstream Wine Git hasn't yet taken those patches but now they are in Wine-Staging 11.1 for further testing by the community.&lt;/p&gt;
    &lt;p&gt;Hopefully the testing will go well and those patches will get picked up by an upstream Wine 11.x bi-weekly development release in the near future.&lt;/p&gt;
    &lt;p&gt;Besides those MSHTML/MSXML3 work to benefit Adobe software, there aren't any other new patches in Wine-Staging 11.1. Those looking for new binaries to test it out can find them via WineHQ.org.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754427</guid><pubDate>Sun, 25 Jan 2026 14:42:39 +0000</pubDate></item><item><title>Show HN: Netfence – Like Envoy for eBPF Filters</title><link>https://github.com/danthegoodman1/netfence</link><description>&lt;doc fingerprint="780d8d538f3988c3"&gt;
  &lt;main&gt;
    &lt;p&gt;Like Envoy xDS, but for eBPF filters.&lt;/p&gt;
    &lt;p&gt;Netfence runs as a daemon on your VM/container hosts and automatically injects eBPF filter programs into cgroups and network interfaces, with a built-in DNS server that resolves allowed domains and populates the IP allowlist.&lt;/p&gt;
    &lt;p&gt;Netfence daemons connect to a central control plane that you implement via gRPC to synchronize allowlists/denylists with your backend.&lt;/p&gt;
    &lt;p&gt;Your control plane pushes network rules like &lt;code&gt;ALLOW *.pypi.org&lt;/code&gt; or &lt;code&gt;ALLOW 10.0.0.0/16&lt;/code&gt; to attached interfaces/cgroups. When a VM/container queries DNS, Netfence resolves it, adds the IPs to the eBPF filter, and drops traffic to unknown IPs before it leaves the host without any performance penalty.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Attach eBPF filters to network interfaces (TC) or cgroups&lt;/item&gt;
      &lt;item&gt;Policy modes: disabled, allowlist, denylist, block-all&lt;/item&gt;
      &lt;item&gt;IPv4 and IPv6 CIDR support with optional TTLs&lt;/item&gt;
      &lt;item&gt;Per-attachment DNS server with domain allowlist/denylist&lt;/item&gt;
      &lt;item&gt;Domain rules support subdomains with specificity-based matching (more specific rules win)&lt;/item&gt;
      &lt;item&gt;Resolved domains auto-populate IP filter&lt;/item&gt;
      &lt;item&gt;Metadata on daemons and attachments for associating with VM ID, tenant, etc.&lt;/item&gt;
      &lt;item&gt;Support for proxying DNS queries to the control plane to make DNS decisions per-attachment&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;+------------------+         +-------------------------+
|  Your Control    |&amp;lt;-------&amp;gt;|  Daemon (per host)      |
|  Plane (gRPC)    |  stream |                         |
+------------------+         |  +-------------------+  |
                             |  | DNS Server        |  |
                             |  | (per-attachment)  |  |
                             |  +-------------------+  |
                             +-------------------------+
                                        |
                                 +------+------+
                                 |             |
                              TC Filter    Cgroup Filter
                              (veth, eth)  (containers)
&lt;/code&gt;
    &lt;p&gt;Each attachment gets a unique DNS address (port) provisioned by the daemon. Containers/VMs should be configured to use their assigned DNS address.&lt;/p&gt;
    &lt;p&gt;Run the daemon, which:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Exposes a local gRPC API (&lt;code&gt;DaemonService&lt;/code&gt;) for attaching/detaching filters&lt;/item&gt;
      &lt;item&gt;Connects to your control plane via bidirectional stream (&lt;code&gt;ControlPlane.Connect&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Loads and manages eBPF programs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Start the daemon:&lt;/p&gt;
    &lt;code&gt;# Start with default config
netfenced start

# Start with custom config file
netfenced start --config /etc/netfence/config.yaml&lt;/code&gt;
    &lt;p&gt;Check daemon status:&lt;/p&gt;
    &lt;code&gt;netfenced status&lt;/code&gt;
    &lt;p&gt;Your orchestration system calls the daemon's local API.&lt;/p&gt;
    &lt;p&gt;RPC:&lt;/p&gt;
    &lt;code&gt;DaemonService.Attach(interface_name: "veth123", metadata: {vm_id: "abc"})
// or
DaemonService.Attach(cgroup_path: "/sys/fs/cgroup/...", metadata: {container_id: "xyz"})
&lt;/code&gt;
    &lt;p&gt;CLI:&lt;/p&gt;
    &lt;code&gt;# Attach to a network interface (TC)
netfenced attach --interface veth123 --metadata vm_id=abc

# Attach to a cgroup
netfenced attach --cgroup /sys/fs/cgroup/... --metadata container_id=xyz

# Attach with metadata
netfenced attach --interface eth0 --metadata tenant=acme,env=prod&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Daemon attaches eBPF filter to the target&lt;/item&gt;
      &lt;item&gt;Daemon sends &lt;code&gt;Subscribed{id, target, type, metadata}&lt;/code&gt;to control plane and waits for&lt;code&gt;SubscribedAck&lt;/code&gt;with initial config (mode, CIDRs, DNS rules)&lt;/item&gt;
      &lt;item&gt;If the control plane doesn't respond within the timeout (default 5s, configurable via &lt;code&gt;control_plane.subscribe_ack_timeout&lt;/code&gt;), the attachment is rolled back and the attach call fails&lt;/item&gt;
      &lt;item&gt;Daemon watches for target removal and sends &lt;code&gt;Unsubscribed&lt;/code&gt;automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;RPC:&lt;/p&gt;
    &lt;code&gt;DaemonService.Detach(id)
&lt;/code&gt;
    &lt;p&gt;CLI:&lt;/p&gt;
    &lt;code&gt;netfenced detach --id &amp;lt;attachment-id&amp;gt;&lt;/code&gt;
    &lt;p&gt;List attachments:&lt;/p&gt;
    &lt;code&gt;netfenced list
netfenced list --all  # fetch all pages&lt;/code&gt;
    &lt;p&gt;Implement &lt;code&gt;ControlPlane.Connect&lt;/code&gt; RPC - a bidirectional stream:&lt;/p&gt;
    &lt;p&gt;Receive from daemon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SyncRequest&lt;/code&gt;on connect/reconnect (lists current attachments)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Subscribed&lt;/code&gt;when new attachments are added&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Unsubscribed&lt;/code&gt;when attachments are removed&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Heartbeat&lt;/code&gt;with stats&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Send to daemon:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;SyncAck&lt;/code&gt;after receiving SyncRequest&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SubscribedAck{mode, cidrs, dns_config}&lt;/code&gt;after receiving Subscribed (required - daemon waits for this)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SetMode{mode}&lt;/code&gt;- change IP filter policy mode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AllowCIDR{cidr, ttl}&lt;/code&gt;/&lt;code&gt;DenyCIDR&lt;/code&gt;/&lt;code&gt;RemoveCIDR&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;SetDnsMode{mode}&lt;/code&gt;- change DNS filtering mode&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;AllowDomain{domain}&lt;/code&gt;/&lt;code&gt;DenyDomain&lt;/code&gt;/&lt;code&gt;RemoveDomain&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;BulkUpdate{mode, cidrs, dns_config}&lt;/code&gt;- full state sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When the daemon receives &lt;code&gt;Subscribed&lt;/code&gt;, it blocks waiting for &lt;code&gt;SubscribedAck&lt;/code&gt; before returning success to the caller. This ensures the attachment has its initial configuration before traffic flows. Use the metadata to identify which VM/tenant/container this attachment belongs to and respond with the appropriate initial rules.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754724</guid><pubDate>Sun, 25 Jan 2026 15:13:46 +0000</pubDate></item><item><title>A macOS app that blurs your screen when you slouch</title><link>https://github.com/tldev/posturr</link><description>&lt;doc fingerprint="719cd7c635d7d38d"&gt;
  &lt;main&gt;
    &lt;p&gt;A macOS app that blurs your screen when you slouch.&lt;/p&gt;
    &lt;p&gt;Posturr uses your Mac's camera and Apple's Vision framework to monitor your posture in real-time. When it detects that you're slouching, it progressively blurs your screen to remind you to sit up straight. Maintain good posture, and the blur clears instantly.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-time posture detection - Uses Apple's Vision framework for body pose and face tracking&lt;/item&gt;
      &lt;item&gt;Progressive screen blur - Gentle visual reminder that intensifies with worse posture&lt;/item&gt;
      &lt;item&gt;Menu bar controls - Easy access to settings, calibration, and status from the menu bar&lt;/item&gt;
      &lt;item&gt;Multi-display support - Works across all connected monitors&lt;/item&gt;
      &lt;item&gt;Privacy-focused - All processing happens locally on your Mac&lt;/item&gt;
      &lt;item&gt;Lightweight - Runs as a background app with minimal resource usage&lt;/item&gt;
      &lt;item&gt;No account required - No signup, no cloud, no tracking&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download the latest &lt;code&gt;Posturr-vX.X.X.zip&lt;/code&gt;from the Releases page&lt;/item&gt;
      &lt;item&gt;Unzip the downloaded file&lt;/item&gt;
      &lt;item&gt;Drag &lt;code&gt;Posturr.app&lt;/code&gt;to your Applications folder&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Since Posturr is not signed with an Apple Developer certificate, macOS Gatekeeper will initially block it:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Right-click (or Control-click) on &lt;code&gt;Posturr.app&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Select "Open" from the context menu&lt;/item&gt;
      &lt;item&gt;Click "Open" in the dialog that appears&lt;/item&gt;
      &lt;item&gt;Grant camera access when prompted&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You only need to do this once. After the first launch, you can open Posturr normally.&lt;/p&gt;
    &lt;p&gt;Posturr requires camera access to monitor your posture. When you first launch the app, macOS will ask for permission. Click "OK" to grant access.&lt;/p&gt;
    &lt;p&gt;If you accidentally denied permission, you can grant it later:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Open System Settings &amp;gt; Privacy &amp;amp; Security &amp;gt; Camera&lt;/item&gt;
      &lt;item&gt;Find Posturr and enable the toggle&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once launched, Posturr appears in your menu bar with a person icon. The app continuously monitors your posture and applies screen blur when slouching is detected.&lt;/p&gt;
    &lt;p&gt;Click the menu bar icon to access:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Status - Shows current state (Monitoring, Slouching, Good Posture, etc.)&lt;/item&gt;
      &lt;item&gt;Enabled - Toggle posture monitoring on/off&lt;/item&gt;
      &lt;item&gt;Recalibrate - Reset your baseline posture (sit up straight, then click)&lt;/item&gt;
      &lt;item&gt;Sensitivity - Adjust how sensitive the slouch detection is (Low, Medium, High, Very High)&lt;/item&gt;
      &lt;item&gt;Dead Zone - Set the tolerance before blur kicks in (None, Small, Medium, Large)&lt;/item&gt;
      &lt;item&gt;Quit - Exit the application (or press Escape anywhere)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Position your camera at eye level when possible&lt;/item&gt;
      &lt;item&gt;Ensure adequate lighting on your face&lt;/item&gt;
      &lt;item&gt;Sit at a consistent distance from your screen&lt;/item&gt;
      &lt;item&gt;The app works best when your shoulders are visible&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr uses Apple's Vision framework to detect body pose landmarks:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Body Pose Detection: Tracks nose, shoulders, and their relative positions&lt;/item&gt;
      &lt;item&gt;Face Detection Fallback: When full body isn't visible, tracks face position&lt;/item&gt;
      &lt;item&gt;Posture Analysis: Measures the vertical distance between nose and shoulders&lt;/item&gt;
      &lt;item&gt;Blur Response: Applies screen blur proportional to posture deviation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The screen blur uses macOS's private CoreGraphics API for efficient, system-level blur that covers all windows and displays.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Xcode Command Line Tools (&lt;code&gt;xcode-select --install&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/yourusername/posturr.git
cd posturr
./build.sh&lt;/code&gt;
    &lt;p&gt;The built app will be in &lt;code&gt;build/Posturr.app&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;# Standard build
./build.sh

# Build with release archive (.zip)
./build.sh --release&lt;/code&gt;
    &lt;code&gt;swiftc -O \
    -framework AppKit \
    -framework AVFoundation \
    -framework Vision \
    -framework CoreImage \
    -o Posturr \
    main.swift&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No code signing: Requires manual Gatekeeper bypass on first launch&lt;/item&gt;
      &lt;item&gt;Camera dependency: Requires a working camera with adequate lighting&lt;/item&gt;
      &lt;item&gt;Detection accuracy: Works best with clear view of upper body/face&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr exposes a file-based command interface for external control:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;capture&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Take a photo and analyze pose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;blur &amp;lt;0-64&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Set blur level manually&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;quit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Exit the application&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Write commands to &lt;code&gt;/tmp/posturr-command&lt;/code&gt;. Responses appear in &lt;code&gt;/tmp/posturr-response&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS 13.0 (Ventura) or later&lt;/item&gt;
      &lt;item&gt;Camera (built-in or external)&lt;/item&gt;
      &lt;item&gt;Approximately 10MB disk space&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Posturr processes all video data locally on your Mac. No images or data are ever sent to external servers. The camera feed is used solely for posture detection and is never stored or transmitted.&lt;/p&gt;
    &lt;p&gt;MIT License - see LICENSE for details.&lt;/p&gt;
    &lt;p&gt;Contributions are welcome! Please feel free to submit issues and pull requests.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Built with Apple's Vision framework for body pose detection&lt;/item&gt;
      &lt;item&gt;Uses private CoreGraphics API for efficient screen blur&lt;/item&gt;
      &lt;item&gt;Inspired by the need for better posture during long coding sessions&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46754944</guid><pubDate>Sun, 25 Jan 2026 15:34:51 +0000</pubDate></item><item><title>Using PostgreSQL as a Dead Letter Queue for Event-Driven Systems</title><link>https://www.diljitpr.net/blog-post-postgresql-dlq</link><description>&lt;doc fingerprint="3f4c263785a28d80"&gt;
  &lt;main&gt;
    &lt;p&gt;While I was working on a project with Wayfair, I got the opportunity to work on a system that generated daily business reports aggregated from multiple data sources flowing through event streams across Wayfair. At a high level, Kafka consumers listened to these events, hydrated them with additional data by calling downstream services, and finally persisted the enriched events into a durable datastoreâCloudSQL PostgreSQL on GCP.&lt;/p&gt;
    &lt;p&gt;When everything was healthy, the pipeline worked exactly as expected. Events flowed in, got enriched, and were stored reliably. The real challenge started when things went wrong, which, in distributed systems, is not an exception but a certainty.&lt;/p&gt;
    &lt;p&gt;There were multiple failure scenarios we had to deal with. Sometimes the APIs we depended on for hydration were down or slow. Sometimes the consumer itself crashed midway through processing. In other cases, events arrived with missing or malformed fields that could not be processed safely. These were all situations outside our direct control, but they still needed to be handled gracefully.&lt;/p&gt;
    &lt;p&gt;This is where the concept of a Dead Letter Queue came into the picture. Whenever we knew an event could not be processed successfully, instead of dropping it or blocking the entire consumer, we redirected it to a DLQ so it could be inspected and potentially reprocessed later.&lt;/p&gt;
    &lt;p&gt;Our first instinct was to use Kafka itself as a DLQ. While this is a common pattern, it quickly became clear that it wasn't a great fit for our needs. Kafka is excellent for moving data, but once messages land in a DLQ topic, they are not particularly easy to inspect. Querying by failure reason, retrying a specific subset of events, or even answering simple questions like "what failed yesterday and why?" required extra tooling and custom consumers. For a system that powered business-critical daily reports, this lack of visibility was a serious drawback.&lt;/p&gt;
    &lt;p&gt;That's when we decided to treat PostgreSQL itself as the Dead Letter Queue.&lt;/p&gt;
    &lt;p&gt;Instead of publishing failed events to another Kafka topic, we persisted them directly into a DLQ table in PostgreSQL. We were already using CloudSQL as our durable store, so operationally this added very little complexity. Conceptually, it also made failures first-class citizens in the system rather than opaque messages lost in a stream.&lt;/p&gt;
    &lt;p&gt; Whenever an event failed processingâdue to an API failure, consumer crash, schema mismatch, or validation errorâwe stored the raw event payload along with contextual information about the failure. Each record carried a simple status field. When the event first landed in the DLQ, it was marked as &lt;code&gt;PENDING&lt;/code&gt;. Once it was successfully reprocessed, the status was updated to &lt;code&gt;SUCCEEDED&lt;/code&gt;. Keeping the state model intentionally minimal made it easy to reason about the lifecycle of a failed event.
                    &lt;/p&gt;
    &lt;head rend="h3"&gt;DLQ Table Schema and Indexing Strategy&lt;/head&gt;
    &lt;p&gt;To support inspection, retries, and long-term operability, the DLQ table was designed to be simple, query-friendly, and retry-aware.&lt;/p&gt;
    &lt;head rend="h4"&gt;Table Schema&lt;/head&gt;
    &lt;code&gt;CREATE TABLE dlq_events (
    id BIGSERIAL PRIMARY KEY,
    event_type VARCHAR(255) NOT NULL,
    payload JSONB NOT NULL,
    error_reason TEXT NOT NULL,
    error_stacktrace TEXT,
    status VARCHAR(20) NOT NULL, -- PENDING / SUCCEEDED
    retry_count INT NOT NULL DEFAULT 0,
    retry_after TIMESTAMP WITH TIME ZONE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);&lt;/code&gt;
    &lt;head rend="h4"&gt;Key Design Considerations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;payload&lt;/code&gt;is stored as&lt;code&gt;JSONB&lt;/code&gt;to preserve the raw event without enforcing a rigid schema.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;status&lt;/code&gt;keeps the lifecycle simple and explicit.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;retry_after&lt;/code&gt;prevents aggressive retries when downstream systems are unstable.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;retry_count&lt;/code&gt;allows retry limits to be enforced without external state.&lt;/item&gt;
      &lt;item&gt;Timestamps make auditing and operational analysis straightforward.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Indexes&lt;/head&gt;
    &lt;code&gt;CREATE INDEX idx_dlq_status
ON dlq_events (status);

CREATE INDEX idx_dlq_status_retry_after
ON dlq_events (status, retry_after);

CREATE INDEX idx_dlq_event_type
ON dlq_events (event_type);

CREATE INDEX idx_dlq_created_at
ON dlq_events (created_at);&lt;/code&gt;
    &lt;p&gt;These indexes allow the retry scheduler to efficiently locate eligible events while still supporting fast debugging and time-based analysis without full table scans.&lt;/p&gt;
    &lt;head rend="h3"&gt;DLQ Retry Mechanism with ShedLock&lt;/head&gt;
    &lt;p&gt;Persisting failed events solved the visibility problem, but we still needed a safe and reliable way to retry them.&lt;/p&gt;
    &lt;p&gt; For this, we introduced a DLQ retry scheduler backed by ShedLock. The scheduler periodically scans the DLQ table for &lt;code&gt;PENDING&lt;/code&gt; events that are eligible for retry and attempts to process them again. Since the service runs on multiple instances, ShedLock ensures that only one instance executes the retry job at any given time. This eliminates duplicate retries without requiring custom leader-election logic.
                    &lt;/p&gt;
    &lt;head rend="h4"&gt;Retry Configuration&lt;/head&gt;
    &lt;code&gt;dlq:
  retry:
    enabled: true
    max-retries: 240
    batch-size: 50
    fixed-rate: 21600000 # 6 hours in milliseconds&lt;/code&gt;
    &lt;head rend="h4"&gt;How Retries Work&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The scheduler runs every six hours.&lt;/item&gt;
      &lt;item&gt;Up to fifty eligible events are picked up per run.&lt;/item&gt;
      &lt;item&gt;Events exceeding the maximum retry count are skipped.&lt;/item&gt;
      &lt;item&gt;Successful retries immediately transition the event status to &lt;code&gt;SUCCEEDED&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Failures remain in &lt;code&gt;PENDING&lt;/code&gt;and are retried in subsequent runs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Query Implementation&lt;/head&gt;
    &lt;p&gt; The retry scheduler uses a SQL query with &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt; to safely select eligible events across multiple instances. This PostgreSQL feature ensures that even if multiple scheduler instances run simultaneously, each will pick up different rows without blocking each other:
                    &lt;/p&gt;
    &lt;code&gt;@QueryHints(@QueryHint(name = "jakarta.persistence.lock.timeout", value = "-2"))
@Query(
    value = "SELECT * FROM dlq_table "
        + "WHERE messagetype = :messageType "
        + "AND retries &amp;lt; :maxRetries "
        + "AND (replay_status IS NULL OR replay_status NOT IN ('COMPLETED')) "
        + "ORDER BY created_at ASC "
        + "FOR UPDATE SKIP LOCKED",
    nativeQuery = true
)&lt;/code&gt;
    &lt;p&gt; The &lt;code&gt;FOR UPDATE SKIP LOCKED&lt;/code&gt; clause is crucial here. It allows each instance to lock and process different rows concurrently, preventing duplicate processing while maintaining high throughput. The query hint sets the lock timeout to &lt;code&gt;-2&lt;/code&gt;, which means "wait indefinitely" but combined with &lt;code&gt;SKIP LOCKED&lt;/code&gt;, it effectively means "skip any rows that are already locked by another transaction."
                    &lt;/p&gt;
    &lt;p&gt;This setup allowed the system to tolerate long downstream outages while avoiding retry storms and unnecessary load on dependent services.&lt;/p&gt;
    &lt;head rend="h3"&gt;Operational Benefits&lt;/head&gt;
    &lt;p&gt;With this approach, failures became predictable and observable rather than disruptive. Engineers could inspect failures using plain SQL, identify patterns, and reprocess only the events that mattered. If a downstream dependency was unavailable for hours or even days, events safely accumulated in the DLQ and were retried later without human intervention. If an event was fundamentally bad, it stayed visible instead of being silently dropped.&lt;/p&gt;
    &lt;p&gt;Most importantly, this design reduced operational stress. Failures were no longer something to fear; they were an expected part of the system with a clear, auditable recovery path.&lt;/p&gt;
    &lt;head rend="h3"&gt;My Thoughts&lt;/head&gt;
    &lt;p&gt;The goal was never to replace Kafka with PostgreSQL. Kafka remained the backbone for high-throughput event ingestion, while PostgreSQL handled what it does bestâdurability, querying, and observability around failures. By letting each system play to its strengths, we ended up with a pipeline that was resilient, debuggable, and easy to operate.&lt;/p&gt;
    &lt;p&gt;In the end, using PostgreSQL as a Dead Letter Queue turned failure handling into something boring and predictable. And in production systems, boring is exactly what you want.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46755115</guid><pubDate>Sun, 25 Jan 2026 15:51:03 +0000</pubDate></item></channel></rss>