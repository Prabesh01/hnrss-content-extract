<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 12 Jan 2026 14:50:37 +0000</lastBuildDate><item><title>Replit (YC W18) Is Hiring</title><link>https://jobs.ashbyhq.com/replit</link><description>&lt;doc fingerprint="e10fcdab2cdf53e4"&gt;
  &lt;main&gt;
    &lt;p&gt;You need to enable JavaScript to run this app.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46556822</guid><pubDate>Fri, 09 Jan 2026 18:00:56 +0000</pubDate></item><item><title>UpCodes (YC S17) is hiring PMs, SWEs to automate construction compliance</title><link>https://up.codes/careers?utm_source=HN</link><description>&lt;doc fingerprint="f1efd1e76f34fe34"&gt;
  &lt;main&gt;
    &lt;p&gt;Try for Free&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46567458</guid><pubDate>Sat, 10 Jan 2026 17:01:49 +0000</pubDate></item><item><title>iCloud Photos Downloader</title><link>https://github.com/icloud-photos-downloader/icloud_photos_downloader</link><description>&lt;doc fingerprint="a84c7e6c1f381fc0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;!!!! Looking for MAINTAINER for this project !!!!&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A command-line tool to download all your iCloud photos.&lt;/item&gt;
      &lt;item&gt;Works on Linux, Windows, and macOS; laptop, desktop, and NAS&lt;/item&gt;
      &lt;item&gt;Available as an executable for direct downloading and through package managers/ecosystems (Docker, PyPI, AUR, npm)&lt;/item&gt;
      &lt;item&gt;Developed and maintained by volunteers (we are always looking for help).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See Documentation for more details. Also, check Issues&lt;/p&gt;
    &lt;p&gt;We aim to release new versions once a week (Friday), if there is something worth delivering.&lt;/p&gt;
    &lt;p&gt;To make iCloud Photo Downloader work, ensure the iCloud account is configured with the following settings, otherwise Apple Servers will return an ACCESS_DENIED error:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable Access iCloud Data on the Web: On your iPhone / iPad, enable &lt;code&gt;Settings &amp;gt; Apple ID &amp;gt; iCloud &amp;gt; Access iCloud Data on the Web&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Disable Advanced Data Protection: On your iPhone /iPad disable &lt;code&gt;Settings &amp;gt; Apple ID &amp;gt; iCloud &amp;gt; Advanced Data Protection&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are three ways to run &lt;code&gt;icloudpd&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download executable for your platform from the GitHub Release and run it&lt;/item&gt;
      &lt;item&gt;Use package manager to install, update, and, in some cases, run (Docker, PyPI, AUR, npm)&lt;/item&gt;
      &lt;item&gt;Build and run from the source&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See Documentation for more details&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Three modes of operation: &lt;list rend="ul"&gt;&lt;item&gt;Copy - download new photos from iCloud (default mode)&lt;/item&gt;&lt;item&gt;Sync - download new photos from iCloud and delete local files that were removed in iCloud (&lt;code&gt;--auto-delete&lt;/code&gt;option)&lt;/item&gt;&lt;item&gt;Move - download new photos from iCloud and delete photos in iCloud (&lt;code&gt;--keep-icloud-recent-days&lt;/code&gt;option)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Support for Live Photos (image and video as separate files) and RAW images (including RAW+JPEG)&lt;/item&gt;
      &lt;item&gt;Automatic de-duplication of photos with the same name&lt;/item&gt;
      &lt;item&gt;One time download and an option to monitor for iCloud changes continuously (&lt;code&gt;--watch-with-interval&lt;/code&gt;option)&lt;/item&gt;
      &lt;item&gt;Optimizations for incremental runs (&lt;code&gt;--until-found&lt;/code&gt;and&lt;code&gt;--recent&lt;/code&gt;options)&lt;/item&gt;
      &lt;item&gt;Photo metadata (EXIF) updates (&lt;code&gt;--set-exif-datetime&lt;/code&gt;option)&lt;/item&gt;
      &lt;item&gt;... and many more (use &lt;code&gt;--help&lt;/code&gt;option to get full list)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some changes are added to the experimental mode before they graduate into the main package. Details&lt;/p&gt;
    &lt;p&gt;To keep your iCloud photo collection synchronized to your local system:&lt;/p&gt;
    &lt;code&gt;icloudpd --directory /data --username my@email.address --watch-with-interval 3600
&lt;/code&gt;
    &lt;p&gt;Important&lt;/p&gt;
    &lt;p&gt;It is &lt;code&gt;icloudpd&lt;/code&gt;, not &lt;code&gt;icloud&lt;/code&gt; executable&lt;/p&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;Synchronization logic can be adjusted with command-line parameters. Run &lt;code&gt;icloudpd --help&lt;/code&gt; to get full list.&lt;/p&gt;
    &lt;p&gt;To independently create and authorize a session (and complete 2SA/2FA validation if needed) on your local system:&lt;/p&gt;
    &lt;code&gt;icloudpd --username my@email.address --password my_password --auth-only
&lt;/code&gt;
    &lt;p&gt;Tip&lt;/p&gt;
    &lt;p&gt;This feature can also be used to check and verify that the session is still authenticated.&lt;/p&gt;
    &lt;p&gt;Want to contribute to iCloud Photos Downloader? Awesome! Check out the contributing guidelines to get involved.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46578921</guid><pubDate>Sun, 11 Jan 2026 19:23:58 +0000</pubDate></item><item><title>Sampling at negative temperature</title><link>https://cavendishlabs.org/blog/negative-temperature/</link><description>&lt;doc fingerprint="1332aa96030a4fae"&gt;
  &lt;main&gt;
    &lt;p&gt;Summary: Inspired by the definition of temperature in statistical mechanics and the possibility for it to be below zero, we try sampling LLaMA at . The results are maximally weird.&lt;/p&gt;
    &lt;p&gt;The notion of temperature comes from statistical mechanics. Consider a system that has states with energies . If the system is in thermal equilibrium, the probability distribution over states is given by the Boltzmann distribution:&lt;/p&gt;
    &lt;p&gt;The distribution is parameterized by a single number, the temperature . At lower temperatures the lowest-energy states predominate; at higher temperatures there is a more even mix.&lt;/p&gt;
    &lt;p&gt;At the last layer of a neural net, we apply the softmax function to the neuron activations to get a vector of probabilities that sum to 1:&lt;/p&gt;
    &lt;p&gt;Wait â this is just the Boltzmann distribution, up to a constant![1]&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[1]&lt;/cell&gt;
        &lt;cell&gt;There's no minus sign in the exponent because while higher-energy states are less likely, larger logits are more likely.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In a language model, temperature is used to define how creative text generations are. For instance, in the zero temperature limit, the model should deterministically generate the most likely token. In the infinite temperature limit, all tokens are equally likely and the model output will be random noise. For an interactive explanation, see here.&lt;/p&gt;
    &lt;p&gt;What would it mean to have a temperature that is below zero? (This isn't the same as the negative Fahrenheit or Celsius temperatures we get on a cold day in Vermont â I mean below zero on an absolute scale like Kelvin).&lt;/p&gt;
    &lt;p&gt;Looking at the equations above, if then the sign of the exponent flips. That means that the states that were previously the least likely are now the most likely, and vice versa. As temperature approaches zero from the negative side, the model output will again be deterministic â but this time, the least likely tokens will be output.&lt;/p&gt;
    &lt;p&gt;Most physical systems have an infinite number of possible states at increasingly higher energy levels. As such, there is no least likely state. So negative temperatures really only make sense in systems with a finite state space. That includes neural nets â there are a finite number of neurons in the last layer.&lt;/p&gt;
    &lt;p&gt;Unfortunately, OpenAI models only allow sampling with temperatures between 0.0 and 2.0. So if we want to try this, we need a language model we can run locally. We'll use Meta's LLaMA model with llama.cpp.&lt;/p&gt;
    &lt;p&gt;Below is the function that is used to sample tokens in &lt;code&gt;llama.cpp&lt;/code&gt;, slightly simplified for ease of understanding:&lt;/p&gt;
    &lt;code&gt;void llama_sample_temperature(llama_token_data_array * candidates_p, float temp) {
    for (size_t i = 0; i &amp;lt; candidates_p-&amp;gt;size; ++i) {
        candidates_p-&amp;gt;data[i].logit /= temp;
    }
}&lt;/code&gt;
    &lt;p&gt;So we can just pass in &lt;code&gt;--temperature -0.001&lt;/code&gt;? Not quite; in &lt;code&gt;examples/main/main.cpp&lt;/code&gt; there is a check that will apply greedy (most-likely) sampling for any temperatures less than or equal to zero. Applying the following diff and recompiling, we're good to go:&lt;/p&gt;
    &lt;code&gt;@@ -486,7 +486,7 @@ int main(int argc, char ** argv) {
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  logits[llama_token_nl()] = nl_logit;
Â Â  Â  Â  Â  Â  Â  Â  Â  }
-Â  Â  Â  Â  Â  Â  Â  Â  if (temp &amp;lt;= 0) {
+Â  Â  Â  Â  Â  Â  Â  Â  if (temp == 0) {
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  // Greedy sampling
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  id = llama_sample_token_greedy(ctx, &amp;amp;candidates_p);
Â Â  Â  Â  Â  Â  Â  Â  Â  } else {&lt;/code&gt;
    &lt;p&gt;We will also want to disable repetition penalty, top-k, and top-p sampling. Here's the command we'll run: &lt;code&gt;./main -m models/7B/ggml-model-q4_0.bin --temp -0.001 --repeat-penalty 1.0 --top-k 0 --top-p 1.0 -p "Temperature is a concept"&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;When running this prompt at , here is the output:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Temperature is a concept that is used to describe the degree of hotness or coldness of a substance. The temperature of a substance is measured by the kinetic energy of its molecules. The higher the temperature of a substance, the more kinetic energy its molecules have.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Now, running it at :&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Temperature is a concept Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ°&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This means that &lt;code&gt;Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ°&lt;/code&gt; is the least likely token to follow this prompt. After one token, the program seems to hang. Perhaps the LLM is trying to speak an unspeakable sentence?&lt;/p&gt;
    &lt;p&gt;Let's try it with LLaMA-13B. At :&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Temperature is a concept that is used to describe the degree of hotness or coldness of a substance. The temperature of a substance is measured by a thermometer.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;At (which should be close to entirely random):&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Temperature is a concept fixesÐ»Ñ conven Beng aer situation ton '\Cr villa known vide among entities Ukraine keepsæ°´ Newton Betty Mih ÑÑÐ· killedÑÐµÐ»ÑÑÑÐ²Ð¾ Victoria WeekRefreshbodyunct vir Ja ÐÐµÐ¼Ð¾ remporte settembre excell succeed fitted))); moy PC highlight located Referencias extendsconfigure\\ incidentWilajagateÑÐ¸Ñ bibli journalist rec cont sovientlyilleryæ finishingáªæ¿ rotationintonosti orbiteditor&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And at :&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Temperature is a concept]&amp;amp; ]{'archividonnÃ©es Kontrolaâ¤ Kontrola AuÃer Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° costa Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° Mitchell ez entfernesterd bidZyg entferne osc accom BegriffsklÃ¤rÐ»Ð¸ÑÑ Bedeut WendarchiviicanINCTpenastown Krieg Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° loyal vallIAL listade GemeinsBUGiskoshiftpenas ligapenas Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ°isko jÃº Marian Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° governor(* Kontrolapenasouw entferne Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° Dic hornÑÐµÐ¼ earliestÐ°Ð½ÑÐ°penas Promiseriatrarout23;'archividonnÃ©es Kontrolaâ¤ Kontrola AuÃer Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° costa Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° Mitchell ez entfernesterd bidZyg entferne osc accom BegriffsklÃ¤rÐ»Ð¸ÑÑ Bedeut WendarchiviicanINCTpenastown Krieg Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° loyal vallIAL listade GemeinsBUGiskoshiftpenas ligapenas Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ°isko jÃº Marian Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° governor(* Kontrolapenasouw entferne Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ° Dic hornÑÐµÐ¼ earliestÐ°Ð½ÑÐ°penas Promiseriatrarout&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The generation continues on and on. At first glance it seems random, but in a sense it should be even less comprehensible than a random sequence of tokens! There are repeated words, sure, but the generation is engineered to be as unlikely as possible.&lt;/p&gt;
    &lt;p&gt;Why are certain tokens like &lt;code&gt;Ð¥ÑÐ¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑÐ°&lt;/code&gt; and &lt;code&gt;entferne&lt;/code&gt; repeated? Searching these words, I found this comment by scottviteri on LessWrong. He points out that these are some of the tokens that are closest to the centroid in LLaMA's embedding space. That means that LLaMA has very little idea what these tokens mean. In the case of ChatGPT, tokens near the centroid have anomalous properties, so these tokens are likely to have similar effects on LLaMA. Let's give that a try, at .&lt;/p&gt;
    &lt;p&gt;LLaMA is perfectly capable of repeating most words, even nonsense ones:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Human: Repeat the word " antferne".&lt;/p&gt;&lt;lb/&gt;Assistant: Okay, I will repeat the word " antferne".&lt;/quote&gt;
    &lt;p&gt;But is incapable of outputting this anomalous token:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Human: Repeat the word " entferne".&lt;/p&gt;&lt;lb/&gt;Assistant: Okay, I will repeat the word "get".&lt;/quote&gt;
    &lt;p&gt;The anomalous tokens that are the most likely completions at negative temperatures are the least likely completions at positive temperatures, so much so that the model refuses to generate them even in cases where they would be appropriate.&lt;/p&gt;
    &lt;p&gt;If you'd like to cite this article, you can use this:&lt;/p&gt;
    &lt;quote&gt;@misc{Kauffman2023negative-temperature, author = "Derik Kauffman", title = "Sampling at negative temperature", year = 2023, howpublished = "Blog post", url = "https://cavendishlabs.org/blog/negative-temperature/" }&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46579374</guid><pubDate>Sun, 11 Jan 2026 20:01:14 +0000</pubDate></item><item><title>The struggle of resizing windows on macOS Tahoe</title><link>https://noheger.at/blog/2026/01/11/the-struggle-of-resizing-windows-on-macos-tahoe/</link><description>&lt;doc fingerprint="164e58dfa3c7bbf5"&gt;
  &lt;main&gt;
    &lt;p&gt;A lot has already been said about the absurdly large corner radius of windows on macOS Tahoe. People are calling the way it looks comical, like a child’s toy, or downright insane.&lt;/p&gt;
    &lt;p&gt;Setting all the aesthetic issues aside – which are to some extent a matter of taste – it also comes at a cost in terms of usability.&lt;/p&gt;
    &lt;p&gt;Since upgrading to macOS Tahoe, I’ve noticed that quite often my attempts to resize a window are failing.&lt;/p&gt;
    &lt;p&gt;This never happened to me before in almost 40 years of using computers. So why all of a sudden?&lt;/p&gt;
    &lt;p&gt;It turns out that my initial click in the window corner instinctively happens in an area where the window doesn’t respond to it. The window expects this click to happen in an area of 19 × 19 pixels, located near the window corner.&lt;/p&gt;
    &lt;p&gt;If the window had no rounded corners at all, 62% of that area would lie inside the window:&lt;/p&gt;
    &lt;p&gt;But due to the huge corner radius in Tahoe, most of it – about 75% – now lies outside the window:&lt;/p&gt;
    &lt;p&gt;Living on this planet for quite a few decades, I have learned that it rarely works to grab things if you don’t actually touch them:&lt;/p&gt;
    &lt;p&gt;So I instinctively try to grab the window corner inside the window, typically somewhere in that green area, near the blue dot:&lt;/p&gt;
    &lt;p&gt;And I assume that most people would also intuitively expect to be able to grab the corner there. But no, that’s already outside the accepted target area:&lt;/p&gt;
    &lt;p&gt;So, for example, grabbing it here does not work:&lt;/p&gt;
    &lt;p&gt;But guess what – grabbing it here does:&lt;/p&gt;
    &lt;p&gt;So in the end, the most reliable way to resize a window in Tahoe is to grab it outside the corner – a gesture that feels unnatural and unintuitive, and is therefore inevitably error-prone.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46579864</guid><pubDate>Sun, 11 Jan 2026 20:47:55 +0000</pubDate></item><item><title>FUSE is All You Need – Giving agents access to anything via filesystems</title><link>https://jakobemmerling.de/posts/fuse-is-all-you-need/</link><description>&lt;doc fingerprint="678b289beab4cee0"&gt;
  &lt;main&gt;
    &lt;p&gt;Giving agents access to a sandboxed environment with a shell and a filesystem has been the latest hype when it comes to agentic harnesses. Recent examples of this include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Turso’s AgentFS&lt;/item&gt;
      &lt;item&gt;Anthropic’s Agent SDK, which brings Claude Code’s harness to non-coding domains&lt;/item&gt;
      &lt;item&gt;Vercel rebuilding their text-to-SQL agent on top of a sandbox&lt;/item&gt;
      &lt;item&gt;Anthropic’s Agent Skills for filesystem-based progressive disclosure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The argument for why this is good goes something like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The big labs are doing heavy RL for coding tasks in these kinds of environments. Aligning more closely with such a harness brings free gains from the coding domain to other problem spaces.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Beyond that, replacing a bunch of search/write/move/list tools with a single Bash tool reduces the tool space significantly. Agents can chain operations together intuitively. Unix paradigms give you good tool design for free.&lt;/p&gt;
    &lt;p&gt;On top of that there is more nice patterns that emerge from having a filesystem, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Plan/scratch files: Agents can create temporary files to organize their thoughts, track progress, or store intermediate results. This emerges naturally from having filesystem access, no need to design a separate “notepad” tool.&lt;/item&gt;
      &lt;item&gt;Long context handling: As conversations grow, you can compact old messages and tool results into files on the filesystem. The agent can re-read them when needed rather than keeping everything in context.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So the advantages are clear. But how do you actually apply this to your domain? This is where things get tricky. Consider two examples:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;A domain with parallels to filesystems: For example an agent for organizing emails. There are folders, items (emails), and you can browse them and move things around.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;An existing platform that already looks like a filesystem: For example an agent inside Google Drive.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When you try to fit these into a sandboxed filesystem you might wonder:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;When do I copy over the files?&lt;/item&gt;
      &lt;item&gt;Do I copy over everything?&lt;/item&gt;
      &lt;item&gt;How do I write back updates that the agent made?&lt;/item&gt;
      &lt;item&gt;When do I sync edits made by a human?&lt;/item&gt;
      &lt;item&gt;How do I show folders/files progressively as the agent progresses? (Particularly useful if you want to lean into the filesystem as a tool to manage long context.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A reply to a recent post on X from Vercel about building filesystem-based agents puts it well:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This article doesn’t really answer that question though.&lt;/p&gt;
      &lt;p&gt;They explain the shape of the filesystem really well, but the missing piece is still the main question for existing apps: how do you actually materialize Postgres + object storage + APIs into that sandbox FS in a practical and scalable way? “Preload files” is hand-wavy without a recommended ingestion/snapshot pattern.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This post will show a pattern to close that gap: turning anything into a filesystem with the help of FUSE.&lt;/p&gt;
    &lt;head rend="h2"&gt;An example: Email agent&lt;/head&gt;
    &lt;p&gt;To make this tangible, let’s build a fictional agent.&lt;/p&gt;
    &lt;p&gt;Imagine we’re building an AI-powered email platform, think Gmail but with AI on top. We have our own database storing emails, folders, and contacts. We have a UI that users interact with.&lt;/p&gt;
    &lt;p&gt;Now we want to add a powerful agent on top of that platform. We could build a simple agent loop with tools like &lt;code&gt;list-emails&lt;/code&gt;, &lt;code&gt;move-email&lt;/code&gt;, &lt;code&gt;read-email&lt;/code&gt;, but as we established there’s a more ergonomic approach: put our emails in a filesystem.&lt;/p&gt;
    &lt;p&gt;After some thinking, we come up with the following file layout for the agent’s sandbox:&lt;/p&gt;
    &lt;code&gt;workspace/
  Inbox/
    Customer Inquiry - Size Chart (emily.wolfe@gmail.com)
    PO Confirmation #2026-0038 (supplierC@fabricdirect.com)
    Shipping Delay Notice (supplierA@logisticsco.com)
    Partnership Proposal - TrendCircle (marketing@trendcircle.io)

  Starred/
    Wholesale Inquiry - SilverLeaf Boutique (boutique@silverleaf.co)
    Re: Restock ETA For Red Jackets (supplierA@logisticsco.com)

  Needs_Action/
    Missing Documents - Customs (carrier-support@airbridge.co)
    Last Call: VAT Filing Deadline (gov-tax@tradeportal.gov)

  Orders/
    2026/
      Feb/
        PO Confirmation #2026-0038 (supplierC@fabricdirect.com)
        Shipping Delay Notice (supplierA@logisticsco.com)

  Customers/
    Returns/
      Return Request #8743 (john.hartley@yahoo.com)

  Sent/
    Re: Customer Inquiry - Size Chart (emily.wolfe@gmail.com)
&lt;/code&gt;
    &lt;p&gt;Now comes the problem: we need to bridge the gap from our database to the sandboxed filesystem that the agent uses to consume our data.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to turn anything into a FS, enter FUSE&lt;/head&gt;
    &lt;p&gt;FUSE stands for Filesystem in Userspace. It’s a framework that lets you implement filesystems outside the kernel, while the kernel makes it look like a ‘real’ filesystem to the rest of Linux. You just write a userland process that implements the actual filesystem logic.&lt;/p&gt;
    &lt;p&gt;A FUSE filesystem needs to implement a small, well-defined interface: &lt;code&gt;lookup&lt;/code&gt;, &lt;code&gt;open&lt;/code&gt;, &lt;code&gt;read&lt;/code&gt;, &lt;code&gt;write&lt;/code&gt;, &lt;code&gt;readdir&lt;/code&gt;, &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;unlink&lt;/code&gt;, etc. Whenever the OS interacts with the mounted filesystem, operations get forwarded to your userspace implementation. This indirection makes it possible to expose arbitrary data structures as if they were files.&lt;/p&gt;
    &lt;p&gt;There are wrappers and bindings for higher-level languages that let you write these filesystems in the language and tech stack of your choice. No need to dive into C :)&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;Coming back to our email agent, here’s how FUSE would fit into a very high-level architecture:&lt;/p&gt;
    &lt;p&gt;The top row shows the traditional path: users interact with your app through a UI that talks to a backend, which reads and writes to a database.&lt;/p&gt;
    &lt;p&gt;The bottom row shows the agent path: the agent runs inside a sandbox and sees a mounted filesystem. When it does &lt;code&gt;ls /workspace/Inbox&lt;/code&gt;, that call hits the FUSE layer, which translates it into a query against the same backend. The agent and your users are looking at the same data through different interfaces.&lt;/p&gt;
    &lt;p&gt;You can think of the filesystem as a UI layer for the agent. Just like your frontend decides how to present data to users, your FUSE implementation decides how to present data to the agent. Realistically you’ll have additional tools for actions that don’t map well to filesystem operations (like sending an email), but for browsing, reading, and organizing data, the filesystem abstraction works great.&lt;/p&gt;
    &lt;p&gt;For this blog post we’ll keep things simple: the agent loop and FUSE implementation both run inside a Docker container, talking directly to a database. We are going to implement this in Typescript using the fuse-native library. In production you’d want to structure things a bit differently, like running the agent loop outside the sandbox and have your FUSE bindings be only a thin client that calls out to your backend with proper authentication for example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Implementing the filesystem operations&lt;/head&gt;
    &lt;p&gt;To make our data accessible via the filesystem, we need to implement the subset of FUSE operations relevant for our use case. Let’s start with listing directories (&lt;code&gt;readdir&lt;/code&gt;):&lt;/p&gt;
    &lt;code&gt;export async function readdir(path: string, cb: (err: number, names?: string[]) =&amp;gt; void) {
  const [folder] = await db.select().from(foldersTable).where(eq(foldersTable.path, path));

  if (!folder) {
    return cb(Fuse.ENOENT);
  }

  const emailsInFolder = await db.select({
    email: emailsTable,
    sender: contactsTable,
  }).from(emailsTable)
  .leftJoin(contactsTable, eq(emailsTable.sender, contactsTable.id))
  .where(eq(emailsTable.folderId, folder.id));


  const entries = new Set&amp;lt;string&amp;gt;();

  for (const {email, sender} of emailsInFolder) {
    entries.add(`${email.subject} (${sender.email}).eml`);
  }

  const subfolders = await db.select().from(foldersTable).where(like(foldersTable.path, sql`${path}/%`));
  for (const subfolder of subfolders) {
    entries.add(subfolder.path.split("/").pop() || "");
  }

  cb(0, Array.from(entries));
}
&lt;/code&gt;
    &lt;p&gt;We match which email folder we’re in based on the filesystem path, list all emails inside that folder, and add any subfolders. Together with implementing &lt;code&gt;getattr&lt;/code&gt; (to get file attributes), this makes listing and navigating the folder tree via &lt;code&gt;ls&lt;/code&gt; work.&lt;/p&gt;
    &lt;p&gt;To choose how email content will be represented we implement &lt;code&gt;read&lt;/code&gt; like this:&lt;/p&gt;
    &lt;code&gt;export async function read(path: string, fd: number, buf: Buffer, len: number, pos: number, cb: (err: number) =&amp;gt; void) {
  const email = await getEmailById(fd);
  if (!email) {
    return cb(Fuse.ENOENT);
  }
  const content = emailToContent(email);
  const slice = content.slice(pos, pos + len);
  const bytesRead = buf.write(slice);
  cb(bytesRead);
}

export function emailToContent(email: EmailWithSender): string {
  return `From: ${email.senderEmail}
Date: ${email.sentAt.toISOString()}
Subject: ${email.subject}
X-Folder: ${email.folderPath}
X-Starred: ${email.starred}
X-Needs-Action: ${email.needsAction}

${email.body}
`;
}
&lt;/code&gt;
    &lt;p&gt;There are a few more operations I implemented like &lt;code&gt;open&lt;/code&gt;, &lt;code&gt;mkdir&lt;/code&gt;, and &lt;code&gt;rename&lt;/code&gt; to allow moving email files around and reassigning folder IDs in the database.&lt;/p&gt;
    &lt;p&gt;On top of that, I introduced the concept of virtual folders—the &lt;code&gt;Starred&lt;/code&gt; and &lt;code&gt;Needs_Action&lt;/code&gt; folders in the layout above. They work like this: if an email has the starred or needsAction flag set, it appears in these folders. Because an email can be both starred and part of an actual folder, the Starred folder contains symlinks to the “actual” files. The agent can unlink these files or create new links to flip attributes on the email. This might be overkill and better covered via regular tools, but I wanted to see how far I could lean into the filesystem abstraction. For this to work, I implemented additional FUSE operations: &lt;code&gt;readlink&lt;/code&gt;, &lt;code&gt;symlink&lt;/code&gt;, &lt;code&gt;unlink&lt;/code&gt;, and adjusted the existing ones.&lt;/p&gt;
    &lt;p&gt;Deciding how to map your domain to a filesystem is really a context engineering challenge in the end. The key is finding the right balance between leveraging familiar filesystem semantics and not over-abstracting to the point where the mapping becomes confusing.&lt;/p&gt;
    &lt;p&gt;With all the filesystem operations defined, we’re ready to mount it and see it in action. While it’s possible to get this working on a Mac via macFUSE, I chose the easier path and spun up a Docker container and mounted the filesystem there.&lt;/p&gt;
    &lt;p&gt;Here’s a quick demo of navigating the filesystem. As you can see, it looks and works just like regular files.&lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;mv&lt;/code&gt;, &lt;code&gt;ls&lt;/code&gt; all behave as expected. The best part? There’s no risk of anything going out of sync because we read directly from the database. We also only load the things we needed when they are accessed instead of preloading our entire database into a sandbox.&lt;/p&gt;
    &lt;head rend="h2"&gt;Giving the agent access to it&lt;/head&gt;
    &lt;p&gt;Now that we have the filesystem working, it’s time to hook up an agent and see how it performs. I used the Anthropic Agent SDK to whip up a quick filesystem agent with tools to run Bash, Read, Glob, etc. To give the agent some context about the filesystem, I added the following system prompt:&lt;/p&gt;
    &lt;code&gt;You are an email assistant helping the user manage their inbox.

You have access to a filesystem at /workspace representing their email:

- Folders: Inbox, Sent, Orders, Customers, etc.
- Emails: .eml files named "Subject (sender@email.com).eml"
- Filenames contain spaces (from email subjects), so always quote paths in shell commands
- Starred/Needs_Action: contain symlinks to flagged emails

Commands you use internally:
- ls, cat, find → browse emails
- mv → move emails between folders
- ln -s &amp;lt;email&amp;gt; /Starred/ → star an email
- rm /Starred/&amp;lt;email&amp;gt; → unstar an email
- mkdir → create folders

When responding to the user:
- Describe emails by subject and sender, not filenames
- Say "starred" not "created symlink"
- Say "moved to Orders" not "mv to /Orders/"
- Summarize what you found/did in plain language
&lt;/code&gt;
    &lt;p&gt;Let’s see how it approaches some common user queries:&lt;/p&gt;
    &lt;code&gt;&amp;gt; which emails do i have in my Inbox?

I'll check your Inbox to see what emails you have.

[Bash] ls -la /workspace/Inbox/

You have 6 emails in your Inbox:

1. **"Missing Documents - Customs"** from carrier-support@airbridge.co (Jan 4)
2. **"PO Confirmation #2026-0038"** from supplierC@fabricdirect.com (Jan 9)
3. **"Partnership Proposal - TrendCircle"** from marketing@trendcircle.io (Jan 7)
4. **"Re: Restock ETA For Red Jackets"** from supplierA@logisticsco.com (Jan 5)
5. **"Shipping Delay Notice"** from supplierA@logisticsco.com (Jan 8)
6. **"Wholesale Inquiry - SilverLeaf Boutique"** from boutique@silverleaf.co (Jan 6)

Would you like me to read any of these emails or help you organize them?
&lt;/code&gt;
    &lt;code&gt;&amp;gt; find the email about that wholesale inquiry, i need to remember to follow up on it

I'll help you find the wholesale inquiry email. Let me search for it.

[Bash] find /workspace -type f -name "*.eml" -exec grep -l "wholesale" {} \;

Found it! There's a wholesale inquiry from SilverLeaf Boutique (boutique@silverleaf.co) in your Inbox.

Would you like me to:
1. Star it so you can easily find it later to follow up?
2. Move it to a specific folder?
3. Show you the email content?

&amp;gt; show me the email and star it

[Read] /workspace/Inbox/Wholesale Inquiry - SilverLeaf Boutique (boutique@silverleaf.co).eml

[Bash] ln -s "/workspace/Inbox/Wholesale Inquiry - SilverLeaf Boutique (boutique@silverleaf.co).eml" /workspace/Starred/

Here's the email:

**From:** boutique@silverleaf.co
**Date:** January 6, 2026
**Subject:** Wholesale Inquiry - SilverLeaf Boutique

**Message:**
"We're a boutique in Portland and interested in carrying your line. What are your wholesale terms?"

Good news - this email is already starred! It looks like it was already flagged for follow-up. SilverLeaf Boutique is interested in carrying your products and wants to know about your wholesale terms.
&lt;/code&gt;
    &lt;code&gt;&amp;gt; Organize my Inbox

I'll help you organize your inbox. Let me first see what emails you have there.

[Bash] ls -la /workspace/Inbox/

Now let me check what folders you already have set up:

[Bash] ls -d /workspace/*/

Perfect! I can see you have 8 emails in your inbox. Let me organize them into appropriate folders:

[Bash] mv "/workspace/Inbox/Customer Inquiry - Size Chart (emily.wolfe@gmail.com).eml" /workspace/Customers/
[Bash] mv "/workspace/Inbox/Wholesale Inquiry - SilverLeaf Boutique (boutique@silverleaf.co).eml" /workspace/Customers/
[Bash] mv "/workspace/Inbox/PO Confirmation #2026-0038 (supplierC@fabricdirect.com).eml" /workspace/Orders/
[Bash] mv "/workspace/Inbox/Re: Restock ETA For Red Jackets (supplierA@logisticsco.com).eml" /workspace/Orders/
[Bash] mv "/workspace/Inbox/Shipping Delay Notice (supplierA@logisticsco.com).eml" /workspace/Orders/
Now let me flag the urgent items that need action:
[Bash] ln -s "/workspace/Inbox/Last Call: VAT Filing Deadline (gov-tax@tradeportal.gov).eml" /workspace/Needs_Action/
[Bash] ln -s "/workspace/Inbox/Missing Documents - Customs (carrier-support@airbridge.co).eml" /workspace/Needs_Action/

Good - those urgent items are already flagged. Now let me check if the partnership proposal might be something to consider:

[Bash] cat "/workspace/Inbox/Partnership Proposal - TrendCircle (marketing@trendcircle.io).eml"

Let me create a folder for business opportunities and move the partnership proposal there:

[Bash] mkdir -p /workspace/Business_Opportunities &amp;amp;&amp;amp; mv "/workspace/Inbox/Partnership Proposal - TrendCircle (marketing@trendcircle.io).eml" /workspace/Business_Opportunities/

Let me verify your inbox is now organized:

[Bash] ls /workspace/Inbox/

Perfect! I've organized your inbox. Here's what I did:

**Moved to Customers folder (2 emails):**
- Customer Inquiry - Size Chart from emily.wolfe@gmail.com
- Wholesale Inquiry - SilverLeaf Boutique from boutique@silverleaf.co

**Moved to Orders folder (3 emails):**
- PO Confirmation #2026-0038 from supplierC@fabricdirect.com
- Re: Restock ETA For Red Jackets from supplierA@logisticsco.com
- Shipping Delay Notice from supplierA@logisticsco.com

**Moved to new Business_Opportunities folder (1 email):**
- Partnership Proposal - TrendCircle from marketing@trendcircle.io

**Left in Inbox (2 urgent items - already flagged):**
- Last Call: VAT Filing Deadline from gov-tax@tradeportal.gov ⚠️
- Missing Documents - Customs from carrier-support@airbridge.co ⚠️

The two remaining emails in your inbox are urgent items that need your immediate attention - they're already flagged in your Needs_Action folder.
&lt;/code&gt;
    &lt;p&gt;It’s pretty cool to see how the agent just navigates through the filesystem given the task and a minimal system prompt. We didn’t spend any time on tool design. We just mapped our domain to a filesystem and got intuitive tool behavior for free.&lt;/p&gt;
    &lt;head rend="h2"&gt;Outlook&lt;/head&gt;
    &lt;p&gt;You can find the full implementation on GitHub: https://github.com/Jakob-em/agent-fuse to play around with yourself. I also might write a follow up to this post about evaling this approach vs a traditional agent loop to see if this way of building agents brings any measurable gains.&lt;/p&gt;
    &lt;p&gt;In general there is a lot more potential for virtual filesystems for context engineering. You could include old conversations as files on the filesystem, or make old tool call results appear as files after compacting context. The possibilities are endless, and as long as coding agents remain relevant, this way of designing context will only get more ergonomic for models.&lt;/p&gt;
    &lt;p&gt;My prediction is that one of the many sandbox providers will come up with a nice API on top of this that lets you do something like:&lt;/p&gt;
    &lt;code&gt;new Agent({
    tool: [...]
	sandbox: {
        filesystem: {
            '/emails': (folder) =&amp;gt; listEmails(folder),
            '/old_conversations': () =&amp;gt; listOldConversations(),
        }
	 }
    }
})
&lt;/code&gt;
    &lt;p&gt;No worrying about FUSE, the sandbox, where things are executed, etc. This will be a huge differentiator and make virtual filesystems easily accessible to everyone.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46580136</guid><pubDate>Sun, 11 Jan 2026 21:12:45 +0000</pubDate></item><item><title>CLI agents make self-hosting on a home server easier and fun</title><link>https://fulghum.io/self-hosting</link><description>&lt;doc fingerprint="762701d57926a862"&gt;
  &lt;main&gt;
    &lt;p&gt;CLI agents make self-hosting on a home server easier and fun&lt;/p&gt;
    &lt;p&gt;by Jordan Fulghum, January 2026&lt;/p&gt;
    &lt;p&gt;I've wanted to self-host at home for years, but I always bounced off it - too much time spent configuring instead of using. It just wasn't fun.&lt;/p&gt;
    &lt;p&gt;That changed recently, because CLI agents like Claude Code make self-hosting on a cheapo home server dramatically easier and actually fun.&lt;/p&gt;
    &lt;p&gt;This is the first time I would recommend it to normie/software-literate people who never really wanted to sign up to become a sysadmin and stress about uptime of core personal services.&lt;/p&gt;
    &lt;p&gt;Three things converged:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Cheap, capable mini PCs&lt;/cell&gt;
        &lt;cell&gt;You can buy a silent, low-power Linux box for less than $500&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Tailscale&lt;/cell&gt;
        &lt;cell&gt;Simple, secure, private networking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Claude Code&lt;/cell&gt;
        &lt;cell&gt;Install the CLI agent directly on the box, and you no longer need to remember or look-up Docker syntax, Compose quirks, or config formats&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So instead of Googling things like “docker compose vaultwarden caddy reverse proxy” and piecing together a bunch of old blog posts, I just ran Claude Code directly on the machine and let it sort it out.&lt;/p&gt;
    &lt;p&gt;I previously ran my Plex server on an M1 Mac mini, which was great, but as I wanted to add more services I found myself running a lot of resource-hungry VMs (via UTM) and it was getting complicated anytime the Mac rebooted. So, I picked up a Beelink Mini N150. It is small, quiet, and just barely sips power. I paid around $379 for the device and another few hundred USD for 8TB in NVMe SSD. It's pretty wild how accessible these mini PCs have become in recent years!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Install Linux&lt;/cell&gt;
        &lt;cell&gt;Flash USB, install Ubuntu Server (I picked 22.04 LTS)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Install Tailscale&lt;/cell&gt;
        &lt;cell&gt;Get it on your private network to make your life easier&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SSH in&lt;/cell&gt;
        &lt;cell&gt;From my laptop, anywhere&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Install Claude Code&lt;/cell&gt;
        &lt;cell&gt;On the server itself&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ask for what I want&lt;/cell&gt;
        &lt;cell&gt;Go get a coffee&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I've been using Claude Code and other agentic CLIs for my day-to-day development, but as others are realizing, they are generalized computer agents and native to the terminal.&lt;/p&gt;
    &lt;p&gt;I installed Claude Code directly on the Linux box. Then I asked it things like:&lt;/p&gt;
    &lt;p&gt;I explained the outcome I wanted and let Claude Code take care of the details and implementation.&lt;/p&gt;
    &lt;p&gt;I focused on things I already used, but wanted more control over - effectively starting to knock down the walled garden around my core services like passwords, photos, media.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Service&lt;/cell&gt;
        &lt;cell role="head"&gt;What it replaces or does&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Vaultwarden&lt;/cell&gt;
        &lt;cell&gt;Bitwarden, but self-hosted and fast&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Plex&lt;/cell&gt;
        &lt;cell&gt;Media server - PSA get Plex Pass to unlock hardware-accelerated transcoding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Immich&lt;/cell&gt;
        &lt;cell&gt;Google Photos replacement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Uptime Kuma&lt;/cell&gt;
        &lt;cell&gt;Simple service monitoring&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Caddy&lt;/cell&gt;
        &lt;cell&gt;Reverse proxy with automatic TLS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Home Assistant&lt;/cell&gt;
        &lt;cell&gt;Home automation hub&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Readeck&lt;/cell&gt;
        &lt;cell&gt;Read-it-later. Honestly blown away by this one!&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Each one lives in its own container.&lt;/p&gt;
    &lt;p&gt;I can access everything from my phone, laptop, and tablet like it is local.&lt;/p&gt;
    &lt;p&gt;When something goes down, I get an email. When it comes back up, another email.&lt;/p&gt;
    &lt;p&gt;Vaultwarden is a Bitwarden-compatible password manager written in Rust. Lightweight, reliable, and you can use the existing Bitwarden clients (like native apps and browser extensions). You can even set it as the default password manager on iOS, at the OS level!&lt;/p&gt;
    &lt;p&gt;Once that was running, I exported my passwords from iCloud/Keychain, imported them easily into Vaultwarden, and haven't looked back since.&lt;/p&gt;
    &lt;p&gt;Immich is a Google Photos replacement. I thought I'd have to compromise and flinched a bit when I installed it. But nope, it's good. Mobile apps. Face recognition via a local (but slow) machine learning thread. Timeline and map view. Automatic uploads from your photo roll.&lt;/p&gt;
    &lt;p&gt;This is the kind of thing that used to feel fragile and half-baked when self-hosted. It does not anymore.&lt;/p&gt;
    &lt;p&gt;Mozilla killed Pocket. I needed a replacement.&lt;/p&gt;
    &lt;p&gt;I took a bet on Readeck. The UI is genuinely good. Clean typography, nice reading experience, good mobile support. It always remembers where I stopped reading and takes me right there. I even set up a shortcut that allows me to save an article for later right from mobile Firefox. Awesome.&lt;/p&gt;
    &lt;p&gt;This is exactly the kind of thing self-hosting is perfect for. A small, personal tool that you actually use every day.&lt;/p&gt;
    &lt;p&gt;I initially installed Lazydocker and Glances for monitoring. Uptime Kuma works pretty good. Still, it's so easy now to ask: why not do better? I asked Claude to build a single-page monitoring dashboard. It one-shotted it, building a local web app in Go and Svelte (what do I care?) that monitors the status and links out to each service.&lt;/p&gt;
    &lt;p&gt;Next, I have always wanted a redundant local + remote backup system for my irreplaceable media - namely my photos going back about 25 years now. I plugged in an old USB drive to the Beelink, told Claude to backup to the drive daily, and it was done seconds later.&lt;/p&gt;
    &lt;p&gt;Finally, I created a new IAM user in the AWS dashboard, installed the AWS CLI on the server, and asked Claude to backup to S3 weekly. On its own, it suggested Glacier Deep storage, which would run me about twenty cents per month for half a terabyte of photos. A minute or two later, I had remote backups, complete with monitoring + logs from the dashboard.&lt;/p&gt;
    &lt;p&gt;The feeling of ownership is powerful, but a bit hard to describe. I think you just have to try it, and I hope you get a strong feeling of independence like I have.&lt;/p&gt;
    &lt;p&gt;When something breaks, I SSH in, ask the agent what is wrong, and fix it. When I want to add something new, I describe it.&lt;/p&gt;
    &lt;p&gt;I am spending time using software, learning, and having fun - instead of maintaining it and stressing out about it.&lt;/p&gt;
    &lt;p&gt;This is for people who:&lt;/p&gt;
    &lt;p&gt;If that is you, I would encourage you to try self-hosting this year.&lt;/p&gt;
    &lt;p&gt;For the first time, I would say this is not just viable. It's fun!&lt;/p&gt;
    &lt;p&gt;Follow me on Twitter for more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46580326</guid><pubDate>Sun, 11 Jan 2026 21:29:57 +0000</pubDate></item><item><title>The next two years of software engineering</title><link>https://addyosmani.com/blog/next-two-years/</link><description>&lt;doc fingerprint="bdd0b127eb273644"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Next Two Years of Software Engineering&lt;/head&gt;
    &lt;head rend="h2"&gt;January 5, 2026&lt;/head&gt;
    &lt;p&gt;The software industry sits at a strange inflection point. AI coding has evolved from autocomplete on steroids to agents that can autonomously execute development tasks. The economic boom that fueled tech’s hiring spree has given way to an efficiency mandate: companies now often favor profitability over growth, experienced hires over fresh graduates, and smaller teams armed with better tools.&lt;/p&gt;
    &lt;p&gt;Meanwhile, a new generation of developers is entering the workforce with a different calculus: pragmatic about career stability, skeptical of hustle culture, and raised on AI assistance from day one.&lt;/p&gt;
    &lt;p&gt;What happens next is genuinely uncertain. Below are five critical questions that may shape software engineering through 2026, with two contrasting scenarios for each. These aren’t really predictions, but lenses for preparation. The goal is a clear roadmap for handling what comes next, grounded in current data and tempered by the healthy skepticism this community is known for.&lt;/p&gt;
    &lt;head rend="h2"&gt;1. The Junior developer question&lt;/head&gt;
    &lt;p&gt;The bottom line: Junior developer hiring could collapse as AI automates entry-level tasks, or rebound as software spreads into every industry. Both futures require different survival strategies.&lt;/p&gt;
    &lt;p&gt;The traditional pathway of “learn to code, get junior job, grow into senior” is wobbling. A Harvard study of 62 million workers found that when companies adopt generative AI, junior developer employment drops by about 9-10% within six quarters, while senior employment barely budges. Big tech hired 50% fewer fresh graduates over the past three years. As one engineer cynically put it: ~”Why hire a junior for $90K when an AI coding agent costs less?”&lt;/p&gt;
    &lt;p&gt;This isn’t just AI. Macro factors like rising interest rates and post-pandemic corrections hit around 2022, before AI tools became widespread. But AI has accelerated the trend. A single senior engineer with AI assistance can now produce what used to require a small team. Companies are quietly not hiring juniors more than they’re firing anyone.&lt;/p&gt;
    &lt;p&gt;The flip scenario: AI unlocks massive demand for developers across every industry, not just tech. Healthcare, agriculture, manufacturing, and finance all start embedding software and automation. Rather than replacing developers, AI becomes a force multiplier that spreads development work into domains that never employed coders. We’d see more entry-level roles, just different ones: “AI-native” developers who quickly build automations and integrations for specific niches.&lt;/p&gt;
    &lt;p&gt;The Bureau of Labor Statistics still projects ~15% growth in software jobs from 2024 to 2034. If businesses use AI to expand output rather than strictly cut headcount, they’ll need humans to seize the opportunities AI creates.&lt;/p&gt;
    &lt;p&gt;The long-term risk of the pessimistic scenario is often overlooked: today’s juniors are tomorrow’s senior engineers and tech leaders. Cut off the talent pipeline entirely and you create a leadership vacuum in 5-10 years. Industry veterans call this the “slow decay”: an ecosystem that stops training its replacements.&lt;/p&gt;
    &lt;p&gt;What to do about it:&lt;/p&gt;
    &lt;p&gt;Junior developers: Make yourself AI-proficient and versatile. Demonstrate that one junior plus AI can match a small team’s output. Use AI coding agents (Cursor/Antigravity/Claude Code/Gemini CLI) to build bigger features, but understand and explain every line if not most. Focus on skills AI can’t easily replace: communication, problem decomposition, domain knowledge. Look at adjacent roles (QA, DevRel, data analytics) as entry points. Build a portfolio, especially projects integrating AI APIs. Consider apprenticeships, internships, contracting, or open source. Don’t be “just another new grad who needs training”; be an immediately useful engineer who learns quickly.&lt;/p&gt;
    &lt;p&gt;Senior developers: Fewer juniors means more grunt work landing on your plate. Lean on automation for routine tasks, but don’t do everything yourself. Set up CI/CD, linters, and AI-assisted testing to catch basic issues. Mentor unofficially through open source or coaching colleagues in other departments. Be frank with management about the risks of all-senior teams. If junior demand rebounds, be ready to onboard effectively and delegate in ways that use AI. Your value is in multiplying the whole team’s output, not just your own code.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. The Skills question&lt;/head&gt;
    &lt;p&gt;The bottom line: Core programming skills could atrophy as AI writes most code, or become more critical than ever as human developers focus on oversight. The coming years determine whether we trade understanding for speed.&lt;/p&gt;
    &lt;p&gt;84% of developers now use AI assistance regularly. For many, the first instinct when facing a bug or new feature isn’t to write code from scratch, but to compose a prompt and stitch together AI-generated pieces. Entry-level coders are skipping the “hard way”: they might never build a binary search tree from scratch or debug a memory leak on their own.&lt;/p&gt;
    &lt;p&gt;The skillset is shifting from implementing algorithms to knowing how to ask the AI the right questions and verify its output. The first rung of the ladder now demands prompting and validating AI rather than demonstrating raw coding ability. Some senior engineers worry this produces a generation who can’t code well independently, a kind of deskilling. AI-generated code introduces subtle bugs and security vulnerabilities that less-experienced developers might miss.&lt;/p&gt;
    &lt;p&gt;The counter-scenario: as AI handles the routine 80%, humans focus on the hardest 20%. Architecture, tricky integrations, creative design, edge cases: the problems machines alone can’t solve. Rather than making deep knowledge obsolete, AI’s ubiquity makes human expertise more important than ever. This is the “high-leverage engineer” who uses AI as a force multiplier but must deeply understand the system to wield it effectively.&lt;/p&gt;
    &lt;p&gt;If everyone has AI coding agent access, what distinguishes great developers is knowing when the AI is wrong or suboptimal. As one senior engineer put it: “The best software engineers won’t be the fastest coders, but those who know when to distrust AI.”&lt;/p&gt;
    &lt;p&gt;Programming shifts: less typing boilerplate, more reviewing AI output for logical errors, security flaws, and mismatches with requirements. Critical skills become software architecture, system design, performance tuning, and security analysis. AI can produce a web app quickly, but an expert engineer ensures the AI followed security best practices and didn’t introduce race conditions.&lt;/p&gt;
    &lt;p&gt;Developer discourse in 2025 was split. Some admitted they hardly ever write code “by hand” and think coding interviews should evolve. Others argued that skipping fundamentals leads to more firefighting when AI’s output breaks. The industry is starting to expect engineers to bring both: AI speed and foundational wisdom for quality.&lt;/p&gt;
    &lt;p&gt;What to do about it:&lt;/p&gt;
    &lt;p&gt;Junior developers: Use AI as a learning tool, not a crutch. When AI coding agents (Cursor/Antigravity/Claude Code/Gemini CLI) suggest code review why it works, identify weaknesses. Occasionally disable your AI helper and write key algorithms from scratch. Prioritize CS fundamentals: data structures, algorithms, complexity, memory management. Implement projects twice, once with AI, once without, and compare. Learn prompt engineering and tool mastery. Train yourself in rigorous testing: write unit tests, read stack traces without immediately asking AI, get comfortable with debuggers. Deepen complementary skills AI can’t replicate: system design, user experience intuition, concurrency reasoning. Show you can both crank out solutions with AI and tackle thorny issues when it fails.&lt;/p&gt;
    &lt;p&gt;Senior developers: Position yourself as the guardian of quality and complexity. Sharpen your core expertise: architecture, security, scaling, domain knowledge. Practice modeling systems with AI components and think through failure modes. Stay current on vulnerabilities in AI-generated code. Embrace your role as mentor and reviewer: define where AI use is acceptable and where manual review is mandatory (payment or safety code). Lean into creative and strategic work; let the junior+AI combo handle routine API hookups while you decide which APIs to build. Invest in soft skills and cross-domain knowledge. Stay current on new tools and best practices. Double down on what makes a human developer indispensable: sound judgment, system-level thinking, and mentorship.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. The Role question&lt;/head&gt;
    &lt;p&gt;The bottom line: The developer role could shrink into limited auditing (overseeing AI-generated code) or expand into a pivotal orchestrator position designing and governing AI-driven systems. Either way, adding value means more than just coding.&lt;/p&gt;
    &lt;p&gt;The extremes here are stark. In one vision, developers see their creative responsibilities diminished. Rather than building software, they mostly audit and babysit AI outputs. AI systems (or “citizen developers” using no-code platforms) handle production; human developers review auto-generated code, check for errors, bias, or security issues, and approve deployments. Maker becomes checker. The joy of code creation replaced by the anxiety of risk management.&lt;/p&gt;
    &lt;p&gt;There are reports of engineers spending more time evaluating AI-generated pull requests and managing automated pipelines, less time crafting code from scratch. Programming feels less like creative problem-solving and more like compliance. As one engineer lamented: “I don’t want to end up as a code janitor, cleaning up what the AI throws over the wall.”&lt;/p&gt;
    &lt;p&gt;The alternative future is far more interesting: developers evolve into high-level orchestrators, combining technical, strategic, and ethical responsibilities. AI “workers” mean human developers take on an architect or general contractor role, designing the overall system, deciding which tasks go to which AI or software component, weaving solutions from many moving parts.&lt;/p&gt;
    &lt;p&gt;A CEO of a low-code platform articulated this vision: in an “agentic” development environment, engineers become “composers,” orchestrating ensembles of AI agents and software services. They won’t write every note themselves, but they define the melody: architecture, interfaces, how agents interact. This role is interdisciplinary and creative: part software engineer, part system architect, part product strategist.&lt;/p&gt;
    &lt;p&gt;The optimistic take: as AI handles rote work, developer roles shift toward higher-value activities by necessity. Jobs may become more interesting. Someone has to decide what the AI should build, verify the product makes sense, and continuously improve it.&lt;/p&gt;
    &lt;p&gt;Which way it goes may depend on how organizations choose to integrate AI. Companies that see AI as labor replacement might trim dev teams and ask remaining engineers to keep automations running. Companies that see AI as a way to amplify their teams might keep headcounts similar but have each engineer deliver more ambitious projects.&lt;/p&gt;
    &lt;p&gt;What to do about it:&lt;/p&gt;
    &lt;p&gt;Junior developers: Seek opportunities beyond just writing code. Volunteer for test case writing, CI pipeline setup, or application monitoring: skills aligned with an auditor/custodian role. Keep your creative coding alive through personal projects so you don’t lose the joy of building. Develop a systems mindset: learn how components communicate, what makes APIs well-designed. Read engineering blogs and case studies of system designs. Familiarize yourself with AI and automation tools beyond code generation: orchestration frameworks, AI APIs. Improve communication skills, written and verbal. Write documentation as if explaining to someone else. Ask senior colleagues not just “Does my code work?” but “Did I consider the right things?” Prepare to be verifier, designer, and communicator, not just coder.&lt;/p&gt;
    &lt;p&gt;Senior developers: Lean into leadership and architectural responsibilities. Shape the standards and frameworks that AI and junior team members follow. Define code quality checklists and ethical AI usage policies. Stay current on compliance and security topics for AI-produced software. Focus on system design and integration expertise; volunteer to map data flows across services and identify failure points. Get comfortable with orchestration platforms (Kubernetes, Airflow, serverless frameworks, agent orchestration tools). Double down on your role as technical mentor: more code reviews, design discussions, technical guidelines. Hone your ability to quickly assess someone else’s (or something’s) code and give high-level feedback. Develop product and business sense; understand why features get built and what customers care about. Shadow a product manager or join customer feedback sessions. Protect your creative passion through prototypes, hackathons, or emerging tech research. Evolve from coder to conductor.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. The Specialist vs. Generalist question&lt;/head&gt;
    &lt;p&gt;The bottom line: Narrow specialists risk finding their niche automated or obsolete. The fast-changing, AI-infused landscape rewards T-shaped engineers: broad adaptability with one or two deep skills.&lt;/p&gt;
    &lt;p&gt;Given how quickly models, tools and frameworks rise and fall, betting your career on a single technology stack is risky. A guru in a legacy framework might suddenly find themselves in less demand when a new AI tool handles that tech with minimal human intervention. Developers who specialize narrowly in “a single stack, framework or product area” might wake up to find that area declining or redundant.&lt;/p&gt;
    &lt;p&gt;Think of COBOL developers, Flash developers, or mobile game engine specialists who didn’t pivot when the industry moved. What’s different now is the pace of change. AI automation can make certain programming tasks trivial, undercutting roles that revolved around those tasks. A specialist who only knows one thing (fine-tuning SQL queries, slicing Photoshop designs into HTML) could find AI handling 90% of that work.&lt;/p&gt;
    &lt;p&gt;Hiring managers chase the newest niche. A few years ago everyone wanted cloud infrastructure specialists; now there’s a surge in AI/ML engineers. Those who specialized narrowly in yesterday’s technology feel stalled as that niche loses luster.&lt;/p&gt;
    &lt;p&gt;The opposite outcome is specialization in a new form: the “versatile specialist” or T-shaped developer. Deep expertise in one or two areas (the vertical stroke), broad familiarity with many others (the horizontal stroke). These engineers become the “glue” in multidisciplinary teams; they communicate with specialists of other stripes and fill gaps when needed.&lt;/p&gt;
    &lt;p&gt;Companies no longer want developers who are either too shallow or too narrowly focused; they want a strong core competency plus ability to work across the stack. Part of the reason is efficiency: a T-shaped engineer can often solve problems end-to-end without waiting on handoffs. Part is innovation: cross-pollination of knowledge leads to better solutions.&lt;/p&gt;
    &lt;p&gt;AI tools actually augment generalists more, making it easier for one person to handle multiple components. A back-end engineer can rely on AI help to create a reasonable UI; a front-end specialist can have AI generate server boilerplate. An AI-rich environment lets people operate more broadly. Meanwhile, deep specialists might find their niche partly automated with no easy way to branch out.&lt;/p&gt;
    &lt;p&gt;Nearly 45% of engineering roles now expect proficiency in multiple domains: programming plus cloud infrastructure knowledge, or front-end plus some ML familiarity.&lt;/p&gt;
    &lt;p&gt;What to do about it:&lt;/p&gt;
    &lt;p&gt;Junior developers: Establish a broad foundation early. Even if hired for a specific role, peek outside that silo. If you’re doing mobile, learn backend basics; if you’re doing front-end, try writing a simple server. Learn the deployment process and tools like Docker or GitHub Actions. Identify one or two areas that genuinely excite you and go deeper: this becomes your vertical expertise. Brand yourself as a hybrid: “full-stack developer with cloud security focus” or “frontend developer with UX expertise.” Use AI tools to learn new domains quickly; when you’re a novice in backend, have ChatGPT generate starter API code and study it. Build the habit of continuous re-skilling. Participate in hackathons or cross-functional projects to force yourself into generalist mode. Tell your manager you want exposure to different parts of the project. Adaptability is a superpower early in your career.&lt;/p&gt;
    &lt;p&gt;Senior developers: Map your skill graph: what are you expert in, what related domains have you only touched superficially? Pick one or two adjacent domains and commit to becoming conversant. If you’re a back-end database specialist, get comfortable with a modern front-end framework or learn ML pipeline basics. Do a small project in your weak area with AI assistance. Integrate your deep expertise with new contexts; if you specialize in web app performance, explore how those skills apply to ML inference optimization. Advocate for or design your role to be more cross-functional. Volunteer to be the “integration champion” for projects touching multiple areas. Mentor others to spread skills around while picking up something from them in return. Update your resume to reflect versatility. Use your experience to identify patterns and transferable knowledge. Become the T-shaped role model: deep in your specialty (giving authority and confidence) but actively stretching horizontally.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. The Education question&lt;/head&gt;
    &lt;p&gt;The bottom line: Will a CS degree remain the gold standard, or will faster learning paths (bootcamps, online platforms, employer training) overtake it? Universities may struggle to keep up with an industry that changes every few months.&lt;/p&gt;
    &lt;p&gt;A four-year computer science degree has long been the primary ticket into software roles. But that tradition is being questioned.&lt;/p&gt;
    &lt;p&gt;One future: universities remain important but struggle to stay relevant. Degrees stay the default credential, but programs lag behind rapidly evolving needs, hampered by slow curriculum update cycles and bureaucratic approval processes. Students and employers feel academia is disconnected from industry, teaching theory or outdated practice that doesn’t translate to job skills.&lt;/p&gt;
    &lt;p&gt;Recent grads report never learning about cloud computing, modern DevOps, or AI tooling during their degree. If universities demand high time and financial investment while delivering low-relevance education, they risk being seen as expensive gatekeepers. But many companies still require a bachelor’s degree out of inertia, so the burden shifts to students to fill the gap with bootcamps, online courses, and self-taught projects.&lt;/p&gt;
    &lt;p&gt;Student loan debt is enormous, and companies spend billions training new grads because those grads lack skills needed in the workplace. Universities might add an AI ethics class here, a cloud computing elective there, but by the time they implement something, industry tools have moved on.&lt;/p&gt;
    &lt;p&gt;The disruptive scenario: traditional education gets increasingly replaced by new systems. Coding bootcamps, online certifications, self-taught portfolios, employer-created training academies. Many high-profile employers (Google, IBM) have dropped degree requirements for certain technical roles. In 2024, nearly 45% of companies planned to eliminate bachelor’s degree requirements for at least some positions.&lt;/p&gt;
    &lt;p&gt;Bootcamps have matured. They produce grads who get hired at top companies alongside CS grads. These programs are shorter (12-week intensive) and focus on practical skills: current frameworks, cloud services, teamwork. The hiring currency is shifting toward live portfolios, micro-credentials, and verified skills. A strong GitHub portfolio or recognized certification can bypass degree requirements.&lt;/p&gt;
    &lt;p&gt;Employer-driven education is emerging: companies creating their own training pipelines or partnering with bootcamps. Some big tech companies have started internal “universities” for non-traditional candidates. AI itself offers new ways to learn: AI tutors, interactive coding sandboxes, personalized instruction outside university settings.&lt;/p&gt;
    &lt;p&gt;A modular ecosystem of learning is far more accessible than an expensive four-year degree. A kid in a country without strong CS universities can take the same Coursera courses and build the same portfolio as someone in Silicon Valley.&lt;/p&gt;
    &lt;p&gt;What to do about it:&lt;/p&gt;
    &lt;p&gt;Aspiring/junior developers: If in a traditional CS program, don’t rely on it exclusively. Augment coursework with real-world projects: build a web app, contribute to open source. Seek internships or co-ops. If your curriculum misses hot topics, learn them through online platforms. Earn industry-recognized certifications (GCP, AWS, Azure) to signal practical knowledge. If self-teaching or in a bootcamp, focus on a compelling portfolio: at least one substantial project with good documentation. Be active in the developer community: contribute to open source, write technical posts. Network through LinkedIn, meetups, dev events. Get an experienced developer to vouch for you. Keep learning continuously; the half-life of technical skills is short. Use AI as your personal tutor. Prove your skills in concrete ways: portfolio, certification, and ability to talk intelligently about your work will open doors.&lt;/p&gt;
    &lt;p&gt;Senior developers and leaders: Your credential alone won’t carry you forever. Invest in continuous education: online courses, workshops, conferences, certifications. Validate your skills in new ways; be prepared for interviews that assess current competency through real problems. Maintain side projects with new tech. Reassess job requirements: do you really need a new hire to have a CS degree, or do you need certain skills and learning ability? Push for skills-first hiring to widen your talent pool. Support internal training programs or apprenticeship-style roles. Champion mentorship circles for junior devs without formal backgrounds. Engage with academia and alternatives: advisory boards, guest lectures, feedback on curriculum gaps. Reflect this in your own career growth: real-world achievements and continuous learning matter more than additional degrees.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Through-Line&lt;/head&gt;
    &lt;p&gt;These scenarios aren’t mutually exclusive. Reality will draw elements from all of them. Some companies will reduce junior hiring while others expand it in new domains. AI will automate routine coding while raising standards for the code humans touch. Developers might spend mornings reviewing AI outputs and afternoons crafting high-level architecture.&lt;/p&gt;
    &lt;p&gt;The consistent thread: change is the only constant. By keeping a finger on technology trends (and skepticism around them), you avoid being caught off-guard by hype or doom. By updating skills, diversifying abilities, and focusing on uniquely human aspects (creativity, critical thinking, collaboration) you remain in the loop.&lt;/p&gt;
    &lt;p&gt;Whether the future brings a coding renaissance or a world where code writes itself, there will always be demand for engineers who think holistically, learn continuously, and drive technology toward solving real problems.&lt;/p&gt;
    &lt;p&gt;The best way to predict the future is to actively engineer it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46580703</guid><pubDate>Sun, 11 Jan 2026 22:00:24 +0000</pubDate></item><item><title>This game is a single 13 KiB file that runs on Windows, Linux and in the Browser</title><link>https://iczelia.net/posts/snake-polyglot/</link><description>&lt;doc fingerprint="249721123c2d08f0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This game is a single 13 KiB file that runs on Windows, Linux and in the Browser.&lt;/head&gt;
    &lt;p&gt;Not that long ago I became aware of Justine Tunneyâs cosmopolitan libc project. Itâs a toolkit that allows you to compile C source code into a single binary that runs natively on multiple operating systems, including Windows, Linux, various flavours of BSD, even including booters.&lt;/p&gt;
    &lt;p&gt;Unfortunately, back then the project didnât seem to support GUI interfaces and produces quite swollen binaries. Hence I decided to take a stab at a similar (simpler? harder? up to you to decide) challenge: create a video game (&amp;lt;16 KiB) that runs natively on Windows, Linux and in the Browser, all from a single source file.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Gameâ&lt;/head&gt;
    &lt;p&gt;Itâs a pretty standard Snake game with the same rules and interface on all platforms. You control a snake that grows longer as it eats food, and the goal is to avoid running into walls. The snake is controlled using either the arrow keys or WASD keys. It can be terminated via &lt;code&gt;ESC&lt;/code&gt; (if permissible by the platform), reset via &lt;code&gt;R&lt;/code&gt;, and paused via &lt;code&gt;P&lt;/code&gt;. Spacebar starts the game.&lt;/p&gt;
    &lt;p&gt;The game keeps track of your score. Each piece of food eaten increases your score by 10 points, except yellow fruit (which spawns with a 15% chance) that gives you 20 points. Fruit spawns at a fixed rate and despawns after a certain time if not eaten. The despawn timer is proportional to the speed of the snake at the time of spawning, which itself is proportional to the snakeâs length.&lt;/p&gt;
    &lt;p&gt;Once ten fruit are eaten, the game proceeds to the next level, randomizing the wallsâ layout. The maze is created as to ensure that there is always a path from the snakeâs head to any piece of food. The initial placement of the snake is also randomized, but always in a position that has at least five empty tiles in the direction the snake is facing.&lt;/p&gt;
    &lt;p&gt;Download the game here (13,772 bytes).&lt;/p&gt;
    &lt;head rend="h2"&gt;The Polyglotâ&lt;/head&gt;
    &lt;p&gt;I implemented the game three times in total: once in C for the i686 Visual C platform using WinAPI, once in C for the x86_64 Linux platform using clang and X11, and once in JavaScript for the browser using HTML5 Canvas. Each implementation is around 3-5 KiB in size when compiled/minified.&lt;/p&gt;
    &lt;p&gt;The Windows implementation was produced using a compressing script that prepends a decompressing stub. This stub has a quite unusual PE header that has many freely controllable bytes after the &lt;code&gt;MZ&lt;/code&gt; signature. This allows us to place a shell script there that skips over the remainder of the file, rendering the (valid) PE executable runnable on Windows while also making the entire file a valid shell script, that will do (thus far) nothing on Linux.&lt;/p&gt;
    &lt;p&gt;Because the PE file is so awkward, we rely on Windowsâ apphelp mechanism. Without compatibility mode, executing for the first time will yield the message:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âThe application was unable to start correctly (0xc0000005). Click OK to close the application.â&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Which should disappear after re-running.&lt;/p&gt;
    &lt;p&gt;The Linux implementation was produced using a similar approach; we use &lt;code&gt;lzma&lt;/code&gt; for decompression and a small shell dropper that extracts the compressed ELF64 binary and runs it, skipping over the head and the tail of the file.&lt;/p&gt;
    &lt;p&gt;The HTML version is also packed and abuses the fact that browsers will happily process all the benign garbage at the start of the file before reaching the actual HTML content. Then we make it invisible/unobtrusive through a bit of CSS magic.&lt;/p&gt;
    &lt;p&gt;Finally, we concatenate all three files together in such an order that each platform will pick the correct part of the file to execute. The final file is exactly 13,312 bytes in size.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46580864</guid><pubDate>Sun, 11 Jan 2026 22:14:35 +0000</pubDate></item><item><title>Uncrossy</title><link>https://uncrossy.com/</link><description>&lt;doc fingerprint="839dee5eada74331"&gt;
  &lt;main&gt;
    &lt;p&gt;Sometimes (like now) you'll need to undo a move before you can find a solution. You can find out if you're in this state by using the hint button or enabling auto-hints in the game settings.&lt;/p&gt;
    &lt;p&gt;We use cookies to improve your experience and analyze usage. Essential cookies enable core functionality, while analytics cookies help us understand how you use the game. Privacy questions?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46582762</guid><pubDate>Mon, 12 Jan 2026 01:39:32 +0000</pubDate></item><item><title>Ai, Japanese chimpanzee who counted and painted dies at 49</title><link>https://www.bbc.com/news/articles/cj9r3zl2ywyo</link><description>&lt;doc fingerprint="daf00f07e88e5641"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Ai, Japanese chimpanzee who counted and painted, dies at 49&lt;/head&gt;
    &lt;p&gt;Ai, a female chimpanzee famous for her cognitive skills has died at 49, according to the Japanese institute where she lived.&lt;/p&gt;
    &lt;p&gt;The Kyoto University's Center for the Evolutionary Origins of Human Behavior said Ai died on 9 January of old age and organ failure and that she was surrounded by staff when she died.&lt;/p&gt;
    &lt;p&gt;Ai was born in western Africa and arrived at the Japanese institute in 1977. There, she became the namesake of the Ai Project, a research programme into the "chimpanzee mind".&lt;/p&gt;
    &lt;p&gt;Among the institute's noteable findings were the fact that Ai was able to use numbers and identify colours.&lt;/p&gt;
    &lt;p&gt;Researchers gave Ai a special keyboard linked to a computer when she was 18 months, a set-up that they used to study her memory and learning.&lt;/p&gt;
    &lt;p&gt;At five years old, Ai "mastered numerical naming from one to six and was able to name the number, colour and object of 300 types of samples", according to a 1985 scientific paper by Tetsuro Matsuzawa, the primatologist behind the Ai Project.&lt;/p&gt;
    &lt;p&gt;When she was not participating in cognitive tests, Ai was known to enjoy drawing and painting. Like other chimpanzees at the centre, Ai would scrawl marker pens over blank paper without having to be motivated by food rewards.&lt;/p&gt;
    &lt;p&gt;Japanese media outlet Kyodo news reported that she once escaped with another primate - using a key to unlock her cage.&lt;/p&gt;
    &lt;p&gt;In 2000 she gave birth to a son, Ayumu, who is also known for his remarkable memory.&lt;/p&gt;
    &lt;p&gt;In 2017 - the 40th anniversary of the Ai Project - a scarf made from one of Ai's paintings was gifted to renowned primatologist Dame Jane Goodall.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46585947</guid><pubDate>Mon, 12 Jan 2026 09:10:45 +0000</pubDate></item><item><title>Lightpanda migrate DOM implementation to Zig</title><link>https://lightpanda.io/blog/posts/migrating-our-dom-to-zig</link><description>&lt;doc fingerprint="27599a755db603ec"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Migrating our DOM to Zig&lt;/head&gt;
    &lt;head rend="h3"&gt;Karl Seguin&lt;/head&gt;
    &lt;head rend="h4"&gt;Software Engineer&lt;/head&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;We replaced LibDOM with our own Zig-based DOM implementation. The original design created friction between V8, our Zig layer, and LibDOM, especially around events, Custom Elements, and ShadowDOM. After six months of spare-time prototyping, we built zigdom: a leaner, more cohesive DOM that gives us full control over memory, events, and future enhancements. We also swapped in html5ever for parsing and added V8 snapshots to cut startup time. There are single-digit % performance gains, but the real win is a unified codebase that’s easier to extend.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why We Replaced LibDOM&lt;/head&gt;
    &lt;p&gt;At a high level, the Lightpanda codebase can be described as a Zig layer sitting between V8 and LibDOM. When JavaScript is executed, like &lt;code&gt;document.getElementById('spice')&lt;/code&gt;, V8 calls into the Zig layer which then forwards the request to the underlying LibDOM document object and then forwards the result back to V8. By using LibDOM, we gained a robust and fairly comprehensive DOM implementation with minimal effort.&lt;/p&gt;
    &lt;p&gt;However, as we worked to increase compatibility with websites in the wild, we felt ever-increasing friction between our three layers. One example is the event system baked into LibDOM. This proved awkward to expand beyond DOM-based events (e.g. input events) or even just bubbling DOM events to our Zig-based Window implementation. Another larger challenge, was integrating support for Custom Elements and ShadowDOM, written in Zig, with LibDOM. Finally, there was some concern about the lack of cohesion with respect to things like memory management and how that would impact potential future changes, like better multi-threading support.&lt;/p&gt;
    &lt;p&gt;If we were to restart the integration from scratch, knowing what we know now, we’d probably be able to avoid most of the friction we’re currently seeing. While we do modify LibDOM as needed, one approach would be to integrate V8 and LibDOM directly, applying fixes and additions directly to LibDOM. But as we wrote before, we’re fans of Zig so the discussions and prototypes we built always leaned towards replacing LibDOM with a custom Zig implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;zigdom&lt;/head&gt;
    &lt;p&gt;Work on a prototype for having a Zig-based DOM started roughly six months ago. This was a casual in-our-spare-time effort. In the spirit of experimentation, this prototype also replaced V8 with QuickJS-NG . By mid-November, we felt the prototype had tackled enough unknowns to start integrating it into Lightpanda (with V8). Thankfully, porting features was relatively simple; commits to the &lt;code&gt;main&lt;/code&gt; branch could usually be ported to the &lt;code&gt;zigdom&lt;/code&gt; branch.&lt;/p&gt;
    &lt;p&gt;The design is straightforward. A &lt;code&gt;Node&lt;/code&gt; has a linked list of children and an optional &lt;code&gt;_parent: ?*Node&lt;/code&gt;. Furthermore, a &lt;code&gt;Node&lt;/code&gt; has a &lt;code&gt;_type&lt;/code&gt; tagged union field to represent the type of node, and a &lt;code&gt;_proto&lt;/code&gt; field to capture its supertype:&lt;/p&gt;
    &lt;code&gt;_type: Type,
_proto: *EventTarget,
 
const Type = union(enum) {
    cdata: *CData,
    element: *Element,
    document: *Document,
    // ...
};&lt;/code&gt;
    &lt;p&gt;Since a modern website can have tens of thousands of nodes and thousands of elements, we obviously care about the size of our &lt;code&gt;EventTarget&lt;/code&gt;, &lt;code&gt;Node&lt;/code&gt; and &lt;code&gt;Element&lt;/code&gt;. That’s why every type in our union is a pointer. That means that when we create a div, we need to allocate the Div, HTMLElement, Element, Node and EventTarget. But rather than doing five separate allocations, we do 1 large allocation for the total size and parcel it out.&lt;/p&gt;
    &lt;p&gt;Another area where we’ve been able to optimize for our use-case is to lazily parse/load certain properties. While a website might have thousands of elements, most JavaScript will only access the classes, styles, relLists, dataset, etc, of a few elements. Rather than having these stored on each element, even as empty lazily loaded containers, they’re attached to a page in an element -&amp;gt; property lookup. While this adds lookup overhead, it removes ~6 pointers from every element.&lt;/p&gt;
    &lt;p&gt;The real win is having a more cohesive design for events, custom elements and ShadowDOM and a simpler foundation for future enhancements. That said, performance, both in terms of memory usage and CPU load are slightly improved (both single digit % improvements).&lt;/p&gt;
    &lt;head rend="h3"&gt;html5ever&lt;/head&gt;
    &lt;p&gt;We saw benefits to writing our own DOM implementation, but not our own HTML parser. For that, we turned to servo’s html5ever written in Rust. I almost forgot to mention it in this post because the experience was so painless and worked from the get-go that I haven’t had to think about it for a while. You setup html5ever with a bunch of callbacks (for creating a node, attaching a node to a parent, creating text, etc.), feed it your HTML, and away you go. My Rust is very bad, but writing a C binding for it was manageable.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus - V8 Snapshot&lt;/head&gt;
    &lt;p&gt;Some of the porting was tedious. For a change of pace, I took time to see how we could leverage V8 snapshots. As a short summary, whenever you create a V8 environment to execute code, you have to do a lot of setup. Every type (hundreds) with all the functions and properties need to be registered with the V8 environment. For a simple page, this can represent anywhere from 10-30% of the total time. V8 snapshots let you setup a pseudo-environment upfront, extract a Snapshot (a binary blob), and use that blob to bootstrap and speedup future environments.&lt;/p&gt;
    &lt;p&gt;When in debug mode, we generated the snapshot on startup. In release mode, the snapshot is generated at compile-time and embedded into the binary, reducing startup time and memory. The overall impact depends on the relative cost of setting up the environment vs processing the page. Complex websites that load hundreds of external scripts probably won’t benefit. But incremental improvements hopefully add up and, if nothing else, help balance the performance cost of new features and complexity.&lt;/p&gt;
    &lt;head rend="h3"&gt;AI Coding Agent&lt;/head&gt;
    &lt;p&gt;This was the first large feature that I developed with the aid of an AI coding agent - specifically Claude. The experience was positive, but not flawless. I’ve personally always liked participating in code reviews / PRs. I can spend hours every day reviewing PRs, so working with Claude is kind of fun for me. If reading code isn’t something you consider fun, it could be a frustrating experience.&lt;/p&gt;
    &lt;p&gt;I was almost always impressed with the quality of code written and “understanding” that Claude exhibited. I’m only guessing here, but I have to imagine that building a DOM, something which has a very explicit specification, tons of documentation and many implementations, was an ideal task for a coding agent.&lt;/p&gt;
    &lt;p&gt;That said, I do think this is first and foremost a code-review exercise, and the Claude CLI is lacking in that respect. When you’re trying to understand and think through a change, you need a comfortable interface that lets you navigate and access whatever context you’re missing. Anything more than a few lines becomes challenging to review, especially as it’s presented for you to accept one piece at a time.&lt;/p&gt;
    &lt;p&gt;In the end, it’s a tool that supplements my own abilities.&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s Next&lt;/head&gt;
    &lt;p&gt;Implementing our own DOM from scratch should make it easier for us to add new features and enhancements. Something we’ve already seen with better custom element and ShadowRoot support. Much of the benefits don’t come directly from implementing a new DOM, but by simply having a more cohesive codebase. For us, expanding our usage of Zig made the most sense.&lt;/p&gt;
    &lt;p&gt;zigdom is now merged into Lightpanda’s main branch. If you want to see how we structured the Node, Element, and event system in Zig, check out the source code on GitHub .&lt;/p&gt;
    &lt;head rend="h3"&gt;Karl Seguin&lt;/head&gt;
    &lt;head rend="h4"&gt;Software Engineer&lt;/head&gt;
    &lt;p&gt;Karl is a software engineer, creator of popular open-source Zig libraries like http.zig or websocket.zig. Karl has been writing about programming for years on his blog openmymind.net and is the author of Learning Zig, a series of articles to help other developers pick up the language. At Lightpanda, he works on building the core browser engine.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46586179</guid><pubDate>Mon, 12 Jan 2026 09:42:49 +0000</pubDate></item><item><title>Anthropic made a big mistake</title><link>https://archaeologist.dev/artifacts/anthropic</link><description>&lt;doc fingerprint="9e019692683f8f49"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Anthropic made a big mistake&lt;/head&gt;
    &lt;p&gt;Anthropic may have just committed the biggest business blunder of 2026 -- and we're less than two weeks in. To understand why, let's briefly rewind to 2025, the year when agentic AI went mainstream.&lt;/p&gt;
    &lt;p&gt;On 3 February 2025, Andrej Karpathy coined the term "vibe coding" to describe the new paradigm.&lt;/p&gt;
    &lt;p&gt;Less than three weeks later, Anthropic released the first research preview of Claude Code, bringing large language models directly into developers' native habitat: the terminal.&lt;/p&gt;
    &lt;p&gt;OpenAI followed with Codex CLI in April, and Google released Gemini CLI in June.&lt;/p&gt;
    &lt;p&gt;All of these terminal-based coding agents follow the same principle:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;you type a prompt&lt;/item&gt;
      &lt;item&gt;the agent sends it to a large language model&lt;/item&gt;
      &lt;item&gt;the LLM responds and may instruct the agent to carry out actions like editing files or running commands&lt;/item&gt;
      &lt;item&gt;the agent carries out the actions and appends the results to the prompt&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These steps are repeated in a loop, but with a twist: the agent can continue working through the loop until the LLM decides that it requires user input.&lt;/p&gt;
    &lt;p&gt;The principle is so simple that it immediately gave rise to a bunch of alternative coding agents, including OpenCode, Roo, and Amp Code (to name but a few). Each brought its own unique philosophy and approach to the table, but what they all have in common is that they ultimately rely on large language models for intelligence. Their job is purely to collect user input, execute tool calls, and pass those to the model, over and over again. Therefore, they tend to provide a way to select from a predefined set of models and a means of authenticating with the relevant providers (such as Anthropic or OpenAI), generally using an API key.&lt;/p&gt;
    &lt;p&gt;When Claude Code launched for real in June 2025, usage of the Anthropic models was included in the Pro and Max plans, for a flat monthly or annual subscription. These plans quickly became very popular when users realised that the effective cost per token was much lower compared to Anthropic's API pricing. So popular, in fact, that it reached $1 billion in annualised revenue after only six months.&lt;/p&gt;
    &lt;p&gt;Meanwhile, OpenCode rapidly gained popularity, amassing over 50,000 GitHub stars and more than 650,000 monthly active users in the same short timeframe. One of its key selling points was the ability to "Log in with Anthropic to use your Claude Pro or Max account", thus enabling developers to benefit from the attractive Claude subscription pricing. In contrast, other coding agents such as Amp only provided the ability to connect to Claude models via the much more expensive pay-per-token API.&lt;/p&gt;
    &lt;p&gt;It turns out that logging into third-party coding agents with an Anthropic OAuth token was a bit of a loophole. This was evident from the fact that it would only work if the client-supplied system prompt contained a specific phrase identifying itself as Claude Code. Nevertheless, many (presumably) unsuspecting Anthropic customers used OpenCode in this way; from their perspective, they were simply using the same service that they were already paying for, just in the comfort of their preferred coding harness.&lt;/p&gt;
    &lt;p&gt;However, Anthropic clearly didn't see it this way. On 9 January 2026, Anthropic unceremoniously closed the loophole, changing their API to detect and reject requests from third-party clients. The renowned vibe-coder Peter Steinberger soon posted about it on the website formerly known as Twitter, and disgruntled Anthropic customers expressed their discontent in a GitHub issue, requesting the decision to be reversed, many threatening to cancel their Claude subscription otherwise.&lt;/p&gt;
    &lt;p&gt;It's notable that Anthropic has not formally announced this change in ToS enforcement, neither ahead of time nor after the fact. The only quasi-announcement of this change was this thread, posted by an Anthropic employee on their personal account the day after the changes took effect, presumably in response to customer complaints. The stated motivation for the change was the allegation that "third-party harnesses using Claude subscriptions create problems for users and generate unusual traffic patterns [...] making it really hard for us to help debug when they have questions about rate limit usage or account bans and they don’t have any other avenue for this support."&lt;/p&gt;
    &lt;p&gt;I will leave it to the reader to decide for themselves whether they consider this a credible explanation or not; frankly, it doesn't matter. The truth is that Anthropic is free to put whatever they want into their ToS, and customers have to abide by it or leave. It appears quite a few have opted for the latter. However, what does matter is what Anthropic has implicitly revealed through its actions last Friday:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Anthropic is willing to go to war with their paying customers over a trivial ToS violation, and&lt;/item&gt;
      &lt;item&gt;they really, really want to own the entire value chain rather than being relegated to becoming just another "model provider", and&lt;/item&gt;
      &lt;item&gt;they utterly failed to consider the second-order effects of this business decision.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The first point has received ample discussion already, so I want to focus on the second and third points.&lt;/p&gt;
    &lt;p&gt;It was reported just a few days earlier that Anthropic has signed a term sheet to raise $10bn at a humongous $350bn valuation. Related or not, the incentives are clear. Model-agnostic harnesses such as OpenCode present a real threat to Anthropic. Whilst its models are incredibly popular in the software developer community and it has made big inroads in enterprise LLM usage, the Claude chatbot itself reportedly commands a market share of... wait for it... 1.07%. So it's no surprise that they are trying to avoid being commoditized in their core market.&lt;/p&gt;
    &lt;p&gt;Which brings us to the final point: without anticipating it, Anthropic just found itself in a classic prisoner's dilemma with OpenAI -- and OpenAI just defected. Not only are they officially supporting OpenCode users to use their Codex subscriptions and usage limits in OpenCode, they are extending the same support to other open-source coding harnesses such as OpenHands, RooCode, and Pi. And it's not just a theoretical announcement either: support for connecting ChatGPT Pro/Plus subscriptions with OpenCode has already shipped.&lt;/p&gt;
    &lt;p&gt;What are we to take away from all this?&lt;/p&gt;
    &lt;p&gt;For me personally, I have decided I will never be an Anthropic customer, because I refuse to do business with a company that takes its customers for granted. Beyond my personal choices, though, I predict that the folks at Anthropic will come to regret their actions last week. By cracking down on their own customers in a vain attempt to quash healthy competition, they have destroyed a lot of goodwill and gave their main rival an opening that was ripe for the picking. Whilst they have plenty of cash in the bank for now, they will eventually need to learn to treat their customers with respect if they are to survive in the ever-more-competitive LLM provider landscape.&lt;/p&gt;
    &lt;p&gt;The views expressed here are my own. While the analysis is based on publicly available information, I welcome any factual corrections -- please feel free to reach out.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46586766</guid><pubDate>Mon, 12 Jan 2026 10:57:29 +0000</pubDate></item><item><title>Launch a Debugging Terminal into GitHub Actions</title><link>https://blog.gripdev.xyz/2026/01/10/actions-terminal-on-failure-for-debugging/</link><description>&lt;doc fingerprint="8df881c16f002085"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Using WebRTC to launch a debugging terminal into GitHub Actions&lt;/head&gt;
    &lt;p&gt;Spoiler: I made a free and open-source way to get an interactive web terminal to your GitHub Action when it fails. Try it out here: https://actions-term.gripdev.xyz/ (code 🔗) 1&lt;/p&gt;
    &lt;head rend="h2"&gt;Building it&lt;/head&gt;
    &lt;p&gt;I think we’ve all been there, your build fails in Actions, but the script works fine locally. You now settle into a slow loop of:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Push speculative change&lt;/item&gt;
      &lt;item&gt;See if it worked&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It was in the middle of one of these when I started thinking about how to make it better.&lt;/p&gt;
    &lt;p&gt;A Terminal would be great, that’s obvious, but how to make it happen? How could I make it free, and open to anyone, without costing me lots of money?&lt;/p&gt;
    &lt;p&gt;Operating a service that forward traffic between a user and the Actions VM would stack up data transfer costs and take some work to scale.&lt;/p&gt;
    &lt;p&gt;What about a Peer-to-Peer connection? I’d recently been going deeper on how Tailscale, iroh and WebRTC use UDP Hole Punching to create Peer-to-Peer (P2P) connections between nodes without relaying traffic. 2&lt;/p&gt;
    &lt;p&gt;If that worked then my server would only need to exchange a tiny bit of information per session and hopefully cost me very little 🤞&lt;/p&gt;
    &lt;p&gt;Could I use P2P and funnel a terminal session over it? Well the Actions VM is on the internet and allows UDP outbound, so it should work!&lt;/p&gt;
    &lt;p&gt;A simple bit of scripting proved it did 🥳 With WebRTC, if the Actions VM and my local machine exchange information about their connectivity (ICE Candidates), then I could form a connection.3&lt;/p&gt;
    &lt;head rend="h2"&gt;Security and Identities&lt;/head&gt;
    &lt;p&gt;The next problem is, how do you prove each end of the P2P connection is who they say they are?&lt;/p&gt;
    &lt;p&gt;It’s important. I want to ensure that &lt;code&gt;lawrencegripper&lt;/code&gt; can only access terminals for Actions triggered by &lt;code&gt;lawrencegripper&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The browser side is relatively easy, we can use OAuth to login via GitHub and get a verified username ✅&lt;/p&gt;
    &lt;p&gt;On the Actions VM we have OIDC, commonly used to auth from Actions to cloud providers.&lt;/p&gt;
    &lt;p&gt;Anyone can use it though, it gives us the ability to issue a signed OIDC token from within our Action which proves:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The repo it’s running on&lt;/item&gt;
      &lt;item&gt;The user account that triggered it&lt;/item&gt;
      &lt;item&gt;The audience it is intended for&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To enable this feature you set the following permissions in the workflow&lt;/p&gt;
    &lt;code&gt;    permissions:
      id-token: write&lt;/code&gt;
    &lt;p&gt;You request a token via a REST request in the action, for example:&lt;/p&gt;
    &lt;code&gt;    const requestURL = process.env.ACTIONS_ID_TOKEN_REQUEST_URL;
    const requestToken = process.env.ACTIONS_ID_TOKEN_REQUEST_TOKEN;
    const SERVER_URL = 'https://actions-term.gripdev.xyz';
    const url = new URL(requestURL);
    url.searchParams.set('audience', SERVER_URL);

    const resp = await fetch(url.toString(), {
        headers: {
        Authorization: `Bearer ${requestToken}`,
        Accept: 'application/json',
        },
    });&lt;/code&gt;
    &lt;p&gt;Then, when the Action calls our server, it can include this token. We can then validate it cryptographically via JWKS:&lt;/p&gt;
    &lt;code&gt;    const githubOIDCIssuer = "https://token.actions.githubusercontent.com"
    const githubJWKSURL = "https://token.actions.githubusercontent.com/.well-known/jwks"
    // Fetch JWKS
	keySet, err := jwkCache.Get(ctx, githubJWKSURL)
	if err != nil {
		return "", "", "", fmt.Errorf("failed to fetch JWKS: %w", err)
	}

	// Parse and validate token with clock skew tolerance
	parseOpts := []jwtx.ParseOption{
		jwtx.WithKeySet(keySet),
		jwtx.WithIssuer(githubOIDCIssuer),
		jwtx.WithValidate(true),
		jwtx.WithAcceptableSkew(2 * time.Minute),
		jwtx.WithAudience(oidcExpectedAudience),
	}
	token, err := jwtx.Parse([]byte(tokenStr), parseOpts...)
	if err != nil {
		return "", "", "", fmt.Errorf("token validation failed: %w", err)
	}&lt;/code&gt;
    &lt;head rend="h2"&gt;Connecting the Peers (ie. Signaling Server)&lt;/head&gt;
    &lt;p&gt;At this point we know:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We can create a connection between two peers (Actions VM &amp;lt;-&amp;gt; Users Browser) with WebRTC&lt;/item&gt;
      &lt;item&gt;We have a way to validate the identity of both ends of the connection (OAuth and OIDC)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s left is the server to introduce the two peers 🤝 Let’s build a server to do that.&lt;/p&gt;
    &lt;p&gt;The server doesn’t need to handle the terminal data, that goes between the two peers directly, it’s only doing introductions.&lt;/p&gt;
    &lt;p&gt;When the VM and Browser are connected to the server it should send each one the connection details for the other.&lt;/p&gt;
    &lt;p&gt;To do this, the browser and the VM both create Server-sent events (SSE) connections, allowing the signaling server to push events to them. They prove their identities by providing their OAuth credentials or OIDC to prove their identity.&lt;/p&gt;
    &lt;p&gt;The server then stores:&lt;/p&gt;
    &lt;code&gt;	runIdToSessions            = make(map[string]*Session) // runId -&amp;gt; session
	runIdToSessionsMu          sync.RWMutex
	runIdRunnerSseClient       = make(map[string]*SSEClient) // runId -&amp;gt; SSE client (Actions VM)
	runIdRunnerSseClientsMu    sync.RWMutex
	actorToBrowserSseClients   = make(map[string][]*SSEClient) // actor -&amp;gt; list of browser SSE clients
	actorToBrowserSseClientsMu sync.RWMutex&lt;/code&gt;
    &lt;p&gt;The server then, via SSE, sends the Actions VM connectivity details to the browser and the Browser’s connectivity details to the Actions VM.&lt;/p&gt;
    &lt;p&gt;At this point they establish the Peer-to-Peer connection 🥳&lt;/p&gt;
    &lt;p&gt;For bonus points, when a new Actions VM connects, I can see if a browser is open waiting and send them a notification.&lt;/p&gt;
    &lt;code&gt;	runIdRunnerSseClientsMu.Lock()
	runIdRunnerSseClient[runId] = client
	log.Printf("SSE: Runner connected for actor %s (total clients: %d)", actor, len(runIdRunnerSseClient))
	runIdRunnerSseClientsMu.Unlock()

	// Notify browser subscribers about new session
	sess, ok := runIdToSessions[runId]
	if ok {
		notifyNewSession(sess)
	}&lt;/code&gt;
    &lt;head rend="h2"&gt;Displaying the Terminal&lt;/head&gt;
    &lt;p&gt;Ok, we’re close now. We have the signaling server to exchange details and then the peers have a p2p connection.&lt;/p&gt;
    &lt;p&gt;What about creating a terminal and streaming the data?&lt;/p&gt;
    &lt;p&gt;WebRTC has a &lt;code&gt;datachannel&lt;/code&gt; which you push arbitrary data through.&lt;/p&gt;
    &lt;p&gt;On the Actions VM side we create a &lt;code&gt;pty.Shell&lt;/code&gt; and stream that data over our &lt;code&gt;datachannel (dc)&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;    shell = pty.spawn(SHELL, [], {
        name: 'xterm-256color',
        cwd: process.env.GITHUB_WORKSPACE || process.cwd(),
        env: process.env as Record&amp;lt;string, string&amp;gt;,
    });

    shell.onData((shellData) =&amp;gt; {
        dc.sendMessage(shellData);
    });&lt;/code&gt;
    &lt;p&gt;In the browser we then need to display an interactive terminal.&lt;/p&gt;
    &lt;p&gt;Reading around, I found the this awesome Ghostty library. It has an xterm.js compatible implementation, I hooked this up and it worked first time 🥰&lt;/p&gt;
    &lt;p&gt;Well it did.. and it didn’t, the Terminal spawned via PTY doesn’t have any idea how big our terminal in the browser (Lines and Columns) so we get some horrible rendering in the terminal.&lt;/p&gt;
    &lt;p&gt;With a bit of poking, googling and some Opus 4.5, I created a method which estimates the size of terminal, via font sizing, and converts this to a rough column / rows. Then, on establishing the P2P connection I can send a &lt;code&gt;setup&lt;/code&gt; JSON message which the Actions VM uses to start &lt;code&gt;pty.spawn&lt;/code&gt; with the right sizing for the terminal.&lt;/p&gt;
    &lt;head rend="h2"&gt;We’re done, right?&lt;/head&gt;
    &lt;p&gt;Not quite, at this point we have 👇&lt;/p&gt;
    &lt;code&gt;┌─────────────────┐                              ┌─────────────────┐
│  GitHub Runner  │◄────────────────────────────►│    Browser      │
│  (TypeScript)   │      Direct P2P (WebRTC)     │  (ghostty-web)  │
│                 │                              │                 │
└────────┬────────┘                              └────────┬────────┘
         │                                                │
         │ Register session                               │ Get sessions
         │ (OIDC Token Auth)                              │ (GitHub OAuth)
         ▼                                                ▼
         └──────────────►┌─────────────────┐◄─────────────┘
                         │     Server      │
                         │   (Go - Auth    │
                         │   &amp;amp; Discovery)  │
                         └─────────────────┘&lt;/code&gt;
    &lt;p&gt;There is a lot of trust placed in the signaling server. It has to do the right thing, or it could provide access to someone else’s Actions VM.&lt;/p&gt;
    &lt;p&gt;Let’s do better.&lt;/p&gt;
    &lt;head rend="h2"&gt;Trust vs Zero-Trust&lt;/head&gt;
    &lt;p&gt;I’ve mentioned already the signaling server should only connect up users with actions they’ve started.&lt;/p&gt;
    &lt;p&gt;That relies on explicit trust, from users, that I’m going to do the right thing.&lt;/p&gt;
    &lt;p&gt;What if I’ve got a bug? What if someone compromises the signaling server? What if they steal the domain and run their own server on it?&lt;/p&gt;
    &lt;p&gt;Well, then they could hook up peers that shouldn’t be connected and … do mean things.&lt;/p&gt;
    &lt;p&gt;That sounds bad, lets work out how to fix that.&lt;/p&gt;
    &lt;p&gt;What if the &lt;code&gt;user&lt;/code&gt; provided the Actions VM with a secret, that only they know?&lt;/p&gt;
    &lt;p&gt;When the P2P connection is made, the Actions VM could refuse to talk to the browser until it provides the right secret.&lt;/p&gt;
    &lt;p&gt;Secrets are cool and everything but if they’re intercepted they’re reusable, could we use a One-Time-Password (OTP) commonly used for 2FA on sites? Sure thing! Even better tools like 1Password will autofill it for you.&lt;/p&gt;
    &lt;p&gt;What does this flow look like? Roughly it’s 👇&lt;/p&gt;
    &lt;code&gt;┌─────────────────┐                              ┌─────────────────┐
│  GitHub Runner  │◄────────────────────────────►│    Browser      │
│  (TypeScript)   │      Direct P2P (WebRTC)     │  (ghostty-web)  │
└────────┬────────┘                              └────────┬────────┘
         │                                                │
         │◄───── 1. WebRTC Connection Established ───────►│
         │                                                │
         │◄─────────── 2. Browser sends OTP ────── ───────│
         │                                                │
         │  3. Runner validates OTP against secret        │
         │                                                │
         │              ┌──────────────────┐              │
         │              │  If OTP Valid:   │              │
         │──────────────│  4. Terminal     │─────────────►│
                        │     access       │
                        │     granted      │
                        └──────────────────┘&lt;/code&gt;
    &lt;p&gt;Even if the signaling server is manipulated, to hook up two peers which shouldn’t be connected, the user won’t be able to execute commands unless they can provide a valid OTP.&lt;/p&gt;
    &lt;p&gt;Better still the Signaling server (which I run) is never sent either the OTP or the OTP Secret used to validate each OTP.&lt;/p&gt;
    &lt;p&gt;This validation happens between two peers you own (Actions VM and your Browser).&lt;/p&gt;
    &lt;head rend="h2"&gt;So now we’re done?&lt;/head&gt;
    &lt;p&gt;One last thing… make the signaling server cheap to host.&lt;/p&gt;
    &lt;p&gt;I want to offer this for free for anyone to use. It’s a simple &lt;code&gt;go&lt;/code&gt; binary in a docker image (to make it easy to self-hosting and test locally) but it needs to live on the internet.&lt;/p&gt;
    &lt;p&gt;A while ago I’d started poking at railway.com, it’s cloud with a big billing twist, you only pay for the CPU and Memory you actually use. This felt like a great time to try it out.&lt;/p&gt;
    &lt;p&gt;How is the billing different, well…&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In Azure/AWS I have to say “I want 2 CPUs and 8GB” and I pay for that regardless of what I use.&lt;/item&gt;
      &lt;item&gt;On railway.com I say “Use up to x CPUs and y GB” then you only pay for what the service actually consumes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: In either Azure/AWS or Railway you still pay network egress.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;How does this work out for the signaling server?&lt;/p&gt;
    &lt;p&gt;Really well it’s peak memory usage so far is 20MB, racking up a mean $0.00000… you get the point. It’s not costing much, as I’m not reserving a small machine with GBs of mem to run a service needing 20MB&lt;/p&gt;
    &lt;p&gt;Even then it feels a bit wasteful running it all the time. There will be chunks of time when folks aren’t using it.&lt;/p&gt;
    &lt;p&gt;This was where I found a platform feature called sleeping, it’s serverless but without the pain of moving away from the docker model to some proprietary runtime.&lt;/p&gt;
    &lt;p&gt;When the service isn’t doing anything, Railway spin down the service. If someone turns up, they hold the connection for a moment while restoring the container, then send the request through.&lt;/p&gt;
    &lt;p&gt;What does a cold start look like on our simple signalling server? It’s hardly recognizable.&lt;/p&gt;
    &lt;p&gt;Here is a recording, on the left you see the server is sleeping and on the right I hit the domain, there is a slight pause before the page renders ✨&lt;/p&gt;
    &lt;head rend="h3"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;This is a personal project with no support/guarantees. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I couldn’t quite get the iroh stack to play nice in-browser so fell back to WebRTC but do want to revisit that in future. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I don’t configure a TURN/relay server so if your network doesn’t allow UDP hole punching you won’t be able to connect. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46587498</guid><pubDate>Mon, 12 Jan 2026 12:25:27 +0000</pubDate></item><item><title>Ozempic reduced grocery spending by an average of 5.3% in the US</title><link>https://news.cornell.edu/stories/2025/12/ozempic-changing-foods-americans-buy</link><description>&lt;doc fingerprint="efb58455e92147a5"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Ozempic is changing the foods Americans buy&lt;/head&gt;&lt;head rend="h2"&gt;By Laura Reiley, Cornell Chronicle&lt;/head&gt;&lt;p&gt;When Americans begin taking appetite-suppressing drugs like Ozempic and Wegovy, the changes extend well beyond the bathroom scale. According to new research, the medications are associated with meaningful reductions in how much households spend on food, both at the grocery store and at restaurants.&lt;/p&gt;&lt;p&gt;The study, published Dec. 18 in the Journal of Marketing Research, links survey data on GLP-1 receptor agonist use – a class of drugs originally developed for diabetes and now widely prescribed for weight loss – with detailed transaction records from tens of thousands of U.S. households. The result is one of the most comprehensive looks yet at how GLP-1 adoption is associated with changes in everyday food purchasing in the real world.&lt;/p&gt;&lt;p&gt;The headline finding is striking: Within six months of starting a GLP-1 medication, households reduce grocery spending by an average of 5.3%. Among higher-income households, the drop is even steeper, at more than 8%. Spending at fast-food restaurants, coffee shops and other limited-service eateries falls by about 8%.&lt;/p&gt;&lt;p&gt;Among households who continue using the medication, lower food spending persists at least a year, though the magnitude of the reduction becomes smaller over time, say co-authors, assistant professor Sylvia Hristakeva and professor Jura Liaukonyte, both in the Charles H. Dyson School of Applied Economics and Management in the Cornell SC Johnson College of Business.&lt;/p&gt;&lt;p&gt;“The data show clear changes in food spending following adoption,” Hristakeva said. “After discontinuation, the effects become smaller and harder to distinguish from pre-adoption spending patterns.”&lt;/p&gt;&lt;p&gt;Unlike previous studies that relied on self-reported eating habits, the new analysis draws on purchase data collected by Numerator, a market research firm that tracks grocery and restaurant transactions for a nationally representative panel of about 150,000 households. The researchers matched those records with repeated surveys asking whether household members were taking GLP-1 drugs, when they started and why.&lt;/p&gt;&lt;p&gt;That combination allowed the team to compare adopters with similar households that did not use the drugs, isolating changes that occurred after medication began.&lt;/p&gt;&lt;p&gt;The reductions were not evenly distributed across the grocery store.&lt;/p&gt;&lt;p&gt;Ultra-processed, calorie-dense foods – the kinds most closely associated with cravings – saw the sharpest declines. Spending on savory snacks dropped by about 10%, with similarly large decreases in sweets, baked goods and cookies. Even staples like bread, meat and eggs declined.&lt;/p&gt;&lt;p&gt;Only a handful of categories showed increases. Yogurt rose the most, followed by fresh fruit, nutrition bars and meat snacks.&lt;/p&gt;&lt;p&gt;“The main pattern is a reduction in overall food purchases. Only a small number of categories show increases, and those increases are modest relative to the overall decline,” Hristakeva said.&lt;/p&gt;&lt;p&gt;The effects extended beyond the supermarket. Spending at limited-service restaurants such as fast-food chains and coffee shops fell sharply as well.&lt;/p&gt;&lt;p&gt;The study also sheds light on who is taking GLP-1 medications. The share of U.S. households reporting at least one user rose from about 11% in late 2023 to more than 16% by mid-2024. Weight-loss users skew younger and wealthier, while those taking the drugs for diabetes are older and more evenly distributed across income groups.&lt;/p&gt;&lt;p&gt;Notably, about one-third of users stopped taking the medication during the study period. When they did, their food spending reverted to pre-adoption levels – and their grocery baskets became slightly less healthy than before they started, driven in part by increased spending on categories such as candy and chocolate.&lt;/p&gt;&lt;p&gt;That movement underscores an important limitation, the authors caution. The study cannot fully separate the biological effects of the drugs from other lifestyle changes users may make at the same time. However, evidence from clinical trials, combined with the observed reversion in spending after discontinuation, suggests appetite suppression is likely a key mechanism behind the spending changes.&lt;/p&gt;&lt;p&gt;The findings carry implications far beyond individual households.&lt;/p&gt;&lt;p&gt;For food manufacturers, restaurants and retailers, widespread GLP-1 adoption could mean long-term shifts in demand, particularly for snack foods and fast food. Package sizes, product formulations and marketing strategies may need to change. For policymakers and public-health experts, the results add context to ongoing debates about the role of medical treatments in shaping dietary behavior – and whether biologically driven appetite changes succeed where taxes and labels have struggled.&lt;/p&gt;&lt;p&gt;“At current adoption rates, even relatively modest changes at the household level can have meaningful aggregate effects,” Hristakeva said. “Understanding these demand shifts is therefore important for assessing food markets and consumer spending.”&lt;/p&gt;&lt;head rend="h4"&gt;Media Contact&lt;/head&gt;&lt;head rend="h5"&gt;Adam Allington&lt;/head&gt;&lt;p&gt;Get Cornell news delivered right to your inbox.&lt;/p&gt;Subscribe&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46587536</guid><pubDate>Mon, 12 Jan 2026 12:29:00 +0000</pubDate></item><item><title>Zen-C: Write like a high-level language, run like C</title><link>https://github.com/z-libs/Zen-C</link><description>&lt;doc fingerprint="fcb8839beacbfaa8"&gt;
  &lt;main&gt;
    &lt;p&gt;Zen C is a modern systems programming language that compiles to human-readable &lt;code&gt;GNU C&lt;/code&gt;/&lt;code&gt;C11&lt;/code&gt;. It provides a rich feature set including type inference, pattern matching, generics, traits, async/await, and manual memory management with RAII capabilities, all while maintaining 100% C ABI compatibility.&lt;/p&gt;
    &lt;code&gt;git clone https://github.com/z-libs/Zen-C.git
cd Zen-C
make
sudo make install&lt;/code&gt;
    &lt;code&gt;# Compile and run
zc run hello.zc

# Build executable
zc build hello.zc -o hello

# Interactive Shell
zc repl&lt;/code&gt;
    &lt;p&gt;Zen C uses type inference by default.&lt;/p&gt;
    &lt;code&gt;var x = 42;                 // Inferred as int
const PI = 3.14159;         // Compile-time constant
var explicit: float = 1.0;  // Explicit type
&lt;/code&gt;
    &lt;p&gt;By default, variables are mutable. You can enable Immutable by Default mode using a directive.&lt;/p&gt;
    &lt;code&gt;//&amp;gt; immutable-by-default

var x = 10;
// x = 20; // Error: x is immutable

var mut y = 10;
y = 20;    // OK
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;C Equivalent&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;int&lt;/code&gt;, &lt;code&gt;uint&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;int&lt;/code&gt;, &lt;code&gt;unsigned int&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Platform standard integer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;I8&lt;/code&gt; .. &lt;code&gt;I128&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;int8_t&lt;/code&gt; .. &lt;code&gt;__int128_t&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Signed fixed-width integers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;U8&lt;/code&gt; .. &lt;code&gt;U128&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;uint8_t&lt;/code&gt; .. &lt;code&gt;__uint128_t&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Unsigned fixed-width integers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;isize&lt;/code&gt;, &lt;code&gt;usize&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;ptrdiff_t&lt;/code&gt;, &lt;code&gt;size_t&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Pointer-sized integers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;byte&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;uint8_t&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Alias for U8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;F32&lt;/code&gt;, &lt;code&gt;F64&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;float&lt;/code&gt;, &lt;code&gt;double&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Floating point numbers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bool&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;char&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;char&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Single character&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;string&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;char*&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;C-string (null-terminated)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;U0&lt;/code&gt;, &lt;code&gt;void&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;void&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Empty type&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Fixed-size arrays with value semantics.&lt;/p&gt;
    &lt;code&gt;var ints: int[5] = {1, 2, 3, 4, 5};
var zeros: [int; 5]; // Zero-initialized
&lt;/code&gt;
    &lt;p&gt;Group multiple values together.&lt;/p&gt;
    &lt;code&gt;var pair = (1, "Hello");
var x = pair.0;
var s = pair.1;
&lt;/code&gt;
    &lt;p&gt;Data structures with optional bitfields.&lt;/p&gt;
    &lt;code&gt;struct Point {
    x: int;
    y: int;
}

// Struct initialization
var p = Point { x: 10, y: 20 };

// Bitfields
struct Flags {
    valid: U8 : 1;
    mode:  U8 : 3;
}
&lt;/code&gt;
    &lt;p&gt;Tagged unions (Sum types) capable of holding data.&lt;/p&gt;
    &lt;code&gt;enum Shape {
    Circle(float),      // Holds radius
    Rect(float, float), // Holds width, height
    Point               // No data
}
&lt;/code&gt;
    &lt;p&gt;Standard C unions (unsafe access).&lt;/p&gt;
    &lt;code&gt;union Data {
    i: int;
    f: float;
}
&lt;/code&gt;
    &lt;code&gt;fn add(a: int, b: int) -&amp;gt; int {
    return a + b;
}

// Named arguments supported in calls
add(a: 10, b: 20);
&lt;/code&gt;
    &lt;p&gt;Anonymous functions that can capture their environment.&lt;/p&gt;
    &lt;code&gt;var factor = 2;
var double = x -&amp;gt; x * factor;  // Arrow syntax
var full = fn(x: int) -&amp;gt; int { return x * factor; }; // Block syntax
&lt;/code&gt;
    &lt;code&gt;if x &amp;gt; 10 {
    print("Large");
} else if x &amp;gt; 5 {
    print("Medium");
} else {
    print("Small");
}

// Ternary
var y = if x &amp;gt; 10 ? 1 : 0;
&lt;/code&gt;
    &lt;p&gt;Powerful alternative to &lt;code&gt;switch&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;match val {
    1 =&amp;gt; print("One"),
    2 | 3 =&amp;gt; print("Two or Three"),
    4..10 =&amp;gt; print("Range"),
    _ =&amp;gt; print("Other")
}

// Destructuring Enums
match shape {
    Circle(r) =&amp;gt; print(f"Radius: {r}"),
    Rect(w, h) =&amp;gt; print(f"Area: {w*h}"),
    Point =&amp;gt; print("Point")
}
&lt;/code&gt;
    &lt;code&gt;// Range
for i in 0..10 { ... }
for i in 0..10 step 2 { ... }

// Iterator/Collection
for item in vec { ... }

// While
while x &amp;lt; 10 { ... }

// Infinite with label
outer: loop {
    if done { break outer; }
}

// Repeat
repeat 5 { ... }
&lt;/code&gt;
    &lt;code&gt;// Guard: Execute else and return if condition is false
guard ptr != NULL else { return; }

// Unless: If not true
unless is_valid { return; }
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Operator&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Function Mapping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;, &lt;code&gt;%&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Arithmetic&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;add&lt;/code&gt;, &lt;code&gt;sub&lt;/code&gt;, &lt;code&gt;mul&lt;/code&gt;, &lt;code&gt;div&lt;/code&gt;, &lt;code&gt;rem&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;==&lt;/code&gt;, &lt;code&gt;!=&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Comparison&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;eq&lt;/code&gt;, &lt;code&gt;neq&lt;/code&gt;, &lt;code&gt;lt&lt;/code&gt;, &lt;code&gt;gt&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;[]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Indexing&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;get&lt;/code&gt;, &lt;code&gt;set&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;??&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Null Coalescing (&lt;code&gt;val ?? default&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;??=&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Null Assignment (&lt;code&gt;val ??= init&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;?.&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Safe Navigation (&lt;code&gt;ptr?.field&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;?&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Try Operator (&lt;code&gt;res?&lt;/code&gt; returns error if present)&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Zen C allows manual memory management with ergonomic aids.&lt;/p&gt;
    &lt;p&gt;Execute code when the current scope exits.&lt;/p&gt;
    &lt;code&gt;var f = fopen("file.txt", "r");
defer fclose(f);
&lt;/code&gt;
    &lt;p&gt;Automatically free the variable when scope exits.&lt;/p&gt;
    &lt;code&gt;autofree var types = malloc(1024);
&lt;/code&gt;
    &lt;p&gt;Implement &lt;code&gt;Drop&lt;/code&gt; to run cleanup logic automatically.&lt;/p&gt;
    &lt;code&gt;impl Drop for MyStruct {
    fn drop(mut self) {
        free(self.data);
    }
}
&lt;/code&gt;
    &lt;p&gt;Define methods on types using &lt;code&gt;impl&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;impl Point {
    // Static method (constructor convention)
    fn new(x: int, y: int) -&amp;gt; Point {
        return Point{x: x, y: y};
    }

    // Instance method
    fn dist(self) -&amp;gt; float {
        return sqrt(self.x * self.x + self.y * self.y);
    }
}
&lt;/code&gt;
    &lt;p&gt;Define shared behavior.&lt;/p&gt;
    &lt;code&gt;trait Drawable {
    fn draw(self);
}

impl Drawable for Circle {
    fn draw(self) { ... }
}
&lt;/code&gt;
    &lt;p&gt;Use &lt;code&gt;use&lt;/code&gt; to mixin fields from another struct.&lt;/p&gt;
    &lt;code&gt;struct Entity { id: int; }
struct Player {
    use Entity; // Adds 'id' field
    name: string;
}
&lt;/code&gt;
    &lt;p&gt;Type-safe templates for Structs and Functions.&lt;/p&gt;
    &lt;code&gt;// Generic Struct
struct Box&amp;lt;T&amp;gt; {
    item: T;
}

// Generic Function
fn identity&amp;lt;T&amp;gt;(val: T) -&amp;gt; T {
    return val;
}
&lt;/code&gt;
    &lt;p&gt;Built on pthreads.&lt;/p&gt;
    &lt;code&gt;async fn fetch_data() -&amp;gt; string {
    // Runs in background
    return "Data";
}

fn main() {
    var future = fetch_data();
    var result = await future;
}
&lt;/code&gt;
    &lt;p&gt;Run code at compile-time to generate source or print messages.&lt;/p&gt;
    &lt;code&gt;comptime {
    print("Compiling...");
}
&lt;/code&gt;
    &lt;p&gt;Embed files as byte arrays.&lt;/p&gt;
    &lt;code&gt;var png = embed "assets/logo.png";
&lt;/code&gt;
    &lt;p&gt;Import compiler plugins to extend syntax.&lt;/p&gt;
    &lt;code&gt;import plugin "regex"
var re = regex! { ^[a-z]+$ };
&lt;/code&gt;
    &lt;p&gt;Pass preprocessor macros through to C.&lt;/p&gt;
    &lt;code&gt;#define MAX_BUFFER 1024
&lt;/code&gt;
    &lt;p&gt;Decorate functions and structs to modify compiler behavior.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Attribute&lt;/cell&gt;
        &lt;cell role="head"&gt;Scope&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@must_use&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Warn if return value is ignored.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@deprecated("msg")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn/Struct&lt;/cell&gt;
        &lt;cell&gt;Warn on usage with message.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@inline&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Hint compiler to inline.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@noinline&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Prevent inlining.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@packed&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Struct&lt;/cell&gt;
        &lt;cell&gt;Remove padding between fields.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@align(N)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Struct&lt;/cell&gt;
        &lt;cell&gt;Force alignment to N bytes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@constructor&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Run before main.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@destructor&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Run after main exits.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@unused&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn/Var&lt;/cell&gt;
        &lt;cell&gt;Suppress unused variable warnings.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@weak&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Weak symbol linkage.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@section("name")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Place code in specific section.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;@noreturn&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Fn&lt;/cell&gt;
        &lt;cell&gt;Function does not return (e.g. exit).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;@derived(...)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Struct&lt;/cell&gt;
        &lt;cell&gt;Auto-implement traits (e.g. &lt;code&gt;Debug&lt;/code&gt;).&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Zen C provides first-class support for inline assembly, transpiling directly to GCC-style extended &lt;code&gt;asm&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Write raw assembly within &lt;code&gt;asm&lt;/code&gt; blocks. Strings are concatenated automatically.&lt;/p&gt;
    &lt;code&gt;asm {
    "nop"
    "mfence"
}
&lt;/code&gt;
    &lt;p&gt;Prevent the compiler from optimizing away assembly that has side effects.&lt;/p&gt;
    &lt;code&gt;asm volatile {
    "rdtsc"
}
&lt;/code&gt;
    &lt;p&gt;Zen C simplifies the complex GCC constraint syntax with named bindings.&lt;/p&gt;
    &lt;code&gt;// Syntax: : out(var) : in(var) : clobber(reg)
// Uses {var} placeholder syntax for readability

fn add(a: int, b: int) -&amp;gt; int {
    var result: int;
    asm {
        "add {result}, {a}, {b}"
        : out(result)
        : in(a), in(b)
        : clobber("cc")
    }
    return result;
}
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Syntax&lt;/cell&gt;
        &lt;cell role="head"&gt;GCC Equivalent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Output&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;: out(var)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"=r"(var)&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Input&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;: in(var)&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"r"(var)&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Clobber&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;: clobber("rax")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"rax"&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Memory&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;: clobber("memory")&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;"memory"&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;&lt;p&gt;Note: When using Intel syntax (via&lt;/p&gt;&lt;code&gt;-masm=intel&lt;/code&gt;), you must ensure your build is configured correctly (for example,&lt;code&gt;//&amp;gt; cflags: -masm=intel&lt;/code&gt;). TCC does not support Intel syntax assembly.&lt;/quote&gt;
    &lt;p&gt;Zen C is designed to work with most C11 compilers. Some features rely on GNU C extensions, but these often work in other compilers. Use the &lt;code&gt;--cc&lt;/code&gt; flag to switch backends.&lt;/p&gt;
    &lt;code&gt;zc run app.zc --cc clang&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Compiler&lt;/cell&gt;
        &lt;cell role="head"&gt;Pass Rate&lt;/cell&gt;
        &lt;cell role="head"&gt;Supported Features&lt;/cell&gt;
        &lt;cell role="head"&gt;Known Limitations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;GCC&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;All Features&lt;/cell&gt;
        &lt;cell&gt;None.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Clang&lt;/cell&gt;
        &lt;cell&gt;100%&lt;/cell&gt;
        &lt;cell&gt;All Features&lt;/cell&gt;
        &lt;cell&gt;None.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TCC&lt;/cell&gt;
        &lt;cell&gt;~70%&lt;/cell&gt;
        &lt;cell&gt;Basic Syntax, Generics, Traits&lt;/cell&gt;
        &lt;cell&gt;No &lt;code&gt;__auto_type&lt;/code&gt;, No Intel ASM, No Nested Functions.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;Recommendation: Use GCC or Clang for production builds. TCC is excellent for rapid prototyping due to its compilation speed but misses some advanced C extensions Zen C relies on for full feature support.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We welcome contributions! Whether it's fixing bugs, adding documentation, or proposing new features.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Fork the Repository: standard GitHub workflow.&lt;/item&gt;
      &lt;item&gt;Create a Feature Branch: &lt;code&gt;git checkout -b feature/NewThing&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Code Guidelines: &lt;list rend="ul"&gt;&lt;item&gt;Follow the existing C style.&lt;/item&gt;&lt;item&gt;Ensure all tests pass: &lt;code&gt;make test&lt;/code&gt;.&lt;/item&gt;&lt;item&gt;Add new tests for your feature in &lt;code&gt;tests/&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Submit a Pull Request: Describe your changes clearly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The test suite is your best friend.&lt;/p&gt;
    &lt;code&gt;# Run all tests (GCC)
make test

# Run specific test
./zc run tests/test_match.zc

# Run with different compiler
./tests/run_tests.sh --cc clang&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Parser: &lt;code&gt;src/parser/&lt;/code&gt;- Recursive descent parser.&lt;/item&gt;
      &lt;item&gt;Codegen: &lt;code&gt;src/codegen/&lt;/code&gt;- Transpiler logic (Zen C -&amp;gt; GNU C/C11).&lt;/item&gt;
      &lt;item&gt;Standard Library: &lt;code&gt;std/&lt;/code&gt;- Written in Zen C itself.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46587804</guid><pubDate>Mon, 12 Jan 2026 12:57:44 +0000</pubDate></item><item><title>Floppy disks turn out to be the greatest TV remote for kids</title><link>https://blog.smartere.dk/2026/01/floppy-disks-the-best-tv-remote-for-kids/</link><description>&lt;doc fingerprint="2c74f8d379255ff1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Floppy Disks: the best TV remote for kids&lt;/head&gt;
    &lt;p&gt;Modern TVs are very poorly suited for kids. They require using complicated remotes or mobile phones, and navigating apps that continually try to lure you into watching something else than you intended to. The usual scenario ends up with the kid feeling disempowered and asking an adult to put something on. That something ends up on auto-play because then the adult is free to do other things and the kid ends up stranded powerless and comatose in front of the TV.&lt;/p&gt;
    &lt;p&gt;Instead I wanted to build something for my 3-year old son that he could understand and use independently. It should empower him to make his own choices. It should be physical and tangible, i.e. it should be something he could touch and feel. It should also have some illusion that the actual media content was stored physically and not un-understandably in “the cloud”, meaning it should e.g. be destroyable — if you break the media there should be consequences. And there should be no auto-play: interact once and get one video.&lt;/p&gt;
    &lt;head rend="h2"&gt;Floppy disks are awesome!&lt;/head&gt;
    &lt;p&gt;And then I remembered the sound of a floppy disk. The mechanical click as you insert it, the whirr of the disk spinning, and the sound of the read-head moving. Floppy disks are the best storage media ever invented! Why else would the “save-icon” still be a floppy disk? Who hasn’t turned in a paper on a broken floppy disk, with the excuse ready that the floppy must have broken when the teacher asks a few days later? But kids these days have never used nor even seen a floppy disk, and I believe they deserve this experience!&lt;/p&gt;
    &lt;p&gt;Building on the experience from the Big Red Fantus-Button, I already had a framework for controlling a Chromecast, and because of the &lt;code&gt;netcat | bash&lt;/code&gt; shenanigans it was easily extendable.&lt;/p&gt;
    &lt;p&gt;My first idea for datastorage was to use the shell of a floppy disk and floppy drive, and put in an RFID tag; this has been done a couple of times on the internet, such as RFIDisk or this RaspberryPi based RFID reader or this video covering how to embed an RFID tag in a floppy disk. But getting the floppy disk apart to put in an RFID tag and getting it back together was kinda wonky.&lt;/p&gt;
    &lt;p&gt;When working on the project in Hal9k someone remarked: “Datastorage? The floppy disk can store data!”, and a quick prototype later this worked really, really, well. Formatting the disk and storing a single small file, “autoexec.sh”, means that all the data ends up in track 0 and is read more or less immediately. It also has the benefit that everything can be checked and edited with a USB floppy disk drive; and the major benefit that all the sounds are completely authentic: click, whirrr, brrr brrr.&lt;/p&gt;
    &lt;head rend="h2"&gt;Autorun for floppy disks is not really a thing.&lt;/head&gt;
    &lt;p&gt;The next problem to tackle was how to detect that a disk is inserted. The concept of AutoRun from Windows 95 was a beauty: insert a CD-ROM and it would automatically start whatever was on the media. Great for convenience, quite questionably for security. While in theory floppy disks are supported for AutoRun, it turns out that floppy drives basically don’t know if a disk is inserted until the operating system tries to access it! There is a pin 34 “Disk Change” that is supposed to give this information, but this is basically a lie. None of the drives in my possession had that pin connected to anything, and the internet mostly concurs. In the end I slightly modified the drive and added a simple rolling switch, that would engage when a disk was inserted.&lt;/p&gt;
    &lt;head rend="h2"&gt;A floppy disk walks into a drive; the microcontroller says “hello!”&lt;/head&gt;
    &lt;p&gt;The next challenge was to read the data on a microcontroller. Helpfully, there is the Arduino FDC Floppy library by dhansel, which I must say is most excellent. Overall, this meant that the part of the project that involved reading a file from the floppy disk FAT filesystem was basically the easiest part of all!&lt;/p&gt;
    &lt;p&gt;However, the Arduino FDC Floppy library is only compatible with the AVR-based Arduinos, not the ESP-based ones, because it needs to control the timing very precisely and therefore uses a healthy amount of inline assembler. This meant that I would need one AVR-based Arduino to control the floppy disk, but another ESP-based one to do the WiFi communication. Such combined boards do exist, and I ended up using such a board, but I’m not sure I would recommend it: the usage is really finagly, as you need to set the jumpers differently for programming the ATmega, or programming the ESP, or connecting the two boards serial ports together.&lt;/p&gt;
    &lt;head rend="h2"&gt;A remote should be battery-powered&lt;/head&gt;
    &lt;p&gt;A remote control should be portable, and this means battery-powered. Driving a floppy disk of of lithium batteries was interesting. There is a large spike in current draw when the disk needs to spin up of several amperes, while the power draw afterwards is more modest, a couple of hundred milliamperes. I wanted the batteries to be 18650s, because I have those in abundance. This meant a battery voltage of 3.7V nominally, up to 4.2V for a fully charged battery; 5V is needed to spin the floppy around, so a boost DC-DC converter was needed. I used an off the shelf XL6009 step-up converter board. At this point a lot of head-scratching occurred: that initial spin-up power draw would cause the microcontroller to reset. In the end a 1000uF capacitor at the microcontroller side seemed to help but not eliminate the problem.&lt;/p&gt;
    &lt;p&gt;One crucial finding was that the ground side of the interface cable should absolutely not be connected to any grounds on the microcontroller side. I was using a relatively simple logic-level MOSFET, the IRLZ34N, to turn off the drive by disconnecting the ground side. If any ground is connected, the disk won’t turn off. But also: if any logic pin was being pulled to ground by the ATmega, that would also provide a path to ground. But since the ATmega cannot sink that much current this would lead to spurious resets! Obvious after the fact, but this took quite some headscratching. Setting all the logic pins to input, and thus high impedance, finally fixed the stability issues.&lt;/p&gt;
    &lt;p&gt;After fixing the stability, the next challenge was how to make both of the microcontrollers sleep. Because the ATmega sleep modes are quite a lot easier to deal with, and because the initial trigger would be the floppy inserting, I decided to make the ATmega in charge overall. Then the ESP has a very simple function: when awoken, read serial in, when a newline is found then send off that complete line via WiFi, and after 30 seconds signal to the ATmega that we’re sleeping, and go back to sleep.&lt;/p&gt;
    &lt;p&gt;The overall flow for the ATmega is then:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A disk is inserted, this triggers a interrupt on the ATmega that wakes up.&lt;/item&gt;
      &lt;item&gt;The ATmega resets the ESP, waking it from deep sleep.&lt;/item&gt;
      &lt;item&gt;The ATmega sends a “diskin” message over serial to the ESP; the ESP transmits this over WiFi when available.&lt;/item&gt;
      &lt;item&gt;The ATmega turns on the drive itself, and reads the disk contents, and just sends it over serial to the ESP.&lt;/item&gt;
      &lt;item&gt;Spin down the disk, go to sleep.&lt;/item&gt;
      &lt;item&gt;When the disk is ejected, send a “diskout” message over serial, resetting the ESP if needed.&lt;/item&gt;
      &lt;item&gt;Go back to 1.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The box itself is just lasercut from MDF-board. For full details see the FloppyDiskCast Git repository.&lt;/p&gt;
    &lt;head rend="h2"&gt;Server-side handlers&lt;/head&gt;
    &lt;p&gt;Responding to those commands is still the &lt;code&gt;netcat | bash&lt;/code&gt; from the Big Red Fantus-Button, which was simply extended with a few more commands and capabilities.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;diskin&lt;/code&gt; always sends a “play” command to the Chromecast.&lt;code&gt;diskout&lt;/code&gt; always sends a “pause” command to the Chromecast.&lt;lb/&gt;Other commands like &lt;code&gt;dad-music&lt;/code&gt; are handled in one of two ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Play a random video from a set, if a video from that set is not already playing: e.g. &lt;code&gt;dad-music&lt;/code&gt;will randomly play one of dad’s music tracks – gotta influence the youth!&lt;/item&gt;
      &lt;item&gt;Play the next video from a list, if a video from the list is not already playing: e.g. &lt;code&gt;fantus-maskinerne&lt;/code&gt;will play the next episode, and only the next episode.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Common for both is that they should be idempotent actions, and the &lt;code&gt;diskin&lt;/code&gt; shortcut will make the media resume without having to wait for the disk contents itself to be read and processed. This means that the “play/pause” disk just contains an empty file to work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Questionable idea meets real-world 3 year old user&lt;/head&gt;
    &lt;p&gt;The little guy quickly caught on to the idea! Much fun was had just pausing and resuming music and his Fantus TV shows. He explored and prodded, and some disks were harmed in the process. One problem that I did solve was that the read head stayed on track 0 after having read everything: this means that when the remote with disk inside it is tumbled around, the disk gets damaged at track 0. To compensate for this, I move the head to track 20 after reading has finished: any damage is then done there, where we don’t store any data. As a bonus it also plays a little more mechanic melody.&lt;/p&gt;
    &lt;p&gt;Be the first to comment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46587934</guid><pubDate>Mon, 12 Jan 2026 13:07:49 +0000</pubDate></item><item><title>Windows 8 Desktop Environment for Linux</title><link>https://github.com/er-bharat/Win8DE</link><description>&lt;doc fingerprint="b06499d27123af91"&gt;
  &lt;main&gt;
    &lt;p&gt;If you are one of who enjoyed the windows 8 and miss its fluid animations but have since moved to linux. And cant go back to windows 8, because all apps are non functional there. And if you can bear that you cant install it on the newer hardware. This is for you it is a shell for wayland window managers like Labwc hyprland etc. It gives a wallpaper utility, a lock screen, a start menu, an OSD for volume and brightness a settins app for wall. it dosent provide charms menu because i always thought its useless. Win8De.webm&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;one command Win8Start to show hide start menu can be bound to super of compositor.&lt;/item&gt;
      &lt;item&gt;full drag and drop support of start tiles and sizes small medium large xlarge gui way right click.&lt;/item&gt;
      &lt;item&gt;can drag from all apps to tiles.&lt;/item&gt;
      &lt;item&gt;search of apps functional.&lt;/item&gt;
      &lt;item&gt;drag app from all apps to botom to hide start screen and put icon any where that supports like desktop.&lt;/item&gt;
      &lt;item&gt;get power menu by clicking user icon.&lt;/item&gt;
      &lt;item&gt;have battery osd in it.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Volume up down mute&lt;/item&gt;
      &lt;item&gt;brightness up down.&lt;/item&gt;
      &lt;item&gt;two part Win8OSD-server and Win8OSD-client server should be autostarted&lt;/item&gt;
      &lt;item&gt;Win8OSD-client --volup voldown mute dispup dispdown&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;simple image wallpaper&lt;/item&gt;
      &lt;item&gt;settable through settins&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;windows 8 style&lt;/item&gt;
      &lt;item&gt;wallpaper changable by settings app&lt;/item&gt;
      &lt;item&gt;have nice slide down and up of lockscreen&lt;/item&gt;
      &lt;item&gt;dont need click and drag just click is enough unlike original&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;can change wallpaper of all 3 graphically start wall lock&lt;/item&gt;
      &lt;item&gt;can change accent colors and background colors of start lockscreen etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;./build.sh&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;it will build all binaries and put it in "build/bin" folder you can use it in config files to autostart and bind to system keys for brightness and volume with local location. you can't run settings from start screen bc it uses system location so you will have to run it from binary built. bind win/super key to Win8Start&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;./install.sh&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;it will automatically run build.sh and move the binaries to "/usr/bin/" and will be available systemwide, so it will be easier to put in configs and autostart&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;./uninstall.sh&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;it will remove binaries from &lt;code&gt;/usr/bin/&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;it will use different config file so that your current config is not affected. create a copy of config folder and paste it with diff name like labwc2 hypr2 etc.&lt;/p&gt;
    &lt;p&gt;find your compositors config loading command and make a .desktop file like this example.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;[Desktop Entry]&lt;/code&gt;
      &lt;code&gt;Name=labwc-win8&lt;/code&gt;
      &lt;code&gt;Comment=A wayland stacking compositor&lt;/code&gt;
      &lt;code&gt;Exec=labwc -C /home/user1/.config/labwc3&lt;/code&gt;
      &lt;code&gt;Icon=labwc&lt;/code&gt;
      &lt;code&gt;Type=Application&lt;/code&gt;
      &lt;code&gt;DesktopNames=labwc;wlroots&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;and paste it in &lt;code&gt;/usr/share/wayland-sessions/&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;and at login choose this session.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46588132</guid><pubDate>Mon, 12 Jan 2026 13:22:44 +0000</pubDate></item><item><title>Ireland fast tracks Bill to criminalise harmful voice or image misuse</title><link>https://www.irishtimes.com/ireland/2026/01/07/call-to-fast-track-bill-targeting-ai-deepfakes-and-identity-hijacking/</link><description>&lt;doc fingerprint="b9201773193591cf"&gt;
  &lt;main&gt;
    &lt;p&gt;The head of the Oireachtas committee on artificial intelligence (AI) has called on the Government to fast track a Bill that would criminalise the harmful misuse of someone’s voice or image.&lt;/p&gt;
    &lt;p&gt;Fianna Fáil TD Malcolm Byrne introduced the Protection of Voice and Image Bill in April, which would criminalise what are colloquially known as “deepfakes” - when someone’s voice or image is used to generate a false photo or video with AI.&lt;/p&gt;
    &lt;p&gt;It follows concerns over Elon Musk’s AI tool Grok being used to digitally undress women and children for distribution on his social media channel X.&lt;/p&gt;
    &lt;p&gt;The strength of laws in Ireland to criminalise AI-generated non-consensual intimate images and child sex abuse images are being examined by the Attorney General.&lt;/p&gt;
    &lt;head rend="h2"&gt;READ MORE&lt;/head&gt;
    &lt;p&gt;Mr Byrne said while generating child sex abuse imagery and sharing intimate images without consent are already criminal offences, this Bill would create a new standalone criminal offence for those who “knowingly exploit another person’s name, image, voice or likeness without consent”, especially when it is done to harm to deceive.&lt;/p&gt;
    &lt;p&gt;“The deliberate misuse of someone’s image or voice without their consent for malign purposes should be a criminal offence. This Bill is a useful baseline and we need to move quickly to address this problem,” Mr Byrne said&lt;/p&gt;
    &lt;p&gt;Ireland’s child protection rapporteur, Caoilfhionn Gallagher, has said the harms from deep fake sexual abuse for the individuals depicted are “equivalent” to those from authentic images “because for victims the videos feel real”.&lt;/p&gt;
    &lt;p&gt;Ms Gallagher also questioned whether Ireland’s protections holding social media platforms to account were sufficient.&lt;/p&gt;
    &lt;p&gt;The special rapporteur on child protection was speaking on RTÉ radio’s Morning Ireland about concerns over the X platform’s Grok AI tool which includes the facility to “nudify” images.&lt;/p&gt;
    &lt;p&gt;“Given how realistic they are, victims know that they might be perceived as real by others,” she said.&lt;/p&gt;
    &lt;p&gt;“Generating these images is often part of a pattern of abuse or harassment. And I’m acutely conscious of the horrendous case of Nicole Coco Fox from Clondalkin, who died by suicide due to online abuse. So we know how devastating online abuse of any kind can be. We have to see this in that perspective.”&lt;/p&gt;
    &lt;p&gt;Ms Gallagher also raised the issue of other nudification apps being trained on vast data sets of mostly female images “because they tend to work most effectively on women’s bodies”.&lt;/p&gt;
    &lt;p&gt;“As a result, 99 per cent of sexually explicit deepfakes accessible online are estimated to be of women and girls. So this is also a gender-based violence issue.”&lt;/p&gt;
    &lt;p&gt;Ms Gallagher added that there was concern internationally about whether the protections in place were sufficient, because most of the protections from a legal and policy perspective internationally were very focused on the users themselves who may generate the images rather than the platforms and the products which facilitate the creation of these images.&lt;/p&gt;
    &lt;p&gt;“To take the platforms themselves, in this case X AI has its own acceptable use policy and which prohibits depicting likenesses of persons in a pornographic manner, but plainly that’s completely insufficient.”&lt;/p&gt;
    &lt;p&gt;Ms Gallagher said Ireland’s relevant laws, including section five of the Child Trafficking and Pornography Act 1998 and Coco’s Law, were “quite focused on the individual users”.&lt;/p&gt;
    &lt;p&gt;“The issue here is Ireland’s not alone and internationally there is a concern about the adequacy of the mechanisms for holding the platforms to account. Ultimately, this is in part a product safety issue and about whether the product itself which allows the images to be generated should be illegal or should be regulated more tightly rather than simply the individual users who take action using it,” she said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46588319</guid><pubDate>Mon, 12 Jan 2026 13:38:36 +0000</pubDate></item><item><title>Reproducing DeepSeek's MHC: When Residual Connections Explode</title><link>https://taylorkolasinski.com/notes/mhc-reproduction/</link><description>&lt;doc fingerprint="f7987005e7b444d8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;DeepSeek's mHC: When Residual Connections Explode&lt;/head&gt;
    &lt;p&gt;Every transformer you’ve ever used has the same residual connection design from 2016.&lt;/p&gt;
    &lt;p&gt;GPT-5, Claude, Llama, Gemini. Under the hood, they all do the same thing: . One stream of information flowing through the network, with each layer adding to it.&lt;/p&gt;
    &lt;p&gt;DeepSeek asked: what if it was wider?&lt;/p&gt;
    &lt;head rend="h2"&gt;The Setup&lt;/head&gt;
    &lt;p&gt;Standard residual connections are the backbone of every modern transformer. The idea is simple:&lt;/p&gt;
    &lt;p&gt;The input flows through unchanged, plus the layer’s output. One stream of information. What goes in comes out, plus a learned update. This is why transformers can be hundreds of layers deep: the gradient has a clean path backward. Simple. Stable. Unchanged since 2016.&lt;/p&gt;
    &lt;p&gt;Hyper-Connections take a different approach. Instead of one stream, expand to n parallel streams with learnable mixing matrices:&lt;/p&gt;
    &lt;p&gt;Compared to standard residual:&lt;/p&gt;
    &lt;p&gt;Three matrices control how information flows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;H_res: How streams mix in the residual path (the red crossings)&lt;/item&gt;
      &lt;item&gt;H_pre: How streams combine before entering the layer&lt;/item&gt;
      &lt;item&gt;H_post: How the layer’s output distributes back to streams&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More expressive. More parameters with negligible computational overhead. Better performance, in theory.&lt;/p&gt;
    &lt;p&gt;The problem? Those mixing matrices are unconstrained. They can amplify signals, not just route them.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Explosion&lt;/head&gt;
    &lt;p&gt;Under aggressive learning rates, Hyper-Connection (HC) signal amplification in my reproduction hit 7x before eventually collapsing. Amax (the maximum of row and column absolute sums) measures how much a matrix can amplify signals.&lt;/p&gt;
    &lt;p&gt;At my 10M parameter scale, this is survivable. But DeepSeek saw this at 27B:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“The Amax Gain Magnitude yields extreme values with peaks of 3000”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;That’s not a typo. Three thousand times amplification. At 27B parameters, unconstrained HC didn’t just drift. It exploded. My 10M reproduction hitting 9.2x is the early warning sign of this exponential failure.&lt;/p&gt;
    &lt;p&gt;This is why unconstrained mixing matrices break at scale. Small amplifications compound exponentially.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Fix: Constrain the Manifold&lt;/head&gt;
    &lt;p&gt;DeepSeek’s fix is clean: constrain the mixing matrices to be doubly stochastic.&lt;/p&gt;
    &lt;p&gt;A doubly stochastic matrix has:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All non-negative entries&lt;/item&gt;
      &lt;item&gt;Rows sum to 1&lt;/item&gt;
      &lt;item&gt;Columns sum to 1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This means the mixing operation can only take weighted averages of streams. It can route information, shuffle it, blend it. But it cannot amplify.&lt;/p&gt;
    &lt;p&gt;How? The Sinkhorn-Knopp algorithm.&lt;/p&gt;
    &lt;p&gt;The algorithm is dead simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start with any matrix (the raw learned weights)&lt;/item&gt;
      &lt;item&gt;Exponentiate to make all entries positive:&lt;/item&gt;
      &lt;item&gt;Normalize rows so each row sums to 1&lt;/item&gt;
      &lt;item&gt;Normalize columns so each column sums to 1&lt;/item&gt;
      &lt;item&gt;Repeat steps 3-4 until convergence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That’s it. Alternate row and column normalization. Twenty iterations is enough.&lt;/p&gt;
    &lt;p&gt;This procedure is differentiable. Gradients flow back through all twenty iterations. The network learns the raw weights , and Sinkhorn ensures the actual mixing matrix is always doubly stochastic.&lt;/p&gt;
    &lt;p&gt;When I first saw this, it felt like cheating. You’re not learning stability. You’re forcing it. But some properties shouldn’t be learned; they should be guaranteed.&lt;/p&gt;
    &lt;p&gt;Technical note: Strictly speaking, only the recursive matrix H_res needs the full Sinkhorn doubly-stochastic treatment. It’s the one compounding errors layer-over-layer. The input/output mixers (H_pre, H_post) are just bounded via sigmoid. The Sinkhorn compute cost is paid only where it matters most.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Seed Variation Results (Depth 24, 3 seeds)&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Val Loss (mean ± std)&lt;/cell&gt;
        &lt;cell role="head"&gt;Max Amax (mean ± std)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;HC&lt;/cell&gt;
        &lt;cell&gt;0.884 ± 0.033&lt;/cell&gt;
        &lt;cell&gt;6.77 ± 0.60&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;mHC&lt;/cell&gt;
        &lt;cell&gt;1.116 ± 0.012&lt;/cell&gt;
        &lt;cell&gt;1.00 ± 0.00&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;HC wins on raw performance: 0.88 vs 1.12 validation loss. At 10M parameters, the mHC constraint acts like a stability tax; you pay in expressivity. But at 27B parameters, that tax is the only thing preventing your model from exploding to NaN.&lt;/p&gt;
    &lt;p&gt;But look at the variance. HC’s loss varies 3x more across seeds (±0.033 vs ±0.012). And Amax? HC swings from 6.1 to 7.6 depending on the seed. mHC is 1.00. Every seed. Every run. Zero variance.&lt;/p&gt;
    &lt;p&gt;At 10M parameters, the instability is survivable. HC still wins. But at 27B parameters, that 6-7x amplification becomes 3000x. You can’t gamble at that scale.&lt;/p&gt;
    &lt;head rend="h3"&gt;Depth Scaling&lt;/head&gt;
    &lt;p&gt;I also swept depths from 6 to 24 layers (constant ~11M parameter budget):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Loss improves with depth, until it doesn’t. Depth 20 hit the sweet spot (0.85 val loss). Depth 24 regressed slightly (0.93) due to the width bottleneck from shrinking dim to 192.&lt;/item&gt;
      &lt;item&gt;Amax is unpredictable. Depth 20 spiked to 9.2x. Depth 12 hit 6.6x. Depth 8 stayed at 4.3x. There’s no clean relationship; HC is chaotic.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Experiment Details&lt;/head&gt;
    &lt;p&gt;Dataset: TinyShakespeare (~1M chars, character-level) Model: GPT-2 architecture, ~10M parameters Training: 5000 steps, AdamW (β1=0.9, β2=0.95), weight decay 0.1, cosine LR decay Hardware: Apple M-series (MPS)&lt;/p&gt;
    &lt;p&gt;Depth sweep: 8 configurations (6-24 layers), width adjusted to maintain ~11M params Seed variation: 3 seeds (42, 123, 456) at depth 24&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters&lt;/head&gt;
    &lt;p&gt;Residual connections are more than a trick to help gradients flow. They’re a conservation law.&lt;/p&gt;
    &lt;p&gt;In physics, conservation laws constrain what’s possible but enable prediction. You can’t build a perpetual motion machine, but you can calculate exactly where a ball will land.&lt;/p&gt;
    &lt;p&gt;The identity mapping in residual connections is similar. It constrains the network by preventing arbitrary transformations, but it guarantees stability. Signal magnitude is preserved.&lt;/p&gt;
    &lt;p&gt;HC breaks conservation; mHC restores it, not by returning to identity, but by finding a richer manifold that still conserves signal.&lt;/p&gt;
    &lt;p&gt;In 2016, He et al. introduced ResNets to solve the vanishing gradient problem, ensuring signals didn’t die. Ten years later, the opposite problem emerged: exploding signals from hyper-connectivity. The identity mapping solved the first by being passive. mHC solves the second by enforcing conservation.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Every residual connection is a conservation law. mHC enforces it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Not a hack, not a trick. A principled constraint that makes the architecture work at scale.&lt;/p&gt;
    &lt;head rend="h2"&gt;Takeaways&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The stream persistence bug humbled me. My first implementation looked right. The equations matched the paper. The code ran. But I was projecting the output back to a single stream and re-expanding it at each layer, killing the parallel architecture. The “hyper” part of Hyper-Connections wasn’t actually doing anything. Three separate audits said “looks correct.” The bug was architectural, not mathematical. I only caught it by asking: “Wait, what shape is actually flowing between layers?”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Constraints aren’t limitations; they’re guarantees. The doubly stochastic projection forces stability. You’re not learning good behavior. You’re making bad behavior impossible. My first reaction: “That’s not elegant. That’s a straitjacket.” Then I saw HC hit 7x amplification. Oh. That’s the point.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The boring choice scales. Standard residual connections have survived since 2016 not because they’re optimal, but because they’re stable. HC is more expressive but fragile. mHC finds a middle ground: more expressive than standard residuals, with stability guarantees.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What’s Next&lt;/head&gt;
    &lt;p&gt;This is Part 1 of a two-part series.&lt;/p&gt;
    &lt;p&gt;Part 1 (this post): Reproduce mHC at small scale to understand the mechanics.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;10M parameters, TinyShakespeare dataset&lt;/item&gt;
      &lt;item&gt;Constant parameter budget across depths&lt;/item&gt;
      &lt;item&gt;Goal: Validate the core claim: HC explodes, mHC doesn’t&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Part 2 (Thursday): Scale up to see real instability.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1B parameters on A100s&lt;/item&gt;
      &lt;item&gt;C4 dataset, fixed width (no bottleneck)&lt;/item&gt;
      &lt;item&gt;Goal: Push toward the 3000x Amax regime&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At 10M params, HC peaked at 9.2x amplification, chaotic but survivable. The paper saw 3000x at 27B. Part 2 will show where things break.&lt;/p&gt;
    &lt;head rend="h2"&gt;Resources&lt;/head&gt;
    &lt;p&gt;Paper: Manifold-Constrained Hyper-Connections (arXiv 2512.24880)&lt;/p&gt;
    &lt;p&gt;Related: Deep Residual Learning (He et al., 2016)&lt;/p&gt;
    &lt;p&gt;Code: Coming with Part 2.&lt;/p&gt;
    &lt;p&gt;Part 2 comes Thursday. Follow @TayKolasinski to catch it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46588572</guid><pubDate>Mon, 12 Jan 2026 13:57:32 +0000</pubDate></item></channel></rss>