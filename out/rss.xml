<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 04 Sep 2025 19:31:53 +0000</lastBuildDate><item><title>Le Chat: Custom MCP Connectors, Memories</title><link>https://mistral.ai/news/le-chat-mcp-connectors-memories</link><description>&lt;doc fingerprint="bdd62a33c0d1a4c0"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Le Chat. Custom MCP connectors. Memories.&lt;/head&gt;&lt;p&gt;Le Chat now integrates with 20+ enterprise platforms—powered by MCP—and remembers what matters with Memories.&lt;/p&gt;&lt;head rend="h2"&gt;Today, we’re giving you more reasons to switch to Le Chat.&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;The widest enterprise-ready connector directory (beta), with custom extensibility, making it easy to bring workflows into your AI assistant.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Directory of 20+ secure connectors—spanning data, productivity, development, automation, commerce, and custom integrations. Search, summarize, and act in tools like Databricks, Snowflake, GitHub, Atlassian, Asana, Outlook, Box, Stripe, Zapier, and more.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Custom extensibility: Add your own MCP connectors to broaden coverage and drive more precise actions and insights.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Flexible deployment: run on mobile, in your browser, or deploy on-premises or in your cloud.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Context that carries: introducing Memories (beta).&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;p&gt;Highly-personalized responses based on your preferences and facts.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Careful and reliable memory handling: saves what matters, slips sensitive or fleeting info.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Complete control over what to store, edit, or delete.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;And… fast import of your memories from ChatGPT.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Everything available on the Free plan.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Plug it right in.&lt;/head&gt;&lt;p&gt;Today, we’re releasing 20+ secure, MCP-powered connectors in Le Chat, enabling you to search, summarize, and take actions with your business-critical tools. Le Chat’s connector directory spans essential categories, simplifying how you integrate your workflows in chats.&lt;/p&gt;&lt;p&gt;The new-look Connectors directory opens direct pipelines into enterprise tools, turning Le Chat into a single surface for data, documents, and actions.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Data: Search and analyze datasets in Databricks (coming soon), Snowflake (coming soon), Pinecone, Prisma Postgres, and DeepWiki.&lt;/item&gt;&lt;item&gt;Productivity: Collaborate on team docs in Box and Notion, spin up project boards in Asana or Monday.com, and triage across Atlassian tools like Jira and Confluence.&lt;/item&gt;&lt;item&gt;Development: Manage issues, pull requests, repositories, and code analysis in GitHub; create tasks in Linear, monitor errors in Sentry, and integrate with Cloudflare Development Platform.&lt;/item&gt;&lt;item&gt;Automation: Extend workflows through Zapier and campaigns in Brevo.&lt;/item&gt;&lt;item&gt;Commerce: Access and act on merchant and payment data from PayPal, Plaid, Square, and Stripe.&lt;/item&gt;&lt;item&gt;Custom: Add your own MCP connectors to extend coverage, so you can query, get summaries, and act on the systems and workflows unique to your business.&lt;/item&gt;&lt;item&gt;Deployment: Run on-prem, in your cloud, or on Mistral Cloud, giving you full control over where your data and workflows live.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Connectors in action.&lt;/head&gt;&lt;head rend="h4"&gt;Databricks and Asana&lt;/head&gt;&lt;p&gt;Summarizing customer reviews in Databricks, then raising a ticket in Asana to address the top issues.&lt;/p&gt;&lt;head rend="h4"&gt;GitHub and Notion&lt;/head&gt;&lt;p&gt;Reviewing open pull requests in GitHub, then creating Jira issues for follow-up and documenting the changes in Notion.&lt;/p&gt;&lt;head rend="h4"&gt;Box&lt;/head&gt;&lt;p&gt;Comparing financial obligations across legal documents in Box, then uploading a concise summary back into Box.&lt;/p&gt;&lt;head rend="h4"&gt;Confluence and Jira&lt;/head&gt;&lt;p&gt;Summarizing active issues from Jira, then drafting a Confluence sprint overview page for team planning.&lt;/p&gt;&lt;head rend="h4"&gt;Stripe and Linear&lt;/head&gt;&lt;p&gt;Retrieving business payment insights from Stripe, then logging anomalies as a development project and task in Linear.&lt;/p&gt;&lt;p&gt;Learn more about Connectors in our Help Center.&lt;/p&gt;&lt;head rend="h3"&gt;Connect any MCP server.&lt;/head&gt;&lt;p&gt;For everything else, you can now connect to any remote MCP server of choice—even if it’s not listed in the Connectors directory—to query, cross-reference, and perform actions on any tool in your stack.&lt;/p&gt;&lt;head rend="h3"&gt;Your rules. Your control.&lt;/head&gt;&lt;p&gt;Admin users can confidently control which connectors are available to whom in their organization, with on-behalf authentication, ensuring users only access data they’re permitted to.&lt;/p&gt;&lt;p&gt;Deploy Le Chat your way—self-hosted, in your private or public cloud, or as a fully managed service in the Mistral Cloud. Talk to our team about enterprise deployments.&lt;/p&gt;&lt;head rend="h2"&gt;Hold that thought.&lt;/head&gt;&lt;p&gt;Memories in Le Chat carry your context across conversations, retrieving insights, decisions, and references from the past when needed. They power more relevant responses, adaptive recommendations tailored for you, and richer answers infused with the specifics of your work—delivering a faster, more relevant, and fully personalized experience.&lt;/p&gt;&lt;p&gt;Memories score high in our evaluations for accuracy and reliability: saving what’s important, avoiding forbidden or sensitive inferences, ignoring ephemeral content, and retrieving the right information without hallucinations.&lt;/p&gt;&lt;p&gt;Most importantly, you stay in full control—add, edit, update, or remove any entry at any time, with clear privacy settings and selective memory handling you can trust.&lt;/p&gt;&lt;head rend="h2"&gt;Get started in Le Chat.&lt;/head&gt;&lt;p&gt;Both Connectors and Memories are available to all Le Chat users.&lt;/p&gt;&lt;p&gt;Try out the new features at chat.mistral.ai, or by downloading the Le Chat mobile by Mistral AI app from the App Store or Google Play Store, for free; no credit card needed.&lt;/p&gt;&lt;p&gt;Reach out to us to learn how Le Chat Enterprise can transform your mission-critical work.&lt;/p&gt;&lt;head rend="h2"&gt;See you at our MCP webinar and hackathon?&lt;/head&gt;&lt;head rend="h3"&gt;Getting Started with MCP in Le Chat, September 9, Online.&lt;/head&gt;&lt;p&gt;Join our webinar on September 9 to dive into Le Chat’s new MCP capabilities with the Mistral team. Learn key insights, ask your questions, and prepare to build cutting-edge projects—all before the hackathon begins.&lt;/p&gt;Sign up now.&lt;head rend="h3"&gt;Mistral AI MCP Hackathon, September 13-14, Paris.&lt;/head&gt;&lt;p&gt;Gather with the best AI engineers for a 2-day overnight hackathon (Sep. 13-14) and turn ideas into reality using your custom MCPs in Le Chat. Network with peers, get hands-on guidance from Mistral experts, and push the boundaries of what’s possible.&lt;/p&gt;&lt;head rend="h2"&gt;We’re hiring!&lt;/head&gt;&lt;p&gt;If you’re interested in joining us on our mission to build world-class AI products, we welcome your application to join our team!&lt;/p&gt;&lt;p&gt;Get in touch.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45125859</guid></item><item><title>Atlassian is acquiring The Browser Company</title><link>https://www.cnbc.com/2025/09/04/atlassian-the-browser-company-deal.html</link><description>&lt;doc fingerprint="a9161654813b714d"&gt;
  &lt;main&gt;
    &lt;p&gt;Atlassian said it has agreed to acquire The Browser Co., a startup that offers a web browser with artificial intelligence features, for $610 million in cash.&lt;/p&gt;
    &lt;p&gt;The companies aim to close the deal in Atlassian's fiscal second quarter, which ends in December.&lt;/p&gt;
    &lt;p&gt;Established in 2019, The Browser Co. has gone up against some of the world's largest companies, including Google, with Chrome, and Apple, which includes Safari on its computers running MacOS.&lt;/p&gt;
    &lt;p&gt;The startup debuted Arc, a customizable browser with a built-in whiteboard and the ability to share groups of tabs, in 2022. The Dia browser, a simpler option that allows people to chat with an AI assistant about multiple browser tabs at once, became available in beta in June.&lt;/p&gt;
    &lt;p&gt;Atlassian co-founder and CEO Mike Cannon-Brookes said he sees shortcomings in the most popular browsers for those who do much of their work on computers.&lt;/p&gt;
    &lt;p&gt;"Whatever it is that you're actually doing in your browser is not particularly well served by a browser that was built in the name to browse," he said in an interview. "It's not built to work, it's not built to act, it's not built to do."&lt;/p&gt;
    &lt;p&gt;Cannon-Brookes said Arc has helped him feel like he can manage his work, with its ability to organize tabs and automatically archive old ones.&lt;/p&gt;
    &lt;p&gt;But only a small percentage of people who used The Browser Co.'s Arc adopted the program's special features.&lt;/p&gt;
    &lt;p&gt;"Our metrics were more like a highly specialized professional tool (like a video editor) than a mass-market consumer product, which we aspired to be closer to," Josh Miller, The Browser Co.'s co-founder and CEO, said in a newsletter update. The startup stopped building new features for Arc, leading to questions of whether it would release the browser under an open-source license.&lt;/p&gt;
    &lt;p&gt;AI search startup Perplexity, which offered Google $34.5 billion for Chrome, talked with The Browser Co. about a possible acquisition in December, The Information reported. OpenAI also held deal talks with The Browser Co., according to the report.&lt;/p&gt;
    &lt;p&gt;Cannon-Brookes wouldn't specify whether Atlassian considered buying Google's browser. Last year, the U.S. Justice Department proposed a divestiture after a federal judge ruled that the company enjoyed an internet search monopoly.&lt;/p&gt;
    &lt;p&gt;"I'm not even sure if there is a bidding competition for Chrome," Cannon-Brookes said. "I didn't see Google putting up an auction just yet. Look, I think we focus on actually getting acquisitions done and actually making those products a part of a coherent whole and delivering value for our customers. I'm not sure that stunt PR acquisition offers are really our thing, but we'll leave that for them to do."&lt;/p&gt;
    &lt;p&gt;Perplexity has been providing early access to its own AI browser, which is named Comet.&lt;/p&gt;
    &lt;p&gt;The Browser Co. was valued at $550 million last year. Investors include Atlassian Ventures, Salesforce Ventures, Figma co-founder Dylan Field and LinkedIn co-founder Reid Hoffman.&lt;/p&gt;
    &lt;p&gt;The browser is central for those using Atlassian products, such as the Jira project management software, which shows existing support requests on the web. But the plan isn't simply to make it nicer to work with Atlassian products online.&lt;/p&gt;
    &lt;p&gt;"It's really about taking Arc's SaaS application experience and power user features, and Dia's AI and elegance and speed and sort of svelte nature, and Atlassian's enterprise know-how, and working out how to put all that together into Dia, or into the AI part of the browser," Cannon-Brookes said.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45126358</guid></item><item><title>Almost anything you give sustained attention to will begin to loop on itself</title><link>https://www.henrikkarlsson.xyz/p/attention</link><description>&lt;doc fingerprint="360018348ba10ade"&gt;
  &lt;main&gt;
    &lt;head rend="h6"&gt;Brioches and Knife, Eliot Hodgkin, 08/1961&lt;/head&gt;
    &lt;head rend="h3"&gt;1.&lt;/head&gt;
    &lt;p&gt;When people talk about the value of paying attention and slowing down, they often make it sound prudish and monk-like. Attention is something we “have to protect.” And we have to “pay”1 attention—like a tribute.&lt;/p&gt;
    &lt;p&gt;But we shouldn’t forget how interesting and overpoweringly pleasurable sustained attention can be. Slowing down makes reality vivid, strange, and hot.&lt;/p&gt;
    &lt;p&gt;Let me start with the most obvious example.&lt;/p&gt;
    &lt;p&gt;As anyone who has had good sex knows, sustained attention and delayed satisfaction are a big part of it. When you resist the urge to go ahead and get what you want and instead stay in the moment, you open up a space for seduction and fantasy. Desire begins to loop on itself and intensify.&lt;/p&gt;
    &lt;p&gt;I’m not sure what is going on here, but my rough understanding is that the expectation of pleasure activates the dopaminergic system in the brain. Dopamine is often portrayed as a pleasure chemical, but it isn’t really about pleasure so much as the expectation that pleasure will occur soon. So when we are being seduced and sense that something pleasurable is coming—but it keeps being delayed, and delayed skillfully—the phasic bursts of dopamine ramp up the levels higher and higher, pulling more receptors to the surface of the cells, making us more and more sensitized to the surely-soon-to-come pleasure. We become hyperattuned to the sensations in our genitals, lips, and skin.&lt;/p&gt;
    &lt;p&gt;And it is not only dopamine ramping up that makes seduction warp our attentional field, infusing reality with intensity and strangeness. There are a myriad of systems that come together to shape our feeling of the present: there are glands and hormones and multiple areas of the brain involved. These are complex physical processes: hormones need to be secreted and absorbed; working memory needs to be cleared and reloaded, and so on. The reason deep attention can’t happen the moment you notice something is that these things take time.&lt;/p&gt;
    &lt;p&gt;What’s more, each of these subsystems update what they are reacting to at a different rate. Your visual cortex can cohere in less than half a second. A stress hormone like cortisol, on the other hand, has a half-life of 60–90 minutes and so can take up to 6 hours to fully clear out after the onset of an acute stressor. This means that if we switch what we pay attention to more often than, say, every 30 minutes, our system will be more or less decohered—different parts will be “attending to” different aspects of reality.2 There will be “attention residue” floating around in our system—leftovers from earlier things we paid attention to (thoughts looping, feelings circling below consciousness, etc.), which crowd out the thing we have in front of us right now, making it less vivid.&lt;/p&gt;
    &lt;p&gt;Inversely, the longer we are able to sustain the attention without resolving it and without losing interest, the more time the different systems of the body have to synchronize with each other, and the deeper the experience gets.&lt;/p&gt;
    &lt;p&gt;Locked in on the same thing, the subsystems begin to reinforce each other: the dopamine makes us aware of our skin, and sensations on the skin ramp up dopamine release, making us even more aware of our skin. A finger touches our belly, and we start to fantasize about where that finger might be going; and so now our fantasies are locked in, too, releasing even more dopamine and making us even more aware of our skin. The more the subsystems lock in, the more intense the feedback loops get. After twenty minutes, our sense of self has evaporated, and we’re in a realm where we do, feel, and think things that would seem surreal in other contexts.&lt;/p&gt;
    &lt;head rend="h3"&gt;2.&lt;/head&gt;
    &lt;p&gt;Similar things happen when we are able to sustain our attention to things other than sex, too. The exact mechanics differ, I presume, but the basic pattern is that when we let our attention linger on something, our bodily systems synchronize and feed each other stimuli in an escalatory loop that restructures our attentional field.&lt;/p&gt;
    &lt;p&gt;Almost anything that we are able to direct sustained attention at will begin to loop on itself and bloom.&lt;/p&gt;
    &lt;p&gt;To take a dark example, if you focus on your anxiety, the anxiety can begin to loop on itself until you hyperventilate and get tunnel vision and become filled with nightmarish thoughts and feelings—a panic attack.&lt;/p&gt;
    &lt;p&gt;And you do the same thing with joy. If you learn to pay sustained attention to your happiness, the pleasant sensation will loop on itself until it explodes and pulls you into a series of almost hallucinogenic states, ending in cessation, where your consciousness lets go and you disappear for a while. This takes practice. The practice is called jhanas, and it is sometimes described as the inverse of a panic attack. I have only ever entered the first jhana, once while spending an hour putting our four-year-old to sleep and meditating on how wonderful it is to lie there next to her. It was really weird and beautiful. If you want to know more about these sorts of mental states, I recommend José Luis Ricón Fernández de la Puente’s recent write-up of his experiences, Nadia Asparouhova on her experiences, and her how-to guide.&lt;/p&gt;
    &lt;p&gt;Here is José, whose blog is normally detailed reflections on cell biology and longevity and metascience, describing the second evening of a jhana retreat:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;So I went down to the beach. “Kinda nice”, I thought. The sky had a particularly vibrant blue color, the waves had ‘the right size’, their roar was pleasant. I started to walk around trying to continue meditating. I focused my awareness on an arising sensation of open heartedness and then I noticed my eyes tearing up (“Huh? I thought”). I looked again at the ocean and then I saw it. It was fucking amazing. So much color and detail: waves within waves, the fractal structure of the foamy crests as they disintegrate back into the ocean. The feeling of the sun on my skin. I felt overwhelmed. As tears ran down my face and lowkey insane grin settled on my face I found myself mumbling “It’s... always been like this!!!!” “What the fuck??!” followed by “This is too much!! Too much!!!”. The experience seemed to be demanding from me to feel more joy and awe than I was born to feel or something like that. In that precise moment I felt what “painfully beautiful” means for the first time in my life.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The fact that we can enter fundamentally different, and often exhilarating, states of mind by learning how to sustain our attention is fascinating. It makes you wonder what other states are waiting out there. What will happen if you properly pay attention to an octopus?3 What about your sense of loneliness?4 A mathematical idea?5 The weights of a neural net?6 The footnotes here take you to examples of people who have done that. There are so many things to pay attention to and experience.&lt;/p&gt;
    &lt;p&gt;One of my favorite things to sustain attention toward is art.&lt;/p&gt;
    &lt;head rend="h3"&gt;3.&lt;/head&gt;
    &lt;p&gt;There was a period in my twenties when I didn’t get art. I thought artists were trying to say something, but I felt superior because I thought there had to be better ways of getting their ideas across (and also, better ideas). But then I realized that good art—at least the art I am spontaneously drawn to—has little to do with communication. Instead, it is about crafting patterns of information that, if you feed them sustained attention, will begin to structure your attentional field in interesting ways. Art is guided meditation. The point isn’t the words, but what happens to your mind when you attend to those words (or images, or sounds). There is nothing there to understand; it is just something to experience, like sex. But the experiences can be very deep and, sometimes, transformative.&lt;/p&gt;
    &lt;p&gt;In 2019, for example, I saw a performance of Jean Sibelius’s 5th Symphony at the University Hall in Uppsala.&lt;/p&gt;
    &lt;p&gt;Before the concert began, I spent a few minutes with my eyes closed, doing a body scan, to be fully present when the music began. As the horns at the opening of the piece called out, I decided to keep my eyes closed, so I wouldn’t be distracted by looking at the hands of the musicians. Then… a sort of daydream started up. The mood suggested to me the image of a cottage overlooking a sloping meadow and a thick wood of pines, a few hours from Helsinki. It was a pretty obvious image, since I knew that Sibelius wrote the piece at Aniola, which is 38 km north of Helsinki. But then I saw an old man walking up the meadow and into the house. The camera cut. Through an open door, I saw the man, alone, working at a desk. I saw it as clearly as if it had been projected on a screen before me: the camera moved slowly toward the back of the man.&lt;/p&gt;
    &lt;p&gt;Through the window above his desk, I could see a light in the distance. Perhaps it was Helsinki? No, it felt alive, like a being—something alive and growing, something that was headed here. But then again, if you were to see a city from space, watching it sped up by 100,000x, it would look like a being moving through the landscape, spreading, getting closer. The old man sat there for a hundred years, watching the light. There was a sinking feeling in my body.&lt;/p&gt;
    &lt;p&gt;One spring, birds fell dead from the sky. They littered the fields, whole droves of them filled the ditches—blue birds, red birds, and black. The man carried them into his woodshed and placed them in waist-high piles.&lt;/p&gt;
    &lt;p&gt;The film kept going, and the emotional intensity and complexity gradually ramped up. For the thirty minutes that it took the orchestra to play the three movements of the symphony, I experienced what felt like two or three feature films, all interconnected by some strange emotional logic. In the third movement, a group of hunter-gatherers was living in a cave that reminded me of the entrance to a nuclear waste facility. A girl hiding behind a tree saw men with cars arrive…&lt;/p&gt;
    &lt;p&gt;The structure of the music was such that it gave me enough predictability and enough surprise to allow my attention to deeply cohere. The melody lines and harmonies dredged up memories and images from my subconscious, weaving them into a rich cinematic web of stories. Guided by the music, my mind could tunnel into an attentional state where I was able to see things I had never seen before and where I could work through some deep emotional pain that seemed to resolve itself through the images.&lt;/p&gt;
    &lt;p&gt;When the music stopped, I barely knew where I was.&lt;/p&gt;
    &lt;p&gt;I opened my eyes and remembered that my brother was sitting next to me.&lt;/p&gt;
    &lt;p&gt;“What did you think?” I said.&lt;/p&gt;
    &lt;p&gt;“I don’t know,” he said. “I felt kind of restless.”&lt;/p&gt;
    &lt;p&gt;Like always, the research for this essay was funded by the contribution of paying subscribers. Thank you! We wouldn’t have been able to do this without you. If you enjoy the essays and want to support Escaping Flatland, we are not yet fully funded:&lt;/p&gt;
    &lt;p&gt;A special thanks to Johanna Karlsson, Nadia Asparouhova, Packy McCormick, and Esha Rana, who all read and commented on drafts of this essay. The image of the University Hall is by Ann-Sofi Cullhed.&lt;/p&gt;
    &lt;p&gt;If you liked this essay, you might also like:&lt;/p&gt;
    &lt;p&gt;In Spanish, you “lend” attention. In Swedish, you “are” attention.&lt;/p&gt;
    &lt;p&gt;It is not like 30 minutes is some ideal. Attention can, under the right conditions, keep getting deeper and more coherent for much longer, as attested by people who meditate for weeks. Inversely, you can, if you have a well developed dorsal attention network and low cortisol level etc, cohere to a high degree in a few minutes. (Though if you have a lot of stress hormones, thirty minutes will not be nearly enough to get out of a flighty mode of attention.) In other words, I don’t think you can put a precise number at it.&lt;lb/&gt;Time to coherence depends on your starting place (mood, hormones, chemical make up in the brain), your skill, and the level of coherence you want to pursue. There is a famous study saying it takes people 23 minutes to get to full productivity after an interruption, which seems like it is correlated to the time it takes them to deeply cohere their attentional field. On the other hand, there is also an upper limit at how long you can cohere, which also depends on a bunch of factors. If I’m working on an essay, I notice that the quality of my thinking drops after about 20 minutes of sustained attention and I need to pause for a few minutes and walk around to get back up to full focus. So in my case, my deepest thinking seem to decohere before I even reach that infamous 23 minute mark! And after 3-4 hours, the quality of my attention goes down so much that everything I write ends up being deleted the day after. For more relaxed attention, like meditation, I haven’t reached the limit for how long I can deepen my coherence—after an hour, which is the longest I’ve gone, I’m still shifting deeper into attention.&lt;/p&gt;
    &lt;p&gt;Charles Darwin:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;[During our stay in Porto Praya,] I was much interested, on several occasions, by watching the habits of an Octopus, or cuttle-fish. Although common in the pools of water left by the retiring tide, these animals were not easily caught. By means of their long arms and suckers, they could drag their bodies into very narrow crevices; and when thus fixed, it required great force to remove them. At other times they darted tail first, with the rapidity of an arrow, from one side of the pool to the other, at the same instant discolouring the water with a dark chestnut-brown ink. These animals also escape detection by a very extraordinary, chameleon-like power of changing their colour. They appear to vary their tints according to the nature of the ground over which they pass: when in deep water, their general shade was brownish purple, but when placed on the land, or in shallow water, this dark tint changed into one of a yellowish green.&lt;/p&gt;&lt;p&gt;The colour, examined more carefully, was a French grey, with numerous minute spots of bright yellow: the former of these varied in intensity; the latter entirely disappeared and appeared again by turns. These changes were effected in such a manner, that clouds, varying in tint between a hyacinth red and a chestnut-brown, were continually passing over the body. Any part, being subjected to a slight shock of galvanism, became almost black: a similar effect, but in a less degree, was produced by scratching the skin with a needle. These clouds, or blushes as they may be called, are said to be produced by the alternate expansion and contraction of minute vesicles containing variously coloured fluids.&lt;/p&gt;&lt;p&gt;This cuttle-fish displayed its chameleon-like power both during the act of swimming and whilst remaining stationary at the bottom. I was much amused by the various arts to escape detection used by one individual, which seemed fully aware that I was watching it. Remaining for a time motionless, it would then stealthily advance an inch or two, like a cat after a mouse; sometimes changing its colour: it thus proceeded, till having gained a deeper part, it darted away, leaving a dusky train of ink to hide the hole into which it had crawled.&lt;/p&gt;&lt;lb/&gt;While looking for marine animals, with my head about two feet above the rocky shore, I was more than once saluted by a jet of water, accompanied by a slight grating noise. At first I could not think what it was, but afterwards I found out that it was this cuttle-fish, which, though concealed in a hole, thus often led me to its discovery. That it possesses the power of ejecting water there is no doubt, and it appeared to me that it could certainly take good aim by directing the tube or siphon on the under side of its body. From the difficulty which these animals have in carrying their heads, they cannot crawl with ease when placed on the ground. I observed that one which I kept in the cabin was slightly phosphorescent in the dark.&lt;/quote&gt;
    &lt;p&gt;Sasha Chapin writes:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;In late winter 2024, I noticed that I wasn’t living up to my stated policy of trying to accept every emotion passing through my system. There were certain shades of existential loneliness that I was pushing away. This was causing some friction. Solitude is simply part of my current life chapter, since Cate is more independent than any of my previous partners, and Berkeley is a place where I don’t feel at home socially.&lt;/p&gt;&lt;lb/&gt;As a response, I made feelings of solitude the central focus of my practice. I tried to become like a sommelier, going out of my way to appreciate all the shades of loneliness that colored my afternoons, trying to zoom in on every micro-pixel and embrace rather than reject.&lt;lb/&gt;Again—normal. This is what, for me, long-term practice often consists of: noticing when my reactions don’t line up with my principles, and seeing if I can bring myself into deeper alignment.&lt;lb/&gt;However, I noticed something odd. Dropping the resistance to loneliness allowed me to slip into deeper sensations of flow. It was almost as if the emotional resistance had been preventing the emergence of a more intuitive part of my will. There were a few memorable walks I took where the feeling of solitude felt like a portal into an exquisitely smooth parallel world. When I allowed my emotions to pierce me more deeply, I fell into a different degree of cooperation with reality. Every step felt precise and necessary, like a choreographed dance.&lt;/quote&gt;
    &lt;p&gt;Michael Nielsen writes about this in an essay where he describes the experience of pushing himself to go deeper than usual in understanding a mathematical proof:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I gradually internalize the mathematical objects I’m dealing with [using spaced repetition]. It becomes easier and easier to conduct (most of) my work in my head. [. . .] Furthermore, as my understanding of the objects change – as I learn more about their nature, and correct my own misconceptions – my sense of what I can do with the objects changes as well. It’s as though they sprout new affordances, in the language of user interface design, and I get much practice in learning to fluidly apply those affordances in multiple ways. [. . .]&lt;/p&gt;
      &lt;p&gt;After going through the [time-consuming process of deeply understanding a proof,] I had a rather curious experience. I went for a multi-hour walk along the San Francisco Embarcadero. I found that my mind simply and naturally began discovering other facts related to the result. In particular, I found a handful (perhaps half a dozen) of different proofs of the basic theorem, as well as noticing many related ideas. This wasn’t done especially consciously – rather, my mind simply wanted to find these proofs.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Chris Olah writes:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Research intimacy is different from theoretical knowledge. It involves internalizing information that hasn’t become part of the “scientific cannon” yet. Observations we don’t (yet) see as important, or haven’t (yet) digested. The ideas are raw.&lt;/p&gt;
      &lt;p&gt;(A personal example: I’ve memorized hundreds of neurons in InceptionV1. I know how they behave, and I know how that behavior is built from earlier neurons. These seem like obscure facts, but they give me powerful, concrete examples to test ideas against.)&lt;/p&gt;
      &lt;p&gt;Research intimacy is also different from research taste. But it does feed into it, and I suspect it’s one of the key ingredients in beating the “research taste market.”&lt;/p&gt;
      &lt;p&gt;As your intimacy with a research topic grows, your random thoughts about it become more interesting. Your thoughts in the shower or on a hike bounce against richer context. Your unconscious has more to work with. Your intuition deepens.&lt;/p&gt;
      &lt;p&gt;I suspect that a lot of “brilliant insights” are natural next steps from someone who has deep intimacy with a research topic. And that actually seems more profound.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45126503</guid></item><item><title>How to build vector tiles from scratch</title><link>https://www.debuisne.com/writing/geo-tiles/</link><description>&lt;doc fingerprint="2082935f2c0712d0"&gt;
  &lt;main&gt;
    &lt;p&gt;As I add more data to the NYC Chaos Dashboard, a website that maps live urban activity, I have been looking for a more efficient way to render the map. Since I collect all of the data in one process and return the Dashboard as one HTML file, I kept wondering how I could optimize the map’s loading time by pre-processing the data as much as possible in the backend. This is where vector tiles come in.&lt;lb/&gt; The code shown in this post is written in Go.&lt;/p&gt;
    &lt;head rend="h1"&gt;Why generate tiles?&lt;/head&gt;
    &lt;p&gt;Initially, all of the map’s data was passed to the rendering library in GeoJSON format (embedded directly in the HTML file). For those who don’t know, GeoJSON is a JSON based standard to represent geographic information. You can go see the full RFC here, but here’s a quick preview of what it looks like so you can get an idea:&lt;/p&gt;
    &lt;code&gt;{
  "type": "Feature",
  "geometry": {
    "type": "Point",
    "coordinates": [-74.04452395542852, 40.68987850656795]
  },
  "properties": {
    "name": "Statue of Liberty",
    "status": "open"
  }
}&lt;/code&gt;
    &lt;p&gt;Now, I’m definitely not JSON’s greatest fan. It’s all text, meaning that a number is stored in a base 10 ASCII representation, where a number like &lt;code&gt;42&lt;/code&gt; gets stored as &lt;code&gt;"4"&lt;/code&gt; and &lt;code&gt;"2"&lt;/code&gt;.
I could go on, but I think you see the problem: it’s not the most efficient way to store data. Nonetheless, JSON has a lot of merits: it’s human-readable and easy to share between systems, so I find myself using it more
than I’d like to - more often than not, simplicity is the way to go, and a simple format like GeoJSON just gets the job done, and that alone makes it a worthy geographic standard.&lt;/p&gt;
    &lt;p&gt;So what happens when I start adding more layers to the map? It gets slow. I’m working on adding flood sensor data (thank you Floodnet for granting me access to the API), LIRR and MetroNorth data, NYISO power data, and many more datasets which will start adding a lot of layers to the map. I can already see that the HTML file, at the time of writing, is 4.5Mb (once decompressed) and takes ~770ms to transfer from Cloudflare’s CDN to my browser. This seems pretty reasonable for now, but it won’t scale as the future datasets are much larger, and running a quick check on the website, chrome is already telling me that the site has performance issues:&lt;/p&gt;
    &lt;p&gt;And if I look more closely, I can see that it’s the result of a long rendering (over 2s!), which I can see here:&lt;/p&gt;
    &lt;p&gt;Now, I suspect this is the result of a few things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;As mentioned above, all of the embedded data is GeoJSON. This means the rendering library (MapLibre GL JS) needs to parse the JSON. There’s a lot, with a lot of properties, so this takes time.&lt;/item&gt;
      &lt;item&gt;MapLibre GL needs to then take the coordinates and then place the lines, points and polygons on the map accordingly. This takes time, and it all happens on your browser.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I don’t like this - I want a map that people can use to check on the status of their city, and not a bloated HTML page that gets slower as more data gets onboarded, ironically making it less and less usable.&lt;/p&gt;
    &lt;head rend="h1"&gt;So, how can I display many large datasets on a map?&lt;/head&gt;
    &lt;p&gt;An obvious, and honestly wise, solution would be to simply load the GeoJSONs separately, via a GET request. You can do this simply in MapLibre:&lt;/p&gt;
    &lt;code&gt;map.addSource('resurfacing', {
    type: 'geojson',
    data: 'https://dash.hudsonshipping.co/data/resurfacing.geojson'
});&lt;/code&gt;
    &lt;p&gt;This is so much better than my current solution:&lt;/p&gt;
    &lt;code&gt;map.addSource('resurfacing', {
    type: 'geojson',
    data: {{.Geo.Resurfacing}}
});&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;{{.Geo.Resurfacing}}&lt;/code&gt; is the code in my HTML template that gets rendered in my Go process.&lt;/p&gt;
    &lt;p&gt;Loading GeoJSONs via a GET request will result in a lighter HTML file and a faster rendering of the Dashboard (the non-map components at least), but MapLibre still needs to parse that JSON and figure out how and where to plot the geometries. This is still not optimal. So how do big companies handle this? To display large amounts of data on map at scale and with performance, they use Vector Tiles, loading geometries sector by sector.&lt;/p&gt;
    &lt;head rend="h1"&gt;What are Vector Tiles?&lt;/head&gt;
    &lt;p&gt;Vector Tiles are small files that also represent geographic features, similar to GeoJSON. The key difference is that they represent a specific sector, a tile (thus the name), at a specific zoom. That means if I want to display a map of the world with Vector Tiles, it will actually be a collection of square tiles pieced together. Imagine taking a globe and cutting it up in multiple squares - that’s what vector tiles are (we’ll ignore distortion and projections for now, that’ll be a post for another time).&lt;/p&gt;
    &lt;p&gt;You may already be familiar with them when interacting with online maps, like Google Maps, where you can notice your features contained in squares, with some squares loaded before others:&lt;/p&gt;
    &lt;p&gt;MapTiler made a great demonstration of Tiles, showing the tile coordinates at different zoom levels.&lt;/p&gt;
    &lt;p&gt;Unlike GeoJSON where all features (point, line or polygon) of a layer are stored in one file, Vector Tiles (MVT) store features in smaller files that represent a specific area (at a specific zoom, or resolution). In the above image, the points from the GeoJSON file are split into 3 tiles (tile 4 being empty).&lt;/p&gt;
    &lt;head rend="h1"&gt;So this is just a raster, then?&lt;/head&gt;
    &lt;p&gt;No, and this is where vector tiles get even more interesting. Like rasters, they represent data in a specific part of the world. But unlike rasters, vector tiles don’t store an image, they store instructions. This is very, very similar (you’ll see just how much in the next section) to a JPEG vs an SVG. This means vector tiles contain commands to draw layers and features that can then be customized in the rendering library:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Have a road you want to display as congested? Just set the line-color to red&lt;/item&gt;
      &lt;item&gt;Have a live event you want to display? Go for it, just add some custom animation to make the point pulsate&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;AS you can see in the above illustration, an Vector Tile file contains commands on how to plot the data using a local coordinate system, and not just a grid of pixels.&lt;/p&gt;
    &lt;head rend="h1"&gt;Ok, so how do I create one of these tiles and load it on a map?&lt;/head&gt;
    &lt;p&gt;Ok, this is the fun part - and it’s a lot easier than I first thought it would be. We’ll use the current standard for vector tiles, which you may have glimpsed in the previous sections: Mapbox Vector Tiles (MVT). MapLibre can handle this natively, and the documentation seems pretty easy:&lt;/p&gt;
    &lt;code&gt;map.addSource('some id', {
    type: 'vector',
    tiles: ['https://dash.hudsonshipping.co/{z}/{x}/{y}/tile.mvt'],
    minzoom: 6,
    maxzoom: 14
});&lt;/code&gt;
    &lt;p&gt;So I need to have an endpoint that serves up MVT files for a given tile (x,y) at a given zoom z. That part isn’t too hard, it’s a simple HTTP endpoint. So now, I need to actually be able to generate the MVT file.&lt;/p&gt;
    &lt;p&gt;Mapbox has done a great job documenting the spec, you can find it here. I used this as the main reference for the project, as it contains everything you need to construct a vector tile. For all transformations, I simply used the code made available my MapTiler.&lt;/p&gt;
    &lt;p&gt;An MVT represents information in binary format - this means it’s not human-readable, unlike GeoJSON. To generate the binary output, Mapbox has opted for Protobuf. If you don’t know Protobuf, it’s Google’s standard to normalize structured data in a binary format with predefined fields and datatypes described in a &lt;code&gt;.proto&lt;/code&gt; file. If you want to learn more about the project, I recommend checking this out. This is what the MVT proto file looks like:&lt;/p&gt;
    &lt;code&gt;message Tile {
        enum GeomType {
             UNKNOWN = 0;
             POINT = 1;
             LINESTRING = 2;
             POLYGON = 3;
        }

        message Value {
                optional string string_value = 1;
                optional float float_value = 2;
                optional double double_value = 3;
                optional int64 int_value = 4;
                optional uint64 uint_value = 5;
                optional sint64 sint_value = 6;
                optional bool bool_value = 7;

                extensions 8 to max;
        }
        message Feature {
                optional uint64 id = 1 [ default = 0 ];
                repeated uint32 tags = 2 [ packed = true ];
                optional GeomType type = 3 [ default = UNKNOWN ];
                repeated uint32 geometry = 4 [ packed = true ];
        }

        message Layer {
                required uint32 version = 15 [ default = 1 ];
                required string name = 1;
                repeated Feature features = 2;
                repeated string keys = 3;
                repeated Value values = 4;
                optional uint32 extent = 5 [ default = 4096 ];

                extensions 16 to max;
        }

        repeated Layer layers = 3;
        extensions 16 to 8191;
}&lt;/code&gt;
    &lt;p&gt;Ok, so looking at the proto, we can see that a tile is essentially an array of layers, each layer being able to contain multiple features. To make this more understandable, here’s an example of what you could put in a tile:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Layer 1: Monuments&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Feature 1: Statue of Liberty (type: Point)&lt;/item&gt;
          &lt;item&gt;Feature 2: Eiffel Tower (type: Point)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Layer 2: Airports&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Feature 1: JFK (type: Point)&lt;/item&gt;
          &lt;item&gt;Feature 2: CDG (type: Point)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’ll use this example throughout this section to construct a tile, and specifically focus on the Statue of Liberty. We can see that a layer represents a dataset you want to display on a map, and a feature is a datapoint in that dataset. A tile can contain multiple layers, and thus multiple datasets. At a certain resolution, we expect the Statue of Liberty to be in the same tile as the JFK airport, same for the Eiffel Tower and CDG, meaning they’ll be in the same MVT file.&lt;/p&gt;
    &lt;p&gt;Ok, so now we understand what’s contained in a tile. But how do you actually construct it? Here are the steps required:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;For a given zoom level, identify what tile (x, y) a feature belongs to&lt;/item&gt;
      &lt;item&gt;Create the tile and add the geometry&lt;/item&gt;
      &lt;item&gt;Add properties via tags&lt;/item&gt;
      &lt;item&gt;Return the tile via HTTP&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;1. Identifying the tiles&lt;/head&gt;
    &lt;p&gt;Let’s restate the problem we’re trying to solve: we have a GeoJSON file that we want to convert into multiple MVT tiles for performance reasons. As stated earlier, to use MVT tiles in MapLibre we need to be able to return a response for:&lt;/p&gt;
    &lt;code&gt;https://dash.hudsonshipping.co/{z}/{x}/{y}/tile.mvt&lt;/code&gt;
    &lt;p&gt;So, this means I need to have multiple tiles generated in the backend, and return the correct one. But how do I know what tile each feature belongs to? For example, when I am currently at zoom level 9, I can see that the the Statue of Liberty will be in the tile &lt;code&gt;x = 150, y = 192&lt;/code&gt;:

And if I zoom to level 10, I can see that the Statue of Liberty will be in the tile
&lt;code&gt;x = 301, y = 385&lt;/code&gt;:
&lt;/p&gt;
    &lt;p&gt;To properly identify the tile, we need to do a few conversions. At this point, all of my geographic data is normalized to use EPSG:4326, a format most people are familiar with to represent coordinates. For example, the Statue of Liberty’s coordinates in EPSG:4326, expressed in degrees, are:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;longitude:-74.04452395542852, latitude: 40.68987850656795&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For vector tiles, the coordinate system used is called Web Mercator (EPSG:3857). In this system, expressed in meters, the Statue is Liberty is located at:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;x = -8242598.70274865, y = 4966705.869136138&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;So we need to convert the Statue of Liberty’s coordinates from degrees to meters, and then identify which square it belongs to at a specific zoom level. This is pretty straight forward, so let’s get right to it.&lt;/p&gt;
    &lt;p&gt;Let’s finally get our hands dirty and write some code (please add proper error handling, this code is only for demonstration purposes).&lt;/p&gt;
    &lt;p&gt;First off, I want to convert my coordinates from &lt;code&gt;EPSG:4326&lt;/code&gt; (lon/lat) to &lt;code&gt;EPSG:3857&lt;/code&gt; (meters):&lt;/p&gt;
    &lt;code&gt;// We will use these constants throughout
const (
	EarthRadius = 6378137 // meters
	TileSize    = 512 // pixels
	OriginShift = 2 * math.Pi * EarthRadius / 2.0
)

func LonLatToMeters(lon float64, lat float64) (x float64, y float64) {
	mx := lon * OriginShift / 180.0
	my := math.Log(math.Tan((90 + lat) * math.Pi / 360.0)) / (math.Pi / 180.0)
	my = my * OriginShift / 180.0
	return mx, my
}&lt;/code&gt;
    &lt;p&gt;The spec refers to 4096 tile sizes, but MapLibre seems to use 512 pixels, so we’ll stick with this. Once we’ve successfully written the function to convert our coordinates, we can now write the code to get the appropriate tile:&lt;/p&gt;
    &lt;code&gt;func Resolution(zoom int) float64 {
	// Returns meters / pixel
	initialResolution := 2 * math.Pi * EarthRadius / TileSize
	return initialResolution / (math.Pow(2, float64(zoom)))
}

func MetersToTile(mx float64, my float64, zoom int) (x int, y int) {
	res := Resolution(zoom) // meters / pixel
	px := (mx + OriginShift) / res
	py := (my + OriginShift) / res

	tx := int(math.Ceil(px / float64(TileSize)) - 1)
	ty := int(math.Ceil(py / float64(TileSize)) - 1)
	return tx, ty
}

func GoogleTile(tx int, ty int, zoom int) (x int, y int) {
	ty = int(math.Pow(2, float64(zoom))-1) - ty
	return tx, ty
}&lt;/code&gt;
    &lt;p&gt;Once we have these utility functions, determining the appropriate tile only requires a few lines of code:&lt;/p&gt;
    &lt;code&gt;mx, my   := LonLatToMeters(lon, lat)
tx, ty   := MetersToTile(mx, my, zoom)
gtx, gty := GoogleTile(tx, ty, zoom)&lt;/code&gt;
    &lt;p&gt;We use &lt;code&gt;GoogleTile()&lt;/code&gt; because it seems MapLibre uses this standard (simply shift the origin to the top left). I couldn’t find documentation on it,
but I was able to see this behavior during my tests. I’ll update this part if I find the relevant documentation.&lt;/p&gt;
    &lt;p&gt;Here is what we now have:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;mx&lt;/code&gt;and&lt;code&gt;my&lt;/code&gt;are the point’s coordinates in meters, using the Web Mercator projection.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;tx&lt;/code&gt;and&lt;code&gt;ty&lt;/code&gt;are the tile coordinates in&lt;code&gt;TMS&lt;/code&gt;format.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gtx&lt;/code&gt;and&lt;code&gt;gty&lt;/code&gt;are the tile coordinates in Google format (also known as the XYZ format).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Great, we know what tile our feature belongs to for a given zoom. Let’s go create the tile.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Create the tile and add the geometry&lt;/head&gt;
    &lt;p&gt;Now that we know which tile a feature belongs to, let’s create the MVT file. You’ll need to install protoc for Go:&lt;/p&gt;
    &lt;code&gt;go install google.golang.org/protobuf/cmd/protoc-gen-go@latest&lt;/code&gt;
    &lt;p&gt;Go ahead and get the official .proto here: https://github.com/mapbox/vector-tile-spec/blob/master/2.1/vector_tile.proto&lt;/p&gt;
    &lt;p&gt;Generate the struct:&lt;/p&gt;
    &lt;code&gt;protoc --go_out=./path/to/dir/mvt ./path/to/dir/mvt/vector_tile.proto&lt;/code&gt;
    &lt;p&gt;This will generate a &lt;code&gt;vector_tile.pb.go&lt;/code&gt; that will look something like this:&lt;/p&gt;
    &lt;code&gt;// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.8
// 	protoc        v3.21.12
// source: internal/geo/mvt/vector_tile.proto

package mvt // Make sure to use the correct package here

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// GeomType is described in section 4.3.4 of the specification
type Tile_GeomType int32

const (
	Tile_UNKNOWN    Tile_GeomType = 0
	Tile_POINT      Tile_GeomType = 1
	Tile_LINESTRING Tile_GeomType = 2
	Tile_POLYGON    Tile_GeomType = 3
)
// ...&lt;/code&gt;
    &lt;p&gt;Once we have generated the protobuf class, we can create an empty tile like this:&lt;/p&gt;
    &lt;code&gt;func NewTile(layerName string) *Tile {
    // The Tile struct comes from the generated protobuf
	version := uint32(0)
	extent := uint32(TileSize)
	layers := []*Tile_Layer{
	    {
	        Name:     &amp;amp;layerName,
	        Version:  &amp;amp;version,
	        Features: make([]*Tile_Feature, 0),
	        Extent:   &amp;amp;extent,
	    },
	}
	return &amp;amp;Tile{Layers: layers}
}
t = NewTile("monuments")&lt;/code&gt;
    &lt;p&gt;Great, we now have an empty tile. Let’s add our monument to ut, as a feature in the first layer called &lt;code&gt;monuments&lt;/code&gt;.
A feature is defined by a few things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A geometry type, in our case a Point&lt;/item&gt;
      &lt;item&gt;Tags (properties for that feature, which we’ll add in the next section)&lt;/item&gt;
      &lt;item&gt;A geometry&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As mentioned earlier, a vector tile behaves similarly to an SVG file: you specify instructions to move a cursor. And that’s exactly what goes into the geometry field: a series of instructions for a cursor. Since our monument is a Point, we only have one instruction: move to location &lt;code&gt;(x, y)&lt;/code&gt;:
&lt;/p&gt;
    &lt;p&gt;As you can see in the above example, we’re telling the cursor to move from the Origin to a specific part of the tile. In this case, we’re letting MapLibre GL know to move the cursor 200 pixels to the right (x axis) and 75 pixels down (y axis).&lt;/p&gt;
    &lt;p&gt;But how do we get these instructions? Instructions are relative to the Origin of the tile, which is located at the top-left corner. Coordinates go from 0 to 512 (the &lt;code&gt;Extent&lt;/code&gt; of the tile, which we defined above).:
&lt;/p&gt;
    &lt;p&gt;This means we need a few things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Get the Web Mercator coordinates of the tile’s Origin&lt;/item&gt;
      &lt;item&gt;Calculate the offset, &lt;code&gt;dx&lt;/code&gt;and&lt;code&gt;dy&lt;/code&gt;for a given Point&lt;/item&gt;
      &lt;item&gt;Convert the offset in meters to an offset in pixels&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Getting the tile’s origin is pretty straight forward:&lt;/p&gt;
    &lt;code&gt;func GetTileOrigin(tx int, ty int, zoom int) (minx int64, maxy int64) {
	res := Resolution(zoom)
	minX := int64(float64(tx) * TileSize * res - OriginShift)
	maxY := int64(float64(ty + 1) * TileSize * res - OriginShift)
	return minX, maxY
}
originX, originY := TileBounds(tx, ty, zoom)&lt;/code&gt;
    &lt;p&gt;Now that we have the Origin’s coordinates in meters, let’s calculate the offsets &lt;code&gt;dx&lt;/code&gt; and &lt;code&gt;dy&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;dx := mx - originX
dy := originY - my&lt;/code&gt;
    &lt;p&gt;Finally, let’s divide the offsets by the resolution, expressed in meters / pixel, to get the parameters for the instruction:&lt;/p&gt;
    &lt;code&gt;res := Resolution(zoom)
shiftX := uint32(math.Floor(float64(dx) / res))
shiftY := uint32(math.Floor(float64(dy) / res))&lt;/code&gt;
    &lt;p&gt;We now have everything we need to construct our feature:&lt;/p&gt;
    &lt;code&gt;geomType := Tile_POINT
feature := Tile_Feature{
    Type: &amp;amp;geomType,
    Geometry: []uint32{
        1 &amp;amp; 0x7 | 1 &amp;lt;&amp;lt; 3, // Command 1 (moveTo), count of 1
        (shiftX &amp;lt;&amp;lt; 1) ^ (shiftX &amp;gt;&amp;gt; 31),
        (shiftY &amp;lt;&amp;lt; 1) ^ (shiftY &amp;gt;&amp;gt; 31),
    },
}&lt;/code&gt;
    &lt;p&gt;The geometry is a little odd, but here’s what’s happening:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We pass in a slice of instructions, which is made up of a command followed by parameters&lt;/item&gt;
      &lt;item&gt;To place a Point, we use the command &lt;code&gt;moveTo&lt;/code&gt;, to move the cursor&lt;/item&gt;
      &lt;item&gt;This command expects two parameters, &lt;code&gt;dX&lt;/code&gt;and&lt;code&gt;dY&lt;/code&gt;, which we pass in using&lt;code&gt;zigzag&lt;/code&gt;encoding&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We can go ahead and add this feature to our tile:&lt;/p&gt;
    &lt;code&gt;t.Layers[0].Features = append(t.Layers[0].Features, &amp;amp;feature)&lt;/code&gt;
    &lt;head rend="h2"&gt;3. Add properties&lt;/head&gt;
    &lt;p&gt;We now have a tile that meets the basic requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1 layer&lt;/item&gt;
      &lt;item&gt;1 feature in the layer with a valid geometry&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I now want to add the properties defined in the GeoJSON, in this case, the name and the status for the Statue of Liberty.&lt;/p&gt;
    &lt;p&gt;As with GeoJSON, you can store properties for each feature. So here, the status could be one of three:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;open&lt;/code&gt;: Open for visit.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;closed&lt;/code&gt;: Closed for visit.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;maintenance&lt;/code&gt;: Closed for maintenance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Adding the status &lt;code&gt;open&lt;/code&gt; to the Statue of Liberty in GeoJSON looks like this:&lt;/p&gt;
    &lt;code&gt;{
  "type": "Feature",
  "geometry": {
    "type": "Point",
    "coordinates": [-74.04452395542852, 40.68987850656795]
  },
  "properties": {
    "name": "Statue of Liberty",
    "status": "open"
  }
}&lt;/code&gt;
    &lt;p&gt;So, if I have 10’000 monuments, I’ll have 10,000 dictionaries like this:&lt;/p&gt;
    &lt;code&gt;"properties": {
    "name": "&amp;lt;Monument Name&amp;gt;",
    "status": "&amp;lt;Monument Status&amp;gt;"
  }&lt;/code&gt;
    &lt;p&gt;That means repeating &lt;code&gt;open&lt;/code&gt;, &lt;code&gt;closed&lt;/code&gt; or &lt;code&gt;maintenance&lt;/code&gt; 10,000 times. Best case scenario, that’s 40,000 bytes (&lt;code&gt;open&lt;/code&gt; 10,000 times),
and you probably want a unique name for your monument, so add another 10,000 strings to your file.
You can definitely improve this by shortening the status to something like &lt;code&gt;o&lt;/code&gt; for &lt;code&gt;open&lt;/code&gt;, and &lt;code&gt;s&lt;/code&gt; for &lt;code&gt;status&lt;/code&gt;, but this will only
get you so far as you scale (especially if you’re dealing with numbers, where these tricks won’t work) and add more properties and features.&lt;/p&gt;
    &lt;p&gt;Vector Tiles use a different approach: tags. In each layer, you define a set of keys and a set of values. In our previous example, the keys would be &lt;code&gt;[name, status]&lt;/code&gt; and the values would be &lt;code&gt;[Statue of Liberty, open, closed, maintenance]&lt;/code&gt;:
&lt;/p&gt;
    &lt;p&gt;So let’s add the known keys and values to our layer:&lt;/p&gt;
    &lt;code&gt;t.Layers[0].Keys = []string{"name", "status"}

SoLName := "Statue of Liberty"
StatusOpen := "open"
StatusClosed := "closed"
StatusMaintenance := "maintenance"
t.Layers[0].Values = []*Tile_Value{
    {StringValue: &amp;amp;SoLName},           // 0
    {StringValue: &amp;amp;StatusOpen},        // 1
    {StringValue: &amp;amp;StatusClosed},      // 2
    {StringValue: &amp;amp;StatusMaintenance}, // 3
}&lt;/code&gt;
    &lt;p&gt;Now that our layer knows what keys and values to expect, we can go ahead and set the property for the Statue of Liberty:&lt;/p&gt;
    &lt;code&gt;t.Layers[0].Features[0].Tags = []uint32{0, 0, 1, 1} // name: Statue of Liberty, status: open&lt;/code&gt;
    &lt;p&gt;If we wanted to update the status to inform users the Statue of Liberty is closed for maintenance, we would simply update the tags to:&lt;/p&gt;
    &lt;code&gt;t.Layers[0].Features[0].Tags = []uint32{0, 0, 1, 3} // name: Statue of Liberty, status: maintenance&lt;/code&gt;
    &lt;head rend="h2"&gt;4. Return the tile via HTTP&lt;/head&gt;
    &lt;p&gt;Our tile &lt;code&gt;t&lt;/code&gt; isready to be returned to the frontend via an HTTP GET request.
To convert our tile to the MVT binary, simply use:&lt;/p&gt;
    &lt;code&gt;import 	"github.com/gogo/protobuf/proto"

out, _ := proto.Marshal(t)&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;out&lt;/code&gt; contains the &lt;code&gt;[]byte&lt;/code&gt; data that represents the .mvt file. Be sure to set the following header in your HTTP response:&lt;/p&gt;
    &lt;code&gt;"Content-Type" : "application/vnd.mapbox-vector-tile"&lt;/code&gt;
    &lt;p&gt;And there it is - a vector tile built entirely from scratch, using geographic information contained in a GeoJSON file. When you’re using a map, you’ll almost always be visualizing more than one tile at a time, and zooming in and out. This means each feature will be on multiple tiles, one per zoom level, so you’ll need to design a strategy to handle this. The below code is a quick snippet of the implementation used for the Dashboard (some of the HTTP code is using an internal library, it can easily be replaced with your own HTTP server implementation / library):&lt;/p&gt;
    &lt;code&gt;type TileMap struct {
	Tiles map[int]map[int]map[int]*Tile // [z][x][y]Tile
}

func NewTileMap() *TileMap {
	tiles := make(map[int]map[int]map[int]*Tile)
	for _, zoom := range Zooms {
		tiles[zoom] = make(map[int]map[int]*Tile)
	}
	return &amp;amp;TileMap{Tiles: tiles}
}

func (tm *TileMap) GetTile(z int, x int, y int) *Tile {
	if xyMap, zExists := tm.Tiles[z]; zExists {
		if yMap, xExists := xyMap[x]; xExists {
			if tile, yExists := yMap[y]; yExists {
				return tile
			}
		}
	}
	return nil
}
tm := NewTileMap()

// Generate the tiles here and populate the tile map using the above code
// ...

h.PublicHandler("GET", "/layer/potholes/{z}/{x}/{y}/tile.mvt", func(r *http.Request) web.HttpResp {
		z := r.PathValue("z")
		x := r.PathValue("x")
		y := r.PathValue("y")
		zInt, _ := strconv.Atoi(z)
		xInt, _ := strconv.Atoi(x)
		yInt, _ := strconv.Atoi(y)
		var out []byte
		if tm == nil {
			out, _ = proto.Marshal(mvt.NewTile("empty"))
		} else {
			tile := tm.GetTile(zInt, xInt, yInt)
			out, err = proto.Marshal(tile)
			if err != nil {
				// Generate an empty tile
				out, _ = proto.Marshal(mvt.NewTile("empty"))
			}
		}
		return web.HttpResp{
			Data:        out,
			StatusCode:  http.StatusOK,
			ContentType: "application/vnd.mapbox-vector-tile",
		}
	})&lt;/code&gt;
    &lt;p&gt;We’ve now created an HTTP server that returns MVT files on demand, as a user explores the map.&lt;/p&gt;
    &lt;head rend="h2"&gt;Did the performance improve?&lt;/head&gt;
    &lt;p&gt;After deploying the MVT tile server for my GeoJSON layers that contain points, this is what we get:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File size decreased to 3.7Mb (I mean, we removed GeoJSON data, so no surprise there), meaning the site now loads in 500ms&lt;/item&gt;
      &lt;item&gt;The paint time has decreased (by about 200ms, not bad for migrating just 2 datasets over), and the performance score went up by 8% according to Lighthouse:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I’m running this process off of a server in my office, and I use Cloudflare Tunnels to expose the site, which unfortunately means the tiles don’t return very fast. This means I’m going to have to move this over to AWS, something I’ve been meaning to do, and I expect the tiles to return a lot faster, hopefully sub 100ms.&lt;/p&gt;
    &lt;p&gt;I still have to move over the non-Point layers that are still GeoJSON, so I expect a big gain in performance there. More soon!&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s next?&lt;/head&gt;
    &lt;p&gt;This simple example only focuses on Points. Implementing lines and polygons requires more work, as they can span multiple tiles. I’ll put out a new post once I’ve implemented these, along with performance metrics of the Dashboard.&lt;/p&gt;
    &lt;p&gt;I’m also looking forward to seeing MapLibre tiles ready for production use, so I can generate MapLibre tiles and get some additional performance gain and contribute to the MapLibre ecosystem.&lt;/p&gt;
    &lt;p&gt;I’m working on making the Go code open source in a small geographic library, which I’ll put on Hudson Shipping Co’s github.&lt;/p&gt;
    &lt;head rend="h3"&gt;Thank you!&lt;/head&gt;
    &lt;p&gt;Thank you for reading until the end! Feel free to reach out to me at [email protected] for comments or questions.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45126586</guid></item><item><title>WiFi signals can measure heart rate–no wearables needed</title><link>https://news.ucsc.edu/2025/09/pulse-fi-wifi-heart-rate/</link><description>&lt;doc fingerprint="a68d012e450b8340"&gt;
  &lt;main&gt;
    &lt;p&gt;Health&lt;/p&gt;
    &lt;head rend="h1"&gt;WiFi signals can measure heart rate—no wearables needed&lt;/head&gt;
    &lt;p&gt;Engineers prove their technique is effective even with the lowest-cost WiFi devices&lt;/p&gt;
    &lt;head rend="h2"&gt;Press Contact&lt;/head&gt;
    &lt;head rend="h2"&gt;Media Access&lt;/head&gt;
    &lt;head rend="h2"&gt;Key takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Pulse-Fi system is highly accurate, achieving clinical-level heart rate monitoring with ultra low-cost WiFi devices, making it useful for low resource settings.&lt;/item&gt;
      &lt;item&gt;The system works with the person in a variety of different positions and from up to 10 feet away.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Heart rate is one of the most basic and important indicators of health, providing a snapshot into a person’s physical activity, stress and anxiety, hydration level, and more.&lt;/p&gt;
    &lt;p&gt;Traditionally, measuring heart rate requires some sort of wearable device, whether that be a smart watch or hospital-grade machinery. But new research from engineers at the University of California, Santa Cruz, shows how the signal from a household WiFi device can be used for this crucial health monitoring with state-of-the-art accuracy—without the need for a wearable.&lt;/p&gt;
    &lt;p&gt;Their proof of concept work demonstrates that one day, anyone could take advantage of this non-intrusive WiFi-based health monitoring technology in their homes. The team proved their technique works with low-cost WiFi devices, demonstrating its usefulness for low resource settings.&lt;/p&gt;
    &lt;p&gt;A study demonstrating the technology, which the researchers have coined “Pulse-Fi,” was published in the proceedings of the 2025 IEEE International Conference on Distributed Computing in Smart Systems and the Internet of Things (DCOSS-IoT).&lt;/p&gt;
    &lt;head rend="h4"&gt;Measuring with WiFi&lt;/head&gt;
    &lt;p&gt;A team of researchers at UC Santa Cruz’s Baskin School of Engineering that included Professor of Computer Science and Engineering Katia Obraczka, Ph.D. student Nayan Bhatia, and high school student and visiting researcher Pranay Kocheta designed a system for accurately measuring heart rate that combines low-cost WiFi devices with a machine learning algorithm.&lt;/p&gt;
    &lt;p&gt;WiFi devices push out radio frequency waves into physical space around them and toward a receiving device, typically a computer or phone. As the waves pass through objects in space, some of the wave is absorbed into those objects, causing mathematically detectable changes in the wave.&lt;/p&gt;
    &lt;p&gt;Pulse-Fi uses a WiFi transmitter and receiver, which runs Pulse-Fi’s signal processing and machine learning algorithm. They trained the algorithm to distinguish even the faintest variations in signal caused by a human heart beat by filtering out all other changes to the signal in the environment or caused by activity like movement.&lt;/p&gt;
    &lt;p&gt;“The signal is very sensitive to the environment, so we have to select the right filters to remove all the unnecessary noise,” Bhatia said.&lt;/p&gt;
    &lt;head rend="h4"&gt;Dynamic results&lt;/head&gt;
    &lt;p&gt;The team ran experiments with 118 participants and found that after only five seconds of signal processing, they could measure heart rate with clinical-level accuracy. At five seconds of monitoring, they saw only half a beat-per-minute of error, with longer periods of monitoring time increasing the accuracy.&lt;/p&gt;
    &lt;p&gt;The team found that the Pulse-Fi system worked regardless of the position of the equipment in the room or the person whose heart rate was being measured—no matter if they were sitting, standing, lying down, or walking, the system still performed. For each of the 118 participants, they tested 17 different body positions with accurate results&lt;/p&gt;
    &lt;p&gt;These results were found using ultra-low-cost ESP32 chips, which retail between $5 and $10 and Raspberry Pi chips, which cost closer to $30. Results from the Raspberry Pi experiments show even better performance. More expensive WiFi devices like those found in commercial routers would likely further improve the accuracy of their system.&lt;/p&gt;
    &lt;p&gt;They also found that their system had accurate performance with a person three meters, or nearly 10 feet, away from the hardware. Further testing beyond what is published in the current study shows promising results for longer distances.&lt;/p&gt;
    &lt;p&gt;“What we found was that because of the machine learning model, that distance apart basically had no effect on performance, which was a very big struggle for past models,” Kocheta said. “The other thing was position—all the different things you encounter in day to day life, we wanted to make sure we were robust to however a person is living.”&lt;/p&gt;
    &lt;head rend="h4"&gt;Creating the dataset&lt;/head&gt;
    &lt;p&gt;To make their heart rate detection system work, the researchers needed to train their machine learning algorithm to distinguish the faint detections in WiFi signals caused by a human heartbeat. They found that there was no existing data for these patterns using an ESP32 device, so they set out to create their own dataset.&lt;/p&gt;
    &lt;p&gt;In the UC Santa Cruz Science and Engineering library, they set up their ESP32 system along with a standard oximeter to gather “ground truth” data. By combining the data from the Pulse-Fi setup with the ground truth data, they could teach a neural network which changes in signals corresponded with heart rate.&lt;/p&gt;
    &lt;p&gt;In addition to the ESP32 dataset they collected, they also tested Pulse-Fi using a dataset produced by a team of researchers in Brazil using a Raspberry Pi device, which created the most extensive existing dataset on WiFi for heart monitoring, as far as the researchers are aware.&lt;/p&gt;
    &lt;head rend="h4"&gt;Beyond heart rate&lt;/head&gt;
    &lt;p&gt;Now, the researchers are working on further research to extend their technique to detect breathing rate in addition to heart rate, which can be useful for the detection of conditions like sleep apnea. Unpublished results show high promise for accurate breathing rate and apnea detection.&lt;/p&gt;
    &lt;p&gt;Those interested in commercial use of this technology can contact Assistant Director of Innovation Transfer Marc Oettinger: marc.oettinger@ucsc.edu.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45127983</guid></item><item><title>Farewell to Meshnet</title><link>https://nordvpn.com/blog/meshnet-shutdown/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45128299</guid></item><item><title>Wikipedia survives while the rest of the internet breaks</title><link>https://www.theverge.com/cs/features/717322/wikipedia-attacks-neutrality-history-jimmy-wales</link><description>&lt;doc fingerprint="7e0edb1322339813"&gt;
  &lt;main&gt;
    &lt;p&gt;When armies invade, hurricanes form, or governments fall, a Wikipedia editor will typically update the relevant articles seconds after the news breaks. So quick are editors to change âisâ to âwasâ in cases of notable deaths that they are said to have the fastest past tense in the West. So it was unusual, according to one longtime editor who was watching the page, that on the afternoon of January 20th, 2025, hours after Elon Musk made a gesture resembling a Nazi salute at a rally following President Donald Trumpâs inauguration and well into the ensuing public outcry, no one had added the incident to the encyclopedia.&lt;/p&gt;
    &lt;p&gt;Then, just before 4PM, an editor by the name of PickleG13 added a single sentence to Muskâs 8,600-word biography: âMusk appeared to perform a Nazi salute,â citing an article in The Jerusalem Post. In a note explaining the change, the editor wrote, âThis controversy will be debated, but it does appear and is being reported that Musk may have performed a Hitler salute.â Two minutes later, another editor deleted the line for violating Wikipediaâs stricter standards for unflattering information in biographies of living people.&lt;/p&gt;
    &lt;p&gt;But PickleG13 was correct. That evening, as the controversy over the gesture became a vortex of global attention, another editor called for an official discussion about whether it deserved to be recorded in Wikipedia. At first, the debate on the articleâs âtalk page,â where editors discuss changes, was much the same as the one playing out across social media and press: it was obviously a Nazi salute vs. it was an awkward wave vs. it couldnât have been a wave, just look at the touch to his shoulder, the angle of his palm vs. heâs autistic vs. no, heâs antisemitic vs. I donât see the biased media calling out Obama for doing a Nazi salute in this photo I found on Twitter vs. thatâs just a still photo, stop gaslighting people about what they obviously saw. But slowly, through the barbs and rebuttals and corrections, the trajectory shifted.&lt;/p&gt;
    &lt;p&gt;Wikipedia is the largest compendium of human knowledge ever assembled, with more than 7 million articles in its English version, the largest and most developed of 343 language projects. Started nearly 25 years ago, the site was long mocked as a byword for the unreliability of information on the internet, yet today it is, without exaggeration, the digital worldâs factual foundation. Itâs what Google puts at the top of search results otherwise awash in ads and spam, what social platforms cite when they deign to correct conspiracy theories, and what AI companies scrape in their ongoing quest to get their models to stop regurgitating info-slurry â and consult with such frequency that they are straining the encyclopediaâs servers. Each day, itâs where approximately 70 million people turn for reliable information on everything from particle physics to rare Scottish sheep to the Erfurt latrine disaster of 1184, a testament both to Wikipediaâs success and to the total degradation of the rest of the internet as an information resource.&lt;/p&gt;
    &lt;p&gt;But as impressive as this archive is, it is the byproduct of something that today looks almost equally remarkable: strangers on the internet disagreeing on matters of existential gravity and breathtaking pettiness and, through deliberation and debate, building a common ground of consensus reality.&lt;/p&gt;
    &lt;p&gt;âOne of the things I really love about Wikipedia is it forces you to have measured, emotionless conversations with people you disagree with in the name of trying to construct the accurate narrative,â said DF Lovett, a Minnesota-based writer and marketer who mostly edits articles about local landmarks and favorite authors but later joined the salute debate to argue that âElon Musk straight-arm gesture controversyâ was a needlessly awkward description. âItâs basically the only place on the internet that doesnât function as a confirmation bias machine,â he said, which is also why he thinks people sometimes get mad at it. Wikipedia is one of the few platforms online where tremendous computing power isnât being deployed in the service of telling you exactly what you want to hear.&lt;/p&gt;
    &lt;p&gt;Whether Musk had made a Nazi salute or was merely awkward, the editors decided, was not for them to say, even if they had their opinions. What was a fact, they agreed, was that on January 20th, Musk had âtwice extended his right arm toward the crowd in an upward angle,â that many observers compared the gesture to a Nazi salute, and that Musk denied any meaning behind the motion. Consensus was reached. The lines were added back. Approximately 7,000 words of deliberation to settle, for a time, three sentences. This was Wikipediaâs process working as intended.&lt;/p&gt;
    &lt;p&gt;It was at this point that Musk himself cannonballed into the discourse, tweeting that the encyclopedia was âlegacy media propaganda!â&lt;/p&gt;
    &lt;p&gt;This was not Muskâs first time attacking the site â that appears to have been in 2019, when he complained that it accurately described him as an early investor in Tesla rather than its founder. But recently he has taken to accusing the encyclopedia of a liberal bias, mocking it as âwokepedia,â and calling for it to be defunded. In so doing, he has joined a growing number of powerful people, groups, and governments that have made the site a target. In August, Republicans on the US House Oversight Committee sent a letter to the Wikimedia Foundation requesting information on attempts to âinject biasâ into the encyclopedia and data about editors suspected of doing so.&lt;/p&gt;
    &lt;p&gt;When governments have cowed the press and flooded social platforms with viral propaganda, Wikipedia has become the next target, and a more stubborn one. Because it is edited by thousands of mostly pseudonymous volunteers around the world â and in theory, by anyone who feels like it â its contributors are difficult for any particular state to persecute. Since itâs supported by donations, there is no government funding to cut off or advertisers to boycott. And it is so popular and useful that even highly repressive governments have been hesitant to block it.&lt;/p&gt;
    &lt;p&gt;Instead, they have developed an array of more sophisticated strategies. In Hong Kong, Russia, India, and elsewhere, government officials and state-aligned media have accused the site of ideological bias while online vigilantes harass editors. In several cases, editors have been sued, arrested, or threatened with violence.&lt;/p&gt;
    &lt;p&gt;When several dozen editors gathered in San Francisco this February, many were concerned that the US could be next. The US, with its strong protections for online speech, has historically been a refuge when the encyclopedia has faced attacks elsewhere in the world. It is where the Wikimedia Foundation, the nonprofit that supports the project, is based. But the site has become a popular target for conservative media and influencers, some of whom now have positions in the Trump administration. In January, the Forward published slides from the Heritage Foundation, the think tank responsible for Project 2025, outlining a plan to reveal the identities of editors deemed antisemitic for adding information critical of Israel, a cudgel that the administration has wielded against academia.&lt;/p&gt;
    &lt;p&gt;âItâs about creating doubt, confusion, attacking sources of trust,â an editor told the assembled group. âIt came for the media and now itâs coming for Wikipedia and we need to be ready.â&lt;/p&gt;
    &lt;p&gt;In 1967, Hannah Arendt published an essay in The New Yorker about what she saw as an inherent conflict between politics and facts. As varieties of truth go, she wrote, facts are fragile. Unlike axioms and mathematical proofs that can be derived by anyone at any time, there is nothing necessary about the fact, to use Arendtâs example, that German troops crossed the border with Belgium on the night of August 4th, 1914, and not some other border at some other time. Like all facts, this one is established through witnesses, testimony, documents, and collective agreement about what counts as evidence â it is political, and as the propaganda machines of the 20th century showed, political power is perfectly capable of destroying it. Furthermore, they will always be tempted to, because facts represent a sort of rival power, a constraint and limit âhated by tyrants who rightly fear the competition of a coercive force they cannot monopolize,â and at risk in democracies, where they are suspiciously impervious to public opinion. Facts, in other words, donât care about your feelings. âUnwelcome facts possess an infuriating stubbornness,â Arendt wrote.&lt;/p&gt;
    &lt;p&gt;This infuriating stubbornness turns out to be important, though. A lie might be more plausible or useful than a fact, but it lacks a factâs dumb arbitrary quality of being the case for no particular reason and no matter your opinion or influence. History once rewritten can be rewritten again and becomes insubstantial. Rather than believe the lie, people stop believing anything at all, and even those in power lose their bearings. This gives facts âgreat resiliencyâ that is âoddly combinedâ with their fragility. Having a stubborn common ground of shared reality turns out to be a basic precondition of collective human life â of politics. Even political power seems to recognize this, Arendt wrote, when it establishes ideally impartial institutions insulated from its own influence, like the judiciary, the press, and academia, charged with producing facts according to methods other than the pure exercise of power.&lt;/p&gt;
    &lt;p&gt;Wikipedia has come to play a similar role of factual ballast to an increasingly unmoored internet, but without the same institutional authority and with its own methods developed piecemeal over the last two decades for arriving at consensus fact. How to defend it from political attacks is not straightforward. At the conference, many editors felt both that attacks from the Trump administration were a genuine threat and that being cast as âthe resistanceâ risked jeopardizing the encyclopediaâs position of trusted neutrality.&lt;/p&gt;
    &lt;p&gt;âI would really argue not to take the attack approach, to really take the passive approach,â said one editor when someone broached the idea of actively debunking some of the false information swamping the rest of the internet. âPeople see us as credible because we donât attack, because we are just providing information to everyone all the time in a boring way. Sometimes boring is good. Boring is credible.â&lt;/p&gt;
    &lt;p&gt;Even the editor at the summit who had been most directly affected by the Trump administration urged against a direct response. Jamie Flood had been a librarian and outreach specialist at the National Agricultural Library, where among other duties she led group trainings and uploaded research on topics like germplasm and childhood nutrition to Wikipedia. Museums and libraries around the world employ such âWikipedians in residenceâ to act as liaisons with the encyclopediaâs community for the same reason that the World Health Organization partnered with Wikipedia during the covid-19 pandemic to make the latest information available: if you want research to reach the public, there is no better place.&lt;/p&gt;
    &lt;p&gt;Along with several other Wikipedians employed by the federal government, Flood had just been laid off by DOGE, collateral damage in a general dismantling of research and archival institutions. âIâm a casualty of this administrationâs war on information,â Flood said.&lt;/p&gt;
    &lt;p&gt;Still, Wikipedia absolutely should not counterattack, Flood said. âWikipedia is always in the background. Theyâre not making a big statement, and I donât think they should. Iâve been training people for a long time and I still go back to this early quote of Jimmy Wales, one of the founders: âImagine a world where all knowledge is freely available to everyone.â Thatâs enough. Thatâs a statement in and of itself. In a time of misinformation, in a time of suppression, having this place where people can come and bring knowledge and share knowledge, that is a statement.â&lt;/p&gt;
    &lt;p&gt;Wikipedia should be, in other words, as stubborn as a fact. But then, facts are fragile things.&lt;/p&gt;
    &lt;p&gt;A common refrain among Wikipedians is that the site works in practice but not in theory. It seems to flout everything weâve learned about human behavior online: anonymous strangers discussing divisive topics and somehow, instead of dissolving into factions and acrimony, working together to build something of value.&lt;/p&gt;
    &lt;p&gt;The projectâs origins go back to 1999. Wales, a former options trader who had founded a laddish web portal called Bomis, wanted to start a free online encyclopedia. He hired an acquaintance from an Ayn Rand listserv that Wales previously ran, a philosophy PhD student named Larry Sanger. Their first attempt, called Nupedia, was not so different from encyclopedias as they have existed since Diderotâs EncyclopÃ©die in 1751. Experts would write articles that went through seven stages of editorial review. It was slow going. After a year, Nupedia had just over 20 articles.&lt;/p&gt;
    &lt;p&gt;In an attempt to speed things along, they decided to experiment with wikis, a web format gaining popularity among open-source software developers that allowed multiple people to collaboratively edit a project. (Wiki is the Hawaiian word for âquick.â) The wiki was intended to be a forum where the general public could contribute draft articles that would then be fed into Nupediaâs peer-review pipeline, but the experts objected and the crowdsourced site was given its own domain, Wikipedia.com. It went live on January 15th, 2001. Within days, it had more articles than all of Nupedia, albeit of varying quality. After a year, Wikipedia had more than 20,000 articles.&lt;/p&gt;
    &lt;p&gt;There were few rules at first, but one that Wales said was ânon-negotiableâ was that Wikipedia should be written from a âneutral point of view.â The policy, abbreviated as NPOV, was imported from the ânonbias policyâ Sanger had written for Nupedia. But on Wikipedia, Wales considered it as much a âsocial concept of cooperationâ as an editorial standard. If this site was going to be open to anyone to edit, the only way to avoid endless flame wars over who is right was, provocatively speaking, to set questions of truth aside. âWe could talk about that and get nowhere,â Wales wrote to the Wikipedia email list. âPerhaps the easiest way to make your writing more encyclopedic is to write about what people believe, rather than what is so,â he explained.&lt;/p&gt;
    &lt;p&gt;Ideally, the neutrality principle would allow people of different views to agree, if not on the matter at hand, then at least on what it was they were disagreeing about. âIf youâve got a kind and thoughtful Catholic priest and a kind and thoughtful Planned Parenthood activist, theyâre never going to agree about abortion, but they can probably work together on an article,â Wales would later say.&lt;/p&gt;
    &lt;p&gt;This view faced an immediate challenge, which is that people believe all sorts of things: that the Earth is 6,000 years old, that climate change is a scam, that the Holocaust was a hoax, that the Irish potato famine was overblown, that chiropractors are all charlatans, that they have discovered a new geometry, and that Mother Teresa was a jerk.&lt;/p&gt;
    &lt;p&gt;In response, the early volunteers added another rule. You canât just say things; any factual claim needs a citation that readers can check for themselves. When people started emailing Wales their proofs that Einstein was wrong about relativity, he clarified that the cited source could not be your own âoriginal research.â Sorry, Wales wrote to an Einstein debunker, it doesnât matter whether your theory is true. When it is published in a physics journal, you can cite that.&lt;/p&gt;
    &lt;p&gt;Instead of trying to ascertain the truth, editors assessed the credibility of sources, looking to signals like whether a publication had a fact-checking department, got cited by other reputable sources, and issued corrections when it got things wrong.&lt;/p&gt;
    &lt;p&gt;At their best, these ground rules ensured debates followed a productive dialectic. An editor might write that human-caused climate change was a fact; another might change the line to say there was ongoing debate; a third editor would add the line back, backed up by surveys of climate scientists, and demand peer-reviewed studies supporting alternate theories. The outcome was a more accurate description of the state of knowledge than many journalists were promoting at the time by giving âboth sidesâ equal weight, and also a lot of work to arrive at. A 2019 study published in Nature found that Wikipediaâs most polarizing articles â eugenics, global warming, Leonardo DiCaprio â are the highest quality, because each side keeps adding citations in support of their views. Wikipedia: a machine for turning conflict into bibliographies.&lt;/p&gt;
    &lt;p&gt;Coupled with some technical features of wikis, like the ability for anyone to edit anyone elseâs writing, and some early administrative rules, like not being allowed to undo someone elseâs edit more than three times per day, users were practically forced to talk through disagreements and arrive at âconsensus.â This became Wikipediaâs governing principle.&lt;/p&gt;
    &lt;p&gt;This may make the process sound more peaceful than it is. Disputes were constant. Early on, Sanger, who had remained partial to a more hierarchical, expert-driven model, clashed repeatedly with editors he decried as âanarchistsâ and demanded greater authority for himself, which the editors rejected. When revenue from Bomis dried up after the dot-com crash, Wales laid Sanger off and took over management of the project.&lt;/p&gt;
    &lt;p&gt;Wales governed from a greater remove, appearing only occasionally to broker peace between warring editors, resolve an impasse, or reassure people that they didnât need to spend time devising procedures to screen out a sudden influx of neo-Nazis that were planning to overwhelm discussion, because if they showed up, âI will personally ban them all if necessary, and thatâs that.â Editors sometimes ironically referred to him as their âGod Kingâ or âbenevolent dictator,â but he described his role as a sort of constitutional monarch safeguarding the community as it developed the processes to fully govern itself. Because Wikipedia was under a Creative Commons license, anyone who didnât like the way the project was run could copy it and start their own, as a group of Spanish users did when the possibility of running ads was raised in 2002. The next year Wales established a nonprofit, the Wikimedia Foundation, to raise funds and handle the technical and legal work required to keep the project running. The encyclopedia itself, however, would be entirely edited and managed by volunteers.&lt;/p&gt;
    &lt;p&gt;In early 2004, Wales delegated his moderating powers to a group of elected editors, called the Arbitration Committee. From that point onward, he was essentially another editor, screenname Jimbo Wales, liable to have his edits undone like anyone else. He attempted several times to update his own birthdate to reflect the fact that his mother says he was born slightly before midnight on August 7th, 1966, not on August 8th, as his birth certificate read, only to be reprimanded for editing his own page and trying to cite his own âoriginal research.â (After several years of debates and citable coverage from reliable sources, August 7th eventually won, with a note explaining the discrepancy.)&lt;/p&gt;
    &lt;p&gt;Over the ensuing two decades, editors amended policies to cope with conspiracy theorists, revisionist historians, militant fandoms, and other perennial goblins of the open web. There were the three core content guidelines of Neutral Point of View, Verifiability, and No Original Research; the five pillars of Wikipedia; and a host of rules around editor conduct, like the injunction to avoid ad hominem attacks and assume good faith of others, defined and refined in interlinked articles and essays. There are specialized forums and noticeboards where editors can turn for help making an article more neutral, figuring out whether a source was reliable, or deciding whether a certain view was fringe or mainstream. By 2005, the pages where editors stipulated policy and debated articles were found to be growing faster than the articles themselves. Today, this administrative backend is at least five times the size of the encyclopedia it supports.&lt;/p&gt;
    &lt;p&gt;The most important thing to know about this system is that, like the neutrality principle from which it arose, it largely ignores content to focus on process. If editors disagree about, for example, whether the article for the uninhabited islands claimed by both Japan and China should be titled âSenkaku Islands,â âDiaoyu Islands,â or âPinnacle Islands,â they first try to reach an agreement on the articleâs Talk page, not by arguing who is correct, but by arguing which sideâs position better accords with specific Wikipedia policies. If they canât agree, they can summon an uninvolved editor to weigh in, or file a ârequest for commentâ and open the issue to wider debate for 30 days.&lt;/p&gt;
    &lt;p&gt;If this fails and editors begin to quarrel, they might get called before the Arbitration Committee, but this elected panel of editors will also not decide who is right. Instead, they will examine the reams of material generated by the debate and rule only on who has violated Wikipedia process. They might ban an editor for 30 days for conspiring off-Wiki to sway debate, or forbid another editor from working on articles about Pacific islands over repeated ad hominem attacks, or in extreme cases ban someone for life. Everyone else can go back to debating, following the process this time.&lt;/p&gt;
    &lt;p&gt;As a result, explosive political controversies and ethnic conflicts are reduced to questions of formatting consistency. But because process decides all, process itself can be a source of intense strife. The topics of âgun controlâ and âthe Balkansâ are officially designated as âcontentiousâ due to recurring edit wars, where people keep reverting each otherâs edits without attempting to build consensus; so, too, are the Wikipedia manual of style and the question of what information belongs in sidebars. In one infamous battle, debate over whether to capitalize âintoâ in the film title Star Trek Into Darkness raged for more than 40,000 words.&lt;/p&gt;
    &lt;p&gt;In 2009, law professors David A. Hoffman and Salil K. Mehra published a paper analyzing conflicts like these on Wikipedia and noted something unusual. Wikipediaâs dispute resolution system does not actually resolve disputes. In fact, it seems to facilitate them continuing forever.&lt;/p&gt;
    &lt;p&gt;These disputes may be crucial to Wikipediaâs success, the researchers wrote. Online communities are in perpetual danger of dissolving into anarchy. But because disputes on Wikipedia are won or lost based on who has better followed Wikipedia process, every dispute becomes an opportunity to reiterate the projectâs rules and principles.&lt;/p&gt;
    &lt;p&gt;Trolls who repeatedly refuse to follow the process eventually get banned, but initial infractions are often met with explanations of how Wikipedia works. Several of the editors I spoke with began as vandals only to be won over by someone explaining to them how they could contribute productively. Editors will often restrict who can work on controversial topics to people who have logged a certain number of edits, ensuring that only those bought into the ethos of the project can participate.&lt;/p&gt;
    &lt;p&gt;In 2016, researchers published a study of 10 years of Wikipedia edits about US politics. They found that articles became more neutral over time â and so, too, did the editors themselves. When editors arrived, they often proposed extreme edits, received pushback, and either left the project or made increasingly moderate contributions.&lt;/p&gt;
    &lt;p&gt;This is obviously not the reigning dynamic of the rest of the internet. The social platforms where culture and politics increasingly play out are governed by algorithms that have the opposite effect of Wikipediaâs bureaucracy in nearly every respect. Optimized to capture attention, they boost the novel, extreme, and sensational rather than subjecting them to increased scrutiny, and by sending content to users most likely to engage with it, they sort people into clusters of mutual agreement. This phenomenon has many names. Filter bubbles, epistemological fragmentation, bespoke realities, the sense that everyone has lost their minds. On Wikipedia, itâs called a âpoint of view split,â and editors banned it early. You are simply not allowed to make a new article on the same topic. Instead, you must make the case for a given perspectiveâs place amid all the others while staying, literally, on the same page.&lt;/p&gt;
    &lt;p&gt;In February, the conservative organization Media Research Center released a report claiming that âWikipedia Effectively Blacklists ALL Right-Leaning Media.â It was essentially a summary of a publicly available policy page on Wikipedia that lists discussions about the reliability of sources and color codes them according to the latest consensus â green for generally reliable, yellow for lack of clear consensus, and red for generally unreliable. ProPublica is green because it has an âexcellent reputation for fact-checking and accuracy, is widely cited by reliable sources, and has received multiple Pulitzer Prizes.â Newsweek is yellow after a decline in editorial standards following its 2013 acquisition and recent use of AI to write articles. Newsmax, the One America News Network, and several other popular right-leaning sources are red due to repeatedly publishing stories that were proven wrong. (As are some left-leaning sources, like Occupy Democrats.) The New York Post (generally unreliable, but marginally reliable on entertainment) used the report as the basis for an editorial titled âBig Tech must block Wikipedia until it stops censoring and pushing disinformation.â&lt;/p&gt;
    &lt;p&gt;The page is called Reliable sources/Perennial sources, as in sources that are perennially discussed. Editors made the page in 2018 as a repository for past discussions that they could refer to instead of having to repeatedly debate the reliability of the Daily Mail â the first publication to be deprecated, the year before â every time someone tried to cite it. It is not a list of preapproved or banned sources, the page reads. Context matters, and consensus can change.&lt;/p&gt;
    &lt;p&gt;But to Wikipediaâs critics, the page has become a symbol of the encyclopediaâs biases. Sanger, the briefly tenured cofounder, has found a receptive audience in right-wing activist Christopher Rufo and other conservatives by claiming Wikipedia has strayed from its neutrality principle by making judgments about the reliability of sources. Instead, he argues, it should present all views equally, including things âmany Republicans believe,â like the existence of widespread fraud in the 2020 election and the FBI playing a role in the January 6th Capitol attack.&lt;/p&gt;
    &lt;p&gt;Last spring, the reliable source page collided with one of the most intense political flashpoints on Wikipedia, the Israel-Palestine conflict. In April, an editor asked whether it was time to reevaluate the reliability of the Anti-Defamation League in light of changes to the way it categorizes antisemitic incidents to include protests of Israel, among other recent controversies. About 120 editors debated the topic for two months, producing text equal to 1.9 The Old Man and the Seas, or âtomats,â a standard unit of Wikipedia discourse. The consensus was that the ADL was reliable on antisemitism generally but not when the Israel-Palestine conflict was involved.&lt;/p&gt;
    &lt;p&gt;Unusually for a Wikipedia administrative process, the decision received enormous attention. The Times of Israel called it a âstaggering blowâ for the ADL, which mustered Jewish groups to petition the foundation to overrule the editors. The foundation responded with a fairly technical explanation of how Wikipediaâs self-governing reliability determinations work.&lt;/p&gt;
    &lt;p&gt;In the year since, conservative and pro-Israel organizations have published a series of reports examining the edit histories of articles to make a case that Wikipedia is biased against Israel. In March, the ADL itself issued one such report, called âEditing for Hate,â claiming that a group of 30 âmalicious editorsâ slanted articles to be critical of Israel and favorable to Palestine. As evidence, the report highlights examples like the removal of the phrase âPalestinian terrorismâ from the introduction of the article on Palestinian political violence.&lt;/p&gt;
    &lt;p&gt;Yet the edit histories show that these examples are often plucked from long editing exchanges, the outcome of which goes unmentioned. The âterrorismâ line that the ADL cited was indeed removed â it had also only just been added, was added back shortly after being cut, then was removed again, added back, and revised repeatedly before editors brokered a compromise on the talk page.&lt;/p&gt;
    &lt;p&gt;Breitbart, Pirate Wires, and other right-leaning publications now regularly mine Wikipediaâs lengthy debates for headlines like âHow Wikipedia Launders Regime Propaganda,â accusing the site of being a mouthpiece for the Democratic Party, or âCover Up: Wikipedia Editors Propose Deleting Page on Iran Advocating for Israelâs Destruction,â despite the article having just been created and the outcome being to merge the contents into the article on Iran-Israel relations. These reports are a dependable source of viral outrage on X. The strategy also appears effective at convincing lawmakers. In May, Rep. Debbie Wasserman Schultz (D-FL) and 22 other members wrote to the Wikimedia Foundation citing the ADL report and demanding Wikimedia ârein in antisemitism, uphold neutrality.â&lt;/p&gt;
    &lt;p&gt;The August letter from House Republicans requesting information on attempts to influence the encyclopedia, data on editors who had been disciplined by Arbcom, and other records also cited the ADL report.&lt;/p&gt;
    &lt;p&gt;While some search for bias in the minutiae of edit histories, others try to encompass all of Wikipedia. Last year, a researcher at the conservative Manhattan Institute scraped Wikipedia for mentions of political terms and public officials and used a GPT language model to analyze them for bias. The report found âa mild to moderateâ tendency to associate figures on the political right with more negative sentiment than those on the left. The study, which was not peer reviewed, has become a regular fixture in claims of liberal bias on Wikipedia.&lt;/p&gt;
    &lt;p&gt;The report still illustrates the challenges of evaluating the neutrality of a text as vast and stripped of subjective opinion as Wikipedia. An examination of the datasets shows that the passages GPT classified as non-neutral are often anodyne factual statements: that a lawmaker won or lost an election, represented a certain district, or died. It also conflated unrelated people of the same name, so, for example, most of the non-neutral statements about Mike Johnson concern not Mike Johnson the current Republican House Majority Speaker but a robber in a 1923 silent film, a prog-rock guitarist, multiple football players, and a famous yodeler.&lt;/p&gt;
    &lt;p&gt;But the more fundamental question is whether balanced sentiment or balanced anything across the contemporary political spectrum is the correct expectation for a project that operates by a different standard, one based on measures of reliability. Supposing the sentiment readings do reflect a real imbalance, is that due to the biases of editors, biases in their sources, or some other external imbalance, like a tendency by right-leaning politicians to express negative sentiments of fear or anger (a possibility the report raises, then dismisses).&lt;/p&gt;
    &lt;p&gt;Wikipedia has a long history of attempting to disentangle and correct its various biases. The siteâs editor community has been overwhelmingly white, male, and based in the United States and Europe since the site began. In 2018, 90 percent of editors were men, and only 18 percent of biographies in the encyclopedia were of women. That year, the Canadian physicist Donna Strickland won a Nobel Prize, and people turning to Wikipedia to learn about her discovered she lacked an article.&lt;/p&gt;
    &lt;p&gt;But the causal connection between these facts was not straightforward. Women have been historically excluded from the sciences, underrepresented in coverage of the sciences, and therefore underrepresented in the sources Wikipedia editors can cite. An editor had tried to make an article on Strickland several months before the Nobel but was overruled due to a lack of coverage in reliable sources. âWikipedia is a mirror of the worldâs biases, not the source of them. We canât write articles about what you donât cover,â tweeted then-executive director Katherine Maher.&lt;/p&gt;
    &lt;p&gt;Wikipediaâs sourcing guidelines are conservative in their deference to traditional institutions of knowledge production, like established newsrooms and academic peer review, and this means that it is sometimes late to ideas in the process of moving from fringe to mainstream. The possibility that covid-19 emerged from a lab was relegated to a section on conspiracy theories and is only now, after reporting by reliable sources, gaining a toehold on the covid pandemic article. Similarly, as awareness grew of the ways Western academic and journalistic institutions have excluded the perspectives of colonized people, critics argued that Wikipediaâs reliance on these same institutions made it impossible for the encyclopedia to be truly comprehensive.&lt;/p&gt;
    &lt;p&gt;Not all the bias comes from the projectâs sources, though. A study that attempted to control for offline inequalities by examining only contemporary sociologists of similar achievement found that male academics were still more likely to have articles. As volunteers, editors work on topics they think are important, and the encyclopediaâs emphases and omissions reflect their demographics. Minor skirmishes in World War II and every episode of The Simpsons have an article, some of which are longer than the articles on the Ethiopian civil war or climate change in the Maldives. In an effort to fill in these gaps, the foundation has for several years funded editor recruitment and training initiatives under the banner of âknowledge equity.â&lt;/p&gt;
    &lt;p&gt;âMost editors on Wikipedia are English-speaking men, and our coverage is of things that are of interest to English-speaking men,â said a retired market analyst in Cincinnati who has been editing for over 20 years. âOur sports coverage is second to none. Video games, we got it covered. Wars, the history of warfare, my god. Trains, radio stations... But our coverage of foods from other countries is very low, and there is an absolute systemic bias against coverage of women and people of color.â For her part, she tries to fill gaps around food, creating new articles whenever she encounters a Peruvian chili sauce or African fufu that lacks one.&lt;/p&gt;
    &lt;p&gt;Yet these initiatives have come under attack as âDEIâ by conservative influencers and Musk, who called for Wikipedia to be defunded until âthey restore balance.â&lt;/p&gt;
    &lt;p&gt;These accusations of bias, familiar from attacks on the media and social platforms, encounter some unique challenges when leveled against Wikipedia. Crucially, if you think something is wrong on Wikipedia, you can fix it yourself, though it will require making a case based on verifiability rather than ideological âbalance.â&lt;/p&gt;
    &lt;p&gt;Over the years, Wikipedia has developed an immune response to outside grievances. When people on X start complaining about Wikipediaâs suppression of UFO sightings or refusal to change the name of the Gulf of Mexico to Gulf of America, an editor often restricts the page to people who are logged in and puts up a notice directing newcomers to read the latest debate. If anything important was missed, they are welcome to suggest it, the notice reads, provided their suggestion meets Wikipediaâs rules, which can be read about on the following pages. That is, Wikipediaâs first and best line of defense is to explain how Wikipedia works.&lt;/p&gt;
    &lt;p&gt;Occasionally, people stick around and learn to edit. More often, they get bored and leave.&lt;/p&gt;
    &lt;p&gt;It was not unusual for skirmishes to break out over the Wikipedia page for Asian News International, or ANI. It is the largest newswire service in India, and as its Wikipedia article explains, it has a history of promoting false anti-Muslim and pro-government propaganda. It was these facts that various anonymous editors â not logged into Wikipedia accounts, so appearing only as IP addresses â attempted to remove last spring.&lt;/p&gt;
    &lt;p&gt;As typically happens, an experienced editor quickly reinstated the deleted sentences, noting that they had been removed without explanation. Then came another drive-by edit: actually, ANI is not propaganda and very credible, someone wrote, citing a YouTube video. Reverted: YouTube commentary is not a reliable source. Then another IP address, deleting a sentence about ANI promoting a false viral story about necrophilia in Pakistan. Reverted again. Another IP address, deleting the mention of propaganda with the explanation that the sources were âleftist dogs and swine.â&lt;/p&gt;
    &lt;p&gt;As the edit battle escalated, an editor locked the page so that only people who were logged in and had made a certain number of edits could make changes, ending the barrage of IP addresses.&lt;/p&gt;
    &lt;p&gt;Two months later, ANI sued.&lt;/p&gt;
    &lt;p&gt;The lawsuit revealed that several of the IP addresses had belonged to representatives of ANI attempting to remove unflattering information about the company. Blocked from doing so, ANI sued for defamation under a recent amendment to Indiaâs equivalent of Section 230 that places stricter requirements on platforms to moderate content. When the Wikimedia Foundation declined to reveal the identities of three editors who had defended the page, the presiding judge said he would ask the government to block the site, threatening to cut off the country with the highest number of English Wikipedia readers after the US and the UK. âIf you donât like India,â the judge said, âplease donât work in India.â&lt;/p&gt;
    &lt;p&gt;During the appeal, Wikimediaâs lawyer argued that disclosing the identities of editors would destroy the encyclopediaâs self-regulating system and expose contributors to reprisals. Also, he noted, the sentences in question, like every assertion on Wikipedia, were only summarizing other sources, and those sources â the publications The Caravan and The Ken â had not been sued for defamation. (As with editors, the foundationâs first response to external threats is often to explain how Wikipedia works.) The judge dismissed the argument, saying that journalism might be âread by a hundred people, you donât bother about itâ¦ it does not have the gravitas.â Wikipedia, however, is read by millions.&lt;/p&gt;
    &lt;p&gt;By this point the case had garnered enough coverage to warrant its own Wikipedia page. This seemed to enrage the judge, particularly the line noting that the judgeâs demand to reveal the identities of editors had been described as âcensorship and a threat to the flow of information.â This âborders on contempt,â the judge said, demanding that the foundation take the page down within 36 hours. In a rare move, the foundation complied.&lt;/p&gt;
    &lt;p&gt;The case alarmed editors around the world. An open letter calling on the Wikimedia Foundation to protect the anonymity of the editors garnered more than 1,300 signatures, the most of any letter directed at the foundation. Nevertheless, last December, the foundation disclosed the editorsâ identities to the judge under seal. Responding to outrage on Wikipediaâs editor forum, Wales asked for calm and urged people not to jump to conclusions.&lt;/p&gt;
    &lt;p&gt;The Wikimedia Foundation has historically taken a hard line against attempts to influence the project. In 2017, when the Turkish government demanded several articles be deleted, Wikipedia refused and was blocked for nearly three years as it fought to the countryâs Constitutional Court and won. For the second half of 2024, the most recent data available, the foundation complied with about 8 percent of requests for user data, compared to Googleâs 82 percent and Metaâs 77 percent. And the data provided was sparse, because Wikipedia retains almost none.&lt;/p&gt;
    &lt;p&gt;But attempts to influence the site have grown more sophisticated. The change is likely due to multiple factors: a global rise of political movements that wish to control independent media, the increased centrality of Wikipedia, and a technical change to the website itself. In 2015, Wikipedia switched to the encrypted HTTPS extension by default, making it impossible to see what pages users visited, only that they were visiting the Wikipedia domain. This meant that governments that had previously been censoring specific articles on opposition figures or historic protests had to choose between blocking all of Wikipedia or none of it. Almost every country save China (and Russia, for several hours) chose to not to block it. This was a victory for open knowledge, but it also meant governments had a greater interest in controlling what was written in the encyclopedia.&lt;/p&gt;
    &lt;p&gt;Instead of brute censorship, what has emerged is a sort of gray-zone information warfare. After mainland China quashed protests against the Hong Kong national security law in 2019, a battle began over how the protests would be remembered. Editors in mainland China â which can edit using VPNs â argued for the inclusion of state-friendly media that described the protests as âriotsâ or âterrorist attacksâ while removing citations to independent media for unreliability and bias. In one case, an editor attempted to strip all citations to one of Hong Kongâs premier papers, Apple Daily, hours before it was shut down by the government. By conspiring offline and using fake accounts, they won elections to admin positions and with them the power to see other editorsâ IP addresses, which they discussed using to reveal their opponentsâ identities to the police. Shortly afterward, the Wikimedia Foundation banned or restricted more than a dozen editors operating from mainland China, saying that the project had been âinfiltratedâ and that âsome users have been physically harmed as a result.â&lt;/p&gt;
    &lt;p&gt;Russia employed similar tactics after its invasion of Ukraine in 2022. State media and government officials attacked Wikipedia in the press with accusations of anti-Russian bias, promulgation of fake news, and foreign manipulation. The site remained accessible, but Russian search engines put a banner above it saying it was in violation of the law. Meanwhile, the government harassed the foundation with a series of fines for publishing âfalseâ information about the military, which the foundation has refused to pay. Finally, on the encyclopedia, state-aligned editors pushed the governmentâs view while vigilantes doxxed and threatened their opposition. Last year, the head of Wikimedia Russia was declared a âforeign agentâ and forced to resign from his job as a professor at Moscow State University.&lt;/p&gt;
    &lt;p&gt;In neighboring Belarus, editor Mark Bernstein was doxxed by a pro-Russian group in 2022, arrested, and sentenced to three years of home confinement. As many as five other editors have been detained by Belarusian authorities in recent months, according to media reports and editors.&lt;/p&gt;
    &lt;p&gt;As these battles continued, the Russian government supported the creation of a more compliant alternative, called Ruwiki, which launched early last year with the copying of 1.9 million articles from the originals, edited to reflect the government view. On Ruwiki, edits must comply with Russian laws and are subject to approval from outside experts. There, the map of Ukraine does not include Donetsk or Kherson, the war is a âspecial operationâ in response to NATO aggression, and accounts of torture in Bucha are fake news.&lt;/p&gt;
    &lt;p&gt;Wikipedia remains online in Russia, but with Ruwiki, the government may now feel emboldened to block it. In May, at a hearing on media safety for children, the head of the Russian Duma Committee on the Protection of the Family said that the encyclopediaâs âinterpretation of our historical events feels so hostile that we need to raise the issue of blocking this information resource,â and that the encyclopediaâs depiction of history is opposed to Russian âtraditional, spiritual values.â&lt;/p&gt;
    &lt;p&gt;The goal of these campaigns is what the Wikimedia Foundation calls âproject capture.â The term originates in an independent report the foundation commissioned in response to the takeover of the Croatian-language Wikipedia by a cabal of far-right editors.&lt;/p&gt;
    &lt;p&gt;In 2010, a group of editors won election to admin positions and began citing far-right alternative media to rewrite history. On Croatian Wikipedia, the Nazis invaded Poland to stop a genocide against the German people, Croatiaâs role in the Holocaust is foreign propaganda, and Ratko MladiÄ was a decorated military leader whose conviction by the UN for genocide (briefly noted quite far down) was the result of an international conspiracy. When other editors attempted to correct the articles, the admins banned them for violating rules against hate speech or harassment.&lt;/p&gt;
    &lt;p&gt;The encyclopedia became so warped that it began receiving press coverage. The Croatian Minister of Education warned students not to use it. In an interview with a Croatian paper, Wales confirmed the foundation was aware of the problem and looking into it. Yet the foundation has a policy of allowing Wikipedia projects to self-govern, and interfering with Croatian Wikipedia risked opening a door to the many governments and companies that want things on Wikipedia changed.&lt;/p&gt;
    &lt;p&gt;Editors mounted a resistance and attempted to vote the admins out, but the admins defeated the attempt using votes from what were later revealed to be dozens of fake accounts. But because the admins were the only ones with the technical ability to trace IP addresses, the opposition had no way to prove this. The cabal now controlled all the levers of power. By 2019, nearly all of the editors who opposed them had been banned or harassed off the project.&lt;/p&gt;
    &lt;p&gt;In 2020, one of the few remaining dissident editors compiled a comprehensive textual and statistical analysis of editing patterns of dozens of accounts and filed a request for an admin to run IP traces to see if they were sock puppets. The admin stalled, then attempted to fudge the traces, but did so in such a transparent way that it was clear the accounts were indeed fakes.&lt;/p&gt;
    &lt;p&gt;This was the evidence required to procedurally break the cabal. High-ranking admins called âstewardsâ from other-language Wikipedias administered a new vote on banning the Croatian admins. This time, the admins lost. Their ringleader, username Kubura, was banned from all Wikipedia projects forever, a punishment that had been leveled against less than a dozen others in Wikipedia history. A local daily covered the incident with the headline âKuburaâs Downfall: Banned Globally, His Followers Retreat, Leaderless.â&lt;/p&gt;
    &lt;p&gt;The foundationâs postmortem analysis compared the takeover to âstate capture, one of the most pressing issues of todayâs worldwide democratic backsliding.â The clique still cited the reliability of sources and invoked rules of debate, but it bent these processes to serve their nationalist purpose. As many governments have discovered, it is extremely difficult to insert propaganda into Wikipedia without running afoul of some rule or another. But what the Croatia capture showed is that Wikipediaâs processes are only effective if they are administered by people who believe in the spirit of the project. If they can be silenced or replaced, it becomes possible to steer the encyclopedia in a different direction.&lt;/p&gt;
    &lt;p&gt;One editor I spoke with, who asked to remain anonymous for reasons that will be obvious, had been editing Wikipedia for several years while living in a Middle Eastern country where much other media is tightly controlled. One day he received a call from a member of the intelligence service inviting him to lunch. He cried for hours â everyone knew what this meant.&lt;/p&gt;
    &lt;p&gt;The meeting was cordial but clear. They didnât want him to stop editing Wikipedia. They wanted his help. They knew the encyclopedia has rules and you canât just insert flagrant propaganda, but as a respected member of the community, maybe he could edit in ways that were a little friendlier to the government, maybe decide in its favor when certain topics came up for debate. In exchange, maybe the service could help him if he ever got in trouble with the police, for example, over his sexuality; he was gay in a country where that was illegal.&lt;/p&gt;
    &lt;p&gt;He fled the country weeks later. He now edits from abroad, but he knows of five to 10 others who have faced arrest or intimidation over their editing. They must do constant battle with editors he believes to be government agents who push the stateâs perspective, debating tirelessly for hours because it is literally their job.&lt;/p&gt;
    &lt;p&gt;Itâs a rare person who is able to uproot their life in the service of a volunteer side project. Understandably, many others faced with such threats become more cautious in their editing or stop altogether. Multiple editors based in India said that they now avoid editing topics related to their country. The ANI case had a chilling effect, as have recurring harassment campaigns. The far-right online publication OpIndia regularly accuses Wikipedia of âanti-Hindu and anti-India bias,â in ways that parallel attacks from the US right, down to citations of Manhattan Institute research and quotes from the disgruntled cofounder, Sanger. The organization has published the real names and employers of editors it accuses of being âleftistsâ or âIslamists,â leading at least one veteran editor to delete their account.&lt;/p&gt;
    &lt;p&gt;Even ancient history can be cause for reprisals. In February, after the release of a Bollywood action film about Chhatrapati Sambhaji Maharaj, a 17th-century king who fought the Mughals, accounts on X began whipping up outrage over several facts on Sambhajiâs Wikipedia page that they deemed to be anti-Hindu. When editors reversed attempts to delete the offending lines, another X user posted their usernames and called on government officials to investigate them. Days later, local press reported that the Maharashtra cyber police opened cases against at least four editors.&lt;/p&gt;
    &lt;p&gt;âVarious editors have left Wikipedia over this persecution, fearing their own safety,â said an Indian Wikipedia editor who asked to remain anonymous out of fear of retaliation. âI believe this is completely useful for the right wing, if you issue cases and file complaints against editors, they tend not to edit those pages anymore, fearing for their safety in real life.â&lt;/p&gt;
    &lt;p&gt;He still edits, but mostly sticks to the safer ground of the Roman Empire.&lt;/p&gt;
    &lt;p&gt;In April, the Trump administrationâs interim US attorney for DC, Edward Martin Jr., sent a letter to the Wikimedia Foundation accusing the organization of disseminating âpropagandaâ and intimating that it had violated its duties as a tax-exempt nonprofit.&lt;/p&gt;
    &lt;p&gt;From a legal perspective, it was an odd document. The tax status of nonprofits is not generally the jurisdiction of the US attorney for DC, and many of the supposed violations, like having foreign nationals on its board or permitting âthe rewriting of key, historical events and biographical information of current and previous American leaders,â are not against the law. Sanger is quoted, criticizing editor anonymity. In several cases, the rules Martin accuses Wikipedia of violating are Wikipediaâs own, like a commitment to neutrality. But the implied threat was clear.&lt;/p&gt;
    &lt;p&gt;âWeâve been anticipating something like this letter happening for some time,â a longtime editor, Lane Rasberry, said. It fits the pattern seen in India and elsewhere. He has been hearing more reports of threats against editors who work on pages related to trans issues and has been conducting security trainings to prevent their identities being revealed. Several US-based editors told me they now avoid politically contentious topics out of fear that they could be doxxed and face professional or legal retaliation. âThere are more Wikipedia editors getting threats, more people getting scared,â Rasberry said.&lt;/p&gt;
    &lt;p&gt;Talking to editors, I encountered a confounding spread of opinions about the seriousness of the threat to Wikipedia, often in the same conversation. The site has sloughed off more than two decades of attacks, and so far the latest round is no different. The Heritage Foundation plan to dox editors has yet to materialize. Muskâs calls for his followers to stop donating have resulted in surges in donations, according to publicly available data.&lt;/p&gt;
    &lt;p&gt;In India, the High Court struck down the order to take down the article about ANIâs defamation case, though the case itself is ongoing. Wikipediaâs critics on the right and in the Silicon Valley elite often propose generative AI as the solution to Wikipediaâs perceived biases, for each user a bespoke source of ideologically agreeable information. Yet all these projects remain wholly reliant on Wikipedia, and so far the most aggressive such initiative, Muskâs Grok, has spent much of its existence flailing between fact-checking Muskâs own conspiracy theories and proclaiming itself MechaHitler.&lt;/p&gt;
    &lt;p&gt;But new threats continue to appear. In August, the foundation lost its case arguing for an exemption from the UKâs Online Safety Act, which would force Wikipedia to verify the identities of its editors, though it is continuing to appeal. In Portugal the foundation received a court order arising from a defamation case brought by Portuguese American businessman Cesar DePaÃ§o, who objected to information on his page about past criminal allegations and links to the far-right Portuguese party Chega. Complying with the ruling, the foundation struck several facts from his biography and disclosed âa small amount of user dataâ about eight editors. The foundation is now bringing the case before the European Court of Human Rights. And in the US, there is the recent House Oversight letter.&lt;/p&gt;
    &lt;p&gt;No matter the outcome, these cases contribute to a general increase in pressure on the projectâs already strained editors. English Wikipedia has fewer than 40,000 active editors, defined as users who have made five or more edits in the last month. The number of active administrators, crucial to maintaining the site and enforcing policy, peaked in 2008 and now stands at around 450. AI threatens to squeeze the editor pipeline further. The more people who get information from AI summaries of Wikipedia rather than the site itself, the fewer people who will wander down a rabbit hole, encounter an error that needs correcting, and become editors themselves.&lt;/p&gt;
    &lt;p&gt;At the same time, people are using AI to add plausible-looking but false or biased information to the encyclopedia, increasing the workload for editors. Harassment, ideological editing campaigns, government investigations, targeted lawsuits â even if they lead nowhere, they will make the prospect of editing more daunting and increase the odds that current editors burn out. âWikipedia should not be taken for granted,â Rasberry said. âThis is an existential threat.â&lt;/p&gt;
    &lt;p&gt;The first reactions to the Martin letter on the Wikipedia editor forums were radical: the foundation should leave the US, maybe for France, or Iceland, or Germany. This would not be unprecedented, an editor pointed out. The EncyclopÃ©distes fled to Switzerland when the ancien rÃ©gime attempted to censor them. Maybe the site should go dark in protest.&lt;/p&gt;
    &lt;p&gt;But moderation soon prevailed. âThe community needs to chill on the blackout talk,â wrote an editor by the name of Tazerdadog. âWeâre not there yet.â Right now, the best response to these threats is to double down on Wikipediaâs policies, particularly the refusal to be censored and its dedication to neutral point of view, they wrote.&lt;/p&gt;
    &lt;p&gt;âI 100% agree with you, Tazerdadog,â replied âJimbo Wales.â âEmphasizing to the WMF that NPOV is non-negotiable is not really the issue.â In fact, Wales wrote, he is chairing a working group on strengthening the policy. The initiative was announced in March, framed as a response to the global rise in threats to sources of neutral information, and to a fragmentation of the publicâs understanding of the very concepts of neutrality and facts. Wikipediaâs response, it seemed, would be to neutral harder.&lt;/p&gt;
    &lt;p&gt;In May, I met Wales for coffee at a members club in Chelsea where he had been granted an honorary membership after giving a talk. (Wikipedia, as journalists have noted for years, did not make Wales a tech billionaire.) Extravagant bouquets of pastel flowers were arranged in an arch above the doorway and festooned the tables of the interior. Wales, dressed to meet his wife at the Chelsea Flower Show, matched the decor in a green linen suit and floral shirt. He does not, he said, normally dress like a leprechaun.&lt;/p&gt;
    &lt;p&gt;He was not particularly concerned about the attacks on Wikipedia, he said, though he warned that he is âpathologically optimistic.â Wikipedia has been attacked since it began. It fought Turkeyâs ban to the Constitutional Court and won. Even Russian Wikipedia has proven resilient. In the US, the government lacks much of the leverage it has deployed against other institutions. Wikipedia doesnât rely on government funding, and protections for online speech are strong. In the last fiscal year, the foundation took in $170 million in donations, with an average size of about $10.&lt;/p&gt;
    &lt;p&gt;As for the accusations of bias, why not investigate? Whether the attacks are in good faith or bad, it doesnât really matter, Wales said. The foundation had already decided that it was a good time, given the fragmented and polarizing world, to examine and bolster Wikipediaâs neutrality processes. Wales, leaning over the coffee table, seemed excited at the prospect.&lt;/p&gt;
    &lt;p&gt;âIf somebody turns up on a talk page and says, âHey, this article is a mess, itâs wrong. Itâs really biased,â the right answer is to not scream at them and run and hide. The right answer is go, âOh, tell me more. Letâs dig in. Where is it biased? How do we think about how do we fix that?ââ&lt;/p&gt;
    &lt;p&gt;Letâs figure out the best methodologies for studying neutrality, Wales said. Letâs look at how editors evaluate the reliability of sources. Maybe Wikipedia does use the label âfar-rightâ more than âfar-left,â Wales said, a criticism that has been leveled at the site. Is that because the media uses the term more, and does Wikipedia use the term more or less than the media does, and does the media use the term more because there are more far-right movements in the world today?&lt;/p&gt;
    &lt;p&gt;âYou have to chew on these things. Thereâs no simple answers.â&lt;/p&gt;
    &lt;p&gt;But there are answers. If the social platforms and language models that increasingly shape our understanding of the world are inscrutable black boxes, Wikipedia is the opposite, maybe the most legible, endlessly explainable information management system ever made. For any sentence, there is a source, and a reason that that source was used, and a reason for that reason.&lt;/p&gt;
    &lt;p&gt;âLetâs dig in,â Wales repeated. âLetâs assess the evidence. Letâs talk to a lot of different people. Letâs really try and understand.â Come, be part of the process. His working group is starting to discuss the best approach. The meetings, Wales acknowledged, have been very tedious so far.&lt;/p&gt;
    &lt;p&gt;As for the letter from the interim DC attorney, Trump withdrew Martinâs nomination in May, though he still has a position leading the Justice Departmentâs retribution-oriented âtask force on weaponization.â In any case, the Wikimedia Foundation responded promptly.&lt;/p&gt;
    &lt;p&gt;âThe foundation staff spent a lot of passion writing it,â Wales said of the reply. âThen they ran it by me for review, and I was ready to jump in, but I was like, actually, itâs perfect.â&lt;/p&gt;
    &lt;p&gt;âItâs very calm,â Wales said. âHere are the answers to your questions, here is what we do.â It explains how Wikipedia works.&lt;/p&gt;
    &lt;head rend="h4"&gt;Credits&lt;/head&gt;
    &lt;p&gt;Editor: Kevin NguyenCreative director: Kristen RadtkeArt director/designer: Cath ViginiaDeveloper: Graham MacAreeCopyeditor: Kallie PlaggeFactchecker: TiÃªn Nguyá» nEngagement editors: Esther Cohen &amp;amp; Tristan CooperManaging editor: Kara VerlaneyEditor-in-chief: Nilay PatelPublisher: Helen Havlak&lt;/p&gt;
    &lt;p&gt;Photos by A. Ghizzi Panizza, Anton Holoborodko, Arkady Zakharov, Barbara Niggl Radloff, Bengt Nyman, C-SPAN, Clister V. Pangantihon, Daniele Venturelli, David Gadd, Edward Kimmel, Eric Chan, Eric Gaba,German Federal Archive, Getty Images, Gnom, Hadi Mohammad, Hendrik Freitag, Iva NjunjiÄ, Jan Ainali, Knight Foundation, Larry Sanger, Marie-Lan Nguyen, Mario Tama, NASA, Nina Aldin Thune, Nostrix, PBS News Hour, Rodhullandemu, Sannse, SiGarb, Studio Incendo, Tyler Merbler, Zack McCune&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45128391</guid></item><item><title>Cache</title><link>https://developer.mozilla.org/en-US/docs/Web/API/Cache</link><description>&lt;doc fingerprint="8a4b63686fd813d1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cache&lt;/head&gt;
    &lt;head&gt; Baseline Widely available &lt;/head&gt;
    &lt;p&gt;This feature is well established and works across many devices and browser versions. Itâs been available across browsers since â¨April 2018â©.&lt;/p&gt;
    &lt;p&gt;Secure context: This feature is available only in secure contexts (HTTPS), in some or all supporting browsers.&lt;/p&gt;
    &lt;p&gt;Note: This feature is available in Web Workers.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;Cache&lt;/code&gt; interface provides a persistent storage mechanism for &lt;code&gt;Request&lt;/code&gt; / &lt;code&gt;Response&lt;/code&gt; object pairs that are cached in long lived memory. How long a &lt;code&gt;Cache&lt;/code&gt; object lives is browser dependent, but a single origin's scripts can typically rely on the presence of a previously populated &lt;code&gt;Cache&lt;/code&gt; object. Note that the &lt;code&gt;Cache&lt;/code&gt; interface is exposed to windowed scopes as well as workers. You don't have to use it in conjunction with service workers, even though it is defined in the service worker spec.&lt;/p&gt;
    &lt;p&gt;An origin can have multiple, named &lt;code&gt;Cache&lt;/code&gt; objects. You are responsible for implementing how your script (e.g., in a &lt;code&gt;ServiceWorker&lt;/code&gt;) handles &lt;code&gt;Cache&lt;/code&gt; updates. Items in a &lt;code&gt;Cache&lt;/code&gt; do not get updated unless explicitly requested; they don't expire unless deleted. Use &lt;code&gt;CacheStorage.open()&lt;/code&gt; to open a specific named &lt;code&gt;Cache&lt;/code&gt; object and then call any of the &lt;code&gt;Cache&lt;/code&gt; methods to maintain the &lt;code&gt;Cache&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;You are also responsible for periodically purging cache entries. Each browser has a hard limit on the amount of cache storage that a given origin can use. &lt;code&gt;Cache&lt;/code&gt; quota usage estimates are available via the &lt;code&gt;StorageManager.estimate()&lt;/code&gt; method. The browser does its best to manage disk space, but it may delete the &lt;code&gt;Cache&lt;/code&gt; storage for an origin. The browser will generally delete all of the data for an origin or none of the data for an origin. Make sure to version caches by name and use the caches only from the version of the script that they can safely operate on. See Deleting old caches for more information.&lt;/p&gt;
    &lt;p&gt;Note: The key matching algorithm depends on the VARY header in the value. So matching a new key requires looking at both key and value for entries in the &lt;code&gt;Cache&lt;/code&gt; object.&lt;/p&gt;
    &lt;p&gt;Note: The caching API doesn't honor HTTP caching headers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Instance methods&lt;/head&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;
        &lt;code&gt;Cache.match()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-1"&gt;&lt;p&gt;Returns a&lt;/p&gt;&lt;code&gt;Promise&lt;/code&gt;that resolves to the response associated with the first matching request in the&lt;code&gt;Cache&lt;/code&gt;object.&lt;/item&gt;
      &lt;item rend="dt-2"&gt;
        &lt;code&gt;Cache.matchAll()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-2"&gt;&lt;p&gt;Returns a&lt;/p&gt;&lt;code&gt;Promise&lt;/code&gt;that resolves to an array of all matching responses in the&lt;code&gt;Cache&lt;/code&gt;object.&lt;/item&gt;
      &lt;item rend="dt-3"&gt;
        &lt;code&gt;Cache.add()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-3"&gt;&lt;p&gt;Takes a URL, retrieves it and adds the resulting response object to the given cache. This is functionally equivalent to calling&lt;/p&gt;&lt;code&gt;fetch()&lt;/code&gt;, then using&lt;code&gt;put()&lt;/code&gt;to add the results to the cache.&lt;/item&gt;
      &lt;item rend="dt-4"&gt;
        &lt;code&gt;Cache.addAll()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;p&gt;Takes an array of URLs, retrieves them, and adds the resulting response objects to the given cache.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-5"&gt;
        &lt;code&gt;Cache.put()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-5"&gt;
        &lt;p&gt;Takes both a request and its response and adds it to the given cache.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-6"&gt;
        &lt;code&gt;Cache.delete()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-6"&gt;&lt;p&gt;Finds the&lt;/p&gt;&lt;code&gt;Cache&lt;/code&gt;entry whose key is the request, returning a&lt;code&gt;Promise&lt;/code&gt;that resolves to&lt;code&gt;true&lt;/code&gt;if a matching&lt;code&gt;Cache&lt;/code&gt;entry is found and deleted. If no&lt;code&gt;Cache&lt;/code&gt;entry is found, the promise resolves to&lt;code&gt;false&lt;/code&gt;.&lt;/item&gt;
      &lt;item rend="dt-7"&gt;
        &lt;code&gt;Cache.keys()&lt;/code&gt;
      &lt;/item&gt;
      &lt;item rend="dd-7"&gt;&lt;p&gt;Returns a&lt;/p&gt;&lt;code&gt;Promise&lt;/code&gt;that resolves to an array of&lt;code&gt;Cache&lt;/code&gt;keys.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Examples&lt;/head&gt;
    &lt;p&gt;This code snippet is from the service worker selective caching sample. (see selective caching live) The code uses &lt;code&gt;CacheStorage.open()&lt;/code&gt; to open any &lt;code&gt;Cache&lt;/code&gt; objects with a &lt;code&gt;Content-Type&lt;/code&gt; header that starts with &lt;code&gt;font/&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The code then uses &lt;code&gt;Cache.match()&lt;/code&gt; to see if there's already a matching font in the cache, and if so, returns it. If there isn't a matching font, the code fetches the font from the network and uses &lt;code&gt;Cache.put()&lt;/code&gt; to cache the fetched resource.&lt;/p&gt;
    &lt;p&gt;The code handles exceptions thrown from the &lt;code&gt;fetch()&lt;/code&gt; operation. Note that an HTTP error response (e.g., 404) will not trigger an exception. It will return a normal response object that has the appropriate error code.&lt;/p&gt;
    &lt;p&gt;The code snippet also shows a best practice for versioning caches used by the service worker. Though there's only one cache in this example, the same approach can be used for multiple caches. It maps a shorthand identifier for a cache to a specific, versioned cache name. The code also deletes all caches that aren't named in &lt;code&gt;CURRENT_CACHES&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In the code example, &lt;code&gt;caches&lt;/code&gt; is a property of the &lt;code&gt;ServiceWorkerGlobalScope&lt;/code&gt;. It holds the &lt;code&gt;CacheStorage&lt;/code&gt; object, by which it can access the &lt;code&gt;CacheStorage&lt;/code&gt; interface.&lt;/p&gt;
    &lt;p&gt;Note: In Chrome, visit &lt;code&gt;chrome://inspect/#service-workers&lt;/code&gt; and click on the "inspect" link below the registered service worker to view logging statements for the various actions the &lt;code&gt;service-worker.js&lt;/code&gt; script is performing.&lt;/p&gt;
    &lt;code&gt;const CACHE_VERSION = 1;
const CURRENT_CACHES = {
  font: `font-cache-v${CACHE_VERSION}`,
};

self.addEventListener("activate", (event) =&amp;gt; {
  // Delete all caches that aren't named in CURRENT_CACHES.
  // While there is only one cache in this example, the same logic
  // will handle the case where there are multiple versioned caches.
  const expectedCacheNamesSet = new Set(Object.values(CURRENT_CACHES));
  event.waitUntil(
    caches.keys().then((cacheNames) =&amp;gt;
      Promise.all(
        cacheNames.map((cacheName) =&amp;gt; {
          if (!expectedCacheNamesSet.has(cacheName)) {
            // If this cache name isn't present in the set of
            // "expected" cache names, then delete it.
            console.log("Deleting out of date cache:", cacheName);
            return caches.delete(cacheName);
          }
          return undefined;
        }),
      ),
    ),
  );
});

self.addEventListener("fetch", (event) =&amp;gt; {
  console.log("Handling fetch event for", event.request.url);

  event.respondWith(
    caches
      .open(CURRENT_CACHES.font)
      .then((cache) =&amp;gt; cache.match(event.request))
      .then((response) =&amp;gt; {
        if (response) {
          // If there is an entry in the cache for event.request,
          // then response will be defined and we can just return it.
          // Note that in this example, only font resources are cached.
          console.log(" Found response in cache:", response);

          return response;
        }

        // Otherwise, if there is no entry in the cache for event.request,
        // response will be undefined, and we need to fetch() the resource.
        console.log(
          " No response for %s found in cache. About to fetch " +
            "from networkâ¦",
          event.request.url,
        );

        // We call .clone() on the request since we might use it
        // in a call to cache.put() later on.
        // Both fetch() and cache.put() "consume" the request,
        // so we need to make a copy.
        // (see https://developer.mozilla.org/en-US/docs/Web/API/Request/clone)
        return fetch(event.request.clone()).then((response) =&amp;gt; {
          console.log(
            "  Response for %s from network is: %O",
            event.request.url,
            response,
          );

          if (
            response.status &amp;lt; 400 &amp;amp;&amp;amp;
            response.headers.has("content-type") &amp;amp;&amp;amp;
            response.headers.get("content-type").match(/^font\//i)
          ) {
            // This avoids caching responses that we know are errors
            // (i.e. HTTP status code of 4xx or 5xx).
            // We also only want to cache responses that correspond
            // to fonts, i.e. have a Content-Type response header that
            // starts with "font/".
            // Note that for opaque filtered responses
            // https://fetch.spec.whatwg.org/#concept-filtered-response-opaque
            // we can't access to the response headers, so this check will
            // always fail and the font won't be cached.
            // All of the Google Web Fonts are served from a domain that
            // supports CORS, so that isn't an issue here.
            // It is something to keep in mind if you're attempting
            // to cache other resources from a cross-origin
            // domain that doesn't support CORS, though!
            console.log("  Caching the response to", event.request.url);
            // We call .clone() on the response to save a copy of it
            // to the cache. By doing so, we get to keep the original
            // response object which we will return back to the controlled
            // page.
            // https://developer.mozilla.org/en-US/docs/Web/API/Request/clone
            cache.put(event.request, response.clone());
          } else {
            console.log("  Not caching the response to", event.request.url);
          }

          // Return the original response object, which will be used to
          // fulfill the resource request.
          return response;
        });
      })
      .catch((error) =&amp;gt; {
        // This catch() will handle exceptions that arise from the match()
        // or fetch() operations.
        // Note that a HTTP error response (e.g. 404) will NOT trigger
        // an exception.
        // It will return a normal response object that has the appropriate
        // error code set.
        console.error("  Error in fetch handler:", error);

        throw error;
      }),
  );
});
&lt;/code&gt;
    &lt;head rend="h3"&gt;Cookies and Cache objects&lt;/head&gt;
    &lt;p&gt;The Fetch API requires &lt;code&gt;Set-Cookie&lt;/code&gt; headers to be stripped before returning a &lt;code&gt;Response&lt;/code&gt; object from &lt;code&gt;fetch()&lt;/code&gt;. So a &lt;code&gt;Response&lt;/code&gt; stored in a &lt;code&gt;Cache&lt;/code&gt; won't contain &lt;code&gt;Set-Cookie&lt;/code&gt; headers, and therefore won't cause any cookies to be stored.&lt;/p&gt;
    &lt;head rend="h2"&gt;Specifications&lt;/head&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell role="head"&gt;Specification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Service Workers&amp;gt;&lt;p&gt;# cache-interface&amp;gt;&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Browser compatibility&lt;/head&gt;
    &lt;p&gt;Loadingâ¦&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45128578</guid></item><item><title>Show HN: I built FlipCards – a flashcard app with variations to improve learning</title><link>https://flipcardsapp.vercel.app/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45128621</guid></item><item><title>Google deletes net-zero pledge from sustainability website</title><link>https://www.nationalobserver.com/2025/09/04/investigations/google-net-zero-sustainability</link><description>&lt;doc fingerprint="ba2420514eb6b5f9"&gt;
  &lt;main&gt;
    &lt;p&gt;Google’s CEO Sundar Pichai stood smiling in a leafy-green California garden in September 2020 and declared that the IT behemoth was entering the “most ambitious decade yet” in its climate action.&lt;/p&gt;
    &lt;p&gt;“Today, I’m proud to announce that we intend to be the first major company to operate carbon free — 24 hours a day, seven days a week, 365 days a year,” he said, in a video announcement.&lt;/p&gt;
    &lt;p&gt;Pichai added that he knew the “road ahead would not be easy,” but Google “aimed to prove that a carbon-free future is both possible and achievable fast enough to prevent the most dangerous impacts of climate change.”&lt;/p&gt;
    &lt;p&gt;Five years on, just how hard Google’s “energy journey” would become is clear. In June, Google’s Sustainability website proudly boasted a headline pledge to achieve net-zero emissions by 2030. By July, that had all changed.&lt;/p&gt;
    &lt;p&gt;An investigation by Canada’s National Observer has found that Google’s net-zero pledge has quietly been scrubbed, demoted from having its own section on the site to an entry in the appendices of the company's sustainability report.&lt;/p&gt;
    &lt;p&gt;Genna Schnurbach, an external spokesperson for Google, referring to the report, told us: “As you can see from the document, Google is still committed to their ambition of net zero by 2030.”&lt;/p&gt;
    &lt;p&gt;By tracing back through the history of Google’s Sustainability website, we found that the company edited it in late June, removing almost all mention of its lauded net-zero goals. (A separate website referring to data centres specifically has maintained its existing language around net-zero commitments.)&lt;/p&gt;
    &lt;p&gt;The page on Operating Sustainably has been rebranded to Operations, and the section on net-zero carbon was deleted. In its place is a new priority area: Energy.&lt;/p&gt;
    &lt;p&gt;“Running the global infrastructure behind our products and services, including AI, takes considerable energy,” said Google in its Environment 2025 report, which explained that it will be almost impossible to meet its erstwhile net-zero ambitions, partly due to its expansion in AI.&lt;/p&gt;
    &lt;p&gt;These significant removals come as Big Tech is racing to build new, power-devouring, hyperscale data centres to capitalize on the global boom in artificial intelligence. They are also coming at a time when the Trump administration has targeted institutions that have environmental ambitions.&lt;/p&gt;
    &lt;p&gt;“While we remain committed to our climate moonshots, it’s become clear that achieving them is now more complex and challenging across every level — from local to global,” the Google report authors state. In the same report last year, Net Zero Carbon was a key priority.&lt;/p&gt;
    &lt;head rend="h2"&gt;First in Big Tech to make net-zero pledge&lt;/head&gt;
    &lt;p&gt;Google, which has data centres in Toronto, Ont., and Montreal, Que., was one of the first tech giants to set sweeping sustainability goals — and it appears to be one of the first to be considering sweeping them out of sight. The latest sustainability reports from Big Tech peers, Microsoft and Amazon, by contrast, still present net-zero emissions as a headline priority area.&lt;/p&gt;
    &lt;p&gt;Google’s yearly electricity consumption increased by 26 per cent in 2024 to 32.2 terrawatt-hours, which is almost as much as the consumption of Ireland. Last month, it released a technical report which revealed that a single chat message to its Gemini AI model consumes 0.24 watt-hours of energy, equivalent to 2.4 minutes of running a small LED bulb.&lt;/p&gt;
    &lt;p&gt;A recent report from McKinsey &amp;amp; Co calculates that by 2030, it will cost $6.7 trillion worldwide in new investment to keep pace with the exploding demand for computing power. Data centres equipped to handle AI processing loads will need $5.2 trillion in investment, the consultancy predicted.&lt;/p&gt;
    &lt;p&gt;The stratospheric rise in forecasted AI workloads could drive about 70 per cent of new electricity demand, McKinsey found, with the estimated global data centre capacity demand in its “mid-range” scenario expected to rise 3.5-fold to close to 250 gigawatts by 2030. In the US, this means the sector would account for almost 12 per cent of total national energy demand, up from just over five per cent today.&lt;/p&gt;
    &lt;p&gt;“The pressure to get any possible source of electricity is rather overwhelming right now, especially in the developed world, which hasn't touched its grid for 40 years,” Michael Barnard, a prominent clean energy technology analyst and self-styled climate futurist, told Canada’s National Observer.&lt;/p&gt;
    &lt;p&gt;This pressure was explored in a recent report from the New Climate Institute, a think-tank, which found that the tech sector is facing a "climate strategy crisis," where emissions targets have “lost their meaning amid soaring energy demand.”&lt;/p&gt;
    &lt;head rend="h2"&gt;AI power demand growth slows climate action&lt;/head&gt;
    &lt;p&gt;For some in the sector, the power demand growth linked to AI expansion has become incompatible with their existing environmental commitments. And this inconvenient truth has coincided with the reelection of Donald Trump, whose administration has signalled it will roll back climate policies, and whose allies have disparaged corporate sustainability efforts as part of a “woke agenda.”&lt;/p&gt;
    &lt;p&gt;Barnard noted “there's a lot of pandering to Trump going on," referring to Google chief investment officer Ruth Porat championing expansion of the use of “incredibly clean” coal plants and other fossil fuels for its future power data centres. Pichai himself attended Trump’s inauguration, to which Google donated $1 million.&lt;/p&gt;
    &lt;p&gt;But Barnard pointed to the fact that Google continues to sign corporate power purchase deals for various renewable energy sources, including hydropower, offshore wind and even advanced geothermal.&lt;/p&gt;
    &lt;p&gt;“Bit of a tight rope for Google,” he said, adding: “Watch what everyone actually does as opposed to what they say when dealing with Trump.”&lt;/p&gt;
    &lt;p&gt;John Lang, co-founder of the Net Zero Tracker, a data analysis firm that focuses on fact-checking net zero in every nation and the largest cities and companies around the globe, told Canada's National Observer that he believes the world is temporarily in what he dubs a "net-zero recession"– but it hasn’t caused companies to abandon climate action.&lt;/p&gt;
    &lt;p&gt;“What we're seeing is we are seeing some backtracking, but it's highly concentrated in two sectors: finance and fossil fuels,” Lang said.&lt;/p&gt;
    &lt;p&gt;In other sectors, Lang said corporations are now recalibrating their early sustainability goals to be more realistic and reduce reliance on carbon credits. This, he added, is “a really, really good thing.”&lt;/p&gt;
    &lt;p&gt;Google, whose parent company Alphabet has a market cap of US$2.79 trillion, has taken a more ambiguous approach. Despite removing its net-zero headline from its Sustainability website, the company insists that it remains committed to its 2030 goal — which relies heavily on carbon offsetting. An external PR representative for Google declined to reply to Canada’s National Observer’s challenge of her claim that it was “still committed” to net zero by 2030, despite the pledge being demoted to an appendix.&lt;/p&gt;
    &lt;p&gt;The UN’s High Level Expert Group on the Net Zero Emissions Commitments of Non-State Entities released a report in 2022, which found that unrealistic sustainability pledges “erode confidence in net zero pledges overall” and “undermine sovereign state commitments.”&lt;/p&gt;
    &lt;p&gt;Lang is more sympathetic to “stretch goals,” like Google’s climate moonshots, as long as the deadlines are set close in the future, as they can motivate urgency.&lt;/p&gt;
    &lt;p&gt;“It still needs to be realistic. You still need to be able to deliver it,” he added. He praised Google’s decision to invest $200 million in durable carbon removals as setting a positive precedent for other companies.&lt;/p&gt;
    &lt;p&gt;It is unclear whether Google’s decision to delete its net-zero pledges from its Sustainability website sets a more worrying precedent.&lt;/p&gt;
    &lt;p&gt;Lang is convinced the setbacks are only temporary and rapid emissions reductions will soon be reprioritized. “It's our one and only solution to climate change. It's as simple as that. There's no other option.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Keep reading&lt;/head&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;I suspect part of the disappearance of Google's plan has to do with the wrath of Trump and his anti-climate change policies. Trump has demanded not only government, but corporations remove climate change information and DEI references. The second part, is the rapid expansion of AI data centres which demand a lot of energy to run them.&lt;/p&gt;
    &lt;p&gt;I wonder as a result of the AI data centre demand for energy, the Trumpian demands are a convenient way to pull back on their net-zero pledge. As energy demands increase rapidly with AI data centres, I suspect meeting that pledge has become far more difficult to achieve when energy demands where more stable and predicable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45128640</guid></item><item><title>Show HN: A Map of All YC Companies (5,300 Startups by Batch and Location)</title><link>https://yc.foundersaround.com/</link><description>&lt;doc fingerprint="f5acabac5dd24e25"&gt;
  &lt;main&gt;
    &lt;p&gt;Discover YC companies worldwide&lt;/p&gt;
    &lt;p&gt;Powered by FoundersAround&lt;/p&gt;
    &lt;p&gt;Get a white-label version for your accelerator, university, or VC firm.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45128652</guid></item><item><title>Launch HN: Slashy (YC S25) – AI that connects to apps and does tasks</title><link>https://news.ycombinator.com/item?id=45129031</link><description>&lt;doc fingerprint="37d0c8b3cf91aeb9"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hi HN! – We’re Pranjali, Dhruv and Harsha, building Slashy (&lt;/p&gt;https://www.slashy.ai&lt;p&gt;). We’re building a general agent that connects to apps and can read data across them and perform actions via custom tools, semantic search, and personalized memory. Here’s a demo: &lt;/p&gt;https://www.youtube.com/watch?v=OeApHMHhccA&lt;p&gt;.&lt;/p&gt;&lt;p&gt;While working on a previous startup, we realized we were spending more time doing busywork in apps than actually building product. We lost hundreds of hours scraping LinkedIn profiles, updating spreadsheets, updating investor reports, and communicating across multiple Slack channels. Our breaking point happened after I checked my screen time and realized I spent 4 hours a day in Gmail. We decided that we could create more value solving this than by working on the original startup (a code generation agent similar to Lovable).&lt;/p&gt;&lt;p&gt;Slashy is an AI agent that uses direct tool calls to services such as Gmail, Calendar, Notion, Sheets and more. We built all of our tools in-house since we found that most MCPs are low quality and add an unnecessary layer of abstraction. Through these tools, the agent is able to semantically search across your apps, get relevant information, and perform actions (e.g. send emails, create calendar events, etc). This solves the problem of context-switching and copy-pasting information from an app back and forth into ChatGPT.&lt;/p&gt;&lt;p&gt;Slashy integrates to 15 different services so far (G-Suite, Slack, Notion, Dropbox, Airtable, Outlook, Phone, Linear, Hubspot, and more). We use a single agent architecture (as we found this reduces hallucinations), and use our own custom tools—doing so allows the model to have higher quality as we can design them to work in a general agent structure, for example we use markdown for Slack/Notion instead of their native text structure.&lt;/p&gt;&lt;p&gt;So what makes Slashy different from the 100 other general agents?&lt;/p&gt;&lt;p&gt;- It Actually Takes Action: Unlike ChatGPT or Claude that just give you information, Slashy researches companies, creates Google Docs with findings, adds contacts to your CRM, schedules follow-ups, and sends personalized emails – all in one workflow.&lt;/p&gt;&lt;p&gt;- Cross-Tool Context: Most automation tools work in silos (one of the biggest problems with MCP). Slashy understands your data across platforms. It can read your previous Slack conversations about a prospect, check your calendar for availability, research their company online, and draft a personalized email. What powers this is our own semantic search functionality.&lt;/p&gt;&lt;p&gt;- User Action Graphs: Our agent over time has memory not just of past conversations, but also forms user actions graphs to know what actions are expected based on previous user conversations.&lt;/p&gt;&lt;p&gt;- No Technical Setup Required: While Zapier requires building complex flows and fails silently, Slashy works through natural language. Just describe what you want automated.&lt;/p&gt;&lt;p&gt;- Custom UI: For our tool calls we design custom UI for each of them to make the UX more natural.&lt;/p&gt;&lt;p&gt;Here are some examples of workflows people use us for:&lt;/p&gt;&lt;p&gt;▪ "Every day look at my calendar and send me a notion doc with in-depth backgrounds on everyone I’m meeting"&lt;/p&gt;&lt;p&gt;▪ "Find the emails of everyone who reacted to my latest LinkedIn post and send personalized outreach"&lt;/p&gt;&lt;p&gt;▪ "Can you make me an investor pitch deck with market research, competitive analysis, and financial projections"&lt;/p&gt;&lt;p&gt;▪ "Doing a full Nvidia Discounted Cash Flow (DCF) analysis"&lt;/p&gt;&lt;p&gt;Slashy.ai is live with a free tier (100 daily credits) along with 500 credits for any new account. You can immediately try out workflows like the ones above and we have a special code for HN (HACKERNEWS at checkout).&lt;/p&gt;&lt;p&gt;Hope you all enjoy Slashy as much as we do :)&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45129031</guid></item><item><title>Stripe Launches L1 Blockchain: Tempo</title><link>https://tempo.xyz</link><description>&lt;doc fingerprint="b7db92cc193df788"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why create a&lt;lb/&gt;new blockchain?&lt;/head&gt;
    &lt;p&gt;Stablecoins enable instant, borderless, programmable transactions, but current blockchain infrastructure isn’t designed for them: existing systems are either fully general or trading-focused. Tempo is a blockchain designed and built for real-world payments.&lt;/p&gt;
    &lt;head rend="h1"&gt;Optimized for&lt;lb/&gt;real-world flows&lt;/head&gt;
    &lt;p&gt;Tempo was started by Stripe and Paradigm, with design input from Anthropic, Coupang, Deutsche Bank, DoorDash, Lead Bank, Mercury, Nubank, OpenAI, Revolut, Shopify, Standard Chartered, Visa, and more.&lt;/p&gt;
    &lt;p&gt;If you’re a company with large, real-world economic flows and would like to help shape the future of Tempo, get in touch.&lt;/p&gt;
    &lt;head rend="h1"&gt;Transform how &lt;lb/&gt;your business &lt;lb/&gt; moves money&lt;/head&gt;
    &lt;head rend="h2"&gt;01 :: Purpose-built payments capabilities&lt;/head&gt;
    &lt;p&gt;Optimize your financial flows with embedded payment features, including memo fields and batch transfers.&lt;/p&gt;
    &lt;head rend="h2"&gt;02 :: Speed and reliability&lt;/head&gt;
    &lt;p&gt;Process over 100,000 transactions per second (TPS) with sub-second finality, enabling real-time payments at a global scale.&lt;/p&gt;
    &lt;head rend="h2"&gt;03 :: Predictable low fees&lt;/head&gt;
    &lt;p&gt;Transform your cost structure with near-zero transaction fees that are highly predictable and can be paid in any stablecoin.&lt;/p&gt;
    &lt;head rend="h2"&gt;04 :: Built-in privacy measures&lt;/head&gt;
    &lt;p&gt;Protect your users by keeping important transaction details private while maintaining compliance standards.&lt;/p&gt;
    &lt;head rend="h1"&gt;Performant and &lt;lb/&gt;scalable for any &lt;lb/&gt;payments &lt;lb/&gt;use case&lt;/head&gt;
    &lt;head rend="h2"&gt;01 :: Remittances&lt;/head&gt;
    &lt;p&gt;Send money across borders instantly, securely, and at a fraction of traditional costs.&lt;/p&gt;
    &lt;head rend="h2"&gt;02 :: Global payouts&lt;/head&gt;
    &lt;p&gt;Pay anyone, anywhere, in any currency—without banking delays or fees.&lt;/p&gt;
    &lt;head rend="h2"&gt;03 :: Embedded finance&lt;/head&gt;
    &lt;p&gt;Build compliant, programmable payments—in any stablecoin—directly into your products.&lt;/p&gt;
    &lt;head rend="h2"&gt;04 :: Microtransactions&lt;/head&gt;
    &lt;p&gt;Enable sub-cent payments for digital goods and on-demand services.&lt;/p&gt;
    &lt;head rend="h2"&gt;05 :: Agentic commerce&lt;/head&gt;
    &lt;p&gt;Facilitate low-cost, instant payments for agents to autonomously execute transactions.&lt;/p&gt;
    &lt;head rend="h2"&gt;06 :: Tokenized deposits&lt;/head&gt;
    &lt;p&gt;Move customer funds onchain for instant settlement and efficient interbank transfers.&lt;/p&gt;
    &lt;head rend="h1"&gt;Technical&lt;lb/&gt;features&lt;/head&gt;
    &lt;head rend="h2"&gt;01 :: Fee flexibility&lt;/head&gt;
    &lt;p&gt;Pay transaction fees in any stablecoin.&lt;/p&gt;
    &lt;head rend="h2"&gt;02 :: Dedicated payments lane&lt;/head&gt;
    &lt;p&gt;Transfer funds cheaply and reliably in blockspace that’s isolated from other activity.&lt;/p&gt;
    &lt;head rend="h2"&gt;03 :: Stablecoin interoperability&lt;/head&gt;
    &lt;p&gt;Swap stablecoins, including custom-issued ones, natively with low fees.&lt;/p&gt;
    &lt;head rend="h2"&gt;04 :: Batch transfers&lt;/head&gt;
    &lt;p&gt;Send multiple transactions onchain at once with native account abstraction.&lt;/p&gt;
    &lt;head rend="h2"&gt;05 :: Blocklists / allowlists&lt;/head&gt;
    &lt;p&gt;Meet compliance standards by setting user-level permissions for transactions.&lt;/p&gt;
    &lt;head rend="h2"&gt;06 :: Memo fields&lt;/head&gt;
    &lt;p&gt;Speed up reconciliation with offchain transactions by adding context that’s compatible with ISO 20022 standards.&lt;/p&gt;
    &lt;head rend="h1"&gt;Frequently&lt;lb/&gt;asked questions&lt;/head&gt;
    &lt;head rend="h2"&gt;01 :: How is Tempo different from other blockchains?&lt;/head&gt;
    &lt;p&gt;Tempo is an EVM-compatible L1 blockchain, purpose-built for payments. It doesn’t displace other general-purpose blockchains; rather, it incorporates design choices that meet the needs of high-volume payment use cases. These include predictable low fees in a dedicated payments lane, stablecoin neutrality, a built-in stablecoin exchange, high throughput, low latency, private transactions, payment memos compatible with standards like ISO 20022, compliance hooks, and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;02 :: Who can build on Tempo?&lt;/head&gt;
    &lt;p&gt;Tempo is a neutral, permissionless blockchain open for anyone to build on. We’re currently collaborating with global partners to test various use cases, including cross-border payouts, B2B payments, remittances, and ecommerce. Interested in working with Tempo? Request access to our private testnet here.&lt;/p&gt;
    &lt;head rend="h2"&gt;03 :: When will Tempo launch?&lt;/head&gt;
    &lt;p&gt;We’re providing select partners with priority access to our testnet now. Contact us here if you’re interested.&lt;/p&gt;
    &lt;head rend="h2"&gt;04 :: Who will run validator nodes?&lt;/head&gt;
    &lt;p&gt;A diverse group of independent entities, including some of Tempo’s design partners, will run validator nodes initially before we transition to a permissionless model.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45129085</guid></item><item><title>We built an interpreter for Swift (a compiled language)</title><link>https://www.bitrig.app/blog/swift-interpreter</link><description>&lt;doc fingerprint="7388290fc1ffe5f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How we built an interpreter for Swift (a compiled language)&lt;/head&gt;
    &lt;p&gt;Bitrig dynamically generates and runs Swift apps on your phone. Normally this would require compiling and signing with Xcode, and you can’t do that on an iPhone.&lt;/p&gt;
    &lt;p&gt;To make it possible to instantly run your app, we built a Swift interpreter. But it’s an unusual interpreter, since it interprets from Swift… to Swift. One of the top questions we’ve gotten is how it’s implemented, so we wanted to share how it works. To make this more accessible and interesting, we simplified some of the more esoteric details. But we hope you’ll come away with a high-level picture of how the interpreter works.&lt;/p&gt;
    &lt;p&gt;The Swift project helpfully provides a way to reuse all of the parsing logic from the compiler: SwiftSyntax. This made our job a lot easier. We can easily take some Swift code and get a parsed tree out of it, which we can use to evaluate and call into to get dynamic runtime values. Let’s dig deeper.&lt;/p&gt;
    &lt;p&gt;We can start with generating the simplest kind of runtime values. For any literals (strings, floating point numbers, integers, and booleans), we can create corresponding Swift instances (&lt;code&gt;String&lt;/code&gt;, &lt;code&gt;Double&lt;/code&gt;, &lt;code&gt;Int&lt;/code&gt;, &lt;code&gt;Bool&lt;/code&gt;) to represent them. Since we’re not compiling this, we don’t know ahead of time what all the types will be, so we need to type erase all instances. Let’s make an enum to represent these runtime interpreter values (since we'll have multiple kinds soon):&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;enum InterpreterValue { case nativeValue(Any) }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Next, we'll expand our interpreter runtime values to be able to represent developer-defined types, too. Let’s say we have a struct with two fields: a string and an integer. We’ll store it as a type that has a dictionary mapping from the property name to the runtime value. When an initializer gets called, we simply need to map the arguments to the property names and populate the dictionary.&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;enum InterpreterValue { case nativeValue(Any) case customInstance(CustomInstance) } struct CustomInstance { var type: InterpreterType var values: [String: InterpreterValue] }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;But, what happens when we want to call an API that comes from a framework, like SwiftUI? For example, let’s say we have a call to &lt;code&gt;Text("Hello World")&lt;/code&gt;. We don’t want to rewrite all of the APIs, the whole benefit of making a native app is being able to call into those implementations! Well, those APIs are available for us to call into since the interpreter is also written in Swift (naturally!). We just need to change from a dynamic call to a compiled one. We can do that by pre-compiling a call to the &lt;code&gt;Text&lt;/code&gt; initializer that can take dynamic arguments. Something like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;func evaluateTextInitializer(arguments: [Argument]) -&amp;gt; Text { Text(arguments.first?.value.stringValue ?? "") }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;But of course, we need more than just the &lt;code&gt;Text&lt;/code&gt; initializer, so we'll generalize this to any initializer we might be called with:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;func evaluateInitializer(type: String, arguments: [Argument]) -&amp;gt; Any? { if type == "Text" { return Text(arguments.first?.value.stringValue ?? "") } else if type == "Image" { return Image(arguments.first?.value.stringValue ?? "") ... }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;We can follow this same pattern for all other API types: function calls, properties, subscripts, etc. The difficult part is that there are a lot of APIs. It’s not practical to hand-write code to call into all of them, but fortunately there is a structured list of all of them: the .swiftinterface file for each framework. We can parse those files to get a list of all of the APIs we need and then generate the necessary code to call into them.&lt;/p&gt;
    &lt;p&gt;One interesting thing about taking this approach to its most extreme is that even very basic operations that you might expect any interpreter to implement, like basic numeric operations, can still call into their framework implementations. So this kind of interpreter doesn’t know how to calculate or evaluate anything itself, and is really more of a glorified foreign function interface, but from dynamic Swift to compiled Swift.&lt;/p&gt;
    &lt;p&gt;One last important challenge is how to make custom types conform to framework protocols. For example, how do we make a custom SwiftUI &lt;code&gt;View&lt;/code&gt;? Well, at runtime, we need a concrete type that conforms to the desired protocol. To do this, we can make stub types that conform to the protocol, but instead of having any logic of their own, simply call out to the interpreter to implement any requirements. Let’s look at &lt;code&gt;Shape&lt;/code&gt;, a simple example:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;struct ShapeStub: Shape { var interpreter: Interpreter var instance: CustomInstance var layoutDirectionBehavior: LayoutDirectionBehavior { instance.instanceMemberProperty("layoutDirectionBehavior", interpreter: interpreter).layoutDirectionBehaviorValue } func path(in rect: CGRect) -&amp;gt; Path { let arguments = [Argument(label: "in", value: rect)] return instance.instanceFunction("path", arguments: arguments, interpreter: interpreter).pathValue } }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;Coming back to &lt;code&gt;View&lt;/code&gt;, this works the same way, with a little extra complexity because of the associated type that we have to type erase:&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;struct ViewStub: View { var interpreter: Interpreter var instance: CustomInstance var body: AnyView { instance.instanceMemberProperty("body", interpreter: interpreter).viewValue .map { AnyView($0) } } }&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;And now we can make views that have dynamic implementations!&lt;/p&gt;
    &lt;p&gt;That’s a broad survey of how the interpreter is implemented. If you want to try it out in practice, download Bitrig!&lt;/p&gt;
    &lt;p&gt;If there’s more you want to know about the interpreter, or Bitrig as a whole, let us know!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45129160</guid></item><item><title>Age Simulation Suit</title><link>https://www.age-simulation-suit.com/</link><description>&lt;doc fingerprint="fdab49420ed58155"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;GERonTologic simulator GERT&lt;/head&gt;
    &lt;p&gt;The age simulation suit GERT offers the opportunity to experience the impairments of older persons even for younger people. &lt;lb/&gt; The age-related impairments are:&lt;/p&gt;
    &lt;p&gt;■ opacity of the eye lens&lt;/p&gt;
    &lt;p&gt;■ narrowing of the visual field&lt;/p&gt;
    &lt;p&gt;■ high-frequency hearing loss&lt;/p&gt;
    &lt;p&gt;■ head mobility restrictions&lt;/p&gt;
    &lt;p&gt;■ joint stiffness&lt;/p&gt;
    &lt;p&gt;■ loss of strength&lt;/p&gt;
    &lt;p&gt;■ reduced grip ability&lt;/p&gt;
    &lt;p&gt;■ reduced coordination skills&lt;/p&gt;
    &lt;head rend="h4"&gt;GERT for only 1390,‑ / £ 1250,-&lt;/head&gt;
    &lt;p&gt;complete as pictured, plus shipping and VAT if applicable&lt;lb/&gt; New: now with 2 pairs of glasses instead of the model shown&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Due to the significant increase in the time and effort required to process orders, in particular as a result of incomplete or incorrect information provided with orders, and the fact that we increasingly have to send reminders for invoices for smaller amounts, we can only accept orders with a value of at least 300 euros or pounds.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Customer reviews:&lt;/p&gt;
    &lt;p&gt;The quality is great and it works how it is supposed to. Im happy with my purchase.&lt;/p&gt;
    &lt;p&gt;Great way to teach about elderly behavior. Ive been using this suit for a while now and its very durable and easy to use. Thanks!!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45129190</guid></item><item><title>A PM's Guide to AI Agent Architecture</title><link>https://www.productcurious.com/p/a-pms-guide-to-ai-agent-architecture</link><description>&lt;doc fingerprint="2cd6bc914903882d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A PM's Guide to AI Agent Architecture: Why Capability Doesn't Equal Adoption&lt;/head&gt;
    &lt;head rend="h3"&gt;A complete guide to agent architecture, orchestration patterns, trust strategies, and adoption plans for PMs building AI agents.&lt;/head&gt;
    &lt;p&gt;Last week, I was talking to a PM who'd in the recent months shipped their AI agent. The metrics looked great: 89% accuracy, sub-second respond times, positive user feedback in surveys. But users were abandoning the agent after their first real problem, like a user with both a billing dispute and a locked account.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;"Our agent could handle routine requests perfectly, but when faced with complex issues, users would try once, get frustrated, and immediately ask for a human."&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This pattern is observed across every product team that focuses on making their agents "smarter" when the real challenge is making architectural decisions that shape how users experience and begin to trust the agent.&lt;/p&gt;
    &lt;p&gt;In this post, I'm going to walk you through the different layers of AI agent architecture. How your product decisions determine whether users trust your agent or abandon it. By the end of this, you'll understand why some agents feel "magical" while others feel "frustrating" and more importantly, how PMs should architect for the magical experience.&lt;/p&gt;
    &lt;p&gt;We'll use a concrete customer support agent example throughout, so you can see exactly how each architectural choice plays out in practice. We’ll also see why the counterintuitive approach to trust (hint: it's not about being right more often) actually works better for user adoption.&lt;/p&gt;
    &lt;head rend="h2"&gt;Let's say you're building a customer support agent&lt;/head&gt;
    &lt;p&gt;You're the PM building an agent that helps users with account issues - password resets, billing questions, plan changes. Seems straightforward, right?&lt;/p&gt;
    &lt;p&gt;But when a user says "I can't access my account and my subscription seems wrong" what should happen?&lt;/p&gt;
    &lt;p&gt;Scenario A: Your agent immediately starts checking systems. It looks up the account, identifies that the password was reset yesterday but the email never arrived, discovers a billing issue that downgraded the plan, explains exactly what happened, and offers to fix both issues with one click.&lt;/p&gt;
    &lt;p&gt;Scenario B: Your agent asks clarifying questions. "When did you last successfully log in? What error message do you see? Can you tell me more about the subscription issue?" After gathering info, it says "Let me escalate you to a human who can check your account and billing."&lt;/p&gt;
    &lt;p&gt;Same user request. Same underlying systems. Completely different products.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Four Layers Where Your Product Decisions Live&lt;/head&gt;
    &lt;p&gt;Think of agent architecture like a stack where each layer represents a product decision you have to make.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 1: Context &amp;amp; Memory (What does your agent remember?)&lt;/head&gt;
    &lt;p&gt;The Decision: How much should your agent remember, and for how long?&lt;/p&gt;
    &lt;p&gt;This isn't just technical storage - it's about creating the illusion of understanding. Your agent's memory determines whether it feels like talking to a robot or a knowledgeable colleague.&lt;/p&gt;
    &lt;p&gt;For our support agent: Do you store just the current conversation, or the customer's entire support history? Their product usage patterns? Previous complaints?&lt;/p&gt;
    &lt;p&gt;Types of memory to consider:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Session memory: Current conversation ("You mentioned billing issues earlier...")&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Customer memory: Past interactions across sessions ("Last month you had a similar issue with...")&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Behavioral memory: Usage patterns ("I notice you typically use our mobile app...")&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Contextual memory: Current account state, active subscriptions, recent activity&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The more your agent remembers, the more it can anticipate needs rather than just react to questions. Each layer of memory makes responses more intelligent but increases complexity and cost.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 2: Data &amp;amp; Integration (How deep do you go?)&lt;/head&gt;
    &lt;p&gt;The Decision: Which systems should your agent connect to, and what level of access should it have?&lt;/p&gt;
    &lt;p&gt;The deeper your agent connects to user workflows and existing systems, the harder it becomes for users to switch. This layer determines whether you're a tool or a platform.&lt;/p&gt;
    &lt;p&gt;For our support agent: Should it integrate with just your Stripe’s billing system, or also your Salesforce CRM, ZenDesk ticketing system , user database, and audit logs? Each integration makes the agent more useful but also creates more potential failure points - think API rate limits, authentication challenges, and system downtime.&lt;/p&gt;
    &lt;p&gt;Here's what's interesting - Most of us get stuck trying to integrate with everything at once. But the most successful agents started with just 2-3 key integrations and added more based on what users actually asked for.&lt;/p&gt;
    &lt;head rend="h3"&gt;Layer 3: Skills &amp;amp; Capabilities (What makes you different?)&lt;/head&gt;
    &lt;p&gt;The Decision: Which specific capabilities should your agent have, and how deep should they go?&lt;/p&gt;
    &lt;p&gt;Your skills layer is where you win or lose against competitors. It's not about having the most features - it's about having the right capabilities that create user dependency.&lt;/p&gt;
    &lt;p&gt;For our support agent: Should it only read account information, or should it also modify billing, reset passwords, and change plan settings? Each additional skill increases user value but also increases complexity and risk.&lt;/p&gt;
    &lt;p&gt;Implementation note: Tools like MCP (Model Context Protocol) are making it much easier to build and share skills across different agents, rather than rebuilding capabilities from scratch.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Layer 4: Evaluation &amp;amp; Trust (How do users know what to expect?)&lt;/head&gt;
    &lt;p&gt;The Decision: How do you measure success and communicate agent limitations to users?&lt;/p&gt;
    &lt;p&gt;This layer determines whether users develop confidence in your agent or abandon it after the first mistake. It's not just about being accurate - it's about being trustworthy.&lt;/p&gt;
    &lt;p&gt;For our support agent: Do you show confidence scores ("I'm 85% confident this will fix your issue")? Do you explain your reasoning ("I checked three systems and found...")? Do you always confirm before taking actions ("Should I reset your password now?")? Each choice affects how users perceive reliability.&lt;/p&gt;
    &lt;p&gt;Trust strategies to consider:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Confidence indicators: "I'm confident about your account status, but let me double-check the billing details"&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reasoning transparency: "I found two failed login attempts and an expired payment method"&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Graceful boundaries: "This looks like a complex billing issue - let me connect you with our billing specialist who has access to more tools"&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Confirmation patterns: When to ask permission vs. when to act and explain&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;The counterintuitive insight: users trust agents more when they admit uncertainty than when they confidently make mistakes.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;So how do you actually architect an agent?&lt;/head&gt;
    &lt;p&gt;Okay, so you understand the layers. Now comes the practical question that every PM asks: "How do I actually implement this? How does the agent talk to the skills? How do skills access data? How does evaluation happen while users are waiting?"&lt;/p&gt;
    &lt;p&gt;Your orchestration choice determines everything about your development experience, your debugging process, and your ability to iterate quickly.&lt;/p&gt;
    &lt;p&gt;Lets walk through the main approaches, and I'll be honest about when each one works and when it becomes a nightmare.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Single-Agent Architecture (Start Here)&lt;/head&gt;
    &lt;p&gt;Everything happens in one agent's context.&lt;/p&gt;
    &lt;p&gt;For our support agent: When the user says "I can't access my account," one agent handles it all - checking account status, identifying billing issues, explaining what happened, offering solutions.&lt;/p&gt;
    &lt;p&gt;Why this works: Simple to build, easy to debug, predictable costs. You know exactly what your agent can and can't do.&lt;/p&gt;
    &lt;p&gt;Why it doesn't: Can get expensive with complex requests since you're loading full context every time. Hard to optimize specific parts.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Most teams start here, and honestly, many never need to move beyond it. If you're debating between this and something more complex, start here.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;2. Skill-Based Architecture (When You Need Efficiency)&lt;/head&gt;
    &lt;p&gt;You have a router that figures out what the user needs, then hands off to specialized skills.&lt;/p&gt;
    &lt;p&gt;For our support agent: Router realizes this is an account access issue and routes to the `LoginSkill`. If the LoginSkill discovers it's actually a billing problem, it hands off to `BillingSkill`.&lt;/p&gt;
    &lt;p&gt;Real example flow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;User: "I can't log in"&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Router → LoginSkill&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LoginSkill checks: Account exists ✓, Password correct ✗, Billing status... wait, subscription expired&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LoginSkill → BillingSkill: "Handle expired subscription for user123"&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;BillingSkill handles renewal process&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why this works: More efficient - you can use cheaper models for simple skills, expensive models for complex reasoning. Each skill can be optimized independently.&lt;/p&gt;
    &lt;p&gt;Why it doesn't: Coordination between skills gets tricky fast. Who decides when to hand off? How do skills share context?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Here's where MCP really helps - it standardizes how skills expose their capabilities, so your router knows what each skill can do without manually maintaining that mapping.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;3. Workflow-Based Architecture (Enterprise Favorite)&lt;/head&gt;
    &lt;p&gt;You predefine step-by-step processes for common scenarios. Think LangGraph, CrewAI, AutoGen, N8N, etc.&lt;/p&gt;
    &lt;p&gt;For our support agent: "Account access problem" triggers a workflow:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Check account status&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If locked, check failed login attempts&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If too many failures, check billing status&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If billing issue, route to payment recovery&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If not billing, route to password reset&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Why this works: Everything is predictable and auditable. Perfect for compliance-heavy industries. Easy to optimize each step.&lt;/p&gt;
    &lt;p&gt;Why it doesn't: When users have weird edge cases that don't fit your predefined workflows, you're stuck. Feels rigid to users.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. Collaborative Architecture (The Future?)&lt;/head&gt;
    &lt;p&gt;Multiple specialized agents work together using A2A (agent-to-agent) protocols.&lt;/p&gt;
    &lt;p&gt;The vision: Your agent discovers that another company's agent can help with issues, automatically establishes a secure connection, and collaborates to solve the customer's problem. Think a booking.com agent interacting with an American Airlines agent!&lt;/p&gt;
    &lt;p&gt;For our support agent: `AuthenticationAgent` handles login issues, `BillingAgent` handles payment problems, `CommunicationAgent` manages user interaction. They coordinate through standardized protocols to solve complex problems.&lt;/p&gt;
    &lt;p&gt;Reality check: This sounds amazing but introduces complexity around security, billing, trust, and reliability that most companies aren't ready for. We're still figuring out the standards.&lt;/p&gt;
    &lt;p&gt;This can produce amazing results for sophisticated scenarios, but debugging multi-agent conversations is genuinely hard. When something goes wrong, figuring out which agent made the mistake and why is like detective work.&lt;/p&gt;
    &lt;p&gt;Here's the thing: start simple. Single-agent architecture handles way more use cases than you think. Add complexity only when you hit real limitations, not imaginary ones.&lt;/p&gt;
    &lt;p&gt;But here's what's interesting - even with the perfect architecture, your agent can still fail if users don't trust it. That brings us to the most counterintuitive lesson about building agents.&lt;/p&gt;
    &lt;head rend="h2"&gt;The trust thing that everyone gets wrong&lt;/head&gt;
    &lt;p&gt;Here's something counterintuitive: Users don't trust agents that are right all the time. They trust agents that are honest about when they might be wrong.&lt;/p&gt;
    &lt;p&gt;Think about it from the user's perspective. Your support agent confidently says "I've reset your password and updated your billing address." User thinks "great!" Then they try to log in and... it doesn't work. Now they don't just have a technical problem - they have a trust problem.&lt;/p&gt;
    &lt;p&gt;Compare that to an agent that says "I think I found the issue with your account. I'm 80% confident this will fix it. I'm going to reset your password and update your billing address. If this doesn't work, I'll immediately escalate to a human who can dive deeper."&lt;/p&gt;
    &lt;p&gt;Same technical capability. Completely different user experience.&lt;/p&gt;
    &lt;p&gt;Building trusted agents requires focus on three things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Confidence calibration: When your agent says it's 60% confident, it should be right about 60% of the time. Not 90%, not 30%. Actual 60%.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Reasoning transparency: Users want to see the agent's work. "I checked your account status (active), billing history (payment failed yesterday), and login attempts (locked after 3 failed attempts). The issue seems to be..."&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Graceful escalation: When your agent hits its limits, how does it hand off? A smooth transition to a human with full context is much better than "I can't help with that."&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A lot of times we obsess over making agents more accurate, when what users actually want was more transparency about the agent's limitations.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's Coming Next&lt;/head&gt;
    &lt;p&gt;In Part 2, I'll dive deeper into the autonomy decisions that keep most PMs up at night. How much independence should you give your agent? When should it ask for permission vs forgiveness? How do you balance automation with user control?&lt;/p&gt;
    &lt;p&gt;We'll also walk through the governance concerns that actually matter in practice - not just theoretical security issues, but the real implementation challenges that can make or break your launch timeline.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45129237</guid></item><item><title>AI Not Affecting Job Market Much So Far, New York Fed Says</title><link>https://money.usnews.com/investing/news/articles/2025-09-04/ai-not-affecting-job-market-much-so-far-new-york-fed-says</link><description>&lt;doc fingerprint="f2a33bd5f7bb5fbc"&gt;
  &lt;main&gt;
    &lt;p&gt;Software that streams data from databases to warehouses in real-time&lt;/p&gt;
    &lt;p&gt;We are building Artie, a real-time data streaming solution focused on databases and data warehouses. Typical ETL solutions leverage batched processes or schedulers (DAGs, Airflow), which cannot achieve real time data syncs. We leverage change data capture (CDC) and stream processing to perform data transfers in a more efficient way, which enables sub-minute latency.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45129267</guid></item><item><title>Artie (YC S23) Is Hiring Engineers, AES, and Senior PMM</title><link>https://www.ycombinator.com/companies/artie/jobs</link><description>&lt;doc fingerprint="f2a33bd5f7bb5fbc"&gt;
  &lt;main&gt;
    &lt;p&gt;Software that streams data from databases to warehouses in real-time&lt;/p&gt;
    &lt;p&gt;We are building Artie, a real-time data streaming solution focused on databases and data warehouses. Typical ETL solutions leverage batched processes or schedulers (DAGs, Airflow), which cannot achieve real time data syncs. We leverage change data capture (CDC) and stream processing to perform data transfers in a more efficient way, which enables sub-minute latency.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45129442</guid></item><item><title>Saquon Is Playing for Equity</title><link>https://www.readtheprofile.com/p/saquon-barkley-investment-portfolio</link><description>&lt;doc fingerprint="3f6099ad496f1cfe"&gt;
  &lt;main&gt;
    &lt;p&gt;Saquon Barkley calls me, but he’s distracted. In the background, two little voices shout “Bye, friends!” as Barkley wrangles his kids, Jada, 7, and Saquon Jr., 3, into the car. He apologizes, then explains they’re headed to an Old Spice photo shoot tied to his latest endorsement — a Saquon-branded shampoo and conditioner called “Saquon Soar.”&lt;/p&gt;
    &lt;p&gt;It’s a cinematic image: one of the NFL’s biggest stars juggling dad duty and the demands of a sponsorship. But Barkley isn’t content to just cash checks and smile for the camera. He has always treated these deals as building blocks for the empire he’s determined to build before football ends. And he knows it could all end suddenly.&lt;/p&gt;
    &lt;p&gt;From the moment he entered the NFL in 2018, Barkley carried a kind of financial paranoia that most athletes don’t confront until retirement. He parked his entire $31.2 million rookie contract into long-term investments like the S&amp;amp;P 500, vowing to live only off endorsements.&lt;/p&gt;
    &lt;p&gt;In 2021, he went further, backing payments app Strike as his first startup investment and pledging to take all marketing income in Bitcoin through the platform. At the time, Bitcoin’s price was approximately $32,000. Today, it’s hovering around $111,000, turning a $10 million income stream into a $35 million asset.&lt;/p&gt;
    &lt;p&gt;That move rattled more than a few people. “Good old American dollars should be the standard,” superagent Leigh Steinberg warned, cautioning athletes against chasing crypto volatility.&lt;/p&gt;
    &lt;p&gt;Barkley heard the critics, but he doubled down. Sitting on cash, he argued, isn’t enough — not when most athletes earn the bulk of their money in their 20s and face inflation, poor financial literacy, and limited access to tools. “A sad yet common reality is many enter bankruptcy later on,” he wrote on X. “We can do better.”&lt;/p&gt;
    &lt;p&gt;His drive isn’t just financial. Barkley has the career most players dream about, but those closest to him say that his humility is born out of a gnawing sense that it could all slip away.&lt;/p&gt;
    &lt;p&gt;For Barkley, equity is a safeguard — a way to wrest back control from the injuries, draft picks, or franchise decisions that can derail a career overnight.&lt;/p&gt;
    &lt;p&gt;Football is fragile, while investing gives him a different relationship to risk. It may be volatile, but it’s a risk he can shape. Guided by business manager Ken Katz, he has avoided the typical athlete playbook of podcasts and clothing lines, instead building a portfolio that looks more like an elite venture fund than a celebrity brand.&lt;/p&gt;
    &lt;p&gt;After reading Peter Thiel’s Zero to One, Barkley was most inspired by the idea of getting ahead by betting on businesses that create something so original that competition becomes irrelevant.&lt;/p&gt;
    &lt;p&gt;The Profile has exclusively learned that Barkley has invested a portion of his earnings so far — a mix of his rookie contract and endorsement income — across more than 10 private startups, typically writing checks between $250,000 and $500,000.&lt;/p&gt;
    &lt;p&gt;The high-growth startups in his portfolio include Anthropic (currently valued at $183 billion), Anduril ($30.5 billion), Ramp ($22.5 billion), Cognition ($9.8 billion), Neuralink ($9 billion), Strike (~$1 billion), and Polymarket (~$1 billion). He’s also a limited partner in funds including Founders Fund, Thrive Capital, Silver Point Capital, and Multicoin Capital.&lt;/p&gt;
    &lt;p&gt;Some of those bets have already appreciated dramatically: Strike, for example, has delivered a 10x return in value since his investment. Barkley has also allocated a portion of his wealth into steadier assets like index funds and real estate, hedging the volatility that comes with venture and crypto.&lt;/p&gt;
    &lt;p&gt;Unlike peers LeBron James and Serena Williams, who command headlines with splashy business ventures, Barkley’s approach is leaner and more surgical. He is making selective bets on technology companies that he believes are creating lasting value for their users.&lt;/p&gt;
    &lt;p&gt;The question is whether his bets will hold. Venture investing is inherently risky — even late-stage startups with multibillion-dollar valuations can falter or see their valuations slashed when markets turn. To date, none of Barkley’s investments have flamed out or depreciated, largely because he prefers to come in at later stages of a company’s growth.&lt;/p&gt;
    &lt;p&gt;In the end, Barkley must confront the question: Is he building something that lasts, or simply trading one kind of risk for another?&lt;/p&gt;
    &lt;p&gt;For more longform profiles of extraordinary people, make sure to sign up for The Profile here:&lt;/p&gt;
    &lt;p&gt;A few days before Thanksgiving last year, Ramp CEO Eric Glyman glanced at his phone and did a double take. His colleague Sam Buck had texted that Saquon Barkley wanted to invest in the expense management startup. Glyman was stunned. Most athletes ask for cash in exchange for an endorsement. Barkley wanted equity.&lt;/p&gt;
    &lt;p&gt;“The best player in the NFL wants to work with us,” Glyman remembers thinking. “Like, what? What’s happening?”&lt;/p&gt;
    &lt;p&gt;Barkley, who came in when Ramp was valued at $7 billion, didn’t just write a check. Convinced by Ramp’s mission to help businesses cut costs and perform more efficiently, he tied his own fortunes to the company’s — and then went to work to make sure it paid off.&lt;/p&gt;
    &lt;p&gt;During the 2025 Super Bowl, Barkley starred in a Ramp commercial: a 15-second spot of him, in full pads, drowning in paperwork before Ramp’s automation saves the day.&lt;/p&gt;
    &lt;p&gt;The ad aired as the Eagles defeated the Kansas City Chiefs, and Barkley set a new single-season rushing yards record for regular season and playoffs combined. For Ramp, it became their biggest traffic day ever. For Barkley, it was one of the rare times an endorsement literally boosted his own net worth.&lt;/p&gt;
    &lt;p&gt;That was by design. Barkley’s process usually begins with his business manager, Ken Katz, reaching out to a company. Their strategy is to target technology companies and source deals through trusted word-of-mouth referrals in Katz’s investor network. The founder then gets invited to dinner with Barkley, where the running back plays interrogator.&lt;/p&gt;
    &lt;p&gt;“It’s about asking them what they stand for, what their mission is, why they think they’ll be successful,” Barkley tells me. “They have to be confident, but arrogance is a turn-off.” If he’s interested, he often gives a verbal commitment on the spot.&lt;/p&gt;
    &lt;p&gt;Sam Buck, Ramp’s head of financial institutions, insists Barkley was never treated like a celebrity mascot. “It was like, ‘Hey, you’re on the cap table now,’” he says. “We’re going to hold you accountable — just like we do with Founders Fund or General Catalyst — to help us grow.”&lt;/p&gt;
    &lt;p&gt;And Barkley delivered. He FaceTimed Ramp partners to help close deals. He joined customer meet-and-greets. He talked about the company any chance he got.&lt;/p&gt;
    &lt;p&gt;Fresh off the Super Bowl, he appeared on the TODAY show, still buzzing from the win. He fielded questions about being a champion, about his family, about life off the field. Then TODAY co-host Savannah Guthrie pivoted. “You have your own Super Bowl commercial, by the way, for Ramp, where you’re an investor,” she said. “So tell me how all this came about.”&lt;/p&gt;
    &lt;p&gt;Barkley didn’t miss a beat. He turned the moment into a plug for his investment: “I fell in love with the team of Ramp … You get a lot of athletes who get involved with brands, and you get a certain dollar to show up and do things. But … as this company grows, I get to grow with it — it takes it to a whole new level for me.”&lt;/p&gt;
    &lt;p&gt;The strategy is working. Barkley invested $500,000 into Ramp at a $7 billion valuation. The company’s valuation has more than tripled since then, which means his stake is now worth roughly $1.5 million. Earlier this year, Ramp announced $700 million in annualized revenue. “We’re growing faster this year than last year,” Glyman says, “and I don’t think it’s unrelated that Saquon has been involved.”&lt;/p&gt;
    &lt;p&gt;But Barkley’s commitment also shows up in less public settings. In May, he flew to a Founders Fund symposium in Montana, an event filled with billionaires, CEOs, and top investors. He spent the day listening and networking, before catching a red-eye back to Philadelphia for a morning speaking obligation. Within hours, he was on a return flight to Montana. After another full day at the symposium, he boarded a midnight flight to make it to his daughter’s soccer game the next morning.&lt;/p&gt;
    &lt;p&gt;Longtime venture investor Brian Singerman noticed his dedication to building meaningful relationships. “That stuff is impossible to fake,” he says.&lt;/p&gt;
    &lt;p&gt;Even so, Barkley isn’t blind to the trade-offs. For someone intent on building generational wealth, venture capital represents both the biggest swing and the biggest vulnerability.&lt;/p&gt;
    &lt;p&gt;“Honestly, putting your money in the S&amp;amp;P is going to beat 90% of venture capital,” says Singerman, who is one of Barkley’s advisers. “But that last 10% is going to crush the S&amp;amp;P.”&lt;/p&gt;
    &lt;p&gt;This version of Barkley — the investor, the owner — didn’t come naturally. He didn’t grow up with stock tips or startup dinners. His financial philosophy was shaped by something starker: watching what happens when the money runs out.&lt;/p&gt;
    &lt;p&gt;When Barkley tells me about growing up in the Bronx, he chooses his words carefully. He had a loving family, he says, and doesn’t remember feeling the financial instability, at least not consciously.&lt;/p&gt;
    &lt;p&gt;But to understand his obsession with money, risk, and generational wealth, you have to start with his parents. Tonya and Alibay were both born in the Bronx and raised five children together.&lt;/p&gt;
    &lt;p&gt;In 2001, when Saquon was four, the family left for Pennsylvania. Over the years, Tonya developed a motto she repeated often: “All things are possible.” She believed it because she had lived it, navigating crises that could have broken the family but didn’t.&lt;/p&gt;
    &lt;p&gt;One of those crises came when Barkley was in elementary school. The family was evicted and spent eight months without a home. The shelter system wouldn’t take fathers, so they were split up. Barkley and his sister went to live with a family friend while his parents scrambled to rebuild.&lt;/p&gt;
    &lt;p&gt;“Financially, we obviously struggled, but my mom and dad never made us feel that,” he says. “It wasn’t like we were on the streets. We had family and friends who took us in, but [the experience] did give me the perspective I have on life right now.”&lt;/p&gt;
    &lt;p&gt;His father carries his own formative scar. A gifted boxer, Alibay had made it to the semifinals of the 1992 New York State Golden Gloves before his shoulder gave out. He couldn’t afford surgery. At 21, he quit — not by choice but by circumstance.&lt;/p&gt;
    &lt;p&gt;“I never had the money to get it fixed,” he says. The missed chance haunted him, and it became the lesson he drilled into his children: never quit on your passion. “If you quit this, it will be easy to quit jobs, quit relationships, quit on your kids,” he says. “That’s probably why Saquon is so adamant when he gets hurt about still being in there.”&lt;/p&gt;
    &lt;p&gt;That “never quit” mantra became Barkley’s compass. At 13, frustrated in school and doubting football, he thought about walking away. His father told him the boxing story again. It stuck.&lt;/p&gt;
    &lt;p&gt;From then on, Barkley zeroed in. He would play football, and he would become great.&lt;/p&gt;
    &lt;p&gt;“Growing up, he had flashes of being great, but there was always someone in front of him,” says childhood friend Nick Shafnisky. “He was never going to let someone say he wasn’t the best.”&lt;/p&gt;
    &lt;p&gt;At Penn State, Barkley fulfilled that prophecy, boasting 3,843 rushing yards and 43 touchdowns in three seasons. Head coach James Franklin drilled players with one message: “Use football to build generational wealth. Don’t let football use you.”&lt;/p&gt;
    &lt;p&gt;Barkley took it to heart. One night, Franklin overheard his wife on a call talking about real estate. He showered, brushed his teeth, and got into bed, and she was still on the phone. When she finally hung up, he asked who it was. “It’s Saquon,” she told him.&lt;/p&gt;
    &lt;p&gt;He was working on his first real estate investment and wanted to understand how to structure the deal and spot potential red flags. “I think curiosity is a really important trait in successful people, and Saquon is very curious,” Franklin tells me.&lt;/p&gt;
    &lt;p&gt;His questions about money and wealth went from theoretical to real when the New York Giants drafted Barkley second overall in the 2018 NFL Draft and handed him a $31 million rookie contract. He delivered immediately: 1,300 rushing yards, 91 receptions, Rookie of the Year, Pro Bowl. The dream was intact — until it wasn’t.&lt;/p&gt;
    &lt;p&gt;In Week 2 of the 2020 season, against the Chicago Bears, Barkley was tackled awkwardly near the sideline. He clutched his knee and was carted off the field. The diagnosis? A torn ACL, partially torn meniscus, and an MCL strain. It was a devastating cocktail of injuries. “I definitely had some dark moments during that time,” Barkley says. “I had those moments of, ‘Man, why me?’”&lt;/p&gt;
    &lt;p&gt;Whether Barkley realized it or not, it was his father’s nightmare revisited: one injury, one twist, and everything could vanish. At Katz’s suggestion, Barkley watched Peter Thiel’s “Competition Is for Losers” lecture at Stanford and read his book Zero to One, both of which left an impression. Thiel’s argument — that the biggest returns come from a few breakthrough bets — resonated. Football suddenly felt fragile, and investing began to feel urgent.&lt;/p&gt;
    &lt;p&gt;His return to the NFL was shaky. In 2021, after months of rehab, he sprained his ankle early in the season and stumbled to a career-low 593 rushing yards. Friends say he wasn’t himself. His mom reminded him: “No matter what, you can always get back up.” He did. By 2022, Barkley was back at 1,300 yards and another Pro Bowl.&lt;/p&gt;
    &lt;p&gt;But it was no longer possible to ignore the reality that his body and career were vulnerable. The Giants reminded him of that in 2023 when they slapped him with the “franchise tag.” It was a one-year deal — not the long-term contract he had earned. To Barkley, it was a gut punch. Even after proving himself, he was treated as expendable.&lt;/p&gt;
    &lt;p&gt;“It was a very contentious situation,” says former NFL defensive great J.J. Watt. “[Barkley] was the star of [the Giants], and they basically kicked him out the door.”&lt;/p&gt;
    &lt;p&gt;Publicly, Barkley stayed composed. Privately, the rejection lit a fire. Running backs have the shortest shelf life in the league, but his bigger worry was what comes after. Approximately 78% of NFL players face financial distress within two years of retiring, and he was determined not to be one of them.&lt;/p&gt;
    &lt;p&gt;By then, he had already built a portfolio and a mindset beyond football. In 2024, Barkley left the Giants for a three-year, $37.75 million deal with the Philadelphia Eagles.&lt;/p&gt;
    &lt;p&gt;It was a declaration that he knew his value, and he wasn’t going to sign a deal that didn’t reflect it.&lt;/p&gt;
    &lt;p&gt;In February 2025, confetti rained down on the Philadelphia Eagles after they defeated the Kansas City Chiefs in the Super Bowl. Barkley had delivered — a standout game, a title, and the biggest win of his career on his 28th birthday. “I don’t think anything is going to top this,” he said on the field moments after the final whistle.&lt;/p&gt;
    &lt;p&gt;That night, Barkley threw an afterparty at St. Pizza in New Orleans. The guest list was surreal, including Leonardo DiCaprio, Pete Davidson, Zac Efron, and Chance the Rapper. But scattered among the celebrities was a quieter, more unexpected group: the founders of Barkley’s portfolio companies.&lt;/p&gt;
    &lt;p&gt;Jack Mallers, CEO of Strike, was one of them. “I felt bad for him,” Mallers says. “Even though he had just won the Super Bowl and it was his birthday, it felt like he was still working. He took a photo with everyone. Answered every question. Asked the founders how business was going. It was unbelievable.”&lt;/p&gt;
    &lt;p&gt;Mallers first met Barkley through an email from business manager Ken Katz with the subject line: Saquon Barkley loves Bitcoin. “It felt like a scam,” Mallers laughs. But after dinner with Barkley, he realized the interest was real and gave him a spot in Strike’s $100 million round. Barkley invested $100,000, and today the company is reportedly valued at over $1 billion, marking a 10x return on his stake.&lt;/p&gt;
    &lt;p&gt;That deal forged a deep relationship. Since they met, Barkley and Mallers have spent hours debating Bitcoin as a store of value. In 2021, Barkley went all in on the digital currency, announcing he would convert 100% of his endorsement income — Nike, Pepsi, Visa, Dunkin’ — into Bitcoin using Strike.&lt;/p&gt;
    &lt;p&gt;The move was yet another example of Barkley using his platform to amplify a company in which he has equity. “The average NFL career is 3 years and inflation is real,” he tweeted. “Saving and preserving money over time is hard… Bitcoin is a proven, safe, global, and open system that allows anyone to save money.”&lt;/p&gt;
    &lt;p&gt;Crypto executives hailed his entrance into the world of Bitcoin. “He’s setting a blueprint for athletes to take charge of their financial future,” says Coinbase president Emilie Choi. But traditional advisers recoiled. “The value of Bitcoin is much more volatile than the value of the U.S. dollar,” one financial planner warned in an op-ed. “How would you feel if your hypothetical Bitcoin paycheck was worth 10% less the day after it was deposited into your account?”&lt;/p&gt;
    &lt;p&gt;For a while, Barkley looked prescient as his holdings appreciated in value. Mallers even joked, “My goal was to help him earn more through Bitcoin and Strike than he would in the NFL. And I think he will.”&lt;/p&gt;
    &lt;p&gt;But then came the crash. Famed crypto exchange FTX collapsed near the end of 2022, its founder was accused of fraud, and Bitcoin’s price plunged below $16,000 — more than 50% below the level at which Barkley had invested.&lt;/p&gt;
    &lt;p&gt;As he scrolled through the taunts on social media, Barkley picked up the phone and called Katz, asking the question on everyone’s mind: “What is going on?”&lt;/p&gt;
    &lt;p&gt;Katz told him to hold, and Barkley did. When I ask about embracing risk in that moment, he likens it to football: one week you’re celebrated, the next you’re vilified. Volatility, he says, is temporary.&lt;/p&gt;
    &lt;p&gt;That discipline is rooted in his partnership with Katz. The two met when Barkley was still at Penn State and Katz was 24, hustling to break into sports management. A decade later, Katz is still his consigliere who pushes him to think like a contrarian.&lt;/p&gt;
    &lt;p&gt;Katz has consistently advised Barkley to use his influence to win deals. “My thesis has always been: Use the fame to get equity in the companies of the best founders in the world,” Katz says.&lt;/p&gt;
    &lt;p&gt;The result is an unusually hands-on approach. Barkley keeps his circle small: just Katz, his financial adviser, and the founders he backs.&lt;/p&gt;
    &lt;p&gt;Compare that to NBA star Kevin Durant, whose investment firm 35V operates with more institutional rigor and a tilt toward safer asset classes like private equity and real estate. “You’re still getting ownership, but it’s just not as risky as venture,” says co-founder Rich Kleiman.&lt;/p&gt;
    &lt;p&gt;While Durant’s portfolio spans more than 100 companies, Barkley approaches investing with the focus of a rogue founder. He doesn’t have a fund structure, and he doesn’t neatly follow a playbook. He makes direct bets on companies that excite him. The strategy is lean, contrarian, and far more exposed.&lt;/p&gt;
    &lt;p&gt;That ethos resonates with Russell Okung, the first NFL player to take part of his salary in Bitcoin.&lt;/p&gt;
    &lt;p&gt;His advice for Barkley? “You’re not an athlete-investor, you’re an investor who allocates your time playing sports. The old model extracted value from athletes. The new model lets athletes extract value from everything else.” He adds, “Why be talent when you can be management? The cap table is where real wealth lives. It's where upside multiplies.”&lt;/p&gt;
    &lt;p&gt;It’s a mindset Barkley has adopted fully, though it comes with a healthy dose of risk. Where most of us see volatility as a warning sign, he recognizes it as something familiar.&lt;/p&gt;
    &lt;p&gt;Growing up without a safety net taught him that uncertainty can be an opening. “I’m not going to just jump into a pool of sharks,” he says, “but I do have a little bit of: ‘I don’t give a you-know-what.’”&lt;/p&gt;
    &lt;p&gt;Two days before the NFL draft, Barkley became a father. His daughter, Jada, arrived before the cameras and before the multimillion-dollar contract.&lt;/p&gt;
    &lt;p&gt;He remembers looking into her eyes and seeing the future flash before his. In that moment, he realized he wasn’t just building for himself anymore. He was building a world she could inherit.&lt;/p&gt;
    &lt;p&gt;“I was 21 when she was born,” he says, as Jada chatters in the car beside him. “I have a different dynamic with my kids than my parents had with me because I want to teach them how to be more financially literate. I’m still trying my best to learn, but as I learn, they’ll learn too.”&lt;/p&gt;
    &lt;p&gt;To Barkley, investing is about one thing: control. He is trying to manufacture permanence while playing a game defined by uncertainty. His ultimate driver, he says, is his family: his fiancée, Anna, and his kids. “If you had asked me this question two years ago, then my answer would’ve been, ‘I want to be the best football player,’ but it’s not about just that anymore.”&lt;/p&gt;
    &lt;p&gt;That shift is born out of urgency. Football is the most clock-driven sport, and his position is the most precarious. Does he feel the same countdown in his own career?&lt;/p&gt;
    &lt;p&gt;“It’s funny you ask me that,” Barkley says. “Because that’s something that was on my mind this morning. I was just thinking about how I can only play for so long, so I really gotta take advantage, keep investing, and create wealth for me and my family.”&lt;/p&gt;
    &lt;p&gt;He knows the day is coming when his body will tell him the game is over. But when his children ask what he built, he doesn’t only want to point to highlight reels or rushing titles. He wants to point to ownership — proof that he wrestled fragility into permanence.&lt;/p&gt;
    &lt;p&gt;That is the paradox of Saquon Barkley: a man willing to gamble with risk in his life so that his kids never have to feel the sharp edge of uncertainty in theirs.&lt;/p&gt;
    &lt;p&gt;-&lt;/p&gt;
    &lt;p&gt;Written by Polina Pompliano | Edited by Laura Entis / Photos by Sharif Fennell Jr. / Video by Matt Marlinski&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45129523</guid></item><item><title>The Atomic Bombs Exploded on Earth (2015)</title><link>https://kottke.org/25/09/the-thousands-of-atomic-bombs-exploded-on-earth</link><description>&lt;doc fingerprint="2f0c13b2ca5275f8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The Thousands of Atomic Bombs Exploded on Earth&lt;/head&gt;
    &lt;p&gt;From Orbital Mechanics, a visualization of the 2153 nuclear weapons exploded on Earth since 1945.&lt;/p&gt;
    &lt;p&gt;2153! I had no idea there had been that much testing. According to Wikipedia, the number is 2119 tests, with most of those coming from the US (1032) and the USSR (727). The largest device ever detonated was Tsar Bomba, a 50-megaton hydrogen bomb set off in the atmosphere above an island in the Barents Sea in 1961. Tsar Bomba had more than three times the yield of the largest bomb tested by the US. The result was spectacular.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The fireball reached nearly as high as the altitude of the release plane and was visible at almost 1,000 kilometres (620 mi) away from where it ascended. The subsequent mushroom cloud was about 64 kilometres (40 mi) high (over seven times the height of Mount Everest), which meant that the cloud was above the stratosphere and well inside the mesosphere when it peaked. The cap of the mushroom cloud had a peak width of 95 kilometres (59 mi) and its base was 40 kilometres (25 mi) wide.&lt;/p&gt;
      &lt;p&gt;All buildings in the village of Severny (both wooden and brick), located 55 kilometres (34 mi) from ground zero within the Sukhoy Nos test range, were destroyed. In districts hundreds of kilometers from ground zero wooden houses were destroyed, stone ones lost their roofs, windows and doors; and radio communications were interrupted for almost one hour. One participant in the test saw a bright flash through dark goggles and felt the effects of a thermal pulse even at a distance of 270 kilometres (170 mi). The heat from the explosion could have caused third-degree burns 100 km (62 mi) away from ground zero. A shock wave was observed in the air at Dikson settlement 700 kilometres (430 mi) away; windowpanes were partially broken to distances of 900 kilometres (560 mi). Atmospheric focusing caused blast damage at even greater distances, breaking windows in Norway and Finland. The seismic shock created by the detonation was measurable even on its third passage around the Earth.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Soviets did not give a fuck, man…what are a few thousand destroyed homes compared to scaring the shit out of the capitalist Amerikanskis with a comically large explosion? Speaking of bonkers Communist dictatorships, the last nuclear test conducted on Earth was in 2013, by North Korea.&lt;/p&gt;
    &lt;p&gt;Update: Since this post was published, North Korea has tested a few more nuclear devices, the last one in 2017.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45130018</guid></item></channel></rss>