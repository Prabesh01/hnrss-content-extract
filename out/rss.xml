<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 02 Jan 2026 10:42:37 +0000</lastBuildDate><item><title>2025 Letter</title><link>https://danwang.co/2025-letter/</link><description>&lt;doc fingerprint="bf3891178fc5067d"&gt;
  &lt;main&gt;
    &lt;p&gt;(This piece is my year in review; I skipped a letter last year)&lt;/p&gt;
    &lt;p&gt;One way that Silicon Valley and the Communist Party resemble each other is that both are serious, self-serious, and indeed, completely humorless.&lt;/p&gt;
    &lt;p&gt;If the Bay Area once had an impish side, it has gone the way of most hardware tinkerers and hippie communes. Which of the tech titans are funny? In public, they tend to speak in one of two registers. The first is the blandly corporate tone we’ve come to expect when we see them dragged before Congressional hearings or fireside chats. The second leans philosophical, as they compose their features into the sort of reverie appropriate for issuing apocalyptic prophecies on AI. Sam Altman once combined both registers at a tech conference when he said: “I think that AI will probably, most likely, sort of lead to the end of the world. But in the meantime, there will be great companies created with serious machine learning.” Actually that was pretty funny.&lt;/p&gt;
    &lt;p&gt;It wouldn’t be news to the Central Committee that only the paranoid survive. The Communist Party speaks in the same two registers as the tech titans. The po-faced men on the Politburo tend to make extraordinarily bland speeches, laced occasionally with a murderous warning against those who cross the party’s interests. How funny is the big guy? We can take a look at an official list of Xi Jinping’s jokes, helpfully published by party propagandists. These wisecracks include the following: “On an inspection tour to Jiangsu, Xi quipped that the true measure of water cleanliness is whether the mayor would dare to swim in the water.” Or try this reminiscence that Xi offered on bad air quality: “The PM2.5 back then was even worse than it is now; I used to joke that it was PM250.” Yes, such a humorous fellow is the general secretary.1&lt;/p&gt;
    &lt;p&gt;It’s nearly as dangerous to tweet a joke about a top VC as it is to make a joke about a member of the Central Committee. People who are dead serious tend not to embody sparkling irony. Yet the Communist Party and Silicon Valley are two of the most powerful forces shaping our world today. Their initiatives increase their own centrality while weakening the agency of whole nation states. Perhaps they are successful because they are remorseless.&lt;/p&gt;
    &lt;p&gt;Earlier this year, I moved from Yale to Stanford. The sun and the dynamism of the west coast have drawn me back. I found a Bay Area that has grown a lot weirder since I lived there a decade ago. In 2015, people were mostly working on consumer apps, cryptocurrencies, and some business software. Though it felt exciting, it looks in retrospect like a more innocent, even a more sedate, time. Today, AI dictates everything in San Francisco while the tech scene plays a much larger political role in the United States. I can’t get over how strange it all feels. In the midst of California’s natural beauty, nerds are trying to build God in a Box; meanwhile, Peter Thiel hovers in the background presenting lectures on the nature of the Antichrist. This eldritch setting feels more appropriate for a Gothic horror novel than for real life.&lt;/p&gt;
    &lt;p&gt;Before anyone gets the wrong idea, I want to say that I am rooting for San Francisco. It’s tempting to gawk at the craziness of the culture, as much of the east coast media tends to do. Yes, one can quickly find people who speak with the conviction of a cultist; no, I will not inject the peptides proffered by strangers. But there’s more to the Bay Area than unusual health practices. It is, after all, a place that creates not only new products, but also new modes of living. I’m struck that some east coast folks insist to me that driverless cars can’t work and won’t be accepted, even as these vehicles populate the streets of the Bay Area. Coverage of Silicon Valley increasingly reminds me of coverage of China, where a legacy media reporter might parachute in, write a dispatch on something that looks deranged, and leave without moving past caricature.&lt;/p&gt;
    &lt;p&gt;I enjoy San Francisco more than when I was younger because I now better appreciate what makes it work. I believe that Silicon Valley possesses plenty of virtues. To start, it is the most meritocratic part of America. Tech is so open towards immigrants that it has driven populists into a froth of rage. It remains male-heavy and practices plenty of gatekeeping. But San Francisco better embodies an ethos of openness relative to the rest of the country. Industries on the east coast — finance, media, universities, policy — tend to more carefully weigh name and pedigree. Young scientists aren’t told they ought to keep their innovations incremental and their attitude to hierarchy duly deferential, as they might hear in Boston. A smart young person could achieve much more over a few years in SF than in DC. People aren’t reminiscing over some lost golden age that took place decades ago, as New Yorkers in media might do.&lt;/p&gt;
    &lt;p&gt;San Francisco is forward looking and eager to try new ideas. Without this curiosity, it wouldn’t be able to create whole new product categories: iPhones, social media, large language models, and all sorts of digital services. For the most part, it’s positive that tech values speed: quick product cycles, quick replies to email. Past success creates an expectation that the next technological wave will be even more exciting. It’s good to keep building the future, though it’s sometimes absurd to hear someone pivot, mid-breath, from declaring that salvation lies in the blockchain to announcing that AI will solve everything.&lt;/p&gt;
    &lt;p&gt;People like to make fun of San Francisco for not drinking; well, that works pretty well for me. I enjoy board games and appreciate that it’s easier to find other players. I like SF house parties, where people take off their shoes at the entrance and enter a space in which speech can be heard over music, which feels so much more civilized than descending into a loud bar in New York. It’s easy to fall into a nerdy conversation almost immediately with someone young and earnest. The Bay Area has converged on Asian-American modes of socializing (though it lacks the emphasis on food). I find it charming that a San Francisco home that is poorly furnished and strewn with pizza boxes could be owned by a billionaire who can’t get around to setting up a bed for his mattress.&lt;/p&gt;
    &lt;p&gt;There’s still no better place for a smart, young person to go in the world than Silicon Valley. It adores the youth, especially those with technical skill and the ability to grind. Venture capitalists are chasing younger and younger founders: the median age of the latest Y Combinator cohort is only 24, down from 30 just three years ago. My favorite part of Silicon Valley is the cultivation of community. Tech founders are a close-knit group, always offering help to each other, but they circulate actively amidst the broader community too. (The finance industry in New York by contrast practices far greater secrecy.) Tech has organizations I think of as internal civic institutions that try to build community. They bring people together in San Francisco or retreats north of the city, bringing together young people to learn from older folks.&lt;/p&gt;
    &lt;p&gt;Silicon Valley also embodies a cultural tension. It is playing with new ideas while being open to newcomers; at the same time, it is a self-absorbed place that doesn’t think so much about the broader world. Young people who move to San Francisco already tend to be very online. They know what they’re signing up for. If they don’t fit in after a few years, they probably won’t stick around. San Francisco is a city that absorbs a lot of people with similar ethics, which reinforces its existing strengths and weaknesses.&lt;/p&gt;
    &lt;p&gt;Narrowness of mind is something that makes me uneasy about the tech world. Effective altruists, for example, began with sound ideas like concern for animal welfare as well as cost-benefit analyses for charitable giving. But these solid premises have launched some of its members towards intellectual worlds very distant from moral intuitions that most people hold; they’ve also sent a few into jail. The well-rounded type might struggle to stand out relative to people who are exceptionally talented in a technical domain. Hedge fund managers have views about the price of oil, interest rates, a reliably obscure historical episode, and a thousand other things. Tech titans more obsessively pursue a few ideas — as Elon Musk has on electric vehicles and space launches — rather than developing a robust model of the world.&lt;/p&gt;
    &lt;p&gt;So the 20-year-olds who accompanied Mr. Musk into the Department of Government Efficiency did not, I would say, distinguish themselves with their judiciousness. The Bay Area has all sorts of autistic tendencies. Though Silicon Valley values the ability to move fast, the rest of society has paid more attention to instances in which tech wants to break things. It is not surprising that hardcore contingents on both the left and the right have developed hostility to most everything that emerges from Silicon Valley.&lt;/p&gt;
    &lt;p&gt;There’s a general lack of cultural awareness in the Bay Area. It’s easy to hear at these parties that a person’s favorite nonfiction book is Seeing Like a State while their aspirationally favorite novel is Middlemarch. Silicon Valley often speaks in strange tongues, starting podcasts and shows that are popular within the tech world but do not travel far beyond the Bay Area. Though San Francisco has produced so much wealth, it is a relative underperformer in the national culture. Indie movie theaters keep closing down while all sorts of retail and art institutions suffer from the crumminess of downtown. The symphony and the opera keep cutting back on performances — after Esa-Pekka Salonen quit the directorship of the symphony, it hasn’t been able to name a successor. Wealthy folks in New York and LA have, for generations, pumped money into civic institutions. Tech elites mostly scorn traditional cultural venues and prefer to fund the next wave of technology instead.&lt;/p&gt;
    &lt;p&gt;One of the things I like about the finance industry is that it might be better at encouraging diverse opinions. Portfolio managers want to be right on average, but everyone is wrong three times a day before breakfast. So they relentlessly seek new information sources; consensus is rare, since there are always contrarians betting against the rest of the market. Tech cares less for dissent. Its movements are more herdlike, in which companies and startups chase one big technology at a time. Startups don’t need dissent; they want workers who can grind until the network effects kick in. VCs don’t like dissent, showing again and again that many have thin skins. That contributes to a culture I think of as Silicon Valley’s soft Leninism. When political winds shift, most people fall in line, most prominently this year as many tech voices embraced the right.&lt;/p&gt;
    &lt;p&gt;The two most insular cities I’ve lived in are San Francisco and Beijing. They are places where people are willing to risk apocalypse every day in order to reach utopia. Though Beijing is open only to a narrow slice of newcomers — the young, smart, and Han — its elites must think about the rest of the country and the rest of the world. San Francisco is more open, but when people move there, they stop thinking about the world at large. Tech folks may be the worst-traveled segment of American elites. People stop themselves from leaving in part because they can correctly claim to live in one of the most naturally beautiful corners of the world, in part because they feel they should not tear themselves away from inventing the future. More than any other topic, I’m bewildered by the way that Silicon Valley talks about AI.&lt;/p&gt;
    &lt;p&gt;Hallucinating the end of history&lt;/p&gt;
    &lt;p&gt;While critics of AI cite the spread of slop and rising power bills, AI’s architects are more focused on its potential to produce surging job losses. Anthropic chief Dario Amodei takes pains to point out that AI could push the unemployment rate to 20 percent by eviscerating white-collar work.2 I wonder whether this message is helping to endear his product to the public.&lt;/p&gt;
    &lt;p&gt;The most-read essay from Silicon Valley this year was AI 2027. The five authors, who come from the AI safety world, outline a scenario in which superintelligence wakes up in 2027; a decade later, it decides to annihilate humanity with biological weapons. My favorite detail in the report is that humanity would persist in a genetically modified form, after the AI reconstructs creatures that are “to humans what corgis are to wolves.” It’s hard to know what to make of this document, because the authors keep tucking important context into footnotes, repeatedly saying they do not endorse a prediction. Six months after publication, they stated that their timelines were lengthening, but even at the start their median forecast for the arrival of superintelligence was later than 2027. Why they put that year in their title remains beyond me.&lt;/p&gt;
    &lt;p&gt;It’s easy for conversations in San Francisco to collapse into AI. At a party, someone told me that we no longer have to worry about the future of manufacturing. Why not? “Because AI will solve it for us.” At another, I heard someone say the same thing about climate change. One of the questions I receive most frequently anywhere is when Beijing intends to seize Taiwan. But only in San Francisco do people insist that Beijing wants Taiwan for its production of AI chips. In vain do I protest that there are historical and geopolitical reasons motivating the desire, that chip fabs cannot be violently seized, and anyway that Beijing has coveted Taiwan for approximately seven decades before people were talking about AI.&lt;/p&gt;
    &lt;p&gt;Silicon Valley’s views on AI made more sense to me after I learned the term “decisive strategic advantage.” It was first used by Nick Bostrom’s 2014 book Superintelligence, which defined it as a technology sufficient to achieve “complete world domination.” How might anyone gain a DSA? A superintelligence might develop cyber advantages that cripple the adversary’s command-and-control capabilities. Or the superintelligence could self-recursively improve such that the lab or state that controls it gains an insurmountable scientific advantage. Once an AI reaches a certain capability threshold, it might need only weeks or hours to evolve into a superintelligence.3 And if an American lab builds it, it might help to lock in the dominance of another American century.&lt;/p&gt;
    &lt;p&gt;If you buy the potential of AI, then you might worry about the corgi-fication of humanity by way of biological weapons. This hope also helps to explain the semiconductor controls unveiled by the Biden administration in 2022. If the policymakers believe that DSA is within reach, then it makes sense to throw almost everything into grasping it while blocking the adversary from the same. And it barely matters if these controls stimulate Chinese companies to invent alternatives to American technologies, because the competition will be won in years, not decades.&lt;/p&gt;
    &lt;p&gt;The trouble with these calculations is that they mire us in epistemically tricky terrain. I’m bothered by how quickly the discussions of AI become utopian or apocalyptic. As Sam Altman once said (and again this is fairly humorous): “AI will be either the best or the worst thing ever.” It’s a Pascal’s Wager, in which we’re sure that the values are infinite, but we don’t know in which direction. It also forces thinking to be obsessively short term. People start losing interest in problems of the next five or ten years, because superintelligence will have already changed everything. The big political and technological questions we need to discuss are only those that matter to the speed of AI development. Furthermore, we must sprint towards a post-superintelligence world even though we have no real idea what it will bring.&lt;/p&gt;
    &lt;p&gt;Effective altruists used to be known for their insistence on thinking about the very long run; much more of the movement now is concerned about the development of AI in the next year. Call me a romantic, but I believe that there will be a future, and indeed a long future, beyond 2027. History will not end. We need to cultivate the skill of exact thinking in demented times.&lt;/p&gt;
    &lt;p&gt;I am skeptical of the decisive strategic advantage when I filter it through my main preoccupation: understanding China’s technology trajectories. On AI, China is behind the US, but not by years. There’s no question that American reasoning models are more sophisticated than the likes of DeepSeek and Qwen. But the Chinese efforts are doggedly in pursuit, sometimes a bit closer to US models, sometimes a bit further. By virtue of being open-source (or at least open-weight), the Chinese models have found receptive customers overseas, sometimes with American tech companies.4 If US labs achieve superintelligence, the Chinese labs are probably on a good footing to follow closely. Unless the DSA is decisive immediately, it’s not obvious that the US will have a monopoly on this technology, just as it could not keep it over the bomb.&lt;/p&gt;
    &lt;p&gt;One advantage for Beijing is that much of the global AI talent is Chinese. We can tell from the CVs of researchers as well as occasional disclosures from top labs (for example from Meta) that a large percentage of AI researchers earned their degrees from Chinese universities. American labs may be able to declare that “our Chinese are better than their Chinese.” But some of these Chinese researchers may decide to repatriate. I know that many of them prefer to stay in the US: their compensation might be higher by an order of magnitude, they have access to compute, and they can work with top peers.5But they may also tire of the uncertainty created by Trump’s immigration policy. It’s never worth forgetting that at the dawn of the Cold War, the US deported Qian Xuesen, the CalTech professor who then built missile delivery systems for Beijing. Or these Chinese researchers expect life in Shanghai to be safer or more fun than in San Francisco. Or they miss mom. People move for all sorts of reasons, so I’m reluctant to believe that the US has a durable talent advantage.&lt;/p&gt;
    &lt;p&gt;China has other advantages in building AI. Superintelligence will demand a superload of power. By now everyone has seen the chart with two curves: US electrical generation capacity, which has barely budged upwards since the year 2000; and China’s capacity, which was one-third US levels in 2000 and more than two-and-a-half times US levels in 2024. Beijing is building so much solar, coal, and nuclear to make sure that no data center shall be in want. Though the US has done a superb job building data centers, it hasn’t prepared enough for other bottlenecks. Especially not as Trump’s dislike of wind turbines has removed this source of growth. Speaking of Trump’s whimsy, he has also been generous with selling close-to-leading chips to Beijing. That’s another reason that data centers might not represent a US advantage for long.&lt;/p&gt;
    &lt;p&gt;Silicon Valley has not demonstrated joined-up thinking for deploying AI. It would help if they learned from the central planners. The AI labs have not shown that they’re thinking seriously about how to diffuse the technology throughout society, which will require extensive regulatory and legal reform. How else will AI be able to fold doctors and lawyers into its tender mercies? Doing politics will also mean reaching out to more of the electorate, who are often uneasy with Silicon Valley’s promises while they see rising electrical bills. Silicon Valley has done a marvelous job in building data centers. But tech titans don’t look ready to plan for later steps in leading the whole-of-society effort into deploying AI everywhere.&lt;/p&gt;
    &lt;p&gt;The Communist Party lives for whole-of-society efforts. That’s what Leninist systems are built for. Beijing has set targets for deploying AI across society, though as usual with planning announcements, these numerical targets should be taken seriously and not literally. Chinese founders talk about AI mostly as a technology to be harnessed rather than a fickle power that might threaten all.6 Rather than building superintelligence, Chinese companies have been more interested in embedding AI into robots and manufacturing lines. Some researchers believe that this sort of embodied AI might present the real path towards superintelligence.7We might furthermore wonder how the US and China will use AI. Since the US is much more services-driven, Americans may be using AI to produce more powerpoints and lawsuits; China, by virtue of being the global manufacturer, has the option to scale up production of more electronics, more drones, and more munitions.&lt;/p&gt;
    &lt;p&gt;Dean Ball, who helped craft the White House’s action plan on AI, has written a perceptive post on how the US is playing to its strengths — software, chips, cloud computing, financing — while China is also focused on leaning on manufacturing excellence. In his view, “the US economy is increasingly a highly leveraged bet on deep learning.” Certainly there’s a lot of money invested here, but it looks risky to be so concentrated. I believe it’s unbecoming for the world’s largest economy to be so levered on one technology. That’s a more appropriate strategy for a small country. Why shouldn’t the US be better positioned across the entirety of the supply chain, from electron production to electronics production?&lt;/p&gt;
    &lt;p&gt;I am not a skeptic of AI. I am a skeptic only of the decisive strategic advantage, which treats awakening the superintelligence as the final goal. Rather than “winning the AI race,” I prefer to say that the US and China need to “win the AI future.” There is no race with a clear end point or a shiny medal for first place. Winning the future is the more appropriately capacious term that incorporates the agenda to build good reasoning models as well as the effort to diffuse it across society. For the US to come ahead on AI, it should build more power, revive its manufacturing base, and figure out how to make companies and workers make use of this technology. Otherwise China might do better when compute is no longer the main bottleneck.&lt;/p&gt;
    &lt;p&gt;The humming tech engine&lt;/p&gt;
    &lt;p&gt;I’ve had Silicon Valley friends tell me that they are planning a trip to China nearly every month this year. Silicon Valley respects and fears companies from only one other country. Game recognizes game, so to speak. Tech founders may begrudge China’s restrictions; and some companies have suffered directly from IP theft. But they also recognize that Chinese companies can move even faster than they do with their teams of motivated workers; and Chinese manufacturers are far ahead of US capabilities on anything involving physical production. Some founders and VCs are impressed with the fact that Chinese AI companies have gotten this far while suffering American tech restrictions, while leading in open-source to boot. VCs are wondering whether they may still invest in Chinese startups or Chinese founders who have moved abroad.&lt;/p&gt;
    &lt;p&gt;2025 is the year that Chinese tech successes have really blossomed into the wider American consciousness. There’s no need to retread the coverage around DeepSeek, the surge of electric vehicle exports, or new developments in robotics. When I first moved from Silicon Valley to China in 2017, I felt some degree of skepticism from my friends that I was taking myself out of the beating heart of the technological universe and into the unknown. But it was clear to me that Chinese firms were improving on quality and taking global market share. I wrote in my 2019 letter: “Chinese workers are working with the latest tools to produce most of the world’s goods; over the longer term, my hypothesis is that they’ll be able to replicate the tooling and make just as good final products.”&lt;/p&gt;
    &lt;p&gt;I think that has become closer to consensus views. I believe that Chinese technological success is now the rule rather than the exception. There are two fields in which China is substantially behind the west: semiconductors and aviation. The chip sector is gingerly attempting to expand under the weight of US restrictions; meanwhile, China’s answer to Airbus and Boeing is on a very long runway. I grant that these are two critical technologies, but China has attained technological leadership almost everywhere else. And I believe its technological momentum will continue rolling onwards to engulf more of their western competitors over the next decade.&lt;/p&gt;
    &lt;p&gt;The electric vehicle industry is the sharp tip of the spear of China’s global success. Chinese EVs have greater functionalities than western models while selling at lower price points. A rule of thumb is that it takes five years from an American, German, or Japanese automaker to dream up a new car design and launch that model on the roads; in China, it’s closer to 18 months. The Chinese market is full of demanding customers as well as fast-iterating automotive suppliers. It also has a more productive workforce. According to Tesla’s corporate disclosures, a worker at a Gigafactory in China produces an average of 47 vehicles a year; a worker at a Gigafactory in California produces an average of 20.8&lt;/p&gt;
    &lt;p&gt;China’s automotive success is biting into Germany more than anywhere else. I keep a scrapbook filled with mournful remarks that German executives offer to newspapers. “Most of what German Mittelstand firms do these days, Chinese companies can do just as well,” said a consultant to the Financial Times. “In my sector they look at the price-point of the market leader and sell for roughly half of that,” the boss of a medical devicemaker told the Economist. It’s never hard to find parades of gloomy Germans. Now more than ever it looks like their core competences are threatened by Chinese firms.&lt;/p&gt;
    &lt;p&gt;I often think of the case of Xiaomi. In 2021, Lei Jun vowed that the company he founded would break into the EV business. Four years later, Xiaomi started shipping cars to customers. Not only that, a Xiaomi EV set a speed record at the Nürburgring racetrack in Germany. Compare Xiaomi to Apple, which spent 10 years and $10 billion studying whether to enter the EV market before it pulled the plug. The world’s most advanced consumer product company could not match Xiaomi’s feat. It’s cases like these that make me skeptical of reasoning about China’s tech successes through financial measures or productivity ratios. As of this writing, Xiaomi’s market value is $130 billion. That is only around half of the market value of AppLovin, the mobile advertisement company. Rather than being an indictment of Xiaomi, I view this imbalance as an indictment of financial valuations. Isn’t it better, from a national power perspective, to develop firms like Xiaomi, which calls its shots and then makes them?&lt;/p&gt;
    &lt;p&gt;This comparison between Xiaomi and Apple motivated an essay I wrote with Dragonomics founder Arthur Kroeber in an issue of Foreign Affairs. Our view is that China’s industrial success has roots in deep infrastructure. That includes not only ports and rail, it also includes data connectivity, electrification, and process knowledge. China’s strength lies in a robust manufacturing ecosystem full of self-reinforcing parts.&lt;/p&gt;
    &lt;p&gt;Chinese tech achievements that were apparent in 2025 were the fruits of investments made a decade ago. Given that China continues to invest massively in technology, I expect we’ll see yet more tech successes for another decade to come. Alexander Grothendieck used an analogy of a walnut to describe different approaches to mathematics, which might also apply to technology development. Some mathematicians crack their problems by finding the right spot to insert a chisel before making a clean strike. Grothendieck described his own approach as coming up with general solutions, as if he were immersing the walnut in a bath for such a long time that mere hand pressure would be enough to open it. The US comes up with exquisite and expensive solutions to its technology problems. China’s industrial ecosystem is more like a rising sea, softening many nuts at once.9&lt;/p&gt;
    &lt;p&gt;When these nuts open, it looks like China is producing a big wave of new products. These are its breakthroughs in drones, electric vehicles, and robotics. Years from now we may see greater success in biotech as well. I am keen to follow along China’s progress in electromagnetism over the next decade. China’s industrial ecosystem is leading the way in replacing combustion with electromagnetic processes. Everything is now drone, as the combination of cheaper batteries and better permanent magnets displaces the engine.10&lt;/p&gt;
    &lt;p&gt;One of the startling geopolitical moves of the year was how quickly Donald Trump withdrew his ~150 percent tariffs on China. Trump folded not out of beneficence, but because Xi Jinping denied rare earth magnets to most of the world, threatening many types of manufacturing operations. And yet I’m struck by Beijing’s relative restraint. Chinese producers are close to being monopolists not only in rare earths, but also electronics products, batteries, and many types of active pharmaceutical ingredients. In case China denies, say, cardiovascular drugs to the elderly, how long could a state hold out?&lt;/p&gt;
    &lt;p&gt;One might have expected the US to have roused itself after this bout of the trade war. But there have been too many declarations of Sputnik Moments without commensurate action. Barack Obama declared a Sputnik with China’s high-speed rail; Mark Warner repeated with Huawei’s 5G; Marc Andreessen called it with DeepSeek. The more that people use the term, the less likely that society spurs itself into taking it seriously.&lt;/p&gt;
    &lt;p&gt;I think the US continues to systematically underrate China’s industrial progress for several reasons.&lt;/p&gt;
    &lt;p&gt;First, too many western elites retain hope that China’s efforts will run out of fuel by its own accord. Industrial progress will be weighed down by demographic drag, the growing debt load, maybe even a political collapse. I won’t rule these out, but I don’t think they are likely to break China’s humming tech engine. Demographics in particular don’t matter for advanced technology — you don’t need a workforce of many millions to have robust production of semiconductors or EVs. South Korea, for example, has one of the world’s fastest shrinking populations while retaining its success in electronics production. And though China suffers broader economic headwinds, technology firms like Xiaomi continue to develop new products and enjoy rising revenues. Technology breakthroughs can occur even in a suffering society. Especially if the state continues to lavish resources on chips or anything that could represent an American chokepoint.&lt;/p&gt;
    &lt;p&gt;Second, western elites keep citing the wrong reasons for China’s success. When members of Congress get around to acknowledging China’s tech advancements, they do not fail to attribute causes to either industrial subsidies (also known as cheating) or IP theft (that is, stealing). These are legitimate claims, but China’s advantages extend far beyond them. That’s the creation of deep infrastructure as well as extensive industrial ecosystems that I describe above.&lt;/p&gt;
    &lt;p&gt;Probably the most underrated part of the Chinese system is the ferocity of market competition. It’s excusable not to see that, given that the party espouses so much Marxism. I would argue that China embodies both greater capitalist competition and greater capitalist excess than America does today. Part of the reason that China’s stock market trends sideways is that everyone’s profits are competed away. Big Tech might enjoy the monopolistic success smiled upon by Peter Thiel, coming almost to genteel agreements not to tread too hard upon each other’s business lines. Chinese firms have to fight it out in a rough-and-tumble environment, expanding all the time into each other’s core businesses, taking Jeff “your margin is my opportunity” Bezos with seriousness.&lt;/p&gt;
    &lt;p&gt;Third, western elites keep holding on to a distinction between “innovation,” which is mostly the remit of the west, and “scaling,” which they accept that China can do. I want to dissolve that distinction. Chinese workers innovate every day on the factory floor. By being the site of production, they have a keen sense of how to make technical improvements all the time. American scientists may be world leaders in dreaming up new ideas. But American manufacturers have been poor at building industries around these ideas. The history books point out that Bell Labs invented the first solar cell in 1957; today, the lab no longer exists while the solar industry moved to Germany and then to China. While Chinese universities have grown more capable at producing new ideas, it’s not clear that the American manufacturing base has grown stronger at commercializing new inventions.&lt;/p&gt;
    &lt;p&gt;I sometimes hear that the US will save manufacturers through automation. The truth is that Chinese factories tend to be ahead on automation: that’s a big part of the reason that Chinese Tesla workers are more productive than California Tesla workers. China regularly installs as many robots as the rest of the world put together. They are also able to provide greater amounts of training data for AI. We have to be careful not to let automation, like superintelligence, become an excuse for magical thinking rather than doing the hard work of capacity building.&lt;/p&gt;
    &lt;p&gt;Outlasting the adversary&lt;/p&gt;
    &lt;p&gt;The China discussions I get into on the east coast tend to focus on the country’s problems. Washington, DC in particular likes to ask questions like: didn’t we think that Japan was going to overrun the world with manufacturing before it fell apart? Isn’t China mostly a mess? These are ultimately variants of the form: how might China fail?&lt;/p&gt;
    &lt;p&gt;The west coast flavor of the discussion is different. People are more inclined to ask: what happens if China succeeds? That reflects, in part, Silicon Valley’s epistemic bias towards securing upside returns rather than minimizing downside risks. They also tend to make more frequent visits to China than folks in DC. “What if China succeeds?” is certainly the more interesting question to me, not only because my career has been studying China’s technological successes. The east coast questions deserve to be taken seriously. But I fear that dwelling on China’s failure modes will coax elites into complacency, serving a narrative that the US needs to change nothing before the adversary will topple, robbing the country of urgency to reform.&lt;/p&gt;
    &lt;p&gt;I want to be clear that though I expect China will overrun advanced technology industries, it won’t make the country a broad success. Over the past five years, it has been mired in disinflationary growth, where young people struggle to find a job and find a spouse. The political system is growing even more opaque, terrifying even the insiders. This year, Xi deposed a dozen generals of the People’s Liberation Army, one of whom was also a sitting Politburo member. I wonder how many people inside the Politburo feel confident about where they stand with Xi.&lt;/p&gt;
    &lt;p&gt;Entrepreneurs are on even worse ground. Earlier this year, investors greeted Xi’s handshake with prominent entrepreneurs (including Jack Ma) as good news. It was so, but who can be sure that Xi will not greet them differently once they revive the economy? Though Xi can cut entrepreneurs some slack, the trend is towards greater party control over business and society. Xi himself doesn’t evince concern that economic growth is lackluster. It’s an acceptable tradeoff for making China’s economy less dependent on foreign powers. None of this is a formula for broad human flourishing. Rather, it is depriving Chinese of contact with the rest of the world.&lt;/p&gt;
    &lt;p&gt;Beijing has been working relentlessly to build up its resilience. While the US talks itself out of Sputnik Moments, Beijing has dedicated immense resources to patching up its own deficiencies. It’s not a theoretical fear that Chinese companies might lose access to American technologies. So the state is pouring more money than ever before into semiconductor makers and research universities. It is investing in clean technologies not so much because it cares about the climate, but because it wants to be self-sufficient in energy. And it is re-writing the rules of the global order, with caution because it has been a giant beneficiary of it, while the US is still wondering about what it wants from China. Beijing has been preparing for Cold War without eagerness for waging it, while the US wants to wage a Cold War without preparing for it.11&lt;/p&gt;
    &lt;p&gt;So here’s a potential way that China succeeds. Beijing’s goal is to make nearly every important product in the world, while everyone else supplies its commodities and services. By making the country mostly self-sufficient, and by vigorously policing the outputs of LLMs and social media, Xi might hope to make China resilient. He is building Fortress China stone by stone in order to outlast the adversary. Beijing doesn’t have to replicate American diplomatic, cultural, and financial superpowerdom. It might hope that its prowess in advanced manufacturing might deter the US. And its success in manufacturing might directly destabilize the US: by delivering the coup de grace to the rustbelt, the US might shed a few million more manufacturing jobs over the next decade. The job losses combined with AI psychosis, social media, and all the problems with phones could make national politics meaningfully worse.&lt;/p&gt;
    &lt;p&gt;I don’t think this scenario is likely to be successful. Authoritarian systems have always hoped for the implosion of liberal democracies, while it is the liberal democracies that have a better track record of endurance. But I also don’t think that authoritarian countries are obviously wrong to bet that western polarization will get worse. So it’s up to the US and Europe to show that they can hold on to their values while absorbing the technological changes coming their way.&lt;/p&gt;
    &lt;p&gt;That task is more challenging as Europe and the US grew more apart in 2025. This year, both regions were able to look upon each other with pity. And both were correct to do so. America’s global trust and favorability measures have collapsed in Trump’s second term. Meanwhile, Europe looks as economically stuck as it has ever been, pushing its politics to increasingly chaotic extremes. But I am still more optimistic for the US.&lt;/p&gt;
    &lt;p&gt;I don’t need to lament the damage done by the Trump administration this year: the erosion of alliances, the cruelty towards the weak, the wasting of time. Manufacturing and re-industrialization, which I spend most of my time thinking of, have been doing worse. The Biden administration tried to fund an ambitious program of industrial policy; but it was so plodding and proceduralist that it built little before voters re-elected Trump. Since Trump imposed tariffs in April, the US has lost around 65,000 manufacturing jobs.12 His administration shows little interest in capturing electromagnetism before China overruns that field. Trump is more interested in protectionism rather than export promotion, which risks turning American industries into fossils like its exquisitely protected and horribly inefficient shipbuilding industry.&lt;/p&gt;
    &lt;p&gt;One of the Trump administration’s biggest blunders was its decision to raid a battery plant in Georgia, which put 300 Korean engineers in chains before deporting them. I suspect that any Korean, Taiwanese, or European engineer would ponder that episode before accepting a job posting to the United States. What a contrast that looks with China’s approach, which for decades has been to welcome managers from Walmart, Apple, or Tesla to train its workforce.&lt;/p&gt;
    &lt;p&gt;Will the US solve manufacturing with AI? Well, maybe, because superintelligence is supposed to solve everything. But there’s a risk that AI will destabilize society before it fixes the industrial base. When I walk around the library at Stanford, I see students plugging everything into AI tools; when they need a break, they’re watching short-form videos on their phones. These videos have been marvelously transformed by AI tools. Shortly after OpenAI released Sora 2, I had brunch with a friend who told me that he created an AI video of himself expertly breakdancing that fooled his five-year-old; another friend piped up to say that she created an AI video of herself that fooled her mother. AI chatbots are skilled at providing emotional companionship: Jasmine Sun discussed how they are able to seduce any segment of society, while pointing to a survey that 52 percent of teens regularly interact with AI companions. I’m not advocating for regulation. But I think it’s reasonable for the world to hope that AI labs will exercise some degree of forbearance before they release their shattering tools.&lt;/p&gt;
    &lt;p&gt;While I feel apprehensive about the US, I am much more gloomy about Europe. I have a hard time squaring the poor prospects of Europe over the next decade with the smugness that Europeans have for themselves. I spent most of the summer in Copenhagen. There’s no doubt that quality of life in most European cities is superb, especially for what I care about: food, opera, walkable streets, access to nature. But a decade of low economic growth is biting. European prices and taxes can be so high while salaries can be so low. For all the American complaints about home affordability, relative housing costs can be even worse in big European cities. London has the house prices of California and the income levels of Mississippi.&lt;/p&gt;
    &lt;p&gt;I remember two vivid episodes from Copenhagen. One day I read the news that the share price of Novo Nordisk — unquestionably one of Europe’s technological successes, along with ASML — collapsed as a result of sustained competition from US-based Eli Lilly as well as its misfortunes navigating the US regulatory system. I also watched Ursula von der Leyen visit Trump in the White House to graciously accept his EU tariffs. It’s already been clear that China has begun to maul European industry. What the Novo Nordisk news made me appreciate was that American companies are comprehensively outworking their European counterparts in biotech in addition to software and finance. Europe is losing the two-front battle against the Chinese on manufacturing and the Americans on services.&lt;/p&gt;
    &lt;p&gt;Perhaps Europe could have recruited some professors from the United States. American academics wouldn’t have needed Trump’s insults to act on their Europhile impulses. And yet European initiatives have not yet been able to brain drain much of this class. That’s mostly because European governments have little funding to offer. European universities have failed to build substantial endowments, so their revenues are dependent on the taxpaying public, which also must support a million other initiatives. An American academic who wants to move to Europe would have to accept more teaching and administrative work, lose tenure, and for the pleasure of all that, probably halve her pay. She would likely also suffer the resentment of European peers, who scoff at the idea that better paid Americans are now refugees. Trump threw a lot against US universities; they are holding up okay, and I think they will remain strong.&lt;/p&gt;
    &lt;p&gt;Europeans are right to gloat they are not under the rule of Trump. But for all of Trump’s ills, I see him as a sign of the underlying dynamism of the US. Who else would have elected so whimsical a leader to this high office? Trump forces questions that Europeans have no appetite to confront, proud as they are in being superior to both Americans and Chinese. I submit that Europeans ought to be more circumspect in their self-satisfaction. Chaos is only one election away. Right-populist parties are outpolling ruling incumbent parties pretty much everywhere, and it is as likely as not that Trumps with European characteristics will engulf the continent by the end of the decade.&lt;/p&gt;
    &lt;p&gt;So I am betting that the US and China are more compelling forces for change. Stalin was fond of telling a story from his experience in Leipzig in 1907, when, to his astonishment, 200 German workers failed to turn up to a socialist meeting because no ticket controller was on the platform to punch their train tickets, citing this experience as proof of the hopelessness of Germanic obedience. Could anyone imagine Chinese or Americans being so obedient? One advantage for the US and China is that both countries are at least interested in growth. You don’t have to convince the elites or the populace that growth is good or that entrepreneurs could be celebrated. Meanwhile in Europe, perhaps 15 percent of the electorate actively believes in degrowth. I feel it’s impossible to convince Europeans to act in their self interest. You can’t even convince them to adopt air conditioning in the summer.&lt;/p&gt;
    &lt;p&gt;The personal is the geopolitical&lt;/p&gt;
    &lt;p&gt;I’m not a doomer on AI or the broader state of the world. Across the US, China, and Europe, people generally enjoy comfortable lives that are free from fear. The market goes up. AI tools improve. Over the years I lived in China, I knew that life was more mundane than the headlines made out. Now that headlines and tweets are more negative everywhere, I know that things are not so bad in most places.&lt;/p&gt;
    &lt;p&gt;What I want is for everyone to do better. I opened my book by saying that Chinese and Americans are the most alike people in the world. They both are driven by a yearning for the future. They feel the draw of better times ahead, which is missing for Europeans, those people who have a sense of optimism only about the past.&lt;/p&gt;
    &lt;p&gt;I believe that modern China is one of the most ahistorical nations in the world. The state and the education system may talk insistently about its thousands of years of continuous history. But no other society has also been so destructive of its own history. The physical past has been disfigured by the attention of the Red Guards and the inattention of urban bulldozers. The social past is contorted by outrageous textbooks, which implement enforced forgetting of major traumas. For tragedies too widely experienced in modern times to be censored — the Cultural Revolution, the one-child policy, Zero Covid — the party discourages reflection in the name of protecting the state’s sensitivity.&lt;/p&gt;
    &lt;p&gt;The United States isn’t so good at celebrating its history either. 2026 is the 250th anniversary of the country’s founding. Where are the monuments to exalt that history? Most of the planned celebrations look small bore. Why hasn’t the federal government built a technological specimen as sublime as the Golden Gate Bridge, the Hoover Dam, or the Apollo missions? Probably because planning for any project should have commenced 10, 20, or 30 years ago. No president would have gotten around to starting a project that has no chance of being completed in his term. Lack of action due to the expectation of long timelines is one of the sins of the lawyerly society.&lt;/p&gt;
    &lt;p&gt;But American problems seem more fixable to me than Chinese problems. That’s why I live here in the US. I made clear in my book that I am drawn to pluralism as well as a broader conception of human flourishing than one that could be delivered by the Communist Party. The United States still draws many of the most ambitious people in the world, few of whom want to move to China. Even now a significant number of Chinese would jump to emigrate to the US if they felt they could be welcomed. But this enduring American advantage should not excuse the US from patching up its deficiencies.&lt;/p&gt;
    &lt;p&gt;A light grab-bag of complaints: While the rich have access to concierge doctors and the world’s best healthcare, the United States cannot organize a pandemic response; it is bioprosperity for the individual and measles for the many. I learned recently that the Bay Area has 26 separate transit agencies; is it really a triumph of democracy to have so many unconsolidated efforts? I wonder whether we can accuse the California government of subverting the will of the people by making so little progress on its high-speed rail, which was approved by referendum in 2008; California rail authorities take more pride in creating jobs than doing the job. I am tempted to use the language from American foreign policy at home. Why talk about American credibility only in terms of combat? Why shouldn’t the failure to deliver on big projects, after spending so much money, constitute a more severe blow to the credibility of the American project? Is the state of the US defense industrial base really deterring adversaries?&lt;/p&gt;
    &lt;p&gt;I won’t belabor issues with American public works or manufacturing. I’ll suggest only that the US ought to be acting with greater curiosity on how to do better. It doesn’t have to become China; but it should better study China’s successes. There is a 21st century playbook for becoming an industrial power and China has written it. This playbook consists of infrastructure development, solicitation of foreign investment, industrial subsidies, and the creation of industrial ecosystems. I hope that the US will stop attributing all of China’s successes to stealing. If such a program would be sufficient for building a world-class industry, then American spooks should dedicate their formidable capabilities to extracting Chinese industrial secrets. The reality is that there is little to be learned from blueprints. By failing to recognize China’s real strengths — the industrial ecosystems pulsating with process knowledge — the US is only cheating itself.&lt;/p&gt;
    &lt;p&gt;The future of US-China competition demands a resounding demonstration of the superiority of one country’s system to perform better for its citizens, which no country has thus achieved. Who’s going to come out ahead? I believe the competition is dynamic. It means we should not rely on static and structural features (like geography or demographics) to predict long-term advantage. One feature that unites American, Chinese, and European elites is the tendency to close ranks behind bad ideas and bad leaders. They are all skilled at dreaming up new ways to squander their advantages. Silicon Valley, for example, succeeds in spite of the generations-long governance failures of California. Imagine how much more vibrant Chinese society could be if it could escape the weight of overbearing censors in Beijing.&lt;/p&gt;
    &lt;p&gt;Competition will be dynamic because people have agency. The country that is ahead at any given moment will commit mistakes driven by overconfidence, while the country that is behind will feel the crack of the whip to reform. Implosion is always an option. In 2021, Xi Jinping was on top of the world, witnessing the omnishambles of the western pandemic response combined with the political disgrace of January 6. So he proceeded to smack around tech founders and initiate a controlled demolition of the property sector, which are two of the policies most responsible for China’s economic sluggishness today. Now, Beijing is trying to get a grip on its weaknesses. If either the US or China falls too far behind the other, the laggard will sweat to catch up. That drive will mean that competition will go on for years and decades.&lt;/p&gt;
    &lt;p&gt;In the competition for who might grow to be more humorous, I give a slight edge to the Chinese rather than to Silicon Valley.&lt;/p&gt;
    &lt;p&gt;No, I don’t expect the Communist Party ever to be funny. But there is a growing contrast between the baleful formality of the political system and the inexhaustible informality of Chinese society. Now that China is bidding farewell to its era of hypergrowth, young people are asking what they want to do with their lives. Fewer of them are interested in doing crazy hours in tech companies or big banks. Some of them are having fun in comedy sketches and stand-up shows. The increasingly gerontocratic Communist Party is not so much hovering over them as existing on a slightly different plane, speaking in strange apocalyptic tongues. Over the long run, I bet that the exuberance and rollicking nature of Chinese society will outlive the lusterless political system.&lt;/p&gt;
    &lt;p&gt;I wish that the tech world could learn to present broader cultural appeal. I hope that Silicon Valley could learn some of the humorousness of New York (or at least LA.) It’s unfortunate that any show or movie made about Silicon Valley is full of awkward nerds; by contrast, Hollywood reliably finds attractive leads when it makes movies about Wall Street. So long as the tech world is talking about the Machine God and the Antichrist, so long as it declines to read more broadly, so long as it is mostly inward looking, it will continue to alienate big parts of the world. But the longer I’m in California, the more easy I find it to be a sunny optimist. So I’m hopeful that the lovable nerds there will be able to present their own smiling optimism to the rest of the world.&lt;/p&gt;
    &lt;p&gt;I thank a number of people for reading a draft of this section and discussing the core ideas with me.&lt;/p&gt;
    &lt;p&gt;***&lt;/p&gt;
    &lt;p&gt;Of all the feedback I’ve received for my book, the most devastating came from my mother. After one of my television appearances, she called me to say: “Son, you looked terrible. Are you sick?” I accept that she, a former TV news anchor, has standing to judge. Still I could only reply with a quavering voice: “Mom, you’re so mean.”13&lt;/p&gt;
    &lt;p&gt;Other readers have been kinder to Breakneck. It reached #3 on the New York Times bestseller list and was also a bestseller on its monthly Business list. I went on podcasts, radio, TV, and spoke at book events. Breakneck was a finalist for the FT/Schroders best business book of the year and it has been a book of the year in several big publications. It’s being translated into 17 languages as of this writing.&lt;/p&gt;
    &lt;p&gt;I’ve learned a lot over the past four months.&lt;/p&gt;
    &lt;p&gt;Why did Breakneck do well? I think four reasons, in descending order of importance. First, timing. It came out in a year of many China headlines — DeepSeek, trade war, 15th Five-Year Plan — and five months after Abundance, which primed readers for the idea that Americans are right to be frustrated by their state. Second, the book had the memetic framing of lawyers and engineers, which also encouraged people to wonder how other countries could be described. (What is India? The UK?) Third, people know my work through these letters. Fourth and least important was the content in the book. An author spends so much time workshopping words and sentences. I accept that a book’s reception is subject to the vagaries of the market and the memelords.&lt;/p&gt;
    &lt;p&gt;I don’t regret a minute of workshopping. I would have liked to workshop some more. Like every author, I wish I had more time to add a finer polish to the entire manuscript. I was heartened when a writer I admire told me that no author is ever more than 85 percent satisfied with their work; to hope for more would be profligate. In any case, I’m proud of the content. If it weren’t in place, I wouldn’t have had positive reviews in mainstream publications like the Financial Times, the Wall Street Journal, the New Yorker, and the Times. I was glad to see praise from both left publications like Jacobin and right publications like American Affairs.&lt;/p&gt;
    &lt;p&gt;I tried to write this book to reach a non-coast audience. Ideally I wanted a lawyer in say Indiana or Ohio to read Breakneck, rather than for it to be picked up only by folks in New York, DC, San Francisco, and the terminally online. So I was happy to hear from a broader cross-section of readers who wrote to tell me that they’d never visited China before and are now curious to do so. It’s a shame that book tours are no longer much of a thing for authors. Publishers don’t necessarily bring authors to book readings in Houston, Los Angeles, New Orleans, or other big cities as a matter of course. I was happy, however, to visit Dallas for the first time this year. After giving a talk in October, I wandered over to the Texas State Fair. Who can resist a place that calls itself “the most Texan place on earth?” I had a fabulous time walking through the fairground, the livestock pens, and the food stalls. The atmosphere made me realize that friendly and pragmatic Texans are what I imagined all Americans to be like, at least in my Canadian mind.&lt;/p&gt;
    &lt;p&gt;I’ve enjoyed opening my inbox to see reader notes. I love hearing from two groups in particular: engineers and other technical people who feel better appreciated for their work; and Chinese readers who tell me that I’ve captured something authentic. Someone emailed a set of book recommendations for the Spanish Civil War. An investor emailed to enlighten me that Copenhagen’s marvelous subways (which I praise for being clean and driverless) were built by Italian construction companies. An agricultural consultant emailed to tell me about her eye-opening experiences visiting big Chinese farms. These notes are small delights for any author. A stranger but still charming event was to see the Blue Book Club. About 20 people gathered in Brooklyn this November to discuss Breakneck, but not before the hosts issued a light exam to make sure that the participants actually read the book.&lt;/p&gt;
    &lt;p&gt;Book promotion made me more of a public figure. I did my best to have fun with it. It wasn’t as hard as I imagined: podcast and TV hosts are as bored by self-serious personalities as the rest of us are. Readers have been friendly as they’ve recognized me in public. There was only one instance of a bit too much friendliness, when someone sidled up to the urinal beside mine in a public bathroom to tell me that he liked my book.&lt;/p&gt;
    &lt;p&gt;I’ve learned it is not possible to value mentors too highly. I am blessed to have good counselors. I mean not only my publishing house, my literary agent, and my writing coach who directly support my work. I am grateful to folks who give me time to reflect on the course of my thinking, especially the ones who have by now mentored me for over a decade. Friends have been generous in all sorts of ways. Eugene, Tina, Maran, Ren, James, Caleb, Alec, and Arthur hosted book parties. Joe Weisenthal wrote in the Odd Lots newsletter: “Total Dan Wang victory” on his view that most of the world is seeing China through the industrial lens I’ve been writing about. Afra hosted a Mandarin-language book discussion in which someone accused me of having a “gentle and vulnerable” voice. Alice, who doesn’t often pick up books on China, told me that my fondness for both the US and China shone through the book. It reconnected me with two friends from Ottawa that I haven’t heard from since high school.&lt;/p&gt;
    &lt;p&gt;I am grateful that Waterstones Piccadilly and Daunt Books in Marylebone have given my book prominent display. One surprise was that my book sold well in the United Kingdom. I’ve been pretty relentless at telling Brits that they are the PPE society and that they excel in the sounding-clever industries — television, journalism, finance, and universities. Upon reflection, it makes sense that the British are reading Breakneck and Abundance. Every problem in the lawyerly society is worse in the UK. I thought that California’s high-speed rail project was an embarrassment; then I learned about the Leeds tram network. First legislated in 1993, mass transit might not come to West Yorkshire until the late 2030s. It reminds me of the lawsuit in Bleak House: “The little plaintiff or defendant who was promised a new rocking-horse when Jarndyce and Jarndyce should be settled has grown up, possessed himself of a real horse, and trotted away into the other world.” At least Californians are struggling over something mighty; I hope that Leeds will one day have a tram.14&lt;/p&gt;
    &lt;p&gt;Homebuilding in London has collapsed. Heathrow has been making plans to build a third runway for twenty years, which is now expected to cost $20 billion. Britain’s electrical network is in even worse disrepair than America’s. I am not sure if it is a geopolitical asset to be able to stiff-upper-lip one’s way through ineffectual government. Maybe it’s more of a liability. But my experience of criticizing Brits resembles my experience of criticizing lawyers. They tend to nod along to my critiques; many of them take me further than where I’d like to go. It’s all very disarming.&lt;/p&gt;
    &lt;p&gt;I’ve been lucky to have smart critics. It’s any author’s dream to see people pick up the book and examine the arguments. Jon Sine wanted to have more specific data on engineers and lawyers, then proceeded to supply it while wrapping it in a narrative on a trip to Wushan. Charles Yang noted that I don’t have much by way of policy suggestions, but he also grasped that I’m trying to change the culture of governing elites while suggesting that Breakneck is an incitement to initiate “tractable mimetic competition.” Jen-Kuan Wang argued that China was not quite the right model for the US, but that Taiwan and the rest of Northeast Asia better show how to survive China Shocks. I am grateful to see constructive engagement with my work. I was unimpressed with only one piece of commentary. Law professors Curtis Milhaupt and Angela Zhang wrote in Project Syndicate: “Lawless State Capitalism Is No Answer to China’s Rise,” as if I were advocating for that. Since the authors mention the book only at the start without engaging with any of the content, I suspect they are critics who chose not to read the book.&lt;/p&gt;
    &lt;p&gt;I learned of Leo Rosten’s quip that it is the weak who are cruel, and gentleness to be expected only from the strong. Every author will hear from online commentators who belligerently misunderstand their work. Saying anything about China tends to rile up the online commentators. Either the hawks will pounce because they believe that the whole country is evil and that its progress is fake; or the tankies will defend the idea that China has achieved socialist utopia. These people live on Twitter and Youtube, offering the stock comment that “this person knows nothing about China.” That’s of course hard to respond to because they offer no analytical content to rebut. Part of what makes the China discourse exasperating is that people have to choose sides all the time, which makes everyone dumber. At least I didn’t have it as bad as Ezra and Derek with Abundance.&lt;/p&gt;
    &lt;p&gt;I’ve learned more about myself as a writer this year. Namely, I like doing it. Writing a book is sometimes enough to make an author forswear the experience for a long time. Then there are the really perverse, for whom a taste of publishing is enough to tempt one into becoming a serial offender. After writing this book, I most looked forward to writing this long-ass letter, the very one you’re reading now.&lt;/p&gt;
    &lt;p&gt;Some writers work like sculptors: they produce something fully chiseled that could stand forever. Novelists tend to be like that. Rather than being a sculptor, I see myself as being a musician. After a performance, no matter how it goes, the musician’s task is to start practicing for the next one. It’s hard for US-China books to rest like sculptures. So I am happy to get back to work, writing iteratively to refine the same few themes that animate me: technology production, industrial ecosystems, US-China competition.&lt;/p&gt;
    &lt;p&gt;Musicians don’t usually practice by running a whole piece from start to finish. Rather, practice sessions tend to focus on particular passages, with a full run-through only before performance. Before I publish this letter, I retype the whole thing from start to finish. It means I take the draft that lives in my Notes app on the left half of my screen while I retype the whole thing into the Google Docs on the right side of my screen. It’s a final check to catch infelicities. More importantly, by simulating the experience of a reader, it’s another way to see if the whole essay stands together.&lt;/p&gt;
    &lt;p&gt;I’ve learned that it is better to wear a tie with a blazer. That was part of my training to be a speaker. The book tour forces you to have answers that last 30 seconds for TV, 30 minutes for a talk, and 3 hours for the more bruising podcasts. I’ve learned that delivering a good talk is a rare skill. I don’t think I could ever be satisfied by a talk I’ll give, because there will always be a stumble, or l’esprit de l’escalier kicks in. The piece of speaking advice I’ve remembered for many years came from Tim Harford: good speaking rewards those who are able to prepare extensively and who are also able to improvise. My favorite book talk took place at the Hoover Institution, hosted by Stephen Kotkin (who is himself peerless at giving excellent lectures). In the summer, I spent two hours asking Kotkin how historians work.&lt;/p&gt;
    &lt;p&gt;One day in October, I went on six podcasts. I haven’t counted the number of podcasts I’ve been on, but I think the number is north of 70. There’s a lot I don’t understand. Are so many people really listening to podcasts? What is the appeal of a video featuring two people with giant microphones in their faces? Do we really have to live in an oral culture world?&lt;/p&gt;
    &lt;p&gt;I’ve noticed the wide range of effort that people put into podcasts. Some hosts edit extensively — Freakonomics Radio stands out for the sheer number of producers and editors. Other hosts release their episodes more or less unedited. Freakonomics stood out to me because Stephen Dubner was able to make the conversation so much fun. Going on Ross Douthat’s Interesting Times was more appropriately serious. Search Engine was impressive for the amount of narrative that PJ Vogt imbued into our more rambling conversation. It felt like a homecoming to return to Odd Lots, where I could tease Tracy Alloway for her country life and Joe Weisenthal over Moby Dick. David Perell read nearly everything I’d written to discuss the writing process. I went on Francis Fukuyama’s podcast to ask him about his relationship with Wang Qishan as well as why he is now banned from China. Works in Progress, Statecraft, and ChinaTalk were each fun in their own way.&lt;/p&gt;
    &lt;p&gt;You don’t really mature into being on podcast mode until you’ve done a lot of them. That’s why I proposed to Tyler to go on his show near the end of the book tour. Conversations with Tyler is the first podcast I regularly started listening to, whose early episodes I still remember well. Before our interview, I told Tyler that he was my final boss. Both of us were playful. I challenged Tyler to enumerate the list of 12th-century popes and teased him about being a New Jersey suburban boy. He told me that America has great infrastructure and healthcare before issuing an intellectual Turing test to see if I could say why he likes Yunnan more than any other place. I had the chance to bring up one of the most sublime pieces of Rossini, the gently entwining trio that concludes Le Comte Ory. Afterwards, commentators wrote that he and I were confrontational. But they should have watched the video, in which Tyler was smiling as much as he ever would.&lt;/p&gt;
    &lt;p&gt;Again, who is listening to all these podcasts? I don’t much look at my book sales, but it doesn’t feel like podcasts move the needle. And a book might create a lot of social media buzz, with all the right people saying all the right things, but Twitter too doesn’t drive sales. It was two platforms that moved a lot of my books: television and radio. People bought after seeing me on CNN or hearing me on NPR. The straightforward explanation is that older people have the time and the money to buy books. Even a brief appearance on TV could reach an ambient audience of millions, a few of whom purchase afterwards. Social media and podcasts are more valuable for driving conversation among the youths.&lt;/p&gt;
    &lt;p&gt;It’s stirring to see that people buy books at all. I do not doubt that we are moving towards an oral culture. But the publishing industry is holding up. A lot of excellent books came out this year, including many on China. Revenues at most of the big trade publishers have been rising. Barnes &amp;amp; Noble is opening 60 new stores in 2026. A lot of the growth in the book trade is coming from romantasy and fairy smut, while the genre of nonfiction is in slight decline. That’s all good, I’m no snob. It’s pleasant to believe that a few decades from now, people might still hold physical books in their hands.&lt;/p&gt;
    &lt;p&gt;I’ve learned that books produce an invitation to all sorts of conversations, both closed and open. A physical book, bound and printed, has a totemic quality. It’s funny that PDFs sometimes circulate better than web-optimized pages; there’s something about strict formatting that establishes authority. Physical books can also last a long time. This letter that you’re reading will no longer be sent around a month from now, while my book can sit unread on shelves for years to gather dust. So I’m still keen to encourage friends to write their books. It’s a great way to sort through one’s ideas and to ease them into the conversation.&lt;/p&gt;
    &lt;p&gt;If I yearned for commercial success in our new oral culture, I would lend my soft voice to narrate romantasy novels. But I worry the superintelligence will devour that job. So I will stick to longform writing. However strange our new world will become, there will always be a class of people who want to engage with essays and books. Over the long term, writing might enjoy the fate of the opera and the symphony. People have been heralding the death of classical music for a century. Yes, much of its audience is pretty old. But there will always be more old people — especially if Silicon Valley delivers on longevity treatments. The job of authors and opera houses is to keep holding on to people who are maturing into pleasures that technological platforms cannot provide. The demographic trend is on our side: the world is producing more old people than youths. I want to be a sunny Californian optimist about everything, including the fate of the written word.&lt;/p&gt;
    &lt;p&gt;***&lt;/p&gt;
    &lt;p&gt;It’s time to talk about (other) books.&lt;/p&gt;
    &lt;p&gt;I last picked up Stendhal’s The Red and the Black a decade ago. I wasn’t certain that the novel, which I keep calling my favorite, would hold up on re-reading. It did gorgeously. The plot centers on Julien Sorel, the handsome son of a poor sawyer. After Julien dons the black garb of the priesthood, he moves from the periphery of his Alpine village into the luminous center of Parisian society. Along the way, he seduces two extraordinary women, the gentle Mme. de Rênal and the magnificent Mathilde, while he commits, in the name of love, acts of extraordinary stupidity. Julien — who is possessed by galloping ambition and extravagant pride — maneuvers his way towards aristocratic distinction and romantic triumph. Then he loses all.&lt;/p&gt;
    &lt;p&gt;More than anything else, Stendhal is funny, especially about love. Only Proust surpasses Stendhal at the skill of guiding the reader into the transports of intoxicating love, only to snap them out of it by skewering the foolishness of Julien or Mathilde. Stendhal doesn’t create the cool detachment that Flaubert or Fontane bring to their characters. Rather, he’s eager to envelop the reader into his passionate embrace. The list of writers who have succumbed to Stendhal includes Nietzsche, Beauvoir, Girard, Balzac, and Robert Alter, who, before he translated the Hebrew Bible, wrote an admiring biography of Stendhal titled A Lion for Love.&lt;/p&gt;
    &lt;p&gt;Why is it that reading Stendhal feels like making a discovery? Stendhal might be just on the cusp of the pantheon because his critics can’t get over the significance of his flaws while his fans cannot forget the delights of his peaks. In that sense, Stendhal is like Rossini. Neither produced a ripe and perfect work; I can’t help but feel some disappointment when I listen to Rossini, who couldn’t achieve the musical perfection of Mozart or the dramatic conviction of Verdi. And yet the peak moments of Stendhal and Rossini produce ecstatic joy. It’s no surprise that Stendhal and Rossini are both renowned for their ravenous appetites, nor that Stendhal wrote his own admiring biography of Rossini, filled with his characteristic amusing falsehoods. Erich Auerbach grasped the point that Stendhal ought to be appreciated for his peaks rather than his average. Stendhal has pride of place in Mimesis, as an author who fluctuated between “realistic candor in general and silly mystification in particulars,” and between “cold self-control, rapturous abandonment to sensual pleasures, and sentimental vaingloriousness.” In other words, Stendhal embodies the spirit of opera buffa in novel.&lt;/p&gt;
    &lt;p&gt;I am often drawn to Ecclesiastes. In Robert Alter’s hands, the gloomy prophet behind the book is named Qohelet, and though I value Alter’s translation, I favor a few of the more iconic lines from King James: “vanity of vanities, all is vanity” and “better to hear the rebuke of the wise than the song of fools.” Melancholy attracts me in any form, and isn’t Ecclesiastes the most melancholic book? The prophet makes small allowances for joy and celebration before hauling the reader back into the house of mourning. There is something deeply satisfying with reading out loud phrases like: “for in mere breath did it come, and into darkness it goes, and in darkness its name is covered.” Though King James is iconic, Robert Alter better conveys overall the literary power of the Hebrew Bible.&lt;/p&gt;
    &lt;p&gt;Marlen Haushofer’s The Wall is short and engrossing. It was deemed a “Cold War” novel by the German press when it was published in 1963. Little about it comes across as being geopolitical today. Rather, Haushofer has written a book about domesticity that manages to be gripping. The heroine spends her days milking her cow, minding her garden, and caring for her cat and dog while living in total isolation in the Alps. She would not survive if she lacked for any of the above. As Katherine Rundell once wrote, “It’s easier to trust a writer who writes great food: they are a person who has paid attention to the world.” Haushofer pays loving attention to the details of life. It never became boring to read about the narrator churning her butter, tending to her potato field, or chopping wood throughout the year.&lt;/p&gt;
    &lt;p&gt;After a man turns 30, he has to choose between specializing in the history of the Roman Empire or the World Wars. Within the latter, one tends to focus on the Pacific Theater, the Western Front, or the Eastern Front. For me, the last theater is the most interesting. No human effort approaches the gargantuan scale of Operation Barbarossa or the Soviet reply. The same fields, one world war earlier, produced other shocks. Nick Lloyd’s The Eastern Front covers the clashes between Imperial Germany and the Russian Empire as well as the Austro-Hungarians against the Italians and the Serbs. Whereas the western front was essentially static throughout the whole war, the east was characterized by the sort of maneuver warfare that most generals had expected to fight. It was the field of legendary confrontations like the Gorlice-Tarnow campaign, the Brusilov offensive, and the 37th Battle of the Isonzo.&lt;/p&gt;
    &lt;p&gt;One of the revelations of Lloyd’s book is how well the Germans fought and how poorly Austro-Hungary performed, ending the war by self-liquidating. Immediately after the war began, German military attachés had already begun to fret that “the major trouble with the Austro-Hungarian Army is currently its weakness in combat.” It became nearly comical how often the Kaiser had to intervene, in the latter half of the war, to stop Emperor Karl from surrendering to the Entente. Perhaps it shouldn’t be surprising that the fighting force of an army where the officers all spoke German and regiments spoke Czech or Croatian could not overwhelm the adversary. The eastern front had diplomatic scheming that was nearly as impressive as the battlefield breakthroughs. It was, after all, the political section of the German general staff that had the imaginative idea to ship Lenin from Switzerland to Russia in order to make revolution.&lt;/p&gt;
    &lt;p&gt;I’m looking for a book that has a clear focus on bigger questions: How did Hohenzollern Prussia outmaneuver Habsburg Austria? And how did they become such firm allies before the war? John Boyer’s Austria 1867-1955 offers parts of the answer, though not in a conceptually organized way. It’s a work of history written for specialists, which means that the narrative serves the footnotes rather than the other way around. Too much of the book is focused on how politicians grappled with each other. Still it yields many morsels. One difference between Austrian nobles and Prussian nobles was that the former did not view a military life as attractive — part of the reason that Austrians performed so badly in war. Austria’s partner was sometimes rooting for the adversary: “a large, successful Prussia was Hungary’s best guarantee that Austria would not gain a superior position to dominate the Hungarian elites.” And this insight feels like a good explanation of the attractiveness of Austrian Catholicism, which “combines a Jansenist, puritanical strain with exuberant baroque piety.” It’s the sort of exuberance that produced a Mozart, rather than more gloomy and ardent Spanish Catholicism that produced the Inquisition.&lt;/p&gt;
    &lt;p&gt;One lesson from the latter years of Austro-Hungary is a good reminder that periods of state decay often correspond with eras of cultural flowering. 1913: The Year Before the Storm presents a whimsical slice of Central Europe. Art historian Florian Illies collates fragments of leading figures month by month, diary-entry style. People were running into each other all the time. Duchamp, d’Annunzio, Debussy at the premiere of the Rite of Spring. Stalin potentially tipping his hat at Hitler, as both residents of Vienna were known to take evening strolls through the gardens of Schönbrunn. Matisse bringing flowers to Picasso while the latter was sick. Rainer Maria Rilke being moody at the seaside with Sidonie Nadherny while she was running off into the arms of Karl Kraus. The celebrated love affairs between Franz Kafka and Felice Bauer, Igor Stravinsky and Coco Chanel, Alma Mahler and Oskar Kokoschka, Alma Mahler with Walter Gropius, Alma Mahler with anyone, really. 1913 is the year that modernism was born; the continent began to shatter the following year.&lt;/p&gt;
    &lt;p&gt;Nan Z. Da’s The Chinese Tragedy of King Lear also has an experimental form. Da is a professor of literature at Johns Hopkins who emigrated from Hangzhou before she was 7. One half of the book is a literary analysis of Shakespeare; the other half of the book is the story of the chaos of Maoist society and her family’s personal experiences of it. The novelty is the weaving of family history with a classic piece of literature. Sometimes these transitions are jarring, perhaps deliberately so. Da has just barely begun musing about the reign of Goneril and Regan before she launches into an exposition: “A history — I am thirty nine years old. My parents left China for the United States at this age.” But I liked this effort to map Mao’s madness onto Lear’s delirium as well as analogizing Deng’s tenacity to Edgar’s determination to lay low. And it convinced me that Lear is the most Chinese of Shakespeare’s plays. It is the marriage of the eastern emphasis on pro forma ceremonies, excessive flattery, and empty speechifying with the western practice of elder abuse. I’d like to read more experimental books like this one.&lt;/p&gt;
    &lt;p&gt;Susannah Clarke’s Piranesi is a glittering jewel. The setting is a mysterious, magical house. The narrator is a radiantly earnest explorer who self-identifies as a “Beloved Child of the House.” His warm curiosity makes this book an adventurer’s diary. I liked the fantasy elements of the first half better than the second half of the book, which disenchanted some of the story, so maybe it’s better to stop halfway through. Afterwards, I read Clarke’s earlier book, Jonathan Strange &amp;amp; Mr Norrell. It’s enjoyable too, especially for its partisanship of Northern English identity, though the book as a whole is wooly. Susannah Clarke offers a good case study of how authors can think about their work over time: an overlong first book that took decades to craft, followed by a shorter and more glittering second work. I can’t wait to see what her third book will be like.&lt;/p&gt;
    &lt;p&gt;(The Neue Galerie’s exhibition this year on New Objectivity led me to the work of German painter Carl Grossberg. This 1925 work spoke to me. Credit: Wikimedia.)&lt;/p&gt;
    &lt;p&gt;I’ve learned that Christmas is a good time to write. Emails stop and all is calm. I submitted my manuscript this time last year in Vietnam. This year, my wife and I are writing from Bali. Tropical Asia makes for great writing retreats. We have lazy mornings that feature a swim and a big breakfast; then we spend the rest of the day writing before going out in the evening for some really spicy food.&lt;/p&gt;
    &lt;p&gt;A few food questions to wrap up:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Is Da Nang the most underrated food city in Asia? Yes, we all know about excellent eating spots in Penang, Tokyo, Yunnan, etc. But I hardly ever hear about Da Nang, which has several Michelin listed places. I am still dreaming about its chewy rice products, the grilled meats, the spice mixes, the seafood soups, the not-too-sweet desserts. It’s well-listed on Michelin guides, but I hardly hear about it. Da Nang is my submission for a food city that ought to be better recognized as a destination.&lt;/item&gt;
      &lt;item&gt;Over the summer in Europe, I found myself wondering why Copenhagen has such amazing baked goods. I think its croissants are even better than in Paris. Then I found myself wondering about the quality distribution of croissants throughout the continent. They are not so good in Spain and Italy. I believe that Italy and Spain have the best overall cuisine in Europe; but they have been less interested in producing excellent baked goods. Is it because they don’t have as good butter? But they still eat a lot of cheese. The US is getting better croissants in big cities, which once more makes me appreciate that America has excellence across many cuisines, though they tend to be scattered.&lt;/item&gt;
      &lt;item&gt;Every winter, I find myself craving vitamin-rich tropical fruits. I mean mostly passionfruit, mango, papaya, eggfruit, and of course durian. American groceries are stocking more rambutan and dragonfruit. I wonder if they could stock even more. It’s always mango season somewhere, for example, so is it possible to find better mangoes throughout the year? Is there a subscription package to receive regular shipments of passionfruit and mango? I realize the durian supply chain is highly complicated (apparently the fruit is pollinated mostly by bats), but still it would be nice to have the fruit occasionally. I realize that tariffs are hurting access to American essentials like coffee and bananas. But I hope that Americans can continue to demand better fruits.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Alex Boyd has translated the what he calls the Collected Jokes of Xi Jinping here. https://www.ramble.media/p/is-xi-jinping-funny↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most prominently on a 60 Minutes segment when Amodei said: “AI could wipe out half of all entry-level white-collar jobs and spike unemployment to 10% to 20% in the next one to five years.” https://www.cbsnews.com/news/anthropic-ceo-dario-amodei-warning-of-ai-potential-dangers-60-minutes-transcript/↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Eliezer’s 2008 post: “‘AI go FOOM.’ Just to be clear on the claim, “fast” means on a timescale of weeks or hours rather than years or decades; and “FOOM” means way the hell smarter than anything else around, capable of delivering in short time periods technological advancements that would take humans decades, probably including full-scale molecular nanotechnology” https://archive.ph/tNdrf↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gavin Leech discusses the diffusion of Chinese LLMs here: https://www.gleech.org/paper↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Matt Sheehan of Carnegie shows that only 10 percent of top AI researchers have left the US between 2019 to 2025. https://carnegieendowment.org/emissary/2025/12/china-ai-researchers-us-talent-pool↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;ChinaTalk produced an enlightening Socratic dialogue on whether Beijing is racing to build superintelligence. Conclusion: probably not. https://www.chinatalk.media/p/is-china-agi-pilled↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pavlo Zvenyhorodskyi and Scott Singer, also of Carnegie, have produced valuable work on embodied AI: https://carnegieendowment.org/research/2025/11/embodied-ai-china-smart-robots↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Calculations from Weijian Shan: “In 2024, Shanghai produced one million vehicles with 20,000 workers, while California produced 464,000 with 22,000 workers.” https://research.gavekal.com/article/unraveling-chinas-productivity-paradox↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;As Grothendieck wrote: “The sea advances insensibly and in silence, nothing seems to happen, nothing moves, the water is so far off you hardly hear it… yet it finally surrounds the resistant substance.” https://webusers.imj-prg.fr/~leila.schneps/grothendieckcircle/Mathbiographies/mclarty1.pdf↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;See Noah Smith for more on the electric tech stack: https://www.noahpinion.blog/p/why-every-country-needs-to-master↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ryan Fedasiuk wrote an excellent essay on the lack of a China strategy across US administrations: https://theamericanenterprise.com/in-search-of-a-china-strategy/↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;MANEMP on FRED: https://fred.stlouisfed.org/series/MANEMP↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You can watch my interview with Fareed Zakaria here: https://edition.cnn.com/2025/10/26/world/video/gps-1026-china-us-trade-showdown↩&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Thanks to Mike Bird for alerting me to the Leeds tram: https://x.com/Birdyword/status/2001570894171500775↩&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454413</guid><pubDate>Thu, 01 Jan 2026 14:32:12 +0000</pubDate></item><item><title>Python numbers every programmer should know</title><link>https://mkennedy.codes/posts/python-numbers-every-programmer-should-know/</link><description>&lt;doc fingerprint="24f9e910fc8082ca"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;There are numbers every Python programmer should know. For example, how fast or slow is it to add an item to a list in Python? What about opening a file? Is that less than a millisecond? Is there something that makes that slower than you might have guessed? If you have a performance sensitive algorithm, which data structure should you use? How much memory does a floating point number use? What about a single character or the empty string? How fast is FastAPI compared to Django?&lt;/p&gt;
      &lt;p&gt;I wanted to take a moment and write down performance numbers specifically focused on Python developers. Below you will find an extensive table of such values. They are grouped by category. And I provided a couple of graphs for the more significant analysis below the table.&lt;/p&gt;
      &lt;p&gt;Acknowledgements: Inspired by Latency Numbers Every Programmer Should Know and similar resources.&lt;/p&gt;
      &lt;head rend="h3"&gt;Source code for the benchmarks&lt;/head&gt;
      &lt;p&gt;This article is posted without any code. I encourage you to dig into the benchmarks. The code is available on GitHub at:&lt;/p&gt;
      &lt;p&gt;https://github.com/mikeckennedy/python-numbers-everyone-should-know&lt;/p&gt;
      &lt;p&gt;The benchmarks were run on the sytem described in this table. While yours may be faster or slower, the most important thing to consider is relative comparisons.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Property&lt;/cell&gt;
          &lt;cell role="head"&gt;Value&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Python Version&lt;/cell&gt;
          &lt;cell&gt;CPython 3.14.2&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Hardware&lt;/cell&gt;
          &lt;cell&gt;Mac Mini M4 Pro&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Platform&lt;/cell&gt;
          &lt;cell&gt;macOS Tahoe (26.2)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Processor&lt;/cell&gt;
          &lt;cell&gt;ARM&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;CPU Cores&lt;/cell&gt;
          &lt;cell&gt;14 physical / 14 logical&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;RAM&lt;/cell&gt;
          &lt;cell&gt;24 GB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Timestamp&lt;/cell&gt;
          &lt;cell&gt;2025-12-30&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;TL;DR; Python Numbers&lt;/head&gt;
      &lt;p&gt;This first version is a quick “pyramid” of growing time/size for common Python ops. There is much more detail below.&lt;/p&gt;
      &lt;p&gt;Python Operation Latency Numbers (the pyramid)&lt;/p&gt;
      &lt;quote&gt; Attribute read (obj.x) 14 ns Dict key lookup 22 ns 1.5x attr Function call (empty) 22 ns List append 29 ns 2x attr f-string formatting 65 ns 3x function Exception raised + caught 140 ns 10x attr orjson.dumps() complex object 310 ns 0.3 μs json.loads() simple object 714 ns 0.7 μs 2x orjson sum() 1,000 integers 1,900 ns 1.9 μs 3x json SQLite SELECT by primary key 3,600 ns 3.6 μs 5x json Iterate 1,000-item list 7,900 ns 7.9 μs 2x SQLite read Open and close file 9,100 ns 9.1 μs 2x SQLite read asyncio run_until_complete (empty) 28,000 ns 28 μs 3x file open Write 1KB file 35,000 ns 35 μs 4x file open MongoDB find_one() by _id 121,000 ns 121 μs 3x write 1KB SQLite INSERT (with commit) 192,000 ns 192 μs 5x write 1KB Write 1MB file 207,000 ns 207 μs 6x write 1KB import json 2,900,000 ns 2,900 μs 3 ms 15x write 1MB import asyncio 17,700,000 ns 17,700 μs 18 ms 6x import json import fastapi 104,000,000 ns 104,000 μs 104 ms 6x import asyncio &lt;/quote&gt;
      &lt;p&gt;Python Memory Numbers (the pyramid)&lt;/p&gt;
      &lt;quote&gt; Float 24 bytes Small int (cached 0-256) 28 bytes Empty string 41 bytes Empty list 56 bytes 2x int Empty dict 64 bytes 2x int Empty set 216 bytes 8x int __slots__ class (5 attrs) 212 bytes 8x int Regular class (5 attrs) 694 bytes 25x int List of 1,000 ints 36,056 bytes 36 KB Dict of 1,000 items 64,952 bytes 65 KB List of 1,000 __slots__ instances 81,000 bytes 81 KB List of 1,000 regular instances 169,000 bytes 169 KB 2x slots list Empty Python process 16,000,000 bytes 16 MB &lt;/quote&gt;
      &lt;head rend="h2"&gt;Python numbers you should know (detailed version)&lt;/head&gt;
      &lt;p&gt;Here is a deeper table comparing many more details.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Category&lt;/cell&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
          &lt;cell role="head"&gt;Memory&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;💾 Memory&lt;/cell&gt;
          &lt;cell&gt;Empty Python process&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;15.73 MB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty string&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;41 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;100-char string&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;141 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Small int (0-256)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Large int&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Float&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;24 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty list&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;56 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List with 1,000 ints&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;35.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List with 1,000 floats&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;32.1 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty dict&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;64 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict with 1,000 items&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;63.4 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty set&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;216 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set with 1,000 items&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;59.6 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Regular class instance (5 attrs)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;__slots__&lt;/code&gt; class instance (5 attrs)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;212 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 regular class instances&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;165.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 &lt;code&gt;__slots__&lt;/code&gt; class instances&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;79.1 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;dataclass instance&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;namedtuple instance&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;228 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;⚙️ Basic Ops&lt;/cell&gt;
          &lt;cell&gt;Add two integers&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Add two floats&lt;/cell&gt;
          &lt;cell&gt;18.4 ns (54.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;String concatenation (small)&lt;/cell&gt;
          &lt;cell&gt;39.1 ns (25.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;f-string formatting&lt;/cell&gt;
          &lt;cell&gt;64.9 ns (15.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;.format()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;103 ns (9.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;%&lt;/code&gt; formatting&lt;/cell&gt;
          &lt;cell&gt;89.8 ns (11.1M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List append&lt;/cell&gt;
          &lt;cell&gt;28.7 ns (34.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List comprehension (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;9.45 μs (105.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Equivalent for-loop (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;11.9 μs (83.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;📦 Collections&lt;/cell&gt;
          &lt;cell&gt;Dict lookup by key&lt;/cell&gt;
          &lt;cell&gt;21.9 ns (45.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set membership check&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List index access&lt;/cell&gt;
          &lt;cell&gt;17.6 ns (56.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List membership check (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;3.85 μs (259.6k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;len()&lt;/code&gt; on list&lt;/cell&gt;
          &lt;cell&gt;18.8 ns (53.3M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item list&lt;/cell&gt;
          &lt;cell&gt;7.87 μs (127.0k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item dict&lt;/cell&gt;
          &lt;cell&gt;8.74 μs (114.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;sum()&lt;/code&gt; of 1,000 ints&lt;/cell&gt;
          &lt;cell&gt;1.87 μs (534.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;🏷️ Attributes&lt;/cell&gt;
          &lt;cell&gt;Read from regular class&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write to regular class&lt;/cell&gt;
          &lt;cell&gt;15.7 ns (63.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read from &lt;code&gt;__slots__&lt;/code&gt; class&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write to &lt;code&gt;__slots__&lt;/code&gt; class&lt;/cell&gt;
          &lt;cell&gt;16.4 ns (60.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read from &lt;code&gt;@property&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;getattr()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;13.8 ns (72.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;hasattr()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;23.8 ns (41.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;📄 JSON&lt;/cell&gt;
          &lt;cell&gt;&lt;code&gt;json.dumps()&lt;/code&gt; (simple)&lt;/cell&gt;
          &lt;cell&gt;708 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.loads()&lt;/code&gt; (simple)&lt;/cell&gt;
          &lt;cell&gt;714 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.dumps()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;2.65 μs (376.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.loads()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;2.22 μs (449.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;orjson.dumps()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;310 ns (3.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;orjson.loads()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;839 ns (1.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;ujson.dumps()&lt;/code&gt; (complex)&lt;/cell&gt;
          &lt;cell&gt;1.64 μs (611.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;msgspec&lt;/code&gt; encode (complex)&lt;/cell&gt;
          &lt;cell&gt;445 ns (2.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Pydantic &lt;code&gt;model_dump_json()&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;1.54 μs (647.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Pydantic &lt;code&gt;model_validate_json()&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;2.99 μs (334.7k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;🌐 Web Frameworks&lt;/cell&gt;
          &lt;cell&gt;Flask (return JSON)&lt;/cell&gt;
          &lt;cell&gt;16.5 μs (60.7k req/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Django (return JSON)&lt;/cell&gt;
          &lt;cell&gt;18.1 μs (55.4k req/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;FastAPI (return JSON)&lt;/cell&gt;
          &lt;cell&gt;8.63 μs (115.9k req/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Starlette (return JSON)&lt;/cell&gt;
          &lt;cell&gt;8.01 μs (124.8k req/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Litestar (return JSON)&lt;/cell&gt;
          &lt;cell&gt;8.19 μs (122.1k req/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;📁 File I/O&lt;/cell&gt;
          &lt;cell&gt;Open and close file&lt;/cell&gt;
          &lt;cell&gt;9.05 μs (110.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read 1KB file&lt;/cell&gt;
          &lt;cell&gt;10.0 μs (99.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1KB file&lt;/cell&gt;
          &lt;cell&gt;35.1 μs (28.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1MB file&lt;/cell&gt;
          &lt;cell&gt;207 μs (4.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;pickle.dumps()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;1.30 μs (769.6k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;pickle.loads()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;1.44 μs (695.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;🗄️ Database&lt;/cell&gt;
          &lt;cell&gt;SQLite insert (JSON blob)&lt;/cell&gt;
          &lt;cell&gt;192 μs (5.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;SQLite select by PK&lt;/cell&gt;
          &lt;cell&gt;3.57 μs (280.3k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;SQLite update one field&lt;/cell&gt;
          &lt;cell&gt;5.22 μs (191.7k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;diskcache set&lt;/cell&gt;
          &lt;cell&gt;23.9 μs (41.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;diskcache get&lt;/cell&gt;
          &lt;cell&gt;4.25 μs (235.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;MongoDB insert_one&lt;/cell&gt;
          &lt;cell&gt;119 μs (8.4k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;MongoDB find_one by _id&lt;/cell&gt;
          &lt;cell&gt;121 μs (8.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;MongoDB find_one by nested field&lt;/cell&gt;
          &lt;cell&gt;124 μs (8.1k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;📞 Functions&lt;/cell&gt;
          &lt;cell&gt;Empty function call&lt;/cell&gt;
          &lt;cell&gt;22.4 ns (44.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Function with 5 args&lt;/cell&gt;
          &lt;cell&gt;24.0 ns (41.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Method call&lt;/cell&gt;
          &lt;cell&gt;23.3 ns (42.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Lambda call&lt;/cell&gt;
          &lt;cell&gt;19.7 ns (50.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;try/except (no exception)&lt;/cell&gt;
          &lt;cell&gt;21.5 ns (46.5M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;try/except (exception raised)&lt;/cell&gt;
          &lt;cell&gt;139 ns (7.2M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;isinstance()&lt;/code&gt; check&lt;/cell&gt;
          &lt;cell&gt;18.3 ns (54.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;⏱️ Async&lt;/cell&gt;
          &lt;cell&gt;Create coroutine object&lt;/cell&gt;
          &lt;cell&gt;47.0 ns (21.3M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;run_until_complete(empty)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;27.6 μs (36.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;asyncio.sleep(0)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;39.4 μs (25.4k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 10 coroutines&lt;/cell&gt;
          &lt;cell&gt;55.0 μs (18.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;create_task()&lt;/code&gt; + await&lt;/cell&gt;
          &lt;cell&gt;52.8 μs (18.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async with&lt;/code&gt; (context manager)&lt;/cell&gt;
          &lt;cell&gt;29.5 μs (33.9k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Memory Costs&lt;/head&gt;
      &lt;p&gt;Understanding how much memory different Python objects consume.&lt;/p&gt;
      &lt;head rend="h3"&gt;An empty Python process uses 15.73 MB&lt;/head&gt;
      &lt;head rend="h3"&gt;Strings&lt;/head&gt;
      &lt;p&gt;The rule of thumb for strings is the core string object takes 41 bytes. Each additional character is 1 byte.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;String&lt;/cell&gt;
          &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty string &lt;code&gt;""&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;41 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;1-char string &lt;code&gt;"a"&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;42 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;100-char string&lt;/cell&gt;
          &lt;cell&gt;141 bytes&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Numbers&lt;/head&gt;
      &lt;p&gt;Numbers are surprisingly large in Python. They have to derive from CPython’s &lt;code&gt;PyObject&lt;/code&gt; and are subject to reference counting for garabage collection, they exceed our typical mental model many of:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;2 bytes = short int&lt;/item&gt;
        &lt;item&gt;4 bytes = long int&lt;/item&gt;
        &lt;item&gt;etc.&lt;/item&gt;
      &lt;/list&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Type&lt;/cell&gt;
          &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Small int (0-256, cached)&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Large int (1000)&lt;/cell&gt;
          &lt;cell&gt;28 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Very large int (10**100)&lt;/cell&gt;
          &lt;cell&gt;72 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Float&lt;/cell&gt;
          &lt;cell&gt;24 bytes&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Collections&lt;/head&gt;
      &lt;p&gt;Collections are amazing in Python. Dynamically growing lists. Ultra high-perf dictionaries and sets. Here is the empty and “full” overhead of each.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Collection&lt;/cell&gt;
          &lt;cell role="head"&gt;Empty&lt;/cell&gt;
          &lt;cell role="head"&gt;1,000 items&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List (ints)&lt;/cell&gt;
          &lt;cell&gt;56 bytes&lt;/cell&gt;
          &lt;cell&gt;35.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List (floats)&lt;/cell&gt;
          &lt;cell&gt;56 bytes&lt;/cell&gt;
          &lt;cell&gt;32.1 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict&lt;/cell&gt;
          &lt;cell&gt;64 bytes&lt;/cell&gt;
          &lt;cell&gt;63.4 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set&lt;/cell&gt;
          &lt;cell&gt;216 bytes&lt;/cell&gt;
          &lt;cell&gt;59.6 KB&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Classes and Instances&lt;/head&gt;
      &lt;p&gt;Slots are an interesting addition to Python classes. They remove the entire concept of a &lt;code&gt;__dict__&lt;/code&gt; for tracking fields and other values. Even for a single instance, slots classes are significantly smaller (212 bytes vs 694 bytes for 5 attributes). If you are holding a large number of them in memory for a list or cache, the memory savings of a slots class becomes very dramatic - over 2x less memory usage. Luckily for most use-cases, just adding a slots entry saves a ton of memory with minimal effort.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Type&lt;/cell&gt;
          &lt;cell role="head"&gt;Empty&lt;/cell&gt;
          &lt;cell role="head"&gt;5 attributes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Regular class&lt;/cell&gt;
          &lt;cell&gt;344 bytes&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;__slots__&lt;/code&gt; class&lt;/cell&gt;
          &lt;cell&gt;32 bytes&lt;/cell&gt;
          &lt;cell&gt;212 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;dataclass&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;694 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;@dataclass(slots=True)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;212 bytes&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;namedtuple&lt;/cell&gt;
          &lt;cell&gt;—&lt;/cell&gt;
          &lt;cell&gt;228 bytes&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;Aggregate Memory Usage (1,000 instances):&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Type&lt;/cell&gt;
          &lt;cell role="head"&gt;Total Memory&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 regular class instances&lt;/cell&gt;
          &lt;cell&gt;165.2 KB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List of 1,000 &lt;code&gt;__slots__&lt;/code&gt; class instances&lt;/cell&gt;
          &lt;cell&gt;79.1 KB&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Basic Operations&lt;/head&gt;
      &lt;p&gt;The cost of fundamental Python operations: Way slower than C/C++/C# but still quite fast. I added a brief comparison to C# to the source repo.&lt;/p&gt;
      &lt;head rend="h3"&gt;Arithmetic&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Add two integers&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Add two floats&lt;/cell&gt;
          &lt;cell&gt;18.4 ns (54.4M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Multiply two integers&lt;/cell&gt;
          &lt;cell&gt;19.4 ns (51.6M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;String Operations&lt;/head&gt;
      &lt;p&gt;String operations in Python are fast as well. f-strings are the fastest formatting style, while even the slowest style is still measured in just nano-seconds.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Concatenation (&lt;code&gt;+&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;39.1 ns (25.6M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;f-string&lt;/cell&gt;
          &lt;cell&gt;64.9 ns (15.4M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;.format()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;103 ns (9.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;%&lt;/code&gt; formatting&lt;/cell&gt;
          &lt;cell&gt;89.8 ns (11.1M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;List Operations&lt;/head&gt;
      &lt;p&gt;List operations are very fast in Python. Adding a single item usually requires 28ns. Said another way, you can do 35M appends per second. This is unless the list has to expand using something like a doubling algorithm. You can see this in the ops/sec for 1,000 items.&lt;/p&gt;
      &lt;p&gt;Surprisingly, list comprehensions are 26% faster than the equivalent for loops with append statements.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;list.append()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;28.7 ns (34.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List comprehension (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;9.45 μs (105.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Equivalent for-loop (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;11.9 μs (83.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Collection Access and Iteration&lt;/head&gt;
      &lt;p&gt;How fast can you get data out of Python’s built-in collections? Here is a dramatic example of how much faster the correct data structure is. &lt;code&gt;item in set&lt;/code&gt; or &lt;code&gt;item in dict&lt;/code&gt; is 200x faster than &lt;code&gt;item in list&lt;/code&gt; for just 1,000 items!&lt;/p&gt;
      &lt;p&gt;The graph below is non-linear in the x-axis.&lt;/p&gt;
      &lt;head rend="h3"&gt;Access by Key/Index&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict lookup by key&lt;/cell&gt;
          &lt;cell&gt;21.9 ns (45.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set membership (&lt;code&gt;in&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List index access&lt;/cell&gt;
          &lt;cell&gt;17.6 ns (56.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List membership (&lt;code&gt;in&lt;/code&gt;, 1,000 items)&lt;/cell&gt;
          &lt;cell&gt;3.85 μs (259.6k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Length&lt;/head&gt;
      &lt;p&gt;&lt;code&gt;len()&lt;/code&gt; is very fast. Maybe we don’t have to optimize it out of the test condition on a while loop looping 100 times after all.&lt;/p&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Collection&lt;/cell&gt;
          &lt;cell role="head"&gt;&lt;code&gt;len()&lt;/code&gt; time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;List (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;18.8 ns (53.3M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Dict (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;17.6 ns (56.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Set (1,000 items)&lt;/cell&gt;
          &lt;cell&gt;18.0 ns (55.5M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Iteration&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item list&lt;/cell&gt;
          &lt;cell&gt;7.87 μs (127.0k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Iterate 1,000-item dict (keys)&lt;/cell&gt;
          &lt;cell&gt;8.74 μs (114.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;sum()&lt;/code&gt; of 1,000 integers&lt;/cell&gt;
          &lt;cell&gt;1.87 μs (534.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Class and Object Attributes&lt;/head&gt;
      &lt;p&gt;The cost of reading and writing attributes, and how &lt;code&gt;__slots__&lt;/code&gt; changes things. Slots saves over 2x the memory usage on large collections, with virtually identical attribute access speed.&lt;/p&gt;
      &lt;head rend="h3"&gt;Attribute Access&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Regular Class&lt;/cell&gt;
          &lt;cell role="head"&gt;&lt;code&gt;__slots__&lt;/code&gt; Class&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read attribute&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;14.1 ns (70.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write attribute&lt;/cell&gt;
          &lt;cell&gt;15.7 ns (63.6M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;16.4 ns (60.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Other Attribute Operations&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read &lt;code&gt;@property&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;19.0 ns (52.8M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;getattr(obj, 'attr')&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;13.8 ns (72.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;hasattr(obj, 'attr')&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;23.8 ns (41.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;JSON and Serialization&lt;/head&gt;
      &lt;p&gt;Comparing standard library JSON with optimized alternatives. &lt;code&gt;orjson&lt;/code&gt; handles more data types and is over 8x faster than standard lib &lt;code&gt;json&lt;/code&gt; for complex objects. Impressive!&lt;/p&gt;
      &lt;head rend="h3"&gt;Serialization (dumps)&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Library&lt;/cell&gt;
          &lt;cell role="head"&gt;Simple Object&lt;/cell&gt;
          &lt;cell role="head"&gt;Complex Object&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json&lt;/code&gt; (stdlib)&lt;/cell&gt;
          &lt;cell&gt;708 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;2.65 μs (376.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;orjson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;60.9 ns (16.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;310 ns (3.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;ujson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;264 ns (3.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;1.64 μs (611.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;msgspec&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;92.3 ns (10.8M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;445 ns (2.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Deserialization (loads)&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Library&lt;/cell&gt;
          &lt;cell role="head"&gt;Simple Object&lt;/cell&gt;
          &lt;cell role="head"&gt;Complex Object&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json&lt;/code&gt; (stdlib)&lt;/cell&gt;
          &lt;cell&gt;714 ns (1.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;2.22 μs (449.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;orjson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;106 ns (9.4M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;839 ns (1.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;ujson&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;268 ns (3.7M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;1.46 μs (682.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;msgspec&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;101 ns (9.9M ops/sec)&lt;/cell&gt;
          &lt;cell&gt;850 ns (1.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Pydantic&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;model_dump_json()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;1.54 μs (647.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;model_validate_json()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;2.99 μs (334.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;model_dump()&lt;/code&gt; (to dict)&lt;/cell&gt;
          &lt;cell&gt;1.71 μs (585.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;model_validate()&lt;/code&gt; (from dict)&lt;/cell&gt;
          &lt;cell&gt;2.30 μs (435.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Web Frameworks&lt;/head&gt;
      &lt;p&gt;Returning a simple JSON response. Benchmarked with &lt;code&gt;wrk&lt;/code&gt; against localhost running 4 works in Granian. Each framework returns the same JSON payload from a minimal endpoint. No database access or that sort of thing. This is just how much overhead/perf do we get from each framework itself. The code we write that runs within those view methods is largely the same.&lt;/p&gt;
      &lt;head rend="h3"&gt;Results&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Framework&lt;/cell&gt;
          &lt;cell role="head"&gt;Requests/sec&lt;/cell&gt;
          &lt;cell role="head"&gt;Latency (p99)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Flask&lt;/cell&gt;
          &lt;cell&gt;16.5 μs (60.7k req/sec)&lt;/cell&gt;
          &lt;cell&gt;20.85 ms (48.0 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Django&lt;/cell&gt;
          &lt;cell&gt;18.1 μs (55.4k req/sec)&lt;/cell&gt;
          &lt;cell&gt;170.3 ms (5.9 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;FastAPI&lt;/cell&gt;
          &lt;cell&gt;8.63 μs (115.9k req/sec)&lt;/cell&gt;
          &lt;cell&gt;1.530 ms (653.6 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Starlette&lt;/cell&gt;
          &lt;cell&gt;8.01 μs (124.8k req/sec)&lt;/cell&gt;
          &lt;cell&gt;930 μs (1.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Litestar&lt;/cell&gt;
          &lt;cell&gt;8.19 μs (122.1k req/sec)&lt;/cell&gt;
          &lt;cell&gt;1.010 ms (990.1 ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;File I/O&lt;/head&gt;
      &lt;p&gt;Reading and writing files of various sizes. Note that the graph is non-linear in y-axis.&lt;/p&gt;
      &lt;head rend="h3"&gt;Basic Operations&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Open and close (no read)&lt;/cell&gt;
          &lt;cell&gt;9.05 μs (110.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read 1KB file&lt;/cell&gt;
          &lt;cell&gt;10.0 μs (99.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read 1MB file&lt;/cell&gt;
          &lt;cell&gt;33.6 μs (29.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1KB file&lt;/cell&gt;
          &lt;cell&gt;35.1 μs (28.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write 1MB file&lt;/cell&gt;
          &lt;cell&gt;207 μs (4.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Pickle vs JSON to Disk&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;pickle.dumps()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;1.30 μs (769.6k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;pickle.loads()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;1.44 μs (695.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.dumps()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;2.72 μs (367.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;json.loads()&lt;/code&gt; (complex obj)&lt;/cell&gt;
          &lt;cell&gt;2.35 μs (425.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Database and Persistence&lt;/head&gt;
      &lt;p&gt;Comparing SQLite, diskcache, and MongoDB using the same complex object.&lt;/p&gt;
      &lt;head rend="h3"&gt;Test Object&lt;/head&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;user_data = {
    "id": 12345,
    "username": "alice_dev",
    "email": "alice@example.com",
    "profile": {
        "bio": "Software engineer who loves Python",
        "location": "Portland, OR",
        "website": "https://alice.dev",
        "joined": "2020-03-15T08:30:00Z"
    },
    "posts": [
        {"id": 1, "title": "First Post", "tags": ["python", "tutorial"], "views": 1520},
        {"id": 2, "title": "Second Post", "tags": ["rust", "wasm"], "views": 843},
        {"id": 3, "title": "Third Post", "tags": ["python", "async"], "views": 2341},
    ],
    "settings": {
        "theme": "dark",
        "notifications": True,
        "email_frequency": "weekly"
    }
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
      &lt;head rend="h3"&gt;SQLite (JSON blob approach)&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Insert one object&lt;/cell&gt;
          &lt;cell&gt;192 μs (5.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Select by primary key&lt;/cell&gt;
          &lt;cell&gt;3.57 μs (280.3k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Update one field&lt;/cell&gt;
          &lt;cell&gt;5.22 μs (191.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Delete&lt;/cell&gt;
          &lt;cell&gt;191 μs (5.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Select with &lt;code&gt;json_extract()&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;4.27 μs (234.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;diskcache&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;cache.set(key, obj)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;23.9 μs (41.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;cache.get(key)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;4.25 μs (235.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;cache.delete(key)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;51.9 μs (19.3k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Check key exists&lt;/cell&gt;
          &lt;cell&gt;1.91 μs (523.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;MongoDB&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;insert_one()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;119 μs (8.4k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;find_one()&lt;/code&gt; by &lt;code&gt;_id&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;121 μs (8.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;find_one()&lt;/code&gt; by nested field&lt;/cell&gt;
          &lt;cell&gt;124 μs (8.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;update_one()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;115 μs (8.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;delete_one()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;30.4 ns (32.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Comparison Table&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;SQLite&lt;/cell&gt;
          &lt;cell role="head"&gt;diskcache&lt;/cell&gt;
          &lt;cell role="head"&gt;MongoDB&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Write one object&lt;/cell&gt;
          &lt;cell&gt;192 μs (5.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;23.9 μs (41.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;119 μs (8.4k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read by key/id&lt;/cell&gt;
          &lt;cell&gt;3.57 μs (280.3k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;4.25 μs (235.5k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;121 μs (8.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Read by nested field&lt;/cell&gt;
          &lt;cell&gt;4.27 μs (234.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;N/A&lt;/cell&gt;
          &lt;cell&gt;124 μs (8.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Update one field&lt;/cell&gt;
          &lt;cell&gt;5.22 μs (191.7k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;23.9 μs (41.8k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;115 μs (8.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Delete&lt;/cell&gt;
          &lt;cell&gt;191 μs (5.2k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;51.9 μs (19.3k ops/sec)&lt;/cell&gt;
          &lt;cell&gt;30.4 ns (32.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;p&gt;Note: MongoDB is a victim of network access version in-process access.&lt;/p&gt;
      &lt;head rend="h2"&gt;Function and Call Overhead&lt;/head&gt;
      &lt;p&gt;The hidden cost of function calls, exceptions, and async.&lt;/p&gt;
      &lt;head rend="h3"&gt;Function Calls&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Empty function call&lt;/cell&gt;
          &lt;cell&gt;22.4 ns (44.6M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Function with 5 arguments&lt;/cell&gt;
          &lt;cell&gt;24.0 ns (41.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Method call on object&lt;/cell&gt;
          &lt;cell&gt;23.3 ns (42.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Lambda call&lt;/cell&gt;
          &lt;cell&gt;19.7 ns (50.9M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Built-in function (&lt;code&gt;len()&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;17.1 ns (58.4M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Exceptions&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;try/except&lt;/code&gt; (no exception raised)&lt;/cell&gt;
          &lt;cell&gt;21.5 ns (46.5M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;try/except&lt;/code&gt; (exception raised)&lt;/cell&gt;
          &lt;cell&gt;139 ns (7.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Type Checking&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;isinstance()&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;18.3 ns (54.7M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;type() == type&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;21.8 ns (46.0M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Async Overhead&lt;/head&gt;
      &lt;p&gt;The cost of async machinery.&lt;/p&gt;
      &lt;head rend="h3"&gt;Coroutine Creation&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Create coroutine object (no await)&lt;/cell&gt;
          &lt;cell&gt;47.0 ns (21.3M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Create coroutine (with return value)&lt;/cell&gt;
          &lt;cell&gt;45.3 ns (22.1M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Running Coroutines&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;run_until_complete(empty)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;27.6 μs (36.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;run_until_complete(return value)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;26.6 μs (37.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Run nested await&lt;/cell&gt;
          &lt;cell&gt;28.9 μs (34.6k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Run 3 sequential awaits&lt;/cell&gt;
          &lt;cell&gt;27.9 μs (35.8k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;asyncio.sleep()&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;
            &lt;code&gt;asyncio.sleep(0)&lt;/code&gt;
          &lt;/cell&gt;
          &lt;cell&gt;39.4 μs (25.4k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Coroutine with &lt;code&gt;sleep(0)&lt;/code&gt;&lt;/cell&gt;
          &lt;cell&gt;41.8 μs (23.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;asyncio.gather()&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 5 coroutines&lt;/cell&gt;
          &lt;cell&gt;49.7 μs (20.1k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 10 coroutines&lt;/cell&gt;
          &lt;cell&gt;55.0 μs (18.2k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;gather()&lt;/code&gt; 100 coroutines&lt;/cell&gt;
          &lt;cell&gt;155 μs (6.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Task Creation&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;create_task()&lt;/code&gt; + await&lt;/cell&gt;
          &lt;cell&gt;52.8 μs (18.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Create 10 tasks + gather&lt;/cell&gt;
          &lt;cell&gt;85.5 μs (11.7k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Async Context Managers &amp;amp; Iteration&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async with&lt;/code&gt; (context manager)&lt;/cell&gt;
          &lt;cell&gt;29.5 μs (33.9k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async for&lt;/code&gt; (5 items)&lt;/cell&gt;
          &lt;cell&gt;30.0 μs (33.3k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;&lt;code&gt;async for&lt;/code&gt; (100 items)&lt;/cell&gt;
          &lt;cell&gt;36.4 μs (27.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h3"&gt;Sync vs Async Comparison&lt;/head&gt;
      &lt;table&gt;
        &lt;row&gt;
          &lt;cell role="head"&gt;Operation&lt;/cell&gt;
          &lt;cell role="head"&gt;Time&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Sync function call&lt;/cell&gt;
          &lt;cell&gt;20.3 ns (49.2M ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
        &lt;row&gt;
          &lt;cell&gt;Async equivalent (&lt;code&gt;run_until_complete&lt;/code&gt;)&lt;/cell&gt;
          &lt;cell&gt;28.2 μs (35.5k ops/sec)&lt;/cell&gt;
        &lt;/row&gt;
      &lt;/table&gt;
      &lt;head rend="h2"&gt;Methodology&lt;/head&gt;
      &lt;head rend="h3"&gt;Benchmarking Approach&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;All benchmarks run multiple times and with warmup not timed&lt;/item&gt;
        &lt;item&gt;Timing uses &lt;code&gt;timeit&lt;/code&gt; or &lt;code&gt;perf_counter_ns&lt;/code&gt; as appropriate&lt;/item&gt;
        &lt;item&gt;Memory measured with &lt;code&gt;sys.getsizeof()&lt;/code&gt; and &lt;code&gt;tracemalloc&lt;/code&gt;&lt;/item&gt;
        &lt;item&gt;Results are median of N runs&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Environment&lt;/head&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;OS: macOS 26.2&lt;/item&gt;
        &lt;item&gt;Python: 3.14.2 (CPython)&lt;/item&gt;
        &lt;item&gt;CPU: ARM - 14 cores (14 logical)&lt;/item&gt;
        &lt;item&gt;RAM: 24.0 GB&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h3"&gt;Code Repository&lt;/head&gt;
      &lt;p&gt;All benchmark code available at: https://github.com/mkennedy/python-numbers-everyone-should-know&lt;/p&gt;
      &lt;head rend="h2"&gt;Key Takeaways&lt;/head&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;Memory overhead: Python objects have significant memory overhead - even an empty list is 56 bytes&lt;/item&gt;
        &lt;item&gt;Dict/set speed: Dictionary and set lookups are extremely fast (O(1) average case) compared to list membership checks (O(n))&lt;/item&gt;
        &lt;item&gt;JSON performance: Alternative JSON libraries like &lt;code&gt;orjson&lt;/code&gt; and &lt;code&gt;msgspec&lt;/code&gt; are 3-8x faster than stdlib &lt;code&gt;json&lt;/code&gt;&lt;/item&gt;
        &lt;item&gt;Async overhead: Creating and awaiting coroutines has measurable overhead - only use async when you need concurrency&lt;/item&gt;
        &lt;item&gt;&lt;code&gt;__slots__&lt;/code&gt; tradeoff: &lt;code&gt;__slots__&lt;/code&gt; saves significant memory (over 2x for collections) with virtually no performance impact&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Last updated: 2026-01-01&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454470</guid><pubDate>Thu, 01 Jan 2026 14:39:23 +0000</pubDate></item><item><title>Show HN: OpenWorkers – Self-hosted Cloudflare workers in Rust</title><link>https://openworkers.com/introducing-openworkers</link><description>&lt;doc fingerprint="ec56b9328dac36f3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing OpenWorkers&lt;/head&gt;
    &lt;p&gt;Self-hosted Cloudflare Workers in Rust&lt;/p&gt;
    &lt;p&gt;OpenWorkers is an open-source runtime for executing JavaScript in V8 isolates. It brings the Cloudflare Workers programming model to your own infrastructure.&lt;/p&gt;
    &lt;head rend="h2"&gt;What works today&lt;/head&gt;
    &lt;quote&gt;
      &lt;code&gt;export default { async fetch(request, env) { const data = await env.KV.get("key"); const rows = await env.DB.query( "SELECT * FROM users WHERE id = $1", [1] ); return Response.json({ data, rows }); } };&lt;/code&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Features&lt;/head&gt;
    &lt;head rend="h3"&gt;Bindings&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;• KV storage (get, put, delete, list)&lt;/item&gt;
      &lt;item&gt;• PostgreSQL database&lt;/item&gt;
      &lt;item&gt;• S3/R2-compatible storage&lt;/item&gt;
      &lt;item&gt;• Service bindings&lt;/item&gt;
      &lt;item&gt;• Environment variables &amp;amp; secrets&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Web APIs&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;• fetch, Request, Response&lt;/item&gt;
      &lt;item&gt;• ReadableStream&lt;/item&gt;
      &lt;item&gt;• crypto.subtle&lt;/item&gt;
      &lt;item&gt;• TextEncoder/Decoder, Blob&lt;/item&gt;
      &lt;item&gt;• setTimeout, AbortController&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;quote&gt;┌─────────────────┐ │ nginx (proxy) │ └────────┬────────┘ │ ┌───────────────┬────────┴──┬───────────────┐ │ │ │ │ │ │ │ │ ┌────────┸────────┐ ┌────┸────┐ ┌────┸────┐ ┌────────┸────────┐ │ dashboard │ │ api │ │ logs * │ │ runner (x3) * │ └─────────────────┘ └────┬────┘ └────┰────┘ └────────┰────────┘ │ │ │ │ │ │ ┌────────┸────────┐ │ ┌────────┸────────┐ │ postgate * │ └──────┥ nats │ └─────────────────┘ └────────┰────────┘ │ │ ┌─────────────────┐ ┌──────┴───────┐ * ─────┥ PostgreSQL │ │ scheduler * │ └─────────────────┘ └──────────────┘&lt;/quote&gt;
    &lt;quote&gt;+-------------+ | nginx proxy | +------+------+ | +-------+-------+-------+--------+ | | | | +--+--+ +--+--+ +--+---+ +----------+-+ | dash| | api | |logs *| | runner * x3| +-----+ +--+--+ +--+---+ +-----+------+ | | | +-----+----+ | +------+-----+ |postgate *| +----+ nats | +-----+----+ +------+-----+ | | +-----+------+ +------+-----+ *-| PostgreSQL | | scheduler *| +------------+ +------------+&lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;V8 Isolates: Sandboxing with CPU (100ms) and memory (128MB) limits per worker.&lt;/item&gt;
      &lt;item&gt;Cron Scheduling: Built-in support for 5 or 6-field cron syntax.&lt;/item&gt;
      &lt;item&gt;Compatibility: Cloudflare Workers syntax compatible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Self-hosting&lt;/head&gt;
    &lt;p&gt;Deployment is designed to be simple. A single PostgreSQL database and a single Docker Compose file is all you need.&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;git clone https://github.com/openworkers/openworkers-infra cd openworkers-infra &amp;amp;&amp;amp; cp .env.example .env docker compose up -d postgres # Run migrations, generate token docker compose up -d&lt;/code&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Why I built this&lt;/head&gt;
    &lt;p&gt;This project has been evolving for about 7 years. I started experimenting with vm2 for sandboxing JS, then Cloudflare launched Workers and I got hooked on the model. When Deno came out, I switched to deno-core and ran on that for two years. Recently, with Claude's help, I rewrote everything on top of rusty_v8 directly.&lt;/p&gt;
    &lt;p&gt;The goal has always been the same: run JavaScript on your own servers, with the same DX as Cloudflare Workers but without vendor lock-in.&lt;/p&gt;
    &lt;p&gt;Next up: Execution recording &amp;amp; replay for deterministic debugging.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454693</guid><pubDate>Thu, 01 Jan 2026 15:09:06 +0000</pubDate></item><item><title>50% of U.S. vinyl buyers don't own a record player</title><link>https://lightcapai.medium.com/the-great-return-from-digital-abundance-to-analog-meaning-cfda9e428752</link><description>&lt;doc fingerprint="88b5f50b0ebe85db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why Gen Z is Driving the Vinyl Record Boom?&lt;/head&gt;
    &lt;head rend="h2"&gt;The Great Return: From Digital Abundance to Analog Meaning&lt;/head&gt;
    &lt;p&gt;In the fall of 2025, I find myself living a paradox that defines my generation. Born at the peak of the digital revolution, I spent the last decade methodically reintroducing analog “friction” into my life. I was collecting vinyl records i can’t play, wandering car-free city streets, and entertaining offline social activities.&lt;/p&gt;
    &lt;p&gt;Younger people around me are leading an analog resurgence. Far from the stereotype that only older generations buy physical media, Gen Z has emerged as the vanguard of “analog seeking” behavior. According to Luminate’s 2023 entertainment report, Gen Z listeners are 27% more likely to purchase vinyl records than the average music consumer, despite having grown up in the era of Spotify. In fact, half of U.S. vinyl buyers today don’t even own a record player – they buy records as tangible tokens of fandom and identity&lt;/p&gt;
    &lt;p&gt;Gen Z is the most online generation in history, yet they report the highest levels of loneliness and digital fatigue. A Global Web Index study found that 32% of Gen Z have taken “digital detox” breaks from the internet (compared to only 19% of Baby Boomers).&lt;/p&gt;
    &lt;p&gt;In a world of deepfakes and infinite streams, an experience only feels “real” if it offers tactile resistance. From mechanical keyboards to Polaroid photographs, young adults are gravitating toward the touch and weight of things, subconsciously assuring themselves that what they encounter isn’t just an algorithmic mirage.&lt;/p&gt;
    &lt;p&gt;I myself periodically disconnect from my devices, experiencing first-hand what Korean-German philosopher Byung-Chul Han describes as the relief from “timeless time” – the frenetic, unanchored temporality of the digital world.&lt;/p&gt;
    &lt;p&gt;Gen Z’s odd affinity for the bygone (film cameras, vinyl, vintage clothes) is a reclamation of the sensory and social richness that earlier technological optimism promised to replace but ultimately could not.&lt;/p&gt;
    &lt;head rend="h2"&gt;Market Data: The Vinyl &amp;amp; Physical Media Revival (2020 – 2024)&lt;/head&gt;
    &lt;p&gt;If the cultural zeitgeist is tilting analog, the hard numbers tell the same story. Nowhere is this more evident than the music industry, which he calls the “canary in the coal mine” for shifts in media value. After a two-decade decline of physical formats, vinyl records have staged an extraordinary comeback in the 2020s – both in the United States and across Europe.&lt;/p&gt;
    &lt;p&gt;According to the Recording Industry Association of America (RIAA), U.S. vinyl record revenues grew 17% in 2022 to $1.2 billion, marking the 16th consecutive year of growth for the format. For the first time since 1987, vinyl albums outsold CDs in the U.S. in units – 41 million vinyl records vs 33 million CDs were sold in 2022.&lt;/p&gt;
    &lt;p&gt;Europe mirrors this trajectory. In the UK, vinyl LP purchases increased for the 16th straight year in 2023, surging 11.7% in units from the prior year. Nearly 5.9 million vinyl records were sold in Britain in 2023, the highest number in over three decades. Notably, the UK still sold about 11 million CDs in 2023 (about twice the units of vinyl), but CD sales are now approaching a plateau after years of decline. Germany, historically a stronghold of CD consumption, is seeing vinyl carve out a growing niche as well; industry data show German vinyl sales climbing by mid-single digits and vinyl now making up roughly 6% of total recorded music revenue in that market. And in a global context, the trend is undeniable: the International Federation of the Phonographic Industry (IFPI) reported global physical music sales grew 13.4% in 2023, a dramatic acceleration from just 3.8% growth the year prior.&lt;/p&gt;
    &lt;p&gt;Streaming (digital) remains dominant in absolute dollars, but its meteoric rise has leveled to single-digit annual growth in the saturated U.S. market. Vinyl, on the other hand, has grown from a novelty revival to a billion-dollar industry on its own. The compound annual growth rate (CAGR) of vinyl revenue from 2018 – 2022 was in the high teens, vastly outpacing the overall music market.&lt;/p&gt;
    &lt;p&gt;When music became an all-you-can-eat digital utility, its “Streaming Devaluation” set in – the marginal value of another song or stream approached zero. In response, value migrated to the scarce complementary good: physical presence and ownership. This is why concert ticket prices and vinyl album sales have surged even as per-stream payouts plummet.&lt;/p&gt;
    &lt;p&gt;the more the digital domain saturates us with instant, on-demand content, the more we prize the unique, unrepeatable or tangible experience.&lt;/p&gt;
    &lt;p&gt;“Experience Premium” –&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454944</guid><pubDate>Thu, 01 Jan 2026 15:45:25 +0000</pubDate></item><item><title>BYD Sells 4.6M Vehicles in 2025, Meets Revised Sales Goal</title><link>https://www.bloomberg.com/news/articles/2026-01-01/byd-sells-4-6-million-vehicles-in-2025-meets-revised-sales-goal</link><description>&lt;doc fingerprint="7d6c0fada4da52de"&gt;
  &lt;main&gt;
    &lt;p&gt;Hyperdrive&lt;/p&gt;
    &lt;head rend="h1"&gt;BYD Hits Sales Goal, Set to Topple Tesla as Biggest EV Maker&lt;/head&gt;
    &lt;p&gt;BYD Co. met its full-year sales target and likely surpassed Tesla Inc. to become the world’s largest electric-vehicle maker in 2025 — milestones overshadowed by a challenging outlook for the Chinese auto market in the year ahead.&lt;/p&gt;
    &lt;p&gt;The Chinese EV giant’s Hong Kong-listed shares rose on the first day of trading in the new year, gaining as much as 2.3%.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46454977</guid><pubDate>Thu, 01 Jan 2026 15:49:42 +0000</pubDate></item><item><title>Cameras and Lenses (2020)</title><link>https://ciechanow.ski/cameras-and-lenses/</link><description>&lt;doc fingerprint="368c0a750d314318"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Cameras and Lenses&lt;/head&gt;
    &lt;p&gt;Pictures have always been a meaningful part of the human experience. From the first cave drawings, to sketches and paintings, to modern photography, we’ve mastered the art of recording what we see.&lt;/p&gt;
    &lt;p&gt;Cameras and the lenses inside them may seem a little mystifying. In this blog post I’d like to explain not only how they work, but also how adjusting a few tunable parameters can produce fairly different results:&lt;/p&gt;
    &lt;p&gt;Over the course of this article we’ll build a simple camera from first principles. Our first steps will be very modest â we’ll simply try to take any picture. To do that we need to have a sensor capable of detecting and measuring light that shines onto it.&lt;/p&gt;
    &lt;head rend="h1"&gt;Recording Light&lt;/head&gt;
    &lt;p&gt;Before the dawn of the digital era, photographs were taken on a piece of film covered in crystals of silver halide. Those compounds are light-sensitive and when exposed to light they form a speck of metallic silver that can later be developed with further chemical processes.&lt;/p&gt;
    &lt;p&gt;For better or for worse, I’m not going to discuss analog devices â these days most cameras are digital. Before we continue the discussion relating to light we’ll use the classic trick of turning the illumination off. Don’t worry though, we’re not going to stay in darkness for too long.&lt;/p&gt;
    &lt;p&gt;The image sensor of a digital camera consists of a grid of photodetectors. AÂ photodetector converts photons into electric current that can be measured â the more photons hitting the detector the higher the signal.&lt;/p&gt;
    &lt;p&gt;In the demonstration below you can observe how photons fall onto the arrangement of detectors represented by small squares. After some processing, the value read by each detector is converted to the brightness of the resulting image pixels which you can see on the right side. I’m also symbolically showing which photosite was hit with a short highlight. The slider below controls the flow of time:&lt;/p&gt;
    &lt;p&gt;The longer the time of collection of photons the more of them are hitting the detectors and the brighter the resulting pixels in the image. When we don’t gather enough photons the image is underexposed, but if we allow the photon collection to run for too long the image will be overexposed.&lt;/p&gt;
    &lt;p&gt;While the photons have the “color” of their wavelength, the photodetectors don’t see that hue â they only measure the total intensity which results in a black and white image. To record the color information we need to separate the incoming photons into distinct groups. We can put tiny color filters on top of the detectors so that they will only accept, more or less, red, green, or blue light:&lt;/p&gt;
    &lt;p&gt;This color filter array can be arranged in many different formations. One of the simplest is a Bayer filter which uses one red, one blue, and two green filters arranged in a 2x2 grid:&lt;/p&gt;
    &lt;p&gt;A Bayer filter uses two green filters because light in green part of the spectrum heavily correlates with perceived brightness. If we now repeat this pattern across the entire sensor we’re able to collect color information. For the next demo we will also double the resolution to an astonishing 1 kilopixel arranged in a 32x32 grid:&lt;/p&gt;
    &lt;p&gt;Note that the individual sensors themselves still only see the intensity, and not the color, but knowing the arrangement of the filters we can recreate the colored intensity of each sensor, as shown on the right side of the simulation.&lt;/p&gt;
    &lt;p&gt;The final step of obtaining a normal image is called demosaicing. During demosaicing we want to reconstruct the full color information by filling in the gaps in the captured RGB values. One of the simplest way to do it is to just linearly interpolate the values between the existing neighbors. I’m not going to focus on the details of many other available demosaicing algorithms and I’ll just present the resulting image created by the process:&lt;/p&gt;
    &lt;p&gt;Notice that yet again the overall brightness of the image depends on the length of time for which we let the photons through. That duration is known as shutter speed or exposure time. For most of this presentation I will ignore the time component and we will simply assume that the shutter speed has been set just right so that the image is well exposed.&lt;/p&gt;
    &lt;p&gt;The examples we’ve discussed so far were very convenient â we were surrounded by complete darkness with the photons neatly hitting the pixels to form a coherent image. Unfortunately, we can’t count on the photon paths to be as favorable in real environments, so let’s see how the sensor performs in more realistic scenarios.&lt;/p&gt;
    &lt;p&gt;Over the course of this article we will be taking pictures of this simple scene. The almost white background of this website is also a part of the scenery â it represents a bright overcast sky. You can drag around the demo to see it from other directions:&lt;/p&gt;
    &lt;p&gt;Let’s try to see what sort of picture would be taken by a sensor that is placed near the objects without any enclosure. I’ll also significantly increase the sensor’s resolution to make the pixels of the final image align with the pixels of your display. In the demonstration below the left side represents a view of the scene with the small greenish sensor present, while the right one shows the taken picture:&lt;/p&gt;
    &lt;p&gt;This is not a mistake. As you can see, the obtained image doesn’t really resemble anything. To understand why this happens let’s first look at the light radiated from the scene.&lt;/p&gt;
    &lt;p&gt;If you had a chance to explore how surfaces reflect light, you may recall that most matte surfaces scatter the incoming light in every direction. While I’m only showing a few examples, every point on every surface of this scene reflects the photons it receives from the whiteish background light source all around itself:&lt;/p&gt;
    &lt;p&gt;The red sphere ends up radiating red light, the green sphere radiates green light, and the gray checkerboard floor reflects white light of lesser intensity. Most importantly, however, the light emitted from the background is also visible to the sensor.&lt;/p&gt;
    &lt;p&gt;The problem with our current approach to taking pictures is that every pixel of the sensor is exposed to the entire environment. Light radiated from every point of the scene and the white background hits every point of the sensor. In the simulation below you can witness how light from different directions hits one point on the surface of the sensor:&lt;/p&gt;
    &lt;p&gt;Clearly, to obtain a discernible image we have to limit the range of directions that affect a given pixel on the sensor. With that in mind, let’s put the sensor in a box that has a small hole in it. The first slider controls the diameter of the hole, while the second one controls the distance between the opening and the sensor:&lt;/p&gt;
    &lt;p&gt;While not shown here, the inner sides of the walls are all black so that no light is reflected inside the box. I also put the sensor on the back wall so that the light from the hole shines onto it. We’ve just built a pinhole camera, let’s see how it performs. Observe what happens to the taken image as we tweak the diameter of the hole with the first slider, or change the distance between the opening and the sensor with the second one:&lt;/p&gt;
    &lt;p&gt;There are so many interesting things happening here! The most pronounced effect is that the image is inverted. To understand why this happens let’s look at the schematic view of the scene that shows the light rays radiated from the objects, going through the hole, and hitting the sensor:&lt;/p&gt;
    &lt;p&gt;As you can see the rays cross over in the hole and the formed image is a horizontal and a vertical reflection of the actual scene. Those two flips end up forming a 180Â° rotation. Since rotated images aren’t convenient to look at, all cameras automatically rotate the image for presentation and for the rest of this article I will do so as well.&lt;/p&gt;
    &lt;p&gt;When we change the distance between the hole and the sensor the viewing angle changes drastically. If we trace the rays falling on the corner pixels of the sensor we can see that they define the extent of the visible section of the scene:&lt;/p&gt;
    &lt;p&gt;Rays of light coming from outside of that shape still go through the pinhole, but they land outside of the sensor and aren’t recorded. As the hole moves further away from the sensor, the angle, and thus the field of view visible to the sensor gets smaller. We can see this in a top-down view of the camera:&lt;/p&gt;
    &lt;p&gt;Coincidentally, this diagram also helps us explain two other effects. Firstly, in the photograph the red sphere looks almost as big as the green one, even though the scene view shows the latter is much larger. However, both spheres end up occupying roughly the same span on the sensor and their size in the picture is similar. It’s also worth noting that the spheres seem to grow when the field of view gets narrower because their light covers larger part of the sensor.&lt;/p&gt;
    &lt;p&gt;Secondly, notice that different pixels of the sensor have different distance and relative orientation to the hole. The pixels right in the center of the sensor see the pinhole straight on, but pixels positioned at an angle to the main axis see a distorted pinhole that is further away. The ellipse in the bottom right corner of the demonstration below shows how a pixel positioned at the blue point sees the pinhole:&lt;/p&gt;
    &lt;p&gt;This change in the visible area of the hole causes the darkening we see in the corners of the photograph. The value of the cosine of the angle I’ve marked with a yellow color is quite important as it contributes to the reduction of visible light in four different ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Two cosine factors from the increased distance to the hole, it’s essentially the inverse square law&lt;/item&gt;
      &lt;item&gt;A cosine factor from the side squeeze of the circular hole seen at an angle&lt;/item&gt;
      &lt;item&gt;A cosine factor from the relative tilt of the receptor&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These four factors conspire together to reduce the illumination by a factor of cos4(Î±) in what is known as cosine-fourth-power law, also described as natural vignetting.&lt;/p&gt;
    &lt;p&gt;Since we know the relative geometry of the camera and the opening we can correct for this effect by simply dividing by the falloff factor and from this point on I will make sure that the images don’t have darkened corners.&lt;/p&gt;
    &lt;p&gt;The final effect we can observe is that when the hole gets smaller the image gets sharper. Let’s see how the light radiated from two points of the scene ends up going through the camera depending on the diameter of the pinhole:&lt;/p&gt;
    &lt;p&gt;We can already see that larger hole size ends up creating a bigger spread on the sensor. Let’s see this situation up close on a simple grid of detecting cells. Notice what happens to the size of the final circle hitting the sensor as that diameter of the hole changes:&lt;/p&gt;
    &lt;p&gt;When the hole is small enough rays from the source only manage to hit one pixel on the sensor. However, at larger radii the light spreads onto other pixels and a tiny point in the scene is no longer represented by a single pixel causing the image to no longer be sharp.&lt;/p&gt;
    &lt;p&gt;It’s worth pointing out that sharpness is ultimately arbitrary â it depends on the size at which the final image is seen, viewing conditions, and visual acuity of the observer. The same photograph that looks sharp on a postage stamp may in fact be very blurry when seen on a big display.&lt;/p&gt;
    &lt;p&gt;By reducing the size of the cone of light we can make sure that the source light affects a limited number of pixels. Here, however, lays the problem. The sensor we’ve been using so far has been an idealized detector capable of flawless adjustment of its sensitivity to the lighting conditions. If we instead were to fix the sensor sensitivity adjustment, the captured image would look more like this:&lt;/p&gt;
    &lt;p&gt;As the relative size of the hole visible to the pixels of the sensor gets smaller, be it due to reduced diameter or increased distance, fewer photons hit the surface and the image gets dimmer.&lt;/p&gt;
    &lt;p&gt;To increase the number of photons we capture we could extend the duration of collection, but increasing the exposure time comes with its own problems â if the photographed object moves or the camera isn’t held steady we risk introducing some motion blur.&lt;/p&gt;
    &lt;p&gt;Alternatively, we could increase the sensitivity of the sensor which is described using the ISO rating. However, boosting the ISO may introduce a higher level of noise. Even with these problems solved an actual image obtained by smaller and smaller holes would actually start getting blurry again due to diffraction effects of light.&lt;/p&gt;
    &lt;p&gt;If you recall how diffuse surfaces reflect light you may also realize how incredibly inefficient a pinhole camera is. A single point on the surface of an object radiates light into its surrounding hemisphere, however, the pinhole captures only a tiny portion of that light.&lt;/p&gt;
    &lt;p&gt;More importantly, however, a pinhole camera gives us minimal artistic control over which parts of the picture are blurry. In the demonstration below you can witness how changing which object is in focus heavily affects what is the primary target of attention of the photograph:&lt;/p&gt;
    &lt;p&gt;Let’s try to build an optical device that would solve both of these problems: we want to find a way to harness a bigger part of the energy radiated by the objects and also control what is blurry and how blurry it is. For the objects in the scene that are supposed to be sharp we want to collect a big chunk of their light and make it converge to the smallest possible point. In essence, we’re looking for an instrument that will do something like this:&lt;/p&gt;
    &lt;p&gt;We could then put the sensor at the focus point and obtain a sharp image. Naturally, the contraption we’ll try to create has to be transparent so that the light can pass through it and get to the sensor, so let’s begin the investigation by looking at a piece of glass.&lt;/p&gt;
    &lt;head rend="h1"&gt;Glass&lt;/head&gt;
    &lt;p&gt;In the demonstration below I put a red stick behind a pane of glass. You can adjust the thickness of this pane with the gray slider below:&lt;/p&gt;
    &lt;p&gt;When you look at the stick through the surface of a thick glass straight on, everything looks normal. However, as your viewing direction changes the stick seen through the glass seems out of place. The thicker the glass and the steeper the viewing angle the bigger the offset.&lt;/p&gt;
    &lt;p&gt;Let’s focus on one point on the surface of the stick and see how the rays of light radiated from its surface propagate through the subsection of the glass. The red slider controls the position of the source and the gray slider controls the thickness. You can drag the demo around to see it from different viewpoints:&lt;/p&gt;
    &lt;p&gt;For some reason the rays passing through glass at an angle are deflected off their paths. The change of direction happens whenever the ray enters or leaves the glass.&lt;/p&gt;
    &lt;p&gt;To understand why the light changes direction we have to peek under the covers of classical electromagnetism and talk a bit more about waves.&lt;/p&gt;
    &lt;head rend="h1"&gt;Waves&lt;/head&gt;
    &lt;p&gt;It’s impossible to talk about wave propagation without involving the time component, so the simulations in this section are animated â you can play and pause them by clickingtapping on the button in their bottom left corner.&lt;/p&gt;
    &lt;p&gt;By default all animations are enabled, but if you find them distracting, or if you want to save power, you can globally pause all the following demonstrations.disabled, but if you’d prefer to have things moving as you read you can globally unpause them and see all the waves oscillating.&lt;/p&gt;
    &lt;p&gt;Let’s begin by introducing the simplest sinusoidal wave:&lt;/p&gt;
    &lt;p&gt;A wave like this can be characterized by two components. Wavelength Î» is the distance over which the shape of the wave repeats. Period T defines how much time a full cycle takes.&lt;/p&gt;
    &lt;p&gt;Frequency f, is just a reciprocal of period and it’s more commonly used â it defines how many waves per second have passed over some fixed point. Wavelength and frequency define phase velocity vp which describes how quickly a point on a wave, e.g. a peak, moves:&lt;/p&gt;
    &lt;p&gt;The sinusoidal wave is the building block of a polarized electromagnetic plane wave. As the name implies electromagnetic radiation is an interplay of oscillations of electric field E and magnetic field B:&lt;/p&gt;
    &lt;p&gt;In an electromagnetic wave the magnetic field is tied to the electric field so I’m going to hide the former and just visualize the latter. Observe what happens to the electric component of the field as it passes through a block of glass. I need to note that dimensions of wavelengths are not to scale:&lt;/p&gt;
    &lt;p&gt;Notice that the wave remains continuous at the boundary and inside the glass the frequency of the passing wave remains constant, However, the wavelength and thus the phase velocity are reduced â you can see it clearly from the side.&lt;/p&gt;
    &lt;p&gt;The microscopic reason for the phase velocity change is quite complicated, but it can be quantified using the index of refraction n, which is the ratio of the speed of light c to the phase velocity vp of lightwave in that medium:&lt;/p&gt;
    &lt;p&gt;The higher the index of refraction the slower light propagates through the medium. In the table below I’ve presented a few different indices of refraction for some materials:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;vacuum&lt;/cell&gt;
        &lt;cell&gt;1.00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;air&lt;/cell&gt;
        &lt;cell&gt;1.0003&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;water&lt;/cell&gt;
        &lt;cell&gt;1.33&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;glass&lt;/cell&gt;
        &lt;cell&gt;1.53&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;diamond&lt;/cell&gt;
        &lt;cell&gt;2.43&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Light traveling through air barely slows down, but in a diamond it’s over twice as slow. Now that we understand how index of refraction affects the wavelength in the glass, let’s see what happens when we change the direction of the incoming wave:&lt;/p&gt;
    &lt;p&gt;The wave in the glass has a shorter wavelength, but it still has to match the positions of its peaks and valleys across the boundary. As such, the direction of propagation must change to ensure that continuity.&lt;/p&gt;
    &lt;p&gt;I need to note that the previous two demonstrations presented a two dimensional wave since that allowed me to show the sinusoidal component oscillating into the third dimension. In real world the lightwaves are three dimensional and I can’t really visualize the sinusoidal component without using the fourth dimension which has its own set of complications.&lt;/p&gt;
    &lt;p&gt;The alternative way of presenting waves is to use wavefronts. Wavefronts connect the points of the same phase of the wave, e.g. all the peaks or valleys. In two dimensions wavefronts are represented by lines:&lt;/p&gt;
    &lt;p&gt;In three dimensions the wavefronts are represented by surfaces. In the demonstration below a single source emits a spherical wave, points of the same phase in the wave are represented by the moving shells:&lt;/p&gt;
    &lt;p&gt;By drawing lines that are perpendicular to the surface of the wavefront we create the familiar rays. In this interpretation rays simply show the local direction of wave propagation which can be seen in this example of a section of a spherical 3D wave:&lt;/p&gt;
    &lt;p&gt;I will continue to use the ray analogy to quantify the change in direction of light passing through materials. The relation between the angle of incidence Î¸1 and angle of refraction Î¸2 can be formalized with the equation known as Snell’s law:&lt;/p&gt;
    &lt;p&gt;It describes how a ray of light changes direction relative to the surface normal on the border between two different media. Let’s see it in action:&lt;/p&gt;
    &lt;p&gt;When traveling from a less to more refractive material the ray bends towards the normal, but when the ray exits the object with higher index of refraction it bends away from the normal.&lt;/p&gt;
    &lt;p&gt;Notice that in some configurations the refracted ray completely disappears, however, this doesn’t paint a full picture because we’re currently completely ignoring reflections.&lt;/p&gt;
    &lt;p&gt;All transparent objects reflect some amount of light. You may have noticed that reflection on a surface of a calm lake or even on the other side of the glass demonstration at the beginning of the previous section. The intensity of that reflection depends on the index of refraction of the material and the angle of the incident ray. Here’s a more realistic demonstration of how light would get refracted and reflected between two media:&lt;/p&gt;
    &lt;p&gt;The relation between transmittance and reflectance is determined by Fresnel equations. Observe that the curious case of missing light that we saw previously no longer occurs â that light is actually reflected. The transition from partial reflection and refraction to the complete reflection is continuous, but near the end it’s very rapid and at some point the refraction completely disappears in the effect known as total internal reflection.&lt;/p&gt;
    &lt;p&gt;The critical angle at which the total internal reflection starts to happen depends on the indices of refraction of the boundary materials. Since that coefficient is low for air, but very high for diamond a proper cut of the faces makes diamonds very shiny.&lt;/p&gt;
    &lt;p&gt;While interesting on its own, reflection in glass isn’t very relevant to our discussion and for the rest of this article we’re not going to pay much attention to it. Instead, we’ll simply assume that the materials we’re using are covered with high quality anti-reflective coating.&lt;/p&gt;
    &lt;head rend="h1"&gt;Manipulating Rays&lt;/head&gt;
    &lt;p&gt;Let’s go back to the example that started the discussion of light and glass. When both sides of a piece of glass are parallel, the ray is shifted, but it still travels in the same direction. Observe what happens to the ray when we change the relative angle of the surfaces of the glass.&lt;/p&gt;
    &lt;p&gt;When we make two surfaces of the glass not parallel we gain the ability to change the direction of the rays. Recall, that we’re trying to make the rays hitting the optical device converge at a certain point. To do that we have to bend the rays in the upper part down and, conversely, bend the rays in the lower part up.&lt;/p&gt;
    &lt;p&gt;Let’s see what happens if we shape the glass to have different angles between its walls at different height. In the demonstration below you can control how many distinct segments a piece of glass is shaped to:&lt;/p&gt;
    &lt;p&gt;As the number of segments approaches infinity we end up with a continuous surface without any edges. If we look at the crossover point from the side you may notice that we’ve managed to converge the rays across one axis, but the top-down view reveals that we’re not done yet. To focus all the rays we need to replicate that smooth shape across all possible directions â we need rotational symmetry:&lt;/p&gt;
    &lt;p&gt;We’ve created a convex thin lens. This lens is idealized, in the later part of the article we’ll discuss how real lenses aren’t as perfect, but for now it will serve us very well. Let’s see what happens to the focus point when we change the position of the red source:&lt;/p&gt;
    &lt;p&gt;When the source is positioned very far away the incoming rays become parallel and after passing through lens they converge at a certain distance away from the center. That distance is known as focal length.&lt;/p&gt;
    &lt;p&gt;The previous demonstration also shows two more general distances: so which is the distance between the object, or source, and the lens, as well as si which is the distance between the image and the lens. These two values and the focal length f are related by the thin lens equation:&lt;/p&gt;
    &lt;p&gt;Focal length of a lens depends on both the index of refraction of the material from which the lens is made and its shape:&lt;/p&gt;
    &lt;p&gt;Now that we understand how a simple convex lens works we’re ready to mount it into the hole of our camera. We will still control the distance between the sensor and the lens, but instead of controlling the diameter of the lens we’ll instead control its focal length:&lt;/p&gt;
    &lt;p&gt;When you look at the lens from the side you may observe how the focal length change is tied to the shape of the lens. Let’s see how this new camera works in action:&lt;/p&gt;
    &lt;p&gt;Once again, a lot of things are going on here! Firstly, let’s try to understand how the image is formed in the first place. The demonstration below shows paths of rays from two separate points in the scene. After going through the lens they end up hitting the sensor:&lt;/p&gt;
    &lt;p&gt;Naturally, this process happens for every single point in the scene which creates the final image. Similarly to a pinhole a convex lens creates an inverted picture â I’m still correcting for this by showing you a rotated photograph.&lt;/p&gt;
    &lt;p&gt;Secondly, notice that the distance between the lens and the sensor still controls the field of view. As a reminder, the focal length of a lens simply defines the distance from the lens at which the rays coming from infinity converge. To achieve a sharp image, the sensor has to be placed at the location where the rays focus and that’s what’s causing the field of view to change.&lt;/p&gt;
    &lt;p&gt;In the demonstration below I’ve visualized how rays from a very far object focus through a lens of adjustable focal length, notice that to obtain a sharp image we must change the distance between the lens and the sensor which in turn causes the field of view to change:&lt;/p&gt;
    &lt;p&gt;If we want to change the object on which a camera with a lens of a fixed focal length is focused, we have to move the image plane closer or further away from the lens which affects the angle of view. This effect is called focus breathing:&lt;/p&gt;
    &lt;p&gt;A lens with a fixed focal length like the one above is often called a prime lens, while lenses with adjustable focal length are called zoom lenses. While the lenses in our eyes do dynamically adjust their focal lengths by changing their shape, rigid glass can’t do that so zoom lenses use a system of multiple glass elements that change their relative position to achieve this effect.&lt;/p&gt;
    &lt;p&gt;In the simulation above notice the difference in sharpness between the red and green spheres. To understand why this happens let’s analyze the rays emitted from two points on the surface of the spheres. In the demonstration below the right side shows the light seen by the sensor just from the two marked points on the spheres:&lt;/p&gt;
    &lt;p&gt;The light from the point in focus converges to a point, while the light from an out-of-focus point spreads onto a circle. For larger objects the multitude of overlapping out-of-focus circles creates a smooth blur called bokeh. With tiny and bright light sources that circle itself is often visible, you may have seen effects like the one in the demonstration below in some photographs captured in darker environments:&lt;/p&gt;
    &lt;p&gt;Notice that the circular shape is visible for lights both in front of and behind the focused distance. As the object is positioned closer or further away from the lens the image plane “slices” the cone of light at different location:&lt;/p&gt;
    &lt;p&gt;That circular spot is called a circle of confusion. While in many circumstances the blurriness of the background or the foreground looks very appealing, it would be very useful to control how much blur there is.&lt;/p&gt;
    &lt;p&gt;Unfortunately, we don’t have total freedom here â we still want the primary photographed object to remain in focus so its light has to converge to a point. We just want to change the size of the circle of out-of-focus objects without moving the central point. We can accomplish that by changing the angle of the cone of light:&lt;/p&gt;
    &lt;p&gt;There are two methods we can use to modify that angle. Firstly, we can change the focal length of the lens â you may recall that with longer focal lengths the cone of light also gets longer. However, changing the focal length and keeping the primary object in focus requires moving the image plane which in turn changes how the picture is framed.&lt;/p&gt;
    &lt;p&gt;The alternative way of reducing the angle of the cone of light is to simply ignore some of the “outer” rays. We can achieve that by introducing a stop with a hole in the path of light:&lt;/p&gt;
    &lt;p&gt;This hole is called an aperture. In fact, even the hole in which the lens is mounted is an aperture of some sort, but what we’re introducing is an adjustable aperture:&lt;/p&gt;
    &lt;p&gt;Let’s try to see how an aperture affects the photographs taken with our camera:&lt;/p&gt;
    &lt;p&gt;In real camera lenses an adjustable aperture is often constructed from a set of overlapping blades that constitute an iris. The movement of those blades changes the size of the aperture:&lt;/p&gt;
    &lt;p&gt;The shape of the aperture also defines the shape of bokeh. This is the reason why bokeh sometimes has a polygonal shape â it’s simply the shape of the “cone” of light after passing through the blades of the aperture. Next time you watch a movie pay a close attention to the shape of out-of-focus highlights, they’re often polygonal:&lt;/p&gt;
    &lt;p&gt;As the aperture diameter decreases, larger and larger areas of the photographed scene remain sharp. The term depth of field is used to define the length of the region over which the objects are acceptably sharp. When describing the depth of field we’re trying to conceptually demark those two boundary planes and see how far apart they are from each other.&lt;/p&gt;
    &lt;p&gt;Let’s see the depth of field in action. The black slider controls the aperture, the blue slider controls the focal length, and the red slider changes the position of the object relative to the camera. The green dot shows the place of perfect focus, while the dark blue dots show the limits, or the depth, of positions between which the image of the red light source will be reasonably sharp, as shown by a single outlined pixel on the sensor:&lt;/p&gt;
    &lt;p&gt;Notice that the larger the diameter of aperture and the shorter the focal length the shorter the distance between the dark blue dots and thus the shallower the depth of field becomes. If you recall our discussion of sharpness this demonstration should make it easier to understand why reducing the angle of the cone increases the depth of field.&lt;/p&gt;
    &lt;p&gt;If you don’t have perfect vision you may have noticed that squinting your eyes make you see things a little better. Your eyelids covering some part of your iris simply act as an aperture that decreases the angle of the cone of light falling into your eyes making things sightly less blurry on your retina.&lt;/p&gt;
    &lt;p&gt;An interesting observation is that aperture defines the diameter of the base of the captured cone of light that is emitted from the object. Twice as large aperture diameter captures roughly four times more light due to increased solid angle. In practice, the actual size of the aperture as seen from the point of view of the scene, or the entrance pupil, depends on all the lenses in front of it as the shaped glass may scale the perceived size of the aperture.&lt;/p&gt;
    &lt;p&gt;On the other hand, when a lens is focused correctly, the focal length defines how large a source object is in the picture. By doubling the focal length we double the width and the height of the object on the sensor thus increasing the area by the factor of four. The light from the source is more spread out and each individual pixel receives less light.&lt;/p&gt;
    &lt;p&gt;The total amount of light hitting each pixel is proportional to the ratio between the focal length f and the diameter of the entrance pupil D. This ratio is known as the f-number:&lt;/p&gt;
    &lt;p&gt;A lens with a focal length of 50 mm and the entrance pupil of 25 mm would have N equal to 2 and the f-number would be known as f/2. Since the amount of light getting to each pixel of the sensor increases with the diameter of the aperture and decreases with the focal length, the f-number controls the brightness of the projected image.&lt;/p&gt;
    &lt;p&gt;The f-number with which commercial lenses are marked usually defines the maximum aperture a lens can achieve and the smaller the f-number the more light the lens passes through. Bigger amount of incoming light allows reduction of exposure time, so the smaller the f-number the faster the lens is. By reducing the size of the aperture we can modify the f-number with which a picture is taken.&lt;/p&gt;
    &lt;p&gt;The f-numbers are often multiples of 1.4 which is an approximation of 2. Scaling the diameter of an adjustable aperture by 2 scales its area by 2 which is a convenient factor to use. Increasing the f-number by a so-called stop halves the amount of received light. The demonstration below shows the relatives sizes of the aperture through which light is being seen:&lt;/p&gt;
    &lt;p&gt;To maintain the overall brightness of the image when stopping down we’d have to either increase the exposure time or the sensitivity of the sensor.&lt;/p&gt;
    &lt;p&gt;While aperture settings let us easily control the depth of field, that change comes at a cost. When the f-number increases and the aperture diameter gets smaller we effectively start approaching a pinhole camera with all its related complications.&lt;/p&gt;
    &lt;p&gt;In the final part of this article we will discuss the entire spectrum of another class of problems that we’ve been conveniently avoiding all this time.&lt;/p&gt;
    &lt;head rend="h1"&gt;Aberrations&lt;/head&gt;
    &lt;p&gt;In our examples so far we’ve been using a perfect idealized lens that did exactly what we want and in all the demonstrations I’ve relied on a certain simplification known as the paraxial approximation. However, the physical world is a bit more complicated.&lt;/p&gt;
    &lt;p&gt;The most common types of lenses are spherical lenses â their curved surfaces are sections of spheres of different radii. These types of lenses are easier to manufacture, however, they actually don’t perfectly converge the rays of incoming light. In the demonstration below you can observe how fuzzy the focus point is for various lens radii:&lt;/p&gt;
    &lt;p&gt;This imperfection is known as spherical aberration. This specific flaw can be corrected with aspheric lenses, but unfortunately there are other types of problems that may not be easily solved by a single lens. In general, for monochromatic light there are five primary types of aberrations: spherical aberration, coma, astigmatism, field curvature, and distortion.&lt;/p&gt;
    &lt;p&gt;We’re still not out of the woods even if we manage to minimize these problems. In normal environments light is very non-monochromatic and nature sets another hurdle into optical system design. Let’s quickly go back to the dark environment as we’ll be discussing a single beam of white light.&lt;/p&gt;
    &lt;p&gt;Observe what happens to that beam when it hits a piece of glass. You can make the sides non-parallel by using the slider:&lt;/p&gt;
    &lt;p&gt;What we perceive as white light is a combination of lights of different wavelengths. In fact, the index of refraction of materials depends on the wavelength of the light. This phenomena called dispersion splits what seems to be a uniform beam of white light into a fan of color bands. The very same mechanism that we see here is also responsible for a rainbow.&lt;/p&gt;
    &lt;p&gt;In a lens this causes different wavelengths of light to focus at different offsets â the effect known as chromatic aberration. We can easily visualize the axial chromatic aberration even on a lens with spherical aberration fixed. I’ll only use red, green, and blue dispersed rays to make things less crowded, but remember that other colors of the spectrum are present in between. Using the slider you can control the amount of dispersion the lens material introduces:&lt;/p&gt;
    &lt;p&gt;Chromatic aberration may be corrected with an achromatic lens, usually in the form of a doublet with two different types of glass fused together.&lt;/p&gt;
    &lt;p&gt;To minimize the impact of the aberrations, camera lenses use more than one optical element on their pathways. In this article I’ve only shown you simple lens systems, but a high-end camera lens may consist of a lot of elements that were carefully designed to balance the optical performance, weight, and cost.&lt;/p&gt;
    &lt;p&gt;While we, in our world of computer simulations on this website, can maintain the illusion of simple and perfect systems devoid of aberrations, vignetting, and lens flares, real cameras and lenses have to deal with all these problems to make the final pictures look good.&lt;/p&gt;
    &lt;head rend="h1"&gt;Further Watching and Reading&lt;/head&gt;
    &lt;p&gt;Over on YouTube Filmmaker IQ channel has a lot of great content related to lenses and movie making. Two videos especially fitting here are The History and Science of Lenses and Focusing on Depth of Field and Lens Equivalents.&lt;/p&gt;
    &lt;p&gt;What Makes Cinema Lenses So Special!? on Potato Jet channel is a great interview with Art Adams from ARRI. The video goes over many interesting details of high-end cinema lens design, for example, how the lenses compensate for focus breathing, or how much attention is paid to the quality of bokeh.&lt;/p&gt;
    &lt;p&gt;For a deeper dive on bokeh itself Jakub TrÃ¡vnÃk’s On Bokeh is a great article on the subject. The author explains how aberrations may cause bokeh of non uniform intensity and shows many photographs of real cameras and lenses.&lt;/p&gt;
    &lt;p&gt;In this article I’ve mostly been using geometrical optics with some soft touches of electromagnetism. For a more modern look at the nature of light and its interaction with matter I recommend Richard Feynman’s QED: The Strange Theory of Light and Matter. The book is written in a very approachable style suited for general audience, but it still lets Feynman’s wits and brilliance shine right through.&lt;/p&gt;
    &lt;head rend="h1"&gt;Final Words&lt;/head&gt;
    &lt;p&gt;Weâve barely scratched the surface of optics and camera lens design, but even the most complex systems end up serving the same purpose: to tell light where to go. In some sense optical engineering is all about taming the nature of light.&lt;/p&gt;
    &lt;p&gt;The simple act of pressing the shutter button in a camera app on a smartphone or on the body of a high-end DSLR is effortless, but itâs at this moment when, through carefully guided rays hitting an array of photodetectors, we immortalize reality by painting with light.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46455872</guid><pubDate>Thu, 01 Jan 2026 17:18:01 +0000</pubDate></item><item><title>Finland detains ship and its crew after critical undersea cable damaged</title><link>https://www.cnn.com/2025/12/31/europe/finland-estonia-undersea-cable-ship-detained-intl</link><description>&lt;doc fingerprint="e406048e7b88547e"&gt;
  &lt;main&gt;
    &lt;p&gt;Finland has detained a ship and its crew after a critical undersea telecommunication cable connecting the country to Estonia was damaged Wednesday, Finnish authorities said.&lt;/p&gt;
    &lt;p&gt;Finnish police said in a statement that the vessel suspected of causing the damage was found with its anchor chain lowered into the sea in Finland’s waters, while the damage site itself was in Estonia’s waters. The police later named the vessel as the Fitburg, a Saint Vincent and the Grenadines flagged cargo ship.&lt;/p&gt;
    &lt;p&gt;The Finnish National Police Commissioner Ilkka Koskimäki said at a news conference on Wednesday afternoon that all 14 members of the ship’s crew have been detained, adding that the crew are citizens of Russia, Georgia, Kazakhstan and Azerbaijan.&lt;/p&gt;
    &lt;p&gt;Incidents like this have become more frequent in recent years, raising suspicions they are the result of sabotage and prompting NATO to launch a project earlier this year specifically aimed at strengthening the protection of critical undersea infrastructure.&lt;/p&gt;
    &lt;p&gt;According to MarineTraffic, which tracks ship movements, the Fitburg departed the Russian port of St. Petersburg on Tuesday and was headed to Haifa in Israel.&lt;/p&gt;
    &lt;p&gt;After the damage was reported, Finnish authorities instructed the ship to stop and raise its anchor, and then took control of it, the police said.&lt;/p&gt;
    &lt;p&gt;Finnish media reported that the ship was seized by special forces police and the coast guard from helicopters.&lt;/p&gt;
    &lt;p&gt;Finland’s President Alexander Stubb said that the government was monitoring the situation closely and that Finland was “prepared for security challenges of various kinds.”&lt;/p&gt;
    &lt;p&gt;The police said they were investigating the incident as aggravated criminal damage, attempted aggravated criminal damage, and aggravated interference with telecommunications.&lt;/p&gt;
    &lt;p&gt;The cable that was damaged runs between the Finnish capital Helsinki and the Estonian capital Talinn. While the extend of the damage was not immediately clear, the incident was serious enough to cause faults that were detected by the Finnish telecommunications provider Elisa, which operates the link.&lt;/p&gt;
    &lt;p&gt;Finnish Prime Minister Petteri Orpo said he spoke to his Estonian counterpart Kristen Michal about the situation, adding that the two countries were cooperating on the issue.&lt;/p&gt;
    &lt;p&gt;Estonia’s Ministry of Justice and Digital Affairs said in a statement that the country’s connections remained sufficiently backed up through other sea and land cables, ensuring the continuity of all services.&lt;/p&gt;
    &lt;p&gt;The ministry said that a second cable, owned by the Swedish company Arelion, was also damaged.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pattern of disruption&lt;/head&gt;
    &lt;p&gt;At least 10 undersea cables have been cut or damaged in the Baltic Sea since 2023. Some officials from Scandinavia, the Baltic states and the European Union have pointed the finger at Russia. They say the incidents appear to be part of what experts say is the Kremlin’s hybrid war on the West.&lt;/p&gt;
    &lt;p&gt;Russia has consistently denied involvement, but some of the ships that have caused damage to the undersea infrastructure in the past were found to have links to Russia.&lt;/p&gt;
    &lt;p&gt;Last year, a Baltic Sea power cable and several data cables were damaged after a Cook Islands-registered vessel dragged its anchor through the seabed for more than 50 miles.&lt;/p&gt;
    &lt;p&gt;Finnish and European officials said the ship, Eagle-S, was part of Russia’s shadow fleet of fuel tankers, and Finland later charged members of its crew. However, a court in Helsinki dismissed the case in October, saying Finland did not have jurisdiction over the issue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46456797</guid><pubDate>Thu, 01 Jan 2026 18:46:07 +0000</pubDate></item><item><title>Dell's version of the DGX Spark fixes pain points</title><link>https://www.jeffgeerling.com/blog/2025/dells-version-dgx-spark-fixes-pain-points</link><description>&lt;doc fingerprint="bdb351577a83619c"&gt;
  &lt;main&gt;
    &lt;p&gt;Dell sent me two of their GB10 mini workstations to test:&lt;/p&gt;
    &lt;p&gt;In this blog post, I'll cover the base system, just one of the two nodes. Cluster testing is ongoing, and I'll cover things like AI model training and networking more in depth next year, likely with comparisons to the Framework Desktop cluster and Mac Studio cluster I've also been testing.&lt;/p&gt;
    &lt;p&gt;But many of the same caveats of the DGX Spark (namely, price to performance is not great if you just want to run LLMs on a small desktop) apply to Dell's GB10 box as well.&lt;/p&gt;
    &lt;p&gt;It costs a little more than the DGX Spark, but does solve a couple pain points people experienced on the DGX Spark:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It has a power LED (seriously, why does the DGX Spark not have one?!)&lt;/item&gt;
      &lt;item&gt;The included power supply is 280W instead of 240W for a little more headroom&lt;/item&gt;
      &lt;item&gt;The thermal design (front-to-back airflow) seems less restricted, so is quieter and capable of keeping the GB10 'AI Superchip' from thermal throttling&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But if this isn't a mini PC to compete with a Mac mini, nor a good value for huge LLMs like a Mac Studio, or AMD's Ryzen AI Max+ 395 machines, what is it and who is it for?&lt;/p&gt;
    &lt;p&gt;Well, it's a $4,000+ box built specifically for developers in Nvidia's ecosystem, deploying code to Nvidia servers that cost half a million dollars each. A major part of the selling point are these built-in 200 gigabit QSFP ports, which would cost $1,500 or so to add on to another system, assuming you have the available PCIe bandwidth:&lt;/p&gt;
    &lt;p&gt;Those ports can't achieve 400 Gbps, but they do hit over 200 Gbps in the right conditions, configured for Infiniband / RDMA. And they hit over 100 Gbps for Ethernet (though only when running multiple TCP streams).&lt;/p&gt;
    &lt;p&gt;So it may seem a little bit of an odd duck for me, since I'm not an 'Nvidia developer' and I don't deploy code to Nvidia's 'AI factories'.&lt;/p&gt;
    &lt;p&gt;If I'm being honest, I'm more interested in the 'Grace' part of the GB10 (or 'Grace Blackwell 10') 'AI Superchip. It's a big.LITTLE Arm CPU co-designed by Mediatek, with 10 Cortex-X925 cores and 10 Cortex-A725 cores.&lt;/p&gt;
    &lt;p&gt;The chip is united to the Blackwell GPU, and shares the same 128 GB pool of LPDDR5X memory. And it's a pretty snappy Arm CPU—just stuck in a $4,000+ system.&lt;/p&gt;
    &lt;p&gt;But like I said, Dell sent me these boxes to test. They aren't paying for this blog post and have no control over what I say.&lt;/p&gt;
    &lt;p&gt;In fact, one of the main things they said was "this is isn't a gaming machine, so don't focus on that."&lt;/p&gt;
    &lt;p&gt;But that got me thinking. What if... I did.&lt;/p&gt;
    &lt;head rend="h2"&gt;Gaming on Arm Linux&lt;/head&gt;
    &lt;p&gt;Valve just announced the Steam Frame, and it runs on Arm.&lt;/p&gt;
    &lt;p&gt;Steam Frame will use FEX for its x86-Arm translation layer, and CodeWeavers' Crossover Preview for Arm64 was just released, so I thought I'd give that a try on DGX OS (Nvidia's Linux OS, currently based on Ubuntu 24.04).&lt;/p&gt;
    &lt;p&gt;I was able to quickly install Steam, and through that, games like Cyberpunk 2077, Doom Eternal, and Ultimate Epic Battle Simulator II.&lt;/p&gt;
    &lt;p&gt;I'll leave the full experience and test results for you to see in this video:&lt;/p&gt;
    &lt;p&gt;But bottom line, the Windows games I typically test on Arm systems through Steam/Proton played very well here, with no stuttering, and decent frame rates (100 fps in Cyberpunk 2077 at 1080p with low settings).&lt;/p&gt;
    &lt;p&gt;But no, I agree with Dell, this box should not be evaluated as a gaming machine. While it performs admirably for an Arm linux box, you could do a lot better with half the budget if you just wanted to build a dedicated gaming rig. Even with RAM prices as they are today.&lt;/p&gt;
    &lt;head rend="h2"&gt;General Purpose Arm Workstation (with tons of VRAM)&lt;/head&gt;
    &lt;p&gt;This machine is built for AI development, but it just so happens to have a very good Arm CPU and tons of RAM, so I wanted to test it for both running LLMs, and as a general Arm Linux workstation.&lt;/p&gt;
    &lt;p&gt;The video above has more depth, and you can find all my benchmark data here, but I wanted to focus on a few things in particular.&lt;/p&gt;
    &lt;head rend="h2"&gt;Software&lt;/head&gt;
    &lt;p&gt;Before we get to benchmarks, I wanted to mention Nvidia's DGX OS. Based on Ubuntu Linux, it's the only supported Linux distribution for GB10 systems. Regular Ubuntu LTS versions are supported for 5 years, with optional Pro support extending that out to 10 or even 15 years. But DGX OS only guarantees updates for two years, though Nvidia doesn't really offer guarantees for its hardware support.&lt;/p&gt;
    &lt;p&gt;Their track record for ongoing support for their hardware is decidedly mixed, and in the absence of any guarantees, I wouldn't expect them to continue supporting the Spark or other GB10 systems beyond a few years.&lt;/p&gt;
    &lt;p&gt;Some people have had luck getting other distros running, but they're still running Nvidia's kernel. So if you buy one of these, know there's no guarantees for ongoing support.&lt;/p&gt;
    &lt;p&gt;Running things on DGX OS, I've found most server/headless software runs great, but there are still desktop tools that are more of a hassle. Like Blender doesn't have a stable release that uses GPU acceleration on Arm. But if you compile it from sourcelike GitHub user CoconutMacaroon did, you can get full acceleration.&lt;/p&gt;
    &lt;p&gt;Just using this box as a little workstation, it is plenty fast for all the things I do, from coding, to browsing the web, to media editing. (Though media workflows are still rough on Linux in general, even on x86.)&lt;/p&gt;
    &lt;head rend="h2"&gt;CPU benchmarks&lt;/head&gt;
    &lt;p&gt;The Grace CPU is a 20-core Arm chip co-designed by Mediatek, fused together with the Blackwell GPU.&lt;/p&gt;
    &lt;p&gt;There must be some inefficiency there, though, because the system's idle power draw is a bit higher than I'm used to for Arm, coming in around 30 watts. A lot higher than Apple's M3 Ultra with 512GB of RAM, or even AMD's Ryzen AI Max+ 395 (these names just roll right off the tongue, don't they?).&lt;/p&gt;
    &lt;p&gt;In my testing, it seems the CPU itself maxes out around 140 watts, leaving another 140 watts of headroom for the GPU, network, and USB-C ports with PD.&lt;/p&gt;
    &lt;p&gt;Geekbench 6 was a little unstable, which was weird, but when I did get it to run, it was about on par with the AMD Ryzen AI Max+ 395 system I tested earlier this year, the Framework Desktop.&lt;/p&gt;
    &lt;p&gt;Apple's 2-generation-old M3 Ultra Mac studio beats both, but it does cost quite a bit more, so that's to be expected.&lt;/p&gt;
    &lt;p&gt;And testing with High Performance Linpack, the Dell Pro Max gets about 675 Gflops:&lt;/p&gt;
    &lt;p&gt;NVIDIA's marketing said the GB10 "offers a petaflop of AI computing performance"—a thousand teraflops! This thing can't even hit one...&lt;/p&gt;
    &lt;p&gt;But in the fine print, NVIDIA says it's a petaflop at FP4 precision. HPL tests FP64, aka double precision, which is more used in scientific computing. A FLOP is not always a FLOP, and even the 'petaflop' claim seems disputed, at least if I'm reading John Carmack's tweets correctly.&lt;/p&gt;
    &lt;p&gt;But at least for FP64 on the CPU, the GB10 is fairly efficient, at least compared to x86 systems I've tested:&lt;/p&gt;
    &lt;head rend="h2"&gt;Networking Performance&lt;/head&gt;
    &lt;p&gt;A huge part of the value is the built-in ConnectX-7 networking. I tested that, and it's fast. But also a bit odd. Here's the maximum TCP performance I was able to get through the fastest interface on each of the three systems I've been comparing:&lt;/p&gt;
    &lt;p&gt;But 106 Gigabits isn't 200, is NVIDIA lying?&lt;/p&gt;
    &lt;p&gt;Well, no... it's a little complicated. For full details, I'll refer you to the ServeTheHome article The NVIDIA GB10 ConnectX-7 200GbE Networking is Really Different.&lt;/p&gt;
    &lt;p&gt;Because the ports are each connected to a x4 PCIe Gen 5 link—which isn't enough bandwidth for 200 Gbps per port. To get a full 200 Gbps, you have to use Infiniband/RDMA and carefully configure the network topology. You won't get more than about 206 Gbps, maximum, in real world throughput, no matter how you set it up.&lt;/p&gt;
    &lt;p&gt;That's still honestly pretty good, but it's not the same as getting 400 Gbps of networking for AI clustering, like I think some of us expected reading the initial press releases in early 2025...&lt;/p&gt;
    &lt;p&gt;From the perspective of someone replicating NVIDIA's networking stack locally, though, having ConnectX ports built in is a boon. If you want replicate this kind of developer setup on AMD, you'd have to spend around the same amount of money, for the Max+ 395 plus a Connect-X 7 card.&lt;/p&gt;
    &lt;p&gt;Many people don't care about clustering use cases, or RDMA or Infiniband, but that doesn't mean it's not extremely useful for the people who do. This stuff's expensive, but to some people, it's not a bad value.&lt;/p&gt;
    &lt;head rend="h2"&gt;AI Performance&lt;/head&gt;
    &lt;p&gt;For now I'm just running two models, both of them with llama.cpp, optimized for each architecture.&lt;/p&gt;
    &lt;p&gt;And for a small model that requires a decent amount of CPU to keep up with the GPU, the GB10 does pretty well, almost hitting 100 tokens/s for inference, which is second to the M3 Ultra:&lt;/p&gt;
    &lt;p&gt;But for prompt processing, which is important for how quickly you start seeing a response from AI models, the GB10 chip is the winner, despite costing less than half the M3 Ultra.&lt;/p&gt;
    &lt;p&gt;And it's a similar story for a huge 'dense' model, Llama 3.1 70B, except here, it gets beat just a little by AMD's Strix Halo in the Framework Desktop:&lt;/p&gt;
    &lt;p&gt;Prompt processing is a strong selling point for these boxes. That's the reason Exo teased running a DGX Spark as the compute node for a Mac Studio cluster.&lt;/p&gt;
    &lt;p&gt;You can have the Spark, or one of these Dell's, handle the thing it's best at, prompt processing, while the Mac Studios handle the thing they're best at, memory bandwidth for token generation.&lt;/p&gt;
    &lt;p&gt;Anyway, these are just two quick AI benchmarks, and I have a lot more in the Dell Pro Max with GB10 issue in my ai-benchmarks repository. I'm doing a lot more testing, including model training and how I clustered two of these things in a tiny mini rack, but you'll have to wait until next year for that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Comments&lt;/head&gt;
    &lt;p&gt;Comparing Macs to Macs?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46457027</guid><pubDate>Thu, 01 Jan 2026 19:11:53 +0000</pubDate></item><item><title>Linux is good now</title><link>https://www.pcgamer.com/software/linux/im-brave-enough-to-say-it-linux-is-good-now-and-if-you-want-to-feel-like-you-actually-own-your-pc-make-2026-the-year-of-linux-on-your-desktop/</link><description>&lt;doc fingerprint="13f391b9904a4bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I'm brave enough to say it: Linux is good now, and if you want to feel like you actually own your PC, make 2026 the year of Linux on (your) desktop&lt;/head&gt;
    &lt;p&gt;Now if you don't mind I'm going to delete the root folder and see what happens.&lt;/p&gt;
    &lt;p&gt;I'm all-in, baby. I'm committed. If upgrading any distinct component of my PC didn't require me taking out a loan right now, I'd be seriously considering switching my GPU over to some kind of AMD thing just to make my life slightly, slightly easier.&lt;/p&gt;
    &lt;p&gt;I've had it with Windows and ascended to the sunlit uplands of Linux, where the trees heave with open-source fruits and men with large beards grep things with their minds.&lt;/p&gt;
    &lt;p&gt;I'm not alone. In last month's Steam hardware survey, the number of Linux users hit a new all-time high for the second month running, reaching the heady summit of a whopping, ah, 3.2% of overall Steam users. Hey, we're beating Mac players.&lt;/p&gt;
    &lt;p&gt;I think that number will only grow as the new year goes by. More and more of us are getting sick of Windows, sure—the AI guff, the constant upselling on Office subs, the middle taskbar*—but also, all my experience goofing about with Linux this year has dispelled a lot of the, frankly, erroneous ideas I had about it. It's really not hard! Really! I know Linux guys have been saying this for three decades, but it's true now!&lt;/p&gt;
    &lt;head rend="h2"&gt;Goated with the open source (sorry)&lt;/head&gt;
    &lt;p&gt;As I've already written about, the bulk of my Linux-futzing time this year has been spent in Bazzite, a distro tailor-made for gaming and also tailor-made to stop idiots (me) from doing something likely to detonate their boot drive.&lt;/p&gt;
    &lt;p&gt;I grew up thinking of Linux as 'the command-line OS that lets you delete your bootloader' and, well, I suppose that's not untrue, but I've been consistently impressed at how simple Bazzite has been to run on my PC, even with my persnickety Nvidia GPU.&lt;/p&gt;
    &lt;p&gt;Everything I've played this year has been as easy—if not easier—to run on a free OS put together by a gaggle of passionate nerds as it is on Windows, the OS made by one of the most valuable corporations on planet Earth. I've never had to dip into the command line (which is, to be frank, a shame, as the command line is objectively cool).&lt;/p&gt;
    &lt;p&gt;Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team.&lt;/p&gt;
    &lt;p&gt;But to be honest, it's not as if the Bazzite team has miraculously made Linux pleasant to use after decades of it seeming difficult and esoteric to normie computer users. I think mainstream Linux distros are just, well, sort of good now. Apart from my gaming PC, I also have an old laptop converted into a media server that lives underneath my television. It runs Debian 13 (which I updated to from Debian 12 earlier in the year) and requires essentially zero input from me at all.&lt;/p&gt;
    &lt;p&gt;What's more, the only software I have on there is software I actually want on there. Oh for a version of Windows that let me do something as zany as, I don't know, uninstall Edge.&lt;/p&gt;
    &lt;p&gt;That's the true nub of it, I think. The stats can say what they like (and they do! We've all heard tales of Windows games actually running better on Linux via Valve's Proton compatibility layer), but the heart of my fatigue with Windows is that, for every new worthless AI gadget Microsoft crams into it and for every time the OS inexplicably boots to a white screen and implores me to "finish setting up" my PC with an Office 365 subscription, the real problem is a feeling that my computer isn't mine, that I am somehow renting this thing I put together with my own two hands from an AI corporation in Redmond.&lt;/p&gt;
    &lt;p&gt;That's fine for consoles. Indeed, part of the whole pitch of an Xbox or PlayStation is the notion that you are handing off a lot of responsibility for your device to Sony and Microsoft's teams of techs, but my PC? That I built? Get your grubby mitts off it.&lt;/p&gt;
    &lt;p&gt;Are there issues? Sure. HDR's still a crapshoot (plus ça change) and, as you've no doubt heard, a lot of live-service games have anticheat software that won't play with Linux. But I think both of these issues are gradually ticking toward their solutions, particularly with Valve making its own push into the living room.&lt;/p&gt;
    &lt;p&gt;So I say make 2026 the year you give Linux a try, if you haven't already. At the very least, you can stick it on a separate boot drive and have a noodle about with it. I suspect you'll find the open (source) water is a lot more hospitable than you might think.&lt;/p&gt;
    &lt;p&gt;*I'm actually fine with the middle taskbar. I'm sorry.&lt;/p&gt;
    &lt;p&gt;2026 Games: This year's upcoming games&lt;lb/&gt;Best PC games: Our all-time favorites&lt;lb/&gt;Free PC games: Freebie fest&lt;lb/&gt;Best FPS games: Finest gunplay&lt;lb/&gt;Best RPGs: Grand adventures&lt;lb/&gt;Best co-op games: Better together&lt;/p&gt;
    &lt;p&gt;One of Josh's first memories is of playing Quake 2 on the family computer when he was much too young to be doing that, and he's been irreparably game-brained ever since. His writing has been featured in Vice, Fanbyte, and the Financial Times. He'll play pretty much anything, and has written far too much on everything from visual novels to Assassin's Creed. His most profound loves are for CRPGs, immersive sims, and any game whose ambition outstrips its budget. He thinks you're all far too mean about Deus Ex: Invisible War.&lt;/p&gt;
    &lt;p&gt;You must confirm your public display name before commenting&lt;/p&gt;
    &lt;p&gt;Please logout and then login again, you will then be prompted to enter your display name.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46457770</guid><pubDate>Thu, 01 Jan 2026 20:35:11 +0000</pubDate></item><item><title>A website to destroy all websites</title><link>https://henry.codes/writing/a-website-to-destroy-all-websites/</link><description>&lt;doc fingerprint="3cc011532966ad5d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A website to destroy all websites.&lt;/head&gt;
    &lt;head rend="h2"&gt;table of contents, of course&lt;/head&gt;
    &lt;head rend="h2"&gt;The internet is bad.&lt;/head&gt;
    &lt;p&gt;Well, the Internet mostly feels bad these days.&lt;/p&gt;
    &lt;p&gt;We were given this vast, holy realm of self-discovery and joy and philosophy and community; a thousand thousand acres of digital landscape, on which to grow our forests and grasslands of imagination, plant our gardens of learning, explore the caves of our making. We were given the chance to know anything about anything, to be our own Prometheus, to make wishes and to grant them.&lt;/p&gt;
    &lt;p&gt;But that’s not what we use the Internet for anymore. These days, instead of using it to make ourselves, most of us are using it to waste ourselves: we’re doom-scrolling brain-rot on the attention-farm, we’re getting slop from the feed.&lt;/p&gt;
    &lt;p&gt;Instead of turning freely in the HTTP meadows we grow for each other, we go to work: we break our backs at the foundry of algorithmic content as this earnest, naïve, human endeavoring to connect our lives with others is corrupted. Our powerful drive to learn about ourselves, each other, and our world, is broken into scant remnants — hollow, clutching phantasms of Content Creation, speed-cut vertical video, listicle thought-leadership, ragebait and the thread emoji.&lt;/p&gt;
    &lt;head rend="h3"&gt;it wasn’t always like this.&lt;/head&gt;
    &lt;p&gt;It used to feel way better to Go Online, and some of us will remember.&lt;/p&gt;
    &lt;p&gt;We used to be able to learn about our hobbies and interests from hundreds of experts on a wealth of websites whose only shared motivation was their passion. Some of those venerable old educational blogs, forums, and wikis still stand, though most have been bulldozed.&lt;/p&gt;
    &lt;p&gt;Now, Learning On The Internet often means fighting ads and endless assaults on one’s attention — it means watching part-1-part-2-part-3 short-form video clips, taped together by action movie psychology hacks, narrated gracelessly by TTS AI voices. We’re down from a thousand and one websites to three, and each of those remaining monolith websites is just a soullessly-regurgitated, compression-down-scaled, AI-up-scaled version of the next.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;We used to make lasting friendships with folks all over the world on shared interest and good humor.&lt;/p&gt;
    &lt;p&gt;But now those social networks, once hand-built and hand-tended, vibrant and organic, are unceremoniously swallowed by social media networks, pens built for trapping us and our little piggy attentions, turning us all into clout-chasers &amp;amp; content-creators, and removing us from what meaningful intimacy &amp;amp; community felt like.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;Even coding for the web used to be different: One could Learn To Code™ to express oneself creatively, imbue one’s online presence with passion and meaning, and for some of us, build a real career.&lt;/p&gt;
    &lt;p&gt;These days, however, we write increasing amounts of complicated, unsecure code to express less and less meaning, in order to infinitely generate shareholder value. We don’t think about the art of our craft and the discipline of its application, we think about throughput and scale.&lt;/p&gt;
    &lt;p&gt;To be very clear: I’m not trying to Good Old Days the internet. None of this is meant to make you feel nostalgic — the Internet used to be slow and less populated and less diverse, and its access was limited to those of a certain class. The Web For All is a marked improvement, widespread global internet access is a marked improvement, and what I’m asking you to consider is what it used to feel like to use these tools, and what we’ve lost in the Big Tech, Web 2.0 and web3 devouring of the ’Net.&lt;/p&gt;
    &lt;head rend="h2"&gt;The invention of the automobile&lt;/head&gt;
    &lt;p&gt;The onset of the automobile was a revelation for access and personal liberty. With the advent of cars, members of society could travel farther, get more done in their day, and bend their limited time more to their creative will!&lt;/p&gt;
    &lt;p&gt;But as time wore on and the industrialization &amp;amp; proliferation of the automobile progressed, its marginal utility diminished — the industry started to offer society fewer &amp;amp; fewer benefits, and take more &amp;amp; more in exchange1.&lt;/p&gt;
    &lt;p&gt;In American cities, for example: though at first the automobile enabled humans to travel further distances, it now demanded that humans travel those distances, and demanded infrastructure be created &amp;amp; maintained to enable it.2 Many now must use an automobile to get everything done in their town in a day, and must pay &amp;amp; take time for that automobile’s fueling &amp;amp; maintenance.3&lt;/p&gt;
    &lt;p&gt;Further than that, the automobile asks all of us to chip in tax revenue to protect its infrastructure, but only certain classes can afford an automobile with which to use that infrastructure, and those classes who can’t afford to do so are relegated to underfunded public transit systems.4&lt;/p&gt;
    &lt;p&gt;No longer a tool to serve our societies, our societies now serve the automobile.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tools for Conviviality, &amp;amp; the industrialization of the Web.&lt;/head&gt;
    &lt;p&gt;In his book Tools For Conviviality, technology philosopher and social critic Ivan Illich identifies these two critical moments, the optimistic arrival &amp;amp; the deadening industrialization, as watersheds of technological advent. Tools are first created to enhance our capacities to spend our energy more freely and in turn spend our days more freely, but as their industrialization increases, their manipulation &amp;amp; usurpation of society increases in tow5.&lt;/p&gt;
    &lt;p&gt;Illich also describes the concept of radical monopoly, which is that point where a technological tool is so dominant that people are excluded from society unless they become its users. We saw this with the automobile, we saw it with the internet, and we even see it with social media.&lt;/p&gt;
    &lt;p&gt;&lt;del&gt;No longer a tool to serve our societies, our societies now serve the automobile.&lt;/del&gt; Instead of designing and using tools to build a society, our society changes to adapt to the demands of our tools.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;Illich’s thesis allows us to reframe our adoption and use of the technologies in our life. We can map fairly directly most technological developments in the last 100 (or even 200) years to this framework: a net lift, followed by a push to extract value and subsequent insistence upon the technology’s ubiquity:&lt;/p&gt;
    &lt;head rend="h3"&gt;the textile revolution&lt;/head&gt;
    &lt;p&gt;The preferred imagery used to mythologize the Industrial Revolution is the woodetchings of textile manufacturers, transformed in the early 19th century by the arrival of automated fabric machinery. Its proponents laud the shift of an agricultural society to a technological one, creating new sectors for labor, and raising up the middle class (we will say nothing of this period’s new punishing conditions for labor in this essay6). But the ultimate ecological and human costs engendered by the increasing availability of cheap fabric production are well-documented: In 2022, the fashion and textile industries employed around 60 million factory workers worldwide7, and less than 2% of those workers earn a living wage. Those workers also endure the full suite of labor exploitation practices, including gender-based harassment, wage theft, and unsafe conditions. On the material side, the induced consumption resulting from ever-cheaper products means the world consumed 400% more textile products globally as 20 years ago8, and bins most of it (the average American generates 82 pounds of textile waste each year).&lt;/p&gt;
    &lt;head rend="h3"&gt;antibiotic technology&lt;/head&gt;
    &lt;p&gt;The arrival of antibiotics in 19289 allowed for revolutionary leaps in fighting bacterial infections like strep throat, pneumonia, and meningitis, but an over-dependence and over-prescription of penicillin and its siblings through the 1950s-70s resulted in the proliferation of antibiotic resistance, which subsequently led to longer hospital stays, higher medical costs, and increased mortality.10&lt;/p&gt;
    &lt;head rend="h3"&gt;space exploration&lt;/head&gt;
    &lt;p&gt;Since the beginning of the space exploration era in the late 1950s, humanity has made leaps and bounds in learning about our own world and its physical systems, telecommunications, imaging, etc. The increasing frequency of commercialization missions in space for satellite systems (and lately tourism) has resulted in immense amounts of space debris being generated — both from active satellites and from jettisoned/destroyed components of previous missions, the debris threatens future missions and has even been destructive to the field of astronomy, making it impossible to use earth-based sensors and photography devices to learn about space.11 So desperate to extract Shareholder Value from the starry sky, we’re blinding our own ability to look at it.&lt;/p&gt;
    &lt;p&gt;The web is no exception to this pattern. A vision of interoperability, accessibility, and usability, the World Wide Web was first conceived in 1989 as a way to universally link documents and other media content in a flexibly-organized system that could make information easily accessed at CERN, and be easily shared with collaborators beyond.12 But the proliferation of access and ultimate social requirement of access has spawned countless troubles for human society, including cyberstalking and bullying, the instantaneous circulation of CSAM, violent images, and misinformation, identity theft, addiction, etcetera.&lt;/p&gt;
    &lt;p&gt;The rampant industrialization and commercialization of the Web predictably develops flashy, insidious patterns of extracting capital from its users: new surfaces for information means new surfaces for advertisement, and new formats of media beget new mechanisms for divorcing you from their ownership.&lt;/p&gt;
    &lt;head rend="h3"&gt;convivial life &amp;amp; convivial tooling&lt;/head&gt;
    &lt;p&gt;Illich poses convivial tools as directly opposed to this industrialized, radically-monopolized set of social systems. Similar to E.F. Schumacher’s concept of “intermediate technology” introduced in his 1973 book Small Is Beautiful: A Study of Economics As If People Mattered, convivial tools are sustainable, energy-efficient (though often labor intensive), local-first, and designed primarily to enhance the autonomy and creativity of their users.13 Illich cites specifically hand tools, bicycles, and telephones as examples, but with its enormous capacity for interoperability and extensibility, the Internet is the perfect workshed in which to design our own Tools For Conviviality.&lt;/p&gt;
    &lt;head rend="h2"&gt;the Web we want&lt;/head&gt;
    &lt;p&gt;let’s reconsider&lt;/p&gt;
    &lt;p&gt;the markers of a decaying 'Net I mentioned before, with convivial tooling in mind:&lt;/p&gt;
    &lt;head rend="h3"&gt;Teaching &amp;amp; learning on the Web&lt;/head&gt;
    &lt;p&gt;Monolithic platforms like YouTube, TikTok, Medium, and Substack draw a ton of creators and educators because of the promise of monetization and large audiences, but they’ve shown time and time again how the lack of ownership creates a problem. When those platforms fail, when they change their rules, when they demand creators move or create a particular way to maintain their access to those audiences, they pit creators or their audiences against the loss of the other. Without adhering to the algorithm’s requirements, writers may not write an impactful document, and without bypassing a paywall, readers can’t read it.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;When those promises of exorbitant wealth and a life of decadence through per-click monetization ultimately dry up (or come with a steep moral or creative cost), creators and learners must look for new solutions for how educational content is shared on the Internet. The most self-evident, convivial answer is an old one: blogs. HTML is free to access by default, RSS has worked for about 130 years[citation needed], and combined with webmentions, it’s never been easier to read new ideas, experiment with ideas, and build upon &amp;amp; grow those ideas with other strong thinkers on the web, owning that content all along.14&lt;/p&gt;
    &lt;head rend="h3"&gt;Connecting with friends on the Web&lt;/head&gt;
    &lt;p&gt;Social media apps have imprisoned us all in this weird content prison — in order to connect with friends we’re sort of forced to create or be vanished by capricious black box algorithms, and all that we do create is, as we’ve already alluded to, subsequently owned by whatever platform we’ve created it on. If Instagram goes away overnight, or decides to pivot catastrophically, your stories and your network of friends goes with it.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;The advent and development of tools &amp;amp; methodologies like POSSE (Publish On your Own Site, Syndicate Elsewhere), ActivityPub, microformats, and ATProto, it’s becoming quite achievable to generate your own social network, interoperable with other networks like Bluesky or Mastodon. That network, designed for ownership and decentralization, is durable, designed around storytelling instead of engagement, and free of the whims of weird tech billionaires.&lt;/p&gt;
    &lt;p&gt;With some basic HTML knowledge and getting-stuff-online knowledge, a handful of scrappy protocols, and a free afternoon or two, one can build their own home to post bangers for the tight homies, make friends, and snipe those new friends with those hits of dopamine they so fiendishly rely on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Coding for the web&lt;/head&gt;
    &lt;p&gt;Lastly, consider the discipline of web engineering:&lt;/p&gt;
    &lt;p&gt;We have been asked to build the same B2B SaaS website with the same featureset n^∞ times, and our answers for the optimal way to do that are increasingly limited. We’ve penned all of our markup into JavaScript templates just in case a product manager needs the wrapper component to post JSON somewhere down the line, and we’ve whittled away at style code until it’s just a mechanism for deploying one of two border-radius-drop-shadow combos to divs. It’s an industrial, production-minded way of approaching a discipline that has all the hallmarks of being a great craft, and that’s understandably uninspiring to many of us.&lt;/p&gt;
    &lt;p&gt;¶&lt;/p&gt;
    &lt;p&gt;Yet our young React shepherds have no need to fear: there are countless more colors than blurple out there, and countless more fonts than Inter. HTML and CSS are better and more generative technologies than they’ve ever been: Thanks to the tireless work of the CSS working groups and browser implementers, etc, there is an unbelievable amount of creative expression possible with basic web tools in a text editor. Even JavaScript is more progressively-enhanceable than ever, and enables interfacing with a rapidly-growing number of exciting browser APIs (still fuck Brendan Eich though). &lt;code&gt;${new Date.getCurrentYear()}&lt;/code&gt; is a veritable renaissance of web code, and it asks of authors only curiosity and a drive to experiment.&lt;/p&gt;
    &lt;head rend="h2"&gt;so where do we go from here?&lt;/head&gt;
    &lt;p&gt;Illich’s thesis is that technology and its derived tools should serve people in a way that enhances their freedom, creativity, independence, and will.&lt;/p&gt;
    &lt;p&gt;The distillation of those principles on the web through manual code, hand-built social networks, and blogs, points luminously to one answer to the question of how the Internet can best serve humans:&lt;/p&gt;
    &lt;head rend="h3"&gt;it’s personal websites.&lt;/head&gt;
    &lt;p&gt;Hand-coded, syndicated, and above all personal websites are exemplary: They let users of the internet to be autonomous, experiment, have ownership, learn, share, find god, find love, find purpose. Bespoke, endlessly tweaked, eternally redesigned, built-in-public, surprising UI and delightful UX. The personal website is a staunch undying answer to everything the corporate and industrial web has taken from us.&lt;/p&gt;
    &lt;p&gt;And how might one claim this ultimate toolchain of conviviality, and build a place on the web that enhances their autonomy and creativity?&lt;/p&gt;
    &lt;p&gt;How might one build a personal website?&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Start small&lt;/head&gt;
        &lt;p&gt;Let yourself start small, have fun trying shit that doesn’t work, document your growth, publish failed ideas &amp;amp; successful ones. Some of the best websites in the world are just HTML, and they belong to their authors. Make friends, let yourself be inspired by others, send friendly emails asking to learn new things, and do not demand of yourself masterpieces.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;head rend="h3"&gt;Reduce friction to publishing&lt;/head&gt;&lt;p&gt;Get the resistance to ship out of your way. Don’t get caught up in tooling and frameworks, just write HTML and get something online. If you’re an engineer, delight that you’re not beholden to the same standards of quality and rigorous testing that you are at work — draft some ideas, hit the&lt;/p&gt;&lt;code&gt;h1&lt;/code&gt;to&lt;code&gt;p&lt;/code&gt;tag combo, and publish. Update and update again; let your ideas grow like gardens, the way they do in your mind. The mutability of the web, often its great weakness, is also one of its great strengths.&lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Don’t worry about design (unless you want to)&lt;/head&gt;
        &lt;p&gt;Don’t worry about design unless that’s the part that brings you joy. Make friends with designers and trade your work for theirs, or trade tips, trade advice. Get comfortable with being joyfully bad at something — from that soil of humility grows a million questions for those who have learned and are excited to share. Iterate until you’ve something you’re proud of, or iterate so much you’ve ruined it and have to go back to bald.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Use the IndieWeb&lt;/head&gt;
        &lt;p&gt;Leverage the IndieWeb and its wonderfully thought-out protocols, tools like brid.gy to syndicate your ideas out to the wider web, and then use Webmentions to bring the ensuing conversations back where the content is. That way, you can publish work where you prefer to, folks on Bluesky can enjoy and discuss it, in the same stroke as folks on Mastodon may, or folks directly on the canonical URL.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Join us in sharing what you’ve made&lt;/head&gt;
        &lt;p&gt;I encourage you to join us in our auspicious website adventure, and if you do, I hope you’ll further join us on personalsit.es, our happy little home for everyone building something humble or thrilling or joyful or deeply accursed, but personal.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;(denouement)&lt;/head&gt;
    &lt;p&gt;You’re not crazy. The internet does feel genuinely so awful right now, and for about a thousand and one reasons. But the path back to feeling like you have some control is to un-spin yourself from the Five Apps of the Apocalypse and reclaim the Internet as a set of tools you use to build something you can own &amp;amp; be proud of — or in most of our cases, be deeply ashamed of. Godspeed and good luck.&lt;/p&gt;
    &lt;p&gt;❦&lt;/p&gt;
    &lt;p&gt;That’s all for me. If you find any issues with this post, please reach out to me by email. Thanks eternally for your time and patience, and thanks for reading. Find me here online at one of my personal websites like henry.codes or strange.website or stillness.digital or strangersbyspring.com, or sometimes on Bluesky and Mastodon.&lt;/p&gt;
    &lt;p&gt;As ever, unionize, free Palestine, trans rights are human rights, fix your heart or die.&lt;/p&gt;
    &lt;p&gt;fin.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46457784</guid><pubDate>Thu, 01 Jan 2026 20:36:46 +0000</pubDate></item><item><title>Can Bundler be as fast as uv?</title><link>https://tenderlovemaking.com/2025/12/29/can-bundler-be-as-fast-as-uv/</link><description>&lt;doc fingerprint="7685981e496335d9"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Can Bundler Be as Fast as uv?&lt;/head&gt;Dec 29, 2025 @ 12:26 pm&lt;p&gt;At RailsWorld earlier this year, I got nerd sniped by someone. They asked “why can’t Bundler be as fast as uv?” Immediately my inner voice said “YA, WHY CAN’T IT BE AS FAST AS UV????”&lt;/p&gt;&lt;p&gt;My inner voice likes to shout at me, especially when someone asks a question so obvious I should have thought of it myself. Since then I’ve been thinking about and investigating this problem, going so far as to give a presentation at XO Ruby Portland about Bundler performance. I firmly believe the answer is “Bundler can be as fast as uv” (where “as fast” has a margin of error lol).&lt;/p&gt;&lt;p&gt;Fortunately, Andrew Nesbitt recently wrote a post called “How uv got so fast”, and I thought I would take this opportunity to review some of the highlights of the post and how techniques applied in uv can (or can’t) be applied to Bundler / RubyGems. I’d also like to discuss some of the existing bottlenecks in Bundler and what we can do to fix them.&lt;/p&gt;&lt;p&gt;If you haven’t read Andrew’s post, I highly recommend giving it a read. I’m going to quote some parts of the post and try to reframe them with RubyGems / Bundler in mind.&lt;/p&gt;&lt;head rend="h2"&gt;Rewrite in Rust?&lt;/head&gt;&lt;p&gt;Andrew opens the post talking about rewriting in Rust:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;uv installs packages faster than pip by an order of magnitude. The usual explanation is âitâs written in Rust.â Thatâs true, but it doesnât explain much. Plenty of tools are written in Rust without being notably fast. The interesting question is what design decisions made the difference.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is such a good quote. I’m going to address “rewrite in Rust” a bit later in the post. But suffice to say, I think if we eliminate bottlenecks in Bundler such that the only viable option for performance improvements is to “rewrite in Rust”, then I’ll call it a success. I think rewrites give developers the freedom to “think outside the box”, and try techniques they might not have tried. In the case of &lt;code&gt;uv&lt;/code&gt;, I think it gave the developers a good way to say “if we don’t have to worry about backwards compatibility, what could we achieve?”.&lt;/p&gt;&lt;p&gt;I suspect it would be possible to write a uv in Python (PyUv?) that approaches the speeds of uv, and in fact much of the blog post goes on to talk about performance improvements that aren’t related to Rust.&lt;/p&gt;&lt;head rend="h2"&gt;Installing code without eval’ing&lt;/head&gt;&lt;quote&gt;&lt;p&gt;pipâs slowness isnât a failure of implementation. For years, Python packaging required executing code to find out what a package needed.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I didn’t know this about Python packages, and it doesn’t really apply to Ruby Gems so I’m mostly going to skip this section.&lt;/p&gt;&lt;p&gt;Ruby Gems are tar files, and one of the files in the tar file is a YAML representation of the GemSpec. This YAML file declares all dependencies for the Gem, so RubyGems can know, without evaling anything, what dependencies it needs to install before it can install any particular Gem. Additionally, RubyGems.org provides an API for asking about dependency information, which is actually the normal way of getting dependency info (again, no &lt;code&gt;eval&lt;/code&gt; required).&lt;/p&gt;&lt;p&gt;There’s only one other thing from this section I’d like to quote:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;PEP 658 (2022) put package metadata directly in the Simple Repository API, so resolvers could fetch dependency information without downloading wheels at all.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Fortunately RubyGems.org already provides the same information about gems.&lt;/p&gt;&lt;p&gt;Reading through the number of PEPs required as well as the amount of time it took to get the standards in place was very eye opening for me. I can’t help but applaud folks in the Python community for doing this. It seems like a mountain of work, and they should really be proud of themselves.&lt;/p&gt;&lt;head rend="h2"&gt;What uv drops&lt;/head&gt;&lt;p&gt;I’m mostly going to skip this section except for one point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Ignoring requires-python upper bounds. When a package says it requires python&amp;lt;4.0, uv ignores the upper bound and only checks the lower. This reduces resolver backtracking dramatically since upper bounds are almost always wrong. Packages declare python&amp;lt;4.0 because they havenât tested on Python 4, not because theyâll actually break. The constraint is defensive, not predictive.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think this is very very interesting. I don’t know how much time Bundler spends on doing “required Ruby version” bounds checking, but it feels like if uv can do it, so can we.&lt;/p&gt;&lt;head rend="h2"&gt;Optimizations that donât need Rust&lt;/head&gt;&lt;p&gt;I really love that Andrew pointed out optimizations that could be made that don’t involve Rust. There are three points in this section that I want to pull out:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Parallel downloads. pip downloads packages one at a time. uv downloads many at once. Any language can do this.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is absolutely true, and is a place where Bundler could improve. Bundler currently has a problem when it comes to parallel downloads, and needs a small architectural change as a fix.&lt;/p&gt;&lt;p&gt;The first problem is that Bundler tightly couples installing a gem with downloading the gem. You can read the installation code here, but I’ll summarize the method in question below:&lt;/p&gt;&lt;code&gt;def install
  path = fetch_gem_if_not_cached
  Bundler::RubyGemsGemInstaller.install path, dest
end
&lt;/code&gt;&lt;p&gt;The problem with this method is that it inextricably links downloading the gem with installing it. This is a problem because we could be downloading gems while installing other gems, but we’re forced to wait because the installation method couples the two operations. Downloading gems can trivially be done in parallel since the &lt;code&gt;.gem&lt;/code&gt; files are just archives that can be fetched independently.&lt;/p&gt;&lt;p&gt;The second problem is the queuing system in the installation code. After gem resolution is complete, and Bundler knows what gems need to be installed, it queues them up for installation. You can find the queueing code here. The code takes some effort to understand. Basically it allows gems to be installed in parallel, but only gems that have already had their dependencies installed.&lt;/p&gt;&lt;p&gt;So for example, if you have a dependency tree like “gem &lt;code&gt;a&lt;/code&gt; depends on gem &lt;code&gt;b&lt;/code&gt; which depends on gem &lt;code&gt;c&lt;/code&gt;” (&lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt;), then no gems will be installed (or downloaded) in parallel.&lt;/p&gt;&lt;p&gt;To demonstrate this problem in an easy-to-understand way, I built a slow Gem server. It generates a dependency tree of &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; (&lt;code&gt;a&lt;/code&gt; depends on &lt;code&gt;b&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; depends on &lt;code&gt;c&lt;/code&gt;), then starts a Gem server.
The Gem server takes 3 seconds to return any Gem, so if we point Bundler at this Gem server and then profile Bundler, we can see the impact of the queueing system and download scheme.&lt;/p&gt;&lt;p&gt;In my test app, I have the following Gemfile:&lt;/p&gt;&lt;code&gt;source "http://localhost:9292"

gem "a"
&lt;/code&gt;&lt;p&gt;If we profile Bundle install with Vernier, we can see the following swim lanes in the marker chart:&lt;/p&gt;&lt;p&gt;The above chart is showing that we get no parallelism during installation. We spend 3 seconds downloading the &lt;code&gt;c&lt;/code&gt; gem, then we install it.
Then we spend 3 seconds downloading the &lt;code&gt;b&lt;/code&gt; gem, then we install it.
Finally we spend 3 seconds downloading the &lt;code&gt;a&lt;/code&gt; gem, and we install it.&lt;/p&gt;&lt;p&gt;Timing the &lt;code&gt;bundle install&lt;/code&gt; process shows we take over 9 seconds to install (3 seconds per gem):&lt;/p&gt;&lt;code&gt;&amp;gt; rm -rf x; rm -f Gemfile.lock; time GEM_PATH=(pwd)/x GEM_HOME=(pwd)/x bundle install
Fetching gem metadata from http://localhost:9292/...
Resolving dependencies...
Fetching c 1.0.0
Installing c 1.0.0
Fetching b 1.0.0
Installing b 1.0.0
Fetching a 1.0.0
Installing a 1.0.0
Bundle complete! 1 Gemfile dependency, 3 gems now installed.
Use `bundle info [gemname]` to see where a bundled gem is installed.

________________________________________________________
Executed in   11.80 secs      fish           external
   usr time  341.62 millis  231.00 micros  341.38 millis
   sys time  223.20 millis  712.00 micros  222.49 millis
&lt;/code&gt;&lt;p&gt;Contrast this with a Gemfile containing &lt;code&gt;d&lt;/code&gt;, &lt;code&gt;e&lt;/code&gt;, and &lt;code&gt;f&lt;/code&gt;, which have no dependencies, but still take 3 seconds to download:&lt;/p&gt;&lt;code&gt;source "http://localhost:9292"

gem "d"
gem "e"
gem "f"
&lt;/code&gt;&lt;p&gt;Timing &lt;code&gt;bundle install&lt;/code&gt; for the above Gemfile shows it takes about 4 seconds:&lt;/p&gt;&lt;code&gt;&amp;gt; rm -rf x; rm -f Gemfile.lock; time GEM_PATH=(pwd)/x GEM_HOME=(pwd)/x bundle install
Fetching gem metadata from http://localhost:9292/.
Resolving dependencies...
Fetching d 1.0.0
Fetching e 1.0.0
Fetching f 1.0.0
Installing e 1.0.0
Installing f 1.0.0
Installing d 1.0.0
Bundle complete! 3 Gemfile dependencies, 3 gems now installed.
Use `bundle info [gemname]` to see where a bundled gem is installed.

________________________________________________________
Executed in    4.14 secs      fish           external
   usr time  374.04 millis    0.38 millis  373.66 millis
   sys time  368.90 millis    1.09 millis  367.81 millis
&lt;/code&gt;&lt;p&gt;We were able to install the same number of gems in a fraction of the time. This is because Bundler is able to download siblings in the dependency tree in parallel, but unable to handle other relationships.&lt;/p&gt;&lt;p&gt;There is actually a good reason that Bundler insists dependencies are installed before the gems themselves: native extensions. When installing native extensions, the installation process must run Ruby code (the &lt;code&gt;extconf.rb&lt;/code&gt; file).
Since the &lt;code&gt;extconf.rb&lt;/code&gt; could require dependencies be installed in order to run, we must install dependencies first.
For example &lt;code&gt;nokogiri&lt;/code&gt; depends on &lt;code&gt;mini_portile2&lt;/code&gt;, but &lt;code&gt;mini_portile2&lt;/code&gt; is only used during the installation process, so it needs to be installed before &lt;code&gt;nokogiri&lt;/code&gt; can be compiled and installed.&lt;/p&gt;&lt;p&gt;However, if we were to decouple downloading from installation it would be possible for us to maintain the “dependencies are installed first” business requirement but speed up installation. In the &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case, we could have been downloading gems &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; at the same time as gem &lt;code&gt;c&lt;/code&gt; (or even while waiting on &lt;code&gt;c&lt;/code&gt; to be installed).&lt;/p&gt;&lt;p&gt;Additionally, pure Ruby gems don’t need to execute any code on installation. If we knew that we were installing a pure Ruby gem, it would be possible to relax the “dependencies are installed first” business requirement and get even more performance increases. The above &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case could install all three gems in parallel since none of them execute Ruby code during installation.&lt;/p&gt;&lt;p&gt;I would propose we split installation in to 4 discrete steps:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;Download the gem&lt;/item&gt;&lt;item&gt;Unpack the gem&lt;/item&gt;&lt;item&gt;Compile the gem&lt;/item&gt;&lt;item&gt;Install the gem&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Downloading and unpacking can be done trivially in parallel. We should unpack the gem to a temporary folder so that if the process crashes or the machine loses power, the user isn’t stuck with a half-installed gem. After we unpack the gem, we can discover whether the gem is a native extension or not. If it’s not a native extension, we “install” the gem simply by moving the temporary folder to the “correct” location. This step could even be a “hard link” step as discussed in the next point.&lt;/p&gt;&lt;p&gt;If we discover that the gem is a native extension, then we can “pause” installation of that gem until its dependencies are installed, then resume (by compiling) at an appropriate time.&lt;/p&gt;&lt;p&gt;Side note: &lt;code&gt;gel&lt;/code&gt;, a Bundler alternative, works mostly in this manner today.
Here is a timing of the &lt;code&gt;a -&amp;gt; b -&amp;gt; c&lt;/code&gt; case from above:&lt;/p&gt;&lt;code&gt;&amp;gt; rm -f Gemfile.lock; time gel install
Fetching sources....
Resolving dependencies...
Writing lockfile to /Users/aaron/git/gemserver/app/Gemfile.lock
Installing c (1.0.0) 
Installing a (1.0.0)
Installing b (1.0.0)
Installed 3 gems  

________________________________________________________
Executed in    4.07 secs      fish           external
   usr time  289.22 millis    0.32 millis  288.91 millis
   sys time  347.04 millis    1.36 millis  345.68 millis
&lt;/code&gt;&lt;p&gt;Lets move on to the next point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Global cache with hardlinks. pip copies packages into each virtual environment. uv keeps one copy globally and uses hardlinks&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think this is a great idea, but I’d actually like to split the idea in two. First, RubyGems and Bundler should have a combined, global cache, full stop. I think that global cache should be in &lt;code&gt;$XDG_CACHE_HOME&lt;/code&gt;, and we should store &lt;code&gt;.gem&lt;/code&gt; files there when they are downloaded.&lt;/p&gt;&lt;p&gt;Currently, both Bundler and RubyGems will use a Ruby version specific cache folder. In other words, if you do &lt;code&gt;gem install rails&lt;/code&gt; on two different versions of Ruby, you get two copies of Rails and all its dependencies.&lt;/p&gt;&lt;p&gt;Interestingly, there is an open ticket to implement this, it just needs to be done.&lt;/p&gt;&lt;p&gt;The second point is hardlinking on installation. The idea here is that rather than unpacking the gem multiple times, once per Ruby version, we simply unpack once and then hard link per Ruby version. I like this idea, but I think it should be implemented after some technical debt is paid: namely implementing a global cache and unifying Bundler / RubyGems code paths.&lt;/p&gt;&lt;p&gt;On to the next point:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;PubGrub resolver&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Actually Bundler already uses a Ruby implementation of the PubGrub resolver. You can see it here. Unfortunately, RubyGems still uses the molinillo resolver.&lt;/p&gt;&lt;p&gt;In other words you use a different resolver depending on whether you do &lt;code&gt;gem install&lt;/code&gt; or &lt;code&gt;bundle install&lt;/code&gt;.
I don’t really think this is a big deal since the vast majority of users will be doing &lt;code&gt;bundle install&lt;/code&gt; most of time.
However, I do think this discrepancy is some technical debt that should be addressed, and I think this should be addressed via unification of RubyGems and Bundler codebases (today they both live in the same repository, but the code isn’t necessarily combined).&lt;/p&gt;&lt;p&gt;Lets move on to the next section of Andrew’s post:&lt;/p&gt;&lt;head rend="h2"&gt;Where Rust actually matters&lt;/head&gt;&lt;p&gt;Andrew first mentions “Zero-copy deserialization”. This is of course an important technique, but I’m not 100% sure where we would utilize it in RubyGems / Bundler. I think that today we parse the YAML spec on installation, and that could be a target. But I also think we could install most gems without looking at the YAML gemspec at all.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Thread-level parallelism. Pythonâs GIL forces parallel work into separate processes, with IPC overhead and data copying.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is an interesting point. I’m not sure what work pip needed to do in separate processes. Installing a pure Ruby, Ruby Gem is mostly an IO bound task, with some ZLIB mixed in. Both of these things (IO and ZLIB processing) release Ruby’s GVL, so it’s possible for us to do things truly in parallel. I imagine this is similar for Python / pip, but I really have no idea.&lt;/p&gt;&lt;p&gt;Given the stated challenges with Python’s GIL, you might wonder whether Ruby’s GVL presents similar parallelism problems for Bundler. I don’t think so, and in fact I think Ruby’s GVL gets kind of a bad rap. It prevents us from running CPU bound Ruby code in parallel. Ractors address this, and Bundler could possibly leverage them in the future, but since installing Gems is mostly an IO bound task I’m not sure what the advantage would be (possibly the version solver, but I’m not sure what can be parallelized in there). The GVL does allow us to run IO bound work in parallel with CPU bound Ruby code. CPU bound native extensions are allowed to release the GVL, allowing Ruby code to run in parallel with the native extension’s CPU bound code.&lt;/p&gt;&lt;p&gt;In other words, Ruby’s GVL allows us to safely run work in parallel. That said, the GVL can work against us because releasing and acquiring the GVL takes time.&lt;/p&gt;&lt;p&gt;If you have a system call that is very fast, releasing and acquiring the GVL could end up being a large percentage of that call. For example, if you do &lt;code&gt;File.binwrite(file, buffer)&lt;/code&gt;, and the buffer is very small, you could encounter a situation where GVL book keeping is the majority of the time.
A bummer is that Ruby Gem packages usually contain lots of very small files, so this problem could be impacting us.
The good news is that this problem can be solved in Ruby itself, and indeed some work is being done on it today.&lt;/p&gt;&lt;quote&gt;&lt;p&gt;No interpreter startup. Every time pip spawns a subprocess, it pays Pythonâs startup cost.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Obviously Ruby has this same problem. That said, we only start Ruby subprocesses when installing native extensions. I think native extensions make up the minority of gems installed, and even when installing a native extension, it isn’t Ruby startup that is the bottleneck. Usually the bottleneck is compilation / linking time (as we’ll see in the next post).&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Compact version representation. uv packs versions into u64 integers where possible, making comparison and hashing fast.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;This is a cool optimization, but I don’t think it’s actually Rust specific. Comparing integers is much faster than comparing version objects. The idea is that you take a version number, say &lt;code&gt;1.0.0&lt;/code&gt;, and then pack each part of the version in to a single integer.
For example, we could represent &lt;code&gt;1.0.0&lt;/code&gt; as &lt;code&gt;0x0001_0000_0000_0000&lt;/code&gt; and &lt;code&gt;1.1.0&lt;/code&gt; as &lt;code&gt;0x0001_0001_0000_0000&lt;/code&gt;, etc.&lt;/p&gt;&lt;p&gt;It should be possible to use this trick in Ruby and encode versions to integer immediates, which would unlock performance in the resolver. Rust has an advantage here - compiled native code comparing u64s will always be faster than Ruby, even with immediates. However, I would bet that with the YJIT or ZJIT in play, this gap could be closed enough that no end user would notice the difference between a Rust or Ruby implementation of Bundler.&lt;/p&gt;&lt;p&gt;I started refactoring the &lt;code&gt;Gem::Version&lt;/code&gt; object so that we might start doing this, but we ended up reverting it because of backwards compatibility (I am jealous of &lt;code&gt;uv&lt;/code&gt; in that regard).
I think the right way to do this is to refactor the solver entry point and ensure all version requirements are encoded as integer immediates before entering the solver.
We could keep the &lt;code&gt;Gem::Version&lt;/code&gt; API as “user facing” and design a more internal API that the solver uses.
I am very interested in reading the version encoding scheme in uv.
My intuition is that minor numbers tend to get larger than major numbers, so would minor numbers have more dedicated bits?
Would it even matter with 64 bits?&lt;/p&gt;&lt;head rend="h2"&gt;Wrapping this up&lt;/head&gt;&lt;p&gt;I’m going to quote Andrew’s last 2 paragraphs:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;uv is fast because of what it doesnât do, not because of what language itâs written in. The standards work of PEP 518, 517, 621, and 658 made fast package management possible. Dropping eggs, pip.conf, and permissive parsing made it achievable. Rust makes it a bit faster still.&lt;/p&gt;&lt;p&gt;pip could implement parallel downloads, global caching, and metadata-only resolution tomorrow. It doesnât, largely because backwards compatibility with fifteen years of edge cases takes precedence. But it means pip will always be slower than a tool that starts fresh with modern assumptions.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;I think these are very good points. The difference is that in RubyGems and Bundler, we already have the infrastructure in place for writing a “fast as uv” package manager. The difficult part is dealing with backwards compatibility, and navigating two legacy codebases. I think this is the real advantage the uv developers had. That said, I am very optimistic that we could “repair the plane mid-flight” so to speak, and have the best of both worlds: backwards compatibility and speed.&lt;/p&gt;&lt;p&gt;I mentioned at the top of the post I would address “rewrite it in Rust”, and I think Andrew’s own quote mostly does that for me. I think we could have 99% of the performance improvements while still maintaining a Ruby codebase. Of course if we rewrote it in Rust, you could squeeze an extra 1% out, but would it be worthwhile? I don’t think so.&lt;/p&gt;&lt;p&gt;I have a lot more to say about this topic, and I feel like this post is getting kind of long, so I’m going to end it here. Please look out for part 2, which I’m tentatively calling “What makes Bundler / RubyGems slow?” This post was very “can we make RubyGems / Bundler do what uv does?” (the answer is “yes”). In part 2 I want to get more hands-on by discussing how to profile Bundler and RubyGems, what specifically makes them slow in the real world, and what we can do about it.&lt;/p&gt;&lt;p&gt;I want to end this post by saying “thank you” to Andrew for writing such a great post about how uv got so fast.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46458302</guid><pubDate>Thu, 01 Jan 2026 21:37:10 +0000</pubDate></item><item><title>WebAssembly as a Python Extension Platform</title><link>https://nullprogram.com/blog/2026/01/01/</link><description>&lt;doc fingerprint="8c499a15de87b2c0"&gt;
  &lt;main&gt;
    &lt;p&gt; nullprogram.com/blog/2026/01/01/ &lt;/p&gt;
    &lt;p&gt; (The author is currently open to employment opportunities in the United States.) &lt;/p&gt;
    &lt;p&gt;Software above some complexity level tends to sport an extension language, becoming a kind of software platform itself. Lua fills this role well, and of course there’s JavaScript for web technologies. WebAssembly generalizes this, and any Wasm-targeting programming language can extend a Wasm-hosting application. It has more friction than supplying a script in a text file, but extension authors can write in their language of choice, and use more polished development tools — debugging, testing, etc. — than typically available for a typical extension language. Python is traditionally extended through native code behind a C interface, but it’s recently become practical to extend Python with Wasm. That is we can ship an architecture-independent Wasm blob inside a Python library, and use it without requiring a native toolchain on the host system. Let’s discuss two different use cases and their pitfalls.&lt;/p&gt;
    &lt;p&gt;Normally we’d extend Python in order to access an external interface that Python cannot access on its own. Wasm runs in a sandbox with no access to the outside world whatsoever, so it obviously isn’t useful for that case. Extensions may also grant Python more speed, which is one of Wasm’s main selling points. We can also use Wasm to access embeddable capabilities written in a different programming language which do not require external access.&lt;/p&gt;
    &lt;p&gt;For preferred non-WASI Wasm runtime is Volodymyr Shymanskyy’s wasm3. It’s plain old C and very friendly to embedding in the same was as, say, SQLite. Performance is middling, though a C program running on wasm3 is still quite a bit faster than an equivalent Python program. It has Python bindings, pywasm3, but it’s distributed only in source code form. That is, the host machine must have a C toolchain in order to use pywasm3, which defeats my purposes here. If there’s a C toolchain, I might as well just use that instead of going through Wasm.&lt;/p&gt;
    &lt;p&gt;For the use cases in this article, the best option is wasmtime-py. The distribution includes binaries for Windows, macOS, and Linux on x86-64 and ARM64, which covers nearly all Python installations. Hosts require nothing more than a Python interpreter, no native toolchains. It’s almost as good as having Wasm built into Python itself. In my tests it’s 3x–10x faster than wasm3, so for my first use case the situation is even better. The catch is that it currently weighs ~18MiB (installed), and in the future will likely rival the Python interpreter itself. The API also breaks on a monthly basis, so you’re signing up for the upgrade treadmill lest your own program perishes to bitrot after a couple of years. This article is about version 40.&lt;/p&gt;
    &lt;head rend="h3"&gt;Usage examples and gotchas&lt;/head&gt;
    &lt;p&gt;The official examples don’t do anything non-trivial or interesting, and so to figure things out I had to study the documentation, which does not offer many hints. Basic setup looks like this:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;import functools
import wasmtime

store    = wasmtime.Store()
module   = wasmtime.Module.from_file(store.engine, "example.wasm")
instance = wasmtime.Instance(store, module, ())
exports  = instance.exports(store)

memory = exports["memory"].get_buffer_ptr(store)
func1  = functools.partial(exports["func1"], store)
func2  = functools.partial(exports["func2"], store)
func3  = functools.partial(exports["func3"], store)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;A store is an allocation region from which we allocate all Wasm objects. It is not possible to free individual objects except to discard the whole store. Quite sensible, honestly. What’s not sensible is how often I have to repeat myself, passing the store back into every object in order to use it. These objects are associated with exactly one store and cannot be used with different stores. Use the wrong store and it panics: It’s already keeping track internally! I do not understand why the interface works this way. So to make things simpler, I use &lt;code&gt;functools.partial&lt;/code&gt; to
bind the &lt;code&gt;store&lt;/code&gt; parameter and so get the interface I expect.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;get_buffer_ptr&lt;/code&gt; object is a buffer protocol object, and if you’re
moving anything other than bytes that’s probably what you want to use to
access memory. The usual caveats apply for this object: If you change the
memory size you probably want to grab a fresh buffer object. For
bytes (e.g. buffers and strings) I prefer the &lt;code&gt;read&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt; methods.&lt;/p&gt;
    &lt;p&gt;Because multi-value is still in an experimental state in the Wasm ecosystem, you will likely not pass structs with Wasm. Anything more complicated than scalars will require pointers and copying data in and out of Wasm linear memory. This involves the usual trap that catches nearly everyone: Wasm interfaces make no distinction between pointers and integers, and Wasm runtimes interpret generally interpret all integers as signed. What that means is your pointers are signed unless you take action. Addresses start at 0, so this is bad, bad news.&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;malloc = functools.partial(exports["func1"], store)

hello = b"hello"
pointer = malloc(len(hello))
assert pointer
memory = exports["memory"].write(store, hello, pointer)  # WRONG!
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To make matters worse, wasmtime-py adds its own footgun: The &lt;code&gt;read&lt;/code&gt; and
&lt;code&gt;write&lt;/code&gt; methods adopt the questionable Python convention of negative
indices acting from the end. If &lt;code&gt;malloc&lt;/code&gt; returns a pointer in the upper
half of memory, the negative pointer will pass the bounds check inside
&lt;code&gt;write&lt;/code&gt; because negative is valid, then quietly store to the wrong
address! Doh!&lt;/p&gt;
    &lt;p&gt;I wondered how common this error, so I searched online. I could find only one non-trivial wasmtime-py use in the wild, in a sandboxed PDF reader. It falls into the negative pointer trap as I expected. Not only that, it’s a buffer overflow into Python’s memory space:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;            buf_ptr = malloc(store, len(pdf_data))
            mem_data = memory.data_ptr(store)

            for i, byte in enumerate(pdf_data):
                mem_data[buf_ptr + i] = byte
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;The &lt;code&gt;data_ptr&lt;/code&gt; method returns a non-bounds-checked raw &lt;code&gt;ctypes&lt;/code&gt; pointer,
so this is actually a double mistake. First, it shouldn’t trust pointers
coming out of Wasm if it cares at all about sandboxing. The second is the
potential negative pointer, which in this case would write outside of the
Wasm memory and in Python’s memory, hopefully seg-faulting.&lt;/p&gt;
    &lt;p&gt;What’s one to do? Every pointer coming out of Wasm must be truncated with a mask:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;pointer = malloc(...) &amp;amp; 0xffffffff   # correct for wasm32!
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;This interprets the result as unsigned. 64-bit Wasm needs a 64-bit mask, though in practice you will never get a valid negative pointer from 64-bit Wasm. This rule applies to JavaScript as well, where the idiom is:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;let pointer = malloc(...) &amp;gt;&amp;gt;&amp;gt; 0
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Wasm runtimes cannot help — they lack the necessary information — and this is perhaps a fundamental flaw in Wasm’s design. Once you know about it you see this mistake happening everywhere.&lt;/p&gt;
    &lt;p&gt;Now that you have a proper address, you can apply it to a buffer protocol view of memory. If you’re using NumPy there are various ways to interact with this memory by wrapping it in NumPy types, though only if you’re on a little endian host. (If you’re on a big endian machine, just give up on running Wasm anyway.) The first use case I have in mind typically involves copying plain Python values in and out. The &lt;code&gt;struct&lt;/code&gt; package is
quite handy here:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;vec2   = malloc(...) &amp;amp; 0xffffffff
memory = exports["memory"].get_buffer_ptr(store)
struct.pack_into("&amp;lt;ii", memory, vec2, x, y)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;It fills a similar role to JavaScript &lt;code&gt;DataView&lt;/code&gt;. If you’re copying
lots of numbers, with CPython it’s faster to construct a custom format
string rather than use a loop:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;nums: list[int] = ...
struct.pack_into(f"&amp;lt;{len(nums)}i", memory, buf, *nums)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;To copy structures back out, use &lt;code&gt;struct.unpack_from&lt;/code&gt;. If you’re moving
strings, you’ll need to &lt;code&gt;.encode()&lt;/code&gt; and &lt;code&gt;.decode()&lt;/code&gt; to convert to and from
&lt;code&gt;bytes&lt;/code&gt;, which are well-suited to &lt;code&gt;read&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In practice with real Wasm programs you’re going to be interacting with the “guest” allocator from the outside, to request memory into which you copy inputs for a function. In my examples I’ve used &lt;code&gt;malloc&lt;/code&gt; because it
requires no elaboration, but as usual a bump allocator solves
this so much better, especially because it doesn’t require stuffing a
whole general purpose allocator inside the Wasm program. Have one global
arena — no other threads will sharing that Wasm instance — rapid fire a
bunch of allocations as needed without any concern for memory management
in the “host”, call the function, which might allocate a result from that
arena, then reset the arena to clean up. In essence a stack for passing
values in and out.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly as faster Python&lt;/head&gt;
    &lt;p&gt;Suppose we noticed a computational hot spot in our Python program in a pure Python function (e.g. not calling out to an extension). Optimizing this function would be wise. Based on my experiments if I re-implement that function in C, compile it to Wasm, then run that bit of Wasm in place of the original function, I can expect around a 10x speed-up. In general C is more like 100x faster than Python, and the overhead of interfacing with Wasm — copying stuff in and out, etc. — can be high, but not so high as to not be profitable. This improves further if I can change the interface, e.g. require callers to use the buffer protocol.&lt;/p&gt;
    &lt;p&gt;Thanks to wasmtime-py, I could introduce this change without fussing with cross-compilers to build distribution binaries, nor require a toolchain on the target, just a hefty Python package. Might be worth it.&lt;/p&gt;
    &lt;p&gt;My main experimental benchmark is a variation on my solution to the “Two Sum” problem, which I originally wrote for JavaScript, then extended to pywasm3 and later wasmtime-py. It’s simple, just interesting enough, and representative of the sort of Wasm drop-in I have in mind. It has the same interface, but implements it with Wasm.&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;# Original Pythonic interface
def twosum(nums: list[int], target: int) -&amp;gt; tuple[int, int] | None:
    ...

# Stateful Wasm interface
class TwoSumWasm():
    def __init__(self):
        store    = wasmtime.Store()
        module   = wasmtime.Module.from_file(store.engine, ...)
        instance = wasmtime.Instance(store, module, ())
        ...

    def twosum(self, nums, target):
        # ... use wasm instance ...
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;There’s some state to it with the Wasm instance in tow. If you hide that by making it global you’ll need to synchronize your threads around it. In a multi-threaded program perhaps these would be lazily-constructed thread locals. I haven’t had to solve this yet.&lt;/p&gt;
    &lt;p&gt;However, the weakness of the wasmtime “store” really shows: Notice how compilation and instantiation are bound together in one store? I cannot compile once and then create disposable instances on the fly, e.g. as required for each run of a WASI program. Every instance permanently extends the compilation store. In practice we must wastefully re-compile the Wasm program for each disposable instance. Despite appearances, compilation and instantiation are not actually distinct steps, as they are in JavaScript’s Wasm API. &lt;code&gt;wasmtime.Instance&lt;/code&gt; accepts a store as its first
argument, suggesting use of a different store for instantiation. That
would solve this problem, but as of this writing it must be the same
store used to compile the module. This is a fatal flaw for certain real
use cases, particularly WASI.&lt;/p&gt;
    &lt;head rend="h3"&gt;WebAssembly as embedded capabilities&lt;/head&gt;
    &lt;p&gt;Loup Vaillant’s Monocypher is a wonderful cryptography library. Lean, efficient, and embedding-friendly, so much so it’s distributed in amalgamated form. It requires no libc or runtime, so we can compile it straight to Wasm with almost any Clang toolchain:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;$ clang --target=wasm32 -nostdlib -O2 -Wl,--no-entry -Wl,--export-all
        -o monocypher.wasm monocypher.c
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;It’s not “Wasm-aware” so I need &lt;code&gt;--export-all&lt;/code&gt; to expose the interface.
This is swell because, as single translation unit, anything with external
linkage is the interface. Though remember what I said about interacting
with the guest allocator? This has no allocator, nor should it. It’s not
so usable in this form because we’d need to manage memory from the
outside. Do-able, but it’s easy to improve by adding a couple more
functions, sticking to a single translation unit:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;#include "monocypher.c"

extern char  __heap_base[];
static char *heap_used;
static char *heap_high;

void *bump_alloc(ptrdiff_t size)
{
    // ...
}

void bump_reset()
{
    ptrdiff_t len = heap_used - __heap_base;
    __builtin_memset(__heap_base, 0, len);  // wipe keys, etc.
    heap_used = __heap_base;
}
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;I’ve discussed &lt;code&gt;__heap_base&lt;/code&gt; before, which is part of the ABI.
We’ll push keys, inputs, etc. onto this “stack”, run our cryptography
routine, copy out the result, then reset the bump allocator, which wipes
out all sensitive data. Often &lt;code&gt;memset&lt;/code&gt; is insufficient — typically it’s
zero-then-free, and compilers see the lifetime about to end — but no
lifetime ends here, and stores to this “heap” memory externally observable
as far as the abstract machine can tell. (Otherwise we couldn’t reliably
copy out our results!)&lt;/p&gt;
    &lt;p&gt;There’s a lot to this API, but I’m only going to look at the AEAD interface. We “lock” up some data in an encrypted box, write any unencrypted label we’d like on the outside. Then later we can unlock the box, which will only open for us if neither the contents of the box nor the label were tampered with. That’s some solid API design:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;void crypto_aead_lock(uint8_t       *cipher_text,
                      uint8_t        mac  [16],
                      const uint8_t  key  [32],
                      const uint8_t  nonce[24],
                      const uint8_t *ad,         size_t ad_size,
                      const uint8_t *plain_text, size_t text_size);
int crypto_aead_unlock(uint8_t       *plain_text,
                       const uint8_t  mac  [16],
                       const uint8_t  key  [32],
                       const uint8_t  nonce[24],
                       const uint8_t *ad,          size_t ad_size,
                       const uint8_t *cipher_text, size_t text_size);
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;By compiling to Wasm we can access this functionality from Python almost like it was pure Python, and interact with other systems using Monocypher.&lt;/p&gt;
    &lt;p&gt;Since Monocypher does not interact with the outside world on its own, it relies on callers to use their system’s CSPRNG to create those nonces and keys, which we’ll do using the &lt;code&gt;secrets&lt;/code&gt; built-in package:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;class Monocypher:
    def __init__(self):
        ...
        self._read   = functools.partial(memory.read, store)
        self._write  = functools.partial(memory.write, store)
        self.__alloc = functools.partial(exports["bump_alloc"], store)
        self._reset  = functools.partial(exports["bump_reset"], store)
        self._lock   = functools.partial(exports["crypto_aead_lock"], store)
        self._unlock = functools.partial(exports["crypto_aead_unlock"], store)
        self._csprng = secrets.SystemRandom()

    def _alloc(self, n):
        return self.__alloc(n) &amp;amp; 0xffffffff

    def generate_key(self):
        return self._csprng.randbytes(32)

    def generate_nonce(self):
        return self._csprng.randbytes(24)

    ...
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;With a solid foundation, all that follows comes easily. A &lt;code&gt;finally&lt;/code&gt;
guarantees secrets are always removed from Wasm memory, and the rest is
just about copying bytes around:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    def aead_lock(self, text, key, ad = b""):
        assert len(key) == 32
        try:
            macptr   = self._alloc(16)
            keyptr   = self._alloc(32)
            nonceptr = self._alloc(24)
            adptr    = self._alloc(len(ad))
            textptr  = self._alloc(len(text))

            self._write(key, keyptr)
            nonce = self.generate_nonce()
            self._write(nonce, nonceptr)
            self._write(ad,    adptr)
            self._write(text,  textptr)

            self._lock(
                textptr,
                macptr,
                keyptr,
                nonceptr,
                adptr, len(ad),
                textptr, len(text),
            )
            return (
                self._read(macptr, macptr+16),
                nonce,
                self._read(textptr, textptr+len(text)),
            )
        finally:
            self._reset()
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;And &lt;code&gt;aead_unlock&lt;/code&gt; is basically the same in reverse, but throws if the box
fails to unlock, perhaps due to tampering:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;    def aead_unlock(self, text, mac, key, nonce, ad = b""):
        assert len(mac) == 16
        assert len(key) == 32
        assert len(nonce) == 24
        try:
            macptr   = self._alloc(16)
            keyptr   = self._alloc(32)
            nonceptr = self._alloc(24)
            adptr    = self._alloc(len(ad))
            textptr  = self._alloc(len(text))

            self._write(mac, macptr)
            self._write(key, keyptr)
            self._write(nonce, nonceptr)
            self._write(ad, adptr)
            self._write(text, textptr)

            if self._unlock(
                textptr,
                macptr,
                keyptr,
                nonceptr,
                adptr, len(ad),
                textptr, len(text),
            ):
                raise ValueError("AEAD mismatch")
            return self._read(textptr, textptr+len(text))
        finally:
            self._reset()
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Usage:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;mc = Monocypher()
key = mc.generate_key()
message = "Hello, world!"
mac, nonce, encrypted = mc.aead_lock(message.encode(), key)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Transmit &lt;code&gt;mac&lt;/code&gt;, &lt;code&gt;nonce&lt;/code&gt;, and &lt;code&gt;encrypted&lt;/code&gt; to the other party (or your
future self), who already has the &lt;code&gt;key&lt;/code&gt;:&lt;/p&gt;
    &lt;div&gt;
      &lt;div&gt;
        &lt;quote&gt;
          &lt;code&gt;decrypted = mc.aead_unlock(encrypted, mac, key, nonce)
&lt;/code&gt;
        &lt;/quote&gt;
      &lt;/div&gt;
    &lt;/div&gt;
    &lt;p&gt;Find the complete source in my scratch repository.&lt;/p&gt;
    &lt;p&gt;While I have a few reservations about wasmtime-py, it fascinates me how well this all works. It’s been my hammer in search of a nail for some time now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46458624</guid><pubDate>Thu, 01 Jan 2026 22:09:16 +0000</pubDate></item><item><title>Why users cannot create Issues directly</title><link>https://github.com/ghostty-org/ghostty/issues/3558</link><description>&lt;doc fingerprint="d3525efe9c762c38"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 1.4k&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Description&lt;/head&gt;
    &lt;p&gt;Users are not allowed to create Issues directly in this repository - we ask that you create a Discussion first.&lt;/p&gt;
    &lt;p&gt;Unlike some other projects, Ghostty does not use the issue tracker for discussion or feature requests. Instead, we use GitHub discussions for that. Once a discussion reaches a point where a well-understood, actionable item is identified, it is moved to the issue tracker. This pattern makes it easier for maintainers or contributors to find issues to work on since every issue is ready to be worked on.&lt;/p&gt;
    &lt;p&gt;This approach is based on years of experience maintaining open source projects and observing that 80-90% of what users think are bugs are either misunderstandings, environmental problems, or configuration errors by the users themselves. For what's left, the majority are often feature requests (unimplemented features) and not bugs (malfunctioning features). Of the features requests, almost all are underspecified and require more guidance by a maintainer to be worked on.&lt;/p&gt;
    &lt;p&gt;Any Discussion which clearly identifies a problem in Ghostty and can be confirmed or reproduced will be converted to an Issue by a maintainer, so as a user finding a valid problem you don't do any extra work anyway. Thank you.&lt;/p&gt;
    &lt;p&gt;For more details, see our CONTRIBUTING.md.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460319</guid><pubDate>Fri, 02 Jan 2026 01:24:51 +0000</pubDate></item><item><title>Extensibility: The "100% Lisp" Fallacy</title><link>https://kyo.iroiro.party/en/posts/100-percent-lisp/</link><description>&lt;doc fingerprint="8622d95d221f3adc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Extensibility: The â100% Lispâ Fallacy&lt;/head&gt;
    &lt;p&gt;So, Iâve seen some articles promoting Emacs-like editors written in Lisp languages, and one of the most common arguments seems to be: âitâs written in This Lisp and also scriptable in This Lisp, and that gives it great extensibility.â 1&lt;/p&gt;
    &lt;p&gt;Itâs not wrong, but I think it does overlook a few things.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;By the way: Happy New Year!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;1. The argumentÂ¶&lt;/head&gt;
    &lt;p&gt;For example, the Lem: An Alternative To Emacs? article from Irreal claims: (emphasis preserved)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;One thing I like about it is that itâs 100% Common Lisp. Thereâs no C core; just Lisp all the way down. That makes it easier to customize or extend any part of the editor.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt; This argument sounds good: Looking at the repository, Lem is &lt;del&gt;100%&lt;/del&gt; 90% Lisp code; since the editor code and user customization live in the Common Lisp runtime, we should be able to extend any part of the editor on the fly, right? &lt;/p&gt;
    &lt;p&gt;Or, does it really?&lt;/p&gt;
    &lt;p&gt; Does it offer &lt;code&gt;composition-function-table&lt;/code&gt; so that you can program your font ligatures from the editorâs scripting language? &lt;/p&gt;
    &lt;p&gt;Does it provide an API to define an arbitrary encoding system and the corresponding charset, beyond what is supported by Unicode or any existing standard?&lt;/p&gt;
    &lt;p&gt;Does it allow you to âoverrideâ its newline character, so that a file is displayed all on a single line?&lt;/p&gt;
    &lt;p&gt;â¦&lt;/p&gt;
    &lt;p&gt;These examples are taken from some of the more obscure features in Emacs, and I can go on and on. I donât think many of the editors out there could possibly support them, as they are probably the 10% non-pure part of a â100% Lispâ system.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;To be honest, I hate these features as theyâve haunted me forever since I started designing an IPC protocol for my Emacs clone. But, dang it, I suffered and suffer from it exactly because of Emacsâs extensibility â not the lack of it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;2. There is no â100% LispâÂ¶&lt;/head&gt;
    &lt;p&gt;For example, Steel Bank Common Lisp, a common Common Lisp runtime, is only mostly written in Lisp because it has to provide threading primitives, interface with the OS, or leverage assembly code. And obviously you wonât be able to customize those bits.&lt;/p&gt;
    &lt;p&gt;Going back to (graphical) editors this is no less true. Usually2, as is with any GUI program, you will want to support font fallback, input methods and screen readers, all of which require interacting with platform specific APIs and are thus much less customizable. FFI helps to some degree by keeping your niche âpureâ, but it canât extend your customizability beyond that boundary: webviews donât expose font ligature internals, and CSS is the only way to control that, albeit quite limited; input methods are trickier, as they interfere with keyboard events and are platform-specific; screen readers are â¦ I donât know, you should really use a library for it if you want portability.&lt;/p&gt;
    &lt;p&gt;Anyway, itâs just impossible these days to have a âpureâ Lisp program with all those platform specific things to deal with, or with all those convenient library bindings to use. Itâs not inherently bad, but it certainly limits how youâre allowed to extend things.&lt;/p&gt;
    &lt;p&gt;However, even with the non-pure parts, provided with suitable building blocks, itâs usually quite surprising to what degree people can work around all those ânon-extensibleâ parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. Workaround-ish extensibilityÂ¶&lt;/head&gt;
    &lt;p&gt;Letâs first have a look at some approaches people adopt to extend their editors:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Neovim and many TUI editors donât have native scrollbar support since they are bounded by what ANSI provides. But guess what? By coloring the rightmost column with &lt;code&gt;extmark&lt;/code&gt;and&lt;code&gt;virt_text&lt;/code&gt;, itâs totally doable to display a pretty scrollbar for Neovim.&lt;/item&gt;
      &lt;item&gt;Stock Emacs does not support cursor animations. And yet, youâve guessed it, people: &lt;list rend="ul"&gt;&lt;item&gt;Come up with patches for this,&lt;/item&gt;&lt;item&gt;Or extend Lisp code with Python, which in turn calls PyQt or compositor commands to control overlayed windows, so as to display things like moving cursors.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Similarly, Emacs Application Framework allows you to write GUI programs for Emacs by â¦ first programming them in PyQt and then sticking them onto the Emacs window, so that the PyQt window âfillsâ the corresponding buffer window.&lt;/p&gt;
        &lt;p&gt;Not all Wayland compositors provide a way to programmatically position user windows, and that can be a problem for EAF.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;And, have you heard of EXWM (Emacs X Window Manager)?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The moral here is that a lot of things are more extensible than one might think. Itâs just amazing how people keep coming up with all kinds of workarounds all the time. And yes, I do think those are all extensibility, regardless of âpurityâ.&lt;/p&gt;
    &lt;head rend="h2"&gt;4. âSpacebar heatingâ extensibilityÂ¶&lt;/head&gt;
    &lt;p&gt;Compared to extending via workarounds, extending in âpure Lispâ can be both easier and harder, as we are still bounded by coding conventions and existing code, and one cannot possibly extend everything without breaking some of them.&lt;/p&gt;
    &lt;p&gt; Letâs start by overriding a single function. For example, when exporting Org-mode files to HTML, Org-mode defaults to generating random HTML ID anchors. To change that, you just override the &lt;code&gt;org-export-get-reference&lt;/code&gt; function that generates the IDs, right? &lt;/p&gt;
    &lt;code&gt;(advice-add #'org-export-get-reference :around #'org-html-stable-ids--get-reference)
&lt;/code&gt;
    &lt;p&gt; Oh no! It turns out that, sometimes Org-mode directly calls &lt;code&gt;org-html--reference&lt;/code&gt;, bypassing our override. That means we also need to redirect &lt;code&gt;org-html--reference&lt;/code&gt;: &lt;/p&gt;
    &lt;code&gt;(advice-add #'org-html--reference :override #'org-html-stable-ids--reference)
&lt;/code&gt;
    &lt;p&gt; Problem solved? No. Conventionally, Emacs Lisp code uses double dashes to tell the users âthis function is internalâ, as is in the &lt;code&gt;org-html--reference&lt;/code&gt; name. Yes, by being free to extend any part of the editor, you are free to modify any internal functions or states, in a way that may or may not be problematic under specific circumstances, with code that can be broken in any future updates. &lt;/p&gt;
    &lt;p&gt;And itâs not the end of it. The el-patch package allows you to apply âpatchesâ on most any Lisp code to modify behaviours nested deep inside a function:&lt;/p&gt;
    &lt;code&gt;;; Original function
(defun company-statistics--load ()
  "Restore statistics."
  (load company-statistics-file 'noerror nil 'nosuffix))

;; Patching
(el-patch-feature company-statistics)
(with-eval-after-load 'company-statistics
  (el-patch-defun company-statistics--load ()
                  "Restore statistics."
                  (load company-statistics-file 'noerror
                        ;; The patch
                        (el-patch-swap nil 'nomessage)
                        'nosuffix)))

;; Patched version
(defun company-statistics--load ()
  "Restore statistics."
  (load company-statistics-file 'noerror 'nomessage 'nosuffix))
&lt;/code&gt;
    &lt;p&gt; Luckily, el-patch provides &lt;code&gt;el-patch-validate&lt;/code&gt; so that you can worry less about your patches going ineffective or unexpectedly destructive. But you still need to maintain all your patches if anything goes wrong. &lt;/p&gt;
    &lt;p&gt;Any extensible system is not void of these problems. If you impose strong enough encapsulation, then eventually something canât get customized; if you expose everything, well, good luck keeping backward compatibility (as the system maintainer) or forward compatibility (as the user doing your modifications). By making it possible to âextend any part of the editor,â you are literally making any part of your code unextensible, and now âevery change breaks someoneâs workflow.â&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Emacsâs cross-language isolation/API might not be perfect, but Iâm very grateful for it. If Emacs were written in pure Lisp code and anything is extensible, my work-in-progress Emacs clone couldnât be remotely possible (because we do want to rid some of the spacebar heating problems while keeping some compatibility).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;5. Extensibility takes effortsÂ¶&lt;/head&gt;
    &lt;p&gt;Allow me to be blunt: the â100% Lispâ argument is lazy marketing. Writing a Lisp-extended editor in Lisp wonât immediately make your editor more extensible. Extensibility comes from careful designing of your API interfaces, it comes from learning from your history, listening to user needs, and, after all these, taking the effort and time actually write the interfacing code.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;This blog entry was adapted from a rant thread I posted to vent my shock about Emacsâs&lt;/p&gt;&lt;code&gt;composition-function-table&lt;/code&gt;. Emacs is just never short of wonders and surprises, especially when youâre creating your own Emacsen by replicating Emacs.&lt;p&gt;By the way, this post is not against Lem, from which Iâve got a lot of inspirations and seen quite some good designs. But, itâs still the most convenient example I can think of. So, pardon me!&lt;/p&gt;&lt;/quote&gt;
    &lt;head rend="h2"&gt;6. FootnotesÂ¶&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;âI want to write a better editor.â&lt;/cell&gt;
        &lt;cell&gt;ð With better accessibility, right?&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;(silent gaze)&lt;/cell&gt;
        &lt;cell&gt;ð With better accessibility, right?&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Also, please donât mistake maintainability for extensibility. For example, Racket has migrated from its previous C core to a Racket core powered by Chez Scheme. Quoting from Rebuilding Racket on Chez Scheme Experience Report:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;â¦ itâs hard to put a number on these things but I can give you one number at least. This is the number of people who have been willing to modify the Racket macro expander.&lt;/p&gt;
      &lt;p&gt;This is when it was in C: two of us did it. And itâs already six people (after the C-to-Chez migration). Remember: the C part that was in the first 16 years and the last 16 years of that implementation, and the six people and the new implementation is just in two years. So weâre really pretty sure itâs gonna be better to maintain.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;So I do believe a mostly Lispy codebase can be better maintained and potentially attract more contributors than one in C. (But Emacs is mostly Lispy anyway.)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460394</guid><pubDate>Fri, 02 Jan 2026 01:36:25 +0000</pubDate></item><item><title>Happy Public Domain Day 2026</title><link>https://publicdomainreview.org/blog/2026/01/public-domain-day-2026/</link><description>&lt;doc fingerprint="3081d05f80b58200"&gt;
  &lt;main&gt;
    &lt;p&gt;The calendar turns, and once again a lively procession of books, images, films, and music leaves copyright behind and steps into the ever-growing public domain! On this year's Public Domain Day (which falls each January 1st) we welcome, in lots of countries around the world, the words of Wallace Stevens, Thomas Mann, Hannah Arendt, and Albert Einstein, and in the US a bevy of brilliant books including William Faulkner’s As I Lay Dying, Langston Hughes’ Not Without Laughter, Agatha Christie’s The Murder at the Vicarage, and, in their original German, Robert Musil’s The Man Without Qualities and Hermann Hesse’s Narcissus and Goldmund.&lt;/p&gt;
    &lt;p&gt;Due to differing copyright laws around the world, there is no one single public domain, but there are three main types of copyright term for historical works which cover most cases. For these three systems, newly entering the public domain today are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;works by people who died in 1955, for countries with a copyright term of “life plus 70 years” (relevant in UK, most of the EU, and South America);&lt;/item&gt;
      &lt;item&gt;works by people who died in 1975, for countries with a term of “life plus 50 years” (relevant to most of Africa and Asia);&lt;/item&gt;
      &lt;item&gt;films and books (incl. artworks featured) published in 1929 (relevant solely to the United States).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some of you may have been following our advent-style countdown calendar which revealed day-by-day through December our highlights for these new public domain entrants. The last window was opened yesterday, and while such a format was fun for the slow reveal, for the sake of a good gorgeable list we’ve exploded the calendar out into a digestible array below. Enjoy!&lt;/p&gt;
    &lt;head rend="h2"&gt;Entering the public domain in the US&lt;/head&gt;
    &lt;head rend="h3"&gt;William Faulkner – As I Lay Dying&lt;/head&gt;
    &lt;p&gt;As I Lay Dying is a Southern Gothic novel by American author William Faulkner, consistently ranked among the best novels of the 20th century. The title is derived from William Marris’s 1925 translation of Homer’s Odyssey, referring to the similar themes of both works.&lt;lb/&gt;The novel traces the story of the death of Addie Bundren and her poor, rural family’s quest to honor her wish to be buried in her hometown of Jefferson, Mississippi, as well as the motives—noble or selfish—they show on the journey. It uses a stream-of-consciousness writing technique and varying chapter lengths, and is narrated by 15 different characters over 59 chapters.&lt;lb/&gt;Faulkner said that he wrote the novel from midnight to 4:00 a.m. over the course of six weeks and that he did not change a word of it. He spent the first eight hours of his twelve-hour shift at the University of Mississippi Power House shoveling coal or directing other works and the remaining four hours handwriting his manuscript on unlined onionskin paper. As I Lay Dying represents a progenitor of the Southern Renaissance, reflecting on being, existence, and other existential metaphysics of everyday life, and helped to solidify Faulkner’s reputation as a pioneer, like James Joyce and Virginia Woolf, of stream of consciousness. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Arthur Ransome - Swallows and Amazons&lt;/head&gt;
    &lt;p&gt;Swallows and Amazons is a children’s adventure novel by English author Arthur Ransome. It is the first book in the Swallows and Amazons series, followed by Swallowdale.&lt;lb/&gt;Set in the summer of 1929 in England’s Lake District, the book relates the outdoor adventures and play of two families of children. These involve sailing, camping, fishing, exploration and piracy. The Walker children (John, Susan, Titty and Roger) are staying at a farm near a lake in the Lake District of England, during the school holidays. They sail a borrowed dinghy named Swallow and meet the Blackett children (Nancy and Peggy), who sail a dinghy named Amazon. When the children meet, they agree to join forces against a common enemy – the Blacketts’ uncle Jim Turner whom they call “Captain Flint” (after the parrot in Treasure Island).&lt;lb/&gt;The book was inspired by a summer spent by Ransome teaching the children of his friends, the Altounyans, to sail. At the time, Ransome had been working as a journalist with the Manchester Guardian, but decided to become a full-time author rather than go abroad as a foreign correspondent. Three of the Altounyan children’s names are adopted directly for the Walker family. However, later in life Ransome tried to downplay the Altounyan connections, changing the initial dedication of Swallows and Amazons and writing a new foreword which gave other sources. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Nan Shepherd – The Weatherhouse&lt;/head&gt;
    &lt;p&gt;The Weatherhouse is the second novel by Anna “Nan” Shepherd, a Scottish modernist writer and poet. The novel concerns interactions between people in a small rural Scottish community. It belongs to the great line of Scottish fiction dealing with the complex interactions of small communities, and especially the community of women — a touching and hilarious network of mothers, daughters, spinsters and widows. It is also a striking meditation on the nature of truth, the power of human longing and the mystery of being.&lt;lb/&gt;Shepherd published three works of fiction. Her short non-fiction book The Living Mountain, inspired by her love for hillwalking, is the book for which she is best known and has been quoted as an influence by prominent nature writers. The landscape and weather of this area play a major role in her novels and provide a focus for her poetry.&lt;lb/&gt;Shepherd’s fiction brings out the sharp conflict between the demands of tradition and the pull of modernity, particularly in women’s lives. All three novels assign a major role to the landscape and weather in small northern Scottish communities they describe. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Langston Hughes – Not Without Laughter&lt;/head&gt;
    &lt;p&gt;Not Without Laughter* is the debut novel of Langston Hughes, the American writer, activist, and leader of the Harlem Renaissance.&lt;lb/&gt;The novel portrays African-American life in Kansas in the 1910s, focusing on the effects of class and religion on the community. In telling the story of Sandy Rogers, a young African American boy in small-town Kansas, and of his family—his mother, Annjee, a housekeeper for a wealthy white family; his irresponsible father, Jimboy, who plays the guitar and travels the country in search of employment; his strong-willed grandmother Hager, who clings to her faith; his Aunt Tempy, who marries a rich man; and his Aunt Harriet, who struggles to make it as a blues singer—Hughes gives the longings and lineaments of Black life in the early twentieth century an important place in the history of racially divided America.&lt;lb/&gt;Hughes said that *Not Without Laughter* is semi-autobiographical, and that a good portion of the characters and setting included in the novel are based on his memories of growing up in Lawrence, Kansas. A review in *The New York Times* said that the novel is “very slow, even tedious, reading in its early chapters, but once it gains its momentum it moves as swiftly as a jazz rhythm”. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Hermann Hesse – Narcissus and Goldmund&lt;/head&gt;
    &lt;p&gt;Narcissus and Goldmund (in German, Narziß und Goldmund), also published in English as Death and the Lover, is a novel written by the German-Swiss author Hermann Hesse. At its publication, it was considered Hesse’s literary triumph.&lt;lb/&gt;The novel is the story of a young man, Goldmund (German for “Gold mouth”), who wanders aimlessly throughout Medieval Germany after leaving a Catholic monastery school in search of what could be described as “the meaning of life”. With the help of Narcissus, a gifted young teacher, and following an epiphanic experience with a beautiful Gypsy woman, Goldmund leaves the monastery and embarks on a wandering existence. He has numerous love affairs, studies art, and encounters human existence at its ugliest when the Black Death devastates the region. Eventually, he is reunited with his friend Narcissus, now an abbot.&lt;lb/&gt;Like most of Hesse’s works, the main theme of this book is the wanderer’s struggle to find himself, as well as the Jungian union of polar opposites (Mysterium Coniunctionis). Goldmund represents nature and the “feminine conscious mind” (but also anima, a man’s unconscious), while Narcissus represents science and logic and God and the “masculine conscious mind” (but also animus, a woman’s unconscious).&lt;lb/&gt;A film adaptation, directed by the Austrian Oscar-winning director Stefan Ruzowitzky, was released in 2020. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;All Quiet on the Western Front (1930 film)&lt;/head&gt;
    &lt;p&gt;All Quiet on the Western Front is a 1930 American pre-Code epic anti-war film based on the 1929 novel of the same name by German novelist Erich Maria Remarque. Directed by Lewis Milestone, it stars Lew Ayres, Louis Wolheim, John Wray, Slim Summerville, and William Bakewell.&lt;lb/&gt;The movie follows a group of German students moved to enlist in the army as part of the new 2nd Company. Their romantic delusions are quickly shattered during their brief but rigorous training under the abusive Sergeant Himmelstoss. After being sent to the Western Front, their idealism is destroyed by the harsh realities of combat.&lt;lb/&gt;Considered a realistic and harrowing account of warfare in World War I, the film opened to wide acclaim in the United States and made the American Film Institute’s first 100 Years... 100 Movies list in 1997. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page&lt;/p&gt;
    &lt;head rend="h3"&gt;Evelyn Waugh – Vile Bodies&lt;/head&gt;
    &lt;p&gt;Vile Bodies is the second novel by Arthur Evelyn St. John Waugh, an English writer of novels, biographies, and travel books, and a prolific journalist and book reviewer. It satirises London’s post–First World War “bright young things” — a group of Bohemian young aristocrats and socialites in London — and the press coverage around them. Waugh originally considered the title Bright Young Things but changed it; the published title echoes a narrator’s remark on crowds and parties: “Those vile bodies”.&lt;lb/&gt;The novel follows a vivid assortment of characters, among them the struggling writer Adam Fenwick-Symes and the glamorous, aristocratic Nina Blount, who hunt fast and furiously for ever greater sensations and the hedonistic fulfillment of their desires. Waugh’s acidly funny satire reveals the darkness and vulnerability beneath the sparkling surface of the high life.&lt;lb/&gt;The book shifts in tone from light-hearted romp to bleak desolation (Waugh himself later attributed it to the breakdown of his first marriage halfway through the book’s composition). Critics have noted the novel’s fragmented scenes, jump-cuts, and telephone dialogue, often linking its method to cinema and to modernist effects. Some have defended the novel’s downbeat ending as a poetically just reversal of the conventions of comic romance.&lt;lb/&gt;David Bowie cited the novel as the primary influence in writing his song “Aladdin Sane”, and a film adaptation, written and directed by Stephen Fry, was released in 2003. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Margaret Ayer Barnes - Years of Grace&lt;/head&gt;
    &lt;p&gt;Years of Grace is the first book by the American playwright, novelist, and short-story writer Margaret Ayer Barnes. It won the Pulitzer Prize for the Novel in 1931.&lt;lb/&gt;The story, beginning in the 1890s and continuing into the 1930s, chronicles the life of Jane Ward Carver from her teens to age 54. This novel follows many of the same themes as Barnes’s other works. Centering on the social manners of upper middle class society, her female protagonists are often traditionalists, struggling to uphold conventional morality in the face of changing social climates. Barnes’s alma mater Bryn Mawr College, along with the characters of college presidents M. Carey Thomas and Marion Park, figure prominently in this work.&lt;lb/&gt;The New York Times commented that “this story of the death of an old order and the birth of a new one, of the perpetually renewed conflict between succeeding generations... holds the reader’s attention to the end.” Despite the success of Years of Grace, it is not Barnes’s best-known work; that honor belongs to Dishonored Lady, a play she co-wrote with Edward Sheldon, which was adapted twice into film. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read free ebook through Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Hellbound Train&lt;/head&gt;
    &lt;p&gt;Hell-Bound Train is a 1930 film written and directed by James and Eloyce Gist. A self-taught husband-and-wife team with a shared religious mission, they produced at least three silent films for African American church audiences, touring them across the United States. Shown alongside sermons, these works used cinema as a vehicle for evangelism. In Hell-Bound Train — which Eloyce is said to have rewritten, re-edited, and partly refilmed after James’s initial version — the viewer passes from carriage to carriage as the filmmakers stage various “Jazz Age” sins, including dancing, drinking, and gambling, all overseen by a mischievous devil conductor. Though Hell-Bound Train has gained some renewed attention via Kino Lorber’s Pioneers of African-American Cinema box set and a brief run on the Criterion Channel, this film — one of the few surviving silent works by an African American woman — is still often absent from retrospectives on early women filmmakers, perhaps because of its modest production values and overtly moralizing tone. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Watch on YouTube&lt;/p&gt;
    &lt;head rend="h3"&gt;Robert Musil – The Man Without Qualities&lt;/head&gt;
    &lt;p&gt;The Man Without Qualities (in German Der Mann ohne Eigenschaften) is an unfinished modernist novel in three volumes and various drafts, by the Austrian writer Robert Musil, published in parts from 1930 to 1943.&lt;lb/&gt;The novel is a “story of ideas”, which takes place in the time of the Austro-Hungarian monarchy’s last days. The plot often veers into allegorical digressions on a wide range of existential themes concerning humanity and feelings. It has a particular concern with the values of truth and opinion and how society organizes ideas about life and society. The book is well over a thousand pages long in its entirety, and no one single theme dominates.&lt;lb/&gt;The story takes place in 1913 in Vienna, the capital of Austria-Hungary, which Musil refers to by the playful term Kakanien. Part I, titled A Sort of Introduction, is an introduction to the protagonist, a mathematician named Ulrich whose ambivalence towards morals and indifference to life make him “a man without qualities”. In Part II, Pseudoreality Prevails, Ulrich joins preparations for a celebration in honor of 70 years of the Austrian Emperor Franz Joseph’s reign. Part III, entitled Into the Millennium (The Criminals), is about Ulrich’s sister Agathe. They experience a mystically incestuous stirring upon meeting after their father’s death.&lt;lb/&gt;Musil worked on the novel for more than twenty years: his detailed portrait of a decaying fin de siècle world has strong autobiographical features. Musil’s almost daily preoccupation with writing left his family in dire financial straits. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read German original on Project Gutenberg&lt;/p&gt;
    &lt;head rend="h3"&gt;T. S. Eliot – Ash Wednesday&lt;/head&gt;
    &lt;p&gt;Ash Wednesday is a long poem written by T. S. Eliot during his 1927 conversion to Anglicanism. Published in 1930, the poem deals with the struggle that ensues when one who has lacked faith in the past strives to move towards God.&lt;lb/&gt;Sometimes referred to as Eliot’s “conversion poem”, Ash Wednesday, with a base of Dante’s Purgatorio, is richly but ambiguously allusive and deals with the move from spiritual barrenness to hope for human salvation. The style is different from his poetry which predates his conversion. Ash Wednesday and the poems that followed had a more casual, melodic, and contemplative method.&lt;lb/&gt;The poem’s title comes from the Western Christian fast day that marks the beginning of Lent, forty days before Easter. It is a poem about the difficulty of religious belief, and concerned with personal salvation in an age of uncertainty. In it, Eliot’s poetic persona, one who has lacked faith in the past, has somehow found the courage, through spiritual exhaustion, to seek faith.&lt;lb/&gt;The initial reception of Ash Wednesday was largely positive, though many of the more secular literati found its groundwork of orthodox Christianity discomfiting. Edwin Muir maintained that “‘Ash Wednesday’ is one of the most moving poems he [Eliot] has written, and perhaps the most perfect”. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on English Verse and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Agatha Christie - The Murder at the Vicarage&lt;/head&gt;
    &lt;p&gt;The Murder at the Vicarage is a work of detective fiction by the British writer Agatha Christie. It is the first novel to feature the character of Miss Marple and her village of St Mary Mead (characters that had previously appeared in short stories).&lt;lb/&gt;The story is set in the quiet English village of St Mary Mead, where life is seemingly peaceful until Colonel Protheroe, the local magistrate and a widely disliked man, is found shot dead in the vicar’s study. The vicar, Leonard Clement, is the narrator of the story. Just before the murder, he had remarked that “anyone who murdered Colonel Protheroe would be doing the world a service” — a comment that comes back to haunt him.&lt;lb/&gt;Several suspects quickly emerge, as well as Miss Marple, who proves, though she appears at first as a nosy old spinster, to have unmatched observational skills and a deep understanding of human nature. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read free ebook through Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Franz Kafka - The Castle (english translation)&lt;/head&gt;
    &lt;p&gt;The Castle (in German, *Das Schloss*) is a 1926 novel by Franz Kafka. In it a protagonist known only as “K.” arrives in a village and struggles to gain access to the mysterious authorities who govern it from a castle supposedly owned by Count Westwest. Kafka died before he could finish the work, but suggested it would end with K. dying in the village, the castle notifying him on his death bed that his “legal claim to live in the village was not valid, yet, taking certain auxiliary circumstances into account, he was permitted to live and work there.” Dark and at times surreal, *The Castle* is often understood to be about alienation, unresponsive bureaucracy, the frustration of trying to conduct business with non-transparent, seemingly arbitrary controlling systems, and the futile pursuit of an unobtainable goal. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read free ebook through Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Sigmund Freud – Civilization and Its Discontents&lt;/head&gt;
    &lt;p&gt;Civilization and Its Discontents is a book by Sigmund Freud, the founder of psychoanalysis. It was written in 1929 and first published in German in 1930 as Das Unbehagen in der Kultur (“The Uneasiness in Civilization”).&lt;lb/&gt;Exploring what Freud saw as a clash between the desire for individuality and the expectations of society, the book is considered one of Freud’s most important and widely read works, and was described in 1989 by historian Peter Gay as one of the most influential and studied books in the field of modern psychology.&lt;lb/&gt;The book espouses a theory grounded in the notion that humans have certain characteristic instincts that are immutable. The primary tension originates from an individual attempting to find instinctive freedom, and civilization’s contrary demand for conformity and repression of instincts. Freud states that when any situation that is desired by the pleasure principle is prolonged, it creates a feeling of mild resentment as it clashes with the reality principle.&lt;lb/&gt;Primitive instincts—for example, the desire to kill and the insatiable craving for sexual gratification—are harmful to the collective wellbeing of a human community. The historical development of laws that prohibit violence, murder, rape, and adultery, he argued, is an inherent quality of civilization that gives rise to perpetual feelings of discontent among individuals. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Stella Benson - The Far-Away Bride&lt;/head&gt;
    &lt;p&gt;The Far-Away Bride is the most famous book by the English feminist, novelist, poet, and travel writer Stella Benson. It was published in the United States first in 1930 and as Tobit Transplanted in Britain in 1931. It won the Femina Vie Heureuse Prize for English writers in 1932.&lt;lb/&gt;The novel deals with a family of Russian émigrés in Manchuria. Its characters are the old, grumbling and tearfully sentimental Russian intellectual, Malinin; his disheveled, kind-hearted and unbearable wife, Anna; and Seryozha, their resourceful 19-year-old son. Spending their time in laziness, indulging in exaggerated Russian disorder and comical quarrels growing out of every trifle, they are incongruously happy. The humorous and adventurous action of the novel starts when Seryozha sets out, on foot, on a business trip to the Korean city of Seoul (where he must recover 200 yens); it is there that he finds his “far-away bride” — a charming and whimsical Russian girl who has already broken seven hearts and whose heart he finally conquers.&lt;lb/&gt;Benson described the novel as an “accurate modernization” of the Book of Tobit, a work of Second Temple Jewish literature dating to the 3rd or early 2nd century BC; The New York Times described The Far-Away Bride, rather, as a “spirited parody of it.” Benson’s novel, writes the reviewer, is “a truly felicitous comedy of the human personality”. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Vladimir Nabokov - The Defense&lt;/head&gt;
    &lt;p&gt;The Defense (in Russian, Zashchita Luzhinais) is the third novel written by Vladimir Nabokov after he had emigrated to Berlin. It appeared first under Nabokov’s pen name V. Sirin in the Russian émigré quarterly Sovremennye zapiski and was thereafter published by the émigré publishing house Slovo as The Luzhin Defense in Berlin.&lt;lb/&gt;The novel tells the story of Luzhin. As a young boy, unattractive, withdrawn, sullen, he takes up chess as a refuge from the anxiety of his everyday life. His talent is prodigious and he rises to the rank of grandmaster, but at a cost: in Luzhin’s obsessive mind, the game of chess gradually supplants the world of reality. His own world falls apart during a crucial championship match, when the intricate defense he has devised withers under his opponent’s unexpected and unpredictable lines of assault.&lt;lb/&gt;The character of Luzhin is based on Curt von Bardeleben, a chess master Nabokov knew personally, and Nabokov links the events in the central chapters to moves as encountered in chess problems. The book was adapted to film in 2000, as The Luzhin Defence. It was directed by Marleen Gorris, and starred John Turturro as Luzhin. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Dashiell Hammett – The Maltese Falcon&lt;/head&gt;
    &lt;p&gt;The Maltese Falcon is a detective novel by American writer Dashiell Hammett, originally serialized in the magazine Black Mask beginning with the September 1929 issue. The story is told entirely in external third-person narrative; there is no description whatsoever of any character’s thoughts or feelings, only what they say and do, and how they look. The novel has been adapted several times for the cinema and is considered part of the hardboiled genre, which Hammett played a major part in popularizing.&lt;lb/&gt;The novel follows Sam Spade, a private detective in San Francisco, in partnership with Miles Archer. The beautiful “Miss Wonderley” hires them to follow Floyd Thursby, who she claims has run off with her sister. Archer takes the first stint but is found shot dead that night. “Miss Wonderley” is soon revealed to be an acquisitive adventuress named Brigid O’Shaughnessy, who is involved in the search for a black statuette of unknown but substantial value. Red herrings abound.&lt;lb/&gt;Although Hammett himself worked for a time as a private detective for the Pinkerton Detective Agency in San Francisco (and used his given name, Samuel, for the story’s protagonist), Hammett asserted that “Spade has no original. He is a dream man in the sense that he is what most of the private detectives I worked with would like to have been, and, in their cockier moments, thought they approached.” (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read on Faded Page and Standard Books&lt;/p&gt;
    &lt;head rend="h3"&gt;Stanisław Ignacy Witkiewicz – Insatiability&lt;/head&gt;
    &lt;p&gt;Insatiability (in Polish Nienasycenie) is a speculative fiction novel by the Polish writer, dramatist, philosopher, painter and photographer, Stanisław Ignacy Witkiewicz (Witkacy). It is Witkiewicz’s third novel, considered by some to be his best.&lt;lb/&gt;Consisting of two parts — Przebudzenie (Awakening) and Obłęd (The Madness) — the novel takes place in the future, circa 2000. Following a battle, modeled after the Bolshevik revolution, Poland is overrun by the army of the last and final Mongol conquest. The nation becomes enslaved to the Chinese leader Murti Bing. His emissaries give everyone a special pill called DAVAMESK B 2 which takes away their abilities to think and to mentally resist. East and West become one, in faceless misery fueled by sexual instincts.&lt;lb/&gt;The book combines chaotic action with deep philosophical and political discussion, and predicts many of the events and political outcomes of the subsequent years, specifically, the invasion of Poland, the postwar foreign domination as well as the totalitarian mind control exerted, first by the Germans, and then by the Soviet Union on Polish life and art. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Read more about Witkiewicz’s artworks in our essay “Documenting Drugs” by Juliette Bretan&lt;/p&gt;
    &lt;head rend="h2"&gt;Entering the public domain in countries with a ‘life plus 70 year’ copyright term&lt;/head&gt;
    &lt;head rend="h3"&gt;Albert Einstein&lt;/head&gt;
    &lt;p&gt;Albert Einstein was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory. His mass–energy equivalence formula E = mc^2, which arises from special relativity, has been called “the world’s most famous equation”. He received the 1921 Nobel Prize in Physics for “his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect”.&lt;lb/&gt;In 1905, sometimes described as his *annus mirabilis* (miracle year), he published four groundbreaking papers. In them, he outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity, and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole. In 1917, Einstein introduced the concepts of spontaneous emission and stimulated emission, the latter of which is the core mechanism behind the laser and maser, and which helped lay the groundwork for later developments in physics such as quantum electrodynamics and quantum optics. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Wikisource&lt;/p&gt;
    &lt;head rend="h3"&gt;Wallace Stevens&lt;/head&gt;
    &lt;p&gt;Wallace Stevens was an American modernist poet. He was born in Reading, Pennsylvania, educated at Harvard and then New York Law School, and spent most of his life working as an executive for an insurance company in Hartford, Connecticut.&lt;lb/&gt;Stevens’s first period begins with the publication of Harmonium (1923), followed by a slightly revised and amended second edition in 1930. It features, among other poems, “The Emperor of Ice-Cream”, “Sunday Morning”, “The Snow Man”, and “Thirteen Ways of Looking at a Blackbird”. His second period commenced with Ideas of Order (1933), included in Transport to Summer (1947). His third and final period began with the publication of The Auroras of Autumn (1950), followed by The Necessary Angel: Essays On Reality and the Imagination (1951).&lt;lb/&gt;Many of Stevens’s poems deal with the making of art and poetry in particular. His Collected Poems (1954) won the Pulitzer Prize for Poetry in 1955 and Stevens is a rare example of a poet whose main output came largely only as he approached 40 years of age. His first major publication (four poems from a sequence titled “Phases” in the November 1914 edition of Poetry) was written at age 35, although as an undergraduate at Harvard, Stevens had written poetry and exchanged sonnets with Santayana. Many of his canonical works were written well after he turned 50. According to the literary scholar Harold Bloom, no Western writer since Sophocles has had such a late flowering of artistic genius. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Charlie Parker&lt;/head&gt;
    &lt;p&gt;Charles Parker Jr. was an American jazz saxophonist, bandleader, and composer. Parker was a highly influential soloist and leading figure in the development of bebop, a form of jazz characterized by fast tempos, virtuosic technique, and advanced harmonies. He was a virtuoso and introduced revolutionary rhythmic and harmonic ideas into jazz, including rapid passing chords, new variants of altered chords, and chord substitutions. Parker primarily played the alto saxophone.&lt;lb/&gt;Parker was an icon for the hipster subculture and later the Beat Generation, personifying the jazz musician as an uncompromising artist and intellectual rather than just an entertainer.&lt;lb/&gt;His style of composition involved interpolation of original melodies over existing jazz forms and standards, a practice known as contrafact and still common in jazz today. Examples include “Ornithology” (which borrows the chord progression of jazz standard “How High the Moon” and is said to be co-written with trumpet player Little Benny Harris), and “Moose The Mooche”. The practice was not uncommon prior to bebop, but it became a signature of the movement as artists began to move away from arranging popular standards and toward composing their own material. Parker contributed greatly to the modern jazz solo, one in which triplets and pick-up notes were used in unorthodox ways to lead into chord tones.&lt;lb/&gt;Miles Davis once said, “You can tell the history of jazz in four words: Louis Armstrong. Charlie Parker.” (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Thomas Mann&lt;/head&gt;
    &lt;p&gt;Paul Thomas Mann was a German novelist, short story writer, social critic, philanthropist, essayist, and the 1929 Nobel Prize in Literature laureate. His highly symbolic and ironic epic novels and novellas are noted for their insight into the psychology of the artist and the intellectual. His analysis and critique of the European and German soul used modernized versions of German and Biblical stories, as well as the ideas of Johann Wolfgang von Goethe, Friedrich Nietzsche, and Arthur Schopenhauer.&lt;lb/&gt;Mann was a member of the hanseatic Mann family and portrayed his family and class in his first novel, Buddenbrooks (1901). Further major novels include The Magic Mountain (1924), the tetralogy Joseph and His Brothers (1933–1943), and Doctor Faustus (1947); he also wrote short stories and novellas, including Death in Venice (1912).&lt;lb/&gt;When Adolf Hitler came to power in 1933, Mann fled to Switzerland and when World War II broke out in 1939, he moved to the United States, then returned to Switzerland in 1952. Mann is one of the best-known exponents of the so-called Exilliteratur, German literature written in exile by those who opposed the Hitler regime. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Project Gutenberg&lt;/p&gt;
    &lt;head rend="h3"&gt;Pierre Teilhard de Chardin&lt;/head&gt;
    &lt;p&gt;Pierre Teilhard de Chardin was a French Jesuit, Catholic priest, scientist, paleontologist, philosopher, mystic, and teacher. He investigated the theory of evolution from a perspective influenced by Henri Bergson and Christian mysticism, writing multiple scientific and religious works on the subject, his most popular being The Phenomenon of Man, published posthumously in 1955. His mainstream scientific achievements include his palaeontological research in China, taking part in the discovery of the significant Peking Man fossils from the Zhoukoudian cave complex near Beijing. His more speculative ideas, sometimes criticized as pseudoscientific, have included a vitalist conception of the Omega Point. Along with Vladimir Vernadsky, he contributed to the development of the concept of the noosphere.&lt;lb/&gt;In 1962, the Holy Office issued a warning regarding Teilhard’s works, alleging ambiguities and doctrinal errors without specifying them. Some eminent Catholic figures, including Pope Benedict XVI and Pope Francis, have made positive comments on some of his ideas since. The response to his writings by scientists has been divided. His work was controversial to some scientists and religious leaders because Teilhard combined theology and metaphysics with science.&lt;lb/&gt;Teilhard served in World War I as a stretcher-bearer. He received several citations, and was awarded the Médaille militaire and the Legion of Honor, the highest French order of merit, both military and civil. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Roger Mais&lt;/head&gt;
    &lt;p&gt;Roger Mais was a Jamaican journalist, novelist, poet, and playwright. He was born to a middle-class family in Kingston, Jamaica. By 1951, he had won ten first prizes in West Indian literary competitions. His integral role in the development of political and cultural nationalism is evidenced in his being awarded the high honour of the Order of Jamaica in 1978.&lt;lb/&gt;He worked at various times as a photographer, insurance salesman, and journalist, launching his journalistic career as a contributor to the weekly newspaper Public Opinion from 1939 to 1952. Mais published more than a hundred short stories, most appearing in Public Opinion and Focus, with others collected in Face and Other Stories and And Most of All Man. He wrote more than thirty stage and radio plays, as well as three novels: The Hills Were Joyful Together (1953), Brother Man (1954), and Black Lightning (1955).&lt;lb/&gt;Mais’ topics most frequently were the social injustice and inequality suffered by black, poor Jamaicans. Accused of sedition for writing the article “Now We Know,” a 1944 denunciation of the British Empire, the Jamaican novelist was tried, convicted and imprisoned for six months. His political activism, anti-colonial writing, and imprisonment helped galvanize Jamaican nationalism. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Saadat Hasan Manto&lt;/head&gt;
    &lt;p&gt;Saadat Hasan Manto was a Pakistani writer, playwright and novelist from Punjab, who is regarded as the greatest short-story author in Urdu literature. He was active from 1933 during British rule till his death in 1955 after independence.&lt;lb/&gt;Writing mainly in Urdu, he produced 22 collections of short stories, a novel, five series of radio plays, three collections of essays, and two collections of personal sketches. He is best known for his stories about the partition of India, which he opposed, immediately following independence in 1947. Manto’s most notable work has been archived by Rekhta.&lt;lb/&gt;Manto was tried six times for alleged obscenity in his writings; thrice before 1947 in British India, and thrice after independence in 1947 in Pakistan, but was never convicted. He started his literary career translating the works of Victor Hugo, Oscar Wilde and Russian writers such as Chekhov and Gorky. His first story was “Tamasha”, based on the Jallianwala Bagh massacre at Amritsar. His final works, which grew from the social climate and his own financial struggles, reflected an innate sense of human impotency towards darkness and contained satire that verged on dark comedy, as seen in his last story, “Toba Tek Singh”. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h2"&gt;Entering the public domain in countries with a ‘life plus 50 year’ copyright term&lt;/head&gt;
    &lt;head rend="h3"&gt;Barbara Hepworth&lt;/head&gt;
    &lt;p&gt;Dame Jocelyn Barbara Hepworth was an English artist and sculptor. Along with artists such as Ben Nicholson and Naum Gabo, Hepworth was a leading figure in the colony of artists who resided in St Ives during the Second World War. Born in Wakefield, Yorkshire, Hepworth studied at Leeds School of Art and the Royal College of Art in the 1920s. She married the sculptor John Skeaping in 1925. In 1931 she fell in love with the painter Ben Nicholson, and in 1933 divorced Skeaping. At this time she was part of a circle of modern artists centred on Hampstead, London, and was one of the founders of the art movement Unit One. At the beginning of the Second World War Hepworth and Nicholson moved to St Ives, Cornwall, where she would remain for the rest of her life. Best known as a sculptor, Hepworth also produced drawings – including a series of sketches of operating rooms following the hospitalisation of her daughter in 1944 – and lithographs. She died in a fire at her studio in 1975. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Hannah Arendt&lt;/head&gt;
    &lt;p&gt;Hannah Arendt was a German and American historian and philosopher. She was one of the most influential political theorists of the twentieth century.&lt;lb/&gt;Her works cover a broad range of topics, but she is best known for those dealing with the nature of wealth, power, fame, and evil, as well as politics, direct democracy, authority, tradition, and totalitarianism. She is also remembered for the controversy surrounding the trial of Adolf Eichmann, for her attempt to explain how ordinary people become actors in totalitarian systems, which was considered by some an apologia, and for the phrase “the banality of evil”.&lt;lb/&gt;In 1933, Arendt was briefly imprisoned by the Gestapo for performing illegal research into antisemitism. On release, she fled Germany, settling in Paris. There she worked for Youth Aliyah, assisting young Jews to emigrate to the British Mandate of Palestine. When Germany invaded France she was detained as an alien, but she escaped and made her way to the United States in 1941. She became a writer and editor and worked for the Jewish Cultural Reconstruction, becoming an American citizen in 1950. With the publication of The Origins of Totalitarianism in 1951, her reputation as a thinker and writer was established, and a series of works followed. These included the books The Human Condition in 1958, as well as Eichmann in Jerusalem and On Revolution in 1963. She taught at many American universities while declining tenure-track appointments. She died suddenly of a heart attack in 1975, leaving her last work, The Life of the Mind, unfinished. (Wikipedia)&lt;/p&gt;
    &lt;head rend="h3"&gt;Walker Evans&lt;/head&gt;
    &lt;p&gt;Walker Evans was an American photographer and photojournalist best known for his work for the Resettlement Administration and the Farm Security Administration (FSA) documenting the effects of the Great Depression. Evans’ published his first photos at the age of 27. Much of Evans’ New Deal work uses the large format, 8 × 10-inch (200×250 mm) view camera. He said that his goal as a photographer was to make pictures that are “literate, authoritative, transcendent”.&lt;lb/&gt;Many of his works are in the permanent collections of museums and have been the subject of retrospectives at such institutions as the Metropolitan Museum of Art or the George Eastman Museum.&lt;lb/&gt;Born in St. Louis, Missouri, Evans took up photography in 1928 around the time he was living in Ossining, New York. The Great Depression years of 1935–36 were a period of remarkable productivity and accomplishment for Evans. In 1936, employed by the National Recovery Administration, he photographed three impoverished sharecropper families in Hale County, Alabama. The photographs became iconic and were praised for effectively capturing the negative effects of the Great Depression in the American South. Between 1940 and 1959, Evans was awarded three Guggenheim Fellowships in Photography to continue his work of making record photographs of contemporary American subjects. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Library of Congress&lt;/p&gt;
    &lt;head rend="h3"&gt;P. G. Wodehouse&lt;/head&gt;
    &lt;p&gt;Sir Pelham Grenville Wodehouse was an English writer and one of the most widely read humorists of the 20th century. His creations include the feather-brained Bertie Wooster and his sagacious valet, Jeeves; the immaculate and loquacious Psmith; Lord Emsworth and the Blandings Castle set; the Oldest Member, with stories about golf; and Mr. Mulliner, with tall tales on subjects ranging from bibulous bishops to megalomaniac movie moguls.&lt;lb/&gt;Born in Guildford, his early novels were mostly school stories, but he later switched to comic fiction. Most of Wodehouse’s fiction is set in his native United Kingdom, although he spent much of his life in the US and used New York and Hollywood as settings for some of his novels and short stories. Wodehouse was a prolific writer throughout his life, publishing more than ninety books, forty plays, two hundred short stories and other writings between 1902 and 1974. Early in his career Wodehouse would produce a novel in about three months, but he slowed in old age to around six months. He used a mixture of Edwardian slang, quotations from and allusions to numerous poets, and several literary techniques to produce a prose style that has been compared to comic poetry and musical comedy. Some critics of Wodehouse have considered his work flippant, but among his fans are former British prime ministers and many of his fellow writers. (Wikipedia)&lt;/p&gt;
    &lt;p&gt;Works at Project Gutenberg&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460440</guid><pubDate>Fri, 02 Jan 2026 01:42:16 +0000</pubDate></item><item><title>Marmot – A distributed SQLite server with MySQL wire compatible interface</title><link>https://github.com/maxpert/marmot</link><description>&lt;doc fingerprint="f99204d0da0b98c"&gt;
  &lt;main&gt;
    &lt;p&gt;Marmot v2 is a leaderless, distributed SQLite replication system built on a gossip-based protocol with distributed transactions and eventual consistency.&lt;/p&gt;
    &lt;p&gt;Key Features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Leaderless Architecture: No single point of failure - any node can accept writes&lt;/item&gt;
      &lt;item&gt;MySQL Protocol Compatible: Connect with any MySQL client (DBeaver, MySQL Workbench, mysql CLI)&lt;/item&gt;
      &lt;item&gt;Distributed Transactions: Percolator-style write intents with conflict detection&lt;/item&gt;
      &lt;item&gt;Multi-Database Support: Create and manage multiple databases per cluster&lt;/item&gt;
      &lt;item&gt;DDL Replication: Distributed schema changes with automatic idempotency and cluster-wide locking&lt;/item&gt;
      &lt;item&gt;Production-Ready SQL Parser: Powered by rqlite/sql AST parser for MySQL→SQLite transpilation&lt;/item&gt;
      &lt;item&gt;CDC-Based Replication: Row-level change data capture for consistent replication&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Start a single-node cluster
./marmot-v2

# Connect with MySQL client
mysql -h localhost -P 3306 -u root

# Or use DBeaver, MySQL Workbench, etc.&lt;/code&gt;
    &lt;code&gt;# Test DDL and DML replication across a 2-node cluster
./scripts/test-ddl-replication.sh

# This script will:
# 1. Start a 2-node cluster
# 2. Create a table on node 1 and verify it replicates to node 2
# 3. Insert data on node 1 and verify it replicates to node 2
# 4. Update data on node 2 and verify it replicates to node 1
# 5. Delete data on node 1 and verify it replicates to node 2

# Manual cluster testing
./examples/start-seed.sh              # Start seed node (port 8081, mysql 3307)
./examples/join-cluster.sh 2 localhost:8081  # Join node 2 (port 8082, mysql 3308)
./examples/join-cluster.sh 3 localhost:8081  # Join node 3 (port 8083, mysql 3309)

# Connect to any node and run queries
mysql --protocol=TCP -h localhost -P 3307 -u root
mysql --protocol=TCP -h localhost -P 3308 -u root

# Cleanup
pkill -f marmot-v2&lt;/code&gt;
    &lt;p&gt;Marmot v2 uses a fundamentally different architecture from other SQLite replication solutions:&lt;/p&gt;
    &lt;p&gt;vs. rqlite/dqlite/LiteFS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;❌ They require a primary node for all writes&lt;/item&gt;
      &lt;item&gt;✅ Marmot allows writes on any node&lt;/item&gt;
      &lt;item&gt;❌ They use leader election (Raft)&lt;/item&gt;
      &lt;item&gt;✅ Marmot uses gossip protocol (no leader)&lt;/item&gt;
      &lt;item&gt;❌ They require proxy layer or page-level interception&lt;/item&gt;
      &lt;item&gt;✅ Marmot uses MySQL protocol for direct database access&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How It Works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Write Coordination: 2PC (Two-Phase Commit) with configurable consistency (ONE, QUORUM, ALL)&lt;/item&gt;
      &lt;item&gt;Conflict Resolution: Last-Write-Wins (LWW) with HLC timestamps&lt;/item&gt;
      &lt;item&gt;Cluster Membership: SWIM-style gossip with failure detection&lt;/item&gt;
      &lt;item&gt;Data Replication: Full database replication - all nodes receive all data&lt;/item&gt;
      &lt;item&gt;DDL Replication: Cluster-wide schema changes with automatic idempotency&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot v2 supports distributed DDL (Data Definition Language) replication without requiring master election:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Cluster-Wide Locking: Each DDL operation acquires a distributed lock per database (default: 30-second lease)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Prevents concurrent schema changes on the same database&lt;/item&gt;
          &lt;item&gt;Locks automatically expire if a node crashes&lt;/item&gt;
          &lt;item&gt;Different databases can have concurrent DDL operations&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Automatic Idempotency: DDL statements are automatically rewritten for safe replay&lt;/p&gt;
        &lt;quote&gt;CREATE TABLE users (id INT) → CREATE TABLE IF NOT EXISTS users (id INT) DROP TABLE users → DROP TABLE IF EXISTS users&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Schema Version Tracking: Each database maintains a schema version counter&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Incremented on every DDL operation&lt;/item&gt;
          &lt;item&gt;Exchanged via gossip protocol for drift detection&lt;/item&gt;
          &lt;item&gt;Used by delta sync to validate transaction applicability&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Quorum-Based Replication: DDL replicates like DML through the same 2PC mechanism&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;No special master node needed&lt;/item&gt;
          &lt;item&gt;Works with existing consistency levels (QUORUM, ALL, etc.)&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[ddl]
# DDL lock lease duration (seconds)
lock_lease_seconds = 30

# Automatically rewrite DDL for idempotency
enable_idempotent = true&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Do: Execute DDL from a single connection/node at a time&lt;/item&gt;
      &lt;item&gt;✅ Do: Use qualified table names (&lt;code&gt;mydb.users&lt;/code&gt;instead of&lt;code&gt;users&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Caution: ALTER TABLE is less idempotent - avoid replaying failed ALTER operations&lt;/item&gt;
      &lt;item&gt;❌ Don't: Run concurrent DDL on the same database from multiple nodes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot v2 uses Change Data Capture (CDC) for replication instead of SQL statement replay:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Row-Level Capture: Instead of replicating SQL statements, Marmot captures the actual row data changes (INSERT/UPDATE/DELETE)&lt;/item&gt;
      &lt;item&gt;Binary Data Format: Row data is serialized as CDC messages with column values, ensuring consistent replication regardless of SQL dialect&lt;/item&gt;
      &lt;item&gt;Deterministic Application: Row data is applied directly to the target database, avoiding parsing ambiguities&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Consistency: Same row data applied everywhere, no SQL parsing differences&lt;/item&gt;
      &lt;item&gt;Performance: Binary format is more efficient than SQL text&lt;/item&gt;
      &lt;item&gt;Reliability: No issues with SQL syntax variations between MySQL and SQLite&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For UPDATE and DELETE operations, Marmot automatically extracts row keys:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uses PRIMARY KEY columns when available&lt;/item&gt;
      &lt;item&gt;Falls back to ROWID for tables without explicit primary key&lt;/item&gt;
      &lt;item&gt;Handles composite primary keys correctly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot can publish CDC events to external messaging systems, enabling real-time data pipelines, analytics, and event-driven architectures. Events follow the Debezium specification for maximum compatibility with existing CDC tooling.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Debezium-Compatible Format: Events conform to the Debezium event structure, compatible with Kafka Connect, Flink, Spark, and other CDC consumers&lt;/item&gt;
      &lt;item&gt;Multi-Sink Support: Publish to multiple destinations simultaneously (Kafka, NATS)&lt;/item&gt;
      &lt;item&gt;Glob-Based Filtering: Filter which tables and databases to publish&lt;/item&gt;
      &lt;item&gt;Automatic Retry: Exponential backoff with configurable limits&lt;/item&gt;
      &lt;item&gt;Persistent Cursors: Survives restarts without losing position&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[publisher]
enabled = true

[[publisher.sinks]]
name = "kafka-main"
type = "kafka"                    # "kafka" or "nats"
format = "debezium"               # Debezium-compatible JSON format
brokers = ["localhost:9092"]      # Kafka broker addresses
topic_prefix = "marmot.cdc"       # Topics: {prefix}.{database}.{table}
filter_tables = ["*"]             # Glob patterns (e.g., "users", "order_*")
filter_databases = ["*"]          # Glob patterns (e.g., "prod_*")
batch_size = 100                  # Events per poll cycle
poll_interval_ms = 10             # Polling interval

# NATS sink example
[[publisher.sinks]]
name = "nats-events"
type = "nats"
format = "debezium"
nats_url = "nats://localhost:4222"
topic_prefix = "marmot.cdc"
filter_tables = ["*"]
filter_databases = ["*"]&lt;/code&gt;
    &lt;p&gt;Events follow the Debezium envelope structure:&lt;/p&gt;
    &lt;code&gt;{
  "schema": { ... },
  "payload": {
    "before": null,
    "after": {"id": 1, "name": "alice", "email": "alice@example.com"},
    "source": {
      "version": "2.0.0",
      "connector": "marmot",
      "name": "marmot",
      "ts_ms": 1702500000000,
      "db": "myapp",
      "table": "users"
    },
    "op": "c",
    "ts_ms": 1702500000000
  }
}&lt;/code&gt;
    &lt;p&gt;Operation Types (per Debezium spec):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Operation&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;op&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;before&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;code&gt;after&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;INSERT&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;c&lt;/code&gt; (create)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;null&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;row data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;UPDATE&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;u&lt;/code&gt; (update)&lt;/cell&gt;
        &lt;cell&gt;old row&lt;/cell&gt;
        &lt;cell&gt;new row&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DELETE&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;d&lt;/code&gt; (delete)&lt;/cell&gt;
        &lt;cell&gt;old row&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;null&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Topics follow the pattern: &lt;code&gt;{topic_prefix}.{database}.{table}&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;marmot.cdc.myapp.users&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;marmot.cdc.myapp.orders&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;marmot.cdc.analytics.events&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Real-Time Analytics: Stream changes to data warehouses (Snowflake, BigQuery, ClickHouse)&lt;/item&gt;
      &lt;item&gt;Event-Driven Microservices: Trigger actions on data changes&lt;/item&gt;
      &lt;item&gt;Cache Invalidation: Keep caches in sync with database changes&lt;/item&gt;
      &lt;item&gt;Audit Logging: Capture all changes for compliance&lt;/item&gt;
      &lt;item&gt;Search Indexing: Keep Elasticsearch/Algolia in sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For more details, see the Integrations documentation.&lt;/p&gt;
    &lt;p&gt;Marmot supports a wide range of MySQL/SQLite statements through its MySQL protocol server. The following table shows compatibility for different statement types:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Statement Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Support&lt;/cell&gt;
        &lt;cell role="head"&gt;Replication&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DML - Data Manipulation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;INSERT&lt;/code&gt; / &lt;code&gt;REPLACE&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Includes qualified table names (db.table)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;UPDATE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Includes qualified table names&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;DELETE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Includes qualified table names&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SELECT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Read operations&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;LOAD DATA&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Bulk data loading&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DDL - Data Definition&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ALTER TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;DROP TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;TRUNCATE TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;RENAME TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP INDEX&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP VIEW&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP TRIGGER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Database Management&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE DATABASE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;DROP DATABASE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ALTER DATABASE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Replicated with cluster-wide locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SHOW DATABASES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Metadata query&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SHOW TABLES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Metadata query&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;USE database&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Session state&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Transaction Control&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;BEGIN&lt;/code&gt; / &lt;code&gt;START TRANSACTION&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;N/A&lt;/cell&gt;
        &lt;cell&gt;Transaction boundary&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;COMMIT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Commits distributed transaction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ROLLBACK&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Aborts distributed transaction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;SAVEPOINT&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Full&lt;/cell&gt;
        &lt;cell&gt;✅ Yes&lt;/cell&gt;
        &lt;cell&gt;Nested transaction support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Locking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;LOCK TABLES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Requires distributed locking coordination&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;UNLOCK TABLES&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Requires distributed locking coordination&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Session Configuration&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;SET&lt;/code&gt; statements&lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Session-local, not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;XA Transactions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;XA START/END/PREPARE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Marmot uses its own 2PC protocol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;XA COMMIT/ROLLBACK&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Not compatible with Marmot's model&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;DCL - Data Control&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;&lt;code&gt;GRANT&lt;/code&gt; / &lt;code&gt;REVOKE&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;User management not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;CREATE/DROP USER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;User management not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;ALTER USER&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;User management not replicated&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Administrative&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;OPTIMIZE TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Node-local administrative command&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;REPAIR TABLE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;✅ Parsed&lt;/cell&gt;
        &lt;cell&gt;❌ No&lt;/cell&gt;
        &lt;cell&gt;Node-local administrative command&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;✅ Full: Fully supported and working&lt;/item&gt;
      &lt;item&gt;✅ Parsed: Statement is parsed and recognized&lt;/item&gt;
      &lt;item&gt;&lt;g-emoji&gt;⚠️&lt;/g-emoji&gt;Limited: Works but has limitations in distributed context&lt;/item&gt;
      &lt;item&gt;❌ No: Not supported or not replicated&lt;/item&gt;
      &lt;item&gt;N/A: Not applicable (read-only or session-local)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Schema Changes (DDL): DDL statements are fully replicated with cluster-wide locking and automatic idempotency. See the DDL Replication section for details.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;XA Transactions: Marmot has its own distributed transaction protocol based on 2PC. MySQL XA transactions are not compatible with Marmot's replication model.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;User Management (DCL): User and privilege management statements are local to each node. For production deployments, consider handling authentication at the application or proxy level.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Table Locking:&lt;/p&gt;&lt;code&gt;LOCK TABLES&lt;/code&gt;statements are recognized but not enforced across the cluster. Use application-level coordination for distributed locking needs.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Qualified Names: Marmot fully supports qualified table names (e.g.,&lt;/p&gt;&lt;code&gt;db.table&lt;/code&gt;) in DML and DDL operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot includes a MySQL-compatible protocol server, allowing you to connect using any MySQL client (DBeaver, MySQL Workbench, mysql CLI, etc.). The server supports:&lt;/p&gt;
    &lt;p&gt;Marmot provides full support for MySQL metadata queries, enabling GUI tools like DBeaver to browse databases, tables, and columns:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;SHOW Commands: &lt;code&gt;SHOW DATABASES&lt;/code&gt;,&lt;code&gt;SHOW TABLES&lt;/code&gt;,&lt;code&gt;SHOW COLUMNS FROM table&lt;/code&gt;,&lt;code&gt;SHOW CREATE TABLE&lt;/code&gt;,&lt;code&gt;SHOW INDEXES&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;INFORMATION_SCHEMA: Queries against &lt;code&gt;INFORMATION_SCHEMA.TABLES&lt;/code&gt;,&lt;code&gt;INFORMATION_SCHEMA.COLUMNS&lt;/code&gt;,&lt;code&gt;INFORMATION_SCHEMA.SCHEMATA&lt;/code&gt;, and&lt;code&gt;INFORMATION_SCHEMA.STATISTICS&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Type Conversion: Automatic SQLite-to-MySQL type mapping for compatibility&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These metadata queries are powered by the rqlite/sql AST parser, providing production-grade MySQL query compatibility.&lt;/p&gt;
    &lt;code&gt;# Using mysql CLI
mysql -h localhost -P 3306 -u root

# Connection string for applications
mysql://root@localhost:3306/marmot&lt;/code&gt;
    &lt;p&gt;Marmot handles various failure and recovery scenarios automatically:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Minority partition&lt;/cell&gt;
        &lt;cell&gt;Writes fail - cannot achieve quorum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Majority partition&lt;/cell&gt;
        &lt;cell&gt;Writes succeed - quorum achieved&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Partition heals&lt;/cell&gt;
        &lt;cell&gt;Delta sync + LWW merges divergent data&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;How it works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;During partition, only the majority side can commit writes (quorum enforcement)&lt;/item&gt;
      &lt;item&gt;When partition heals, nodes exchange transaction logs via &lt;code&gt;StreamChanges&lt;/code&gt;RPC&lt;/item&gt;
      &lt;item&gt;Conflicts resolved using Last-Writer-Wins (LWW) with HLC timestamps&lt;/item&gt;
      &lt;item&gt;Higher node ID breaks ties for simultaneous writes&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Scenario&lt;/cell&gt;
        &lt;cell role="head"&gt;Recovery Method&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Brief outage&lt;/cell&gt;
        &lt;cell&gt;Delta sync - replay missed transactions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Extended outage&lt;/cell&gt;
        &lt;cell&gt;Snapshot transfer + delta sync&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;New node joining&lt;/cell&gt;
        &lt;cell&gt;Full snapshot from existing node&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Anti-Entropy Background Process:&lt;/p&gt;
    &lt;p&gt;Marmot v2 includes an automatic anti-entropy system that continuously monitors and repairs replication lag across the cluster:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lag Detection: Every 60 seconds (configurable), each node queries peers for their replication state&lt;/item&gt;
      &lt;item&gt;Smart Recovery Decision: &lt;list rend="ul"&gt;&lt;item&gt;Delta Sync if lag &amp;lt; 10,000 transactions AND &amp;lt; 1 hour: Streams missed transactions incrementally&lt;/item&gt;&lt;item&gt;Snapshot Transfer if lag exceeds thresholds: Full database file transfer for efficiency&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Gap Detection: Detects when transaction logs have been GC'd and automatically falls back to snapshot&lt;/item&gt;
      &lt;item&gt;Multi-Database Support: Tracks and syncs each database independently&lt;/item&gt;
      &lt;item&gt;GC Coordination: Garbage collection respects peer replication state - logs aren't deleted until all peers have applied them&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Delta Sync Process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lagging node queries &lt;code&gt;last_applied_txn_id&lt;/code&gt;for each peer/database&lt;/item&gt;
      &lt;item&gt;Requests transactions since that ID via &lt;code&gt;StreamChanges&lt;/code&gt;RPC&lt;/item&gt;
      &lt;item&gt;Gap Detection: Checks if first received txn_id has a large gap from requested ID &lt;list rend="ul"&gt;&lt;item&gt;If gap &amp;gt; delta_sync_threshold_txns, indicates missing (GC'd) transactions&lt;/item&gt;&lt;item&gt;Automatically falls back to snapshot transfer to prevent data loss&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Applies changes using LWW conflict resolution&lt;/item&gt;
      &lt;item&gt;Updates replication state tracking (per-database)&lt;/item&gt;
      &lt;item&gt;Progress logged every 100 transactions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GC Coordination with Anti-Entropy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Transaction logs are retained with a two-tier policy: &lt;list rend="ul"&gt;&lt;item&gt;Min retention (2 hours): Must be &amp;gt;= delta sync threshold, respects peer lag&lt;/item&gt;&lt;item&gt;Max retention (24 hours): Force delete after this time to prevent unbounded growth&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Config validation enforces: &lt;code&gt;gc_min &amp;gt;= delta_threshold&lt;/code&gt;and&lt;code&gt;gc_max &amp;gt;= 2x delta_threshold&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Each database tracks replication progress per peer&lt;/item&gt;
      &lt;item&gt;GC queries minimum applied txn_id across all peers before cleanup&lt;/item&gt;
      &lt;item&gt;Gap detection prevents data loss if GC runs while nodes are offline&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Write Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;ONE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns after 1 node ACK (fast, less durable)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;QUORUM&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns after majority ACK (default, balanced)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;ALL&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns after all nodes ACK (slow, most durable)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Conflict Resolution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All conflicts resolved via LWW using HLC timestamps&lt;/item&gt;
      &lt;item&gt;No data loss - later write always wins deterministically&lt;/item&gt;
      &lt;item&gt;Tie-breaker: higher node ID wins for equal timestamps&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Selective Table Watching: All tables in a database are replicated. Selective table replication is not supported.&lt;/item&gt;
      &lt;item&gt;WAL Mode Required: SQLite must use WAL mode for reliable multi-process changes.&lt;/item&gt;
      &lt;item&gt;Eventually Consistent: Rows may sync out of order. &lt;code&gt;SERIALIZABLE&lt;/code&gt;transaction assumptions may not hold across nodes.&lt;/item&gt;
      &lt;item&gt;Concurrent DDL: Avoid running concurrent DDL operations on the same database from multiple nodes (protected by cluster-wide lock with 30s lease).&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;IMPORTANT: Marmot automatically converts&lt;/p&gt;&lt;code&gt;INT AUTO_INCREMENT&lt;/code&gt;to&lt;code&gt;BIGINT&lt;/code&gt;&lt;p&gt;This is a breaking change from standard MySQL/SQLite behavior. Marmot does not respect 32-bit&lt;/p&gt;&lt;code&gt;INT&lt;/code&gt;for auto-increment columns - they are automatically promoted to&lt;code&gt;BIGINT&lt;/code&gt;to support distributed ID generation.&lt;/quote&gt;
    &lt;p&gt;Why?&lt;/p&gt;
    &lt;p&gt;In a distributed, leaderless system, each node must generate unique IDs independently without coordination. Marmot uses HLC-based (Hybrid Logical Clock) 64-bit IDs to ensure:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Global Uniqueness: IDs are unique across all nodes without central coordination&lt;/item&gt;
      &lt;item&gt;Monotonicity: IDs increase over time (within each node)&lt;/item&gt;
      &lt;item&gt;No Collisions: Unlike auto-increment sequences, HLC IDs cannot collide between nodes&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;How It Works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;DDL Transformation: When you create a table with&lt;/p&gt;&lt;code&gt;AUTO_INCREMENT&lt;/code&gt;:&lt;quote&gt;CREATE TABLE users (id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(100)) -- Becomes internally: CREATE TABLE users (id BIGINT PRIMARY KEY, name TEXT)&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;DML ID Injection: When inserting with&lt;/p&gt;&lt;code&gt;0&lt;/code&gt;or&lt;code&gt;NULL&lt;/code&gt;for an auto-increment column:&lt;quote&gt;INSERT INTO users (id, name) VALUES (0, 'alice') -- Becomes internally: INSERT INTO users (id, name) VALUES (7318624812345678901, 'alice')&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Explicit IDs Preserved: If you provide an explicit non-zero ID, it is used as-is:&lt;/p&gt;
        &lt;quote&gt;INSERT INTO users (id, name) VALUES (12345, 'bob') -- Remains: INSERT INTO users (id, name) VALUES (12345, 'bob')&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Important Considerations:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Aspect&lt;/cell&gt;
        &lt;cell role="head"&gt;Behavior&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ID Range&lt;/cell&gt;
        &lt;cell&gt;64-bit (up to 9.2 quintillion) instead of 32-bit (4.2 billion)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ID Format&lt;/cell&gt;
        &lt;cell&gt;HLC-based, not sequential integers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;SQLite ROWID&lt;/cell&gt;
        &lt;cell&gt;Not used - Marmot manages IDs explicitly&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Client Libraries&lt;/cell&gt;
        &lt;cell&gt;Ensure your client handles &lt;code&gt;BIGINT&lt;/code&gt; correctly (some JSON serializers may lose precision)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Existing Data&lt;/cell&gt;
        &lt;cell&gt;Migrate existing &lt;code&gt;INT&lt;/code&gt; columns to &lt;code&gt;BIGINT&lt;/code&gt; before enabling Marmot&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Schema-Based Detection:&lt;/p&gt;
    &lt;p&gt;Marmot automatically detects auto-increment columns by querying SQLite schema directly. A column is considered auto-increment if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It is a single-column &lt;code&gt;INTEGER PRIMARY KEY&lt;/code&gt;(SQLite rowid alias), or&lt;/item&gt;
      &lt;item&gt;It is a single-column &lt;code&gt;BIGINT PRIMARY KEY&lt;/code&gt;(Marmot's transformed columns)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No registration required - columns are detected from schema at runtime&lt;/item&gt;
      &lt;item&gt;Works across restarts - no need to re-execute DDL statements&lt;/item&gt;
      &lt;item&gt;Works with existing databases - tables created directly on SQLite work too&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot v2 uses a TOML configuration file (default: &lt;code&gt;config.toml&lt;/code&gt;). All settings have sensible defaults.&lt;/p&gt;
    &lt;code&gt;node_id = 0  # 0 = auto-generate
data_dir = "./marmot-data"&lt;/code&gt;
    &lt;code&gt;[transaction]
heartbeat_timeout_seconds = 10  # Transaction timeout without heartbeat
conflict_window_seconds = 10    # Conflict resolution window
lock_wait_timeout_seconds = 50  # Lock wait timeout (MySQL: innodb_lock_wait_timeout)&lt;/code&gt;
    &lt;p&gt;Note: Transaction log garbage collection is managed by the replication configuration to coordinate with anti-entropy. See &lt;code&gt;replication.gc_min_retention_hours&lt;/code&gt; and &lt;code&gt;replication.gc_max_retention_hours&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[connection_pool]
pool_size = 4              # Number of SQLite connections
max_idle_time_seconds = 10 # Max idle time before closing
max_lifetime_seconds = 300 # Max connection lifetime (0 = unlimited)&lt;/code&gt;
    &lt;code&gt;[grpc_client]
keepalive_time_seconds = 10    # Keepalive ping interval
keepalive_timeout_seconds = 3  # Keepalive ping timeout
max_retries = 3                # Max retry attempts
retry_backoff_ms = 100         # Retry backoff duration&lt;/code&gt;
    &lt;code&gt;[coordinator]
prepare_timeout_ms = 2000 # Prepare phase timeout
commit_timeout_ms = 2000  # Commit phase timeout
abort_timeout_ms = 2000   # Abort phase timeout&lt;/code&gt;
    &lt;code&gt;[cluster]
grpc_bind_address = "0.0.0.0"
grpc_port = 8080
seed_nodes = []                # List of seed node addresses
cluster_secret = ""            # PSK for cluster authentication (see Security section)
gossip_interval_ms = 1000      # Gossip interval
gossip_fanout = 3              # Number of peers to gossip to
suspect_timeout_ms = 5000      # Suspect timeout
dead_timeout_ms = 10000        # Dead timeout&lt;/code&gt;
    &lt;p&gt;Marmot supports Pre-Shared Key (PSK) authentication for cluster communication. This is strongly recommended for production deployments.&lt;/p&gt;
    &lt;code&gt;[cluster]
# All nodes in the cluster must use the same secret
cluster_secret = "your-secret-key-here"&lt;/code&gt;
    &lt;p&gt;Environment Variable (Recommended):&lt;/p&gt;
    &lt;p&gt;For production, use the environment variable to avoid storing secrets in config files:&lt;/p&gt;
    &lt;code&gt;export MARMOT_CLUSTER_SECRET="your-secret-key-here"
./marmot&lt;/code&gt;
    &lt;p&gt;The environment variable takes precedence over the config file.&lt;/p&gt;
    &lt;p&gt;Generating a Secret:&lt;/p&gt;
    &lt;code&gt;# Generate a secure random secret
openssl rand -base64 32&lt;/code&gt;
    &lt;p&gt;Behavior:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If &lt;code&gt;cluster_secret&lt;/code&gt;is empty and&lt;code&gt;MARMOT_CLUSTER_SECRET&lt;/code&gt;is not set, authentication is disabled&lt;/item&gt;
      &lt;item&gt;A warning is logged at startup when authentication is disabled&lt;/item&gt;
      &lt;item&gt;All gRPC endpoints (gossip, replication, snapshots) are protected when authentication is enabled&lt;/item&gt;
      &lt;item&gt;Nodes with mismatched secrets will fail to communicate (connection rejected with "invalid cluster secret")&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Marmot provides admin HTTP endpoints for managing cluster membership (requires &lt;code&gt;cluster_secret&lt;/code&gt; to be configured):&lt;/p&gt;
    &lt;p&gt;Node Lifecycle:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New/restarted nodes auto-join via gossip - no manual intervention needed&lt;/item&gt;
      &lt;item&gt;Nodes marked REMOVED via admin API cannot auto-rejoin - must be explicitly allowed&lt;/item&gt;
      &lt;item&gt;This prevents decommissioned nodes from accidentally rejoining the cluster&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# View cluster members and quorum info
curl -H "X-Marmot-Secret: your-secret" http://localhost:8080/admin/cluster/members

# Remove a node from the cluster (excludes from quorum, blocks auto-rejoin)
curl -X POST -H "X-Marmot-Secret: your-secret" http://localhost:8080/admin/cluster/remove/2

# Allow a removed node to rejoin (node must then restart to join)
curl -X POST -H "X-Marmot-Secret: your-secret" http://localhost:8080/admin/cluster/allow/2&lt;/code&gt;
    &lt;p&gt;See the Operations documentation for detailed usage and examples.&lt;/p&gt;
    &lt;code&gt;[replication]
default_write_consistency = "QUORUM"      # Write consistency level: ONE, QUORUM, ALL
default_read_consistency = "LOCAL_ONE"    # Read consistency level
write_timeout_ms = 5000                   # Write operation timeout
read_timeout_ms = 2000                    # Read operation timeout

# Anti-Entropy: Background healing for eventual consistency
# - Detects and repairs divergence between replicas
# - Uses delta sync for small lags, snapshot for large lags
# - Includes gap detection to prevent incomplete data after GC
enable_anti_entropy = true                 # Enable automatic catch-up for lagging nodes
anti_entropy_interval_seconds = 60         # How often to check for lag (default: 60s)
delta_sync_threshold_transactions = 10000  # Delta sync if lag &amp;lt; 10K txns
delta_sync_threshold_seconds = 3600        # Snapshot if lag &amp;gt; 1 hour

# Garbage Collection: Reclaim disk space by deleting old transaction records
# - gc_min must be &amp;gt;= delta_sync_threshold (validated at startup)
# - gc_max should be &amp;gt;= 2x delta_sync_threshold (recommended)
# - Set gc_max = 0 for unlimited retention
gc_min_retention_hours = 2   # Keep at least 2 hours (&amp;gt;= 1 hour delta threshold)
gc_max_retention_hours = 24  # Force delete after 24 hours&lt;/code&gt;
    &lt;p&gt;Anti-Entropy Tuning:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Small clusters (2-3 nodes): Use default settings (60s interval)&lt;/item&gt;
      &lt;item&gt;Large clusters (5+ nodes): Consider increasing interval to 120-180s to reduce network overhead&lt;/item&gt;
      &lt;item&gt;High write throughput: Increase &lt;code&gt;delta_sync_threshold_transactions&lt;/code&gt;to 50000+&lt;/item&gt;
      &lt;item&gt;Long-running clusters: Keep &lt;code&gt;gc_max_retention_hours&lt;/code&gt;at 24+ to handle extended outages&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;GC Configuration Rules (Validated at Startup):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;gc_min_retention_hours&lt;/code&gt;must be &amp;gt;=&lt;code&gt;delta_sync_threshold_seconds&lt;/code&gt;(in hours)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;gc_max_retention_hours&lt;/code&gt;should be &amp;gt;= 2x&lt;code&gt;delta_sync_threshold_seconds&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Violating these rules will cause startup failure with helpful error messages&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;[query_pipeline]
transpiler_cache_size = 10000  # LRU cache for MySQL→SQLite transpilation
validator_pool_size = 8        # SQLite connection pool for validation&lt;/code&gt;
    &lt;code&gt;[mysql]
enabled = true
bind_address = "0.0.0.0"
port = 3306
max_connections = 1000&lt;/code&gt;
    &lt;code&gt;[publisher]
enabled = false  # Enable CDC publishing to external systems

[[publisher.sinks]]
name = "kafka-main"              # Unique sink name
type = "kafka"                   # "kafka" or "nats"
format = "debezium"              # Debezium-compatible JSON (only option)
brokers = ["localhost:9092"]     # Kafka broker addresses
topic_prefix = "marmot.cdc"      # Topic pattern: {prefix}.{db}.{table}
filter_tables = ["*"]            # Glob patterns for table filtering
filter_databases = ["*"]         # Glob patterns for database filtering
batch_size = 100                 # Events to read per poll cycle
poll_interval_ms = 10            # Polling interval (default: 10ms)
retry_initial_ms = 100           # Initial retry delay on failure
retry_max_ms = 30000             # Max retry delay (30 seconds)
retry_multiplier = 2.0           # Exponential backoff multiplier&lt;/code&gt;
    &lt;p&gt;See the Integrations documentation for details on event format, Kafka/NATS configuration, and use cases.&lt;/p&gt;
    &lt;code&gt;[logging]
verbose = false          # Enable verbose logging
format = "console"       # Log format: console or json&lt;/code&gt;
    &lt;code&gt;[prometheus]
enabled = true  # Metrics served on gRPC port at /metrics endpoint&lt;/code&gt;
    &lt;p&gt;Accessing Metrics:&lt;/p&gt;
    &lt;code&gt;# Metrics are multiplexed with gRPC on the same port
curl http://localhost:8080/metrics

# Prometheus scrape config
scrape_configs:
  - job_name: 'marmot'
    static_configs:
      - targets: ['node1:8080', 'node2:8080', 'node3:8080']&lt;/code&gt;
    &lt;p&gt;See &lt;code&gt;config.toml&lt;/code&gt; for complete configuration reference with detailed comments.&lt;/p&gt;
    &lt;p&gt;Performance benchmarks on a local development machine (Apple M-series, 3-node cluster, single machine):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Nodes&lt;/cell&gt;
        &lt;cell&gt;3 (ports 3307, 3308, 3309)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Threads&lt;/cell&gt;
        &lt;cell&gt;16&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Batch Size&lt;/cell&gt;
        &lt;cell&gt;10 ops/transaction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Consistency&lt;/cell&gt;
        &lt;cell&gt;QUORUM&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;4,175 ops/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TX Throughput&lt;/cell&gt;
        &lt;cell&gt;417 tx/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Records Loaded&lt;/cell&gt;
        &lt;cell&gt;200,000&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Errors&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Throughput&lt;/cell&gt;
        &lt;cell&gt;3,370 ops/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TX Throughput&lt;/cell&gt;
        &lt;cell&gt;337 tx/sec&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Duration&lt;/cell&gt;
        &lt;cell&gt;120 seconds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Total Operations&lt;/cell&gt;
        &lt;cell&gt;404,930&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Errors&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Retries&lt;/cell&gt;
        &lt;cell&gt;37 (0.09%)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Operation Distribution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;READ: 20%&lt;/item&gt;
      &lt;item&gt;UPDATE: 30%&lt;/item&gt;
      &lt;item&gt;INSERT: 35%&lt;/item&gt;
      &lt;item&gt;DELETE: 5%&lt;/item&gt;
      &lt;item&gt;UPSERT: 10%&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Percentile&lt;/cell&gt;
        &lt;cell role="head"&gt;Latency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P50&lt;/cell&gt;
        &lt;cell&gt;4.3ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P90&lt;/cell&gt;
        &lt;cell&gt;14.0ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P95&lt;/cell&gt;
        &lt;cell&gt;36.8ms&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;P99&lt;/cell&gt;
        &lt;cell&gt;85.1ms&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All 3 nodes maintained identical row counts (346,684 rows) throughout the test, confirming consistent replication.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: These benchmarks are from a local development machine with all nodes on the same host. Production deployments across multiple machines will have different characteristics based on network latency.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46460676</guid><pubDate>Fri, 02 Jan 2026 02:21:57 +0000</pubDate></item><item><title>FreeBSD: Home NAS, part 1 – configuring ZFS mirror (RAID1)</title><link>https://rtfm.co.ua/en/freebsd-home-nas-part-1-configuring-zfs-mirror-raid1/</link><description>&lt;doc fingerprint="1e35285ba76e8ee5"&gt;
  &lt;main&gt;
    &lt;p&gt;I have an idea to set up a home NAS on FreeBSD.&lt;/p&gt;
    &lt;p&gt;For this purpose, I bought a Lenovo ThinkCentre M720s SFF – it’s quiet, compact, and offers the possibility to install 2 SATA III SSDs plus a separate M.2 slot for an NVMe SSD.&lt;/p&gt;
    &lt;p&gt;What is planned:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;on NVMe SSD: UFS and FreeBSD&lt;/item&gt;
      &lt;item&gt;on SATA SSDs: ZFS with RAID1&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While waiting for the drives to arrive, let’s test how it all works on a virtual machine.&lt;/p&gt;
    &lt;p&gt;We will be installing FreeBSD 14.3, although version 15 is already out, but it has some interesting changes that I’ll play with separately.&lt;/p&gt;
    &lt;p&gt;Of course, I could have gone with TrueNAS, which is based on FreeBSD – but I want “vanilla” FreeBSD to do everything manually.&lt;/p&gt;
    &lt;p&gt;All posts in this blog series:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;(current) FreeBSD: Home NAS, part 1 – configuring ZFS mirror (RAID1)&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 2 – introduction to Packet Filter (PF) firewall&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 3 – WireGuard VPN, Linux peer, and routing&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 4 – Local DNS with Unbound&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 5 – ZFS pool, datasets, snapshots, and ZFS monitoring&lt;/item&gt;
      &lt;item&gt;FreeBSD: Home NAS, part 6 – Samba server and client connections&lt;/item&gt;
      &lt;item&gt;… to be continued&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contents&lt;/p&gt;
    &lt;head rend="h1"&gt;Installing FreeBSD via SSH&lt;/head&gt;
    &lt;p&gt;We will perform the installation over SSH using &lt;code&gt;bsdinstall&lt;/code&gt; – boot the system in LiveCD mode, enable SSH, and then proceed with the installation from a workstation laptop.&lt;/p&gt;
    &lt;p&gt;The virtual machine has three disks – mirroring the future ThinkCentre setup:&lt;/p&gt;
    &lt;p&gt;Select Live System:&lt;/p&gt;
    &lt;p&gt;Login as &lt;code&gt;root&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Bring up the network:&lt;/p&gt;
    &lt;quote&gt;# ifconfig em0 up # dhclient em0&lt;/quote&gt;
    &lt;head rend="h2"&gt;Configuring SSH on FreeBSD LiveCD&lt;/head&gt;
    &lt;p&gt;For SSH, we need to set a &lt;code&gt;root&lt;/code&gt; password and make changes to &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt;, but currently, this doesn’t work because the system is mounted as read-only:&lt;/p&gt;
    &lt;p&gt;Check the current partitions:&lt;/p&gt;
    &lt;p&gt;And apply a “dirty hack”:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;mount a new &lt;code&gt;tmpfs&lt;/code&gt;file system in RAM at&lt;code&gt;/mnt&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;copy the contents of &lt;code&gt;/etc&lt;/code&gt;from the LiveCD there&lt;/item&gt;
      &lt;item&gt;mount &lt;code&gt;tmpfs&lt;/code&gt;over&lt;code&gt;/etc&lt;/code&gt;(overlaying the read-only directory from the ISO)&lt;/item&gt;
      &lt;item&gt;copy the prepared files from &lt;code&gt;/mnt&lt;/code&gt;back into the new&lt;code&gt;/etc&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Execute:&lt;/p&gt;
    &lt;quote&gt;# mount -t tmpfs tmpfs /mnt # cp -a /etc/* /mnt/ # mount -t tmpfs tmpfs /etc # cp -a /mnt/* /etc/&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;mount&lt;/code&gt; syntax for &lt;code&gt;tmpfs&lt;/code&gt; is &lt;code&gt;mount -t &amp;lt;fstype&amp;gt; &amp;lt;source&amp;gt; &amp;lt;mountpoint&amp;gt;&lt;/code&gt;. Since the &lt;code&gt;source&lt;/code&gt; value is required, we specify &lt;code&gt;tmpfs&lt;/code&gt; again.&lt;/p&gt;
    &lt;p&gt;Now, set the password with &lt;code&gt;passwd&lt;/code&gt; and start &lt;code&gt;sshd&lt;/code&gt; using &lt;code&gt;onestart&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;# passwd # service sshd onestart&lt;/quote&gt;
    &lt;p&gt;However, SSH will still deny access because &lt;code&gt;root&lt;/code&gt; login is disabled by default:&lt;/p&gt;
    &lt;quote&gt;$ ssh [email protected] ([email protected]) Password for root@: ([email protected]) Password for root@: ([email protected]) Password for root@:&lt;/quote&gt;
    &lt;p&gt;Set &lt;code&gt;PermitRootLogin yes&lt;/code&gt; in &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt; and restart &lt;code&gt;sshd&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;# echo "PermitRootLogin yes" &amp;gt;&amp;gt; /etc/ssh/sshd_config # service sshd onerestart&lt;/quote&gt;
    &lt;p&gt;Now we can log in:&lt;/p&gt;
    &lt;quote&gt;$ ssh [email protected] ([email protected]) Password for root@: Last login: Sun Dec 7 12:19:25 2025 FreeBSD 14.3-RELEASE (GENERIC) releng/14.3-n271432-8c9ce319fef7 Welcome to FreeBSD! ... root@:~ #&lt;/quote&gt;
    &lt;head rend="h1"&gt;Installation with &lt;code&gt;bsdinstall&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;Run &lt;code&gt;bsdinstall&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;# bsdinstall&lt;/quote&gt;
    &lt;p&gt;Select the components to add to the system – &lt;code&gt;ports&lt;/code&gt; is necessary, &lt;code&gt;src&lt;/code&gt; is optional but definitely worth it for a real NAS:&lt;/p&gt;
    &lt;head rend="h2"&gt;Disk partitioning&lt;/head&gt;
    &lt;p&gt;We’ll do a minimal disk partition, so select Manual:&lt;/p&gt;
    &lt;p&gt;We will install the system on &lt;code&gt;ada0&lt;/code&gt;, select it, and click Create:&lt;/p&gt;
    &lt;p&gt;Next, choose a partition scheme. It’s standard for 2025 – GPT:&lt;/p&gt;
    &lt;p&gt;Confirm the changes, and now we have a new partition table on the system drive &lt;code&gt;ada0&lt;/code&gt;:&lt;/p&gt;
    &lt;head rend="h3"&gt;The &lt;code&gt;freebsd-boot&lt;/code&gt; Partition&lt;/head&gt;
    &lt;p&gt;Now we need to create the partitions themselves.&lt;/p&gt;
    &lt;p&gt;Select &lt;code&gt;ada0&lt;/code&gt; again, click Create, and create a partition for &lt;code&gt;freebsd-boot&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is just for the virtual machine; on the actual ThinkCentre, we would use type &lt;code&gt;efi&lt;/code&gt; with a size of about 200-500 MB.&lt;/p&gt;
    &lt;p&gt;For now, set:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;freebsd-boot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Size: 512K&lt;/item&gt;
      &lt;item&gt;Mountpoint: empty&lt;/item&gt;
      &lt;item&gt;Label: empty&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Confirm and proceed to the next partition.&lt;/p&gt;
    &lt;head rend="h3"&gt;The &lt;code&gt;freebsd-swap&lt;/code&gt; Partition&lt;/head&gt;
    &lt;p&gt;Click Create again to add Swap.&lt;/p&gt;
    &lt;p&gt;Given that on the ThinkCentre we will have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;8 – 16 GB RAM&lt;/item&gt;
      &lt;item&gt;no sleep/hibernate&lt;/item&gt;
      &lt;item&gt;UFS and ZFS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2 gigabytes will be enough.&lt;/p&gt;
    &lt;p&gt;Set:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;freebsd-swap&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Size: 2GB&lt;/item&gt;
      &lt;item&gt;Mountpoint: empty&lt;/item&gt;
      &lt;item&gt;Label: empty&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Root Partition with UFS&lt;/head&gt;
    &lt;p&gt;The main system will be on UFS because it is very stable, doesn’t require much RAM, mounts quickly, is easy to recover, and lacks complex caching mechanisms (UPD: however, after getting to know ZFS and its capabilities better, I decided to use it for the system disk as well)&lt;/p&gt;
    &lt;p&gt;Set:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Type: &lt;code&gt;freebsd-ufs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Size: 14GB&lt;/item&gt;
      &lt;item&gt;Mountpoint: /&lt;/item&gt;
      &lt;item&gt;Label: rootfs – just a name for us&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’ll configure the rest of the disks later; for now, select Finish and Commit:&lt;/p&gt;
    &lt;head rend="h2"&gt;Finishing Installation&lt;/head&gt;
    &lt;p&gt;Wait for the copying to complete:&lt;/p&gt;
    &lt;p&gt;Configure the network:&lt;/p&gt;
    &lt;p&gt;Select Timezone:&lt;/p&gt;
    &lt;p&gt;In System Configuration – select &lt;code&gt;sshd&lt;/code&gt;, no mouse, enable &lt;code&gt;ntpd&lt;/code&gt; and &lt;code&gt;powerd&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;System Hardening – considering this will be a home NAS, but I might open external access (even behind a firewall), it makes sense to tune the security a bit:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;read_msgbuf&lt;/code&gt;: allow&lt;code&gt;dmesg&lt;/code&gt;access for root only&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;proc_debug&lt;/code&gt;: allow&lt;code&gt;ptrace&lt;/code&gt;for root only&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;random_pid&lt;/code&gt;: randomize PID numbers&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clear_tmp&lt;/code&gt;: clear&lt;code&gt;/tmp&lt;/code&gt;on reboot&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;secure_console&lt;/code&gt;: require&lt;code&gt;root&lt;/code&gt;password for login from the physical console&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add a user:&lt;/p&gt;
    &lt;p&gt;Everything is ready – reboot the machine:&lt;/p&gt;
    &lt;head rend="h1"&gt;Creating a ZFS RAID&lt;/head&gt;
    &lt;p&gt;Log in as the regular user:&lt;/p&gt;
    &lt;quote&gt;$ ssh [email protected] ... FreeBSD 14.3-RELEASE (GENERIC) releng/14.3-n271432-8c9ce319fef7 Welcome to FreeBSD! ... setevoy@test-nas-1:~ $&lt;/quote&gt;
    &lt;p&gt;Install &lt;code&gt;vim&lt;/code&gt; 🙂&lt;/p&gt;
    &lt;quote&gt;# pkg install vim&lt;/quote&gt;
    &lt;p&gt;Check our disks.&lt;/p&gt;
    &lt;p&gt;Using &lt;code&gt;geom disk&lt;/code&gt; for physical device info, and &lt;code&gt;gpart show&lt;/code&gt; to see partitions on the disks.&lt;/p&gt;
    &lt;p&gt;Check disks – there are three:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # geom disk list Geom name: ada0 Providers: 1. Name: ada0 Mediasize: 17179869184 (16G) Sectorsize: 512 Mode: r2w2e3 descr: VBOX HARDDISK ident: VB262b53f7-adc5cd2c rotationrate: unknown fwsectors: 63 fwheads: 16 Geom name: ada1 Providers: 1. Name: ada1 Mediasize: 17179869184 (16G) Sectorsize: 512 Mode: r0w0e0 descr: VBOX HARDDISK ident: VB059f9d08-4b0e1f56 rotationrate: unknown fwsectors: 63 fwheads: 16 Geom name: ada2 Providers: 1. Name: ada2 Mediasize: 17179869184 (16G) Sectorsize: 512 Mode: r0w0e0 descr: VBOX HARDDISK ident: VB3941028c-3ea0d485 rotationrate: unknown fwsectors: 63 fwheads: 16&lt;/quote&gt;
    &lt;p&gt;And with &lt;code&gt;gpart&lt;/code&gt; – current &lt;code&gt;ada0&lt;/code&gt; where the system was installed:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show =&amp;gt; 40 33554352 ada0 GPT (16G) 40 1024 1 freebsd-boot (512K) 1064 4194304 2 freebsd-swap (2.0G) 4195368 29359024 3 freebsd-ufs (14G)&lt;/quote&gt;
    &lt;p&gt;Disks &lt;code&gt;ada1&lt;/code&gt; and &lt;code&gt;ada2&lt;/code&gt; will be used for ZFS and its mirror (RAID1).&lt;/p&gt;
    &lt;p&gt;If there was anything on them – wipe it:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart destroy -F ada1 gpart: arg0 'ada1': Invalid argument root@test-nas-1:/home/setevoy # gpart destroy -F ada2 gpart: arg0 'ada2': Invalid argument&lt;/quote&gt;
    &lt;p&gt;Since this is a VM and the disks are empty, “Invalid argument” is expected and fine.&lt;/p&gt;
    &lt;p&gt;Create GPT partition tables on &lt;code&gt;ada1&lt;/code&gt; and &lt;code&gt;ada2&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart create -s gpt ada1 ada1 created root@test-nas-1:/home/setevoy # gpart create -s gpt ada2 ada2 created&lt;/quote&gt;
    &lt;p&gt;Check:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show ada1 =&amp;gt; 40 33554352 ada1 GPT (16G) 40 33554352 - free - (16G)&lt;/quote&gt;
    &lt;p&gt;Create partitions for ZFS:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart add -t freebsd-zfs ada1 ada1p1 added root@test-nas-1:/home/setevoy # gpart add -t freebsd-zfs ada2 ada2p1 added&lt;/quote&gt;
    &lt;p&gt;Check again:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show ada1 =&amp;gt; 40 33554352 ada1 GPT (16G) 40 33554352 1 freebsd-zfs (16G)&lt;/quote&gt;
    &lt;head rend="h2"&gt;Creating a ZFS mirror with &lt;code&gt;zpool&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The “magic” of ZFS is that everything works “out of the box” – you don’t need a separate LVM and its groups, and you don’t need &lt;code&gt;mdadm&lt;/code&gt; for RAID.&lt;/p&gt;
    &lt;p&gt;For managing disks in ZFS, the main utility is &lt;code&gt;zpool&lt;/code&gt;, and for managing data (datasets, file systems, snapshots), it’s &lt;code&gt;zfs&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To combine one or more disks into a single logical storage, ZFS uses a pool – the equivalent of a volume group in Linux LVM.&lt;/p&gt;
    &lt;p&gt;Create the pool:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zpool create tank mirror ada1p1 ada2p1&lt;/quote&gt;
    &lt;p&gt;Here, tank is the pool name, &lt;code&gt;mirror&lt;/code&gt; specifies that it will be RAID1, and we provide the list of partitions included in this pool.&lt;/p&gt;
    &lt;p&gt;Check:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zpool status pool: tank state: ONLINE config: NAME STATE READ WRITE CKSUM tank ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 ada1p1 ONLINE 0 0 0 ada2p1 ONLINE 0 0 0 errors: No known data errors&lt;/quote&gt;
    &lt;p&gt;ZFS immediately mounts this pool at &lt;code&gt;/tank&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # mount /dev/ada0p3 on / (ufs, local, soft-updates, journaled soft-updates) devfs on /dev (devfs) tank on /tank (zfs, local, nfsv4acls)&lt;/quote&gt;
    &lt;p&gt;Check partitions now:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # gpart show =&amp;gt; 40 33554352 ada0 GPT (16G) 40 1024 1 freebsd-boot (512K) 1064 4194304 2 freebsd-swap (2.0G) 4195368 29359024 3 freebsd-ufs (14G) =&amp;gt; 40 33554352 ada1 GPT (16G) 40 33554352 1 freebsd-zfs (16G) =&amp;gt; 40 33554352 ada2 GPT (16G) 40 33554352 1 freebsd-zfs (16G)&lt;/quote&gt;
    &lt;p&gt;If we want to change the mountpoint – execute &lt;code&gt;zfs set mountpoint&lt;/code&gt;:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zfs set mountpoint=/data tank&lt;/quote&gt;
    &lt;p&gt;And it immediately mounts to the new directory:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # mount /dev/ada0p3 on / (ufs, local, soft-updates, journaled soft-updates) devfs on /dev (devfs) tank on /data (zfs, local, nfsv4acls)&lt;/quote&gt;
    &lt;p&gt;Enable data compression – useful for a NAS, see Compression and Compressing ZFS File Systems.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;lz4&lt;/code&gt; is the current default option, let’s enable it:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zfs set compression=lz4 tank&lt;/quote&gt;
    &lt;p&gt;Since we installed the system on UFS, we need to add a few parameters to autostart for ZFS to work.&lt;/p&gt;
    &lt;p&gt;Configure the boot loader in &lt;code&gt;/boot/loader.conf&lt;/code&gt; to load kernel modules:&lt;/p&gt;
    &lt;quote&gt;zfs_load="YES"&lt;/quote&gt;
    &lt;p&gt;Or, to avoid manual editing, use &lt;code&gt;sysrc&lt;/code&gt; with the &lt;code&gt;-f&lt;/code&gt; flag:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # sysrc -f /boot/loader.conf zfs_load="YES"&lt;/quote&gt;
    &lt;p&gt;And add to &lt;code&gt;/etc/rc.conf&lt;/code&gt; to start the &lt;code&gt;zfsd&lt;/code&gt; daemon and mount the file systems:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # sysrc zfs_enable="YES" zfs_enable: NO -&amp;gt; YES&lt;/quote&gt;
    &lt;p&gt;Reboot and check:&lt;/p&gt;
    &lt;quote&gt;root@test-nas-1:/home/setevoy # zpool status pool: tank state: ONLINE config: NAME STATE READ WRITE CKSUM tank ONLINE 0 0 0 mirror-0 ONLINE 0 0 0 ada1p1 ONLINE 0 0 0 ada2p1 ONLINE 0 0 0&lt;/quote&gt;
    &lt;p&gt;Everything is in place.&lt;/p&gt;
    &lt;p&gt;Now you can proceed with further tuning – configuring separate datasets, snapshots, etc.&lt;/p&gt;
    &lt;p&gt;For a Web UI, you could try Seafile or FileBrowser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46462108</guid><pubDate>Fri, 02 Jan 2026 06:48:32 +0000</pubDate></item><item><title>Round the tree, yes, but not round the squirrel</title><link>https://www.futilitycloset.com/2026/01/02/round-and-round/</link><description>&lt;doc fingerprint="f75c93007bda195e"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;‘I had quite a bit of fun playing hide-and-seek with a squirrel,’ he said. ‘You know that little round glade with a lone birch in the centre? It was on this tree that a squirrel was hiding from me. As I emerged from a thicket, I saw its snout and two bright little eyes peeping from behind the trunk. I wanted to see the little animal, so I started circling round along the edge of the glade, mindful of keeping the distance in order not to scare it. I did four rounds, but the little cheat kept backing away from me, eyeing me suspiciously from behind the tree. Try as I did, I just could not see its back.’&lt;/p&gt;
      &lt;p&gt;‘But you have just said yourself that you circled round the tree four times,’ one of the listeners interjected.&lt;/p&gt;
      &lt;p&gt;‘Round the tree, yes, but not round the squirrel.’&lt;/p&gt;
      &lt;p&gt;‘But the squirrel was on the tree, wasn’t it?’&lt;/p&gt;
      &lt;p&gt;‘So it was.’&lt;/p&gt;
      &lt;p&gt;‘Well, that means you circled round the squirrel too.’&lt;/p&gt;
      &lt;p&gt;‘Call that circling round the squirrel when I didn’t see its back?’&lt;/p&gt;
      &lt;p&gt;‘What has its back to do with the whole thing? The squirrel was on the tree in the centre of the glade and you circled round the tree. In other words, you circled round the squirrel.’&lt;/p&gt;
      &lt;p&gt;‘Oh no, I didn’t. Let us assume that I’m circling round you and you keep turning, showing me just your face. Call that circling round you?’&lt;/p&gt;
      &lt;p&gt;‘Of course, what else can you call it?’&lt;/p&gt;
      &lt;p&gt;‘You mean I’m circling round you though I’m never behind you and never see your back?’&lt;/p&gt;
      &lt;p&gt;‘Forget the back! You’re circling round me and that’s what counts. What has the back to do with it?’&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;— Yakov Perelman, Mathematics Can Be Fun, 1927&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46462592</guid><pubDate>Fri, 02 Jan 2026 08:16:21 +0000</pubDate></item><item><title>Standard Ebooks: Public Domain Day 2026 in Literature</title><link>https://standardebooks.org/blog/public-domain-day-2026</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46462702</guid><pubDate>Fri, 02 Jan 2026 08:40:41 +0000</pubDate></item><item><title>Going immutable on macOS, using Nix-Darwin</title><link>https://carette.xyz/posts/going_immutable_macos/</link><description>&lt;doc fingerprint="2a945a3cf7adab95"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Going immutable on macOS&lt;/head&gt;
    &lt;p&gt;· 8 min read&lt;/p&gt;
    &lt;p&gt;With no surprise the end of one year marks the start of the next.&lt;lb/&gt; And the beginning of a year is always synonymous with… a fresh macOS system!&lt;/p&gt;
    &lt;p&gt;But managing a good working environment on macOS has long been a game of “hope for the best.” We’ve all been there: a &lt;code&gt;curl | sh&lt;/code&gt; here, a manual &lt;code&gt;brew install&lt;/code&gt; there, and six months later, you’re staring at a broken &lt;code&gt;PATH&lt;/code&gt; and a Python environment that seems to have developed its own consciousness.&lt;/p&gt;
    &lt;p&gt;I’ve spent a lot of time recently moving my entire workflow into a declarative system using nix. From my zsh setup to my odin toolchain, here is why the transition from the imperative world of Homebrew to the immutable world of nix-darwin has been both a revelation and a fight.&lt;/p&gt;
    &lt;head rend="h2"&gt;The problem to solve: imperative rot #&lt;/head&gt;
    &lt;p&gt;Homebrew is great.&lt;lb/&gt; From the Linux world, it was the perfect missing package manager for macOS at first, promoted by a lot of developers, and it (mostly) works. But Homebrew has a problem: it is imperative.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;brew install&lt;/code&gt; you are changing the state of your machine in a way that is difficult to reverse or replicate exactly.&lt;lb/&gt; As an example, if I set up a new Mac today, &lt;code&gt;brew install neovim&lt;/code&gt; might give me version 0.10. However, if I do it six months from now, the version might changed and I had to reconfigure some components because of that change. The consequence is me, spending a few hours debugging my environment instead of writing code.&lt;/p&gt;
    &lt;p&gt;A solution to that is postfix every brew package by its version, but this is not possible for every package.&lt;/p&gt;
    &lt;p&gt;Another solution is system immutability and the Nix store.&lt;/p&gt;
    &lt;head rend="h2"&gt;System immutablility, and the Nix store #&lt;/head&gt;
    &lt;p&gt;Nix approaches the problem from a functional programming perspective.&lt;lb/&gt; Your system is not a collection of side effects, but a pure function of your configuration.&lt;/p&gt;
    &lt;p&gt;This allows different strong points like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;reproducibility: Every package lives in &lt;code&gt;/nix/store&lt;/code&gt;with a unique hash. This means I can have three different versions of the Odin compiler side-by-side, and they will never see each other, or even mess up with the different versions.&lt;/item&gt;
      &lt;item&gt;rollbacks: If a system update breaks my shell, I don’t panic. I just boot into a previous generation. The previous state of my system is still sitting in the store, untouched.&lt;/item&gt;
      &lt;item&gt;flakes: Using &lt;code&gt;flake.lock&lt;/code&gt;, I pin my setup to specific git commits. If it works on my laptop then it will work on yours. Period.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rollbacks on macOS exist, and are great to go back to a previous version “that works”.&lt;lb/&gt; As an example, to list all my previous generations, I can do:&lt;/p&gt;
    &lt;code&gt;&amp;gt; sudo nix-env --list-generations -p /nix/var/nix/profiles/system
   1   2025-12-28 01:21:38
   2   2025-12-28 01:33:47
   3   2025-12-28 09:34:27
   4   2025-12-28 09:43:15
   5   2025-12-28 11:00:49
   6   2025-12-28 11:10:12
   7   2025-12-28 11:19:03
   ...
&lt;/code&gt;
    &lt;p&gt;Each generation is also stored in a read-only specific volume on your system, which prevents accidental mutability of your packages.&lt;/p&gt;
    &lt;p&gt;On macOS &lt;code&gt;nix-darwin&lt;/code&gt; allows me to modify everything, from Finder settings to the Dock, or the Trackpad.&lt;/p&gt;
    &lt;p&gt;As an example, this is a part of my setup:&lt;/p&gt;
    &lt;code&gt;...
configuration = { pkgs, ... }: {
    # Firewall settings
    networking.applicationFirewall = {
        enable = true;
        enableStealthMode = true;
        allowSigned = true;
        allowSignedApp = true;
    };

    system.defaults = {
        CustomUserPreferences = {
          # Disable siri
          "com.apple.Siri" = {
            "UAProfileCheckingStatus" = 0;
            "siriEnabled" = 0;
          };
          # Disable personalized ads 
          "com.apple.AdLib" = {
            allowApplePersonalizedAdvertising = false;
          };
        };

        # Show battery percentage in the menu bar
        controlcenter.BatteryShowPercentage = true;

        # Allow touch to click
        trackpad.Clicking = true;

        # Hide the dock after a small delay of inactivity
        dock.autohide = true;
        dock.autohide-delay = 0.25;

        ...
    };
}
&lt;/code&gt;
    &lt;p&gt;And all those settings are documented in the official nix-darwin manual here.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Nix flake #&lt;/head&gt;
    &lt;p&gt;Before diving into the pros and cons of Nix, we need to address the engine under the hood: The Nix Flake.&lt;/p&gt;
    &lt;p&gt;In the traditional Nix world, things were a bit loose.&lt;lb/&gt; You had “channels”, but “channels” that could change behind your back… and we don’t want to get back to mutable systems.&lt;lb/&gt; Nix flakes fixed this by introducing a strictly defined structure.&lt;/p&gt;
    &lt;p&gt;Every Flake has a &lt;code&gt;flake.nix&lt;/code&gt; (the blueprint) and a &lt;code&gt;flake.lock&lt;/code&gt; (the time capsule).&lt;/p&gt;
    &lt;p&gt;The lockfile is actually the secret sauce.&lt;lb/&gt; It records the exact git commit of every dependency. When I build my system, I’m not just asking for “the latest version of the Odin programming language”, but for the exact version of Odin that existed at a specific moment in history.&lt;/p&gt;
    &lt;p&gt;So, when you run &lt;code&gt;darwin-rebuild switch --flake .&lt;/code&gt; to update your system (depending on configuration changes), Nix will look first at the lockfile.
If the lockfile says you are using &lt;code&gt;nixpkgs&lt;/code&gt; from October 12th at 10:45 PM, that is exactly what it fetches.
Nix doesn’t matter if you are on a new MacBook or an old iMac. If the lockfile is the same, then the resulting environment will be identical to your previous setup.&lt;/p&gt;
    &lt;p&gt;This turns system administration into a pure function: the same inputs always produce the same result.&lt;/p&gt;
    &lt;head rend="h2"&gt;The “get and forget” workflow #&lt;/head&gt;
    &lt;p&gt;One of the most liberating features is the “ephemeral shell” provided by Nix.&lt;lb/&gt; Imagine you need to test a script against &lt;code&gt;Python 3.15&lt;/code&gt;, but you don’t want to install a pre-release version globally and risk breaking your system tools…&lt;/p&gt;
    &lt;p&gt;With Nix, you run:&lt;/p&gt;
    &lt;code&gt;nix shell nixpkgs#python314
&lt;/code&gt;
    &lt;p&gt;Suddenly, &lt;code&gt;python --version&lt;/code&gt; returns 3.14.
You run your tests, you exit the shell, and it’s gone.&lt;/p&gt;
    &lt;p&gt;No leftovers, no site-packages conflicts, no trace. It is the ultimate “get and forget” utility, without messing up your system.&lt;/p&gt;
    &lt;p&gt;However be careful as “ephemeral” does not mean “sandboxed” or “isolated” in this context! A common misconception is that &lt;code&gt;nix shell&lt;/code&gt; is like a Docker container or a VM, but it is not as &lt;code&gt;nix shell&lt;/code&gt; only isolated the environment variables, and is still running directly on the darwin kernel.&lt;/p&gt;
    &lt;head rend="h2"&gt; The learning &lt;del rend="overstrike"&gt;curve&lt;/del&gt; cliff # &lt;/head&gt;
    &lt;p&gt;Is this configuration exempt from defaults? Absolutely not.&lt;lb/&gt; In fact, its biggest “default” is one of the most significant pain points in modern software: it arrives wrapped in complicated concepts and dense foundations.&lt;/p&gt;
    &lt;p&gt;The learning curve for Nix is actually not a curve, but a vertical cliff face.&lt;/p&gt;
    &lt;p&gt;Unlike Homebrew, where you just type a command and forget it, Nix requires you to learn a domain-specific language called… &lt;code&gt;Nix&lt;/code&gt; (I know, it is confusing). You have to understand how symbolic links work on macOS, how the Nix daemon interacts with Apple’s read-only system volume, and why your applications don’t magically appear in Spotlight.&lt;/p&gt;
    &lt;p&gt;For instance, getting GUI apps to show up in the Applications folder requires “trampoline” binaries or custom activation scripts using &lt;code&gt;mkalias&lt;/code&gt;.&lt;lb/&gt; After a few minutes debugging and scrolling forums it works great, but many users will simply stop there and walking away with the mistaken impression that &lt;code&gt;nix-darwin&lt;/code&gt; is an immature project, rather than a deeply powerful one.&lt;/p&gt;
    &lt;p&gt;The documentation is great, but maybe not well arranged.&lt;lb/&gt; As an example, this is the manual to write your own &lt;code&gt;flake.nix&lt;/code&gt;: https://nix-darwin.github.io/nix-darwin/manual/.
You can be lost very easily in the manual if you do not know what you are looking for…&lt;/p&gt;
    &lt;head rend="h2"&gt;The hybrid reality #&lt;/head&gt;
    &lt;p&gt;Despite my love for Nix, it isn’t perfect for everything. macOS GUI applications like Thunderbird, Firefox, or CrossOver, often fight against the read-only nature of the Nix store.&lt;lb/&gt; Those software expect to self-update and live in &lt;code&gt;/Applications&lt;/code&gt; on your system.&lt;/p&gt;
    &lt;p&gt;I found the sane path to be an hybrid approach: I use &lt;code&gt;nix-darwin&lt;/code&gt; to manage my critical state (compilers, LSPs, shell aliases), and use the homebrew module inside my Nix config to handle the big GUI pieces. It looks like this:&lt;/p&gt;
    &lt;code&gt;// file: brew.nix
homebrew = {
  enable = true;
  casks = [ 
    "firefox" 
    "thunderbird" 
    "crossover" 
  ];
};
&lt;/code&gt;
    &lt;p&gt;That’s it! And using this, I am sure Nix will use &lt;code&gt;brew&lt;/code&gt; to install those software on my system, as I did manually with my &lt;code&gt;brew install&lt;/code&gt; scripts.&lt;/p&gt;
    &lt;p&gt;This gives me the best of both worlds: Homebrew handles the macOS-specific integration, but Nix still “records” that they should exist in my declarative blueprint.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to start #&lt;/head&gt;
    &lt;p&gt;If you want to move away from “hope-based” configuration:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;install the Determinate Nix Installer: it handles the messy macOS multi-user setup for you.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;initialize a Flake and&lt;/p&gt;&lt;code&gt;home-manager&lt;/code&gt;: Create a&lt;code&gt;flake.nix&lt;/code&gt;and a&lt;code&gt;home.nix&lt;/code&gt;for your home-manager instance.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;think “modularity”, like keeping your system settings in&lt;/p&gt;&lt;code&gt;system.nix&lt;/code&gt;, your dev tools in&lt;code&gt;dev.nix&lt;/code&gt;, and your GUI apps in&lt;code&gt;brew.nix&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;version everything and commit your&lt;/p&gt;&lt;code&gt;flake.lock&lt;/code&gt;! It is the source of truth for your system.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you are curious I’ve documented my current setup, including my security hardening (firewall, stealth mode, etc.) here: k0pernicus/dotfiles/nix-darwin-config.&lt;/p&gt;
    &lt;p&gt;Keep in mind this configuration is personal, and might not be interesting (and certainly not useful) for everyone.&lt;/p&gt;
    &lt;p&gt;A few links that helped me to make a first setup version for my mac:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;https://nixcademy.com/posts/nix-on-macos/&lt;/item&gt;
      &lt;item&gt;https://dreamsofcode.io/blog/nix-darwin-my-favorite-package-manager-for-macos&lt;/item&gt;
      &lt;item&gt;https://github.com/torgeir/nix-darwin&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s more work upfront, certainly. But there is a profound peace of mind in knowing that your system is defined by code, not by a history of forgotten terminal commands.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46462719</guid><pubDate>Fri, 02 Jan 2026 08:42:34 +0000</pubDate></item></channel></rss>