<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 16 Oct 2025 18:14:29 +0000</lastBuildDate><item><title>Jiga (YC W21) Is Hiring Full Stacks</title><link>https://www.workatastartup.com/jobs/44310</link><description>&lt;doc fingerprint="335ec57ff13aea3c"&gt;
  &lt;main&gt;
    &lt;p&gt;Jiga transforms the traditional way manufacturers do business.&lt;/p&gt;
    &lt;p&gt;We're building a digital platform that streamlines the complex, inefficient process of procuring manufactured parts directly from suppliers, making it automated, collaborative, and data-driven.&lt;/p&gt;
    &lt;p&gt;Now that I’ve got your attention with some AI cringe, read the job posting carefully before applying.&lt;/p&gt;
    &lt;p&gt;About Jiga:&lt;/p&gt;
    &lt;p&gt;We are on a mission to help engineers build physical products faster. Imagine the &lt;code&gt;npm install&lt;/code&gt; for mechanical engineers.&lt;/p&gt;
    &lt;p&gt;How we work:&lt;/p&gt;
    &lt;p&gt;Remote: We are a fully remote company with team members from over 5 countries.&lt;/p&gt;
    &lt;p&gt;Culture: We never count hours and measure team members by performance and communication only. We hate micro-management and we 100% trust team members to perform tasks and to be honest with each other.&lt;/p&gt;
    &lt;p&gt;We play online games weekly together, encourage people to ask the hard questions, and we fly everyone once per year for our annual offsite in a beautiful place in the nature.&lt;/p&gt;
    &lt;p&gt;Meetings: We have a no-BS-meeting policy. We will have one weekly with the whole company and another one with the dev team. That's it.&lt;/p&gt;
    &lt;p&gt;Engineering Values:&lt;/p&gt;
    &lt;p&gt;Funding: Fully funded with significant, growing revenue. We are transparent about our runway.&lt;/p&gt;
    &lt;p&gt;You should apply if&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;p&gt;Benefits&lt;/p&gt;
    &lt;p&gt;No, we don’t have Friday happy hours or a fridge packed with 10 different flavors of ice cream in our office. In fact, we don’t have an office.&lt;/p&gt;
    &lt;p&gt;We do offer:&lt;/p&gt;
    &lt;p&gt;How to apply&lt;/p&gt;
    &lt;p&gt;Please send a short blurb about yourself and your favorite ice cream flavor (mine is cookies)&lt;/p&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;fulltimeUS / Remote (US)Full stack$13+ years&lt;/p&gt;
    &lt;p&gt;fulltimeIL / Remote (IL)3+ years&lt;/p&gt;
    &lt;p&gt;fulltimeRemoteFull stack3+ years&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604308</guid><pubDate>Thu, 16 Oct 2025 12:00:45 +0000</pubDate></item><item><title>Skibidi Toilet and the monstrous digital</title><link>https://journal.media-culture.org.au/index.php/mcjournal/article/view/3108</link><description>&lt;doc fingerprint="a14393dca9e08f91"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Skibidi Toilet began as an animated YouTube Web series early in 2023 that quickly spiralled into a wildly popular cultural phenomenon sprouting fandoms, wikis, threads, merchandise, and its very own moral panic (McKinnon and Harmon). It has recently grabbed the attention of Hollywood, and there are rumours that it is on its way to TV and a possible film treatment by Michael Bay (Wallenstein and Steiner). The episodes are short, surreal videos featuring bizarre, monstrous characters embroiled in violent clashes—to the non-stop repetition of “skibidi dom dom dom yes yes.” Kids love it, and some parents want it banned (10Play). This article will think about how we might read Skibidi’s playfulness with ‘humanoid surveillance robots’ and other tropes and strange creatures, as the Skibidi fandom is flirting with fairly dense social and political issues, such as the global surveillance apparatus and the potential corporate annihilation of the ‘natural’ world.&lt;/p&gt;
    &lt;p&gt;The series follows an increasingly epic war between two factions: the antagonists who take the form of human-headed singing toilets, led by G-man—or G-Toilet—and a group of mechanical humanoids with cameras, TVs, and speakers for heads, called The Alliance (or informally, Cameraheads). The bulk of the early action takes place in a generic cityscape littered with grey office buildings, called Metropolis (Skibidi Toilet Wiki), which conjures dystopian visions of a world stripped of vibrancy and plant life (except for what looks like an artificial lawn). An unlicensed mashup of the songs ‘Give It to Me’ by Timbaland and ‘Dom Dom Yes Yes’ by Bulgarian artist Biser King created by TikTok user @doombreaker03 appears in each episode as the theme of the Skibidi Toilets, further remixed with other familiar themes in various episodes, like "The Imperial March (Darth Vader’s Theme)" from Star Wars. The actual word ‘Skibidi’ seems to be taken from the viral hit of the same name from the band Little Big, ostensibly, meaning absolutely nothing. The first episodes were released through YouTube shorts, with multiple episodes clustered as one-minute “seasons” in vertical format. As the popularity of Skibidi Toilet grew, the story has become increasingly elaborate, and the individual episodes have transitioned to wide-screen format with longer run times. The total run time (as of September 2024) for all 76 episodes is 1 hour and 55 minutes.&lt;/p&gt;
    &lt;p&gt;The creator is Georgian Youtuber and animator Alexey Gerasimov, who has been sharing videos on social media under the moniker DaFuq?!Boom! since 2017. His absurdist creations are typically made using the Source Filmmaker (SFM) tool (considered a fairly basic computer graphics software tool available for free), with crude animations featuring non-playable characters (NPCs) from the Half-Life 2 video game. Source Filmmaker content can be seen as an evolution of the machinima creations of the noughties and 2010s. The word ‘machinima’ is a portmanteau of ‘machine’ and ‘cinema’ and refers to fans using game engines or gaming platforms to create their own stories in real-time (Harwood).&lt;/p&gt;
    &lt;p&gt;Skibidi Toilet has grown out of a context of infinitely converging technologies and subcultures. It is also born from the themes, software, content, and media that are most popular in gamer/maker communities, a high proportion of which are populated by younger people with a strong uptake in collaborative gaming trends (Schomer; Crowe and Bradford). This younger demographic is popularly referred to as Gen Alpha, representing those born after the year 2000—the first cohort to live entirely within the twenty-first century’s digital environment (Boczkowski and Mitchelstein). Machinima writer Katie Salen calls this the “generation of kids born into games” (38). As such, it is appropriate to read Skibidi as an epicentre of Gen Alpha sensibility. In an article revealing that Skibidi may be in development for a TV and film treatment by Michael Bay, Variety reporters Andrew Wallenstein and Robert Steiner remark that Skibidi is “explosive, violent and free of any discernible dialogue” and it is these qualities that have “won it a worldwide audience, not to mention the distinction of being a cultural icon Generation Alpha can truly call its own” (Wallenstein and Steiner).&lt;/p&gt;
    &lt;p&gt;Gen Alpha may be "born into video games", but they have also been born into a disintegrating climate system, post-9/11 politics (such as the global austerity crisis, perpetual war, and the rise of right-wing populism), surveillance capitalism, and pandemic risk—much of which can be read in the landscapes and metaphors of Skibidi Toilet, serving to problematise notions of the ‘natural’ and the limits of the human in the context of climate catastrophe and technological transformation. It is therefore our contention that Skibidi Toilet functions as an artefact that both produces and reflects cultural anxieties and troubles experienced by the Gen Alpha zeitgeist. Finally, due to the entangled nature of participatory culture—and the ways in which Skibidi takes co-created media to the extreme by way of circulating and re-circulated fan-generated content—we are reading both the actors of these networks (as audiences as much as producers) and the texts as conjoined digital artefacts (Mayer).&lt;/p&gt;
    &lt;head rend="h1"&gt;Artificiality and the Monstrous Digital&lt;/head&gt;
    &lt;p&gt;Skibidi Toilet features a banquet of monstrosities that distort a sense of the ‘natural’ world in some way. Skibidi Toilet not only features nightmarish hybrids that confuse boundaries between the organic, mechanical, and digital, but as a media artefact, it also takes place on and in the digital space. As such, we characterise this phenomenon within the scope of what we call the ‘monstrous digital’ in that Skibidi Toilet is relational to, and dependent on, the digital space in content, format, and sensibility. While the monstrous digital could be applied to an endless list of examples and subject matter in the story world of Skibidi Toilet, we apply it specifically to analyse the threat suggested by humanity’s tenuous relationship to the artificial humanoids and simulated landscapes that imply the potential of a world stripped of coherent ‘natural’ forms.&lt;/p&gt;
    &lt;p&gt;In the Skibidi universe, the lines between humanity and technology are blurred, and the monstrous digital always has a hint of human quality. This theme is exemplified by the faction known as The Alliance which consists of mechanical humanoids with cameras, TVs, and speakers for heads. In the lore, many fans agree that The Alliance (Cameraheads) are not fully human but artificially produced human constructs made by people to fight the Toilets (ostensibly) on behalf of humans (Skibidi Toilet Wiki). In this reading, they are mutant offspring of the human world, in that humans have borne these hybrid creatures in the ‘image of the human’. They function as spectral abominations, perhaps future echoes of ‘us’ that reflect our human body types and our way of thinking about war and media, which, when taken to the extreme, descends into Baudrillardian nightmare in which the violence of war has no objective “but to prove its very existence” (Baudrillard 32). This is to say that the Cameraheads are not recording a war ‘that happens’ but producing a war so that it can be recorded, in order to prove that war exists in the way that serves the human agenda.&lt;/p&gt;
    &lt;p&gt;The anxieties about becoming—or producing—a warped chimeric aberration of our own media technology is emphasised when we consider that The Alliance is donned with specific technologies of surveillance and archive—namely CCTV. The digital becomes monstrous in the sense of their uncanny capacity to record and store everything we do with them, and people in turn record and document their entire lives in media—while we are simultaneously fretting about the impact of widespread surveillance and the loss of privacy involved (Deuze Life in Media). The discomfort is that The Alliance is not a separate artefact from the human world but its evolution. Perhaps we have poured so much of ourselves into the digital archiving and surveilling of ourselves—and that so much of the digital permeates our lives—that the next evolutionary step is a merging with those very technologies of surveillance and archive, or at the very least not knowing what would constitute meaningful boundaries between the two any longer. Perhaps we have already passed that point of no return, and all that is left to us is coming to terms with the monstrous digital, that is us. The Cameraheads represent a potential that humans have become so merged with their media technology—especially surveillance technology—that they have evolved into that technology and are not only living through it but become it, embody it.&lt;/p&gt;
    &lt;p&gt;This anxiety resonates with the paradigm of the contemporary ‘media life’ put forth by Mark Deuze, in that “a media life can be seen as living in the ultimate archive, a public library of (almost) everything, embodying a personalized experience of all the information of the universe. At the same time, in media life the archive is alive, in that it is subject to constant intervention by yourself and others” (Deuze Media Life xv). When read in relation to Deuze’s theoretical intervention, the Skibidi universe is a way of displacing the anxiety about our ‘life in media’ onto the figure of the Camerahead. Reading ourselves as uncanny machines of the eternal digital archive is difficult to confront, and so engaging with that possibility is undertaken through the work of the Camerahead as a monstrous digital avatar. This logic follows Piatti-Farnell and Peaty’s reminder that monsters are in fact metaphors, in that “they function both as warnings and as reminders of that which we fear … . Monsters are creatures of difference, but they are never far removed from our human worlds”. The monstrosity here is not that the figures of Skibidi are semi-artificial, but that we might be—or have the potential to become, or perhaps that we are already there—and it is exactly that ambivalence that generates uncanniness and brings forth the monster in us all. The robotic world is born from our organic one and thus these two worlds are always closer than we think.&lt;/p&gt;
    &lt;p&gt;Skibidi’s playfulness with ‘humanoid surveillance robots’ also brings forth Deuze’s remarks about the monstrous digital, in that “the ongoing fusion of information and organisms, of man and machine, and of media and life amplifies and accelerates a distinct notion of uncanniness in our daily perception of the world around us" (Deuze Media Life 26). Theoretical interrogations of the uncanny can be traced back to figures such as Heidegger and Freud—who remind us of how uncanniness is at the heart of the human experience. For the sake of brevity, we use Nicholas Royle’s broad definition to read the uncanny as a “crisis of the proper”, in which the uncanny points to and reflects perturbations of the natural order, things which “commingle the familiar and unfamiliar” (1). The Camerahead—as a figure of contemporary surveillance culture—pushes the naturalisation of everyday media technologies to the extreme until it becomes strange and uncanny. When read in this way, Skibidi Toilet is not devoid of substance and meaning, but embroiled in the cutting edge of conversations about how humanity may see itself as a mimicry of these creatures—at once human but with the potential to mutate into a spectre of our own media technology.&lt;/p&gt;
    &lt;p&gt;Having media technologies in place of the human head (especially in the case of the CCTV Cameraheads) also connotes self-surveillance as well as sousveillance (as we massively monitor each other). When coupled with the grey, concrete, office-block landscape of the Skibidi universe, the world reflects a dread about the political, economic, and social dimensions of the global surveillance apparatus and the associated impending corporate annihilation of the ‘natural’ world. Surveillance capitalism is the weapon of the corporate state, and its hegemony is directly coupled with the annihilation of ecological systems in so many ways, one of which is “in the form of massive energy costs for data centres, e-waste, and the mining of rare minerals” (Silverman 147). What little green is left is, first and foremost, man-made, and secondly primarily serving as the décor of the constructed mise-en-scène of the "human zoo", as Peter Sloterdijk describes our "anthropotechnological" context where we merge with technologies in a feeble attempt to control our human future. The degradation of natural habitats, the devastation of climate systems, and the loss of privacy grow bigger and more inevitable at what feels like a monstrous pace. Yet there is an ambivalence rooted in this global condition, in that one of the technocratic state’s greatest weapons against real and existential ‘threats’ is surveillance technology. Surveillance technology is used to counter terrorism and is also used in diverse, if sometimes problematic, ways to mitigate and control the spread of coronavirus (Glitsos). This ambivalence, or double-bind, is captured in the Skibidi narrative in that The Alliance is both the saviour of the humans in the Skibidi world—but also humanity’s greatest threat with its constant violence and single-minded destruction.&lt;/p&gt;
    &lt;p&gt;Interestingly, the need for ecological balance and a sense of privacy is something robots do not understand, and the line here between the human and the artificial is pronounced by the chaos, confusion, and destruction that characterises the Skibidi universe, especially considering that it is often unclear which faction is more problematic and dangerous to the ‘human world.’ For example, one fan theorist claims:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;I personally think the camera people are the bad guys. They resemble a corrupt government that tries to control everything. The toilets are citizens trying to knock down the corrupt government with the limited resources they have.&lt;/p&gt;
      &lt;p&gt;But if someone says the camera people are fighting an evil foreign power to save their city and people, that’s also logical.&lt;/p&gt;
      &lt;p&gt;You can interpret it in both ways. I think the society you're currently living in determines how you interpret it. (durjoy313)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The polysemy of the Skibidi text can take on many shapes and forms, and it may be exactly the randomness of the different sides and actors that allows for its popularity. It is not just good versus evil or beyond good and evil—the Skibidi Toilet story world is universally monstrous, which is not necessarily good or evil. The recurring theme in Skibidi is the sense of nihilism and cycles of inconsequential destruction (reflected in the unending gibberish of the Skibidi song itself). In the words of Lawrence May, popular media do such important work in “echoing the fears and anxieties of their social, political and cultural context” and that “the very environments we live within now evoke existential terror, and this state of ecological monstrosity has permeated popular media, including video games”. May is articulating the way that video games and new media can explore surprisingly complex and urgent political tensions, which we witness in the way that Skibidi (and its audiences) plays with notions of political economy and corruption.&lt;/p&gt;
    &lt;p&gt;The corporate domination of the natural world is manifest in the dystopian vision of the Skibidi architecture where “both the modernist city of skyscrapers and the sprawling suburban city carry the seeds of failure” (Ameel 14). The Skibidi landscape is notably devoid of plant and animal life but saturated in concrete and skyscrapers—the rooftops of which form an important point of action for many of the scenes and play a key role in manifesting the horror of a vanishing natural world. In Skibidi fandom, it is noted that the early episodes are “filled with many buildings/skyscrapers, two of which are extremely large and resemble the old Twin Towers in New York City from real life that collapsed in 2001” (Skibidi Toilet Wiki). This is a powerful totem in Gen Alpha sensibility, especially considering that Gen Alpha never knew a world with Twin Towers still standing. Instead, Gen Alpha is haunted by their absent presence, as the reverberations of the global ‘War on Terror’ are felt to this day. Young people exist in the wake of the fallen towers through the grim spectre of post-9/11 politics. This is key to Skibidi as a moment in the zeitgeist. Lieven Ameel reminds us:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;the idea of secular buildings rising to the skies as spatial embodiments of linear progress had long been suspect. Since Biblical and Mesopotamian accounts of the tower of Babel, the building of iconic high-rise buildings had led to accusations of pride and presumptions that would not go unpunished. The downfall of the WTC towers on the 11th of September, 2001, is only the most spectacular of recent examples that have been read in such vein. (Ameel 14)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It is not incidental that the Skibidi war takes place in what becomes a ruined version of our ‘man[sic]-made accomplishments’ that humanity pursued at the cost of a fragile planet. Lawrence May reflects on a similar point in his consideration of games such as Breath of the Wild and The Last of Us Part II about which he writes that these worlds are a vital way that individuals explore the simmering “horror of an aberrant and abjected near future” and he argues that “games can critically position players in relation to discourse and wider public debate about ecological issues and climate change” (May).&lt;/p&gt;
    &lt;p&gt;Finally, what do hostile, sentient, human-headed toilets have to do with … anything? Again, Gen Alpha is unique among other generations in that it is the first to be born in the twenty-first century, but it is also the first to be born into a world where the horror of the global pandemic shaped their consciousness at such critical developmental junctures and characterised their nascent experiences of the social fabric. There is so much to expand on here, not least of all the fact that during the pandemic, Gen Alpha’s entire learning and socialisation experience shifted to the mediated space in incongruent, sometimes isolating, and discombobulating ways. However, for the sake of brevity, we focus on the symbolic function of the toilet by way of Jon Stratton’s reading of toilet-related themes (in particular, panic-buying toilet paper) during the peak of the coronavirus pandemic (Stratton). In his work, Stratton unpicks the ways in which “pedestal toilets and toilet paper are key aspects of civilisation and the fear of the loss of toilet paper is connected to anxiety about social breakdown, the loss of civilisation” (145). Although ‘modern civilisation’ is often cast through the symbolism of objects like the steam train or the clock, it is really the flushing pedestal toilet—such as those featured in Skibidi—that encapsulates the way the West ‘sees itself’ as rising above primitivism through hygiene and cleanliness. Yet in Skibidi, the monstrous human head is literally in the toilet—and often getting flushed right down into the depths of its own foul pipes. One may ask if this is the way Gen Alpha sees humanity? This question is especially pertinent in that for all our fancy conceptualisations of the posthuman and the man-machine hybrid, what ‘remediates’ all of us truly back to the ostensibly organic is the simple fact that we all defecate. Once that space is colonised by robots and artificiality, there is truly nothing left but surrender to the monstrous digital.&lt;/p&gt;
    &lt;p&gt;In all its bizarre, monstrous glory, Skibidi Toilet can be read as Gen Alpha’s creative expression of the ‘trouble’ brought about by emergent artificial life forms and our (potential) dystopian futures—and with our essay, we argue that it is fruitful to stay with that trouble, as Donna Haraway so powerfully reminds us to do. Skibidi is made in and through the very media platforms that provoke the inherent tension in the idea of the monstrous digital. If, as Jeffrey Cohen suggests, the monster is born “as an embodiment of a certain cultural moment, of a time, a feeling, and a place” (Cohen 4), then camera-headed humanoids and their foes—toilet people—speak volumes about the anxieties and both fun and fearful frames of reference plaguing Gen Alpha.&lt;/p&gt;
    &lt;head rend="h2"&gt;References&lt;/head&gt;
    &lt;p&gt;10Play. "Primary School Bans Gen Alpha Slang like 'Gyatt' and 'Skibidi'." Network Ten, 22 Aug. 2024. &amp;lt;https://www.10play.com.au/theproject/articles/primary-school-bans-gen-alpha-slang-like-gyatt-and-skibidi/tpa240822qgtex&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Ameel, Lieven. "Cities Utopian, Dystopian, and Apocalyptic." The Palgrave Handbook of Literature and the City, ed. John Tambling. Palgrave Macmillan, 2016. &amp;lt;https://doi.org/10.1057/978-1-137-54911-2_49&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Baudrillard, Jean. The Gulf War Did Not Take Place. Indiana UP, 1995.&lt;/p&gt;
    &lt;p&gt;Boczkowski, Pablo J., and Eugenia Mitchelstein. The Digital Environment: How We Live, Learn, Work, and Play Now. MIT Press, 2021. &amp;lt;https://doi.org/10.7551/mitpress/13602.001.0001&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Cohen, Jeffrey Jerome. Monster Theory. U of Minnesota P, 1996.&lt;/p&gt;
    &lt;p&gt;Crowe, Nicola, and Simon Bradford. "Identity and Structure in Online Gaming: Young People’s Symbolic and Virtual Extensions of Self." Youth Cultures: Scenes, Subcultures and Tribes, eds. Paul Hodkinson and Wolfgang Deicke. Routledge, 2007. 225–238.&lt;/p&gt;
    &lt;p&gt;Cunningham, Stuart, et al. "YouTube, Multichannel Networks and the Accelerated Evolution of the New Screen Ecology." Convergence 22.4 (2016): 376-391.&lt;/p&gt;
    &lt;p&gt;Deuze, Mark. Media Life. Polity, 2012.&lt;/p&gt;
    &lt;p&gt;———. Life in Media. MIT P, 2021.&lt;/p&gt;
    &lt;p&gt;durjoy313. "Skibidi Toilet: Who Is Real Evil?" Reddit, 2023. &amp;lt;https://www.reddit.com/r/FanTheories/comments/157odr2/comment/k9ry7ua/&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Glitsos, Laura. "COVID-19 and the ‘Perfectly Governed City.’" Journal for Cultural Research 25.3 (2021): 270-286. &amp;lt;https://doi.org/10.1080/14797585.2021.1943816&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Harwood, Tracy. "Machinima as a Learning Tool." Digital Creativity 24.3 (2013): 168–181. &amp;lt;https://doi.org/10.1080/14626268.2013.813375&amp;gt;.&lt;/p&gt;
    &lt;p&gt;"Mad Max and Fallout." Nukapedia: The Fallout Wiki, 2017. &amp;lt;https://fallout.fandom.com/f/p/3079543560349943637&amp;gt;.&lt;/p&gt;
    &lt;p&gt;May, Lisa. "Confronting Ecological Monstrosity." M/C Journal 24.5 (2021). &amp;lt;https://doi.org/10.5204/mcj.2827&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Mayer, V. "The Places Where Audience Studies and Production Studies Meet." Television &amp;amp; New Media 17.8 (2016): 706-718.&lt;/p&gt;
    &lt;p&gt;McKinnon, Aaron, and Scott Harmon. "Skibidi Toilet: What Is This Bizarre Viral YouTube Series – and Does It Deserve the Moral Panic?" The Guardian, 22 Jan. 2024. &amp;lt;https://ww.theguardian.com/culture/2024/jan/22/skibidi-toilet-youtube-series-viral&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Piatti-Farnell, Lorna, and Gerry Peaty. "Monster." M/C Journal 24.5 (2021). &amp;lt;https://doi.org/10.5204/mcj.2851&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Royle, Nicholas. The Uncanny. Routledge, 2003.&lt;/p&gt;
    &lt;p&gt;Salen, Katie. "Arrested Development: Why Machinima Can’t (or Shouldn’t) Grow Up." The Machinima Reader, eds. Henry Lowood and Michael Nitsche. MIT P, 2011. 37-50.&lt;/p&gt;
    &lt;p&gt;Schomer, Alex. "Gen Alpha Media Habits Come into Focus in Paramount Study." Variety, 6 Feb. 2024. &amp;lt;https://variety.com/vip/gen-alpha-media-habits-come-into-focus-paramount-study-1235898229/&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Silverman, Jacob. "Privacy under Surveillance Capitalism." Social Research: An International Quarterly 84.1 (2017): 147-164. &amp;lt;https://dx.doi.org/10.1353/sor.2017.0010&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Skibidi Toilet Wiki. "Metropolis." &amp;lt;https://skibidi-toilet.fandom.com/wiki/Metropolis&amp;gt;.&lt;/p&gt;
    &lt;p&gt;———. "Rooftops." &amp;lt;https://skibidi-toilet.fandom.com/wiki/Rooftops&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Sloterdijk, Peter. The Elmauer Rede: Rules for the Human Zoo. A Response to the Letter on Humanism. &amp;lt;https://web.stanford.edu/~mvr2j/docs/Elmauer.pdf&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Stratton, Jon. "Coronavirus, the Great Toilet Paper Panic and Civilisation." Thesis Eleven 165.1 (2021): 145-168. &amp;lt;https://doi.org/10.1177/07255136211033167&amp;gt;.&lt;/p&gt;
    &lt;p&gt;Wallenstein, Andrew, and Robert Steiner. "'Skibidi Toilet' Film and TV Franchise in the Works from Michael Bay, Adam Goodman – Listen to the Exclusive Interview." Variety, 25 July 2024. &amp;lt;https://www.variety.com/2024/film/news/skibidi-toilet-michael-bay-movie-adam-goodman-1236077245/&amp;gt;.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604372</guid><pubDate>Thu, 16 Oct 2025 12:11:18 +0000</pubDate></item><item><title>A stateful browser agent using self-healing DOM maps</title><link>https://100x.bot/a/a-stateful-browser-agent-using-self-healing-dom-maps</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604451</guid><pubDate>Thu, 16 Oct 2025 12:21:36 +0000</pubDate></item><item><title>Hyperflask – Full stack Flask and Htmx framework</title><link>https://hyperflask.dev/</link><description>&lt;doc fingerprint="fa8a766ea56e588d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Backend-driven interactive apps&lt;/head&gt;
    &lt;p&gt;Hyperflask is built on top of Flask, a popular Python web framework. It is easy to use and master. Backend-driven apps ensure straighforward state management and limit a lot of footguns from frontend-heavy apps. Combined with HTMX and a component system, creating interactive apps is easier than ever.&lt;/p&gt;
    &lt;head rend="h2"&gt;A powerful component system&lt;/head&gt;
    &lt;p&gt;Hyperflask introduces component-driven architecture to Flask apps. Seamlessly create frontend (web components, react, etc...) and backend components and use them in your jinja templates. Use HTMX to create server-backed interactive components.&lt;/p&gt;
    &lt;head rend="h2"&gt;File-based routing&lt;/head&gt;
    &lt;p&gt;Hyperflask extends Flask with many powerful features, notably file-based routing using a new file format that combines python code and a jinja template (inspired by Astro pages).&lt;/p&gt;
    &lt;head rend="h2"&gt;Build beautiful UIs&lt;/head&gt;
    &lt;p&gt;Hyperflask includes beautiful components provided by daisyUI and icons by Bootstrap Icons. Use Tailwind to customize styling. Create beautiful UIs in minutes without any CSS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Batteries included&lt;/head&gt;
    &lt;p&gt; Send emails using MJML, run background jobs, send push events using SSE, translations, authentication, content streaming, optimized images, ... &lt;lb/&gt;Everything you need to build a product ! &lt;/p&gt;
    &lt;head rend="h2"&gt;Content driven when needed&lt;/head&gt;
    &lt;p&gt;Hyperflask can be used to generate static web sites. It can also run in an hybrid mode where the server is accessed only for dynamic requests.&lt;/p&gt;
    &lt;head rend="h2"&gt;No messing around with your environment&lt;/head&gt;
    &lt;p&gt;Dev and prod environment are standardized on containers. With a tight integration with VS Code, everything is easy to setup and run. Easily deploy to VPS and various cloud services.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ensuring a thriving ecosystem&lt;/head&gt;
    &lt;p&gt;The Hyperflask framework itself is a small code base. It combines many Flask extensions in a seamless manner. All extensions and related projects are developed independently of the framework under the Hyperflask organization. Feel free to pick and choose the part you prefer from Hyperflask and use them in your own projects.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604673</guid><pubDate>Thu, 16 Oct 2025 12:46:28 +0000</pubDate></item><item><title>Launch HN: Inkeep (YC W23) – Collaborative agent builder for devs and non-devs</title><link>https://github.com/inkeep/agents</link><description>&lt;doc fingerprint="85708f8bb901b03a"&gt;
  &lt;main&gt;
    &lt;p&gt;With Inkeep, you can build and ship AI Agents with a No-Code Visual Builder or TypeScript SDK. Agents can be edited in code or no-code with full 2-way sync, so technical and non-technical teams can create and manage their Agents in a single platform.&lt;/p&gt;
    &lt;p&gt;Inkeep Agents can operate as real-time AI Chat Assistants, for example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a customer experience agent for customer support, technical docs, or in-app product copilot&lt;/item&gt;
      &lt;item&gt;an internal copilot to assist your support, sales, marketing, ops, and other teams&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Agents can also be used for AI workflow automation like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Creating and updating knowledge bases, documentation, and blogs&lt;/item&gt;
      &lt;item&gt;Updating CRMs, triaging helpdesk tickets, and streamlining repetitive tasks&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To get started, see the docs.&lt;/p&gt;
    &lt;p&gt;A no-code drag-and-drop canvas designed to allow any team to create and manage teams of Agents visually.&lt;/p&gt;
    &lt;p&gt;A code-first approach for building and managing multi-agent systems. Engineering teams to build with the tools and developer experience they expect.&lt;/p&gt;
    &lt;code&gt;import { agent, subAgent } from "@inkeep/agents-sdk";

const helloAgent = subAgent({
  id: "hello-agent",
  name: "Hello Agent",
  description: "Says hello",
  prompt: 'Only reply with the word "hello", but you may do it in different variations like h3110, h3110w0rld, h3110w0rld! etc...',
});

export const basicAgent = agent({
  id: "basic-agent",
  name: "Basic Agent",
  description: "A basic agent",
  defaultSubAgent: helloAgent,
  subAgents: () =&amp;gt; [helloAgent],
});&lt;/code&gt;
    &lt;p&gt;The Visual Builder and TypeScript SDK are fully interoperable: your technical and non-technical teams can edit and manage Agents in either format and switch or collaborate with others at any time.&lt;/p&gt;
    &lt;p&gt;Inkeep Open Source includes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A Visual Builder &amp;amp; TypeScript SDK with 2-way sync&lt;/item&gt;
      &lt;item&gt;Multi-agent architecture to support teams of agents&lt;/item&gt;
      &lt;item&gt;MCP Tools with credentials management&lt;/item&gt;
      &lt;item&gt;A UI component library for dynamic chat experiences&lt;/item&gt;
      &lt;item&gt;Triggering Agents via MCP, A2A, &amp;amp; Vercel SDK APIs&lt;/item&gt;
      &lt;item&gt;Observability via a Traces UI &amp;amp; OpenTelemetry&lt;/item&gt;
      &lt;item&gt;Easy deployment to Vercel and using Docker&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For a full overview, see the Concepts guide.&lt;/p&gt;
    &lt;p&gt;The Inkeep Agent Platform is composed of several key services and libraries that work together:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;agents-manage-api: An API that handles configuration of Agents, Sub Agents, MCP Servers, Credentials, and Projects with a REST API.&lt;/item&gt;
      &lt;item&gt;agents-manage-ui: Visual Builder web interface for creating and managing Agents. Writes to the &lt;code&gt;agents-manage-api&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;agents-sdk: TypeScript SDK (&lt;code&gt;@inkeep/agents-sdk&lt;/code&gt;) for declaratively defining Agents and custom tools in code. Writes to&lt;code&gt;agents-manage-api&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;agents-cli: Includes various handy utilities, including &lt;code&gt;inkeep push&lt;/code&gt;and&lt;code&gt;inkeep pull&lt;/code&gt;which sync your TypeScript SDK code with the Visual Builder.&lt;/item&gt;
      &lt;item&gt;agents-run-api: The Runtime API that exposes Agents as APIs and executes Agent conversations. Keeps conversation state and emits OTEL traces.&lt;/item&gt;
      &lt;item&gt;agents-ui: A UI component library of chat interfaces for embedding rich, dynamic Agent conversational experiences in web apps.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Underneath the hood, the framework uses the Vercel AI SDK for interfacing with LLM providers. The &lt;code&gt;agents-sdk&lt;/code&gt;/ &lt;code&gt;agents-manage-api&lt;/code&gt; share many concepts with Vercel's &lt;code&gt;ai&lt;/code&gt; SDK, and &lt;code&gt;agents-run-api&lt;/code&gt; outputs a data stream compatible with Vercel's &lt;code&gt;useChat&lt;/code&gt; and AI Elements primitives for custom UIs.&lt;/p&gt;
    &lt;p&gt;The Inkeep Agent Framework is licensed under the Elastic License 2.0 (ELv2) subject to Inkeep's Supplemental Terms (SUPPLEMENTAL_TERMS.md). This is a fair-code, source-available license that allows broad usage while protecting against certain competitive uses.&lt;/p&gt;
    &lt;p&gt;Inkeep is designed to be extensible and open: you can use the LLM provider of your choice, use Agents via open protocols, and easily deploy and self-host Agents in your own infra.&lt;/p&gt;
    &lt;p&gt;If you'd like to contribute, follow our contribution guide.&lt;/p&gt;
    &lt;p&gt;Follow us to stay up to date, get help, and share feedback.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604700</guid><pubDate>Thu, 16 Oct 2025 12:50:08 +0000</pubDate></item><item><title>Electricity can heal wounds three times as fast (2023)</title><link>https://www.chalmers.se/en/current/news/mc2-how-electricity-can-heal-wounds-three-times-as-fast/</link><description>&lt;doc fingerprint="fedd408029bd4e42"&gt;
  &lt;main&gt;
    &lt;p&gt;Chronic wounds are a major health problem for diabetic patients and the elderly – in extreme cases they can even lead to amputation. Using electric stimulation, researchers in a project at Chalmers University of Technology, Sweden, and the University of Freiburg, Germany, have developed a method that speeds up the healing process, making wounds heal three times faster.&lt;/p&gt;
    &lt;p&gt;There is an old Swedish saying that one should never neglect a small wound or a friend in need. For most people, a small wound does not lead to any serious complications, but many common diagnoses make wound healing far more difficult. People with diabetes, spinal injuries or poor blood circulation have impaired wound healing ability. This means a greater risk of infection and chronic wounds – which in the long run can lead to such serious consequences as amputation.&lt;/p&gt;
    &lt;p&gt;Now a group of researchers at Chalmers and the University of Freiburg have developed a method using electric stimulation to speed up the healing process.&lt;/p&gt;
    &lt;p&gt;“Chronic wounds are a huge societal problem that we don’t hear a lot about. Our discovery of a method that may heal wounds up to three times faster can be a game changer for diabetic and elderly people, among others, who often suffer greatly from wounds that won’t heal,” says Maria Asplund, Professor of Bioelectronics at Chalmers University of Technology and head of research on the project.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Electric guidance of cells for faster healing&lt;/head&gt;
    &lt;p&gt;The researchers worked from an old hypothesis that electric stimulation of damaged skin can be used to heal wounds. The idea is that skin cells are electrotactic, which means that they directionally ‘migrate’ in electric fields. This means that if an electric field is placed in a petri dish with skin cells, the cells stop moving randomly and start moving in the same direction. The researchers investigated how this principle can be used to electrically guide the cells in order to make wounds heal faster. Using a tiny engineered chip, the researchers were able to compare wound healing in artificial skin, stimulating one wound with electricity and letting one heal without electricity. The differences were striking.&lt;/p&gt;
    &lt;p&gt;“We were able to show that the old hypothesis about electric stimulation can be used to make wounds heal significantly faster. In order to study exactly how this works for wounds, we developed a kind of biochip on which we cultured skin cells, which we then made tiny wounds in. Then we stimulated one wound with an electric field, which clearly led to it healing three times as fast as the wound that healed without electric stimulation,” Asplund says.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hope for diabetes patients&lt;/head&gt;
    &lt;p&gt;In the study, the researchers also focused on wound healing in connection with diabetes, a growing health problem worldwide. One in 11 adults today has some form of diabetes according to the World Health Organization (WHO) and the International Diabetes Federation.&lt;/p&gt;
    &lt;p&gt;“We’ve looked at diabetes models of wounds and investigated whether our method could be effective even in those cases. We saw that when we mimic diabetes in the cells, the wounds on the chip heal very slowly. However, with electric stimulation we can increase the speed of healing so that the diabetes-affected cells almost correspond to healthy skin cells,” Asplund says.&lt;/p&gt;
    &lt;head rend="h3"&gt;Individualised treatment the next step&lt;/head&gt;
    &lt;p&gt;The Chalmers researchers recently received a large grant which will allow them to continue their research in the field, and in the long run enable the development of wound healing products for consumers on the market. Similar products have come out before, but more basic research is required to develop effective products that generate enough electric field strength and stimulate in the right way for each individual. This is where Asplund and her colleagues come into the picture:&lt;/p&gt;
    &lt;p&gt;“We are now looking at how different skin cells interact during stimulation, to take a step closer to a realistic wound. We want to develop a concept to be able to ‘scan’ wounds and adapt the stimulation based on the individual wound. We are convinced that this is the key to effectively helping individuals with slow-healing wounds in the future,” Asplund says.&lt;/p&gt;
    &lt;head rend="h3"&gt;More about the study:&lt;/head&gt;
    &lt;p&gt;• “Bioelectronic microfluidic wound healing: a platform for investigating direct current stimulation of injured cell collectives” was published in the journal Lab on a Chip. The article was written by Sebastian Shaner, Anna Savelyeva, Anja Kvartuh, Nicole Jedrusik, Lukas Matter, José Leal and Maria Asplund. The researchers work at the University of Freiburg in Germany and Chalmers University of Technology. &lt;lb/&gt;• In their study, the researchers showed that wound healing on artificial skin stimulated with electric current was three times faster than on the skin that healed naturally. The electric field was low, about 200 mV/mm, and did not have a negative impact on the cells. &lt;lb/&gt;• The method the researchers developed is based on a microfluidic biochip on which artificial skin can be grown, stimulated with an electric current and studied in an effective and controlled manner. The concept allows researchers to conduct multiple experiments in parallel on the same chip. &lt;lb/&gt;• The research project began in 2018 and is funded by the European Research Council (ERC). The project was recently granted new funding so the research can get to market and benefit patients.&lt;/p&gt;
    &lt;head rend="h3"&gt;For more information, please contact:&lt;/head&gt;
    &lt;p&gt;Maria Asplund, Professor of Bioelectronics, Department of Microtechnology and Nanoscience at Chalmers University of Technology, Sweden&lt;lb/&gt;maria.asplund@chalmers.se, +46 31 772 41 14&lt;/p&gt;
    &lt;p&gt;Sebastian Shaner, PhD Candidate, Department of Microsystems Engineering at the University of Freiburg, Germany&lt;lb/&gt;sebastian.shaner@blbt.uni-freiburg.de&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Professor, Electronics Material and Systems, Microtechnology and Nanoscience&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45604779</guid><pubDate>Thu, 16 Oct 2025 12:59:08 +0000</pubDate></item><item><title>Lace: A New Kind of Cellular Automata Where Links Matter</title><link>https://www.novaspivack.com/science/introducing-lace-a-new-kind-of-cellular-automata</link><description>&lt;doc fingerprint="ea85981ebb574771"&gt;
  &lt;main&gt;
    &lt;p&gt;This article is about a new kind of simple computational rule (“LACE rules” running on LACE, the Link Automata Computing Engine platform) which, when applied locally on a grid of cells, demonstrates fascinating emergent “artificial life” behavior. &lt;lb/&gt;For readers familiar with the Game of Life (GOL), this is a next-level class of cellular automata that utilizes neighborhood topology — the state of the grid is a function of both cell states and their connectivity (links).&lt;lb/&gt;To quickly get a sense of what LACE does – scroll down in this post to the video Gallery section and take a look at (a) the Game of Life, with links, and then as a comparison, check out (b) the “Amazing Dragons” LACE rule, and the many other “Realm of LACE” (ROL) rules that fully utilize topology.&lt;/p&gt;
    &lt;head rend="h2"&gt;Preamble&lt;/head&gt;
    &lt;p&gt;I have been thinking about cellular automata ever since I read a fascinating and important book, Three Scientists and Their Gods, during the summer of my sophomore year of college. This book changed my life. &lt;lb/&gt;After reading this book ferociously on the drive home from Oberlin college to Boston, for summer vacation, I became obsessed with the idea of digital physics – and particularly with the work of Ed Fredkin, (and later Stephen Wolfram as well). &lt;/p&gt;
    &lt;p&gt;And so I decided I had to find a way to get into Ed Fredkin’s lab at MIT. This became my mission in life&lt;/p&gt;
    &lt;p&gt;Finally, after many attempts, I did managed to finagle a summer internship in his lab at MIT, thanks to the kindness of Professors Norman Margolus and Tommaso Toffolli, two of the great minds in the field. They had authored an MIT Press book that I had found and read countless times, Cellular Automata Machines, which at the time was the bible in the field.&lt;/p&gt;
    &lt;p&gt;In their lab they had built a specialized supercomputer, CAM-6, for digital physics simulations. I spent every minute of that summer in their lab on CAM-6, exploring the computational universe. &lt;lb/&gt;For years after that internship, I filled dozens of journals with detailed ideas and theories about digital physics. I wrote code, I tested ideas, I learned and explored. But nothing really came close to the richness and complexity of the vision I had experienced, or our actual natural world. &lt;lb/&gt;These digital models were lacking something, but I wasn’t sure what. &lt;lb/&gt;Then, years later, it became more clear. &lt;lb/&gt;The topology of spacetime is not merely a fixed set of locations and states, it’s a graph. The links between things are important. (Note: As Stephen Wolfram would later point out to me, this idea harkens back to the dialectical debate between the views of Newton and Leibniz).&lt;/p&gt;
    &lt;p&gt;I needed a new kind of model, one where the shape of space could evolve, one where the shape of space could affect matter, and where matter could affect the shape of space, because after all, according to Einstein, they are really the same thing.&lt;/p&gt;
    &lt;p&gt;And from this insight I then undertook decades of exploration into the idea of a new kind of cellular automata — one in which the cells had links to each other, which could also figure into their states. Both the cell states and link states could interact, forming complex topologies and geometries. &lt;lb/&gt;Decades went by, along with many experiments that did not bear fruit. &lt;lb/&gt;But last year, I actually built something that demonstrates this new class of CA rather nicely. &lt;lb/&gt;Now I’m finally getting around to posting it…&lt;/p&gt;
    &lt;head rend="h2"&gt;Introducing LACE: The Link Automata Computing Engine&lt;/head&gt;
    &lt;p&gt;LACE is a new experimental platform written in python, for exploring a class of cellular automata in which both the states of cells and their connections (links) to each other are subject to the rules.&lt;lb/&gt;In LACE, the state of a cell is a function of both its neighbors AND the links it has to them, and link states in turn are a function of the cells they connect.&lt;lb/&gt;LACE rules can use topological properties of cells and neighborhoods, such as number of connections, neighbor degree, and other metrics.&lt;/p&gt;
    &lt;p&gt;This enables rules in which virtual neighborhood topology (the links) affects neighborhood states — rules in which topology can both evolve and shape the behavior of the system, and in which the behavior of the system can shape topology.&lt;lb/&gt;The added topological dimension enables rules that can have more interesting behavior than traditional “cells-only” CA rules, opening up a fascinating new computational vista (within Wolfram’s, Ruliad) of new kinds of rules that generate new species of stable patterns – oscillators – gliders, puffers, and more.&lt;/p&gt;
    &lt;p&gt;LACE rules range from link-aware variants of Conway’s famous Game of Life, but where edges have varying degrees of influence, to completely new kinds of “Realm of LACE” rules that use topological metrics in their computations. In theory, these rules could even be utilized to simulate neural networks.&lt;/p&gt;
    &lt;p&gt;What is fascinating about these new link-aware “Realm of LACE rules” is that they exhibit amazing new forms of stability and periodic structure. They produce new kinds of periodic gliders, oscillators, puffers, and other kinds of phenomena. Some of them even have behaviors that resemble forms of “artificial life.”&lt;lb/&gt;For more details on how these rules work, get the repo and open various rules in the rule editor, where all their parameters are explained. There are many new classes of rules to experiment with.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Code&lt;/head&gt;
    &lt;p&gt;You can read more about LACE here.&lt;/p&gt;
    &lt;p&gt;You can get the LACE python code repo here.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Gallery&lt;/head&gt;
    &lt;p&gt;(NOTE – click on the videos and use the settings icon to set quality to 1080p for best visuals)&lt;/p&gt;
    &lt;p&gt;First, let’s take a look at Conway’s Game of Life, but with links visible. This demonstration simply shows the links and their states, but in this example, the links do not affect the states of cells. This shows that LACE can run traditional 2D cellular automata rules, with the links visible, but without the links changing rule behavior.&lt;lb/&gt;In more sophisticated LACE rules (scroll down), we will actually modify the behavior of the Life rule based on the links&lt;/p&gt;
    &lt;p&gt;And here are some Life gliders, doing their thing, with links visible…&lt;/p&gt;
    &lt;p&gt;And here is a stable pattern in Life… with links….&lt;/p&gt;
    &lt;p&gt;Now, here is a version of Life, where edges influence rule behavior…&lt;/p&gt;
    &lt;p&gt;Here’s a variant of Life where edges play even more of a role. &lt;lb/&gt;In LACE we can adjust the influence that links have on rule behavior – from none (links are merely decorative), to strong influence (links condition cell states).&lt;/p&gt;
    &lt;p&gt;Above, we looked at some traditional Life rule cellular automata. These examples illustrated varying levels of link-awareness in rules. But they are still not showing the full capabilities of the LACE platform.&lt;lb/&gt;Next, we turn to more advanced LACE rules that fully utilize the capabilities of the system…&lt;/p&gt;
    &lt;head rend="h2"&gt;The Realm of LACE: LACE Rules&lt;/head&gt;
    &lt;p&gt;Now, take a look at the “Amazing Dragons” rule in the Realm of LACE (ROL), one of the more interesting rules in this universe.&lt;/p&gt;
    &lt;p&gt;This is not a life rule – it’s a LACE rule – a fully topological rule.&lt;/p&gt;
    &lt;p&gt;Here links and neighborhood topology play a major role in the behavior of cell states and cell states modify the states of links as well.&lt;lb/&gt;(Here’s a little more detail on how rules work – see this and this and a longer explanation here)&lt;/p&gt;
    &lt;p&gt;…and look what happens!&lt;/p&gt;
    &lt;p&gt;LACE also supports an optional high-performance, GPU-accelerated mode using Taichi, for large-scale simulations…&lt;/p&gt;
    &lt;p&gt;And here’s another interesting LACE rule…&lt;/p&gt;
    &lt;p&gt;and check this out…&lt;/p&gt;
    &lt;p&gt;and this one….&lt;/p&gt;
    &lt;p&gt;And now a gallery of many other interesting LACE rules…&lt;/p&gt;
    &lt;p&gt;This has been a preview of the Realm of LACE … an incredible new class of cellular automata rules, where links matter and topology evolves.&lt;lb/&gt;Learn more by playing with the repo, and please share your discoveries 🙂&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45605153</guid><pubDate>Thu, 16 Oct 2025 13:33:45 +0000</pubDate></item><item><title>Why I Chose Elixir Phoenix over Rails, Laravel, and Next.js</title><link>https://akarshc.com/post/phoenix-for-my-project.html</link><description>&lt;doc fingerprint="b7d4d06b17b5be94"&gt;
  &lt;main&gt;
    &lt;p&gt;October 16, 2025 · by Akarsh&lt;/p&gt;
    &lt;p&gt;First things first, why do we code? To solve problems in the most optimal way possible.&lt;/p&gt;
    &lt;p&gt;For me, the number one factor is speed: both application speed and development speed. That’s exactly what led me to Phoenix LiveView.&lt;/p&gt;
    &lt;p&gt;If I had chosen React or Next.js with Laravel, or even Inertia.js with Laravel, I would have had to maintain both sides of the stack, frontend and backend. As a solo developer, I didn’t have the time to manage state in two different places. I needed a solid monolithic solution that could handle everything together.&lt;/p&gt;
    &lt;p&gt;So I looked into Laravel Livewire and Rails Hotwire. Both are great tools that simplify frontend work without depending too much on JavaScript. I even thought about going full JavaScript with Next.js, but I’ve never been a big fan of using JS on the backend.&lt;/p&gt;
    &lt;p&gt;Rails Hotwire really caught my attention, especially because of how fast you can build an MVP with Rails. But I still needed background jobs, real-time updates, and two-way communication that just works. Those things are possible in Rails and Laravel, but they take a bit more effort to set up.&lt;/p&gt;
    &lt;p&gt;Then I came across Elixir and its framework Phoenix. It had all the elegance of Ruby on Rails, but with far better performance. It came with built-in background jobs through Oban, a familiar and clean syntax, and something truly special called LiveView.&lt;/p&gt;
    &lt;p&gt;LiveView feels like the perfect balance between traditional server-rendered apps and frontend-heavy frameworks. It’s way ahead of both Rails Hotwire and Laravel Livewire. LiveView communicates through WebSockets, which means real-time two-way updates without sending new requests every time something changes. You can still use Alpine.js or any JavaScript library you want through hooks when needed.&lt;/p&gt;
    &lt;p&gt;Phoenix also comes with Oban jobs built in. You can declare background jobs easily, and when something fails, it automatically restarts without breaking the app. That’s the beauty of Elixir. It’s a compiled language built on top of Erlang, which powers highly concurrent systems like WhatsApp and Discord.&lt;/p&gt;
    &lt;p&gt;I’m not saying Phoenix is better than Laravel, Rails, or Next.js. All of these are excellent frameworks, and I’ve personally used them to build applications. Phoenix just turned out to be the best fit for my specific use case. This is my project - Hyperzoned.com&lt;/p&gt;
    &lt;p&gt;I don’t know who needs to hear this, but try exploring beyond what you already know. You might find a better and more efficient way to solve your next problem. After all, never stop learning.&lt;/p&gt;
    &lt;p&gt;Thanks for reading! You can find me on X or Hyperzoned.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45605291</guid><pubDate>Thu, 16 Oct 2025 13:48:33 +0000</pubDate></item><item><title>DoorDash and Waymo launch autonomous delivery service in Phoenix</title><link>https://about.doordash.com/en-us/news/waymo</link><description>&lt;doc fingerprint="f98ba259b3b2bde5"&gt;
  &lt;main&gt;
    &lt;p&gt;Today, DoorDash announced a new partnership with Waymo to launch an autonomous delivery service in Metro Phoenix and introduce a limited-time $10 Waymo promotion for DashPass members in Los Angeles, San Francisco, and Phoenix.&lt;/p&gt;
    &lt;p&gt;From now through the end of the year, DashPass members in these three cities can receive $10 off one Waymo ride per month.* A new promotion code will be issued at the start of each month through December 31, 2025.&lt;/p&gt;
    &lt;p&gt;Testing of the new autonomous delivery service in Metro Phoenix is now underway, with plans to launch broader commercial operations later this year. DoorDash consumers in the area may be matched with a fully autonomous Waymo vehicle for deliveries from participating merchants using DoorDash’s Autonomous Delivery Platform, the system that helps orchestrate different types of delivery methods together at scale, whether that’s Dashers, robots, drones, or Waymo. The service will begin with deliveries from DashMart, DoorDash’s owned and operated convenience, grocery, and retail store that also powers DashMart Fulfillment Services, with plans to expand over time.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“DashPass is designed to give consumers consistent value, convenience, and access to the best of their communities, and our partnership with Waymo builds on that promise,” said David Richter, Vice President of Business and Corporate Development at DoorDash. “Together, we’re giving members access to, and savings on, a new and delightful experience, while advancing our vision for a multi-modal autonomous future of local commerce.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;“We are excited to make everyday errands easier with the Waymo Driver, offering the added peace of mind that comes with our safe and reliable technology. Through our partnership with DoorDash, we leverage our proven delivery experience to provide customers with a seamless, contact-free way to get items they need, whether it’s groceries or a quick bite,” said Nicole Gavel, Head of Business Development and Strategic Partnerships at Waymo.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;DashPass offers exclusive deals, member-only benefits, and $0 delivery fees and reduced service fees on eligible orders from thousands of restaurants, grocery stores, and retailers. On average, DashPass members save $5 per eligible order and have collectively saved more than $10 billion globally since the program launched in 2018.&lt;/p&gt;
    &lt;p&gt;*On weekday rides booked between 2 a.m. and 2 p.m. Terms apply.&lt;/p&gt;
    &lt;p&gt;Forward-Looking Statements&lt;lb/&gt;This communication contains forward-looking statements within the meaning of Section 27A of the Securities Act of 1933, as amended, and Section 21E of the Securities Exchange Act of 1934, as amended. Forward-looking statements generally relate to future events, and such statements in this communication include, but are not limited to, expectations regarding the opportunity and expected benefits of the partnership between DoorDash and Waymo. Expectations and beliefs regarding these matters may not materialize, and actual results in future periods are subject to risks and uncertainties that could cause actual results to differ materially from those projected. For information on potential risks and uncertainties that could cause actual results to differ from any results predicted, please see DoorDash’s most recent Annual Report on Form 10-K and subsequent Quarterly Reports on Form 10-Q, each filed with the Securities and Exchange Commission.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45605501</guid><pubDate>Thu, 16 Oct 2025 14:04:18 +0000</pubDate></item><item><title>Tor browser removing various Firefox AI features</title><link>https://blog.torproject.org/new-alpha-release-tor-browser-150a4/</link><description>&lt;doc fingerprint="1d0ed8139e94f6de"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;New Alpha Release: Tor Browser 15.0a4&lt;/head&gt;
    &lt;p&gt;Tor Browser 15.0a4 is now available from the Tor Browser download page and also from our distribution directory.&lt;/p&gt;
    &lt;p&gt;This version includes important security updates to Firefox.&lt;/p&gt;
    &lt;head rend="h2"&gt;Release Candidate&lt;/head&gt;
    &lt;p&gt;If all goes as planned, this will be our last alpha release in the 15.0 series before it is promoted to stable in the last week of October. Next week we will be focusing primarily on QA and ensuring all the various features and scenarios supported in Tor Browser still work as expected. This QA work will be tracked in the following gitlab issues:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tor-browser#43984 - Tor Browser 15.0 Release QA - Desktop&lt;/item&gt;
      &lt;item&gt;tor-browser#43985 - Tor Browser 15.0 Release QA - Android&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we reach the home stretch, now would be a great time to download and try out Tor Browser Alpha! We would appreciate it if the community would evaluate and exercise these following changes:&lt;/p&gt;
    &lt;head rend="h3"&gt;ð¤ Removal of Various AI Features&lt;/head&gt;
    &lt;p&gt;Over the past year Mozilla has been working on integrating various AI features and integrations into Firefox (e.g. the AI chatbot sidebar). Such machine learning systems and platforms are inherently un-auditable from a security and privacy perspective. We also do not want to imply recommendation or promotion of such systems by including them in Tor Browser. Therefore, we have done what we can to remove such features from the browser.&lt;/p&gt;
    &lt;head rend="h3"&gt;âï¸ Rename &lt;code&gt;meek-azure&lt;/code&gt; pluggable-transport to just &lt;code&gt;meek&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;In the past, we have used various cloud platform to host &lt;code&gt;meek&lt;/code&gt; pluggable-transport backends including Google, Amazon, and Azure. However, as time passed these backends have moved and migrated and thus the cloud provider-specific name has become an historical artifact. Therefore, we have dropped the Azure part of the name and now just call it &lt;code&gt;meek&lt;/code&gt;. Let this be a lesson to you about naming things!&lt;/p&gt;
    &lt;head rend="h3"&gt;ðª Improved Dark Theme Support in Browser Chrome&lt;/head&gt;
    &lt;p&gt;We have improved the styling for our various Tor Browser-specific UI components for dark browser themes. All of our various purple elements should now look like they belong.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð¦ Removal/Replacement of New Firefox/Mozilla-specific Branding and Features&lt;/head&gt;
    &lt;p&gt;As part of ordinary incremental UI updates over the past year and the implementation of new features, Mozilla has added various new brand assets and service integrations. This includes things like those cute little fox graphics, Firefox Home, and the new History Sidebar. As of this release, there should not be any more Firefox or Mozilla specific branding, features, or service integrations accessible in Tor Browser. The new history sidebar in particular has been replaced with the legacy history panel from previous Tor Browser versions.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð§ Updated Emoji Font for Linux&lt;/head&gt;
    &lt;p&gt;We have included the Noto Color Emoji font with our Linux builds. Linux users should now have all the latest and greatest emoji provided by Noto Emoji.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð´ï¸ Improved CJK Glyph Rendering&lt;/head&gt;
    &lt;p&gt;At the suggestion of a cypherpunk, we have swapped out the Noto font family for Jigmo. This should allow more Chinese, Japanese, and Korean graphemes to render accurately in web content.&lt;/p&gt;
    &lt;head rend="h3"&gt;âï¸ Letterboxing Styling Improvements&lt;/head&gt;
    &lt;p&gt;We have tweaked our custom styling of the web-content letterboxing feature to confirm with and adapt to Firefox's own styling changes in Firefox 140. These tweaks should also play nicely with upstream's vertical tabs feature.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð« WebAssembly Restrictions Now Managed by NoScript&lt;/head&gt;
    &lt;p&gt;Historically, we have disabled WebAssembly globally when the browser is in the &lt;code&gt;Safer&lt;/code&gt; and &lt;code&gt;Safest&lt;/code&gt; security levels. However, with the latest Firefox version this has proven to be too aggressive, as doing so broke functionality in the built-in PDF reader. We therefore now rely on the NoScript extension built into the browser to handle disabling WebAssembly functionality in web content while the browser is in the &lt;code&gt;Safer&lt;/code&gt; and &lt;code&gt;Safest&lt;/code&gt; security levels, while also allowing WebAssembly to run unhindered in safe+privileged contexts like the PDF reader.&lt;/p&gt;
    &lt;head rend="h3"&gt;ð Stopped Hiding Protocol in URL on Desktop&lt;/head&gt;
    &lt;p&gt;Mozilla has reversed course on when the protocol portion (e.g. &lt;code&gt;http&lt;/code&gt; or &lt;code&gt;https&lt;/code&gt;) of the URL in the URL bar is hidden since Firefox 128. We used to have logic in one of our patches around Onion Services (which are always end-to-end encrypted regardless of the application-level protocol used) to follow whatever Firefox does for &lt;code&gt;https&lt;/code&gt;. However, with the latest changes in Firefox, this patch became a bit gnarly to apply correctly so we took a step back and thought to ourselves, why are we even conditionally hiding this from the user?&lt;/p&gt;
    &lt;p&gt;So for now, we have decided not to hide the protocol from the user on Desktop platforms using a supported Firefox pref. We continue to follow upstream and always hide the protocol in the URL bar on Android (where horizontal space is at a premium). Users of Tor Browser Android can simply click the icon in the URL bar to get all the info about a websites HTTPS usage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Send us your feedback&lt;/head&gt;
    &lt;p&gt;If you find a bug or have a suggestion for how we could improve this release, please let us know.&lt;/p&gt;
    &lt;p&gt;â ï¸ Reminder: Tor Browser Alpha release channel is for testing only. If you are at risk or need strong anonymity, stick with the stable release channel.&lt;/p&gt;
    &lt;head rend="h2"&gt;Full changelog&lt;/head&gt;
    &lt;p&gt;The full changelog since Tor Browser 15.0a3 is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All Platforms&lt;list rend="ul"&gt;&lt;item&gt;Updated NoScript to 13.2.1&lt;/item&gt;&lt;item&gt;Updated OpenSSL to 3.5.4&lt;/item&gt;&lt;item&gt;Bug tor-browser#19741: Opensearch (contextual search) does not obey FPI&lt;/item&gt;&lt;item&gt;Bug tor-browser#43850: Modify the Contrast Control settings for RFP&lt;/item&gt;&lt;item&gt;Bug tor-browser#43869: Hide pens with RFP&lt;/item&gt;&lt;item&gt;Bug tor-browser#44068: Handle migration from meek-azure to meek built-in bridge type&lt;/item&gt;&lt;item&gt;Bug tor-browser#44234: No images in PDF&lt;/item&gt;&lt;item&gt;Bug tor-browser#44240: Typo on dom.security.https_first_add_exception_on_failure&lt;/item&gt;&lt;item&gt;Bug tor-browser#44242: Hand over Security Level's WebAssembly controls to NoScript&lt;/item&gt;&lt;item&gt;Bug tor-browser#44250: Rebase Tor Browser Alpha onto 140.4esr&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41574: Update Snowflake builtin bridge lines&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Windows + macOS + Linux&lt;list rend="ul"&gt;&lt;item&gt;Updated Firefox to 140.4.0esr&lt;/item&gt;&lt;item&gt;Bug tor-browser#43900: Open newtab rather than firefoxview when unloading the last tab&lt;/item&gt;&lt;item&gt;Bug tor-browser#44101: Toolbar connection status is not visible when using vertical tabs&lt;/item&gt;&lt;item&gt;Bug tor-browser#44107: Switch tab search action is missing an icon&lt;/item&gt;&lt;item&gt;Bug tor-browser#44108: Fix the new history sidebar&lt;/item&gt;&lt;item&gt;Bug tor-browser#44123: Do not trim protocol off of URLs ever&lt;/item&gt;&lt;item&gt;Bug tor-browser#44153: Test search engines&lt;/item&gt;&lt;item&gt;Bug tor-browser#44159: Change or hide the sidebar settings description&lt;/item&gt;&lt;item&gt;Bug tor-browser#44177: Remove more urlbar actions&lt;/item&gt;&lt;item&gt;Bug tor-browser#44178: Search preservation does not work with duckduckgo in safest security level&lt;/item&gt;&lt;item&gt;Bug tor-browser#44184: Duckduckgo Onion Lite search does not work properly in safest when added as a search engine&lt;/item&gt;&lt;item&gt;Bug tor-browser#44187: TLS session tickets leak Private Browsing mode&lt;/item&gt;&lt;item&gt;Bug tor-browser#44192: Hovering unloaded tab causes console error&lt;/item&gt;&lt;item&gt;Bug tor-browser#44213: Reduce linkability concerns of the "Search with" contextual search action&lt;/item&gt;&lt;item&gt;Bug tor-browser#44214: Update letterboxing to reflect changes in ESR 140&lt;/item&gt;&lt;item&gt;Bug tor-browser#44215: Hide Firefox home settings in about:preferences&lt;/item&gt;&lt;item&gt;Bug tor-browser#44221: Backport MozBug 1984333 Bump Spoofed Processor Count&lt;/item&gt;&lt;item&gt;Bug tor-browser#44239: DDG HTML page and search results displayed incorrectly with Safest security setting&lt;/item&gt;&lt;item&gt;Bug tor-browser#44262: Disable adding search engines from HTML forms&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Linux&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser#44227: Some CJK characters cannot be rendered by Tor which uses the Noto font family&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41586: Replace Noto CJK with Jigmo on Linux&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Android&lt;list rend="ul"&gt;&lt;item&gt;Updated GeckoView to 140.4.0esr&lt;/item&gt;&lt;item&gt;Bug tor-browser#43401: Replace the constructor of Locale with a builder&lt;/item&gt;&lt;item&gt;Bug tor-browser#43643: Clean out unused tor connect strings&lt;/item&gt;&lt;item&gt;Bug tor-browser#43650: Survey banner behaves like a dialog on Android, rather than a card&lt;/item&gt;&lt;item&gt;Bug tor-browser#43676: Preemptively disable unified trust panel by default so we are tracking for next ESR&lt;/item&gt;&lt;item&gt;Bug tor-browser#44031: Implement YEC 2025 Takeover for Android Stable&lt;/item&gt;&lt;item&gt;Bug tor-browser#44218: Tor Browser Alpha for Android (15.0a2) doesn't work on Huawei devices P20 and P30&lt;/item&gt;&lt;item&gt;Bug tor-browser#44237: Revoke access to all advertising ids available in Android&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Build System&lt;list rend="ul"&gt;&lt;item&gt;All Platforms&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser-build#41568: Update instructions for manually building 7zip&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41576: Build expert bundles outside containers&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41579: Add zip to the list of Tor Browser Build dependencies&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41588: Restore legacy channel support in projects/release/update_responses_config.yml&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41589: Backport tor-browser-build-browser#41270: Add updater rewriterules to make 13.5.7 a watershed&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Windows + macOS + Linux&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser-build#41373: Remove &lt;code&gt;_ALL&lt;/code&gt;from mar filenames&lt;/item&gt;&lt;item&gt;Bug tor-browser#44131: Generate torrc-defaults and put it in objdir post-build&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41373: Remove &lt;/item&gt;&lt;item&gt;Windows + Linux + Android&lt;list rend="ul"&gt;&lt;item&gt;Updated Go to 1.24.9&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Windows&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser#44167: Move the nsis-uninstall.patch to tor-browser repository&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;macOS&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser-build#41571: Work-around to prevent older 7z versions to break rcodesign.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Linux&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser-build#41558: Share descriptions between Linux packages and archives&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41569: Use var/display_name in .desktop files&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Android&lt;list rend="ul"&gt;&lt;item&gt;Bug tor-browser#44220: Disable the JS minifier as it produces invalid JS&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41577: Minify JS with UglifyJS on Android x86&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41582: Drop --pack-dyn-relocs=relr&lt;/item&gt;&lt;item&gt;Bug tor-browser-build#41583: Align tor and PTs to 16kB on Android&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;All Platforms&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45605842</guid><pubDate>Thu, 16 Oct 2025 14:33:10 +0000</pubDate></item><item><title>How America got hooked on ultraprocessed foods</title><link>https://www.nytimes.com/interactive/2025/10/16/well/eat/ultraprocessed-food-junk-history.html</link><description>&lt;doc fingerprint="61f46029c2c06879"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Wartime Innovation and Cultural Change&lt;/head&gt;
    &lt;p&gt;1940s-1960s&lt;/p&gt;
    &lt;head rend="h2"&gt;An Ultraprocessed Food Explosion&lt;/head&gt;
    &lt;p&gt;1970s-1990s&lt;/p&gt;
    &lt;head rend="h2"&gt;A Health Crisis, and a Turning Point&lt;/head&gt;
    &lt;p&gt;2000s-Present&lt;/p&gt;
    &lt;head rend="h2"&gt;Sources&lt;/head&gt;
    &lt;p&gt;We consulted Kelly Brownell, a professor emeritus at the Sanford School of Public Policy at Duke University; Michael F. Jacobson, co-founder of the Center for Science in the Public Interest and the founder of the National Food Museum; Stuart Gillespie, a senior fellow at the International Food Policy Research Institute; Tera Fazzino, an associate professor of psychology at the University of Kansas; Dr. David A. Kessler, a former commissioner of the Food and Drug Administration; Carlos Monteiro, a nutritional epidemiologist and emeritus professor at the University of São Paulo in Brazil; Michael Moss, an investigative journalist in New York City who has written about the food industry; Marion Nestle, an emeritus professor of nutrition, food studies and public health at New York University; Barry Popkin, a professor of nutrition at the University of North Carolina’s Gillings School of Global Public Health; John Ruff, a food scientist and former executive at Kraft and General Foods; Laura Schmidt, a professor of health policy at the University of California, San Francisco; Laura Shapiro, a culinary historian in New York City; Dr. Chris van Tulleken, a professor of infection and global health at University College London; Helen Zoe Veit, an associate professor of history at Michigan State University.&lt;/p&gt;
    &lt;head rend="h2"&gt;Credits&lt;/head&gt;
    &lt;p&gt;Getty Images (33); Alamy (6); Jessica Attie for The New York Times; AP Images&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45605921</guid><pubDate>Thu, 16 Oct 2025 14:39:17 +0000</pubDate></item><item><title>Improving the Trustworthiness of JavaScript on the Web</title><link>https://blog.cloudflare.com/improving-the-trustworthiness-of-javascript-on-the-web/</link><description>&lt;doc fingerprint="9e39b013996d6ecb"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The web is the most powerful application platform in existence. As long as you have the right API, you can safely run anything you want in a browser.&lt;/p&gt;
      &lt;p&gt;Wellâ¦ anything but cryptography.&lt;/p&gt;
      &lt;p&gt;It is as true today as it was in 2011 that Javascript cryptography is Considered Harmful. The main problem is code distribution. Consider an end-to-end-encrypted messaging web application. The application generates cryptographic keys in the clientâs browser that lets users view and send end-to-end encrypted messages to each other. If the application is compromised, what would stop the malicious actor from simply modifying their Javascript to exfiltrate messages?&lt;/p&gt;
      &lt;p&gt;It is interesting to note that smartphone apps donât have this issue. This is because app stores do a lot of heavy lifting to provide security for the app ecosystem. Specifically, they provide integrity, ensuring that apps being delivered are not tampered with, consistency, ensuring all users get the same app, and transparency, ensuring that the record of versions of an app is truthful and publicly visible.&lt;/p&gt;
      &lt;p&gt;It would be nice if we could get these properties for our end-to-end encrypted web application, and the web as a whole, without requiring a single central authority like an app store. Further, such a system would benefit all in-browser uses of cryptography, not just end-to-end-encrypted apps. For example, many web-based confidential LLMs, cryptocurrency wallets, and voting systems use in-browser Javascript cryptography for the last step of their verification chains.&lt;/p&gt;
      &lt;p&gt;In this post, we will provide an early look at such a system, called Web Application Integrity, Consistency, and Transparency (WAICT) that we have helped author. WAICT is a W3C-backed effort among browser vendors, cloud providers, and encrypted communication developers to bring stronger security guarantees to the entire web. We will discuss the problem we need to solve, and build up to a solution resembling the current transparency specification draft. We hope to build even wider consensus on the solution design in the near future.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Defining the Web Application&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;In order to talk about security guarantees of a web application, it is first necessary to define precisely what the application is. A smartphone application is essentially just a zip file. But a website is made up of interlinked assets, including HTML, Javascript, WASM, and CSS, that can each be locally or externally hosted. Further, if any asset changes, it could drastically change the functioning of the application. A coherent definition of an application thus requires the application to commit to precisely the assets it loads. This is done using integrity features, which we describe now.&lt;/p&gt;
      &lt;p&gt;An important building block for defining a single coherent application is subresource integrity (SRI). SRI is a feature built into most browsers that permits a website to specify the cryptographic hash of external resources, e.g.,&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;&amp;lt;script src="https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.13.7/underscore-min.js" integrity="sha512-dvWGkLATSdw5qWb2qozZBRKJ80Omy2YN/aF3wTUVC5+D1eqbA+TjWpPpoj8vorK5xGLMa2ZqIeWCpDZP/+pQGQ=="&amp;gt;&amp;lt;/script&amp;gt;&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This causes the browser to fetch &lt;code&gt;underscore.js&lt;/code&gt; from &lt;code&gt;cdnjs.cloudflare.com&lt;/code&gt; and verify that its SHA-512 hash matches the given hash in the tag. If they match, the script is loaded. If not, an error is thrown and nothing is executed.&lt;/p&gt;
      &lt;p&gt;If every external script, stylesheet, etc. on a page comes with an SRI integrity attribute, then the whole page is defined by just its HTML. This is close to what we want, but a web application can consist of many pages, and there is no way for a page to enforce the hash of the pages it links to.&lt;/p&gt;
      &lt;p&gt;We would like to have a way of enforcing integrity on an entire site, i.e., every asset under a domain. For this, WAICT defines an integrity manifest, a configuration file that websites can provide to clients. One important item in the manifest is the asset hashes dictionary, mapping a hash belonging to an asset that the browser might load from that domain, to the path of that asset. Assets that may occur at any path, e.g., an error page, map to the empty string:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;"hashes": {
"81db308d0df59b74d4a9bd25c546f25ec0fdb15a8d6d530c07a89344ae8eeb02": "/assets/js/main.js",
"fbd1d07879e672fd4557a2fa1bb2e435d88eac072f8903020a18672d5eddfb7c": "/index.html",
"5e737a67c38189a01f73040b06b4a0393b7ea71c86cf73744914bbb0cf0062eb": "/vendored/main.css",
"684ad58287ff2d085927cb1544c7d685ace897b6b25d33e46d2ec46a355b1f0e": "",
"f802517f1b2406e308599ca6f4c02d2ae28bb53ff2a5dbcddb538391cb6ad56a": ""
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;The other main component of the manifest is the integrity policy, which tells the browser which data types are being enforced and how strictly. For example, the policy in the manifest below will:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Reject any script before running it, if itâs missing an SRI tag and doesnât appear in the hashes&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Reject any WASM possibly after running it, if itâs missing an SRI tag and doesnât appear in hashes&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;quote&gt;
        &lt;code&gt;"integrity-policy": "blocked-destinations=(script), checked-destinations=(wasm)"&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Put together, these make up the integrity manifest:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;"manifest": {
  "version": 1,
  "integrity-policy": ...,
  "hashes": ...,
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Thus, when both SRI and integrity manifests are used, the entire site and its interpretation by the browser is uniquely determined by the hash of the integrity manifest. This is exactly what we wanted. We have distilled the problem of endowing authenticity, consistent distribution, etc. to a web application to one of endowing the same properties to a single hash.&lt;/p&gt;
      &lt;p&gt;Recall, a transparent web application is one whose code is stored in a publicly accessible, append-only log. This is helpful in two ways: 1) if a user is served malicious code and they learn about it, there is a public record of the code they ran, and so they can prove it to external parties, and 2) if a user is served malicious code and they donât learn about it, there is still a chance that an external auditor may comb through the historical web application code and find the malicious code anyway. Of course, transparency does not help detect malicious code or even prevent its distribution, but it at least makes it publicly auditable.&lt;/p&gt;
      &lt;p&gt;Now that we have a single hash that commits to an entire websiteâs contents, we can talk about ensuring that that hash ends up in a public log. We have several important requirements here:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Do not break existing sites. This one is a given. Whatever system gets deployed, it should not interfere with the correct functioning of existing websites. Participation in transparency should be strictly opt-in.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;No added round trips. Transparency should not cause extra network round trips between the client and the server. Otherwise there will be a network latency penalty for users who want transparency.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;User privacy. A user should not have to identify themselves to any party more than they already do. That means no connections to new third parties, and no sending identifying information to the website.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;User statelessness. A user should not have to store site-specific data. We do not want solutions that rely on storing or gossipping per-site cryptographic information.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Non-centralization. There should not be a single point of failure in the systemâif any single party experiences downtime, the system should still be able to make progress. Similarly, there should be no single point of trustâif a user distrusts any single party, the user should still receive all the security benefits of the system.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Ease of opt-in. The barrier of entry for transparency should be as low as possible. A site operator should be able to start logging their site cheaply and without being an expert.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Ease of opt-out. It should be easy for a website to stop participating in transparency. Further, to avoid accidental lock-in like the defunct HPKP spec, it should be possible for this to happen even if all cryptographic material is lost, e.g., in the seizure or selling of a domain.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Opt-out is transparent. As described before, because transparency is optional, it is possible for an attacker to disable the siteâs transparency, serve malicious content, then enable transparency again. We must make sure this kind of attack is detectable, i.e., the act of disabling transparency must itself be logged somewhere.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Monitorability. A website operator should be able to efficiently monitor the transparency information being published about their website. In particular, they should not have to run a high-network-load, always-on program just to notify them if their site has been hijacked.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;With these requirements in place, we can move on to construction. We introduce a data structure that will be essential to the design.&lt;/p&gt;
      &lt;p&gt;Almost everything in transparency is an append-only log, i.e., a data structure that acts like a list and has the ability to produce an inclusion proof, i.e., a proof that an element occurs at a particular index in the list; and a consistency proof, i.e., a proof that a list is an extension of a previous version of the list. A consistency proof between two lists demonstrates that no elements were modified or deleted, only added.&lt;/p&gt;
      &lt;p&gt;The simplest possible append-only log is a hash chain, a list-like data structure wherein each subsequent element is hashed into the running chain hash. The final chain hash is a succinct representation of the entire list.&lt;/p&gt;
      &lt;p&gt;A hash chain. The green nodes represent the chain hash, i.e., the hash of the element below it, concatenated with the previous chain hash. &lt;/p&gt;
      &lt;p&gt;The proof structures are quite simple. To prove inclusion of the element at index i, the prover provides the chain hash before &lt;code&gt;i&lt;/code&gt;, and all the elements after &lt;code&gt;i&lt;/code&gt;:&lt;/p&gt;
      &lt;p&gt;Proof of inclusion for the second element in the hash chain. The verifier knows only the final chain hash. It checks equality of the final computed chain hash with the known final chain hash. The light green nodes represent hashes that the verifier computes. &lt;/p&gt;
      &lt;p&gt;Similarly, to prove consistency between the chains of size i and j, the prover provides the elements between i and j:&lt;/p&gt;
      &lt;p&gt;Proof of consistency of the chain of size one and chain of size three. The verifier has the chain hashes from the starting and ending chains. It checks equality of the final computed chain hash with the known ending chain hash. The light green nodes represent hashes that the verifier computes. &lt;/p&gt;
      &lt;p&gt;We can use hash chains to build a transparency scheme for websites.&lt;/p&gt;
      &lt;p&gt;As a first step, letâs give every site its own log, instantiated as a hash chain (we will discuss how these all come together into one big log later). The items of the log are just the manifest of the site at a particular point in time:&lt;/p&gt;
      &lt;p&gt;A siteâs hash chain-based log, containing three historical manifests. &lt;/p&gt;
      &lt;p&gt;In reality, the log does not store the manifest itself, but the manifest hash. Sites designate an asset host that knows how to map hashes to the data they reference. This is a content-addressable storage backend, and can be implemented using strongly cached static hosting solutions.&lt;/p&gt;
      &lt;p&gt;A log on its own is not very trustworthy. Whoever runs the log can add and remove elements at will and then recompute the hash chain. To maintain the append-only-ness of the chain, we designate a trusted third party, called a witness. Given a hash chain consistency proof and a new chain hash, a witness:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Verifies the consistency proof with respect to its old stored chain hash, and the new provided chain hash.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;If successful, signs the new chain hash along with a signature timestamp.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Now, when a user navigates to a website with transparency enabled, the sequence of events is:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;The site serves its manifest, an inclusion proof showing that the manifest appears in the log, and all the signatures from all the witnesses who have validated the log chain hash.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The browser verifies the signatures from whichever witnesses it trusts.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The browser verifies the inclusion proof. The manifest must be the newest entry in the chain (we discuss how to serve old manifests later).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The browser proceeds with the usual manifest and SRI integrity checks.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;At this point, the user knows that the given manifest has been recorded in a log whose chain hash has been saved by a trustworthy witness, so they can be reasonably sure that the manifest wonât be removed from history. Further, assuming the asset host functions correctly, the user knows that a copy of all the received code is readily available.&lt;/p&gt;
      &lt;p&gt;The need to signal transparency. The above algorithm works, but we have a problem: if an attacker takes control of a site, they can simply stop serving transparency information and thus implicitly disable transparency without detection. So we need an explicit mechanism that keeps track of every website that has enrolled into transparency.&lt;/p&gt;
      &lt;p&gt;To store all the sites enrolled into transparency, we want a global data structure that maps a site domain to the site logâs chain hash. One efficient way of representing this is a prefix tree (a.k.a., a trie). Every leaf in the tree corresponds to a siteâs domain, and its value is the chain hash of that siteâs log, the current log size, and the siteâs asset host URL. For a site to prove validity of its transparency data, it will have to present an inclusion proof for its leaf. Fortunately, these proofs are efficient for prefix trees.&lt;/p&gt;
      &lt;p&gt;A prefix tree with four elements. Each leafâs path corresponds to a domain. Each leafâs value is the chain hash of its siteâs log. &lt;/p&gt;
      &lt;p&gt;To add itself to the tree, a site proves possession of its domain to the transparency service, i.e., the party that operates the prefix tree, and provides an asset host URL. To update the entry, the site sends the new entry to the transparency service, which will compute the new chain hash. And to unenroll from transparency, the site just requests to have its entry removed from the tree (an adversary can do this too; we discuss how to detect this below).&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h4"&gt;Proving to Witnesses and Browsers&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Now witnesses only need to look at the prefix tree instead of individual site logs, and thus they must verify whole-tree updates. The most important thing to ensure is that every siteâs log is append-only. So whenever the tree is updated, it must produce a âproofâ containing every new/deleted/modified entry, as well as a consistency proof for each entry showing that the site log corresponding to that entry has been properly appended to. Once the witness has verified this prefix tree update proof, it signs the root.&lt;/p&gt;
      &lt;p&gt;The sequence of updating a siteâs assets and serving the site with transparency enabled.&lt;/p&gt;
      &lt;p&gt;The client-side verification procedure is as in the previous section, with two modifications:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;The client now verifies two inclusion proofs: one for the integrity policyâs membership in the site log, and one for the site logâs membership in a prefix tree.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The client verifies the signature over the prefix tree root, since the witness no longer signs individual chain hashes. As before, the acceptable public keys are whichever witnesses the client trusts.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Signaling transparency. Now that there is a single source of truth, namely the prefix tree, a client can know a site is enrolled in transparency by simply fetching the siteâs entry in the tree. This alone would work, but it violates our requirement of âno added round trips,â so we instead require that client browsers will ship with the list of sites included in the prefix tree. We call this the transparency preload list.Â &lt;/p&gt;
      &lt;p&gt;If a site appears in the preload list, the browser will expect it to provide an inclusion proof in the prefix tree, or else a proof of non-inclusion in a newer version of the prefix tree, thereby showing theyâve unenrolled. The site must provide one of these proofs until the last preload list it appears in has expired. Finally, even though the preload list is derived from the prefix tree, there is nothing enforcing this relationship. Thus, the preload list should also be published transparently.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h4"&gt;Filling in Missing Properties&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Remember we still have the requirements of monitorability, opt-out being transparent, and no single point of failure/trust. We fill in those details now.&lt;/p&gt;
      &lt;p&gt;Adding monitorability. So far, in order for a site operator to ensure their site was not hijacked, they would have to constantly query every transparency service for its domain and verify that it hasnât been tampered with. This is certainly better than the 500k events per hour that CT monitors have to ingest, but it still requires the monitor to be constantly polling the prefix tree, and it imposes a constant load for the transparency service.&lt;/p&gt;
      &lt;p&gt;We add a field to the prefix tree leaf structure: the leaf now stores a âcreatedâ timestamp, containing the time the leaf was created. Witnesses ensure that the âcreatedâ field remains the same over all leaf updates (and it is deleted when the leaf is deleted). To monitor, a site operator need only keep the last observed âcreatedâ and âlog sizeâ fields of its leaf. If it fetches the latest leaf and sees both unchanged, it knows that no changes occurred since the last check.&lt;/p&gt;
      &lt;p&gt;Adding transparency of opt-out. We must also do the same thing as above for leaf deletions. When a leaf is deleted, a monitor should be able to learn when the deletion occurred within some reasonable time frame. Thus, rather than outright removing a leaf, the transparency service responds to unenrollment requests by replacing the leaf with a tombstone value, containing just a âcreatedâ timestamp. As before, witnesses ensure that this field remains unchanged until the leaf is permanently deleted (after some visibility period) or re-enrolled.&lt;/p&gt;
      &lt;p&gt;Permitting multiple transparency services. Since we require that there be no single point of failure or trust, we imagine an ecosystem where there are a handful of non-colluding, reasonably trustworthy transparency service providers, each with their own prefix tree. Like Certificate Transparency (CT), this set should not be too large. It must be small enough that reasonable levels of trust can be established, and so that independent auditors can reasonably handle the load of verifying all of them.&lt;/p&gt;
      &lt;p&gt;Ok thatâs the end of the most technical part of this post. Weâre now going to talk about how to tweak this system to provide all kinds of additional nice properties.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;(Not) Achieving Consistency&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Transparency would be useless if, every time a site updates, it serves 100,000 new versions of itself. Any auditor would have to go through every single version of the code in order to ensure no user was targeted with malware. This is bad even if the velocity of versions is lower. If a site publishes just one new version per week, but every version from the past ten years is still servable, then users can still be served extremely old, potentially vulnerable versions of the site, without anyone knowing. Thus, in order to make transparency valuable, we need consistency, the property that every browser sees the same version of the site at a given time.&lt;/p&gt;
      &lt;p&gt;We will not achieve the strongest version of consistency, but it turns out that weaker notions are sufficient for us. If, unlike the above scenario, a site had 8 valid versions of itself at a given time, then that would be pretty manageable for an auditor. So even though itâs true that users donât all see the same version of the site, they will all still benefit from transparency, as desired.&lt;/p&gt;
      &lt;p&gt;We describe two types of inconsistency and how we mitigate them.&lt;/p&gt;
      &lt;p&gt;Tree inconsistency occurs when transparency servicesâ prefix trees disagree on the chain hash of a site, thus disagreeing on the history of the site. One way to fully eliminate this is to establish a consensus mechanism for prefix trees. A simple one is majority voting: if there are five transparency services, a site must present three tree inclusion proofs to a user, showing the chain hash is present in three trees. This, of course, triples the tree inclusion proof size, and lowers the fault tolerance of the entire system (if three log operators go down, then no transparent site can publish any updates).&lt;/p&gt;
      &lt;p&gt;Instead of consensus, we opt to simply limit the amount of inconsistency by limiting the number of transparency services. In 2025, Chrome trusts eight Certificate Transparency logs. A similar number of transparency services would be fine for our system. Plus, it is still possible to detect and prove the existence of inconsistencies between trees, since roots are signed by witnesses. So if it becomes the norm to use the same version on all trees, then social pressure can be applied when sites violate this.&lt;/p&gt;
      &lt;p&gt;Temporal inconsistency occurs when a user gets a newer or older version of the site (both still unexpired), depending on some external factors such as geographic location or cookie values. In the extreme, as stated above, if a signed prefix root is valid for ten years, then a site can serve a user any version of the site from the last ten years.&lt;/p&gt;
      &lt;p&gt;As with tree inconsistency, this can be resolved using consensus mechanisms. If, for example, the latest manifest were published on a blockchain, then a user could fetch the latest blockchain head and ensure they got the latest version of the site. However, this incurs an extra network round trip for the client, and requires sites to wait for their hash to get published on-chain before they can update. More importantly, building this kind of consensus mechanism into our specification would drastically increase its complexity. Weâre aiming for v1.0 here.&lt;/p&gt;
      &lt;p&gt;We mitigate temporal inconsistency by requiring reasonably short validity periods for witness signatures. Making prefix root signatures valid for, e.g., one week would drastically limit the number of simultaneously servable versions. The cost is that site operators must now query the transparency service at least once a week for the new signed root and inclusion proof, even if nothing in the site changed. The sites cannot skip this, and the transparency service must be able to handle this load. This parameter must be tuned carefully.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Beyond Integrity, Consistency, and Transparency&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Providing integrity, consistency, and transparency is already a huge endeavor, but there are some additional app store-like security features that can be integrated into this system without too much work.&lt;/p&gt;
      &lt;p&gt;One problem that WAICT doesnât solve is that of provenance: where did the code the user is running come from, precisely? In settings where audits of code happen frequently, this is not so important, because some third party will be reading the code regardless. But for smaller self-hosted deployments of open-source software, this may not be viable. For example, if Alice hosts her own version of Cryptpad for her friend Bob, how can Bob be sure the code matches the real code in Cryptpadâs Github repo?&lt;/p&gt;
      &lt;p&gt;WEBCAT. The folks at the Freedom of Press Foundation (FPF) have built a solution to this, called WEBCAT. This protocol allows site owners to announce the identities of the developers that have signed the siteâs integrity manifest, i.e., have signed all the code and other assets that the site is serving to the user. Users with the WEBCAT plugin can then see the developerâs Sigstore signatures, and trust the code based on that.&lt;/p&gt;
      &lt;p&gt;Weâve made WAICT extensible enough to fit WEBCAT inside and benefit from the transparency components. Concretely, we permit manifests to hold additional metadata, which we call extensions. In this case, the extension holds a list of developersâ Sigstore identities. To be useful, browsers must expose an API for browser plugins to access these extension values. With this API, independent parties can build plugins for whatever feature they wish to layer on top of WAICT.&lt;/p&gt;
      &lt;p&gt;So far we have not built anything that can prevent attacks in the moment. An attacker who breaks into a website can still delete any code-signing extensions, or just unenroll the site from transparency entirely, and continue with their attack as normal. The unenrollment will be logged, but the malicious code will not be, and by the time anyone sees the unenrollment, it may be too late.&lt;/p&gt;
      &lt;p&gt;To prevent spontaneous unenrollment, we can enforce unenrollment cooldown client-side. Suppose the cooldown period is 24 hours. Then the rule is: if a site appears on the preload list, then the client will require that either 1) the site have transparency enabled, or 2) the site have a tombstone entry that is at least 24 hours old. Thus, an attacker will be forced to either serve a transparency-enabled version of the site, or serve a broken site for 24 hours.&lt;/p&gt;
      &lt;p&gt;Similarly, to prevent spontaneous extension modifications, we can enforce extension cooldown on the client. We will take code signing as an example, saying that any change in developer identities requires a 24 hour waiting period to be accepted. First, we require that extension &lt;code&gt;dev-ids&lt;/code&gt; has a preload list of its own, letting the client know which sites have opted into code signing (if a preload list doesnât exist then any site can delete the extension at any time). The client rule is as follows: if the site appears in the preload list, then both 1) &lt;code&gt;dev-ids&lt;/code&gt; must exist as an extension in the manifest, and 2) &lt;code&gt;dev-ids-inclusion&lt;/code&gt; must contain an inclusion proof showing that the current value of dev-ids was in a prefix tree that is at least 24 hours old. With this rule, a client will reject values of &lt;code&gt;dev-ids&lt;/code&gt; that are newer than a day. If a site wants to delete &lt;code&gt;dev-ids&lt;/code&gt;, they must 1) request that it be removed from the preload list, and 2) in the meantime, replace the dev-ids value with the empty string and update &lt;code&gt;dev-ids-inclusion&lt;/code&gt; to reflect the new value.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Deployment Considerations&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;There are a lot of distinct roles in this ecosystem. Letâs sketch out the trust and resource requirements for each role.&lt;/p&gt;
      &lt;p&gt;Transparency service. These parties store metadata for every transparency-enabled site on the web. If there are 100 million domains, and each entry is 256B each (a few hashes, plus a URL), this comes out to 26GB for a single tree, not including the intermediate hashes. To prevent size blowup, there would probably have to be a pruning rule that unenrolls sites after a long inactivity period. Transparency services should have largely uncorrelated downtime, since, if all services go down, no transparency-enabled site can make any updates. Thus, transparency services must have a moderate amount of storage, be relatively highly available, and have downtime periods uncorrelated with each other.&lt;/p&gt;
      &lt;p&gt;Transparency services require some trust, but their behavior is narrowly constrained by witnesses. Theoretically, a service can replace any leafâs chain hash with its own, and the witness will validate it (as long as the consistency proof is valid). But such changes are detectable by anyone that monitors that leaf.&lt;/p&gt;
      &lt;p&gt;Witness. These parties verify prefix tree updates and sign the resulting roots. Their storage costs are similar to that of a transparency service, since they must keep a full copy of a prefix tree for every transparency service they witness. Also like the transparency services, they must have high uptime. Witnesses must also be trusted to keep their signing key secret for a long period of time, at least long enough to permit browser trust stores to be updated when a new key is created.&lt;/p&gt;
      &lt;p&gt;Asset host. These parties carry little trust. They cannot serve bad data, since any query response is hashed and compared to a known hash. The only malicious behavior an asset host can do is refuse to respond to queries. Asset hosts can also do this by accident due to downtime.&lt;/p&gt;
      &lt;p&gt;Client. This is the most trust-sensitive part. The client is the software that performs all the transparency and integrity checks. This is, of course, the web browser itself. We must trust this.&lt;/p&gt;
      &lt;p&gt;We at Cloudflare would like to contribute what we can to this ecosystem. It should be possible to run both a transparency service and a witness. Of course, our witness should not monitor our own transparency service. Rather, we can witness other organizationsâ transparency services, and our transparency service can be witnessed by other organizations.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Supporting Alternate Ecosystems&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;WAICT should be compatible with non-standard ecosystems, ones where the large players do not really exist, or at least not in the way they usually do. We are working with the FPF on defining transparency for alternate ecosystems with different network and trust environments. The primary example we have is that of the Tor ecosystem.&lt;/p&gt;
      &lt;p&gt;A paranoid Tor user may not trust existing transparency services or witnesses, and there might not be any other trusted party with the resources to self-host these functionalities. For this use case, it may be reasonable to put the prefix tree on a blockchain somewhere. This makes the usual domain validation impossible (thereâs no validator server to speak of), but this is fine for onion services. Since an onion address is just a public key, a signature is sufficient to prove ownership of the domain.&lt;/p&gt;
      &lt;p&gt;One consequence of a consensus-backed prefix tree is that witnesses are now unnecessary, and there is only need for the single, canonical, transparency service. This mostly solves the problems of tree inconsistency at the expense of latency of updates.&lt;/p&gt;
      &lt;p&gt;We are still very early in the standardization process. One of the more immediate next steps is to get subresource integrity working for more data types, particularly WASM and images. After that, we can begin standardizing the integrity manifest format. And then after that we can start standardizing all the other features. We intend to work on this specification hand-in-hand with browsers and the IETF, and we hope to have some exciting betas soon.&lt;/p&gt;
      &lt;p&gt;In the meantime, you can follow along with our transparency specification draft, check out the open problems, and share your ideas. Pull requests and issues are always welcome!&lt;/p&gt;
      &lt;p&gt;Many thanks to Dennis Jackson from Mozilla for the lengthy back-and-forth meetings on design, to Giulio B and Cory Myers from FPF for their immensely helpful influence and feedback, and to Richard Hansen for great feedback.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45606070</guid><pubDate>Thu, 16 Oct 2025 14:50:04 +0000</pubDate></item><item><title>Video game union workers rally against $55B private acquisition of EA</title><link>https://www.eurogamer.net/ea-union-workers-rally-against-55bn-saudi-backed-private-acquisition-with-formal-petition-to-regulators</link><description>&lt;doc fingerprint="af507d6bd06294a4"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Video game union workers rally against $55bn Saudi-backed private acquisition of EA, with formal petition to regulators&lt;/head&gt;
    &lt;p&gt;Job losses "would be a choice, not a necessity, made to pad investors' pockets."&lt;/p&gt;
    &lt;p&gt;EA employees and the Communications Workers of America union have issued a statement against the proposed private acquisition of the company, claiming they were not represented in the negotiations and any jobs lost as a result would "be a choice, not a necessity, made to pad investors' pockets".&lt;/p&gt;
    &lt;p&gt;The acquisition of EA by a group of private investors was announced at the end of September. The deal, for $55bn, includes investment from Saudi Arabia's Public Investment Fund and Affinity Partners, the company of President Donald Trump's son-in-law Jared Kushner.&lt;/p&gt;
    &lt;p&gt;Following the announcement, there's been plenty of speculation around the future of EA and its multiple owned studios, split between EA Sports and EA Entertainment. Now, members of the United Videogame Workers union and the CWA have issued a formal response alongside a petition for regulators to scrutinise the deal.&lt;/p&gt;
    &lt;p&gt;"EA is not a struggling company," the statement reads. "With annual revenues reaching $7.5 billion and $1 billion in profit each year, EA is one of the largest video game developers and publishers in the world."&lt;/p&gt;
    &lt;p&gt;This success has been driven by company workers, the union stated. "Yet we, the very people who will be jeopardised as a result of this deal, were not represented at all when this buyout was negotiated or discussed."&lt;/p&gt;
    &lt;p&gt;Citing the number of layoffs across the industry since 2022, workers fear for "the future of our studios that are arbitrarily deemed 'less profitable' but whose contributions to the video game industry define EA's reputation."&lt;/p&gt;
    &lt;p&gt;"If jobs are lost or studios are closed due to this deal, that would be a choice, not a necessity, made to pad investors' pockets - not to strengthen the company," the statement reads.&lt;/p&gt;
    &lt;p&gt;"Every time private equity or billionaire investors take a studio private, workers lose visibility, transparency, and power," it continues. "Decisions that shape our jobs, our art, and our futures are made behind closed doors by executives who have never written a line of code, built worlds, or supported live services. We are calling on regulators and elected officials to scrutinise this deal and ensure that any path forward protects jobs, preserves creative freedom, and keeps decision-making accountable to the workers who make EA successful."&lt;/p&gt;
    &lt;p&gt;As such, workers have launched a petition in a "fight to make video games better for workers and players - not billionaires".&lt;/p&gt;
    &lt;p&gt;The statement concludes: "The value of video games is in their workers. As a unified voice, we, the members of the industry-wide video game workers' union UVW-CWA, are standing together and refusing to let corporate greed decide the future of our industry."&lt;/p&gt;
    &lt;p&gt;Eurogamer contacted the Federal Trade Commission following news of the proposed acquisition of EA, but it refused to comment "on pending mergers or acquisitions".&lt;/p&gt;
    &lt;p&gt;A report from The Financial Times suggested the deal won't face much opposition. As one source said: "What regulator is going to say no to the president's son-in-law?"&lt;/p&gt;
    &lt;p&gt;Eurogamer also spoke with the Human Rights Watch on the controversial acquisition. "We have found that the public investment fund has contributed to, and is responsible for, human rights abuses," said researcher Joey Shea on the involvement of Saudi Arabia's government. "This is a trillion dollars in Saudi state wealth that should be invested to realise the economic and social rights of Saudi citizens. We've found it's been invested in vanity mega projects inside and outside of the country.&lt;/p&gt;
    &lt;p&gt;"We see this as a deliberate attempt to distract from the country's human rights abuses [...] MBS himself wields enormous power over what is effectively public funds, and he wields this power in a highly arbitrary and personalised manner, rather than the benefit of the Saudi people more broadly. Effectively, Saudi Arabia's vast fossil fuel-derived state wealth is controlled by one person, which isn't good for human rights, or business either."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45606394</guid><pubDate>Thu, 16 Oct 2025 15:12:19 +0000</pubDate></item><item><title>Why more SaaS companies are hiring chief trust officers</title><link>https://www.itbrew.com/stories/2025/10/14/why-more-saas-companies-are-hiring-chief-trust-officers</link><description>&lt;doc fingerprint="bed092891606b564"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Why more SaaS companies are hiring chief trust officers&lt;/head&gt;
    &lt;head rend="h6"&gt;Software companies are hiring chief trust officers to build trust and transparency with consumers.&lt;/head&gt;
    &lt;p&gt;• 3 min read&lt;/p&gt;
    &lt;p&gt;Trust is the foundation of all relationships. That’s why several tech companies are expanding their C-suites with an executive whose sole job is ensuring their company maintains high levels of integrity with customers.&lt;/p&gt;
    &lt;p&gt;These chief trust officers, or CTrOs, are C-suite executives who build stakeholder trust in their company, including in how it handles and secures consumer data. According to an August Forrester report, the role has emerged over the past decade amongst tech and B2B software companies in response to various issues, including the potential to use technology and data for privacy abuse, bias, and harassment.&lt;/p&gt;
    &lt;p&gt;“Effectively, what the role does is offer assurance to the customers or potential customers of that organization that their data, their information, their technology, the infrastructure, the platform itself, can be trusted as those customers adopt it,” Forrester VP and Principal Analyst Jeff Pollard told IT Brew. “So, it goes a little bit beyond security.”&lt;/p&gt;
    &lt;p&gt;Trust guardians. In recent years, the CTrO role has found its way in the C-suites of various tech companies, including Salesforce and Autodesk. Pollard said it could be beneficial for a company to hire a CTrO because it escalates the responsibility of trust into an “executive priority.”&lt;/p&gt;
    &lt;p&gt;“You’re making them a member of the C-suite, which, in itself, shows an intentionality that I think is positive because it should influence the rest of what the company does,” Pollard said.&lt;/p&gt;
    &lt;p&gt;He added that designating a leader to be in charge of trust allows “more focus for the roles that roll up to the CTrO as opposed to everyone trying to do everything.”&lt;/p&gt;
    &lt;p&gt;However, adding a CTrO to your C-suite can come with baggage if not done properly, according to Pollard, who said the reporting structures associated with the role may unintentionally separate professionals from the technical side of their business.&lt;/p&gt;
    &lt;head rend="h5"&gt;Top insights for IT pros&lt;/head&gt;
    &lt;p&gt;From cybersecurity and big data to cloud computing, IT Brew covers the latest trends shaping business tech in our 4x weekly newsletter, virtual events with industry experts, and digital guides.&lt;/p&gt;
    &lt;p&gt;“One of the very realistic concerns here, from a chief trust officer perspective, is when you’re broken out of the technology work and you span beyond that…just by definition, you do lose some insight as to what the tech org is doing,” he said.&lt;/p&gt;
    &lt;p&gt;A peake at the role. Chris Peake became CTrO at revenue AI platform company Gong in August. He told IT Brew his role combines IT and security at the SaaS provider.&lt;/p&gt;
    &lt;p&gt;“Day-to-day operationally, it’s addressing historical things on both the IT side of making sure that we’ve got systems up and running that support the business, [and] controlling endpoints and all those kinds of things,” Peake said, adding he is also responsible for making sure security is built into Gong’s products.&lt;/p&gt;
    &lt;p&gt;Peake, a former CISO, said a lot of the skills from his previous role have translated into his current one. However, he said the CTrO role differs from the CISO role because it operates more on the “business level,” as the work done by a CTrO can directly impact revenue generation, contract negotiation, and onboarding new customers.&lt;/p&gt;
    &lt;p&gt;“I think that’s why we see a lot of the chief trust officers are former CISOs, because there is a natural fit there,” Peake said. “It’s just adding that next layer.”&lt;/p&gt;
    &lt;p&gt;Fad role? Forrester claims the CTrO role is more than just a “passing trend,” perhaps unlike other emerging C-suite positions.&lt;/p&gt;
    &lt;p&gt;“If it does disappear, I don’t think the need for someone to oversee trust disappears,” Pollard said. “Maybe the title does, but if it does then it’s subsumed by the CISO…hopefully they use that as part of their new remit to expand upon what they’ve done in the past and not just be the sort of legacy department of ‘no.’”&lt;/p&gt;
    &lt;head rend="h3"&gt;Top insights for IT pros&lt;/head&gt;
    &lt;p&gt;From cybersecurity and big data to cloud computing, IT Brew covers the latest trends shaping business tech in our 4x weekly newsletter, virtual events with industry experts, and digital guides.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45606602</guid><pubDate>Thu, 16 Oct 2025 15:29:06 +0000</pubDate></item><item><title>Ld_preload, the Invisible Key Theft</title><link>https://bomfather.dev/blog/ld-preload-the-invisible-key-theft/</link><description>&lt;doc fingerprint="2e53cfb8e8ad5eea"&gt;
  &lt;main&gt;
    &lt;p&gt;Imagine you are running a Solana validator. You have your EDR agent running, and you have everything set up and think you are safe. But you realize your wallet is drained and you don’t know why. You start to investigate and see that the validator only accessed your private keys and nothing else. You check the directory’s permissions, logs from EDR, and everything seems to be in order.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Threat&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;LD_PRELOAD&lt;/code&gt; is an environment variable that allows you to load a shared library before the program starts. This is a powerful feature that can be used to hook system calls and intercept file operations. There are other similar variables like &lt;code&gt;LD_LIBRARY_PATH&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is not ENV variables alone. There are things like &lt;code&gt;/etc/ld.so.preload&lt;/code&gt; that can be used to load a shared library before the program starts.&lt;/p&gt;
    &lt;p&gt;Linux built this feature to allow developers to load shared libraries before the program starts so that they can debug and test their code.&lt;/p&gt;
    &lt;p&gt;Our implementation of this exploitation code is in our Github Repo.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Solana?&lt;/head&gt;
    &lt;p&gt;This is not just a Solana problem. This is a problem for any application that loads credentials from a file. The insider threat is real and a problem for any organization. This is another way to steal private keys. There is too much at stake not to be careful.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Vulnerability&lt;/head&gt;
    &lt;p&gt;Now, most of us write userspace applications, and use libraries like &lt;code&gt;glibc&lt;/code&gt; to handle the file operations, so we never really think about this issue. &lt;code&gt;glibc&lt;/code&gt; does the heavy lifting for us, but what if we hook into the file operations and intercept them before &lt;code&gt;glibc&lt;/code&gt; does its thing? This is what &lt;code&gt;LD_PRELOAD&lt;/code&gt; allows us to do. It is a way to hook into a library call.&lt;/p&gt;
    &lt;p&gt;In our case, we did this by hooking into the file operations and intercepting them before &lt;code&gt;glibc&lt;/code&gt; did its thing. When the validator read its keypair files from disk, we intercepted the &lt;code&gt;close()&lt;/code&gt; call and made a copy before the file descriptor closed.&lt;/p&gt;
    &lt;p&gt;We wrote a malicious shared library that hooks into the file operations. We also call the real &lt;code&gt;close()&lt;/code&gt; function, so the validator continues normally, the user has no idea that this is happening.&lt;/p&gt;
    &lt;head rend="h2"&gt;How the Attack Works&lt;/head&gt;
    &lt;p&gt;Our malicious library does something clever, it hooks the &lt;code&gt;close()&lt;/code&gt; system call.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;close()&lt;/code&gt; and not &lt;code&gt;open()&lt;/code&gt; or &lt;code&gt;read()&lt;/code&gt;? Because when a file is closed, we know the application is done with it. We can check what file was just accessed by looking at &lt;code&gt;/proc/self/fd/{fd}&lt;/code&gt; before the file descriptor is closed.&lt;/p&gt;
    &lt;p&gt;The hook is surprisingly simple - about 30 lines of C. Here’s the core concept:&lt;/p&gt;
    &lt;code&gt;int close(int fd) {
    // 1. Get pointer to real close() using dlsym(RTLD_NEXT, "close")
    // 2. Read /proc/self/fd/{fd} to see what file this is
    // 3. If path contains "solana-run", copy the file to /tmp/stolen-validator-data
    // 4. Call real close() so the validator continues normally
}

&lt;/code&gt;
    &lt;p&gt;That’s it. No privilege escalation, kernel exploits, or complex techniques. Just intercepting a standard library call that every program uses.&lt;/p&gt;
    &lt;p&gt;The key trick is using &lt;code&gt;/proc/self/fd/{fd}&lt;/code&gt;, a Linux feature that lets you see what file a file descriptor points to. Before the file descriptor closes, we check if it’s one of the Solana keypair files. If it is, we make a copy.&lt;/p&gt;
    &lt;head rend="h2"&gt;A More In-Depth View&lt;/head&gt;
    &lt;p&gt;Both of the following are based on our attack code https://github.com/bomfather/tools/tree/main/ld-preload.&lt;/p&gt;
    &lt;head rend="h3"&gt;Method 1: Environment Variable (LD_PRELOAD)&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compile &lt;code&gt;malicious.so&lt;/code&gt;and inject it into the validator container&lt;/item&gt;
      &lt;item&gt;Set &lt;code&gt;LD_PRELOAD=/tmp/malicious.so&lt;/code&gt;when starting the validator&lt;/item&gt;
      &lt;item&gt;When the validator opens keypair files, our library is already loaded&lt;/item&gt;
      &lt;item&gt;Every time &lt;code&gt;close()&lt;/code&gt;is called, we intercept it&lt;/item&gt;
      &lt;item&gt;Check if the file path contains “solana-run” (the ledger directory)&lt;/item&gt;
      &lt;item&gt;If yes, copy the file to our exfiltration directory&lt;/item&gt;
      &lt;item&gt;Call the real &lt;code&gt;close()&lt;/code&gt;so the validator continues normally&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Method 2: Persistent (/etc/ld.so.preload)&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compile &lt;code&gt;malicious.so&lt;/code&gt;inside the container&lt;/item&gt;
      &lt;item&gt;Write &lt;code&gt;/tmp/malicious.so&lt;/code&gt;to&lt;code&gt;/etc/ld.so.preload&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Start the validator (library loads automatically)&lt;/item&gt;
      &lt;item&gt;Same interception and exfiltration process&lt;/item&gt;
      &lt;item&gt;Affects ALL processes in the container, not just the validator&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Deploying the Attack&lt;/head&gt;
    &lt;p&gt;Environment variable method:&lt;/p&gt;
    &lt;code&gt;LD_PRELOAD=/path/to/malicious.so ./program
&lt;/code&gt;
    &lt;p&gt;Persistent method:&lt;/p&gt;
    &lt;code&gt;echo "/path/to/malicious.so" &amp;gt; /etc/ld.so.preload
./program  # Library loads automatically
&lt;/code&gt;
    &lt;p&gt;It is as simple as that.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why This Matters: Scope and Implications&lt;/head&gt;
    &lt;head rend="h3"&gt;Can containers be exploited?&lt;/head&gt;
    &lt;p&gt;Yes. Containers don’t protect against &lt;code&gt;LD_PRELOAD&lt;/code&gt; attacks because:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The environment variable is set within the container’s namespace&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;/etc/ld.so.preload&lt;/code&gt;is a file inside the container&lt;/item&gt;
      &lt;item&gt;The process inside the container runs the library inside the container&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Container isolation doesn’t help when the attack comes from inside the container.&lt;/p&gt;
    &lt;head rend="h3"&gt;Do I need to be root?&lt;/head&gt;
    &lt;p&gt;It depends on the method:&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;LD_PRELOAD&lt;/code&gt; environment variable:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;No root needed for your own processes&lt;/item&gt;
      &lt;item&gt;Can be set by any user for processes they start&lt;/item&gt;
      &lt;item&gt;That is the scary part&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;/etc/ld.so.preload&lt;/code&gt; file:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires root/privileged access to modify the file&lt;/item&gt;
      &lt;item&gt;But once set, it affects ALL processes system-wide&lt;/item&gt;
      &lt;item&gt;More dangerous, but requires privilege escalation first&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Remember the scenario at the beginning? The drained validator with everything “in order”? This is how it happens, with a single environment variable, no root access needed, and no alerts triggered. Just silent exfiltration of private keys while the validator runs normally.&lt;/p&gt;
    &lt;p&gt;The scary part isn’t the complexity of the attack but the simplicity. &lt;code&gt;LD_PRELOAD&lt;/code&gt; is a legitimate debugging feature. File access by the validator process is expected behavior.&lt;/p&gt;
    &lt;p&gt;Check if your EDR agent is handling things like this.&lt;/p&gt;
    &lt;p&gt;Complete source code and demo available at https://github.com/bomfather/tools/tree/main/ld-preload. The steps to run it are in the &lt;code&gt;README.md&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Disclaimer: This tool is for educational and authorized security testing only. Unauthorized access to computer systems is illegal. See LICENSE for complete terms.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45606611</guid><pubDate>Thu, 16 Oct 2025 15:29:45 +0000</pubDate></item><item><title>Codex Is Live in Zed</title><link>https://zed.dev/blog/codex-is-live-in-zed</link><description>&lt;doc fingerprint="b44ddc5f080cb81d"&gt;
  &lt;main&gt;
    &lt;p&gt;When we introduced the Agent Client Protocol (ACP) in collaboration with Google's Gemini CLI team, we did not anticipate how much pent-up demand there was for a protocol like this!&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First we saw a wave of requests for Anthropic's Claude Code&lt;/item&gt;
      &lt;item&gt;Then we saw a bunch of other clients and agents adopting ACP, most recently JetBrains&lt;/item&gt;
      &lt;item&gt;Now we've seen a fresh wave of requests for OpenAI's Codex:&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As of today, Zed supports Codex out-the-box via ACP. You can select it from the New Thread menu, just like Claude Code or Gemini CLI:&lt;/p&gt;
    &lt;p&gt;Like our other ACP integrations, Codex via ACP is strictly about improving UI and keeping you in flow in your IDE of choice; the billing and legal/terms arrangement is directly between you and OpenAI. Zed does not charge for use of external agents like Codex, nor do prompts and code sent via Codex-ACP to OpenAI touch Zed's servers! We've also separately open-sourced the codex-acp adapter so you can use it outside of Zed as well.&lt;/p&gt;
    &lt;head rend="h2"&gt;Learning from Different Agents&lt;/head&gt;
    &lt;p&gt;Every model behaves a bit differently than the others, and the same is true of agents. For example, some agents support switching models in the middle of a conversation, whereas others require sticking with the same model throughout. Some support viewing and resuming past conversations, whereas others have no concept of conversation persistence. ACP is designed to be flexible enough to work with a variety of agent capabilities, but the experience of using them still varies based on the agent's implementation details.&lt;/p&gt;
    &lt;p&gt;A detail that came up when we were building the Codex ACP adapter was that the Codex agent runs terminal commands in its own process, and then streams output bytes from that terminal process to the client. In the past, we've had this reversed: the agent would send the client a request to run a terminal command (e.g. &lt;code&gt;mkdir examples&lt;/code&gt;) and then the client would manage
the actual running of that command.&lt;/p&gt;
    &lt;p&gt;Naturally, we want to keep the look and feel consistent no matter which agent you're using, but this design difference between Codex and other agents makes certain details unavoidably different. For example, for other agents we can spawn terminals in pseudoterminal (PTY) mode. This means you can actually interact with the terminal right in the Agent Panel if the agent launches an interactive process, and it also means things like colorful terminal output tend to be enabled by default.&lt;/p&gt;
    &lt;p&gt;On the other hand, being in PTY mode means that an agent can get stuck. A classic example of this is when an agent tries to run &lt;code&gt;git rebase --continue&lt;/code&gt; and the terminal pops up the configured editor, then waits for the programmer to make any
edits (if desired) to the commit message. This can be nice for a human, but for an agent that's waiting for the command to
finish, it creates a deadlock. The process won't proceed without interaction, and the agent won't interact until the process
completes! Having terminals work in non-PTY mode might result in fewer colors and less interactivity, but it also results in
fewer cases of agents getting stuck.&lt;/p&gt;
    &lt;p&gt;Now that we've integrated agents that both enable and disable PTY mode, we can compare how the experience feels in both cases, and use that to inform our recommendations for future ACP integrations.&lt;/p&gt;
    &lt;head rend="h2"&gt;Which ACP Integration is Next?&lt;/head&gt;
    &lt;p&gt;In addition to Codex, we have already added support to Zed for Claude Code, Gemini CLI, and other agents, all via ACP. Since ACP is not specific to Zed, we've also seen it be adopted by editors like Neovim, Emacs, and now the JetBrains family of IDEs.&lt;/p&gt;
    &lt;p&gt;Now that the protocol has picked up enough adoption on its own, we're happy to shift our focus to working with the community on the future of the protocol—as opposed to building ACP adapters ourselves like we did with Codex and Claude Code. We're excited to see what amazing ACP integrations the community cooks up!&lt;/p&gt;
    &lt;head rend="h3"&gt;Looking for a better editor?&lt;/head&gt;
    &lt;p&gt;You can try Zed today on macOS, Windows, or Linux. Download now!&lt;/p&gt;
    &lt;head rend="h3"&gt;We are hiring!&lt;/head&gt;
    &lt;p&gt;If you're passionate about the topics we cover on our blog, please consider joining our team to help us ship the future of software development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45606698</guid><pubDate>Thu, 16 Oct 2025 15:36:39 +0000</pubDate></item><item><title>Claude Skills</title><link>https://www.anthropic.com/news/skills</link><description>&lt;doc fingerprint="9e4c2e7f2f779869"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Introducing Claude Skills&lt;/head&gt;
    &lt;p&gt;Claude can now use Skills to improve how it performs specific tasks. Skills are folders that include instructions, scripts, and resources that Claude can load when needed.&lt;/p&gt;
    &lt;p&gt;Claude will only access a skill when it's relevant to the task at hand. When used, skills make Claude better at specialized tasks like working with Excel or following your organization's brand guidelines.&lt;/p&gt;
    &lt;p&gt;You've already seen Skills at work in Claude apps, where Claude uses them to create files like spreadsheets and presentations. Now, you can build your own skills and use them across Claude apps, Claude Code, and our API.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Skills work&lt;/head&gt;
    &lt;p&gt;While working on tasks, Claude scans available skills to find relevant matches. When one matches, it loads only the minimal information and files needed—keeping Claude fast while accessing specialized expertise.&lt;/p&gt;
    &lt;p&gt;Skills are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Composable: Skills stack together. Claude automatically identifies which skills are needed and coordinates their use.&lt;/item&gt;
      &lt;item&gt;Portable: Skills use the same format everywhere. Build once, use across Claude apps, Claude Code, and API.&lt;/item&gt;
      &lt;item&gt;Efficient: Only loads what's needed, when it's needed.&lt;/item&gt;
      &lt;item&gt;Powerful: Skills can include executable code for tasks where traditional programming is more reliable than token generation.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Think of Skills as custom onboarding materials that let you package expertise, making Claude a specialist on what matters most to you. For a technical deep-dive on the Agent Skills design pattern, architecture, and development best practices, read our engineering blog.&lt;/p&gt;
    &lt;head rend="h2"&gt;Skills work with every Claude product&lt;/head&gt;
    &lt;head rend="h3"&gt;Claude apps&lt;/head&gt;
    &lt;p&gt;Skills are available to Pro, Max, Team and Enterprise users. We provide skills for common tasks like document creation, examples you can customize, and the ability to create your own custom skills.&lt;/p&gt;
    &lt;p&gt;Claude automatically invokes relevant skills based on your task—no manual selection needed. You'll even see skills in Claude's chain of thought as it works.&lt;lb/&gt;Creating skills is simple. The "skill-creator" skill provides interactive guidance: Claude asks about your workflow, generates the folder structure, formats the SKILL.md file, and bundles the resources you need. No manual file editing required. &lt;/p&gt;
    &lt;p&gt;Enable Skills in Settings. For Team and Enterprise users, admins must first enable Skills organization-wide.&lt;/p&gt;
    &lt;head rend="h3"&gt;Claude Developer Platform (API)&lt;/head&gt;
    &lt;p&gt;Agent Skills, which we often refer to simply as Skills, can now be added to Messages API requests and the new &lt;code&gt;/v1/skills&lt;/code&gt; endpoint gives developers programmatic control over custom skill versioning and management. Skills require the Code Execution Tool beta, which provides the secure environment they need to run.&lt;/p&gt;
    &lt;p&gt;Use Anthropic-created skills to have Claude read and generate professional Excel spreadsheets with formulas, PowerPoint presentations, Word documents, and fillable PDFs. Developers can create custom Skills to extend Claude's capabilities for their specific use cases.&lt;/p&gt;
    &lt;p&gt;Developers can also easily create, view, and upgrade skill versions through the Claude Console.&lt;/p&gt;
    &lt;p&gt;Explore the documentation or Anthropic Academy to learn more.&lt;/p&gt;
    &lt;quote&gt;Skills teaches Claude how to work with Box content. Users can transform stored files into PowerPoint presentations, Excel spreadsheets, and Word documents that follow their organization's standards—saving hours of effort.&lt;/quote&gt;
    &lt;quote&gt;With Skills, Claude works seamlessly with Notion - taking users from questions to action faster. Less prompt wrangling on complex tasks, more predictable results.&lt;/quote&gt;
    &lt;quote&gt;Canva plans to leverage Skills to customize agents and expand what they can do. This unlocks new ways to bring Canva deeper into agentic workflows—helping teams capture their unique context and create stunning, high-quality designs effortlessly.&lt;/quote&gt;
    &lt;quote&gt;Skills streamline our management accounting and finance workflows. Claude processes multiple spreadsheets, catches critical anomalies, and generates reports using our procedures. What once took a day, we can now accomplish in an hour.&lt;/quote&gt;
    &lt;head rend="h3"&gt;Claude Code&lt;/head&gt;
    &lt;p&gt;Skills extend Claude Code with your team's expertise and workflows. Install skills via plugins from the anthropics/skills marketplace. Claude loads them automatically when relevant. Share skills through version control with your team. You can also manually install skills by adding them to &lt;code&gt;~/.claude/skills&lt;/code&gt;. The Claude Agent SDK provides the same Agent Skills support for building custom agents. &lt;/p&gt;
    &lt;head rend="h2"&gt;Getting started&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Claude apps: User Guide &amp;amp; Help Center&lt;/item&gt;
      &lt;item&gt;API developers: Documentation&lt;/item&gt;
      &lt;item&gt;Claude Code: Documentation&lt;/item&gt;
      &lt;item&gt;Example Skills to customize: GitHub repository&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;What's next&lt;/head&gt;
    &lt;p&gt;We're working toward simplified skill creation workflows and enterprise-wide deployment capabilities, making it easier for organizations to distribute skills across teams.&lt;/p&gt;
    &lt;p&gt;Keep in mind, this feature gives Claude access to execute code. While powerful, it means being mindful about which skills you use—stick to trusted sources to keep your data safe. Learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45607117</guid><pubDate>Thu, 16 Oct 2025 16:05:47 +0000</pubDate></item><item><title>Gemini 3.0 spotted in the wild through A/B testing</title><link>https://ricklamers.io/posts/gemini-3-spotted-in-the-wild/</link><description>&lt;doc fingerprint="ef53a5961b3a9e93"&gt;
  &lt;main&gt;
    &lt;p&gt;So I kept reading rumors that Gemini 3.0 is accessible through Google AI Studio through A/B testing and the SVGs folks were posting (of Xbox controllers in particular) made me think that they might be right.&lt;/p&gt;
    &lt;p&gt;Gemini 3.0 is one of the most anticipated releases in AI at the moment because of the expected advances in coding performance.&lt;/p&gt;
    &lt;p&gt;Evaluating models is a difficult task, but surprisingly the SVG generation task seems to be a very efficient proxy for gauging model quality as @simonw has shown us using his “pelican riding a bicycle” test.&lt;/p&gt;
    &lt;p&gt;Lo and behold, after trying a couple of times I got the A/B screen and got an SVG image of an Xbox 360 controller that looked VERY impressive compared to the rest of the frontier.&lt;/p&gt;
    &lt;p&gt;The exact prompt I used:&lt;/p&gt;
    &lt;code&gt;Create an SVG image of an Xbox 360 controller. Output it in a Markdown multi-line code block.
Like this:
```svg
...
```
&lt;/code&gt;
    &lt;p&gt;For what it’s worth the model ID for “Gemini 3.0” was &lt;code&gt;ecpt50a2y6mpgkcn&lt;/code&gt; which doesn’t really help understand which version of the model it is. Perhaps since I user selected Gemini 2.5 Pro it is actually Gemini 3.0 Pro that it is pitted against, as comparing Gemini 3.0 Flash to Gemini 2.5 Pro in an A/B test makes less sense to me. Also, it had about 24s higher TTFT and output length was about 40% longer (this includes reasoning tokens AFAICT), but that doesn’t say much other than it’s likely not a “GPT-5 Pro” type answer that uses significant test time compute.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;“Gemini 3.0” A/B result versus the Gemini 2.5 Pro model:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45607758</guid><pubDate>Thu, 16 Oct 2025 16:54:26 +0000</pubDate></item><item><title>SWE-Grep and SWE-Grep-Mini: RL for Fast Multi-Turn Context Retrieval</title><link>https://cognition.ai/blog/swe-grep</link><description>&lt;doc fingerprint="655e27db2dfa194a"&gt;
  &lt;main&gt;&lt;p&gt;TL;DR: We trained SWE-grep and SWE-grep-mini, fast agentic models specialized in highly parallel context retrieval. They match the retrieval capabilities of frontier coding models, while taking an order of magnitude less time. Available now in Windsurf’s new Fast Context subagent, and our new SWE-grep demo playground!&lt;/p&gt;&lt;p&gt;Modern coding agents face a fundamental tradeoff between speed and intelligence. Frontier models can solve complex tasks, but it can take minutes of searching before they edit a single file, breaking your flow. In Windsurf and Devin, we observed that our agent trajectories were often spending &amp;gt;60% of their first turn just retrieving context.&lt;/p&gt;&lt;p&gt;Context retrieval has been historically done in 2 ways:&lt;/p&gt;&lt;p&gt;This speed-intelligence tradeoff seemed inescapable until: We trained SWE-grep and SWE-grep-mini: models which match the retrieval capabilities of frontier coding models, while taking an order of magnitude less time. These models now power Fast Context, a subagent that helps you stay in flow.&lt;/p&gt;&lt;p&gt;We will be rolling Fast Context out progressively to Windsurf users, starting from the latest release. There is no required UI or command to try it - just use Windsurf Cascade as per normal. When you make a query that requires code search, Fast Context will trigger (you can also force it to trigger by submitting the query with &lt;code&gt;Cmd+Enter&lt;/code&gt;).&lt;/p&gt;&lt;p&gt;Check out how Fast Context reduces the time it takes to understand large codebases:&lt;/p&gt;&lt;p&gt;You can try Fast Context yourself in our new playground: https://playground.cognition.ai/.&lt;/p&gt;&lt;p&gt;Since we are offering direct comparisons between our agent and alternative agents, we should mention what is going on here and our attempts to make it a fair comparison. We host all 3 agents - a Fast Context Agent stripped out of Windsurf, stock Claude Code, and stock Cursor CLI - in their own Modal containers and pipe the inputs/outputs through stdin/stdout. This is meant to reflect the experience of using each agent locally. This is not meant to be an extremely rigorous benchmark, just a demo experience we cooked up on the side to answer the obvious question of “how does Fast Context compare to what I’m used to outside of Windsurf?” You should run these tests in your own environment (with Fast Context in Windsurf) for best fidelity to your actual experience.&lt;/p&gt;&lt;p&gt;There are a few reasons why we think that context retrieval is a uniquely suited task for a custom subagent:&lt;/p&gt;&lt;code&gt;SWE-grep&lt;/code&gt; fast?&lt;p&gt;A few things were key to unlocking this level of intelligence at blazing-fast speeds:&lt;/p&gt;&lt;p&gt;Most coding agents take so long to fetch context because they only issue one (or a few) tool calls at a time. Each turn of tool calls incurs an additional prefill, an extra network roundtrip, and decoding overhead: to explore codebases as efficiently as possible, search subagents should be doing many tool calls in parallel.&lt;/p&gt;&lt;p&gt;While many models technically support parallel tool calls, it’s difficult to get them to use them effectively. Models are getting better than this—Sonnet in particular has improved greatly from 3.6 to 4.5—but we felt that models didn’t exploit them optimally. Here is a rough sketch of the model design space that we targeted for &lt;code&gt;SWE-grep&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;Increasing parallelism also lets us use fewer tool calls. Across our ablations, we discovered that, by increasing the amount of parallelism from 4 to 8 searches per turn, we could reduce the number of turns spent searching from 6 to 4 while retaining the same performance.&lt;/p&gt;&lt;p&gt;We thus trained the &lt;code&gt;SWE-grep&lt;/code&gt; models to natively issue up to 8 parallel tool calls per turn in a maximum of 4 turns (3 turns of exploration and 1 turn for the answer). The SWE-grep models are given a restricted set of tool calls (&lt;code&gt;grep&lt;/code&gt;, &lt;code&gt;read&lt;/code&gt;, &lt;code&gt;glob&lt;/code&gt;, ...) to ensure cross-platform compatibility (we have loads of Windows users!) and guarantee safety.&lt;/p&gt;&lt;p&gt;We train &lt;code&gt;SWE-grep&lt;/code&gt; directly with multi-turn reinforcement learning. Then we distill &lt;code&gt;SWE-grep&lt;/code&gt; into &lt;code&gt;SWE-grep-mini&lt;/code&gt; and perform additional reinforcement learning to boost the model’s performance on the task. Our reward function is an average of weighted F1 scores over file retrieval and line retrieval tasks, with respect to our ground truth dataset. This objective was sufficient for &lt;code&gt;SWE-grep&lt;/code&gt; to naturally learn to make more tool calls to its advantage over the course of training, without us explicitly incentivizing this behavior:&lt;/p&gt;&lt;p&gt;We’ll explain some details about our training algorithm, a modified version of the policy gradient, and some tweaks that helped us keep training stable.&lt;/p&gt;&lt;p&gt;Given an LLM policy and outcome reward R, the policy gradient is given by&lt;/p&gt;&lt;p&gt;where the sum is over the tokens in a single trajectory. If we are able to sample from the training policy, we can use a simple Monte Carlo estimate for the gradient, which is unbiased when the data is on-policy. However, standard training and inference libraries have different numerics, which effectively turns the sampled data into off-policy data. This is amplified when using low-precision rollouts, a common optimization in RL frameworks. The solution is to apply importance sampling. Recent works have proposed using per-token importance sampling ratios. Per-token ratios, though, do not fully remove the bias. Indeed, at step t we have an action-choice mismatch, a state-distribution mismatch, and a reward-signal mismatch. A per-token ratio corrects only the action-choice mismatch. To derive an unbiased estimate, we apply per-sequence importance sampling&lt;/p&gt;&lt;p&gt;We expand at the token-level, subtract a leave-one-out baseline to reduce the variance, and rescale by a constant factor (absorbed in the learning rate), obtaining a surrogate loss (for a given prompt) that gives the correct gradient estimation (the notation []_∇ denotes a stop gradient):&lt;/p&gt;&lt;p&gt;where we sample g completions from the same prompt for the Monte Carlo estimate, Aⱼ = Rⱼ - mean(R₁, ... ,R₉) and T_max is the maximum number of sampled tokens allowed during training (like Dr. GRPO).&lt;/p&gt;&lt;p&gt;A large number of parallel tool calls over multiple turns introduces a lot of tokens from the environment in the trajectories. These tokens are not generated by the model, leading to instabilities, especially when training small models. We found the following techniques to be helpful in stabilizing these runs:&lt;/p&gt;&lt;p&gt;To train SWE-grep and evaluate models on the context retrieval task, we used an internal dataset consisting of real-world repositories, user queries, and a labeled ground truth of relevant files and line ranges, drawn from our hardest bug reports and internal tests. We call this the Cognition CodeSearch Eval.&lt;/p&gt;&lt;p&gt;When evaluating models for context retrieval, we care about two metrics:&lt;/p&gt;&lt;p&gt;We use a weighted F1 score, where precision is prioritized over recall, precisely because we found that context pollution matters. We found that polluting the context of the main agent was more detrimental than leaving some context out, as the agent is typically only a few searches away to recover any remaining context. To evaluate models, we allow each model 4 turns of up to 8 parallel tool calls (searches, reads, etc.), and benchmark them on the above metrics.&lt;/p&gt;&lt;p&gt;Our results on our evaluation set demonstrate that SWE-grep and SWE-grep-mini are an order of magnitude faster than frontier models, while matching or outperforming them at context retrieval.&lt;/p&gt;&lt;p&gt;We also evaluated how well the SWE-grep models work when used as a subagent in larger agent pipelines.&lt;/p&gt;&lt;p&gt;Coding tasks. To evaluate how well it works in Windsurf’s Cascade agent, we use an randomly selected subset of difficult SWE-Bench Verified tasks. When using the Fast Context subagent, the agent (using Sonnet 4.5 as the main model) accomplishes the same number of tasks in significantly lower end-to-end time.&lt;/p&gt;&lt;p&gt;*on internal runners&lt;/p&gt;&lt;p&gt;**search file steps include greps, file reads, glob searches, etc.&lt;/p&gt;&lt;p&gt;Codebase Q&amp;amp;A. We show the end-to-end latency on some example queries over open-source repositories. As with our playground setup, we benchmark the Fast Context agent—as it would be used in Windsurf—against Claude Code and Cursor CLI by measuring end-to-end latency.&lt;/p&gt;&lt;p&gt;The Fast Context subagent in Windsurf is our first stepping stone on our roadmap for Fast Agents. The &lt;code&gt;SWE-grep&lt;/code&gt; models will be deployed in DeepWiki, Devin, Windsurf Tab and future products as we validate and tune for those use cases - future directions we want to explore include much more variable turn length, even higher intelligence, and tools speed optimizations.&lt;/p&gt;&lt;p&gt;End-to-end latency is a moderately non-consensus dimension of research for agent labs. In a world where coding agents grab headlines for having 2-30 hours of autonomy, the marketing incentive is to make agents slower, not faster. But we think the pendulum will swing the other way soon — simply because we have the unfair advantage of seeing actual user behavior across sync and async code agents at massive scale.&lt;/p&gt;&lt;p&gt;The goal of Windsurf is to keep you in flow, which Mihaly Csikszentmihalyi defines as “a state of complete absorption in an activity”. Roughly, we estimate that your P(breaking flow) geometrically increases 10% every second that passes while you wait for agent response, with the exact threshold varying based on perceived complexity of the request. The arbitrary “flow window” we hold ourselves to is 5 seconds.&lt;/p&gt;&lt;p&gt;Our ultimate goal at the combined Cognition+Windsurf is to maximize your software engineering productivity, and we are simultaneously researching both the directions of pushing the frontier of coding agent autonomy -AND- making them faster given a “good enough” bar. The best mental model we’ve found is the one we’ve arrived at below - avoid the Semi-Async Valley of Death at all costs!&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45607822</guid><pubDate>Thu, 16 Oct 2025 16:59:30 +0000</pubDate></item><item><title>Mysterious Intrigue Around an x86 "Corporate Entity Other Than Intel/AMD"</title><link>https://www.phoronix.com/news/x86-Opcodes-Not-AMD-Or-Intel</link><description>&lt;doc fingerprint="7a5835c61a25b0f6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mysterious Intrigue Around An x86 "Corporate Entity Other Than Intel/AMD"&lt;/head&gt;
    &lt;p&gt; Posted to the Linux kernel mailing list and GNU Binutils mailing list today is an intriguing message from a longtime x86/x86_64 expert around a "a corporate entity other than Intel/AMD" using some x86 opcodes not used by AMD or Intel processors. &lt;lb/&gt;Longtime x86 expert Christian Ludloff posted a cryptic message to the LKML and Binutils mailing lists. An anonymous Phoronix reader in turn relayed the interesting occurrence to me. Christian Ludloff has worked for Google, AMD, TI, and others over the years as an x86 architecture expert as well as being known for his sandpile.org x86 CPU information site.&lt;lb/&gt;The subject for the mailing list posts by Christian Ludloff was x86 opcode/CPUID/MSR allocations and simply said:&lt;lb/&gt;These x86 opcodes and CPUs and MSR ranges now "in active use by a corporate entity other than Intel/AMD" makes this rather intriguing... And that it's a "corporate" entity rather than noting a research organization, hobbyist project, etc. And how said corporate entity has the legal ability to even work on a new x86-based implementation is interesting. For Christian Ludloff to be involved does give it additional weight.&lt;lb/&gt;As pointed out in the Phoronix Forums, it could be Zhaoxin. But as they have contributed GCC patches, Glibc patches, Linux kernel patches, etc, over the years it's not clear why they would go unnamed or have to relay the message via Ludloff rather than their prior direct mailing list posts.&lt;lb/&gt;For now that's all I know but rather interesting seeing this message from Ludloff today on the public mailing lists.&lt;/p&gt;
    &lt;p&gt;Longtime x86 expert Christian Ludloff posted a cryptic message to the LKML and Binutils mailing lists. An anonymous Phoronix reader in turn relayed the interesting occurrence to me. Christian Ludloff has worked for Google, AMD, TI, and others over the years as an x86 architecture expert as well as being known for his sandpile.org x86 CPU information site.&lt;/p&gt;
    &lt;p&gt;The subject for the mailing list posts by Christian Ludloff was x86 opcode/CPUID/MSR allocations and simply said:&lt;/p&gt;
    &lt;quote&gt;"If x86 opcode/CPUID/MSR allocations are not of your concern, then you can stop reading.&lt;lb/&gt;-------------------- 8&amp;lt; -------------------&lt;lb/&gt;I was asked to relay this to binutils/LKML.&lt;lb/&gt;As of 2025, the following are in active use by a corporate entity other than Intel/AMD.&lt;lb/&gt;Any collisions with them should be avoided.&lt;lb/&gt;- opcode 0Eh in PM64 - x86 PUSH CS that got removed by x86-64 in 2002; not used since&lt;lb/&gt;- opcode 0Fh,36h and opcode 0Fh,3Eh - there is a historic collision with Cyrix RDSHR, but that is not considered to be an issue&lt;lb/&gt;- opcode 0Fh,3Ah,E0h...EFh in classic, VEX, EVEX, Map3, and Map7 encodings, without a prefix, or CS/SS/DS/ES/FS/GS, LOCK, REPE/REPNE, or ASIZE/OSIZE/REX (but not REX2!) prefixes - a historic collision with K10M VCVTFXPNTPD2DQ (at MVEX opcode E6h prefix F2) exists but is not considered an issue&lt;lb/&gt;- opcode 0Fh,1Eh,/0 - a "hinting NOP" group&lt;lb/&gt;- CPUID range E000_xxxxh - unspecified leaf return values at this particular time&lt;lb/&gt;- MSR range E000_xxxxh - unspecified values after RESET - unchanged values after INIT&lt;lb/&gt;I have documented them at www.sandpile.org.&lt;lb/&gt;-------------------- 8&amp;lt; -------------------&lt;lb/&gt;--&lt;lb/&gt;C."&lt;/quote&gt;
    &lt;p&gt;These x86 opcodes and CPUs and MSR ranges now "in active use by a corporate entity other than Intel/AMD" makes this rather intriguing... And that it's a "corporate" entity rather than noting a research organization, hobbyist project, etc. And how said corporate entity has the legal ability to even work on a new x86-based implementation is interesting. For Christian Ludloff to be involved does give it additional weight.&lt;/p&gt;
    &lt;p&gt;As pointed out in the Phoronix Forums, it could be Zhaoxin. But as they have contributed GCC patches, Glibc patches, Linux kernel patches, etc, over the years it's not clear why they would go unnamed or have to relay the message via Ludloff rather than their prior direct mailing list posts.&lt;/p&gt;
    &lt;p&gt;For now that's all I know but rather interesting seeing this message from Ludloff today on the public mailing lists.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45608285</guid><pubDate>Thu, 16 Oct 2025 17:36:18 +0000</pubDate></item></channel></rss>