<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 02 Sep 2025 09:11:36 +0000</lastBuildDate><item><title>Cloudflare Radar: AI Insights</title><link>https://radar.cloudflare.com/ai-insights</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093090</guid></item><item><title>Ask HN: Who is hiring? (September 2025)</title><link>https://news.ycombinator.com/item?id=45093192</link><description>&lt;doc fingerprint="3651195a341ae364"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Please state the location and include REMOTE for remote work, REMOTE (US) or similar if the country is restricted, and ONSITE when remote work is &lt;/p&gt;not&lt;p&gt; an option.&lt;/p&gt;&lt;p&gt;Please only post if you personally are part of the hiring company—no recruiting firms or job boards. One post per company. If it isn't a household name, explain what your company does.&lt;/p&gt;&lt;p&gt;Please only post if you are actively filling a position and are committed to responding to applicants.&lt;/p&gt;&lt;p&gt;Commenters: please don't reply to job posts to complain about something. It's off topic here.&lt;/p&gt;&lt;p&gt;Readers: please only email if you are personally interested in the job.&lt;/p&gt;&lt;p&gt;Searchers: try https://dheerajck.github.io/hnwhoishiring/, https://amber-williams.github.io/hackernews-whos-hiring/, http://nchelluri.github.io/hnjobs/, https://hnresumetojobs.com, https://hnhired.fly.dev, https://kennytilton.github.io/whoishiring/, https://hnjobs.emilburzo.com, or this (unofficial) Chrome extension: https://chromewebstore.google.com/detail/hn-hiring-pro/mpfal....&lt;/p&gt;&lt;p&gt;Don't miss these other fine threads:&lt;/p&gt;&lt;p&gt;Who wants to be hired? https://news.ycombinator.com/item?id=45093190&lt;/p&gt;&lt;p&gt;Freelancer? Seeking freelancer? https://news.ycombinator.com/item?id=45093191&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45093192</guid></item><item><title>Adaptive LLM routing under budget constraints</title><link>https://arxiv.org/abs/2508.21141</link><description>&lt;doc fingerprint="cc18340bf0f78482"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 28 Aug 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Adaptive LLM Routing under Budget Constraints&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Large Language Models (LLMs) have revolutionized natural language processing, but their varying capabilities and costs pose challenges in practical applications. LLM routing addresses this by dynamically selecting the most suitable LLM for each query/task. Previous approaches treat this as a supervised learning problem, assuming complete knowledge of optimal query-LLM pairings. However, real-world scenarios lack such comprehensive mappings and face evolving user queries. We thus propose to study LLM routing as a contextual bandit problem, enabling adaptive decision-making using bandit feedback without requiring exhaustive inference across all LLMs for all queries (in contrast to supervised routing). To address this problem, we develop a shared embedding space for queries and LLMs, where query and LLM embeddings are aligned to reflect their affinity. This space is initially learned from offline human preference data and refined through online bandit feedback. We instantiate this idea through Preference-prior Informed Linucb fOr adaptive rouTing (PILOT), a novel extension of LinUCB. To handle diverse user budgets for model routing, we introduce an online cost policy modeled as a multi-choice knapsack problem, ensuring resource-efficient routing.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45094421</guid></item><item><title>One of Britain's largest stocks of second-hand books ever amassed</title><link>https://www.worldofinteriors.com/story/richard-axe-second-hand-books-yorkshire</link><description>&lt;doc fingerprint="fa91101555a054a1"&gt;
  &lt;main&gt;
    &lt;p&gt;One of Britain’s largest stocks of second-hand books ever amassed can be found in the unlikeliest of locations: a vast former youth hostel in a pretty corner of the Yorkshire Dales. Meticulously sorted into subject areas, from naval history to 19th-century literature, architecture to zoology, over 150,000 volumes fill some 25 high-ceilinged rooms spread over four floors. To withstand the sheer weight of all those hardbacks, the building, which began life as a prep school in c1878, must surely be as strong as a Romanesque church.&lt;/p&gt;
    &lt;p&gt;Certainly the collection has been assembled with an almost religious zeal by sole trader Richard Axe, a spry 70-something who spoke to me from the Philippines, where he lives with his wife roughly half the year. Unlike the more commercially oriented of his peers, he has sold books primarily so that he could acquire more for himself. Of the Harrogate shop he owned prior to his move here he says: ‘Its main purpose was not to sell at all, but rather to buy and increase my buying profile.’ I had fondly imagined that Richard chose Aysgarth in order to lure customers from the nearby falls, a daytrippers’ magnet. But in fact, he says he’s never advertised, nor had more than four visitors per week, and all of them were by appointment. Yet very rarely did a week pass by without him shifting at least £1,000 worth of books.&lt;/p&gt;
    &lt;p&gt;So how did his business, which has been highly successful, operate? Most professionals in the field swim in the waters of either ‘good quality but relatively ordinary second-hand books’ or ‘very specialist, expensive antiquarian books’; Richard has ended up being a big fish in both streams. In his heyday, he would reckon to drive some 25,000 miles a year, attending auctions from Plymouth to Glasgow. There he’d bid for large lots, sometimes whole libraries, subsequently selling a handful of important titles to private customers and international dealers, while creaming off books for his own collection. Hence, in part, the appeal of North Yorkshire. Far from being ‘the back of beyond’, the A1, M1 and East Coast mainline railway offer superb connections. ‘I like the idea of being in the middle of the country.’&lt;/p&gt;
    &lt;p&gt;For the eagle-eyed expert, the condition of a book is critical. Richard gives the example of Dickens – ‘incredibly popular in his own lifetime, and so a first edition in reasonably good nick might fetch £50. But a really fine copy could go for ten times that.’ As is the case with all antiques, the internet has had a polarising effect. In the past, you might have traipsed round bookshops for a lifetime not finding the missing piece in your authorial jigsaw, but often such editions previously thought rare can now be quite easily unearthed via online search engines, and so their value has slumped. ‘But what’s transpired is that things that are genuinely uncommon have shot up.’&lt;/p&gt;
    &lt;p&gt;The classic case is JK Rowling’s Harry Potter and the Philosopher’s Stone. Given that it was the first in the sequence, Bloomsbury, with no inkling of the monster the brand would become, published the smallest viable number of copies – 500 – in its first edition. It is thus truly rare, says Richard, much more so than a Jane Austen equivalent. As a consequence: ‘A fine copy is worth, certainly, £50,000.’ Similarly, photograph albums and manuscripts, by definition one-off items, have seen skyrocketing interest in cyberspace – and among a new, younger breed of collector to boot. Indeed, the most expensive item my interlocutor has ever sold was not a book at all, but a scrapbook owned by Cecil Beaton, which included the photographer’s own drawing of Mick Jagger, a personal friend, and sketches by Jean Cocteau.&lt;/p&gt;
    &lt;p&gt;When Richard moved to his property in 2005, consolidating a warehouse, shop and large house in Harrogate, little structural work was required to the sturdily built edifice apart from new guttering, though he had to strip out rows of urinals and a few municipal-style kitchens. Inevitably, however, two men were employed full time for a year just building wooden shelves, which now stretch to a combined length of over a mile. The original plan was to carve out some domestic space in this behemoth of a building, but Richard states that mixing residence and business had potential VAT implications. And then there was the small issue of his ever-encroaching library… So he moved into the two-bedroom modern cottage previously inhabited by the youth-hostel manager. Presumably you had a no-book policy here, I ask Richard. ‘Well, if I did, it didn’t last,’ he replies ruefully. ‘Let’s just say it was restricted.’&lt;/p&gt;
    &lt;p&gt;Philosophical follies, such as doorknobs surreally attached to tree trunks, dot the unusual sculpture garden that Richard has proudly created out of a capacious paddock that’s part of the estate. In the middle of the pond sits an inaccessible table and chairs overhung with a metal fruit basket you also cannot reach. ‘It’s all about, you know, the unattainable.’ This modern take on the Tantalus myth strikes me as the perfect symbol for the avid collector, feverishly hunting down the final piece of a set, mourning the rarities that slipped through one’s fingers. The psychological make-up of the type emerged early on. Even as a Dulwich College scholarship boy, ‘I collected stamps rather more avidly than most.’ Later, at Bristol University, ‘I bought books initially to read, but the physical possession of having quite a lot, of having a substantial range of bookshelves, became important as a manifestation of knowledge and understanding and culture.’ Even now, he can find himself transfixed when, say, a history of signposts tumbles into his lap.&lt;/p&gt;
    &lt;p&gt;The physical demands of his job, combined with declining health, mean Richard ‘must face the facts of age’. With great reluctance, he is now selling his estate and collection, lock, stock and barrel, for around £1.5 million. His greatest fear is that the land, library and buildings get sold off bit by bit, ‘my life’s work just disappearing’. He is even prepared to play the role of éminence grise to the putative lucky buyer, sharing his contacts and experience.&lt;/p&gt;
    &lt;p&gt;But what about the keepers? Surely, there are a few titles he’d like to hold on to? Well, there’s his extensive collection of antiquarian books on Yorkshire; those on another great love, football (‘though the Philippines is pretty much the only country in the world that doesn’t have an interest’); books on the fluctuatingly fashionable Ruskin, as well as 18th- and 19th-century folding maps that the chin-stroking aesthete might have consulted himself while hiking in the Lakes. Oh, and let’s not forget the collection of 1750s–1870s books in their original drab boards or cloth. ‘Cheaper to buy at the time, because they were unbound, they are now valuable because of their fragility.’ The bookseller’s eyes sparkle.&lt;/p&gt;
    &lt;p&gt;To enquire about purchasing the property plus stock, ring Elaine Williams-Bird on 07798 818651 or email nellybirdpress@gmail.com&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45094692</guid></item><item><title>Implementing a Foil Sticker Effect</title><link>https://www.4rknova.com/blog/2025/08/30/foil-sticker</link><description>&lt;doc fingerprint="7aab8b3d79ef373f"&gt;
  &lt;main&gt;
    &lt;p&gt;In this post, I’ll walk you through how to create a custom shader in Three.js that simulates the look of a foil sticker, complete with angle-dependent iridescence and sparkling metallic flakes. The goal is to capture that premium, holographic effect you see on collectible stickers, trading cards, and high-end packaging, but to render it in real time directly in the browser.&lt;/p&gt;
    &lt;head rend="h1"&gt;Iridescence&lt;/head&gt;
    &lt;p&gt;If you’ve ever tilted a holographic sticker or watched sunlight catch on a soap bubble, you’ve seen iridescence in action. In the real world, this rainbow shimmer comes from thin-film interference. When light waves bounce between layers of a surface, some wavelengths are reinforced while others cancel out, causing colors to shift depending on your viewing angle.&lt;/p&gt;
    &lt;p&gt;In real-time computer graphics, we don’t need to simulate the exact physics. Instead, we can approximate this by mapping view angle to hue, as the surface tilts relative to the camera, its color smoothly shifts through a spectrum. This gives that dynamic, “alive” quality you expect from foil stickers.&lt;/p&gt;
    &lt;head rend="h1"&gt;Foil Flakes&lt;/head&gt;
    &lt;p&gt;Alongside the shifting colors, there’s another key detail: foil flakes. Real metallic foils have tiny reflective particles embedded in them, creating hundreds of bright, sharp highlights that twinkle as you move. These aren’t smooth reflections but randomized sparkles, giving the surface its tactile, premium feel.&lt;/p&gt;
    &lt;p&gt;To replicate this in a shader, we’ll introduce procedural noise to generate small random patches of brightness across the surface. When combined with lighting, they look like metallic specks catching the light. Together, angular hue shifts and flake sparkles create a convincing illusion of printed holographic foil without expensive rendering tricks.&lt;/p&gt;
    &lt;head rend="h1"&gt;Implementation&lt;/head&gt;
    &lt;p&gt;This implementation simulates a peeling, iridescent sticker with foil flakes using Three.js. While I will borrow concepts such as metalness, roughness, and Fresnel from Physically Based Rendering (PBR), this shader is not physically based. The goal is to create a visually plausible, artistic effect.&lt;/p&gt;
    &lt;p&gt;Below is a live demo of the shader, where you can modify its parameters and experiment with different configurations. Use your mouse to rotate the sticker around and see how the material reacts to the lighting.&lt;/p&gt;
    &lt;head rend="h2"&gt;Vertex Shader&lt;/head&gt;
    &lt;p&gt;The vertex shader handles the peel geometry and passes useful information to the fragment shader.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Uniform / Varying&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uPeelAmount&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Overall peel strength (0 = flat, 1 = fully peeled).&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uPeelAngle&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Peel direction in degrees.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;vUv&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;vec2&lt;/cell&gt;
        &lt;cell&gt;UV coordinates for texture mapping.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;vWorldPos&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;vec3&lt;/cell&gt;
        &lt;cell&gt;Vertex position in world space.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;vNormal&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;vec3&lt;/cell&gt;
        &lt;cell&gt;Transformed normal for lighting.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;vAOIntensity&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Distance moved by vertex, used to darken lifted areas.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The shader goes through the following simple steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute vector from hinge to current vertex.&lt;/item&gt;
      &lt;item&gt;Calculate the peel factor and angle.&lt;/item&gt;
      &lt;item&gt;Define the rotation axis and apply Rodrigues’ rotation formula to rotate the vertex around that axis.&lt;/item&gt;
      &lt;item&gt;Apply the same rotation to the normal.&lt;/item&gt;
      &lt;item&gt;Calculate a fake ambient occlusion term.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s the full vertex shader code:&lt;/p&gt;
    &lt;code&gt;uniform float uPeelAmount;  // Strength of peel (0.0 → no peel, 1.0 → full peel)
uniform float uPeelAngle;   // Peel angle in degrees (converted to radians in shader)
varying vec2  vUv;          // UV coordinates
varying vec3  vWorldPos;    // Vertex position in world space
varying vec3  vNormal;      // Transformed vertex normal
varying float vAOIntensity; // Ambient occlusion or peel intensity factor

void main() {
    vUv = vec2(uv.x, 1.0 - uv.y);
    vec3 pos = position;

    // Define hinge point for peel
    vec3 hinge = vec3(0.0, 0.0, 0.0);

    // Vector from hinge to current vertex
    vec3 toVertex = pos - hinge;

    // Peel factor calculation
    // Interpolates peel strength diagonally
    // (bottom-left → top-right)
    float peelFactor = (uv.x + uv.y) * 0.5;

    // Convert peel angle to radians
    // Final angle is scaled by peelAmount
    // and per-vertex peelFactor
    float radAngle = radians(uPeelAngle);
    float angle = radAngle * uPeelAmount * peelFactor;

    // Define rotation axis for peel
    // Diagonal axis pointing from top-left 
    // to bottom-right
    vec3 axis = normalize(vec3(-1.0, 1.0, 0.0));
    float cosA = cos(angle);
    float sinA = sin(angle);

    // Apply Rodrigues' rotation formula
    // Rotates the vertex around the diagonal axis
    vec3 rotated = toVertex * cosA +
                   cross(axis, toVertex) * sinA +
                   axis * dot(axis, toVertex) * (1.0 - cosA);

    // Update vertex position after rotation
    pos = hinge + rotated;

    // Rotate vertex normal the same way to
    // ensure lighting matches the peeled
    // geometry
    vec3 rotatedNormal = normal * cosA +
                         cross(axis, normal) * sinA +
                         axis * dot(axis, normal) * (1.0 - cosA);

    // Transform normal into view space
    vNormal = normalize(normalMatrix * rotatedNormal);

    // Transform vertex to world space
    vec4 worldPos = modelMatrix * vec4(pos, 1.0);
    vWorldPos = worldPos.xyz;

    // Ambient Occlusion term based on distance moved
    // from original vertex position
    vAOIntensity = length(toVertex - rotated);

    // Final projection
    gl_Position = projectionMatrix * viewMatrix * worldPos;
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Fragment Shader&lt;/head&gt;
    &lt;p&gt;The fragment shader handles all lighting, reflections, iridescence, and foil flakes. It layers procedural effects to create a rich, dynamic look.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Uniform&lt;/cell&gt;
        &lt;cell role="head"&gt;Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;map&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;sampler2D&lt;/cell&gt;
        &lt;cell&gt;Sticker albedo + alpha.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;envMap2D&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;sampler2D&lt;/cell&gt;
        &lt;cell&gt;Environment map for reflections.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uCameraPos&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;vec3&lt;/cell&gt;
        &lt;cell&gt;Camera position for view vector.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uAlphaCutoff&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Discard pixels below this alpha.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uFlakesEnabled&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Toggle foil flakes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uFlakeSize&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Size of flakes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uFlakeReduction&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Randomness threshold for flakes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uFlakeThreshold&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Brightness threshold to show flakes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uFlakeBrightness&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Base brightness of flakes.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uMetalness&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;PBR-like metal reflectivity control.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uRoughness&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Controls reflection sharpness.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uEnvIntensity&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Scales environment contribution.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uMetalmask&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Mask controlling metallic regions.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;uIridescence&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Strength of angle-dependent rainbow effect.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;uIriMin&lt;/code&gt;, &lt;code&gt;uIriRange&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Range for simulated film thickness.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;&lt;code&gt;uPeelAmount&lt;/code&gt;, &lt;code&gt;uPeelAngle&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;float&lt;/cell&gt;
        &lt;cell&gt;Peel geometry info for shading.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is how this works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Alpha cutoff to discard transparent pixels early.&lt;/item&gt;
      &lt;item&gt;Back-face shading to render the rear surface as plain white or darkened, depending on peel.&lt;/item&gt;
      &lt;item&gt;Foil flakes are computed using procedural noise. Normals are perturbed slightly to create sparkle variation. The environment map is sampled to get an iridescent tint.&lt;/item&gt;
      &lt;item&gt;Iridescence (thin-film approximation) is calculated using sine-based waves to shift hue by view angle.&lt;/item&gt;
      &lt;item&gt;Environment reflections are modulated by Fresnel.&lt;/item&gt;
      &lt;item&gt;Final shading combines diffuse base, reflections, iridescence, and flakes.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s the full vertex shader code:&lt;/p&gt;
    &lt;code&gt;precision highp float;

#define PI  3.14159265

varying vec2 vUv;
varying vec3 vNormal;
varying vec3 vWorldPos;
varying float vAOIntensity;

uniform sampler2D map;      // sticker albedo + alpha
uniform sampler2D envMap2D; // LDR equirectangular environment

uniform vec3  uCameraPos;
uniform float uAlphaCutoff;
uniform float uMaxMip;
uniform float uFlakesEnabled;
uniform float uFlakeSize;
uniform float uFlakeReduction;
uniform float uFlakeThreshold;
uniform float uFlakeBrightness;
uniform float uPeelAmount;
uniform float uPeelAngle;
uniform float uMetalness;
uniform float uRoughness;
uniform float uEnvIntensity;
uniform float uMetalmask;
uniform float uIridescence;
uniform float uIriMin;
uniform float uIriRange;

float hash(vec2 p) {
    return fract(sin(dot(p, vec2(127.1, 311.7))) * 43758.5453123);
}

// Map 3D dir to 2D equirect UV
vec2 dirToEquirectUv(vec3 dir) {
    dir = normalize(dir);
    float phi = atan(dir.z, dir.x);
    float theta = acos(clamp(dir.y, -1.0, 1.0));
    return vec2((phi + 3.14159265) / (2.0 * 3.14159265), theta / 3.14159265);
}

vec3 sampleEnvRough(vec3 R, float roughness) {
    vec2 uv = dirToEquirectUv(R);

    // Map roughness to LOD level
    float lod = roughness * uMaxMip;
    vec3 color = texture2DLodEXT(envMap2D, uv, lod).rgb;

    return color;
}

// Iridescence / thin-film color
vec3 iridescenceColor(float cosTheta) {
    float thickness = uIriMin + uIriRange * (1.0 - cosTheta);
    float phase = 6.28318 * thickness * 0.01; // scaled for visuals
    vec3 rainbow = 0.5 + 0.5 * vec3(sin(phase), sin(phase + 2.094), sin(phase + 4.188));
    return mix(vec3(1.0), rainbow, uIridescence);
}

// Convert RGB to perceived luminance (Rec.709)
float luminance(vec3 color) {
    return dot(color, vec3(0.2126, 0.7152, 0.0722));
}

void main() {

    vec4 base = texture2D(map, vUv);
    if(base.a &amp;lt; uAlphaCutoff)
        discard;

    if(!gl_FrontFacing) {
        float col = 1.0;
        if(uPeelAngle &amp;gt; 0.0) {
            col = mix(1.0, 0.2, vAOIntensity);
        }
        // Render back side as white
        gl_FragColor = vec4(vec3(col), base.a);
        return;
    }

    vec3 N = normalize(vNormal);
    vec3 V = normalize(uCameraPos - vWorldPos);
    vec3 R = reflect(-V, N);

    // Ambient occlusion / peel shadow
    float peelShadow = 0.0;

    if(uPeelAngle &amp;lt; 0.0) {
        peelShadow = smoothstep(0.0, 0.3, vAOIntensity);
        base.rgb *= mix(1.0, 0.3, peelShadow);
    }

    // Flakes
    float flakeIntensity = 0.0;
    vec3 flakeEnv = vec3(0.0);

    float brightness = luminance(base.rgb);

    if(uFlakesEnabled &amp;gt; 0.5) {
        // Procedural flake mask
        float flake = hash(floor(vUv * uFlakeSize));
        float flakeMask = smoothstep(uFlakeReduction, 1.0, flake);

        // Base brightness influence
        float flakeBoost = smoothstep(uFlakeThreshold, 1.0, brightness);

        // Perturbed flake normal
        float angleOffset = (hash(vec2(flake, flake + 3.0)) - 0.5) * 0.25;
        vec3 perturbedNormal = normalize(N + vec3(angleOffset, 0.0, angleOffset));

        // Reflection for sparkle
        vec3 PR = reflect(-V, perturbedNormal);

        // Dynamic flicker factor (only brightens, never darkens)
        float flakePhase = hash(floor(vUv * uFlakeSize) + floor(PR.xy * 15.0));
        float phaseMod = mix(1.0, 1.8, flakePhase);
        
        // Core sparkle factor (glimmer preserved)
        float flakeSpec = pow(clamp(dot(perturbedNormal, V) * 0.5 + 0.5, 0.0, 1.0), 8.0);
        flakeSpec = max(flakeSpec, 0.15); // always visible

        // Environment tint (never too dark, controlled by uniform)
        float flakeRough = clamp(uRoughness * 0.4, 0.0, 1.0);
        flakeEnv = sampleEnvRough(PR, flakeRough) * mix(0.9, 1.2, brightness);
        flakeEnv = max(flakeEnv, vec3(uFlakeBrightness));

        vec3 flakeIri = iridescenceColor(dot(perturbedNormal, V));
        flakeEnv *= mix(vec3(1.0), flakeIri, 0.9);

        // Final intensity
        flakeIntensity = flakeMask * flakeBoost * flakeSpec * phaseMod * 18.0;
        flakeIntensity = clamp(flakeIntensity, 0.0, 1.0);
    }

    // Final roughness modulation
    float finalRough = clamp(mix(uRoughness, 1.0, flakeIntensity), 0.0, 1.0);

    // Environment reflection
    vec3 env = sampleEnvRough(R, finalRough) * uEnvIntensity;

    // Blend in flake environment contribution
    env = mix(env, flakeEnv, clamp(flakeIntensity, 0.0, 1.0));

    // Fresnel term
    float cosTheta = clamp(dot(N, V), 0.0, 1.0);
    float F0 = mix(0.04, 1.0, uMetalness);
    float fres = F0 + (1.0 - F0) * pow(1.0 - cosTheta, 5.0);

    // Iridescence
    float metalicMask = mix(uMetalmask, 1.0, brightness);
    vec3 iriCol = iridescenceColor(cosTheta) * metalicMask;

    // Final color
    vec3 diffuse = base.rgb * (1.0 - uMetalness);
    vec3 spec = env * fres * iriCol * (1.0 - finalRough * 0.85);
    vec3 color = diffuse + spec;

    gl_FragColor = vec4(color, base.a);
}
&lt;/code&gt;
    &lt;head rend="h1"&gt;Licensing&lt;/head&gt;
    &lt;p&gt;The code in this page is licensed under Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0). Feel free to share and adapt the code for non-commercial purposes with proper attribution. If you wish to use the code commercially, please contact me for a separate license agreement.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45095460</guid></item><item><title>The future of 32-bit support in the kernel</title><link>https://lwn.net/SubscriberLink/1035727/4837b0d3dccf1cbb/</link><description>&lt;doc fingerprint="744b991fd8863dfd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The future of 32-bit support in the kernel&lt;/head&gt;
    &lt;head rend="h2"&gt;[LWN subscriber-only content]&lt;/head&gt;
    &lt;quote&gt;
      &lt;head&gt;Welcome to LWN.net&lt;/head&gt;
      &lt;p&gt;The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider subscribing to LWN. Thank you for visiting LWN.net!&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Arnd Bergmann started his Open Source Summit Europe 2025 talk with a clear statement of position: 32-bit systems are obsolete when it comes to use in any sort of new products. The only reason to work with them at this point is when there is existing hardware and software to support. Since Bergmann is the overall maintainer for architecture support in the kernel, he is frequently asked whether 32-bit support can be removed. So, he concluded, the time has come to talk more about that possibility.&lt;/p&gt;
    &lt;p&gt;People naturally think about desktop machines first, he continued. If you were running Linux in the 1990s, you had a 32-bit, desktop system. Unix systems, though, moved to 64-bit platforms around 30 years ago, and the Linux desktop made that move about 20 years ago. Even phones and related devices have been 64-bit for the last decade. If those systems were all that Linux had to support, 32-bit support would have long since been removed from the kernel. He summarized the situation with this slide, showing how the non-embedded architectures have transitioned to either 64-bit or nonexistence over time:&lt;/p&gt;
    &lt;p&gt;The world is not all desktops — or servers — though; embedded Linux exists as well. About 90% of those systems are running on Arm processors. The kernel has accumulated a lot of devicetree files describing those systems over the years; only in this last year has the number of devicetrees for armv8 (64-bit) systems exceeded the number for armv7 (32-bit) systems.&lt;/p&gt;
    &lt;p&gt;For Arm processors with pre-armv7 architectures, there are only three for which it is still possible to buy hardware, but a number are still supported by the kernel community:&lt;/p&gt;
    &lt;p&gt;Many other pre-armv7 CPUs are out of production, but the kernel still has support for them. Of those, he said, there are about ten that could be removed now. It would be nice to be able to say that support for the others will be removed after a fixed period, ten years perhaps, but hardware support does not work that way. Instead, one has to think in terms of half lives; every so often, it becomes possible to remove support for half of the platforms. It all depends on whether there are users for the processors in question.&lt;/p&gt;
    &lt;p&gt;The kernel is still adding support for some 32-bit boards, he said, but at least ten new 64-bit boards gain support for each 32-bit one.&lt;/p&gt;
    &lt;p&gt;There are a number of non-Arm 32-bit architectures that still have support in the kernel; these include arc, microblaze, nios2, openrisc, rv32, sparc/leon, and xtensa. All of them are being replaced by RISC-V processors in new products. RISC-V is what you use if you don't care about Arm compatibility, he said.&lt;/p&gt;
    &lt;p&gt; Then, there is the dusty corner where nommu (processors without a memory-management unit) live; these include armv7-m, m68k, superh, and xtensa. Nobody is building anything with this kind of hardware now, and the only people who are working on them in any way are those who have to support existing systems. "&lt;quote&gt;Or to prove that it can be done&lt;/quote&gt;." &lt;/p&gt;
    &lt;p&gt;There are still some people who need to run 32-bit applications that cannot be updated; the solution he has been pushing people toward is to run a 32-bit user space on a 64-bit kernel. This is a good solution for memory-constrained systems; switching to 32-bit halves the memory usage of the system. Since, on most systems, almost all memory is used by user space, running a 64-bit kernel has a relatively small cost. Please, he asked, do not run 32-bit kernels on 64-bit processors.&lt;/p&gt;
    &lt;p&gt;There are some definite pain points that come with maintaining 32-bit support; most of the complaints, he said, come from developers in the memory-management subsystem. The biggest problem there is the need to support high memory; it is complex, and requires support throughout the kernel. High memory is needed when the kernel lacks the address space to map all of the installed physical memory; that tends to be at about 800MB on 32-bit systems. (See this article for more information about high memory).&lt;/p&gt;
    &lt;p&gt; Currently the kernel is able to support 32-bit systems with up to 16GB of installed memory. Such systems are exceedingly rare, though, and support for them will be going away soon. There are a few 4GB systems out there, including some Chromebooks. Systems with 2GB are a bit more common. Even these systems, he said, are "&lt;quote&gt;a bit silly&lt;/quote&gt;" since the memory costs more than the CPU does. There are some use cases for such systems, though. Most 32-bit systems now have less than 1GB of installed memory. The kernel, soon, will not try to support systems with more than 4GB. &lt;/p&gt;
    &lt;p&gt;There are some ideas out there for how to support the larger-memory 32-bit systems without needing the high-memory abstraction. Linus Walleij is working on entirely separating the kernel and user-space address spaces, giving each 4GB to work with; this is a variant on the "4G/4G" approach that has occasionally been tried for many years. It is difficult to make such a system work efficiently, so this effort may never succeed, Bergmann said.&lt;/p&gt;
    &lt;p&gt;Another approach is the proposed "densemem" memory model, which does some fancy remapping to close holes in the physical address space. Densemem can support up to 2GB and is needed to replace the SPARSEMEM memory model, the removal of which which will eventually be necessary in any case. This work has to be completed before high memory can be removed; Bergmann said that he would be interested in hearing from potential users of the densemem approach.&lt;/p&gt;
    &lt;p&gt;One other possibility is to drop high memory, but allow the extra physical memory to be used as a zram swap device. That would not be as efficient as accessing the memory directly, but it is relatively simple and would make it possible to drop the complexity of high memory.&lt;/p&gt;
    &lt;p&gt;Then, there is the year-2038 problem, which he spent several years working on. The kernel-side work was finished in 2020; the musl C library was updated that same year, and the GNU C Library followed the year after. Some distributors have been faster than others to incorporate this work; Debian and Ubuntu have only become year-2038-safe this year.&lt;/p&gt;
    &lt;p&gt;The year-2038 problem is not yet completely solved, though; there are a lot of packages that have unfixed bugs in this area. Anything using futex(), he said, has about a 50% chance of getting time handling right. The legacy 32-bit system calls, which are not year-2038 safe, are still enabled in the kernel, but they will go away at some point, exposing more bugs. There are languages, including Python and Rust, that have a lot of broken language bindings. Overall, he said, he does not expect any 32-bit desktop system to survive the year-2038 apocalypse.&lt;/p&gt;
    &lt;p&gt;A related problem is big-endian support, which is also 32-bit only, and also obsolete. Its removal is blocked because IBM is still supporting big-endian mainframe and PowerPC systems; as long as that support continues, big-endian support will stay in the kernel.&lt;/p&gt;
    &lt;p&gt; A number of other types of support are under discussion. There were once 32-bit systems with more than eight CPUs, but nobody is using those machines anymore, so support could be removed. Support for armv4 processors, such as the DEC StrongARM CPU, should be removed. Support for early armv6 CPUs, including the omap2 and i.mx31, "&lt;quote&gt;complicates everything&lt;/quote&gt;"; he would like to remove it, even though there are still some Nokia 770 systems in the wild. The time is coming for the removal of all non-devicetree board files. Removal of support for Cortex M CPUs, which are nommu systems, is coming in a couple of years. Developers are eyeing i486 CPU support, but that will not come out yet. Bergmann has sent patches to remove support for KVM on 32-bit CPUs, but there is still "&lt;quote&gt;one PowerPC user&lt;/quote&gt;", so that support will be kept for now. &lt;/p&gt;
    &lt;p&gt;To summarize, he said, the kernel will have to retain support for armv7 systems for at least another ten years. Boards are still being produced with these CPUs, so even ten years may be optimistic for removal. Everything else, he said, will probably fade away sooner than that. The removal of high-memory support has been penciled in for sometime around 2027, and nommu support around 2028. There will, naturally, need to be more discussion before these removals can happen.&lt;/p&gt;
    &lt;p&gt;An audience member asked how developers know whether a processor is still in use or not; Bergmann acknowledged that it can be a hard question. For x86 support, he looked at a lot of old web pages to make a list of which systems existed, then showed that each of those systems was already broken in current kernels for other reasons; the lack of complaints showed that there were no users. For others, it is necessary to dig through the Git history, see what kinds of changes are being made, and ask the developers who have worked on the code; they are the ones who will know who is using that support.&lt;/p&gt;
    &lt;p&gt; Another person asked about whether the kernel would support big-endian RISC-V systems. Bergmann answered that those systems are not supported now, and he hoped that it would stay that way. "&lt;quote&gt;With RISC-V, anybody can do anything, so they do, but it is not always a good idea&lt;/quote&gt;". The final question was about support for nommu esp32 CPUs; he answered that patches for those CPUs exist, but have not been sent upstream. Those processors are "&lt;quote&gt;a cool toy&lt;/quote&gt;", but he does not see any practical application for them. &lt;/p&gt;
    &lt;p&gt;The slides for this talk are available. The curious may also want to look at Bergmann's 2020 take on this topic.&lt;/p&gt;
    &lt;p&gt; [Thanks to the Linux Foundation, LWN's travel sponsor, for supporting my travel to this event.]&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Index entries for this article&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Kernel&lt;/cell&gt;
        &lt;cell&gt;Architectures&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Conference&lt;/cell&gt;
        &lt;cell&gt;Open Source Summit Europe/2025&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt; Posted Sep 1, 2025 19:55 UTC (Mon) by jrtc27 (subscriber, #107748) [Link] (4 responses) I don't understand this point. 64-bit big-endian systems exist in the form of sparc64 and s390x (and powerpc64, though that's less of a thing given powerpc64le). Posted Sep 1, 2025 21:47 UTC (Mon) by mjw (subscriber, #16740) [Link] (3 responses) Posted Sep 1, 2025 22:27 UTC (Mon) by sam_c (subscriber, #139836) [Link] (1 responses) (Of course, BE isn't the future, I just don't think the platforms are all rotting near-universally as 32-bit ones are on average.) Posted Sep 2, 2025 8:07 UTC (Tue) by arnd (subscriber, #8866) [Link] For the moment, the problems are mainly in newly written code, where portability to both big-endian and 32-bit targets relies on someone actively testing those and submitting fixes. This is in contrast to nommu and highmem systems that no longer have the same level of commercial backing and are much further into the inevitable bitrot. [I believe the article citing "big-endian support, which is also 32-bit only" was a transcription error in an otherwise excellent report -- what I actually said is that the majority of big-endian (embedded) systems are 32-bit, while pointing to s390x/powerpc64 servers as the ones that keep them viable.] Posted Sep 2, 2025 7:05 UTC (Tue) by glaubitz (subscriber, #96452) [Link] Does IBM already know that s390x is supposed to be obsolete now? Posted Sep 1, 2025 19:58 UTC (Mon) by jrtc27 (subscriber, #107748) [Link] (2 responses) That's not really true. Halving is the limit of what you could hope to achieve, if every data type in your system was a machine word or pointer. But char, short, int and long long don't change, (u)intNN_t don't change, and float/double don't change, so depending on what data you're actually operating on you can see anywhere between almost no change (e.g. some purely computational workload on large amounts of raw data) and halving the memory usage. Posted Sep 1, 2025 20:24 UTC (Mon) by arnd (subscriber, #8866) [Link] (1 responses) I had expected to see a much smaller difference between the two environments like you describe, the numbers I saw for anything I tried were always roughly 100% overhead for the 64-bit code compared to armhf. I think this is a combination of multiple effects I measured in addition to the obvious sizeof(long) difference: arm64 has longer instruction words than armv7/thumb2, small malloc() calls are aligned to larger cache lines (128 vs 64 bytes), ELF sections have a larger default alignment (64K vs 4K), etc. For many real workloads, a larger portion of RAM if of course going to be filled with the same user data (text, video, sensor input, network packages, ...) so the difference becomes smaller; for the most memory constrained systems, 100% overhead is surprisingly good estimate that holds true for both kernel data and userland. Posted Sep 2, 2025 5:59 UTC (Tue) by jrtc27 (subscriber, #107748) [Link] Posted Sep 1, 2025 20:03 UTC (Mon) by jrtc27 (subscriber, #107748) [Link] (5 responses) Unfortunately people are building those systems and others are proposing supporting them[1]. It is a real shame this is the approach that's being taken rather than having dedicated byte-swapping load/store instructions to accelerate processing big-endian data structures like network packets, which is the main use case that I'm aware of, outside of being compatible with old software written for big-endian mainframes. [1] https://lore.kernel.org/all/20250822165248.289802-1-ben.d... Posted Sep 2, 2025 7:06 UTC (Tue) by glaubitz (subscriber, #96452) [Link] (4 responses) Why is this unfortunate? I don't understand. Is Linux supposed to run only on architectures that a small group of people likes? Posted Sep 2, 2025 7:55 UTC (Tue) by jrtc27 (subscriber, #107748) [Link] (3 responses) What I have a problem with is needless fragmentation and needless complexity within one specific architecture, in this case RISC-V. Sometimes the right design choice is to say no to options. Nobody was building big-endian RISC-V hardware, nobody was seriously writing big-endian RISC-V software and everything was just fine, but now everyone is expected to support yet another variant of RISC-V that doesn't really achieve anything except create a whole lot of work. It's not some new and interesting architecture that approaches things differently that happens to be big-endian, it is just RISC-V but with big-endian because nobody was willing to say no. Posted Sep 2, 2025 8:06 UTC (Tue) by glaubitz (subscriber, #96452) [Link] (2 responses) I wasn't actually looking at the username and only realized now it was you ;-). &amp;gt; What I have a problem with is needless fragmentation and needless complexity within one specific architecture, in this case RISC-V. I think this is a developer-centric perspective rather than a user-centric perspective. If there are legitimate use cases such as building open-source networking hardware based on RISC-V, then it should be legitimate to add big-endian support if someone is willing to maintain it. Posted Sep 2, 2025 8:43 UTC (Tue) by arnd (subscriber, #8866) [Link] (1 responses) Adding big-endian Armv8 support to Linux made sense in 2013 as users were still porting software from big-endian MIPS/Octeon or PowerPC/QorIQ systems, but it ended up being a mistake for the same reason that Jessica explained about RISC-V: it's an extra ABI that requires testing resources in order to keep running, for very little practical use. Now that all the Linux networking applications moved to arm64le, the only remaining use case for arm64be is to have an easily available platform find and fix endianness bugs, typically in a VM guest running on a LE host. This means we can't easily remove from the arm64 kernel, but it would still be a mistake to add it to riscv64 Linux. Posted Sep 2, 2025 9:01 UTC (Tue) by glaubitz (subscriber, #96452) [Link] How do you know that? Posted Sep 1, 2025 20:53 UTC (Mon) by ajb (subscriber, #9694) [Link] (5 responses) There are also going to be many products where it doesn't make much difference whether an ancient processor or an up to date riscv is used, and manufacturers are very likely to continue to choose ones where the licence fee is already paid off. Posted Sep 1, 2025 22:11 UTC (Mon) by smoogen (subscriber, #97) [Link] (3 responses) Posted Sep 2, 2025 1:44 UTC (Tue) by willy (subscriber, #9762) [Link] (1 responses) Posted Sep 2, 2025 4:56 UTC (Tue) by ajb (subscriber, #9694) [Link] Posted Sep 2, 2025 7:30 UTC (Tue) by taladar (subscriber, #68407) [Link] Posted Sep 1, 2025 22:54 UTC (Mon) by iabervon (subscriber, #722) [Link] Posted Sep 2, 2025 0:42 UTC (Tue) by wileypob (subscriber, #139361) [Link] Posted Sep 2, 2025 2:44 UTC (Tue) by wtarreau (subscriber, #51152) [Link] (1 responses) Posted Sep 2, 2025 6:06 UTC (Tue) by jrtc27 (subscriber, #107748) [Link] Posted Sep 2, 2025 6:30 UTC (Tue) by andy_shev (subscriber, #75870) [Link] (1 responses) Posted Sep 2, 2025 7:33 UTC (Tue) by taladar (subscriber, #68407) [Link] Posted Sep 2, 2025 7:04 UTC (Tue) by glaubitz (subscriber, #96452) [Link] Neither M68k nor SuperH are nommu only. That statement is simply incorrect. And people are actually building new m68k-based hardware with the help of FPGAs. The retro community around the Amiga, Atari, Macintosh 68k as well as SuperH-based video game consoles such as the Sega Saturn and especially the Sega Dreamcast. Not sure why this article is so much focused on commercial applications. And I don't think big-endian targets are going to be obsolete any time soon unless someone convinces IBM to drop s390x. &lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;lb/&gt; The slides don't say that big-endian is 32-bit only, but "Equally obsolete as 32-bit".&lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;head&gt;64-bit big-endian&lt;/head&gt;&lt;head&gt;Not all types are words / pointers&lt;/head&gt;&lt;head&gt;Not all types are words / pointers&lt;/head&gt;&lt;head&gt;Not all types are words / pointers&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;Big-endian RISC-V&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;longevity&lt;/head&gt;&lt;head&gt;Embedded systems&lt;/head&gt;&lt;head&gt;MIPS ?&lt;/head&gt;&lt;head&gt;MIPS ?&lt;/head&gt;&lt;head&gt;Industrial computers&lt;/head&gt;&lt;head&gt;Industrial computers&lt;/head&gt;&lt;head&gt;M68k and SuperH are MMU ports!&lt;/head&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45095475</guid></item><item><title>Thoughts on (Amazonian) leadership</title><link>https://www.daemonology.net/blog/2025-09-01-Thoughts-on-Amazonian-Leadership.html</link><description>&lt;doc fingerprint="d7850520cbaafdc"&gt;
  &lt;main&gt;&lt;head rend="h2"&gt;Thoughts on (Amazonian) Leadership&lt;/head&gt;Amazon's Leadership Principles are famous, not just within Amazon but also in the tech world at large. While they're frequently mocked — including by Amazonians — they're also generally sensible rules by which to run a company. I've been an Amazon customer for over 25 years and an AWS customer for almost 20 years, and also an AWS Hero for 6 years, and while I've never worked for Amazon I feel that I've seen behind the curtain enough to offer some commentary on a few of these principles.&lt;list rend="ul"&gt;&lt;item&gt; Customer Obsession: Leaders start with the customer and work backwards. They work vigorously to earn and keep customer trust. Although leaders pay attention to competitors, they obsess over customers. &lt;lb/&gt;Customer Obsession is great, but I often see Amazonians taking this too simplistically: "Start with the customer" doesn't have to mean "ask customers what they want and then give them faster horses". In the early days of AWS I saw a lot of what I call "cool engineering driven" products: When EC2 launched, it wasn't really clear what people would do with it, but it was very cool and it was clear that it could be a big deal in some form, sooner or later. Some time around 2012, the culture in AWS seemed to shift from "provide cool building blocks" to "build what customers are asking for" and in my view this was a step in the wrong direction (mind you, not nearly as much as the ca. 2020 shift to "build what analysts are asking for in quarterly earnings calls").&lt;p&gt;This tension of what customers are asking for vs what customers really need shows up in areas like resilience. Amazon's "Well-Architected Framework" strongly exhorts customers to avoid building production workloads in a single Availability Zone — but Amazon's cross-AZ bandwidth pricing is painful, and Amazon doesn't provide useful tools for building durable multi-AZ applications. Most customers are not going to implement Paxos, and very few customers — certainly not executives who are removed from actual development processes — are going to ask Amazon for Paxos-as-a-service; but if Amazonians sat down and asked themselves "what do customers need in order to design their applications well" they could probably come up with several services which Amazon already has internally. AWS should return to its roots and release important building blocks — the things customers will need, not necessarily what they're asking for.&lt;/p&gt;&lt;/item&gt;&lt;item&gt; Ownership: Leaders are owners. They think long term and don't sacrifice long-term value for short-term results. They act on behalf of the entire company, beyond just their own team. They never say "that's not my job." &lt;lb/&gt;This principle is both too narrow, and not being fulfilled, in my view. It's not enough to simply act on behalf of the entire company: It's important to act on behalf of the entire technological ecosystem. Some Amazonians are great at this — I recently commited patches to FreeBSD's bhyve because an Amazonian was putting together a standard for interrupt handling in large VMs, and even though Amazon doesn't make any use of bhyve (at least, I don't think it does!) he understood the importance of getting standards widely accepted across the entire virtualization space rather than narrowly in the code Amazon relied upon. There's a saying in computer security, that anything which makes one of us less secure makes all of us less secure: Attackers will leverage an exploit against one system to allow them to attack another system. While the same does not directly apply in other fields, working with others to produce the best results for everyone will be much better in the long-term than focusing solely on what Amazon needs right now.&lt;p&gt;But in general Amazon doesn't even live up to its stated (narrow) promise of having leaders acting on behalf of the entire company — it's simply too siloed. Amazon is famously secretive, and this applies internally as well as externally: When AWS launches two similar services, it's often because two teams didn't know what each other was working on. How can leaders act across the entire company if nobody knows what's happening outside of their team? They can't; and if Amazon wants to allow its best people to be true Owners, Amazon needs to start breaking down walls.&lt;/p&gt;&lt;/item&gt;&lt;item&gt; Bias for Action: Speed matters in business. Many decisions and actions are reversible and do not need extensive study. We value calculated risk taking. &lt;lb/&gt;Amazonians talk about "one-way doors" and "two-way doors", and it is quite true that many decisions are can be reversed... but that doesn't always mean that there is no cost associated with reversing a decision. There is a clear and widely recognized tension between "Bias for Action" and another principle, "Insist on the Highest Standards"; but there is also a tension between this and earning and keeping customer trust. When AWS ships a service which is half-baked, it diminishes customer trust in AWS as a whole; even if the problems in that service ultimately get corrected (either by fixing them or in some cases by simply getting rid of a service which should never have existed in the first place) the memory of a failed launch will live on in customers' minds for years to come.&lt;p&gt;During my seven-year tenure as FreeBSD Security Officer, people knew me as the guy sending out security advisories; but the most important thing I did was not to ship Security Advisories — that is, it was to stop the train and say "no, we are not going to send this out yet". I knew that for all the importance of getting patches into people's hands in a timely manner, it was even more important to establish trust: If I gave people a broken patch, even once, they would be much slower to install security updates in the future. My team became familiar with the phrase "convince me that this is correct", and I'd like to see more of that at senior levels of Amazon: Principal and Distinguished Engineers need to step in with a bias for inaction, and use the respect they have earned to stop projects which do not meet the highest standards before they undermine trust. Amazon's hiring process famously includes "bar raisers" who can veto hiring decisions; they should also have service bar raisers who can veto launches.&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45095545</guid></item><item><title>Amazon has mostly sat out the AI talent war</title><link>https://www.businessinsider.com/amazon-ai-talent-wars-internal-document-2025-8</link><description>&lt;doc fingerprint="cf8db5f163666cc"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Amazon struggles to attract AI talent due to its pay model and perception of falling behind others.&lt;/item&gt;
      &lt;item&gt;Amazon's compensation model has long caused complaints from employees.&lt;/item&gt;
      &lt;item&gt;Competitors like Meta and OpenAI offer more attractive packages for AI engineers.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As the AI talent war sweeps across Silicon Valley, Amazon has largely sat on the sidelines. A confidential internal document, and accounts from people familiar with the matter, reveal why.&lt;/p&gt;
    &lt;p&gt;The company has flagged its unique pay structure, lagging AI reputation, and rigid return-to-office rules as major hurdles. Now, the tech giant is being pushed to rethink its recruiting strategy as it scrambles to compete for top talent.&lt;/p&gt;
    &lt;p&gt;The document, from late last year, was written by the HR team covering Amazon's non-retail businesses, including Amazon Web Services, advertising, devices, entertainment, and the newly formed artificial general intelligence team.&lt;/p&gt;
    &lt;p&gt;"GenAI hiring faces challenges like location, compensation, and Amazon's perceived lag in the space," the document noted. "Competitors often provide more comprehensive and aggressive packages." Business Insider obtained a copy of the document.&lt;/p&gt;
    &lt;p&gt;Amazon's absence from recent splashy AI hires underscores those concerns. Meta has pulled in high-profile talent from ScaleAI, Apple, and OpenAI. Google and OpenAI continue to be top destinations for AI experts, while Microsoft has even drafted a wish list of Meta AI employees it hopes to recruit.&lt;/p&gt;
    &lt;p&gt;Amazon's spokesperson initially told BI that the company continues to "adapt our approach to remain highly competitive, maintaining flexibility in both our compensation packages and work arrangements to attract and retain the best AI talent in this dynamic market."&lt;/p&gt;
    &lt;p&gt;Hours later, the spokesperson updated the statement, saying the premise of the story was "wrong," without providing any specifics.&lt;/p&gt;
    &lt;p&gt;"We continue to attract and retain some of the best people in the world and they're building and deploying GenAI applications at a rapid clip. Our compensation is competitive, but we also want missionaries who are passionate about inventing things that will make a meaningful difference for customers — for those kinds of people, there's no better place in the world to build."&lt;/p&gt;
    &lt;head rend="h2"&gt;Door desks and 'egalitarian' pay&lt;/head&gt;
    &lt;p&gt;Amazon is famously frugal. One of its origin stories recounts how the company bought cheap doors from Home Depot and hacked them together as office desks. This became guiding symbol of Amazon's cautious spending, with founder Jeff Bezos still using door desks today.&lt;/p&gt;
    &lt;p&gt;This penny-pinching culture has smashed straight into an AI hiring battle that's being fueled by unprecedented spending, putting Amazon in a tricky situation.&lt;/p&gt;
    &lt;p&gt;The internal document described compensation as one of the "hotly debated topics" among Amazon recruiters, citing the company's strict use of fixed salary bands for each role. Amazon's "egalitarian philosophy" on pay leaves its offers "below par" compared with top rivals, it added.&lt;/p&gt;
    &lt;p&gt;"The lack of salary range increases for several key job families over the past few years does not position Amazon as an employer of choice for top tech talent," the document warned.&lt;/p&gt;
    &lt;p&gt;For Amazon, missing out on top AI talent is a potential risk. The pool of top-tier AI researchers and engineers is limited, and without experts with deep knowhow, it's hard to compete at the frontier of the field. Indeed, Amazon has yet to find a blockbuster AI product like OpenAI's ChatGPT or Anthropic's Claude, although its Bedrock AI cloud service has made progress.&lt;/p&gt;
    &lt;p&gt;Amazon's pay structure has been a long-standing source of tension.&lt;/p&gt;
    &lt;p&gt;Several people who spoke to Business Insider cited the 2020 departure of Amazon robotics VP Brad Porter as evidence of the company's frugal approach hampering talent recruitment and retention. Porter left in part after Amazon refused to raise his pay band.&lt;/p&gt;
    &lt;p&gt;Amazon's stock vesting schedule is also heavily backloaded, a structure that can be less attractive to new hires. The policy extends even to top executives, who generally receive no cash bonuses.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Voting with their feet'&lt;/head&gt;
    &lt;p&gt;In addition to highlighting Amazon's "perceived lag in the AI space," the internal document said generative AI has further intensified the competition for specialized talent, particularly individuals with expertise in large language models.&lt;/p&gt;
    &lt;p&gt;An August report from venture capital firm SignalFire shows Amazon is on the lower end of engineering retention, far below Meta, OpenAI, and Anthropic. Jarod Reyes, SignalFire's head of developer community, told Business Insider that Amazon rivals are making bigger strides in AI, across open models, foundational research, and developer tooling.&lt;/p&gt;
    &lt;p&gt;"Amazon hasn't clearly positioned itself as a leader in the generative AI wave," Reyes said. "Engineers are paying attention and they're voting with their feet."&lt;/p&gt;
    &lt;p&gt;Some investors share that view. On Amazon's earnings call last month, Morgan Stanley analyst Brian Nowak pressed CEO Andy Jassy on Wall Street's "narrative right now that AWS is falling behind" in AI and fears of losing market share to rivals. Jassy's response fell flat, sending Amazon's stock lower during the call.&lt;/p&gt;
    &lt;p&gt;Amazon intends to tackle these concerns. According to the document, the company will refine its "compensation and location strategy" and host more events designed to highlight its generative AI capabilities. It also intends to set up dedicated recruiting teams for generative AI within business units like AWS to boost efficiency.&lt;/p&gt;
    &lt;head rend="h2"&gt;'Hubs' constrain talent&lt;/head&gt;
    &lt;p&gt;Another point of contention is Amazon's aggressive return-to-office mandate, which has already caused logistical issues.&lt;/p&gt;
    &lt;p&gt;The company's new "hub" policy — which requires employees to relocate to a central office or risk termination — has further limited its access to "high-demand talent like those with GenAI skills," according to the internal document.&lt;/p&gt;
    &lt;p&gt;"Hubs constrain market availability," it stated.&lt;/p&gt;
    &lt;p&gt;Amazon is exploring ways to allow for more "location-flexible" roles, the document added.&lt;/p&gt;
    &lt;p&gt;Amazon's spokesperson told BI that the company is "always looking for ways to optimize our recruiting strategies and looking at alternate talent rich locations."&lt;/p&gt;
    &lt;p&gt;Amazon hasn't been entirely on the sidelines. Last year, it brought on Adept CEO David Luan as part of a licensing deal with the AI startup. Luan now heads Amazon's AI agents lab. But the company has also seen departures, including senior AI leaders like chip designer Rami Sinno and VP Vasi Philomin, who worked on Bedrock.&lt;/p&gt;
    &lt;p&gt;One Amazon recruiter told Business Insider that a growing number of job candidates started declining offers last year because of the company's RTO policy. Even if a competitor pays less, people are open to taking the job if they can stay remote, this person said.&lt;/p&gt;
    &lt;p&gt;"We are losing out on talent," this person added.&lt;/p&gt;
    &lt;p&gt;Indeed, Bloomberg reported recently that Oracle has hired away more than 600 Amazon employees in the past two years because Amazon's strict RTO policy has made poaching easier.&lt;/p&gt;
    &lt;head rend="h2"&gt;Staying the course&lt;/head&gt;
    &lt;p&gt;The internal Amazon document dates to late last year, leaving open the possibility that the company has since adjusted its compensation approach to make exceptions for top AI talent.&lt;/p&gt;
    &lt;p&gt;Still, multiple people familiar with the situation told Business Insider there haven't been any formal updates to internal pay guidelines. One current Amazon manager said it remains almost impossible for the company to enact sweeping changes, given its long track record of sticking to the existing system. The people who spoke with Business Insider asked not to be identified discussing sensitive matters.&lt;/p&gt;
    &lt;p&gt;"Based on how we run our business and what we have achieved, there are more risks than potential benefits from changing an approach that has been so successful for our shareholders over the past several decades," Amazon wrote this year about executive compensation in its annual proxy statement.&lt;/p&gt;
    &lt;p&gt;Of course, the AI talent war may end up being an expensive and misguided strategy, stoked by hype and investor over-exuberance.&lt;/p&gt;
    &lt;p&gt;Some of the high-profile recruits Meta recently lured have already departed.&lt;/p&gt;
    &lt;p&gt;Have a tip? Contact this reporter via email at ekim@businessinsider.com or Signal, Telegram, or WhatsApp at 650-942-3061. Use a personal email address, a nonwork WiFi network, and a nonwork device; here's our guide to sharing information securely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45095603</guid></item><item><title>Patrick Winston: How to Speak (2018) [video]</title><link>https://www.youtube.com/watch?v=Unzc731iCUY</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket © 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45095849</guid></item><item><title>The buyer-pull and seller-push theories of sales</title><link>https://howtogrow.substack.com/p/the-physics-of-sales</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45096254</guid></item><item><title>Raspberry Pi 5 support (OpenBSD)</title><link>https://marc.info/?l=openbsd-cvs&amp;m=175675287220070&amp;w=2</link><description>&lt;doc fingerprint="e73f12d23de611fc"&gt;
  &lt;main&gt;&lt;quote&gt;[prev in list] [next in list] [prev in thread] [next in thread] List: openbsd-cvs Subject: CVS: cvs.openbsd.org: src From: Marcus Glocker &amp;lt;mglocker () cvs ! openbsd ! org&amp;gt; Date: 2025-09-01 18:56:04 Message-ID: dd1203a530237b22 () cvs ! openbsd ! org [Download RAW message or body] CVSROOT: /cvs Module name: src Changes by: mglocker@cvs.openbsd.org 2025/09/01 12:56:04 Modified files: distrib/arm64/iso: Makefile distrib/arm64/ramdisk: Makefile install.md list Log message: Add Raspberry Pi 5 Model B support for RAMDISK. Known issues: * Booting from PCIe storage HATs doesn't work because of missing U-Boot support. * WiFi on the Raspberry Pi 5 Model B "d0" boards doesn't work. * The active cooler (fan) doesn't work because of missing pwm/clock drivers (some work is in-progress). ok kettenis@, deraadt@ [prev in list] [next in list] [prev in thread] [next in thread] &lt;/quote&gt;&lt;lb/&gt;Configure |
About |
News |
Add a list |
Sponsored by KoreLogic&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45096585</guid></item><item><title>Kazeta: An operating system that brings the console gaming experience of 90s</title><link>https://kazeta.org/</link><description>&lt;doc fingerprint="859b1d2912326fbb"&gt;
  &lt;main&gt;&lt;p&gt;An operating system that brings the console gaming experience of the '90s to modern PC hardware and games: insert cart, power on, play.&lt;/p&gt;Explore Kazeta&lt;p&gt;An operating system that brings the console gaming experience of the '90s to modern PC hardware and games: insert cart, power on, play.&lt;/p&gt;Explore Kazeta&lt;p&gt;Insert a game cart, press power, and you're gaming instantly. Relive that nostalgic golden age where nothing stood between you and the games you love.&lt;/p&gt;&lt;p&gt;Transform your digital library into something tangible and permanent. Create physical game carts from your DRM-free titles and build a collection that you can play forever.&lt;/p&gt;&lt;p&gt;Say goodbye to the complexities of modern gaming and just play.&lt;/p&gt;&lt;p&gt;Save data is captured automatically, so you never lose progress. When no cart is inserted, boot into a retro console inspired BIOS menu to manage your saves.&lt;/p&gt;&lt;p&gt;Play almost any DRM-free game from platforms past or present.&lt;/p&gt;&lt;p&gt;Bring back the family-friendly simplicity of gaming's distant past. Perfect for kids, parents, and grandparents who just want to play.&lt;/p&gt;&lt;p&gt;Download Kazeta today and rediscover the joy of pure gaming.&lt;/p&gt;Download Now&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45098269</guid></item><item><title>Apple pulls iPhone torrent app from AltStore PAL in Europe</title><link>https://www.theverge.com/news/767344/apple-removes-itorrent-altstore-pal-ios-marketplace</link><description>&lt;doc fingerprint="a484c98b531e11d0"&gt;
  &lt;main&gt;
    &lt;p&gt;Apple has removed the iPhone torrenting client, iTorrent, from AltStore PAL’s alternative iOS marketplace in the EU, showing that it can still exert control over apps that aren’t listed on the official App Store. iTorrent developer Daniil Vinogradov told TorrentFreak that Apple has revoked his distribution rights to publish apps in any alternative iOS stores, but it seems the issue is related to government sanctions, rather than a block on torrenting.&lt;/p&gt;
    &lt;head rend="h1"&gt;Apple pulls iPhone torrent app from AltStore PAL in Europe&lt;/head&gt;
    &lt;p&gt;iTorrent’s developer has been blocked from distributing apps on alternative iOS stores.&lt;/p&gt;
    &lt;p&gt;iTorrent’s developer has been blocked from distributing apps on alternative iOS stores.&lt;/p&gt;
    &lt;p&gt;In a statement to The Verge, Apple spokesperson Peter Ajemian said, “Notarization for this app was removed in order to comply with government sanctions-related rules in various jurisdictions. We have communicated this to the developer.”&lt;/p&gt;
    &lt;p&gt;While Apple bans torrent apps on its own iOS store, the EU’s Digital Markets Act gave iPhone users within the bloc greater freedom to install apps from third-party app stores that the Cupertino company doesn’t directly manage.&lt;/p&gt;
    &lt;p&gt;Last month, Vinogradov said on iTorrent’s GitHub page that Apple “removed Alternative Distribution functionality from iTorrent’s Developer Portal without any warning.” Apple didn’t provide a reason for the removal, according Vinogradov, and distribution was revoked at the Apple Dev Account level.&lt;/p&gt;
    &lt;p&gt;Update, August 28th: Added a statement from Apple.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45098411</guid></item><item><title>FreeDroidWarn</title><link>https://github.com/woheller69/FreeDroidWarn</link><description>&lt;doc fingerprint="a7a090d3f3147310"&gt;
  &lt;main&gt;
    &lt;p&gt;This library shows an alert dialog with a deprecation warning informing that Google will require developer verification for Android apps outside the Play Store from 2026/2027 which the developer is not going to provide.&lt;/p&gt;
    &lt;code&gt;Google has announced that, starting in 2026/2027, all apps on certified Android devices
will require the developer to submit personal identity details directly to Google.
Since the developers of this app do not agree to this requirement, this app will no longer 
work on certified Android devices after that time.
&lt;/code&gt;
    &lt;p&gt;https://www.androidauthority.com/android-developer-verification-requirements-3590911/&lt;/p&gt;
    &lt;p&gt;https://developer.android.com/developer-verification&lt;/p&gt;
    &lt;p&gt;Add the JitPack repository to your root build.gradle at the end of repositories:&lt;/p&gt;
    &lt;code&gt;allprojects {
  repositories {
    ...
    maven { url 'https://jitpack.io' }
  }
}&lt;/code&gt;
    &lt;p&gt;Add the library dependency to your build.gradle file.&lt;/p&gt;
    &lt;code&gt;dependencies {
    implementation 'com.github.woheller69:FreeDroidWarn:V1.3'
}&lt;/code&gt;
    &lt;p&gt;In onCreate of your app just add:&lt;/p&gt;
    &lt;code&gt;     FreeDroidWarn.showWarningOnUpgrade(this, BuildConfig.VERSION_CODE);

&lt;/code&gt;
    &lt;p&gt;This library is licensed under the GPLv3.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45098722</guid></item><item><title>The Wetware Crisis: The Thermocline of Truth (2008)</title><link>https://brucefwebster.com/2008/04/15/the-wetware-crisis-the-themocline-of-truth/</link><description>&lt;doc fingerprint="ab4192145f241bef"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Wetware Crisis: the Thermocline of Truth&lt;/head&gt;&lt;p&gt;[Updated 09/12/13 — fixed some links and added a few.]&lt;/p&gt;&lt;p&gt;[Copyright 2008 by Bruce F. Webster. All rights reserved. Adapted from Surviving Complexity (forthcoming).]&lt;/p&gt;&lt;p&gt;A thermocline is a distinct temperature barrier between a surface layer of warmer water and the colder, deeper water underneath. It can exist in both lakes and oceans. A thermocline can prevent dissolved oxygen from getting to the lower layer and vital nutrients from getting to the upper layer.&lt;/p&gt;&lt;p&gt;In many large or even medium-sized IT projects, there exists a thermocline of truth, a line drawn across the organizational chart that represents a barrier to accurate information regarding the project’s progress. Those below this level tend to know how well the project is actually going; those above it tend to have a more optimistic (if unrealistic) view.&lt;/p&gt;&lt;p&gt;Several major (and mutually reinforcing) factors tend to create this thermocline. First, the IT software development profession largely lacks — or fails to put into place — automated, objective and repeatable metrics that can measure progress and predict project completion with any reasonable degree of accuracy. Instead, we tend to rely on seat-of-the-pants (or, less politely, out-of-one’s-butt) estimations by IT engineers or managers that a given subsystem or application is “80% done”. This, in turn, leads to the old saw that the first 90% of a software project takes 90% of the time, and the last 10% of a software projects takes the other 90% of the time. I’ll discuss the metrics issue at greater length in another chapter; suffice it to say that the actual state of completion of a major system is often truly unknown until an effort is made to put it into a production environment.&lt;/p&gt;&lt;p&gt;Second, IT engineers by nature tend to be optimists, as reflected in the common acronym SMOP: “simple matter of programming.” Even when an IT engineer doesn’t have a given subsystem completed, he tends to carry with him the notion that he whip everything into shape with a few extra late nights and weekends of effort, even though he may actually face weeks (or more) of work. (NOTE: my use of male pronouns is deliberate; it is almost always male IT engineers who have this unreasonable optimism. Female IT engineers in my experience are generally far more conservative and realistic, almost to a fault, which is why I prefer them. I just wish they weren’t so hard to find.)&lt;/p&gt;&lt;p&gt;Third, managers (including IT managers) like to look good and usually don’t like to give bad news, because their continued promotion depends upon things going well under their management. So even when they have problems to report, they tend to understate the problem, figuring they can somehow shuffle the work among their direct reports so as to get things back on track.&lt;/p&gt;&lt;p&gt;Fourth, upper management tends to reward good news and punish bad news, regardless of the actual truth content. Honesty in reporting problems or lack of progress is seldom rewarded; usually it is discouraged, subtly or at times quite bluntly. Often, said managers believe that true executive behavior comprises brow-beating and threatening lower managers in order to “motivate” them to solve whatever problems they might have.&lt;/p&gt;&lt;p&gt;As the project delivery deadline draws near, the thermocline of truth starts moving up the levels of management because it is becoming harder and harder to deny or hide just where the project stands. Even with that, the thermocline may not reach the top level of management until weeks or even just days before the project is scheduled to ship or go into production. This leads to the classic pattern of having a major schedule slip — or even outright project failure — happen just before the ship/production date.&lt;/p&gt;&lt;p&gt;Sometimes, even then management may not be willing to hear or acknowledge where things really are but instead insist on a “quick fix” to get things done. Or management will order the project to be shipped or put into production, at which point all parties discover (a) that the actual business drivers and requirements never successfully made it down through the thermocline to those building the system, (b) that there are serious (and perhaps fatal) quality issues with the delivered systems, and thus (c) that the delivered project doesn’t do what top management really requires.&lt;/p&gt;&lt;p&gt;[INSERTED – 04/30/08]&lt;/p&gt;&lt;p&gt;Since Jerry Weinberg (see comments) and others have disputed that the thermocline is “distinct”, let me insert two real-world examples that I have personal knowledge of from over a decade ago. Both examples involve Fortune 100 corporations that were undergoing Y2K remediation across the entire enterprise. In the first case, the corporate Y2K coordinator had a weekly meeting with the heads of ~20 divisions and departments within the corporation in which those senior executives would report on their division/department’s Y2K remediation status with a green/yellow/red code. Four weeks before Y2K remediation was scheduled to be completed, virtually all the division/departments were reporting green, with a few yellows. Just one week later — three weeks before remediation was to be completely — almost all the department/division heads suddenly reported their status to be yellow or red. The Y2K coordinator (who told me about the meeting right afterwards) looked around the room and asked, “So, what do you know today that you didn’t know a week ago?” No one had much of an answer.&lt;/p&gt;&lt;p&gt;A year later, I was asked by a major corporation to come in and review their Y2K remediation because almost exactly the same thing had happened: almost all the departments/division had been reporting each week that they were on schedule to complete their Y2K remediation until roughly two weeks before the remediation was supposed to be completed — and then suddenly about 70% of the departments/divisions said they weren’t going to be done on time. The mass shift from “on schedule” to “not on schedule” took place in exactly one week and happened just a few weeks before the deadline. I came in, interviewed some 40 people (under strict confidentiality, in spite of pressure from top management to reveal who said what), and wrote up an honest assessment of where things stood, with a plan for getting things done. The corporation then asked me to come in and implement that plan, so I ended up commuting over 2000 miles/week (back and forth) for 2-3 months to do just that.&lt;/p&gt;&lt;p&gt;I have seen the same pattern repeatedly in IT systems failure lawsuits I have worked on, particularly when I’ve had large numbers of internal e-mails and memos to review. At times, I can identify right where the thermocline is and how it creeps up the management chain as the deadline draws near. In such cases, it usually doesn’t reach the top of the management chain (which, in the case of these lawsuits, means the developer notifying the customer) until shortly (&amp;lt;1 month) before the reported deadline. In fact, this syndrome goes hand-in-hand with the IT system failure lawsuit pattern I call “The Never-Ending Story“.&lt;/p&gt;&lt;p&gt;[06/16/08]: In fact, here’s a real-world IT project review memo, written several years ago, that described a “thermocline of truth” with a very distinct and discrete boundary.&lt;/p&gt;&lt;p&gt;In short, Jerry’s arguments notwithstanding, I’ve seen the thermocline of truth, I’ve seen it be very distinct, and I’ve seen it work its way up the management chain — just as I’ve described. I’m not writing this to be clever or glib; I’m writing it because it really happens.&lt;/p&gt;&lt;p&gt;[END INSERTION]&lt;/p&gt;&lt;p&gt;Successful large-scale IT projects require active efforts to pierce the thermocline, to break it up, and to keep it from reforming. That, in turn, requires the honesty and courage at the lower levels of the project not just to tell the truth as to where things really stand, but to get up on the table and wave your arms until someone pays attention. It also requires the upper reaches of management to reward honesty, particularly when it involves bad news. That may sound obvious, but trust me — in many, many organizations that have IT departments, honesty is neither desired nor rewarded.&lt;/p&gt;&lt;p&gt;I know that first hand. I can think of one project — being developed by one firm (the one that retained me) for another company (the customer) — where I was in on a consulting basis as a chief architect. In the final planning meeting before submitting the bid to the customer, the project manager set forth an incredibly aggressive and unachievable schedule to be given to the customer. I objected forcefully in the meeting — after all, we didn’t even have an architecture yet, much less a design, yet the project manager already had a fixed completion date — and later that afternoon, I wrote up a memo listing thirteen (13) major risks I saw to the project. While some of the engineers on the project cheered the memo, management told me in so many words to shut up and architect.&lt;/p&gt;&lt;p&gt;However, less that two months later, I wrote a new memo — based on the old one — and pointed out that 12 of the 13 risks I had pointed out had actually come to pass. Shortly after that, the project manager had to go back to the customer with a new delivery schedule that was twice as long as the original one. A month or two after that, my role as an architect came to an end. I had a final lunch with the two head honchos in upper management, and to their credit, they asked for my final assessment. I told them that many of the bumps and potholes were just part of the software development process — but that they should never have given that blatantly unrealistic schedule to the customer. As I told them, “When you do something like that, in the end you look either dishonest or incompetent or both. And there’s no upside to that.”&lt;/p&gt;&lt;p&gt;A few months after I left, I got word that the schedule had slipped to three times the original length, and not long after that, I got word that the customer had canceled the project altogether. As I said: just no upside.&lt;/p&gt;&lt;p&gt;[UPDATE: 09/12/13]: Here’s a $1 billion failed USAF project that appears to have largely foundered on the thermocline of truth.&lt;/p&gt;&lt;p&gt;[For a discussion of where the thermocline analogy originally came to me, see this post at And Still I Persist.]&lt;/p&gt;&lt;head rend="h3"&gt;About the Author: bfwebster&lt;/head&gt;Webster is Principal and Founder at at Bruce F. Webster &amp;amp; Associates, as well as an Adjunct Professor for the BYU Computer Science Department. He works with organizations to help them with troubled or failed information technology (IT) projects. He has also worked in several dozen legal cases as a consultant and as a testifying expert, both in the United States and Japan. He can be reached at 303.502.4141 or at bwebster@bfwa.com.&lt;head rend="h3"&gt;Comments (31)&lt;/head&gt;&lt;p&gt;Trackback URL | Comments RSS Feed&lt;/p&gt;&lt;head rend="h3"&gt;Sites That Link to this Post&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt;Pitfall: Asking the wrong questions : Webster &amp;amp; Associates LLC | April 27, 2008&lt;/item&gt;&lt;item&gt;Some thoughts on “Up or Out” : Bruce F. Webster | April 29, 2008&lt;/item&gt;&lt;item&gt;Gender differences in coding styles? : Bruce F. Webster | June 9, 2008&lt;/item&gt;&lt;item&gt;Anatomy of a runaway IT project : Bruce F. Webster | June 16, 2008&lt;/item&gt;&lt;item&gt;The Termocline of Truth | Corpus Scriptorum Crumbum | June 17, 2008&lt;/item&gt;&lt;item&gt;The Human ESB at vedovini.net | June 19, 2008&lt;/item&gt;&lt;item&gt;The thermocline of truth — at NASA : Bruce F. Webster | August 26, 2008&lt;/item&gt;&lt;item&gt;The thermocline of innovation (NASA, again) : Bruce F. Webster | January 30, 2009&lt;/item&gt;&lt;item&gt;links for 2010-11-09 « AB's reflections | November 9, 2010&lt;/item&gt;&lt;item&gt;The Thermocline of Knowledge : Bruce F. Webster | April 8, 2011&lt;/item&gt;&lt;item&gt;Rupert Jones » Archive » Potemkinism | June 1, 2011&lt;/item&gt;&lt;item&gt;RISE: The Psychology of Computer Programming (Gerald M. Weinberg, 1971/1998) : Webster &amp;amp; Associates LLC | May 21, 2012&lt;/item&gt;&lt;item&gt;More IT failure news from England : Webster &amp;amp; Associates LLC | September 4, 2013&lt;/item&gt;&lt;item&gt;$1 billion example of the Thermocline of Truth : Webster &amp;amp; Associates LLC | September 9, 2013&lt;/item&gt;&lt;item&gt;Obamacare and the Thermocline of Truth : And Still I Persist… | September 26, 2013&lt;/item&gt;&lt;item&gt;Obamacare: descent into the maelstrom : And Still I Persist… | October 9, 2013&lt;/item&gt;&lt;item&gt;Thermocline of truth | Internet Scofflaw | October 25, 2013&lt;/item&gt;&lt;item&gt;Thermocline of Truth | Senior DBA | February 27, 2015&lt;/item&gt;&lt;item&gt;Teaching CS 428 (Software Engineering) at BYU : Bruce F. Webster | January 12, 2017&lt;/item&gt;&lt;item&gt;“The Surgical Team” in XXI Century | Tech Programing | January 4, 2021&lt;/item&gt;&lt;item&gt;Why Software Companies Die | Tech Programing | January 5, 2021&lt;/item&gt;&lt;item&gt;The Mind of David Krider | July 19, 2021&lt;/item&gt;&lt;/list&gt;&lt;list rend="ol"&gt;&lt;item&gt;&lt;p&gt;It’s an honor to get critiqued by someone like Jerry Weinberg, one of my personal IT heroes — I own a dozen or so of his books, all of which I’ve read.&lt;/p&gt;&lt;p&gt;It’s even more fun when I think he’s wrong, or at least a bit sloppy, in his critique. To wit:&lt;/p&gt;&lt;p&gt;The word “distinct” is where the model is most wrong. The barrier to truth percolating up in an organization is not generally “distinct.” Instead, it’s an incremental process, where each layer, each person, up the chain of command adds a little fudge to the data, so that by the time it reaches the top, it bears little or no resemblance to what went in at the bottom.&lt;/p&gt;&lt;p&gt;Actually, “distinct” was applied to the oceanographic definition, not necessarily the IT definition. But even so, it is my experience that when an IT project is in trouble in a large organization (the premises I set forth), there is a relatively distinct layer — usually no more than one layer of management — that forms the thermocline. Most of those below the layer are pretty convinced the project is in trouble; most of those above are convinced the project is doing well. As the project nears its deadline, that thermocline moves up the organization chart.&lt;/p&gt;&lt;p&gt;That said, I don’t disagree (and, in fact, my article agrees) that the fudging per se tends to take place at each level — my observations stands, however, that there often is a distinct layer where the troubled/not troubled flip occurs.&lt;/p&gt;&lt;p&gt;Another way the article is wrong is the belief that “metrics” will solve this problem. Any decent manager (and some not so decent) knows how to fudge any measurement you can concoct.&lt;/p&gt;&lt;p&gt;Two errors here on Jerry’s part. First, I never stated that metrics would fix the problem; I merely noted that the IT industry lacks “automated, objective and repeatable metrics” that can predict when an IT project will be completed, which is precisely why misinformation gets passed up the chain. Second, “automated, objective and repeatable” by definition pretty much precludes “fudging”, since it describes metrics that upper management can run and review independent of lower management.&lt;/p&gt;&lt;p&gt;f there is a “solution” to this problem, it requires higher levels of management to drop down through the levels and do some validation personally. This is not easy, for many reasons, but the best managers do this. Much of my work as a consultant to upper managers is exactly this kind of validation, bypassing the fudging layers.&lt;/p&gt;&lt;p&gt;Again, Jerry apparently did not read the paragraph that said “Successful large-scale IT projects require active efforts to pierce the thermocline, to break it up, and to keep it from reforming. That, in turn, requires the honesty and courage at the lower levels of the project not just to tell the truth as to where things really stand, but to get up on the table and wave your arms until someone pays attention. It also requires the upper reaches of management to reward honesty, particularly when it involves bad news. That may sound obvious, but trust me — in many, many organizations that have IT departments, honesty is neither desired nor rewarded.”&lt;/p&gt;&lt;p&gt;First, that describes — in more detail — what Jerry is referring to and notes that it requires effort not just from upper management but from those in the trenches as well.&lt;/p&gt;&lt;p&gt;Second, Jerry’s comment assumes that upper management wants to know the honest truth. I know from first-hand experience — as I suspect Jerry does — that often upper management doesn’t want to hear any bad news, they simply want the system to somehow go into production, magically or otherwise. (I personally dealt with this while reviewing a half-billion dollar project that was two years late and nowhere near completion.)&lt;/p&gt;&lt;p&gt;In other words, when a thermocline of truth forms, it is usually because upper management doesn’t want to hear the truth — they just want good news. As Jerry knows well (and has written about), few large organizations are set up to reward failure and honesty. ..bruce..&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;You wrote: “It’s an honor to get critiqued by someone like Jerry Weinberg, one of my personal IT heroes — I own a dozen or so of his books, all of which I’ve read. It’s even more fun when I think he’s wrong, or at least a bit sloppy, in his critique.”&lt;/p&gt;&lt;p&gt;I appreciate your feedback pointing out my sloppiness. I think blogs may tend to encourage sloppiness in me. I’ll try to do better as I clarify our areas of disagreement, which are small compared to the whole problem of lying about project progress, which we obviously agree on, even if we don’t agree precisely on the solutions.&lt;/p&gt;&lt;p&gt;You wrote:&lt;/p&gt;&lt;lb/&gt;“… I don’t disagree (and, in fact, my article agrees) that the fudging per se tends to take place at each level — my observations stands, however, that there often is a distinct layer where the troubled/not troubled flip occurs.”&lt;p&gt;Well, we work in different organizations, so it’s natural that we’d observe different patterns. I’m sure your observations are accurate for the organizations with which you work. In my experience, there may be places where the troubled/not troubled flip occurs, but it’s more an individual choice of managers at different levels in different departments. But there might be some correlation in levels brought about by roughly the same amount of fudging at each level. (There tends to be a limit to the amount of fudge any manager can do, and that limit tends to be constant throughout the organization.)&lt;/p&gt;&lt;p&gt;You wrote: “First, I never stated that metrics would fix the problem; I merely noted that the IT industry lacks “automated, objective and repeatable metrics” that can predict when an IT project will be completed, which is precisely why misinformation gets passed up the chain.”&lt;/p&gt;&lt;p&gt;Fair enough. I read too much into this: the implication that such “automated, objective and repeatable metrics” were somehow within reach, so that people should use them. I you and I we agree that they aren’t available (in spite of claims) to most organizations today. Certainly not the ones who fudge, fudge, fudge their project data.&lt;/p&gt;&lt;p&gt;Then you say, ‘Second, “automated, objective and repeatable” by&lt;/p&gt;&lt;lb/&gt;definition pretty much precludes “fudging”, since it describes metrics that upper management can run and review independent of lower management.’&lt;p&gt;Well, that could be taken as a definition, but maybe you really believe this kind of measurement system is easy to find, or even possible.&lt;/p&gt;&lt;p&gt;[As an added note, I have a chapter in my forthcoming book&lt;/p&gt;&lt;lb/&gt;entitled “Lies, Damned Lies, and Metrics”.]&lt;p&gt;[As an added note, I have an entire forthcoming book entitled “Perfect Software: and other fallacies about testing.” Apparently we both feel that this field is rife with lies and fallacies and myths.]&lt;/p&gt;&lt;p&gt;I wrote: “If there is a “solution” to this problem, it requires higher levels of management to drop down through the levels and do some validation personally. This is not easy, for many reasons, but the best managers do this. Much of my work as a consultant to upper managers is exactly this kind of validation, bypassing the fudging layers.”&lt;/p&gt;&lt;p&gt;You replied: “Again, Jerry apparently did not read the paragraph that said “Successful large-scale IT projects require active efforts to pierce the thermocline, to break it up, and to keep it from reforming. That, in turn, requires the honesty and courage at the lower levels of the project not just to tell the truth as to where things really stand, but to get up on the table and wave your arms until someone pays attention.”&lt;/p&gt;&lt;p&gt;I disagreed with your “pierce the thermocline” metaphor, obviously, because I didn’t agree with your thermocline metaphor to begin with. Where I’ve worked, there’s no thermocline to pierce, but a lot of layers of management to bypass and go straight to the source of the data.&lt;/p&gt;&lt;p&gt;And, I don’t expect a “thermocline” organization to magically produce “honesty and courage” at the lower levels. It’s the job of the upper management to create an environment where a whole lot of courage isn’t required for people to be honest. And that may require a lot more work than people imagine. [see my article with Jean McLendon, “Beyond Blaming,”&lt;/p&gt;&lt;p&gt;http://www.ayeconference.com/beyondblaming/ .&lt;/p&gt;&lt;p&gt;Also see my article “Destroying Communication and Control in Software Development”&lt;/p&gt;&lt;p&gt;http://www.stsc.hill.af.mil/crosstalk/2003/04/weinberg.html&lt;/p&gt;&lt;p&gt;You wrote: “It also requires the upper reaches of management to reward honesty, particularly when it involves bad news. That may sound obvious, but trust me — in many, many organizations that have IT departments, honesty is neither desired nor rewarded. … First, that describes — in more detail — what Jerry is referring to and notes that it requires effort not just from upper management but from those in the trenches as well.”&lt;/p&gt;&lt;p&gt;Here we totally agree. But generally, the effort will die if upper management is not proactive. That’s what they’re supposedly paid for.&lt;/p&gt;&lt;p&gt;You then write: “Second, Jerry’s comment assumes that upper management wants to know the honest truth. I know from first-hand experience — as I suspect Jerry does — that often upper management doesn’t want to hear any bad news, they simply want the system to somehow go into production, magically or otherwise.”&lt;/p&gt;&lt;p&gt;We may be in agreement here, but perhaps differ on the definition of “upper management.” If you’re talking about to upper management in the IT department, then I agree. But when I deal with the entire organization’s top management, outside of IT, one of the most intense questions they ask is, “How can I get honest reporting out of IT projects?” To them, IT is just one component of a successful organization, and a component they usually can’t understand on their own. They know they cannot do their job with underlings who lie, but they don’t know how to change this dynamic, whether it be thermoclines or fudgings. And I have to tell them is that the only way to do that is to bypass the cline or the fudge and “get to the bottom of things.” That’s why they’re paid those fabulous salaries.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Jerry:&lt;/p&gt;&lt;p&gt;I’m honored (honest!) that you’d take the time to come over here and expand on your critique; it’ll all help with the final product. I think we’re largely in violent agreement here, with the exception of whether thermoclines exist in organizations. I’ve seen them personally, usually when I’ve been called in to review a troubled project (though also when I’ve worked on as an expert on IT systems failure lawsuits). In other words, I’ve been able to pinpoint the layer at which the ‘flip’ occurs.&lt;/p&gt;&lt;p&gt;And, no, I don’t think that “automated, objective, repeatable metrics” are easy, and I’m not entirely clear they exist.&lt;/p&gt;&lt;p&gt;I look forward to your new books, as always! ..bruce..&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;I’ve run across similar problems in managing large, complicated&lt;/p&gt;&lt;lb/&gt;systems and one explanation I came up with can be explained by&lt;lb/&gt;a crude mathematical model.&lt;p&gt;Suppose we have a perfectly spherical elephant, whose mass may&lt;/p&gt;&lt;lb/&gt;be neglected … er, no suppose we have an organisation with&lt;lb/&gt;four levels of management. The lowest level is the one dealing&lt;lb/&gt;with the hardware, or the environment, and is therefore the level&lt;lb/&gt;with the most accurate and complete knowledge of reality. Let’s&lt;lb/&gt;say (for ease of calculation) that things are coasting along,&lt;lb/&gt;there’s equal amounts of good news and bad news at the coalface.&lt;p&gt;Now generally, people like to hear good news, and dislike to hear&lt;/p&gt;&lt;lb/&gt;bad news; in extreme cases, a “shoot the messenger” policy can be&lt;lb/&gt;in force. So it’s only human nature that reports of good news get&lt;lb/&gt;a bit exaggerated, and reports of bad news get trimmed back a touch.&lt;lb/&gt;Not by much; to make the maths easier, say good news gets increased&lt;lb/&gt;by 10 percent on each reporting, and bad news is reduced by the same&lt;lb/&gt;amount.&lt;p&gt;So after three levels of reporting, with equal amounts of good news&lt;/p&gt;&lt;lb/&gt;and bad news coming into the organisation, the man at the top hears&lt;lb/&gt;1 x 1.1 x 1.1 x 1.1 units of good news, and 1 x 0.9 x 0.9 x 0.9 units&lt;lb/&gt;of bad news; 1.33 to 0.73, or nearly twice as much good news as bad.&lt;lb/&gt;“Hey”, says the boss, “things are going really, really well.”&lt;p&gt;This, of course, assumes that information travels only through regular&lt;/p&gt;&lt;lb/&gt;channels, and doesn’t explain the thermocline effect, but does explain&lt;lb/&gt;most of the actual examples I’ve seen in managing large systems. My&lt;lb/&gt;(partial) cure was to be very pleased if anyone told me any bad news,&lt;lb/&gt;another approach would be to develop accurate reporting systems that&lt;lb/&gt;bypass multiple levels, but that would greatly upset the managers thus&lt;lb/&gt;bypassed. The real answer is to hire competent managers, but that’s&lt;lb/&gt;not always an option.&lt;p&gt;C W Rose&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Simply the best article Ive ever read.&lt;/p&gt;&lt;lb/&gt;So much so I had to blog about it&lt;p&gt;http://stevefouracre.blogspot.co.uk/2012/09/this-best-article-about-it-i-simply.html&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;This principle is covered by this well-known story:&lt;/p&gt;&lt;p&gt;http://ogun.stanford.edu/~bnayfeh/plan.html&lt;/p&gt;&lt;p&gt;“It is a vessel of fertilizer, and none may abide its strength.”&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;“… but that they should never have given that blatantly unrealistic schedule to the customer. ”&lt;/p&gt;&lt;p&gt;They would not have gotten the contract without the blatantly unrealistic schedule.&lt;/p&gt;&lt;/item&gt;&lt;item&gt;&lt;p&gt;Thank you for sharing Bruce. This article really struck home for a couple of reasons. I am constantly trying to direct my management in what it takes to successfully develop IT projects. In the end I still am still failing to deliver metrics that management approves of and understands. My management has limited IT background, which has presented a greater challenge in my career. As part of my job I realize that this this is something I need to improve upon, not management. Coincidently, part of my groups responsibility has involved us writing algorithms and software related to thermoclines in water bodies…&lt;/p&gt;&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45098867</guid></item><item><title>WinBoat: Run Windows apps on Linux with seamless integration</title><link>https://github.com/TibixDev/winboat</link><description>&lt;doc fingerprint="39bff56e5f27b5dc"&gt;
  &lt;main&gt;
    &lt;p&gt;WinBoat is currently in beta, so expect to occasionally run into hiccups and bugs. You should be comfortable with some level of troubleshooting if you decide to try it, however we encourage you to give it a shot anyway.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🎨 Elegant Interface: Sleek and intuitive interface that seamlessly integrates Windows into your Linux desktop environment, making it feel like a native experience&lt;/item&gt;
      &lt;item&gt;📦 Automated Installs: Simple installation process through our interface - pick your preferences &amp;amp; specs and let us handle the rest&lt;/item&gt;
      &lt;item&gt;🚀 Run Any App: If it runs on Windows, it can run on WinBoat. Enjoy the full range of Windows applications as native OS-level windows in your Linux environment&lt;/item&gt;
      &lt;item&gt;🖥️ Full Windows Desktop: Access the complete Windows desktop experience when you need it, or run individual apps seamlessly integrated into your Linux workflow&lt;/item&gt;
      &lt;item&gt;📁 Filesystem Integration: Your home directory is mounted in Windows, allowing easy file sharing between the two systems without any hassle&lt;/item&gt;
      &lt;item&gt;✨ And many more: Smartcard passthrough, resource monitoring, and more features being added regularly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Before running WinBoat, ensure your system meets the following requirements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RAM: At least 4 GB of RAM&lt;/item&gt;
      &lt;item&gt;CPU: At least 2 CPU threads&lt;/item&gt;
      &lt;item&gt;Storage: At least 32 GB free space in &lt;code&gt;/var&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Virtualization: KVM enabled in BIOS/UEFI&lt;/item&gt;
      &lt;item&gt;Docker: Required for containerization&lt;/item&gt;
      &lt;item&gt;Docker Compose v2: Required for compatibility with docker-compose.yml files&lt;/item&gt;
      &lt;item&gt;Docker User Group: Add your user to the &lt;code&gt;docker&lt;/code&gt;group&lt;/item&gt;
      &lt;item&gt;FreeRDP: Required for remote desktop connection (Please make sure you have Version 3.x.x with sound support included)&lt;/item&gt;
      &lt;item&gt;Kernel Modules: &lt;code&gt;iptables&lt;/code&gt;and&lt;code&gt;iptable_nat&lt;/code&gt;modules must be loaded&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can download the latest Linux builds under the Releases tab. We currently offer two variants:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;AppImage: A popular &amp;amp; portable app format which should run fine on most distributions&lt;/item&gt;
      &lt;item&gt;Unpacked: The raw unpacked files, simply run the executable (&lt;code&gt;linux-unpacked/winboat&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Podman is unsupported for now&lt;/item&gt;
      &lt;item&gt;Docker Desktop is unsupported for now&lt;/item&gt;
      &lt;item&gt;Distros that emulate Docker through a Podman socket are unsupported&lt;/item&gt;
      &lt;item&gt;Any rootless containerization solution is currently unsupported&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;For building you need to have NodeJS and Go installed on your system&lt;/item&gt;
      &lt;item&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Build the app and the guest server using &lt;code&gt;npm run build:linux-gs&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;You can now find the built app under &lt;code&gt;dist&lt;/code&gt;with an AppImage and an Unpacked variant&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Make sure you meet the prerequisites&lt;/item&gt;
      &lt;item&gt;Additionally, for development you need to have NodeJS and Go installed on your system&lt;/item&gt;
      &lt;item&gt;Clone the repo (&lt;code&gt;git clone https://github.com/TibixDev/WinBoat&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Install the dependencies (&lt;code&gt;npm i&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Build the guest server (&lt;code&gt;npm run build-guest-server&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Run the app (&lt;code&gt;npm run dev&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Contributions are welcome! Whether it's bug fixes, feature improvements, or documentation updates, we appreciate your help making WinBoat better.&lt;/p&gt;
    &lt;p&gt;Please note: We maintain a focus on technical contributions only. Pull requests containing political/sexual content, or other sensitive/controversial topics will not be accepted. Let's keep things focused on making great software! 🚀&lt;/p&gt;
    &lt;p&gt;Feel free to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Report bugs and issues&lt;/item&gt;
      &lt;item&gt;Submit feature requests&lt;/item&gt;
      &lt;item&gt;Contribute code improvements&lt;/item&gt;
      &lt;item&gt;Help with documentation&lt;/item&gt;
      &lt;item&gt;Share feedback and suggestions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Check out our issues page to get started, or feel free to open a new issue if you've found something that needs attention.&lt;/p&gt;
    &lt;p&gt;WinBoat is licensed under the MIT license&lt;/p&gt;
    &lt;p&gt;These past few years some cool projects have surfaced with similar concepts, some of which we've also taken inspirations from.&lt;lb/&gt; They're awesome and you should check them out:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;WinApps&lt;/item&gt;
      &lt;item&gt;Cassowary&lt;/item&gt;
      &lt;item&gt;dockur/windows (🌟 Also used in WinBoat)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;🌐 Website: winboat.app&lt;/item&gt;
      &lt;item&gt;🐦 Twitter/X: @winboat_app&lt;/item&gt;
      &lt;item&gt;🦋 Bluesky: winboat.app&lt;/item&gt;
      &lt;item&gt;🗨️ Discord: Join our community&lt;/item&gt;
      &lt;item&gt;📧 Email: staff@winboat.app&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45099124</guid></item><item><title>Collecting All Causal Knowledge</title><link>https://causenet.org/</link><description>&lt;doc fingerprint="ef076201388bcbc9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Collecting All Causal Knowledge&lt;/head&gt;
    &lt;p&gt;CauseNet aims at creating a causal knowledge base that comprises all human causal knowledge and to separate it from mere causal beliefs, with the goal of enabling large-scale research into causal inference.&lt;/p&gt;
    &lt;head rend="h1"&gt;CauseNet: Towards a Causality Graph Extracted from the Web&lt;/head&gt;
    &lt;p&gt;Causal knowledge is seen as one of the key ingredients to advance artificial intelligence. Yet, few knowledge bases comprise causal knowledge to date, possibly due to significant efforts required for validation. Notwithstanding this challenge, we compile CauseNet, a large-scale knowledge base of claimed causal relations between causal concepts. By extraction from different semi- and unstructured web sources, we collect more than 11 million causal relations with an estimated extraction precision of 83\% and construct the first large-scale and open-domain causality graph. We analyze the graph to gain insights about causal beliefs expressed on the web and we demonstrate its benefits in basic causal question answering. Future work may use the graph for causal reasoning, computational argumentation, multi-hop question answering, and more.&lt;/p&gt;
    &lt;head rend="h2"&gt;Download&lt;/head&gt;
    &lt;p&gt;We provide three versions of our causality graph CauseNet:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CauseNet-Full: The complete dataset&lt;/item&gt;
      &lt;item&gt;CauseNet-Precision: A subset of CauseNet-Full with higher precision&lt;/item&gt;
      &lt;item&gt;CauseNet-Sample: A small sample dataset for first explorations and experiments without provenance data&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Statistics&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Relations&lt;/cell&gt;
        &lt;cell role="head"&gt;Concepts&lt;/cell&gt;
        &lt;cell role="head"&gt;File Size&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CauseNet-Full&lt;/cell&gt;
        &lt;cell&gt;11,609,890&lt;/cell&gt;
        &lt;cell&gt;12,186,195&lt;/cell&gt;
        &lt;cell&gt;1.8GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;CauseNet-Precision&lt;/cell&gt;
        &lt;cell&gt;199,806&lt;/cell&gt;
        &lt;cell&gt;80,223&lt;/cell&gt;
        &lt;cell&gt;135MB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;CauseNet-Sample&lt;/cell&gt;
        &lt;cell&gt;264&lt;/cell&gt;
        &lt;cell&gt;524&lt;/cell&gt;
        &lt;cell&gt;54KB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Data Model&lt;/head&gt;
    &lt;p&gt;The core of CauseNet consists of causal concepts which are connected by causal relations. Each causal relation has comprehensive provenance data on where and how it was extracted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Examples of Causal Relations&lt;/head&gt;
    &lt;p&gt;Causal relations are represented as shown in the following example. Provenance data is omitted.&lt;/p&gt;
    &lt;code&gt;{
    "causal_relation": {
        "cause": {
            "concept": "disease"
        },
        "effect": {
            "concept": "death"
        }
    }
}
&lt;/code&gt;
    &lt;p&gt;For CauseNet-Full and CauseNet-Precision, we include comprehensive provenance data. In the following, we give one example per source.&lt;/p&gt;
    &lt;p&gt;For relations extracted from natural language sentences we provide:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;surface&lt;/code&gt;: the surface form of the sentence, i.e., the original string&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;path_pattern&lt;/code&gt;: the linguistic path pattern used for extraction&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;ClueWeb12 Sentences&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;clueweb12_page_id&lt;/code&gt;: page id as provided in the ClueWeb12 corpus&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clueweb12_page_reference&lt;/code&gt;: page reference as provided in the ClueWeb12 corpus&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;clueweb12_page_timestamp&lt;/code&gt;: page access data as stated in the ClueWeb12 corpus&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
    "causal_relation":{
        "cause":{
            "concept":"smoking"
        },
        "effect":{
            "concept":"disability"
        }
    },
    "sources":[
        {
            "type":"clueweb12_sentence",
            "payload":{
                "clueweb12_page_id":"urn:uuid:4cbae00e-8c7f-44b1-9f02-d797f53d448a",
                "clueweb12_page_reference":"http://atlas.nrcan.gc.ca/site/english/maps/health/healthbehaviors/smoking",
                "clueweb12_page_timestamp":"2012-02-23T21:10:45Z",
                "sentence": "In Canada, smoking is the most important cause of preventable illness, disability and premature death.",
                "path_pattern":"[[cause]]/N\t-nsubj\tcause/NN\t+nmod:of\t[[effect]]/N"
            }
        }
    ]
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Wikipedia Sentences&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;wikipedia_page_id&lt;/code&gt;: the Wikipedia page id&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;wikipedia_page_title&lt;/code&gt;: the Wikipedia page title&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;wikipedia_revision_id&lt;/code&gt;: the Wikipedia revision id of the last edit&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;wikipedia_revision_timestamp&lt;/code&gt;: the timestamp of the Wikipedia revision id of the last edit&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sentence_section_heading&lt;/code&gt;: the section heading where the sentence comes from&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;sentence_section_level&lt;/code&gt;: the level where the section heading comes from&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
    "causal_relation":{
        "cause":{
            "concept":"human_activity"
        },
        "effect":{
            "concept":"climate_change"
        }
    },
    "sources":[
        {
            "type":"wikipedia_sentence",
            "payload":{
                "wikipedia_page_id":"13109",
                "wikipedia_page_title":"Global warming controversy",
                "wikipedia_revision_id":"860220175",
                "wikipedia_revision_timestamp":"2018-09-19T04:52:18Z",
                "sentence_section_heading":"Global warming controversy",
                "sentence_section_level":"1",
                "sentence": "The controversy is, by now, political rather than scientific: there is a scientific consensus that climate change is happening and is caused by human activity.",
                "path_pattern":"[[cause]]/N\t-nmod:agent\tcaused/VBN\t+nsubjpass\t[[effect]]/N"
            }
        }
    ]
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Wikipedia Lists&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;list_toc_parent_title&lt;/code&gt;: The heading of the parent section the list appears in&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;list_toc_section_heading&lt;/code&gt;: The heading of the section the list appears in&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;list_toc_section_level&lt;/code&gt;: The nesting level of the section within the table of content (toc)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
    "causal_relation":{
        "cause":{
            "concept":"separation_from_parents"
        },
        "effect":{
            "concept":"stress_in_early_childhood"
        }
    },
    "sources":[
        {
            "type":"wikipedia_list",
            "payload":{
                "wikipedia_page_id":"33096801",
                "wikipedia_page_title":"Stress in early childhood",
                "wikipedia_revision_id":"859225864",
                "wikipedia_revision_timestamp":"2018-09-12T16:22:05Z",
                "list_toc_parent_title":"Stress in early childhood",
                "list_toc_section_heading":"Causes",
                "list_toc_section_level":"2"
            }
        }
    ]
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;Wikipedia Infoboxes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;infobox_template&lt;/code&gt;: The Wikipedia template of the infobox&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;infobox_title&lt;/code&gt;: The title of the Wikipedia infobox&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;infobox_argument&lt;/code&gt;: The argument of the infobox (the key of the key-value pair)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;{
    "causal_relation":{
        "cause":{
            "concept":"alcohol"
        },
        "effect":{
            "concept":"cirrhosis"
        }
    },
    "sources":[
        {
            "type":"wikipedia_infobox",
            "payload":{
                "wikipedia_page_id":"21365918",
                "wikipedia_page_title":"Cirrhosis",
                "wikipedia_revision_id":"861860835",
                "wikipedia_revision_timestamp":"2018-09-30T15:40:21Z",
                "infobox_template":"Infobox medical condition (new)",
                "infobox_title":"Cirrhosis",
                "infobox_argument":"causes"
            }
        }
    ]
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Loading CauseNet into Neo4j&lt;/head&gt;
    &lt;p&gt;We provide sample code to load CauseNet into the graph database Neo4j.&lt;/p&gt;
    &lt;p&gt;The following figure shows an excerpt of CauseNet within Neo4j (showing a coronavirus causing the disease SARS):&lt;/p&gt;
    &lt;head rend="h2"&gt;Concept Spotting Datasets&lt;/head&gt;
    &lt;p&gt;For the construction of CauseNet, we employ a causal concept spotter as a causal concept can be composed of multiple words (e.g., “global warming”, “human activity”, or “lack of exercise”). We determine the exact start and end of a causal concept in a sentence with a sequence tagger. Our training and evaluation data is available as part of our concept spotting datasets: one for Wikipedia infoboxes, Wikipedia lists, and ClueWeb sentences. We split each dataset into 80% training, 10% development and 10% test set&lt;/p&gt;
    &lt;head rend="h2"&gt;Paper&lt;/head&gt;
    &lt;p&gt;CauseNet forms the basis for our CIKM 2020 paper CauseNet: Towards a Causality Graph Extracted from the Web. Please make sure to refer to it as follows:&lt;/p&gt;
    &lt;code&gt;@inproceedings{heindorf2020causenet,
  author    = {Stefan Heindorf and
               Yan Scholten and
               Henning Wachsmuth and
               Axel-Cyrille Ngonga Ngomo and
               Martin Potthast},
  title     = {CauseNet: Towards a Causality Graph Extracted from the Web},
  booktitle = {{CIKM}},
  publisher = {{ACM}},
  year      = {2020}
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Contact&lt;/head&gt;
    &lt;p&gt;For questions and feedback please contact:&lt;/p&gt;
    &lt;p&gt;Stefan Heindorf, Paderborn University&lt;lb/&gt; Yan Scholten, Technical University of Munich&lt;lb/&gt; Henning Wachsmuth, Paderborn University&lt;lb/&gt; Axel-Cyrille Ngonga Ngomo, Paderborn University&lt;lb/&gt; Martin Potthast, Leipzig University&lt;/p&gt;
    &lt;head rend="h2"&gt;Licenses&lt;/head&gt;
    &lt;p&gt;The code is licensed under a MIT license. The data is licensed under a Creative Commons Attribution 4.0 International license.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45099418</guid></item><item><title>Next.js Is Infuriating</title><link>https://blog.meca.sh/3lxoty3shjc2z</link><description>&lt;doc fingerprint="d05093007b4164b8"&gt;
  &lt;main&gt;
    &lt;p&gt;Hey, it's finally happened. I've decided to write a blog post. And if you're reading this, I've also finished one. I have wanted to do this for a long time, but could never find the motivation to start. But you know what they say: anger is the best motivator. They do say that, right?&lt;/p&gt;
    &lt;head rend="h3"&gt;Some context that's in the background&lt;/head&gt;
    &lt;p&gt;We're going on a journey, you and I. But first, we need to set the scene. Imagine we're working for $COMPANY and one of our Next.js services did an oopsie. This being Next.js, we of course have no idea what actually happened since the default logging is only enabled during development.&lt;/p&gt;
    &lt;p&gt;Our quest is to go in and setup some production ready logging. It's not going to be easy, but then again, nothing ever is.&lt;/p&gt;
    &lt;head rend="h3"&gt;Middleware? Middle of nowhere!&lt;/head&gt;
    &lt;p&gt;The first step of our journey is the middleware. The documentation even states this:&lt;/p&gt;
    &lt;quote&gt;Middleware executes before routes are rendered. It's particularly useful for implementing custom server-side logic like authentication, logging, or handling redirects.&lt;/quote&gt;
    &lt;p&gt;Alright, looks simple enough. Time to pick a logging library. I went with pino since we have used it before. Anything is an upgrade over &lt;code&gt;console.log&lt;/code&gt; anyways. We'll get this done before lunch.&lt;/p&gt;
    &lt;p&gt;Let's set up a basic middleware:&lt;/p&gt;
    &lt;code&gt;// middleware.ts
import { NextResponse, NextRequest } from "next/server";

export async function middleware(request: NextRequest) {
  return new NextResponse.next({
    request: request,
    headers: request.headers,
    // status: 200,
    // statusText: 'OK'
  });
}

export const config = {
  matcher: "/:path*",
};&lt;/code&gt;
    &lt;p&gt;I think we already have a problem here. You can pass a grand total of 4 parameters from your middleware. The only thing that actually affects the invoked route is the &lt;code&gt;headers&lt;/code&gt;. Let's not skip over the fact that you can't have multiple middlewares or chain them either. How do you fuck this up so bad? We've had middlewares since at least the early 2010s when Express came out.&lt;/p&gt;
    &lt;p&gt;Anyways, we're smart and modern Node.js has some pretty nifty tools. Let's reach for &lt;code&gt;AsyncLocalStorage&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;// app/logger.ts
import { AsyncLocalStorage } from "async_hooks";
import { Logger, pino } from "pino";

const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "trace",
});

export const LoggerStorage = new AsyncLocalStorage&amp;lt;Logger&amp;gt;();

export function logger(): Logger | null {
  return LoggerStorage.getStore() ?? null;
}

export function requestLogger(): Logger {
  return loggerInstance.child({ requestId: crypto.randomUUID() });
}

// middleware.ts
export async function middleware(request: NextRequest) {
  LoggerStorage.enterWith(requestLogger());
  logger()?.debug({ url: request.url }, "Started processing request!");

  return NextResponse.next();
}&lt;/code&gt;
    &lt;p&gt;Whew, hard work done. Let's test it out. Visit localhost:3000 and we see this:&lt;/p&gt;
    &lt;code&gt;{ requestId: 'ec7718fa-b1a2-473e-b2e2-8f51188efa8f' } { url: 'http://localhost:3000/' } 'Started processing request!'
 GET / 200 in 71ms
{ requestId: '09b526b1-68f4-4e90-971f-b0bc52ad167c' } { url: 'http://localhost:3000/next.svg' } 'Started processing request!'
{ requestId: '481dd2ff-e900-4985-ae15-0b0a1eb5923f' } { url: 'http://localhost:3000/vercel.svg' } 'Started processing request!'
{ requestId: 'e7b29301-171c-4c91-af25-771471502ee4' } { url: 'http://localhost:3000/file.svg' } 'Started processing request!'
{ requestId: '13766de3-dd00-42ce-808a-ac072dcfd4c6' } { url: 'http://localhost:3000/window.svg' } 'Started processing request!'
{ requestId: '317e054c-1a9a-4dd8-ba21-4c0201fbeada' } { url: 'http://localhost:3000/globe.svg' } 'Started processing request!'&lt;/code&gt;
    &lt;p&gt;I don't know if you've ever used pino before, but this is wrong. Can you figure out why?&lt;/p&gt;
    &lt;p&gt;Unlike Next.js I won't keep you waiting in limbo. This is the browser output. Why? Well, it's because the default Next.js middleware runtime is &lt;code&gt;edge&lt;/code&gt;. We can of course switch to the &lt;code&gt;nodejs&lt;/code&gt; runtime which should work. Except, of course, it might not.&lt;/p&gt;
    &lt;p&gt;I've tried it on a fresh Next.js project and it does work. But it didn't when I was trying it out on our actual project. I swear I'm not crazy. Anyways, this isn't the main issue. We're slowly getting there.&lt;/p&gt;
    &lt;head rend="h3"&gt;Paging the local mental asylum&lt;/head&gt;
    &lt;p&gt;Logging in the middleware is cool and all, but it's not where the bulk of the magic happens. For that, we need to log in pages and layouts. Let's try it out.&lt;/p&gt;
    &lt;code&gt;// app/page.tsx
export default function Home() {
  logger()?.info("Logging from the page!");

  return &amp;lt;div&amp;gt;Real simple website!&amp;lt;/div&amp;gt;
}&lt;/code&gt;
    &lt;p&gt;Refresh the page and we get this:&lt;/p&gt;
    &lt;code&gt;✓ Compiled / in 16ms
 GET / 200 in 142ms&lt;/code&gt;
    &lt;p&gt;That's it? That's it. Nothing. Nada. Zilch.&lt;/p&gt;
    &lt;p&gt;For posterity's sake, this is what it's supposed to look like:&lt;/p&gt;
    &lt;code&gt;✓ Compiled / in 2.2s
[11:38:59.259] INFO (12599): Logging from the page!
    requestId: "2ddef9cf-6fee-4d1d-8b1e-6bb16a3e636b"
 GET / 200 in 2520ms&lt;/code&gt;
    &lt;p&gt;Ok,this is getting a bit long, so I'll get to the point. The &lt;code&gt;logger&lt;/code&gt; function returns &lt;code&gt;null&lt;/code&gt;. Why? I'm not entirely sure, but it seems like rendering is not executed in the same async context as the middleware.&lt;/p&gt;
    &lt;p&gt;What's the solution then? You're not going to believe this. Remember how the only value you can pass from the middleware is &lt;code&gt;headers&lt;/code&gt;? Yeah. That's what we need to use.&lt;/p&gt;
    &lt;p&gt;The following is for people with strong stomachs:&lt;/p&gt;
    &lt;code&gt;// app/log/serverLogger.ts
import { pino } from "pino";

export const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "info",
});

// app/log/middleware.ts
// Yes, we need to split up the loggers ...
// Mostly the same as before
import { loggerInstance } from "./serverLogger";

export function requestLogger(requestId: string): Logger {
  return loggerInstance.child({ requestId });
}

// app/log/server.ts
import { headers } from "next/headers";
import { loggerInstance } from "./serverLogger";
import { Logger } from "pino";
import { NextRequest } from "next/server";

const REQUEST_ID_HEADER = "dominik-request-id";

export function requestHeaders(
  request: NextRequest,
  requestId: string,
): Headers {
  const head = new Headers(request.headers);
  head.set(REQUEST_ID_HEADER, requestId);
  return head;
}

// Yeah, this has to be async ...
export async function logger(): Promise&amp;lt;Logger&amp;gt; {
  const hdrs = await headers();
  const requestId = hdrs.get(REQUEST_ID_HEADER);

  return loggerInstance.child({ requestId });
}

// middleware.ts
import { logger, LoggerStorage, requestLogger } from "./app/log/middleware";
import { requestHeaders } from "./app/log/server";

export async function middleware(request: NextRequest) {
  const requestId = crypto.randomUUID();
  LoggerStorage.enterWith(requestLogger(requestId));

  logger()?.debug({ url: request.url }, "Started processing request!");

  return NextResponse.next({ headers: requestHeaders(request, requestId) });
}

// app/page.tsx
export default async function Home() {
  (await logger())?.info("Logging from the page!");

  // ...
}&lt;/code&gt;
    &lt;p&gt;Isn't it beautiful? I especially appreciate how it's now possible to import the middleware logging code from the server. Which of course won't work. Or import the server logging code from the middleware. Which also won't work. Better not mess up. Also, we haven't even touched upon logging in client components, which despite the name also run on the server. Yeah, that's a third split.&lt;/p&gt;
    &lt;head rend="h3"&gt;Congratulations, you're being coddled. Please do not resist.&lt;/head&gt;
    &lt;p&gt;Listen. I wanted to apologize, because I've led you into this trap. You see, I have already fallen into it several times before. A middleware system can be pretty useful when designed correctly and I wanted you to see what it looks like when it's not. The reason for writing this blog post actually started here.&lt;/p&gt;
    &lt;p&gt;I think every one of us has reached a point in their lives where they've had enough. For me, it was right here. Fuck it, let's use a custom server.&lt;/p&gt;
    &lt;quote&gt;A custom Next.js server allows you to programmatically start a server for custom patterns. The majority of the time, you will not need this approach. However, it's available if you need to eject.&lt;/quote&gt;
    &lt;p&gt;Let's take a look at the example from the documentation:&lt;/p&gt;
    &lt;code&gt;import { createServer } from 'http'
import { parse } from 'url'
import next from 'next'
 
const port = parseInt(process.env.PORT || '3000', 10)
const dev = process.env.NODE_ENV !== 'production'
const app = next({ dev })
const handle = app.getRequestHandler()
 
app.prepare().then(() =&amp;gt; {
  createServer((req, res) =&amp;gt; {
    const parsedUrl = parse(req.url!, true)
    handle(req, res, parsedUrl)
  }).listen(port)
 
  console.log(
    `&amp;gt; Server listening at http://localhost:${port} as ${
      dev ? 'development' : process.env.NODE_ENV
    }`
  )
})&lt;/code&gt;
    &lt;p&gt;Note that once again, &lt;code&gt;handle&lt;/code&gt; doesn't really take any parameters. Only the request URL and the raw request and response.&lt;/p&gt;
    &lt;p&gt;Anyways, we still have &lt;code&gt;AsyncLocalStorage&lt;/code&gt; so this doesn't concern us too much. Let's modify the example a bit.&lt;/p&gt;
    &lt;code&gt;// app/logger.ts
// Reverted back to our AsyncLocalStorage variaton
import { pino, Logger } from "pino";
import { AsyncLocalStorage } from "async_hooks";

const loggerInstance = pino({
  // Whatever config we need
  level: process.env.LOG_LEVEL ?? "info",
});

export const LoggerStorage = new AsyncLocalStorage&amp;lt;Logger&amp;gt;();

export function logger(): Logger | null {
  return LoggerStorage.getStore() ?? null;
}

export function requestLogger(): Logger {
  return loggerInstance.child({ requestId: crypto.randomUUID() });
}

// server.ts
import { logger, LoggerStorage, requestLogger } from "./app/logger";

app.prepare().then(() =&amp;gt; {
  createServer(async (req, res) =&amp;gt; {
    // This is new
    LoggerStorage.enterWith(requestLogger());
    logger()?.info({}, "Logging from server!");

    const parsedUrl = parse(req.url!, true);
    await handle(req, res, parsedUrl);
  }).listen(port);
});

// middleware.ts
import { logger } from "./app/logger";

export async function middleware(request: NextRequest) {
  logger()?.info({}, "Logging from middleware!");
  return NextResponse.next();
}

// app/page.tsx
import { logger } from "./logger";

export default async function Home() {
  logger()?.info("Logging from the page!");
  
  // ...
}&lt;/code&gt;
    &lt;p&gt;Ok, let's test it out. Refresh the browser and ...&lt;/p&gt;
    &lt;code&gt;&amp;gt; Server listening at http://localhost:3000 as development
[12:29:52.183] INFO (19938): Logging from server!
    requestId: "2ffab9a2-7e15-4188-8959-a7822592108f"
 ✓ Compiled /middleware in 388ms (151 modules)
 ○ Compiling / ...
 ✓ Compiled / in 676ms (769 modules)&lt;/code&gt;
    &lt;p&gt;That's it. Are you fucking kidding me right now? What the fuck?&lt;/p&gt;
    &lt;p&gt;Now, you might be thinking that this is just not how &lt;code&gt;AsyncLocalStorage&lt;/code&gt; works. And you might be right. But I would like to point out that &lt;code&gt;headers()&lt;/code&gt; and &lt;code&gt;cookies()&lt;/code&gt; use &lt;code&gt;AsyncLocalStorage&lt;/code&gt;. This is a power that the Next.js devs have that we don't.&lt;/p&gt;
    &lt;p&gt;As far as I can tell there are only two ways to pass information from a middleware to a page.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Headers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;NextResponse.redirect&lt;/code&gt;/&lt;code&gt;NextResponse.rewrite&lt;/code&gt;to a route with extra params (eg. /[requestId]/page.tsx)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As you might have noticed, neither of these are very pleasant to use in this case.&lt;/p&gt;
    &lt;p&gt;You are being coddled. The Next.js devs have a vision and it's either their way or the highway. Note that if it was just the middleware, I wouldn't be sitting here, wasting away my weekend, ranting about a React framework. Believe it or not, I've got better things to do. It's constant pain you encounter daily when working with Next.js.&lt;/p&gt;
    &lt;head rend="h3"&gt;Vercel can do better&lt;/head&gt;
    &lt;p&gt;What's infuriating about this example is that Vercel can very much do better. I don't want to sing too many praises at Svelte(Kit) because I have some misgivings about its recent direction, but it's so much better than Next.js. Let's look at their middleware docs:&lt;/p&gt;
    &lt;quote&gt;handle - This function runs every time the SvelteKit server receives a request [...] This allows you to modify response headers or bodies, or bypass SvelteKit entirely (for implementing routes programmatically, for example).&lt;/quote&gt;
    &lt;p&gt;Looking good so far.&lt;/p&gt;
    &lt;quote&gt;locals - To add custom data to the request, which is passed to handlers in&lt;code&gt;+server.js&lt;/code&gt;and server&lt;code&gt;load&lt;/code&gt;functions, populate the&lt;code&gt;event.locals&lt;/code&gt;object, as shown below.&lt;/quote&gt;
    &lt;p&gt;I'm crying tears of joy right now. You can also stuff real objects/classes in there. Like a logger for instance.&lt;/p&gt;
    &lt;quote&gt;You can define multiple handle functions and execute them with sequence.&lt;/quote&gt;
    &lt;p&gt;This is what real engineering looks like. SvelteKit is a Vercel product. How is the flagship offering worse than what is essentially a side project. What the hell?&lt;/p&gt;
    &lt;head rend="h3"&gt;Scientists discover a new super massive black hole at https://github.com/vercel/next.js/issues&lt;/head&gt;
    &lt;p&gt;I don't have anything else to add, but while I'm here I feel like I have to talk about the GitHub issue tracker. This is perhaps the crown jewel of the dumpster fire that is Next.js. It's a place where hopes and issues come to die. The mean response time for a bug report is never. I've made it a sport to search the issue tracker/discussion for problems I'm currently facing and bet on how many years it takes to even get a response from a Next.js dev.&lt;/p&gt;
    &lt;p&gt;You think I'm joking? There are hundreds of issues with as many 👍 emojis with no official response for years. And when you finally get a response, it's to tell you that what you're doing is wrong and a solution to your real problems is on the way. Then they proceed to keep the "solution" in canary for years on end.&lt;/p&gt;
    &lt;p&gt;I personally reported two issues a year ago. Keep in mind that to have a valid bug report, you need a reproduction.&lt;/p&gt;
    &lt;p&gt;So, what do you get for taking the time to make a minimal reproduction? That's right. Complete silence.&lt;/p&gt;
    &lt;p&gt;I would have reported about a dozen other issues I have encountered, but after this experience I gave up.&lt;/p&gt;
    &lt;p&gt;Honestly, I don't even know if the issues are still valid.&lt;/p&gt;
    &lt;head rend="h3"&gt;Have we learned anything?&lt;/head&gt;
    &lt;p&gt;I don't know. For me, personally, I don't want to use Next.js anymore. You might think that this is just a singular issue and I'm overreacting. But there's bugs and edge cases around every corner. How did they manage to make TypeScript compile slower than Rust? Why make a distinction between code running on client and server and then not give me any tools to take advantage of that? Why? Why? Why?&lt;/p&gt;
    &lt;p&gt;I don't think I quite have enough pull to move us out of Next.js land. But, I think I will voice my opinion if we end up writing another app. We'll see if the grass is any greener on the other side.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45099922</guid></item><item><title>Kapa.ai (YC S23) is hiring research and software engineers</title><link>https://www.ycombinator.com/companies/kapa-ai/jobs</link><description>&lt;doc fingerprint="bdc48a3b7f7b6dd6"&gt;
  &lt;main&gt;
    &lt;p&gt;The fastest way to build AI assistants on technical content&lt;/p&gt;
    &lt;p&gt;We make it easy for technical companies to build AI assistants. Companies like Docker, Grafana and Mixpanel deploy kapa in the following ways:&lt;/p&gt;
    &lt;p&gt;We leverage companies existing technical knowledge sources including documentation, tutorials, forum posts, Slack channels, GitHub issues and many more to generate AI assistants that can handle complicated technical questions. More than 200 companies use kapa and we have answered more than 10 million questions to date.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45099939</guid></item><item><title>A motto for programmers: "Tuere usorem, data, veritatem"</title><link>https://koas.dev/a-motto-for-programming/</link><description>&lt;doc fingerprint="f343109fce07e7d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Yesterday, while walking down the street, I saw some words written in Latin on a terrace: “Primum non nocere”. I looked them up on Google and found that they mean “First, do no harm”. It’s a motto for health-related professions that emphasizes their main objective: the well-being of the patient and avoiding any harm.&lt;/p&gt;
    &lt;p&gt;I liked the idea, and after doing some research, I found that there’s nothing similar for programmers. So I started thinking and came up with a possible motto for our profession: “Tuere usorem, data, veritatem”.&lt;/p&gt;
    &lt;p&gt;Tuere: protect&lt;lb/&gt;usorem: the user&lt;lb/&gt;data: the data&lt;lb/&gt;veritatem: the truth&lt;/p&gt;
    &lt;p&gt;The user: as programmers, our main objective should be the user, making their experience as pleasant as possible and ensuring our program makes their life easier or more comfortable. In the case of open-source code, it can also help them learn.&lt;/p&gt;
    &lt;p&gt;The data: after the user, data is the most important thing because it’s almost always irreplaceable. It’s our responsibility to do everything possible to keep it safe and accessible.&lt;/p&gt;
    &lt;p&gt;The truth: today’s technology can be used to falsify information and spread lies, and the pace of change only makes this worse. As creators of much of this technology, we have a moral commitment to the truth.&lt;/p&gt;
    &lt;p&gt;What do you think? Does it make sense? Is there already a similar slogan that I haven’t found? I’d love to hear your thoughts!&lt;/p&gt;
    &lt;p&gt;PS: I don’t know any Latin, ChatGPT did the translation for me, so there may be some mistakes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45100163</guid></item></channel></rss>