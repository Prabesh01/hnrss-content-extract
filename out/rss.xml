<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 30 Oct 2025 15:11:36 +0000</lastBuildDate><item><title>Kafka is Fast ‚Äì I'll use Postgres</title><link>https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks</link><description>&lt;doc fingerprint="6f5090de6e0009c6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Intro&lt;/head&gt;
    &lt;p&gt;I feel like the tech world lives in two camps.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;One camp chases buzzwords.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This camp tends to adopt whatever‚Äôs popular without thinking hard about whether it‚Äôs appropriate. They tend to fall for all the purported benefits the sales pitch gives them - real-time, infinitely scale, cutting-edge, cloud-native, serverless, zero-trust, AI-powered, etc.&lt;/p&gt;
    &lt;p&gt;You see this everywhere in the Kafka world: Streaming Lakehouse‚Ñ¢Ô∏è, Kappa‚Ñ¢Ô∏è Architecture, Streaming AI Agents1.&lt;/p&gt;
    &lt;p&gt;This phenomenon is sometimes known as resume-driven design. Modern practices actively encourage this. Consultants push ‚Äúinnovative architectures‚Äù stuffed with vendor tech via ‚Äúinsight‚Äù reports2. System design interviews expect you to design Google-scale architectures that are inevitably at a scale 100x higher than the company you‚Äôre interviewing for would ever need. Career progression rewards you for replatforming to the Hot New Stack‚Ñ¢Ô∏è, not for being resourceful.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The other camp chases common sense&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This camp is far more pragmatic. They strip away unnecessary complexity and steer clear of overengineered solutions. They reason from first principles before making technology choices. They resist marketing hype and approach vendor claims with healthy skepticism.&lt;/p&gt;
    &lt;p&gt;Historically, it has felt like Camp 1 definitively held the upper hand in sheer numbers and noise. Today, it feels like the pendulum may be beginning to swing back, at least a tiny bit. Two recent trends are on the side of Camp 2:&lt;/p&gt;
    &lt;p&gt;Trend 1 - the ‚ÄúSmall Data‚Äù movement. People are realizing two things - their data isn‚Äôt that big and their computers are becoming big too. You can rent a 128-core, 4 TB of RAM instance from AWS. AMD just released 192-core CPUs this summer. That ought to be enough for anybody.3&lt;/p&gt;
    &lt;p&gt;Trend 2 - the Postgres Renaissance. The space is seeing incredible growth and investment4. In the last 2 years, the phrase ‚ÄúJust Use Postgres (for everything)‚Äù has gained a ton of popularity. The basic premise is that you shouldn‚Äôt complicate things with new tech when you don‚Äôt need to, and that Postgres alone solves most problems pretty well. Postgres competes with purpose-built solutions like:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Elasticsearch (functionality supported by Postgres‚Äô &lt;code&gt;tsvector&lt;/code&gt;/&lt;code&gt;tsquery&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;MongoDB (&lt;code&gt;jsonb&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Redis (&lt;code&gt;CREATE UNLOGGED TABLE&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;AI Vector Databases (&lt;code&gt;pgvector&lt;/code&gt;,&lt;code&gt;pgai&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Snowflake (&lt;code&gt;pg_mooncake&lt;/code&gt;,&lt;code&gt;pg_duckdb&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;and‚Ä¶ Kafka (this blog).&lt;/p&gt;
    &lt;p&gt;The claim isn‚Äôt that Postgres is functionally equivalent to any of these specialized systems. The claim is that it handles 80%+ of their use cases with 20% of the development effort. (Pareto Principle)&lt;/p&gt;
    &lt;p&gt;When you combine the two trends, the appeal becomes obvious. Postgres is a battle-tested, well-known system that is simple, scalable and reliable. Pair it with today‚Äôs powerful hardware and you quickly begin to realize that, more often than not, you do not need the state-of-the-art highly optimized and complex distributed system in order to handle your organization‚Äôs scale.&lt;/p&gt;
    &lt;p&gt;Despite being somebody who is biased towards Kafka, I tend to agree. Kafka is similar to Postgres in that it‚Äôs stable, mature, battle-tested and boasts a strong community. It also scales a lot further. Despite that, I don‚Äôt think it‚Äôs the right choice for a lot of cases. Very often I see it get adopted where it doesn‚Äôt make sense.&lt;/p&gt;
    &lt;p&gt;A 500 KB/s workload should not use Kafka. There is a scalability cargo cult in tech that always wants to choose ‚Äúthe best possible‚Äù tech for a problem - but this misses the forest for the trees. The ‚Äúbest possible‚Äù solution frequently isn‚Äôt a technical question - it‚Äôs a practical one. Adriano makes an airtight case for why you should opt for simple tech in his PG as Queue blog (2023) that originally inspired me to write this.&lt;/p&gt;
    &lt;p&gt;Enough background. In this article, we will do three simple things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Benchmark how far Postgres can scale for pub/sub messaging - # PG as a Pub/Sub&lt;/item&gt;
      &lt;item&gt;Benchmark how far Postgres can scale for queueing - # PG as a Queue&lt;/item&gt;
      &lt;item&gt;Concisely touch upon when Postgres can be a fit for these use cases - # Should You Use Postgres?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I am not aiming for an exhaustive in-depth evaluation. Benchmarks are messy af. Rather, my goal is to publish some reasonable data points which can start a discussion.&lt;/p&gt;
    &lt;p&gt;(while this article is for Postgres, feel free to replace it with your database of choice)&lt;/p&gt;
    &lt;head rend="h1"&gt;Results TL;DR&lt;/head&gt;
    &lt;p&gt;If you‚Äôd like to skip straight to the results, here they are:&lt;/p&gt;
    &lt;head&gt;üî• The Benchmark Results&lt;/head&gt;
    &lt;head rend="h3"&gt;Pub-Sub Results&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;‚úçÔ∏è Write&lt;/cell&gt;
        &lt;cell role="head"&gt;üìñ Read&lt;/cell&gt;
        &lt;cell role="head"&gt;üî≠ e2e Latency5 (p99)&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1√ó c7i.xlarge&lt;/cell&gt;
        &lt;cell&gt;4.8 MiB/s&lt;p&gt;5036 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;24.6 MiB/s&lt;p&gt;25 183 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;60 ms&lt;/cell&gt;
        &lt;cell&gt;~60 % CPU; 4 partitions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;3√ó c7i.xlarge (replicated)&lt;/cell&gt;
        &lt;cell&gt;4.9 MiB/s&lt;p&gt;5015 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;24.5 MiB/s&lt;p&gt;25 073 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;186 ms&lt;/cell&gt;
        &lt;cell&gt;~65 % CPU; cross-AZ RF‚âà2.5; 4 partitions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1√ó c7i.24xlarge&lt;/cell&gt;
        &lt;cell&gt;238 MiB/s&lt;p&gt;243,000 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1.16 GiB/s&lt;p&gt;1,200,000 msg/s (5x fanout)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;853 ms&lt;/cell&gt;
        &lt;cell&gt;~10 % CPU (idle); 30 partitions&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Queue Results&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Setup&lt;/cell&gt;
        &lt;cell role="head"&gt;üì¨ Throughput (read + write)&lt;/cell&gt;
        &lt;cell role="head"&gt;üî≠ e2e Latency5 (p99)&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;1√ó c7i.xlarge&lt;/cell&gt;
        &lt;cell&gt;2.81 MiB/s&lt;p&gt;2885 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;17.7 ms&lt;/cell&gt;
        &lt;cell&gt;~60 % CPU; read-client bottleneck&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;3√ó c7i.xlarge (replicated)&lt;/cell&gt;
        &lt;cell&gt;2.34 MiB/s&lt;p&gt;2397 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;920 ms ‚ö†Ô∏è6&lt;/cell&gt;
        &lt;cell&gt;replication lag inflated E2E latency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;1√ó c7i.24xlarge&lt;/cell&gt;
        &lt;cell&gt;19.7 MiB/s&lt;p&gt;20,144 msg/s&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;930 ms ‚ö†Ô∏è6&lt;/cell&gt;
        &lt;cell&gt;~50 % CPU; single-table bottleneck&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Make sure to at least read the last section of the article where we philosophize - # Should You Use Postgres?&lt;/p&gt;
    &lt;head rend="h1"&gt;PG as a Pub/Sub&lt;/head&gt;
    &lt;p&gt;There are dozens of blogs out there using Postgres as a queue, but interestingly enough I haven‚Äôt seen one use it as a pub-sub messaging system.&lt;/p&gt;
    &lt;p&gt;A quick distinction between the two because I often see them get confused:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Queues are meant for point-to-point communication. They‚Äôre widely used for asynchronous background jobs: worker apps (clients) process a task in the queue like sending an e-mail or pushing a notification. The event is consumed once and it‚Äôs done with. A message is immediately deleted (popped) off the queue once it‚Äôs consumed. Queues do not have strict ordering guarantees7.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pub-sub messaging differs from the queue in that it is meant for one-to-many communication. This inherently means there is a large read fanout - more than one reader client is interested in any given message. Good pub-sub systems decouple readers from writers by storing data on disks. This allows them to not impose a max queue depth limit - something in-memory queues need to do in order to prevent them from going OOM.&lt;/p&gt;
        &lt;p&gt;There is also a general expectation that there is strict order - events should be read in the same order that they arrived in the system.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Postgres‚Äô main competitor here is Kafka, which is the standard in pub-sub today. Various (mostly-proprietary) alternatives exist.8&lt;/p&gt;
    &lt;p&gt;Kafka uses the Log data structure to hold messages. You‚Äôll see my benchmark basically reconstructs a log from Postgres primitives.&lt;/p&gt;
    &lt;p&gt;Postgres doesn‚Äôt seem to have any popular libraries for pub-sub9 use cases, so I had to write my own. The Kafka-inspired workflow I opted for is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Writers produce batches of messages per statement10 (&lt;code&gt;INSERT INTO&lt;/code&gt;). Each transaction carries one batch insert and targets a single&lt;code&gt;topicpartition&lt;/code&gt;table11&lt;/item&gt;
      &lt;item&gt;Each writer is sticky to one table, but in aggregate they produce to multiple tables.&lt;/item&gt;
      &lt;item&gt;Each message has a unique monotonically-increasing offset number. A specific row in a special &lt;code&gt;log_counter&lt;/code&gt;table denotes the latest offset for a given&lt;code&gt;topicpartition&lt;/code&gt;table.&lt;/item&gt;
      &lt;item&gt;Write transactions atomically update both the &lt;code&gt;topicpartition&lt;/code&gt;data and the&lt;code&gt;log_counter&lt;/code&gt;row. This ensures consistent offset tracking across concurrent writers.&lt;/item&gt;
      &lt;item&gt;Readers poll for new messages. They consume the &lt;code&gt;topicpartition&lt;/code&gt;table(s) sequentially, starting from the lowest offset and progressively reading up.&lt;/item&gt;
      &lt;item&gt;Readers are split into consumer groups. Each group performs separate, independent reads and makes progress on the &lt;code&gt;topicpartition&lt;/code&gt;tables.&lt;/item&gt;
      &lt;item&gt;Each group contains 1 reader per &lt;code&gt;topicpartition&lt;/code&gt;table.&lt;/item&gt;
      &lt;item&gt;Readers store their progress in a &lt;code&gt;consumer_offsets&lt;/code&gt;table, with a row for each&lt;code&gt;topicpartition,group&lt;/code&gt;pair.&lt;/item&gt;
      &lt;item&gt;Each reader updates the latest processed offset (claiming the records), selects the records and processes them inside a single transaction.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This ensures Kafka-like semantics - gapless, monotonically-increasing offsets and at-least-once/at-most-once processing. This test in particular uses at-least-once semantics, but neither choice should impact the benchmark results.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pub-Sub Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;Table&lt;/head&gt;
    &lt;head rend="h4"&gt;Writes&lt;/head&gt;
    &lt;p&gt;The benchmark runs &lt;code&gt;N&lt;/code&gt; writer goroutines. These represent writer clients.
Each one loops and atomically inserts &lt;code&gt;$BATCH_SIZE&lt;/code&gt; records while updating the latest offset:&lt;/p&gt;
    &lt;head rend="h4"&gt;Reads&lt;/head&gt;
    &lt;p&gt;The benchmark also runs &lt;code&gt;N&lt;/code&gt; reader goroutines. Each reader is assigned a particular consumer group and partition. The group as a whole reads all partitions while each reader in the group reads only one partition at a time.&lt;/p&gt;
    &lt;p&gt;The reader loops, opens a transaction, optimistically claims &lt;code&gt;$BATCH_SIZE&lt;/code&gt; records (by advancing the offset mark beyond them), selects them and processes the records.
If successful, it commits the transaction and through that advances the offset for the group.&lt;/p&gt;
    &lt;p&gt;It is a pull-based read (just like Kafka), rather than push-based. If the reader has no records to poll, it sleeps for a bit.&lt;/p&gt;
    &lt;p&gt;First it opens a transaction:&lt;/p&gt;
    &lt;p&gt;Then it claims the offsets:&lt;/p&gt;
    &lt;p&gt;Followed by selecting the claimed records:&lt;/p&gt;
    &lt;p&gt;Finally, the data gets processed by the business logic (no-op in our benchmark) and the transaction is closed:&lt;/p&gt;
    &lt;p&gt;If you‚Äôre wondering ‚Äúwhy no &lt;code&gt;NOTIFY/LISTEN&lt;/code&gt;?‚Äù - my understanding of that feature is that it‚Äôs an optimization and cannot be fully relied upon, so polling is required either way12. Given that, I just copied Kafka‚Äôs relatively simple design.&lt;/p&gt;
    &lt;head rend="h2"&gt;Pub-Sub Results&lt;/head&gt;
    &lt;p&gt;The full code and detailed results are all published on GitHub at stanislavkozlovski/pg-queue-pubsub-benchmark. I ran three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The results are the average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;mostly default Postgres settings (synchronous commit, fsync); &lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;4 topicpartition tables&lt;/item&gt;
      &lt;item&gt;10 writers (2 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;20 reader clients total (4 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 100 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 5036 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 4.8 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 38.7ms p99 / 6.2ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 25,183 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 24.6 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 27.3ms p99 (varied 8.9ms-47ms b/w runs); 4.67ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 60ms p99 / 10.6ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~60% CPU;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;disk was at ~1200 writes/s with iostat claiming 46 MiB/s&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are pretty good results. It‚Äôs funny to think that the majority of people run a complex distributed system like Kafka for similar workloads13.&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Tri-Node&lt;/head&gt;
    &lt;p&gt;Now, a replicated setup to more accurately mimic the durability and availability guarantees of Kafka.&lt;/p&gt;
    &lt;p&gt;The average of two 5-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume &lt;list rend="ul"&gt;&lt;item&gt;each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)&lt;/item&gt;&lt;item&gt;one &lt;code&gt;sync&lt;/code&gt;replica and one&lt;code&gt;potential&lt;/code&gt;14 replica&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a few custom Postgres settings like &lt;code&gt;wal_compression&lt;/code&gt;,&lt;code&gt;max_worker_processes&lt;/code&gt;,&lt;code&gt;max_parallel_workers&lt;/code&gt;,&lt;code&gt;max_parallel_workers_per_gather&lt;/code&gt;and of course -&lt;code&gt;hot_standby&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;4 topicpartition tables&lt;/item&gt;
      &lt;item&gt;10 writers (2 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;readers only access the primary DB15; readers are in the same AZ as the primary;&lt;/item&gt;
      &lt;item&gt;20 reader clients total (4 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 100 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 5015 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 4.9 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 153.45ms p99 / 6.8ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 25,073 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 24.5 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 57ms p99; 4.91ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 186ms p99 / 12ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~65% CPU;&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;disk was at ~1200 writes/s with iostat claiming 46 MiB/s&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now these are astonishing results! Throughput was not impacted at all. Latency increased but not extremely. Our p99 e2e latency 3x‚Äôd (60ms vs 185ms), but the p95 barely moved from 10.6ms to 12ms.&lt;/p&gt;
    &lt;p&gt;This shows that a simple 3-node Postgres cluster can pretty easily sustain what is a very common Kafka workload - 5 MB/s ingest and 25 MB/s egress. Not only that, but for a cheap cost too. Just $11,514 per year.16&lt;/p&gt;
    &lt;p&gt;Typically, you‚Äôd expect Postgres to run more expensive than Kafka at a certain scale, simply because it wasn‚Äôt designed to be efficient for this use case. Not here though. Running Kafka yourself would cost the same. Running the same workload through a Kafka vendor will cost you at least $50,000 a year. ü§Ø&lt;/p&gt;
    &lt;p&gt;By the way, in Kafka it‚Äôs customary to apply client-side compression on your data. If we assume your messages were 5 KB in size and your clients applied a pretty regular compression ratio of 4x17 - Postgres is actually handling 20 MB/s ingress and 100 MB/s egress.&lt;/p&gt;
    &lt;head rend="h3"&gt;96 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;Ok, let‚Äôs see how far Postgres will go.&lt;/p&gt;
    &lt;p&gt;The results are the average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.24xlarge (96 vCPU, 192 GiB RAM) Postgres server instance /w 250GB io2 12,000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;modified Postgres settings (&lt;code&gt;huge_pages&lt;/code&gt;on, other settings scaled to match the machine);&lt;list rend="ul"&gt;&lt;item&gt;still kept fsync &amp;amp; synchronous_commit on for durability.&lt;/item&gt;&lt;item&gt;&lt;code&gt;autovacuum_analyze_scale_factor = 0.05&lt;/code&gt;set on the partition tables too (unclear if it has an effect)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;30 topicpartition tables&lt;/item&gt;
      &lt;item&gt;100 writers (~3.33 writers per partition on average)&lt;/item&gt;
      &lt;item&gt;5x read fanout via 5 consumer groups&lt;/item&gt;
      &lt;item&gt;150 reader clients total (5 readers per group)&lt;/item&gt;
      &lt;item&gt;write batch size: 200 records&lt;/item&gt;
      &lt;item&gt;read batch size: 200 records&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;write message rate: 243,000 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write throughput: 238 MiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;write latency: 138ms p99 / 47ms p95&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message rate: 1,200,000 msg/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read message throughput: 1.16 GiB/s&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;read latency: 24.6ms p99&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;end-to-end latency5: 853ms p99 / 242ms p95 / 23.4ms p50&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;server kept at ~10% CPU (basically idle);&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;bottleneck: The bottleneck was the write rate per partition. It seems like the test wasn‚Äôt able to write at a higher rate than 8 MiB/s (8k msg/s) per table with this design. I didn‚Äôt push further, but I do wonder now as I write this - how far would writes have scaled?&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Reads were trivial to scale. Adding more consumer groups was trivial - I tried with 10x fanout and still ran at low CPU. I didn‚Äôt include it because I didn‚Äôt feel the need to push to an unrealistic read-fanout extreme.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;240 MiB/s ingress and 1.16 GiB/s egress are pretty good! The 96 vCPU machine was overkill for this test - it could have done a lot more, or we could have simply opted for a smaller machine. For what it‚Äôs worth, I do think it‚Äôs worth it to deploy a separate Kafka cluster at this scale. Kafka can save you a lot of money here because it can be more efficient in how it handles cross-zone network traffic with features like Diskless Kafka.&lt;/p&gt;
    &lt;head rend="h3"&gt;Pub-Sub Test Summary&lt;/head&gt;
    &lt;p&gt;The summarized table with the three test results can be found here ‚Üí üëâ stanislavkozlovski/pg-queue-pubsub-benchmark&lt;/p&gt;
    &lt;p&gt;These tests seem to show that Postgres is pretty competitive with Kafka at low scale.&lt;/p&gt;
    &lt;p&gt;You may have noticed none of these tests were particularly long-running. From my understanding, the value in longer-running tests is to test table vacuuming in Postgres, as that can have negative performance effects. In the pub-sub section, vacuuming doesn‚Äôt apply because the tables are append-only. My other reasoning for running shorter tests was to keep costs in check and not spend too much time18.&lt;/p&gt;
    &lt;p&gt;In any case, no benchmark is perfect. My goal wasn‚Äôt to indisputably prove &lt;code&gt;$MY_CLAIM&lt;/code&gt;. Rather, I want to start a discussion by showing that what‚Äôs possible is likely larger than what most people assume. I certainly didn‚Äôt assume I‚Äôd get such good numbers, especially with the pub-sub part.&lt;/p&gt;
    &lt;head rend="h1"&gt;PG as a Queue&lt;/head&gt;
    &lt;p&gt;In Postgres, a queue can be implemented with &lt;code&gt;SELECT FOR UPDATE SKIP LOCKED&lt;/code&gt;. This command selects an unlocked row and locks it. It also skips reading already-locked rows. That‚Äôs how mutual exclusion is achieved - a worker can‚Äôt get other workers‚Äô jobs.&lt;/p&gt;
    &lt;p&gt;Postgres has a very popular pgmq library that offers a slick queue API. To keep it simple and understand the end-to-end flow better, I decided to write my own queue. The basic version of it is very easy. My workflow is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;add job (&lt;code&gt;INSERT&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;lock row &amp;amp; take job (&lt;code&gt;SELECT FOR UPDATE SKIP LOCKED&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;process job (&lt;code&gt;{your business logic}&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;mark job as ‚Äúdone‚Äù (&lt;code&gt;UPDATE&lt;/code&gt;a field or&lt;code&gt;DELETE &amp;amp; INSERT&lt;/code&gt;the row into a separate table)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Postgres competes with RabbitMQ, AWS SQS, NATS, Redis19 and to an extent Kafka20 here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Queue Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;Table&lt;/head&gt;
    &lt;p&gt;We use a simple &lt;code&gt;queue&lt;/code&gt; table. When an element is processed off the queue, it‚Äôs moved into the archive table.&lt;/p&gt;
    &lt;head rend="h4"&gt;Writes&lt;/head&gt;
    &lt;p&gt;We again run &lt;code&gt;N&lt;/code&gt; writer client goroutines.
Each one simply loops and sequentially inserts a single random item into the table:&lt;/p&gt;
    &lt;p&gt;It only inserts one message per statement, which is pretty inefficient at scale.&lt;/p&gt;
    &lt;head rend="h4"&gt;Reads&lt;/head&gt;
    &lt;p&gt;We again run &lt;code&gt;M&lt;/code&gt; reader client goroutines. Each reader loops and processes one message.
The processing is done inside a database transaction.&lt;/p&gt;
    &lt;p&gt;Each reader again only works with one message at a time per transaction.&lt;/p&gt;
    &lt;head rend="h2"&gt;Queue Results&lt;/head&gt;
    &lt;p&gt;I again ran the same three setups - a single-node 4 vCPU, a 3-node replicated 4 vCPU and a single-node 96 vCPU setup. Here are the summarized results for each:&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The results are the average of two 15-minute tests. I also ran three 2-minute tests. They all performed similarly. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.xlarge Postgres server /w 25GB gp3 9000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;all default Postgres settings21&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;10 writer clients, 15 reader clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 2885 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 2.81 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 2.46ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 4.2ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency5: 17.72ms p99&lt;/item&gt;
      &lt;item&gt;server kept at ~60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What I found Postgres wasn‚Äôt good at was handling client count. The bottleneck in this setup was the read clients. Each client could not read more than ~192 messages a second because of its median read latency and sequential read nature.&lt;/p&gt;
    &lt;p&gt;Increasing client count boosted throughput but violated my ~60% CPU target. Trying to run 50 write and 50 read clients got to 4000 msg/s without increasing the queue depth but pegged the server‚Äôs CPU to 100%. I wanted to keep the benchmarks realistic for what you may run in production, rather than maxing out what a machine can do. This would be easily alleviated with a connection pooler (standard across all prod PG deployments) or a larger machine.&lt;/p&gt;
    &lt;p&gt;Another thing worth mentioning is that the workload could sustain a lot more writes than reads. If I didn‚Äôt throttle the benchmark, it would write at 12,000 msg/s and read at 2,800 msg/s. In the spirit of simplicity, I didn‚Äôt debug further and instead throttled my writes to see at what point I could get a stable 1:1 workload.&lt;/p&gt;
    &lt;head rend="h3"&gt;4 vCPU Tri-Node&lt;/head&gt;
    &lt;p&gt;A single 10-minute test. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;3x c7i.xlarge Postgres servers /w 25GB gp3 9000 IOPS EBS volume &lt;list rend="ul"&gt;&lt;item&gt;each on a separate AZ (us-east-1a, us-east-1b, us-east-1c)&lt;/item&gt;&lt;item&gt;one &lt;code&gt;sync&lt;/code&gt;replica and one&lt;code&gt;potential&lt;/code&gt;14 replica&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;a few custom Postgres settings like &lt;code&gt;wal_compression&lt;/code&gt;,&lt;code&gt;max_worker_processes&lt;/code&gt;,&lt;code&gt;max_parallel_workers&lt;/code&gt;,&lt;code&gt;max_parallel_workers_per_gather&lt;/code&gt;and of course -&lt;code&gt;hot_standby&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;10 writer clients, 15 reader clients&lt;/item&gt;
      &lt;item&gt;readers only access the primary DB15; readers are in the same AZ as the primary;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 2397 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 2.34 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 3.3ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 7.6ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency5: 920ms p99 ‚ö†Ô∏è6; 536ms p95; 7ms p50&lt;/item&gt;
      &lt;item&gt;server kept at ~60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As expected, throughput and latency were impacted somewhat. But not that much. It‚Äôs still over 2000 messages a second, which is pretty good for an HA queue!&lt;/p&gt;
    &lt;head rend="h3"&gt;96 vCPU Single Node&lt;/head&gt;
    &lt;p&gt;The average of three 2-minute tests. [full results link]&lt;/p&gt;
    &lt;p&gt;Setup:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;c7i.24xlarge Postgres server instance /w 250GB io2 12,000 IOPS EBS volume&lt;/item&gt;
      &lt;item&gt;modified Postgres settings (&lt;code&gt;huge_pages&lt;/code&gt;on, other settings scaled to match the machine);&lt;list rend="ul"&gt;&lt;item&gt;still kept fsync &amp;amp; synchronous_commit on for durability.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;each row‚Äôs payload is 1 KiB (1024 bytes)&lt;/item&gt;
      &lt;item&gt;100 writer clients, 200 reader clients&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;message rate: 20,144 msg/s&lt;/item&gt;
      &lt;item&gt;throughput: 19.67 MiB/s&lt;/item&gt;
      &lt;item&gt;write latency: 9.42ms p99&lt;/item&gt;
      &lt;item&gt;read latency: 22.6ms p99&lt;/item&gt;
      &lt;item&gt;end-to-end latency: 930ms p99 ‚ö†Ô∏è6; 709ms p95; 12.6ms p50&lt;/item&gt;
      &lt;item&gt;server at 40-60% CPU;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This run wasn‚Äôt that impressive. There is some bottleneck in the single-table queue approach at this scale which I didn‚Äôt bother figuring out. I figured that it wasn‚Äôt important to reach absurd numbers on a single table, since all realistic scenarios would have multiple queues and never reach 20,000 msg/s on a single one. The 96 vCPU instance would likely scale far further were we to run a few separate queue tables in parallel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Queue Test Summary&lt;/head&gt;
    &lt;p&gt;The summarized table with the three test results can be found here ‚Üí üëâ stanislavkozlovski/pg-queue-pubsub-benchmark&lt;/p&gt;
    &lt;p&gt;Even a modest Postgres node can durably push thousands of queue ops/sec, which already covers the scale 99% of companies ever hit with a single queue. As I said earlier, the last 2 years have seen the Just Use Postgres slogan become mainstream. The &lt;code&gt;pgmq&lt;/code&gt; library‚Äôs star history captures this trend perfectly:
&lt;/p&gt;
    &lt;head rend="h1"&gt;Should You Use Postgres?&lt;/head&gt;
    &lt;p&gt;Most of the time - yes. You should always default to Postgres until the constraints prove you wrong.&lt;/p&gt;
    &lt;p&gt;Kafka is obviously better optimized for pub-sub workloads. Queue systems are obviously better optimized for queue workloads. The point is that picking a technology based on technical optimization alone is a flawed approach. To throw an analogy:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;a Formula One car is optimized to drive faster, but I still use a sedan to go to work. I am way more comfortable driving my sedan than an F1 car.&lt;/p&gt;
      &lt;p&gt;(seriously, see the steering wheel on these things)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The Postgres sedan comes with many quality-of-life comforts that the F1 Kafka does not:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ability to debug messages with regular SQL&lt;/item&gt;
      &lt;item&gt;ability to delete, re-order or edit messages in place&lt;/item&gt;
      &lt;item&gt;ability to join pub-sub data with regular tables&lt;/item&gt;
      &lt;item&gt;ability to trivially read specific data via rich SQL queries (&lt;code&gt;ID=54&lt;/code&gt;,&lt;code&gt;name="John"&lt;/code&gt;,&lt;code&gt;cost&amp;gt;1000&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Giving up these comforts is a justified sacrifice for your F1 car to go at 378 kmh (235 mph), but masochistic if you plan on driving at 25kmh (15 mph).&lt;/p&gt;
    &lt;p&gt;Donald Knuth warned us in 1974 - premature optimization is the root of all evil. Deploying Kafka at small scale is premature optimization. The point of this article is to show you that this ‚Äúsmall scale‚Äù number has grown further than what people remember it to be - it can comfortably mean many megabytes per second.&lt;/p&gt;
    &lt;p&gt;We are in a Postgres Renaissance for a reason: Postgres is frequently good enough. Modern NVMEs and cheap RAM allow it to scale absurdly high.&lt;/p&gt;
    &lt;p&gt;What‚Äôs the alternative?&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom Solutions for Everything?&lt;/head&gt;
    &lt;p&gt;Naive engineers tend to adopt a specialized technology at the slightest hint of a need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Need a cache? Redis, of course!&lt;/item&gt;
      &lt;item&gt;Search? Let‚Äôs deploy Elasticsearch!&lt;/item&gt;
      &lt;item&gt;Offline data analysis? BigQuery or Snowflake - that‚Äôs what our data analysts used at their last job.&lt;/item&gt;
      &lt;item&gt;No schemas? We need a NoSQL database like MongoDB.&lt;/item&gt;
      &lt;item&gt;Have to crunch some numbers on S3? Let‚Äôs use Spark!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A good engineer thinks through the bigger picture.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Does this new technology move the needle?&lt;/item&gt;
      &lt;item&gt;Is shaving a few milliseconds off our query worth the extra organizational complexity introduced with the change?&lt;/item&gt;
      &lt;item&gt;Will our users notice?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At small scale, these systems hurt you more than they benefit you. Distributed systems - both in terms of node count and system cardinality - should be respected, feared, avoided and employed only as a weapon of last resort against particularly gnarly problems. Everything with a distributed system becomes more challenging and time-consuming.&lt;/p&gt;
    &lt;p&gt;The problem is the organizational overhead. The organizational overhead of adopting a new system, learning its nuances, configs, establishing monitoring, establishing processes around deployments and upgrades, attaining operational expertise on how to manage it, creating runbooks, testing it, debugging it, adopting its clients and API, using its UI, keeping up with its ecosystem, etc.&lt;/p&gt;
    &lt;p&gt;All of these are real organizational costs that can take months to get right, even if the system in question isn‚Äôt difficult (a lot are). Managed SaaS offerings trade off some of the organizational overhead for greater financial costs - but they still don‚Äôt remove it all. And until you reach the scale where the technology is necessary, you pay these extra {financial, organizational} costs for zero significant gain.&lt;/p&gt;
    &lt;p&gt;If the same can be done with tech for which you‚Äôve already paid the organizational costs for (e.g Postgres), adopting something else prematurely is most definitely an anti-pattern. You don‚Äôt need web-scale technologies when you don‚Äôt have web-scale problems.&lt;/p&gt;
    &lt;head rend="h2"&gt;MVI (a better alternative)&lt;/head&gt;
    &lt;p&gt;What I think is a better approach is to search for the minimum viable infrastructure (MVI): build the smallest amount of system while still providing value.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;choose good-enough technology your org is already familiar with &lt;list rend="ul"&gt;&lt;item&gt;good-enough == meets your users‚Äô needs without being too slow/expensive/insecure&lt;/item&gt;&lt;item&gt;familiar == your org has prior experience, has runbooks/ops setups, monitoring, UI, etc&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;solve a real problem with it&lt;/item&gt;
      &lt;item&gt;use the minimum set of features &lt;list rend="ul"&gt;&lt;item&gt;the fewer features you use, the more flexibility you have to move off the infra in question in the future (e.g if locked in with a vendor)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bonus points if that technology:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;is widely adopted so finding good engineers for it is trivial (Postgres - check)&lt;/item&gt;
      &lt;item&gt;has a strong and growing network effect (Postgres - check)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The MVI approach reduces the surface area of your infra. The fewer moving parts you have, the fewer failure modes you worry about and the less glue code you have to maintain.&lt;/p&gt;
    &lt;p&gt;Unfortunately, it‚Äôs human nature to go against this. Just like startups suffer due to MVP bloat (one more feature!), infra teams suffer due to MVI bloat (one more system!)&lt;/p&gt;
    &lt;head rend="h2"&gt;Why are we like this?&lt;/head&gt;
    &lt;p&gt;I won‚Äôt pretend to be able to map out the exact path-dependent outcome, but my guess is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;the zero interest rate era gave us abundant speculative money that was invested in any company that could grow fast&lt;/item&gt;
      &lt;item&gt;a lot of viral internet companies were growing at speeds that led old infra to become obsolete fast&lt;/item&gt;
      &lt;item&gt;this prompted the next wave of ZIRP investment - specialized data infrastructure companies (in a gold rush, sell shovels!); some of these data infra startups spun off directly from the high-growth companies themselves&lt;/item&gt;
      &lt;item&gt;each well-funded data infra vendor is financially motivated to evangelize their product and have you adopt it even when you don‚Äôt need to (Everyone is Talking Their Book). They had deep pockets for marketing and used them.&lt;/item&gt;
      &lt;item&gt;innovative infrastructure software got engineered. It was exciting - so engineers got nerd-sniped into it&lt;/item&gt;
      &lt;item&gt;a web-scale craze/cargo cult developed, where everybody believed they need to be able to scale from zero to millions of RPS because they may go viral any day.&lt;/item&gt;
      &lt;item&gt;a trend developed to copy whatever solutions the most successful, largest digital-native companies were using (Amazon, Google, Uber, etc.)&lt;/item&gt;
      &lt;item&gt;the trend became a self-perpetuating prophecy: these technologies became a sought-after skill on resumes &lt;list rend="ul"&gt;&lt;item&gt;system design interview questions were adapted to test for knowledge of these systems&lt;/item&gt;&lt;item&gt;within an organization, engineers (knowingly or not) pushed for projects that are exciting and helped build their resumes;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This trend continues to grow while there is no strong competing force that is sufficiently motivated to push the opposite view. Even engineers inside a company, who ought to be motivated to keep things simple, have strong incentives to pursue extra complexity. It benefits their career by giving them a project to use as ammo for their next promotion and improves their resume (cool tech/story on there) for their next job-hop. Plus it‚Äôs simply more fun.&lt;/p&gt;
    &lt;p&gt;This is why I think we, as an industry, don‚Äôt always use the simplest solution available.&lt;/p&gt;
    &lt;p&gt;In most cases, Postgres is that simplest solution that is available.&lt;/p&gt;
    &lt;head rend="h2"&gt;But It Won‚Äôt Scale!&lt;/head&gt;
    &lt;p&gt;I want to wrap this article up, but one rebuttal I can‚Äôt miss addressing is the ‚Äúit won‚Äôt scale argument‚Äù.&lt;/p&gt;
    &lt;p&gt;The argument goes something like this: ‚Äúin today‚Äôs age we can go viral at a moment‚Äôs notice; these viral moments are very valuable for our business so we need to aggressively design in a way that keeps our app stable under traffic spikes‚Äù&lt;/p&gt;
    &lt;p&gt;I have three arguments against this:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Postgres Scales&lt;/head&gt;
    &lt;p&gt;As of 2025, OpenAI still uses an unsharded Postgres architecture with only one primary instance for writes22. OpenAI is the poster-child of rapid viral growth. They hold the record for the fastest startup to reach 100 million users.&lt;/p&gt;
    &lt;p&gt;Bohan Zhang, a member of OpenAI‚Äôs infrastructure team and co-founder of OtterTune (a Postgres tuning service), can be quoted as saying23:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúAt OpenAI, we utilize an unsharded architecture with one writer and multiple readers, demonstrating that PostgreSQL can scale gracefully under massive read loads.‚Äù&lt;/p&gt;
      &lt;p&gt;‚ÄúThe main message of my talk was that if you are not too write heavy, you can scale Postgres to a very high read throughput with read replicas using only a single master! That is exactly the message that needs to be spelled out as that covers the vast majority of apps.‚Äù&lt;/p&gt;
      &lt;p&gt;‚ÄúPostgres is probably the default choice for developers right now. You can use Postgres for a very long time. If you are building a startup with read-heavy workloads, just start with Postgres. If you hit a scalability issue, increase the instance size. You can scale it to a very large scale. If in the future the database becomes a bottleneck, congratulations. You have built a successful startup. It‚Äôs a good problem to have.‚Äù&lt;/p&gt;
      &lt;p&gt;(slightly edited for clarity and grammar)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Despite their rapid growth to a user base of more than 800 million, OpenAI has still NOT opted for a web-scale distributed database. If they haven‚Äôt‚Ä¶ why does your unproven project need to?&lt;/p&gt;
    &lt;head rend="h3"&gt;2. You Have More Time To Scale Than You Think&lt;/head&gt;
    &lt;p&gt;Let‚Äôs say it‚Äôs a good principle to design/test for ~10x your scale. Here are the years of consistent growth rate it takes to get to 10x your current scale:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;annual growth&lt;/cell&gt;
        &lt;cell role="head"&gt;years to hit 10√ó scale&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;10 %&lt;/cell&gt;
        &lt;cell&gt;24.16 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;25 %&lt;/cell&gt;
        &lt;cell&gt;10.32 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;50 %&lt;/cell&gt;
        &lt;cell&gt;5.68 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;75 %&lt;/cell&gt;
        &lt;cell&gt;4.11 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;100 %&lt;/cell&gt;
        &lt;cell&gt;3.32 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;150 %&lt;/cell&gt;
        &lt;cell&gt;2.51 y&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;200 %&lt;/cell&gt;
        &lt;cell&gt;2.10 y&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;It goes to show that even at extreme growth levels, you still have years to migrate between solutions. The majority of developers, though, work at companies in the 0-50% growth rate. They are more likely to have moved on to another job by the time the solution needs to change (if ever).&lt;/p&gt;
    &lt;head rend="h3"&gt;3. It‚Äôs Overdesign&lt;/head&gt;
    &lt;p&gt;In an ideal world, you would build for scale and any other future problem you may hit in 10 years.&lt;/p&gt;
    &lt;p&gt;In the real world, you have finite bandwidth, so you have to build for the most immediate, highest ROI problem.&lt;/p&gt;
    &lt;p&gt;Commenter snej on lobste.rs captured it well:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Planning your infrastructure around being able to handle that is sort of like buying a huge Marshall stack as your first guitar amp because your garage band might get invited to open for Coldplay.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Just use Postgres until it breaks.&lt;/p&gt;
    &lt;head rend="h3"&gt;Disclaimers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Title inspiration comes from a great recent piece - ‚ÄúRedis is fast - I‚Äôll cache in Postgres‚Äù&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I‚Äôm a complete Postgres noob. You may see a lot of dumb mistakes here. Feel free to call me out on them - I‚Äôm happy to learn. I used AI to help a lot with some of the PG tools to use. This both shows how inexperienced I am in the context and how easy it is to start. I am generally skeptical of AI‚Äôs promise (in the short-term), but there‚Äôs no denying it has made a large dent in democratizing niche/low-level knowledge.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you‚Äôd like to reach out to me, you can find me on LinkedIn or X (Twitter).&lt;/p&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Don‚Äôt worry if you don‚Äôt fully understand these terms. I work full-time in the industry that spews these things and I don‚Äôt have a great grasp either. It‚Äôs marketing slop. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Gartner and others push embarrassing recommendations that aren‚Äôt tech driven. It‚Äôs frequently the opposite - they‚Äôre profit driven. Gartner makes $6.72B purely off a consulting service that charges organizations $50k per seat solely for access to reports that recommend these slop architectures. It‚Äôs not crazy to believe, hence many people are converging with the idea that it is a pay-to-win racket model. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Seriously, the improvement in hardware is something I find most senior engineers haven‚Äôt properly appreciated. Newest gen AMD CPUs boast 192 cores. Modern SSDs can do 5.5 million random reads a second, or ~28GB/s sequential reads. Both are a 10-20x improvement over the last 10 years alone. Single nodes are more powerful than ever. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Just in the last 6 months - Snowflake acquired Crunchy Data for ~$250M, Databricks acquired Neon for ~$1 billion; In the last 12 months, Supabase more than 5x‚Äôd its valuation from ($900M to $5B), raising $380M across three series (!!!). Within a single year! ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;End-to-end latency here is defined as&lt;/p&gt;&lt;code&gt;now() - event_create_time&lt;/code&gt;; In essence, it tracks how long a brand new persisted event takes to get consumed. It helps show cases where queue lag spikes like when consumers temporarily fall behind the write rate. ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4 ‚Ü©5 ‚Ü©6 ‚Ü©7&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some queue tests showed higher E2E latencies which I believe was due to a bug. In the pub-sub tests, I ensured readers start before the writers via a 1000ms sleep. For the queue tests, though, I didn‚Äôt do this. The result is that queue tests immediately spike queue depth at startup because the writers manage to get a head start before the readers. I believe the E2E latency is artificially high because of this flaw in the test. ‚Ü© ‚Ü©2 ‚Ü©3 ‚Ü©4&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Actually, things are ordered in the happy path. Only during retries can you get out of order processing. e.g at t=0, client A reads task N; At t=1, client B reads task N+1 and processes it successfully; At t=2, A fails and is unable to process task N; At t=3, client B takes the next available task - which is N. B therefore executes the tasks in order [N+1, N], whereas proper order would have been [N, N+1] ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open-source projects include Apache Pulsar (open source), RedPanda (source-available), AutoMQ (a fork of Kafka) and a lot of proprietary offerings - AWS Kinesis, Google Pub/Sub, Azure Event Hubs, Confluent Kora, Confluent WarpStream, Bufstream to name a few. What‚Äôs common in 90% of these projects is that they all implement the Apache Kafka API, making Kafka undoubtedly the protocol standard in the space. There‚Äôs also an open-source project which exposes a Kafka API on top of a pluggable Postgres or S3 backend - Tansu (Rust, btw :] ) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The most popular library I could find is pg-pubsub with 106 stars as of writing (Oct 2025). Its last commit was 3 months ago. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Batching messages per client is very important for scalability here. It is one of Kafka‚Äôs least-talked-about performance ‚Äútricks‚Äù. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;These tables act as different log data structures. You can see them as separate topics, or partitions of one topic (shards). ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Postgres stores all&lt;/p&gt;&lt;code&gt;NOTIFY&lt;/code&gt;events in a single, global queue. If this queue becomes full, transactions calling&lt;code&gt;NOTIFY&lt;/code&gt;will fail when committing. (src) ‚Ü©&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A report by RedPanda found that ~55% of respondents use Kafka for &amp;lt; 1 MB/s. Kafka-vendor Aiven similarly shared that 50% of their Kafka deployments have an ingest rate of below 10 MB/s. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This replication is equivalent to RF=2 in Kafka with one extra non-synchronous replica. Call it RF=2.5. The client receives a response when the one&lt;/p&gt;&lt;code&gt;sync&lt;/code&gt;replica confirms the change. The other&lt;code&gt;potential&lt;/code&gt;replica is replicating asynchronously without blocking the write path. It will become promoted to&lt;code&gt;sync&lt;/code&gt;if the other one was to die. ‚Ü© ‚Ü©2&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The tests didn‚Äôt direct any read traffic to the standbys. This caused extra load on the primary - most production workloads would read from the standbys. Despite that, the results were still good! In my tests, I found that the extra read workload didn‚Äôt seem to have a negative effect on the database - it seems such tail reads were served exclusively from cache. ‚Ü© ‚Ü©2&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The node and its disk cost $1826 per year. Since we run three of those, it‚Äôs $5478/yr. The networking in AWS costs $0.02/GB and our setup is replicating 4.9MB/s across two instances - that results in 294.74TB cross-zone networking per year. That‚Äôs $6036 per year in replication networking. Assuming your clients are in the same zone as the database they‚Äôre writing to / reading from, that networking is free. That results in an annual cost of $11,514. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;We can realistically achieve a 10x+ compression ratio if operating on compressible data like logs (something Kafka is used for frequently). The only gotcha is that we need to compress larger batches - eg 25KB+ - so that requires a bit of a re-design in the pub-sub data model. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I had already spent enough business days working on this benchmark and re-running tests numerous, numerous times as I iterated on the benchmark and the methodology. On the larger instances, the cost accumulates fast and running longer tests at high MB/s rates requires deploying much larger and more expensive disks in order to store all the accumulated data. The effort spent matches the goal I have with the article. If any Postgres vendor wants to sponsor a more thorough investigation - let me know! ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Surprisingly (to me), Redis is a very popular queue-like backend choice for background jobs. Most popular open-source libraries use it. Although I‚Äôm sure Postgres can do just as good a job, many devs will prefer to use an established library rather than build one from scratch or use something less well-maintained. I do think PG-backed libraries should get developed though! ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kafka has historically never been a queue. To use it as one, you had to develop some difficult workarounds. Today, however, it is in the middle of implementing a first-class Queue-like interface (currently in Preview) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Most importantly, synchronous commit and fsync are both on. This means every write is durably persisted to disk. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The optimizations they did to support this scale are cool, but not novel. See these two talks at a) PGConf.dev 2025 (my transcript) and b) POSETTE (my transcript) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;From the talks PGConf.dev 2025 (my transcript) and POSETTE (my transcript) ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45747018</guid><pubDate>Wed, 29 Oct 2025 14:06:01 +0000</pubDate></item><item><title>Replacing EBS and Rethinking Postgres Storage from First Principles</title><link>https://www.tigerdata.com/blog/fluid-storage-forkable-ephemeral-durable-infrastructure-age-of-agents</link><description>&lt;doc fingerprint="3d79b05f0f864bea"&gt;
  &lt;main&gt;
    &lt;p&gt;Category: All posts&lt;/p&gt;
    &lt;p&gt;Oct 29, 2025&lt;/p&gt;
    &lt;p&gt;Fluid Storage is a new next-generation storage architecture: a distributed block layer that reimagines systems like EBS, combining zero-copy forks, true elasticity, and synchronous replication. Because it operates at the block layer, Fluid Storage is fully compatible with Postgres, and even with other databases and file systems as well. Every database in Tiger Cloud‚Äôs free tier now runs on Fluid Storage, giving developers direct access to these capabilities.&lt;/p&gt;
    &lt;p&gt;Try it now: Get started instantly with the Tiger CLI and MCP Server.&lt;/p&gt;
    &lt;p&gt;Partner with us: If you‚Äôre building an agentic or infrastructure platform, we‚Äôre opening early-access partnerships. Please get in touch to learn more.&lt;/p&gt;
    &lt;p&gt;Agents are the new developers, and they need a new storage layer built for how they work.&lt;/p&gt;
    &lt;p&gt;Agents spin up environments, test code, and evolve systems continuously. They need storage that can do the same: forking, scaling, and provisioning instantly, without manual work or waste.&lt;/p&gt;
    &lt;p&gt;Storage itself has evolved through eras: from static systems built for durability, to dynamic systems built for elasticity through innovations like separating compute from storage. But today‚Äôs ‚Äúelastic‚Äù infrastructure isn‚Äôt truly elastic. After operating tens of thousands of Postgres instances in Tiger Cloud, we‚Äôve seen those limits firsthand: systems that scale slowly, waste capacity, and block iteration. We‚Äôre now entering the era of fluid systems: storage that moves as continuously as the workloads it serves.&lt;/p&gt;
    &lt;p&gt;Fluid Storage was built for that world: where data flows, systems iterate autonomously, and elasticity and iteration converge into a single operation.&lt;/p&gt;
    &lt;p&gt;At its core, Fluid Storage is a distributed block layer that unifies these properties through a disaggregated architecture. It combines a horizontally scalable NVMe-backed block store, a proxy layer that exposes copy-on-write volumes, and a user-space storage device driver that makes it all look like a local disk to PostgreSQL. The result: instant forks and snapshots, automatic scaling up or down, and no downtime or over-provisioning.&lt;/p&gt;
    &lt;p&gt;Each Fluid Storage cluster manages tens of thousands of volumes across workloads and tenants‚Äîfrom ephemeral sandboxes to production-scale systems‚Äîwith consistent performance and predictable cost. In benchmark testing, a single volume sustains 110,000+ IOPS and 1.4 GB/s throughput while retaining all copy-on-write and elasticity guarantees.&lt;/p&gt;
    &lt;p&gt;It‚Äôs storage that looks like a local disk but scales like a cloud service. A new storage foundation for the age of agents.&lt;/p&gt;
    &lt;p&gt;Fluid Storage now runs every free-tier database in Tiger Cloud, giving every developer (and agent) a firsthand experience of what truly fluid infrastructure feels like. Sign up in the Tiger console or get started instantly with the Tiger CLI.&lt;/p&gt;
    &lt;p&gt;Elastic storage isn‚Äôt truly elastic.&lt;/p&gt;
    &lt;p&gt;Every engineer who‚Äôs managed databases at scale has seen it firsthand. A CI pipeline stalls waiting for a restore. A schema migration hangs mid-run because the database can‚Äôt be safely cloned. A ‚Äúresize volume‚Äù request on EBS sits in ‚Äúoptimizing‚Äù for hours. A database read replica lags for days behind a write-heavy primary. You wait, sometimes all day, not because Postgres is slow, but because the storage substrate beneath it isn‚Äôt nearly as elastic as it claims.&lt;/p&gt;
    &lt;p&gt;Cloud storage solved many problems, but true elasticity wasn‚Äôt one of them. Amazon EBS, for instance, is billed as elastic, yet volumes can only grow once every six to twenty-four hours. You can‚Äôt shrink them, and every operation that changes IOPS or throughput has a similar cooldown. Worse, you pay for the space you allocate, not what you actually use. To avoid running out of disk, you over-provision, and that excess sits idle, wasted.&lt;/p&gt;
    &lt;p&gt;This rigidity made sense in the first two eras of storage. The static era focused on durability: keeping data alive across hardware failures. The dynamic era focused on decoupling compute and storage so each could scale independently. Both have become table stakes. We believe that the next era is the fluid era: storage that scales, forks, and contracts instantly. Not just incrementally elastic, but continuous. Storage that behaves more like software than hardware.&lt;/p&gt;
    &lt;p&gt;The arrival of agents makes this shift urgent. Agents create, modify, and deploy code autonomously. They spin up sandboxes, run migrations, benchmark results, and tear everything down again, all in seconds. Each agent needs its own isolated, ephemeral environment, but with the performance and durability of production, operating on production data. This is not just experimentation; this is how agents work. Traditional storage can‚Äôt keep up with that pace or economics.&lt;/p&gt;
    &lt;p&gt;Fluid Storage was built for this reality: a distributed block store that unifies elasticity, iteration, and durability in a single substrate. It treats scaling and forking as standard operations, not exceptions. To Postgres, it looks like a normal disk. In truth, it‚Äôs a disaggregated system that delivers high throughput, fast recovery, and instant forks‚Äîstorage that finally moves as fluidly as the systems built on top of it.&lt;/p&gt;
    &lt;p&gt;Fluid Storage already now serves as the default substrate for our recently-announced free database service on Tiger.&lt;/p&gt;
    &lt;p&gt;For the past five years, we‚Äôve managed tens of thousands of Postgres instances in Tiger Cloud and seen firsthand the limitations of today‚Äôs ‚Äúelastic‚Äù cloud infrastructure.&lt;/p&gt;
    &lt;p&gt;When we launched Tiger Cloud in 2020, we built on Amazon EBS as our durable storage. It was reliable, well-understood, and integrated neatly with the rest of AWS. But as our fleet scaled into the thousands of customers, the limitations of EBS became clear across five dimensions: cost, scale-up performance, scale-down performance, elasticity, and recovery.&lt;/p&gt;
    &lt;p&gt;Cost.&lt;/p&gt;
    &lt;p&gt;EBS charges for allocation, not usage. A database storing 200 GB of data on a 1 TB provisioned volume still pays for the full terabyte. Tiger Cloud hides this complexity from users‚Äîwe bill for storage consumed, not allocated‚Äîbut that simply shifts the inefficiency from the user to us. It becomes a COGS problem instead of a usability one.&lt;/p&gt;
    &lt;p&gt;To manage it, we built adaptive algorithms that estimate a ‚Äúgood‚Äù allocation size based on each user‚Äôs consumption and rate of change. In practice, this was a constant balancing act: undershoot and risk running out of disk; overshoot and pay for unused capacity. For safety and user experience, we erred on the side of over-allocation.&lt;/p&gt;
    &lt;p&gt;The problem of EBS cost only compounds as customers scale their services horizontally: every read replica doubles the storage cost, since EBS charges per volume, not per dataset.&lt;/p&gt;
    &lt;p&gt;This cost impacts both sides: vendors through higher COGS, and customers through higher storage prices (which is why usage-based storage always costs more than allocation-based pricing on a per gigabyte basis).&lt;/p&gt;
    &lt;p&gt;Scale-up performance.&lt;/p&gt;
    &lt;p&gt;EBS volumes impose fixed ceilings on performance. Until recently, gp3 volumes topped out at 16,000 IOPS and 16 TB per volume, while io2 volumes offered higher limits‚Äîup to 64,000 IOPS and 64 TB‚Äîbut at far higher cost. Those gp3 limits have recently improved, but they also don‚Äôt address another scaling challenge: the difficulty of scaling horizontally through snapshots.&lt;/p&gt;
    &lt;p&gt;Ideally, you‚Äôd take a volume snapshot and use it to initialize a new read replica, seeding it with an exact copy of data before WAL replay begins. In theory, EBS snapshots allow this. In practice, they hydrate slowly. A restored snapshot appears immediately available, but data is fetched lazily from S3, so the replica experiences high read latency until the volume is fully loaded. We implement pre-warming strategies informed by a database‚Äôs real usage statistics, but these are effective primarily for small databases. For large, mission-critical ones, pre-warming the full working set simply takes too long.&lt;/p&gt;
    &lt;p&gt;In our experience operating Tiger Cloud, it was often faster to spin up a database from a Postgres backup stored in S3 than from an EBS snapshot in S3.&lt;/p&gt;
    &lt;p&gt;Scale-down performance.&lt;/p&gt;
    &lt;p&gt;EBS also limits how many volumes can attach to a single EC2 instance, currently twenty-four. While Tiger Cloud runs a containerized environment, this cap directly constrains how many database services we can host per server. It makes it difficult to run large numbers of small, mostly idle instances in a cost-efficient way. In effect, EBS became a bottleneck for building a true free tier.&lt;/p&gt;
    &lt;p&gt;The alternative for a free or low-cost tier that avoided isolated containers and volumes per service‚Äîpacking multiple logical databases into a single PostgreSQL cluster‚Äîwould have introduced operational trade-offs we wanted to avoid: little performance isolation, poor backup and restore options, and difficult seamless scaling.&lt;/p&gt;
    &lt;p&gt;Elasticity.&lt;/p&gt;
    &lt;p&gt;EBS technically supports resizing, but only once every 6‚Äì24 hours. That cooldown applies not just to disk capacity increases, but also to any changes in IOPS or throughput. Further, you can grow capacity, but you can‚Äôt shrink, and you can‚Äôt adjust continuously. We constantly fought these limitations when running our adaptive auto-disk-scaling algorithm.&lt;/p&gt;
    &lt;p&gt;The result is a system that simulates elasticity rather than embodying it, and these limitations ultimately leaked through to the user experience. (You see similar elasticity limitations in other managed database platforms, e.g., Supabase.)&lt;/p&gt;
    &lt;p&gt;Failure recovery.&lt;/p&gt;
    &lt;p&gt;Operational recovery was another bottleneck. In theory, we could detach a failed volume and reattach it to a new node to restore service quickly, even for users who weren‚Äôt paying for full HA replication due to cost concerns. In practice, ‚Äúclean‚Äù shutdowns from EBS worked fast (10s of seconds), but any hard failure can often take 10‚Äì20 minutes before the EBS volume detaches and becomes available for remounting elsewhere. That delay compounded user downtime precisely when fast recovery mattered most.&lt;/p&gt;
    &lt;p&gt;Agents further expose these limitations&lt;/p&gt;
    &lt;p&gt;Agents make these limitations even sharper. They often operate on sandboxed replicas of production to avoid risk, scale up to work, and scale down just as fast when done. Their workloads are ephemeral, demanding storage that can respond instantly. When many agents work on the same data in parallel, cost efficiency becomes critical.&lt;/p&gt;
    &lt;p&gt;As we began exploring agents for both internal and customer use, it became clear that today‚Äôs cloud infrastructure needed to be rethought.&lt;/p&gt;
    &lt;p&gt;Alternative architectures we discarded&lt;/p&gt;
    &lt;p&gt;We also evaluated other architectures‚Äîmost notably local NVMe and Aurora-like page-server systems‚Äîbut ultimately set them aside.&lt;/p&gt;
    &lt;p&gt;Local NVMe (e.g., as offered by PlanetScale Metal) offered the raw performance we wanted, but not the durability. If an instance fails, the data disappears with it. To compensate, every customer would need a two-or three-node database cluster for redundancy, which we found cost-prohibitive. NVMe also lacks true storage elasticity: scaling a multi-terabyte service requires copying terabytes of data to a new node and failing over, a process that takes hours or even days. Spinning up a read replica is equally slow. The throughput and latency numbers look impressive in isolation, and NVMe remains a great substrate for caching, but it revives the pre-cloud model‚Äîdedicated boxes with fixed compute and disk‚Äîwhich isn‚Äôt the foundation for an elastic, managed database platform.&lt;/p&gt;
    &lt;p&gt;Another option was a distributed page-server architecture, first introduced by Amazon Aurora and later adopted by Neon and Google AlloyDB. These systems replace PostgreSQL‚Äôs local storage layer with remote operations to a distributed page-storage service that stores pages remotely and uses a separate distributed log for write-ahead logging. It‚Äôs an elegant design for elasticity, but it comes at a cost: achieving this requires forking PostgreSQL and rewriting significant portions of its storage internals to communicate with the new remote layer. That coupling introduces real risks. Maintaining parity with upstream PostgreSQL becomes a constant need as new versions evolve; features that depend on storage semantics must be reimplemented and/or revalidated; and debugging and performance tuning also shift from a well-understood ecosystem to a proprietary one. And the result is a database-specific system, rather than a general-purpose storage substrate.&lt;/p&gt;
    &lt;p&gt;We chose to solve these problems at the storage layer. Elasticity, durability, and iteration should be properties of the underlying substrate, not features baked into a single database engine. By working at the block-storage level, we could keep PostgreSQL unchanged while making the system extensible to any workload that runs on disks. A forkable storage layer offers a more general foundation: volumes that can be cloned, branched, or scaled independently of the systems that use them (and not limited to Postgres databases).&lt;/p&gt;
    &lt;p&gt;Fluid Storage emerged from that design choice. A distributed system built for strong durability, true elasticity, and zero-copy forks. It charges for storage consumed, not allocated, and scales fluidly in both directions. Not a faster EBS, but a storage system re-architected for true elasticity and native iteration.&lt;/p&gt;
    &lt;p&gt;When we set out to design our storage architecture, the goal wasn‚Äôt just to make storage faster. It was to make it behave differently, to meet the evolving needs of existing workloads and the new needs of agentic workloads. We focused on six core objectives:&lt;/p&gt;
    &lt;p&gt;Fork-first. Forks, clones, and snapshots aren‚Äôt exceptional operations; they‚Äôre primitives. Copy-on-write happens at the block layer, and creating a new volume or database fork is a highly-efficient metadata update, not a data copy. With such capabilities, one can quickly branch full data environments and test changes in isolation, all without copying or reloading data.&lt;/p&gt;
    &lt;p&gt;Truly elastic, in both directions. Volumes can scale up or down fluidly, adapting to changing workloads and costs. Storage expands as data grows and contracts as it‚Äôs deleted or pruned. Developers or platform operators no longer need to over-provision storage based on future need, and are not prevented from downscaling if needed.&lt;/p&gt;
    &lt;p&gt;Usage-based and resource-efficient. You pay for what you use, not what you allocate. Fluid Storage‚Äôs multi-tenant design automatically reclaims unused space. Efficiency comes from the architecture itself, not by hiding platform-level waste.&lt;/p&gt;
    &lt;p&gt;Predictable performance. Elasticity shouldn‚Äôt mean variability. Fluid maintains stable latency and throughput across tenants through intelligent scheduling and load-balanced data sharding.&lt;/p&gt;
    &lt;p&gt;Postgres-compatible foundation. Applications see a normal Linux block storage device, so PostgreSQL‚Äîand any other database or file system‚Äîruns unmodified. This ensures immediate compatibility with existing tooling and software systems.&lt;/p&gt;
    &lt;p&gt;Built for both developers and platforms. A single developer can spin up a fork per commit; a platform operator can run thousands of Fluid-backed databases, each isolated, elastic, and cost efficient.&lt;/p&gt;
    &lt;p&gt;The next section describes how the architecture implements them in practice.&lt;/p&gt;
    &lt;p&gt;Fluid Storage consists of three cooperating layers, each designed to make elasticity and iteration first-class operations within the storage substrate itself, not conveniences layered on top.&lt;/p&gt;
    &lt;p&gt;Fluid Storage consists of three cooperating layers:&lt;/p&gt;
    &lt;p&gt;The lowest layer of Fluid Storage is the DBS, a transactional distributed key-value store that provides elastic, horizontally scalable block storage. DBS runs on local NVMe drives for performance. Volumes are divided into shards, each shard mapped to a replica set within the cluster. This architecture allows a single logical volume to scale seamlessly by spreading I/O across shards (and therefore many block servers) in the cluster. Cluster capacity can be dynamically increased or decreased without downtime by adding or removing DBS block servers, and shards are rebalanced transparently across the cluster.&lt;/p&gt;
    &lt;p&gt;DBS maps block addresses (keys) to disk blocks (values). Every write is versioned, allowing old and new values to coexist until garbage collection reclaims the obsolete ones. Transactions provide atomicity and consistency. Data is replicated synchronously across multiple DBS replicas as part of each write operation, ensuring strong consistency and availability. A single DBS cluster can typically manage 100s of TBs of logical storage‚Äîthe data visible to clients‚Äîwhile the underlying physical storage is larger to accommodate replication.&lt;/p&gt;
    &lt;p&gt;The result is a durable, multi-tenant, horizontally scalable storage layer that supports versioned block writes and enables efficient copy-on-write behavior above it.&lt;/p&gt;
    &lt;p&gt;Above DBS sits a layer of storage proxies, which present virtual volumes to clients and coordinate all I/O. The proxies translate network block operations into DBS reads and writes, manage volume lineage, and enforce per-volume performance and capacity limits.&lt;/p&gt;
    &lt;p&gt;Each volume in Fluid Storage maintains metadata that tracks which blocks exist in which generation of a volume. This metadata defines the lineage of the volume and enables efficient copy-on-write across snapshots and forks. This metadata answers whether a specific block ID exists in a given generation of volume, yet only adds roughly 0.003% overhead, small enough to retain in memory for fast querying.&lt;/p&gt;
    &lt;p&gt;When a snapshot is taken, the system increments the parent volume's generation number (say, 6), and the child volume is created starting at that same new generation number (6). The child stores which previous parent generation it was forked from (5), establishing its lineage. Both parent and child now have their own separate generation directories for future writes. They also maintain separate block metadata for their current generation (6), but have the same information for previous generations (0 through 5). This allows the child to locate and read all of the parent's data without copying any actual blocks.&lt;/p&gt;
    &lt;p&gt;On writes, the proxy allocates a new block in DBS tagged with its volume's current generation number, updates that generation's directory to map the block ID to the new data location, and marks the block as present in that generation. Data from earlier generations remains unchanged.&lt;/p&gt;
    &lt;p&gt;On reads, the system traverses generations from newest to oldest, checking the block metadata for each generation to see if the block exists there. It stops at the first generation where the block is found and reads from that generation's data. This lookup completes in microseconds, as checking membership is extremely fast.&lt;/p&gt;
    &lt;p&gt;This lineage mechanism allows multiple volumes to safely share unmodified blocks. Snapshots are fast and storage-efficient, adding only metadata overhead. Forks are similarly fast and zero-copy, duplicating snapshot mappings without data movement. Physical storage grows only as data diverges and new blocks are written.&lt;/p&gt;
    &lt;p&gt;The storage proxy also provides control and safety at the system boundary. It authenticates clients, maintains secure isolation between tenants, and can enforce per-volume IOPS and storage limits. It can cache frequently accessed blocks to improve locality, though caching is an optimization rather than a requirement.&lt;/p&gt;
    &lt;p&gt;Because agents often spin up parallel instances of themselves, consistency management had to be lightweight, version-aware, and tolerant of many concurrent forks operating on shared data.&lt;/p&gt;
    &lt;p&gt;Together, these responsibilities make the storage proxy the coordination and lineage layer of Fluid Storage. It‚Äôs the component that turns distributed blocks into coherent volumes.&lt;/p&gt;
    &lt;p&gt;PostgreSQL expects a block device. Rewriting its storage engine to speak a custom API would break decades of compatibility. So instead, we integrated at the kernel boundary through a user-space storage device driver.&lt;/p&gt;
    &lt;p&gt;The storage device driver exposes Fluid Storage volumes as standard Linux block devices mountable with filesystems such as ext4 or xfs. It manages multiple I/O queues pinned to CPU cores for concurrency, supports zero-copy operations when possible, and allows volumes to be resized dynamically while online. It also provides device recovery primitives which massively simplify our rollouts and error recovery.&lt;/p&gt;
    &lt;p&gt;Because this integration happens at the Linux block layer, Fluid Storage can leverage existing OS-level mechanisms for resource control. Per-volume IOPS and throughput limits can be managed through the Linux environment‚Äîusing cgroups or similar controls‚Äîallowing predictable performance isolation without requiring specialized kernel modifications. Additionally, we can also rely on kernel tools for monitoring and testing our block device.&lt;/p&gt;
    &lt;p&gt;Crucially, this design also means Fluid Storage inherits advances from the broader ecosystem. PostgreSQL 18 introduced support for Linux io_uring, which significantly improves asynchronous I/O throughput, and Linux continues to evolve its block and memory subsystems in similar ways. Because Fluid Storage operates at this layer, it already benefits from recent PostgreSQL and Linux improvements and will continue to inherit future performance gains automatically.&lt;/p&gt;
    &lt;p&gt;Within our Kubernetes infrastructure we seamlessly integrate with our existing orchestration software, because Fluid Storage is presented as a storage class. This enables operations such as resizing volumes or taking Postgres-consistent snapshots to work out-of-the-box, similar to how they function on EBS.&lt;/p&gt;
    &lt;p&gt;Putting these three layers together, one can trace read and write operations in the system. In this case, from the perspective of PostgreSQL backed by Fluid Storage as its underlying block storage.&lt;/p&gt;
    &lt;p&gt;Read Path&lt;/p&gt;
    &lt;p&gt;Write Path&lt;/p&gt;
    &lt;p&gt;While this describes the basic I/O flow, it omits the mechanisms that can improve performance and predictability, such as block caching at either the local instance (through the user-space device driver) or the storage proxy, and management of volume IOPS and throughput at both layers. These controls don‚Äôt alter the high-level behavior of Fluid Storage; they make the system more efficient, responsive, and predictable under varying workloads.&lt;/p&gt;
    &lt;p&gt;To PostgreSQL, Fluid Storage simply appears as a local disk. Underneath, it is a disaggregated storage system that scales elastically, replicates safely, and supports fast zero-copy forks. All while maintaining standard block storage semantics.&lt;/p&gt;
    &lt;p&gt;Architecture is meaningful only in what it enables. In Fluid Storage, the results appear along two dimensions: the technical properties that define how the system behaves, and the developer outcomes that emerge from them.&lt;/p&gt;
    &lt;p&gt;Forkability and snapshots: Snapshots are metadata-only and complete very quickly, regardless of volume size. Forks are writable snapshots: they start as zero-copy, then store just the blocks that are updated from its parent snapshot. Storage grows only as data diverges.&lt;/p&gt;
    &lt;p&gt;We ran microbenchmarks on the Fluid Storage layer to measure end-to-end latency for snapshot and fork creation as handled by the storage proxy, across volumes ranging from 1 GB to 100 GB. In all cases, both operations completed within roughly 500‚Äì600 milliseconds. These measurements exclude orchestration time (e.g., Kubernetes provisioning and mounting a new volume at a client) and any application-level coordination that may precede a snapshot, such as issuing a PostgreSQL checkpoint. They represent the raw efficiency of the underlying storage system‚Äîthe baseline latency achievable for instant forks and snapshots under normal load.&lt;/p&gt;
    &lt;p&gt;Elasticity: Volumes expand and contract fluidly with workload changes, eliminating waste from unused allocation. Cluster throughput scales linearly with demand as demand grows, with I/O distributed across DBS block servers. Each volume is sharded across many block servers, reducing hotspots and enabling parallel reads, writes, and recovery.&lt;/p&gt;
    &lt;p&gt;Cost and efficiency: Fluid Storage‚Äôs multi-tenant design keeps utilization high across large numbers of colocated volumes. Unmodified blocks are shared across database forks and read replicas, so new forks or replicas consume only the incremental changes they write. Because the platform continuously reclaims and reuses space, it doesn‚Äôt need to over-allocate or reserve idle capacity. Usage is billed based on actual consumption, not provisioned size, and the efficiency of this architecture translates directly into lower cost for users.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Latency - p50&lt;p&gt;(ms)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Latency - p99&lt;p&gt;(ms)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput&lt;p&gt;(IOPS)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Throughput&lt;p&gt;(MB/s)&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Read (random)&lt;/cell&gt;
        &lt;cell&gt;1.30&lt;/cell&gt;
        &lt;cell&gt;1.84&lt;/cell&gt;
        &lt;cell&gt;110,436&lt;/cell&gt;
        &lt;cell&gt;1377&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Read (seq)&lt;/cell&gt;
        &lt;cell&gt;0.97&lt;/cell&gt;
        &lt;cell&gt;1.74&lt;/cell&gt;
        &lt;cell&gt;118,743&lt;/cell&gt;
        &lt;cell&gt;1375&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Write (random)&lt;/cell&gt;
        &lt;cell&gt;5.3&lt;/cell&gt;
        &lt;cell&gt;7.9&lt;/cell&gt;
        &lt;cell&gt;67,137&lt;/cell&gt;
        &lt;cell&gt;689&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Write (seq)&lt;/cell&gt;
        &lt;cell&gt;5.4&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
        &lt;cell&gt;40,038&lt;/cell&gt;
        &lt;cell&gt;494&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Benchmark results from fio workloads on a Fluid Storage cluster running in a standard production environment on AWS. Reads and writes are generated from user space using direct I/O to bypass the local page cache, exercising the full I/O path described in ‚ÄúLife of a Request.‚Äù Latency and IOPS benchmarks use 4 KB blocks, while throughput (MB/s) benchmarks use 512 KB blocks to emulate client-side write coalescing.&lt;/p&gt;
    &lt;p&gt;Performance: Latency remains low and stable across diverse client workloads, supported by asynchronous I/O and distributed sharding. Benchmarks from a standard Fluid Storage deployment in our production environment‚Äîrun on production-scale client instances without IOPS rate limiting‚Äîdemonstrate high and consistent performance.&lt;/p&gt;
    &lt;p&gt;As shown in the table above, a single Fluid Storage volume sustains read throughput exceeding 110,000 IOPS and 1.375 GB/s (with read throughput bottlenecked by network bandwidth in its current server configuration). It sustained write throughput between 40,000‚Äì67,000 IOPS and 500‚Äì700 MB/s. Single-block read latency is typically around 1 ms, and write latency around 5 ms. All writes are synchronously replicated in the DBS before returning to the client, ensuring durability without sacrificing stability.&lt;/p&gt;
    &lt;p&gt;These capabilities change how developers and agents teams build, test, and evolve their systems. In continuous integration and deployment (CI/CD) pipelines, every pull request can run against its own isolated database fork, eliminating the need to queue behind shared test environments. Schema migrations can be rehearsed safely on full-fidelity clones before any production change, reducing rollout risk while preserving realistic data fidelity. Analytics teams can create short-lived copies of production data (without having to actually copy data or pay for it twice), explore results, and discard them when finished.&lt;/p&gt;
    &lt;p&gt;The same primitives that let developers iterate faster also unlock new possibilities for agentic systems.&lt;/p&gt;
    &lt;p&gt;Agents can begin from a clean slate, spinning up an empty database for rapid prototyping, or start from a fork of production data to extend existing behavior or add new capabilities. Each agent operates on its own isolated fork, allowing parallel experiments and reasoning paths to run independently. Once experiments complete, results can be compared: a single branch promoted as the new primary, or code changes merged back into a production fork.&lt;/p&gt;
    &lt;p&gt;In this model, compute can become more ephemeral. Agents and workflows start instantly, run in isolation, and tear down when finished, leaving behind only snapshots that capture stable intermediate states. Snapshots become the natural unit of iteration, something to branch, backtrack, or extend as needed. Fluid Storage makes this loop‚Äîfork, test, recover, evolve‚Äîboth fast and resource-efficient, turning operational data from a single artifact into a dynamic substrate for iteration.&lt;/p&gt;
    &lt;p&gt;In a world where workloads can self-orchestrate‚Äîspinning up, scaling, and shutting down autonomously‚Äîreliability must be continuous, not coordinated.&lt;/p&gt;
    &lt;p&gt;Reliability and Resilience&lt;/p&gt;
    &lt;p&gt;Fluid Storage is engineered for reliability through four independent layers of resilience‚Äîstorage replication, database durability, compute recovery, and region-level isolation‚Äîeach reinforcing the others to ensure consistent operation under failure. All of these mechanisms operate transparently; users never need to manage replicas, tune recovery, or coordinate failover.&lt;/p&gt;
    &lt;p&gt;1. Storage replication.&lt;/p&gt;
    &lt;p&gt;The blocks comprising each volume in Fluid Storage are synchronously replicated across multiple block servers within a DBS cluster. The system automatically detects and compensates for replica failures, rebalancing data and restoring full replication without operator intervention. The storage proxy continuously monitors block-server health, routing around failed nodes to maintain continuity. At the client boundary, the storage device driver retries I/O through its current proxy when transient failures occur, and reconnects to a different proxy if the existing connection is lost. These processes are fully automatic, ensuring strong consistency and continuous availability within the storage tier.&lt;/p&gt;
    &lt;p&gt;2. Database durability.&lt;/p&gt;
    &lt;p&gt;Beyond block-level replication, PostgreSQL durability is maintained through incremental backups and continuous WAL streaming. Each database retains enough WAL for arbitrary point-in-time recovery (PITR); even free services on Tiger Cloud provide 24-hour PITR. These backups and WAL segments are stored independently in S3, ensuring operational isolation from the active storage tier.&lt;/p&gt;
    &lt;p&gt;In the unlikely event of a Fluid Storage tier failure, new database volumes are automatically provisioned and restored from S3. The system also supports transparent migration between EBS-backed storage and Fluid Storage, allowing databases to move seamlessly across tiers if needed.&lt;/p&gt;
    &lt;p&gt;3. Compute recovery.&lt;/p&gt;
    &lt;p&gt;If a compute instance fails (the node running PostgreSQL itself), Tiger Cloud automatically provisions a replacement, reattaches the existing Fluid Storage volume, restarts PostgreSQL, and triggers it to replay its latest WAL segments. Recovery typically completes within tens of seconds, even for single-instance databases without HA replicas. Because the storage proxy maintains no client-side state, reconnection and recovery are immediate once the new compute is available, all without user action.&lt;/p&gt;
    &lt;p&gt;4. Region resilience.&lt;/p&gt;
    &lt;p&gt;Fluid Storage supports both single- and multi-availability zone (AZ) deployments for a single DBS cluster. In multi-AZ configurations, sharded replica sets are distributed across zones to tolerate AZ-level failures. These modes represent a tradeoff: cross-AZ replication improves resilience but increases latency and inter-AZ network costs.&lt;/p&gt;
    &lt;p&gt;Current deployments favor single-AZ clusters for lower latency and cost efficiency, while cross-AZ durability is achieved through PostgreSQL‚Äôs own high-availability replication. In this model, each database node in a multi-AZ cluster uses an independent Fluid Storage cluster in its respective zone. This design requires a full copy of storage per zone (at higher cost) but provides stronger fault isolation: each cluster operates with its own control plane, minimizing correlated failures across zones. This coordination is handled automatically by the system; the complexity is abstracted away from the user.&lt;/p&gt;
    &lt;p&gt;Availability and Access&lt;/p&gt;
    &lt;p&gt;Fluid Storage already serves customer-facing workloads within Tiger Cloud and powers all databases in our new free tier. Developers can create, pause, resume, fork, and snapshot databases directly through the cloud console, REST API, Tiger CLI, or the Tiger MCP server, making it easy to experiment with elastic and fork-first behavior in practice.&lt;/p&gt;
    &lt;p&gt;The system is available today as a public beta for the free tier, with larger-scale workloads being onboarded gradually through early-access programs. General availability will follow sustained operation across a broad set of customer environments, ensuring that Fluid Storage meets the standards of maturity, reliability, and performance expected of a core database storage platform.&lt;/p&gt;
    &lt;p&gt;Fluid Storage introduces our next-generation storage architecture: a distributed block layer that combines synchronous replication, true elasticity, and zero-copy forks. It delivers predictable performance, efficient scaling up or down, and rapid recovery. It does this all while remaining fully compatible with PostgreSQL and, because it operates at the block storage layer, also remaining fully compatible with other databases and file systems as well.&lt;/p&gt;
    &lt;p&gt;Every database in Tiger Cloud‚Äôs free tier now runs on Fluid Storage, giving developers direct access to these capabilities.&lt;/p&gt;
    &lt;p&gt;This foundation opens new ways to build, test, and extend data-driven systems. From faster iteration in developer workflows to more autonomous, agentic applications, Fluid Storage makes data as dynamic as the systems built on it. Because if agents are the new developers, storage must evolve to match their speed.&lt;/p&gt;
    &lt;p&gt;You can try Fluid Storage today through Tiger‚Äôs free services: get started instantly with the Tiger CLI and MCP Server.&lt;/p&gt;
    &lt;p&gt;And if you‚Äôre building an agentic or infrastructure platform and want to explore how Tiger‚Äôs databases powered by Fluid Storage can support your workloads, we‚Äôre opening early-access partnerships. Please get in touch to learn more.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45748484</guid><pubDate>Wed, 29 Oct 2025 15:49:18 +0000</pubDate></item><item><title>Tell HN: Azure outage</title><link>https://news.ycombinator.com/item?id=45748661</link><description>&lt;doc fingerprint="7e5891556f0627d5"&gt;
  &lt;main&gt;
    &lt;p&gt;15:45 UTC on 29 October 2025 ‚Äì Customer impact began.&lt;/p&gt;
    &lt;p&gt;16:04 UTC on 29 October 2025 ‚Äì Investigation commenced following monitoring alerts being triggered.&lt;/p&gt;
    &lt;p&gt;16:15 UTC on 29 October 2025 ‚Äì We began the investigation and started to examine configuration changes within AFD.&lt;/p&gt;
    &lt;p&gt;16:18 UTC on 29 October 2025 ‚Äì Initial communication posted to our public status page.&lt;/p&gt;
    &lt;p&gt;16:20 UTC on 29 October 2025 ‚Äì Targeted communications to impacted customers sent to Azure Service Health.&lt;/p&gt;
    &lt;p&gt;17:26 UTC on 29 October 2025 ‚Äì Azure portal failed away from Azure Front Door.&lt;/p&gt;
    &lt;p&gt;17:30 UTC on 29 October 2025 ‚Äì We blocked all new customer configuration changes to prevent further impact.&lt;/p&gt;
    &lt;p&gt;17:40 UTC on 29 October 2025 ‚Äì We initiated the deployment of our ‚Äòlast known good‚Äô configuration.&lt;/p&gt;
    &lt;p&gt;18:30 UTC on 29 October 2025 ‚Äì We started to push the fixed configuration globally.&lt;/p&gt;
    &lt;p&gt;18:45 UTC on 29 October 2025 ‚Äì Manual recovery of nodes commenced while gradual routing of traffic to healthy nodes began after the fixed configuration was pushed globally.&lt;/p&gt;
    &lt;p&gt;23:15 UTC on 29 October 2025 - PowerApps mitigation of dependency, and customers confirm mitigation.&lt;/p&gt;
    &lt;p&gt;00:05 UTC on 30 October 2025 ‚Äì AFD impact confirmed mitigated for customers.&lt;/p&gt;
    &lt;p&gt;Unfortunately,that is also typical. I've seen it take longer than that for AWS to update their status page.&lt;/p&gt;
    &lt;p&gt;The reason is probably because changes to the status page require executive approval, because false positives could lead to bad publicity, and potentially having to reimburse customers for failing to meet SLAs.&lt;/p&gt;
    &lt;p&gt;Not to put too fine a point on it, but if I have a dark passenger in my tech life it is almost entirely caused by what Microsoft wants to inflict on humanity - and more importantly; how successful they are at doing it.&lt;/p&gt;
    &lt;p&gt;Yes. But the point is compared to Azure in places the statement was very much the pot commenting on the kettles sooty arse. And git makes no particular pretence to be particularly friendly, just that it does a particular job efficiently.&lt;/p&gt;
    &lt;p&gt;Feel like I have to defend windows phone here, I liked it! Although I swore off the platform after the hardware I bought wasn‚Äôt eligible for the windows phone 8 upgrade even though the hardware was less than two years old. They punished early adopters&lt;/p&gt;
    &lt;p&gt;&amp;gt; In Microsoft's defense, Azure has always been a complete joke. It's extremely developer unfriendly, buggy and overpriced.&lt;/p&gt;
    &lt;p&gt;Don't forget extremely insecure. There is a quarterly critical cross-tenant CVE with trivial exploitation for them, and it has been like that for years.&lt;/p&gt;
    &lt;p&gt;I've only used Azure, to me it seems fine ish. Some things are rather overcomplicated and it's far from perfect but I assumed the other providers were similarly complicated and imperfect.&lt;/p&gt;
    &lt;p&gt;Can't say I've experienced many bugs in there either. It definitely is overpriced but I assume they all are?&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. In addition. customers may experience issues accessing the Azure Portal. Customers can attempt to use programmatic methods (PowerShell, CLI, etc.) to access/utilize resources if they are unable to access the portal directly. We have failed the portal away from Azure Front Door (AFD) to attempt to mitigate the portal access issues and are continuing to assess the situation.&lt;/p&gt;
    &lt;p&gt;We are actively assessing failover options of internal services from our AFD infrastructure. Our investigation into the contributing factors and additional recovery workstreams continues. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Update: 16:35 UTC:&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing Azure Front Door issues resulting in a loss of availability of some services. We suspect that an inadvertent configuration change as the trigger event for this issue. We are taking two concurrent actions where we are blocking all changes to the AFD services and at the same time rolling back to our last known good state.&lt;/p&gt;
    &lt;p&gt;We have failed the portal away from Azure Front Door (AFD) to mitigate the portal access issues. Customers should be able to access the Azure management portal directly.&lt;/p&gt;
    &lt;p&gt;We do not have an ETA for when the rollback will be completed, but we will update this communication within 30 minutes or when we have an update.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 17:17 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;"We have initiated the deployment of our 'last known good' configuration. This is expected to be fully deployed in about 30 minutes from which point customers will start to see initial signs of recovery. Once this is completed, the next stage is to start to recover nodes while we route traffic through these healthy nodes."&lt;/p&gt;
    &lt;p&gt;"This message was last updated at 18:11 UTC on 29 October 2025"&lt;/p&gt;
    &lt;p&gt;At this stage, we anticipate full mitigation within the next four hours as we continue to recover nodes. This means we expect recovery to happen by 23:20 UTC on 29 October 2025. We will provide another update on our progress within two hours, or sooner if warranted.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 19:57 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;Where do these alerts supposedly come from? I started having issues around 4PM (GMT), couldn't access portal, and couldn't make AKV requests from the CLI, and initially asked our Ops guys but with no info and a vague "There may be issues with Portal" on their status page, that was me done for the day.&lt;/p&gt;
    &lt;p&gt;in many cases: no service health alerts, no status page updates and no confirmations from the support team in tickets. still we can confirm these issues from different customers accross europe. Mostly the issues are regional dependent.&lt;/p&gt;
    &lt;p&gt;This is the single most frustrating thing about these incidents. As you're harmstrung on what you can do or how you can react until Microsoft officially acknowledges a problem. Took nearly 90mins both today and when it happened on 9th October.&lt;/p&gt;
    &lt;p&gt;Whilst the status message acknowledge's the issue with Front Door (AFD), it seems as though the rest of the actions are about how to get Portal/internal services working without relying on AFD. For those of us using Front Door does that mean we're in for a long haul?&lt;/p&gt;
    &lt;p&gt;It's pretty unlikely. AWS published a public 'RCA' https://aws.amazon.com/message/101925/. A race condition in a DNS 'record allocator' causing all DNS records for DDB to be wiped out.&lt;/p&gt;
    &lt;p&gt;I'm simplifying a bit, but I don't think it's likely that Azure has a similar race condition wiping out DNS records on _one_ system than then propagates to all others. The similarity might just end at "it was DNS".&lt;/p&gt;
    &lt;p&gt;That RCA was fun. A distributed system with members that don't know about each other, don't bother with leader elections, and basically all stomp all over each other updating the records. It "worked fine" until one of the members had slightly increased latency and everything cascade-failed down from there. I'm sure there was missing (internal) context but it did not sound like a well-architected system at all.&lt;/p&gt;
    &lt;p&gt;THIS is the real deal. Some say it's always DNS but many times it's some routing fuckup with BGP. two most cursed 3 letter acronym technologies out there&lt;/p&gt;
    &lt;p&gt;Yeah, I am guessing it's just a placeholder till they get more info. I thought I saw somewhere that internally within Microsoft it's seen as a "Sev 1" with "all hands on deck" - Annoyingly I can't remember where I saw it, so if someone spots it before I do, please credit that person :D&lt;/p&gt;
    &lt;p&gt;It's a Sev 0 actually (as one would expect - this isn't a big secret). I was on the engineering bridge call earlier for a bit. The Azure service I work on was minimally impacted (our customer facing dashboard could not load, but APIs and data layer were not impacted) but we found a workaround.&lt;/p&gt;
    &lt;p&gt;yea I saw that, but im not sure on how accurate that is. a few large apps/companies I know to be 100% on AWS in us-east-1 are cranking along just fine.&lt;/p&gt;
    &lt;p&gt;We already had to do it for large files served from Blob Storage since they would cap out at 2MB/s when not in cache of the nearest PoP. If you‚Äôve ever experienced slow Windows Store or Xbox downloads it‚Äôs probably the same problem.&lt;/p&gt;
    &lt;p&gt;I had a support ticket open for months about this and in the end the agent said ‚Äúthis is to be expected and we don‚Äôt plan on doing anything about it‚Äù.&lt;/p&gt;
    &lt;p&gt;We‚Äôve moved to Cloudflare and not only is the performance great, but it costs less.&lt;/p&gt;
    &lt;p&gt;Only thing I need to move off Front Door is a static website for our docs served from Blob Storage, this incident will make us do it sooner rather than later.&lt;/p&gt;
    &lt;p&gt;we are considering the same but because our website uses APEX domain we would need to move all DNS resolver to cloudfront right ? Does it have as a nice "rule set builder" as azure ?&lt;/p&gt;
    &lt;p&gt;Unless you pay for CloudFlare‚Äôs Enterpise plan, you‚Äôre required to have them host your DNS zone, you can use a different registrar as long as you just point your NS records to Cloudflare.&lt;/p&gt;
    &lt;p&gt;Be aware that if you‚Äôre using Azure as your registrar, it‚Äôs (probably still) impossible to change your NS records to point to CloudFlare‚Äôs DNS server, at least it was for me about 6 months ago.&lt;/p&gt;
    &lt;p&gt;This also makes it impossible to transfer your domain to them either, as CloudFlare‚Äôs domain transfer flow requires you set your NS records to point to them before their interface shows a transfer option.&lt;/p&gt;
    &lt;p&gt;In our case we had to transfer to a different registrar, we used Namecheap.&lt;/p&gt;
    &lt;p&gt;However, transferring a domain from Azure was also a nightmare. Their UI doesn‚Äôt have any kind of transfer option, I eventually found an obscure document (not on their Learn website) which had an az command which would let you get a transfer code which I could give to Namecheap.&lt;/p&gt;
    &lt;p&gt;Then I had to wait over a week for the transfer timeout to occur because there is no way on Azure side that I could find to accept the transfer immediately.&lt;/p&gt;
    &lt;p&gt;I found CloudFlare‚Äôs way of building rules quite easy to use, different from Front Door but I‚Äôm not doing anything more complex than some redirects and reverse proxying.&lt;/p&gt;
    &lt;p&gt;I will say that Cloudflare‚Äôs UI is super fast, with Front Door I always found it painfully slow when trying to do any kind of configuration.&lt;/p&gt;
    &lt;p&gt;Cloudflare also doesn‚Äôt have the problem that Front Door has where it requires a manual process every 6 months or so to renew the APEX certificate.&lt;/p&gt;
    &lt;p&gt;Thanks :). We don't use Azure as our registrar. It seems I'll have to plan for this then, we also had another issue, AFD has a hard 500ms tls handshake timeout (doesn't matter how much you put on the origin timeout settings) which means if our server was slow for some reason we would get 504 origin timeout.&lt;/p&gt;
    &lt;p&gt;They briefly had a statement about using Traffic Manager to work with your AFD to work around this issue, with a link to learn.microsoft.com/...traffic-manager, and the link didn't work. Due to the same issue affecting everyone right now.&lt;/p&gt;
    &lt;p&gt;They quickly updated the message to REMOVE the link. Comical at this point.&lt;/p&gt;
    &lt;p&gt;I noticed that Starbucks mobile ordering was down and thought ‚Äúwelp, I guess I‚Äôll order a bagel and coffee on Grubhub‚Äù, then GrubHub was down. My next stop was HN to find the common denominator, and y‚Äôall did not disappoint.&lt;/p&gt;
    &lt;p&gt;I‚Äôve seen this up close twice and I‚Äôm surprised it‚Äôs only twice. Between March and September one year, 6 people on one team had to get new hard drives in their thinkpads and rebuild their systems. All from the same PO but doled out over the course of a project rampup. That was the first project where the onboarding docs were really really good, since we got a lot of practice in a short period of time.&lt;/p&gt;
    &lt;p&gt;Long before that, the first raid array anyone set up for my (teams‚Äô) usage, arrived from Sun with 2 dead drives out of 10. They RMA‚Äôd us 2 more drives and one of those was also DOA. That was a couple years after Sun stopped burning in hardware for cost savings, which maybe wasn‚Äôt that much of a savings all things considered.&lt;/p&gt;
    &lt;p&gt;Many years ago (13?), I was around when Amazon moved SABLE from RAM to SSDs. A whole rack came from a single batch, and something like 128 disks went out at once.&lt;/p&gt;
    &lt;p&gt;I was an intern but everyone seemed very stressed.&lt;/p&gt;
    &lt;p&gt;Why? Starbucks is not providing a critical service. Spending less money and resources and just accepting the risk that occasionally you won't be able to sell coffee for a few hours is a completely valid decision from both management and engineering pov.&lt;/p&gt;
    &lt;p&gt;It's absolutely batshit that an in-person transaction with cash becomes impossible when the computers are down.&lt;/p&gt;
    &lt;p&gt;I've seen it multiple times at various stores; only once did I see them taking cash and writing things down (probably to enter into the system later when it came back up).&lt;/p&gt;
    &lt;p&gt;If I were a Starbucks shareholder I wouldn't be happy that my company is throwing away revenue because of the CTO's decision to outsource accountability&lt;/p&gt;
    &lt;p&gt;Time and time again it's shown that AWS is far more expensive than other solutions, just easier for the Execs to offshore the blame.&lt;/p&gt;
    &lt;p&gt;I noticed it when my Netatmo rigamajig stopped notifying me of bad indoor air quality. Lovely. Why does it need to go through the cloud if the data is right there in the home network‚Ä¶&lt;/p&gt;
    &lt;p&gt;Same here for netatmo - ironically I replied to an incident report with netatmo saying all was OK when the whole system was falling over.&lt;/p&gt;
    &lt;p&gt;However netatmo does need to have a server to store data as you need to consolidate acreoss devices plus you can query gfor a year's data and that won't and can't be held locally.&lt;/p&gt;
    &lt;p&gt;My inner Nelson-from-the-Simpsons wishes I was on your team today, able to flaunt my flask of tea and homemade packed sandwiches. I would tease you by saying 'ha ha!' as your efforts to order coffee with IP packets failed.&lt;/p&gt;
    &lt;p&gt;I always go everywhere adequately prepared for beverages and food. Thanks to your comment, I have a new reason to do so. Take out coffees are actually far from guaranteed. Payment systems could go down, my bank account could be hacked or maybe the coffee shop could be randomly closed. Heck, I might even have an accident crossing the road. Anything could happen. Hence, my humble flask might not have the top beverage in it but at least it works.&lt;/p&gt;
    &lt;p&gt;We all design systems with redundancy, backups and whatnot, but few of us apply this thinking to our food and drink. Maybe get a kettle for the office and a backup kettle, in case the first one fails?&lt;/p&gt;
    &lt;p&gt;It still surprises me how much essential services like public transport are completely reliant on cloud providers, and don't seem to have backups in place.&lt;/p&gt;
    &lt;p&gt;Here in The Netherlands, almost all trains were first delayed significantly, and then cancelled for a few hours because of this, which had real impact because today is also the day we got to vote for the next parlement (I know some who can't get home in time before the polls close, and they left for work before they opened).&lt;/p&gt;
    &lt;p&gt;Is voting there a one day only event? If not, I feel the solution to that particular problem is quite clear. There‚Äôs a million things that could go wrong causing you to miss something when you try to do it in a narrow time range (today after work before polls close)&lt;/p&gt;
    &lt;p&gt;If it‚Äôs a multi day event, it‚Äôs probably that way for a reason. Partially the same as the solution to above.&lt;/p&gt;
    &lt;p&gt;In europe, voting typically happens in one day, where everyone physically goes to their designated voting place and puts papers in a transparent box. You can stay there and wait for the count at the end of the day if you want to. Tom Scott has a very good video about why we don't want electronic/mail voting: https://www.youtube.com/watch?v=w3_0x6oaDmI&lt;/p&gt;
    &lt;p&gt;Well "mail in voting" in Washington state pretty much means you drop off your ballot in a drop box in your neighborhood. Which is pretty much the same thing as putting it in a ballot box.&lt;/p&gt;
    &lt;p&gt;The description of voting in the Netherlands is that you can see your ballot physically go into a clear box and stay to see that exact box be opened and all ballots tallied.&lt;/p&gt;
    &lt;p&gt;Dropping a ballot in a box in tour neighborhood helps ensure nothing with regards to the actually ballot count.&lt;/p&gt;
    &lt;p&gt;Here in NZ when I've been to vote, there are usually a couple of party affiliates at the voting location, doing what one of the parent posts described:&lt;/p&gt;
    &lt;p&gt;&amp;gt; You can stay there and wait for the count at the end of the day if you want to.&lt;/p&gt;
    &lt;p&gt;And if you watch the election night news, you'll see footage of multiple people counting the votes from the ballot boxes, again with various people observing to check that nothing dodgy is going on.&lt;/p&gt;
    &lt;p&gt;Having everyone just put their ballots in a postbox seems like a good way remove public trust from the electoral system, because noone's standing around waiting for the postie to collect the mail, or looking at what happens in the mail truck, or the rest of the mail distribution process.&lt;/p&gt;
    &lt;p&gt;I'm sure I've seen reports in the US of people burning postboxes around election time. Things like this give more excuses to treat election results as illegitimate, which I believe has been an issue over there.&lt;/p&gt;
    &lt;p&gt;(Yes, we do also have advanced voting in NZ, but I think they're considered "special votes" and are counted separately .. the elections are largely determined on the day by in-person votes, with the special votes being confirmed some days later)&lt;/p&gt;
    &lt;p&gt;I‚Äôm not sure what‚Äôs so special in Oregon‚Äôs ballot boxes. But, tampering that is detected (don‚Äôt need much special to detect a burning box I guess!) is not a complete failure for a system. If any elections were close enough for a box to matter, they could have rerun them.&lt;/p&gt;
    &lt;p&gt;In Sweden, mail/early votes get sent through the postal system to the official ballot box for those votes. In 2018, a local election had to be redone because the post delivered votes late. Mail delivery occasionally have packaged delayed or lost, and votes are note immune to this problem. In one case the post also gave the votes to an unauthorized person, through the votes did end up at the right place.&lt;/p&gt;
    &lt;p&gt;It is a small but distinct difference between mail/early voting and putting the votes directly into the ballot box.&lt;/p&gt;
    &lt;p&gt;With proper mail voting you have a way to verify that your mailed in vote is counted.&lt;/p&gt;
    &lt;p&gt;(AI generated explanation) How the double-envelope system works&lt;/p&gt;
    &lt;p&gt;Inner ‚Äúsecrecy‚Äù envelope&lt;/p&gt;
    &lt;p&gt;You mark your ballot, fold it, and slip it into an unmarked inner envelope. No name or identifying info is on this envelope, so your choices stay anonymous. Outer declaration envelope&lt;/p&gt;
    &lt;p&gt;The inner envelope goes inside a larger outer envelope that carries: ‚Äì A ballot ID/barcode unique to you. ‚Äì A signature line that must match the one on file with your election office. In many states, a detachable privacy flap or perforated strip hides the signature until election officials open the outer envelope, keeping the ballot secret.&lt;/p&gt;
    &lt;p&gt;If you wish, you can write a phrase on your ballot. The phrases and their corresponding vote are broadcast (on tv, internet, etc). So if you want to validate that your vote was tallied correctly, write a unique phrase. Or you could pick a random 30 digit number, collisions should be zero-probability, right?&lt;/p&gt;
    &lt;p&gt;I mean, this would be annoying because people would write slurs and advertisements, and the government would have to broadcast them. But, it seems pretty robust.&lt;/p&gt;
    &lt;p&gt;I‚Äôd suggest the state handle the number issuing, but then they could record who they issues which numbers to, and the winning party could go about rounding up their opposition, etc.&lt;/p&gt;
    &lt;p&gt;Googling around a bit, it sounds like there are systems that let you verify that your ballot made it, but not necessarily that it was counted correctly. (For this reason, I guess?)&lt;/p&gt;
    &lt;p&gt;You have to trust that whole system. Maybe you do, I don't know the details of how any of that works.&lt;/p&gt;
    &lt;p&gt;When I vote in person, I know all the officials there from various parties are just like...looking at the box for the whole day to make sure everything is counted. It's much easier to understand and trust.&lt;/p&gt;
    &lt;p&gt;Off the top of my head, I can't think of an EU country that does not have some form of advance voting.&lt;/p&gt;
    &lt;p&gt;Here in Latvia the "election day" is usually (always?) on weekend, but the polling stations are open for some (and different!) part of every weekday leading up. Something like couple hours on monday morning, couple hours on tuesday evening, couple around midday wednesday, etc. In my opinion, it's a great system. You have to have a pretty convoluted schedule for at least one window not to line up for you.&lt;/p&gt;
    &lt;p&gt;I think they meant "don't have it" as in except in special circumstances, and that form says:&lt;/p&gt;
    &lt;p&gt;&amp;gt; You may use this form to apply for a postal vote if, due to the circumstances of your work/service or your full-time study in the State, you cannot go to your polling station on polling day.&lt;/p&gt;
    &lt;p&gt;Which seems to indicate that's only for people who can't go to the polling station, otherwise you do have to go there.&lt;/p&gt;
    &lt;p&gt;I think that a lot of Ireland's voting practices come from having a small population but a huge diaspora. I imagine the percentage of people living outside Ireland what would be eligible to vote in many other countries is significant enough to effect elections, certainly if they are close.&lt;/p&gt;
    &lt;p&gt;As someone who spent the first 30 years of my life in Ireland but is now part of that diaspora, it's frustrating but I get it. I don't get to vote, but neither do thousands of plastic paddys who have very little genuine connection to Ireland.&lt;/p&gt;
    &lt;p&gt;That said, I'm sure they could expand the voting window to a couple of days at least without too much issue.&lt;/p&gt;
    &lt;p&gt;Italy has mail-in vote only for citizen residing abroad. The rest vote on the election Sunday (and Monday morning in some cases, at least in the past).&lt;/p&gt;
    &lt;p&gt;You don't have to attribute any name to the transaction, just a voting booth ID and the vote. The actual benefit is just that it is hard to tamper and easy to trace where tampering happened.&lt;/p&gt;
    &lt;p&gt;But I still prefer the paper vote and I usually a blockchain apathetic.&lt;/p&gt;
    &lt;p&gt;Anonymous voting means that you can't sell your vote. Like, if I pay you $5 to vote for X, but I can't actually verify that you voted for X and not Y, then I wouldn't bother trying. Or if I'm your boss and I want you to vote for X... etc.&lt;/p&gt;
    &lt;p&gt;Washington State having full vote-by-mail (there is technically a layer of in-person voting as a fallback for those who need it for accessibility reasons or who missed the registration deadline) has spoiled me rotten, I couldn't imagine having to go back to synchronous on-site voting on a single day like I did in Illinois. Awful. Being able to fill my ballot at my leisure, at home, where I can have all the research material open, and drive it to a ballot drop box whenever is convenient in a 2-3 week window before 20:00 on election night, is a game-changer for democracy. Of course this also means that people who serve to benefit from disenfranchising voters and making it more difficult to vote, absolutely hate our system and continually attack it for one reason or another.&lt;/p&gt;
    &lt;p&gt;As a Dutchman, I have to go vote in person on a specific day. But to be honest: I really don't mind doing so. If you live in a town or city, there'll usually be multiple voting locations you can choose from within 10 minutes walking distance. I've never experienced waiting times more than a couple of minutes. Opening times are pretty good, from 7:30 til 21:00. The people there are friendly. What's not to like? (Except for some of the candidates maybe, but that's a whole different story. :-))&lt;/p&gt;
    &lt;p&gt;We're on year five of one of the two parties telling voters to not trust early voting. Their choice is because of the Fear, Uncertainty, and Doubt created by the propaganda they are fed.&lt;/p&gt;
    &lt;p&gt;"No mail-in or 'Early' Voting, Yes to Voter ID! Watch how totally dishonest the California Prop Vote is! Millions of Ballots being 'shipped.' GET SMART REPUBLICANS, BEFORE IT IS TOO LATE!!!"&lt;/p&gt;
    &lt;p&gt;That's all happening too, but it's honestly a different topic altogether. We have the ability to vote early. Whether you trust it or politicians are trying to undermine your trust in it, etc.... whole other can of worms&lt;/p&gt;
    &lt;p&gt;Please lookup US voting poll overflow issues that come up every election cycle. Just because you experience a well streamlined process doesn't mean that it's the norm everywhere.&lt;/p&gt;
    &lt;p&gt;So, if you have a minor emergency, like a kidney stone and hospitalized for the day - you just miss your chance to vote in that election?&lt;/p&gt;
    &lt;p&gt;If so, I see a lot to dislike. As the point I was making is you can‚Äôt anticipate what might come up. Just because it‚Äôs worked thus far doesn‚Äôt mean it‚Äôs designed for resilience. There‚Äôs a lot of ways you could miss out in that type of situation. I seems silly to make sure everything else is redundant and fault tolerant in the name of democracy when the democratic process itself isn‚Äôt doing the same.&lt;/p&gt;
    &lt;p&gt;How is that an acceptable response? Honestly. You‚Äôre in the hospital, in pain, likely having a minor surgery, and having someone cast your vote for you is going to be on your mind too? Do you have your voting card in your pocket just in case this were to play out?&lt;/p&gt;
    &lt;p&gt;That‚Äôs just ridiculous in my opinion. Makes me wonder how many well intentioned would be voters end up missing out each election cause shit happens and voting is pretty optional&lt;/p&gt;
    &lt;p&gt;Mild curiosity, no idea whether it would be statistically relevant but asking the question is the first step. If you knew the answer, you might want to extend the voting window even if it wouldn't effect an elections outcome it would be a quantified number of people excluded from the democratic process for simply having bad luck at the wrong time.&lt;/p&gt;
    &lt;p&gt;If India can have voters vote and tally all the votes in one day, then so can everyone else. It‚Äôs the best way to avoid fraud and people going with whoever is ahead. I am sympathetic with emergency protocols for deadly pandemics, but for all else, in-person on a given day.&lt;/p&gt;
    &lt;p&gt;&amp;gt; If India can have voters vote and tally all the votes in one day, then so can everyone else.&lt;/p&gt;
    &lt;p&gt;In most countries, in the elections you vote or the member of parliament you want. Presidential elections, and city council elections are held separately, but are also equally simple. But in one election you cast your vote for one person, and that's it.&lt;/p&gt;
    &lt;p&gt;With this kind of elections, many countries manage to hold the elections on paper ballots, count them all by hand, and publish results by midnight.&lt;/p&gt;
    &lt;p&gt;But on an American ballot, you vote for, for example:&lt;/p&gt;
    &lt;p&gt;- US president - US senator - US member of congress - state governor - state senator - state member of congress - several votes for several different state judge positions - several other state officer positions - several votes for several local county officers - local sheriff - local school board member - several yes/no votes for several proposed laws, whether they should be passed or not&lt;/p&gt;
    &lt;p&gt;I don't think it would be possible to calculate all these 20 or 40 votes, if calculated by hand. That's why they use voting machines in America.&lt;/p&gt;
    &lt;p&gt;Say, how many voting stations are there in a typical city/county in the US?&lt;/p&gt;
    &lt;p&gt;Here in Indonesia, in a city of 2 million people there are over 7000 voting stations. While we vote for 5 ballots (President, Legislative (National, Province, and City/Regency), we still use paper ballots and count them by hand.&lt;/p&gt;
    &lt;p&gt;How is it not possible? It's just additional votes, there isn't anything actually stopping counting by hand, is there? How was it counted historically without voting machines?&lt;/p&gt;
    &lt;p&gt;If it's not a national holiday where the vast majority of people don't have to work, and if there aren't polling places reasonably near every voting age citizen, it's voter suppression.&lt;/p&gt;
    &lt;p&gt;In particular India has a law that no one shall be made to walk more than 2km to vote. The Indian military will literally deploy a voting booth into the jungle so that a single caretaker of an old temple can vote.&lt;/p&gt;
    &lt;p&gt;Finance is increasingly reliant on it too, my bank moved their entire system to AWS. The amount of power being handed over to these cloud companies in exchange for ‚Äúconvenience‚Äù is astonishing.&lt;/p&gt;
    &lt;p&gt;It should be noted that the article isn't complete: while the travel planner and ticket machines were the first to fail, trains were cancelled soon after; it took a few hours before everything restarted.&lt;/p&gt;
    &lt;p&gt;Based on what the conductors said, I would speculate that the train drivers digital schedule was not operative, so they didn't know where to go next.&lt;/p&gt;
    &lt;p&gt;I don't find a detailed statistic on the overall delays, but the per-station statistics for Amsterdam Centraal say 5% of trains were cancelled and 17% were delayed by 5 minutes or more (mostly by 10 minutes): https://www.rijdendetreinen.nl/en/train-archive/2025-10-29/a...&lt;/p&gt;
    &lt;p&gt;Here in Belgium voting is usually done during the weekend, although it shouldn't matter because voting is a civic duty (unless you have a good reason you have to go vote or you'll be fined), so those who work during the weekend have a valid reason to come in late or leave early.&lt;/p&gt;
    &lt;p&gt;In the US, where I assume a lot of the griping comes from, election day is not a national holiday, nor is it on a weekend (in fact, by law it is defined as "the Tuesday next after the first Monday in November"), and even though it is acknowledged as an important civic duty, only about half of the states have laws on the books that require employers provide time off to vote. There are no federal laws to that effect, so it's left entirely to states to decide.&lt;/p&gt;
    &lt;p&gt;In Australia there are so many places to vote, it is almost popping out to get milk level if convenience. Just detour your dog walk slightly. Always at the weekend.&lt;/p&gt;
    &lt;p&gt;In Australia there are so many places to vote, it is almost popping out to get milk level if convenience. (At least in urbia and suburbia) Just detour your dog walk slightly. Always at the weekend.&lt;/p&gt;
    &lt;p&gt;In the US getting milk involves driving multiple miles, finding parking, walking to the store, finding a shopping cart, finding the grocery department, navigating the aisles to the dairy section, finding the milk, waiting in line to check out, returning the cart if you‚Äôre courteous, and driving back. Could take an hour or so.&lt;/p&gt;
    &lt;p&gt;In washington we have a 100% mail-in voting system (for all intents and purposes). I can put my ballot back in the mail or drop at any number of drop-boxes throughout the city (less dropboxes in rural areas i'm sure). I think there are some allowances for in-person voting but I don't think they are often used.&lt;/p&gt;
    &lt;p&gt;There is a ballot tracking system as well, I can see and be notified as my ballot moves through the counting system. It's pretty cool.&lt;/p&gt;
    &lt;p&gt;I actually just got back from dropping off my local elections ballot 15m ago, quick bike trip maybe a mile or so away and back.&lt;/p&gt;
    &lt;p&gt;Of course, because it makes it easy for people to vote, the republicans want to do away with it. If you have to stand in line for several hours (which seems to be very normal in most cities) and potentially miss work to do it that's going to all but guarantee that working people and the less motivated will not vote.&lt;/p&gt;
    &lt;p&gt;So yes in places that only do in person voting, national or state holiday.&lt;/p&gt;
    &lt;p&gt;Yet... deploy on two clouds and you'll get tax payers scream at you for "wasting money" preparing for a black swan event. Can't have both, either reliability or lower cost.&lt;/p&gt;
    &lt;p&gt;Organizations who had their own datacenters were chided for being resistant to modernizing, and now they modernized to use someone else's shared computers and they stopped working.&lt;/p&gt;
    &lt;p&gt;I really do feel the only viable future for clouds is hybrid or agnostic clouds.&lt;/p&gt;
    &lt;p&gt;i'm not sure this is an easily solvable problem. i remember reading an article arguing that your cloud provider is part of your tech stack and it's close to impossible/a huge PITA to make a non-trivial service provider-agnostic. they'd have to run their own openstack in different datacenters, which would be costly and have their own points of failure.&lt;/p&gt;
    &lt;p&gt;I run non trivial services on EC2, using that service as a VPS. My deploy script works just as well on provisioned Digital Ocean services and on docker containers using docker-compose.&lt;/p&gt;
    &lt;p&gt;I do need a human to provision a few servers and configure e.g. load balancing and when to spin up additional servers under load. But that is far less of a PITA than having my systems tied to a specific provider or down whenever a cloud precipitates.&lt;/p&gt;
    &lt;p&gt;The moment you choose to use S3 instead of hosting your own object store, though, you either use AWS because S3 and IAM already have you or spend more time on the care and feeding of your storage system as opposed to actually doing the thing you customers are paying you to do.&lt;/p&gt;
    &lt;p&gt;It's not impossible, just complicated and difficult for any moderately complex architecture.&lt;/p&gt;
    &lt;p&gt;dang even zealand didn't survive! new zealand got some soul searching with this outage which took down government person ID service, it's called RealME and it can be used to file your taxes apply for passport etc&lt;/p&gt;
    &lt;p&gt;The Flemish bus company (de Lijn) uses Azure and I couldn't activate my ticket when I came home after training a couple of hours ago. I should probably start using physical tickets again, because at least those work properly. It's just stupid that there's so much stuff being moved to digital only (often even only being accessible through an Android or iOS app, despite the parent companies of those two being utterly atrocious) when the physical alternatives are more reliable.&lt;/p&gt;
    &lt;p&gt;can't believe it's 2025 and some still need to go to some place to vote. I can vote since I can remember(at least 20 years) by mail for anything, we also vote multiple times a year(4-6 times), we just get 1 Month before the things to vote by mail and then mail in back votes. Hope we can soon vote online to get rid of the paper overhead.&lt;/p&gt;
    &lt;p&gt;Personally I am thinking more and more about hetzner, yes I know its not an apples to orange comparison. But its honestly so good&lt;/p&gt;
    &lt;p&gt;Someone had created a video where they showed the underlying hardware etc., I am wondering if there is something like https://vpspricetracker.com/ but with geek-benchmarks as well.&lt;/p&gt;
    &lt;p&gt;This video was affiliated with scalahosting but still I don't think that there was too much bias of them and they showed at around 3:37 a graph comparison with prices https://www.youtube.com/watch?v=9dvuBH2Pc1g&lt;/p&gt;
    &lt;p&gt;Now it shows how contabo has better hardware but I am pretty sure that there might be some other issues, and honestly I feel a sense of trust with hetzner I am not sure about others.&lt;/p&gt;
    &lt;p&gt;Either hetzner or self hosting stuff personally or just having a very cheap vps and going to hetzner if need be but hetzner already is pretty cheap or I might use some free service that I know of are good as well.&lt;/p&gt;
    &lt;p&gt;Probably not, but at least you don‚Äôt delude yourself into thinking reliability is a solved problem just because you‚Äôre paying through the nose for compute and storage.&lt;/p&gt;
    &lt;p&gt;One of recent (4 months ago) Cloudflare outages (I think it was even workers) was caused by Google Cloud being down and Cloudflare hosting an essential service there&lt;/p&gt;
    &lt;p&gt;Hm it seemed that they hosted a critical service for cloudflare kv on google itself, but I wonder about the update.&lt;/p&gt;
    &lt;p&gt;Personally I just trust cloudflare more than google, given how their focus is on security whereas google feels googly...&lt;/p&gt;
    &lt;p&gt;I have heard some good things about google cloud run and the google's interface feels the best out of AWS,Azure,GCloud but I still would just prefer cloudflare/hetzner iirc&lt;/p&gt;
    &lt;p&gt;Another question: Has there ever been a list of all major cloud outages, like I am interested how many times google cloud and all cloud providers went majorly down I guess y'know? is there a website/git project that tracks this?&lt;/p&gt;
    &lt;p&gt;For some reason an Azure outage does not faze me in the same way that an AWS outage does.&lt;/p&gt;
    &lt;p&gt;I have never had much confidence in Azure as a cloud provider. The vertical integration of all the things for a Microsoft shop was initially very compelling. I was ready to fight that battle. But, this fantasy was quickly ruined by poor execution on Microsoft's part. They were able to convince me to move back to AWS by simply making it difficult to provision compute resources. Their quota system &amp;amp; availability issues are a nightmare to deal with compared to EC2.&lt;/p&gt;
    &lt;p&gt;At this point I'd rather use GCP over Azure and I have zero seconds of experience with it. The number of things Microsoft gets right in 2025 can be counted single-handedly. The things they do get right are quite good, but everything else tends to be extremely awful.&lt;/p&gt;
    &lt;p&gt;The "Blades" experience [0] where instead of navigating between pages it just kept opening things to the side and expanding horizontally?&lt;/p&gt;
    &lt;p&gt;Yeah, that had some fun ideas but was way more confusing than it needed to be. But also that was quite a few years back now. The Portal ditched that experience relatively quickly. Just long enough to leave a lot of awful first impressions, but not long enough for it to be much more than a distant memory at this point, several redesigns later.&lt;/p&gt;
    &lt;p&gt;[0] The name "Blades" for that came from the early years of the Xbox 360, maybe not the best UX to emulate for a complex control panel/portal.&lt;/p&gt;
    &lt;p&gt;Azure to me has always suffered from a belief that ‚ÄúUI innovations can solve UX complexity if you just try hard enough.‚Äù&lt;/p&gt;
    &lt;p&gt;Like, AWS, and GCP to a lesser extent, has a principled approach where simple click-ops goals are simple. You can access the richer metadata/IAM object model at any time, but the wizards you see are dumb enough to make easy things easy.&lt;/p&gt;
    &lt;p&gt;With Azure, those blades allow tremendously complex ‚Äúyou need to build an X Container and a Container Bucket to be able to add an X‚Äù flows to coexist on the same page. While this exposes the true complexity, and looks cool/works well for power users, it is exceedingly unintuitive. Inline documentation doesn‚Äôt solve this problem.&lt;/p&gt;
    &lt;p&gt;I sometimes wonder if this is by design: like QuickBooks, there‚Äôs an entire economy of consultants who need to be Certified and thus will promote your product for their own benefit! Making the interface friendly to them and daunting to mere mortals is a feature, not a bug.&lt;/p&gt;
    &lt;p&gt;But in Azure‚Äôs case it‚Äôs hard to tell how much this is intentional.&lt;/p&gt;
    &lt;p&gt;I still feel lost just trying to view my application logs.&lt;/p&gt;
    &lt;p&gt;I don't want to pay for or lock myself into, "Azure Insights".&lt;/p&gt;
    &lt;p&gt;I just want to see the logging, that I know if I can remember the right buttons to click, are available.&lt;/p&gt;
    &lt;p&gt;The worst place to try is "Monitoring &amp;gt; Logs", this is where you get faced up front with a query designer. I've never worked out how to do a simple "list by time" on that query designer, but it doesn't matter, because if you suffer through that UX, you find out that's not actually where the logs are anyway.&lt;/p&gt;
    &lt;p&gt;You have to go down a different path. Don't be distracted by "Log Stream", that's not it either, it sounds useful but it's not. By default it doesn't log anything. If you do configure it to log, then it still doesn't actually log everything.&lt;/p&gt;
    &lt;p&gt;What you have to actually do, and I've had to open the portal to check this, is click "Diagnose and Solve Problems" and then look for "Diagnostic tools" and then a small link to "Application Event Logs".&lt;/p&gt;
    &lt;p&gt;Finally you get to your logs, although it's still a bad way to try to view logs, it's at least marginally better than the real windows event viewer, an application that feels like it hasn't been updated since NT4. ( Although some might suggest that's a good thing. )&lt;/p&gt;
    &lt;p&gt;(I think that's from near the transition because it has full "windowing" controls of minimize/maximize/close buttons. I recall a period with only close buttons.)&lt;/p&gt;
    &lt;p&gt;All that blue space you could keep filling with more "blades" as you clicked on things until the entire page started scrolling horizontally to switch between "blades". Almost everything you could click opened in a new blade rather than in place in the existing blade. (Like having "Open in New Window" as your browser default.)&lt;/p&gt;
    &lt;p&gt;It was trying to merge the needs of a configurable Dashboard and a "multi-window experience". You could save collections of blades (a bit like Niri workspaces) as named Dashboards. Overall it was somewhere between overkill and underthought.&lt;/p&gt;
    &lt;p&gt;(Also someone reminded me that many "blades" still somewhat exist in the modern Portal, because, of course, Microsoft backwards compatibility. Some of the pages are just "maximized Blades" and you can accidentally unmaximize them and start horizontally scrolling into new blades.)&lt;/p&gt;
    &lt;p&gt;azure likes to open new sections on the same tab / page as opposed to reloading or opening a new page / tab (overlays? modals? I'm lost on graphic terms)&lt;/p&gt;
    &lt;p&gt;depending on the resource you're accessing, you can get 5+ sections each with their own ui/ux on the same page/tab and it can be confusing to understand where you're at in your resources&lt;/p&gt;
    &lt;p&gt;if you're having trouble visualizing it, imagine an url where each new level is a different application with its own ui/ux and purpose all on the same webpage&lt;/p&gt;
    &lt;p&gt;AWS' UI is similarly messy, and to this day. They regularly remove useful data from the UI, or change stuff like the default sort order of database snapshots from last created to initial instance created date.&lt;/p&gt;
    &lt;p&gt;I never understood why a clear and consistent UI and improved UX isn't more of a priority for the big three cloud providers. Even though you talk mostly via platform SDK's, I would consider better UI especially initially, a good way to bind new customers and pick your platform over others.&lt;/p&gt;
    &lt;p&gt;I guess with their bottom line they don't need it (or cynically, you don't want to learn and invest in another cloud if you did it once).&lt;/p&gt;
    &lt;p&gt;It‚Äôs more than just the UI itself (which is horrible), it‚Äôs the whole thing that is very hostile to new users even if they‚Äôre experienced. It‚Äôs such an incoherent mess. The UI, the product names, the entire product line itself, with seemingly overlapping or competing products‚Ä¶ and now it‚Äôs AI this and AI that. If you don‚Äôt know exactly what you‚Äôre looking for, good luck finding it. It‚Äôs like they‚Äôre deliberately trying to make things as confusing as possible.&lt;/p&gt;
    &lt;p&gt;For some reason this applies to all AWS, GCP and Azure. Seems like the result of dozens of acquisitions.&lt;/p&gt;
    &lt;p&gt;I still find it much easier to just self host than learn cloud and I‚Äôve tried a few times but it just seems overly complex for the sake of complexity. It seems they tie in all their services to jack up charges, eg. I came for S3 but now I‚Äôm paying for 5 other things just to get it working.&lt;/p&gt;
    &lt;p&gt;Any time something is that unintuitive to get started, I automatically assume that if I encounter a problem that I‚Äôll be unable to solve it. That thought alone leads me to bounce every time.&lt;/p&gt;
    &lt;p&gt;100% agree. I've been working in the industry for almost 20 years, I'm a full stack developer and I manage my servers. I've tried signing up for AWS and I noped out.&lt;/p&gt;
    &lt;p&gt;AWS Is a complete mess. Everything is obscured behind other products, and they're all named in the most confusing way possible.&lt;/p&gt;
    &lt;p&gt;GCP console is not the best but as a long term multicloud user, I can assure you that GCP is much better than Azure portal and/or Azure APIs which is fucking hell&lt;/p&gt;
    &lt;p&gt;Cloud Run is incredible. It‚Äôs one of those things I wish more devs knew about. Even at work where we use GCP all the ‚Äúsmart‚Äù devs insist on GKE for their ‚Äúwebscale‚Äù services that get dozens of requests a second. Dozens!&lt;/p&gt;
    &lt;p&gt;I know for some people the prospect of losing their Google Cloud access due to an automated terms of service violation on some completely unrelated service is worrisome.&lt;/p&gt;
    &lt;p&gt;I'd hope you can create a Google Cloud account under a completely different email address, but I do as little business with Google as I can get away with, so I have no idea.&lt;/p&gt;
    &lt;p&gt;That's generally speaking a good practice anyways. My Amazon shopping account has a different email than my Amazon Web Services account. I intuited that they need to be different from the get go.&lt;/p&gt;
    &lt;p&gt;Microsoft has the regulatory capture. All the European privacy and regulatory laws are good for Azure. That's why your average European government or baking app runs most likely on Azure. (or Oracle, but more likely Azure)&lt;/p&gt;
    &lt;p&gt;The problem is that in some industries, Microsoft is the only option. Many of these regulated industries are just now transitioning from the data center to the cloud, and they've barely managed to get approval for that with all of the Microsoft history in their organization. AWS or GCloud are complete non-starters.&lt;/p&gt;
    &lt;p&gt;I moved a 100% MS shop to AWS circa 2015. We ran our DCs on EC2 instances just as if they were on prem. At some point we installed the AAD connector and bridged some stuff to Azure for office/mail/etc., but it was all effectively in AWS. We were selling software to banks so we had a lot of due diligence to suffer. AWS Artifact did much of the heavy lifting for us. We started with Amazon's compliance documentation and provided our own feedback on top where needed.&lt;/p&gt;
    &lt;p&gt;I feel like compliance is the entire point of using these cloud providers. You get a huge head start. Maintaining something like PCI-DSS when you own the real estate is a much bigger headache than if it's hosted in a provider who is already compliant up through the physical/hardware/networking layers. Getting application-layer checkboxes ticked off is trivial compared to "oops we forgot to hire an armed security team". I just took a look and there are currently 316 certifications and attestations listed under my account.&lt;/p&gt;
    &lt;p&gt;I've found that lift and shifting to EC2 is also generally cheaper than the equivalent VMs on Azure.&lt;/p&gt;
    &lt;p&gt;Microsoft really wants you to use their PaaS offerings, and so things on Azure are priced accordingly. A Microsoft shop just wanting to lift-and-shift, Azure isn't the best choice unless the org has that "nobody ever got fired for buying Microsoft" attitude.&lt;/p&gt;
    &lt;p&gt;Microsoft is better at regulatory capture, so Azure has many customers in the public sector. So an Azure outage probably affects the public sector more (see example above about trains).&lt;/p&gt;
    &lt;p&gt;What Amazon, Azure, and Google are showing with their platform crashes amid layoffs, while they supports governments that are Oppressing's Citizens and Ignoring the Law, is that they do not care about anything other than the bottom line.&lt;/p&gt;
    &lt;p&gt;They think they have the market captured, but I think what their dwindling quality and ethics are really going to drive is adoption of self hosting, distributed computing frameworks. Nerds are the ones who drove adoption of these platforms, and we can eventually end if we put in the work.&lt;/p&gt;
    &lt;p&gt;Seriously with container technology, and a bit more work / adoption on distributed compute systems and file storage (IPFS,FileCoin) there is a future where we dont have to use big brothers compute platform. Fuck these guys.&lt;/p&gt;
    &lt;p&gt;These were my thoughts exactly. I may have my tinfoil hat on, but outages these close together between the largest cloud providers amid social unrest, my wonder is the government / tech companies implementing some update that adds additional spyware / blackout functionality.&lt;/p&gt;
    &lt;p&gt;I really hope this pushes the internet back to how it used to be, self hosted, privacy, anonymity. I truly hope that's where we're headed, but the masses seem to just want to stay comfortable as long as their show is on TV&lt;/p&gt;
    &lt;p&gt;All I'm saying Its a complete 360 from the marketing they did to establish their brand... you know, save the world kind of stuff, especially from google.&lt;/p&gt;
    &lt;p&gt;From 2000-2016 most tech marketing\branding was aimed at some kind of social benefit.&lt;/p&gt;
    &lt;p&gt;At least some bits of it do. I was writing something to pull logs the other day and the redirect was to an azure bucket. It also returned a 401 with the valid temporary authed redirect in the header. I was a bit worried I'd found a massive security hole but it appears after some testing it just returned the wrong status code.&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door. But it meant that when the system came back online, if the customers card was denied, the customer got free groceries.&lt;/p&gt;
    &lt;p&gt;Yea, good old store and forward. We implemented it in our PoS system. Now, we do non PCI integrations so we arn't in PCI scope, but depending on the processor, it can come with some limitations. Like, you can do store and forward, but only up to X number of transactions. I think for one integration, it's 500-ish store wide (it uses a local gateway that store and forwards to the processors gateway). The other integration we have, its 250, but store and forward on device, per device.&lt;/p&gt;
    &lt;p&gt;In many places it's also possibly just a left over feature from older times. I worked at a major UK supermarket in the mid-00s, and their checkout system had this feature. But it was like that because that's how it was originally built, it wasn't a 'feature' they added.&lt;/p&gt;
    &lt;p&gt;Credit card information would be recorded by the POS, synced to a mini-server in the back office (using store-and-forward to handle network issues) and then in a batch process overnight, sent to HQ where the payment was processed.&lt;/p&gt;
    &lt;p&gt;It wasn't until chip-and-PIN was rolled out that they started supporting "online" (i.e. processed then and there) card transactions, and even then the old method still worked if there was a network issues or power failure (all POSes has their own UPS).&lt;/p&gt;
    &lt;p&gt;The only real risk at the time was that someone tried to pay with a cancelled credit card - the bank would always honour the payment otherwise. But that was pretty uncommon back then, as you'd have to phone your bank to do it, not just press a button in an app.&lt;/p&gt;
    &lt;p&gt;IIRC, the grocery chain I worked for used to have an offline mode to move customers out the door.&lt;/p&gt;
    &lt;p&gt;Chick-fil-a has this.&lt;/p&gt;
    &lt;p&gt;One of the tech people there was on HN a few years ago describing their system. Credit card approval slows down the line, so the cards are automatically "approved" at the terminal, and the transaction is added to a queue.&lt;/p&gt;
    &lt;p&gt;The loss from fraudulent transactions turns out to be less than the loss from customers choosing another restaurant because of the speed of the lines.&lt;/p&gt;
    &lt;p&gt;The POS I work on also has this feature. Line busters take the order and payment but we have a toggle where you can immediately ‚Äúapprove‚Äù and queue it up. If the payment fails then the person handing you your food will see it on the order and ask you for alternative payment. It helps prevent loss and speeds up the line overall.&lt;/p&gt;
    &lt;p&gt;I was shopping at a mall with a visa vanilla card once. I got it as a gift and didn't know the limit. No matter what I bought the card kept going -- and I never got a balance of what was on the card. Eventually, later that day it stopped. I called customer support and asked how much was left on the balance. They told me they had no idea my balance - but everything I bought was mine.&lt;/p&gt;
    &lt;p&gt;I remember that banks will try to honor the transactions, even if the customer's balance/credit limit is exhausted. It doesn't apply only to some gift cards.&lt;/p&gt;
    &lt;p&gt;There's a Family Dollar by my house that is down at least 2 full days per month because of bad inet connectivity. I live close enough that with a small tower on my roof i can get line of sight to theirs. I've thought about offering them a backup link off my home inet if they give me 50% of sales whenever its in use. It would be a pretty good deal for them, better some sales when their inet is down vs none.&lt;/p&gt;
    &lt;p&gt;It's Family Dollar, margin has to be almost nothing and sales per day is probably &amp;lt; $1k. That's why I said 50% of sales and not profit.&lt;/p&gt;
    &lt;p&gt;I go there daily because it's a nice 30min round trip walk and I wfh. I go up there to get a diet coke or something else just to get out of the house. It amazes me when i see a handwritten sign on the door "closed, system is down". I've gotten to know the cashiers so I asked and it's because the internet connection goes down all the time. That store has to one of the most poorly run things i've ever seen yet it stays in business somehow.&lt;/p&gt;
    &lt;p&gt;I think the point people are trying and failing to make is that asking for half of means sales is half of revenue not half of net and that you‚Äôre out of your goddamned mind if you think a store with razor thin margins would sell at a massive loss rather than just close due to connectivity problems.&lt;/p&gt;
    &lt;p&gt;Your responses imply that you think people are questioning whether you would lose money on the deal while we are instead saying you‚Äôll get laughed out of the store, or possibly asked never to come back.&lt;/p&gt;
    &lt;p&gt;Unfortunately they are largely corporate, which is how they can sell items for such a cheap price. The store manager probably has zero say in nearly anything. Even if they wanted to "break the rules," I doubt they could make use of your connection as a backup, but I've also worked for smaller companies that were able to sell internet access to individual locations like Denny's and various large hotels in the US. Being able to somehow share sales would be the difficult part, since all sales are reported back to corporate.&lt;/p&gt;
    &lt;p&gt;Good luck if you make this work for you, it would be exciting to hear about if you're able to get them to work with you.&lt;/p&gt;
    &lt;p&gt;2-3%, bit higher on perishables. Though i'd just ask lump sum payments in cash since it likely has to no go through corporate (as in, avoid the corporation).&lt;/p&gt;
    &lt;p&gt;You'd think any SeriousBusiness would have a backup way to take customers' money. This is the one thing you always want to be able to do: accept payment. If they made it so they can't do that, they deserve the hit to their revenue. People should just walk out of the store with the goods if they're not being charged.&lt;/p&gt;
    &lt;p&gt;Why doesn't someone in the store at least have one of those manual kachunk-kachunk carbon copy card readers in the back that they can resuscitate for a few days until the technology is turned back on? Did they throw them all away?&lt;/p&gt;
    &lt;p&gt;The kachunk-kachunk credit card machines need raised digits on the cards, and I don't think most banks have been issuing those for years at this point. Mine have been smooth for at least 10 years.&lt;/p&gt;
    &lt;p&gt;My card tied to my main financial institution have the raised digits, but most cards you'd sign up for online now no longer have the raised digits (and often allow you to select art to appear on the card face).&lt;/p&gt;
    &lt;p&gt;I think a lot of payment terminals have an option to record transactions offline and upload them later, but apparently it's not enabled by default - probably because it increases your risk that someone pays with a bad card.&lt;/p&gt;
    &lt;p&gt;If they used standalone merchant terminals, then those typically use the local LAN which can rollover to cellular or PoT in the event of a network outage. The store can process a card transaction with the merchant terminal and then reconcile with the end of day chit. This article from 2008 describes their PoS https://www.retailtouchpoints.com/topics/store-operations/ca...&lt;/p&gt;
    &lt;p&gt;These stores appear everywhere, even in areas with high income. You'd be surprised, but often people with those high incomes shop for certain products at very low rates, and that's how they keep their savings. A good example is garbage bags. Most people don't care too much about the quality of their garbage bags, unless they rip on the way to the bin.&lt;/p&gt;
    &lt;p&gt;In Germany many stores still accept cash and some even only accept cash and we are ridiculed for this... Seems like one of the rare instances where this is useful :D&lt;/p&gt;
    &lt;p&gt;It's sad the number of stores I've seen where they just shut down when they can't use the checkout machines; the clerks aren't allowed to do math even if they could.&lt;/p&gt;
    &lt;p&gt;Whereas the smaller, owner-run stores have more leeway; the local tiny grocery "sold" all freezer/refrigerator food for cheap/free during a power failure. The big Walmart closed and threw everything away the next day.&lt;/p&gt;
    &lt;p&gt;Just to add - this particular supermarket wasn‚Äôt fully down, it took ages for them to press ‚Äúsub total‚Äù and then pick the payment method. I suspect it was slow waiting for a request to timeout perhaps&lt;/p&gt;
    &lt;p&gt;I remember last mechanical cash registers in my country in 90s and when these got replaced by early electronic ones with blue vacuum fluorescent tubes. Then everything got smaller and smaller. Now I'm pestered to "add the item to the cart" by software.&lt;/p&gt;
    &lt;p&gt;Last week I couldn't pay for flowers for grandma's grave because smartphone-sized card terminal refused to work - it stuck on charging-booting loop so I had to get cash. Tho my partner thinks she actually wanted to get cash without a receipt for herself excluding taxes&lt;/p&gt;
    &lt;p&gt;Most retailers trust their cashiers a bit less than they trust the customers. They'd rather shut down during a power/Internet failure than give any autonomy to the worker drones.&lt;/p&gt;
    &lt;p&gt;You can, but it's all about risk mitigation. Most processors have some form of store and forward (and it can have limitations like only X number of transactions). Some even have controls to limit the amount you can store-and-forward (for instance, only charges under $50). But ultimately, it's still risk mitigation. You can store-and-forward, but you're trusting that the card/account has the funds. If it doesn't, you loose and ain't shit you can do about it. If you can't tolerate any risk, you don't turn on store and forward systems and then you can't process cards offline.&lt;/p&gt;
    &lt;p&gt;Its not the we are not capable. Its, is the business willing to assume the risk?&lt;/p&gt;
    &lt;p&gt;Currently standing in a half closed supermarket because the tills are down and they cant take payments&lt;/p&gt;
    &lt;p&gt;There's a fairly large supermarket near me that has both kinds of outages.&lt;/p&gt;
    &lt;p&gt;Occasionally it can't take cards because the (fiber? cable?) internet is down, so it's cash only.&lt;/p&gt;
    &lt;p&gt;Occasionally it can't take cash because the safe has its own cellular connection, and the cell tower is down.&lt;/p&gt;
    &lt;p&gt;I was at Frank's Pizza in downtown Houston a few weeks ago and they were giving slices of pizza away because the POS terminal died, and nobody knew enough math to take cash. I tried to give them a $10 and told them to keep the change, but "keep the change" is an unknown phrase these days. They simply couldn't wrap their brains around it. But hey, free pizza!&lt;/p&gt;
    &lt;p&gt;We are very dependent on Azure and Microsoft Authentication and Microsoft 365 and haven‚Äôt had weekly or even monthly issues. I can think of maybe three issues this year.&lt;/p&gt;
    &lt;p&gt;I‚Äôve been migrating our services off of Azure slowly for the past couple of years. The last internet facing things remaining are a static assets bucket and an analytics VM running Matomo. Working with Front Door has been an abysmal experience, and today was the push I needed to finally migrate our assets to Cloudflare.&lt;/p&gt;
    &lt;p&gt;I feel pretty justified in my previous decisions to move away from Azure. Using it feels like building on quicksand‚Ä¶&lt;/p&gt;
    &lt;p&gt;We‚Äôve been experimenting with multi-cluster failover for Kubernetes workloads, and one open-source project that actually works really well is k8gb .&lt;/p&gt;
    &lt;p&gt;It acts as a GSLB controller inside Kubernetes ‚Äî doing DNS-level health checks, region awareness, and automatic failover between clusters when one goes down.&lt;/p&gt;
    &lt;p&gt;It integrates with ExternalDNS and supports multiple DNS providers (Infoblox, Route53, Azure DNS, NS1, etc.), so it can handle failover across both on-prem and cloud clusters.&lt;/p&gt;
    &lt;p&gt;It‚Äôs not a silver bullet for every architecture, but it‚Äôs one of the few OSS projects that make multi-region failover actually manageable in practice.&lt;/p&gt;
    &lt;p&gt;I have had intermittent issues with winget today. I use UniGetUI for a front-end, and anything tied to Microsoft has failed for me. Judging by the logs, it's mostly retrieving the listing of versions (I assume similar to what 'apt-get update' does, I'm fairly new to using winget for Windows package management).&lt;/p&gt;
    &lt;p&gt;We‚Äôre 100% on Azure but so far there‚Äôs no impact for us.&lt;/p&gt;
    &lt;p&gt;Luckily, we moved off Azure Front Door about a year ago. We‚Äôd had three major incidents tied to Front Door and stopped treating it as a reliable CDN.&lt;/p&gt;
    &lt;p&gt;They weren‚Äôt global outages, more like issues triggered by new deployments. In one case, our homepage suddenly showed a huge Microsoft banner about a ‚Äúpost-quantum encryption algorithm‚Äù or something along those lines.&lt;/p&gt;
    &lt;p&gt;Kinda wild that a company that big can be so shaky on a CDN, which should be rock solid.&lt;/p&gt;
    &lt;p&gt;We battled https://learn.microsoft.com/en-us/answers/questions/1331370/... for over a year, and finally decided to move off since there was no any resolution. Unfortunately our API servers were still behind AFD so they were affected by today's stuff...&lt;/p&gt;
    &lt;p&gt;Pretty much every single Microsoft domain I've tried to access loads for a looooong time before giving me some bare html. I wonder if someone can explain why that's happening.&lt;/p&gt;
    &lt;p&gt;it took a good half hour after we detected the problem to see a notification on the Azure status page. Thanks to those who responded to my question as it validated the issue was global and we contacted our users t right away&lt;/p&gt;
    &lt;p&gt;And querying https://www.microsoft.com/ results in HTTP 200 on the root document, but the page elements return errors (a 504 on the .css/.js documents, a 404 on some fonts, Name Not Resolved on scripts.clarity.ms, Connection Timed Out on wcpstatic.microsoft.com and mem.gfx.ms). That many different kinds of errors is actually kind of impressive.&lt;/p&gt;
    &lt;p&gt;I'm gonna say this was a networking/routing issue. The CDN stayed up, but everything else non-CDN became unroutable, and different requests traveled through different paths/services, but each eventually hit the bad network path, and that's what created all the different responses. Could also have been a bad deploy or a service stopped running and there's different things trying to access that service in different ways, leading to the weird responses... but that wouldn't explain the failed DNS propagation.&lt;/p&gt;
    &lt;p&gt;I've been doing it since 1998 in my bedroom with a dual T1 (and on to real DCs later). While I've had some outages for sure it makes me feel better I am not that divergent in uptime in the long run vs big clouds.&lt;/p&gt;
    &lt;p&gt;They added a message at the same time as your comment:&lt;/p&gt;
    &lt;p&gt;"We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly."&lt;/p&gt;
    &lt;p&gt;We all need to move away from these big cloud providers. Two medium size smaller providers is enough.&lt;/p&gt;
    &lt;p&gt;-Cloudflare for R2 (object storage) and CDN (Fastly+backblaze also available). -Two VPS/Server providers with a decent reputation and mid-size (using a comparison site like https://serversearcher.com or look directly into people like Hetzner or latitude) -PlanetScale or Neon for database if you don't co-locate it, though better to use someone like digital ocean, vultr or latitude who offer databases too)&lt;/p&gt;
    &lt;p&gt;&amp;gt; We all need to move away from these big cloud providers.&lt;/p&gt;
    &lt;p&gt;But then who do we blame when things are down? If we manage our own infrastructure we have to stay late to fix it when it breaks instead of saying ‚Äúsorry, Microsoft, nothing we can do‚Äù and magically our clients accepting that‚Ä¶&lt;/p&gt;
    &lt;p&gt;Yeah just took down the prod site for one of our clients since we host the front-end out of their CDN. Just got wrapped up panic hosting it somewhere else for the past hour, very quickly reminds you about the pain of cookies...&lt;/p&gt;
    &lt;p&gt;Yep, even massive cloud providers slip up. Reminds me that depending on ‚Äúsomeone else‚Äôs infrastructure‚Äù still means losing access when things go sideways.&lt;/p&gt;
    &lt;p&gt;Pretty much all Azure services seem to be down. Their status page says it's only the portal since 16:00. It would be nice if these mega-companies could update their status page when they take down a large fraction of the Internet and thousands of services that use them.&lt;/p&gt;
    &lt;p&gt;Same playbook for AWS. When they admitted that Dynamo was inaccessible, they failed to provide context that their internal services are heavily dependent on Dynamo&lt;/p&gt;
    &lt;p&gt;It's only after the fact they are transparent about the impact&lt;/p&gt;
    &lt;p&gt;The Internet is supposed to be decentralized. The big three seem to have all the power now (Amazon, Microsoft, and Google) plus Cloudflare/Oracle.&lt;/p&gt;
    &lt;p&gt;How did we get here? Is it because of scale? Going to market in minutes by using someone else's computers instead of building out your own, like co-location or dedicated servers, like back in the day.&lt;/p&gt;
    &lt;p&gt;A lot of money and years of marketing the cloud as the responsible business decision led us here. Now that the cloud providers have vendor lock-in, few will leave, and customers will continue to wildly overpay for cloud services.&lt;/p&gt;
    &lt;p&gt;Not sure how the current situation is better. Being stranded with no way whatsoever to access most/all of your services sounds way more terrifying than regular issues limited to a couple of services at a time&lt;/p&gt;
    &lt;p&gt;&amp;gt; no way whatsoever to access most/all of your services&lt;/p&gt;
    &lt;p&gt;I work on a product hosted on Azure. That's not the case. Except for front door, everything else is running fine. (Front door is a reverse proxy for static web sites.)&lt;/p&gt;
    &lt;p&gt;The product itself (an iot stormwater management system) is running, but our customers just can't access the website. If they need to do something, they can go out to the sites or call us and we can "rub two sticks together" and bypass the website. (We could also bypass front door if someone twisted our arms.)&lt;/p&gt;
    &lt;p&gt;Most customers only look at the website a few times a year.&lt;/p&gt;
    &lt;p&gt;---&lt;/p&gt;
    &lt;p&gt;That being said, our biggest point of failure is a completely different iot vendor who you probably won't hear about on Hacker News when they, or their data networks, have downtime.&lt;/p&gt;
    &lt;p&gt;&amp;gt; Big Tech lobbying is riding the EU‚Äôs deregulation wave by spending more, hiring more, and pushing more, according to a new report by NGO‚Äôs Corporate Europe Observatory and LobbyControl on Wednesday (29 October).&lt;/p&gt;
    &lt;p&gt;&amp;gt; Based on data from the EU‚Äôs transparency register, the NGOs found that tech companies spend the most on lobbying of any sector, spending ‚Ç¨151m a year on lobbying ‚Äî a 33 percent increase from ‚Ç¨113m in 2023.&lt;/p&gt;
    &lt;p&gt;Gee whizz, I really do wonder how they end up having all the power!&lt;/p&gt;
    &lt;p&gt;I think the response lies in the surrounding ecosystem.&lt;/p&gt;
    &lt;p&gt;If you have a company it's easier to scale your team if you use AWS (or any other established ecosystem). It's way easier to hire 10 engineers that are competent with AWS tools than it is to hire 10 engineers that are competent with the IBM tools.&lt;/p&gt;
    &lt;p&gt;And from the individuals perspective it also make sense to bet on larger platforms. If you want to increase your odds of getting a new job, learning the AWS tools gives you a better ROI than learning the IBM tools.&lt;/p&gt;
    &lt;p&gt;But the cloud compute market is basically centralized into 2.5 companies at this point. The point of paying companies like Azure here is that they've in theory centralized the knowledge and know-how of running multiple, distributed datacenters, so as to be resilient.&lt;/p&gt;
    &lt;p&gt;But that we keep seeing outages encompassing more than a failure domain, then it should be fair game for engineers / customers to ask "what am I paying for, again?"&lt;/p&gt;
    &lt;p&gt;Moreover, this seems to be a classic case of large barriers to entry (the huge capital costs associated with building out a datacenter) barring new entrants into the market, coupled with "nobody ever got fired for buying IBM" level thinking. Are outages like these truly factored into the napkin math that says externalizing this is worth it?&lt;/p&gt;
    &lt;p&gt;Consolidation is the inevitable outcome of free unregulated markets.&lt;/p&gt;
    &lt;p&gt;In our highly interconnected world, decentralization paradoxically requires a central authority to enforce decentralization by restricting M&amp;amp;A, cartels, etc.&lt;/p&gt;
    &lt;p&gt;A natural monopoly is a monopoly in an industry in which high infrastructure costs and other barriers to entry relative to the size of the market give the largest supplier in an industry, often the first supplier in a market, an overwhelming advantage over potential competitors. Specifically, an industry is a natural monopoly if a single firm can supply the entire market at a lower long-run average cost than if multiple firms were to operate within it. In that case, it is very probable that a company (monopoly) or a minimal number of companies (oligopoly) will form, providing all or most of the relevant products and/or services.&lt;/p&gt;
    &lt;p&gt;The outage was really weird. For me, parts of the portal worked, other parts didn't. I had access to a couple of resource groups, but no resources visible in those groups. Azure Devops Pipelines that needed do download from packages.microsoft.com didn't work.&lt;/p&gt;
    &lt;p&gt;The Microsoft status page mostly referenced the portal outage, but it was more than that.&lt;/p&gt;
    &lt;p&gt;I hate these failures because you end up with things that keep working fine because the login credentials are cached, etc; but if you restart or otherwise refresh, you're doomed.&lt;/p&gt;
    &lt;p&gt;For us, it looks like most services are still working (eastus and eastus2). Our AKS cluster is still running and taking requests. Failures seem limited to management portal.&lt;/p&gt;
    &lt;p&gt;I was a little puzzled as we got notified our apps were down, and then I tried to login in the Azure portal with no success. But the Azure status page reported no incident, so I posted here and quickly confirmed that others were impacted! They did a pretty bad job with their status page as the front door service was shown green all along&lt;/p&gt;
    &lt;p&gt;The paradox of cloud provider crashes is that if the provider goes down and takes the whole world with it, it's actually good advertisement. Because, that means so many things rely on it, it's critically important, and has so many big customers. That might be why Amazon stock went up after AWS crash.&lt;/p&gt;
    &lt;p&gt;If Azure goes down and nobody feels it, does Azure really matter?&lt;/p&gt;
    &lt;p&gt;People feel it, but usually not general consumers like they do when AWS goes down.&lt;/p&gt;
    &lt;p&gt;If Azure goes down, it's mostly affecting internal stuff at big old enterprises. Jane in accounting might notice, but the customers don't. Contrast with AWS which runs most of the world's SaaS products.&lt;/p&gt;
    &lt;p&gt;People not being able to do their jobs internally for a day tends not to make headlines like "100 popular internet services down for everyone" does.&lt;/p&gt;
    &lt;p&gt;High availability is touted as a reason for their high prices, but I swear I read about major cloud outages far more than I experience any outages at Hetzner.&lt;/p&gt;
    &lt;p&gt;I think the biggest features of the big cloud vendors is that when they are down, not only you but your customers and your competitors usually have issues at the same time so everybody just shrug and have a lazy/off day at the same time. Even on call teams reall just have to wait and stay on standby because there is very little they can do. Doing a failover can be slower than waiting for the recovery, not help at all if outage is spanned accross several region, or bring aditional risks.&lt;/p&gt;
    &lt;p&gt;And more importantly nobody lose any reputation except AWS/Azure/Google.&lt;/p&gt;
    &lt;p&gt;The real reason is that outages are not your fault. Its the new version of "nobody ever got fired for buying IBM" - later it became MS, and now its any big cloud provider.&lt;/p&gt;
    &lt;p&gt;For one it‚Äôs statistics - Hetzner simply runs far fewer major services than hyperscalers. And the services they run are also more affluent, with larger customer bases, so downtimes are systemically critical. Therefore it‚Äôs louder.&lt;/p&gt;
    &lt;p&gt;On the merits though, I agree, haven‚Äôt had any serious issues with Hetzner.&lt;/p&gt;
    &lt;p&gt;DO has been shockingly reliable for me. I shut down a neglected box almost 900 days uptime the other day. In that time AWS has randomly dropped many of my boxes with no warning requiring a manual stop/start action to recover them... But everybody keeps telling me that DO isn't "as reliable" as the big three are.&lt;/p&gt;
    &lt;p&gt;To be fair, in the AWS/Azure outages, I don't think any individual (already created) boxes went down, either. In AWS' case you couldn't start up new EC2 instances, and presumably same for Azure (unless you bypass the management portal, I guess). And obviously services like DynamoDB and Front Door, respectively, went down. Hetzner/DO don't offer those, right? Or at least they're not very popular.&lt;/p&gt;
    &lt;p&gt;Nope, more than the portal. For instance, I just searched for "Azure Front Door" because I hadn't heard of it before (I now know it's a CDN), and neither the product page itself [1] nor the technical docs [2] are coming up for me.&lt;/p&gt;
    &lt;p&gt;we use front door (as does miccrosoft.com) and our website was down, I was able to change the DNS records to point directly to our server and will leave it like that for a few hours until everything is green&lt;/p&gt;
    &lt;p&gt;They admit in their update blurb azure front door is having issues but still report azure front door as having no issues on their status page.&lt;/p&gt;
    &lt;p&gt;And it's very clear from these updates that they're more focused on the portal than the product, their updates haven't even mentioned fixing it yet, just moving off of it, as if it's some third party service that's down.&lt;/p&gt;
    &lt;p&gt;Unsubstantiated idea: So the support contract likely says there is a window between each reporting step and the status page is the last one and the one in the legal documents giving them several more hours before the clauses trigger.&lt;/p&gt;
    &lt;p&gt;Do Microsoft still say "If the government has a broader voluntary national security program to gather customer data, we don't participate in it" today (which PRISM proved very false), or are they at least acknowledging they're participating in whatever NSA has deployed today?&lt;/p&gt;
    &lt;p&gt;PRISM wasn't voluntary. Also there are 3 levels here:&lt;/p&gt;
    &lt;p&gt;1. Mandatory&lt;/p&gt;
    &lt;p&gt;2. "Voluntary"&lt;/p&gt;
    &lt;p&gt;3. Voluntary&lt;/p&gt;
    &lt;p&gt;And I suspect that very little of what the NSA does falls into category 3. As Sen Chuck Schumer put it "you take on the intelligence community, they have six ways from Sunday at getting back at you"&lt;/p&gt;
    &lt;p&gt;This is funny but also possibly true because: business/MBA types see these outages as a way to prove how critical some services are, leading to investors deciding to load up on the vendor's stock.&lt;/p&gt;
    &lt;p&gt;I may or may not have been known to temporarily take a database down in the past to make a point to management about how unreliable some old software is.&lt;/p&gt;
    &lt;p&gt;Seeing users having issues with the "Modern Outlook", specifically empty accounts. Switching back to the "Legacy Outlook" which functions largely without the help of the cloud fixes the issue. How ironic.&lt;/p&gt;
    &lt;p&gt;Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;----&lt;/p&gt;
    &lt;p&gt;Azure Portal Access Issues&lt;/p&gt;
    &lt;p&gt;We are investigating an issue with the Azure Portal where customers may be experiencing issues accessing the portal. More information will be provided shortly.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:18 UTC on 29 October 2025&lt;/p&gt;
    &lt;p&gt;I was having issues a few hours ago. I'm now able to access the portal, although I get lots of errors in the browser console, and things are loading slowly. I have services in the US-East region.&lt;/p&gt;
    &lt;p&gt;I have been having issues with GitHub and the winget tool for updates throughout the day as well. I imagine things are pulling from the same locations on Azure for some of the software I needed to update (NPM dependencies, and some .NET tooling).&lt;/p&gt;
    &lt;p&gt;The sad thing is - $MSFT isn't even down by 1%. And IIRC, $AMZN actually went up during their previous outage.&lt;/p&gt;
    &lt;p&gt;So if we look at these companies' bottom lines, all those big wigs are actually doing something right. Sales and lobbying capacity is way more effective than reliability or good engineering (at least in the short term).&lt;/p&gt;
    &lt;p&gt;I think he was implying that those companies think they are so important that it doesnt matter they are down, they wont loose any customers over it because they are too big and important.&lt;/p&gt;
    &lt;p&gt;That's a good thing. Stock prices shouldn't go down because of rare incidents which don't accurately represent how successful a company is likely to be in the future.&lt;/p&gt;
    &lt;p&gt;I looked into this before and the stocks of these large corps simply does not move when outages happens. Maybe intra-day, I don't have that data, but in general no effect.&lt;/p&gt;
    &lt;p&gt;There's no way to tell, and after about 30 minutes, the release process on VS Code Marketplace failed with a cryptic message: "Repository signing for extension file failed.". And there's no way to restart/resume it.&lt;/p&gt;
    &lt;p&gt;"We‚Äôre investigating an issue impacting Azure Front Door services. Customers may experience intermittent request failures or latency. Updates will be provided shortly."&lt;/p&gt;
    &lt;p&gt;Could be DNS, I'm seeing SERVFAIL trying to resolve what look to be MS servers when I'm hitting (just one example) mygoodtogo.com (trying to pay a road toll bill, and failing).&lt;/p&gt;
    &lt;p&gt;Azure goes down all the time. On Friday we had an entire regional service down all day. Two weeks ago same thing different region. You only hear about it when it's something everyone uses like the portal, because in general nobody uses Azure unless they're held hostage.&lt;/p&gt;
    &lt;p&gt;Portal and Azure CDN are down here in the SF Bay Area. Tenant azureedge.net DNS A queries are taking 2-6 seconds and most often return nothing. I got a couple successful A response in the last 10 minutes.&lt;/p&gt;
    &lt;p&gt;Edit: As of 9:19 AM Pacific time, I'm now getting successful A responses but they can take several seconds. The web server at that address is not responding.&lt;/p&gt;
    &lt;p&gt;It is much more than azure. One of my kids needs a key for their laptop and can't reach that either. Great excuse though, 'Azure ate my homework'. What a ridiculous world we are building. Fuck MS and their account requirements for windows.&lt;/p&gt;
    &lt;p&gt;It begs the question from a noob like me... Where should they host the status page? Surely it shouldn't be on the same infra that it's supposed to be monitoring. Am I correct in thinking that?&lt;/p&gt;
    &lt;p&gt;‚Äú Starting at approximately 16:00 UTC, we began experiencing DNS issues resulting in availability degradation of some services. Customers may experience issues accessing the Azure Portal. We have taken action that is expected to address the portal access issues here shortly. We are actively investigating the underlying issue and additional mitigation actions. More information will be provided within 60 minutes or sooner.&lt;/p&gt;
    &lt;p&gt;This message was last updated at 16:35 UTC on 29 October 2025‚Äù&lt;/p&gt;
    &lt;p&gt;I'd say DNS/Front Door (or some carrier interconnect) is the thing affected, since I can auth just fine in a few places. (I'm at MS, but not looped into anything operational these days, so I'm checking my personal subscription).&lt;/p&gt;
    &lt;p&gt;On our end, our VMs are still working, so our gitlab instance is still up. Our services using Azure App Services are available through their provided url. However, Front Door is failing to resolve any domains that it was responsible for.&lt;/p&gt;
    &lt;p&gt;SSO is down, Azure Portal Down and more, seems like a major outage. Already a lot of services seem to be affected: banks, airlines, consumer apps, etc.&lt;/p&gt;
    &lt;p&gt;The portal is up for me and their status page confirms they did a failover for it. Definitely not disputing that its reach is wide, but a lot of smaller setups probably aren't using Front Door.&lt;/p&gt;
    &lt;p&gt;Looks like MyGet is impacted too. Seems like they use Azure:&lt;/p&gt;
    &lt;p&gt;&amp;gt;What is required to be able to use MyGet? ... MyGet runs its operations from the Microsoft Azure in the West Europe region, near Amsterdam, the Netherlands.&lt;/p&gt;
    &lt;p&gt;Part of this outage involves outlook hanging and then blaming random addins. Pretty terrible practice by Microsoft to blame random vendors for their own outage.&lt;/p&gt;
    &lt;p&gt;This is the eternal tension for early-stage builders, isn't it? Multi-cloud gives you resilience, but adds so much complexity that it can actually slow down shipping features and iterating.&lt;/p&gt;
    &lt;p&gt;I'm curious‚Äîat what point did you decide the overhead was worth it? Was it after experiencing an outage, or did you architect for it from day one?&lt;/p&gt;
    &lt;p&gt;As someone launching a product soon (more on the builder/product side than infra-engineer), I keep wrestling with this. The pragmatist in me says "start simple, prove the concept, then layer in resilience." But then you see events like this week and think "what if this happens during launch?"&lt;/p&gt;
    &lt;p&gt;How did you handle the operational complexity? Did you need dedicated DevOps folks, or are there patterns/tools that made it manageable for a smaller team?&lt;/p&gt;
    &lt;p&gt;I don't think I would recommend multi-cloud right out of the gate unless you already have a lot of experience in the space or there is a strong demand from your customers. There's a tremendous amount of overhead with security/compliance, incident management, billing, tooling, entitlements, etc. There are a number of external factors that drove our decision to do it, resiliency is just one of them. But we are a pretty big shop, spending ~$10M/mo on cloud infra and have ~100 people in the platform management department.&lt;/p&gt;
    &lt;p&gt;I would recommend focusing on multi-region within a single CSP instead (both for workloads AND your tooling), which covers the vast majority of incidents and lays some of the architectural foundation for multi-cloud down the road. Develop failover plans for each service in your architecture (eg. planned/tested runbooks to migrate to Traffic Manager in the event AFD goes down)&lt;/p&gt;
    &lt;p&gt;Also choose your provider wisely. We experience 3-5x the number of service-impacting incidents on Azure that we do on AWS. I'm sure others have different experiences, but I would never personally start a company on Azure. AWS has its own issues, of course, but reliability has not been a major one (relatively speaking) over the past 10 years. Last week's incident with DynamoDB in us-east-1 had zero impact on our AWS workloads in other regions.&lt;/p&gt;
    &lt;p&gt;I don't think it's meant to be serious. It's a comment on Microsoft laying off their staff and stuffing their Azure and Dotnet teams with AI product managers.&lt;/p&gt;
    &lt;p&gt;That said, I don't hear about GCP outages all that often. I do think AWS might be leading in outages, but that's a gut feeling, I didn't look up numbers.&lt;/p&gt;
    &lt;p&gt;Does (should, could) DownDetector also say what customer-facing services are down, when some infrastructure is unworking? Or is that the info that the malefactors are seeking?&lt;/p&gt;
    &lt;p&gt;Thank you. I was wondering what was going on at a company whose web app I need to access. I just checked with BuiltWith and it seems they are on Azure.&lt;/p&gt;
    &lt;p&gt;I absolutely love the utility aspect of LLMs but part of me is curious if moving faster by using AI is going to make these sorts of failure more and more often.&lt;/p&gt;
    &lt;p&gt;Unable to access the portal and any hit to SSO for other corporate accesses is also broken. Seems like there's something wrong in their Identity services.&lt;/p&gt;
    &lt;p&gt;Apologies, but this just reads like a low effort critique of big things.&lt;/p&gt;
    &lt;p&gt;To be clear, they should get criticism. They should be held liable for any damage they cause.&lt;/p&gt;
    &lt;p&gt;But that they remain the biggest cloud offering out there isn't something you'd expect to change from a few outages that, by most all evidence, potential replacements have, as well? More, a lot of the outages potential replacements have are often more global in nature.&lt;/p&gt;
    &lt;p&gt;I would say you are explaining why they get a free pass so they still get one - they are bad but their main competitors are even worse!&lt;/p&gt;
    &lt;p&gt;I thought one of the major selling points of the big cloud providers was that they were more reliable than running your own stuff (by which i mean anything from a VPS to multiple data centres depending on your scale. Compared to those alternatives they seem to be less reliable in practice!&lt;/p&gt;
    &lt;p&gt;The solution is to have a multi-region, or even multi-cloud setup, but then bang goes the "they do all the work for you" argument (which i doubt anyway).&lt;/p&gt;
    &lt;p&gt;That isn't a free pass. You have no data showing how many people did go to competitors over this. You are asserting it is zero, but why do you think that? Going on the talks here, you can find plenty of folks that opted not to go with or stay on them.&lt;/p&gt;
    &lt;p&gt;You are further asserting that these outages prove they are not still more reliable than home spun. Is that the case? More than a few people aren't ready for a single hard drive to crash on the stuff they are doing.&lt;/p&gt;
    &lt;p&gt;Another big cloud outage ‚Äî even ‚Äúenterprise‚Äù systems aren‚Äôt bulletproof. Feels like every layer depends on too much hidden glue. Makes you wonder if real redundancy even exists anymore.&lt;/p&gt;
    &lt;p&gt;Yeah, I have non prod environments that don't use FD that are functioning. Routing through FD does not work. And a different app, nonprod doesn't use FD (and is working) but loads assets from the CDN (which is not working).&lt;/p&gt;
    &lt;p&gt;FD and CDN are global resources and are experiencing issues. Probably some other global resources as well.&lt;/p&gt;
    &lt;p&gt;Hate to say it, but DNS is looking like it's still the undisputed champ.&lt;/p&gt;
    &lt;p&gt;HTTPSConnectionPool(host='schemas.xmlsoap.org', port=443): Max retries exceeded with url: /soap/encoding/ (Caused by SSLError(CertificateError("hostname 'schemas.xmlsoap.org' doesn't match '*.azureedge.net'")))&lt;/p&gt;
    &lt;p&gt;A service we rely on that isn't even running on Azure is inaccessible due to this issue. For an asset that probably never changes. Wild for that to be the SPOF.&lt;/p&gt;
    &lt;p&gt;&amp;gt; An inadvertent tenant configuration change within Azure Front Door (AFD) triggered a widespread service disruption affecting both Microsoft services and customer applications dependent on AFD for global content delivery. The change introduced an invalid or inconsistent configuration state that caused a significant number of AFD nodes to fail to load properly, leading to increased latencies, timeouts, and connection errors for downstream services.&lt;/p&gt;
    &lt;p&gt;&amp;gt; As unhealthy nodes dropped out of the global pool, traffic distribution across healthy nodes became imbalanced, amplifying the impact and causing intermittent availability even for regions that were partially healthy. We immediately blocked all further configuration changes to prevent additional propagation of the faulty state and began deploying a ‚Äòlast known good‚Äô configuration across the global fleet. Recovery required reloading configurations across a large number of nodes and rebalancing traffic gradually to avoid overload conditions as nodes returned to service. This deliberate, phased recovery was necessary to stabilize the system while restoring scale and ensuring no recurrence of the issue.&lt;/p&gt;
    &lt;p&gt;&amp;gt; The trigger was traced to a faulty tenant configuration deployment process. Our protection mechanisms, to validate and block any erroneous deployments, failed due to a software defect which allowed the deployment to bypass safety validations. Safeguards have since been reviewed and additional validation and rollback controls have been immediately implemented to prevent similar issues in the future.&lt;/p&gt;
    &lt;p&gt;So, so far they're saying it's a combination of bad config + their config-validator had a bug. Would love more details.&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;AWS, now Azure - wasn't this a plot point in Terminator where SkyNet was causing computer systems to have issues much before it finally become self-aware?&lt;/p&gt;
    &lt;p&gt;Funnily enough, AI has been training on its own data as generated by users writing AI conversations back to the internet - there's a feedback loop at play.&lt;/p&gt;
    &lt;p&gt;downdetector reports coincident cloudflare outage. is microsoft using cloudflare for management plane, or is there common infra? data center problem somewhere, maybe fiber backbone? BGP?&lt;/p&gt;
    &lt;p&gt;Yeah the graph for that one looks exactly the same shape. I wonder if they were depending on some azure component somehow, or maybe there were things hosted on both and the azure failure made enough things failover to AWS that AWS couldn't cope? If that was the case I'd expect to see something similar with GCP too though.&lt;/p&gt;
    &lt;p&gt;Edit: nope looks like there's actually a spike on GCP as well&lt;/p&gt;
    &lt;p&gt;Definitely also a strong possibility. I wish I had paid more attention during the AWS one earlier to see what other things looked like on there at the time.&lt;/p&gt;
    &lt;p&gt;When you look at the scale of the reports, you find they are much lower than Azure's. seeing a bunch of 24-hour sparkline type graphs next to each other can make it look like they are equally impacted, but AWS has 500 reports and Azure has 20,000. The scale is hidden by the choice of graph.&lt;/p&gt;
    &lt;p&gt;In other words, people reporting outages at AWS are probably having trouble with microsoft-run DNS services or caching proxies. It's not that the issues aren't there, it's that the internet is full of intermingled complexity. Just that amount of organic false-positives can make it look like an unrelated major service is impacted.&lt;/p&gt;
    &lt;p&gt;winget upgrade fabric Failed in attempting to update the source: winget An unexpected error occurred while executing the command: InternetOpenUrl() failed. 0x80072ee7 : unknown error&lt;/p&gt;
    &lt;p&gt;As of now Azure Status page still shows no incident. It must be manually updated, someone has to actively decide to acknowledge an issue, and they're just... not. It undermines confidence in that status page.&lt;/p&gt;
    &lt;p&gt;I know how to fix this but this community is too close minded and argumentative egocentric sensitive pedantic threatened angry etc to bother discussing it&lt;/p&gt;
    &lt;p&gt;I noticed issues on Azure so I went to the status page. It said everything was fine even though the Azure Portal was down. It took more than 10 minutes for that status page to update.&lt;/p&gt;
    &lt;p&gt;How can one of the richest companies in the world not offer a better service?&lt;/p&gt;
    &lt;p&gt;My best guess at the moment is something global like the CDN is having problems affecting things everywhere. I'm able to use a legacy application we have that goes directly to resources in uswest3, but I'm not able to use our more modern application which uses APIM/CDN networks at all.&lt;/p&gt;
    &lt;p&gt;From Azure status page: "Customers can consider implementing failover strategies with Azure Traffic Manager, to fail over from Azure Front Door to your origins".&lt;/p&gt;
    &lt;p&gt;I especially like how Nadella speaks of layoffs as some kind of uncontrollable natural disaster, like a hurricane, caused by no-one in particular. A kind of "God works in mysterious ways".&lt;/p&gt;
    &lt;p&gt;&amp;gt; ‚ÄúMicrosoft is being recognized and rewarded at levels never seen before,‚Äù Nadella wrote. ‚ÄúAnd yet, at the same time, we‚Äôve undergone layoffs. This is the enigma of success in an industry that has no franchise value.‚Äù &amp;gt; Nadella explained the disconnect between thriving financials and layoffs by stating that ‚Äúprogress isn‚Äôt linear‚Äù and that it is ‚Äúsometimes dissonant, and always demanding.‚Äù&lt;/p&gt;
    &lt;p&gt;I've read the whole memo and it's actually worse than those excerpts. Nadella doesn't even claim these were low performers:&lt;/p&gt;
    &lt;p&gt;&amp;gt; These decisions are among the most difficult we have to make. They affect people we‚Äôve worked alongside, learned from, and shared countless moments with‚Äîour colleagues, teammates, and friends.&lt;/p&gt;
    &lt;p&gt;Ok, so Microsoft is thriving, these were friends and people "we've learned from", but they must go because... uh... "progress isn't linear". Well, thanks Nadella! That explains so much!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45748661</guid><pubDate>Wed, 29 Oct 2025 16:01:18 +0000</pubDate></item><item><title>Minecraft removing obfuscation in Java Edition</title><link>https://www.minecraft.net/en-us/article/removing-obfuscation-in-java-edition</link><description>&lt;doc fingerprint="370203ca08b7cced"&gt;
  &lt;main&gt;
    &lt;p&gt;Do you like to mod Java, tinker with builds, or take deep dives into Minecraft‚Äôs code? Then this article is for you!&lt;/p&gt;
    &lt;p&gt;For a long time, Java Edition has used obfuscation (hiding parts of the code) ‚Äì a common practice in the gaming industry. Now we‚Äôre changing how we ship Minecraft: Java Edition to remove obfuscation completely. We hope that, with this change, we can pave a future for Minecraft: Java Edition where it‚Äôs easier to create, update, and debug mods.&lt;/p&gt;
    &lt;head rend="h2"&gt;An obfuscated history&lt;/head&gt;
    &lt;p&gt;Minecraft: Java Edition has been obfuscated since its release. This obfuscation meant that people couldn‚Äôt see our source code. Instead, everything was scrambled ‚Äì and those who wanted to mod Java Edition had to try and piece together what every class and function in the code did.&lt;/p&gt;
    &lt;p&gt;But we encourage people to get creative both in Minecraft and with Minecraft ‚Äì so in 2019 we tried to make this tedious process a little easier by releasing ‚Äúobfuscation mappings‚Äù. These mappings were essentially a long list that allowed people to match the obfuscated terms to un-obfuscated terms. This alleviated the issue a little, as modders didn‚Äôt need to puzzle out what everything did, or what it should be called anymore. But why stop there?&lt;/p&gt;
    &lt;head rend="h2"&gt;Removing obfuscation in Java Edition&lt;/head&gt;
    &lt;p&gt;To make things even easier ‚Äì and remove these intermediary steps ‚Äì we‚Äôre removing obfuscation altogether! Starting with the first snapshot following the complete Mounts of Mayhem launch, we will no longer obfuscate Minecraft: Java Edition. This means that this build (and all future builds) will have all of our original names* ‚Äì now with variable names and other names ‚Äì included by default to make modding even easier.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45748879</guid><pubDate>Wed, 29 Oct 2025 16:12:56 +0000</pubDate></item><item><title>Tailscale Peer Relays</title><link>https://tailscale.com/blog/peer-relays-beta</link><description>&lt;doc fingerprint="66f1d873751076c5"&gt;
  &lt;main&gt;
    &lt;p&gt;Tailscale Peer Relays provides a customer-deployed and managed traffic relaying mechanism. By advertising itself as a peer relay, a Tailscale node can relay traffic for any peer nodes on the tailnet, even for traffic bound to itself. Tailscale Peer Relays can only relay traffic for nodes on your tailnet, and only for nodes that have access to the peer relay. Because they‚Äôre managed entirely by the customer, peer relays are less throughput-constrained than Tailscale‚Äôs managed DERP relays, and can provide higher throughput connections for traffic to and from locked-down cloud infrastructure, or behind strict network firewalls.&lt;/p&gt;
    &lt;p&gt;In testing with early design partners, we‚Äôve seen throughputs nearing that of a direct connection; often multiple orders of magnitude higher than Tailscale‚Äôs managed DERP fleet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Moving past hard NAT&lt;/head&gt;
    &lt;p&gt;Over the past few weeks, you‚Äôve heard us talk about improvements we‚Äôve made to Network Address Translation (NAT) traversal techniques, so that Tailscale can establish direct connections wherever possible (hint: it‚Äôs over 90% of the time). However, we‚Äôve also outlined places where this isn‚Äôt possible or desirable today for a variety of reasons, especially in cloud environments. And, we‚Äôve postulated a bit about where we think the industry is going.&lt;/p&gt;
    &lt;p&gt;While we‚Äôve been keeping your network reliably connected for years with DERP, we‚Äôve heard from customers that the throughput and performance aspects of a QoS-aware managed relay fleet makes deployments in certain environments difficult or untenable. Furthermore, customers have noted that it‚Äôs non-trivial to deploy and manage custom DERP fleets (which run as a separate service and binary).&lt;/p&gt;
    &lt;p&gt;DERP provides an incredibly valuable service, setting up reliable connections between Tailscale clients anywhere in the world (including negotiating connections through peer relays). But often, DERP‚Äôs focus as a reliability and NAT traversal tool results in performance tradeoffs.&lt;/p&gt;
    &lt;p&gt;By contrast, Tailscale Peer Relays is designed as a performant connectivity tool, and can perform at a level rivaling direct connections. Peer relays can be placed right next to the resources they serve, and peer relays also run on top of UDP, both characteristics beneficial to lower latency and resource overhead. And, they are built into the Tailscale client itself for ease of deployment.&lt;/p&gt;
    &lt;p&gt;We want to move past even more hard NATs, and put Tailscale‚Äôs relaying technology in our customers‚Äô hands, so they can use Tailscale at scale, anywhere, with ease. We believe our new Tailscale Peer Relays connectivity option‚Äîunique to Tailscale‚Äîgives customers the best performance and flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Peer relays are configured with a single UDP port that must be available to both sides of a connection. Tailscale Peer Relays is built right into the Tailscale client, and can be enabled with a simple command, using the &lt;code&gt;tailscale set --relay-server-port&lt;/code&gt; flag from the Tailscale CLI. Once enabled via the steps in our documentation, clients can connect to infrastructure in hard NAT environments over the peer relay.&lt;/p&gt;
    &lt;p&gt;And don‚Äôt worry, we still prefer to fly direct. Tailscale prefers direct connections wherever possible. Clients can then fall back to available peer relays, and finally leverage Tailscale‚Äôs managed DERP fleet, or any customer-deployed custom DERPs, to ensure you have connectivity wherever you need it. All of this traffic, over any connection, is still end-to-end encrypted via WireGuard¬Æ.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays is designed for the real world, based on the feedback we‚Äôve received from customers and our own hard-earned networking expertise. It allows customers to make just one firewall exception for connections only coming from their tailnet. Peer relays scale across regions, are resilient to real-world network conditions, and graciously fall back to DERP (Tailscale‚Äôs or custom). Your network maintains its shape, but gains all kinds of flexibility.&lt;/p&gt;
    &lt;head rend="h2"&gt;Connectivity, everywhere, at warp speed&lt;/head&gt;
    &lt;p&gt;Customers can now maintain performance benchmarks even where direct connections aren‚Äôt possible, by enabling Tailscale Peer Relays to build a deterministic and high-throughput relay topology.&lt;/p&gt;
    &lt;p&gt;We‚Äôve had customers use peer relays to provide access into unmanaged networks, allowing their partners or customers to provide a controllable and auditable connectivity path without sacrificing performance.&lt;/p&gt;
    &lt;p&gt;In strict private networks, customers can build predictable access paths. Tailscale Peer Relays can be placed in public subnets with VPC peering to private subnets, allowing security teams to efficiently segment their private network infrastructure, while enabling networking teams to roll Tailscale out in full-mesh mode across the subnet.&lt;/p&gt;
    &lt;p&gt;Today, customers are using peer relays to establish relayed connections at near-direct speeds across a variety of environments and settings.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enable high-throughput traffic through cloud NATs, like AWS Managed NAT Gateways: Applications and services behind a Managed NAT Gateway can leverage peer relays to relay traffic that can‚Äôt establish direct connections.&lt;/item&gt;
      &lt;item&gt;Relay through network firewalls: Workloads running in strictly firewalled environments can predictably expose a single UDP port, limiting the Tailscale surface area and fast-tracking the approval process for firewall exceptions.&lt;/item&gt;
      &lt;item&gt;Offload from Custom and Managed DERP: Minimize data-in-transit through Tailscale‚Äòs managed DERP network, and remove the need for customer-maintained DERP servers.&lt;/item&gt;
      &lt;item&gt;Provide access to locked down customer networks: Data plane traffic can be relayed through predictable endpoints in customer networks, so that they only need to open minimal numbers of ports to facilitate cross network connections.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;It‚Äôs not perfect, but we‚Äôre getting there&lt;/head&gt;
    &lt;p&gt;Tailscale Peer Relays is available today as a public beta. We‚Äôve yet to establish all the connectivity paths we want to, and there‚Äôs still visibility and debugging improvements to work through. However, we‚Äôve reliably seen our early design partners move to peer relay deployments with relative ease, and we‚Äôre ready for you to give it a try on your tailnet.&lt;/p&gt;
    &lt;p&gt;Tailscale Peer Relays can be enabled on all plans, including free (it‚Äôs our little way of working through the kinks of the modern internet with our customers). All customers can use two peer relays, for free, forever. As your needs scale, so will the number of available peer relays. To add even more peer relays to your tailnet, come have a chat with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45749017</guid><pubDate>Wed, 29 Oct 2025 16:21:36 +0000</pubDate></item><item><title>AOL to be sold to Bending Spoons for $1.5B</title><link>https://www.axios.com/2025/10/29/aol-bending-spoons-deal</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45749161</guid><pubDate>Wed, 29 Oct 2025 16:28:56 +0000</pubDate></item><item><title>OpenAI‚Äôs promise to stay in California helped clear the path for its IPO</title><link>https://www.wsj.com/tech/ai/openais-promise-to-stay-in-california-helped-clear-the-path-for-its-ipo-3af1c31c</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45750425</guid><pubDate>Wed, 29 Oct 2025 17:44:34 +0000</pubDate></item><item><title>The Internet runs on free and open source software and so does the DNS</title><link>https://www.icann.org/en/blogs/details/the-internet-runs-on-free-and-open-source-softwareand-so-does-the-dns-23-10-2025-en</link><description>&lt;doc fingerprint="8ac2858004c20ddf"&gt;
  &lt;main&gt;
    &lt;p&gt;Free and open-source software (FOSS) is not merely common on the Internet; it is a deeply embedded and essential foundation of the Domain Name System (DNS), the backbone of how we connect online.&lt;/p&gt;
    &lt;p&gt;The ICANN Security and Stability Advisory Committee (SSAC) is pleased to announce the publication of SAC132: The Domain Name System Runs on Free and Open Source Software (FOSS).&lt;/p&gt;
    &lt;head rend="h3"&gt;Why This Matters Now&lt;/head&gt;
    &lt;p&gt;As governments around the world explore new cybersecurity regulations, the ubiquity of FOSS in DNS operations‚Äîfrom domain registration to retrieval‚Äîmeans that policy decisions made today will have direct implications for the Internet's security and resilience tomorrow. SAC132 provides timely, nontechnical guidance to ensure that new policy and regulation serve to strengthen, rather than inadvertently weaken, this critical infrastructure.&lt;/p&gt;
    &lt;head rend="h3"&gt;Key Insights for Policymakers&lt;/head&gt;
    &lt;p&gt;SAC132 is a foundational guide designed to empower policymakers to strategically manage and sustain the FOSS ecosystem. The report provides:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Clear Foundations ‚Äì An accessible overview of the DNS and the FOSS development model for nontechnical audiences.&lt;/item&gt;
      &lt;item&gt;Policy Assessment ‚Äì Analysis of cybersecurity regulations in the United States, United Kingdom, and European Union, with a focus on how they account for FOSS in the DNS ecosystem.&lt;/item&gt;
      &lt;item&gt;Practical Guidance ‚Äì Concrete findings and recommendations to help policymakers support and secure FOSS as a cornerstone of global connectivity.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We invite all policymakers, technical experts, and stakeholders to read the full report.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Call to Engage&lt;/head&gt;
    &lt;p&gt;By publishing SAC132, SSAC seeks to raise awareness of the indispensable role of FOSS in maintaining a secure, stable, and resilient Internet. We invite policymakers, technical experts, and all stakeholders to read the full report and join us in conversations about its findings.&lt;/p&gt;
    &lt;p&gt;You can engage with SSAC and the broader community at ICANN84, whether in Dublin or by participating remotely. Together, we can ensure that the FOSS ecosystem‚Äîand the Internet it supports‚Äîremains strong, sustainable, and open for all.&lt;/p&gt;
    &lt;p&gt;Finally, we thank all SSAC members and invited experts who contributed to this work, especially co-chairs Maarten Aertsen and Barry Leiba, for their leadership.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45750875</guid><pubDate>Wed, 29 Oct 2025 18:16:07 +0000</pubDate></item><item><title>Dithering ‚Äì Part 1</title><link>https://visualrambling.space/dithering-part-1/</link><description>&lt;doc fingerprint="1336983308d69b52"&gt;
  &lt;main&gt;
    &lt;p&gt;Understanding how dithering works, visually.&lt;/p&gt;
    &lt;p&gt;tap/click the right side of the screen to go forward ‚Üí&lt;/p&gt;
    &lt;p&gt;I‚Äôve always been fascinated by the dithering effect. It has a unique charm that I find so appealing.&lt;/p&gt;
    &lt;p&gt;‚Üê tap/click the left side to go back&lt;/p&gt;
    &lt;p&gt;I was even more amazed when I learned how dithering works.&lt;/p&gt;
    &lt;p&gt;‚Üê or use arrow keys to navigate ‚Üí&lt;/p&gt;
    &lt;p&gt;Look closely, and you‚Äôll see this animation is made of alternating black and white pixels.&lt;/p&gt;
    &lt;p&gt;But these black and white pixels are specifically arranged to create the illusion of multiple shades.&lt;/p&gt;
    &lt;p&gt;That‚Äôs what dithering does: it simulates more color variations than what are actually used.&lt;/p&gt;
    &lt;p&gt;Here, it uses black and white to give the impression of multiple gray shades.&lt;/p&gt;
    &lt;p&gt;To me, dithering is about creating the most out of what we have, and that's what amazes me the most!&lt;/p&gt;
    &lt;p&gt;It inspired me to learn more about it, and now I want to share what I‚Äôve learned.&lt;/p&gt;
    &lt;p&gt;Please note that this is just part one out of three, so I‚Äôll only scratch the surface here.&lt;/p&gt;
    &lt;p&gt;I‚Äôll go deeper in the next parts, which will come soon. Stay tuned!&lt;/p&gt;
    &lt;p&gt;First, let‚Äôs explore the dithering basics with this grayscale image example.&lt;/p&gt;
    &lt;p&gt;A grayscale image has various gray shades, from black to white.&lt;/p&gt;
    &lt;p&gt;Imagine a display that only shows black or white pixels, no grays. We must turn some pixels black and others white‚Äîbut how?&lt;/p&gt;
    &lt;p&gt;One way is to map each pixel to the closest available color.&lt;/p&gt;
    &lt;p&gt;Pixels darker than medium gray turn black and lighter ones turn white.&lt;/p&gt;
    &lt;p&gt;This splits pixels into black or white groups.&lt;/p&gt;
    &lt;p&gt;However, this creates a harsh image with abrupt black-white transitions.&lt;/p&gt;
    &lt;p&gt;Shadow details vanish as gray pixels become fully black or white.&lt;/p&gt;
    &lt;p&gt;Dithering fixes this by selectively pushing some pixels towards the opposite color.&lt;/p&gt;
    &lt;p&gt;Some light gray pixels that are closer to white turn black.&lt;/p&gt;
    &lt;p&gt;Likewise, some dark grays turn white.&lt;/p&gt;
    &lt;p&gt;And it's done in a way that produces special patterns which simulate shades by varying the black-and-white pixel densities.&lt;/p&gt;
    &lt;p&gt;Denser black pixels are used in darker areas, while denser white pixels are used in lighter ones.&lt;/p&gt;
    &lt;p&gt;Next question: How are these patterns generated?&lt;/p&gt;
    &lt;p&gt;One simple dithering method, known as ordered dithering, uses a threshold map.&lt;/p&gt;
    &lt;p&gt;A threshold map is a grid of values representing brightness levels, from 0 (darkest) to 1 (brightest).&lt;/p&gt;
    &lt;p&gt;To dither, we compare each input pixel‚Äôs brightness to a corresponding threshold value.&lt;/p&gt;
    &lt;p&gt;If a pixel‚Äôs brightness exceeds the threshold (it‚Äôs brighter than the threshold), the pixel turns white. Otherwise, it turns black.&lt;/p&gt;
    &lt;p&gt;Repeating this for all pixels gives us the black-and-white dither patterns.&lt;/p&gt;
    &lt;p&gt;The threshold map is designed to output patterns where the black-and-white pixel density matches the input image‚Äôs shades.&lt;/p&gt;
    &lt;p&gt;So brighter input produces patterns with more white, while darker input produces more black.&lt;/p&gt;
    &lt;p&gt;These black-and-white density variations are what create the illusion of gray shades when viewed from a distance.&lt;/p&gt;
    &lt;p&gt;To dither larger images, we extend the threshold map to match the image size and follow the same principle:&lt;/p&gt;
    &lt;p&gt;Compare each pixel‚Äôs brightness to the threshold map, then turn it black or white accordingly.&lt;/p&gt;
    &lt;p&gt;The image now uses only two colors, but its overall appearance is preserved.&lt;/p&gt;
    &lt;p&gt;The variations in shades are now replaced by variations in black/white pixel density of the dithering patterns.&lt;/p&gt;
    &lt;p&gt;And that‚Äôs how dithering works in a nutshell: it replicates shades with fewer colors, which are strategically placed to maintain the original look.&lt;/p&gt;
    &lt;p&gt;I find it a bit ironic how I used to think dithering ‚Äòadds‚Äô a cool effect, when what it actually does is ‚Äòremove‚Äô colors!&lt;/p&gt;
    &lt;p&gt;That's all for now! We‚Äôve reached the end, but there‚Äôs still a lot more to explore.&lt;/p&gt;
    &lt;p&gt;For example, we haven‚Äôt explored the algorithm to create a threshold map. (spoiler: there are many ways!)&lt;/p&gt;
    &lt;p&gt;There‚Äôs also another algorithm called error diffusion, which doesn‚Äôt use a threshold map.&lt;/p&gt;
    &lt;p&gt;Each algorithm creates a distinct, unique look, which I believe deserves its own article.&lt;/p&gt;
    &lt;p&gt;And that's why I decided to break this series into three parts.&lt;/p&gt;
    &lt;p&gt;In the next part, I‚Äôll dive into various algorithms for creating threshold maps.&lt;/p&gt;
    &lt;p&gt;In the final part, I‚Äôll focus on the error diffusion algorithm.&lt;/p&gt;
    &lt;p&gt;We'll dive even deeper into dithering's mechanisms in these next 2 parts, so stay tuned!&lt;/p&gt;
    &lt;p&gt;Thank you for reading!&lt;/p&gt;
    &lt;p&gt;visualrambling.space is a personal project by Damar, someone who loves to learn about different topics and rambling about them visually.&lt;/p&gt;
    &lt;p&gt;If you like this kind of visual article, please consider following me on X/Twitter and sharing this with your friends.&lt;/p&gt;
    &lt;p&gt;I'll keep creating more visual articles like this!&lt;/p&gt;
    &lt;p&gt;https://x.com/damarberlari&lt;/p&gt;
    &lt;p&gt;_blank&lt;/p&gt;
    &lt;p&gt;Topics covered: Three.js, WebGL, dithering, visualization, interactive learning&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45750954</guid><pubDate>Wed, 29 Oct 2025 18:21:35 +0000</pubDate></item><item><title>Uv is the best thing to happen to the Python ecosystem in a decade</title><link>https://emily.space/posts/251023-uv</link><description>&lt;doc fingerprint="14982e51cdc48290"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;uv is the best thing to happen to the Python ecosystem in a decade&lt;/head&gt;
    &lt;p&gt;23 October 2025 | Reading time: 6 minutes&lt;/p&gt;
    &lt;p&gt;It‚Äôs 2025. Does installing Python, managing virtual environments, and synchronizing dependencies between your colleagues really have to be so difficult? Well‚Ä¶ no! A brilliant new tool called uv came out recently that revolutionizes how easy installing and using Python can be.&lt;/p&gt;
    &lt;p&gt;uv is a free, open-source tool built by Astral, a small startup that has been churning out Python tools (like the excellent linter Ruff) for the past few years. uv can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Install any Python version for you&lt;/item&gt;
      &lt;item&gt;Install packages&lt;/item&gt;
      &lt;item&gt;Manage virtual environments&lt;/item&gt;
      &lt;item&gt;Solve dependency conflicts extremely quickly (very important for big projects.)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What‚Äôs best is that it can do all of the above better than any other tool, in my opinion. It‚Äôs shockingly fast, written in Rust, and works on almost any operating system or platform.&lt;/p&gt;
    &lt;head rend="h2"&gt;Installing uv&lt;/head&gt;
    &lt;p&gt;uv is straightforward to install. There are a few ways, but the easiest (in my opinion) is this one-liner command ‚Äî for Linux and Mac, it‚Äôs:&lt;/p&gt;
    &lt;code&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/code&gt;
    &lt;p&gt;or on Windows in powershell:&lt;/p&gt;
    &lt;code&gt;powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"&lt;/code&gt;
    &lt;p&gt;You can then access uv with the command &lt;code&gt;uv&lt;/code&gt;. Installing uv will not mess up any of your existing Python installations ‚Äî it‚Äôs a separate tool, so it‚Äôs safe to install it just to try it out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Managing Python for a project&lt;/head&gt;
    &lt;p&gt;It‚Äôs always a good idea to work with virtual environments for any Python project. It keeps different bits of code and dependencies ringfenced from one another, and in my experience, it can save a lot of hassle to get into the habit of using virtual environments as soon as you can. uv naturally uses virtual environments, so it‚Äôs very easy to start using them if you get into using uv.&lt;/p&gt;
    &lt;p&gt;uv will build a Python environment for you based on what‚Äôs specified in a &lt;code&gt;pyproject.toml&lt;/code&gt; file in the directory (or parent directories) you‚Äôre working in. &lt;code&gt;pyproject.toml&lt;/code&gt; files are a standard, modern format for specifying dependencies for a Python project. A barebones one might look a bit like this:&lt;/p&gt;
    &lt;code&gt;[project]
name = "my_project"
version = "1.0.0"
requires-python = "&amp;gt;=3.9,&amp;lt;3.13"
dependencies = [
  "astropy&amp;gt;=5.0.0",
  "pandas&amp;gt;=1.0.0,&amp;lt;2.0",
]&lt;/code&gt;
    &lt;p&gt;In essence, it just has to specify which Python version to use and some dependencies. Adding a name and version number also aren‚Äôt a bad idea.&lt;/p&gt;
    &lt;p&gt;(Sidenote: for projects that you publish as packages, such as to the Python Package Index that pip and uv use, &lt;code&gt;pyproject.toml&lt;/code&gt; files are a modern way to specify everything you need to publish your package.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Making a new project with uv&lt;/head&gt;
    &lt;p&gt;To start a new Python project with uv, you can run&lt;/p&gt;
    &lt;code&gt;uv init&lt;/code&gt;
    &lt;p&gt;Which will create a new project for you, with a &lt;code&gt;pyproject.toml&lt;/code&gt;, a &lt;code&gt;README.md&lt;/code&gt;, and other important bits of boilerplate.&lt;/p&gt;
    &lt;p&gt;There are a lot of different ways to run this command, like &lt;code&gt;uv init --bare&lt;/code&gt; (which only creates a pyproject.toml), &lt;code&gt;uv init --package&lt;/code&gt; (which sets up a new Python package), and more. I recommend running &lt;code&gt;uv init --help&lt;/code&gt; to read about them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Once you have/if you already have a &lt;code&gt;pyproject.toml&lt;/code&gt; file&lt;/head&gt;
    &lt;p&gt;Once you initialize a project ‚Äî or if you already have a &lt;code&gt;pyproject.toml&lt;/code&gt; file in your project ‚Äî it‚Äôs very easy to start using uv. You just need to do&lt;/p&gt;
    &lt;code&gt;uv sync&lt;/code&gt;
    &lt;p&gt;in the directory that your &lt;code&gt;pyproject.toml&lt;/code&gt; file is in. This command (and in fact, most uv commands if you haven‚Äôt ran it already) will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Automatically install a valid version of Python&lt;/item&gt;
      &lt;item&gt;Install all dependencies to a new virtual environment in the directory &lt;code&gt;.venv&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Create a &lt;code&gt;uv.lock&lt;/code&gt;file in your directory, which saves the exact, platform-agnostic version of every package installed ‚Äî meaning that other colleagues can replicate your Python environment exactly.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In principle, you can ‚Äòactivate‚Äô this new virtual environment like any typical virtual environment that you may have seen in other tools, but the most ‚Äòuv-onic‚Äô way to use uv is simply to prepend any command with &lt;code&gt;uv run&lt;/code&gt;. This command automatically picks up the correct virtual environment for you and runs your command with it. For instance, to run a script ‚Äî instead of&lt;/p&gt;
    &lt;code&gt;source .venv/bin/activate
python myscript.py&lt;/code&gt;
    &lt;p&gt;you can just do&lt;/p&gt;
    &lt;code&gt;uv run myscript.py&lt;/code&gt;
    &lt;p&gt;which will have the same effect. Likewise, to use a ‚Äòtool‚Äô like Jupyter Lab, you can just do&lt;/p&gt;
    &lt;code&gt;uv run jupyter lab&lt;/code&gt;
    &lt;p&gt;in your project‚Äôs directory, as opposed to first ‚Äòactivating‚Äô the environment and then running &lt;code&gt;jupyter lab&lt;/code&gt; separately.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adding dependencies&lt;/head&gt;
    &lt;p&gt;You can always just edit your &lt;code&gt;pyproject.toml&lt;/code&gt; file manually: uv will detect the changes and rebuild your project‚Äôs virtual environment. But uv also has easier ways to add dependencies ‚Äî you can just do&lt;/p&gt;
    &lt;code&gt;uv add numpy&amp;gt;=2.0&lt;/code&gt;
    &lt;p&gt;to add a package, including specifying version constraints (like the above.) This command automatically edits your &lt;code&gt;pyproject.toml&lt;/code&gt; for you. &lt;code&gt;uv add&lt;/code&gt; is also extremely powerful for adding remote dependencies from git or elsewhere on your computer (but I won‚Äôt get into that here.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Pinning a Python version&lt;/head&gt;
    &lt;p&gt;Finally, I think that one of the most useful things uv can do is to pin a specific Python version for your project. Doing&lt;/p&gt;
    &lt;code&gt;uv python pin 3.12.9&lt;/code&gt;
    &lt;p&gt;would pin the current project to exactly Python 3.12.9 for you, and anyone else using uv ‚Äî meaning that you really can replicate the exact same Python install across multiple machines.&lt;/p&gt;
    &lt;head rend="h2"&gt;uvx: ignore all of the above and just run a tool, now!&lt;/head&gt;
    &lt;p&gt;But sometimes, you might just want to run a tool quickly ‚Äî like using Ruff to lint code somewhere, or starting a Jupyter notebook server without an environment, or even just quickly starting an IPython session with pandas installed so you can open up a file. The &lt;code&gt;uv tool&lt;/code&gt; command, which has a short alias &lt;code&gt;uvx&lt;/code&gt;, makes this insanely easy. Running a command like&lt;/p&gt;
    &lt;code&gt;uvx ruff&lt;/code&gt;
    &lt;p&gt;will automatically download the tool you want to use and run it in a one-off virtual environment. Once the tool has been downloaded before, this is lightning-fast because of how uv uses caches.&lt;/p&gt;
    &lt;p&gt;There are a lot of occasions when I might want to do this ‚Äî a common one might be to quickly start an IPython session with pandas installed (using &lt;code&gt;--with&lt;/code&gt; to add dependencies) so that I can quickly open &amp;amp; look at a parquet file. For instance:&lt;/p&gt;
    &lt;code&gt;uvx --with pandas,pyarrow ipython&lt;/code&gt;
    &lt;p&gt;Or, maybe just starting a Jupyter Lab server so that I can quickly open a Jupyter notebook that a student sent me:&lt;/p&gt;
    &lt;code&gt;uvx jupyter lab&lt;/code&gt;
    &lt;p&gt;Or honestly just so many other weird, one-off use cases where &lt;code&gt;uvx&lt;/code&gt; is really nice to have around. I don‚Äôt feel like I‚Äôm missing out by always using virtual environments, because &lt;code&gt;uvx&lt;/code&gt; always gives you a ‚Äòget out of jail free‚Äô card whenever you need it.&lt;/p&gt;
    &lt;head rend="h2"&gt;If that hasn‚Äôt sold you: a personal note&lt;/head&gt;
    &lt;p&gt;I first discovered uv last year, while working together with our other lovely developers on building The Astrosky Ecosystem ‚Äî a wonderful project to build open-source social media integrations for astronomers online. But with multiple developers all working asynchronously on multiple operating systems, managing Python installations quickly became a huge task.&lt;/p&gt;
    &lt;p&gt;uv is an incredibly powerful simplification for us that we use across our entire tech stack. As developers, we can all work with identical Python installations, which is especially important given a number of semi-experimental dependencies that we use that have breaking changes with every version. On GitHub Actions, we‚Äôre planning to use uv to quickly build a Python environment and run our unit tests. In production, uv already manages Python for all of our servers.&lt;/p&gt;
    &lt;p&gt;It‚Äôs just so nice to always know that Python and package installation will always be handled consistently and correctly across all of our machines. That‚Äôs why uv is the best thing to happen to the Python ecosystem in a decade.&lt;/p&gt;
    &lt;head rend="h2"&gt;Find out more&lt;/head&gt;
    &lt;p&gt;There‚Äôs a lot more on the uv docs, including a getting started page, more in-depth guides, explanations of important concepts, and a full command reference.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45751400</guid><pubDate>Wed, 29 Oct 2025 18:57:29 +0000</pubDate></item><item><title>Raspberry Pi Pico Bit-Bangs 100 Mbit/S Ethernet</title><link>https://www.elektormagazine.com/news/rp2350-bit-bangs-100-mbit-ethernet</link><description>&lt;doc fingerprint="4f9925396be232bf"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Raspberry Pi RP2040 or RP2350 Bit-Bangs 100 Mbit/s Ethernet&lt;/head&gt;
    &lt;p&gt;on&lt;/p&gt;
    &lt;p&gt;Three years ago, @kingyoPiyo‚Äôs Pico-10BASE-T project drew wide attention right here on Elektor for implementing 10 Mbit/s Ethernet on the Raspberry Pi Pico using just a few resistors. In 2023, another milestone followed with bit-banged USB, showing how far the RP2040‚Äôs (and now RP2350) programmable I/O could be pushed.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Can an RP2350 Bit-Bang Next?&lt;/head&gt;
    &lt;p&gt;Now, developer Steve Markgraf (GitHub @steve-m) has extended the concept with Pico-100BASE-TX ‚Äî a 100 Mbit/s Fast Ethernet transmitter running entirely in software.&lt;lb/&gt; Markgraf‚Äôs implementation uses the PIO and DMA to perform MLT-3 encoding, 4B5B line coding, and scrambling at a 125 MHz symbol rate. The result is a functioning 100 Mbit/s link capable of streaming about 11 Mbyte/s over UDP, demonstrated by real-time audio and ADC data streams.&lt;/p&gt;
    &lt;p&gt;As before, this is a transmit-only proof of concept and must not be connected to PoE-enabled hardware. A pulse transformer or intermediary Ethernet switch is recommended for isolation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Check Out the Rest of His Repo&lt;/head&gt;
    &lt;p&gt;Example applications in the repository include a counter, internal-ADC streamer, and an audio demo using a PCM1802 converter at 75 kHz. The library supports both the RP2040 and the newer RP2350 (Pico 2) and builds with the standard Pico SDK.&lt;/p&gt;
    &lt;p&gt;Source: Pico-100BASE-TX on GitHub ‚Äî check it in action in the video there.&lt;/p&gt;
    &lt;p&gt;Beyond the technical achievement, projects like this hint at new possibilities for low-cost, high-speed data acquisition and streaming using microcontrollers that were never designed for it. A Pico capable of pushing 11 MB/s over Ethernet could form the basis of compact, inexpensive test instruments, remote sensors, or experimental network interfaces ‚Äî all without a dedicated PHY chip. As these bit-banged interfaces become faster and more capable, the question naturally follows: how far can software-defined hardware really go on a two-dollar microcontroller?&lt;/p&gt;
    &lt;p/&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45754439</guid><pubDate>Wed, 29 Oct 2025 23:21:51 +0000</pubDate></item><item><title>IRCd service (2024)</title><link>https://example.fi/blog/ircd.html</link><description>&lt;doc fingerprint="3516fd1c7c8c88c8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;IRCd service&lt;/head&gt;&lt;head rend="h2"&gt;10.6.2024&lt;/head&gt; Ok. We have IRC at example.fi&lt;lb/&gt;Internet Relay Chat (IRC) is a form of real-time text communication developed by Jarkko Oikarinen in 1988. Initially created to replace a local BBS system at the University of Oulu in Finland, IRC quickly gained global popularity, becoming a foundational technology for online chat communities and influencing the development of modern instant messaging and social media platforms. Its significance lies in its pioneering role in connecting people across the internet, fostering early online communities, and setting the stage for contemporary digital communication.&lt;lb/&gt;To commemorate this pivotal technology, example.fi provides a simple and limited IRC server. This server is uniquely written in AWK, a scripting language traditionally used for text processing, highlighting the adaptability and enduring legacy of IRC. This creative implementation serves as both an educational tool and a tribute to the foundational role of IRC in the evolution of online communication.&lt;lb/&gt;In the following picture you see Irssi in the background and Hexchat on top of it:&lt;lb/&gt;Note: if you plan to connect to example.fi, make sure you do not use any fancy features. In irssi, use -nocap option. In Windows, use for example hexchat.
As this is written in gawk, most IRC protocol features are not implemented. This includes, for example, channel and user listings, topics, the concept of "operator" etc.
Technical fun fact: Total code count is around 60 lines of awk and a few lines of bash.&lt;quote&gt; $ telnet example.fi ircd Trying 65.108.91.190... Connected to example.fi. Escape character is '^]'. USER foo NICK bar :example.fi 001 bar :Welcome to Internet Relay Network bar!~foo@65.108.91.190 :example.fi 375 test :- example.fi Message of the day - :example.fi 372 test :- Current time is @787.188.beats :example.fi 376 test :End of MOTD command. Connection closed by foreign host. &lt;/quote&gt;&lt;lb/&gt;Don't worry, we'll publish the code when it's "ready" :)&lt;p&gt;This site is HTML 2.0 compliant.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45755788</guid><pubDate>Thu, 30 Oct 2025 02:31:43 +0000</pubDate></item><item><title>Hello-World iOS App in Assembly</title><link>https://gist.github.com/nicolas17/966a03ce49f949dd17b0123415ef2e31</link><description>&lt;doc fingerprint="ad2ec40e6b187f6b"&gt;
  &lt;main&gt;
    &lt;p&gt; Last active &lt;relative-time&gt;October 30, 2025 14:58&lt;/relative-time&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;tool-tip&gt;Save nicolas17/966a03ce49f949dd17b0123415ef2e31 to your computer and use it in GitHub Desktop.&lt;/tool-tip&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; hello-world iOS app &lt;/p&gt;
    &lt;p&gt; This file contains hidden or bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters &lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;.global _main&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;.extern _putchar&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;.align 4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;_main:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; prolog; save fp,lr,x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x29, x30, [sp, #-0x20]!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str x19, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x29, sp&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; make space for 2 dword local vars&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sub sp, sp, #0x10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; save argc/argv&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x0, x1, [sp]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; create autorelease pool and save into x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_autoreleasePoolPush&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; initialize app delegate class&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl initAppDelegate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; create CFString with delegate class name&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, 0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x1, str_AppDelegate@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x1, x1, str_AppDelegate@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x2, 0x0600 ; kCFStringEncodingASCII&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _CFStringCreateWithCString&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = UIApplicationMain(argc, argv, nil, CFSTR("AppDelegate"));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x3, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldr x0, [sp]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldr x1, [sp, #0x8]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x2, #0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _UIApplicationMain&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x7, x0 ; save retval&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; pop autorelease pool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_autoreleasePoolPop&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; epilog&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; restore stack pointer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add sp, sp, 0x10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; restore saved registers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldr x19, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp x29, x30, [sp], #0x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; get retval&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ret&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;initAppDelegate:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; prolog; save fp,lr,x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x29, x30, [sp, #-0x20]!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str x20, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x29, sp&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; Class c = objc_allocateClassPair(objc_getClass("NSObject"), "AppDelegate", 0);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_NSObject@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_NSObject@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getClass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x1, str_AppDelegate@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x1, x1, str_AppDelegate@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x2, 0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_allocateClassPair&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; save the class since we'll clobber x0 several times&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x20, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; class_addProtocol(c, objc_getProtocol("UIApplicationDelegate"));&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_UIAppDelegate@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_UIAppDelegate@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getProtocol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _class_addProtocol&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; class_addMethod(c, S("application:didFinishLaunchingWithOptions:"), didFinishLaunching, "B@:@@");&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_didFinishLaunchingSel@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_didFinishLaunchingSel@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adr x2, didFinishLaunching&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x3, str_typestr@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x3, x3, str_typestr@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _class_addMethod&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; objc_registerClassPair(c);&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_registerClassPair&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; epilog&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldr x20, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp x29, x30, [sp], #0x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ret&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; parameters:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0: self&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x1: _sel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x2: application&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x3: launchOptions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;didFinishLaunching:&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; prolog, save fp, lr, x19-x22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x29, x30, [sp, #-0x30]!&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x19, x20, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp x21, x22, [sp, #0x20]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x29, sp&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;sub sp, sp, 0x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 = @selector(mainScreen)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_mainScreen@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_mainScreen@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; objc_getClass("UIScreen")&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_UIScreen@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_UIScreen@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getClass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x20 = [UIScreen mainScreen]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x20, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 is now free&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x1 = @selector(bounds)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_bounds@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_bounds@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; [x20 bounds]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp d0, d1, [sp]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;stp d2, d3, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 = @selector(initWithFrame:)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_initWithFrame@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_initWithFrame@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = UIWindow&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_UIWindow@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_UIWindow@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getClass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = class_createInstance(x0)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, #0x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _class_createInstance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 now has the instance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x20 = [x0 initWithFrame:d]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x19 ;initWithFrame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp d0, d1, [sp]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp d2, d3, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x20, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 = @selector(init)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_init@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_init@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = UIViewController&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_UIViewController@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_UIViewController@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getClass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = class_createInstance(UIViewController)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, #0x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _class_createInstance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 now has the instance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x21 = [x0 init]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x19 ;init&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x21, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 = @selector(yellowColor)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_yellowColor@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_yellowColor@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x22 = [UIColor yellowColor]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_UIColor@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_UIColor@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_getClass&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x22, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x19 = @selector(setBackgroundColor:)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_setBackgroundColor@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_setBackgroundColor@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x19, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x1 = @selector(view)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_view@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_view@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; x0 = [[controller view] setBackgroundColor: x22];&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x2, x22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_setRoot@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_setRoot@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; [window setRootViewController:viewController]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x2, x21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; [x20 makeKeyAndVisible]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;adrp x0, str_makeKeyAndVisible@PAGE&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add x0, x0, str_makeKeyAndVisible@PAGEOFF&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _sel_getUid&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x1, x0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;bl _objc_msgSend&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; return YES&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;mov x0, #0x1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;; epilog&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;add sp, sp, 0x20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp x19, x20, [sp, #0x10]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp x21, x22, [sp, #0x20]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ldp x29, x30, [sp], #0x30&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ret&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;.data&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_NSObject: .asciz "NSObject"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_AppDelegate: .asciz "AppDelegate"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_UIAppDelegate: .asciz "UIApplicationDelegate"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_UIScreen: .asciz "UIScreen"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_UIWindow: .asciz "UIWindow"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_UIViewController: .asciz "UIViewController"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_UIColor: .asciz "UIColor"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_typestr: .asciz "B@:@@"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_didFinishLaunchingSel: .asciz "application:didFinishLaunchingWithOptions:"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_mainScreen: .asciz "mainScreen"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_bounds: .asciz "bounds"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_initWithFrame: .asciz "initWithFrame:"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_makeKeyAndVisible: .asciz "makeKeyAndVisible"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_init: .asciz "init"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_view: .asciz "view"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_setBackgroundColor: .asciz "setBackgroundColor:"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;str_yellowColor: .asciz "yellowColor"&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;str_setRoot: .asciz "setRootViewController:"&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Could you provide build/deploy steps?&lt;/p&gt;
    &lt;quote&gt;
      &lt;code&gt;; make space for 2 dword local vars&lt;/code&gt;
    &lt;/quote&gt;
    &lt;p&gt;not an ARM guy, but shouldn't it be 4 dwords or 2 qwords?&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;not an ARM guy, but shouldn't it be 4 dwords or 2 qwords?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;On ARM (32-bit and 64-bit) platforms, "word" commonly refers to as a 32-bit value&lt;/p&gt;
    &lt;p&gt;good to know, thought it was the same as x86&lt;/p&gt;
    &lt;p&gt;Line 37, do you have a guarantee that _objc_autoreleasePoolPop preserves x7?&lt;/p&gt;
    &lt;p&gt;You could also pop x0 and x1 at once on line 33.&lt;/p&gt;
    &lt;p&gt; Sign up for free to join this conversation on GitHub. Already have an account? Sign in to comment &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45755821</guid><pubDate>Thu, 30 Oct 2025 02:37:35 +0000</pubDate></item><item><title>Language models are injective and hence invertible</title><link>https://arxiv.org/abs/2510.15511</link><description>&lt;doc fingerprint="ec9c2c28c6b5a68f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Machine Learning&lt;/head&gt;&lt;p&gt; [Submitted on 17 Oct 2025 (v1), last revised 21 Oct 2025 (this version, v3)]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Language Models are Injective and Hence Invertible&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:Transformer components such as non-linear activations and normalization are inherently non-injective, suggesting that different inputs could map to the same output and prevent exact recovery of the input from a model's representations. In this paper, we challenge this view. First, we prove mathematically that transformer language models mapping discrete input sequences to their corresponding sequence of continuous representations are injective and therefore lossless, a property established at initialization and preserved during training. Second, we confirm this result empirically through billions of collision tests on six state-of-the-art language models, and observe no collisions. Third, we operationalize injectivity: we introduce SipIt, the first algorithm that provably and efficiently reconstructs the exact input text from hidden activations, establishing linear-time guarantees and demonstrating exact invertibility in practice. Overall, our work establishes injectivity as a fundamental and exploitable property of language models, with direct implications for transparency, interpretability, and safe deployment.&lt;/quote&gt;&lt;head rend="h2"&gt;Submission history&lt;/head&gt;From: Andrea Santilli [view email]&lt;p&gt;[v1] Fri, 17 Oct 2025 10:25:30 UTC (3,980 KB)&lt;/p&gt;&lt;p&gt;[v2] Mon, 20 Oct 2025 07:29:02 UTC (3,980 KB)&lt;/p&gt;&lt;p&gt;[v3] Tue, 21 Oct 2025 14:44:49 UTC (3,980 KB)&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;p&gt; IArxiv Recommender (What is IArxiv?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45758093</guid><pubDate>Thu, 30 Oct 2025 09:47:24 +0000</pubDate></item><item><title>Show HN: In a single HTML file, an app to encourage my children to invest</title><link>https://roberdam.com/en/dinversiones.html</link><description>&lt;doc fingerprint="26c9c0412db7d00b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;I Built an App to Encourage My Kids to Invest √¢ Just One HTML File&lt;/head&gt;
    &lt;p&gt;√¢What comes with the milk, leaves with the soul√¢&lt;lb/&gt; √¢ Russian proverb.&lt;/p&gt;
    &lt;p&gt;Access the app:&lt;lb/&gt; Click here to open and install D-i&lt;del&gt;n&lt;/del&gt;vestments&lt;/p&gt;
    &lt;p&gt;One thing that school doesn√¢t teach you (not even high school) is how to manage your personal finances.&lt;/p&gt;
    &lt;p&gt;As my eldest son√¢s birthday was approaching, we suggested that instead of asking for physical gifts, he ask for their equivalent in money. That way, he gathered a decent amount of capital for his first investment adventure.&lt;/p&gt;
    &lt;p&gt;I explained to my kids that investing is like having a magic box that generates more money over time. To make it more visual and interactive, I decided to create a small app where they could see their investment grow day by day.&lt;/p&gt;
    &lt;head rend="h1"&gt;From Idea to App&lt;/head&gt;
    &lt;p&gt;My first idea was to build a physical piggy bank with a display, showing the accumulated amount. However, that mixed up the concept of saving with investing, and also required buying extra hardware.&lt;/p&gt;
    &lt;p&gt;So I looked for a quicker, cheaper way: revive an old smartphone and create a simple app using plain HTML.&lt;/p&gt;
    &lt;p&gt;The result was D-i&lt;del&gt;n&lt;/del&gt;vestments, a mix between Diversions and Investments.&lt;/p&gt;
    &lt;head rend="h1"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;The app is essentially a single HTML file that installs on the phone as a PWA (Progressive Web App).&lt;/p&gt;
    &lt;p&gt;The phone is attached to the fridge and works as a panel or dashboard where my kids can see their money growing each day.&lt;/p&gt;
    &lt;p&gt;I act as their investment agent, assigning realistic interest rates √¢ high enough to keep them motivated, but moderate enough to reflect how the real world works.&lt;/p&gt;
    &lt;head rend="h2"&gt;Configuration Screen&lt;/head&gt;
    &lt;p&gt;The app includes a screen where you can enter:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The kids√¢ names&lt;/item&gt;
      &lt;item&gt;The invested amount&lt;/item&gt;
      &lt;item&gt;The interest rate&lt;/item&gt;
      &lt;item&gt;The start date&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With that data, the app automatically calculates and displays:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Daily gain&lt;/item&gt;
      &lt;item&gt;Weekly gain&lt;/item&gt;
      &lt;item&gt;Monthly gain&lt;/item&gt;
      &lt;item&gt;Total updated balance&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Materials Used&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;An old smartphone&lt;/item&gt;
      &lt;item&gt;A suction mount to attach it to the fridge&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The D-iNvestments app, in HTML format&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Installation&lt;/head&gt;
    &lt;p&gt;The process is as simple as opening the link from a smartphone and tapping √¢Install√¢ when prompted by the browser.&lt;lb/&gt; From then on, it behaves like a native app.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Access the app:&lt;/p&gt;&lt;lb/&gt;Click here to open and install D-i&lt;del&gt;n&lt;/del&gt;vestments&lt;/quote&gt;
    &lt;head rend="h1"&gt;Final Reflection&lt;/head&gt;
    &lt;p&gt;The goal wasn√¢t just to teach my kids the value of money, but to show them visually how investment and time work as allies.&lt;/p&gt;
    &lt;p&gt;Each day, as they watch their small fund grow, they grasp the magic of compound interest √¢ and that, more than any gift, is a lesson I hope will stay with them for life.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;√∞¬¨ Want to comment or improve the app? Contact me at:&lt;/p&gt;&lt;lb/&gt;@roberdam&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45758421</guid><pubDate>Thu, 30 Oct 2025 10:39:21 +0000</pubDate></item><item><title>Estimating the Perceived 'Claustrophobia' of New York City's Streets (2024)</title><link>http://mfranchi.net/posts/claustrophobic-streets/</link><description>&lt;doc fingerprint="20890016823e03c7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Estimating the Perceived 'Claustrophobia' of New York City's Streets&lt;/head&gt;
    &lt;p&gt;Hi! I‚Äôm back with a supplement to a piece in the New York Times‚Äô Street Wars series, which is all about the battle for navigable urban space in New York City. In this article, I‚Äôll go more in-depth about how I devised &amp;amp; computed the claustrophobic metric. We plan to explore this metric further through an in-progress research paper; keep an eye out for that later this year! Let‚Äôs begin.&lt;/p&gt;
    &lt;p&gt;New York City is a large place; almost 469 square miles of pretty dense civilization. Within the city, there are thousands of miles of sidewalks. As you walk through different neighborhoods, you may experience a variety of different atmospheres. In Cobble Hill, Brooklyn, it‚Äôs quaint and quiet. In SoHo these days, there are so many pedestrians that they spill off the narrow sidewalks. While a neighborhood‚Äôs atmosphere is, of course, a function of time, it is possible to get an average consensus of how ‚Äòcrowded‚Äô each neighborhood feels by averaging over time. When we say ‚Äòcrowded‚Äô, we mean not just with people; we also mean with static objects, or street furniture, or, to get even more colloquial, ‚Äòclutter‚Äô. When we mix ‚Äòcrowdedness‚Äô within the narrow environment of NYC‚Äôs sidewalks, we endeavor to call this feeling ‚Äòclaustrophobia‚Äô, a direct mapping to the definition in psychology.&lt;/p&gt;
    &lt;p&gt;Now, we‚Äôll discuss how the metric of sidewalk ‚Äòclaustrophobia‚Äô was calculated. Then, I‚Äôll talk briefly about how this metric might be interesting and useful to a variety of different stakeholders.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology - Segmentization&lt;/head&gt;
    &lt;p&gt;We start with the official planimetric database of NYC‚Äôs sidewalks from NYC OpenData at this link. However, the geometries for each sidewalk here are stored as multi-polygons, instead of at the per-segment level. Further, the geometry can be quite complicated, in fact, overly complex for the purposes of our analysis. To mitigate these problems, we perform the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Simplify geometry using Shapely library&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Here, we first simplify the sidewalk geometry to reduce some of the complexity in the street network. We visually inspect several different neighborhoods and find that this minimally changes the shape of the network while moderately reducing the number of points after segmentization.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Segmentize points along sidewalks at least every 50 feet.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Then, we segmentize the simplified sidewalk network. Segmentization is a process that evenly samples points along each sidewalk, at a predetermined threshold. We use a threshold of 50 feet to balance computational complexity and storage constraints with accuracy.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Methodology - Bringing in Clutter&lt;/head&gt;
    &lt;p&gt;For this computational analysis, ‚Äòclutter‚Äô is anything that takes up space on the sidewalk. Narratively, some clutter is aesthetic or unminded by pedestrians (like trees, most seating); then, things like scaffolding are denotatively and connotatively ‚Äòclutter‚Äô. To identify different types of clutter, I took walks around several different neighborhoods in Brooklyn, Queens, and Manhattan, writing down the different things that I saw. At this point, I tried to match each type of street furniture I saw with a dataset on NYC OpenData, which is a great, official portal that stores hundred of city-related datasets from dozens of city agencies like the Department of Transportation and NYC Parks. To save space, I list all of the datasets I used in next section‚Äôs table, along with an access link.&lt;/p&gt;
    &lt;p&gt;We assign points to different clutters with spatial joins. For each point, we add a buffer (think of this as a larger ring, centered at the point) of 25 feet. These buffers act as a net, ‚Äòcatching‚Äô nearby pieces of clutter. Multiple points may count a piece of clutter as ‚Äòtheirs‚Äô if the clutter is within both points‚Äô buffer area.&lt;/p&gt;
    &lt;head rend="h3"&gt;Methodology - Weighting&lt;/head&gt;
    &lt;p&gt;We apply a weight to each clutter type based on its estimated size. I admit this is quite naive, and solely based on my ‚Äòexperience‚Äô as a pedestrian. In the below table, we present the weight allotted to each clutter type.&lt;/p&gt;
    &lt;p&gt;Possible ways to refine this include conducting a survey, or actually taking into account the square footage of each clutter type. Since some clutters have non-uniform sizes (for example, there are several different configurations of bus stops, each with a different size), and size data was unavailable for some clutter types, we stick with the naive approach for now.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Clutter Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Weight&lt;/cell&gt;
        &lt;cell role="head"&gt;OpenData Link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bus Stop Shelters&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Trash Can&lt;/cell&gt;
        &lt;cell&gt;0.5&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LinkNYC&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CityBench&lt;/cell&gt;
        &lt;cell&gt;1.5&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bicycle Parking Shelter&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Bicycle Rack&lt;/cell&gt;
        &lt;cell&gt;1.5&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tree&lt;/cell&gt;
        &lt;cell&gt;0.15&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Newsstand&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Parking Meter&lt;/cell&gt;
        &lt;cell&gt;0.15&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scaffolding&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Fire Hydrant&lt;/cell&gt;
        &lt;cell&gt;0.25&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Street Signs&lt;/cell&gt;
        &lt;cell&gt;0.05&lt;/cell&gt;
        &lt;cell&gt;link&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Methodology - Traffic&lt;/head&gt;
    &lt;p&gt;We derive our foot traffic estimates via large-scale, crowdsourced dashcam data provided by Nexar, Inc. Nexar is a company that manufactures dashcams and explores how downstream data can help make more useful and accurate maps. Of course, these images tell us nothing about pedestrians by themselves. We detect the number of pedestrians in each image via YOLOv7-E6E, a state-of-the-art object detection model with well-documented success in this task.&lt;/p&gt;
    &lt;p&gt;Dashcam data points are stored at the latitude/longitude level with a 0-360 ranged directional heading. So, in other words, if you were plotting each point on a map, you‚Äôd know exactly where to put it on the map, and you‚Äôd also know which direction to put an arrow facing outwards from the point. We then project the points from a Cartesian coordinate system to a NYC-specific projected coordinate system for increased accuracy. To combine the position and the heading, we further increase positional accuracy by creating ‚Äòcones‚Äô to represent the actual field-of-view of the vehicle at the time/place of capture.&lt;/p&gt;
    &lt;p&gt;With access to sidewalk width (in feet) from the basemap described earlier, we compute the number of pedestrians per foot of sidewalk width, at each image. This traffic data is sliced at one-hour increments (this is arbitrary), but our main plots don‚Äôt go to this granularity and instead converge at ‚Äòa typical day of traffic‚Äô in NYC in August 2023.&lt;/p&gt;
    &lt;head rend="h2"&gt;Limitations&lt;/head&gt;
    &lt;p&gt;This work is very exciting for us, but still has some unconquered limitations.&lt;/p&gt;
    &lt;head rend="h3"&gt;Missing Data Streams&lt;/head&gt;
    &lt;p&gt;First, there are several clutter types that we identified in our walks around the city, but couldn‚Äôt find matching datasets for. This includes, but is not limited to: - Sidewalk eating - Roadside eating - Street lights (could infer via 311 complaints, but haven‚Äôt yet done) - Red legacy FDNY alarm boxes - Cellars (not a problem unless open) - Sidewalk plant beds - USPS / Package drop-off containers - Streetside produce markets&lt;/p&gt;
    &lt;p&gt;Separately, for our dashcam-computed foot traffic counts, we are missing data for about 36.11% of the segmentized points in NYC. This isn‚Äôt a huge problem, as we aggregate data at much larger geographic groupings like Neighborhood Tabulation Areas and Census Tracts. Nonetheless, it would be more ideal to use a dataset with more complete coverage of the city. The more temporally and geographically dense data we have, the more we can ‚Äòzoom‚Äô in.&lt;/p&gt;
    &lt;head rend="h3"&gt;Imprecise Data Streams&lt;/head&gt;
    &lt;p&gt;In addition to missing data streams, we also use some that are notably imprecise.&lt;/p&gt;
    &lt;p&gt;For example, New York City‚Äôs sidewalk scaffolding is logged at the building permit level, so we compute a radial 150-foot outward buffer to capture all nearby points; in reality, only part of a building‚Äôs perimeter will actually have the scaffolding.&lt;/p&gt;
    &lt;p&gt;Other imprecision comes from ‚Äúold‚Äù data; for most of the clutter types, we‚Äôre able to filter by construction date, meaning that clutters built after the end of our traffic data aren‚Äôt included. However, this isn‚Äôt possible for all clutter types (we look at this more closely in the ‚Äúclutter.ipynb‚Äù notebook on the claustrophobic-streets GitHub repository).&lt;/p&gt;
    &lt;head rend="h2"&gt;Results&lt;/head&gt;
    &lt;p&gt;Let‚Äôs start with some spatial visualizations. Below, we map our calculated levels of claustrophobia at the Neighborhood Tabulation Area (NTA) level. NTAs are roughly approximate to New Yorkers‚Äô mental maps of neighborhoods. Some interesting trends emerge that made sense at first glance, at least for me! Namely, most of Midtown Manhattan sees the highest ‚Äòclaustrophobia‚Äô levels. This aligns with my anecdotal experience of trudging through crowds in and around Times Square that were quite literally stationary for 15 seconds at a time. However, at this small granularity, we can‚Äôt really see where other hotspots emerge clearly, at least from the map‚Äôs coloring. Let‚Äôs try zooming in.&lt;/p&gt;
    &lt;p&gt;Now, we map our calculated levels of claustrophobia at the Census Tract (CT) level. CTs are much smaller; in 2020, NYC was composed of 2,327 of them. Here, more interesting visual trends emerge. I see areas in Queens colored in red that I remember being extremely crowded when I visited; including Jackson Heights (near LaGuardia Airport) and Flushing (much further out in Queens, at the end of the 7 train). Downtown Brooklyn and Williamsburg also see notably higher-than-average levels of claustrophobia, which lines up with my own experiences. For both maps, Staten Island tends to be colored entirely in blue, meaning lower-than-average claustrophobia; I‚Äôve still not taken the ferry over, so I won‚Äôt make any definitive claims, but this at least aligns with the borough‚Äôs higher usage of cars, relative to the rest of the city.&lt;/p&gt;
    &lt;p&gt;Lastly, for name recognition, we plot the top 20 and bottom 20 neighborhoods, relative to the city average. See if your neighborhood pops up in either list!&lt;/p&gt;
    &lt;p&gt;For some additional plots, including zoom-ins of each borough, and density maps of foot traffic and street clutter, check out the GitHub repository for this project at github.com/mattwfranchi/claustrophobic-streets. If you have any suggestions or questions, feel free to email me at mwf62 AT cornell.edu, or open an issue on the GitHub repository. Thanks for reading!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45759649</guid><pubDate>Thu, 30 Oct 2025 13:10:03 +0000</pubDate></item><item><title>The International Criminal Court wants to become independent of USA technology</title><link>https://www.heise.de/en/news/International-Criminal-Court-Kicks-Out-Microsoft-10964189.html</link><description>&lt;doc fingerprint="67d9a05ff64d88c7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;International Criminal Court Kicks Out Microsoft&lt;/head&gt;
    &lt;p&gt;According to Handelsblatt, the International Criminal Court is kicking out US service providers like Microsoft. And is relying on German alternatives.&lt;/p&gt;
    &lt;p&gt;The International Criminal Court (ICC) wants to become independent of technology from the USA ‚Äì for fear of reprisals from Donald Trump, Handelsblatt has learned. The institution in The Hague wants to replace the Microsoft software currently used on workstations with OpenDesk.&lt;/p&gt;
    &lt;p&gt;According to Handelsblatt, the decision is to be seen against the backdrop of sanctions by the current US administration under President Donald Trump against employees such as Chief Prosecutor Karim Khan. Microsoft simply blocked his email access. He therefore had to switch to the Swiss email service Proton. Since the ICC is highly dependent on service providers like Microsoft, its work is being paralyzed, it was stated in May.&lt;/p&gt;
    &lt;p&gt;Furthermore, the US government in Washington is examining further measures against the International Criminal Court, Handelsblatt further reports. This could also significantly restrict the institution's ability to work.&lt;/p&gt;
    &lt;head rend="h3"&gt;Achieving Digital Sovereignty&lt;/head&gt;
    &lt;p&gt;The OpenDesk software is developed by the Center for Digital Sovereignty (Zendis), a federal company. Its task is to help resolve critical dependencies on individual technology providers.&lt;/p&gt;
    &lt;p&gt;Videos by heise&lt;/p&gt;
    &lt;p&gt;At the International Criminal Court, it's ‚Äúonly‚Äù about 1800 workstations that are to be freed from US dependency. However, Handelsblatt sees this as an indication that geopolitics is increasingly revolving around technology. Businesses and politicians recognize the dependence on US digital corporations as a problem, especially regarding the fact that the USA uses technology as a means of pressure.&lt;/p&gt;
    &lt;p&gt;The ICC is not alone in these ambitions. For example, the Public Health Service wants to use OpenDesk, and the German Armed Forces has a framework agreement with Zendis has concluded a framework agreement on ‚Äúsovereign communication and collaboration solutions‚Äù such as OpenDesk.&lt;/p&gt;
    &lt;p&gt;(dmk)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45759891</guid><pubDate>Thu, 30 Oct 2025 13:35:38 +0000</pubDate></item><item><title>3D solar tower increases capacity factor 50%, triples solar surface area</title><link>https://www.pv-magazine.com/2025/10/27/3d-solar-tower-increases-capacity-factor-50-triples-solar-surface-area/</link><description>&lt;doc fingerprint="6951b37b1ab6c53b"&gt;
  &lt;main&gt;
    &lt;p&gt;From pv magazine USA&lt;/p&gt;
    &lt;p&gt;Vertical three-dimensional solar tower developer Janta Power announced it has closed a $5.5 million seed funding round led by Mac Venture Capital and Collab Capital.&lt;/p&gt;
    &lt;p&gt;The funds are expected to help the startup scale its patented 3D solar towers that are designed to have high levels of energy density for space-constrained areas. The product has applications for data centers, EV charging hubs, telecom towers, universities, and a range of industrial facilities, said Janta Power.&lt;/p&gt;
    &lt;p&gt;Janta said its single-axis sun tracking tower‚Äôs geometry achieves approximately three times the solar surface area exposure of traditional flat arrays in the same land footprint. Each tower‚Äôs stacked vertical design captures both low-angle morning and evening light, producing a dual-peak power curve that better aligns with real energy demand, said the company.&lt;/p&gt;
    &lt;p&gt;The company said its solar tower has a capacity factor 50% greater than traditional solar installations. The capacity factor is the amount of actual electricity produced as compared to the theoretical maximum capacity of solar installed. While typical flat installations have a capacity factor of about 22%, Janta Power said its typical installations achieve a capacity factor of about 32%.&lt;/p&gt;
    &lt;p&gt;The company said it can achieve a levelized cost of electricity (LCOE) as low as $0.05 kWh.&lt;/p&gt;
    &lt;p&gt;The company currently offers a 5 kW solar tower, with 1.5 kW rooftop models as well as 8.5 kW and 10 kW towers under development.&lt;/p&gt;
    &lt;p&gt;The towers are currently deployed in pilot programs at major global airports, including Munich International Airport, and Dallas‚ÄìFort Worth International Airport.&lt;/p&gt;
    &lt;p&gt;Each structure is engineered to withstand winds of at least 110 miles per hour and as much as 170 mph, and features steel construction and modular helical or pier foundations that eliminate the need for extensive grading, said Janta Power.&lt;/p&gt;
    &lt;p&gt;While single-axis tracker installations concentrate output near midday, Janta said its 3D configuration smooths energy delivery throughout the day, reducing ramp stress on grids and lowering reliance on peaker plants.&lt;/p&gt;
    &lt;p&gt;This content is protected by copyright and may not be reused. If you want to cooperate with us and would like to reuse some of our content, please contact: editors@pv-magazine.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45759922</guid><pubDate>Thu, 30 Oct 2025 13:38:40 +0000</pubDate></item><item><title>US declines to join more than 70 countries in signing UN cybercrime treaty</title><link>https://therecord.media/us-declines-signing-cybercrime-treaty?</link><description>&lt;doc fingerprint="8b0db7af9a95f28f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;US declines to join more than 70 countries in signing UN cybercrime treaty&lt;/head&gt;
    &lt;p&gt;More than 70 countries signed the landmark U.N. Convention against Cybercrime in Hanoi this weekend, a significant step in the yearslong effort to create a global mechanism to counteract digital crime.&lt;/p&gt;
    &lt;p&gt;The U.K. and European Union joined China, Russia, Brazil, Nigeria and dozens of other nations in signing the convention, which lays out new mechanisms for governments to coordinate, build capacity and track those who use technology to commit crimes.&lt;/p&gt;
    &lt;p&gt;In his speech at the event, U.N. Secretary-General Ant√≥nio Guterres said cyberspace ‚Äúhas become fertile ground for criminals‚Äù and has allowed them to ‚Äúdefraud families, steal livelihoods, and drain billions of dollars from our economies.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúThe UN Cybercrime Convention is a powerful, legally binding instrument to strengthen our collective defences against cybercrime,‚Äù Guterres said.&lt;/p&gt;
    &lt;p&gt;‚ÄúIllicit flows of money, concealed through cryptocurrencies and digital transactions, finance the trafficking of drugs, arms, and terror. And businesses, hospitals, and airports are brought to a standstill by ransomware attacks.‚Äù&lt;/p&gt;
    &lt;p&gt;He added that the convention would be critical for governments in the Global South that need assistance and funding for the training required to address cybercrime ‚Äî which the U.N. estimates costs $10.5 trillion around the world annually.&lt;/p&gt;
    &lt;p&gt;While many countries did not sign the treaty, the most notable missing signature was that of the U.S.&lt;/p&gt;
    &lt;p&gt;Officials at the State Department told Recorded Future News on Friday that Marc Knapper, the U.S. ambassador to Vietnam, and representatives from the U.S. Mission to Vietnam would be attending the signing.&lt;/p&gt;
    &lt;p&gt;The State Department confirmed on Monday that the U.S. did not sign the treaty.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe United States continues to review the treaty,‚Äù a State Department spokesperson said in a brief statement.&lt;/p&gt;
    &lt;p&gt;The U.N. Convention against Cybercrime was adopted by the General Assembly in December 2024 and will enter into force 90 days after being ratified by the 40th signatory. Signatories will have to ratify the convention according to their own procedures.&lt;/p&gt;
    &lt;p&gt;At the ceremony, UNODC Executive Director Ghada Waly argued that cybercrime is changing the face of organized crime and required global coordination to address. Waly said the convention would be a ‚Äúvital tool‚Äù that will ensure ‚Äúa safer digital world for all.‚Äù&lt;/p&gt;
    &lt;p&gt;U.N. officials said the convention would help governments address terrorism, human trafficking, money laundering and drug smuggling, all of which have been turbo-charged by the internet.&lt;/p&gt;
    &lt;p&gt;The U.N. noted that the convention is the first global framework ‚Äúfor the collection, sharing and use of electronic evidence for all serious offenses‚Äù ‚Äî noting that until now there have been no broadly accepted international standards on electronic evidence.&lt;/p&gt;
    &lt;p&gt;It is also the first global treaty to criminalize crimes that depend on the internet and is the first international treaty ‚Äúto recognize the non-consensual dissemination of intimate images as an offense.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúIt creates the first global 24/7 network where countries can quickly initiate cooperation,‚Äù the U.N. said. ‚ÄúIt recognizes and promotes the need to build capacity in countries to pursue and cooperate on fast-moving cybercrimes.‚Äù&lt;/p&gt;
    &lt;p&gt;The convention has been heavily criticized by the tech industry, which has warned that it criminalizes cybersecurity research and exposes companies to legally thorny data requests.&lt;/p&gt;
    &lt;p&gt;Human rights groups warned on Friday that it effectively forces member states to create a broad electronic surveillance dragnet that would include crimes that have nothing to do with technology.&lt;/p&gt;
    &lt;p&gt;Many expressed concern that the convention will be abused by dictatorships and rogue governments who will deploy it against critics or protesters ‚Äî even those outside of a regime‚Äôs jurisdiction.&lt;/p&gt;
    &lt;p&gt;It also creates legal regimes to monitor, store and allow cross-border sharing of information without specific data protections. Access Now‚Äôs Raman Jit Singh Chima said the convention effectively justifies ‚Äúcyber authoritarianism at home and transnational repression across borders.‚Äù&lt;/p&gt;
    &lt;p&gt;Any countries ratifying the treaty, he added, risks ‚Äúactively validating cyber authoritarianism and facilitating the global erosion of digital freedoms, choosing procedural consensus over substantive human rights protection.‚Äù&lt;/p&gt;
    &lt;p&gt;In his speech, Guterres referenced the backlash to the convention, telling member states that the treaty has to be a ‚Äúpromise that fundamental human rights such as privacy, dignity, and safety must be protected both offline and online.‚Äù&lt;/p&gt;
    &lt;p&gt;But at its core, according to Guterres, the convention solves one of the thorniest issues law enforcement agencies have faced over the last two decades. Countries have only recently begun to share digital evidence across borders but the convention would increase that practice.&lt;/p&gt;
    &lt;p&gt;‚ÄúThis has long been a major obstacle to justice ‚Äî with perpetrators in one country, victims in another, and data stored in a third,‚Äù he said. ‚ÄúThe Convention provides a clear pathway for investigators and prosecutors to finally overcome this barrier.‚Äù&lt;/p&gt;
    &lt;p&gt;Jonathan Greig&lt;/p&gt;
    &lt;p&gt;is a Breaking News Reporter at Recorded Future News. Jonathan has worked across the globe as a journalist since 2014. Before moving back to New York City, he worked for news outlets in South Africa, Jordan and Cambodia. He previously covered cybersecurity at ZDNet and TechRepublic.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45760328</guid><pubDate>Thu, 30 Oct 2025 14:22:44 +0000</pubDate></item><item><title>Ventoy: Create Bootable USB Drive for ISO/WIM/IMG/VHD(x)/EFI Files</title><link>https://github.com/ventoy/Ventoy</link><description>&lt;doc fingerprint="9d3bb85bbdb73488"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt; Ventoy is an open source tool to create bootable USB drive for ISO/WIM/IMG/VHD(x)/EFI files. &lt;lb/&gt; With ventoy, you don't need to format the disk over and over, just copy the image files to the USB drive and boot them. You can copy many image files at a time and ventoy will give you a boot menu to select them. &lt;lb/&gt; You can also browse ISO/WIM/IMG/VHD(x)/EFI files in local disk and boot them.&lt;lb/&gt; x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI, ARM64 UEFI and MIPS64EL UEFI are supported in the same way.&lt;lb/&gt; Both MBR and GPT partition style are supported in the same way.&lt;lb/&gt; Most type of OS supported(Windows/WinPE/Linux/Unix/ChromeOS/Vmware/Xen...) &lt;lb/&gt; 1200+ ISO files are tested (List). 90%+ distros in distrowatch.com supported (Details). &lt;lb/&gt;Official Website: https://www.ventoy.net &lt;/head&gt;
    &lt;p&gt;Windows&lt;lb/&gt; Windows 7, Windows 8, Windows 8.1, Windows 10, Windows 11, Windows Server 2012, Windows Server 2012 R2, Windows Server 2016, Windows Server 2019, Windows Server 2022, Windows Server 2025, WinPE&lt;/p&gt;
    &lt;p&gt;Linux&lt;lb/&gt; Debian, Ubuntu, CentOS(6/7/8/9), RHEL(6/7/8/9), Deepin, Fedora, Rocky Linux, AlmaLinux, EuroLinux(6/7/8/9), openEuler, OpenAnolis, SLES, openSUSE, MX Linux, Manjaro, Linux Mint, Endless OS, Elementary OS, Solus, Linx, Zorin, antiX, PClinuxOS, Arch, ArcoLinux, ArchLabs, BlackArch, Obarun, Artix Linux, Puppy Linux, Tails, Slax, Kali, Mageia, Slackware, Q4OS, Archman, Gentoo, Pentoo, NixOS, Kylin, openKylin, Ubuntu Kylin, KylinSec, Lubuntu, Xubuntu, Kubuntu, Ubuntu MATE, Ubuntu Budgie, Ubuntu Studio, Bluestar, OpenMandriva, ExTiX, Netrunner, ALT Linux, Nitrux, Peppermint, KDE neon, Linux Lite, Parrot OS, Qubes, Pop OS, ROSA, Void Linux, Star Linux, EndeavourOS, MakuluLinux, Voyager, Feren, ArchBang, LXLE, Knoppix, Calculate Linux, Clear Linux, Pure OS, Oracle Linux, Trident, Septor, Porteus, Devuan, GoboLinux, 4MLinux, Simplicity Linux, Zeroshell, Android-x86, netboot.xyz, Slitaz, SuperGrub2Disk, Proxmox VE, Kaspersky Rescue, SystemRescueCD, MemTest86, MemTest86+, MiniTool Partition Wizard, Parted Magic, veket, Sabayon, Scientific, alpine, ClearOS, CloneZilla, Berry Linux, Trisquel, Ataraxia Linux, Minimal Linux Live, BackBox Linux, Emmabunt√ºs, ESET SysRescue Live,Nova Linux, AV Linux, RoboLinux, NuTyX, IPFire, SELKS, ZStack, Enso Linux, Security Onion, Network Security Toolkit, Absolute Linux, TinyCore, Springdale Linux, Frost Linux, Shark Linux, LinuxFX, Snail Linux, Astra Linux, Namib Linux, Resilient Linux, Virage Linux, Blackweb Security OS, R-DriveImage, O-O.DiskImage, Macrium, ToOpPy LINUX, GNU Guix, YunoHost, foxclone, siduction, Adelie Linux, Elive, Pardus, CDlinux, AcademiX, Austrumi, Zenwalk, Anarchy, DuZeru, BigLinux, OpenMediaVault, Ubuntu DP, Exe GNU/Linux, 3CX Phone System, KANOTIX, Grml, Karoshi, PrimTux, ArchStrike, CAELinux, Cucumber, Fatdog, ForLEx, Hanthana, Kwort, MiniNo, Redcore, Runtu, Asianux, Clu Linux Live, Uruk, OB2D, BlueOnyx, Finnix, HamoniKR, Parabola, LinHES, LinuxConsole, BEE free, Untangle, Pearl, Thinstation, TurnKey, tuxtrans, Neptune, HefftorLinux, GeckoLinux, Mabox Linux, Zentyal, Maui, Reborn OS, SereneLinux , SkyWave Linux, Kaisen Linux, Regata OS, TROM-Jaro, DRBL Linux, Chalet OS, Chapeau, Desa OS, BlankOn, OpenMamba, Frugalware, Kibojoe Linux, Revenge OS, Tsurugi Linux, Drauger OS, Hash Linux, gNewSense, Ikki Boot, SteamOS, Hyperbola, VyOS, EasyNAS, SuperGamer, Live Raizo, Swift Linux, RebeccaBlackOS, Daphile, CRUX, Univention, Ufficio Zero, Rescuezilla, Phoenix OS, Garuda Linux, Mll, NethServer, OSGeoLive, Easy OS, Volumio, FreedomBox, paldo, UBOS, Recalbox, batocera, Lakka, LibreELEC, Pardus Topluluk, Pinguy, KolibriOS, Elastix, Arya, Omoikane, Omarine, Endian Firewall, Hamara, Rocks Cluster, MorpheusArch, Redo, Slackel, SME Server, APODIO, Smoothwall, Dragora, Linspire, Secure-K OS, Peach OSI, Photon, Plamo, SuperX, Bicom, Ploplinux, HP SPP, LliureX, Freespire, DietPi, BOSS, Webconverger, Lunar, TENS, Source Mage, RancherOS, T2, Vine, Pisi, blackPanther, mAid, Acronis, Active.Boot, AOMEI, Boot.Repair, CAINE, DaRT, EasyUEFI, R-Drive, PrimeOS, Avira Rescue System, bitdefender, Checkra1n Linux, Lenovo Diagnostics, Clover, Bliss-OS, Lenovo BIOS Update, Arcabit Rescue Disk, MiyoLinux, TeLOS, Kerio Control, RED OS, OpenWrt, MocaccinoOS, EasyStartup, Pyabr, Refracta, Eset SysRescue, Linpack Xtreme, Archcraft, NHVBOOT, pearOS, SeaTools, Easy Recovery Essentional, iKuai, StorageCraft SCRE, ZFSBootMenu, TROMjaro, BunsenLabs, Todo en Uno, ChallengerOS, Nobara, Holo, CachyOS, Peux OS, Vanilla OS, ShredOS, paladin, Palen1x, dban, ReviOS, HelenOS, XeroLinux, Tiny 11, chimera linux, CuteFish, DragonOs, Rhino Linux, vanilladpup, crystal, IGELOS, MiniOS, gnoppix, PikaOS, UwUntu, Noble, PocketHandyBox, DiskGenius, ......&lt;/p&gt;
    &lt;p&gt;Unix&lt;lb/&gt; DragonFly, FreeBSD, pfSense, OPNsense, GhostBSD, FreeNAS, TrueNAS, XigmaNAS, FuryBSD, HardenedBSD, MidnightBSD, ClonOS, EmergencyBootKit, helloSystem&lt;/p&gt;
    &lt;p&gt;ChromeOS&lt;lb/&gt; FydeOS, CloudReady, ChromeOS Flex&lt;/p&gt;
    &lt;p&gt;Other&lt;lb/&gt; VMware ESXi, Citrix XenServer, Xen XCP-ng&lt;/p&gt;
    &lt;p&gt;„ÄêHow to report a successfully tested image file„Äë&lt;/p&gt;
    &lt;p&gt;With Ventoy, you can also browse ISO/WIM/IMG/VHD(x)/EFI files in local disk and boot them. Notes&lt;/p&gt;
    &lt;p&gt;A GUI Ventoy plugin configurator. VentoyPlugson&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;100% open source&lt;/item&gt;
      &lt;item&gt;Simple to use&lt;/item&gt;
      &lt;item&gt;Fast (limited only by the speed of copying iso file)&lt;/item&gt;
      &lt;item&gt;Can be installed in USB/Local Disk/SSD/NVMe/SD Card&lt;/item&gt;
      &lt;item&gt;Directly boot from ISO/WIM/IMG/VHD(x)/EFI files, no extraction needed&lt;/item&gt;
      &lt;item&gt;Support to browse and boot ISO/WIM/IMG/VHD(x)/EFI files in local disk&lt;/item&gt;
      &lt;item&gt;No need to be continuous in disk for ISO/WIM/IMG/VHD(x)/EFI files&lt;/item&gt;
      &lt;item&gt;MBR and GPT partition style supported (1.0.15+)&lt;/item&gt;
      &lt;item&gt;x86 Legacy BIOS, IA32 UEFI, x86_64 UEFI, ARM64 UEFI, MIPS64EL UEFI supported&lt;/item&gt;
      &lt;item&gt;IA32/x86_64 UEFI Secure Boot supported (1.0.07+)&lt;/item&gt;
      &lt;item&gt;Linux Persistence supported (1.0.11+)&lt;/item&gt;
      &lt;item&gt;Windows auto installation supported (1.0.09+)&lt;/item&gt;
      &lt;item&gt;Linux auto installation supported (1.0.09+)&lt;/item&gt;
      &lt;item&gt;Variables Expansion supported for Windows/Linux auto installation script&lt;/item&gt;
      &lt;item&gt;FAT32/exFAT/NTFS/UDF/XFS/Ext2(3)(4) supported for main partition&lt;/item&gt;
      &lt;item&gt;ISO files larger than 4GB supported&lt;/item&gt;
      &lt;item&gt;Menu alias, Menu tip message supported&lt;/item&gt;
      &lt;item&gt;Password protect supported&lt;/item&gt;
      &lt;item&gt;Native boot menu style for Legacy &amp;amp; UEFI&lt;/item&gt;
      &lt;item&gt;Most types of OS supported, 1200+ iso files tested&lt;/item&gt;
      &lt;item&gt;Linux vDisk boot supported&lt;/item&gt;
      &lt;item&gt;Not only boot but also complete installation process&lt;/item&gt;
      &lt;item&gt;Menu dynamically switchable between List/TreeView mode&lt;/item&gt;
      &lt;item&gt;"Ventoy Compatible" concept&lt;/item&gt;
      &lt;item&gt;Plugin Framework and GUI plugin configurator&lt;/item&gt;
      &lt;item&gt;Injection files to runtime environment&lt;/item&gt;
      &lt;item&gt;Boot configuration file dynamically replacement&lt;/item&gt;
      &lt;item&gt;Highly customizable theme and menu&lt;/item&gt;
      &lt;item&gt;USB drive write-protected support&lt;/item&gt;
      &lt;item&gt;USB normal use unaffected&lt;/item&gt;
      &lt;item&gt;Data nondestructive during version upgrade&lt;/item&gt;
      &lt;item&gt;No need to update Ventoy when a new distro is released&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;See https://www.ventoy.net/en/doc_start.html for detailed instructions.&lt;/p&gt;
    &lt;p&gt;Please refer to BuildVentoyFromSource.txt&lt;/p&gt;
    &lt;p&gt;See https://www.ventoy.net/en/faq.html for detail&lt;/p&gt;
    &lt;p&gt;It would be much appreciated if you want to make a small donation to support my work!&lt;lb/&gt; Alipay, WeChat Pay, PayPal and Bitcoin are available for donation. You can choose any of them.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Alipay&lt;/cell&gt;
        &lt;cell role="head"&gt;WeChat Pay&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;PayPal&lt;lb/&gt; You can transfer to my paypal account &lt;code&gt;admin@ventoy.net&lt;/code&gt; or just click https://www.paypal.me/ventoy&lt;/p&gt;
    &lt;p&gt;Bitcoin&lt;lb/&gt; Bitcoin Address &lt;code&gt;19mZDWzZgzkHCi9YX9H3fYCUuCHq3W6wfT&lt;/code&gt;&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45760340</guid><pubDate>Thu, 30 Oct 2025 14:23:57 +0000</pubDate></item></channel></rss>