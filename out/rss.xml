<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Thu, 18 Sep 2025 16:11:24 +0000</lastBuildDate><item><title>Pnpm has a new setting to stave off supply chain attacks</title><link>https://pnpm.io/blog/releases/10.16</link><description>&lt;doc fingerprint="7674f93dd943850b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;pnpm 10.16&lt;/head&gt;
    &lt;head rend="h2"&gt;Minor Changes&lt;/head&gt;
    &lt;head rend="h3"&gt;New setting for delayed dependency updates&lt;/head&gt;
    &lt;p&gt;There have been several incidents recently where popular packages were successfully attacked. To reduce the risk of installing a compromised version, we are introducing a new setting that delays the installation of newly released dependencies. In most cases, such attacks are discovered quickly and the malicious versions are removed from the registry within an hour.&lt;/p&gt;
    &lt;p&gt;The new setting is called &lt;code&gt;minimumReleaseAge&lt;/code&gt;. It specifies the number of minutes that must pass after a version is published before pnpm will install it. For example, setting &lt;code&gt;minimumReleaseAge: 1440&lt;/code&gt; ensures that only packages released at least one day ago can be installed.&lt;/p&gt;
    &lt;p&gt;If you set &lt;code&gt;minimumReleaseAge&lt;/code&gt; but need to disable this restriction for certain dependencies, you can list them under the &lt;code&gt;minimumReleaseAgeExclude&lt;/code&gt; setting. For instance, with the following configuration pnpm will always install the latest version of webpack, regardless of its release time:&lt;/p&gt;
    &lt;code&gt;minimumReleaseAgeExclude:&lt;/code&gt;
    &lt;p&gt;Related issue: #9921.&lt;/p&gt;
    &lt;head rend="h3"&gt;Advanced dependency filtering with finder functions&lt;/head&gt;
    &lt;p&gt;Added support for &lt;code&gt;finders&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;In the past, &lt;code&gt;pnpm list&lt;/code&gt; and &lt;code&gt;pnpm why&lt;/code&gt; could only search for dependencies by name (and optionally version). For example:&lt;/p&gt;
    &lt;code&gt;pnpm why minimist&lt;/code&gt;
    &lt;p&gt;prints the chain of dependencies to any installed instance of &lt;code&gt;minimist&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;verdaccio 5.20.1&lt;/code&gt;
    &lt;p&gt;What if we want to search by other properties of a dependency, not just its name? For instance, find all packages that have &lt;code&gt;react@17&lt;/code&gt; in their peer dependencies?&lt;/p&gt;
    &lt;p&gt;This is now possible with "finder functions". Finder functions can be declared in &lt;code&gt;.pnpmfile.cjs&lt;/code&gt; and invoked with the &lt;code&gt;--find-by=&amp;lt;function name&amp;gt;&lt;/code&gt; flag when running &lt;code&gt;pnpm list&lt;/code&gt; or &lt;code&gt;pnpm why&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Let's say we want to find any dependencies that have React 17 in peer dependencies. We can add this finder to our &lt;code&gt;.pnpmfile.cjs&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;module.exports = {&lt;/code&gt;
    &lt;p&gt;Now we can use this finder function by running:&lt;/p&gt;
    &lt;code&gt;pnpm why --find-by=react17&lt;/code&gt;
    &lt;p&gt;pnpm will find all dependencies that have this React in peer dependencies and print their exact locations in the dependency graph.&lt;/p&gt;
    &lt;code&gt;@apollo/client 4.0.4&lt;/code&gt;
    &lt;p&gt;It is also possible to print out some additional information in the output by returning a string from the finder. For example, with the following finder:&lt;/p&gt;
    &lt;code&gt;module.exports = {&lt;/code&gt;
    &lt;p&gt;Every matched package will also print out the license from its &lt;code&gt;package.json&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;@apollo/client 4.0.4&lt;/code&gt;
    &lt;p&gt;Related PR: #9946.&lt;/p&gt;
    &lt;head rend="h2"&gt;Patch Changes&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Fix deprecation warning printed when executing pnpm with Node.js 24 #9529.&lt;/item&gt;
      &lt;item&gt;Throw an error if &lt;code&gt;nodeVersion&lt;/code&gt;is not set to an exact semver version #9934.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;pnpm publish&lt;/code&gt;should be able to publish a&lt;code&gt;.tar.gz&lt;/code&gt;file #9927.&lt;/item&gt;
      &lt;item&gt;Canceling a running process with Ctrl-C should make &lt;code&gt;pnpm run&lt;/code&gt;return a non-zero exit code #9626.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45286526</guid><pubDate>Thu, 18 Sep 2025 07:12:56 +0000</pubDate></item><item><title>This Website Has No Class</title><link>https://aaadaaam.com/notes/no-class/</link><description>&lt;doc fingerprint="2bdc203e2ba735d9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This website has no class&lt;/head&gt;
    &lt;p&gt;In my recent post, “There’s no such thing as a CSS reset”, I wrote this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Think of elements like components, but ones that come packed in the browser. Custom elements, without the “custom” part. You can just like, use them.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The line continued to rattle around in my head, and a few weeks later when I was digging into some cleanup work I came to an uncomfortable realization; I wasn’t really taking my own advice. Sure, I was setting some default element styles, but I was leaving a lot on the table. I felt attacked. Called out even. Present me, positively roasted by past me. There was only one possible solution; refactor my website.&lt;/p&gt;
    &lt;p&gt;I like to apply severe constraints in designing and building this site – I think constraints lead to interesting, creative solutions – and it was no different this time around. Instead of relying on built in elements a bit more, I decided to banish classes from my website completely. I haven’t used a class-free approach since the CSS Zen Garden days, and wanted to se how it felt with modern HTML and CSS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Doubling down on styled defaults&lt;/head&gt;
    &lt;p&gt;CSS for the site was structured around 3 cascade layers; &lt;code&gt;base&lt;/code&gt;, &lt;code&gt;components&lt;/code&gt;, and &lt;code&gt;utilities&lt;/code&gt;. Everything in &lt;code&gt;base&lt;/code&gt; was already tag selectors, so the task at hand was to change my approach for components, and eliminate utilities completely.&lt;/p&gt;
    &lt;p&gt;Step 1? Mitigation. There was plenty of code that could have been styled defaults but wasn’t, so I gave all my markup a thorough review, increasing use of semantic elements, extracting common patterns in the form of new element defaults, and making more use of contextual element styling. By contextual styling, I mean going from something like this:&lt;/p&gt;
    &lt;code&gt;.header-primary {
  margin-block: clamp(var(--size-sm), 4vw, var(--size-lg)) var(--size-flex);
}&lt;/code&gt;
    &lt;p&gt;To something like this:&lt;/p&gt;
    &lt;code&gt;body {
  background-color: var(--color-sheet);

  &amp;amp; &amp;gt; header {
    margin-block: clamp(var(--size-sm), 4vw, var(--size-lg)) var(--size-flex);
  }
}&lt;/code&gt;
    &lt;p&gt;It was a good start, and modern features like nesting, &lt;code&gt;:where()&lt;/code&gt;, and &lt;code&gt;:has()&lt;/code&gt; made this feel better that it did 20 years ago, but I took things way too far with contextual styles. Taken to the extreme, you end up with overloaded selector definitions and progressively more esoteric selector patterns. I knew I was down the rabbit hole when I did something like this:&lt;/p&gt;
    &lt;code&gt;li {
  &amp;amp;:has( &amp;gt; a + p) {
    padding-block: var(--size-lg);
    border-block-end: var(--border-default);
    text-wrap: balance;

    &amp;amp; &amp;gt; a {
      font-size: var(--font-xxl);
    }

    &amp;amp; &amp;gt; p {
      margin-block: var(--size-sm);
    }
  }
}&lt;/code&gt;
    &lt;p&gt;I still needed a “real” solution for components, and a way to manage variants.&lt;/p&gt;
    &lt;head rend="h2"&gt;Custom tags &amp;amp; custom attributes&lt;/head&gt;
    &lt;p&gt;I had an inkling of a solution, which is to leverage patterns from custom elements and web components, sans js. By virtue of their progressively enhanced nature, custom tag names and custom attributes are 100% valid HTML, javascript or no. That inkling turned into fervent belief after reading Keith Cirkel’s excellent post “CSS classes considered harmful”.&lt;/p&gt;
    &lt;p&gt;Revisiting the example above, now we’ve got a pattern like this:&lt;/p&gt;
    &lt;code&gt;note-pad {
  padding-block: var(--size-lg);
  border-block-end: var(--border-default);
  text-wrap: balance;

  &amp;amp; a {
    font-size: var(--font-xxl);
  }

  &amp;amp; p {
    margin-block: var(--size-sm);
  }
}&lt;/code&gt;
    &lt;p&gt;Custom attributes become a go-to for handling former BEM modifiers, but instead of relying on stylistic writing convention to fake a key-value pair, you get an actual key-value pair.&lt;/p&gt;
    &lt;code&gt;random-pattern {
  &amp;amp; [shape-type="1"] {
    border: 0.1rem solid var(--color-sheet);
    background-color: var(--color-sheet);
    filter: url("#noise1");
  }

  &amp;amp; [shape-type="2"] {
    background: var(--pattern-lines-horizontal);
    background-size: var(--pattern-scale);
  }
}&lt;/code&gt;
    &lt;p&gt;Now, you can use &lt;code&gt;data-whatever&lt;/code&gt; for attributes, but really, any two dash-separated words are safe. Personally, I think dropping the &lt;code&gt;data&lt;/code&gt; prefix feels better and allows for richer semantics.&lt;/p&gt;
    &lt;p&gt;You can argue that both of these techniques are re-inventing classes in various ways. Kind of! You can use custom element names in lieu of semantic tags, just like you can slap a class on a div. But these techniques, particularly with how you can seamlessly enhance to true custom elements or web components, feels like a coherent end-to-end system in a way that class-based approaches don’t. It’s tags and attributes, all the way down.&lt;/p&gt;
    &lt;head rend="h2"&gt;Would I do this again?&lt;/head&gt;
    &lt;p&gt;On the plus side, the user outcomes are decidedly positive; I removed a non-trivial amount of CSS (now about ~5KB of CSS over the wire for the entire site), and accessibility is without question better due to having to paid much closer attention to markup. Also, just look at that markup. So clean. So shiny.&lt;/p&gt;
    &lt;p&gt;On the flipside, this feels like an approach that simply asks more of authors. It requires more careful planning compared to pure component approaches; you can’t think of things in purely isolated terms. All to say, I’m very happy to ship this on my personal website, I’d be less likely to advocate for this approach on a large project with varied levels of frontend knowledge.&lt;/p&gt;
    &lt;p&gt;There’s a variation here that’s more encapsulated (use custom tag names with abandon), but that pulls on what feels like an unresolved thread; replacing a semantic element with a custom tag name that has no semantic value feels bad, and adding extra wrappers around everything also feels bad.&lt;/p&gt;
    &lt;p&gt;All to say, I’m not quite ready to say that this is The One True Way I’ll build all sites from now on, but I also can’t help but feel like I’ve crossed some kind of threshold. I used to think classes were fine. Now I’m not so sure. I don’t know exactly where it’ll lead yet, but this feels like one of those exercises that’ll have a lasting influence on my work.&lt;/p&gt;
    &lt;p&gt;A mea culpa; I only got 99% of the way there. I use 11ty’s syntax highlighting plugin, which uses classes for styling. I gave syntax-highlight a hard look, but I don’t love the idea of introducing client-side js where none need exist, and the authoring experience would be a step back, so I begrudgingly left it alone for now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45287155</guid><pubDate>Thu, 18 Sep 2025 08:41:32 +0000</pubDate></item><item><title>Fast Fourier Transforms Part 1: Cooley-Tukey</title><link>https://connorboyle.io/2025/09/11/fft-cooley-tukey.html</link><description>&lt;doc fingerprint="a499143b0d082236"&gt;
  &lt;main&gt;&lt;p&gt;Find me on:&lt;/p&gt;&lt;p&gt;Posts:&lt;/p&gt;&lt;p&gt;by Connor Boyle&lt;/p&gt;&lt;p&gt;tags: mathematicssoftware&lt;/p&gt;&lt;p&gt;I’m planning to write a series of posts about fast Fourier transform algorithms. This first post covers the Cooley-Tukey algorithm, which is the original and most well-known FFT algorithm.&lt;/p&gt;&lt;p&gt;If \(x\) is a sequence of complex numbers with a length \(\lvert x \rvert\) and a starting index of 0, then the discrete Fourier transform of \(x\), \(\mathcal{F} \{ x \}\), is defined as follows:&lt;/p&gt;\[|\mathcal{F} \{ x \}| = |x|\] \[\mathcal{F} \{ x \}[k] = \sum_{j=0}^{|x|-1} x[j] \cdot e^{-i 2 \pi jk \frac{1}{|x|}}\]&lt;p&gt;Since complex exponentation is so commonly used in Fourier transforms, we’ll define a helpful term \(W_N\) as follows:&lt;/p&gt;\[W_N \triangleq e^{-i 2 \pi \frac{1}{N}}\]&lt;p&gt;i.e. \(W_N = W_N^1\) is a \(\frac{1}{N}\)-turn rotation in the complex plane (starting at 1). \(W_N^2\) is a \(\frac{2}{N}\)-turn rotation in the complex plane, etc. Substituting to the original discrete Fourier transform definition, we get:&lt;/p&gt;\[\mathcal{F} \{ x \}[k] = \sum_{j=0}^{|x|-1} x[j] \cdot W_{|x|}^{jk}\]&lt;p&gt;Naïvely evaluating this equation for each of the \(\lvert x \rvert\) different output frequency buckets of the DFT (\(k = 0, 1, \ldots, \lvert x \rvert - 2, \lvert x \rvert - 1\)) requires a summation of complex products of the \(\lvert x \rvert\) samples in the signal, thus giving any naïve DFT algorithm a time complexity of \(O(\lvert x \rvert^2)\).&lt;/p&gt;&lt;p&gt;If \(\lvert x \rvert\) is a composite number, we can pick two natural numbers \(r\) and \(d\), such that:&lt;/p&gt;\[\lvert x \rvert = r \cdot d\]&lt;p&gt;This allows us to change the single summation over \(j\) into nested summations:&lt;/p&gt;\[\mathcal{F} \{ x \}[k] = \sum_{j_1=0}^{d-1} \sum_{j_0=0}^{r-1} x[j_1 r + j_0] \cdot W_{\lvert x \rvert}^{(j_1 r + j_0)k}\] \[= \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{\lvert x \rvert}^{(j_1 r + j_0)k}\]&lt;p&gt;Similarly, we can define variables \(k_0\) and \(k_1\) such that \(k = k_1 d + k_0\). Let:&lt;/p&gt;\[k_1 \triangleq \lfloor \frac{k}{d} \rfloor\] \[k_0 \triangleq k - k_1 d\]&lt;p&gt;In other words, \(k_1\) is the quotient and \(k_0\) is the remainder of the Euclidean division1 of \(k\) by \(d\).&lt;/p&gt;&lt;p&gt;This allows us to again re-formulate the discrete Fourier transform:&lt;/p&gt;\[\mathcal{F} \{ x \}[k] = \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{|x|}^{(j_1 r + j_0) (k_1 d + k_0)}\] \[= \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{|x|}^{j_1 r k_1 d + j_1 r k_0 + j_0 (k_1 d + k_0)}\] \[= \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{|x|}^{j_1 k_1 |x| + j_1 r k_0 + j_0 k}\] \[= \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{|x|}^{j_1 k_1 |x|} \cdot W_{|x|}^{j_1 r k_0} \cdot W_{|x|}^{j_0 k}\]&lt;p&gt;Since \(W_{\lvert x \rvert}^{j_1 k_1 \lvert x \rvert} = (e^{-i \frac{2 \pi \lvert x \rvert}{\lvert x \rvert}})^{j_1 k_1} = 1^{j_1 k_1} = 1\), therefore:&lt;/p&gt;\[\mathcal{F} \{ x \}[k] = \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x[j_1 r + j_0] \cdot W_{\lvert x \rvert}^{j_1 r k_0} \cdot W_{\lvert x \rvert}^{j_0 k}\]&lt;p&gt;At this point, we can split the elements of \(x\) into sub-sequences corresponding to modulo classes. Let \(x_{r}^{j_0}\) be a sequence whose elements are equal to the elements of \(x\) whose indices are equivalent \(j_0\) modulo \(r\). More formally, these sequences (of which there are \(r\) total) can be defined as follows:&lt;/p&gt;\[x_r^{j_0}[j_1] = x[j_1 r + j_0]\] \[|x_r^{j_0}| = \frac{|x|}{r} = d\]&lt;p&gt;Substituting this sequence definition, we get:2&lt;/p&gt;\[\mathcal{F} \{ x \}[k] = \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x_r^{j_0}[j_1] W_{|x|}^{k_0 j_1 r} W_{|x|}^{j_0 k}\] \[= \sum_{j_0=0}^{r-1} \sum_{j_1=0}^{d-1} x_r^{j_0}[j_1] W_d^{k_0 j_1} W_{|x|}^{j_0 k}\] \[= \sum_{j_0=0}^{r-1} \mathcal{F} \{ x_r^{j_0} \}[k_0] W_{|x|}^{j_0 k}\]&lt;p&gt;Let’s consider how long it will take to evaluate the discrete Fourier transform in this formulation:&lt;/p&gt;&lt;p&gt;Added together, these two sub-routines require \(O(\lvert x \rvert \cdot d + \lvert x \rvert \cdot r) = O(\lvert x \rvert \cdot (d + r))\) operations, possibly a significant improvement from the original \(O(\lvert x \rvert^2)\) complexity of the original naive formulation, depending on the values of \(r\) &amp;amp; \(d\). More importantly, this manipulation can be applied recursively. Specifically, each of the \(r\) discrete Fourier transforms of the \(d\)-length sequences \(x_r^{j_0}\) can be broken down into \(r'\) Fourier transforms of length \(d'\), assuming that two natural numbers exist such that \(d = r' \cdot d'\).3 In the ideal4 case where \(\lvert x \rvert = 2^n\), \(n \in \mathbb{N}\), calculating the Cooley-Tukey algorithm will require \(O(\lvert x \rvert \cdot (2 + 2 + \ldots + 2)) = O(\lvert x \rvert \cdot 2 \cdot \log_2(\lvert x \rvert)) = O (\lvert x \rvert \cdot \log(\lvert x \rvert))\) operations.&lt;/p&gt;&lt;p&gt;The Cooley-Tukey algorithm can also be used to calculate the inverse discrete Fourier transform with only very slight modification. In fact, the original Cooley-Tukey paper (see “related reading”) specifically described an algorithm to compute the inverse discrete Fourier transform, not the “forward” DFT. I will leave the Cooley-Tukey iDFT algorithm as an exercise for the reader.&lt;/p&gt;&lt;p&gt;However, note that the Cooley-Tukey algorithm gives no speed-up for input sequences of prime length, and provides relatively little speed-up when the factors of the input length contain large primes. To efficiently compute the DFT for sequences of prime or even non-highly-composite lengths, we will need additional algorithms. Ultimately, however, these other FFT algorithms generally depend on Cooley-Tukey for part of the computation. I plan to cover at least one of these techniques—Bluestein’s algorithm—in a future blog post(s).&lt;/p&gt;&lt;p&gt;This visualization shows how the discrete Fourier transform of some signal \(x\) is computed using the Cooley-Tukey algorithm. The black boxes at the very bottom are the input signal. While the DFT can be applied to complex signals, I’ve restricted the sample values of the input signal to be real numbers, for simplicity’s sake (this mimics some real-world applications, such as performing a DFT on an audio recording). You can click and drag on the input boxes to change their values.&lt;/p&gt;&lt;p&gt;The grey circles and the sometimes visible white “clock hands” inside of them represent the complex exponent \(W_N^x = e^{-2 i \pi \frac{x}{N}}\) for some \(N\) (e.g. \(\lvert x \rvert\), \(d\), \(d'\), etc.) and some \(x\). These complex exponents, which are equivalent to rotations in the complex plane, are applied to the relevant input value. Unlike the usual convention, I’ve decided to show the real component of the complex plane as vertical (“up” is positive-real) and the imaginary component as horizontal (“right” is imaginary-positive). You can see where the input value is drawn from by hovering the mouse over a given “rotation” box.&lt;/p&gt;&lt;p&gt;The sum of those rotated input values is added together to calculate one element of a discrete Fourier transform. Hover over a white “output” box of a discrete Fourier transform in the visualization to highlight the column of “rotation” boxes that it was summed from.&lt;/p&gt;&lt;p&gt;\(|x| =\)&lt;/p&gt;&lt;p&gt;Available factors:&lt;/p&gt;&lt;p&gt;I’ve noticed an irritating and confusing tendency among many people–including published authors–when talking about discrete Fourier transforms. I find they often use the phrase “fast Fourier transform” (or perhaps more often, the abbreviation “FFT”) when they mean “discrete Fourier transform” (or “DFT”). I think this is wrong and confusing; to understand why, imagine you have a list:&lt;/p&gt;&lt;code&gt;x = [10, 3, 2, 19, -2]
&lt;/code&gt;&lt;p&gt;would it make sense to refer to the following list as the “mergesort” of the previous list?&lt;/p&gt;&lt;code&gt;y = [-2, 2, 3, 10, 19]
&lt;/code&gt;&lt;p&gt;I think most people would find that very strange. We can say that the second list is the result of sorting the first list, but we don’t know anything about the specific algorithm that was used to sort the list. It could have been sorted using mergesort, quicksort, heapsort, bubblesort, bogosort, or any other sorting algorithm (in reality, I just sorted this one in my head). However, even if I had used mergesort, &lt;code&gt;y&lt;/code&gt; still wouldn’t be the “mergesort” of
&lt;code&gt;x&lt;/code&gt;, it would still just be “the result of sorting &lt;code&gt;x&lt;/code&gt;”.&lt;/p&gt;&lt;p&gt;Similarly, the output of an FFT algorithm should not be referred to as “an/the FFT of” anything. In theory, calculating the DFT of a sequence using Cooley-Tukey gives the exact same result as calculating that DFT using a naïvely-implemented DFT algorithm. In practice, Cooley-Tukey will probably give a slightly more accurate result since there are fewer total calculations and therefore fewer opportunities for floating point round-off error.&lt;/p&gt;&lt;p&gt;This is not just irritating to me; I think it causes confusion among the public. A friend of mine–an intelligent mathematics major and software engineer–recently asked me “what would I lose by taking the fast Fourier transform instead of the ‘normal’ Fourier transform? I’ve always just taken the normal Fourier transform”. He probably inferred from the way many people throw around the phrase “FFT” that the “fast Fourier transform” is some kind of approximation or related concept yet distinct from the discrete Fourier transform or Fourier transforms in general, rather than an algorithm for calculating such.&lt;/p&gt;&lt;p&gt;Thank you to my friend Andre Archer, who helped to proofread an earlier version of this post. Any mistakes are my own.&lt;/p&gt;&lt;p&gt;Footnotes:&lt;/p&gt;&lt;p&gt;As far as I am aware, there is no widely-standardized notation for Euclidean division in mathematics. Personally, I find this quite silly. ↩&lt;/p&gt;&lt;p&gt;If \(r\) is chosen to be equal to \(\lvert x \rvert\), and therefore \(d = 1\), then calculating \(\mathcal{F} \{ x \}\) using Cooley-Tukey is equivalent to calculating \(\mathcal{F} \{ x \}\) naïvely, taking \(O(\lvert x \rvert^2)\) steps. ↩&lt;/p&gt;&lt;p&gt;By “ideal” I do not necessarily mean optimal, in any sense. \(|x| = 2^n\) is “ideal” in that it is very simple to reason about. ↩&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45287513</guid><pubDate>Thu, 18 Sep 2025 09:28:40 +0000</pubDate></item><item><title>CircuitHub (YC W12) Is Hiring Operations Research Engineers (UK/Remote)</title><link>https://www.ycombinator.com/companies/circuithub/jobs/UM1QSjZ-operations-research-engineer</link><description>&lt;doc fingerprint="29da4270258e37c0"&gt;
  &lt;main&gt;
    &lt;p&gt;On-Demand Electronics Manufacturing&lt;/p&gt;
    &lt;p&gt;CircuitHub is reshaping electronics manufacturing with The Grid , a factory-scale robotics platform designed to make small-batch, high-mix electronics assembly radically more efficient. Think semiconductor-fab levels of precision applied to the chaotic world of prototyping and low-volume production. The result? A 10x throughput improvement in one of the world's most foundational industries.&lt;/p&gt;
    &lt;p&gt;We've raised $20M from top-tier investors, including Y Combinator and Google Ventures , and we're already profitable. Our customers include industry leaders like Tesla, Meta, and Zipline.&lt;/p&gt;
    &lt;p&gt;The Grid isn't a prototype. It's live, scaling fast, and delivering real revenue. We're now building the engineering core that will scale it further.&lt;/p&gt;
    &lt;p&gt;We're looking for an engineer to found our Operations Research Team. You'll tackle our trickiest scheduling and pricing optimization problems to help make the world's most efficient electronics factory. This is an incredibly high-impact role that will directly contribute to 3x revenue growth over the next year.&lt;/p&gt;
    &lt;p&gt;This role is only suitable for someone with direct experience with operations research problems.&lt;/p&gt;
    &lt;p&gt;Decide what our factory should build and when&lt;/p&gt;
    &lt;p&gt;Develop novel quoting algorithms&lt;/p&gt;
    &lt;p&gt;Predict forward-looking revenue based on capacity&lt;/p&gt;
    &lt;p&gt;Remote or work from one of our labs in the UK (London, Cambridge) or USA (Boston)&lt;/p&gt;
    &lt;p&gt;The work of the operations research team will be critical to making electronics manufacturing economically viable in the USA and Europe. This is a career-defining opportunity for a high-agency engineer. If you thrive on ownership and solving real problems using advanced research techniques, there's no better place to be.&lt;/p&gt;
    &lt;p&gt;Python&lt;/p&gt;
    &lt;p&gt;Google OR-Tools&lt;/p&gt;
    &lt;p&gt;Gurobi&lt;/p&gt;
    &lt;p&gt;MiniZinc&lt;/p&gt;
    &lt;p&gt;We're building a future where hardware companies can design and iterate as fast as software companies&lt;/p&gt;
    &lt;p&gt;CircuitHub is on a mission to fix rapid electronics prototyping. We are the first automated electronics factory built around a modern tech stack. We help hardware companies producing self driving cars, satellites, 3D printers, robotics, &amp;amp; more to rapidly prototype electronics and get to market faster.&lt;/p&gt;
    &lt;p&gt;We've raised $20M from top investors that include Y Combinator , Google Ventures, &amp;amp; more. With business growing fast we are looking to fill roles in Massachusetts, USA and London, UK .&lt;/p&gt;
    &lt;p&gt;Join us to solve real world problems while shaping the future of automated manufacturing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45287551</guid><pubDate>Thu, 18 Sep 2025 09:33:07 +0000</pubDate></item><item><title>Nvidia buys $5B in Intel stock in seismic deal</title><link>https://www.tomshardware.com/pc-components/cpus/nvidia-and-intel-announce-jointly-developed-intel-x86-rtx-socs-for-pcs-with-nvidia-graphics-also-custom-nvidia-data-center-x86-processors-nvidia-buys-usd5-billion-in-intel-stock-in-seismic-deal</link><description>&lt;doc fingerprint="27f84771ddaa5063"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Nvidia and Intel announce jointly developed 'Intel x86 RTX SOCs' for PCs with Nvidia graphics, also custom Nvidia data center x86 processors — Nvidia buys $5 billion in Intel stock in seismic deal&lt;/head&gt;
    &lt;p&gt;Cats and Dogs, living together!&lt;/p&gt;
    &lt;p&gt;In a surprise announcement that finds two long-time rivals working together, Nvidia and Intel announced today that the companies will jointly develop multiple new generations of x86 products together — a seismic shift with profound implications for the entire world of technology. Before the news broke, Tom's Hardware spoke with Nvidia representatives to learn more details about the company’s plans.&lt;/p&gt;
    &lt;p&gt;The products include x86 Intel CPUs tightly fused with an Nvidia RTX graphics chiplet for the consumer gaming PC market, named the ‘Intel x86 RTX SOCs.’ Nvidia will also have Intel build custom x86 data center CPUs for its AI products for hyperscale and enterprise customers. Additionally, Nvidia will buy $5 billion in Intel common stock at $23.28 per share, representing a roughly 5% ownership stake in Intel. (Intel stock is now up 33% in premarket trading.)&lt;/p&gt;
    &lt;p&gt;The partnership between the two companies is in the very early stages, Nvidia told us, so the timeline for product releases along with any product specifications will be disclosed at a later, unspecified date. (Given the traditionally long lead-times for new processors, it is rational to expect these products will take at least a year, and likely longer, to come to market.)&lt;/p&gt;
    &lt;p&gt;Nvidia emphasized that the companies are committed to multi-generation roadmaps for the co-developed products, which represents a strong investment in the x86 ecosystem. But representatives tells us it also remains fully committed to other announced product roadmaps and architectures, including the company's Arm-based GB10 Grace Blackwell processors for workstations and the Nvidia Grace CPUs for data centers, as well as the next-gen Vera CPUs. Nvidia says it also remains committed to products on its internal roadmaps that haven’t been publicly disclosed yet, indicating that the new roadmap with Intel will merely be additive to existing initiatives.&lt;/p&gt;
    &lt;p&gt;The chip giant hasn’t disclosed whether it will use Intel Foundry to produce any of these products yet. However, while Intel has used TSMC to manufacture some recent products, its goal is to bring production of most high-performance products back into its own foundries.&lt;/p&gt;
    &lt;p&gt;Some products never left. For instance, Intel’s existing Granite Rapids data center processors use the ‘Intel 3’ node, and the upcoming Clearwater Forest Xeons will use Intel’s own 18A process node for compute. This suggests that at least some of the Nvidia-custom x86 silicon, particularly for the data center, could be fabbed on Intel nodes. Intel also uses TSMC to fabricate many of its client x86 processors, however, so we won’t know for sure until official announcements are made — particularly for the RTX GPU chiplet.&lt;/p&gt;
    &lt;p&gt;In either case, Nvidia has been mulling using Intel Foundry since 2022, has fabbed test chips there, and participates in the U.S. Defense Dept.'s RAMP-C project with Intel. The DoD project involves Nvidia already making chips on Intel's 18A process node, so it wouldn't be a total surprise.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;While the two companies have engaged in heated competition in some market segments, Intel and Nvidia have partnered for decades, ensuring interoperability between their hardware and software for products spanning both the client and data center markets. And the PCIe interface has long been used to connect Intel CPUs and Nvidia GPUs. The new partnership will find tighter integration using the NVLink interface for CPU-to-GPU communication, which affords up to 14 times more bandwidth along with lower latency than PCIe, thus granting the new x86 products access to the highest performance possible when paired with GPUs. Let’s dive into the details we’ve learned so far.&lt;/p&gt;
    &lt;head rend="h2"&gt;Intel x86 RTX SOCs for the PC gaming market&lt;/head&gt;
    &lt;p&gt;For the PC market, the Intel x86 RTX SoC chips will come with an x86 CPU chiplet tightly connected with an Nvidia RTX GPU chiplet via the NVLink interface. This type of processor will have both CPU and GPU units merged into one compact chip package that externally looks much like a standard CPU, rivaling AMD’s competing APU products.&lt;/p&gt;
    &lt;p&gt;This type of tight integration packs all the gaming prowess into one package without an external discrete GPU, providing power and footprint advantages. As such, these chips will be heavily focused on thin-and-light gaming laptops and small form-factor PCs, much like today’s APUs from AMD. However, it’s possible the new Nvidia/Intel chips could come in multiple flavors and permeate further into the Intel stack over time.&lt;/p&gt;
    &lt;p&gt;Intel has worked on a similar type of chip before with AMD; there is at least one significant technical difference between these initiatives, however. Intel launched its Kaby Lake-G chip in 2017 with an Intel processor fused into the same package as an AMD Radeon GPU chiplet, much the same as the description of the new Nvidia/Intel chips. You can see an image of the Intel/AMD chip below.&lt;/p&gt;
    &lt;p&gt;This SoC had a CPU at one end connected via a PCIe connection to the separate AMD GPU chiplet, which is flanked by a small, dedicated memory package. This separate memory package was only usable by the GPU. The Nvidia/Intel products will have an RTX GPU chiplet connected to the CPU chiplet via the faster and more efficient NVLink interface, and we’re told it will have uniform memory access (UMA), meaning both the CPU and GPU will be able to access the same pool of memory.&lt;/p&gt;
    &lt;p&gt;Intel notoriously axed the Kaby Lake-G products in 2019, and the existing systems were left without proper driver support for quite some time, in part because Intel was responsible for validating the drivers, and then finger-pointing ensued. We’re told that both Intel and Nvidia will be responsible for their respective drivers for the new models, with Nvidia naturally providing its own GPU drivers. However, Intel will build and sell the consumer processors.&lt;/p&gt;
    &lt;p&gt;We haven’t spoken with Intel yet, but the limited scope of this project means that Intel’s proprietary Xe graphics architecture will most assuredly live on as the primary integrated GPU (iGPU) for its mass-market products.&lt;/p&gt;
    &lt;p&gt;Intel's new x86 RTX CPUs will compete directly with AMD's APUs. For AMD, that means it faces intensifying competition from a company with the leading market share in notebook CPUs (Intel ships ~79% of laptop chips worldwide) that's now armed with GPU tech from Nvidia, which ships 92% of the world's gaming GPUs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nvidia's first x86 data center CPUs&lt;/head&gt;
    &lt;p&gt;Intel will fabricate custom x86 data center CPUs for Nvidia, which Nvidia will then sell as its own products to enterprise and data center customers. However, the entirety and extent of the modification are currently unknown. We do know that Nvidia will employ its NVLink interface, which tells us the chips could leverage Nvidia’s new NVLink Fusion tech that enables custom CPUs and accelerators to enable faster, more efficient communication with Nvidia’s GPUs than found with the PCIe interface.&lt;/p&gt;
    &lt;p&gt;Intel has long offered custom Xeons to its customers, primarily hyperscalers, often with relatively minor tweaks to clock rates, cache capacities, and other specifications. In fact, these mostly slightly-modified custom Xeon models once comprised more than 50% of Intel’s Xeon shipments. Intel has endured several years of market share erosion due to AMD’s advances, most acutely in the hyperscale market. Therefore, it is unclear if the 50% number still holds true, as hyperscalers were the primary customers for custom models.&lt;/p&gt;
    &lt;p&gt;Intel has long said that it will design completely custom x86 chips for customers as part of its IDM 2.0 strategy. However, aside from a recent announcement of custom AWS chips that sound like the slightly modified Xeons mentioned above, we haven’t heard of any large-scale uptake for significantly modified custom x86 processors. Intel announced a new custom chip design unit just two weeks ago, so it will be interesting to learn the extent of the customization for Nvidia’s x86 data center CPUs.&lt;/p&gt;
    &lt;p&gt;Nvidia already uses Intel’s Xeons in several of its systems, like the Nvidia DGX B300, but these systems still use the PCIe interface to communicate with the CPU. Intel’s new collaboration with Nvidia will obviously open up new opportunities, given the tighter integration with NVLink and all the advantages it brings with it.&lt;/p&gt;
    &lt;p&gt;The likelihood of AMD adopting NVLink Fusion is somewhere around zero, as the company is heavily invested in its own Infinity Fabric (XGMI) and Ultra Accelerator Link (UALink) initiatives, which aim to provide an open-standard interconnect to rival NVLink and democratize rack-scale interconnect technologies. Intel is also a member of UALink, which uses AMD’s Infinity Fabric protocol as the foundation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dollar and Cents, Geopolitics&lt;/head&gt;
    &lt;p&gt;Nvidia’s $5 billion purchase of Intel common stock will come at $23.28 a share, roughly 6% below the current market value, but several aspects of this investment remain unclear. Nvidia hasn’t stated whether it will have a seat on the board (which is unlikely) or how it will vote on matters requiring shareholder approval. It is also unclear if Intel will issue new stock (primary issuance) for Nvidia to purchase, as it did when the U.S. government recently became an Intel shareholder (that is likely). Naturally, the investment is subject to approval from regulators.&lt;/p&gt;
    &lt;p&gt;Nvidia’s buy-in comes on the heels of the U.S government buying $10 billion of newly-created Intel stock, granting the country a 9.9% ownership stake at $20.47 per share. The U.S. government won’t have a seat on the board and agreed to vote with Intel’s board on matters requiring shareholder approval “with limited exceptions.” Softbank has also recently purchased $2 billion worth of primary issuance Intel stock at $23 per share.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Row 0 - Cell 0&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Total&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Share Price&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Stake in Intel&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Nvidia&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$5 Billion&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$23.28&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~5%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;U.S. Government&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$9 Billion&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$20.47&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~9.9%&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Softbank&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$2 Billion&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;$23&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Row 3 - Cell 3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The U.S. government says it invested in Intel with the goal of bolstering US technology, manufacturing, and national security, and the investments from the private sector also help solidify the struggling Intel. Altogether, these investments represent a significant cash influx for Intel as it attempts to maintain the heavy cap-ex investments required to compete with TSMC, all while struggling with a negative amount of free cash flow.&lt;/p&gt;
    &lt;p&gt;“AI is powering a new industrial revolution and reinventing every layer of the computing stack — from silicon to systems to software. At the heart of this reinvention is Nvidia’s CUDA architecture,” said Nvidia CEO Jensen Huang. “This historic collaboration tightly couples NVIDIA’s AI and accelerated computing stack with Intel’s CPUs and the vast x86 ecosystem—a fusion of two world-class platforms. Together, we will expand our ecosystems and lay the foundation for the next era of computing.”&lt;/p&gt;
    &lt;p&gt;“Intel’s x86 architecture has been foundational to modern computing for decades – and we are innovating across our portfolio to enable the workloads of the future,” said Intel CEO Lip-Bu Tan. “Intel’s leading data center and client computing platforms, combined with our process technology, manufacturing and advanced packaging capabilities, will complement Nvidia's AI and accelerated computing leadership to enable new breakthroughs for the industry. We appreciate the confidence Jensen and the Nvidia team have placed in us with their investment and look forward to the work ahead as we innovate for customers and grow our business.”&lt;/p&gt;
    &lt;p&gt;We’ll learn more details of the new partnership later today when Nvidia CEO Jensen Huang and Intel CEO Lip-Bu Tan hold a webcast press conference at 10 am PT.&lt;/p&gt;
    &lt;p&gt;This is breaking news…more to come.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our up-to-date news, analysis, and reviews in your feeds. Make sure to click the Follow button!&lt;/p&gt;
    &lt;p&gt;Paul Alcorn is the Editor-in-Chief for Tom's Hardware US. He also writes news and reviews on CPUs, storage, and enterprise hardware.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;header&gt;bolweval&lt;/header&gt;WOW, didn't see that coming, but it makes good business sense, especially for Intel..Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;usertests&lt;/header&gt;Wow.Reply&lt;lb/&gt;So this is still kind of like an integrated dGPU, unlike the chiplet/tile-based CPUs with integrated graphics they're already making?&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;watzupken&lt;/header&gt;AMD must be sweating now. An Intel and Nvidia tag team is going to give AMD some tough competition.Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;hotaru251&lt;/header&gt;Reply&lt;quote/&gt;pretty much.... basically it keeps intel afloat by riding on nvidia and nvidia gets to take its monopoly even further.watzupken said:AMD must be sweating now. An Intel and Nvidia tag team is going to give AMD some tough competition.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;usertests&lt;/header&gt;Reply&lt;quote/&gt;What products are they trying to compete with?watzupken said:AMD must be sweating now. An Intel and Nvidia tag team is going to give AMD some tough competition.&lt;lb/&gt;Intel's mobile graphics were already competitive with AMD's phoned-in 128-bit APUs, with Lunar Lake being around Strix Point for example. They don't need an Nvidia chiplet to compete.&lt;lb/&gt;Intel never cared to make a desktop APU from the aforementioned graphics.&lt;lb/&gt;So that leaves Strix Halo. And Apple products.&lt;lb/&gt;They also get to outsource drivers and software to Nvidia, and CUDA will work with these. That could be a bigger deal than any hardware match-up.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;rluker5&lt;/header&gt;Reply&lt;quote/&gt;I wonder how much this will help transitioning common CPU tasks to GPU like OneAPI was supposed to?usertests said:What products are they trying to compete with?&lt;lb/&gt;Intel's mobile graphics were already competitive with AMD's phoned-in 128-bit APUs, with Lunar Lake being around Strix Point for example. They don't need an Nvidia chiplet to compete.&lt;lb/&gt;Intel never cared to make a desktop APU from the aforementioned graphics.&lt;lb/&gt;So that leaves Strix Halo. And Apple products.&lt;lb/&gt;They also get to outsource drivers and software to Nvidia, and CUDA will work with these. That could be a bigger deal than any hardware match-up.&lt;quote/&gt;Isn't buying common shares is the opposite of dilution? It sounds like same amount of shares enriched with more money.dalek1234 said:So another share dilution, yet Intel stock price is going up.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;aldaia&lt;/header&gt;"Nvidia announced that it will buy $5 billion in Intel common stock"Reply&lt;lb/&gt;Wondering if this is the start of a takeover process.&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;blppt&lt;/header&gt;Wait a minute, if they start moving in this direction, wouldn't that make Intel's growing GPU division superfluous or redundant?Reply&lt;/item&gt;
      &lt;item&gt;&lt;header&gt;tamalero&lt;/header&gt;So an outright overpowered monopoly trying to prop up a falling monopoly by setting up joint products.. what can go wrong?Reply&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45288161</guid><pubDate>Thu, 18 Sep 2025 11:04:48 +0000</pubDate></item><item><title>Mirror Life Worries</title><link>https://www.science.org/content/blog-post/mirror-life-worries</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45288222</guid><pubDate>Thu, 18 Sep 2025 11:16:29 +0000</pubDate></item><item><title>You Had No Taste Before AI</title><link>https://matthewsanabria.dev/posts/you-had-no-taste-before-ai/</link><description>&lt;doc fingerprint="a49ea3d18a83c5cd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You Had No Taste Before AI&lt;/head&gt;
    &lt;head class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden"&gt;Table of Contents&lt;/head&gt;
    &lt;p&gt;There’s been an influx of people telling others to develop taste to use AI. Designers. Marketers. Developers. All of them touting the same message. It’s ironic, though. These are the same people who never questioned why their designs all look identical, never iterated beyond the first draft, and never asked if their work actually solved the problem at hand.&lt;/p&gt;
    &lt;p&gt;They’re not alone. The loudest voices preaching about taste and AI are often the ones who never demonstrated taste before AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Taste? #&lt;/head&gt;
    &lt;p&gt;The technology industry has a tendency to use words that mean multiple things without describing which definition they are referring to. When I read about taste and AI I usually see people referring to the following definition.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Critical judgment, discernment, or appreciation of aesthetic quality.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In the context of AI, this definition manifests itself in several ways.&lt;/p&gt;
    &lt;p&gt;Contextual Appropriateness: Knowing when AI-generated content fits the situation and when it doesn’t. Put another way, knowing when a human touch is needed (e.g., a message to a loved one).&lt;/p&gt;
    &lt;p&gt;Quality Recognition: Being able to distinguish between useful AI-generated content and slop. This requires domain knowledge to truly discern aesthetic quality rather than just functional quality.&lt;/p&gt;
    &lt;p&gt;Iterative Refinement: Understanding that AI is a starting point that requires further iteration. This point is most similar to how culinary taste is applied to refine a dish by iterating on the recipe and presentation.&lt;/p&gt;
    &lt;p&gt;Ethical Boundaries: Recognizing when AI crosses the lines of authenticity, legality, and respect. Basically, don’t use AI to do bad things.&lt;/p&gt;
    &lt;p&gt;None of these skills are new. These are the same skills we should have been applying to our work all along. Why are we asking about taste and AI now when we should have been applying taste the whole time? Perhaps people advocating for taste are telling on themselves.&lt;/p&gt;
    &lt;head rend="h2"&gt;Being Tasteless #&lt;/head&gt;
    &lt;p&gt;Some people have no taste. In the best case that may be due to lack of experience but in the worst case it may be due to ignorance. I’m noticing that many people worried about tasteless AI-generated content are often guilty of producing tasteless content themselves, usually manifesting as the following.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Copying and pasting code without understanding it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Sending resumes and emails that aren’t proofread and edited.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Asking others to review code without giving it a self review.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Noticing a quality issue and failing to document or fix it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Designing websites that look exactly like every other company’s website.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Regurgitating content from the trending influencer of the week.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Where’s the taste here? Where’s the critical judgment, discernment, or appreciation of aesthetic quality that separates mediocrity from excellence?&lt;/p&gt;
    &lt;p&gt;It’s not there because most people haven’t developed their taste yet. AI didn’t create this tasteless problem. People did. Now that everyone can generate content at the speed of thought we’re noticing that not all content is actually good. To play on a popular quote from Ratatouille, anyone can cook, but not everyone is a chef. Don’t complain about mediocre work when you’re producing mediocre work yourself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spectrum of Taste #&lt;/head&gt;
    &lt;p&gt;What about the nature of taste itself? Should people focus on developing depth of taste in specific domains or breadth of taste across many domains? My short answer is a bit of both, if possible.&lt;/p&gt;
    &lt;p&gt;Depth of taste means becoming an expert within a particular domain. We’ve all met such experts and even asked them for help on tricky, bespoke topics within their domain. A person with depth of taste can recognize when AI-generated content is refined and of high quality versus merely functional. This kind of taste comes from years of experience in a specific role coupled with deep domain knowledge.&lt;/p&gt;
    &lt;p&gt;Breadth of taste means becoming knowledgeable across multiple domains and understanding how those domains interface with one another. A person with breadth of taste can recognize when AI-generated content is contextually appropriate, authentic, and of enough quality to use for their needs. This kind of taste comes from years of experience across multiple roles coupled with moderate domain knowledge.&lt;/p&gt;
    &lt;p&gt;Breadth of taste is more valuable with AI. When using AI, you’re constantly switching between domains: a software engineer writing documentation, a marketer creating designs. Breadth lets you maintain quality across these contexts while recognizing when you need domain expertise. You iterate faster because you have opinions about what “good enough” looks like across multiple domains.&lt;/p&gt;
    &lt;p&gt;The people I see being most effective with AI developed a breadth of taste that they use to determine what good AI-generated content looks like, regardless of domain. They can recognize when something feels off, even if they can’t articulate exactly why. They understand their own limitations and know when to seek expertise in a specific domain. That’s not to say those with depth of taste can’t be successful with AI, but I see those people reluctant to use AI because they are more knowledgeable than AI in a particular domain.&lt;/p&gt;
    &lt;head rend="h2"&gt;It Tastes Bitter #&lt;/head&gt;
    &lt;p&gt;If you’re reading this thinking you have to spend time developing your taste, good! Perhaps I’ve left a bitter taste in your mouth. The good news is you’re not alone. There are many people that need to hear this to better their taste, myself included. The challenge here is recognizing that it’s not about developing taste for AI but rather about developing taste, period. If you’ve had poor taste before AI you’ll have poor taste with AI. If you’ve had good taste before AI, you’ll be able to apply that taste with AI.&lt;/p&gt;
    &lt;p&gt;Instead of treating AI taste as some mystical new skill, focus on the fundamentals that were always important. Here are some actionable ways to develop your taste.&lt;/p&gt;
    &lt;p&gt;Tomorrow: Pick one piece of work you’re proud of and one you’re not. Write down specifically what makes them different. That’s taste in action.&lt;/p&gt;
    &lt;p&gt;This week: Find three examples of excellence in a domain you work in. Study them. What patterns emerge? What choices did the creators make?&lt;/p&gt;
    &lt;p&gt;This month: Take something you’ve created with or without AI and iterate on it a few times. Each iteration should have a specific improvement based on a specific critique.&lt;/p&gt;
    &lt;p&gt;Always: When someone preaches about AI taste, ask them to show you their work from before AI. If they can’t demonstrate taste in their pre-AI work, they’re not qualified to lecture you about it now.&lt;/p&gt;
    &lt;p&gt;The people succeeding with AI aren’t the ones who suddenly discovered taste. They’re the ones who already had it and simply adapted their standards to a new tool. Develop your taste with or without AI. The medium doesn’t matter, the fundamentals do.&lt;/p&gt;
    &lt;p&gt;Stop waiting for AI to force you to develop taste. Start now.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45288551</guid><pubDate>Thu, 18 Sep 2025 12:00:44 +0000</pubDate></item><item><title>KDE is now my favorite desktop</title><link>https://kokada.dev/blog/kde-is-now-my-favorite-desktop/</link><description>&lt;doc fingerprint="7416641b31042abb"&gt;
  &lt;main&gt;
    &lt;p&gt;From my last blog post, I am now using KDE as the desktop environment for my gaming rig. The reason is because I want a reasonably easy to use Linux desktop for when my wife needs to use the PC for something other than gaming, and this was the reason why my "traditional" Sway setup was a no-go.&lt;/p&gt;
    &lt;p&gt;But, after using KDE for a while I am starting to really appreciate how good it is. And no, this is not compared to other Linux desktops, but also with both Windows and macOS (that I need to use often, especially the later since my job gave me a MacBook Pro).&lt;/p&gt;
    &lt;p&gt;To start, KDE is surprisingly feature-complete. For example, the network applet gives lots of information that in other operational systems are either not available or difficult to access. It is easy to see in the screenshot below:&lt;/p&gt;
    &lt;p&gt;You can see things like channel, signal strength, frequency, MAC address, BSSID address (so the MAC address of the router). It even includes a handy button to share the Wi-Fi information via QR code, so you can easily setup a new mobile device like Android.&lt;/p&gt;
    &lt;p&gt;By the way, the crop and blur from that screenshot above? I made everything using the integrated screenshot tool. I didn't need to open an external application even once. It is also really smart, I need to redo this screenshot a few times and it kept the cropping to the exact area I was taking the screenshot before.&lt;/p&gt;
    &lt;p&gt;Another example, I wanted Steam to start automatically with the system, but it has the bad habit of putting its main window at the top. Really annoying since it sometimes ended up stealing up the focus. However KDE has this "Window Rules" feature inside "Window Management" settings where you can pretty much control whatever you want about application windows. Really useful tool.&lt;/p&gt;
    &lt;p&gt;KDE also has lots of really well integrated tools. For example, I am using some Flatpak applications and I can easily configure the permissions via System Settings. Or if I want hardware information like SMART status, I can just open Info Center. I can prevent the screen and computer to sleep at the click of a button (something that in both Windows and macOS I need to install a separate program). The list goes on, I keep getting surprised how many things that I used to need a third-party program that KDE just has available by default.&lt;/p&gt;
    &lt;p&gt;But not only KDE is fully featured, it is also fast. Now to be clear, this is a completely subjective analysis but I find KDE faster than Windows 11 in the same hardware, especially for things integrated in the system itself. For example, while opening Windows settings it can take a few seconds after a cold boot, the KDE's System Settings is pretty much instantaneous. Even compared with macOS in my MacBook Pro M2 Pro (that is of course comparing Apples and Bananas), KDE just feels snappier. I actually can't find much difference between KDE and my Sway setup to be honest, except maybe for the heavy use of animations (that can be disabled, but I ended up liking it after a while).&lt;/p&gt;
    &lt;p&gt;I will not say KDE is perfect though. At the first launch I got one issue where it started without the task bar because I connected this PC to both my monitor and TV, but the TV is used exclusively for gaming. However, KDE considered my TV the primary desktop and put the task bar only in that monitor, and even disabling the TV didn't add the task bar to my monitor. Easily fixed by manually adding a task bar, but an annoying problem (especially when you're not used to the desktop). There were also a few other minor issues that I don't remember right now.&lt;/p&gt;
    &lt;p&gt;After using KDE for about a week I can say that this is the first time that I really enjoy a desktop environment on Linux, after all those years. Props for the KDE developers for making the experience so good.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45288690</guid><pubDate>Thu, 18 Sep 2025 12:17:51 +0000</pubDate></item><item><title>The quality of AI-assisted software depends on unit of work management</title><link>https://blog.nilenso.com/blog/2025/09/15/ai-unit-of-work/</link><description>&lt;doc fingerprint="2651f55330438f4e"&gt;
  &lt;main&gt;
    &lt;p&gt;The craft of AI-assisted software creation is substantially about correctly managing units of work.&lt;/p&gt;
    &lt;p&gt;When I was new to this emerging craft of AI-assisted coding, I was getting lousy results, despite the models being rather intelligent. Turns out the major bottleneck is not intelligence, but rather providing the correct context.&lt;/p&gt;
    &lt;p&gt;Andrej Karpathy, while referencing my earlier article on this topic, described the work of AI-assisted engineering as “putting AI on a tight leash”. What does a tight leash look like for a process where AI agents are operating on your code more independently than ever? He dropped a hint: work on small chunks of a single concrete thing.&lt;/p&gt;
    &lt;head rend="h2"&gt;The right sized unit of work respects the context&lt;/head&gt;
    &lt;p&gt;I like the term context engineering, because it has opened up the vocabulary to better describe why managing units of work is perhaps the most important technique to get better results out of AI tools. It centers our discussion around the “canvas” against which our AI is generating code.&lt;/p&gt;
    &lt;p&gt;I like Anthropic’s visualisation from their docs:&lt;/p&gt;
    &lt;p&gt;The generated output of the LLM is a sample of the next token probability. Every time we generate a token, what has already been generated in the previous iteration is appended to the context window. What this context window looks like has a huge influence on the quality of your generated output.&lt;/p&gt;
    &lt;p&gt;Drew Breunig wrote an excellent article about all kinds of things that can go wrong with your context and proposed various techniques to fix them.&lt;/p&gt;
    &lt;p&gt;The best AI-assisted craftsmen are often thinking about the design and arrangement of their context to get the AI to one-shot a solution. This is tricky and effortful, contrary to what the AI coding hype suggests.&lt;/p&gt;
    &lt;p&gt;If you don’t provide the necessary information in the context to do a good job, your AI will hallucinate or generate code that is not congruent with the practices of your codebase. It is especially brittle at integration points of your software system.&lt;/p&gt;
    &lt;p&gt;On the other hand, if you fill up the context with too much information, and the quality of your output degrades, because of a lack of focused attention.&lt;/p&gt;
    &lt;p&gt;Breaking down your task into “right-sized” units of work, which describe just the right amount of detail is perhaps the most powerful lever to improve your context window, and thus the correctness and quality of the generated code.&lt;/p&gt;
    &lt;head rend="h2"&gt;The right sized unit of work controls the propagation of errors&lt;/head&gt;
    &lt;p&gt;Time for some napkin maths.&lt;/p&gt;
    &lt;p&gt;Let’s say your AI agent has a 5% chance of making a mistake. I’m not just referring to hallucinations—it could be a subtle mistake because it forgot to look up some documentation or you missed a detail in your specification.&lt;/p&gt;
    &lt;p&gt;In an agentic multi-turn workflow, which is what all coding workflows are converging to, this error compounds. If your task takes 10 turns to implement, you will have a (1 – 0.95)10 = 59.9% chance of success. Not very high.&lt;/p&gt;
    &lt;p&gt;Utkarsh Kanwat in his blog post has made the same argument. His conclusion was that any AI agent would need some kind of pause-and-verify gating mechanism at each step for a long-horizon task.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Per-action&lt;p&gt;error rate&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Overall Success Rate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;5 turns&lt;/cell&gt;
        &lt;cell&gt;10 turns&lt;/cell&gt;
        &lt;cell&gt;20 turns&lt;/cell&gt;
        &lt;cell&gt;50 turns&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;0.1%&lt;/cell&gt;
        &lt;cell&gt;99.5%&lt;/cell&gt;
        &lt;cell&gt;99.0%&lt;/cell&gt;
        &lt;cell&gt;98.0%&lt;/cell&gt;
        &lt;cell&gt;95.1%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;1%&lt;/cell&gt;
        &lt;cell&gt;95.1%&lt;/cell&gt;
        &lt;cell&gt;90.4%&lt;/cell&gt;
        &lt;cell&gt;81.8%&lt;/cell&gt;
        &lt;cell&gt;60.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;5%&lt;/cell&gt;
        &lt;cell&gt;77.4%&lt;/cell&gt;
        &lt;cell&gt;59.9%&lt;/cell&gt;
        &lt;cell&gt;35.8%&lt;/cell&gt;
        &lt;cell&gt;7.7%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;10%&lt;/cell&gt;
        &lt;cell&gt;59.0%&lt;/cell&gt;
        &lt;cell&gt;34.9%&lt;/cell&gt;
        &lt;cell&gt;12.2%&lt;/cell&gt;
        &lt;cell&gt;0.5%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;20%&lt;/cell&gt;
        &lt;cell&gt;32.8%&lt;/cell&gt;
        &lt;cell&gt;10.7%&lt;/cell&gt;
        &lt;cell&gt;1.2%&lt;/cell&gt;
        &lt;cell&gt;0.0%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;What does the state of the art for multi-turn error rates look like? METR recently published a popular chart describing how AI models are getting better at long-horizon tasks. Currently GPT-5 is at the top of the leaderboard, where it can perform ~2-hour long tasks at around a 70% success rate. Working backwards (let’s say a 2 hour task is 50+ turns) this would amount to a sub-1% error rate per action.&lt;/p&gt;
    &lt;p&gt;Doesn’t a &amp;lt;1% error rate per action seem suspicious to you? As a regular user of agentic coding tools (my current one is Codex CLI), I’ll eat my shoe if GPT-5 starts nailing my tasks 99.9% of the time.&lt;/p&gt;
    &lt;p&gt;My intuition derived from experience tells me that even the best AI right now isn’t even 95% likely to be correct. So where is the difference coming from? It needs a closer look at the actual paper:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Our tasks typically use environments that do not significantly change unless directly acted upon by the agent. In contrast, real tasks often occur in the context of a changing environment.&lt;/p&gt;
      &lt;p&gt;[…]&lt;/p&gt;
      &lt;p&gt;Similarly, very few of our tasks are punishing of single mistakes. This is in part to reduce the expected cost of collecting human baselines.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is not at all like the tasks I am doing.&lt;/p&gt;
    &lt;p&gt;METR acknowledges the messiness of the real world. They have come up with a “messiness rating” for their tasks, and the “mean messiness” of their tasks is 3.2/16.&lt;/p&gt;
    &lt;p&gt;By METR’s definitions, the kind of software engineering work that I’m mostly exposed to would score at least around 7-8, given that software engineering projects are path-dependent, dynamic and without clear counterfactuals. I have worked on problems that get to around 13/16 levels of messiness.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An increase in task messiness by 1 point reduces mean success rates by roughly 8.1%&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Extrapolating from METR’s measured effect of messiness, GPT-5 would go from 70% to around 40% success rate for 2-hour tasks. This maps to my experienced reality.&lt;/p&gt;
    &lt;p&gt;I am not certain that pure intelligence can solve for messiness. Robustness to environmental chaos and the fuzzy nature of reality is fundamentally about managing context well. Until we find the magic sauce that solves this, it is clear that we need a workflow that can break down our problem into units of work, with verifiable checkpoints to manage the compounding of errors.&lt;/p&gt;
    &lt;p&gt;These verifiable checkpoints need to be legible to humans.&lt;/p&gt;
    &lt;head rend="h2"&gt;So, what is the “right sized” unit of work?&lt;/head&gt;
    &lt;p&gt;The right sized unit of work needs to be small and describe the desired outcome concisely.&lt;/p&gt;
    &lt;p&gt;The desired outcome on completion of a unit of work needs to be human-legible. I argue that it needs to provide legible business value. Ultimately, the users of software are going to be humans (or systems that model human constructs). Therefore, an elegant way to break down a project is to model it as small units of work that provide legible business value at each checkpoint. This will serve the purpose of respecting the context window of the LLM and help manage the propagation of errors.&lt;/p&gt;
    &lt;p&gt;Software engineers have already defined a unit of work that provides business value and serve as the placeholder for all the context and negotiation of scope—User Stories. I think they are a good starting point to help us break down a large problem into smaller problems that an LLM can one-shot, while providing a concrete result. They center user outcomes, which unlike “tasks”, are robust to the messy dynamic environment of software development.&lt;/p&gt;
    &lt;p&gt;Deliverable business value is also what all stakeholders can understand and work with. Software is not built in a vacuum by developers—it needs the coordination of teams, product owners, business people and users. The fact that AI agents work in their own context environment separate from the other stakeholders hurts effectiveness and transfer of its benefits. I think this is an important gap that needs to be bridged.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;unit size&lt;/cell&gt;
        &lt;cell role="head"&gt;outcome of completion&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TODO item&lt;/cell&gt;
        &lt;cell&gt;small&lt;/cell&gt;
        &lt;cell&gt;incremental technical value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;“Plan Mode”&lt;/cell&gt;
        &lt;cell&gt;large&lt;/cell&gt;
        &lt;cell&gt;technical value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon Kiro Spec&lt;/cell&gt;
        &lt;cell&gt;small&lt;/cell&gt;
        &lt;cell&gt;technical value&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;User Story&lt;/cell&gt;
        &lt;cell&gt;small&lt;/cell&gt;
        &lt;cell&gt;business value&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Most AI agents today have well-functioning “planning” modes. These are good at keeping the agent on rails, but they mostly provide technical value, and not necessarily a legible business outcome. I believe planning is complementary to our idea of breaking down a project into small units of business value. My proposed unit of work can be planned with existing planning tools. And I believe this is superior to planning over a large unit of work due to the context rot issues described earlier.&lt;/p&gt;
    &lt;p&gt;Of course, plain old User Stories as described in the Agile canon is not sufficient. It needs to be accompanied by “something more” that can nudge the agents to gather the right context that serves the business value outcome of the stories. What that “something more” could look like is something we hope to answer in the coming months.&lt;/p&gt;
    &lt;head rend="h2"&gt;The StoryMachine experiment&lt;/head&gt;
    &lt;p&gt;To test whether user stories with “something more” can indeed serve as optimal units of work that that have the properties I described above, we are running an experiment called StoryMachine. Currently StoryMachine does not do much—it reads your PRD and Tech Specs and produces story cards. It is still early days. But we will set up an evaluation system that will help us iterate to a unit of work description that helps us build useful software effortlessly. I hope to share updates on what we find in the coming months.&lt;/p&gt;
    &lt;p&gt;I want the craft of AI-assisted development to be less effortful and less like a slot-machine. And our best lever to get there is managing the unit of work.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45289168</guid><pubDate>Thu, 18 Sep 2025 13:06:03 +0000</pubDate></item><item><title>Flipper Zero Geiger Counter</title><link>https://kasiin.top/blog/2025-08-04-flipper_zero_geiger_counter_module/</link><description>&lt;doc fingerprint="1c73dc206bab88f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Flipper Zero Geiger Counter&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;注意：所有模块均在第三方固件中测试，推荐使用:&lt;/p&gt;&lt;lb/&gt;unleashed固件，链接：[https://github.com/DarkFlippers/unleashed-firmware]&lt;lb/&gt;Momentum固件，链接：[https://github.com/Next-Flip/Momentum-Firmware]&lt;/quote&gt;
    &lt;head rend="h2"&gt;Compatible apps&lt;/head&gt;
    &lt;head rend="h2"&gt;Geiger counter&lt;/head&gt;
    &lt;p&gt;This app gives you a graph view with counts per second (instantaneous measure of the radioactivity) as CPS and per minute as CPM.&lt;/p&gt;
    &lt;p&gt;There is extra functionality to record, zoom, change units, etc.&lt;/p&gt;
    &lt;p&gt;(credits to nmrr)&lt;/p&gt;
    &lt;p&gt;CPS: counts per second (instantaneous measure of the radioactivity). CPS is alway displayed on the left corner.&lt;/p&gt;
    &lt;p&gt;CPM: counts per minute (the sum of CPS over a period of one minute). Other units of measurement can be chosen with Left/Right.&lt;/p&gt;
    &lt;p&gt;New CPS bar measure appears on the left every second.&lt;/p&gt;
    &lt;p&gt;A4 GPIO can be connected on A7 GPIO to test this application without using a geiger tube. A4 GPIO generates a signal with a frequency that varies every second.&lt;/p&gt;
    &lt;head rend="h3"&gt;Button assignments:&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;button&lt;/cell&gt;
        &lt;cell role="head"&gt;function&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ok [long press]&lt;/cell&gt;
        &lt;cell&gt;Clear the graph&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Left/Right [short press]&lt;/cell&gt;
        &lt;cell&gt;Choose unit on the right corner (cpm, μSv/h, mSv/y, Rad/h, mRad/h, uRad/h), cps on the left is always displayed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Up [long press]&lt;/cell&gt;
        &lt;cell&gt;Enable/disable recording, led of Flipper Zero is colored in red when recording&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Up/Down [short press]&lt;/cell&gt;
        &lt;cell&gt;Zoom/unzoom&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Down [long press]&lt;/cell&gt;
        &lt;cell&gt;Display version of the application&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Back [long press]&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Use cases&lt;/head&gt;
    &lt;p&gt;Ambient radioactivity (descendants of radon gas are detected, not radon itself):&lt;/p&gt;
    &lt;p&gt;Measurement of a sample of uranium ore within a lead container:&lt;/p&gt;
    &lt;p&gt;Note: measures in Sv or Rad are not precise&lt;/p&gt;
    &lt;p&gt;Measurement of a sample of uranium ore (the most radioactive part):&lt;/p&gt;
    &lt;p&gt;Measurement of radium dial pointers:&lt;/p&gt;
    &lt;p&gt;All prior measurements in sequence (the scale of the graph is automatically adjusted):&lt;/p&gt;
    &lt;p&gt;Measurement of uranium orange pottery:&lt;/p&gt;
    &lt;p&gt;Measurement of americium-241 button from a smoke detector (descendants of americium or radioisotope impurities are detected, not americium itself):&lt;/p&gt;
    &lt;p&gt;A4 GPIO on A7 GPIO (to test this program without a geiger board):&lt;/p&gt;
    &lt;p&gt;Zoom levels (the third picture is the default zoom):&lt;/p&gt;
    &lt;p&gt;Version of the application (press down button during 1 sec to display version):&lt;/p&gt;
    &lt;head rend="h3"&gt;Recording function&lt;/head&gt;
    &lt;p&gt;Output CSV files are stored in the root directory of the SD card. Date and time are incorporated into the file name (example: geiger-2023-07-03--23-48-15.csv)&lt;/p&gt;
    &lt;p&gt;Data sample:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;epoch&lt;/cell&gt;
        &lt;cell role="head"&gt;cps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;4&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: J305 geiger tube is only sensible to beta and gamma rays. Alpha rays cannot be detected.&lt;/p&gt;
    &lt;p&gt;Usable radioactive sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;natural uranium (alpha, beta, gamma)&lt;/item&gt;
      &lt;item&gt;natural thorium (alpha, beta, gamma)&lt;/item&gt;
      &lt;item&gt;radium-226 (alpha, beta, gamma)&lt;/item&gt;
      &lt;item&gt;cobalt-60 (beta &amp;amp; gamma)&lt;/item&gt;
      &lt;item&gt;iodine-131 (beta &amp;amp; gamma)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Not really usable radioactive sources (must be in contact with the geiger tube to be detected):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;americium-241 (alpha &amp;amp; low gamma, some strong beta/gamma rays are emitted during radioactive cascade or due to the presence of radioisotope impurities)&lt;/item&gt;
      &lt;item&gt;high purity metallic uranium/thorium (same as am241)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Totaly unusable radioactive sources:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;polonium-210 (pure alpha)&lt;/item&gt;
      &lt;item&gt;tritium (very low beta)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Atomic dice roller&lt;/head&gt;
    &lt;p&gt;With this app you can get a really random dice based on the geiger counter. Make your decisions and board games extra exciting.&lt;/p&gt;
    &lt;p&gt;(credits to nmrr)&lt;/p&gt;
    &lt;p&gt;This application generates true random numbers by hashing timestamps obtained when a tick is produced by the geiger counter (i.e. when a beta or gamma ray is detected). Timestamps have 32 bit resolution and are produced from a 64 MHz signal.&lt;/p&gt;
    &lt;p&gt;Two hash methods have been implemented:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CRC32: 8 ticks are needed to obtain a hash, for low activity sources&lt;/item&gt;
      &lt;item&gt;MD5: 32 ticks are needed to obtain a hash, for high activity sources&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dice rolls are produced by transforming a single hash into a number between 1 and 6. Out of scope values are ignored so the dice is really balanced. Modulo-based methods are ugly because they are usually unbalanced.&lt;/p&gt;
    &lt;p&gt;It's possible to roll the dice without using a radioactive isotope. Air contains radon gas that is radioactive. Geiger board can detect descendants of radon gas that emit strong beta or gamma rays.&lt;/p&gt;
    &lt;p&gt;In the left corner, counts per second (cps) indicates the activity. In the right corner, availiable dice rolls are indicated. 64 rolls can be stored.&lt;/p&gt;
    &lt;head rend="h3"&gt;Button assignments:&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;button&lt;/cell&gt;
        &lt;cell role="head"&gt;function&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ok [short short]&lt;/cell&gt;
        &lt;cell&gt;Roll the dice&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Left [long press]&lt;/cell&gt;
        &lt;cell&gt;Set CRC32 as hash method&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Right [long press]&lt;/cell&gt;
        &lt;cell&gt;Set MD5 as hash method&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Up [long press]&lt;/cell&gt;
        &lt;cell&gt;Set 0-1 as output range (coin flipper)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Down [long press]&lt;/cell&gt;
        &lt;cell&gt;Set 1-6 as output range (dice roller)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Back [long press]&lt;/cell&gt;
        &lt;cell&gt;Exit&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Warning&lt;/head&gt;
    &lt;p&gt;These apps are for educational purposes only. Please use this code responsibly and only use these apps on your own equipment.&lt;lb/&gt; 本模块和软件只能用作教育和学习用途，一切使用责任请自负，请仅在您拥有的设备上使用&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45289453</guid><pubDate>Thu, 18 Sep 2025 13:28:59 +0000</pubDate></item><item><title>Luau – fast, small, safe, gradually typed scripting language derived from Lua</title><link>https://luau.org/</link><description>&lt;doc fingerprint="f76b3cfdc32baad4"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Motivation&lt;/head&gt;
    &lt;p&gt;Around 2006, Roblox started using Lua 5.1 as a scripting language for games. Over the years we ended up substantially evolving the implementation and the language; to support growing sophistication of games on the Roblox platform, growing team sizes and large internal teams writing a lot of code for application/editor (1+MLOC as of 2020), we had to invest in performance, ease of use and language tooling, and introduce a gradual type system to the language. More…&lt;/p&gt;
    &lt;head rend="h2"&gt;Sandboxing&lt;/head&gt;
    &lt;p&gt;Luau limits the set of standard libraries exposed to the users and implements extra sandboxing features to be able to run unprivileged code (written by our game developers) side by side with privileged code (written by us). This results in an execution environment that is different from what is commonplace in Lua. More…&lt;/p&gt;
    &lt;head rend="h2"&gt;Compatibility&lt;/head&gt;
    &lt;p&gt;Whenever possible, Luau aims to be backwards-compatible with Lua 5.1 and at the same time to incorporate features from later revisions of Lua. However, Luau is not a full superset of later versions of Lua - we do not always agree with Lua design decisions, and have different use cases and constraints. All post-5.1 Lua features, along with their support status in Luau, are documented here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Syntax&lt;/head&gt;
    &lt;p&gt;Luau is syntactically backwards-compatible with Lua 5.1 (code that is valid Lua 5.1 is also valid Luau); however, we have extended the language with a set of syntactical features that make the language more familiar and ergonomic. The syntax is described here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Analysis&lt;/head&gt;
    &lt;p&gt;To make it easier to write correct code, Luau comes with a set of analysis tools that can surface common mistakes. These consist of a linter and a type checker, colloquially known as script analysis, and are integrated into &lt;code&gt;luau-analyze&lt;/code&gt; command line executable. The linting passes are described here, and the type checking user guide can be found here.&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;In addition to a completely custom front end that implements parsing, linting and type checking, Luau runtime features new bytecode, interpreter and compiler that are heavily tuned for performance. Luau interpreter can be competitive with LuaJIT interpreter depending on the program. An optional component for manual Just-In-Time compilation is also available for x64 and arm64 platforms, which can considerably speed up certain programs. We continue to optimize the runtime and rewrite portions of it to be even more efficient. While our overall goal is to minimize the amount of time programmers spend tuning performance, some details about the performance characteristics are provided for inquisitive minds.&lt;/p&gt;
    &lt;head rend="h2"&gt;Libraries&lt;/head&gt;
    &lt;p&gt;As a language, Luau is a full superset of Lua 5.1. As far as standard library is concerned, some functions had to be removed from the builtin libraries, and some functions had to be added; refer to full documentation for details. When Luau is embedded into an application, the scripts normally get access to extra library features that are application-specific.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45289558</guid><pubDate>Thu, 18 Sep 2025 13:38:49 +0000</pubDate></item><item><title>Automatic Differentiation Can Be Incorrect</title><link>https://www.stochasticlifestyle.com/the-numerical-analysis-of-differentiable-simulation-automatic-differentiation-can-be-incorrect/</link><description>&lt;doc fingerprint="2f9fc97326a315ca"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Numerical Analysis of Differentiable Simulation: Automatic Differentiation Can Be Incorrect&lt;/head&gt;
    &lt;head rend="h4"&gt;April 20 2025 in Differential Equations, Julia, Mathematics, Scientific ML | Tags: | Author: Christopher Rackauckas&lt;/head&gt;
    &lt;head rend="h1"&gt;The Numerical Analysis of Differentiable Simulation: How Automatic Differentiation of Physics Can Give Incorrect Derivatives&lt;/head&gt;
    &lt;p&gt;Scientific machine learning (SciML) relies heavily on automatic differentiation (AD), the process of constructing gradients which include machine learning integrated into mechanistic models for the purpose of gradient-based optimization. While these differentiable programming approaches pitch an idea of “simply put the simulator into a loss function and use AD”, it turns out there are a lot more subtle details to consider in practice. In this talk we will dive into the numerical analysis of differentiable simulation and ask the question: how numerically stable and robust is AD? We will use examples from the Python-based Jax (diffrax) and PyTorch (torchdiffeq) libraries in order to demonstrate how canonical formulations of AD and adjoint methods can give inaccurate gradients in the context of ODEs and PDEs. We demonstrate cases where the methodologies are “mathematically correct”, but due to the intricacies of numerical error propagation, their approaches can give 60% and greater error even in simple cases like linear ODEs. We’ll then describe some of the non-standard modifications to AD which are done in the Julia SciML libraries to overcome these numerical instabilities and achieve accurate results, crucially also describing the engineering trade-offs which are required to be made in the process. The audience should leave with a greater appreciation of the greater numerical challenges which still need to be addressed in the field of AD for SciML.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45289829</guid><pubDate>Thu, 18 Sep 2025 14:01:35 +0000</pubDate></item><item><title>Geizhals Preisvergleich Donates USD 10k to the Perl and Raku Foundation</title><link>https://www.perl.com/article/geizhals-donates-to-tprf/</link><description>&lt;doc fingerprint="bfc211d91dba202a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Geizhals Preisvergleich Donates USD 10,000 to The Perl and Raku Foundation&lt;/head&gt;
    &lt;p&gt;Today The Perl and Raku Foundation is thrilled to announce a donation of USD 10,000 from Geizhals Preisvergleich. This gift helps to secure the future of The Perl 5 Core Maintenance Fund.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Perl has been an integral part of our product price comparison platform from the start of the company 25 years ago. Supporting the Perl 5 Core Maintenance Fund means supporting both present and future of a substantial pillar of Modern Open Source Computing, for us and other current or prospective users.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;– Michael Kröll of Geizhals Preisvergleich&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“Geizhals is not only providing core funding for the Perl ecosystem, but also supporting developers, actively contributing to European conferences, and employing Perl coders. Their interest in the strategic maintenance and development of Perl and CPAN is of great value to us all, and their investment is very much appreciated.”&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;– Stuart J Mackintosh, President of The Perl and Raku Foundation&lt;/p&gt;
    &lt;p&gt;But who exactly is Geizhals, and why does their support matter so much to the Perl community?&lt;/p&gt;
    &lt;p&gt;Geizhals Preisvergleich began in July of 1997 as a hobby project—and yes, “Geizhals” literally translates to “skinflint” in English (they even operate skinflint.co.uk for UK users!). From those humble beginnings, they’ve leveraged the power of Perl to scale up to serving 4.3 million monthly users. With Perl being a key part of their infrastructure, they have generously decided to support the Perl 5 Core Maintenance Fund.&lt;/p&gt;
    &lt;p&gt;While many of us know about the Core Maintenance Fund, the specific problems it addresses often remain invisible to users. I reached out to the maintainers whose work is supported by this fund. This is what core maintainer Tony Cook had to say:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;My work tends to be little things, I review other people’s work which I think improves quality and velocity, and fix more minor issues, some examples would be:&lt;/p&gt;
      &lt;p&gt;a fix to signal handling where perl could crash where an external library created threads (#22487)&lt;/p&gt;
      &lt;p&gt;fix a segmentation fault in smartmatch against a sub if the sub exited via a loop exit op (such as last) (#16608)&lt;/p&gt;
      &lt;p&gt;fixed a bug where a regexp warning could leak memory.&lt;/p&gt;
      &lt;p&gt;prevent a confusing undefined warning message when accessing a sub parameter that was placeholder for a hash element indexed by an undef key (#22423)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;What Tony has highlighted are the kinds of bug fixes which collectively help to ensure that Perl remains stable, secure and reliable for the many organisations and individuals who depend on it.&lt;/p&gt;
    &lt;p&gt;With organizations like Geizhals Preisvergleich funding the work which Tony and others put into maintaining the Perl 5 core, we can work together to ensure that the Perl core continues to receive the maintenance which it deserves, for many years to come. Whether you’re a startup using Perl for rapid prototyping or an enterprise running mission-critical systems, your support helps ensure Perl remains reliable for everyone. Please join us on this journey.&lt;/p&gt;
    &lt;p&gt;For more information on how to become a sponsor, please contact: olaf@perlfoundation.org&lt;/p&gt;
    &lt;p&gt;Tags&lt;/p&gt;
    &lt;head rend="h3"&gt;Olaf Alders&lt;/head&gt;
    &lt;p&gt;Dad, Perl hacker, guitar player and swimmer. Olaf is a founder of the MetaCPAN project.&lt;/p&gt;
    &lt;head rend="h5"&gt;Browse their articles&lt;/head&gt;
    &lt;head rend="h3"&gt;Feedback&lt;/head&gt;
    &lt;p&gt;Something wrong with this article? Help us out by opening an issue or pull request on GitHub&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45289834</guid><pubDate>Thu, 18 Sep 2025 14:01:45 +0000</pubDate></item><item><title>Fuck, you're still sad?</title><link>https://bessstillman.substack.com/p/oh-fuck-youre-still-sad</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45290021</guid><pubDate>Thu, 18 Sep 2025 14:17:40 +0000</pubDate></item><item><title>TernFS – An exabyte scale, multi-region distributed filesystem</title><link>https://www.xtxmarkets.com/tech/2025-ternfs/</link><description>&lt;doc fingerprint="65ba0c171fb742d2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;TernFS — an exabyte scale, multi-region distributed filesystem&lt;/head&gt;
    &lt;p&gt;September 2025&lt;/p&gt;
    &lt;p&gt;XTX is an algorithmic trading firm: it builds statistical models that produce price forecasts for over 50,000 financial instruments worldwide. We use those forecasts to make trades. As XTX's research efforts to build better models ramped up, the demand for resources kept increasing.&lt;/p&gt;
    &lt;p&gt;The firm started out with a couple of desktops and an NFS server, and 10 years later ended up with tens of thousands of high-end GPUs, hundreds of thousands of CPUs, and hundreds of petabytes of storage.&lt;/p&gt;
    &lt;p&gt;As compute grew, storage struggled to keep up. We rapidly outgrew NFS first and existing open-source and commercial filesystems later. After evaluating a variety of third-party solutions, we made the decision to implement our own filesystem, which we called TernFS[1].&lt;/p&gt;
    &lt;p&gt;We have decided to open source our efforts: TernFS is available as free software on our public GitHub. This post motivates TernFS, explains its high-level architecture, and then explores some key implementation details. If you just want to spin up a local TernFS cluster, head to the README.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another filesystem?&lt;/head&gt;
    &lt;p&gt;There's a reason why every major tech company has developed its own distributed filesystem — they're crucial to running large-scale compute efforts, and liable to cause intense disruption if they malfunction. [2]&lt;/p&gt;
    &lt;p&gt;XTX was in the same position, so we designed TernFS to be a one-stop solution for most of our storage needs, going from relatively 'cold' storage of raw market data to short-lived random-access data used to communicate between GPU jobs running on our cluster.&lt;/p&gt;
    &lt;p&gt;TernFS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Is designed to scale up to tens of exabytes, trillions of files, millions of concurrent clients.&lt;/item&gt;
      &lt;item&gt;Stores file contents redundantly to protect against drive failures.&lt;/item&gt;
      &lt;item&gt;Has no single point of failure in its metadata services.&lt;/item&gt;
      &lt;item&gt;Supports file snapshot to protect against accidental file deletion.&lt;/item&gt;
      &lt;item&gt;Can span across multiple regions.&lt;/item&gt;
      &lt;item&gt;Is hardware agnostic and uses TCP/IP to communicate.&lt;/item&gt;
      &lt;item&gt;Utilizes different types of storage (such as flash vs. hard disks) cost effectively.&lt;/item&gt;
      &lt;item&gt;Exposes read/write access through its own API over TCP and UDP, and a Linux kernel filesystem module.&lt;/item&gt;
      &lt;item&gt;Requires no external service and has a minimal set of build dependencies. [3]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Specifically, C++ and Go are needed to build the various TernFS components.&lt;/p&gt;
    &lt;p&gt;The C++ and Go processes depend on a handful of vendored libraries, most notably RocksDB for C++.&lt;/p&gt;
    &lt;p&gt;Naturally, there are some limitations, the main ones being:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Files are immutable — once they're written they can't be modified.&lt;/item&gt;
      &lt;item&gt;TernFS should not be used for tiny files — our median file size is 2MB.&lt;/item&gt;
      &lt;item&gt;The throughput of directory creation and removal is significantly constrained compared to other operations.&lt;/item&gt;
      &lt;item&gt;TernFS is permissionless, deferring that responsibility to other services.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We started designing TernFS in early 2022 and began putting it into production in summer 2023. By mid-2024 all of our machine learning efforts were driven out of TernFS, and we're migrating the rest of the firm's storage needs onto it as well.&lt;/p&gt;
    &lt;p&gt;As of September 2025, our TernFS deployment stores more than 500PB across 30,000 disks, 10,000 flash drives, and three data centres. At peak we serve multiple terabytes per second. To this day, we haven't lost a single byte.&lt;/p&gt;
    &lt;head rend="h2"&gt;High-level overview&lt;/head&gt;
    &lt;p&gt;Now that the stage is set, we're ready to explain the various components that make up TernFS. TernFS' core API is implemented by four services:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Metadata shards store the directory structure and file metadata.&lt;/item&gt;
      &lt;item&gt;The cross-directory coordinator (or CDC) executes cross-shard transactions.&lt;/item&gt;
      &lt;item&gt;Block services store file contents.&lt;/item&gt;
      &lt;item&gt;The registry stores information about all the other services and monitors them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;
 A ──► B means "A sends requests to B" 
                                       
                                       
 ┌────────────────┐                    
 │ Metadata Shard ◄─────────┐          
 └─┬────▲─────────┘         │          
   │    │                   │          
   │    │                   │          
   │ ┌──┴──┐                │          
   │ │ CDC ◄──────────┐     │          
   │ └──┬──┘          │     │          
   │    │             │ ┌───┴────┐     
   │    │             └─┤        │     
 ┌─▼────▼────┐          │ Client │     
 │ Registry  ◄──────────┤        │     
 └──────▲────┘          └─┬──────┘     
        │                 │            
        │                 │            
 ┌──────┴────────┐        │            
 │ Block Service ◄────────┘            
 └───────────────┘

&lt;/code&gt;
    &lt;p&gt;In the next few sections, we'll describe the high-level design of each service and then give more background on other relevant implementation details.[4]&lt;/p&gt;
    &lt;p&gt;Note that TernFS' multi-region capabilities are orthogonal to much of its high-level design, and they're therefore explained separately.&lt;/p&gt;
    &lt;head rend="h3"&gt;Metadata&lt;/head&gt;
    &lt;p&gt;To talk about metadata, we first need to explain what metadata is in TernFS. The short answer is: 'everything that is not file contents.' The slightly longer answer is:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Directory entries, including all files and directory names.&lt;/item&gt;
      &lt;item&gt;File metadata including creation/modification/access time, logical file size, and so on.&lt;/item&gt;
      &lt;item&gt;The mapping between files and the blocks containing their contents.&lt;/item&gt;
      &lt;item&gt;Other ancillary data structures to facilitate maintenance operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;TernFS' metadata is split into 256 logical shards. Shards never communicate with each other. This is a general principle in TernFS: each service is disaggregated from the others, deferring to the clients to communicate with each service directly.[5]&lt;/p&gt;
    &lt;p&gt;There are some exceptions — most notably the shards execute requests from the CDC, and all services check into the registry.&lt;/p&gt;
    &lt;p&gt;A logical shard is further split into five physical instances, one leader and four followers, in a typical distributed consensus setup. The distributed consensus engine is provided by a purpose-built Raft-like implementation, which we call LogsDB, while RocksDB is used to implement read/write capabilities within a shard instance.&lt;/p&gt;
    &lt;p&gt;Currently all reads and writes go through the leader, but it would be trivial to allow clients to read from followers, and with a bit more effort to switch to a write-write setup.&lt;/p&gt;
    &lt;code&gt;    ┌─────────┐ ┌─────────┐       ┌───────────┐ 
    │ Shard 0 │ │ Shard 1 │  ...  │ Shard 255 │ 
    └─────────┘ │         │       └───────────┘ 
            ┌───┘         └───────────────────┐ 
            │                                 │ 
            │                  ┌────────────┐ │ 
            │ ┌───────────┐    │ Replica 0  │ │ 
            │ │           ◄────► (follower) │ │ 
 ┌────────┐ │ │ Replica 3 ◄──┐ └────────────┘ │ 
 │ Client ├─┼─► (leader)  ◄─┐│ ┌────────────┐ │ 
 └────────┘ │ │           ◄┐│└─► Replica 1  │ │ 
            │ └───────────┘││  │ (follower) │ │ 
            │              ││  └────────────┘ │ 
            │              ││  ┌────────────┐ │ 
            │              │└──► Replica 2  │ │ 
            │              │   │ (follower) │ │ 
            │              │   └────────────┘ │ 
            │              │   ┌────────────┐ │ 
            │              └───► Replica 4  │ │ 
            │                  │ (follower) │ │ 
            │                  └────────────┘ │ 
            └─────────────────────────────────┘ 
&lt;/code&gt;
    &lt;p&gt;Splitting the metadata into 256 shards from the get-go simplifies the design, given that horizontal scaling of metadata requires no rebalancing, just the addition of more metadata servers.&lt;/p&gt;
    &lt;p&gt;For instance, our current deployment can serve hundreds of petabytes and more than 100,000 compute nodes with just 10 metadata servers per data centre, with each server housing roughly 25 shard leaders and 100 shard followers.&lt;/p&gt;
    &lt;p&gt;Given that the metadata servers are totally decoupled from one another, this means that we can scale metadata performance by 25× trivially, and by 100× if we were to start offloading metadata requests to followers.&lt;/p&gt;
    &lt;p&gt;TernFS shards metadata by assigning each directory to a single shard. This is done in a simple round-robin fashion by the cross-directory coordinator. Once a directory is created, all its directory entries and the files in it are housed in the same shard.&lt;/p&gt;
    &lt;p&gt;This design decision has downsides: TernFS assumes that the load will be spread across the 256 logical shards naturally. This is not a problem in large deployments, given that they will contain many directories, but it is something to keep in mind.[6]&lt;/p&gt;
    &lt;head rend="h3"&gt;Cross-directory transactions&lt;/head&gt;
    &lt;p&gt;Most of the metadata activity is contained within a single shard:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;File creation, same-directory renames, and deletion.&lt;/item&gt;
      &lt;item&gt;Listing directory contents.&lt;/item&gt;
      &lt;item&gt;Getting attributes of files or directories.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;However, some operations do require coordination between shards, namely directory creation, directory removal, and moving directory entries across different directories.&lt;/p&gt;
    &lt;p&gt;The cross-directory coordinator (CDC) performs these distributed transactions using a privileged metadata shard API. The CDC transactions are stateful, and therefore the CDC uses RocksDB and LogsDB much like the metadata shards themselves to persist its state safely.&lt;/p&gt;
    &lt;code&gt; ┌────────┐    ┌──────────┐ ┌───────────┐ 
 │ Client ├─┐  │ Shard 32 │ │ Shard 103 │ 
 └────────┘ │  └────────▲─┘ └─▲─────────┘ 
 ┌─────┬────┼───────────┼─────┼─┐         
 │ CDC │  ┌─▼──────┐    │     │ │         
 ├─────┘  │ Leader ├────┴─────┘ │         
 │        └─────▲──┘            │         
 │              │               │         
 │       ┌──────┴───────┐       │         
 │       │              │       │         
 │ ┌─────▼────┐    ┌────▼─────┐ │         
 │ │ Follower │ .. │ Follower │ │         
 │ └──────────┘    └──────────┘ │         
 └──────────────────────────────┘   
&lt;/code&gt;
    &lt;p&gt;The CDC executes transactions in parallel, which increases throughput considerably, but it is still a bottleneck when it comes to creating, removing, or moving directories. This means that TernFS has a relatively low throughput when it comes to CDC operations.[7]&lt;/p&gt;
    &lt;head rend="h3"&gt;Block services, or file contents&lt;/head&gt;
    &lt;p&gt;In TernFS, files are split into chunks of data called blocks. Blocks are read and written to by block services. A block service is typically a single drive (be it a hard disk or a flash drive) storing blocks. At XTX a typical storage server will contain around 100 hard disks or 25 flash drives — or in TernFS parlance 100 or 25 block services.[8]&lt;/p&gt;
    &lt;p&gt;Read/write access to the block service is provided using a simple TCP API currently implemented by a Go process. This process is hardware agnostic and uses the Go standard library to read and write blocks to a conventional local file system. We originally planned to rewrite the Go process in C++, and possibly write to block devices directly, but the idiomatic Go implementation has proven performant enough for our needs so far.&lt;/p&gt;
    &lt;head rend="h3"&gt;The registry&lt;/head&gt;
    &lt;p&gt;The final piece of the TernFS puzzle is the registry. The registry stores the location of each instance of service (be it a metadata shard, the CDC, or a block storage node). A client only needs to know the address of the registry to mount TernFS — it'll then gather the locations of the other services from it.&lt;/p&gt;
    &lt;p&gt;In TernFS all locations are IPv4 addresses. Working with IPv4 directly simplifies the kernel module considerably, since DNS lookups are quite awkward in the Linux kernel. The exception to this rule is addressing the registry itself, for which DNS is used.&lt;/p&gt;
    &lt;p&gt;The registry also stores additional information, such as the capacity and available size of each drive, who is a follower or a leader in LogsDB clusters, and so on.&lt;/p&gt;
    &lt;p&gt;Predictably, the registry itself is a RocksDB and LogsDB C++ process, given its statefulness.&lt;/p&gt;
    &lt;head rend="h3"&gt;Going global&lt;/head&gt;
    &lt;p&gt;TernFS tries very hard not to lose data, by storing both metadata and file contents on many different drives and servers. However, we also want to be resilient to the temporary or even permanent loss of one entire data centre. Therefore, TernFS can transparently scale across multiple locations.&lt;/p&gt;
    &lt;p&gt;The intended use for TernFS locations is for each location to converge to the same dataset. This means that each location will have to be provisioned with roughly equal resources.[9] Both metadata and file contents replication are asynchronous. In general, we judge the event of losing an entire data centre rare enough to tolerate a time window where data is not fully replicated across locations.&lt;/p&gt;
    &lt;p&gt;Metadata replication is set up so that one location is the metadata primary. Write operations in non-primary locations pay a latency price since they are acknowledged only after they are written to the primary location, replicated, and applied in the originating location. In practice this hasn't been an issue since metadata write latencies are generally overshadowed by writing file contents.&lt;/p&gt;
    &lt;p&gt;There is no automated procedure to migrate off a metadata primary location — again, we deem it a rare enough occurrence to tolerate manual intervention. In the future we plan to move from the current protocol to a multi-master protocol where each location can commit writes independently, which would reduce write latencies on secondary locations and remove the privileged status of the primary location.&lt;/p&gt;
    &lt;p&gt;File contents, unlike metadata, are written locally to the location the client is writing from. Replication to other locations happens in two ways: proactively and on-demand. Proactive replication is performed by tailing the metadata log and replicating new file contents. On-demand replication happens when a client requests file content which has not been replicated yet.&lt;/p&gt;
    &lt;head rend="h2"&gt;Important Details&lt;/head&gt;
    &lt;p&gt;Now that we've laid down the high-level design of TernFS, we can talk about several key implementation details that make TernFS safer, more performant, and more flexible.&lt;/p&gt;
    &lt;head rend="h3"&gt;Talking to TernFS&lt;/head&gt;
    &lt;head rend="h4"&gt;Speaking TernFS' language&lt;/head&gt;
    &lt;p&gt;The most direct way to talk to TernFS is by using its own API. All TernFS messages are defined using a custom serialization format we call bincode. We chose to develop a custom serialization format since we needed it to work within the confines of the Linux kernel and to be easily chopped into UDP packets.&lt;/p&gt;
    &lt;p&gt;We intentionally kept the TernFS API stateless, in the sense that each request executes without regard to previous requests made by the same client. This is in contrast to protocols like NFS, whereby each connection is very stateful, holding resources such as open files, locks, and so on.&lt;/p&gt;
    &lt;p&gt;A stateless API dramatically simplifies the state machines that make up the TernFS core services, therefore simplifying their testing. It also forces each request to be idempotent, or in any case have clear retry semantics, since they might have to be replayed, which facilitates testing further.&lt;/p&gt;
    &lt;p&gt;It also allows the metadata shards and CDC API to be based on UDP rather than TCP, which makes the server and clients (especially the kernel module) simpler, due to doing away with the need for keeping TCP connections. The block service API is TCP based, since it is used to stream large amounts of contiguous data, and any UDP implementation would have to re-implement a reliable stream protocol. The registry API is also TCP-based, given that it is rarely used by clients, and occasionally needs to return large amounts of data.&lt;/p&gt;
    &lt;p&gt;While the TernFS API is simple out-of-the-box, we provide a permissively licensed Go library implementing common tasks that clients might want to perform, such as caching directory policies and retrying requests. This library is used to implement many TernFS processes that are not part of the core TernFS services, such as scrubbing, garbage collection, migrations, and the web UI.&lt;/p&gt;
    &lt;head rend="h4"&gt;Making TernFS POSIX-shaped&lt;/head&gt;
    &lt;p&gt;While the Go library is used for most ancillary tasks, some with high performance requirements, the main way to access TernFS at XTX is through its Linux kernel module.&lt;/p&gt;
    &lt;p&gt;This is because, when migrating our machine learning workflows to TernFS, we needed to support a vast codebase working with files directly. This not only meant that we needed to expose TernFS as a normal filesystem, but also that said normal filesystem API needed to be robust and performant enough for our machine learning needs.[10]&lt;/p&gt;
    &lt;p&gt;For this reason, we opted to work with Linux directly, rather than using FUSE. Working directly with the Linux kernel not only gave us the confidence that we could achieve our performance requirements but also allowed us to bend the POSIX API to our needs, something that would have been more difficult if we had used FUSE.[11]&lt;/p&gt;
    &lt;p&gt;The main obstacle when exposing TernFS as a 'normal' filesystem is that TernFS files are immutable. More specifically, TernFS files are fully written before being 'linked' into the filesystem as a directory entry. This is intentional: it lets us cleanly separate the API for 'under construction' files and 'completed files', and it means that half-written files are not visible.&lt;/p&gt;
    &lt;p&gt;However this design is essentially incompatible with POSIX, which endows the user with near-absolute freedom when it comes to manipulating a file. Therefore, the TernFS kernel module is not POSIX-compliant, but rather exposes enough POSIX to allow many programs to work without modifications, but not all.&lt;/p&gt;
    &lt;p&gt;In practice this means that programs which write files left-to-right and never modify the files' contents will work out-of-the-box. While this might seem very restrictive, we found that a surprising number of programs worked just fine.[12] Programs that did not follow this pattern were modified to first write to a temporary file and then copy the finished file to TernFS.&lt;/p&gt;
    &lt;p&gt;While we feel that writing our own kernel module was the right approach, it proved to be the trickiest part of TernFS, and we would not have been able to implement it without some important safety checks in the TernFS core services.[13]&lt;/p&gt;
    &lt;head rend="h4"&gt;S3 gateway&lt;/head&gt;
    &lt;p&gt;Almost all the storage-related activity at XTX is due to our machine-learning efforts, and for those purposes the TernFS' kernel module has served us well. However, as TernFS proved itself there, we started to look into offering TernFS to the broader firm.&lt;/p&gt;
    &lt;p&gt;Doing so through the kernel module presented multiple challenges. For starters installing a custom kernel module on every machine that needed to reach TernFS is operationally cumbersome. Moreover, while all machine-learning happens in clusters housed in the same data centre as TernFS itself, we wanted to expose TernFS in a way that's more amenable to less local networks, for instance by removing the need for UDP. Finally, TernFS does not have any built-in support for permissions or authentication, which is a requirement in multi-tenant scenarios.&lt;/p&gt;
    &lt;p&gt;To solve all these problems, we implemented a gateway for TernFS, which exposes a TernFS subtree using the S3 API. The gateway is a simple Go process turning S3 calls into TernFS API calls. The S3 gateway is not currently open sourced since it is coupled to authentication services internal to XTX, but we have open sourced a minimal S3 gateway to serve as a starting point for third-party contributors to build their own.&lt;/p&gt;
    &lt;p&gt;We've also planned an NFS gateway to TernFS, but we haven't had a pressing enough need yet to complete it.&lt;/p&gt;
    &lt;head rend="h4"&gt;The web UI and the JSON interface&lt;/head&gt;
    &lt;p&gt;Finally, a view of TernFS is provided by its web UI. The web UI is a stateless Go program which exposes most of the state of TernFS in an easy-to-use interface. This state includes the full filesystem contents (both metadata and file contents), the status of each service including information about decommissioned block services, and so on.&lt;/p&gt;
    &lt;p&gt;Moreover, the web UI also exposes the direct TernFS API in JSON form, which is very useful for small scripts and curl-style automation that does not warrant a full-blown Go program.&lt;/p&gt;
    &lt;head rend="h3"&gt;Directory Policies&lt;/head&gt;
    &lt;p&gt;To implement some of the functionality we'll describe below, TernFS adopts a system of per-directory policies.&lt;/p&gt;
    &lt;p&gt;Policies are used for all sorts of decisions, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How to redundantly store files.&lt;/item&gt;
      &lt;item&gt;On which type of drive to store files.&lt;/item&gt;
      &lt;item&gt;How long to keep files around after deletion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each of the topics above (and a few more we haven't mentioned) correspond to a certain policy tag. The body of the policies are stored in the metadata together with the other directory attributes.&lt;/p&gt;
    &lt;p&gt;Policies are inherited: if a directory does not contain a certain policy tag, it transitively inherits from the parent directory. TernFS clients store a cache of policies to allow for traversal-free policy lookup for most directories.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping blocks in check&lt;/head&gt;
    &lt;p&gt;A filesystem is no good if it loses, leaks, corrupts, or otherwise messes up its data. TernFS deploys a host of measures to minimize the chance of anything going wrong. So far, these have worked: we've never lost data in our production deployment of TernFS. This section focuses on the measures in place to specifically safeguard files' blocks.&lt;/p&gt;
    &lt;head rend="h4"&gt;Against bitrot, or CRC32-C&lt;/head&gt;
    &lt;p&gt;The first and possibly most obvious measure consists of aggressively checksumming all TernFS' data. The metadata is automatically checksummed by RocksDB, and every block is stored in a format interleaving 4KiB pages with 4byte CRC32-C checksums.&lt;/p&gt;
    &lt;p&gt;CRC32-C was picked since it is a high-quality checksum and implemented on most modern silicon.[14] It also exhibits some desirable properties when used together with Reed-Solomon coding.&lt;/p&gt;
    &lt;p&gt;Peter Cawley's fast-crc32 repository provides a general framework to compute CRC32-C quickly, together with state-of-the-art implementations for x86 and aarch64 architectures.&lt;/p&gt;
    &lt;p&gt;4KiB was picked since it is the read boundary used by Linux filesystems and is fine-grained while still being large enough to render the storage overhead of the 4byte checksums negligible.&lt;/p&gt;
    &lt;p&gt;Interleaving the CRCs with the block contents does not add any safety, but it does improve operations in two important ways. First, it allows for safe partial reads: clients can demand only a few pages from a block which is many megabytes in size and still check the reads against its checksum. Second, it allows scrubbing files locally on the server which hosts the blocks, without communicating with other services at all.&lt;/p&gt;
    &lt;head rend="h4"&gt;Storing files redundantly, or Reed-Solomon codes&lt;/head&gt;
    &lt;p&gt;We've been talking about files being split into blocks, but we haven't really explained how files become blocks.&lt;/p&gt;
    &lt;p&gt;The first thing we do to a file is split it into spans. Spans are at most 100MiB and are present just to divide files into sections of a manageable size.&lt;/p&gt;
    &lt;p&gt;Then each span is divided into D data blocks, and P parity blocks. D and P are determined by the corresponding directory policy in which the file is created. When D is 1, the entire contents of the span become a single block, and that block is stored D+P times. This scheme is equivalent to a simple mirroring scheme and allows it to lose up to P blocks before losing file data.&lt;/p&gt;
    &lt;p&gt;While wasteful, mirroring the entire contents of the file can be useful for very hot files, since TernFS clients will pick a block at random to read from, thereby sharing the read load across many block services. And naturally files which we do not care much for can be stored with D = 1 and P = 0, without any redundancy.&lt;/p&gt;
    &lt;p&gt;That said, most files will not be stored using mirroring but rather using Reed-Solomon coding. Other resources can be consulted to understand the high-level idea and the low-level details of Reed-Solomon coding, but the gist is it allows us to split a span into D equally sized blocks (some padding might be necessary), and then generate P blocks of equal size such that up to any P blocks can be lost while retaining the ability to reconstruct all the other blocks.&lt;/p&gt;
    &lt;p&gt;As mentioned, D and P are fully configurable, but at XTX we tend to use D = 10 and P = 4, which allows us to lose up to any four drives for any file.&lt;/p&gt;
    &lt;head rend="h4"&gt;Drive type picking&lt;/head&gt;
    &lt;p&gt;We now know how to split files into a bunch of blocks. The next question is: which drives to pick to store the blocks on. The first decision is which kind of drive to use. At XTX we separate drives into two broad categories for this purpose — flash and spinning disks.&lt;/p&gt;
    &lt;p&gt;When picking between these two, we want to balance two needs: minimizing the cost of hardware by utilizing hard disks if we can [15], and maximizing hard disk productivity by having them reading data most of the time, rather than seeking.&lt;/p&gt;
    &lt;p&gt;To achieve that, directory policies offer a way to tune how large each block will be, and to tune which drives will be picked based on block size. This allows us to configure TernFS so that larger files that can be read sequentially are stored on hard disks, while random-access or small files are stored on flash. [16]&lt;/p&gt;
    &lt;p&gt;Currently this system is not adaptive, but we found that in practice it's easy to carve out sections of the filesystem which are not read sequentially. We have a default configuration which assumes sequential reads and then uses hard disks down to roughly 2.5MB blocks, below which hard disks stop being productive enough and blocks start needing to be written to flash.&lt;/p&gt;
    &lt;head rend="h4"&gt;Block service picking&lt;/head&gt;
    &lt;p&gt;OK, we now know what type of drive to select for our files, but we still have tens of thousands of individual drives to pick from. Picking the 'right' individual drive requires some sophistication.&lt;/p&gt;
    &lt;p&gt;The first thing to note is that drive failures or unavailability are often correlated. For instance, at XTX a single server handles 102 spinning disks. If the server is down, faulty, or needs to be decommissioned, it'll render its 102 disks temporarily or permanently unavailable.&lt;/p&gt;
    &lt;p&gt;It's therefore wise to spread a file's blocks across many servers. To achieve this, each TernFS block service (which generally corresponds to a single drive) has a failure domain. When picking block services in which to store the blocks for a given file, TernFS will make sure that each block is in a separate failure domain. In our TernFS deployment a failure domain corresponds to a server, but other users might wish to tie it to some other factor as appropriate.&lt;/p&gt;
    &lt;p&gt;TernFS also tries hard to avoid write bottlenecks by spreading the current write load across many disks. Moreover, since new drives can be added at any time, it tries to converge to a situation where each drive is roughly equally filled by assigning writing more to drives with more available space.&lt;/p&gt;
    &lt;p&gt;Mechanically this is achieved by having each shard periodically request a set of block services to use for writing from the registry. When handing out block services to shards, the registry selects block services according to several constraints:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It never gives block services from the same failure domain to the same shard&lt;/item&gt;
      &lt;item&gt;It minimizes the variance in how many shards each block service is currently assigned to&lt;/item&gt;
      &lt;item&gt;It prioritizes block services which have more available space.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Then when a client wants to write a new span, requiring D+P blocks, the shard simply selects D+P block services randomly amongst the ones it last received from the registry.&lt;/p&gt;
    &lt;p&gt;One concept currently absent from TernFS is what is often known as 'copyset replication'. When assigning disks to files at random (even with the caveat of failure domains) the probability of rendering at least one file unreadable quickly becomes a certainty as more and more drives fail:&lt;/p&gt;
    &lt;p&gt;Copysets reduce the likelihood of data loss occurring by choosing blocks out of a limited number of sets of drives, as opposed to picking the drives randomly. This dramatically reduces the probability of data loss[17]. They are generally a good idea, but we haven't found them to be worthwhile, for a few reasons.&lt;/p&gt;
    &lt;p&gt;Note that while copysets reduce the failure probability, they do not (and cannot) reduce the expected amount of data loss. That is, instead of a large probability of a relatively small amount of data loss we have a very small probability of a catastrophic loss.&lt;/p&gt;
    &lt;p&gt;First, evacuating a 20TB drive takes just a few minutes, and in the presence of multiple failed drives the migrator process evacuates first the files which are present in multiple failed drives to get ahead of possible data loss. This means that for TernFS to lose data within a single data centre tens of drives would have to fail within a matter of seconds.&lt;/p&gt;
    &lt;p&gt;More importantly, our TernFS deployment is replicated across three data centres. This replication eliminates the chance of losing data due to 'independent' drive failures — thousands of drives would need to fail at once. Obviously, data centre wide events can cause a large proportion of the drives within it to fail, but having such an event in three data centres at once is exceedingly unlikely.&lt;/p&gt;
    &lt;p&gt;Finally, copysets are not without drawbacks or complications. Assigning drives at random is an optimal strategy when it comes to evacuating drives quickly, since the files with blocks in the drives to be evacuated will be evenly spread over the rest of the filesystem, and since we only ever need to replace the failed blocks given that we're not constrained by fitting the new set of blocks in predetermined copysets. This means that the evacuation procedure will not be bottlenecked by drive throughput, which is what enables evacuation to finish in a matter of minutes. Moreover, the algorithm to distribute drives to shards is significantly simpler and more flexible than if it needed to care about copysets.&lt;/p&gt;
    &lt;p&gt;However, users that wish to deploy TernFS within a single data centre might wish to implement some form of copyset replication. Such a change would be entirely contained to the registry and would not change any other component.&lt;/p&gt;
    &lt;head rend="h4"&gt;Block Proofs&lt;/head&gt;
    &lt;p&gt;We now have a solid scheme to store files redundantly (thanks to Reed-Solomon codes) and protect against bitrot (thanks to the checksums). However, said schemes are only as good as their implementation.&lt;/p&gt;
    &lt;p&gt;As previously mentioned, TernFS clients communicate their intention to write a file to metadata servers, the metadata servers select block services that the blocks should be written to, and the clients then write the blocks to block services independently of the metadata services. The same happens when a client wants to erase blocks: the client first communicates its intentions to delete the blocks to the right metadata shard and then performs the erasing itself.&lt;/p&gt;
    &lt;p&gt;This poses a challenge. While verifying the correctness of the core TernFS services is feasible, verifying all clients is not, but we'd still like to prevent buggy clients from breaking key invariants of the filesystem.&lt;/p&gt;
    &lt;p&gt;Buggy clients can wreak havoc in several ways:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;They can leak data by writing blocks to block services that are not referenced anywhere in the metadata.&lt;/item&gt;
      &lt;item&gt;They can lose data by erasing blocks which are still referenced in metadata.&lt;/item&gt;
      &lt;item&gt;They can corrupt data by telling the metadata services they'll write something and then writing something else.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We address all these points by using what we call block proofs. To illustrate how block proofs work, it's helpful to go through the steps required to write new data to a file.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;When a client is creating a file, it'll do so by adding its file spans one-by-one. For each span the client wants to add it sends an 'initiate span creation' request to the right metadata shard. This request contains both the overall checksum of the span, and the checksum of each block in it (including parity blocks).&lt;/item&gt;
      &lt;item&gt;The metadata shard checks the consistency of the checksum of the span and of its blocks, something it can do thanks to some desirable mathematical properties of CRCs.&lt;/item&gt;
      &lt;item&gt;The shard picks block services for the blocks to be written in and returns this information to the client together with a signature for each 'block write' instruction.&lt;/item&gt;
      &lt;item&gt;The client forwards this signature to the block services, which will refuse to write the block without it. Crucially, the cryptographic signature ranges over a unique identity for the block (ensuring we only write the block we mean to write), together with its checksum, ensuring we don't write the wrong data.[18]&lt;/item&gt;
      &lt;item&gt;After committing the block to disk, the block service returns a 'block written' signature to the client.&lt;/item&gt;
      &lt;item&gt;Finally, the client forwards the block written signature back to the shard, which certifies that the span has been written only when it has received the signatures for all the blocks that make up the span. [19]&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This kind of scheme was described more generally in a separate blog post.&lt;/p&gt;
    &lt;p&gt;Similarly, when a client wants to delete a span, it first asks the metadata shard to start doing so. The metadata shard marks the span as 'in deletion' and returns a bunch of 'block erase' signatures to the client. The client then forwards the signatures to the block services that hold the blocks, which delete the blocks, and return a 'block erased' signature. The clients forward these signatures back to the metadata shards, which can then forget about the span entirely.&lt;/p&gt;
    &lt;p&gt;We use AES to generate the signatures for simplicity but note that the goal here is not protecting ourselves from malicious clients — just buggy ones. The keys used for the signature are not kept secret, and CRC32-C is not a secure checksum. That said, we've found this scheme enormously valuable in the presence of complex clients. We spent considerable efforts making the core services very simple so we could then take more implementation risks in the clients, with the knowledge that we would have a very low chance of corrupting the filesystem itself.&lt;/p&gt;
    &lt;head rend="h4"&gt;Scrubbing&lt;/head&gt;
    &lt;p&gt;Finally, if things go wrong, we need to notice. The most common failure mode for a drive is for it to fail entirely, in which case our internal hardware monitoring system will pick it up and migrate from it automatically. The more insidious (and still very common) case is a single sector failing in a drive, which will only be noticed when we try to read the block involving that sector.&lt;/p&gt;
    &lt;p&gt;This is acceptable for files which are read frequently, but some files might be very 'cold' but still very important.&lt;/p&gt;
    &lt;p&gt;Consider the case of raw market data taps which are immediately converted to some processed, lossy format. While we generally will use the file containing the processed data, it's paramount to store the raw market data forever so that if we ever want to include more information from the original market data, we can. So important cold files might go months or even years without anyone reading them, and in the meantime, we might find that enough blocks have been corrupted to render them unreadable.[20]&lt;/p&gt;
    &lt;p&gt;To make sure this does not happen, a process called the scrubber continuously reads every block that TernFS stores, and replaces blocks with bad sectors before they can cause too much damage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Snapshots and garbage collection&lt;/head&gt;
    &lt;p&gt;We've talked at length about what TernFS does to try to prevent data loss due to hardware failure or bugs in clients. However, the most common type of data loss is due to human error — the &lt;code&gt;rm —rf / home/alice/notes.txt&lt;/code&gt; scenario.&lt;/p&gt;
    &lt;p&gt;To protect against these scenarios, TernFS implements a lightweight snapshotting system. When files or directories are deleted, their contents aren't actually deleted. Instead, a weak reference to them is created. We call such weak references snapshot directory entries.&lt;/p&gt;
    &lt;p&gt;Snapshot entries are not be visible through the kernel module or the S3 gateway, but are visible through the direct API, and at XTX we have developed internal tooling to easily recover deleted files through it.[21] Deleted files are also visible through the TernFS web UI.&lt;/p&gt;
    &lt;p&gt;Given that 'normal' file operations do not delete files, but rather make them a snapshot, the task of freeing up space is delegated to an external Go process, the garbage collector. The garbage collector traverses the filesystem and removes expired snapshots, which involves deleting their blocks permanently. Snapshot expiry is predictably regulated by directory policies.&lt;/p&gt;
    &lt;head rend="h3"&gt;Keeping TernFS healthy&lt;/head&gt;
    &lt;p&gt;This last section covers how we (humans of XTX) notice problems in TernFS, and how TernFS self-heals when things go wrong — both key topics if we want to ensure no data loss and notice performance problems early.&lt;/p&gt;
    &lt;head rend="h4"&gt;Performance metrics&lt;/head&gt;
    &lt;p&gt;TernFS exposes a plethora of performance metrics through the HTTP InfluxDB line protocol. While connecting TernFS to a service which ingests these metrics is optional, it is highly recommended for any production service.&lt;/p&gt;
    &lt;p&gt;Moreover, the kernel module exposes many performance metrics itself through DebugFS.&lt;/p&gt;
    &lt;p&gt;Both types of metrics, especially when used in tandem, have proved invaluable to resolve performance problems quickly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Logging and alerts&lt;/head&gt;
    &lt;p&gt;TernFS services log their output to files in a simple line-based format. The internal logging API is extremely simple and includes support for syslog levels out-of-the-box. At XTX we run TernFS as normal systemd services and use journalctl to view logs.&lt;/p&gt;
    &lt;p&gt;As with metrics, the kernel module includes various logging facilities as well. The first type of logging is just through dmesg, but the kernel module also includes numerous tracepoints for low-overhead opt-in logging of many operations.&lt;/p&gt;
    &lt;p&gt;TernFS is also integrated with XTX's internal alerting system, called XMon, to page on call developers when things go wrong. XMon is not open source, but all the alerts are also rendered as error lines in logs. [22] We plan to eventually move to having alerts feed off performance metrics, which would make them independent from XMon, although we don't have plans to do so in the short-term.&lt;/p&gt;
    &lt;head rend="h4"&gt;Migrations&lt;/head&gt;
    &lt;p&gt;Finally, there's the question of what to do when drives die — and they will die, frequently, when you have 50,000 of them. While drives dying is not surprising, we've been surprised at the variety of different drive failures. [23] A malfunctioning drive might:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Produce IO errors when reading specific files. This is probably due to a single bad sector.&lt;/item&gt;
      &lt;item&gt;Produce IO errors when reading or writing anything. This might happen because enough bad sectors have gone bad and the drive cannot remap them, or for a variety of other reasons.&lt;/item&gt;
      &lt;item&gt;Return wrong data. This is usually caught by the built-in error correction codes in the hard drives, but not always.&lt;/item&gt;
      &lt;item&gt;Lie about data being successfully persisted. This can manifest in a variety of ways: file size being wrong on open, file contents being partially zero'd out, and so on.&lt;/item&gt;
      &lt;item&gt;Disappear from the mount list, only to reappear when the machine is rebooted, but missing some data.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When clients fail to read from a drive, they'll automatically fall back on other drives to reconstruct the missing data, which is extremely effective in hiding failures from the end-user. That said, something needs to be done about the bad drives, and done quickly to avoid permanent data loss.&lt;/p&gt;
    &lt;p&gt;The TernFS registry allows marking drives as faulty. Faulty drives are then picked up by the migrator, a Go process which waits for bad drives and then stores all its blocks onto freshly picked block services.&lt;/p&gt;
    &lt;p&gt;TernFS also tries to mark drives as bad automatically using a simple heuristic based on the rate of IO errors the drive is experiencing. The number of drives automatically marked as faulty is throttled to avoid having this check go awry and mark the whole cluster as faulty, which would not be catastrophic but would still be messy to deal with.&lt;/p&gt;
    &lt;p&gt;Moreover, drives that are faulty in subtle ways might not be picked up by the heuristics, which means that occasionally a sysadmin will need to mark a drive as faulty manually, after which the migrator will evacuate them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Closing thoughts&lt;/head&gt;
    &lt;p&gt;At XTX we feel strongly about utilizing our resources efficiently. When it comes to software, this means having software that gets close to some theoretical optimum when it comes to total cost of ownership. This culture was borne out by competing hard for technological excellence when doing on-exchange trading at first, and by our ever-growing hardware costs as our business has grown later.&lt;/p&gt;
    &lt;p&gt;Such idealized tools might not exist or be available yet, in which case we're happy to be the tool makers. TernFS is a perfect example of this and we're excited to open source this component of our business for the community.&lt;/p&gt;
    &lt;p&gt;Crucially, the cost of implementation of a new solution is often overblown compared to the cost of tying yourself to an ill-fitting, expensive third-party solution. Designing and implementing a solution serving exactly your needs allows for much greater simplicity. If the requirements do change, as often happens, changes can be implemented very quickly, again only catering to your needs.&lt;/p&gt;
    &lt;p&gt;That said, we believe that TernFS' set of trade-offs are widely shared across many organizations dealing with large-scale storage workloads, and we hope we'll contribute to at least slowing down the seemingly constant stream of new filesystems.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45290245</guid><pubDate>Thu, 18 Sep 2025 14:36:44 +0000</pubDate></item><item><title>A better future for JavaScript that won't happen</title><link>https://drewdevault.com/2025/09/17/2025-09-17-An-impossible-future-for-JS.html</link><description>&lt;doc fingerprint="ad41d20b9a838791"&gt;
  &lt;main&gt;
    &lt;p&gt;In the wake of the largest supply-chain attack in history, the JavaScript community could have a moment of reckoning and decide: never again. As the panic and shame subsides, after compromised developers finish re-provisioning their workstations and rotating their keys, the ecosystem might re-orient itself towards solving the fundamental flaws that allowed this to happen.&lt;/p&gt;
    &lt;p&gt;After all, people have been sounding the alarm for years that this approach to dependency management is reckless and dangerous and broken by design. Maybe this is the moment when the JavaScript ecosystem begins to understand the importance and urgency of this problem, and begins its course correction. It could leave behind its sprawling dependency trees full of micro-libraries, establish software distribution based on relationships of trust, and incorporate the decades of research and innovation established by more serious dependency management systems.&lt;/p&gt;
    &lt;p&gt;Perhaps Google and Mozilla, leaders in JavaScript standards and implementations, will start developing a real standard library for JavaScript, which makes micro-dependencies like left-pad a thing of the past. This could be combined with a consolidation of efforts, merging micro-libraries into larger packages with a more coherent and holistic scope and purpose, which prune their own dependency trees in turn.&lt;/p&gt;
    &lt;p&gt;This could be the moment where npm comes to terms with its broken design, and with a well-funded effort (recall that, ultimately, npm is GitHub is Microsoft, market cap $3 trillion USD), will develop and roll out the next generation of package management for JavaScript. It could incorporate the practices developed and proven in Linux distributions, which rarely suffer from these sorts of attacks, by de-coupling development from packaging and distribution, establishing package maintainers who assemble and distribute curated collections of software libraries. By introducing universal signatures for packages of executable code, smaller channels and webs of trust, reproducible builds, and the many other straightforward, obvious techniques used by responsible package managers.&lt;/p&gt;
    &lt;p&gt;Maybe other languages that depend on this broken dependency management model, like Cargo, PyPI, RubyGems, and many more, are watching this incident and know that the very same crisis looms in their future. Maybe they will change course, too, before the inevitable.&lt;/p&gt;
    &lt;p&gt;Imagine if other large corporations who depend on and profit from this massive pile of recklessly organized software committed their money and resources to it, through putting their engineers to the task of fixing these problems, through coming together to establish and implement new standards, through direct funding of their dependencies and by distributing money through institutions like NLNet, ushering in an era of responsible, sustainable, and secure software development.&lt;/p&gt;
    &lt;p&gt;This would be a good future, but it’s not the future that lies in wait for us. The future will be more of the same. Expect symbolic gestures – mandatory 2FA will be rolled out in more places, certainly, and the big players will write off meager donations in the name of “OSS security and resilience” in their marketing budgets.&lt;/p&gt;
    &lt;p&gt;No one will learn their lesson. This has been happening for decades and no one has learned anything from it yet. This is the defining hubris of this generation of software development.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45290748</guid><pubDate>Thu, 18 Sep 2025 15:18:28 +0000</pubDate></item><item><title>Tesla is looking to redesign its door handles following trapped-passenger report</title><link>https://www.cnn.com/2025/09/18/business/telsa-door-handle-redesign</link><description>&lt;doc fingerprint="dead98488947f5d1"&gt;
  &lt;main&gt;
    &lt;p&gt;Tesla is looking into redesigning the way to open its car doors in an emergency following several accidents where passengers were reportedly trapped in burning vehicles because rescuers could not open them.&lt;/p&gt;
    &lt;p&gt;Even without an accident, other Tesla owners have reported having to break their own car windows after buckling their children in and then being unable to get in the car again, according to an investigation launched by the National Highway Traffic Safety Administration, citing owner complaints. An investigation by Bloomberg found 140 incidents of people being trapped in their Teslas due to problems with the door handles, including several that resulted in horrific injuries.&lt;/p&gt;
    &lt;p&gt;Tesla design chief Franz von Holzhausen said in a podcast Wednesday that the company is looking to combine the manual and electronic release mechanisms in the doors, which are now separate, in order to make escaping the car easier and quicker in a “panic situation.”&lt;/p&gt;
    &lt;p&gt;“The idea of combining the electronic one and the manual one together into one button, I think, makes a lot of sense,” he said in an interview for Bloomberg’s Hot Pursuit! podcast. “That’s something that we’re working on.”&lt;/p&gt;
    &lt;p&gt;It’s unclear if Tesla is working on a fix for all of its cars or just the new ones, but it’s unlikely the company will be able to combine the two ways of opening the door on existing models.&lt;/p&gt;
    &lt;p&gt;The door handles of the Tesla Model S and Model X extend and retract, while the door handles on the Model 3 and Y mechanically flip out when pressed in. But when that fails, the doors need to be opened with a manual door release inside the vehicle to get out. That can happen if there’s a loss of power following a crash.&lt;/p&gt;
    &lt;p&gt;And the location of the manual release buttons can be difficult to find, especially to a young child alone in a car, or a passenger who is not familiar with the vehicle, like in an emergency situation. The Bloomberg investigation found numerous instances of passengers who were killed or severely burned by post-crash fires when rescuers on the scene were unable to open the doors.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45290865</guid><pubDate>Thu, 18 Sep 2025 15:28:42 +0000</pubDate></item><item><title>Launch HN: Cactus (YC S25) – AI inference on smartphones</title><link>https://github.com/cactus-compute/cactus</link><description>&lt;doc fingerprint="b50a9a40fcb997a8"&gt;
  &lt;main&gt;
    &lt;p&gt;Energy-efficient AI inference framework &amp;amp; kernels for phones &amp;amp; AI-native hardware. Budget and mid-range phones control over 70% of the market, but frameworks today optimise for the highend phones with advanced chips. Cactus is designed bottom-up with no dependencies for all mobile devices.&lt;/p&gt;
    &lt;p&gt;Example (CPU-only):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Model: Qwen3-600m-INT8&lt;/item&gt;
      &lt;item&gt;File size: 370-420mb&lt;/item&gt;
      &lt;item&gt;16-20 t/s on Pixel 6a, Galaxy S21, iPhone 11 Pro&lt;/item&gt;
      &lt;item&gt;50-70 t/s on Pixel 9, Galaxy S25, iPhone 16&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Cactus exposes 4 levels of abstraction.&lt;/p&gt;
    &lt;code&gt;┌─────────────────┐
│   Cactus FFI    │ ←── OpenAI compatible C API for integration  
└─────────────────┘
         │
┌─────────────────┐
│  Cactus Engine  │ ←── High-level transformer engine
└─────────────────┘
         │
┌─────────────────┐  
│  Cactus Graph   │ ←── Unified zero-copy computation graph 
└─────────────────┘
         │
┌─────────────────┐
│ Cactus Kernels  │ ←── Low-level ARM-specific SIMD operations
└─────────────────┘
&lt;/code&gt;
    &lt;p&gt;Cactus Graph is a general numerical computing framework that runs on Cactus Kernels. Great for implementing custom models and scientific computing, like JAX for phones.&lt;/p&gt;
    &lt;code&gt;#include cactus.h

CactusGraph graph;

auto a = graph.input({2, 3}, Precision::FP16);
auto b = graph.input({3, 4}, Precision::INT8);

auto x1 = graph.matmul(a, b, false);
auto x2 = graph.transpose(x1);
auto result = graph.matmul(b, x2, true);

float a_data[6] = {1.1f, 2.3f, 3.4f, 4.2f, 5.7f, 6.8f};
float b_data[12] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12};

graph.set_input(a, a_data, Precision::FP16);
graph.set_input(b, b_data, Precision::INT8);
graph.execute();

void* output_data = graph.get_output(result);
graph.hard_reset(); 
&lt;/code&gt;
    &lt;p&gt;Cactus Engine is an transformer inference engine built on top of Cactus Graphs. It is abstracted via the minimalist Cactus Foreign Function Interface.&lt;/p&gt;
    &lt;code&gt;#include cactus.h

const char* model_path = "path/to/weight/folder";
cactus_model_t model = cactus_init(model_path, 2048);

const char* messages = R"([
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "/nothink My name is Henry Ndubuaku"}
])";

const char* options = R"({
    "temperature": 0.1,
    "top_p": 0.95,
    "top_k": 20,
    "max_tokens": 50,
    "stop_sequences": ["&amp;lt;|im_end|&amp;gt;"]
})";

char response[1024];
int result = cactus_complete(model, messages, response, sizeof(response), options, nullptr, nullptr, nullptr);&lt;/code&gt;
    &lt;p&gt;With tool support:&lt;/p&gt;
    &lt;code&gt;const char* tools = R"([
    {
        "function": {
            "name": "get_weather",
            "description": "Get weather for a location",
            "parameters": {
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City name",
                        "required": true
                    }
                },
                "required": ["location"]
            }
        }
    }
])";

int result = cactus_complete(model, messages, response, sizeof(response), options, tools, nullptr, nullptr);&lt;/code&gt;
    &lt;p&gt;This makes it easy to write Cactus bindings for any language. Header files are self-documenting for each but documentation contributions are welcome.&lt;/p&gt;
    &lt;p&gt;Cactus SDKs run 500k+ weekly inference tasks in production today, try them!&lt;/p&gt;
    &lt;p&gt;You can run these codes directly on Macbooks with Apple chips due to their design. Performance gain is observed in mobile devices but for testing during development, Vanilla M3 CPU-only can run Qwen3-600m-INT8 at 60-70 toks/sec, use the following:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Generate weights from HuggingFace model:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;python3 tools/convert_hf.py Qwen/Qwen3-0.6B weights/qwen3-600m-i8/ --precision INT8&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Build and test:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./tests/run.sh # remember to chmod +x any script first time
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Gemma, SmolVLM, Liquid, Kitten, Vosk etc.&lt;/item&gt;
      &lt;item&gt;SMMLA, NPU &amp;amp; DSP for high-end phones.&lt;/item&gt;
      &lt;item&gt;INT4 support for 1B+ models.&lt;/item&gt;
      &lt;item&gt;Python tools for porting Torch/JAX cactus.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Preliminary results:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Qwen3-4B-INT4 on iPhone 16 Pro NPU = 21 t/s&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;While Cactus can be used for all Apple devices including Macbooks, for computers/AMD/Intel/Nvidia generally, please use HuggingFace, Llama.cpp, Ollama, vLLM, MLX. They're built for those, support x86, and are all great!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45291024</guid><pubDate>Thu, 18 Sep 2025 15:40:29 +0000</pubDate></item><item><title>American Prairie unlocks another 70k acres in Montana</title><link>https://earthhope.substack.com/p/victory-for-public-access-american</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45291132</guid><pubDate>Thu, 18 Sep 2025 15:47:40 +0000</pubDate></item><item><title>New bill aims to block both online adult content and VPNs</title><link>https://www.cnet.com/tech/services-and-software/new-bill-aims-to-block-both-online-adult-content-and-vpns/</link><description>&lt;doc fingerprint="bb4a9b143187946b"&gt;
  &lt;main&gt;
    &lt;p&gt;If you live in Michigan, you might not be able to legally use a VPN soon if a new bill is passed into law. On Sept. 11, Michigan Republican representatives proposed far-reaching legislation banning adult internet content.&lt;/p&gt;
    &lt;p&gt;The bill, called the Anticorruption of Public Morals Act and advanced by six Republican representatives, would ban a wide variety of adult content online, ranging from ASMR and adult manga to AI content and any depiction of transgender people. It also seeks to ban all use of VPNs, foreign or US-produced.&lt;/p&gt;
    &lt;p&gt;Don't miss any of our unbiased tech content and lab-based reviews. Add CNET as a preferred Google source.&lt;/p&gt;
    &lt;p&gt;VPNs, which can simulate internet access originating from outside regions, are often used as workarounds to avoid similar bans that have passed in states like Texas, Louisiana and Mississippi, as well as the UK.&lt;/p&gt;
    &lt;p&gt;But Michigan's bill would charge internet service providers with detecting and blocking VPN use, as well as banning the sale of VPNs in the state. Associated fines would be up to $500,000.&lt;/p&gt;
    &lt;head rend="h2"&gt;What the ban could mean for VPNs&lt;/head&gt;
    &lt;p&gt;Unlike some laws banning access to adult content, this Michigan bill is comprehensive. It applies to all residents of Michigan, adults or children, targets an extensive range of content and includes language that could ban not only VPNs but any method of bypassing internet filters or restrictions.&lt;/p&gt;
    &lt;p&gt;That could spell trouble for VPN owners and other internet users who leverage these tools to improve their privacy, protect their identities online, prevent ISPs from gathering data about them or increase their device safety when browsing on public Wi-Fi.&lt;/p&gt;
    &lt;p&gt;The Anticorruption of Public Morals Act has not passed the Michigan House of Representatives committee nor been voted on by the Michigan Senate, and it's not clear how much support the bill currently has beyond the six Republican representatives who have proposed it. As we've seen with state legislation in the past, sometimes bills like these can serve as templates for other representatives who may want to propose similar laws in their own states.&lt;/p&gt;
    &lt;p&gt;The Michigan ACLU didn't immediately respond to a request for comment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45291169</guid><pubDate>Thu, 18 Sep 2025 15:50:18 +0000</pubDate></item></channel></rss>