<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 27 Dec 2025 13:03:06 +0000</lastBuildDate><item><title>Clearspace (YC W23) Is Hiring a Founding Network Engineer (VPN and Proxy)</title><link>https://www.ycombinator.com/companies/clearspace/jobs/5LtM86I-founding-network-engineer-at-clearspace</link><description>&lt;doc fingerprint="b792f12e5d21eddd"&gt;
  &lt;main&gt;
    &lt;p&gt;Eliminate compulsive phone usage&lt;/p&gt;
    &lt;p&gt;About Clearspace&lt;/p&gt;
    &lt;p&gt;Clearspace is building the intentionality layer of the internet. Our mission is to build technology as effective at protecting human attention as social media is at exploiting it (infinite scrolling, short-form feeds, manipulative notifications, etc). Our category defining mobile app has been featured on Huberman Lab, New York Times Wirecutter, NPR Marketplace, Forbes, TBPN.&lt;/p&gt;
    &lt;p&gt;People that want a better relationship with their devices have nowhere to turn except for willpower. We are building a system allows users to control what their devices see by processing and filtering network traffic against natural language rules.&lt;/p&gt;
    &lt;p&gt;About The Role&lt;/p&gt;
    &lt;p&gt;We’re looking for a networking-obsessed engineer to own the VPN + first-hop policy proxy that powers our AI agent. You don’t need to be an IKEv2 expert, we care more about deep networking intuition, debugging skill, and the desire to go all the way down the stack until the truth reveals itself.&lt;/p&gt;
    &lt;p&gt;You might be a great fit if you’ve built:&lt;/p&gt;
    &lt;p&gt;What You’ll Build&lt;/p&gt;
    &lt;p&gt;Qualifications&lt;/p&gt;
    &lt;p&gt;Nice to Have&lt;/p&gt;
    &lt;p&gt;Compensation&lt;/p&gt;
    &lt;p&gt;At Clearspace we help people reduce compulsive phone usage.&lt;/p&gt;
    &lt;p&gt;We exist to protect people's attention from the exploits of modern technology platforms and make space for the things that matter to them most.&lt;/p&gt;
    &lt;p&gt;We believe the technology to protect someones attention should be just as sophisticated and effective as the tech that is exploiting it and are building a world-class engineering team to arm the world with a comprehensive attention protection stack.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46385600</guid><pubDate>Thu, 25 Dec 2025 17:01:29 +0000</pubDate></item><item><title>Package managers keep using Git as a database, it never works out</title><link>https://nesbitt.io/2025/12/24/package-managers-keep-using-git-as-a-database.html</link><description>&lt;doc fingerprint="b32422190df047d9"&gt;
  &lt;main&gt;
    &lt;p&gt;Using git as a database is a seductive idea. You get version history for free. Pull requests give you a review workflow. It’s distributed by design. GitHub will host it for free. Everyone already knows how to use it.&lt;/p&gt;
    &lt;p&gt;Package managers keep falling for this. And it keeps not working out.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cargo&lt;/head&gt;
    &lt;p&gt;The crates.io index started as a git repository. Every Cargo client cloned it. This worked fine when the registry was small, but the index kept growing. Users would see progress bars like “Resolving deltas: 74.01%, (64415/95919)” hanging for ages, the visible symptom of Cargo’s libgit2 library grinding through delta resolution on a repository with thousands of historic commits.&lt;/p&gt;
    &lt;p&gt;The problem was worst in CI. Stateless environments would download the full index, use a tiny fraction of it, and throw it away. Every build, every time.&lt;/p&gt;
    &lt;p&gt;RFC 2789 introduced a sparse HTTP protocol. Instead of cloning the whole index, Cargo now fetches files directly over HTTPS, downloading only the metadata for dependencies your project actually uses. (This is the “full index replication vs on-demand queries” tradeoff in action.) By April 2025, 99% of crates.io requests came from Cargo versions where sparse is the default. The git index still exists, still growing by thousands of commits per day, but most users never touch it.&lt;/p&gt;
    &lt;head rend="h2"&gt;Homebrew&lt;/head&gt;
    &lt;p&gt;GitHub explicitly asked Homebrew to stop using shallow clones. Updating them was “an extremely expensive operation” due to the tree layout and traffic of homebrew-core and homebrew-cask.&lt;/p&gt;
    &lt;p&gt;Users were downloading 331MB just to unshallow homebrew-core. The .git folder approached 1GB on some machines. Every &lt;code&gt;brew update&lt;/code&gt; meant waiting for git to grind through delta resolution.&lt;/p&gt;
    &lt;p&gt;Homebrew 4.0.0 in February 2023 switched to JSON downloads for tap updates. The reasoning was blunt: “they are expensive to git fetch and git clone and GitHub would rather we didn’t do that… they are slow to git fetch and git clone and this provides a bad experience to end users.”&lt;/p&gt;
    &lt;p&gt;Auto-updates now run every 24 hours instead of every 5 minutes, and they’re much faster because there’s no git fetch involved.&lt;/p&gt;
    &lt;head rend="h2"&gt;CocoaPods&lt;/head&gt;
    &lt;p&gt;CocoaPods is the package manager for iOS and macOS development. It hit the limits hard. The Specs repo grew to hundreds of thousands of podspecs across a deeply nested directory structure. Cloning took minutes. Updating took minutes. CI time vanished into git operations.&lt;/p&gt;
    &lt;p&gt;GitHub imposed CPU rate limits. The culprit was shallow clones, which force GitHub’s servers to compute which objects the client already has. The team tried various band-aids: stopping auto-fetch on &lt;code&gt;pod install&lt;/code&gt;, converting shallow clones to full clones, sharding the repository.&lt;/p&gt;
    &lt;p&gt;The CocoaPods blog captured it well: “Git was invented at a time when ‘slow network’ and ‘no backups’ were legitimate design concerns. Running endless builds as part of continuous integration wasn’t commonplace.”&lt;/p&gt;
    &lt;p&gt;CocoaPods 1.8 gave up on git entirely for most users. A CDN became the default, serving podspec files directly over HTTP. The migration saved users about a gigabyte of disk space and made &lt;code&gt;pod install&lt;/code&gt; nearly instant for new setups.&lt;/p&gt;
    &lt;head rend="h2"&gt;Nixpkgs&lt;/head&gt;
    &lt;p&gt;Nix already solved the client-side problem. The package manager fetches expressions as tarballs via channels, served from S3 and CDN, not git clones. Binary caches serve built packages over HTTP. End users never touch the git repository.&lt;/p&gt;
    &lt;p&gt;But the repository itself is stress-testing GitHub’s infrastructure. In November 2025, GitHub contacted the NixOS team about periodic maintenance jobs failing and causing “issues achieving consensus between replicas.” If unresolved, the repository could have become read-only.&lt;/p&gt;
    &lt;p&gt;The repository totals 83GB with half a million tree objects and 20,000 forks. A local clone is only 2.5GB. The rest is GitHub’s fork network storing every pull request branch and merge commit. The CI queries mergeability daily, creating new merge commits each time.&lt;/p&gt;
    &lt;head rend="h2"&gt;vcpkg&lt;/head&gt;
    &lt;p&gt;vcpkg is Microsoft’s C++ package manager. It uses git tree hashes to version its ports, with the curated registry at github.com/Microsoft/vcpkg containing over 2,000 libraries.&lt;/p&gt;
    &lt;p&gt;The problem is that vcpkg needs to retrieve specific versions of ports by their git tree hash. When you specify a &lt;code&gt;builtin-baseline&lt;/code&gt; in your vcpkg.json (functioning like a lockfile for reproducible builds), vcpkg looks up historical commits to find the exact port versions you need. This only works if you have the full commit history.&lt;/p&gt;
    &lt;p&gt;Shallow clones break everything. GitHub Actions uses shallow clones by default. DevContainers shallow-clone vcpkg to save space. CI systems optimize for fast checkouts. All of these result in the same error: “vcpkg was cloned as a shallow repository… Try again with a full vcpkg clone.”&lt;/p&gt;
    &lt;p&gt;The workarounds are ugly. One proposed solution involves parsing vcpkg.json to extract the baseline hash, deriving the commit date, then fetching with &lt;code&gt;--shallow-since=&amp;lt;date&amp;gt;&lt;/code&gt;. Another suggests including twelve months of history, hoping projects upgrade before their baseline falls off the cliff. For GitHub Actions, you need &lt;code&gt;fetch-depth: 0&lt;/code&gt; in your checkout step, downloading the entire repository history just to resolve dependencies.&lt;/p&gt;
    &lt;p&gt;A vcpkg team member explained the fundamental constraint: “Port versions don’t use commit hashes, we use the git tree hash of the port directory. As far as I know, there is no way to deduce the commit that added a specific tree hash.” An in-product fix is infeasible. The architecture baked in git deeply enough that there’s no escape hatch.&lt;/p&gt;
    &lt;p&gt;Unlike Cargo, Homebrew, and CocoaPods, vcpkg hasn’t announced plans to move away from git registries. Custom registries must still be git repositories. The documentation describes filesystem registries as an alternative, but these require local or mounted paths rather than HTTP access. There’s no CDN, no sparse protocol, no HTTP-based solution on the horizon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Go modules&lt;/head&gt;
    &lt;p&gt;Grab’s engineering team went from 18 minutes for &lt;code&gt;go get&lt;/code&gt; to 12 seconds after deploying a module proxy. That’s not a typo. Eighteen minutes down to twelve seconds.&lt;/p&gt;
    &lt;p&gt;The problem was that &lt;code&gt;go get&lt;/code&gt; needed to fetch each dependency’s source code just to read its go.mod file and resolve transitive dependencies. Cloning entire repositories to get a single file.&lt;/p&gt;
    &lt;p&gt;Go had security concerns too. The original design wanted to remove version control tools entirely because “these fragment the ecosystem: packages developed using Bazaar or Fossil, for example, are effectively unavailable to users who cannot or choose not to install these tools.” Beyond fragmentation, the Go team worried about security bugs in version control systems becoming security bugs in &lt;code&gt;go get&lt;/code&gt;. You’re not just importing code; you’re importing the attack surface of every VCS tool on the developer’s machine.&lt;/p&gt;
    &lt;p&gt;GOPROXY became the default in Go 1.13. The proxy serves source archives and go.mod files independently over HTTP. Go also introduced a checksum database (sumdb) that records cryptographic hashes of module contents. This protects against force pushes silently changing tagged releases, and ensures modules remain available even if the original repository is deleted.&lt;/p&gt;
    &lt;head rend="h2"&gt;Beyond package managers&lt;/head&gt;
    &lt;p&gt;The same pattern shows up wherever developers try to use git as a database.&lt;/p&gt;
    &lt;p&gt;Git-based wikis like Gollum (used by GitHub and GitLab) become “somewhat too slow to be usable” at scale. Browsing directory structure takes seconds per click. Loading pages takes longer. GitLab plans to move away from Gollum entirely.&lt;/p&gt;
    &lt;p&gt;Git-based CMS platforms like Decap hit GitHub’s API rate limits. A Decap project on GitHub scales to about 10,000 entries if you have a lot of collection relations. A new user with an empty cache makes a request per entry to populate it, burning through the 5,000 request limit quickly. If your site has lots of content or updates frequently, use a database instead.&lt;/p&gt;
    &lt;p&gt;Even GitOps tools that embrace git as a source of truth have to work around its limitations. ArgoCD’s repo server can run out of disk space cloning repositories. A single commit invalidates the cache for all applications in that repo. Large monorepos need special scaling considerations.&lt;/p&gt;
    &lt;head rend="h2"&gt;The pattern&lt;/head&gt;
    &lt;p&gt;The hosting problems are symptoms. The underlying issue is that git inherits filesystem limitations, and filesystems make terrible databases.&lt;/p&gt;
    &lt;p&gt;Directory limits. Directories with too many files become slow. CocoaPods had 16,000 pod directories in a single Specs folder, requiring huge tree objects and expensive computation. Their fix was hash-based sharding: split directories by the first few characters of a hashed name, so no single directory has too many entries. Git itself does this internally with its objects folder, splitting into 256 subdirectories. You’re reinventing B-trees, badly.&lt;/p&gt;
    &lt;p&gt;Case sensitivity. Git is case-sensitive, but macOS and Windows filesystems typically aren’t. Check out a repo containing both &lt;code&gt;File.txt&lt;/code&gt; and &lt;code&gt;file.txt&lt;/code&gt; on Windows, and the second overwrites the first. Azure DevOps had to add server-side enforcement to block pushes with case-conflicting paths.&lt;/p&gt;
    &lt;p&gt;Path length limits. Windows restricts paths to 260 characters, a constraint dating back to DOS. Git supports longer paths, but Git for Windows inherits the OS limitation. This is painful with deeply nested node_modules directories, where &lt;code&gt;git status&lt;/code&gt; fails with “Filename too long” errors.&lt;/p&gt;
    &lt;p&gt;Missing database features. Databases have CHECK constraints and UNIQUE constraints; git has nothing, so every package manager builds its own validation layer. Databases have locking; git doesn’t. Databases have indexes for queries like “all packages depending on X”; with git you either traverse every file or build your own index. Databases have migrations for schema changes; git has “rewrite history and force everyone to re-clone.”&lt;/p&gt;
    &lt;p&gt;The progression is predictable. Start with a flat directory of files. Hit filesystem limits. Implement sharding. Hit cross-platform issues. Build server-side enforcement. Build custom indexes. Eventually give up and use HTTP or an actual database. You’ve built a worse version of what databases already provide, spread across git hooks, CI pipelines, and bespoke tooling.&lt;/p&gt;
    &lt;p&gt;None of this means git is bad. Git excels at what it was designed for: distributed collaboration on source code, with branching, merging, and offline work. The problem is using it for something else entirely. Package registries need fast point queries for metadata. Git gives you a full-document sync protocol when you need a key-value lookup.&lt;/p&gt;
    &lt;p&gt;If you’re building a package manager and git-as-index seems appealing, look at Cargo, Homebrew, CocoaPods, vcpkg, Go. They all had to build workarounds as they grew, causing pain for users and maintainers. The pull request workflow is nice. The version history is nice. You will hit the same walls they did.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46391514</guid><pubDate>Fri, 26 Dec 2025 12:46:36 +0000</pubDate></item><item><title>LearnixOS</title><link>https://www.learnix-os.com</link><description>&lt;doc fingerprint="7fe43b9d48af5e9f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Learnix Operating System&lt;/head&gt;
    &lt;p&gt;"If you can't explain it simply, you don't understand it well enough." - Albert Einstein&lt;/p&gt;
    &lt;p&gt;Hello there!1&lt;/p&gt;
    &lt;p&gt;In this book we are going to write and learn about operating systems together!&lt;/p&gt;
    &lt;p&gt;We are going to implement an entire POSIX compliant OS in Rust and not use ANY2 external libraries. All of the thought process, code and implementations will be explained and documented here as well as in this repo which all the code snippets are from.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: ALL the syntax highlighting of the Rust code is custom and create by me! If you see and bug, please write in the comments or submit an issue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Base Knowledge&lt;/head&gt;
    &lt;p&gt;This book will be technical, and will assume a little bit of a programming knowledge background, but not necessarily in rust&lt;/p&gt;
    &lt;p&gt;If you are not coming from a low level programming knowledge that's fine!&lt;/p&gt;
    &lt;p&gt;Just make sure you know this stuff or learn it as you read. Also if in any place on this book I take some things for granted, please, open an issue here and let me know so I could explain it better.&lt;/p&gt;
    &lt;p&gt;Some of the base knowledge that you would need to have:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Some assembly knowledge. (just understand simple movs, and arithmetic operations, at a very basic level3)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Some knowledge on memory. (what's a pointer, what's an address)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A knowledge in rust is not that important, but knowing at least one programming language is. I myself have some more learning in Rust, and in this book I will also explain some great features that it has!&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A lot of motivation to learn and understand because it is a complex subject.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Roadmap of this book&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Compiling a stand alone binary&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Boot loading, Debugging, stages and some legacy stuff&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Important cpu modes and instructions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Paging, writing our own malloc&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Utilizing the Interrupt Descriptor Table&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;File systems and Disk Drivers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Thinking in terms of processes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Writing a shell&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Running our first program! (Which off course will be Doom)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;To be continued (Hopefully virtualization section and loading a vm of other OS)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46391599</guid><pubDate>Fri, 26 Dec 2025 12:59:56 +0000</pubDate></item><item><title>Show HN: Xcc700: Self-hosting mini C compiler for ESP32 (Xtensa) in 700 lines</title><link>https://github.com/valdanylchuk/xcc700</link><description>&lt;doc fingerprint="3702985821161ae9"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A compiler you can fully grasp and tweak, on a modern platform where small is still cool.&lt;/item&gt;
      &lt;item&gt;Basic features, not too entrenched, easy to morph into your language of choice.&lt;/item&gt;
      &lt;item&gt;Reusable ELF writer, and a basic Xtensa bytecodes emitter.&lt;/item&gt;
      &lt;item&gt;Possibly useful for hotfixes, CI, quick test/debug turnaround on esp32.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./xcc700 xcc700.c -o xcc700.elf 

[ xcc700 ] BUILD COMPLETED &amp;gt; OK
&amp;gt; IN  : 700 Lines / 7977 Tokens
&amp;gt; SYM : 69 Funcs / 91 Globals
&amp;gt; REL : 152 Literals / 1027 Patches
&amp;gt; MEM : 1041 B .rodata / 17120 B .bss
&amp;gt; OUT : 27735 B .text / 33300 B ELF
[ 40 ms ] &amp;gt;&amp;gt; 17500 Lines/sec &amp;lt;&amp;lt;
&lt;/code&gt;
    &lt;p&gt;Note: that timing is from esp32-s3. Timings on Mac/POSIX will be reported 1000x slower than they are, as on esp32 ticks are millisecond, and on POSIX microsecond, but there is no adjustment here.&lt;/p&gt;
    &lt;head class="px-3 py-2"&gt;xcc700_demo10s.mov&lt;/head&gt;
    &lt;p&gt;.&lt;/p&gt;
    &lt;p&gt;Several options:&lt;/p&gt;
    &lt;p&gt;A. Compile with &lt;code&gt;gcc xcc700.c&lt;/code&gt; and run it on your computer as a cross-compiler. It is fairly portable, tested on Mac x86_64 and arm64.&lt;/p&gt;
    &lt;p&gt;B. Compile for esp32 using xtensa-gcc or xcc700 from the option A (yes it can compile and cross-compile itself). Or grab the gcc-compiled version here: xcc700.elf (16kB). Run with ESP-IDF elf_loader.&lt;/p&gt;
    &lt;p&gt;C. Adapt the source code and call it as a function in your firmware.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;C features: minimum required to write something like this compiler. While loop, if/then/else, limited support for int/char/pointers/arrays, function calls and definitions, basic arithmetic and bitwise operators.&lt;/item&gt;
      &lt;item&gt;Single source .c file as input, single REL ELF file as output.&lt;/item&gt;
      &lt;item&gt;The output files can be run directly by the ESP-IDF elf_loader component, which links them on load via relocation table to anything you have exposed in your firmware: newlib libc, LVGL, your custom functions, anything you like. Just declare the functions you use.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The rest of the C: for/do, include/define, long/float/double, struct/union/typedef, switch/case, array initializers, .data section, multi-line comments, too much to list.&lt;/item&gt;
      &lt;item&gt;Many features are implemented only partially. E.g. you can have .bss globals but not global initializers; ++/-- are only supported in prefix position, assignment as statement not expression, types are mostly not checked, etc.&lt;/item&gt;
      &lt;item&gt;Error handling and reporting. It is wildly optimistic, enforces nothing, has only a few error checks, and will crash in spectacular and unexpected ways on the most trivial errors.&lt;/item&gt;
      &lt;item&gt;Optimization. It treats the Xtensa CPU as a stack machine, with no attempt at register allocation, and no benefit from the sliding window. It is a major sacrifice of performance for simplicity. GCC-compiled: 16kB, 17,500 lines/s; self-compiled: 33kB, 3,900 lines/s.&lt;/item&gt;
      &lt;item&gt;Miss a feature? Just fork it! With a working foundation in only 700 lines, it is fairly easy to get started.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is free software under MIT License, see LICENSE.&lt;/p&gt;
    &lt;p&gt;While I do not believe the world needs another C99 implementation, and do not intend to add features here, I am dead curious to see where the other creative minds can take a tiny self-hosting compiler on esp32.&lt;/p&gt;
    &lt;p&gt;If you organize hackathons, or assign coursework, or write tutorials, please consider xcc700 as a base to fork and extend! It can run on the available PCs, or on a $5 MCU if you want real cool hardware for the final test. Or you can port it to other systems, and use ld to link those ELF files.&lt;/p&gt;
    &lt;p&gt;I was making an esp32 "cyberdeck", and thought it cool to build some binaries directly on it. Esp32 is underrated in userland. It can do everything a 90s PC could do and more.&lt;/p&gt;
    &lt;p&gt;You can also take this as an artistic statement, and ask yourself:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How many Watts do you need to do fun/useful stuff on a computer?&lt;/item&gt;
      &lt;item&gt;Isn't it nice to have simple, tinker-friendly versions of common apps?&lt;/item&gt;
      &lt;item&gt;Do we really need 300MB mouse drivers?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Have fun!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46392736</guid><pubDate>Fri, 26 Dec 2025 15:07:01 +0000</pubDate></item><item><title>Show HN: Witr – Explain why a process is running on your Linux system</title><link>https://github.com/pranshuparmar/witr</link><description>&lt;doc fingerprint="471a943f3dafc432"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1. Purpose&lt;/item&gt;
      &lt;item&gt;2. Goals&lt;/item&gt;
      &lt;item&gt;3. Core Concept&lt;/item&gt;
      &lt;item&gt;4. Supported Targets&lt;/item&gt;
      &lt;item&gt;5. Output Behavior&lt;/item&gt;
      &lt;item&gt;6. Flags &amp;amp; Options&lt;/item&gt;
      &lt;item&gt;7. Example Outputs&lt;/item&gt;
      &lt;item&gt;8. Installation&lt;/item&gt;
      &lt;item&gt;9. Platform Support&lt;/item&gt;
      &lt;item&gt;10. Success Criteria&lt;/item&gt;
      &lt;item&gt;11. AI Assistance Disclaimer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;witr exists to answer a single question:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Why is this running?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;When something is running on a system—whether it is a process, a service, or something bound to a port—there is always a cause. That cause is often indirect, non-obvious, or spread across multiple layers such as supervisors, containers, services, or shells.&lt;/p&gt;
    &lt;p&gt;Existing tools (&lt;code&gt;ps&lt;/code&gt;, &lt;code&gt;top&lt;/code&gt;, &lt;code&gt;lsof&lt;/code&gt;, &lt;code&gt;ss&lt;/code&gt;, &lt;code&gt;systemctl&lt;/code&gt;, &lt;code&gt;docker ps&lt;/code&gt;) expose state and metadata. They show what is running, but leave the user to infer why by manually correlating outputs across tools.&lt;/p&gt;
    &lt;p&gt;witr makes that causality explicit.&lt;/p&gt;
    &lt;p&gt;It explains where a running thing came from, how it was started, and what chain of systems is responsible for it existing right now, in a single, human-readable output.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Explain why a process exists, not just that it exists&lt;/item&gt;
      &lt;item&gt;Reduce time‑to‑understanding during debugging and outages&lt;/item&gt;
      &lt;item&gt;Work with zero configuration&lt;/item&gt;
      &lt;item&gt;Be safe, read‑only, and non‑destructive&lt;/item&gt;
      &lt;item&gt;Prefer clarity over completeness&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Not a monitoring tool&lt;/item&gt;
      &lt;item&gt;Not a performance profiler&lt;/item&gt;
      &lt;item&gt;Not a replacement for systemd/docker tooling&lt;/item&gt;
      &lt;item&gt;Not a remediation or auto‑fix tool&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;witr treats everything as a process question.&lt;/p&gt;
    &lt;p&gt;Ports, services, containers, and commands all eventually map to PIDs. Once a PID is identified, witr builds a causal chain explaining why that PID exists.&lt;/p&gt;
    &lt;p&gt;At its core, witr answers:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;What is running?&lt;/item&gt;
      &lt;item&gt;How did it start?&lt;/item&gt;
      &lt;item&gt;What is keeping it running?&lt;/item&gt;
      &lt;item&gt;What context does it belong to?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;witr supports multiple entry points that converge to PID analysis.&lt;/p&gt;
    &lt;code&gt;witr node
witr nginx&lt;/code&gt;
    &lt;p&gt;A single positional argument (without flags) is treated as a process or service name. If multiple matches are found, witr will prompt for disambiguation by PID.&lt;/p&gt;
    &lt;code&gt;witr --pid 14233&lt;/code&gt;
    &lt;p&gt;Explains why a specific process exists.&lt;/p&gt;
    &lt;code&gt;witr --port 5000&lt;/code&gt;
    &lt;p&gt;Explains the process(es) listening on a port.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Single screen by default (best effort)&lt;/item&gt;
      &lt;item&gt;Deterministic ordering&lt;/item&gt;
      &lt;item&gt;Narrative-style explanation&lt;/item&gt;
      &lt;item&gt;Best-effort detection with explicit uncertainty&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What the user asked about.&lt;/p&gt;
    &lt;p&gt;Executable, PID, user, command, start time and restart count.&lt;/p&gt;
    &lt;p&gt;A causal ancestry chain showing how the process came to exist. This is the core value of witr.&lt;/p&gt;
    &lt;p&gt;The primary system responsible for starting or supervising the process (best effort).&lt;/p&gt;
    &lt;p&gt;Examples:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;systemd unit&lt;/item&gt;
      &lt;item&gt;docker container&lt;/item&gt;
      &lt;item&gt;pm2&lt;/item&gt;
      &lt;item&gt;cron&lt;/item&gt;
      &lt;item&gt;interactive shell&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Only one primary source is selected.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Working directory&lt;/item&gt;
      &lt;item&gt;Git repository name and branch&lt;/item&gt;
      &lt;item&gt;Docker container name / image&lt;/item&gt;
      &lt;item&gt;Public vs private bind&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Non‑blocking observations such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Process is running as root&lt;/item&gt;
      &lt;item&gt;Process is listening on a public interface (0.0.0.0 / ::)&lt;/item&gt;
      &lt;item&gt;Restarted multiple times (warning only if above threshold)&lt;/item&gt;
      &lt;item&gt;Process is using high memory (&amp;gt;1GB RSS)&lt;/item&gt;
      &lt;item&gt;Process has been running for over 90 days&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;--pid &amp;lt;n&amp;gt;         Explain a specific PID
--port &amp;lt;n&amp;gt;        Explain port usage
--short           One-line summary
--tree            Show full process ancestry tree
--json            Output result as JSON
--warnings        Show only warnings
--no-color        Disable colorized output
--env             Show only environment variables for the process
--help            Show this help message
&lt;/code&gt;
    &lt;p&gt;A single positional argument (without flags) is treated as a process or service name.&lt;/p&gt;
    &lt;code&gt;witr node&lt;/code&gt;
    &lt;code&gt;Target      : node

Process     : node (pid 14233)
User        : pm2
Command     : node index.js
Started     : 2 days ago (Mon 2025-02-02 11:42:10 +05:30)
Restarts    : 1

Why It Exists :
  systemd (pid 1) → pm2 (pid 5034) → node (pid 14233)

Source      : pm2

Working Dir : /opt/apps/expense-manager
Git Repo    : expense-manager (main)
Listening   : 127.0.0.1:5001
&lt;/code&gt;
    &lt;code&gt;witr --port 5000 --short&lt;/code&gt;
    &lt;code&gt;systemd (pid 1) → PM2 v5.3.1: God (pid 1481580) → python (pid 1482060)
&lt;/code&gt;
    &lt;code&gt;witr --pid 1482060 --tree&lt;/code&gt;
    &lt;code&gt;systemd (pid 1)
  └─ PM2 v5.3.1: God (pid 1481580)
    └─ python (pid 1482060)
&lt;/code&gt;
    &lt;code&gt;witr node&lt;/code&gt;
    &lt;code&gt;Multiple matching processes found:

[1] PID 12091  node server.js  (docker)
[2] PID 14233  node index.js   (pm2)
[3] PID 18801  node worker.js  (manual)

Re-run with:
  witr --pid &amp;lt;pid&amp;gt;
&lt;/code&gt;
    &lt;code&gt;witr nginx&lt;/code&gt;
    &lt;code&gt;Ambiguous target: "nginx"

The name matches multiple entities:

[1] PID 2311   nginx: master process   (service)
[2] PID 24891  nginx: worker process   (manual)

witr cannot determine intent safely.
Please re-run with an explicit PID:
  witr --pid &amp;lt;pid&amp;gt;
&lt;/code&gt;
    &lt;p&gt;witr is distributed as a single static Linux binary.&lt;/p&gt;
    &lt;p&gt;The easiest way to install witr is via the install script.&lt;/p&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/pranshuparmar/witr/main/install.sh | bash&lt;/code&gt;
    &lt;code&gt;curl -fsSL https://raw.githubusercontent.com/pranshuparmar/witr/main/install.sh -o install.sh
cat install.sh
chmod +x install.sh
./install.sh&lt;/code&gt;
    &lt;p&gt;The script will:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Detect your CPU architecture (&lt;code&gt;amd64&lt;/code&gt;or&lt;code&gt;arm64&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;Download the latest released binary and man page&lt;/item&gt;
      &lt;item&gt;Install it to &lt;code&gt;/usr/local/bin/witr&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Install the man page to &lt;code&gt;/usr/local/share/man/man1/witr.1&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You may be prompted for your password to write to system directories.&lt;/p&gt;
    &lt;p&gt;If you prefer manual installation, follow these simple steps for your architecture:&lt;/p&gt;
    &lt;code&gt;# Download the binary
curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/witr-linux-amd64 -o witr-linux-amd64

# Verify checksum (Optional, should print OK)
curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/SHA256SUMS -o SHA256SUMS
grep witr-linux-amd64 SHA256SUMS | sha256sum -c -

# Rename and install
mv witr-linux-amd64 witr &amp;amp;&amp;amp; chmod +x witr
sudo mv witr /usr/local/bin/witr

# Install the man page (Optional)
sudo curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/witr.1 -o /usr/local/share/man/man1/witr.1
sudo mandb &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 || true&lt;/code&gt;
    &lt;code&gt;# Download the binary
curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/witr-linux-arm64 -o witr-linux-arm64

# Verify checksum (Optional, should print OK)
curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/SHA256SUMS -o SHA256SUMS
grep witr-linux-arm64 SHA256SUMS | sha256sum -c -

# Rename and install
mv witr-linux-arm64 witr &amp;amp;&amp;amp; chmod +x witr
sudo mv witr /usr/local/bin/witr

# Install the man page (Optional)
sudo curl -fsSL https://github.com/pranshuparmar/witr/releases/latest/download/witr.1 -o /usr/local/share/man/man1/witr.1
sudo mandb &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 || true&lt;/code&gt;
    &lt;p&gt;Explanation:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download only the binary for your architecture and the SHA256SUMS file.&lt;/item&gt;
      &lt;item&gt;Verify the checksum for your binary only (prints OK if valid).&lt;/item&gt;
      &lt;item&gt;Rename to witr, make it executable, and move to your PATH.&lt;/item&gt;
      &lt;item&gt;Install man page.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;witr --version
man witr&lt;/code&gt;
    &lt;p&gt;To completely remove witr:&lt;/p&gt;
    &lt;code&gt;sudo rm -f /usr/local/bin/witr
sudo rm -f /usr/local/share/man/man1/witr.1&lt;/code&gt;
    &lt;p&gt;If you use Nix, you can build witr from source and run without installation:&lt;/p&gt;
    &lt;code&gt;nix run github:pranshuparmar/witr -- --port 5000&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Linux&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;witr inspects &lt;code&gt;/proc&lt;/code&gt; and may require elevated permissions to explain certain processes.&lt;/p&gt;
    &lt;p&gt;If you are not seeing the expected information (e.g., missing process ancestry, user, working directory or environment details), try running witr with sudo for elevated permissions:&lt;/p&gt;
    &lt;code&gt;sudo witr [your arguments]&lt;/code&gt;
    &lt;p&gt;witr is successful if:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A user can answer "why is this running?" within seconds&lt;/item&gt;
      &lt;item&gt;It reduces reliance on multiple tools&lt;/item&gt;
      &lt;item&gt;Output is understandable under stress&lt;/item&gt;
      &lt;item&gt;Users trust it during incidents&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This project was developed with assistance from AI/LLMs (including GitHub Copilot, ChatGPT, and related tools), supervised by a human who occasionally knew what he was doing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46392910</guid><pubDate>Fri, 26 Dec 2025 15:20:36 +0000</pubDate></item><item><title>Experts explore new mushroom which causes fairytale-like hallucinations</title><link>https://nhmu.utah.edu/articles/experts-explore-new-mushroom-which-causes-fairytale-hallucinations</link><description>&lt;doc fingerprint="a778f61f8d52f750"&gt;
  &lt;main&gt;
    &lt;p&gt;uv installs packages faster than pip by an order of magnitude. The usual explanation is “it’s written in Rust.” That’s true, but it doesn’t explain much. Plenty of tools are written in Rust without being notably fast. The interesting question is what design decisions made the difference.&lt;/p&gt;
    &lt;p&gt;Charlie Marsh’s Jane Street talk and a Xebia engineering deep-dive cover the technical details well. The interesting parts are the design decisions: standards that enable fast paths, things uv drops that pip supports, and optimizations that don’t require Rust at all.&lt;/p&gt;
    &lt;head rend="h2"&gt;The standards that made uv possible&lt;/head&gt;
    &lt;p&gt;pip’s slowness isn’t a failure of implementation. For years, Python packaging required executing code to find out what a package needed.&lt;/p&gt;
    &lt;p&gt;The problem was setup.py. You couldn’t know a package’s dependencies without running its setup script. But you couldn’t run its setup script without installing its build dependencies. PEP 518 in 2016 called this out explicitly: “You can’t execute a setup.py file without knowing its dependencies, but currently there is no standard way to know what those dependencies are in an automated fashion without executing the setup.py file.”&lt;/p&gt;
    &lt;p&gt;This chicken-and-egg problem forced pip to download packages, execute untrusted code, fail, install missing build tools, and try again. Every install was potentially a cascade of subprocess spawns and arbitrary code execution. Installing a source distribution was essentially &lt;code&gt;curl | bash&lt;/code&gt; with extra steps.&lt;/p&gt;
    &lt;p&gt;The fix came in stages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PEP 518 (2016) created pyproject.toml, giving packages a place to declare build dependencies without code execution. The TOML format was borrowed from Rust’s Cargo, which makes a Rust tool returning to fix Python packaging feel less like coincidence.&lt;/item&gt;
      &lt;item&gt;PEP 517 (2017) separated build frontends from backends, so pip didn’t need to understand setuptools internals.&lt;/item&gt;
      &lt;item&gt;PEP 621 (2020) standardized the &lt;code&gt;[project]&lt;/code&gt;table, so dependencies could be read by parsing TOML rather than running Python.&lt;/item&gt;
      &lt;item&gt;PEP 658 (2022) put package metadata directly in the Simple Repository API, so resolvers could fetch dependency information without downloading wheels at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PEP 658 went live on PyPI in May 2023. uv launched in February 2024. uv could be fast because the ecosystem finally had the infrastructure to support it. A tool like uv couldn’t have shipped in 2020. The standards weren’t there yet.&lt;/p&gt;
    &lt;p&gt;Other ecosystems figured this out earlier. Cargo has had static metadata from the start. npm’s package.json is declarative. Python’s packaging standards finally bring it to parity.&lt;/p&gt;
    &lt;head rend="h2"&gt;What uv drops&lt;/head&gt;
    &lt;p&gt;Speed comes from elimination. Every code path you don’t have is a code path you don’t wait for.&lt;/p&gt;
    &lt;p&gt;uv’s compatibility documentation is a list of things it doesn’t do:&lt;/p&gt;
    &lt;p&gt;No .egg support. Eggs were the pre-wheel binary format. pip still handles them; uv doesn’t even try. The format has been obsolete for over a decade.&lt;/p&gt;
    &lt;p&gt;No pip.conf. uv ignores pip’s configuration files entirely. No parsing, no environment variable lookups, no inheritance from system-wide and per-user locations.&lt;/p&gt;
    &lt;p&gt;No bytecode compilation by default. pip compiles .py files to .pyc during installation. uv skips this step, shaving time off every install. You can opt in if you want it.&lt;/p&gt;
    &lt;p&gt;Virtual environments required. pip lets you install into system Python by default. uv inverts this, refusing to touch system Python without explicit flags. This removes a whole category of permission checks and safety code.&lt;/p&gt;
    &lt;p&gt;Stricter spec enforcement. pip accepts malformed packages that technically violate packaging specs. uv rejects them. Less tolerance means less fallback logic.&lt;/p&gt;
    &lt;p&gt;Ignoring requires-python upper bounds. When a package says it requires &lt;code&gt;python&amp;lt;4.0&lt;/code&gt;, uv ignores the upper bound and only checks the lower. This reduces resolver backtracking dramatically since upper bounds are almost always wrong. Packages declare &lt;code&gt;python&amp;lt;4.0&lt;/code&gt; because they haven’t tested on Python 4, not because they’ll actually break. The constraint is defensive, not predictive.&lt;/p&gt;
    &lt;p&gt;First-index wins by default. When multiple package indexes are configured, pip checks all of them. uv picks from the first index that has the package, stopping there. This prevents dependency confusion attacks and avoids extra network requests.&lt;/p&gt;
    &lt;p&gt;Each of these is a code path pip has to execute and uv doesn’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimizations that don’t need Rust&lt;/head&gt;
    &lt;p&gt;Some of uv’s speed comes from Rust. But not as much as you’d think. Several key optimizations could be implemented in pip today:&lt;/p&gt;
    &lt;p&gt;HTTP range requests for metadata. Wheel files are zip archives, and zip archives put their file listing at the end. uv tries PEP 658 metadata first, falls back to HTTP range requests for the zip central directory, then full wheel download, then building from source. Each step is slower and riskier. The design makes the fast path cover 99% of cases. None of this requires Rust.&lt;/p&gt;
    &lt;p&gt;Parallel downloads. pip downloads packages one at a time. uv downloads many at once. Any language can do this.&lt;/p&gt;
    &lt;p&gt;Global cache with hardlinks. pip copies packages into each virtual environment. uv keeps one copy globally and uses hardlinks (or copy-on-write on filesystems that support it). Installing the same package into ten venvs takes the same disk space as one. Any language with filesystem access can do this.&lt;/p&gt;
    &lt;p&gt;Python-free resolution. pip needs Python running to do anything, and invokes build backends as subprocesses to get metadata from legacy packages. uv parses TOML and wheel metadata natively, only spawning Python when it hits a setup.py-only package that has no other option.&lt;/p&gt;
    &lt;p&gt;PubGrub resolver. uv uses the PubGrub algorithm, originally from Dart’s pub package manager. Both pip and PubGrub use backtracking, but PubGrub applies conflict-driven clause learning from SAT solvers: when it hits a dead end, it analyzes why and skips similar dead ends later. This makes it faster on complex dependency graphs and better at explaining failures. pip could adopt PubGrub without rewriting in Rust.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Rust actually matters&lt;/head&gt;
    &lt;p&gt;Some optimizations do require Rust:&lt;/p&gt;
    &lt;p&gt;Zero-copy deserialization. uv uses rkyv to deserialize cached data without copying it. The data format is the in-memory format. Libraries like FlatBuffers achieve this in other languages, but rkyv integrates tightly with Rust’s type system.1&lt;/p&gt;
    &lt;p&gt;Thread-level parallelism. Python’s GIL forces parallel work into separate processes, with IPC overhead and data copying. Rust can parallelize across threads natively, sharing memory without serialization boundaries. This matters most for resolution, where the solver explores many version combinations.1&lt;/p&gt;
    &lt;p&gt;No interpreter startup. Every time pip spawns a subprocess, it pays Python’s startup cost. uv is a single static binary with no runtime to initialize.&lt;/p&gt;
    &lt;p&gt;Compact version representation. uv packs versions into u64 integers where possible, making comparison and hashing fast. Over 90% of versions fit in one u64. This is micro-optimization that compounds across millions of comparisons.&lt;/p&gt;
    &lt;p&gt;These are real advantages. But they’re smaller than the architectural wins from dropping legacy support and exploiting modern standards.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design over language&lt;/head&gt;
    &lt;p&gt;uv is fast because of what it doesn’t do, not because of what language it’s written in. The standards work of PEP 518, 517, 621, and 658 made fast package management possible. Dropping eggs, pip.conf, and permissive parsing made it achievable. Rust makes it a bit faster still.&lt;/p&gt;
    &lt;p&gt;pip could implement parallel downloads, global caching, and metadata-only resolution tomorrow. It doesn’t, largely because backwards compatibility with fifteen years of edge cases takes precedence. But it means pip will always be slower than a tool that starts fresh with modern assumptions.&lt;/p&gt;
    &lt;p&gt;Other package managers could learn from this: static metadata, no code execution to discover dependencies, and the ability to resolve everything upfront before downloading. Cargo and npm have operated this way for years. If your ecosystem requires running arbitrary code to find out what a package needs, you’ve already lost.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46393936</guid><pubDate>Fri, 26 Dec 2025 17:07:53 +0000</pubDate></item><item><title>How uv got so fast</title><link>https://nesbitt.io/2025/12/26/how-uv-got-so-fast.html</link><description>&lt;doc fingerprint="a778f61f8d52f750"&gt;
  &lt;main&gt;
    &lt;p&gt;uv installs packages faster than pip by an order of magnitude. The usual explanation is “it’s written in Rust.” That’s true, but it doesn’t explain much. Plenty of tools are written in Rust without being notably fast. The interesting question is what design decisions made the difference.&lt;/p&gt;
    &lt;p&gt;Charlie Marsh’s Jane Street talk and a Xebia engineering deep-dive cover the technical details well. The interesting parts are the design decisions: standards that enable fast paths, things uv drops that pip supports, and optimizations that don’t require Rust at all.&lt;/p&gt;
    &lt;head rend="h2"&gt;The standards that made uv possible&lt;/head&gt;
    &lt;p&gt;pip’s slowness isn’t a failure of implementation. For years, Python packaging required executing code to find out what a package needed.&lt;/p&gt;
    &lt;p&gt;The problem was setup.py. You couldn’t know a package’s dependencies without running its setup script. But you couldn’t run its setup script without installing its build dependencies. PEP 518 in 2016 called this out explicitly: “You can’t execute a setup.py file without knowing its dependencies, but currently there is no standard way to know what those dependencies are in an automated fashion without executing the setup.py file.”&lt;/p&gt;
    &lt;p&gt;This chicken-and-egg problem forced pip to download packages, execute untrusted code, fail, install missing build tools, and try again. Every install was potentially a cascade of subprocess spawns and arbitrary code execution. Installing a source distribution was essentially &lt;code&gt;curl | bash&lt;/code&gt; with extra steps.&lt;/p&gt;
    &lt;p&gt;The fix came in stages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PEP 518 (2016) created pyproject.toml, giving packages a place to declare build dependencies without code execution. The TOML format was borrowed from Rust’s Cargo, which makes a Rust tool returning to fix Python packaging feel less like coincidence.&lt;/item&gt;
      &lt;item&gt;PEP 517 (2017) separated build frontends from backends, so pip didn’t need to understand setuptools internals.&lt;/item&gt;
      &lt;item&gt;PEP 621 (2020) standardized the &lt;code&gt;[project]&lt;/code&gt;table, so dependencies could be read by parsing TOML rather than running Python.&lt;/item&gt;
      &lt;item&gt;PEP 658 (2022) put package metadata directly in the Simple Repository API, so resolvers could fetch dependency information without downloading wheels at all.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;PEP 658 went live on PyPI in May 2023. uv launched in February 2024. uv could be fast because the ecosystem finally had the infrastructure to support it. A tool like uv couldn’t have shipped in 2020. The standards weren’t there yet.&lt;/p&gt;
    &lt;p&gt;Other ecosystems figured this out earlier. Cargo has had static metadata from the start. npm’s package.json is declarative. Python’s packaging standards finally bring it to parity.&lt;/p&gt;
    &lt;head rend="h2"&gt;What uv drops&lt;/head&gt;
    &lt;p&gt;Speed comes from elimination. Every code path you don’t have is a code path you don’t wait for.&lt;/p&gt;
    &lt;p&gt;uv’s compatibility documentation is a list of things it doesn’t do:&lt;/p&gt;
    &lt;p&gt;No .egg support. Eggs were the pre-wheel binary format. pip still handles them; uv doesn’t even try. The format has been obsolete for over a decade.&lt;/p&gt;
    &lt;p&gt;No pip.conf. uv ignores pip’s configuration files entirely. No parsing, no environment variable lookups, no inheritance from system-wide and per-user locations.&lt;/p&gt;
    &lt;p&gt;No bytecode compilation by default. pip compiles .py files to .pyc during installation. uv skips this step, shaving time off every install. You can opt in if you want it.&lt;/p&gt;
    &lt;p&gt;Virtual environments required. pip lets you install into system Python by default. uv inverts this, refusing to touch system Python without explicit flags. This removes a whole category of permission checks and safety code.&lt;/p&gt;
    &lt;p&gt;Stricter spec enforcement. pip accepts malformed packages that technically violate packaging specs. uv rejects them. Less tolerance means less fallback logic.&lt;/p&gt;
    &lt;p&gt;Ignoring requires-python upper bounds. When a package says it requires &lt;code&gt;python&amp;lt;4.0&lt;/code&gt;, uv ignores the upper bound and only checks the lower. This reduces resolver backtracking dramatically since upper bounds are almost always wrong. Packages declare &lt;code&gt;python&amp;lt;4.0&lt;/code&gt; because they haven’t tested on Python 4, not because they’ll actually break. The constraint is defensive, not predictive.&lt;/p&gt;
    &lt;p&gt;First-index wins by default. When multiple package indexes are configured, pip checks all of them. uv picks from the first index that has the package, stopping there. This prevents dependency confusion attacks and avoids extra network requests.&lt;/p&gt;
    &lt;p&gt;Each of these is a code path pip has to execute and uv doesn’t.&lt;/p&gt;
    &lt;head rend="h2"&gt;Optimizations that don’t need Rust&lt;/head&gt;
    &lt;p&gt;Some of uv’s speed comes from Rust. But not as much as you’d think. Several key optimizations could be implemented in pip today:&lt;/p&gt;
    &lt;p&gt;HTTP range requests for metadata. Wheel files are zip archives, and zip archives put their file listing at the end. uv tries PEP 658 metadata first, falls back to HTTP range requests for the zip central directory, then full wheel download, then building from source. Each step is slower and riskier. The design makes the fast path cover 99% of cases. None of this requires Rust.&lt;/p&gt;
    &lt;p&gt;Parallel downloads. pip downloads packages one at a time. uv downloads many at once. Any language can do this.&lt;/p&gt;
    &lt;p&gt;Global cache with hardlinks. pip copies packages into each virtual environment. uv keeps one copy globally and uses hardlinks (or copy-on-write on filesystems that support it). Installing the same package into ten venvs takes the same disk space as one. Any language with filesystem access can do this.&lt;/p&gt;
    &lt;p&gt;Python-free resolution. pip needs Python running to do anything, and invokes build backends as subprocesses to get metadata from legacy packages. uv parses TOML and wheel metadata natively, only spawning Python when it hits a setup.py-only package that has no other option.&lt;/p&gt;
    &lt;p&gt;PubGrub resolver. uv uses the PubGrub algorithm, originally from Dart’s pub package manager. Both pip and PubGrub use backtracking, but PubGrub applies conflict-driven clause learning from SAT solvers: when it hits a dead end, it analyzes why and skips similar dead ends later. This makes it faster on complex dependency graphs and better at explaining failures. pip could adopt PubGrub without rewriting in Rust.&lt;/p&gt;
    &lt;head rend="h2"&gt;Where Rust actually matters&lt;/head&gt;
    &lt;p&gt;Some optimizations do require Rust:&lt;/p&gt;
    &lt;p&gt;Zero-copy deserialization. uv uses rkyv to deserialize cached data without copying it. The data format is the in-memory format. Libraries like FlatBuffers achieve this in other languages, but rkyv integrates tightly with Rust’s type system.1&lt;/p&gt;
    &lt;p&gt;Thread-level parallelism. Python’s GIL forces parallel work into separate processes, with IPC overhead and data copying. Rust can parallelize across threads natively, sharing memory without serialization boundaries. This matters most for resolution, where the solver explores many version combinations.1&lt;/p&gt;
    &lt;p&gt;No interpreter startup. Every time pip spawns a subprocess, it pays Python’s startup cost. uv is a single static binary with no runtime to initialize.&lt;/p&gt;
    &lt;p&gt;Compact version representation. uv packs versions into u64 integers where possible, making comparison and hashing fast. Over 90% of versions fit in one u64. This is micro-optimization that compounds across millions of comparisons.&lt;/p&gt;
    &lt;p&gt;These are real advantages. But they’re smaller than the architectural wins from dropping legacy support and exploiting modern standards.&lt;/p&gt;
    &lt;head rend="h2"&gt;Design over language&lt;/head&gt;
    &lt;p&gt;uv is fast because of what it doesn’t do, not because of what language it’s written in. The standards work of PEP 518, 517, 621, and 658 made fast package management possible. Dropping eggs, pip.conf, and permissive parsing made it achievable. Rust makes it a bit faster still.&lt;/p&gt;
    &lt;p&gt;pip could implement parallel downloads, global caching, and metadata-only resolution tomorrow. It doesn’t, largely because backwards compatibility with fifteen years of edge cases takes precedence. But it means pip will always be slower than a tool that starts fresh with modern assumptions.&lt;/p&gt;
    &lt;p&gt;Other package managers could learn from this: static metadata, no code execution to discover dependencies, and the ability to resolve everything upfront before downloading. Cargo and npm have operated this way for years. If your ecosystem requires running arbitrary code to find out what a package needs, you’ve already lost.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46393992</guid><pubDate>Fri, 26 Dec 2025 17:13:07 +0000</pubDate></item><item><title>How Lewis Carroll computed determinants (2023)</title><link>https://www.johndcook.com/blog/2023/07/10/lewis-carroll-determinants/</link><description>&lt;doc fingerprint="358c12c671a59fa2"&gt;
  &lt;main&gt;
    &lt;p&gt;Charles Dodgson, better known by his pen name Lewis Carroll, discovered a method of calculating determinants now known variously as the method of contractants, Dodgson condensation, or simply condensation.&lt;/p&gt;
    &lt;p&gt;The method was devised for ease of computation by hand, but it has features that make it a practical method for computation by machine.&lt;/p&gt;
    &lt;head rend="h2"&gt;Overview&lt;/head&gt;
    &lt;p&gt;The basic idea is to repeatedly condense a matrix, replacing it by a matrix with one less row and one less column. Each element is replaced by the determinant of the 2×2 matrix formed by that element and its neighbors to the south, east, and southeast. The bottom row and rightmost column have no such neighbors and are removed. There is one additional part of the algorithm that will be easier to describe after introducing some notation.&lt;/p&gt;
    &lt;head rend="h2"&gt;Details&lt;/head&gt;
    &lt;p&gt;Let A be the matrix whose determinant we want to compute and let A(k) be the matrix obtained after k steps of the condensation algorithm.&lt;/p&gt;
    &lt;p&gt;The matrix A(1) is computed as described in the overview:&lt;/p&gt;
    &lt;p&gt;Starting with A(2) the terms are similar, except each 2×2 determinant is divided by an element from two steps back:&lt;/p&gt;
    &lt;p&gt;Dodgson’s original paper from 1867 is quite readable, surprisingly so given that math notation and terminology changes over time.&lt;/p&gt;
    &lt;p&gt;One criticism I have of the paper is that it is hard to understand which element should be in the denominator, whether the subscripts should be i and j or i+1 and j+1. His first example doesn’t clarify this because these elements happen to be equal in the example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Example&lt;/head&gt;
    &lt;p&gt;Here’s an example using condensation to find the determinant of a 4×4 matrix.&lt;/p&gt;
    &lt;p&gt;We can verify this with Mathematica:&lt;/p&gt;
    &lt;quote&gt;Det[{{3, 1, 4, 1}, {5, 9, 2, 6}, {0, 7, 1, 0}, {2, 0, 2, 3}}]&lt;/quote&gt;
    &lt;p&gt;which also produces 228.&lt;/p&gt;
    &lt;head rend="h2"&gt;Division&lt;/head&gt;
    &lt;p&gt;The algorithm above involves a division and so we should avoid dividing by zero. Dodgson says to&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Arrange the given block, if necessary, so that no ciphers [zeros] occur in its interior. This may be done either by transposing rows or columns, or by adding to certain rows the several terms of other rows multiplied by certain multipliers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;He expands on this remark and gives examples. I’m not sure whether this preparation is necessary only to avoid division by zero, but it does avoid the problem of dividing by a zero.&lt;/p&gt;
    &lt;p&gt;If the original matrix has all integer entries, then the division in Dodgson’s condensation algorithm is exact. The sequence of matrices produced by the algorithm will all have integer entries.&lt;/p&gt;
    &lt;head rend="h2"&gt;Efficiency&lt;/head&gt;
    &lt;p&gt;Students usually learn cofactor expansion as their first method of calculating determinants. This rule is easy to explain, but inefficient since the number of steps required is O(n!).&lt;/p&gt;
    &lt;p&gt;The more efficient way to compute determinants is by Gaussian elimination with partial pivoting. As with condensation, one must avoid dividing by zero, hence the partial pivoting.&lt;/p&gt;
    &lt;p&gt;Gaussian elimination takes O(n³) operations, and so does Dodgson’s condensation algorithm. Condensation is easy to teach and easy to carry out by hand, but unlike cofactor expansion it scales well.&lt;/p&gt;
    &lt;p&gt;If a matrix has all integer entries, Gaussian elimination can produce non-integer values in intermediate steps. Condensation does not. Also, condensation is inherently parallelizable: each of the 2 × 2 determinants can be calculated simultaneously.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46395106</guid><pubDate>Fri, 26 Dec 2025 19:03:32 +0000</pubDate></item><item><title>T-Ruby is Ruby with syntax for types</title><link>https://type-ruby.github.io/</link><description>&lt;doc fingerprint="bc38bb01e975fe15"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Example&lt;/head&gt;
    &lt;p&gt;Write typed Ruby, compile to standard Ruby with generated type signatures.&lt;/p&gt;
    &lt;code&gt;# hello.trb&lt;/code&gt;
    &lt;code&gt;# hello.rb&lt;/code&gt;
    &lt;code&gt;# hello.rbs&lt;/code&gt;
    &lt;head rend="h2"&gt;Existing Methods&lt;/head&gt;
    &lt;p&gt;Compare with existing Ruby typing solutions and see why T-Ruby is different.&lt;/p&gt;
    &lt;head rend="h3"&gt;How it works&lt;/head&gt;
    &lt;p&gt;A static type checker for Ruby developed by Stripe. Uses sig blocks to declare types on methods.&lt;/p&gt;
    &lt;code&gt;# typed: strict&lt;/code&gt;
    &lt;head rend="h3"&gt;Limitations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires runtime dependency (sorbet-runtime gem)&lt;/item&gt;
      &lt;item&gt;Types must be written separately in sig blocks, like comments above function code.&lt;/item&gt;
      &lt;item&gt;Requires learning sig block's unique DSL syntax.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;T-Ruby Approach&lt;/head&gt;
    &lt;p&gt;T-Ruby uses inline types like TypeScript without runtime dependencies, and generates standard RBS files.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick Start&lt;/head&gt;
    &lt;head rend="h3"&gt;Initialize project&lt;/head&gt;
    &lt;code&gt;gem install t-ruby&lt;/code&gt;
    &lt;head rend="h3"&gt;Start watch mode&lt;/head&gt;
    &lt;code&gt;trc --watch&lt;/code&gt;
    &lt;head rend="h3"&gt;Write typed Ruby&lt;/head&gt;
    &lt;code&gt;def greet(name: String): String&lt;/code&gt;
    &lt;head rend="h2"&gt;Works with your tools&lt;/head&gt;
    &lt;p&gt;T-Ruby integrates seamlessly with the Ruby ecosystem.&lt;/p&gt;
    &lt;head rend="h3"&gt;Editors&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VS Code Extension&lt;/item&gt;
      &lt;item&gt;JetBrains Plugin&lt;/item&gt;
      &lt;item&gt;Neovim Plugin&lt;/item&gt;
      &lt;item&gt;Language Server (LSP)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Type Checkers&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Steep&lt;/item&gt;
      &lt;item&gt;Ruby LSP&lt;/item&gt;
      &lt;item&gt;Sorbet (via RBS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Ruby Ecosystem&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RBS Compatible&lt;/item&gt;
      &lt;item&gt;Any Ruby version&lt;/item&gt;
      &lt;item&gt;All gems work&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Join the Journey&lt;/head&gt;
    &lt;p&gt;T-Ruby is an open source project. Your contribution makes a difference.&lt;lb/&gt;It's still experimental. The core compiler works, but there's much to improve.&lt;lb/&gt;Feedback and suggestions are always welcome!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46395871</guid><pubDate>Fri, 26 Dec 2025 20:27:50 +0000</pubDate></item><item><title>Toys with the highest play-time and lowest clean-up-time</title><link>https://joannabregan.substack.com/p/toys-with-the-highest-play-time-and</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46395885</guid><pubDate>Fri, 26 Dec 2025 20:28:53 +0000</pubDate></item><item><title>Always bet on text (2014)</title><link>https://graydon2.dreamwidth.org/193447.html</link><description>&lt;doc fingerprint="2d54993729ec488d"&gt;
  &lt;main&gt;&lt;head rend="h3"&gt;always bet on text&lt;/head&gt;Oct. 13th, 2014 12:34 pm&lt;p&gt; graydon2&lt;/p&gt;&lt;p&gt;I figured I should just post this somewhere so I can make future reference to how I feel about the matter, anytime someone asks me about such-and-such video, 3D, game or "dynamic" multimedia system. Don't get me wrong, I like me some illustrations, photos, movies and music.&lt;lb/&gt;But text wins by a mile. Text is everything. My thoughts on this are quite absolute: text is the most powerful, useful, effective communication technology ever, period.&lt;lb/&gt;Text is the oldest and most stable communication technology (assuming we treat speech/signing as natural phenomenon -- there are no human societies without it -- whereas textual capability has to be transmitted, taught, acquired) and it's incredibly durable. We can read texts from five thousand years ago, almost the moment they started being produced. It's (literally) "rock solid" -- you can readily inscribe it in granite that will likely outlast the human species.&lt;lb/&gt;Text is the most flexible communication technology. Pictures may be worth a thousand words, when there's a picture to match what you're trying to say. But let's hit the random button on wikipedia and pick a sentence, see if you can draw a picture to convey it, mm? Here:&lt;lb/&gt;Not a chance. Text can convey ideas with a precisely controlled level of ambiguity and precision, implied context and elaborated content, unmatched by anything else. It is not a coincidence that all of literature and poetry, history and philosophy, mathematics, logic, programming and engineering rely on textual encodings for their ideas.&lt;lb/&gt;Text is the most efficient communication technology. By orders of magnitude. This blog post is likely to take perhaps 5000 bytes of storage, and could compress down to maybe 2000; by comparison the following 20-pixel-square image of the silhouette of a tweeting bird takes 4000 bytes: . At every step of communication technology, textual encoding comes first, everything else after. Because it's vastly cheaper on a symbol-by-symbol basis. You have a working optical telegraph network running in 1790 in France. You the better part of a century of electrical telegraphy, trans-oceanic cables and everything, before anyone bothers with trying to carry voice. You have decades of teleprinter and text-only computer networking, mail and news, chat and publishing, editing and diagnostics, before bandwidth gets cheap enough for images, voice and video. You have pagers, SMS, WAP, USSD and blackberries before iPhones. You have Teletext and BBSs, netnews and gopher before the web. And today many of the best, and certainly the most efficient parts of the web remain text-centric. I can download all of wikipedia and carry it around on the average smartphone.&lt;lb/&gt;Text is the most socially useful communication technology. It works well in 1:1, 1:N, and M:N modes. It can be indexed and searched efficiently, even by hand. It can be translated. It can be produced and consumed at variable speeds. It is asynchronous. It can be compared, diffed, clustered, corrected, summarized and filtered algorithmically. It permits multiparty editing. It permits branching conversations, lurking, annotation, quoting, reviewing, summarizing, structured responses, exegesis, even fan fic. The breadth, scale and depth of ways people use text is unmatched by anything. There is no equivalent in any other communication technology for the social, communicative, cognitive and reflective complexity of a library full of books or an internet full of postings. Nothing else comes close.&lt;lb/&gt;So this is my stance on text: always pick text first. As my old boss might have said: always bet on text. If you can use text for something, use it. It will very seldom let you down.&lt;/p&gt;&lt;p&gt;But text wins by a mile. Text is everything. My thoughts on this are quite absolute: text is the most powerful, useful, effective communication technology ever, period.&lt;/p&gt;&lt;p&gt;Text is the oldest and most stable communication technology (assuming we treat speech/signing as natural phenomenon -- there are no human societies without it -- whereas textual capability has to be transmitted, taught, acquired) and it's incredibly durable. We can read texts from five thousand years ago, almost the moment they started being produced. It's (literally) "rock solid" -- you can readily inscribe it in granite that will likely outlast the human species.&lt;/p&gt;&lt;p&gt;Text is the most flexible communication technology. Pictures may be worth a thousand words, when there's a picture to match what you're trying to say. But let's hit the random button on wikipedia and pick a sentence, see if you can draw a picture to convey it, mm? Here:&lt;/p&gt;&lt;quote&gt;"Human rights are moral principles or norms that describe certain standards of human behaviour, and are regularly protected as legal rights in national and international law."&lt;/quote&gt;&lt;p&gt;Not a chance. Text can convey ideas with a precisely controlled level of ambiguity and precision, implied context and elaborated content, unmatched by anything else. It is not a coincidence that all of literature and poetry, history and philosophy, mathematics, logic, programming and engineering rely on textual encodings for their ideas.&lt;/p&gt;&lt;p&gt;Text is the most efficient communication technology. By orders of magnitude. This blog post is likely to take perhaps 5000 bytes of storage, and could compress down to maybe 2000; by comparison the following 20-pixel-square image of the silhouette of a tweeting bird takes 4000 bytes: . At every step of communication technology, textual encoding comes first, everything else after. Because it's vastly cheaper on a symbol-by-symbol basis. You have a working optical telegraph network running in 1790 in France. You the better part of a century of electrical telegraphy, trans-oceanic cables and everything, before anyone bothers with trying to carry voice. You have decades of teleprinter and text-only computer networking, mail and news, chat and publishing, editing and diagnostics, before bandwidth gets cheap enough for images, voice and video. You have pagers, SMS, WAP, USSD and blackberries before iPhones. You have Teletext and BBSs, netnews and gopher before the web. And today many of the best, and certainly the most efficient parts of the web remain text-centric. I can download all of wikipedia and carry it around on the average smartphone.&lt;/p&gt;&lt;p&gt;Text is the most socially useful communication technology. It works well in 1:1, 1:N, and M:N modes. It can be indexed and searched efficiently, even by hand. It can be translated. It can be produced and consumed at variable speeds. It is asynchronous. It can be compared, diffed, clustered, corrected, summarized and filtered algorithmically. It permits multiparty editing. It permits branching conversations, lurking, annotation, quoting, reviewing, summarizing, structured responses, exegesis, even fan fic. The breadth, scale and depth of ways people use text is unmatched by anything. There is no equivalent in any other communication technology for the social, communicative, cognitive and reflective complexity of a library full of books or an internet full of postings. Nothing else comes close.&lt;/p&gt;&lt;p&gt;So this is my stance on text: always pick text first. As my old boss might have said: always bet on text. If you can use text for something, use it. It will very seldom let you down.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46397379</guid><pubDate>Fri, 26 Dec 2025 23:09:40 +0000</pubDate></item><item><title>Exe.dev</title><link>https://exe.dev/</link><description>&lt;doc fingerprint="5649e739e2d32d86"&gt;
  &lt;main&gt;
    &lt;p&gt;Login ssh exe.dev _ The disk persists. You have sudo.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46397609</guid><pubDate>Fri, 26 Dec 2025 23:42:46 +0000</pubDate></item><item><title>Publishing your work increases your luck</title><link>https://github.com/readme/guides/publishing-your-work</link><description>&lt;doc fingerprint="284a195f3fe6261d"&gt;
  &lt;main&gt;
    &lt;p&gt;No matter how hard you work, it still takes a little bit of luck for something to hit. That can be discouraging, since luck feels like a force outside our control. But the good news is that we can increase our chances of encountering good luck. That may sound like magic, but it’s not supernatural. The trick is to increase the number of opportunities we have for good fortune to find us. The simple act of publishing your work is one of the best ways to invite a little more luck into your life.&lt;/p&gt;
    &lt;p&gt;Before we get into the “how,” it’s important to get on the same page about the “what.” What are we talking about when we say “luck?” There are a lot of definitions that could apply, but let’s stick with a simple one: Luck is when something unexpected and good happens to you. Unexpected and good. Who doesn’t want to increase the odds of something unexpected and good?&lt;/p&gt;
    &lt;p&gt;In our world, luck can include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Having your OSS library take off&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Being invited to speak at a conference&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Landing a new job&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Getting a new consulting client&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Being invited onto a podcast&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Making new friends in your community&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;None of these things are totally in your control, which can at times feel frustrating.&lt;/p&gt;
    &lt;p&gt;How can we increase the odds of finding luck? By being a person who works in public. By doing work and being public about it, you build a reputation for yourself. You build a track record. You build a public body of work that speaks on your behalf better than any resume ever could.&lt;/p&gt;
    &lt;p&gt;The goal is not to become famous, the goal is to increase the chances of luck finding us. For me, one of the most helpful ways to think about this has always been the concept of the “Luck Surface Area,” described in an old post by Jason Roberts. He wrote (and note, the emphasis is mine):&lt;/p&gt;
    &lt;p&gt;"The amount of serendipity that will occur in your life, your Luck Surface Area, is directly proportional to the degree to which you do something you’re passionate about combined with the total number of people to whom this is effectively communicated."&lt;/p&gt;
    &lt;p&gt;Going further, he codifies it into a formula where:&lt;/p&gt;
    &lt;code&gt;Luck = [Doing Things] * [Telling People]&lt;/code&gt;
    &lt;p&gt;The more things you do multiplied by the more people you tell, the larger your Luck Surface Area becomes. The larger your Luck Surface Area, the more likely you are to catch luck as it flows by.&lt;/p&gt;
    &lt;head rend="h3"&gt;Source: Jason Roberts&lt;/head&gt;
    &lt;head rend="h2"&gt;Doing the work&lt;/head&gt;
    &lt;p&gt;Before you can publish your work, you have to actually do the work. The good news for you is that by even reading this Guide on The ReadME Project, you’ve probably already self-selected into a group of people for whom “doing things” comes somewhat naturally. You’re a developer, a designer, a creator, an author, or something else entirely. Whatever moniker you want to give yourself, you’re built to do things, and that’s the important part.&lt;/p&gt;
    &lt;p&gt;If that doesn’t ring true for you, you may fall into one of two groups:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You actually are doing things, you’ve just trained yourself to think that anything you do isn’t worth sharing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You want to be doing things, but you can’t bring yourself to get started.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you’re in the first group, you may need to step back and reframe the work you’re already doing. This is a common blind spot for people who are executing at a high level! They’ve forgotten just how much they know. They think that they’re not doing anything interesting because they assume that everyone knows as much as they do. This effect is only exacerbated when everyone in your immediate vicinity is at a similar—or higher—skill level. As you become more of an expert, your quality bar gets higher and higher and you forget that everything you know is not known by everyone.&lt;/p&gt;
    &lt;p&gt;If you’re in this group I want to give you a challenge: Watch the communities where you hang out and see what people are sharing and what gets noticed. Is it something you could have done? Is it something you’ve already done? At its worst this could lead you in the direction of becoming bitter, critical, and thinking that you’re smarter than everyone. To that I say “resist!” There is no life there. My encouragement to you is to view that as objective evidence that people want to know all of the things that you already know! There is a huge opportunity for you, should you decide to start sharing your work.&lt;/p&gt;
    &lt;p&gt;If you’re in the second group, you just need to start. Start anywhere, start on anything, start something. You’ll never come up with the perfect idea for an OSS library, a business, a podcast, or an article by just thinking about it. Start on something, today. It won’t be the perfect version of the thing you have in your head, but you’ll be in motion. Motion begets motion, progress begets progress. Pick the smallest thing you can do and get started.&lt;/p&gt;
    &lt;p&gt;Doing the work is the most important part. It’s the nucleus around which everything else revolves. What that “work” looks like, though, is entirely up to you! That’s the fun part. It can take any form and be in any domain. Wherever your curiosity or expertise draw you, dive into that.&lt;/p&gt;
    &lt;p&gt;Projects outside of work are a good place to dive into your curiosity.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to make a thermal receipt printer that prints GitHub issues, you should.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to turn a prefabricated shed into an office, go for it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to go all in on an SVG drawing tool, do it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to write tens of thousands of words about the infrastructure of modern money, that’s a newsletter.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Your curiosity will naturally pull you in certain directions, so don’t be afraid to go super deep into a topic that you’re interested in. When a person is truly interested in the thing they’re writing or talking about, their excitement is contagious. Whatever you’re excited about, be excited about it publicly. Whatever you’re curious about, be curious about it publicly. People will want to follow along and you’ll inspire people along the way.&lt;/p&gt;
    &lt;p&gt;Projects at work can be a good place to dive into your expertise.&lt;/p&gt;
    &lt;p&gt;It's likely you're constantly solving problems and learning interesting things at your job. This is a great opportunity to take what you’re already doing and repurpose it for the benefit of others. You can turn those learnings into blog posts, conference talks, meetups, podcasts, or open source projects.&lt;/p&gt;
    &lt;p&gt;Of course not everything you do at work is shareable. If the specifics aren’t shareable, the concepts, lessons, and takeaways likely are. While you’re working, keep a scratch pad open and jot down any problems you come across, interesting patterns you see, or things you found confusing. Do this for a month and you’ll have more things to share than you know what to do with!&lt;/p&gt;
    &lt;p&gt;You’ve done the work, now it's time to tell people.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hitting the publish button&lt;/head&gt;
    &lt;p&gt;This part of the formula can be harder for most of us. Most of us really enjoy the building aspect but start to get a little shy when it comes to telling people about the stuff we’ve built. That could be for any number of reasons: fear, embarrassment, self-preservation, or an aversion to being perceived as hawking your wares.&lt;/p&gt;
    &lt;p&gt;It’s a valuable exercise to investigate whether or not you resonate with any of those reasons. Are you afraid people are going to make fun of what you built? Are you embarrassed that it isn’t up to your own (admittedly high) standards? Are you waiting for some elusive perfect moment? Do you have an aversion to “marketing” and don’t want to become the thing you hate? Whatever it is for you, I encourage you to really dig into it and see if that fear is worth keeping around.&lt;/p&gt;
    &lt;p&gt;Sharing things you’re learning or making is not prideful. People are drawn to other people in motion. People want to follow along, people want to learn things, people want to be a part of your journey. It’s not bragging to say, “I’ve made a thing and I think it’s cool!” Bringing people along is a good thing for everyone. By publishing your work you’re helping people learn. You’re inspiring others to create.&lt;/p&gt;
    &lt;p&gt;You can “publish” anywhere. For me that’s mostly Twitter because that’s where most of my peers hang out. It doesn’t have to be Twitter for you. It could be GitHub, a newsletter, a podcast, forums, your blog, YouTube, or something completely different that’s not even on my radar. Anywhere that’s not your hard drive counts!&lt;/p&gt;
    &lt;p&gt;Publishing is a skill, it’s something you can learn. You’ll need to build your publishing skill just like you built every other skill you have.&lt;/p&gt;
    &lt;p&gt;Don’t be afraid to publish along the way. You don’t have to wait until you’re done to drop a perfect, finished artifact from the sky (in fact, you may use that as an excuse to never publish). People like stories, so use that to your benefit. Share the wins, the losses, and the thought processes. Bring us along! If you haven’t been in the habit of sharing your work, it’s going to feel weird when you start. That’s normal! Keep going, you get used to it.&lt;/p&gt;
    &lt;p&gt;You’ve done the work. You’ve hit the publish button. You’ve done your part!&lt;/p&gt;
    &lt;head rend="h2"&gt;Capturing the luck&lt;/head&gt;
    &lt;p&gt;You’ve increased the odds that good, unexpected things will come your way. The exact form is hard to predict, but here are a few potential outcomes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;People start to know you as the person that talks about X, Y, and Z.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You start to get emails from people saying that they read your stuff and liked it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You get a DM about a job you might be interested in.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;People ask you if you’re taking on new clients.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Someone you’ve never met or interacted with will mention you as being an expert in your area.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A meetup asks you to come talk about the things you’ve been sharing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You become friends with other people in your industry.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your OSS library starts gaining mindshare.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is not a random list of made-up examples, it’s a list of things that have literally happened to me once I got over my fears and started sharing my work. I had been doing the work all along, but was too afraid to publish. Once I overcame that fear, my Luck Surface Area expanded and good, unexpected things started happening.&lt;/p&gt;
    &lt;p&gt;The formula is simple.&lt;/p&gt;
    &lt;p&gt;Do the work. Don’t be afraid to dive deep into your curiosity and your expertise. We need more people that are intensely curious. We need more people with deep expertise.&lt;/p&gt;
    &lt;p&gt;Tell people. Press publish, bring us along, share the journey. Tell us what you’ve learned, what you’ve built, or what you’re excited about.&lt;/p&gt;
    &lt;p&gt;The formula may be simple, but I’ll admit it’s not always easy. It’s scary to put yourself out there. It’s hard to open yourself up to criticism. People online can be mean. But for every snarky comment, there are ten times as many people quietly following along and admiring not only your work, but your bravery to put it out publicly. And at some point, one of those people quietly following along will reach out with a life-changing opportunity and you’ll think, “Wow, that was lucky.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46397991</guid><pubDate>Sat, 27 Dec 2025 00:43:04 +0000</pubDate></item><item><title>The battle to stop clever people betting</title><link>https://www.economist.com/christmas-specials/2025/12/18/the-battle-to-stop-clever-people-betting</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46398170</guid><pubDate>Sat, 27 Dec 2025 01:11:05 +0000</pubDate></item><item><title>QNX Self-Hosted Developer Desktop</title><link>https://devblog.qnx.com/qnx-self-hosted-developer-desktop-initial-release/</link><description>&lt;doc fingerprint="38a649794034b286"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;QNX Self-Hosted Developer Desktop -- Initial Release&lt;/head&gt;
    &lt;p&gt;Try out the initial release of the QNX Developer Desktop -- a self-hosted development environment for QNX. No more cross-compilation!&lt;/p&gt;
    &lt;p&gt;The team and I are beyond excited to share what we've been cooking up over the last little while: a full desktop environment running on QNX 8.0, with support for self-hosted compilation! This environment both makes it easier for newly-minted QNX developers to get started with building for QNX, but it also vastly simplifies the process of porting Linux applications and libraries to QNX 8.0.&lt;/p&gt;
    &lt;p&gt;This self-hosted target environment is pre-loaded with many of the ports you'll find on the QNX Open-source Dashboard. (The portal currently includes over 1,400 ports across various targets, QNX versions, and architectures, of which more than 600 are unique ports!)&lt;/p&gt;
    &lt;p&gt;In this initial release, you can grab a copy of the QEMU image and give it a try for yourself. There's still so much more to add, but it's in a great place today for this first release. The team is really passionate about this one, and we're eagerly looking forward to your feedback!&lt;/p&gt;
    &lt;head rend="h1"&gt;What's Included&lt;/head&gt;
    &lt;p&gt;For the initial release of Desktop, we tried to cover all the basics: windowing, terminal, IDEs, browser, file management, and samples. To that end, here's what makes up the QNX Developer Desktop:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A customizable XFCE desktop environment running on Wayland&lt;/item&gt;
      &lt;item&gt;The tools you need to compile and/or run your code (&lt;code&gt;clang&lt;/code&gt;, gcc,&lt;code&gt;clang++&lt;/code&gt;, Python,&lt;code&gt;make&lt;/code&gt;,&lt;code&gt;cmake&lt;/code&gt;,&lt;code&gt;git&lt;/code&gt;, etc)&lt;/item&gt;
      &lt;item&gt;A web browser (can you join the QNX Discord from the QNX Desktop? 🏅👀)&lt;/item&gt;
      &lt;item&gt;Ports of popular IDEs/editors, like Geany, Emacs, Neovim, and vim&lt;/item&gt;
      &lt;item&gt;Thunar, for file management&lt;/item&gt;
      &lt;item&gt;Preloaded samples, like Hello World in C, C++, and Python, and GTK demos OpenGL ES demos&lt;/item&gt;
      &lt;item&gt;... and of course, a terminal.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;System Requirements&lt;/head&gt;
    &lt;p&gt;This environment runs as a virtual machine, using QEMU on Ubuntu. To try the image, you'll need:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ubuntu 22.04 or 24.04&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Try It Yourself&lt;/head&gt;
    &lt;p&gt;(Keep in mind this is the first release, so it takes a minute to get started and it's a bit rough around the edges.)&lt;/p&gt;
    &lt;p&gt;With a free QNX license, you can find this release in QNX Software Center. On the Available tab of the Manage Installation pane, search for "quick start" and install the "QNX SDP 8.0 Quick Start Target Image for QEMU".&lt;/p&gt;
    &lt;p&gt;You'll find the image in your QNX installation directory, usually &lt;code&gt;~/qnx800/images&lt;/code&gt; by default. Follow the &lt;code&gt;README.md&lt;/code&gt; file in the &lt;code&gt;qemu&lt;/code&gt; directory to extract &amp;amp; combine the multiple QNX packages downloaded under the hood.&lt;/p&gt;
    &lt;p&gt;Next, follow the PDF instructions found in the new &lt;code&gt;./qemu_qsti/docs/&lt;/code&gt; directory to install the required dependencies and boot up.&lt;/p&gt;
    &lt;head rend="h1"&gt;What's Next&lt;/head&gt;
    &lt;p&gt;This is just the very first release! Over the next few months and beyond, we'll drop more updates of Desktop. You can look forward to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;QEMU images for Windows &amp;amp; macOS, and native images for x86&lt;/item&gt;
      &lt;item&gt;A native Desktop image on Raspberry Pi&lt;/item&gt;
      &lt;item&gt;Enhanced documentation&lt;/item&gt;
      &lt;item&gt;Features to help use this self-hosted environment in CI jobs&lt;/item&gt;
      &lt;item&gt;More samples &amp;amp; stability&lt;/item&gt;
      &lt;item&gt;... and more! Have suggestions? Let us know.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Get Help and Share Feedback&lt;/head&gt;
    &lt;p&gt;Lastly, if you want some help with your QNX journey, you can find the QNX team and community:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;in Discord here: discord.gg/Jj4EkkrFTT&lt;/item&gt;
      &lt;item&gt;on Reddit at: reddit.com/r/qnx&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46398201</guid><pubDate>Sat, 27 Dec 2025 01:16:53 +0000</pubDate></item><item><title>Inside the proton, the ‘most complicated thing you could possibly imagine’ (2022)</title><link>https://www.quantamagazine.org/inside-the-proton-the-most-complicated-thing-imaginable-20221019/</link><description>&lt;doc fingerprint="496cd10f413617a3"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Inside the Proton, the ‘Most Complicated Thing You Could Possibly Imagine’&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;More than a century after Ernest Rutherford discovered the positively charged particle at the heart of every atom, physicists are still struggling to fully understand the proton.&lt;/p&gt;
    &lt;p&gt;High school physics teachers describe them as featureless balls with one unit each of positive electric charge — the perfect foils for the negatively charged electrons that buzz around them. College students learn that the ball is actually a bundle of three elementary particles called quarks. But decades of research have revealed a deeper truth, one that’s too bizarre to fully capture with words or images.&lt;/p&gt;
    &lt;p&gt;“This is the most complicated thing that you could possibly imagine,” said Mike Williams, a physicist at the Massachusetts Institute of Technology. “In fact, you can’t even imagine how complicated it is.”&lt;/p&gt;
    &lt;p&gt;The proton is a quantum mechanical object that exists as a haze of probabilities until an experiment forces it to take a concrete form. And its forms differ drastically depending on how researchers set up their experiment. Connecting the particle’s many faces has been the work of generations. “We’re kind of just starting to understand this system in a complete way,” said Richard Milner, a nuclear physicist at MIT.&lt;/p&gt;
    &lt;p&gt;As the pursuit continues, the proton’s secrets keep tumbling out. Most recently, a monumental data analysis published in August found that the proton contains traces of particles called charm quarks that are heavier than the proton itself.&lt;/p&gt;
    &lt;p&gt;The proton “has been humbling to humans,” Williams said. “Every time you think you kind of have a handle on it, it throws you some curveballs.”&lt;/p&gt;
    &lt;p&gt;Recently, Milner, together with Rolf Ent at Jefferson Lab, MIT filmmakers Chris Boebel and Joe McMaster, and animator James LaPlante, set out to transform a set of arcane plots that compile the results of hundreds of experiments into a series of animations of the shape-shifting proton. We’ve incorporated their animations into our own attempt to unveil its secrets.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cracking Open the Proton&lt;/head&gt;
    &lt;p&gt;Proof that the proton contains multitudes came from the Stanford Linear Accelerator Center (SLAC) in 1967. In earlier experiments, researchers had pelted it with electrons and watched them ricochet off like billiard balls. But SLAC could hurl electrons more forcefully, and researchers saw that they bounced back differently. The electrons were hitting the proton hard enough to shatter it — a process called deep inelastic scattering — and were rebounding from point-like shards of the proton called quarks. “That was the first evidence that quarks actually exist,” said Xiaochao Zheng, a physicist at the University of Virginia.&lt;/p&gt;
    &lt;p&gt;After SLAC’s discovery, which won the Nobel Prize in Physics in 1990, scrutiny of the proton intensified. Physicists have carried out hundreds of scattering experiments to date. They infer various aspects of the object’s interior by adjusting how forcefully they bombard it and by choosing which scattered particles they collect in the aftermath.&lt;/p&gt;
    &lt;p&gt;By using higher-energy electrons, physicists can ferret out finer features of the target proton. In this way, the electron energy sets the maximum resolving power of a deep inelastic scattering experiment. More powerful particle colliders offer a sharper view of the proton.&lt;/p&gt;
    &lt;p&gt;Higher-energy colliders also produce a wider array of collision outcomes, letting researchers choose different subsets of the outgoing electrons to analyze. This flexibility has proved key to understanding quarks, which careen about inside the proton with different amounts of momentum.&lt;/p&gt;
    &lt;p&gt;By measuring the energy and trajectory of each scattered electron, researchers can tell if it has glanced off a quark carrying a large chunk of the proton’s total momentum or just a smidgen. Through repeated collisions, they can take something like a census — determining whether the proton’s momentum is mostly bound up in a few quarks, or distributed over many.&lt;/p&gt;
    &lt;p&gt;Even SLAC’s proton-splitting collisions were gentle by today’s standards. In those scattering events, electrons often shot out in ways suggesting that they had crashed into quarks carrying a third of the proton’s total momentum. The finding matched a theory from Murray Gell-Mann and George Zweig, who in 1964 posited that a proton consists of three quarks.&lt;/p&gt;
    &lt;p&gt;Gell-Mann and Zweig’s “quark model” remains an elegant way to imagine the proton. It has two “up” quarks with electric charges of +2/3 each and one “down” quark with a charge of −1/3, for a total proton charge of +1.&lt;/p&gt;
    &lt;p&gt;But the quark model is an oversimplification that has serious shortcomings.&lt;/p&gt;
    &lt;p&gt;It fails, for instance, when it comes to a proton’s spin, a quantum property analogous to angular momentum. The proton has half a unit of spin, as do each of its up and down quarks. Physicists initially supposed that — in a calculation echoing the simple charge arithmetic — the half-units of the two up quarks minus that of the down quark must equal half a unit for the proton as a whole. But in 1988, the European Muon Collaboration reported that the quark spins add up to far less than one-half. Similarly, the masses of two up quarks and one down quark only comprise about 1% of the proton’s total mass. These deficits drove home a point physicists were already coming to appreciate: The proton is much more than three quarks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Much More Than Three Quarks&lt;/head&gt;
    &lt;p&gt;The Hadron-Electron Ring Accelerator (HERA), which operated in Hamburg, Germany, from 1992 to 2007, slammed electrons into protons roughly a thousand times more forcefully than SLAC had. In HERA experiments, physicists could select electrons that had bounced off of extremely low-momentum quarks, including ones carrying as little as 0.005% of the proton’s total momentum. And detect them they did: HERA’s electrons rebounded from a maelstrom of low-momentum quarks and their antimatter counterparts, antiquarks.&lt;/p&gt;
    &lt;p&gt;The results confirmed a sophisticated and outlandish theory that had by then replaced Gell-Mann and Zweig’s quark model. Developed in the 1970s, it was a quantum theory of the “strong force” that acts between quarks. The theory describes quarks as being roped together by force-carrying particles called gluons. Each quark and each gluon has one of three types of “color” charge, labeled red, green and blue; these color-charged particles naturally tug on each other and form a group — such as a proton — whose colors add up to a neutral white. The colorful theory became known as quantum chromodynamics, or QCD.&lt;/p&gt;
    &lt;p&gt;According to QCD, gluons can pick up momentary spikes of energy. With this energy, a gluon splits into a quark and an antiquark — each carrying just a tiny bit of momentum — before the pair annihilates and disappears. It’s this “sea” of transient gluons, quarks and antiquarks that HERA, with its greater sensitivity to lower-momentum particles, detected firsthand.&lt;/p&gt;
    &lt;p&gt;HERA also picked up hints of what the proton would look like in more powerful colliders. As physicists adjusted HERA to look for lower-momentum quarks, these quarks — which come from gluons — showed up in greater and greater numbers. The results suggested that in even higher-energy collisions, the proton would appear as a cloud made up almost entirely of gluons.&lt;/p&gt;
    &lt;p&gt;The gluon dandelion is exactly what QCD predicts. “The HERA data are direct experimental proof that QCD describes nature,” Milner said.&lt;/p&gt;
    &lt;p&gt;But the young theory’s victory came with a bitter pill: While QCD beautifully described the dance of short-lived quarks and gluons revealed by HERA’s extreme collisions, the theory is useless for understanding the three long-lasting quarks seen in SLAC’s gentle bombardment.&lt;/p&gt;
    &lt;p&gt;QCD’s predictions are easy to understand only when the strong force is relatively weak. And the strong force weakens only when quarks are extremely close together, as they are in short-lived quark-antiquark pairs. Frank Wilczek, David Gross and David Politzer identified this defining feature of QCD in 1973, winning the Nobel Prize for it 31 years later.&lt;/p&gt;
    &lt;p&gt;But for gentler collisions like SLAC’s, where the proton acts like three quarks that mutually keep their distance, these quarks pull on each other strongly enough that QCD calculations become impossible. Thus, the task of further demystifying the three-quark view of the proton has fallen largely to experimentalists. (Researchers who run “digital experiments,” in which QCD predictions are simulated on supercomputers, have also made key contributions.) And it’s in this low-resolution picture that physicists keep finding surprises.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Charming New View&lt;/head&gt;
    &lt;p&gt;Recently, a team led by Juan Rojo of the National Institute for Subatomic Physics in the Netherlands and VU University Amsterdam analyzed more than 5,000 proton snapshots taken over the last 50 years, using machine learning to infer the motions of quarks and gluons inside the proton in a way that sidesteps theoretical guesswork.&lt;/p&gt;
    &lt;p&gt;The new scrutiny picked up a background blur in the images that had escaped past researchers. In relatively soft collisions just barely breaking the proton open, most of the momentum was locked up in the usual three quarks: two ups and a down. But a small amount of momentum appeared to come from a “charm” quark and charm antiquark — colossal elementary particles that each outweigh the entire proton by more than one-third.&lt;/p&gt;
    &lt;p&gt;Short-lived charms frequently show up in the “quark sea” view of the proton (gluons can split into any of six different quark types if they have enough energy). But the results from Rojo and colleagues suggest that the charms have a more permanent presence, making them detectable in gentler collisions. In these collisions, the proton appears as a quantum mixture, or superposition, of multiple states: An electron usually encounters the three lightweight quarks. But it will occasionally encounter a rarer “molecule” of five quarks, such as an up, down and charm quark grouped on one side and an up quark and charm antiquark on the other.&lt;/p&gt;
    &lt;p&gt;Such subtle details about the proton’s makeup could prove consequential. At the Large Hadron Collider, physicists search for new elementary particles by bashing high-speed protons together and seeing what pops out; to understand the results, researchers need to know what’s in a proton to begin with. The occasional apparition of giant charm quarks would throw off the odds of making more exotic particles.&lt;/p&gt;
    &lt;p&gt;And when protons called cosmic rays hurtle here from outer space and slam into protons in Earth’s atmosphere, charm quarks popping up at the right moments would shower Earth with extra-energetic neutrinos, researchers calculated in 2021. These could confound observers searching for high-energy neutrinos coming from across the cosmos.&lt;/p&gt;
    &lt;p&gt;Rojo’s collaboration plans to continue exploring the proton by searching for an imbalance between charm quarks and antiquarks. And heavier constituents, such as the top quark, could make even rarer and harder-to-detect appearances.&lt;/p&gt;
    &lt;p&gt;Next-generation experiments will seek still more unknown features. Physicists at Brookhaven National Laboratory hope to fire up the Electron-Ion Collider in the 2030s and pick up where HERA left off, taking higher-resolution snapshots that will enable the first 3D reconstructions of the proton. The EIC will also use spinning electrons to create detailed maps of the spins of the internal quarks and gluons, just as SLAC and HERA mapped out their momentums. This should help researchers to finally pin down the origin of the proton’s spin, and to address other fundamental questions about the baffling particle that makes up most of our everyday world.&lt;/p&gt;
    &lt;p&gt;Correction: October 20, 2022&lt;lb/&gt; A previous version of the article erroneously implied that lower-momentum quarks live shorter lives than higher-momentum quarks in the quark sea. The text has been updated to clarify that all these quarks are lower-momentum and shorter-lived than those in the three quark-picture.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46398666</guid><pubDate>Sat, 27 Dec 2025 03:00:00 +0000</pubDate></item><item><title>More dynamic cronjobs</title><link>https://george.mand.is/2025/09/more-dynamic-cronjobs/</link><description>&lt;doc fingerprint="29aaa31a29a63e9c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;More dynamic cronjobs&lt;/head&gt;&lt;p&gt;• ~600 words • 3 minute read&lt;/p&gt;&lt;p&gt;I remember learning about cronjobs in the early 2000s. I could tell the computer to go do something, on a recurring basis, forever, even when I wasn't there. They felt like magic!&lt;/p&gt;&lt;p&gt;We didn't have Crontab.guru or AI to ask for figuring out some of the more complex specifications. Just the man pages and good old-fashioned trial and error—mostly error in my case.&lt;/p&gt;&lt;p&gt;But while you could do fun, complex specifications of recurring intervals, you couldn't quite specify something quite as dynamic as "run this script every Tuesday at 7am unless it's the last Tuesday of the month..."&lt;/p&gt;&lt;p&gt;Or at least, you couldn't strictly through the crontab specification syntax. But I had a recent, mildly embarrassing epiphany that it's not hard at all to add arbitrary checks to your crontab to account for more complex and dynamic scenarios.&lt;/p&gt;&lt;p&gt;Want to run a script every Tuesday of the month at 7am except for the last Tuesday? That's easy—set up your crontab to run every Tuesday at 7am, but add a little check to make sure the next week is still part of the same month:&lt;/p&gt;&lt;code&gt;0 7 * * Tue [ "$(date -v+7d '+%m')" = "$(date '+%m')" ] &amp;amp;&amp;amp; /path/to/your_command
&lt;/code&gt;&lt;p&gt;If it's not part of the same month, that means we're on the last Tuesday for the month and the script won't run.&lt;/p&gt;&lt;p&gt;Note: The &lt;code&gt;-v&lt;/code&gt; flag is for the macOS/BSD flavors of &lt;code&gt;date&lt;/code&gt;. On Linux you'd want to use &lt;code&gt;-d +7 days&lt;/code&gt; instead.&lt;/p&gt;&lt;p&gt;This really has nothing to do with cronjobs at all and everything to do with the POSIX "test" command which is the thing we're using with those square brackets. I'm used to seeing and utilizing them in shell scripts, but for whatever reason I never thought to reach for that tool here in the crontab.&lt;/p&gt;&lt;p&gt;You could just as easily rewrite it like this, skipping the bracket shorthand, which is probably easier to read:&lt;/p&gt;&lt;code&gt;0 7 * * Tue test "$(date -v+7d '+%m')" = "$(date '+%m')" &amp;amp;&amp;amp; /path/to/your_command
&lt;/code&gt;&lt;p&gt;It never crossed my mind until recently to add slightly more complex checks at the crontab level.&lt;/p&gt;&lt;head rend="h3"&gt;Other clever cronjob things you can do:&lt;/head&gt;&lt;head rend="h4"&gt;Holiday-only cronjobs&lt;/head&gt;&lt;p&gt;Maybe fetch a list of all the US Holidays for a given year and store them in a handy &lt;code&gt;HOLIDAYS.txt&lt;/code&gt; file somewhere:&lt;/p&gt;&lt;code&gt;curl -s https://date.nager.at/api/v3/PublicHolidays/2025/US | jq -r '.[].date' &amp;gt; HOLIDAYS.txt
&lt;/code&gt;&lt;p&gt;Now you can update your cronjob to run every Tuesday at 7am except on Holidays:&lt;/p&gt;&lt;code&gt;0 7 * * Tue ! grep -qx "$(date +%F)" HOLIDAYS.txt &amp;amp;&amp;amp; /path/to/your_command
&lt;/code&gt;&lt;p&gt;Or inversely, maybe run a holiday-only script that checks once a day&lt;/p&gt;&lt;code&gt;@daily grep -qx "$(date +%F)" HOLIDAYS.txt &amp;amp;&amp;amp; /path/to/your_special_holiday_command
&lt;/code&gt;&lt;head rend="h4"&gt;Only run on sunny days&lt;/head&gt;&lt;p&gt;The National Weather Service makes all kinds of fun data available (if you can find it...). How about a script that runs every hour, but only when the weather is clear?&lt;/p&gt;&lt;code&gt;@hourly curl -s "https://api.weather.gov/gridpoints/TOP/32,81/forecast/hourly" | jq -r '.properties.periods[0].shortForecast' | grep -qi clear &amp;amp;&amp;amp; /path/to/your_command
&lt;/code&gt;&lt;p&gt;Or maybe when the weather is cloudy?&lt;/p&gt;&lt;code&gt;@hourly curl -s "https://api.weather.gov/gridpoints/TOP/32,81/forecast/hourly" | jq -r '.properties.periods[0].shortForecast' | grep -qi cloudy &amp;amp;&amp;amp; /path/to/your_command
&lt;/code&gt;&lt;head rend="h4"&gt;Only run when there's something newsworthy&lt;/head&gt;&lt;p&gt;Or maybe we get in line with every-other-startup I'm aware of and throw AI at the problem, only running our script when the LLM gods have decided there is something newsworthy:&lt;/p&gt;&lt;code&gt;@hourly curl -s "https://news.google.com/rss?hl=en-US&amp;amp;gl=US&amp;amp;ceid=US:en" | llm --system "Reply strictly 'yes' or 'no'. Does anything in the news today suggest it is a good reason to run a script that I only want to send when the world is on fire and crazy and terrible things are happening?"  | tr -d '[:space:]' | tr '[:upper:]' '[:lower:]' | grep -qx yes &amp;amp;&amp;amp; /path/to/oh_no
&lt;/code&gt;--&lt;p&gt;Published on Sunday, September 21st 2025. Read this post as Markdown or plain-text.&lt;/p&gt;&lt;p&gt;If you enjoyed reading this consider signing-up for my newsletter, sharing it on Hacker News or hiring me.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46399576</guid><pubDate>Sat, 27 Dec 2025 06:10:42 +0000</pubDate></item><item><title>Show HN: Ez FFmpeg – Video editing in plain English</title><link>http://npmjs.com/package/ezff</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46400251</guid><pubDate>Sat, 27 Dec 2025 08:45:46 +0000</pubDate></item><item><title>Cursed Bundler: Using go get to install Ruby Gems</title><link>https://nesbitt.io/2025/12/25/cursed-bundler-using-go-get-to-install-ruby-gems.html</link><description>&lt;doc fingerprint="3e2106188de334dd"&gt;
  &lt;main&gt;
    &lt;p&gt;Here’s a thought experiment. What if Ruby had &lt;code&gt;require "github.com/rails/rails"&lt;/code&gt; and you used &lt;code&gt;go get&lt;/code&gt; to fetch it? Set GOPATH to a Ruby load path, and Go’s module fetcher becomes your transport layer. The Go team did not intend this. But it works. Consider this a gift from the Ghost of Package Managers Yet to Come.&lt;/p&gt;
    &lt;p&gt;The setup would look something like this:&lt;/p&gt;
    &lt;code&gt;export GOPATH=/usr/local/lib/ruby/vendor_gems
go get github.com/rack/[email protected]
&lt;/code&gt;
    &lt;p&gt;Go fetches the module, and now you have:&lt;/p&gt;
    &lt;code&gt;/usr/local/lib/ruby/vendor_gems/pkg/mod/
  github.com/
    rack/
      [email protected]/
        lib/
          rack.rb
          rack/
            request.rb
            response.rb
            ...
&lt;/code&gt;
    &lt;p&gt;Build your load path from the lockfile:&lt;/p&gt;
    &lt;code&gt;RUBYLIB=/usr/local/lib/ruby/vendor_gems/pkg/mod/github.com/rack/[email protected]/lib
&lt;/code&gt;
    &lt;p&gt;Now &lt;code&gt;require "rack"&lt;/code&gt; just works. Ruby doesn’t care how the files got there. The version resolution happened once, when you built the load path. And because each version lives in its own directory on disk, multiple versions coexist without conflict. Go’s filesystem layout handles what Ruby’s load path never did gracefully.&lt;/p&gt;
    &lt;head rend="h3"&gt;Self-describing paths&lt;/head&gt;
    &lt;p&gt;Go’s import path convention makes this possible. When you write &lt;code&gt;import "github.com/foo/bar"&lt;/code&gt;, Go doesn’t look up “bar” in some central index. The path itself contains everything needed to find the code: the hosting domain, the org, the repo. It’s self-describing. Compare this to &lt;code&gt;gem install foo&lt;/code&gt;, where “foo” is a magic string that only means something if you know to ask rubygems.org. Without the registry, “foo” is just noise.&lt;/p&gt;
    &lt;p&gt;This decentralisation is unusual in package management. Most systems work the other way: short names resolve through a central index. npm’s &lt;code&gt;lodash&lt;/code&gt; is meaningless without npmjs.com. PyPI’s &lt;code&gt;requests&lt;/code&gt; is meaningless without pypi.org. Central indexes come with social costs too: governance, trust, gatekeeping, decisions about who gets to publish what. Go’s approach embeds the registry into the import path itself. You can host your own modules anywhere, and the path tells clients exactly where to find them.&lt;/p&gt;
    &lt;head rend="h3"&gt;The proxy and the sumdb&lt;/head&gt;
    &lt;p&gt;Now here’s where it gets interesting. Go doesn’t just fetch code from GitHub directly. It goes through proxy.golang.org, a caching proxy run by Google that mirrors every public Go module. And every module version gets an entry in sum.golang.org, a transparency log that records cryptographic hashes of module contents. First fetch wins: once a hash is logged, it’s permanent. This matters because a compromised maintainer can’t silently replace a version. If they try, the hash won’t match and every Go client will refuse the download. Anyone can audit the log for tampering. The security properties are genuinely good.&lt;/p&gt;
    &lt;p&gt;When you run &lt;code&gt;go get github.com/rack/[email protected]&lt;/code&gt;, here’s what actually happens:&lt;/p&gt;
    &lt;code&gt;1. Ask proxy.golang.org for github.com/rack/[email protected]
2. Proxy checks its cache, or fetches from GitHub
3. Proxy returns a zip file of the module contents
4. Go computes SHA-256 hash of the zip
5. Ask sum.golang.org: "what's the hash for this module?"
6. If first fetch ever: sumdb records the hash permanently
7. If seen before: verify hash matches the logged one
8. Unzip to $GOPATH/pkg/mod/github.com/rack/[email protected]/
&lt;/code&gt;
    &lt;p&gt;Your Ruby gem just got the same integrity guarantees as a Go module. The hash is in a Merkle tree. It’s auditable. It’s permanent.&lt;/p&gt;
    &lt;p&gt;What does the proxy actually check? Not much. It would like a go.mod file in the repo, but versions come from git tags. The go.mod doesn’t even need to be valid Go. Run &lt;code&gt;go mod init github.com/you/your-gem&lt;/code&gt; in your Ruby project, push, and you’re done. The sumdb hashes whatever zip file it receives. It doesn’t parse Go code. It doesn’t verify that the module contains valid Go packages. It just slurps up the zip and logs the hash.&lt;/p&gt;
    &lt;p&gt;People already abuse this. You’ll find protobuf definitions hosted as Go modules, with no Go code at all. JSON schemas. Terraform modules. Random data files. As long as there’s a go.mod at the root, proxy.golang.org will cache it and sum.golang.org will log it. The Go infrastructure doesn’t care what’s inside.&lt;/p&gt;
    &lt;p&gt;So: put a go.mod in your Ruby gem’s repo. Push a tag. Run &lt;code&gt;go get&lt;/code&gt;. Your gem is now cached forever in Google’s infrastructure, with a cryptographic hash in a tamper-evident transparency log. You’ve achieved better supply chain integrity than actual RubyGems by pretending your gems are Go modules. RubyGems doesn’t have a transparency log. sum.golang.org does. And you’ve quietly sidestepped rubygems.org entirely.&lt;/p&gt;
    &lt;p&gt;To make this a real package manager, you’d need recursion. Parse the gemspec, find dependencies, &lt;code&gt;go get&lt;/code&gt; those too. You’re one SAT solver away from reinventing Bundler with Go as the transport layer. The dependency resolution logic doesn’t change. Only the fetching does.&lt;/p&gt;
    &lt;code&gt;# Hypothetical go-bundler
1. go get github.com/rack/[email protected]
2. Parse rack.gemspec, find: depends on "github.com/rack/rack-session"
3. go get github.com/rack/[email protected]
4. Parse rack-session.gemspec, find: depends on "github.com/rack/rack"
5. Already have rack, skip
6. Write go.sum (it's a lockfile now)
&lt;/code&gt;
    &lt;p&gt;The dependency graph is the same graph Bundler would compute. You’ve just outsourced the fetching and integrity checking to Google.&lt;/p&gt;
    &lt;p&gt;One difference: Go uses Minimal Version Selection. If you require v1.2.0, you get v1.2.0, not the latest. This makes go.sum almost an afterthought. Bundler and most package managers prefer the newest matching version, which means Gemfile.lock is load-bearing. Without it, you get whatever’s latest today, which might not be what you tested against yesterday. Go’s approach trades “always up to date” for “boringly predictable.” If you actually built this, you might find yourself adopting MVS too. It’s simpler than SAT solving and doesn’t need backtracking. Faster, more deterministic, but more restrictive.&lt;/p&gt;
    &lt;p&gt;There are some cursed details. Go has case-folding escapes because macOS and Windows treat &lt;code&gt;A&lt;/code&gt; and &lt;code&gt;a&lt;/code&gt; as the same file. A repo named &lt;code&gt;BurntSushi/toml&lt;/code&gt; becomes &lt;code&gt;!burnt!sushi/toml&lt;/code&gt; on disk. If you’re building Ruby tooling on top of this, you inherit Go’s filesystem workarounds whether you want them or not. Your &lt;code&gt;require&lt;/code&gt; statements would get weird.&lt;/p&gt;
    &lt;p&gt;Native extensions are where this falls apart. Go expects source or pre-compiled binaries. Ruby gems often need to run &lt;code&gt;make&lt;/code&gt; to compile C code. Pure Ruby gems work fine; anything with native code doesn’t.&lt;/p&gt;
    &lt;head rend="h3"&gt;Trade-offs&lt;/head&gt;
    &lt;p&gt;Why hasn’t anyone done this for real? Partly because it’s absurd. But also because the Go import style has real trade-offs, and most language communities decided they weren’t worth it.&lt;/p&gt;
    &lt;p&gt;Deno tried URL imports. &lt;code&gt;import { serve } from "https://deno.land/std/http/server.ts"&lt;/code&gt; looks a lot like Go imports. It has the same self-describing property: the URL tells you exactly where the code lives. No central registry required. It also has the same problems: verbose paths, no human-friendly short names, squatting is hard because you’d need to squat the domain. Deno eventually retreated to JSR, a more traditional registry with short names.&lt;/p&gt;
    &lt;p&gt;The trade-offs stack up differently depending on what you value:&lt;/p&gt;
    &lt;p&gt;Self-describing paths mean no registry lookup, but they’re long and ugly. &lt;code&gt;require "github.com/rails/rails"&lt;/code&gt; is worse than &lt;code&gt;require "rails"&lt;/code&gt; if you’re typing it by hand. Decentralisation means no single point of failure, but also no single point of governance. Who removes malware from GitHub? Central registries can act on abuse reports. Git hosting is a different trust model.&lt;/p&gt;
    &lt;p&gt;Short names are ergonomic but enable squatting. Anyone can register &lt;code&gt;request&lt;/code&gt; on npm and hope you typo &lt;code&gt;requests&lt;/code&gt;. Domain-based paths are squatting-resistant because you’d need to actually control the domain. But they’re verbose, and nobody wants to type &lt;code&gt;require "github.com/psf/requests"&lt;/code&gt; in every Python file.&lt;/p&gt;
    &lt;p&gt;Go’s approach works for Go because Go chose it from the start and the community built around it. Retrofitting it onto Ruby or Python or JavaScript would require changing how everyone writes import statements. The tooling works. The migration doesn’t.&lt;/p&gt;
    &lt;p&gt;Still, the underlying idea is sound. What if every package manager shared a content-addressed, transparency-logged, globally-cached distribution layer? You wouldn’t need to pretend your gems are Go modules. You’d just have the same infrastructure available natively. The costs of running a reliable package CDN are substantial.&lt;/p&gt;
    &lt;p&gt;In the meantime, Go’s module system sits there, accidentally universal, logging hashes of whatever you throw at it. The FOSDEM talk writes itself: “We achieved cryptographic supply chain integrity for Ruby by pretending all gems were Go modules. The Go team was confused about why their sumdb was full of .rb files.”&lt;/p&gt;
    &lt;p&gt;Nobody should actually do this. I couldn’t resist anyway: go-bundler is a proof of concept. But it reveals something interesting about package management design.&lt;/p&gt;
    &lt;p&gt;This thought experiment is part of a larger question I’ve been exploring: what are the fundamental components of a package manager, and which ones could be shared across ecosystems? Most people think of package managers as monolithic, but they’re really several systems bolted together:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Naming - how you refer to packages&lt;/item&gt;
      &lt;item&gt;Discovery - finding what exists&lt;/item&gt;
      &lt;item&gt;Resolution - solving the version constraint problem&lt;/item&gt;
      &lt;item&gt;Transport - fetching bits&lt;/item&gt;
      &lt;item&gt;Integrity - verifying you got what you expected&lt;/item&gt;
      &lt;item&gt;Installation - putting files where they need to go&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Go made unusual choices at naming, transport, and integrity that happen to be language-agnostic. That’s what makes the Ruby hack possible. It hints at infrastructure we maybe should have built intentionally. Go built an anonymous, transparency-logged package proxy with minimal governance, then let anyone use it for free.&lt;/p&gt;
    &lt;p&gt;Somewhere in Mountain View, a Go module proxy is serving a zip file full of Ruby code, hashing it into a Merkle tree, and wondering what it did to deserve this.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46400927</guid><pubDate>Sat, 27 Dec 2025 11:05:46 +0000</pubDate></item><item><title>Splice a Fibre</title><link>https://react-networks-lib.rackout.net/fibre</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46401190</guid><pubDate>Sat, 27 Dec 2025 11:57:17 +0000</pubDate></item></channel></rss>