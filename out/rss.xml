<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 25 Oct 2025 20:10:39 +0000</lastBuildDate><item><title>Public Montessori programs strengthen learning outcomes at lower costs: study</title><link>https://phys.org/news/2025-10-national-montessori-early-outcomes-sharply.html</link><description>&lt;doc fingerprint="ba34a29a1e098ba9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;National study finds public Montessori programs strengthen early learning outcomes—at sharply lower costs&lt;/head&gt;
    &lt;head rend="h5"&gt;Sadie Harley&lt;/head&gt;
    &lt;p&gt;scientific editor&lt;/p&gt;
    &lt;head rend="h5"&gt;Robert Egan&lt;/head&gt;
    &lt;p&gt;associate editor&lt;/p&gt;
    &lt;p&gt;The first national randomized trial of public Montessori preschool students showed stronger long-term outcomes by kindergarten, including elevated reading, memory, and executive function as compared to non-Montessori preschoolers.&lt;/p&gt;
    &lt;p&gt;The study of 588 children across two dozen programs nationwide shows an imperative to follow and study these outcomes through graduation and beyond.&lt;/p&gt;
    &lt;p&gt;A new national study led by researchers from the University of Virginia, University of Pennsylvania and the American Institutes for Research found that Montessori preschool programs (ages 3 to 6) in public schools deliver stronger early learning outcomes for children—and at a sharply lower cost to school districts and taxpayers.&lt;/p&gt;
    &lt;p&gt;The first randomized controlled trial of its kind, published in Proceedings of the National Academy of Sciences, tracked nearly 600 children across 24 public Montessori programs nationwide.&lt;/p&gt;
    &lt;p&gt;By the end of kindergarten, children who won a random lottery to attend public Montessori preschools outperformed their peers in reading, executive function, short-term memory, and social understanding—all while costing approximately $13,000 less per child than traditional preschool programs.&lt;/p&gt;
    &lt;p&gt;Those costs do not include anticipated savings from improved teacher morale and retention, a dynamic demonstrated in other data.&lt;/p&gt;
    &lt;p&gt;The findings, which have been vetted by third parties, contrast sharply with the prior common findings, where impacts of preschool were observed immediately following the program but then seemed to disappear by the end of kindergarten.&lt;/p&gt;
    &lt;p&gt;"These findings affirm what Maria Montessori believed over a century ago—that when we trust children to learn with purpose and curiosity, they thrive," said Angeline Lillard, Commonwealth Professor of Psychology at the University of Virginia. "Public Montessori programs are not only effective but cost-efficient."&lt;/p&gt;
    &lt;p&gt;"Montessori preschool programs are already being used in hundreds of U.S. public schools, and our research shows that they are having a positive impact in key areas of early learning," said Karen Manship, co-author and Managing Director at the American Institutes for Research.&lt;/p&gt;
    &lt;p&gt;"These findings provide valuable evidence to policymakers and educational leaders who are seeking to deliver better outcomes with increasingly limited resources."&lt;/p&gt;
    &lt;p&gt;"Montessori began in the low-income housing of early 20th century Rome," noted David Loeb of the University of Pennsylvania. "This research shows it still delivers on that promise for America's children today."&lt;/p&gt;
    &lt;head rend="h2"&gt;Key findings:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Stronger early learning: Montessori children scored significantly higher in reading, memory, executive function, and understanding others' perspectives by the end of kindergarten.&lt;/item&gt;
      &lt;item&gt;Sustained benefits: Unlike many preschool programs where gains fade, Montessori students' relative outcomes improved over time.&lt;/item&gt;
      &lt;item&gt;Cost savings: When compared to traditional public preschool, Public Montessori programs cost $13,000 less per child across the three years from ages 3–6, due primarily to more efficient class structures, including harnessing the benefits of children teaching each other across age groups.&lt;/item&gt;
      &lt;item&gt;Teacher morale and retention: In practice, those cost savings are likely even higher due to prior prevailing evidence that Montessori teachers experience higher job satisfaction and lower turnover.&lt;/item&gt;
      &lt;item&gt;Benefits for all children: Effects were strongest among children from lower-income families, although children of all backgrounds benefited. These and other findings are a helpful reminder that Montessori was originally designed to reach low-income communities.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Dr. Maria Montessori opened her first classroom in 1907 in the working-class tenements of Rome, and pioneered an educational model rooted in children's natural drive to learn. Today, more than 600 U.S. public schools offer Montessori education.&lt;/p&gt;
    &lt;p&gt;This national study affirms that Montessori's century-old model is a highly effective approach to early education—delivering enduring benefits for children and communities alike.&lt;/p&gt;
    &lt;p&gt;The research also appears highly actionable for policymakers, because the results found the Montessori programs delivered better outcomes at sharply lower costs, and studies have demonstrated improved teacher morale and retention for Montessori programs.&lt;/p&gt;
    &lt;p&gt;The paper's co-authors include researchers from the American Institutes for Research (Juliette Berg, Maya Escueta, Alison Hauser) and UVA graduate student Emily Daggett.&lt;/p&gt;
    &lt;p&gt;More information: Lillard, Angeline S., A national randomized controlled trial of the impact of public Montessori preschool at the end of kindergarten, Proceedings of the National Academy of Sciences (2025). DOI: 10.1073/pnas.2506130122&lt;/p&gt;
    &lt;p&gt;Journal information: Proceedings of the National Academy of Sciences&lt;/p&gt;
    &lt;p&gt;Provided by University of Virginia&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45674002</guid><pubDate>Wed, 22 Oct 2025 19:29:59 +0000</pubDate></item><item><title>Context engineering is sleeping on the humble hyperlink</title><link>https://mbleigh.dev/posts/context-engineering-with-links/</link><description>&lt;doc fingerprint="a74f911328bb1b36"&gt;
  &lt;main&gt;
    &lt;p&gt;As we all learn more about Context Engineering for LLMs (see Anthropic’s post for an excellent primer), we’ve identified a few important limitations. Conversations should be append-only to maximize cacheability. Models are typically more responsive to “fresh” context close to the end of the window. Models typically perform worse when overwhelmed with large amounts of context.&lt;/p&gt;
    &lt;p&gt;With this in mind, a key tension comes into focus: the model needs access to all valuable context, BUT ONLY when that context is relevant to the task at hand.&lt;/p&gt;
    &lt;p&gt;Context engineering is effectively the practice of finding ways to manage this tension. Popular solutions include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Retrieval Augmented Generation (RAG), which attempts to dynamically discover and load specific relevant context for the current query proactively.&lt;/item&gt;
      &lt;item&gt;Subagents, which encapsulate specialized instructions and tools to avoid polluting the main thread.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;get_*&lt;/code&gt;Tools, which allow the model to proactively request information that it deems relevant using tool calls.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There’s one technique that I feel is woefully underutilized by agents today: the humble hyperlink.&lt;/p&gt;
    &lt;head rend="h2"&gt;The obligatory human analogy&lt;/head&gt;
    &lt;p&gt;If you, a human, need to learn something without an LLM (let’s say something about an open source library), you will probably follow a trajectory that looks something like the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Do a Google search for the topic you need to understand&lt;/item&gt;
      &lt;item&gt;Click a relevant link to e.g. a docs page, read a high-level guide&lt;/item&gt;
      &lt;item&gt;Depending on your needs, maybe Cmd+Click a few more pages or the reference docs to open them in new tabs to review&lt;/item&gt;
      &lt;item&gt;Refer between your various open tabs as you complete your task&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once you found an entrypoint through search, you were able to incrementally explore the topic through discovered links, filling your mental context with relevant information.&lt;/p&gt;
    &lt;p&gt;We can do the same thing with LLMs.&lt;/p&gt;
    &lt;head rend="h2"&gt;HATEOAS in the era of Agents&lt;/head&gt;
    &lt;p&gt;The power of linked data is nothing new. Folks who have been building HTTP APIs for a long time might be familiar with HATEOAS, or “Hypertext as the Engine of Application State”. Purists have long claimed that a “truly” RESTful API should be fully self-describing, such that a client can explore and interact with it knowing nothing but an entrypoint in advance, with hyperlinks providing all necessary context to discover and consume additional endpoints.&lt;/p&gt;
    &lt;p&gt;This never worked in practice. Building hypertext APIs was too cumbersome and to actually consume APIs a human needed to understand the API structure in a useful manner anyway. So it was more useful just to have a “REST-ish” API and a good documentation page that humans could use. Creating “machine-readable” hyperlinked APIs that machines could navigate in theory but not in practice just wasn’t practical. LLMs change this dramatically.&lt;/p&gt;
    &lt;p&gt;When the machine can not only parse but also navigate the context and relevance of hyperlinks you have an actually useful paradigm: Hypertext as the Engine of Agent State.&lt;/p&gt;
    &lt;p&gt;This does apply to the web and HTTP APIs — I expect in the next few years we’ll see a resurgence of hypermedia concepts to make APIs more self-documenting (“dump the entire API schema as OpenAPI” is a start but not really sufficient).&lt;/p&gt;
    &lt;p&gt;But it also applies to local data, agent-specific data, really any data we want an agent to be able to discover and read.&lt;/p&gt;
    &lt;p&gt;So, how do we make all of our context linkable for agents?&lt;/p&gt;
    &lt;head rend="h2"&gt;One Tool to Read Them All&lt;/head&gt;
    &lt;p&gt;The scaffolding required to implement a powerful link-based context system is lightweight enough to be trivial. You need only:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A tool that accepts a list of URIs as arguments.&lt;/item&gt;
      &lt;item&gt;An entrypoint that brings at least one URI into context.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s a demonstrative example using Genkit in JS that uses system instructions as an entrypoint.&lt;/p&gt;
    &lt;p&gt;If we run the above code with a few different prompts, we see links in action:&lt;/p&gt;
    &lt;p&gt;The results are exactly what we’d hope:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the first test, no link reading was required and no links were read. The answer was right in the system prompt!&lt;/item&gt;
      &lt;item&gt;In the second test, reading one link was sufficient to reach a conclusion.&lt;/item&gt;
      &lt;item&gt;In the final test, the model understood to recursively fetch context linked from the first loaded document to reach a conclusion.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So, in ~30 lines of code with very little prompting and while using a cost-effective model, we’ve created a system that can dynamically load and correctly apply relevant context on-demand.&lt;/p&gt;
    &lt;head rend="h2"&gt;Benefits of Links&lt;/head&gt;
    &lt;p&gt;Links are a powerful tool in the context engineering toolbelt because of their simplicity, flexibility, and efficiency.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Links are trivial to implement, and current models are “intuitively” good at understanding how to follow links.&lt;/item&gt;
      &lt;item&gt;Links can surface anywhere in the flow of a conversation. They can be specified in a system prompt, provided by the user, or returned by a tool.&lt;/item&gt;
      &lt;item&gt;Links are token-efficient because they use a small number of tokens to provide on-demand access to specific information. The model can load a link if it needs it but if it doesn’t few tokens are wasted.&lt;/item&gt;
      &lt;item&gt;Links are tool-efficient because they consolidate many types of reads into a single tool. You can have a &lt;code&gt;data://me&lt;/code&gt;link that dynamically loads information about the current user, a&lt;code&gt;file://foo.md&lt;/code&gt;link that loads a local file, and a&lt;code&gt;prompt://pet-help&lt;/code&gt;link that returns static instructions. You don’t need a separate tool for each type of data.&lt;/item&gt;
      &lt;item&gt;Links provide just-in-time context mitigating issues of context rot and recency bias in models. Because linked context is loaded when it’s needed by the model the context is “fresher” instead of overloading a system prompt.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;MCP Resources: The future is now(ish)&lt;/head&gt;
    &lt;p&gt;To make link-based context engineering a universal feature, we need a way to provide the linked content to the model. Many agents (and some model APIs) have built in various forms of “fetch URL” or “search the web” tools that can automatically fetch data from public sources. But the content we want to link might not always be available on the public internet.&lt;/p&gt;
    &lt;p&gt;The good news is we already have the exact primitive we need to solve this problem: MCP Resources. Resources allows servers to register URIs (or patterns of URIs) that can then be read on-demand by clients to provide static or dynamic content. Sounds perfect, right?&lt;/p&gt;
    &lt;p&gt;The bad news is I’m not aware of a single MCP client that makes MCP resources consumable by the model. Today, they only allow resources to be inserted by the user via @-mentions and similar devices. This doesn’t enable linking. But we’re so close!. We just need one critical thing:&lt;/p&gt;
    &lt;p&gt;Every MCP-enabled agent should expose a &lt;code&gt;read_resources&lt;/code&gt; tool that accepts one or more URIs and aggregates reading across all connected MCP servers (and probably web URLs as well).&lt;/p&gt;
    &lt;p&gt;There are other changes that would help: a mechanism of indexing / listing available MCP resources and exposing that in the system instructions, maybe even indexing MCP resources so that they can be searched using RAG techniques. But just enabling agents to access linked content opens up a world of new context engineering techniques.&lt;/p&gt;
    &lt;head rend="h2"&gt;Working with what you have&lt;/head&gt;
    &lt;p&gt;If you’re building your own agent, you don’t need any new technology to start making linked context — you can do it in a few dozen lines of code like I demonstrated above. But even if you’re trying to integrate with existing agents like Gemini CLI, Claude Code, or Cursor, you can still build with linked data patterns.&lt;/p&gt;
    &lt;p&gt;The Firebase MCP Server recently launched new capabilities including a &lt;code&gt;/firebase:init&lt;/code&gt; slash command for setting up Firebase in a project. We had some specific and pretty complex use cases in mind, so we built linked context into the MCP server itself:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We added support for MCP Resources to the Firebase MCP Server.&lt;/item&gt;
      &lt;item&gt;We created a read_resources MCP tool that was capable of reading resources (from the Firebase MCP Server only).&lt;/item&gt;
      &lt;item&gt;We created an MCP prompt that creates a guided workflow to walk the model step-by-step through configuring Firebase, including linked branching paths.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’ve tested out this initialization flow against several popular coding agents that support MCP Prompts — each of them is able to understand and follow hyperlinks using our MCP server’s &lt;code&gt;read_resources&lt;/code&gt; tool and we’ve made onboarding to Firebase all the easier for it.&lt;/p&gt;
    &lt;p&gt;Effective context engineering is constantly evolving as models and agent harnesses improve; however, hyperlinks are such a powerfully efficient mechanism for information traversal that I can’t imagine a future of agents that doesn’t include linked context.&lt;/p&gt;
    &lt;p&gt;The next time you’re thinking of building half a dozen &lt;code&gt;get_*&lt;/code&gt; or &lt;code&gt;list_*&lt;/code&gt; tools for your agent, take a step back and consider: could the humble hyperlink get the job done instead?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45682164</guid><pubDate>Thu, 23 Oct 2025 14:24:47 +0000</pubDate></item><item><title>First convex polyhedron found that can't pass through itself</title><link>https://www.quantamagazine.org/first-shape-found-that-cant-pass-through-itself-20251024/</link><description>&lt;doc fingerprint="a390899286a00301"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;First Shape Found That Can’t Pass Through Itself&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;Imagine you’re holding two equal-size dice. Is it possible to bore a tunnel through one die that’s big enough for the other to slide through?&lt;/p&gt;
    &lt;p&gt;Perhaps your instinct is to say “Surely not!” If so, you’re not alone. In the late 1600s, an unidentified person placed a bet to that effect with Prince Rupert of the Rhine. Rupert — a nephew of Charles I of England who commanded the Royalist forces in the English Civil War — spent his sunset years studying metallurgy and glassmaking in his laboratory at Windsor Castle.&lt;/p&gt;
    &lt;p&gt;Rupert won the bet. The mathematician John Wallis, recounting the story in 1693, didn’t say whether Rupert wrote a proof or bored a hole through an actual cube. But Wallis himself proved mathematically that, if you drill a straight tunnel in the direction of one of the cube’s inner diagonals, it can be made wide enough to allow another cube through. It’s a tight squeeze: If you make the second cube just 4% larger, it will no longer fit.&lt;/p&gt;
    &lt;p&gt;It’s natural to wonder which other shapes have this property. “I think of this problem as being quite canonical,” said Tom Murphy, a software engineer at Google who has explored the question extensively in his free time. It “would have gotten rediscovered and rediscovered — aliens would have come to this one.”&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;The full menagerie of shapes is too diverse to get a handle on, so mathematicians tend to focus on convex polyhedra: shapes, like the cube, that have flat sides and no protrusions or indentations. When such a shape is much wider in some directions than others, it’s usually easy to find a straight tunnel that will allow another copy of the shape to pass through. But many famous convex polyhedra — for instance the dodecahedron, or the truncated icosahedron, the shape that forms a soccer ball — are highly symmetric and difficult to analyze. Among these, “for hundreds of years we only knew of the cube,” said Jakob Steininger, a mathematician at Statistics Austria, Austria’s federal statistics organization.&lt;/p&gt;
    &lt;p&gt;Then, in 1968, Christoph Scriba proved that the tetrahedron and octahedron also have the “Rupert property,” as mathematicians now call it. And in a burst of activity over the past decade, professional mathematicians and hobbyists have found Rupert tunnels through many of the most widely studied convex polyhedra, including the dodecahedron, icosahedron and soccer ball.&lt;/p&gt;
    &lt;p&gt;The Rupert property appeared to be so widespread that mathematicians conjectured a general rule: Every convex polyhedron will have the Rupert property. No one could find one that didn’t — until now.&lt;/p&gt;
    &lt;p&gt;In a paper posted online in August, Steininger and Sergey Yurkevich — a researcher at A&amp;amp;R Tech, an Austrian transportation systems company — describe a shape with 90 vertices and 152 faces that they’ve named the Noperthedron (after “Nopert,” a coinage by Murphy that combines “Rupert” and “nope”). Steininger and Yurkevich proved that no matter how you bore a straight tunnel through a Noperthedron, a second Noperthedron cannot fit through.&lt;/p&gt;
    &lt;p&gt;The proof required a mix of theoretical advances and massive computer calculations, and relies on a delicate property of the Noperthedron’s vertices. “It’s a miracle that it works,” Steininger said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Passing Through the Shadows&lt;/head&gt;
    &lt;p&gt;To see how one cube can pass through another, imagine holding a cube over a table and examining its shadow (assuming it’s illuminated from above). If you hold the cube in the standard position, the shadow is a square. But if you point one of the corners directly upward, the shadow is a regular hexagon.&lt;/p&gt;
    &lt;p&gt;In 1693, Wallis showed that the square shadow fits inside the hexagon, leaving a thin margin. That means that if you point a cube’s corner upward, you can bore a vertical tunnel that’s big enough for a second cube to pass through. About a century later, Pieter Nieuwland showed that a different orientation casts an even better shadow — one that can accommodate a cube more than 6% larger than the cube with the tunnel.&lt;/p&gt;
    &lt;p&gt;Mark Belan/Quanta Magazine&lt;/p&gt;
    &lt;p&gt;Every subsequent analysis of more complicated shapes has relied on this process of turning the shape in different directions and looking for one shadow that fits inside another. With the aid of computers, mathematicians have found Rupert passages through a wide variety of shapes. Some are incredibly tight fits — for instance, the passage in a “triakis tetrahedron” has a margin that’s only about 0.000002 times the length of the shape’s radius. “The world of mixing computation and discrete geometry has flowered to make these kinds of calculations possible,” said Joseph O’Rourke, an emeritus professor at Smith College.&lt;/p&gt;
    &lt;p&gt;Researchers who have written algorithms to find Rupert passages have noticed a curious dichotomy: For any given convex polyhedron, the algorithm seems to either find a passage almost immediately, or not find one at all. In the past five years, mathematicians have accumulated a small collection of holdout shapes for which no passage has been found.&lt;/p&gt;
    &lt;p&gt;“I’ve had my desktop churn for two weeks on trying the rhombicosidodecahedron,” said Benjamin Grimmer, an applied mathematician at Johns Hopkins University, referring to a solid made of 62 regular triangles, squares and pentagons. “That one just seems to resist any attempt.”&lt;/p&gt;
    &lt;p&gt;But such resistance doesn’t prove that a shape is a Nopert. There are infinitely many ways to orient a shape, and a computer can only check finitely many. Researchers don’t know whether the holdouts are true Noperts or just shapes whose Rupert passages are hard to find.&lt;/p&gt;
    &lt;p&gt;What they do know is that candidate Noperts are incredibly rare. Starting last year, Murphy began to construct hundreds of millions of shapes. These include random polyhedra, polyhedra whose vertices lie on a sphere, polyhedra with special symmetries, and polyhedra in which he moved one vertex to intentionally mess up a previous Rupert passage. His algorithm easily found Rupert tunnels for nearly every one.&lt;/p&gt;
    &lt;p&gt;The contrast between these quick results and the stubbornness of the Nopert holdouts made some mathematicians suspect that true Noperts do exist. But until August, all they had were suspicions.&lt;/p&gt;
    &lt;head rend="h2"&gt;No Passage&lt;/head&gt;
    &lt;p&gt;Steininger, now 30, and Yurkevich, 29, have been friends since they participated together as teenagers in mathematics Olympiad competitions. Even though both eventually left academia (after a doctorate for Yurkevich and a master’s for Steininger), they have continued to explore unsolved problems together.&lt;/p&gt;
    &lt;p&gt;“We just had pizza three hours ago, and we talked about math almost the whole time,” Steininger told Quanta. “That’s what we do.”&lt;/p&gt;
    &lt;p&gt;Five years ago, the pair happened upon a YouTube video of one cube passing through another, and they were instantly smitten. They developed an algorithm to search for Rupert tunnels and soon became convinced that some shapes were Noperts. In a 2021 paper, they conjectured that the rhombicosidodecahedron is not Rupert. Their work, which preceded Murphy’s and Grimmer’s recent explorations, was, “I think, the first to conjecture that there might be solids that don’t have this property,” Steininger said.&lt;/p&gt;
    &lt;p&gt;If you want to prove that a shape is a Nopert, you must rule out Rupert tunnels for every possible orientation of the two shapes. Each orientation can be written down as a collection of rotation angles. This collection of angles can then be represented as a point in a higher-dimensional “parameter space.”&lt;/p&gt;
    &lt;p&gt;Florentina Stadlbauer; Courtesy of Jakob Steininger&lt;/p&gt;
    &lt;p&gt;Suppose you choose an orientation for your two shapes, and the computer tells you that the second shadow sticks out past the border of the first shadow. This rules out one point in the parameter space.&lt;/p&gt;
    &lt;p&gt;But you may be able to rule out much more than a single point. If the second shadow sticks out significantly, it would require a big change to move it inside the first shadow. In other words, you can rule out not just your initial orientation but also “nearby” orientations — an entire block of points in the parameter space. Steininger and Yurkevich came up with a result they called their global theorem, which quantifies precisely how large a block you can rule out in these cases. By testing many different points, you can potentially rule out block after block in the parameter space.&lt;/p&gt;
    &lt;p&gt;If these blocks cover the entire parameter space, you’ll have proved that your shape is a Nopert. But the size of each block depends on how far the second shadow sticks out beyond the first, and sometimes it doesn’t stick out very far. For instance, suppose you start with the two shapes in exactly the same position, and then you slightly rotate the second shape. Its shadow will at most stick out just a tiny bit past the first shadow, so the global theorem will only rule out a tiny box. These boxes are too small to cover the whole parameter space, leaving the possibility that some point you’ve missed might correspond to a Rupert tunnel.&lt;/p&gt;
    &lt;p&gt;To deal with these small reorientations, the pair came up with a complement to their global theorem that they called the local theorem. This result deals with cases where you can find three vertices (or corner points) on the boundary of the original shadow that satisfy some special requirements. For instance, if you connect those three vertices to form a triangle, it must contain the shadow’s center point. The researchers showed that if these requirements are met, then any small reorientation of the shape will create a shadow that pushes at least one of the three vertices further outward. So the new shadow can’t lie inside the original shadow, meaning it doesn’t create a Rupert tunnel.&lt;/p&gt;
    &lt;p&gt;If your shape casts a shadow that lacks three appropriate vertices, the local theorem won’t apply. And all the previously identified Nopert candidates have at least one shadow with this problem. Steininger and Yurkevich sifted through a database of hundreds of the most symmetric and beautiful convex polyhedra, but they couldn’t find any shape whose shadows all worked. So they decided to generate a suitable shape themselves.&lt;/p&gt;
    &lt;p&gt;They developed an algorithm to construct shapes and test them for the three-vertices property. Eventually, the algorithm produced the Noperthedron, which is made of 150 triangles and two regular 15-sided polygons. It looks like a rotund crystal vase with a wide base and top; one fan of the work has already 3D-printed a copy to use as a pencil holder.&lt;/p&gt;
    &lt;p&gt;Peter Lely&lt;/p&gt;
    &lt;p&gt;Steininger and Yurkevich then divided the parameter space of orientations into approximately 18 million tiny blocks, and tested the center point of each block to see if its corresponding orientation produced a Rupert passage. None of them did. Next, the researchers showed that each block satisfied either the local or global theorem, allowing them to rule out the entire block. Since these blocks fill out the entire parameter space, this meant that there is no Rupert passage through the Noperthedron.&lt;/p&gt;
    &lt;p&gt;The “natural conjecture has been proved false,” O’Rourke said.&lt;/p&gt;
    &lt;p&gt;It remains to be seen whether mathematicians can use the new method to generate other Noperts, or if they can find a different local theorem that can handle candidates like the rhombicosidodecahedron. But now that mathematicians know that Noperts do exist, “we’re on sound footing to study other shapes,” Murphy said.&lt;/p&gt;
    &lt;p&gt;Murphy, who like Steininger and Yurkevich has been exploring the question for its own sake, independent of his day job, feels a kinship across the centuries with Prince Rupert. “I like that he chose to use his retirement to do math and science in his castle,” he said.&lt;/p&gt;
    &lt;p&gt;Meanwhile, Steininger and Yurkevich are on the lookout for new questions to tackle. “We’re just humble mathematicians — we love working on such problems,” Steininger said. “We’ll keep doing that.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45694856</guid><pubDate>Fri, 24 Oct 2025 14:12:00 +0000</pubDate></item><item><title>Unlocking free WiFi on British Airways</title><link>https://www.saxrag.com/tech/reversing/2025/06/01/BAWiFi.html</link><description>&lt;doc fingerprint="2f3d91353c233d96"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Unlocking free WiFi on British Airways&lt;/head&gt;
    &lt;p&gt;I was recently flying between HKG &amp;amp; LHR via British Airways. Iâd done the same flight back in 2023, and remember relying on the in-flight entertainment for the 14 hour journey. However, this time on my way to London, they had an interesting offer: Free WiFi for âMessagingâ, for members of âThe British Airways Clubâ.&lt;/p&gt;
    &lt;p&gt;I was pretty sure I wasnât a member of any sort of club (Iâm only flying economy anyway); but turns out this is just the name of their frequent flyer program. Conveniently enough, youâre able to sign up for this via the captive portal while in the sky; and although it asks for your E-Mail you donât need to verify it (thereby allowing you to complete the signup without access to the internet).&lt;/p&gt;
    &lt;p&gt;Once signed in, the captive portal invited me to âStart sessionâ, which true to itâs word, let me start texting people. I tried Whatsapp, Signal, Wechat and Discord. The first three worked (though not for images), Discord expectedly did not. Not bad for free wifi!&lt;/p&gt;
    &lt;head rend="h2"&gt;How does it know?&lt;/head&gt;
    &lt;p&gt;This was the first question I had as soon as I confirmed messaging did work. Itâs 2025; everything should be encrypted in transit. So how does it know if Iâm using Whatsapp vs. Discord? One idea I had is it just somehow capped the bandwidth / data transfer of individual TCP connections; so when youâre sending a single message or two it gets through, but something larger would fail.&lt;/p&gt;
    &lt;p&gt;To test this, I used my phone to open up the classic: example.com. Unfortunately this didnât load - so there mustâve been a bit more going onâ¦&lt;/p&gt;
    &lt;p&gt;Thankfully I had my laptop on me, so the next step was to connect to WiFi with the devtools open to the network tab, and wireshark on the side for good measure. After registering for the WiFi again, it was time to play around a bit. Opening up something like example.com revealed a TCP reset in the wireshark, right after the Client Hello, and my brain immediately jumped to SNI. Itâs something thatâs really annoyed me about the TLS spec since itâs widely used by ISPs in India to block websites (although there is work being done to fix this; ECH (which was itself previously ESNI)).&lt;/p&gt;
    &lt;p&gt;tl;dr SNI reveals the domain name of EVERY website you connect to in the TLS handshake, before the tunnel is established! Although the actual contents of what youâre doing, on say, totallynondodgywebsite.com are encrypted, anyone on the wire can see that you connected to it (including ISPs). My guess was that they had a set of whitelisted domains used by messaging apps, and if they see anything else, they just reset the connection.&lt;/p&gt;
    &lt;p&gt;Sidebar: peopleâs reactions when I try to tell this are always extremely varied. Many of my non-technical friends think anything you do without a VPN is visible to everyone, while some slightly technical ones still think that the URL (including query params) is visible, but the responses are not. Finally there is some subset of people who believe TLS means all data is encrypted in transit between client &amp;amp; server, though they had no idea SNI leaks all the domains they visit!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing out the theory&lt;/head&gt;
    &lt;p&gt;Although BA blocks DNS queries to all (well all I could remember) public resolvers, they do resolve any domain you throw at them, including MX, TXT, HTTPS records. (This itself could be an interesting area of exploration; especially since the DNS resolution can be triggered before signing up for free WiFi. Something along the lines of arbitrary subdomains which represent the request payload, and a custom nameserver that returns responses via the TXT record or something. Anywayâ¦).&lt;/p&gt;
    &lt;p&gt;Getting the A record of my personal server, I made a TLS handshake to the IP address directly, without any SNI. This was then reset by BA; so the lack of SNI is also blocked!&lt;/p&gt;
    &lt;code&gt;$ openssl s_client -connect 95.217.167.10:443
Connecting to 95.217.167.10
CONNECTED(00000003)
write:errno=104
---
no peer certificate available
---
No client certificate CA names sent
---
SSL handshake has read 0 bytes and written 302 bytes
Verification: OK
---
New, (NONE), Cipher is (NONE)
Protocol: TLSv1.3
This TLS version forbids renegotiation.
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 0 (ok)
---
&lt;/code&gt;
    &lt;p&gt;The next step was to try and test some SNI that might go through. Off the top of my head, I knew &lt;code&gt;wa.me&lt;/code&gt; was used by Whatsapp for some stuff, so I decided to use it. The way SNI works is it tells the server which host you want to connect to, so it can present the right TLS certificate. In my case, my server did not have any cert for &lt;code&gt;wa.me&lt;/code&gt; , but NGINX seemingly just ignores the SNI if it doesnât exist and returns the first cert (I think; could also be related to my config but I didnât look to much into this).&lt;/p&gt;
    &lt;p&gt;But basically, as long as I (the client) donât care, I can complete the TLS connection for any random cert the server offers me, even if in the SNI I provide a domain I donât control (e.g. &lt;code&gt;wa.me&lt;/code&gt; in this case).&lt;/p&gt;
    &lt;code&gt;$ openssl s_client -connect 95.217.167.10:443 -servername wa.me
Connecting to 95.217.167.10
CONNECTED(00000003)
depth=2 C=US, O=Internet Security Research Group, CN=ISRG Root X1
verify return:1
depth=1 C=US, O=Let's Encrypt, CN=R3
verify return:1
depth=0 CN=mijia.mywaifu.best
verify error:num=10:certificate has expired
notAfter=Jul 22 13:03:02 2023 GMT
verify return:1
depth=0 CN=mijia.mywaifu.best
notAfter=Jul 22 13:03:02 2023 GMT
verify return:1
---
Certificate chain
&amp;lt;snip&amp;gt;
&lt;/code&gt;
    &lt;p&gt;Success! Using a Whatsapp SNI tricked BA into thinking Iâm âmessagingâ, which allowed the TLS tunnel to be established. Since I am connected to the server, to make sure it works I wrote an HTTP/1.1 request within the socket; using the host header of a real website on my NGINX instance&lt;/p&gt;
    &lt;code&gt;GET / HTTP/1.1
Host: saxrag.com

HTTP/1.1 200 OK
Server: nginx/1.18.0 (Ubuntu)
Date: Fri, 09 May 2025 19:14:46 GMT
Content-Type: text/html
Content-Length: 4968
Last-Modified: Wed, 09 Apr 2025 07:52:54 GMT
Connection: keep-alive
ETag: "67f62756-1368"
Cache-Control: no-cache
Accept-Ranges: bytes
&amp;lt;snip&amp;gt;
&lt;/code&gt;
    &lt;p&gt;I successfully managed to request and receive my homepage! All ~5KiB of it, not bad. Now the challenge was to extend this to browse any websiteâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Enemies to Lovers&lt;/head&gt;
    &lt;p&gt;Ok, my relationship with SNI isnât as cliche as that, and I think weâre still enemies. But this opens up some exciting opportunities to say the least. If I can convince BA that Iâm connecting to &lt;code&gt;wa.me&lt;/code&gt;, I can potentially do whatever I want over that connection (under the guise of âmessagingâ). So the requirments were:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Establish a TLS connection using the SNI &lt;code&gt;wa.me&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Tunnel arbitrary traffic through that connection&lt;/item&gt;
      &lt;item&gt;Do all this without actually owning / controlling the &lt;code&gt;wa.me&lt;/code&gt;domain&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From my past experiences w/ reverse-engineering etc., the most obvious way to do this seemed to be an HTTPS proxy. It had to be HTTPS specifically, since the connection to proxy was going to be what Iâd âfakeâ as Whatsapp. If the TLS handshake to the HTTPS proxy had the SNI &lt;code&gt;wa.me&lt;/code&gt; , BA should let it through, and then I can make the real requests I want via the proxy.&lt;/p&gt;
    &lt;p&gt;Unfortunately I was in the air, and without easy access to the internet to manage my servers and the like, I couldnât quite set all of this up; Iâd have to do that while on holiday and test it on the flight back. I could try and emulate the BA restrictions etc. while on thr ground, but I decided to YOLO it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Setup&lt;/head&gt;
    &lt;p&gt;I managed to find one of my VPSs that wasnât already using port 443. Letâs assume the public IP was &lt;code&gt;333.333.333.333&lt;/code&gt; (yes I know octets donât go beyond &lt;code&gt;0xFF&lt;/code&gt;, if you really want my IP check the screenshots below). I then setup an HTTP proxy on it using tinyproxy. However this just sets up a basic HTTP proxy, which was listening on &lt;code&gt;127.0.0.1:8080&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To add the TLS layer, I used stunnel. For the TLS setup of stunnel, I just generated some self-signed certs via openSSL using all defaults, except the common name (CN), for which I used &lt;code&gt;wa.me&lt;/code&gt;, since I wanted to try and ensure max compatibility (e.g. the client doesnât reject due to unexpected SNI vs. CN, or the server not knowing which cert to provide).&lt;/p&gt;
    &lt;code&gt;openssl req -nodes -newkey ed25519 -keyout ssl.key -x509 -days 365 -out ssl.crt
&lt;/code&gt;
    &lt;p&gt;UPDATE: actually, on the client I decided to ignore TLS errors (self-signed cert), and stunnel didnât care about SNI, so this (&lt;code&gt;CN&lt;/code&gt;) didnât matter too much. But for more legit use cases it definitely does!&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing it out&lt;/head&gt;
    &lt;p&gt;To just make sure the proxy worked as expected, I tried it via curl directly on the IP:&lt;/p&gt;
    &lt;code&gt;$ curl -x https://user:pass@333.333.333.333:443 ifconfig.co -v
*   Trying 333.333.333.333:443...
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
*  CAfile: /etc/ssl/certs/ca-certificates.crt
*  CApath: none
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.3 (IN), TLS change cipher, Change cipher spec (1):
* TLSv1.3 (IN), TLS handshake, Encrypted Extensions (8):
* TLSv1.3 (IN), TLS handshake, Certificate (11):
* TLSv1.3 (OUT), TLS alert, unknown CA (560):
* SSL certificate problem: self-signed certificate
* closing connection #0
curl: (60) SSL certificate problem: self-signed certificate
More details here: https://curl.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the webpage mentioned above.
&lt;/code&gt;
    &lt;p&gt;Of course! I just randomly generated the certs on my VPS, not signed by a âtrustedâ CA or anything. Well, we can tell cURL to ignore TLS errors for the proxy with the &lt;code&gt;--proxy-insecure&lt;/code&gt; flag, and now it works; the response is the IP of my VPS.&lt;/p&gt;
    &lt;p&gt;However thereâs a problem - if I connect to the proxy directly via the IP, there is no SNI extension set, so this would get blocked. The SNI extension is set when connection to a domain, so I need to configure &lt;code&gt;wa.me&lt;/code&gt; to point to &lt;code&gt;333.333.333.333&lt;/code&gt;. This can be done via the hosts file of course, but cURL also provides a quick CLI hack via &lt;code&gt;--resolve&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;curl --resolve wa.me:443:333.333.333.333 -x https://username:password@wa.me ifconfig.co --proxy-insecure -v
&lt;/code&gt;
    &lt;p&gt;This tells cURL how to resolve the IP. With this, I could now see the SNI being set to &lt;code&gt;wa.me&lt;/code&gt; via wireshark, and the connection to the proxy succeeding (TLS errors about the self-signed cert ignored of course). Not bad, now time to wait for my flight back to Hong Kongâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Testing it in-flight&lt;/head&gt;
    &lt;p&gt;If Iâd messed something up, I was cooked, since without internet access I wouldnât be able to fix it! My flight back was at 1935hrs local time, but Iâd been up since 0400 thanks to an early morning flight in from Edinburgh, and then spent the day browsing the markets, having pints and watching the Emilia Romagna Grand Prix. The place I went to even had screens above the urinals!&lt;/p&gt;
    &lt;p&gt;Anyway, despite being up for ~16 hours already, I was ready to see if my work would, well, work. Once we were sky-high, I connected to the WiFi (from my laptop), signed up for the BA loyalty program, and activated the âMessagingâ plan. Trying the curl command from above, I got back an HTTP 200 from &lt;code&gt;ifconfig.co&lt;/code&gt; with my VPS IP; it worked! For good measure I tried cURLing some more websites like example.com, google.com etc. to make sure stuff seemed fine.&lt;/p&gt;
    &lt;p&gt;The next challenge was to extend this to web browsing. Thankfully most modern browsers support sending traffic through an HTTPS proxy, and chromium even has a flag to disable TLS cert warnings (so it wonât complain about my self-signed cert, which obviously doesnât belong to the real &lt;code&gt;wa.me&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;I also had to set a DNS record for &lt;code&gt;wa.me&lt;/code&gt; to &lt;code&gt;333.333.333.333&lt;/code&gt; in my hosts file, so chromium would set the SNI to &lt;code&gt;wa.me&lt;/code&gt; in the TLS handshake, but the connection would be made to my VPS. Since the bandwidth would probably be quite limited (owing to not just the internet on an airplane, but proxying it via a VPS in Netherlands), I decided to load a very simple, text-heavy website: Hacker News.&lt;/p&gt;
    &lt;p&gt;Bada bing bada boom! Looks like we cooked. I can actually browse HN using BAâs free âmessagingâ WiFi! (Note: the reason you can see the HTTP requests in plaintext in wireshark is because I used SSLKEYLOGFILE and configured wireshark to decrypt TLS).&lt;/p&gt;
    &lt;p&gt;Unfortunately, trying to load websites with heavier assets would fail, with images on simple text blogs loading line-by-line. Well, at least its some dial-up nostalgia!&lt;/p&gt;
    &lt;p&gt;My guess is that on the free WiFi, apart from the SNI checks, they also throttle the bandwidth. Maybe they anticipated this kind of circumvention. On the other hand, if this is really the internet speed that the full plan unlocksâ¦&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: ECH&lt;/head&gt;
    &lt;p&gt;Earlier above I talked about work being done to fix the SNI leakage: ECH. The scope of explaining how it works is out of scope here, but I do encourage you to read up on it. Pretty good stuff! Itâll help this section make more sense.&lt;/p&gt;
    &lt;p&gt;I operate an ECH test website, so I decided to do some more setup before my flight. I basically created another ECHConfig, with the public_name set to &lt;code&gt;wa.me&lt;/code&gt;. Iâve a bit of a guide on how to do this btw, though it could do with some improvements.&lt;/p&gt;
    &lt;p&gt;Anyway, since ECH world, the public SNI is purely for the server to complete the outer ClientHello, and since ECH clients set the public SNI based on the ECHConfig, I can type in my real domain in firefox, which will still use the &lt;code&gt;wa.me&lt;/code&gt; domain as the public SNI. The inner Client Hello will then occur securely, containing the real SNI of &lt;code&gt;rfc5746.mywaifu.best&lt;/code&gt;, and the handshake will complete with the âlegitâ CA-signed certificate for that domain.&lt;/p&gt;
    &lt;p&gt;This worked as well, and without any TLS ignore flags, since the actual cert for &lt;code&gt;rfc5746.mywaifu.best&lt;/code&gt; was signed by a âtrusted CAâ (Letâs Encrypt). Whatâs more interesting is that this worked even on a non-standart TLS port: &lt;code&gt;7443&lt;/code&gt;! Not sure exactly why, but Iâm not complaining.&lt;/p&gt;
    &lt;head rend="h3"&gt;A note on ECHConfig resolution&lt;/head&gt;
    &lt;p&gt;Typically, ECHConfigs should be resolved via encrypted DNS, such as DNS-over-HTTPS. I believe this is what firefox does by default. I am not 100% sure if this is what happened while I was in flight, since Iâd think the DoH would be blocked on messaging WiFi? Or maybe they allow the DoH SNI as well, since newer phones default to that. If any of you are flying BA anytime soon, try it out and let me know!&lt;/p&gt;
    &lt;head rend="h2"&gt;SNI: Donât blindly trust it&lt;/head&gt;
    &lt;p&gt;SNI, as the name indicates (sorry) is just a âhintâ of sorts, from the client to the server. If someone controls both sides (client &amp;amp; server), they can put whatever fake value they want in here, for middleboxes to sniff out and try to analyze. While this unfortunately does work for applications like censorship (where an ISP or country is trying to block a particular website), for use cases such as threat detection it should not be relied on; malwre authors can âspoofâ the SNI when connecting to their C&amp;amp;C, since they donât actually need it, but it may look more innocent to middleboxes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Questions?&lt;/head&gt;
    &lt;p&gt;I would be happy to answer any questions you have! You can contact me via email, and please use my PGP key to encrypt all communications. (Backup E-Mail (PGP Key))&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45695134</guid><pubDate>Fri, 24 Oct 2025 14:40:34 +0000</pubDate></item><item><title>Code like a surgeon</title><link>https://www.geoffreylitt.com/2025/10/24/code-like-a-surgeon</link><description>&lt;doc fingerprint="355491dd119110ed"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;October 2025&lt;/head&gt;
    &lt;head rend="h1"&gt;Code like a surgeon&lt;/head&gt;
    &lt;p&gt;A lot of people say AI will make us all “managers” or “editors”…but I think this is a dangerously incomplete view!&lt;/p&gt;
    &lt;p&gt;Personally, I’m trying to code like a surgeon.&lt;/p&gt;
    &lt;p&gt;A surgeon isn’t a manager, they do the actual work! But their skills and time are highly leveraged with a support team that handles prep, secondary tasks, admin. The surgeon focuses on the important stuff they are uniquely good at.&lt;/p&gt;
    &lt;p&gt;My current goal with AI coding tools is to spend 100% of my time doing stuff that matters. (As a UI prototyper, that mostly means tinkering with design concepts.)&lt;/p&gt;
    &lt;p&gt;It turns out there are a LOT of secondary tasks which AI agents are now good enough to help out with. Some things I’m finding useful to hand off these days:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Before attempting a big task, write a guide to relevant areas of the codebase&lt;/item&gt;
      &lt;item&gt;Spike out an attempt at a big change. Often I won’t use the result but I’ll review it as a sketch of where to go&lt;/item&gt;
      &lt;item&gt;Fix typescript errors or bugs which have a clear specification&lt;/item&gt;
      &lt;item&gt;Write documentation about what I’m building&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I often find it useful to run these secondary tasks async in the background – while I’m eating lunch, or even literally overnight!&lt;/p&gt;
    &lt;p&gt;When I sit down for a work session, I want to feel like a surgeon walking into a prepped operating room. Everything is ready for me to do what I’m good at.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mind the autonomy slider&lt;/head&gt;
    &lt;p&gt;Notably, there is a huge difference between how I use AI for primary vs secondary tasks.&lt;/p&gt;
    &lt;p&gt;For the core design prototyping work, I still do a lot of coding by hand, and when I do use AI, I’m more careful and in the details. I need fast feedback loops and good visibility. (eg, I like Cursor tab-complete here)&lt;/p&gt;
    &lt;p&gt;Whereas for secondary tasks, I’m much much looser with it, happy to let an agent churn for hours in the background. The ability to get the job done eventually is the most important thing; speed and visibility matter less. Claude Code has been my go-to for long unsupervised sessions but Codex CLI is becoming a strong contender there too, possibly my new favorite.&lt;/p&gt;
    &lt;p&gt;These are very different work patterns! Reminds me of Andrej Karpathy’s “autonomy slider” concept. It’s dangerous to conflate different parts of the autonomy spectrum – the tools and mindset that are needed vary quite a lot.&lt;/p&gt;
    &lt;head rend="h2"&gt;Your agent doesn’t need a career trajectory&lt;/head&gt;
    &lt;p&gt;The “software surgeon” concept is a very old idea – Fred Brooks attributes it to Harlan Mills in his 1975 classic “The Mythical Man-Month”. He talks about a “chief programmer” who is supported by various staff including a “copilot” and various administrators. Of course, at the time, the idea was to have humans be in these support roles.&lt;/p&gt;
    &lt;p&gt;OK, so there is a super obvious angle here, that “AI has now made this approach economically viable where it wasn’t before”, yes yes… but I am also noticing a more subtle thing at play, something to do with status hierarchies.&lt;/p&gt;
    &lt;p&gt;A lot of the “secondary” tasks are “grunt work”, not the most intellectually fulfilling or creative part of the work. I have a strong preference for teams where everyone shares the grunt work; I hate the idea of giving all the grunt work to some lower-status members of the team. Yes, junior members will often have more grunt work, but they should also be given many interesting tasks to help them grow.&lt;/p&gt;
    &lt;p&gt;With AI this concern completely disappears! Now I can happily delegate pure grunt work. And the 24/7 availability is a big deal. I would never call a human intern at 11pm and tell them to have a research report on some code ready by 7am… but here I am, commanding my agent to do just that!&lt;/p&gt;
    &lt;head rend="h2"&gt;Notion is for surgeons?&lt;/head&gt;
    &lt;p&gt;Finally I’ll mention a couple thoughts on how this approach to work intersects with my employer, Notion.&lt;/p&gt;
    &lt;p&gt;First, as an employee, I find it incredibly valuable right now to work at a place that is bullish on AI coding tools. Having support for heavy use of AI coding tools, and a codebase that’s well setup for it, is enabling serious productivity gains for me – especially as a newcomer to a big codebase.&lt;/p&gt;
    &lt;p&gt;Secondly, as a product – in a sense I would say we are trying to bring this way of working to a broader group of knowledge workers beyond programmers. When I think about how that will play out, I like the mental model of enabling everyone to “work like a surgeon”.&lt;/p&gt;
    &lt;p&gt;The goal isn’t to delegate your core work, it’s to identify and delegate the secondary grunt work tasks, so you can focus on the main thing that matters.&lt;/p&gt;
    &lt;head rend="h2"&gt;Related reads&lt;/head&gt;
    &lt;p&gt;If you liked this perspective, you might enjoy reading these other posts I’ve written about the nature of human-AI collaboration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Enough AI copilots! We need AI HUDs: “anyone serious about designing for AI should consider non-copilot form factors that more directly extend the human mind…”&lt;/item&gt;
      &lt;item&gt;AI-generated tools can make programming more fun: “Instead, I used AI to build a custom debugger UI… which made it more fun for me to do the coding myself…”&lt;/item&gt;
      &lt;item&gt;ChatGPT as muse, not oracle: “What if we were to think of LLMs not as tools for answering questions, but as tools for asking us questions and inspiring our creativity?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45695621</guid><pubDate>Fri, 24 Oct 2025 15:25:17 +0000</pubDate></item><item><title>Harnessing America's heat pump moment</title><link>https://www.heatpumped.org/p/harnessing-america-s-heat-pump-moment</link><description>&lt;doc fingerprint="676f107f6a85b4ba"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Heat Pumped&lt;/item&gt;
      &lt;item&gt;Posts&lt;/item&gt;
      &lt;item&gt;Harnessing America’s Heat Pump Moment&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Harnessing America’s Heat Pump Moment&lt;/head&gt;
    &lt;head rend="h2"&gt;The tech works. The policy’s in place. So why are heat pumps still a hard sell?&lt;/head&gt;
    &lt;p&gt;Editor’s note: This is a guest post by Joseph DeNatale, an entrepreneur and project coordinator at Jetson Home. It originally appeared in Climate Drift earlier this year, and is republished on Heat Pumped with permission.&lt;/p&gt;
    &lt;p&gt;Joseph interviewed me when he was researching the piece, and I was excited to see that the final product touched many topics that I've been wanting to write about.&lt;/p&gt;
    &lt;p&gt;A big thank you to Joseph and Climate Drift for sharing with the Heat Pumped community - it's incredibly in-depth. Since there’s so much to digest, we’re splitting it up into 5 parts that we'll be sharing over the next few weeks.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Execution Is Everything: A Personal Perspective&lt;/head&gt;
    &lt;p&gt;As a small business owner, I’ve built a career not around inventing new things, but around making things happen: making sure systems run smoothly, projects get completed on time, and clients feel taken care of.&lt;/p&gt;
    &lt;p&gt;My work has been rooted in the real-world, hands-on, often chaotic rhythm of operations, logistics, and direct client service. Whether it’s organizing teams to execute live events, refining workflows to scale a growing business, or managing the delicate art of closing a sale, I’ve learned one simple truth: the hardest part is never the idea. It’s the execution.&lt;/p&gt;
    &lt;p&gt;So when I began diving into the world of home electrification—particularly heat pumps—that same truth surfaced again, just with higher stakes.&lt;/p&gt;
    &lt;p&gt;The technology isn’t the issue. In fact, the technology is there. It’s been there for decades, and it is continuing to improve. We’re not waiting on some magical breakthrough or futuristic device.&lt;/p&gt;
    &lt;p&gt;We’re waiting on people—mostly homeowners and home contractors, but also manufacturers and policy makers—to embrace, understand, and implement what already works.&lt;/p&gt;
    &lt;p&gt;This piece isn’t about reinventing the wheel. It’s about understanding why we’re not using the wheel we already have—and what it’s going to take, from the human side of the equation, to make heat pumps the obvious, accessible, and default choice for millions of American homes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Heat Pumps Aren’t New—But This Moment Is&lt;/head&gt;
    &lt;p&gt;In the world of climate solutions, it’s easy to get distracted by what’s shiny and new—sleek devices, breakthrough technologies, futuristic models of sustainability.&lt;/p&gt;
    &lt;p&gt;But not every climate solution is some new-fangled wonder gadget. Some of them already exist. Some of them are sitting in basements and behind houses, quietly doing the work.&lt;/p&gt;
    &lt;p&gt;The heat pump is one of them.&lt;/p&gt;
    &lt;p&gt;Heat pumps are not new. In fact, the idea has been around for well over a century, and the technology has been used widely for decades—mostly in Europe and Asia, but also in pockets of the U.S.—for everything from water heating to whole-home climate control.&lt;/p&gt;
    &lt;p&gt;Modern heat pumps are highly efficient—anywhere from 2-4x more efficient than a furnace—and capable of replacing both a furnace and an air conditioner with a single system in virtually every climate. For millions of homes across the country, they offer a cleaner, quieter, and more precise way to stay comfortable year-round.&lt;/p&gt;
    &lt;p&gt;Importantly, heat pumps have also been shown to match or beat the operating costs of even the cheapest heating option—natural gas—in many cases. This has been demonstrated through both local and national studies. One study showed that over 90% of American households would save on energy bills by replacing worn-out heating equipment with the right-sized heat pump.&lt;/p&gt;
    &lt;p&gt;Installation costs vary wildly depending on many factors in a home, but with the introduction of generous incentives via the Inflation Reduction Act (IRA) and additional state programs, even these costs can be on-par with fossil fuel alternatives.&lt;/p&gt;
    &lt;p&gt;So why aren’t they everywhere?&lt;/p&gt;
    &lt;p&gt;The answer isn’t technical. It’s cultural, economic, and human.&lt;/p&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Heat pumps are proven, efficient, and climate-friendly—but adoption is still slow.&lt;/p&gt;
    &lt;p&gt;The barrier isn’t the tech. It’s people:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Contractors who default to what they know&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Homeowners who need education and guidance&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A fragmented market full of noise and misinformation&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This piece discusses these challenges, and then explores five keys to accelerating adoption:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Educate homeowners so heat pumps feel familiar and trustworthy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Train the next-gen workforce and upskill legacy HVAC pros.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Leverage better tools and data to size and install systems right.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Prioritize quality and trust to build social proof and demand.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Align policy to phase out one-way ACs and normalize heat pumps.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Execution—not invention—is what will move the needle.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hold On.. What’s A Heat Pump Again?&lt;/head&gt;
    &lt;p&gt;If you’re reading this piece, you probably know what a heat pump is (and you can feel free to skip this section).&lt;/p&gt;
    &lt;p&gt;But if you’re among the uninitiated – like, believe it or not, most people – here’s a (very) quick primer. (Editor’s note: check out Heat Pumps 101 if you want to dive deeper)&lt;/p&gt;
    &lt;p&gt;A heat pump works by drawing thermal energy (heat) out of the atmosphere and “pumping” it into the home. This process works in reverse for cooling. (Source)&lt;/p&gt;
    &lt;head rend="h3"&gt;The 2-Way AC&lt;/head&gt;
    &lt;p&gt;The term “heat pump”, it turns out, is a fairly unhelpful name for most people. In fact, there are some leaders in the home electrification industry who believe the name itself is one of the barriers to adoption. It’s one of many ways that the heat pump is misunderstood.&lt;/p&gt;
    &lt;p&gt;Think of a heat pump as a “2-way AC.” An air conditioner cools your home by pulling heat from inside an enclosed space and transferring it outside. Your refrigerator works the same way.&lt;/p&gt;
    &lt;p&gt;A heat pump does the same thing, but can also reverse the process to bring heat into the home. It uses a few key components to make this happen:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;The fan pulls air across the system’s coils to help move heat in or out of the space.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The evaporator coil absorbs heat from the air inside your home (in cooling mode) or from the outside air (in heating mode).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The compressor pressurizes and moves a fluid called refrigerant through the system, enabling the heat transfer process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The refrigerant is the working fluid that captures and carries heat from one place to another—either out of your home or into it.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s important to understand is that a heat pump does not create heat. It also doesn’t create cold (cold is the absence of heat, just like darkness is the absence of light). A heat pump simply transfers – pumps! – heat from one place to another.&lt;/p&gt;
    &lt;p&gt;“The difference between a heat pump and a one-way AC is just one valve. It still works perfectly fine as an air conditioner—there’s no difference. That’s why we’ve started calling them “two-way ACs” as an education tool. It helps people compare a two-way AC, which has a reverse gear, with a one-way AC—which, in my mind, is basically broken.”&lt;/p&gt;
    &lt;p&gt;But what about in the winter when it’s below freezing? In any environment where the temperature is above absolute zero (remember the Kelvin scale?) there is still a significant amount of heat in the air in the form of thermal energy.&lt;/p&gt;
    &lt;p&gt;That’s why a heat pump can still heat your home even on the coldest day of the year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Heat Pumps Matter&lt;/head&gt;
    &lt;p&gt;The fact that heat pumps simply transfer heat—and do not create it—gives them the potential to heat homes without doing the thing that humans have done since time immemorial to keep warm: burn stuff.&lt;/p&gt;
    &lt;p&gt;In the U.S., over half of all homes still rely on burning fossil fuels for heat. Replacing those systems with electric, air source heat pumps (ASHPs) can significantly reduce household emissions, especially as the grid gets cleaner and moves towards a higher percentage of renewable energy (i.e. not burning stuff).&lt;/p&gt;
    &lt;p&gt;And, because they’re so efficient, heat pumps can lower operating costs over time—although this is highly dependent on where you live, as the cost of fuel and electricity varies widely. They’re also safer (no burning stuff), can improve indoor air quality (again, no burning), and create healthier, more comfortable homes.&lt;/p&gt;
    &lt;p&gt;Finally, heat pumps are a crucial component of an energy-independent home. Paired with solar panels and battery storage, a homeowner can heat and cool their home entirely with energy they generate on their own. Try that with a furnace!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Metric&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Gas Furnace&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Air-Source Heat Pump (ASHP)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Fuel Source&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Natural gas, propane, or oil&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Electricity&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Heating/Cooling&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Heating only&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Heats and cools (dual function)&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Air Quality&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Can introduce combustion byproducts; potential for CO&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;No combustion; generally better indoor air quality&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Health/Safety&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Risk of gas leaks, carbon monoxide&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;No combustion risk; safer for indoor environments&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Comfort&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Delivers blasts of hot air; on/off “short cycles”&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;More consistent, even heating/cooling with variable-speed options&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Initial Cost&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Typically lower (although the cost of a furnace + AC if replaced at the same time is often higher)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Often higher upfront, especially for cold-climate models. Costs can be lowered via incentive programs.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Operating Cost&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Depends on gas prices; cheaper where gas is low&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Can be lower, especially with efficient models + incentives and/or when paired with solar + battery storage&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Emissions&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Emits CO₂ and other GHGs&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Zero onsite emissions; cleaner with a green grid&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;p&gt;Climate Suitability&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Performs well in all climates&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Cold-climate models now perform down to -15 to -20°F&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Incentives/Rebates&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Limited (varies by region)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Significant federal/state incentives available&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is not a marginal climate solution. According to the IEA, global heat pump adoption could reduce carbon emissions by half a billion tons annually—roughly equivalent to the annual emissions of all cars in Europe.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Heat Pump Moment Has Arrived&lt;/head&gt;
    &lt;p&gt;For years, heat pumps were a niche topic, something discussed by green building enthusiasts, early adopters, or homeowners with unusually high energy awareness.&lt;/p&gt;
    &lt;p&gt;But that’s no longer the case. Here are four reasons why:&lt;/p&gt;
    &lt;head rend="h3"&gt;Cultural Momentum Is Building&lt;/head&gt;
    &lt;p&gt;The electrification movement is no longer a fringe concept. The push to “electrify everything” has gained traction among policymakers, climate advocates, startups, utilities, and even popular media.&lt;/p&gt;
    &lt;p&gt;From Substack newsletters to YouTube explainers, there’s growing awareness that building decarbonization—and especially heating and cooling—is one of the most practical, scalable ways for regular people to cut their emissions. Campaigns like Rewiring America’s “Go Electric” initiative frame heat pumps not just as energy-efficient appliances, but as a gateway to modern, climate-aligned homes.&lt;/p&gt;
    &lt;p&gt;This momentum is turning into real action. Heat pumps have now outsold gas furnaces in the U.S. every year since 2022.&lt;/p&gt;
    &lt;head rend="h3"&gt;Federal and State Policy Is Aligned (For Now)&lt;/head&gt;
    &lt;p&gt;For the time being (Republicans’ “One Big, Beautiful Bill” notwithstanding), both federal and state governments are backing this transition with significant financial and structural support. Editor’s note: Ouch. Since this piece was originally written, OBBB passed, and most tax credits at the federal level phase out at the end of this year. If you’ve been on the fence about getting a heat pump, now might be a good time to act!&lt;/p&gt;
    &lt;p&gt;The Inflation Reduction Act (IRA) has introduced a suite of rebates, tax credits, and grant programs designed to make heat pumps more affordable and accessible. Single-family households can receive up to $8,000 in upfront rebates for heat pump installations and up to $2,000 in federal tax credits, not to mention additional support for electrical panel upgrades and home energy audits. Editor’s note: the IRA rebates are federally funded, but implemented at the state level. Not all states are participating, and some that are haven’t rolled out their programs yet. In other states like California, the funds are already exhausted.&lt;/p&gt;
    &lt;p&gt;State and local governments are also leading the way in the transition away from fossil fuels on both the demand and supply sides. Programs like Efficiency Maine, TECH Clean California, and Mass Save offer generous incentives and no-interest financing to homeowners that drive the cost of electrification upgrades down even further. Meanwhile, New York City has banned gas in new construction, and Massachusetts has ordered public utilities to begin phasing out natural gas, a move which is being studied in at least 11 other states.&lt;/p&gt;
    &lt;head rend="h3"&gt;Private Capital Is Following&lt;/head&gt;
    &lt;p&gt;The heat pump space is no longer just a niche for contractors and utilities—it’s attracting serious private investment. VC-backed companies like Quilt are reimagining the user experience with sleek, design-forward equipment and app-based controls. Others, like Elephant Energy and Forge, are building “heat pump concierge” platforms that manage the customer journey end-to-end—from sales to install to rebate navigation.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Cold-Climate Performance Myth Has Been Fully Debunked&lt;/head&gt;
    &lt;p&gt;One of the biggest myths about heat pumps—that they can’t handle cold weather—is now being debunked at scale. While older, single-speed models may have struggled in colder temperatures, especially when size and installed incorrectly, modern cold-climate, variable-speed air-source heat pumps can provide reliable heating even at outdoor temperatures of -20°F.&lt;/p&gt;
    &lt;p&gt;These systems are already in use in northern New England, the upper Midwest, and Canada. In Nordic countries—some of the coldest climates in the word—the technology has been viable for decades.&lt;/p&gt;
    &lt;p&gt;And yet, despite all this momentum, heat pump adoption is still slow.&lt;/p&gt;
    &lt;p&gt;Why? Because the hardest part isn’t scaling the technology. It’s aligning the people—contractors, homeowners, policymakers, and market actors—who need to make it happen.&lt;/p&gt;
    &lt;p&gt;“We’ve had the technology dialed for 20, 30, 40 years, depending on how you’re arguing it—but it’s not being applied. It’s a human problem. It’s not a technical one. The technical one has been solved.”&lt;/p&gt;
    &lt;p&gt;That’s where we go next.&lt;/p&gt;
    &lt;p&gt;This is part 1 in a 5 part series about challenges and solutions in accelerating heat pump adoption across the US. Stay tuned for the next issue!&lt;/p&gt;
    &lt;head rend="h2"&gt;Want a heat pump in your own home?&lt;/head&gt;
    &lt;p&gt;The first Heat Pumped group buy generated lots of enthusiasm! There are still a handful of slots left, but you’ll have to act fast if you’re interested. Sign-ups close later this month (or when all the slots fill, whichever comes first).&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;head&gt;Do you want to participate in this group buy?&lt;/head&gt;
          &lt;p&gt;Fair &amp;amp; transparent heat pump pricing in the SF Bay Area and LA&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45698554</guid><pubDate>Fri, 24 Oct 2025 20:05:07 +0000</pubDate></item><item><title>The Swift SDK for Android</title><link>https://www.swift.org/blog/nightly-swift-sdk-for-android/</link><description>&lt;doc fingerprint="360e51139294ee0b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Announcing the Swift SDK for Android&lt;/head&gt;
    &lt;p&gt;Swift has matured significantly over the past decade — extending from cloud services to Windows applications, browser apps, and microcontrollers. Swift powers apps and services of all kinds, and thanks to its great interoperability, you can share code across platforms.&lt;/p&gt;
    &lt;p&gt;The Android workgroup is an open group, free for anyone to join, that aims to expand Swift to Android. Today, we are pleased to announce nightly preview releases of the Swift SDK for Android.&lt;/p&gt;
    &lt;p&gt;This milestone reflects months of effort by the Android workgroup, building on many years of grassroots community effort. With the SDK, developers can begin developing Android applications in Swift, opening new avenues for cross-platform development and accelerating innovation across the mobile ecosystem.&lt;/p&gt;
    &lt;p&gt;The Swift SDK for Android is available today, bundled with the Windows installer or downloadable separately for use on Linux or macOS.&lt;/p&gt;
    &lt;head rend="h2"&gt;Getting Started&lt;/head&gt;
    &lt;p&gt;We’ve published a Getting Started guide to help you set up your first native Swift code on an Android device. The Swift for Android Examples help demonstrate end‑to‑end application workflows on Android.&lt;/p&gt;
    &lt;p&gt;With the Swift SDK for Android, you can now start porting your Swift packages to Android. Over 25% of packages in the Swift Package Index already build for Android, and the Community Showcase now indicates Android compatibility.&lt;/p&gt;
    &lt;p&gt;The swift-java project enables you to interoperate between Java and Swift. It is both a library and a code generator, enabling you to integrate Swift and Java in both directions by automatically generating safe and performant bindings. To learn about generating bindings to bring your business logic to Android, check out the recent Swift Server Side meetup talk by Mads Odgaard.&lt;/p&gt;
    &lt;head rend="h2"&gt;Next Steps&lt;/head&gt;
    &lt;p&gt;This preview release opens many new opportunities to continue improving these tools. We encourage you to share your experiences, ideas, tools and apps on the Swift forums. This post has been published on an associated thread for discussion, and new posts can be shared in the Android category.&lt;/p&gt;
    &lt;p&gt;The Android workgroup is drafting a vision document, currently under review, for directing future work regarding Swift on Android. This vision will outline priority areas and guide community efforts to maximize impact across the ecosystem. In addition, we maintain a project board that tracks the status of major efforts, as well as official CI for the Swift SDK for Android.&lt;/p&gt;
    &lt;p&gt;If you’re as excited as we are, join us and help make this ecosystem even better!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45698570</guid><pubDate>Fri, 24 Oct 2025 20:06:52 +0000</pubDate></item><item><title>Study: MRI contrast agent causes harmful metal buildup in some patients</title><link>https://www.ormanager.com/briefs/study-mri-contrast-agent-causes-harmful-metal-buildup-in-some-patients/</link><description>&lt;doc fingerprint="eeda31d2584a08f1"&gt;
  &lt;main&gt;&lt;p&gt;Editor's Note&lt;/p&gt;&lt;p&gt;New research offers a potential explanation for why some patients retain toxic metals long after undergoing an MRI.&lt;/p&gt;&lt;p&gt;Published in the journal Magnetic Resonance Imaging, the findings show that gadolinium contrast agents used in MRI scans may react with common dietary compounds to form harmful metal nanoparticles in the body. As detailed in an April 7 Newsweek report on the study, gadolinium-based contrast agents are injected to sharpen MRI images and are typically excreted without causing harm. However, gadolinium particles have been found lingering in the brain, kidneys, blood, and urine years after exposure, and the US Food and Drug Administration links retained gadolinium to nephrogenic systemic fibrosis (NSF).&lt;/p&gt;&lt;p&gt;The study specifically identifies a chemical reaction between gadolinium and oxalic acid—a compound found naturally in foods and produced in the body after ingesting vitamin C—as a likely contributor, Newsweek reports. Lab tests showed oxalic acid caused gadolinium to separate from its chelating agent and form nanoparticles capable of infiltrating cells in various organs.&lt;/p&gt;&lt;p&gt;Lead author Dr Brent Wagner told Newsweek he personally avoids vitamin C when undergoing MRI with contrast, citing its potential to increase gadolinium reactivity. “Metabolic milieu,” including high oxalic acid levels, could explain why some individuals experience severe symptoms while others do not, he said.&lt;/p&gt;&lt;p&gt;According to the article, nearly half of the patients found to have gadolinium traces in the body had received the contrast agent only once, suggesting that individual biology—not dosage—may influence risk. Dr Wagner theorized that nanoparticle formation could trigger a disproportionate immune response, with affected cells sending distress signals that intensify the body’s reaction.&lt;/p&gt;&lt;p&gt;The research team is now building an international patient registry to further study gadolinium accumulation. According to the article, the registry will collect blood, urine, hair, and fingernail samples to help identify individuals at greatest risk and understand long-term retention patterns.&lt;/p&gt;Read More &amp;gt;&amp;gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45698909</guid><pubDate>Fri, 24 Oct 2025 20:48:46 +0000</pubDate></item><item><title>Dead soldiers' teeth reveal diseases that doomed Napoleon's army</title><link>https://www.washingtonpost.com/science/2025/10/24/napoleons-army-diseases-ancient-dna-teeth/</link><description>&lt;doc fingerprint="9bbe08afcd469a02"&gt;
  &lt;main&gt;
    &lt;p&gt;Contents Close What is Intelligence? Foreword Preface Introduction Origins Abiogenesis Symbiogenesis Reproductive Functions Life as Computation Artificial Life Thermodynamics Dynamic Stability Complexification Virality Compression Embodiment Daisyworld Élan Vital Survival Being in Time Batting Average (No) Things in Themselves Anthropic Principle The Umwelt Within Latent Variables Modeling Learning by Evolving Cause by Effect Goodness and Truth Are Feelings Real? Interlude The Prehistory of Computation Cybernetics Love and War Killer App Behavior, Purpose, and Teleology Negative Feedback How We Know Universals Perceptrons Deep Learning Closing the Loop Learning Unkneading Transfer Green Screen Grandmother Cell Final Causes Meathead Neuromodulators Bootstrapping Beyond Reward Other Minds Forking Paths Children of Time Sphexish Matryoshka Dolls Intelligence Explosion Crew of Eight Homunculus Illusion and Reality Many Worlds Au Revoir Will What You Will What It Is Like to Be Weird Entanglement Zombie-Free Alters M-I-B The Interpreter Multifractal Boundaries Ourselves Block Diagram Recurrence Efference Copy Phenomenality Blindsight Subbasement Neocortex Social Neuroscience Transformers Language Sequence to Sequence Prediction Is All You Need Semantic Cosmology Alignment Attention But Is It Neuroscience? No Introspection Step by Step Generality Single System Hive Mind Modalities Pure Speech Babel Fish Testament Long Tails In-Context Learning Mary’s Room Parity Check As If Interlude No Perfect Heroes or Villains Evolutionary Transition Periodization Transitions Vulnerability Pecking Order Economics X-Risk Free Lunch Utility Big Tent Limits to Growth Tears of Joy Beyond Alignment Acknowledgments About the Author The Antikythera Book Series Glossary Bibliography Foreword Blaise Agüera y Arcas Lessons from AI About Evolution, Computing, and Minds ' Foreword&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45699285</guid><pubDate>Fri, 24 Oct 2025 21:28:39 +0000</pubDate></item><item><title>What is intelligence? (2024)</title><link>https://whatisintelligence.antikythera.org/</link><description>&lt;doc fingerprint="9bbe08afcd469a02"&gt;
  &lt;main&gt;
    &lt;p&gt;Contents Close What is Intelligence? Foreword Preface Introduction Origins Abiogenesis Symbiogenesis Reproductive Functions Life as Computation Artificial Life Thermodynamics Dynamic Stability Complexification Virality Compression Embodiment Daisyworld Élan Vital Survival Being in Time Batting Average (No) Things in Themselves Anthropic Principle The Umwelt Within Latent Variables Modeling Learning by Evolving Cause by Effect Goodness and Truth Are Feelings Real? Interlude The Prehistory of Computation Cybernetics Love and War Killer App Behavior, Purpose, and Teleology Negative Feedback How We Know Universals Perceptrons Deep Learning Closing the Loop Learning Unkneading Transfer Green Screen Grandmother Cell Final Causes Meathead Neuromodulators Bootstrapping Beyond Reward Other Minds Forking Paths Children of Time Sphexish Matryoshka Dolls Intelligence Explosion Crew of Eight Homunculus Illusion and Reality Many Worlds Au Revoir Will What You Will What It Is Like to Be Weird Entanglement Zombie-Free Alters M-I-B The Interpreter Multifractal Boundaries Ourselves Block Diagram Recurrence Efference Copy Phenomenality Blindsight Subbasement Neocortex Social Neuroscience Transformers Language Sequence to Sequence Prediction Is All You Need Semantic Cosmology Alignment Attention But Is It Neuroscience? No Introspection Step by Step Generality Single System Hive Mind Modalities Pure Speech Babel Fish Testament Long Tails In-Context Learning Mary’s Room Parity Check As If Interlude No Perfect Heroes or Villains Evolutionary Transition Periodization Transitions Vulnerability Pecking Order Economics X-Risk Free Lunch Utility Big Tent Limits to Growth Tears of Joy Beyond Alignment Acknowledgments About the Author The Antikythera Book Series Glossary Bibliography Foreword Blaise Agüera y Arcas Lessons from AI About Evolution, Computing, and Minds ' Foreword&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45700663</guid><pubDate>Sat, 25 Oct 2025 01:21:43 +0000</pubDate></item><item><title>Key IOCs for Pegasus and Predator Spyware Removed with iOS 26 Update</title><link>https://iverify.io/blog/key-iocs-for-pegasus-and-predator-spyware-cleaned-with-ios-26-update</link><description>&lt;doc fingerprint="f60d4907cab1114d"&gt;
  &lt;main&gt;
    &lt;p&gt;Blog&lt;/p&gt;
    &lt;head rend="h1"&gt;Key IOCs for Pegasus and Predator Spyware Cleaned With iOS 26 Update&lt;/head&gt;
    &lt;p&gt;By Matthias Frielingsdorf, VP of Research&lt;/p&gt;
    &lt;p&gt;Oct 21, 2025&lt;/p&gt;
    &lt;p&gt;As iOS 26 is being rolled out, our team noticed a particular change in how the operating system handles the shutdown.log file: it effectively erases crucial evidence of Pegasus and Predator spyware infections. This development poses a serious challenge for forensic investigators and individuals seeking to determine if their devices have been compromised at a time when spyware attacks are becoming more common.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;The Power of the shutdown.log&lt;/head&gt;
    &lt;p&gt;For years, the shutdown.log file has been an invaluable, yet often overlooked, artifact in the detection of iOS malware. Located within the Sysdiagnoses in the Unified Logs section (specifically, Sysdiagnose Folder -&amp;gt; system_logs.logarchive -&amp;gt; Extra -&amp;gt; shutdown.log), it has served as a silent witness to the activities occurring on an iOS device, even during its shutdown sequence.&lt;/p&gt;
    &lt;p&gt;In 2021, the publicly known version of Pegasus spyware was found to leave discernible traces within this shutdown.log. These traces provided a critical indicator of compromise, allowing security researchers to identify infected devices. However, the developers behind Pegasus, NSO Group, are constantly refining their techniques, and by 2022 Pegasus had evolved.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Pegasus's Evolving Evasion Tactics&lt;/head&gt;
    &lt;p&gt;While still leaving evidence in the shutdown.log, their methods became more sophisticated. Instead of leaving obvious entries, they began to completely wipe the shutdown.log file. Yet, even with this attempted erasure, their own processes still left behind subtle traces. This meant that even a seemingly clean shutdown.log that began with evidence of a Pegasus sample was, in itself, an indicator of compromise. Multiple cases of this behavior were observed until the end of 2022, highlighting the continuous adaptation of these malicious actors.&lt;/p&gt;
    &lt;p&gt;Following this period, it is believed that Pegasus developers implemented even more robust wiping mechanisms, likely monitoring device shutdown to ensure a thorough eradication of their presence from the shutdown.log. Researchers have noted instances where devices known to be active had their shutdown.log cleared, alongside other IOCs for Pegasus infections. This led to the conclusion that a cleared shutdown.log could serve as a good heuristic for identifying suspicious devices.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Predator's Similar Footprint&lt;/head&gt;
    &lt;p&gt;The sophisticated Predator spyware, observed in 2023, also appears to have learned from the past. Given that Predator was actively monitoring the shutdown.log, and considering the similar behavior seen in earlier Pegasus samples, it is highly probable that Predator, too, left traces within this critical log file.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;iOS 26: An Unintended Cleanse&lt;/head&gt;
    &lt;p&gt;With iOS 26 Apple introduced a changeâeither an intentional design decision or an unforeseen bugâthat causes the shutdown.log to be overwritten on every device reboot instead of appended with a new entry every time, preserving each as its own snapshot. This means that any user who updates to iOS 26 and subsequently restarts their device will inadvertently erase all evidence of older Pegasus and Predator detections that might have been present in their shutdown.log.&lt;/p&gt;
    &lt;p&gt;This automatic overwriting, while potentially intended for system hygiene or performance, effectively sanitizes the very forensic artifact that has been instrumental in identifying these sophisticated threats. It could hardly come at a worse time - spyware attacks have been a constant in the news and recent headlines show that high-power executives and celebrities, not just civil society, are being targeted.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Identifying Pegasus 2022: A Specific IOC&lt;/head&gt;
    &lt;p&gt;For those still on iOS versions prior to 26, a specific IOC for Pegasus 2022 infections involved the presence of a /private/var/db/com.apple.xpc.roleaccountd.staging/com.apple.WebKit.Networking entry within the shutdown.log. This particular IOC also revealed a significant shift in NSO Group's tactics: they began using normal system process names instead of easily identifiable, similarly named processes, making detection more challenging.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Correlating Logs for Deeper Insight (&amp;lt; iOS 18)&lt;/head&gt;
    &lt;p&gt;For devices running iOS 18 or earlier, a more comprehensive approach to detection involved correlating containermanagerd log entries with shutdown.log events. Containermanagerd logs contain boot events and can retain data for several weeks. By comparing these boot events with shutdown.log entries, investigators could identify discrepancies. For example, if numerous boot events were observed before shutdown.log entries, it suggested that something was amiss and potentially being hidden.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;lb/&gt;Before You Update&lt;/head&gt;
    &lt;p&gt;Given the implications of iOS 26's shutdown.log handling, it is crucial for users to take proactive steps:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Before updating to iOS 26, immediately take and save a sysdiagnose of your device. This will preserve your current shutdown.log and any potential evidence it may contain.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Consider holding off on updating to iOS 26 until Apple addresses this issue, ideally by releasing a bug fix that prevents the overwriting of the shutdown.log on boot.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;More Blogs&lt;/head&gt;
    &lt;head rend="h3"&gt;Get Our Latest Blog Posts Delivered Straight to Your Inbox&lt;/head&gt;
    &lt;p&gt;Subscribe to our blog to receive the latest research and industry trends delivered straight to your inbox. Our blog content covers sophisticated mobile threats, unpatched vulnerabilities, smishing, and the latest industry news to keep you informed and secure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45700946</guid><pubDate>Sat, 25 Oct 2025 02:31:55 +0000</pubDate></item><item><title>React vs. Backbone in 2025</title><link>https://backbonenotbad.hyperclay.com/</link><description>&lt;doc fingerprint="a7f21d051da72e1d"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;15 Years of Progress&lt;/head&gt;
    &lt;p&gt;Look at the two implementations above. The code is roughly the same length. They do exactly the same thing. One was written with a framework from 2010, the other with a framework that's had countless developer hours and a massive ecosystem behind it for over a decade.&lt;/p&gt;
    &lt;p&gt;The interesting part is not how much better React is—it's how little progress we've actually made.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Illusion of Simplicity&lt;/head&gt;
    &lt;p&gt;React looks cleaner. It reads better at first glance. But that readability comes at a cost: you're trading explicit simplicity for abstraction complexity.&lt;/p&gt;
    &lt;p&gt;The Backbone code is brutally honest about what it's doing. An event fires, a handler runs, you build some HTML, you put it in the DOM. It's verbose, sure, but there's no mystery. A junior developer can trace exactly what happens and when. The mental model is straightforward: "when this happens, do this."&lt;/p&gt;
    &lt;p&gt;The React code hides a lot. And once you move past simple examples, you hit problems that don't make sense until you understand React's internals.&lt;/p&gt;
    &lt;p&gt; Your input mysteriously clears itself. Turns out you switched a list item's key from a stable ID to an index, so React thinks it's a completely different component and remounts it, wiping state. Or maybe you forgot that &lt;code&gt;value&lt;/code&gt; can't be &lt;code&gt;undefined&lt;/code&gt;—React saw it flip from uncontrolled to controlled and reset the input.
        &lt;/p&gt;
    &lt;p&gt; You add a &lt;code&gt;useEffect&lt;/code&gt; to fetch data, and suddenly your app is stuck in an infinite loop. The dependency array includes an object that gets recreated every render, so React thinks it changed and runs the effect again. Now you need &lt;code&gt;useMemo&lt;/code&gt; and &lt;code&gt;useCallback&lt;/code&gt; sprinkled everywhere to "stabilize identities," which is a thing you never had to think about before.
        &lt;/p&gt;
    &lt;p&gt; Your click handler sees old state even though you just set it. That's a stale closure—the function captured the value from when it was created, and later renders don't magically update it. You either need to put the state in the dependency array (creating a new handler every time) or use functional updates like &lt;code&gt;setState(x =&amp;gt; x + 1)&lt;/code&gt;. Both solutions feel like workarounds.
        &lt;/p&gt;
    &lt;head rend="h3"&gt;Magic Has a High Price&lt;/head&gt;
    &lt;p&gt;These aren't edge cases. They're normal problems you hit building moderately complex apps. And debugging them requires understanding reconciliation algorithms, render phases, and how React's scheduler batches updates. Your code "just works" without you needing to understand why it works, which is nice until it breaks.&lt;/p&gt;
    &lt;p&gt;People say "you need to rebuild React from scratch to really understand it," and they're right. But that's kind of damning, isn't it? You shouldn't need to understand virtual DOM diffing, scheduling priorities, and concurrent rendering to build a password validator.&lt;/p&gt;
    &lt;p&gt;Backbone might be tedious, but it doesn't lie to you. jQuery is hackable. You can view source, understand it, and add to it easily. It's just DOM methods. React's abstraction layers make that much harder.&lt;/p&gt;
    &lt;head rend="h3"&gt;So, What's Next?&lt;/head&gt;
    &lt;p&gt;We understand the problem: event + state = UI. That's it. That's what both of these implementations are solving.&lt;/p&gt;
    &lt;p&gt;For massive apps with 1,000 components on the same page, maybe React's complexity is justified. But what the other 99% of apps? What about small apps that just want to do a job and don't need all the magic?&lt;/p&gt;
    &lt;p&gt;Is there a better model? Something feels as hard and steady as the DOM, but still feels intuitive to write? Something hackable like Backbone and jQuery were, where you can pop open devtools and understand what's happening?&lt;/p&gt;
    &lt;p&gt;— panphora&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45702558</guid><pubDate>Sat, 25 Oct 2025 09:43:54 +0000</pubDate></item><item><title>Making a micro Linux distro (2023)</title><link>https://popovicu.com/posts/making-a-micro-linux-distro/</link><description>&lt;doc fingerprint="579b191fd9a42fad"&gt;
  &lt;main&gt;
    &lt;p&gt;In this article, we’ll talk about building up a tiny (micro) Linux “distribution” from scratch. This distribution really won’t do much, but it will be built from scratch.&lt;/p&gt;
    &lt;p&gt;We will build the Linux kernel on our own, and write some software to package our micro-distro.&lt;/p&gt;
    &lt;p&gt;Lastly, we are doing this example on the RISC-V architecture, specifically QEMU’s &lt;code&gt;riscv64 virt&lt;/code&gt; machine. There’s very little in this article that is specific to this architecture, so you might as well do an almost identical exercise for other architectures like &lt;code&gt;x86&lt;/code&gt;. We recently went through the RISC-V boot process with SBI and bare metal programming for RISC-V, so this is just a continuation up the software stack.&lt;/p&gt;
    &lt;p&gt;Warning: This article is a very simplified view of a Linux distribution. There are things written below that are not 100% accurate, but more like 99.9%. This article is meant for beginners and helping them form a basic mental framework for understanding Linux systems. More advanced users may be triggered by over-simplification in some parts.&lt;/p&gt;
    &lt;head rend="h2"&gt;Table of contents&lt;/head&gt;
    &lt;head&gt;Open Table of contents&lt;/head&gt;
    &lt;head rend="h2"&gt;What is an OS kernel?&lt;/head&gt;
    &lt;p&gt;Let’s assume we’re working on a single-core machine. They’re still around us, maybe not in our laptops and phones, but in some smaller devices, and historically they have been actually widely used even in our “big” personal devices like desktops. The latter ones have been capable of running multiple program simultaneously for many years, even as single cores. We’ll get into what simultaneous really means in a bit, but for now let’s just note that the one of the operating system kernel’s big tasks is to make that happen.&lt;/p&gt;
    &lt;p&gt;If you go back to the articles about bare metal programming and SBI on RISC-V, you can see how at the lowest layers of software we interact with our I/O devices. It usually (most often, but not necessarily always) boils down to the CPU writing some data at the appropriate address. Imagine if the application developers had to keep all these addresses in mind and they had to know which values exactly to send to those addresses! That would mean we’d have far fewer applications today, but we don’t, and that’s owing to the operating system kernels which abstract away these details and provide some simple high-level interfaces instead. In the RISC-V SBI article, we looked at an example of such interface for Linux on &lt;code&gt;x86&lt;/code&gt; — instead of knowing which addresses to write to and what values to send there, we focused on the logic and basically just told to the OS kernel that “we want message so and so written to the standard output”, and then the OS kernel dealt with the details of interacting with the hardware. So that’s another big task for the OS kernel: managing the hardware on the machine and making the interaction with it easier.&lt;/p&gt;
    &lt;p&gt;Going further, the OS kernel offers some really high-level programming interfaces like the filesystems. This may or may not be about managing some hardware and abstracting operations over it. For example, the most common case for the filesystems, of course, is to store some data on the disk and retrieve it later, and this has to do with the OS kernel managing the hardware related to disks on the machine (i.e. sending some data to certain addresses, which makes those hard disk devices respond in some way). However, this is not always the case, the files are not always some data stored on disk, and so filesystem is an interface exposed to us, meaning it’s a way of talking to the OS kernel, not necessarily a way to talk to the data. We’ll cover the filesystems in great detail in some other article, but let’s keep this in mind for now — the OS kernel needs to provide a straightforward way of doing high-level things through multiple interfaces.&lt;/p&gt;
    &lt;p&gt;Finally, the last thing I wanted to cover about the kernels is that they provide a programming model. Remember how we mentioned (as I’m sure you already know) that multiple programs can run even on a single-core device simultaneously? The OS enables the running applications to be programmed to not even know about each other, in other words, an application can live its lifecycle acting like it is the only application running on the computer and no one else is touching its memory. Imagine a world where your Python Django server needs to know about that texting app on your device in order to be working — we’d have far fewer Django apps and texting apps, for sure, as coding them would quickly get gnarly. However, the apps can also know about each other’s existence on the same machine. The operating system kernel facilitates both. It gives a programming model in which you can insulate applications from each other, or join a few apps in isolation from other apps, etc.&lt;/p&gt;
    &lt;p&gt;Basically, the OS kernel does a lot of heavy lifting to enable you to run your code easily on a very generic and complicated machinery such as your smartphone. What is written above probably doesn’t do full justice to the kernels, they do a whole lot of things, but the few paragraphs above should give a fairly good idea of kernel’s main tasks, and there are many.&lt;/p&gt;
    &lt;p&gt;Linux is an extremely popular operating system kernel. It can be built to run on many architectures (really, a lot), it is open source and free to use. And a lot of people are “Linux users”, but what does it exactly mean that someone “uses Linux”? Those Linux users typically install something like Debian, or Ubuntu on their machines, and they use Linux that way, and what does that mean?&lt;/p&gt;
    &lt;head rend="h2"&gt;What is a Linux distribution?&lt;/head&gt;
    &lt;p&gt;We talked above about what kernels do, i.e. what are their tasks and we said Linux is an OS kernel, but can we really just take bare Linux and as end users who just want to watch YouTube, do something with it? The answer is likely no, we need a lot more layers on top of Linux to get to firing up a Chrome browser and watching YouTube.&lt;/p&gt;
    &lt;p&gt;How to go all the way towards the top of the software stack where we can just use those super simple and intuitive apps like graphical web browsers? We have previously discussed the boot process, and we went all the way from the very first operations on the machine after the power on, to the moment we land in the operating system kernel. We did not cover the bootloaders in any detail, we just briefly mentioned them because we were able to get QEMU to directly load our fake kernel into the memory in one go, which is typically not possible with full blown systems like desktop Linux (there is an intermediate booting stage where the bootloader fetches the OS image from something like a disk, or maybe even network and loads it into the memory). The kernel we wrote was a fake little stub that does effectively nothing, and so we ended our last article at the point where the OS kernel is in memory and ready to go, it’s just we had no kernel to run.&lt;/p&gt;
    &lt;p&gt;Based on what we see above, I think the right mental model for the kernel right now is that it is the infrastructure for running user applications on a complex machine, but it really doesn’t do anything for the user’s business logic. This is what I meant when I said the bare Linux on its own cannot fire up Chrome and let you watch YouTube — it is merely the infrastructure that the application developer uses to implement Chrome, and its streaming capabilities.&lt;/p&gt;
    &lt;p&gt;However, the kernel alone is not infrastructure for Chrome to run. We need to run sort of “infrastructure on top of infrastructure” to achieve the full infrastructure to run Chrome. Again, much like in the SBI article, we’re just layering abstractions on top of each other in some way, so essentially there is nothing new here, just the way we do it.&lt;/p&gt;
    &lt;p&gt;For example, in order for a machine to connect to the Internet, the OS kernel first needs to be able to drive the network device on the machine to send the signals out of the machine (to the switch, router, another machine or whatever it is connected to). However, in Linux, there is more or less where the kernel stops. Which networks you connect to, are you using VPN, how do you assign IPs to your machine (statically or dynamically) and that kind of business, it happens in the upper layers of the infrastructure.&lt;/p&gt;
    &lt;p&gt;You may now guess where this is going — a Linux distribution is really the Linux kernel plus the infrastructure on top of the kernel infrastructure. Let’s dig into it.&lt;/p&gt;
    &lt;head rend="h2"&gt;How does “infrastructure on top of infrastructure” run?&lt;/head&gt;
    &lt;p&gt;Again, the kernel does a whole bunch of things, a million times more than what we can cover in a single article, but it definitely has its limits and it doesn’t do all the heavy lifting on your everyday personal device — and this is where something outside of the kernel gets into the picture.&lt;/p&gt;
    &lt;p&gt;Disclaimer: You can get really creative with Linux in a million different ways, and from this point on we’re going with a very basic, textbook-like, simple view of what happens in the mainstream distributions. There are many super complex things we can do, and there are lots of details we’re leaving out, but my hope here is that you get a general idea and enough knowledge to be able to understand more advanced material on this topic; there is plenty of it on the Internet.&lt;/p&gt;
    &lt;p&gt;The reason why I wrote the disclaimer above is mainly because we’re going to be assuming that your Linux has a filesystem going forward, as this is the most common path. How many times have you seen a Linux deployment without a filesystem? It certainly seems possible to do, but it may be borderline useless except for some super edge/advanced cases, and we’ll disregard them in this article. Check out this page to get more idea of what I’m talking about.&lt;/p&gt;
    &lt;p&gt;So what is the stuff outside the kernel? It’s what we call the user code! It’s just a normal code that runs within the Linux environment, just like you run basically anything on your Linux machine. Sure, some code is more privileged than the other, and there are a million more details that can get involved, but let’s just focus on the main distinction here: when you are running Linux on a machine, there is kernel code running, as well as the user code running, and everything that’s a part of the kernel itself is running in the kernel space, and everything that is running on the machine that is not a part of the kernel is running in the user space, and they are fairly isolated from each other.&lt;/p&gt;
    &lt;p&gt;So this “infrastructure on top of infrastructure” that we have talked about runs in the user space. Sure, it needs to bubble down to the kernel for many primitives, and we’ve seen already how that happens. Linux has a well defined ABI that exposes a set of services that the user space code can invoke in the kernel space. And where does this user space code come into the picture?&lt;/p&gt;
    &lt;head rend="h2"&gt;The &lt;code&gt;init&lt;/code&gt; process (and its “children”)&lt;/head&gt;
    &lt;p&gt;Once the kernel is done loading and making itself comfortable on the machine, it kicks off the first bit of the code in user space — the &lt;code&gt;init&lt;/code&gt; process. This is a piece of user space code that lives in a binary that sits somewhere on your filesystem, and the kernel will look for it in a few locations, beginning with &lt;code&gt;/init&lt;/code&gt; (if it doesn’t find it there, it will give a few more shots at different locations before throwing its hands up). Let’s say the kernel found a binary in the filesystem at &lt;code&gt;/init&lt;/code&gt; — it’s going to start it and assign the ID &lt;code&gt;1&lt;/code&gt;. This is basically the only user process that the kernel will start: the &lt;code&gt;init&lt;/code&gt; process then is the ancestor of all other user space processes. This means that &lt;code&gt;init&lt;/code&gt; will start some other processes, these other processes will in turn start some other processes, and so on. Very shortly you have a bunch of processes running on your machine, hopefully each one of them useful for the desired operations on the machine. The machine should at this point start actively interacting with the world around it: whether we’re talking about a smartphone giving the UI to its user, an embedded device that collects data off the sensors and sending it into the cloud, etc. Additionally, the machine will often have various tools available that are not actively running on the machine, but can be invoked in certain situations for some high level operations (e.g. a Python script can invoke a couple of tools like &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;cat&lt;/code&gt; or something to get a snapshot of what’s going on with the machine and then sending the data somewhere). Quick note is that even these periodically-started or ad-hoc tools are in some way descendants of &lt;code&gt;init&lt;/code&gt;; it’s not too important to know now, but it’s good to keep in mind.&lt;/p&gt;
    &lt;p&gt;The collection of kernel, the processes that get launched right after the kernel, and the tools that are available at your disposal represent the Linux distribution. It’s essentially a packaging for the kernel alongside all these useful tools that do more around the machine than what the kernel alone does (but it still provides the infrastructure for everything outside of the kernel to run, nothing bypasses the kernel).&lt;/p&gt;
    &lt;p&gt;Even a distrbution minimally useful for everyday use can get crufty pretty quickly. If you go onto the path of building your own custom little distro, as we actually will now, you will almost inevitably hit a lot of roadblocks where something that you expect to be working is just not working and the full solution is either to code some of your own software to talk to the kernel to get something done on the system, or just use an off-the-shelf software to do so. The latter is the path of least resistance, and you’ll likely keep adding stuff until you end up with a deployment that can do something remotely useful for you. At this point, you will have likely accumulated a significant number of software packages.&lt;/p&gt;
    &lt;p&gt;On the other hand, you have probably heard people criticizing certain distributions as being “bloated”, probably meaning they accumulated so much complexity in their packaging, they waste a lot of hardware resources doing things that are not useful, etc. Without discipline, I can easily see distrbution developers just randomly throwing different tools at the system just to get that one missing thing going, without retroactively cleaning up the excess later and just moving onto the next feature where they do the same — a (sadly) common pattern in software engineering.&lt;/p&gt;
    &lt;p&gt;Some distributions draw the line at different places where they just make a decision for the user and do something on the system, versus letting the user make the full decision and be more hands on. For example, you can install Arch Linux in a minimal way where it’s just a little more than the kernel booted up with a shell. All the subsequent decisions are on you, and you have to be very hands on in order to get it to a point where it’s very graphical and highly interactive. Or you can decide it’s just not worth your time setting it up so much, and just install a very user-friendly Ubuntu distrbution, which may be “bloated” for someone’s taste, but it gets you up and running very fast (I personally like it).&lt;/p&gt;
    &lt;head rend="h2"&gt;Building our almost useless Linux micro distrbibution&lt;/head&gt;
    &lt;p&gt;Let’s get our hands dirty and build something that’s basically useless but we’ll actually end up booting it for real. You may want to refresh your memory on the RISC-V boot process, I think it will be rewarding here.&lt;/p&gt;
    &lt;p&gt;First things first, let’s build the kernel.&lt;/p&gt;
    &lt;head rend="h3"&gt;Building a Linux operating system for RISC-V&lt;/head&gt;
    &lt;p&gt;I’m on an &lt;code&gt;x86&lt;/code&gt; platform here, so I will depend heavily on the cross-platform toolchain to build things for RISC-V. You will likely do something similar (I’m not sure I have yet seen someone build the RISC-V kernel on RISC-V itself).&lt;/p&gt;
    &lt;p&gt;Let’s get the source code for Linux. Linux development is done on top of the Git version control system, but we’ll take a shortcut here and just download a tarball with the sources for one branch, we won’t be syncing the whole Linux codebase with all the Git branches, experimental stuff and so on. We’ll be downloading the tarball from &lt;code&gt;kernel.org&lt;/code&gt; for version &lt;code&gt;6.5.2&lt;/code&gt; (here). You can also just download any tarball for whatever the latest stable version is from kernel.org homepage. Once it’s downloaded, go ahead and unpack that. Let’s also &lt;code&gt;cd&lt;/code&gt; into that directory.&lt;/p&gt;
    &lt;p&gt;Now is the time to configure the build. The first step is to make the &lt;code&gt;defconfig&lt;/code&gt; which basically initiates your configuration file.&lt;/p&gt;
    &lt;p&gt;Note: Here and below, you may want to use a different &lt;code&gt;CROSS_COMPILE&lt;/code&gt; prefix, depending on how the cross compilation tool is identified on your machine&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- defconfig&lt;/code&gt;
    &lt;p&gt;This was hopefully quick and the &lt;code&gt;.config&lt;/code&gt; file should be generated. The config file should contain a lot of IDs for individual configurations and the values for those, very often in yes/no format (e.g. &lt;code&gt;CONFIG_FOO=y&lt;/code&gt; or &lt;code&gt;CONFIG_FOO=n&lt;/code&gt;). You could edit the file manually, but I personally wouldn’t recommend it, especially as a beginner (I don’t consider myself an expert at this either). A better way to edit this is through the &lt;code&gt;curses&lt;/code&gt;-based pseudo-interface. You can get there by running&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- menuconfig&lt;/code&gt;
    &lt;p&gt;This interface has a few benefits.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;You have a more readable, folder-like overview of the configs.&lt;/item&gt;
      &lt;item&gt;There are insights into dependencies between the configs, i.e. it may only make sense to be able to enable config &lt;code&gt;foo&lt;/code&gt;if&lt;code&gt;bar&lt;/code&gt;and&lt;code&gt;baz&lt;/code&gt;are also enabled.&lt;/item&gt;
      &lt;item&gt;This interface has a search feature, activated by pressing the &lt;code&gt;/&lt;/code&gt;button (I don’t think you’ll get far by searching there in natural language; my way of getting around here is by searching on Google and finding which exactly config key am I looking for, for example&lt;code&gt;CONFIG_TTY_PRINTK&lt;/code&gt;). When you find what you’re looking for, hit the button you see in the parentheses.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We won’t be tweaking anything here for now, let’s just exit and move on.&lt;/p&gt;
    &lt;p&gt;It’s time to build the kernel! Quick note here, the make process famously has the &lt;code&gt;-j&lt;/code&gt; flag, which basically sets the concurrency in the build process, meaning it allows the build process to run a few things simultaneously. If you want to build faster, but not sure what to do, count the number of cores, and if it’s something like 8, just pass the flag &lt;code&gt;-j8&lt;/code&gt; below, as so. I will run the command like this (I’m on a 16-core machine):&lt;/p&gt;
    &lt;code&gt;make ARCH=riscv CROSS_COMPILE=riscv64-linux-gnu- -j16&lt;/code&gt;
    &lt;p&gt;This can take some time, though for the RISC-V build, it shouldn’t take awfully long, but I would expect at least a few minutes.&lt;/p&gt;
    &lt;p&gt;Once this is done, you will probably see something like this near the very bottom:&lt;/p&gt;
    &lt;code&gt;OBJCOPY arch/riscv/boot/Image&lt;/code&gt;
    &lt;p&gt;and this is the file we will be feeding to QEMU.&lt;/p&gt;
    &lt;p&gt;Great, let’s fire up QEMU!&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image&lt;/code&gt;
    &lt;p&gt;Switching to the UART view, we see that OpenSBI tidily started and the Linux took over! Great! We even see some references to the SBI layer that we have discussed before:&lt;/p&gt;
    &lt;code&gt;[    0.000000] Linux version 6.5.2 (uros@uros-debian-desktop) (riscv64-linux-gnu-gcc (Debian 10.2.1-6) 10.2.1 20210110, GNU ld (GNU Binutils for Debian) 2.35.2) #1 SMP Mon Sep 11 00:45:40 PDT 2023
[    0.000000] Machine model: riscv-virtio,qemu
[    0.000000] SBI specification v0.2 detected
[    0.000000] SBI implementation ID=0x1 Version=0x8
[    0.000000] SBI TIME extension detected
[    0.000000] SBI IPI extension detected
[    0.000000] SBI RFENCE extension detected&lt;/code&gt;
    &lt;p&gt;After reading about the boot process, we should now have a full understanding of what is going on here. This happened super early in the boot phase. There is a lot happening in these logs, and I’ll highlight a few things:&lt;/p&gt;
    &lt;code&gt;[    0.000000] riscv: base ISA extensions acdfim&lt;/code&gt;
    &lt;p&gt;Seems like Linux is capable of dynamically figuring out the capability of the underlying RISC-V hardware. I’m not sure what exactly is the mechanism behind it, could it be somehow passed through the device tree that we mentioned in the previous article, or something in the ISA itself tells this to the kernel, I’m not sure.&lt;/p&gt;
    &lt;code&gt;[    0.000000] Kernel command line:&lt;/code&gt;
    &lt;p&gt;This is interesting, a kernel has a command line? Turns out that the kernel, much like your everyday binaries, has startup flags. The kernel bootloader usually sets those up — after all, it knows how to fire up the kernel, and this could simply be a part of the starting process. With QEMU, remember, we’re sort of short circuiting the whole bootloader thing, and with passing the &lt;code&gt;-kernel&lt;/code&gt; flag, we let QEMU also wear the bootloader hat here by loading the kernel image into the memory and starting it up. QEMU actually has a flag called &lt;code&gt;-append&lt;/code&gt; with which you can append to this kernel command line. The command line itself is baked into the config file under &lt;code&gt;Boot options&lt;/code&gt; somewhere, I leave it to the reader to search for it, and the QEMU flag basically lets you adjust it with a VM launch, instead of having to rebuild the kernel to tweak the command line. In this case, the command line is just blank by default.&lt;/p&gt;
    &lt;code&gt;[    0.003376] printk: console [tty0] enabled&lt;/code&gt;
    &lt;p&gt;I guess this means that &lt;code&gt;printk&lt;/code&gt; will now write to &lt;code&gt;tty0&lt;/code&gt;? &lt;code&gt;printk&lt;/code&gt; is basically a way to write out messages from the kernel space. Remember, your typical &lt;code&gt;printf&lt;/code&gt; from C’s &lt;code&gt;stdio.h&lt;/code&gt; is meant for running in the user space, not kernel space, so kernel space must have its own solution, and it is &lt;code&gt;printk&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;[    0.211634] Serial: 8250/16550 driver, 4 ports, IRQ sharing disabled
[    0.221544] 10000000.uart: ttyS0 at MMIO 0x10000000 (irq = 12, base_baud = 230400) is a 16550A
[    0.222659] printk: console [ttyS0] enabled&lt;/code&gt;
    &lt;p&gt;Great, Linux knows there is UART at &lt;code&gt;0x10000000&lt;/code&gt;, just like we established before. Linux can now choose whether to use the SBI interface to drive the UART, or talk to it directly (if the S-mode allows it on that machine, that is). On many platforms, the OS can disregard that a lower level software like BIOS may offer to interact with the hardware, and from what I hear, this actually indeed happens a lot.&lt;/p&gt;
    &lt;p&gt;There’s also a lot of other stuff in the kernel logs:&lt;/p&gt;
    &lt;code&gt;[    0.250030] SuperH (H)SCI(F) driver initialized&lt;/code&gt;
    &lt;p&gt;I don’t think we need this? I guess we can go back to the kernel config and not bake this driver into the kernel and thus slim the kernel down. What we’re building here is a generic build, really. We didn’t customize anything and presumably the authors of the default config thought this is a reasonable default that should just run on a lot of different setups, so they probably included a lot of things to be on the safe side. If you’re working on smaller hardware, with less generous memory, CPU, etc. you do have to carefully choose what gets baked into the kernel and what doesn’t.&lt;/p&gt;
    &lt;p&gt;Additionally, this generic build is smart enough to figure out that the console should go to the right UART device, which is really handy for us. Otherwise, we’d probably have to do a bunch of configs like making sure TTY (let’s not overfocus on what this is now) is enabled, we want to enable printing to UART as the kernel boots, etc. All this is basically configurable in the &lt;code&gt;menuconfig&lt;/code&gt; interface.&lt;/p&gt;
    &lt;p&gt;We’ll keep it simple in this article, and we won’t customize anything in the kernel unless we have to.&lt;/p&gt;
    &lt;head rend="h4"&gt;First obstacles&lt;/head&gt;
    &lt;p&gt;Scrolling down closer to the bottom of the output, we see this:&lt;/p&gt;
    &lt;code&gt;[    0.330411] /dev/root: Can't open blockdev
[    0.330743] VFS: Cannot open root device "" or unknown-block(0,0): error -6
[    0.330984] Please append a correct "root=" boot option; here are the available partitions:
[    0.331648] List of all bdev filesystems:
[    0.331785]  ext3
[    0.331803]  ext2
[    0.331882]  ext4
[    0.331950]  vfat
[    0.332028]  msdos
[    0.332098]  iso9660
[    0.332181]
[    0.332405] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0)
[    0.332756] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 6.5.2 #1
[    0.333018] Hardware name: riscv-virtio,qemu (DT)
[    0.333248] Call Trace:
[    0.333442] [&amp;lt;ffffffff8000537a&amp;gt;] dump_backtrace+0x1c/0x24
[    0.333940] [&amp;lt;ffffffff808890f8&amp;gt;] show_stack+0x2c/0x38
[    0.334138] [&amp;lt;ffffffff80894a48&amp;gt;] dump_stack_lvl+0x3c/0x54
[    0.334318] [&amp;lt;ffffffff80894a74&amp;gt;] dump_stack+0x14/0x1c
[    0.334493] [&amp;lt;ffffffff80889500&amp;gt;] panic+0x102/0x29e
[    0.334683] [&amp;lt;ffffffff80a015c6&amp;gt;] mount_root_generic+0x1e8/0x29c
[    0.334891] [&amp;lt;ffffffff80a0186c&amp;gt;] mount_root+0x1f2/0x224
[    0.335108] [&amp;lt;ffffffff80a01a68&amp;gt;] prepare_namespace+0x1ca/0x222
[    0.335320] [&amp;lt;ffffffff80a010c8&amp;gt;] kernel_init_freeable+0x23e/0x262
[    0.335539] [&amp;lt;ffffffff80896264&amp;gt;] kernel_init+0x1e/0x10a
[    0.335714] [&amp;lt;ffffffff800034c2&amp;gt;] ret_from_fork+0xa/0x1c
[    0.336208] ---[ end Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0) ]---&lt;/code&gt;
    &lt;p&gt;Whoops, we crashed! The kernel has fallen into a panic.&lt;/p&gt;
    &lt;p&gt;Remember how we talked that pretty much always Linux needs a filesystem to be useful and how all the “infrastructure on top of infrastructure” is in the user space? Well, we didn’t really pass anything related to the filesystem explicitly and we surely didn’t pass any user space code to serve as the &lt;code&gt;init&lt;/code&gt;, though we didn’t even get to the latter.&lt;/p&gt;
    &lt;p&gt;You might imagine that the filesystem needs to be on a disk, but that’s not necessarily the case. We’ll talk some other time about filesystems in great detail, but you can really have a filesystem be backed by RAM memory too. And this is actually very often used by Linux, most notably in the boot up phase. When the kernel gets to where it crashed for us just now, in a normal, typical situation, it will find the whole, fully functional filesystem actually loaded into the RAM. If this confuses you, just think about it this way — a disk is just a bunch of bytes, just like RAM is, though RAM is faster but much smaller; conceptually they’re basically the same. Who and how loads this memory?&lt;/p&gt;
    &lt;p&gt;One way is to bake the filesystem directly into the kernel image. In this case, as the kernel loads, so does the initial, memory-backed filesystem, and our system would be ready to go if we had done that. If you don’t want to bulk up your kernel image and you want your initial filesystem to be loaded by some other means, like through a bootloader or something, then you package it separately. In QEMU case, we can shortcircuit things a little bit again, and make it wear a few more hats — we’ll make it also load the initial filesystem into the memory as well. If you’re interested in building the filesystem into the kernel, read the discussion here and try it as an exercise after you’re done with this guide.&lt;/p&gt;
    &lt;p&gt;This initial filesystem has a name: &lt;code&gt;initramfs&lt;/code&gt;. You’ll often hear it called &lt;code&gt;initrd&lt;/code&gt; too (I imagine &lt;code&gt;rd&lt;/code&gt; is short for ramdisk?). The latter is how QEMU takes in the filesystem for loading (&lt;code&gt;-initrd&lt;/code&gt; flag).&lt;/p&gt;
    &lt;p&gt;The filesystem is packaged as a &lt;code&gt;cpio&lt;/code&gt; archive, which is conceptually similar to &lt;code&gt;tar&lt;/code&gt;, but it’s not the same binary format. Short discussion can be read here.&lt;/p&gt;
    &lt;head rend="h4"&gt;Building the &lt;code&gt;initramfs&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The only real requirement for the &lt;code&gt;initramfs&lt;/code&gt; from the kernel is that it has a binary it can start up as the &lt;code&gt;init&lt;/code&gt; process, and the first place where the kernel will look for it is at the filesystem root, so the path is &lt;code&gt;/init&lt;/code&gt;. If you have absolutely nothing else on your filesystem, it’s questionably useful, but this is the bare requirement. Let’s start by writing the &lt;code&gt;init&lt;/code&gt; process in C. This process can be really anything, Linux won’t stop you from writing a useless &lt;code&gt;init&lt;/code&gt;, it will happily just execute it. We can go with a ‘hello world’ then?&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main(int argc, char *argv[]) {
  printf("Hello world\n");
  return 0;
}&lt;/code&gt;
    &lt;p&gt;Great, now let’s package it up into a &lt;code&gt;cpio&lt;/code&gt; archive.&lt;/p&gt;
    &lt;code&gt;riscv64-linux-gnu-gcc -static -o init init.c
cpio -o -H newc &amp;lt; file_list.txt &amp;gt; initramfs.cpio&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;file_list.txt&lt;/code&gt; has a single line:&lt;/p&gt;
    &lt;code&gt;init&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We’re building a static binary because we do not want to dynamically depend on the standard C library. The filesystem won’t have it, we’re making a filesystem with &lt;code&gt;init&lt;/code&gt;alone.&lt;/item&gt;
      &lt;item&gt;Linux expects the &lt;code&gt;initramfs&lt;/code&gt;archive to be built with the&lt;code&gt;-H newc&lt;/code&gt;flag.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Let’s run QEMU.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /PATH/TO/NEWLY_BUILT/initramfs.cpio&lt;/code&gt;
    &lt;p&gt;The kernel stil falls into a panic, but a different one!&lt;/p&gt;
    &lt;code&gt;[    0.351894] Run /init as init process
Hello world
[    0.379006] Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000000
[    0.379360] CPU: 0 PID: 1 Comm: init Not tainted 6.5.2 #1
[    0.379597] Hardware name: riscv-virtio,qemu (DT)
[    0.379812] Call Trace:
[    0.380005] [&amp;lt;ffffffff8000537a&amp;gt;] dump_backtrace+0x1c/0x24
[    0.380724] [&amp;lt;ffffffff808890f8&amp;gt;] show_stack+0x2c/0x38
[    0.380906] [&amp;lt;ffffffff80894a48&amp;gt;] dump_stack_lvl+0x3c/0x54
[    0.381095] [&amp;lt;ffffffff80894a74&amp;gt;] dump_stack+0x14/0x1c
[    0.381283] [&amp;lt;ffffffff80889500&amp;gt;] panic+0x102/0x29e
[    0.381447] [&amp;lt;ffffffff80013fd0&amp;gt;] do_exit+0x760/0x766
[    0.381623] [&amp;lt;ffffffff80014154&amp;gt;] do_group_exit+0x24/0x70
[    0.381806] [&amp;lt;ffffffff800141b8&amp;gt;] __wake_up_parent+0x0/0x20
[    0.382009] [&amp;lt;ffffffff80895482&amp;gt;] do_trap_ecall_u+0xe6/0xfa
[    0.382218] [&amp;lt;ffffffff8000337c&amp;gt;] ret_from_exception+0x0/0x64
[    0.382808] ---[ end Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000000 ]---&lt;/code&gt;
    &lt;p&gt;I guess this just means &lt;code&gt;init&lt;/code&gt; shouldn’t finish, so it should be easy to fix? Let’s just make it print something every 10 seconds and never stop. Important to note: our output worked, we see a “Hello world” string!&lt;/p&gt;
    &lt;p&gt;We’ll write a new &lt;code&gt;init&lt;/code&gt;, but let’s also make our &lt;code&gt;initramfs&lt;/code&gt; a little more complex too. Let’s remember how we said that &lt;code&gt;init&lt;/code&gt; starts up all the other processes on the machine. Wouldn’t it be nice if we actually had some sort of a shell? After all, that’s what we typically have with Linux — shells go well with Linux. We’ll build a useless shell, the one that just tells us what we asked it to do (echoes back the input).&lt;/p&gt;
    &lt;p&gt;Let’s first write the &lt;code&gt;init&lt;/code&gt; process. Before it begins looping and printing something every 10 seconds, it has an important job of spawning our “little shell”. The way a process can spawn another process in Linux is through 2 operations: &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;exec&lt;/code&gt;. &lt;code&gt;fork&lt;/code&gt; will start a new process by literally cloning the current process at the moment of &lt;code&gt;fork&lt;/code&gt;. The way the underlying code can differentiate the “parent” and “child” processes after that is by checking the return value of the &lt;code&gt;fork&lt;/code&gt; operation. If it is 0, this means the process is the child process, and it’s a parent otherwise (-1 is returned in an error case).&lt;/p&gt;
    &lt;p&gt;Next, it’s not useful for us here to just keep executing the &lt;code&gt;init&lt;/code&gt; program in 2 different processes. That’s where one of the many &lt;code&gt;exec&lt;/code&gt; operations come into the picture. When I say there are many &lt;code&gt;exec&lt;/code&gt; operations available on Linux, I mean there are &lt;code&gt;execl&lt;/code&gt;, &lt;code&gt;execlp&lt;/code&gt;, &lt;code&gt;execle&lt;/code&gt;, etc. Take a look at more documentation here, please. We’re going with &lt;code&gt;execl&lt;/code&gt; here, and the first parameter is which binary do we want to launch. We’ll package our fake shell as the &lt;code&gt;little_shell&lt;/code&gt; binary on the root. The rest of the parameters do not really matter (as evidenced by the value of the second parameter). More important, the mechanism of this operation is that we’re calling into the kernel to take whatever is running in the current process and replace it with the program that is loaded for execution from the binary listed as the first parameter. This is how programs get launched on Linux and when you’re working in your Bash shell, and you end up launching a program, this is what happens — a sequence of &lt;code&gt;fork&lt;/code&gt; and &lt;code&gt;exec&lt;/code&gt;-style calls.&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;
#include &amp;lt;unistd.h&amp;gt;

int main(int argc, char *argv[]) {
  pid_t pid = fork();

  if (pid == -1) {
    printf("Unable to fork!");
    return -1;
  }

  if (pid == 0) {
    // This is a child process.
    int status = execl("/little_shell", "irrelevant", NULL);

    if (status == -1) {
      printf("Forked process cannot start the little_shell");
      return -2;
    }
  }

  int count = 1;

  while (1) {
    printf("Hello from the original init! %d\n", count);
    count++;
    sleep(10);
  }

  return 0;
}&lt;/code&gt;
    &lt;p&gt;We build the &lt;code&gt;init&lt;/code&gt; the same way as we did before:&lt;/p&gt;
    &lt;code&gt;riscv64-linux-gnu-gcc -static -o init init.c&lt;/code&gt;
    &lt;p&gt;For the “shell” we’re building, I want to get a little more creative. Why don’t we write this one in Go instead of old school C?&lt;/p&gt;
    &lt;code&gt;package main

import (
	"bufio"
	"fmt"
	"os"
)

func main() {
	fmt.Println("Hello world from Go!")

	reader := bufio.NewReader(os.Stdin)

	for {
		fmt.Print("Enter your command: ")
		line, _ := reader.ReadString('\n')
		fmt.Printf("Your command is: %s", line)
	}
}&lt;/code&gt;
    &lt;p&gt;I am able to cross compile this to RISC-V out-of-the-box with my &lt;code&gt;go&lt;/code&gt; compiler.&lt;/p&gt;
    &lt;code&gt;GOOS=linux GOARCH=riscv64 go build little_shell.go&lt;/code&gt;
    &lt;p&gt;Nice thing that I really like about Go is that it’s very easy to reference other remote repositories on GitHub to include libraries, and things get neatly packaged up statically. I’m not going to lie, the &lt;code&gt;little_shell&lt;/code&gt; Go binary is pretty thick, weighing in at 1.9M on my machine, compared to only 454K for the statically-linked simple init, but in the days of desktops/laptops/phones with hundreds of GB of storage, if you’re building a distro for these kinds of devices, you may want to consider the tradeoff.&lt;/p&gt;
    &lt;p&gt;Note, there are situations where you may not be able to simply run your Go binary just like that on top of a bare kernel, it could start throwing Go panics all over the place. In order to run Go, you need to build your kernel with the right features in it, futex support feature being one of them (I think I’ve identified only 2 in my past experience). If you encounter any problems running the Go applications and you suspect you may not have the right kernel support, carefully read through the panics and you will be able to identify what is missing. Good news here is that the default config for the RISC-V kernel is good enough for running Go.&lt;/p&gt;
    &lt;p&gt;Let’s update our &lt;code&gt;file_list.txt&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;init
little_shell&lt;/code&gt;
    &lt;p&gt;Pack it all up again:&lt;/p&gt;
    &lt;code&gt;cpio -o -H newc &amp;lt; file_list.txt &amp;gt; initramfs.cpio&lt;/code&gt;
    &lt;p&gt;Let’s run it!&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /PATH/TO/NEWLY_BUILT/initramfs.cpio&lt;/code&gt;
    &lt;code&gt;[    0.356314] Run /init as init process
Hello from the original init! 1
Hello world from Go!
Enter your command: [[[mkdir hello]]]
Your command is: mkdir hello
Enter your command: [[[ls]]]
Your command is: ls
Enter your command: Hello from the original init! 2
[[[echo 123]]
Your command is: echo 123
Enter your command: [[[exit]]]
Your command is: exit
Enter your command: Hello from the original init! 3
[[[I give up!]]]
Your command is: I give up!&lt;/code&gt;
    &lt;p&gt;The bits in this console excerpt enclosed with triple square brackets are my user-provided input over UART. You can see 3 things interleaved on the UART&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Original &lt;code&gt;init&lt;/code&gt;’s period output every 10 seconds.&lt;/item&gt;
      &lt;item&gt;Output from the &lt;code&gt;little_shell&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Input from the user.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We are using the sole UART device on the virtual machine for all this, but that is not the only reason why everything is mixed up here. &lt;code&gt;init&lt;/code&gt; process prints to the standard output, just like &lt;code&gt;little_shell&lt;/code&gt; does, and you may not be aware of it, but any sort of print on Linux is a print to an open file. Standard output, as far as Linux knows, is a file that is opened by a process and you are printing to the standard output by writing to that file. When we &lt;code&gt;fork&lt;/code&gt;-ed the &lt;code&gt;little_shell&lt;/code&gt; from &lt;code&gt;init&lt;/code&gt;, the &lt;code&gt;little_shell&lt;/code&gt; inherited the open files from &lt;code&gt;init&lt;/code&gt;. So they are literally sharing all the standard input and output streams. Even if we had multiple I/O devices that we used on this machine, they’d still be sending outputs over to the same output stream. When &lt;code&gt;init&lt;/code&gt; was started, its standard output was set to produce content over to UART, and this behavior was simply inherited by the &lt;code&gt;little_shell&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;And there we have it, we have a pretty useless, but home-made Linux distribution! Go ahead and send it over to your friends! :)&lt;/p&gt;
    &lt;p&gt;Jokes aside, you can make an exercise out of this and implement some sort of a mini shell out of this &lt;code&gt;little_shell&lt;/code&gt;. Instead of just echoing back the commands given to it, you could make it actually understand what &lt;code&gt;mkdir&lt;/code&gt; is. You can even have it fork off a process to execute that elsewhere. Sky is the limit, you’re in the Linux userspace!&lt;/p&gt;
    &lt;p&gt;Let’s just step back a little and see if Linux kernel achieved the initial few promises for us:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;It’s abstracting away the hardware. Our&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;and our shell didn’t know anything about the UART. All they knew was they’re writing to some Linux file handle. It happens to be mapped to something abstract in the Linux kernel that invokes the UART driver in the Linux kernel, which may or may not use the SBI under the hood (I have honestly not verified if the kernel removes its dependence on SBI after it boots).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;It offers some high-level programming paradigms, like filesystems. Our&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;process located the other binary through the filesystem (the path was trivial, the binary was right in the root, but still, the paradigm is there).&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;There is a pretty clean isolation between the processes running. Once the shell was forked off from the&lt;/p&gt;&lt;code&gt;init&lt;/code&gt;, the processes were basically running independently. The memory was not shared between them and they didn’t have to worry about each other’s memory layout. They did share something else, though, like the file handles, but this is a consequence of how they were launched into running. Linux enables you to actually change some of this behavior, e.g. you can set up some shared memory between the processes, if you explicitly want to.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;There are many other things the kernel does for us, but let’s just stop here for now and appreciate this. It may not look like a lot, but the kernel gives us a pretty solid, portable infrastructure with which we can develop high level software while often disregarding the complexities of the underlying machine.&lt;/p&gt;
    &lt;head rend="h3"&gt;So what is an operating system?&lt;/head&gt;
    &lt;p&gt;This is now a game of words in my opinion. In my view, what matters is that the reader now has an understanding of what Linux as the kernel is, what “infrastructure” it offers, and what is running in the user space and what is running in the kernel space.&lt;/p&gt;
    &lt;p&gt;Some people may call the kernel itself an operating system, some people will refer to the whole distribution as the operating system, or they may come up with something completely different. I hope that at this point you have a good understanding of what is happening on a machine once Linux is started and where the responsibilities of each component end (or you can at least imagine the boundaries on a more complex system).&lt;/p&gt;
    &lt;p&gt;I hope this was useful!&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus section: making an actually useful micro distribution with &lt;code&gt;u-root&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;I thought about wrapping up here, but it wouldn’t make for a flashy demo. Why don’t we instead boot into something that’s actually useful, meaning that you can do things you would typically do on a Linux-based system, like run your &lt;code&gt;ls&lt;/code&gt;, &lt;code&gt;mkdir&lt;/code&gt;, &lt;code&gt;echo&lt;/code&gt; and whatnot. Let’s stick with the kernel we have previously built, and add some useful “infrastructure on top of infrastructure” in the user space domain to make the whole machine more useful.&lt;/p&gt;
    &lt;p&gt;I really like the u-root project for this.&lt;/p&gt;
    &lt;p&gt;Note: The title of their project mentions Go bootloaders, and this may stump you because as a careful reader, you know that Go programs are not really something you can run on bare metal. These bootloaders are somewhat exotic userspace bootloaders, meaning that they will actually run on top of a live Linux kernel, and then use this amazing Linux mechanism called &lt;code&gt;kexec&lt;/code&gt; to re-load a different kernel into the memory from user space. We won’t be using these bootloaders for now, we’ll just focus on the other user space goodies they have available, but I thought a quick paragraph here would help the confused readers.&lt;/p&gt;
    &lt;p&gt;The reason why I like the &lt;code&gt;u-root&lt;/code&gt; project is because it’s so insanely easy to use. Its usage is a bit creative though, so there are really 2 steps here:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install &lt;code&gt;u-root&lt;/code&gt;per their instructions. You should end up with a&lt;code&gt;u-root&lt;/code&gt;binary in your&lt;code&gt;PATH&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Now to actually generate a functional &lt;code&gt;initramfs&lt;/code&gt;with&lt;code&gt;u-root&lt;/code&gt;, the easiest way is to clone their Git repo and&lt;code&gt;cd&lt;/code&gt;your way into the directory that you just cloned. From there, you can cross-compile a fully functional user space set of tools with a single command.&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;git clone https://github.com/u-root/u-root.git
cd u-root
GOOS=linux GOARCH=riscv64 u-root&lt;/code&gt;
    &lt;p&gt;I get a few lines of output, the last being:&lt;/p&gt;
    &lt;code&gt;18:31:31 Successfully built "/tmp/initramfs.linux_riscv64.cpio" (size 14827284).&lt;/code&gt;
    &lt;p&gt;And that’s really it, this &lt;code&gt;cpio&lt;/code&gt; file can now be just ran with QEMU and you’ll boot right into a shell! Go through the &lt;code&gt;u-root&lt;/code&gt; documentation to understand how you can customize this &lt;code&gt;initramfs&lt;/code&gt; image you get, including what sort of changes you can make to the &lt;code&gt;init&lt;/code&gt; process behavior, but I think the default setup is so amazing to explore with.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /tmp/initramfs.linux_riscv64.cpio&lt;/code&gt;
    &lt;p&gt;Wow, this booted really smoothly! Providing the bottom of the UART output.&lt;/p&gt;
    &lt;code&gt;[    0.400269] Run /init as init process
2023/09/12 01:34:33 Welcome to u-root!
                              _
   _   _      _ __ ___   ___ | |_
  | | | |____| '__/ _ \ / _ \| __|
  | |_| |____| | | (_) | (_) | |_
   \__,_|    |_|  \___/ \___/ \__|
&lt;/code&gt;
    &lt;p&gt;And as you can see by the little &lt;code&gt;/#&lt;/code&gt; prompt, you’re actually in a shell! &lt;code&gt;u-root&lt;/code&gt;’s &lt;code&gt;init&lt;/code&gt; forked off a shell process and gave it the control over the UART.&lt;/p&gt;
    &lt;code&gt;/# ls
bbin
bin
buildbin
dev
env
etc
go
init
lib
lib64
proc
root
sys
tcz
tmp
ubin
usr
var
/# pwd
/
/# echo "Hello world!"
Hello world!&lt;/code&gt;
    &lt;p&gt;This little shell that &lt;code&gt;u-root&lt;/code&gt; gives even supports Tab-completion! I will say I have encountered some hiccups occassionally with it, it’s definitely not your full blown Bash, but it’s more than just a toy.&lt;/p&gt;
    &lt;p&gt;The standard tools like &lt;code&gt;ls&lt;/code&gt; seem to be taking the standard flags:&lt;/p&gt;
    &lt;code&gt;/# ls -lah
dtrwxrwxrwx root 0 420 B  Sep 12 01:35 .
drwxr-xr-x  root 0 2.1 kB Jan  1 00:00 bbin
drwxr-xr-x  root 0 80 B   Jan  1 00:00 bin
drwxrwxrwx  root 0 40 B   Sep 12 01:34 buildbin
drwxr-xr-x  root 0 12 kB  Sep 12 01:34 dev
drwxr-xr-x  root 0 40 B   Sep 12 01:35 directory
drwxr-xr-x  root 0 40 B   Jan  1 00:00 env
drwxr-xr-x  root 0 80 B   Sep 12 01:34 etc
drwxrwxrwx  root 0 60 B   Sep 12 01:34 go
Lrwxrwxrwx  root 0 9 B    Jan  1 00:00 init -&amp;gt; bbin/init
drwxrwxrwx  root 0 40 B   Sep 12 01:34 lib
drwxr-xr-x  root 0 40 B   Jan  1 00:00 lib64
dr-xr-xr-x  root 0 0 B    Sep 12 01:34 proc
drwx------  root 0 40 B   Sep 11 07:43 root
dr-xr-xr-x  root 0 0 B    Sep 12 01:34 sys
drwxr-xr-x  root 0 40 B   Jan  1 00:00 tcz
dtrwxrwxrwx root 0 60 B   Sep 12 01:34 tmp
drwxr-xr-x  root 0 40 B   Jan  1 00:00 ubin
drwxr-xr-x  root 0 60 B   Jan  1 00:00 usr
drwxr-xr-x  root 0 60 B   Jan  1 00:00 var&lt;/code&gt;
    &lt;head rend="h3"&gt;Visit google.com from this!&lt;/head&gt;
    &lt;p&gt;One last flashy thing — let’s connect to google.com from this VM with our custom user-land!&lt;/p&gt;
    &lt;p&gt;First, we need to attach a network device. We add &lt;code&gt;-device virtio-net-device,netdev=usernet -netdev user,id=usernet,hostfwd=tcp::10000-:22&lt;/code&gt; to our QEMU CLI. I think the last 2 numbers do not really matter as we won’t be SSH’ing into this machine (maybe you can do that exercise yourself, but I’m afraid it won’t be easy). The default kernel build should indeed bake in the &lt;code&gt;virtio&lt;/code&gt; network device drivers, so this should more or less just work.&lt;/p&gt;
    &lt;p&gt;We’ll need a working IP address, and we’ll use something from &lt;code&gt;u-root&lt;/code&gt; to obtain it. That something requires 3 things present in the kernel config: &lt;code&gt;CONFIG_VIRTIO_PCI&lt;/code&gt;, &lt;code&gt;CONFIG_HW_RANDOM_VIRTIO&lt;/code&gt; and &lt;code&gt;CONFIG_CRYPTO_DEV_VIRTIO&lt;/code&gt;. My default settings for the kernel have all that flipped to &lt;code&gt;y&lt;/code&gt;, so I’m good to go and you should be too, but you can double check just in case. If you have changed any kernel settings, please rebuild the kernel image.&lt;/p&gt;
    &lt;p&gt;Finally, we need to attach an RNG (doesn’t matter what it is) device to our QEMU machine so we can obtain our IP address. We simply add &lt;code&gt;-device virtio-rng-pci&lt;/code&gt; to our QEMU CLI.&lt;/p&gt;
    &lt;code&gt;qemu-system-riscv64 -machine virt -kernel arch/riscv/boot/Image -initrd /tmp/initramfs.linux_riscv64.cpio -device virtio-net-device,netdev=usernet -netdev user,id=usernet,hostfwd=tcp::10000-:22 -device virtio-rng-pci&lt;/code&gt;
    &lt;p&gt;Once we’re in, we can run &lt;code&gt;ip addr&lt;/code&gt; to see what’s our IP address.&lt;/p&gt;
    &lt;code&gt;/# ip addr
1: lo: &amp;lt;UP,LOOPBACK&amp;gt; mtu 65536 state UNKNOWN
    link/loopback
    inet 127.0.0.1 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1 scope host
       valid_lft forever preferred_lft forever
2: eth0: &amp;lt;BROADCAST,MULTICAST&amp;gt; mtu 1500 state DOWN
    link/ether 52:54:00:12:34:56
3: sit0: &amp;lt;0&amp;gt; mtu 1480 state DOWN
    link/sit&lt;/code&gt;
    &lt;p&gt;Our Ethernet is not set up. Let’s enable IPv4 networking (we don’t need 6). In this little setup, QEMU is running a virtualized network and it embeds a little DHCP server which can dynamically assign IPs (documentation is here). So let’s run a DHCP helper from &lt;code&gt;u-root&lt;/code&gt; for this by running&lt;/p&gt;
    &lt;code&gt;dhclient -ipv6=false&lt;/code&gt;
    &lt;p&gt;The output I got was the following:&lt;/p&gt;
    &lt;code&gt;2023/09/12 03:46:59 Bringing up interface eth0...
2023/09/12 03:47:00 Attempting to get DHCPv4 lease on eth0
2023/09/12 03:47:00 Got DHCPv4 lease on eth0: DHCPv4 Message
  opcode: BootReply
  hwtype: Ethernet
  hopcount: 0
  transaction ID: 0x05f008e1
  num seconds: 0
  flags: Unicast (0x00)
  client IP: 0.0.0.0
  your IP: 10.0.2.15
  server IP: 10.0.2.2
  gateway IP: 0.0.0.0
  client MAC: 52:54:00:12:34:56
  server hostname:
  bootfile name:
  options:
    Subnet Mask: ffffff00
    Router: 10.0.2.2
    Domain Name Server: 10.0.2.3
    IP Addresses Lease Time: 24h0m0s
    DHCP Message Type: ACK
    Server Identifier: 10.0.2.2
2023/09/12 03:47:00 Configured eth0 with IPv4 DHCP Lease IP 10.0.2.15/24
2023/09/12 03:47:00 Finished trying to configure all interfaces.&lt;/code&gt;
    &lt;p&gt;The QEMU documentation will tell you why pinging won’t work, so let’s not bother with pinging. Let’s just “visit” google.com!&lt;/p&gt;
    &lt;code&gt;wget http://google.com&lt;/code&gt;
    &lt;p&gt;You can now read the downloaded &lt;code&gt;index.html&lt;/code&gt; file!&lt;/p&gt;
    &lt;code&gt;cat index.html&lt;/code&gt;
    &lt;p&gt;You’ll get a lot of obfuscated JavaScript, but this is great! It means we have successfully visited google.com through &lt;code&gt;wget&lt;/code&gt;! I hope this sparks your imagination to do some other cool things with &lt;code&gt;u-root&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Package managers&lt;/head&gt;
    &lt;p&gt;You might intuitively understand at this point that some of the most important software of a Linux distro is the package manager. It’s really the gateway to getting the functionality on your machine that you need. What we went through here is more of an embedded flow: we generated these somewhat monolithic software images and if we want to update something, we rebuild the whole image and re-image the device. This doesn’t work for desktops, phones, etc. Package managers are there to update, add or remove the software on our machines. We won’t be talking about them here, just giving them a brief shoutout and you can hopefully imagine from the high level how they work and what do they do.&lt;/p&gt;
    &lt;head rend="h2"&gt;The monster of &lt;code&gt;init&lt;/code&gt;&lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;init&lt;/code&gt; we created is definitely just a toy, and in the end it just started some sort of a shell. However, make no mistake about it, &lt;code&gt;init&lt;/code&gt; is an incredibly important thing on a Linux system and getting it right is a science. You’ll see a lot of strong opinions on different &lt;code&gt;init&lt;/code&gt; systems for Linux online. &lt;code&gt;init&lt;/code&gt; doesn’t usually just spawn one process off and call it a day, it can set up a whole bunch of things like different devices, for example. As an exercise, just run &lt;code&gt;ls /dev&lt;/code&gt; from your &lt;code&gt;u-root&lt;/code&gt;-based build and see all those devices set up. A lot of them come from the &lt;code&gt;init&lt;/code&gt;’s setup and many are extremely useful. You can then read some of the &lt;code&gt;u-root&lt;/code&gt; source code to see what’s going on there in &lt;code&gt;init&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;GitHub repo&lt;/head&gt;
    &lt;p&gt;The code for this guide is available here, where you can just sync and build the &lt;code&gt;initramfs&lt;/code&gt; images.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45703556</guid><pubDate>Sat, 25 Oct 2025 13:01:39 +0000</pubDate></item><item><title>TigerBeetle and Synadia pledge $512k to the Zig Software Foundation</title><link>https://tigerbeetle.com/blog/2025-10-25-synadia-and-tigerbeetle-pledge-512k-to-the-zig-software-foundation/#blog-post</link><description>&lt;doc fingerprint="bf70d8bf88a3ab96"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Synadia and TigerBeetle Pledge $512,000 to the Zig Software Foundation&lt;/head&gt;
    &lt;p&gt;Synadia and TigerBeetle have together pledged $512,000 to the Zig Software Foundation over the next two years in support of the language, leadership, and communities building the future of simpler systems software.&lt;/p&gt;
    &lt;p&gt;I first saw Zig in 2018, seven years ago. Two years later, I chose Zig over C or Rust for TigerBeetle.&lt;/p&gt;
    &lt;p&gt;In 2020, I was following Rust closely. At the time, Rustâs default memory philosophy was to crash when out of memory (OOM). However, for TigerBeetle, I wanted explicit static allocation, following NASAâs Power of Ten Rules for Safety-Critical Code, which would become a part of TigerStyle, a methodology for creating safer software in less time.&lt;/p&gt;
    &lt;p&gt;What I learned is that if you could centralize resource allocation in time and space (the dimensions that prove tricky for humans writing software) then this could not only simplify memory management, to design away some of the need for a borrow checker in the first place, but, more importantly, also be a forcing function for propagating good design, to encourage teams to think through the explicit limits or physics of the software (you have no choice).&lt;/p&gt;
    &lt;p&gt;From a performance perspective, I didnât want TigerBeetle to be fearlessly multithreaded. Transaction processing workloads tend to have inherent contention, even to the point of power law, precluding partitioning and necessitating a single-threaded architecture. Therefore, Rustâs borrow checker, while a phenomenal tool for the class of problems it targets, made less sense for TigerBeetle. TigerBeetle never frees memory and never runs multithreaded, instead using explicit submission/completion queue interfaces by design.&lt;/p&gt;
    &lt;p&gt;Finally, while the borrow checker could achieve local memory safety, TigerBeetle needed more correctness properties. TigerBeetle needed to be always correct, and across literally thousands of invariants. As matklad would say, this is a harder problem! I had also spent enough time in memory safe languages to know that local memory safety is no guarantee of local correctness, let alone distributed system correctness. Per systems thinking, I believe that total correctness is a design problem, not a language problem. Language is valuable. But no human language can guarantee the next Hemingway or Nabokov. For this you need philosophy. Even then itâs not a guarantee but a probability.&lt;/p&gt;
    &lt;p&gt;With Rust off the table, the choice fell to C or Zig. A language of the past or future?&lt;/p&gt;
    &lt;p&gt;Zig was early, which gave me pause, but I felt that the quality of Andrew Kelleyâs design decisions in the language, the standard library (e.g. the unmanaged hashmap interface) and the cross-compilation toolchain, even five years ago, was already exceptional.&lt;/p&gt;
    &lt;p&gt;Andrewsâs philosophy resonated with what I wanted to explore in TigerStyle. No hidden memory allocations. No hidden control flow. No preprocessor. No macros. And then you get things like comptime, reducing the grammar and dimensionality of the language, while simultaneously multiplying its power. The primary benefit of Zig is the favorable ratio of expressivity to language complexity.&lt;/p&gt;
    &lt;p&gt;As a replacement for C, Zig fixed not only the small cuts, such as explicit alignment in the type system for Direct I/O, or safer casts, but the showstoppers of spatial memory safety through bounds checking, and, to a lesser degree (but not guarantee), temporal memory safety through the debug allocator.&lt;/p&gt;
    &lt;p&gt;Zig also enabled checked arithmetic by default in safe builds, which is something I believe only Ada and Swift do (remarkably, Rust disables checked arithmetic by default in safe buildsâa default I would love to see changed). TigerBeetle separates the data plane from the control plane by design, through batching, so the runtime cost of these safety checks was not material, being amortized in the data plane across bigger buffers. While a borrow checker or static allocation can simplify memory management, getting logic and arithmetic correct remains hard. Of course, you can enable checked arithmetic in other languages, but I appreciated Andrewâs concern for checked arithmetic and stricter operands by default.&lt;/p&gt;
    &lt;p&gt;In all these things, what impressed me most was Zigâs approach to safety when working with the metal. Not in terms of an on/off decision, but as a spectrum. Not aiming for 100% guarantees across 1 or 2 categories, but 90% and then across more categories. Not eliminating classes of bugs, but downgrading their probability. All while preserving the power-to-weight ratio of the language, to keep the language beautifully simple.&lt;/p&gt;
    &lt;p&gt;Many languages start simple and grow complex as features are added. Zigâs simplicity is unusual in that it comes from a subtractive discipline (e.g. no private fields) rather than a deferred complexity; minimizing surface area is part of the ethos of the language. The simplicity of Zig meant that we could hire great programmers from any language backgroundâthey could pick up Zig in a weekend. Indeed, Iâve never had to talk to a new hire about learning Zig.&lt;/p&gt;
    &lt;p&gt;Finally, there was the timing. Recognizing that TigerBeetle would take time to reach production (we shipped production in 2024, after 3.5 years of development), giving Zig time to mature, for our trajectories to intersect.&lt;/p&gt;
    &lt;p&gt;Investing in creating a database like TigerBeetle is a long term effort. Databases tend to have a long half life (e.g. Postgres is 30 years old). And so, while Zig being early in 2020 did give me pause, nevertheless Zigâs quality, philosophy and simplicity made sense for a multi-decade horizon.&lt;/p&gt;
    &lt;p&gt;How has the decision for Zig panned out?&lt;/p&gt;
    &lt;p&gt;TigerBeetle is tested end-to-end under some pretty extreme fuzzing. We did have three bugs that would have been prevented by the borrow checker, but these were caught by our fuzzers and online verification. We run a fuzzing fleet of 1,000 dedicated CPU cores 24/7. We invest in deterministic simulation testing (e.g. VOPR), as well as non-deterministic fault-injection harnesses (e.g. VÃ¶rtex). We engaged Kyle Kingsbury in one of the longest Jepsen audits to dateâfour times the typical duration. Through all this, Zigâs quality held up flawlessly.&lt;/p&gt;
    &lt;p&gt;Zig has also been a huge part of our success as a company. TigerBeetle is only 5 years old but is already migrating some of the largest brokerages, exchanges and wealth managements in their respective jurisdictions. Several of our key enterprise contracts were thanks to the CTOs and even CEOs of these companies also following Zig and seeing the quality we wanted to achieve with it. I donât think we could have written TigerBeetle as it is, in any other language, at least not to the same tight tolerances, let alone with the same velocity.&lt;/p&gt;
    &lt;p&gt;Zigâs language specification will only reach 1.0 when all experimental areas of the language (e.g. async I/O) are finally done. For TigerBeetle, we care only about the stable language features we use, testing our binaries end to end, as we would for any language. Nevertheless, upgrading to new versions, even with breaking changes, has only been a pleasure for us as a team. The upgrade work is usually fully paid for by compilation time reduction. For example, the upgrade from Zig 0.14.1 to Zig 0.15.2 (with the native x86_64 backend) makes debug builds 2x faster, and even LLVM release builds become 1.6x faster. With each release, you can literally feel the sheer amount of effort that the entire Zig core team put into making Zig the worldâs most powerful programming languageâand toolchain.&lt;/p&gt;
    &lt;p&gt;Back in 2020, from a production perspective, Zig was more or less a frontend to LLVM, the same compiler used by Rust, Swift and other languages. However, by not shying away from also investing in its own independent compiler backends and toolchain, by appreciating the value of replacing LLVM long term, Zig is becoming well positioned to gain a low-level precision and compilation speed that generic LLVM wonât always be able to match.&lt;/p&gt;
    &lt;p&gt;We want Andrew to take his time, to get these things right for the long term. Fred Brooks once said that conceptual integrity is âthe most important considerationâ in system design, that the design must proceed from one mind.&lt;/p&gt;
    &lt;p&gt;In this spirit, I am grateful for Andrewâs remarkably strong leadership (and taste) in the design of the language and toolchain. There can be thankless pressure on an open source project to give in to the tragedy of the commons. But if anything, in hindsight I think this is what Iâve most appreciated about choosing Zig for TigerBeetle, that Zig has a strong BDFL.&lt;/p&gt;
    &lt;p&gt;Of course, some may hear âBDFLâ and see governance risk. But I fear the opposite: conceptual risk, the harder problem. Brooks was rightâconceptual integrity is almost certainly doomed by committee. Whereas governance is easier solved: put it in the hands, not of the corporates, but of the people. The individuals who choose each day to continue to donate.&lt;/p&gt;
    &lt;p&gt;This is why our pledge today, along with all other ZSF donors, is a simple donation with no strings attached. The Zig Software Foundation is well managed, transparent and independent. We want it to remain this way. The last thing we want is some kind of foundation âseatâ. Andrew is Chef. We want to let him cook, and pay his core team sustainably (e.g. 92% percent of budget goes to directly paying contributors).&lt;/p&gt;
    &lt;p&gt;If cooking is one metaphor, then surfing is another. I believe that technology moves in waves. The art is not in paddling to the wave with a thousand surfers on it. But in spotting the swell before it breaks. And then enjoying the ride with the early adopters who did the same. River, Ghostty, Bun, Mach and many fellow surfers.&lt;/p&gt;
    &lt;p&gt;In fact, it was through Zig that I met Derek Collison, who like me had been sponsoring the language in his personal capacity since 2018. As a former CTO at VMware, Derek was responsible for backing antirez to work full time on Redis. Derek later went on to create NATS, founding Synadia.&lt;/p&gt;
    &lt;p&gt;As we were about to increase TigerBeetleâs yearly donation to Zig, I reached out to Derek, and we decided to do a joint announcement, following Mitchell Hashimotoâs lead. For each of our companies to donate $256,000 in monthly installments over the next two years, with Synadia matching TigerBeetle, for a total of $512,000âthe first installment already made.&lt;/p&gt;
    &lt;p&gt;Please consider donating or increasing your donation if you can. And if you are a CEO or CTO, please team up with another company to outmatch us! Thanks Andrew for creating something special, and to all who code for the joy of the craft:&lt;/p&gt;
    &lt;p&gt;Together we serve the users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45703926</guid><pubDate>Sat, 25 Oct 2025 13:54:33 +0000</pubDate></item><item><title>Against SQL</title><link>https://www.scattered-thoughts.net/writing/against-sql/</link><description>&lt;doc fingerprint="e3c401461f672ad1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Rock Tumbler Instructions&lt;/head&gt;&lt;head rend="h3"&gt;Directions for Turning Rough Rocks into Beautiful Tumbled Stones&lt;/head&gt;&lt;p&gt;Working to transform rough rock into beautiful tumbled stones gives most people a great feeling of accomplishment. It doesn't matter how old you are or how many batches of rock you have tumbled in the past - when you finish the last tumbling step, rinse off the polish, and see a super-bright luster on colorful polished stones - you are amazed at what you have done.&lt;/p&gt;&lt;head rend="h2"&gt;Rock Tumbling Is Easy&lt;/head&gt;&lt;p&gt;Using a rock tumbler to convert rough rock into sparkling tumbled stones is easy if you follow a simple procedure and observe a few rules. We are writing this to share the procedure that we have used for many years with a number of rotary tumblers.&lt;/p&gt;&lt;p&gt;This procedure works well with materials that have the following properties:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;of adequate quality to accept a polish&lt;/item&gt;&lt;item&gt;a Mohs hardness between 6 and 7&lt;/item&gt;&lt;item&gt;a size between 3/8" and 1 1/2"&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Chalcedony&lt;/head&gt;: agate, bloodstone, carnelian, chrysoprase, jasper, chert, flint, and petrified (silicified) wood.&lt;head rend="h3"&gt;Quartz&lt;/head&gt;: amethyst, aventurine, citrine, milky quartz, rock crystal, rose quartz, smoky quartz, tiger's-eye.&lt;head rend="h3"&gt;Rock Types&lt;/head&gt;: andesite, basalt, diorite, gabbro, granite, mookaite, novaculite, quartzite, unakite.&lt;head rend="h2"&gt;The "Golden Rules" of Rock Tumbling&lt;/head&gt;&lt;p&gt;We follow three "Golden Rules" in all aspects of rock tumbling. They are:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;"Garbage in means garbage out"&lt;/item&gt;&lt;item&gt;"Avoid contamination"&lt;/item&gt;&lt;item&gt;"Great results take time."&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Tumbling will enable you to turn the rough rock on the left side of this photo into the sparkling tumbled stones on the right side of the photo. The results are amazing!&lt;/p&gt;&lt;head rend="h3"&gt;"Garbage in Means Garbage Out"&lt;/head&gt;&lt;p&gt;If you start with garbage (low-quality rough), you should expect low-quality tumbled stones. So, don't hesitate to discard a rock that is porous, fractured, misshapen, or that is not expected to produce an attractive tumbled stone.&lt;/p&gt;&lt;p&gt;You will spend a lot of time and valuable supplies tumbling a batch of rocks. Using quality rough saves time, gives you better value for your money, and produces tumbled stones that are of much higher quality.&lt;/p&gt;&lt;p&gt;We buy lots of tumbling rough from online vendors as part of our hobby and to educate ourselves. We have the best experience buying rough from vendors who: 1) provide clear written descriptions and large clear photos of the rough they are selling, 2) show photos of tumbled stones that they produced themselves from the rock they are selling, and, 3) provide a detailed description of the steps that they followed to tumble the stones. We have the best experience buying rough from people who are actively involved in rock tumbling.&lt;/p&gt;&lt;head rend="h3"&gt;"Avoid Contamination"&lt;/head&gt;&lt;p&gt;You will use a different size tumbler grit for each step of the tumbling process. If coarse grit gets into your medium grit step, it will scratch up the rocks and you might need to do the medium grit step over again.&lt;/p&gt;&lt;p&gt;Avoiding this type of contamination is easy: just thoroughly clean the rocks, the tumbler barrel, and your tools when you change from one grit size to another.&lt;/p&gt;&lt;p&gt;Another way that contamination occurs is when you include rocks that are brittle, or have a granular texture. These rocks might break or shed grains in the tumbler. These grains and broken pieces can scratch up every rock in the barrel.&lt;/p&gt;&lt;p&gt;Here is a test that we use to detect rocks that will shed grains in the tumbler. We pick up a piece of rough in each hand. We then rub them together while applying a bit of pressure. If we are easily dislodging grains from the rock, we believe that the rock will likely shed grains during tumbling.&lt;/p&gt;&lt;p&gt;This type of contamination is also easy to avoid. Simply examine your rocks before tumbling, and don't tumble suspect rocks in the same barrel with quality rough. Tumble new types of rough or suspicious materials separately.&lt;/p&gt;&lt;head rend="h3"&gt;"Great Results Take Time"&lt;/head&gt;&lt;p&gt;Don't be in a hurry. Spend time doing a great job. If you tumble a batch of rocks through the coarse grit step and they still have a few rough edges or are not nicely rounded, don't hesitate to run them through the coarse grit step again. Also, spend the time needed to thoroughly clean your work area, tumbler barrel, rocks, and tools between steps to avoid contamination.&lt;/p&gt;&lt;p&gt;"Garbage in means garbage out." The rocks in this photo do not have the potential to become nice tumbled stones. A rock with voids should be thrown away - the voids will trap grit and contaminate your pre-polish and polishing steps. Protrusions can be trimmed off with a rock saw - and that might yield two nicely rounded rocks.&lt;/p&gt;&lt;head rend="h2"&gt;Inspecting Your Rough&lt;/head&gt;&lt;p&gt;Remember the rule "garbage in means garbage out." Practice that by starting with quality rough, and you will have a chance to produce high-quality tumbled stones. We prepare to tumble by examining our rough rock. If we find porous pieces that might carry grit from one step to the next, we discard them.&lt;/p&gt;&lt;p&gt;Rocks that are fractured will break while tumbling and scratch other rocks in the batch. When we see a fractured rock in our rough, we discard it or break it along that fracture before it is placed in the barrel.&lt;/p&gt;&lt;p&gt;For best results, your tumbler barrel should be loaded with rocks of mixed sizes (from about 1/4 inch up to about 1 1/2 inches in diameter for a 2-pound or 3-pound-capacity barrel). If we need more rocks to fill the barrel to the proper level, we often add rocks that were previously polished but have a rough spot or a blemish that, if ground away, will improve the rock's appearance.&lt;/p&gt;&lt;p&gt;Two final tips before we load the barrel:&lt;/p&gt;&lt;p&gt;1.) Tumbling works best when all of the rocks in the barrel are about the same hardness. If soft rocks are tumbled with harder rocks, the softer rocks will wear away quickly - before the harder rocks are properly shaped and smoothed.&lt;/p&gt;&lt;p&gt;2.) Tumbling works best when all rocks in the barrel are of the same type. If you mix rock types, problems can result - and they will be difficult to diagnose.&lt;/p&gt;&lt;p&gt;When loading the tumbler barrel, you should have pieces of rough with a range of particle sizes. We would mix the above sizes together in the barrel. If you load the barrel with just a few large pieces, there will be very few points of contact between the rocks in the load. Those points of contact are where grit is trapped between the rocks and where grinding occurs. If you have lots of small pieces of rough between the big pieces, there will be many points of contact between the rocks of the load, and the tumbling process will be faster and more effective.&lt;/p&gt;&lt;p&gt;If you don't have small pieces of rock to tumble, you can add small ceramic media to the tumbler barrel. Ceramic media are used as small-size "filler" in tumbling. These tiny cylinders will also act like roller bearings in the barrel and make your load tumble with a smooth action - that smooth action will improve the grinding in the barrel and keep your stones from being bruised. See our video about selecting the right tumbling media.&lt;/p&gt;&lt;head rend="h2"&gt;The Four-Step Tumbling Process&lt;/head&gt;&lt;p&gt;Now you are ready to begin what most people call the "Four-Step Tumbling Process." This is described below for a rotary tumbler with a three-pound-capacity barrel such as the Thumler's Model A-R1, Thumler's Model A-R2, Lortone Model 3A, or the Lortone Model 33B.&lt;/p&gt;&lt;p&gt;If you are tumbling with the Thumler's Model MP-1 tumbler (which has a two-pound-capacity barrel), you can follow the instructions below, but use about two level tablespoons of grit or polish in each of the tumbling steps (Step 1 through Step 4).&lt;/p&gt;&lt;head rend="h2"&gt;Loading the Tumbler Barrel&lt;/head&gt;&lt;p&gt;Before you load the tumbler barrel, be sure that it is perfectly clean. There should be no grit or rock fragments left in the barrel from a previous tumble. To prevent leaks, the rim of the barrel and the lid should be totally free from grit or rock particles.&lt;/p&gt;&lt;p&gt;Once you have a clean barrel, add enough rock to fill the barrel about 1/2 to 2/3 full. With small tumblers it is best to tumble rocks that are between about 1/4" and 1 1/2 inches in size. If you don't have enough rough to fill the barrel at least 2/3 full, the rocks might be tossed around in the tumbler and bruised. (Varieties of quartz bruise very easily.)&lt;/p&gt;&lt;p&gt;It is best to add a variety of rock sizes to the barrel. If you use only large pieces there will be very few contact points between the rocks and very little grinding will occur. If you add a range of rock sizes the small rocks will fill the spaces between the large rocks, creating many more points of contact between the rocks. Grinding occurs when particles of grit get caught between the rocks - so the more points of contact you have, the more effective the grinding.&lt;/p&gt;&lt;p&gt;When tumbling you will place enough rocks in the barrel to make it about 1/2 to 2/3 full. Then, add about two level tablespoons of grit for each pound of rock. Finally, add enough water to almost cover the rock. Now seal the barrel and place it on the tumbler.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 1 - Coarse Grind&lt;/head&gt;&lt;p&gt;The first step of the four-step tumbling process is to run the rocks in the tumbler with coarse grit. We begin with a barrel that is about 1/2 to 2/3 full of tumbling rough, then add two level tablespoons of coarse grit (we use 60/90 grit silicon carbide) for each pound of rock. Then, add water until the water line is just below the top of the rocks. Seal the barrel and run for about seven days.&lt;/p&gt;&lt;p&gt;At the end of seven days, open the barrel. You will find a barrel of rocks in very muddy water! Dump the contents into a screen or a colander over a plastic bucket and rinse off every speck of grit and mud. Wear safety glasses to protect your eyes from a splash of mud.&lt;/p&gt;&lt;p&gt;Used grit and rock mud should never be washed down a drain. It can clog your plumbing system. We wash rocks in a plastic colander over a plastic bucket to keep the mud out of the drain.&lt;/p&gt;&lt;head rend="h3"&gt;Inspecting the Rocks:&lt;/head&gt;Now that you have washed the rocks, it is time to inspect them. Your goal is to determine if they are ready to move on to STEP 2, or if another week in STEP 1 would improve their appearance. We almost always tumble the rocks for a second week in coarse grit. We believe that improves their shape and removes more blemishes from their surface. Then, we usually move all of the rocks to the medium grit step.&lt;head rend="h3"&gt;Perfectionist Tumbling:&lt;/head&gt;Some people want to have more control over the tumbling process and only admit excellent rocks into STEP 2. These people sort their rocks into three categories:&lt;list rend="ul"&gt;&lt;item&gt;1) those that are ready for STEP 2&lt;/item&gt;&lt;item&gt;2) those that could be improved by another week in STEP 1&lt;/item&gt;&lt;item&gt;3) those that should be discarded or trimmed and returned to STEP 1&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Here are some rocks right out of STEP 1. Note how they are covered with a gray "mud." This mud is spent grit and tiny rock particles that were worn off of the rocks during tumbling. Wash the rocks thoroughly so none of this grit goes into STEP 2. We wash our rocks in a colander over a plastic bucket so none of the mud goes down the drain. &lt;lb/&gt;IT IS VERY IMPORTANT TO WASH THE MUD FROM THE ROCKS IMMEDIATELY. If the rock mud is allowed to dry on the rocks, it is almost impossible to wash off.&lt;/p&gt;&lt;p&gt;IT IS VERY IMPORTANT TO WASH THE MUD FROM THE ROCKS IMMEDIATELY. If the rock mud is allowed to dry on the rocks, it is almost impossible to wash off.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 2 - Medium Grind&lt;/head&gt;&lt;p&gt;The second step of the four-step tumbling process is to run the rocks in the tumbler with medium grit. Before you begin it is extremely important to clean all of the coarse grit and rock mud from the rocks, from the tumbler barrel, and from the barrel lid. It is very important to avoid having even a few grains of coarse grit in the medium grind step.&lt;/p&gt;&lt;p&gt;During STEP 1, your rocks were reduced in size. When you return them to the barrel for STEP 2, they will probably not fill the barrel to the recommended 1/2 to 2/3 full level. If the barrel is only 1/2 full or less, the rocks can be tossed violently around in the tumbler. This can break or damage fragile materials such as quartz. So, when tumbling quartz or another fragile material, we always add enough ceramic media (or some rocks that need a little more tumbling) to bring the barrel up to the 1/2 to 2/3 full level.&lt;/p&gt;&lt;p&gt;(This is less important with varieties of chalcedony because it is a more durable material. However, if your tumbler barrel travels at more than about 60 revolutions per minute, we recommend adding enough ceramic media to bring it up to the 2/3 full level regardless of what type of rock is being tumbled.)&lt;/p&gt;&lt;p&gt;After your barrel is at the proper level, add two level tablespoons of medium grit (we use 110/220 grit or 150/220 grit silicon carbide) for each pound of rock (and ceramic media). Then add water until the water line is just below the top of the rocks. Now tumble for seven days.&lt;/p&gt;&lt;p&gt;At the end of seven days, open the barrel and clean all of the grit from the rocks, barrel, and lid (don't let any grit go down the drain). At this point in the tumbling process, a dry rock should have a smooth frosted surface. Inspect the rocks, looking for any that are cracked or broken. If you find any, these rocks should be discarded or saved for the next time you run Step 1.&lt;/p&gt;&lt;p&gt;Used grit and rock mud should never be washed down a drain. It can clog your plumbing system. We wash rocks in a plastic colander over a plastic bucket to keep the mud out of the drain.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 3 - Fine Grind / Pre-polish&lt;/head&gt;&lt;p&gt;The third step of the four-step tumbling process is a week in a fine grit such as 600 grit or 500 grit silicon carbide. Begin with a barrel that is perfectly clean. Place your rough and any ceramics that are with them into the barrel, and add two level tablespoons of fine grit per pound of material. Then add water until it fills the barrel up to just below the top of the rocks. Run this for about seven days, and then do a thorough cleaning of the rocks, the barrel, and the lid.&lt;/p&gt;&lt;p&gt;Remove any rocks that have broken or show signs of fracturing. At this point in the process, the rocks should be extremely smooth, and some of them might display a slight luster.&lt;/p&gt;&lt;p&gt;Be very clean! Before you replace the lid on your barrel, be sure that both the lid and the rim are perfectly clean. This will allow the lid to fit tightly and prevent leaks.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 4 - Polish&lt;/head&gt;&lt;p&gt;Now you are down to the final step - the one that puts a bright shine onto your tumbled stones. Be sure that the rocks and the equipment are perfectly clean. (Some people have an extra barrel that they use only for the polishing step.) A few specks of grit could ruin a great polish.&lt;/p&gt;&lt;p&gt;Place the rocks in the barrel and add two level tablespoons of rock polish (we use TXP aluminum oxide powder for almost all of our rotary tumbling) per pound of material in the barrel. Add water to just below the top of the rocks. Then, close the barrel and run for about seven days.&lt;/p&gt;&lt;p&gt;When you finish this step, your rocks should be bright and shiny. If they are, congratulations! Admire them for a while and share them with your friends.&lt;/p&gt;&lt;p&gt;If the stones have an extremely smooth surface but do not shine, they might need to be cleaned up using the burnishing step described below. If they have scratches on them, then you will need to go back to STEP 2 and repeat the medium grind, fine grind, and polishing steps.&lt;/p&gt;&lt;p&gt;For burnishing we grate up a bar of Ivory Soap with a vegetable grater. Then we add 1/2 tablespoon of grated soap for each pound of rock plus enough warm water to almost cover the rocks. See our video about burnishing polished stones.&lt;/p&gt;&lt;head rend="h2"&gt;Burnishing&lt;/head&gt;&lt;p&gt;Sometimes our stones are a little "hazy" when they come out of the polish, or small particles of polish are in micro-size crevices. We shine and clean them up by tumbling for an hour or so in soapy water. This is called "burnishing."&lt;/p&gt;&lt;p&gt;To burnish, we place the stones in our polish barrel with the normal amount of water, and then we add about 1/2 tablespoon of grated "Ivory" bar soap for each pound of rock (we use "ORIGINAL" Ivory soap - don't use a soap with aloe or abrasive or any other additive - honestly, just get a bar of Ivory soap). Burnishing usually makes the tumbled stones a little brighter, but sometimes it really kicks up the shine.&lt;/p&gt;&lt;p&gt;Print a copy of our free tumbling log and use it to keep your records.&lt;/p&gt;&lt;p&gt;Here are a few of our favorite tumbled stones!&lt;/p&gt;&lt;head rend="h2"&gt;Keeping Records&lt;/head&gt;&lt;p&gt;It is easy to forget what day you started the tumbler or what type of grit was used - especially if you are running multiple tumblers. Keeping records will keep you on track and provide a history that will help you learn. We record material tumbled, start date, abrasive used, media used, finishing date and duration, along with any comments or observations about the results.&lt;/p&gt;&lt;p&gt;To help you with your record keeping, we have prepared a printable tumbling log.&lt;/p&gt;&lt;p&gt;We usually have multiple tumblers running here, and we record every barrel of rock that we tumble on these logs. Even if your memory is better than ours, record-keeping is a good idea. When you learn something that works or something that doesn't, you will have it recorded. This information can help you repeat great results and avoid repeating bad ones. Also, we have trouble remembering which day a barrel of rocks was started. Using the log takes away the chance of forgetting.&lt;/p&gt;&lt;head rend="h2"&gt;Happy Tumbling!&lt;/head&gt;&lt;head rend="h3"&gt;RockTumbler.com Authors&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Hobart M. King has decades of rock tumbling experience and writes most of the articles on RockTumbler.com. He has a PhD in geology and is a GIA graduate gemologist. He also writes the articles about rocks, minerals and gems on Geology.com.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45704419</guid><pubDate>Sat, 25 Oct 2025 15:00:06 +0000</pubDate></item><item><title>Rock Tumbler Instructions: Turning Rough Rocks into Beautiful Tumbled Stones</title><link>https://rocktumbler.com/tips/rock-tumbler-instructions/</link><description>&lt;doc fingerprint="e3c401461f672ad1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Rock Tumbler Instructions&lt;/head&gt;&lt;head rend="h3"&gt;Directions for Turning Rough Rocks into Beautiful Tumbled Stones&lt;/head&gt;&lt;p&gt;Working to transform rough rock into beautiful tumbled stones gives most people a great feeling of accomplishment. It doesn't matter how old you are or how many batches of rock you have tumbled in the past - when you finish the last tumbling step, rinse off the polish, and see a super-bright luster on colorful polished stones - you are amazed at what you have done.&lt;/p&gt;&lt;head rend="h2"&gt;Rock Tumbling Is Easy&lt;/head&gt;&lt;p&gt;Using a rock tumbler to convert rough rock into sparkling tumbled stones is easy if you follow a simple procedure and observe a few rules. We are writing this to share the procedure that we have used for many years with a number of rotary tumblers.&lt;/p&gt;&lt;p&gt;This procedure works well with materials that have the following properties:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;of adequate quality to accept a polish&lt;/item&gt;&lt;item&gt;a Mohs hardness between 6 and 7&lt;/item&gt;&lt;item&gt;a size between 3/8" and 1 1/2"&lt;/item&gt;&lt;/list&gt;&lt;head rend="h3"&gt;Chalcedony&lt;/head&gt;: agate, bloodstone, carnelian, chrysoprase, jasper, chert, flint, and petrified (silicified) wood.&lt;head rend="h3"&gt;Quartz&lt;/head&gt;: amethyst, aventurine, citrine, milky quartz, rock crystal, rose quartz, smoky quartz, tiger's-eye.&lt;head rend="h3"&gt;Rock Types&lt;/head&gt;: andesite, basalt, diorite, gabbro, granite, mookaite, novaculite, quartzite, unakite.&lt;head rend="h2"&gt;The "Golden Rules" of Rock Tumbling&lt;/head&gt;&lt;p&gt;We follow three "Golden Rules" in all aspects of rock tumbling. They are:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;"Garbage in means garbage out"&lt;/item&gt;&lt;item&gt;"Avoid contamination"&lt;/item&gt;&lt;item&gt;"Great results take time."&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Tumbling will enable you to turn the rough rock on the left side of this photo into the sparkling tumbled stones on the right side of the photo. The results are amazing!&lt;/p&gt;&lt;head rend="h3"&gt;"Garbage in Means Garbage Out"&lt;/head&gt;&lt;p&gt;If you start with garbage (low-quality rough), you should expect low-quality tumbled stones. So, don't hesitate to discard a rock that is porous, fractured, misshapen, or that is not expected to produce an attractive tumbled stone.&lt;/p&gt;&lt;p&gt;You will spend a lot of time and valuable supplies tumbling a batch of rocks. Using quality rough saves time, gives you better value for your money, and produces tumbled stones that are of much higher quality.&lt;/p&gt;&lt;p&gt;We buy lots of tumbling rough from online vendors as part of our hobby and to educate ourselves. We have the best experience buying rough from vendors who: 1) provide clear written descriptions and large clear photos of the rough they are selling, 2) show photos of tumbled stones that they produced themselves from the rock they are selling, and, 3) provide a detailed description of the steps that they followed to tumble the stones. We have the best experience buying rough from people who are actively involved in rock tumbling.&lt;/p&gt;&lt;head rend="h3"&gt;"Avoid Contamination"&lt;/head&gt;&lt;p&gt;You will use a different size tumbler grit for each step of the tumbling process. If coarse grit gets into your medium grit step, it will scratch up the rocks and you might need to do the medium grit step over again.&lt;/p&gt;&lt;p&gt;Avoiding this type of contamination is easy: just thoroughly clean the rocks, the tumbler barrel, and your tools when you change from one grit size to another.&lt;/p&gt;&lt;p&gt;Another way that contamination occurs is when you include rocks that are brittle, or have a granular texture. These rocks might break or shed grains in the tumbler. These grains and broken pieces can scratch up every rock in the barrel.&lt;/p&gt;&lt;p&gt;Here is a test that we use to detect rocks that will shed grains in the tumbler. We pick up a piece of rough in each hand. We then rub them together while applying a bit of pressure. If we are easily dislodging grains from the rock, we believe that the rock will likely shed grains during tumbling.&lt;/p&gt;&lt;p&gt;This type of contamination is also easy to avoid. Simply examine your rocks before tumbling, and don't tumble suspect rocks in the same barrel with quality rough. Tumble new types of rough or suspicious materials separately.&lt;/p&gt;&lt;head rend="h3"&gt;"Great Results Take Time"&lt;/head&gt;&lt;p&gt;Don't be in a hurry. Spend time doing a great job. If you tumble a batch of rocks through the coarse grit step and they still have a few rough edges or are not nicely rounded, don't hesitate to run them through the coarse grit step again. Also, spend the time needed to thoroughly clean your work area, tumbler barrel, rocks, and tools between steps to avoid contamination.&lt;/p&gt;&lt;p&gt;"Garbage in means garbage out." The rocks in this photo do not have the potential to become nice tumbled stones. A rock with voids should be thrown away - the voids will trap grit and contaminate your pre-polish and polishing steps. Protrusions can be trimmed off with a rock saw - and that might yield two nicely rounded rocks.&lt;/p&gt;&lt;head rend="h2"&gt;Inspecting Your Rough&lt;/head&gt;&lt;p&gt;Remember the rule "garbage in means garbage out." Practice that by starting with quality rough, and you will have a chance to produce high-quality tumbled stones. We prepare to tumble by examining our rough rock. If we find porous pieces that might carry grit from one step to the next, we discard them.&lt;/p&gt;&lt;p&gt;Rocks that are fractured will break while tumbling and scratch other rocks in the batch. When we see a fractured rock in our rough, we discard it or break it along that fracture before it is placed in the barrel.&lt;/p&gt;&lt;p&gt;For best results, your tumbler barrel should be loaded with rocks of mixed sizes (from about 1/4 inch up to about 1 1/2 inches in diameter for a 2-pound or 3-pound-capacity barrel). If we need more rocks to fill the barrel to the proper level, we often add rocks that were previously polished but have a rough spot or a blemish that, if ground away, will improve the rock's appearance.&lt;/p&gt;&lt;p&gt;Two final tips before we load the barrel:&lt;/p&gt;&lt;p&gt;1.) Tumbling works best when all of the rocks in the barrel are about the same hardness. If soft rocks are tumbled with harder rocks, the softer rocks will wear away quickly - before the harder rocks are properly shaped and smoothed.&lt;/p&gt;&lt;p&gt;2.) Tumbling works best when all rocks in the barrel are of the same type. If you mix rock types, problems can result - and they will be difficult to diagnose.&lt;/p&gt;&lt;p&gt;When loading the tumbler barrel, you should have pieces of rough with a range of particle sizes. We would mix the above sizes together in the barrel. If you load the barrel with just a few large pieces, there will be very few points of contact between the rocks in the load. Those points of contact are where grit is trapped between the rocks and where grinding occurs. If you have lots of small pieces of rough between the big pieces, there will be many points of contact between the rocks of the load, and the tumbling process will be faster and more effective.&lt;/p&gt;&lt;p&gt;If you don't have small pieces of rock to tumble, you can add small ceramic media to the tumbler barrel. Ceramic media are used as small-size "filler" in tumbling. These tiny cylinders will also act like roller bearings in the barrel and make your load tumble with a smooth action - that smooth action will improve the grinding in the barrel and keep your stones from being bruised. See our video about selecting the right tumbling media.&lt;/p&gt;&lt;head rend="h2"&gt;The Four-Step Tumbling Process&lt;/head&gt;&lt;p&gt;Now you are ready to begin what most people call the "Four-Step Tumbling Process." This is described below for a rotary tumbler with a three-pound-capacity barrel such as the Thumler's Model A-R1, Thumler's Model A-R2, Lortone Model 3A, or the Lortone Model 33B.&lt;/p&gt;&lt;p&gt;If you are tumbling with the Thumler's Model MP-1 tumbler (which has a two-pound-capacity barrel), you can follow the instructions below, but use about two level tablespoons of grit or polish in each of the tumbling steps (Step 1 through Step 4).&lt;/p&gt;&lt;head rend="h2"&gt;Loading the Tumbler Barrel&lt;/head&gt;&lt;p&gt;Before you load the tumbler barrel, be sure that it is perfectly clean. There should be no grit or rock fragments left in the barrel from a previous tumble. To prevent leaks, the rim of the barrel and the lid should be totally free from grit or rock particles.&lt;/p&gt;&lt;p&gt;Once you have a clean barrel, add enough rock to fill the barrel about 1/2 to 2/3 full. With small tumblers it is best to tumble rocks that are between about 1/4" and 1 1/2 inches in size. If you don't have enough rough to fill the barrel at least 2/3 full, the rocks might be tossed around in the tumbler and bruised. (Varieties of quartz bruise very easily.)&lt;/p&gt;&lt;p&gt;It is best to add a variety of rock sizes to the barrel. If you use only large pieces there will be very few contact points between the rocks and very little grinding will occur. If you add a range of rock sizes the small rocks will fill the spaces between the large rocks, creating many more points of contact between the rocks. Grinding occurs when particles of grit get caught between the rocks - so the more points of contact you have, the more effective the grinding.&lt;/p&gt;&lt;p&gt;When tumbling you will place enough rocks in the barrel to make it about 1/2 to 2/3 full. Then, add about two level tablespoons of grit for each pound of rock. Finally, add enough water to almost cover the rock. Now seal the barrel and place it on the tumbler.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 1 - Coarse Grind&lt;/head&gt;&lt;p&gt;The first step of the four-step tumbling process is to run the rocks in the tumbler with coarse grit. We begin with a barrel that is about 1/2 to 2/3 full of tumbling rough, then add two level tablespoons of coarse grit (we use 60/90 grit silicon carbide) for each pound of rock. Then, add water until the water line is just below the top of the rocks. Seal the barrel and run for about seven days.&lt;/p&gt;&lt;p&gt;At the end of seven days, open the barrel. You will find a barrel of rocks in very muddy water! Dump the contents into a screen or a colander over a plastic bucket and rinse off every speck of grit and mud. Wear safety glasses to protect your eyes from a splash of mud.&lt;/p&gt;&lt;p&gt;Used grit and rock mud should never be washed down a drain. It can clog your plumbing system. We wash rocks in a plastic colander over a plastic bucket to keep the mud out of the drain.&lt;/p&gt;&lt;head rend="h3"&gt;Inspecting the Rocks:&lt;/head&gt;Now that you have washed the rocks, it is time to inspect them. Your goal is to determine if they are ready to move on to STEP 2, or if another week in STEP 1 would improve their appearance. We almost always tumble the rocks for a second week in coarse grit. We believe that improves their shape and removes more blemishes from their surface. Then, we usually move all of the rocks to the medium grit step.&lt;head rend="h3"&gt;Perfectionist Tumbling:&lt;/head&gt;Some people want to have more control over the tumbling process and only admit excellent rocks into STEP 2. These people sort their rocks into three categories:&lt;list rend="ul"&gt;&lt;item&gt;1) those that are ready for STEP 2&lt;/item&gt;&lt;item&gt;2) those that could be improved by another week in STEP 1&lt;/item&gt;&lt;item&gt;3) those that should be discarded or trimmed and returned to STEP 1&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Here are some rocks right out of STEP 1. Note how they are covered with a gray "mud." This mud is spent grit and tiny rock particles that were worn off of the rocks during tumbling. Wash the rocks thoroughly so none of this grit goes into STEP 2. We wash our rocks in a colander over a plastic bucket so none of the mud goes down the drain. &lt;lb/&gt;IT IS VERY IMPORTANT TO WASH THE MUD FROM THE ROCKS IMMEDIATELY. If the rock mud is allowed to dry on the rocks, it is almost impossible to wash off.&lt;/p&gt;&lt;p&gt;IT IS VERY IMPORTANT TO WASH THE MUD FROM THE ROCKS IMMEDIATELY. If the rock mud is allowed to dry on the rocks, it is almost impossible to wash off.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 2 - Medium Grind&lt;/head&gt;&lt;p&gt;The second step of the four-step tumbling process is to run the rocks in the tumbler with medium grit. Before you begin it is extremely important to clean all of the coarse grit and rock mud from the rocks, from the tumbler barrel, and from the barrel lid. It is very important to avoid having even a few grains of coarse grit in the medium grind step.&lt;/p&gt;&lt;p&gt;During STEP 1, your rocks were reduced in size. When you return them to the barrel for STEP 2, they will probably not fill the barrel to the recommended 1/2 to 2/3 full level. If the barrel is only 1/2 full or less, the rocks can be tossed violently around in the tumbler. This can break or damage fragile materials such as quartz. So, when tumbling quartz or another fragile material, we always add enough ceramic media (or some rocks that need a little more tumbling) to bring the barrel up to the 1/2 to 2/3 full level.&lt;/p&gt;&lt;p&gt;(This is less important with varieties of chalcedony because it is a more durable material. However, if your tumbler barrel travels at more than about 60 revolutions per minute, we recommend adding enough ceramic media to bring it up to the 2/3 full level regardless of what type of rock is being tumbled.)&lt;/p&gt;&lt;p&gt;After your barrel is at the proper level, add two level tablespoons of medium grit (we use 110/220 grit or 150/220 grit silicon carbide) for each pound of rock (and ceramic media). Then add water until the water line is just below the top of the rocks. Now tumble for seven days.&lt;/p&gt;&lt;p&gt;At the end of seven days, open the barrel and clean all of the grit from the rocks, barrel, and lid (don't let any grit go down the drain). At this point in the tumbling process, a dry rock should have a smooth frosted surface. Inspect the rocks, looking for any that are cracked or broken. If you find any, these rocks should be discarded or saved for the next time you run Step 1.&lt;/p&gt;&lt;p&gt;Used grit and rock mud should never be washed down a drain. It can clog your plumbing system. We wash rocks in a plastic colander over a plastic bucket to keep the mud out of the drain.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 3 - Fine Grind / Pre-polish&lt;/head&gt;&lt;p&gt;The third step of the four-step tumbling process is a week in a fine grit such as 600 grit or 500 grit silicon carbide. Begin with a barrel that is perfectly clean. Place your rough and any ceramics that are with them into the barrel, and add two level tablespoons of fine grit per pound of material. Then add water until it fills the barrel up to just below the top of the rocks. Run this for about seven days, and then do a thorough cleaning of the rocks, the barrel, and the lid.&lt;/p&gt;&lt;p&gt;Remove any rocks that have broken or show signs of fracturing. At this point in the process, the rocks should be extremely smooth, and some of them might display a slight luster.&lt;/p&gt;&lt;p&gt;Be very clean! Before you replace the lid on your barrel, be sure that both the lid and the rim are perfectly clean. This will allow the lid to fit tightly and prevent leaks.&lt;/p&gt;&lt;head rend="h2"&gt;STEP 4 - Polish&lt;/head&gt;&lt;p&gt;Now you are down to the final step - the one that puts a bright shine onto your tumbled stones. Be sure that the rocks and the equipment are perfectly clean. (Some people have an extra barrel that they use only for the polishing step.) A few specks of grit could ruin a great polish.&lt;/p&gt;&lt;p&gt;Place the rocks in the barrel and add two level tablespoons of rock polish (we use TXP aluminum oxide powder for almost all of our rotary tumbling) per pound of material in the barrel. Add water to just below the top of the rocks. Then, close the barrel and run for about seven days.&lt;/p&gt;&lt;p&gt;When you finish this step, your rocks should be bright and shiny. If they are, congratulations! Admire them for a while and share them with your friends.&lt;/p&gt;&lt;p&gt;If the stones have an extremely smooth surface but do not shine, they might need to be cleaned up using the burnishing step described below. If they have scratches on them, then you will need to go back to STEP 2 and repeat the medium grind, fine grind, and polishing steps.&lt;/p&gt;&lt;p&gt;For burnishing we grate up a bar of Ivory Soap with a vegetable grater. Then we add 1/2 tablespoon of grated soap for each pound of rock plus enough warm water to almost cover the rocks. See our video about burnishing polished stones.&lt;/p&gt;&lt;head rend="h2"&gt;Burnishing&lt;/head&gt;&lt;p&gt;Sometimes our stones are a little "hazy" when they come out of the polish, or small particles of polish are in micro-size crevices. We shine and clean them up by tumbling for an hour or so in soapy water. This is called "burnishing."&lt;/p&gt;&lt;p&gt;To burnish, we place the stones in our polish barrel with the normal amount of water, and then we add about 1/2 tablespoon of grated "Ivory" bar soap for each pound of rock (we use "ORIGINAL" Ivory soap - don't use a soap with aloe or abrasive or any other additive - honestly, just get a bar of Ivory soap). Burnishing usually makes the tumbled stones a little brighter, but sometimes it really kicks up the shine.&lt;/p&gt;&lt;p&gt;Print a copy of our free tumbling log and use it to keep your records.&lt;/p&gt;&lt;p&gt;Here are a few of our favorite tumbled stones!&lt;/p&gt;&lt;head rend="h2"&gt;Keeping Records&lt;/head&gt;&lt;p&gt;It is easy to forget what day you started the tumbler or what type of grit was used - especially if you are running multiple tumblers. Keeping records will keep you on track and provide a history that will help you learn. We record material tumbled, start date, abrasive used, media used, finishing date and duration, along with any comments or observations about the results.&lt;/p&gt;&lt;p&gt;To help you with your record keeping, we have prepared a printable tumbling log.&lt;/p&gt;&lt;p&gt;We usually have multiple tumblers running here, and we record every barrel of rock that we tumble on these logs. Even if your memory is better than ours, record-keeping is a good idea. When you learn something that works or something that doesn't, you will have it recorded. This information can help you repeat great results and avoid repeating bad ones. Also, we have trouble remembering which day a barrel of rocks was started. Using the log takes away the chance of forgetting.&lt;/p&gt;&lt;head rend="h2"&gt;Happy Tumbling!&lt;/head&gt;&lt;head rend="h3"&gt;RockTumbler.com Authors&lt;/head&gt;&lt;table&gt;&lt;row&gt;&lt;cell&gt;Hobart M. King has decades of rock tumbling experience and writes most of the articles on RockTumbler.com. He has a PhD in geology and is a GIA graduate gemologist. He also writes the articles about rocks, minerals and gems on Geology.com.&lt;/cell&gt;&lt;/row&gt;&lt;/table&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45705125</guid><pubDate>Sat, 25 Oct 2025 16:32:40 +0000</pubDate></item><item><title>We do not have sufficient links to the UK for Online Safety Act to be applicable</title><link>https://libera.chat/news/advised</link><description>&lt;doc fingerprint="1e189153792602e7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The good advice&lt;/head&gt;
    &lt;p&gt;First of all, a massive thank you to everyone who donated since our last post. Our income on Liberapay has roughly quadrupled from what it was before the post. We have also had people reach out to us for large one-time monetary and hardware donations. Your support is truly appreciated!&lt;/p&gt;
    &lt;p&gt;And now for a followup from our last post. TL;DR: the legal firm we’ve engaged has sent us a memo indicating that in their opinion we can reasonably argue we do not have sufficient links to the UK for the Online Safety Act to be applicable to us. They also believe we would be at low risk of attempted enforcement action even if Ofcom does consider us to be in-scope for the OSA. We will continue to ensure that this is the case by keeping internal estimates of our UK user base and by continuing with our current efforts to keep Libera.Chat reasonably safe. We have no plans to institute any ID requirements for the forseeable future.&lt;/p&gt;
    &lt;p&gt;If that’s all you wanted to know, then feel free to stop here. However, we feel it’s in the best interest of online communities like ours for us to summarise the advice we were given in hopes that it will be useful to others. This is not legal advice from us to you. This advice was provided to Libera Chat as an assessment of our specific case. We accept no responsibility if you decide to apply advice given to us to your own online service.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why does this even matter?&lt;/head&gt;
    &lt;p&gt;You might be asking why we’ve even bothered to get legal advice on this matter. Libera Chat (the non-profit that runs the Libera.Chat IRC network) is based in Sweden. Our bank is Swedish, and we do not rely on any UK-based payment providers. We have a few servers in the UK, but they can be migrated on short notice. In other words, the British government has relatively little authority over us. The most damaging action they can reasonably take is to instruct internet service providers in the UK to deny access to us.&lt;/p&gt;
    &lt;p&gt;Relatedly, some online communities have decided that they want to minimise the authority the British government has over them. In response to critical analyses of the OSA pointing out its potential for regulatory overreach, some online communities have taken the understandable precaution of entirely blocking access from known UK IP addresses, thus cutting off any reasonable argument that they somehow have links to the UK (more on that later).&lt;/p&gt;
    &lt;p&gt;The end result is the same: a denial of service to people in the UK solely because of the country they live in. It’s not an insurmountable barrier to access in either case, but it shouldn’t be necessary for individuals in the UK to look into censorship-defeating proxies just to engage with free software developers and peer-directed projects that choose to have a community on our IRC network. It doesn’t serve our users, it doesn’t serve our communities, and it doesn’t serve the UK open source movement. Therefore, it’s in everyone’s best interest for us look into what’s necessary to keep things from getting to the point where users in the UK cannot access Libera.Chat, and that means getting guidance on the OSA.&lt;/p&gt;
    &lt;head rend="h2"&gt;Who does the OSA apply to?&lt;/head&gt;
    &lt;p&gt;As the OSA is fairly vague in its definitions, Ofcom has significant latitude in deciding where the thresholds are for whether an organisation meets certain criteria or not. Ofcom also hasn’t been forthcoming with its opinions on where those thresholds are, so there are relatively few hard guarantees about the applicability of the OSA. Still, there is a strong argument that while we definitely meet one of the criteria for the OSA to apply to us, we do not meet the other.&lt;/p&gt;
    &lt;p&gt;The OSA applies to online service providers that provide a regulated service and have links to the UK. We unarguably provide a regulated service because Libera.Chat is a so-called U2U service, i.e. it “allows ‘user-generated content’ to be encountered by another user of the service”. This is an incredibly broad class of services. Some exceptions are made for user content that is posted in relation to service content (e.g. the comment section of a blog) and a few other service types, but none of them reasonably apply to us. Every chat service, forum, federated social media server, or code forge counts as a regulated service, and therefore meets one of the criteria for the OSA to apply to them.&lt;/p&gt;
    &lt;p&gt;So be it. What about our links to the UK? To quote the memo:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;An online service provider has “links to the UK” for the purposes of the OSA if any one or more of the following apply:&lt;/p&gt;
      &lt;item&gt;the service has a “significant number of UK users”&lt;/item&gt;
      &lt;item&gt;UK users form a “target market” of the service; or&lt;/item&gt;
      &lt;item&gt;the service is capable of being used by individuals in the UK, and there are reasonable grounds to believe that there is a material risk of significant harm to individuals in the UK presented by the content generated by the service.&lt;/item&gt;
    &lt;/quote&gt;
    &lt;p&gt;One factor that does not automatically give us links to the UK is the fact that we have staff members in the UK. Curiously, employees of the service provider who do not engage with that service as users are actually excluded for the purposes of determining whether a service has a “significant number of UK users”. Our staffers are also users, but our UK staffers make up an insignificant portion of our user base.&lt;/p&gt;
    &lt;p&gt;Speaking of which, the memo implies that “significance” in this context is interpreted as being relative to the population of the UK, not relative to the user base of the service. We have seen risk assessments that take the other interpretation and consider their UK user base to be “significant” because it makes up a large portion of their overall user base, but the advice we received suggests we should not use this interpretation. The exact fraction of the UK’s online population that must use a given service to be considered “significant” is unknown, but based on our counsel’s observations of Ofcom’s previous regulatory actions, it appears to be much higher than our internal estimates of how large our UK user base is.&lt;/p&gt;
    &lt;p&gt;The “target market” criterion is meant to capture services with a low number of UK users that target the UK specifically. While our target market (people interested in using an IRC-based platform for discussing free software or other peer-directed projects) is inclusive of UK users, it isn’t specifically for them. Our network is predominantly English-speaking, but we do not promote, direct, or tailor our service to UK users in particular.&lt;/p&gt;
    &lt;p&gt;Finally, there is no atypical material risk of significant harm to individuals in the UK presented by the messages on Libera.Chat. We block spam and client exploits. We are proactive in ensuring that our network’s acceptable use policy is upheld. We do not tolerate incitement to violence, doxxing, or defamation. And finally, we do not provide file hosting that can be used to distribute pornographic or sexual abuse media, though when we sought legal advice from the firm, we acknowledged the existence of DCC as a commonly-supported mechanism for transferring files using an IRC network to establish a peer-to-peer connection.&lt;/p&gt;
    &lt;p&gt;In the coming weeks, we will be finalising a statement similar to this risk assessment that we can provide to Ofcom should we ever be contacted by them about the OSA.&lt;/p&gt;
    &lt;head rend="h2"&gt;What if the OSA does apply to us?&lt;/head&gt;
    &lt;p&gt;While it is our opinion that the OSA does not apply to us, Ofcom might disagree, and appealing that disagreement would likely involve further legal expenses. So, what is the risk that Ofcom would decide to try to impose fines or other regulatory penalties on us?&lt;/p&gt;
    &lt;p&gt;For the time being, services like ours do not appear to be Ofcom’s priority. Currently, according to our legal sources, the focus appears to be file and image hosts that are at high risk of being used to transmit sexually-explicit depictions of minors. IRC has been used as a way to facilitate piracy, but those days are generally in the past thanks to more attractive options. Even if they weren’t, using Libera.Chat for this purpose is risky. We prefer to exercise the minimum power necessary to keep the network clean, but that doesn’t mean we don’t have the tools necessary to proactively stop the network from being used for piracy or CSAM distribution.&lt;/p&gt;
    &lt;p&gt;We have also been reassured that Ofcom is very likely to contact us with concerns before attempting any sort of action against us. There are some classes of concerns that we would certainly be willing to hear out, and we do prefer a constructive approach to problem resolution where possible. We’re confident that there isn’t anything for them to be reasonably concerned about, but we are willing to engage with good-faith reports of potential abuse of our service.&lt;/p&gt;
    &lt;head rend="h2"&gt;Will Libera.Chat ever require my ID?&lt;/head&gt;
    &lt;p&gt;We have no plans to require users to provide us with proof of identity and will take every reasonable measure to avoid requiring it. The justification for us to compromise the privacy of our users given the content we forbid on Libera.Chat is not adequate, and the risk of material harm should an identity verification mechanism compromise our users’ privacy far outweighs the plausible harms caused by not having such a system. Such violations of privacy aren’t hypothetical; another chat platform recently was affected by a data breach that potentially exposed the legal identities of tens of thousands of its users.&lt;/p&gt;
    &lt;p&gt;That said, it’s conceivable that legislation will be created that could apply to us and could force us to identify or spy on our users. If that happens, we will evaluate our options once drafts of such legislation reach a point where they can conceivably pass. Until then, we hope that the general public will remain vocally opposed to such attempts at overreach. Popular opposition stalled Chat Control earlier this month. There will probably always be efforts to compromise the free internet, but their success is not inevitable.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45705381</guid><pubDate>Sat, 25 Oct 2025 17:07:40 +0000</pubDate></item><item><title>Switzerland is spending millions revamping its vast network of bunkers</title><link>https://www.washingtonpost.com/world/2025/10/25/switzerland-nuclear-bunkers-overhaul/</link><description>&lt;doc fingerprint="64adaad95b4fa3aa"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I want to build a little device that connects two computers together via usb, and send keystrokes from one to the other.&lt;/p&gt;
      &lt;p&gt;(I would use it to use a laptop keyboard on a headless computers).&lt;/p&gt;
      &lt;p&gt;I am looking for an easy solution, it does not have to be the cheapest.&lt;/p&gt;
      &lt;p&gt;ChatGPT points me to Arduino, but as far as I can see, there's no arduino with 2 usb ports. It also points me to Raspery pi zero, but that's a computer, not a microcontroller, so not sure if it's suitable.&lt;/p&gt;
      &lt;p&gt;If anyone with experience can give me some pointers, it would be greatly appreciated!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706077</guid><pubDate>Sat, 25 Oct 2025 18:47:55 +0000</pubDate></item><item><title>Ask HN: Advice for creating a USB device linking 2 computers</title><link>https://news.ycombinator.com/item?id=45706169</link><description>&lt;doc fingerprint="64adaad95b4fa3aa"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;I want to build a little device that connects two computers together via usb, and send keystrokes from one to the other.&lt;/p&gt;
      &lt;p&gt;(I would use it to use a laptop keyboard on a headless computers).&lt;/p&gt;
      &lt;p&gt;I am looking for an easy solution, it does not have to be the cheapest.&lt;/p&gt;
      &lt;p&gt;ChatGPT points me to Arduino, but as far as I can see, there's no arduino with 2 usb ports. It also points me to Raspery pi zero, but that's a computer, not a microcontroller, so not sure if it's suitable.&lt;/p&gt;
      &lt;p&gt;If anyone with experience can give me some pointers, it would be greatly appreciated!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706169</guid><pubDate>Sat, 25 Oct 2025 19:01:51 +0000</pubDate></item><item><title>The Journey Before main()</title><link>https://amit.prasad.me/blog/before-main</link><description>&lt;doc fingerprint="1b83387246253a75"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;&amp;gt; The Journey Before main()_&lt;/head&gt;
    &lt;p&gt;October 25, 2025 · Amit Prasad&lt;/p&gt;
    &lt;p&gt;A while back, I worked on a RISC-V-based userspace simulator for fun. In doing so, taught myself a lot more than I wanted to know about what happens in-between when the Kernel is asked to run a program, and when the first line of our program’s &lt;code&gt;main&lt;/code&gt; function is actually executed. Here’s a summary of that rabbit hole.&lt;/p&gt;
    &lt;head rend="h2"&gt;In the beginning…&lt;/head&gt;
    &lt;p&gt;First question: When is the OS kernel actually asked to run any program? The answer, at least on Linux, is the &lt;code&gt;execve&lt;/code&gt; system call (“syscall”). Let’s take a quick look at that:&lt;/p&gt;
    &lt;code&gt;int execve(const char *filename, char *const argv[], char *const envp[]);&lt;/code&gt;
    &lt;p&gt;This is actually quite straightforward! We pass the name of the exectuable file, a list of arguments, and a list of environment variables. This signals to the kernel where, and how, to start loading the program.&lt;/p&gt;
    &lt;p&gt;Many programming languages provide an interface to execute commands that eventually call &lt;code&gt;execve&lt;/code&gt; under the hood. For example, in Rust, we have:&lt;/p&gt;
    &lt;code&gt;use std::process::Command;

Command::new("ls").arg("-l").spawn();&lt;/code&gt;
    &lt;p&gt;In these higher-level wrappers, the language’s standard library often handles translation of the command name to a full path, acting similarly to how a shell would resolve the command via the &lt;code&gt;PATH&lt;/code&gt; environment variable. The kernel itself, however, expects a proper path to an executable file.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;A note on interpreters: If the executable file starts with a shebang (&lt;/p&gt;&lt;code&gt;#!&lt;/code&gt;), the kernel will use the shebang-specified interpreter to run the program. For example,&lt;code&gt;#!/usr/bin/python3&lt;/code&gt;will run the program using the Python interpreter,&lt;code&gt;#!/bin/bash&lt;/code&gt;will run the program using the Bash shell, etc.&lt;/quote&gt;
    &lt;head rend="h2"&gt;ELF&lt;/head&gt;
    &lt;p&gt;What does an executable file look like? On Linux, it’s ELF, which the kernel knows how to parse. Other operating systems have different formats (e.g. Mach-O on MacOS, PE on Windows), but ELF is the most common format on Linux. I won’t go into too much detail here, to keep things brief, but ELF files have grown out of the original &lt;code&gt;a.out&lt;/code&gt; format, and are expressive enough to support pretty much every program you’ll ever write. Here’s what the header of an ELF file looks like:&lt;/p&gt;
    &lt;code&gt;% readelf -h main # main is an ELF file
ELF Header:
  Magic:   7f 45 4c 46 01 01 01 03 00 00 00 00 00 00 00 00
  Class:                             ELF32
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - GNU
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           RISC-V
  Version:                           0x1
  Entry point address:               0x10358
  Start of program headers:          52 (bytes into file)
  Start of section headers:          675776 (bytes into file)
  Flags:                             0x1, RVC, soft-float ABI
  Size of this header:               52 (bytes)
  Size of program headers:           32 (bytes)
  Number of program headers:         7
  Size of section headers:           40 (bytes)
  Number of section headers:         32
  Section header string table index: 31&lt;/code&gt;
    &lt;p&gt;The important parts here are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The “ELF Magic” bytes, which tell the kernel that this is, indeed, an ELF file. &lt;code&gt;45 4c 46&lt;/code&gt;is ASCII for “ELF”!&lt;/item&gt;
      &lt;item&gt;“Class” tells us we’re dealing with a 32-bit executable.&lt;/item&gt;
      &lt;item&gt;“Start of …” tells us where things are in the file, and “Size of …” tells us how big they are; The kernel is effectively given a map of the file.&lt;/item&gt;
      &lt;item&gt;“Entry point address” — Relatively self-explanatory! But we’ll be coming back to this.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Other ELF files will have different entries and specific values, but the general structure is what we’re after here.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;As you can see by the numerous mentions to “RISC-V”, this is an ELF file I compiled and linked targeting the RV32 architecture (which the aforementioned emulator is built for), hence the “32” in “ELF32”, the “RVC” flag, and the “RISC-V” machine type.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h3"&gt;More than just a header&lt;/head&gt;
    &lt;p&gt;ELF files contain everything our program needs to run, including the code, data, symbols, and more. We can see this again with the &lt;code&gt;readelf&lt;/code&gt; command with the &lt;code&gt;-a&lt;/code&gt; flag. Here’s what we care about:&lt;/p&gt;
    &lt;code&gt;Section Headers:
  [Nr] Name              Type            Addr     Off    Size
  [ 0]                   NULL            00000000 000000 000000
  [ 1] .note.ABI-tag     NOTE            00010114 000114 000020
  [ 2] .rela.plt         RELA            00010134 000134 00000c
  [ 3] .plt              PROGBITS        00010140 000140 000010
  [ 4] .text             PROGBITS        00010150 000150 03e652
  [ 5] .rodata           PROGBITS        0004e7b0 03e7b0 01b208
  ...
  [16] .data             PROGBITS        0007a008 069008 000dec
  [17] .sdata            PROGBITS        0007adf4 069df4 000004
  [18] .bss              NOBITS          0007adf8 069df8 002b6c
  ...
  [29] .symtab           SYMTAB          00000000 095124 009040
  [30] .strtab           STRTAB          00000000 09e164 006d10&lt;/code&gt;
    &lt;p&gt;These sections contain code (&lt;code&gt;.text&lt;/code&gt;), data (&lt;code&gt;.data&lt;/code&gt;), space for global variables (&lt;code&gt;.bss&lt;/code&gt;), shims for accessing shared library functions (&lt;code&gt;.plt&lt;/code&gt;), and quite a bit more (including symbol tables for debugging, relocation tables, etc.), most of which we won’t be discussing.&lt;/p&gt;
    &lt;p&gt;So evidently, there’s some code that we care about in the &lt;code&gt;.text&lt;/code&gt; section, so we copy that and call it a day? Not quite. There’s a massive amount of machinery inside the kernel to make all sorts of programs under all sorts of conditions run.&lt;/p&gt;
    &lt;p&gt;For example, the “PLT” (Procedure Linkage Table) is a section that allows us to call functions in “shared libraries”, for example, &lt;code&gt;libc&lt;/code&gt;, without having to package them alongside our program (“dynamically” vs “statically linking”). The ELF file contains a dynamic section which tells the kernel which shared libraries to load, and another section which tells the kernel to dynamically “relocate” pointers to those functions, so everything checks out.&lt;/p&gt;
    &lt;quote&gt;&lt;code&gt;libc&lt;/code&gt;is the C standard library, which contains all the “useful” functions:&lt;code&gt;printf&lt;/code&gt;,&lt;code&gt;malloc&lt;/code&gt;, etc. Various flavors implementing the&lt;code&gt;libc&lt;/code&gt;interfaces exist, most commonly&lt;code&gt;glibc&lt;/code&gt;and&lt;code&gt;musl&lt;/code&gt;. Most of the binaries that are discussed in this post are compiled and linked against&lt;code&gt;musl&lt;/code&gt;, since it’s much easier to statically link.&lt;/quote&gt;
    &lt;p&gt;The symbol table looks something like this:&lt;/p&gt;
    &lt;code&gt;Symbol table '.symtab' contains 2308 entries:
   Num:    Value  Size Type    Bind   Vis      Ndx Name
     0: 00000000     0 NOTYPE  LOCAL  DEFAULT  UND
     1: 00010114     0 SECTION LOCAL  DEFAULT    1 .note.ABI-tag
     2: 00010134     0 SECTION LOCAL  DEFAULT    2 .rela.plt
     3: 00010140     0 SECTION LOCAL  DEFAULT    3 .plt
     4: 00010150     0 SECTION LOCAL  DEFAULT    4 .text
     ...
     1782: 00010358    30 FUNC    GLOBAL HIDDEN     4 _start
     ...
     1917: 00010430    52 FUNC    GLOBAL DEFAULT    4 main
     2201: 00010506   450 FUNC    GLOBAL HIDDEN     4 __libc_start_main
     ...&lt;/code&gt;
    &lt;p&gt;You may ask: “Wow! &lt;code&gt;2308&lt;/code&gt; looks like a lot, right? What behemoth of a program could possibly need that many symbols?“.&lt;/p&gt;
    &lt;p&gt;Good question! Here’s the behemoth:&lt;/p&gt;
    &lt;code&gt;#include &amp;lt;stdio.h&amp;gt;

int main() {
  printf("Hello, World!\n");
  return 0;
}&lt;/code&gt;
    &lt;p&gt;Yeah, that’s it. Now, &lt;code&gt;2308&lt;/code&gt; may be slightly bloated because we link against &lt;code&gt;musl&lt;/code&gt; instead of &lt;code&gt;glibc&lt;/code&gt;, but the point still stands: There’s a lot of stuff going on behind the scenes here.&lt;/p&gt;
    &lt;p&gt;The kernel’s job here is to iterate over each section, loading those marked as “loadable”. Some security mitigations start to become relevant here, such as moving sections around in memory (ASLR — Address Space Layout Randomization), marking sections as non-executable (NX bit — hardware-level security), etc. But ultimately, the kernel loads the code and data into memory, sets up the stack, and prepares to jump to the entry point of the program.&lt;/p&gt;
    &lt;head rend="h2"&gt;The stack&lt;/head&gt;
    &lt;p&gt;Ah yes, the infamous stack! Fortunately for most of us, the stack is something we take for granted. Unfortunately for the kernel, the stack is not some omnipotent magical space that just exists — it needs to be set up properly before our program can run.&lt;/p&gt;
    &lt;p&gt;As a reminder: stack space is typically used for variables, function arguments, “frames” (to keep track of function-local variables, call trees, etc), and a variety of other things, depending on what, and how your program is running.&lt;/p&gt;
    &lt;p&gt;Hypothetically, if we simplify a bit and say that the ELF file is loaded into memory starting at the zero address, the stack is typically placed at the “opposite end” of the memory, from a high address, and grows “downwards” towards the lower addresses, with the space in-between used as heap space, and for other data (shared libraries, mmapped files, etc). This is a simplification, but in fairness, there is significant ambiguity as much of the semantics here depend on the program itself.&lt;/p&gt;
    &lt;p&gt;The stack is also something that is non-empty! Remember &lt;code&gt;argv&lt;/code&gt; and &lt;code&gt;envp&lt;/code&gt; from the &lt;code&gt;execve&lt;/code&gt; call above? Those are passed to the program via the stack. In most programming languages we frequently access these via the various &lt;code&gt;args&lt;/code&gt; and &lt;code&gt;env&lt;/code&gt; utilities, whether that be directly, like in C, or more indirectly, like in Rust (&lt;code&gt;std::env&lt;/code&gt;) or Python (&lt;code&gt;sys.argv&lt;/code&gt;).&lt;/p&gt;
    &lt;p&gt;The kernel also stores something called the “ELF auxiliary vector” in the nascent stack. This “auxv” contains information about the environment, such as the memory page size, metadata from the ELF file, and other system information. These are important! For example, &lt;code&gt;musl&lt;/code&gt; uses the “page size” entry of the auxv so that &lt;code&gt;malloc&lt;/code&gt; can request and manage memory more optimally. There are over 30 entries in the auxiliary vectors, but not all of them are used by every program (and some may not be defined by the kernel).&lt;/p&gt;
    &lt;p&gt;Let’s pretend we’re the kernel. Here’s a simplified version of how we might setup the stack of a new process (taken and simplified from my RISC-V emulator, which also emulates parts of the kernel):&lt;/p&gt;
    &lt;code&gt;// Choose an arbitrary high address for the stack
let mut sp = 0xCFFF_F000u32; // sp = "stack pointer"
let mut stack_init: Vec&amp;lt;u32&amp;gt; = vec![]; // The stack begins empty.

stack_init.push(args.len()); // argc: number of arguments
for &amp;amp;arg in args.iter().rev() {
    // Copy each argument to the stack
    sp -= arg.len() // move "downwards" in address space
    mem.copy_to(sp, arg);

    // Keep track of the arg pointer in the init vector
    stack_init.push(sp);
}
stack_init.push(0); // argv NULL terminator

// Environment variables are similar:
for &amp;amp;e in env.iter().rev() {
    sp -= e.len();
    mem.copy_to(sp, e);

    stack_init.push(sp);
}
stack_init.push(0); // envp NULL terminator

// Setup the auxiliary vector
stack_init.push(libc_riscv32::AT_PAGESZ); // Keys for auxv
stack_init.push(0x1000); // Values for auxv; this specifies a 4 KiB page size
stack_init.push(libc_riscv32::AT_ENTRY);
stack_init.push(self.pc); // N.B.: We'll be coming back to this
// ...

// Copy the stack init vector, with all the pointers, to the stack
sp -= (stack_init.len() * 4);

mem.copy_to(sp, &amp;amp;stack_init)&lt;/code&gt;
    &lt;p&gt;A diagram might help illustrate what the address space looks like at this point:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Argument Count (argc)&lt;/item&gt;
      &lt;item&gt;Arguments (argv)&lt;/item&gt;
      &lt;item&gt;Environment variables (envp)&lt;/item&gt;
      &lt;item&gt;Auxiliary Vector (auxv)&lt;/item&gt;
      &lt;item&gt;Local variables, stack frames, function calls&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;↓&lt;/p&gt;
    &lt;p&gt;Grows downward&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Shared libraries (libc, etc.)&lt;/item&gt;
      &lt;item&gt;Memory-mapped files&lt;/item&gt;
      &lt;item&gt;Dynamic linker/loader&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;↑&lt;/p&gt;
    &lt;p&gt;Grows upward&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;malloc(), calloc(), realloc() allocations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.bss &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Uninitialized global variables&lt;/item&gt;
      &lt;item&gt;Static variables initialized to zero&lt;/item&gt;
      &lt;item&gt;Zero-filled by kernel on program start&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.data &lt;lb/&gt; "read-write"&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Initialized global variables&lt;/item&gt;
      &lt;item&gt;Static variables with initial values&lt;/item&gt;
      &lt;item&gt;Read-write data from the executable&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.rodata &lt;lb/&gt; "read-only"&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Literals ("Hello, World!")&lt;/item&gt;
      &lt;item&gt;Constant data&lt;/item&gt;
      &lt;item&gt;Read-only variables&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;.text &lt;lb/&gt; "code"&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Program instructions/machine code&lt;/item&gt;
      &lt;item&gt;_start function (entry point)&lt;/item&gt;
      &lt;item&gt;User code&lt;/item&gt;
      &lt;item&gt;Library function code&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Entrypoint&lt;/head&gt;
    &lt;p&gt;Finally, we get to the “entry point” address, mentioned at several points. This is the address of the first instruction to run in the process. Typically, this is under a function called &lt;code&gt;_start&lt;/code&gt;. Both glibc and musl provide implementations of &lt;code&gt;_start&lt;/code&gt;, but it’s also possible to write your own. Again, here’s a Rust example:&lt;/p&gt;
    &lt;code&gt;// Disable the language runtime, we're DIYing it.
#![no_std]
#![no_main]

#[panic_handler]
fn panic(_info: &amp;amp;core::panic::PanicInfo) -&amp;gt; ! {
    loop {}
}

#[no_mangle]
pub extern "C" fn _start() -&amp;gt; ! {
    // Instead of "waiting" for main, we can immediately start execution.
    loop {}
}&lt;/code&gt;
    &lt;p&gt;Depending on your program, &lt;code&gt;_start&lt;/code&gt; may be the only thing between the entrypoint and your main function, but most languages have some sort of runtime that needs to be initialized first. For example, Rust has &lt;code&gt;std::rt::lang_start&lt;/code&gt;. It’s at this part that things like global constructors, thread-local storage, and other language-specific features are set up.&lt;/p&gt;
    &lt;p&gt;Here, our journey comes to an end — things become much more language-specific from this point on. Most languages will set up their own runtimes (yes, even C and C++ have a “runtime”!), and eventually call the standard &lt;code&gt;main&lt;/code&gt; function we’re normally familiar with.&lt;/p&gt;
    &lt;p&gt;In Rust, the generated code ends up looking like the following:&lt;/p&gt;
    &lt;code&gt;// the user defined main function
fn main() { println!("Hello, world!"); }

// the generated _start function
fn _start() -&amp;gt; {
    let argc = ...; // get argc from stack
    let argv = ...; // get argv from stack
    let envp = ...; // get envp from stack
    let main_fn = main; // pointer to user main function
    std::rt::lang_start(argc, argv, main_fn);
}&lt;/code&gt;
    &lt;p&gt;With the &lt;code&gt;lang_start&lt;/code&gt; function (defined here)[https://github.com/rust-lang/rust/blob/04ff05c9c0cfbca33115c5f1b8bb20a66a54b799/library/std/src/rt.rs#L199] and taking care of the rest.&lt;/p&gt;
    &lt;p&gt;C and C++ have similar, minimal setups. Languages that are traditionally thought to have “heavier” runtimes, such as Java or Python, work the same way, but with the &lt;code&gt;std::rt::lang_start&lt;/code&gt; equivalent doing far more than the Rust/C/C++ runtimes.&lt;/p&gt;
    &lt;p&gt;And there you have it! I’m missing lots of detail here, but hopefully this gives a rough idea of what happens before &lt;code&gt;main()&lt;/code&gt; gets called. I’ve left out complexity that is mostly internal to “real” linux kernels, such as how the kernel sets up address space, the process tables, various group semantics, and et cetera, but I hope this still serves as a decent primer.&lt;/p&gt;
    &lt;p&gt;Feel free to reach out to me with any questions or corrections!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45706380</guid><pubDate>Sat, 25 Oct 2025 19:33:22 +0000</pubDate></item></channel></rss>