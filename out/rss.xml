<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 23 Sep 2025 09:11:07 +0000</lastBuildDate><item><title>I'm spoiled by Apple Silicon but still love Framework</title><link>https://simonhartcher.com/posts/2025-09-22-why-im-spoiled-by-apple-silicon-but-still-love-framework/</link><description>&lt;doc fingerprint="9eb0593b034f91dc"&gt;
  &lt;main&gt;
    &lt;p&gt;I mainly use my MacBook M1 Pro with Apple Silicon for work. Recently I‚Äôve had a few weeks off. It‚Äôs been sitting in my laptop bag waiting for me to return to work.&lt;/p&gt;
    &lt;p&gt;Since I‚Äôm back to work tomorrow I got it out to see if it was charged enough for the train journey in the morning. 90% remaining. After 3 weeks. It wasn‚Äôt turned off, just closed.&lt;/p&gt;
    &lt;p&gt;In contrast, my Framework 13 with an AMD Ryzen 7840HS is almost always dead when I go to use it. It‚Äôs very frustrating. I‚Äôm not using it every day so it often goes 2-3 days without being opened. I haven‚Äôt measured it but I read that I should expect it to lose 3-4% in suspend every hour. Is that a joke?&lt;/p&gt;
    &lt;p&gt;I was running Fedora Workstation on the 13 for a long while until I had a short stint with Arch Linux. It wasn‚Äôt stable enough so now I‚Äôm on Fedora Silverblue which has been delightful.&lt;/p&gt;
    &lt;p&gt;However, the Framework‚Äôs battery life continues to be an issue.&lt;/p&gt;
    &lt;p&gt;I‚Äôm a true believer of Framework‚Äôs mission. The technology is awesome. But why is the battery life of modern laptops (other than Apple Silicon) so bad? I don‚Äôt think it‚Äôs unique to Framework, but it affects my love for this groundbreaking device.&lt;/p&gt;
    &lt;p&gt;Apple Silicon is built upon ARM64 which is apparently core to such great battery life. Do I need to get an ARM mainboard as soon as Framework offers it as an upgrade? I‚Äôm not sure. It appears to be way more complicated than just switching to ARM64.&lt;/p&gt;
    &lt;p&gt;I still love my Framework, despite its flaws. I will just keep it plugged in so that it‚Äôs ready to go when I want to use it.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45332859</guid><pubDate>Mon, 22 Sep 2025 13:03:10 +0000</pubDate></item><item><title>Cap'n Web: a new RPC system for browsers and web servers</title><link>https://blog.cloudflare.com/capnweb-javascript-rpc-library/</link><description>&lt;doc fingerprint="e228a11afa2902c6"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Allow us to introduce Cap'n Web, an RPC protocol and implementation in pure TypeScript.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is a spiritual sibling to Cap'n Proto, an RPC protocol I (Kenton) created a decade ago, but designed to play nice in the web stack. That means:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Like Cap'n Proto, it is an object-capability protocol. ("Cap'n" is short for "capabilities and".) We'll get into this more below, but it's incredibly powerful.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Unlike Cap'n Proto, Cap'n Web has no schemas. In fact, it has almost no boilerplate whatsoever. This means it works more like the JavaScript-native RPC system in Cloudflare Workers.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;That said, it integrates nicely with TypeScript.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Also unlike Cap'n Proto, Cap'n Web's underlying serialization is human-readable. In fact, it's just JSON, with a little pre-/post-processing.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It works over HTTP, WebSocket, and postMessage() out-of-the-box, with the ability to extend it to other transports easily.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It works in all major browsers, Cloudflare Workers, Node.js, and other modern JavaScript runtimes.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;The whole thing compresses (minify+gzip) to under 10√Ç kB with no dependencies.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;It's open source under the MIT license.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Cap'n Web is more expressive than almost every other RPC system, because it implements an object-capability RPC model. That means it:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;Supports bidirectional calling. The client can call the server, and the server can also call the client.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports passing functions by reference: If you pass a function over RPC, the recipient receives a "stub". When they call the stub, they actually make an RPC back to you, invoking the function where it was created. This is how bidirectional calling happens: the client passes a callback to the server, and then the server can call it later.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Similarly, supports passing objects by reference: If a class extends the special marker type &lt;code&gt;RpcTarget&lt;/code&gt;, then instances of that class are passed by reference, with method calls calling back to the location where the object was created.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports promise pipelining. When you start an RPC, you get back a promise. Instead of awaiting it, you can immediately use the promise in dependent RPCs, thus performing a chain of calls in a single network round trip.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Supports capability-based security patterns.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;In short, Cap'n Web lets you design RPC interfaces the way you'd design regular JavaScript APIs √¢ while still acknowledging and compensating for network latency.&lt;/p&gt;
      &lt;p&gt;The best part is, Cap'n Web is absolutely trivial to set up.&lt;/p&gt;
      &lt;p&gt;A client looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { newWebSocketRpcSession } from "capnweb";

// One-line setup.
let api = newWebSocketRpcSession("wss://example.com/api");

// Call a method on the server!
let result = await api.hello("World");

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And here's a complete Cloudflare Worker implementing an RPC server:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { RpcTarget, newWorkersRpcResponse } from "capnweb";

// This is the server implementation.
class MyApiServer extends RpcTarget {
  hello(name) {
    return `Hello, ${name}!`
  }
}

// Standard Workers HTTP handler.
export default {
  fetch(request, env, ctx) {
    // Parse URL for routing.
    let url = new URL(request.url);

    // Serve API at `/api`.
    if (url.pathname === "/api") {
      return newWorkersRpcResponse(request, new MyApiServer());
    }

    // You could serve other endpoints here...
    return new Response("Not found", {status: 404});
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;That's it. That's the app.&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;You can add more methods to &lt;code&gt;MyApiServer&lt;/code&gt;, and call them from the client.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can have the client pass a callback function to the server, and then the server can just call it.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;You can define a TypeScript interface for your API, and easily apply it to the client and server.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;It just works.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Why RPC? (And what is RPC anyway?)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Remote Procedure Calls (RPC) are a way of expressing communications between two programs over a network. Without RPC, you might communicate using a protocol like HTTP. With HTTP, though, you must format and parse your communications as an HTTP request and response, perhaps designed in REST style. RPC systems try to make communications look like a regular function call instead, as if you were calling a library rather than a remote service. The RPC system provides a "stub" object on the client side which stands in for the real server-side object. When a method is called on the stub, the RPC system figures out how to serialize and transmit the parameters to the server, invoke the method on the server, and then transmit the return value back.&lt;/p&gt;
      &lt;p&gt;The merits of RPC have been subject to a great deal of debate. RPC is often accused of committing many of the fallacies of distributed computing.&lt;/p&gt;
      &lt;p&gt;But this reputation is outdated. When RPC was first invented some 40 years ago, async programming barely existed. We did not have Promises, much less async and await. Early RPC was synchronous: calls would block the calling thread waiting for a reply. At best, latency made the program slow. At worst, network failures would hang or crash the program. No wonder it was deemed "broken".&lt;/p&gt;
      &lt;p&gt;Things are different today. We have Promise and async and await, and we can throw exceptions on network failures. We even understand how RPCs can be pipelined so that a chain of calls takes only one network round trip. Many large distributed systems you likely use every day are built on RPC. It works.&lt;/p&gt;
      &lt;p&gt;The fact is, RPC fits the programming model we're used to. Every programmer is trained to think in terms of APIs composed of function calls, not in terms of byte stream protocols nor even REST. Using RPC frees you from the need to constantly translate between mental models, allowing you to move faster.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;When should you use Cap'n Web?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cap'n Web is useful anywhere where you have two JavaScript applications speaking to each other over a network, including client-to-server and microservice-to-microservice scenarios. However, it is particularly well-suited to interactive web applications with real-time collaborative features, as well as modeling interactions over complex security boundaries.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is still new and experimental, so for now, a willingness to live on the cutting edge may also be required!&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;Features, features, features√¢¬¶&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Here's some more things you can do with Cap'n Web.&lt;/p&gt;
      &lt;p&gt;Sometimes a WebSocket connection is a bit too heavyweight. What if you just want to make a quick one-time batch of calls, but don't need an ongoing connection?&lt;/p&gt;
      &lt;p&gt;For that, Cap'n Web supports HTTP batch mode:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { newHttpBatchRpcSession } from "capnweb";

let batch = newHttpBatchRpcSession("https://example.com/api");

let result = await batch.hello("World");

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;(The server is exactly the same as before.)&lt;/p&gt;
      &lt;p&gt;Note that once you've awaited an RPC in the batch, the batch is done, and all the remote references received through it become broken. To make more calls, you need to start over with a new batch. However, you can make multiple calls in a single batch:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let batch = newHttpBatchRpcSession("https://example.com/api");

// We can call make multiple calls, as long as we await them all at once.
let promise1 = batch.hello("Alice");
let promise2 = batch.hello("Bob");

let [result1, result2] = await Promise.all([promise1, promise2]);

console.log(result1);
console.log(result2);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;And that brings us to another feature√¢¬¶&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Chained calls (Promise Pipelining)&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Here's where things get magical.&lt;/p&gt;
      &lt;p&gt;In both batch mode and WebSocket mode, you can make a call that depends on the result of another call, without waiting for the first call to finish. In batch mode, that means you can, in a single batch, call a method, then use its result in another call. The entire batch still requires only one network round trip.&lt;/p&gt;
      &lt;p&gt;For example, say your API is:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;class MyApiServer extends RpcTarget {
  getMyName() {
    return "Alice";
  }

  hello(name) {
    return `Hello, ${name}!`
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;You can do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let namePromise = batch.getMyName();
let result = await batch.hello(namePromise);

console.log(result);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Notice the initial call to &lt;code&gt;getMyName()&lt;/code&gt; returned a promise, but we used the promise itself as the input to &lt;code&gt;hello()&lt;/code&gt;, without awaiting it first. With Cap'n Web, this just works: The client sends a message to the server saying: "Please insert the result of the first call into the parameters of the second."&lt;/p&gt;
      &lt;p&gt;Or perhaps the first call returns an object with methods. You can call the methods immediately, without awaiting the first promise, like:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let batch = newHttpBatchRpcSession("https://example.com/api");

// Authencitate the API key, returning a Session object.
let sessionPromise = batch.authenticate(apiKey);

// Get the user's name.
let name = await sessionPromise.whoami();

console.log(name);
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This works because the promise returned by a Cap'n Web call is not a regular promise. Instead, it's a JavaScript Proxy object. Any methods you call on it are interpreted as speculative method calls on the eventual result. These calls are sent to the server immediately, telling the server: "When you finish the call I sent earlier, call this method on what it returns."&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Did you spot the security?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;This last example shows an important security pattern enabled by Cap'n Web's object-capability model.&lt;/p&gt;
      &lt;p&gt;When we call the authenticate() method, after it has verified the provided API key, it returns an authenticated session object. The client can then make further RPCs on the session object to perform operations that require authorization as that user. The server code might look like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;class MyApiServer extends RpcTarget {
  authenticate(apiKey) {
    let username = await checkApiKey(apiKey);
    return new AuthenticatedSession(username);
  }
}

class AuthenticatedSession extends RpcTarget {
  constructor(username) {
    super();
    this.username = username;
  }

  whoami() {
    return this.username;
  }

  // ...other methods requiring auth...
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Here's what makes this work: It is impossible for the client to "forge" a session object. The only way to get one is to call authenticate(), and have it return successfully.&lt;/p&gt;
      &lt;p&gt;In most RPC systems, it is not possible for one RPC to return a stub pointing at a new RPC object in this way. Instead, all functions are top-level, and can be called by anyone. In such a traditional RPC system, it would be necessary to pass the API key again to every function call, and check it again on the server each time. Or, you'd need to do authorization outside the RPC system entirely.&lt;/p&gt;
      &lt;p&gt;This is a common pain point for WebSockets in particular. Due to the design of the web APIs for WebSocket, you generally cannot use headers nor cookies to authorize them. Instead, authorization must happen in-band, by sending a message over the WebSocket itself. But this can be annoying for RPC protocols, as it means the authentication message is "special" and changes the state of the connection itself, affecting later calls. This breaks the abstraction.&lt;/p&gt;
      &lt;p&gt;The authenticate() pattern shown above neatly makes authentication fit naturally into the RPC abstraction. It's even type-safe: you can't possibly forget to authenticate before calling a method requiring auth, because you wouldn't have an object on which to make the call. Speaking of type-safety√¢¬¶&lt;/p&gt;
      &lt;p&gt;If you use TypeScript, Cap'n Web plays nicely with it. You can declare your RPC API once as a TypeScript interface, implement in on the server, and call it on the client:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Shared interface declaration:
interface MyApi {
  hello(name: string): Promise&amp;lt;string&amp;gt;;
}

// On the client:
let api: RpcStub&amp;lt;MyApi&amp;gt; = newWebSocketRpcSession("wss://example.com/api");

// On the server:
class MyApiServer extends RpcTarget implements MyApi {
  hello(name) {
    return `Hello, ${name}!`
  }
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Now you get end-to-end type checking, auto-completed method names, and so on.&lt;/p&gt;
      &lt;p&gt;Note that, as always with TypeScript, no type checks occur at runtime. The RPC system itself does not prevent a malicious client from calling an RPC with parameters of the wrong type. This is, of course, not a problem unique to Cap'n Web √¢ JSON-based APIs have always had this problem. You may wish to use a runtime type-checking system like Zod to solve this. (Meanwhile, we hope to add type checking based directly on TypeScript types in the future.)&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;An alternative to GraphQL?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;If you√¢ve used GraphQL before, you might notice some similarities. One benefit of GraphQL was to solve the √¢waterfall√¢ problem of traditional REST APIs by allowing clients to ask for multiple pieces of data in one query. For example, instead of making three sequential HTTP calls:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;GET /user
GET /user/friends
GET /user/friends/photos&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;√¢¬¶you can write one GraphQL query to fetch it all at once.&lt;/p&gt;
      &lt;p&gt;That√¢s a big improvement over REST, but GraphQL comes with its own tradeoffs:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;New language and tooling. You have to adopt GraphQL√¢s schema language, servers, and client libraries. If your team is all-in on JavaScript, that√¢s a lot of extra machinery.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Limited composability. GraphQL queries are declarative, which makes them great for fetching data, but awkward for chaining operations or mutations. For example, you can√¢t easily say: √¢create a user, then immediately use that new user object to make a friend request, all-in-one round trip.√¢&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Different abstraction model. GraphQL doesn√¢t look or feel like the JavaScript APIs you already know. You√¢re learning a new mental model rather than extending the one you use every day.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;How Cap'n Web goes further&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Cap'n Web solves the waterfall problem without introducing a new language or ecosystem. It√¢s just JavaScript. Because Cap'n Web supports promise pipelining and object references, you can write code that looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let user = api.createUser({ name: "Alice" });
let friendRequest = await user.sendFriendRequest("Bob");&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;What happens under the hood? Both calls are pipelined into a single network round trip:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Create the user.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Take the result of that call (a new User object).&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Immediately invoke sendFriendRequest() on that object.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;All of this is expressed naturally in JavaScript, with no schemas, query languages, or special tooling required. You just call methods and pass objects around, like you would in any other JavaScript code.&lt;/p&gt;
      &lt;p&gt;In other words, GraphQL gave us a way to flatten REST√¢s waterfalls. Cap'n Web lets us go even further: it gives you the power to model complex interactions exactly the way you would in a normal program, with no impedance mismatch.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;But how do we solve arrays?&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;With everything we've presented so far, there's a critical missing piece to seriously consider Cap'n Web as an alternative to GraphQL: handling lists. Often, GraphQL is used to say: "Perform this query, and then, for every result, perform this other query." For example: "List the user's friends, and then for each one, fetch their profile photo."&lt;/p&gt;
      &lt;p&gt;In short, we need an &lt;code&gt;array.map()&lt;/code&gt; operation that can be performed without adding a round trip.&lt;/p&gt;
      &lt;p&gt;Cap'n Proto, historically, has never supported such a thing.&lt;/p&gt;
      &lt;p&gt;But with Cap'n Web, we've solved it. You can do:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let user = api.authenticate(token);

// Get the user's list of friends (an array).
let friendsPromise = user.listFriends();

// Do a .map() to annotate each friend record with their photo.
// This operates on the *promise* for the friends list, so does not
// add a round trip.
// (wait WHAT!?!?)
let friendsWithPhotos = friendsPromise.map(friend =&amp;gt; {
  return {friend, photo: api.getUserPhoto(friend.id))};
}

// Await the friends list with attached photos -- one round trip!
let results = await friendsWithPhotos;
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;&lt;code&gt;.map()&lt;/code&gt; takes a callback function, which needs to be applied to each element in the array. As we described earlier, normally when you pass a function to an RPC, the function is passed "by reference", meaning that the remote side receives a stub, where calling that stub makes an RPC back to the client where the function was created.&lt;/p&gt;
      &lt;p&gt;But that is NOT what is happening here. That would defeat the purpose: we don't want the server to have to round-trip to the client to process every member of the array. We want the server to just apply the transformation server-side.&lt;/p&gt;
      &lt;p&gt;To that end, &lt;code&gt;.map() &lt;/code&gt;is special. It does not send JavaScript code to the server, but it does send something like "code", restricted to a domain-specific, non-Turing-complete language. The "code" is a list of instructions that the server should carry out for each member of the array. In this case, the instructions are:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;
          &lt;p&gt;Invoke &lt;code&gt;api.getUserPhoto(friend.id)&lt;/code&gt;.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Return an object &lt;code&gt;{friend, photo}&lt;/code&gt;, where friend is the original array element and photo is the result of step 1.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;But the application code just specified a JavaScript method. How on Earth could we convert this into the narrow DSL?&lt;/p&gt;
      &lt;p&gt;The answer is record-replay: On the client side, we execute the callback once, passing in a special placeholder value. The parameter behaves like an RPC promise. However, the callback is required to be synchronous, so it cannot actually await this promise. The only thing it can do is use promise pipelining to make pipelined calls. These calls are intercepted by the implementation and recorded as instructions, which can then be sent to the server, where they can be replayed as needed.&lt;/p&gt;
      &lt;p&gt;And because the recording is based on promise pipelining, which is what the RPC protocol itself is designed to represent, it turns out that the "DSL" used to represent "instructions" for the map function is just the RPC protocol itself. √∞¬§¬Ø&lt;/p&gt;
      &lt;p&gt;Cap'n Web's underlying protocol is based on JSON √¢ but with a preprocessing step to handle special types. Arrays are treated as "escape sequences" that let us encode other values. For example, JSON does not have an encoding for &lt;code&gt;Date&lt;/code&gt; objects, but Cap'n Web does. You might see a message that looks like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;{
  event: "Birthday Week",
  timestamp: ["date", 1758499200000]
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;To encode a literal array, we simply double-wrap it in &lt;code&gt;[]&lt;/code&gt;:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;{
  names: [["Alice", "Bob", "Carol"]]
}
&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;In other words, an array with just one element which is itself an array, evaluates to the inner array literally. An array whose first element is a type name, evaluates to an instance of that type, where the remaining elements are parameters to the type.&lt;/p&gt;
      &lt;p&gt;Note that only a fixed set of types are supported: essentially, "structured clonable" types, and RPC stub types.&lt;/p&gt;
      &lt;p&gt;On top of this basic encoding, we define an RPC protocol inspired by Cap'n Proto √¢ but greatly simplified.&lt;/p&gt;
      &lt;p&gt;Since Cap'n Web is a symmetric protocol, there is no well-defined "client" or "server" at the protocol level. There are just two parties exchanging messages across a connection. Every kind of interaction can happen in either direction.&lt;/p&gt;
      &lt;p&gt;In order to make it easier to describe these interactions, I will refer to the two parties as "Alice" and "Bob".&lt;/p&gt;
      &lt;p&gt;Alice and Bob start the connection by establishing some sort of bidirectional message stream. This may be a WebSocket, but Cap'n Web also allows applications to define their own transports. Each message in the stream is JSON-encoded, as described earlier.&lt;/p&gt;
      &lt;p&gt;Alice and Bob each maintain some state about the connection. In particular, each maintains an "export table", describing all the pass-by-reference objects they have exposed to the other side, and an "import table", describing the references they have received. Alice's exports correspond to Bob's imports, and vice versa. Each entry in the export table has a signed integer ID, which is used to reference it. You can think of these IDs like file descriptors in a POSIX system. Unlike file descriptors, though, IDs can be negative, and an ID is never reused over the lifetime of a connection.&lt;/p&gt;
      &lt;p&gt;At the start of the connection, Alice and Bob each populate their export tables with a single entry, numbered zero, representing their "main" interfaces. Typically, when one side is acting as the "server", they will export their main public RPC interface as ID zero, whereas the "client" will export an empty interface. However, this is up to the application: either side can export whatever they want.&lt;/p&gt;
      &lt;p&gt;From there, new exports are added in two ways:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;When Alice sends a message to Bob that contains within it an object or function reference, Alice adds the target object to her export table. IDs assigned in this case are always negative, starting from -1 and counting downwards.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Alice can send a "push" message to Bob to request that Bob add a value to his export table. The "push" message contains an expression which Bob evaluates, exporting the result. Usually, the expression describes a method call on one of Bob's existing exports √¢ this is how an RPC is made. Each "push" is assigned a positive ID on the export table, starting from 1 and counting upwards. Since positive IDs are only assigned as a result of pushes, Alice can predict the ID of each push she makes, and can immediately use that ID in subsequent messages. This is how promise pipelining is achieved.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;After sending a push message, Alice can subsequently send a "pull" message, which tells Bob that once he is done evaluating the "push", he should proactively serialize the result and send it back to Alice, as a "resolve" (or "reject") message. However, this is optional: Alice may not actually care to receive the return value of an RPC, if Alice only wants to use it in promise pipelining. In fact, the Cap'n Web implementation will only send a "pull" message if the application has actually awaited the returned promise.&lt;/p&gt;
      &lt;p&gt;Putting it together, a code sequence like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;let namePromise = api.getMyName();
let result = await api.hello(namePromise);

console.log(result);&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Might produce a message exchange like this:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;// Call api.getByName(). `api` is the server's main export, so has export ID 0.
-&amp;gt; ["push", ["pipeline", 0, "getMyName", []]
// Call api.hello(namePromise). `namePromise` refers to the result of the first push,
// so has ID 1.
-&amp;gt; ["push", ["pipeline", 0, "hello", [["pipeline", 1]]]]
// Ask that the result of the second push be proactively serialized and returned.
-&amp;gt; ["pull", 2]
// Server responds.
&amp;lt;- ["resolve", 2, "Hello, Alice!"]&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;For more details about the protocol, check out the docs.&lt;/p&gt;
      &lt;p&gt;Cap'n Web is new and still highly experimental. There may be bugs to shake out. But, we're already using it today. Cap'n Web is the basis of the recently-launched "remote bindings" feature in Wrangler, allowing a local test instance of workerd to speak RPC to services in production. We've also begun to experiment with it in various frontend applications √¢ expect more blog posts on this in the future.&lt;/p&gt;
      &lt;p&gt;In any case, Cap'n Web is open source, and you can start using it in your own projects now.&lt;/p&gt;
      &lt;p&gt;Check it out on GitHub.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45332883</guid><pubDate>Mon, 22 Sep 2025 13:05:32 +0000</pubDate></item><item><title>Why haven't local-first apps become popular?</title><link>https://marcobambini.substack.com/p/why-local-first-apps-havent-become</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45333021</guid><pubDate>Mon, 22 Sep 2025 13:17:59 +0000</pubDate></item><item><title>PlanetScale for Postgres is now GA</title><link>https://planetscale.com/blog/planetscale-for-postgres-is-generally-available</link><description>&lt;doc fingerprint="af51e4d16e9161c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;PlanetScale for Postgres is now GA&lt;/head&gt;
    &lt;p&gt;By Sam Lambert |&lt;/p&gt;
    &lt;p&gt;PlanetScale for Postgres is now generally available and out of private preview. To create a Postgres database, sign up or log in to your PlanetScale account, create a new database, and select Postgres. If you are looking to migrate from another Postgres provider to PlanetScale, you can use our migration guides to get started. Finally, if you have a large or complex migration, we can help you via our sales team at postgres@planetscale.com.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is PlanetScale for Postgres?&lt;/head&gt;
    &lt;p&gt;Our mission is simple: bring you the fastest and most reliable databases with the best developer experience. We have done this for 5 years now with our managed Vitess product, allowing companies like Cursor, Intercom, and Block to scale beyond previous limits.&lt;/p&gt;
    &lt;p&gt;We are so excited to bring this to Postgres. Our proprietary operator allows us to bring the maturity of PlanetScale and the performance of Metal to an even wider audience. We bring you the best of Postgres and the best of PlanetScale in one product.&lt;/p&gt;
    &lt;head rend="h2"&gt;Customers on PlanetScale for Postgres&lt;/head&gt;
    &lt;p&gt;Hundreds of companies already trust PlanetScale for Postgres to power their production workloads. We say this every time we launch something, but we prefer you hear about real-world usage straight from our customers. Read through some of their stories about their migration to PlanetScale for Postgres below.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Convex: Powered by PlanetScale&lt;/item&gt;
      &lt;item&gt;Supermemory just got faster on PlanetScale&lt;/item&gt;
      &lt;item&gt;Scaling Real√¢Time Discovery: Inside Layers√¢ PlanetScale Migration&lt;/item&gt;
      &lt;item&gt;Why We Migrated from Neon to PlanetScale&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Vitess for Postgres&lt;/head&gt;
    &lt;p&gt;Neki is our Postgres sharding solution. Built by the team behind Vitess combining the best of Vitess and Postgres. Neki is not a fork of Vitess. Vitess√¢ achievements are enabled by leveraging MySQL√¢s strengths and engineering around its weaknesses. To achieve Vitess√¢ power for Postgres we are architecting from first principles and building alongside design partners at scale. When we are ready we will release Neki as an open source project suitable for running the most demanding Postgres workloads. To sign up for the Neki waitlist visit neki.dev.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45334545</guid><pubDate>Mon, 22 Sep 2025 15:10:48 +0000</pubDate></item><item><title>OpenAI and Nvidia announce partnership to deploy 10GW of Nvidia systems</title><link>https://openai.com/index/openai-nvidia-systems-partnership/</link><description>&lt;doc fingerprint="a9561c5e408ccdb9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;OpenAI and NVIDIA announce strategic partnership to deploy 10 gigawatts of NVIDIA systems&lt;/head&gt;
    &lt;p&gt;News&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Strategic partnership enables OpenAI to build and deploy at least 10 gigawatts of AI datacenters with NVIDIA systems representing millions of GPUs for OpenAI‚Äôs next-generation AI infrastructure.&lt;/item&gt;
      &lt;item&gt;To support the partnership, NVIDIA intends to invest up to $100 billion in OpenAI progressively as each gigawatt is deployed.&lt;/item&gt;
      &lt;item&gt;The first gigawatt of NVIDIA systems will be deployed in the second half of 2026 on NVIDIA‚Äôs Vera Rubin platform.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;San Francisco and Santa Clara‚ÄîSeptember 22, 2025‚ÄîNVIDIA and OpenAI today announced a letter of intent for a landmark strategic partnership to deploy at least 10 gigawatts of NVIDIA systems for OpenAI‚Äôs next-generation AI infrastructure to train and run its next generation of models on the path to deploying superintelligence. To support this deployment including datacenter and power capacity, NVIDIA intends to invest up to $100 billion in OpenAI as the new NVIDIA systems are deployed. The first phase is targeted to come online in the second half of 2026 using NVIDIA‚Äôs Vera Rubin platform.&lt;/p&gt;
    &lt;p&gt;‚ÄúNVIDIA and OpenAI have pushed each other for a decade, from the first DGX supercomputer to the breakthrough of ChatGPT,‚Äù said Jensen Huang, founder and CEO of NVIDIA. ‚ÄúThis investment and infrastructure partnership mark the next leap forward‚Äîdeploying 10 gigawatts to power the next era of intelligence.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúEverything starts with compute,‚Äù said Sam Altman, co-founder and CEO of OpenAI. ‚ÄúCompute infrastructure will be the basis for the economy of the future, and we will utilize what we‚Äôre building with NVIDIA to both create new AI breakthroughs and empower people and businesses with them at scale.‚Äù&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôve been working closely with NVIDIA since the early days of OpenAI,‚Äù said Greg Brockman, co-founder and President of OpenAI. ‚ÄúWe‚Äôve utilized their platform to create AI systems that hundreds of millions of people use every day. We‚Äôre excited to deploy 10 gigawatts of compute with NVIDIA to push back the frontier of intelligence and scale the benefits of this technology to everyone.‚Äù&lt;/p&gt;
    &lt;p&gt;OpenAI will work with NVIDIA as a preferred strategic compute and networking partner for its AI factory growth plans. OpenAI and NVIDIA will work together to co-optimize their roadmaps for OpenAI's model and infrastructure software and NVIDIA‚Äôs hardware and software.&lt;/p&gt;
    &lt;p&gt;This partnership complements the deep work OpenAI and NVIDIA are already doing with a broad network of collaborators, including Microsoft, Oracle, SoftBank, and Stargate partners, focused on building the world‚Äôs most advanced AI infrastructure.&lt;/p&gt;
    &lt;p&gt;OpenAI has grown to over 700 million weekly active users and strong adoption across global enterprises, small businesses, and developers. This partnership will help OpenAI advance its mission to build artificial general intelligence that benefits all of humanity.&lt;lb/&gt;NVIDIA and OpenAI look forward to finalizing the details of this new phase of strategic partnership in the coming weeks.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45335474</guid><pubDate>Mon, 22 Sep 2025 16:10:15 +0000</pubDate></item><item><title>Testing is better than data structures and algorithms</title><link>https://nedbatchelder.com/blog/202509/testing_is_better_than_dsa.html</link><description>&lt;doc fingerprint="bf6210976d050c95"&gt;
  &lt;main&gt;
    &lt;p&gt;People should spend less time learning DSA, more time learning testing.&lt;/p&gt;
    &lt;p&gt;I see new learners asking about ‚ÄúDSA‚Äù a lot. Data Structures and Algorithms are of course important: considered broadly, they are the two ingredients that make up all programs. But in my opinion, ‚ÄúDSA‚Äù as an abstract field of study is over-emphasized.&lt;/p&gt;
    &lt;p&gt;I understand why people focus on DSA: it‚Äôs a concrete thing to learn about, there are web sites devoted to testing you on it, and most importantly, because job interviews often involve DSA coding questions.&lt;/p&gt;
    &lt;p&gt;Before I get to other opinions, let me make clear that anything you can do to help you get a job is a good thing to do. If grinding leetcode will land you a position, then do it.&lt;/p&gt;
    &lt;p&gt;But I hope companies hiring entry-level engineers aren‚Äôt asking them to reverse linked lists or balance trees. Asking about techniques that can be memorized ahead of time won‚Äôt tell them anything about how well you can work. The stated purpose of those interviews is to see how well you can figure out solutions, in which case memorization will defeat the point.&lt;/p&gt;
    &lt;p&gt;The thing new learners don‚Äôt understand about DSA is that actual software engineering almost never involves implementing the kinds of algorithms that ‚ÄúDSA‚Äù teaches you. Sure, it can be helpful to work through some of these puzzles and see how they are solved, but writing real code just doesn‚Äôt involve writing that kind of code.&lt;/p&gt;
    &lt;p&gt;Here is what I think in-the-trenches software engineers should know about data structures and algorithms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Data structures are ways to organize data. Learn some of the basics: linked list, array, hash table, tree. By ‚Äúlearn‚Äù I mean understand what it does and why you might want to use one.&lt;/item&gt;
      &lt;item&gt;Different data structures can be used to organize the same data in different ways. Learn some of the trade-offs between structures that are similar.&lt;/item&gt;
      &lt;item&gt;Algorithms are ways of manipulating data. I don‚Äôt mean named algorithms like Quicksort, but algorithms as any chunk of code that works on data and does something with it.&lt;/item&gt;
      &lt;item&gt;How you organize data affects what algorithms you can use to work with the data. Some data structures will be slow for some operations where another structure will be fast.&lt;/item&gt;
      &lt;item&gt;Algorithms have a ‚Äútime complexity‚Äù (Big O): how the code slows as the data grows. Get a sense of what this means.&lt;/item&gt;
      &lt;item&gt;Python has a number of built-in data structures. Learn how they work, and the time complexity of their operations.&lt;/item&gt;
      &lt;item&gt;Learn how to think about your code to understand its time complexity.&lt;/item&gt;
      &lt;item&gt;Read a little about more esoteric things like Bloom filters, so you can find them later in the unlikely case you need them.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here are some things you don‚Äôt need to learn:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The details of a dozen different sorting algorithms. Look at two to see different ways of approaching the same problem, then move on.&lt;/item&gt;
      &lt;item&gt;The names of ‚Äúimportant‚Äù algorithms. Those have all been implemented for you.&lt;/item&gt;
      &lt;item&gt;The answers to all N problems on some quiz web site. You won‚Äôt be asked these exact questions, and they won‚Äôt come up in your real work. Again: try a few to get a feel for how some algorithms work. The exact answers are not what you need.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of course some engineers need to implement hash tables, or sorting algorithms or whatever. We love those engineers: they write libraries we can use off the shelf so we don‚Äôt have to implement them ourselves.&lt;/p&gt;
    &lt;p&gt;There have been times when I implemented something that felt like An Algorithm (for example, Finding fuzzy floats), but it was more about considering another perspective on my data, looking at the time complexity, and moving operations around to avoid quadratic behavior. It wasn‚Äôt opening a textbook to find the famous algorithm that would solve my problem.&lt;/p&gt;
    &lt;p&gt;Again: if it will help you get a job, deep-study DSA. But don‚Äôt be disappointed when you don‚Äôt use it on the job.&lt;/p&gt;
    &lt;p&gt;If you want to prepare yourself for a career, and also stand out in job interviews, learn how to write tests:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This will be a skill you use constantly. Real-world software means writing tests much more than school teaches you to.&lt;/item&gt;
      &lt;item&gt;In a job search, testing experience will stand out more than DSA depth. It shows you‚Äôve thought about what it takes to write high-quality software instead of just academic exercises.&lt;/item&gt;
      &lt;item&gt;It‚Äôs not obvious how to test code well. It‚Äôs a puzzle and a problem to solve. If you like figuring out solutions to tricky questions, focus on how to write code so that it can be tested, and how to test it.&lt;/item&gt;
      &lt;item&gt;Testing not only gives you more confidence in your code, it helps you write better code in the first place.&lt;/item&gt;
      &lt;item&gt;Testing applies everywhere, from tiny bits of code to entire architectures, assisting you in design and implementation at all scales.&lt;/item&gt;
      &lt;item&gt;If pursued diligently, testing is an engineering discipline in its own right, with a fascinating array of tools and techniques.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Less DSA, more testing.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45335635</guid><pubDate>Mon, 22 Sep 2025 16:21:35 +0000</pubDate></item><item><title>Qwen3-Omni: Native Omni AI model for text, image and video</title><link>https://github.com/QwenLM/Qwen3-Omni</link><description>&lt;doc fingerprint="a44ecd1b2dbb5a97"&gt;
  &lt;main&gt;
    &lt;p&gt; üíú Qwen Chat | ü§ó Hugging Face | ü§ñ ModelScope | üìë Blog | üìö Cookbooks | üìë Paper &lt;lb/&gt; üñ•Ô∏è Hugging Face Demo | üñ•Ô∏è ModelScope Demo | üí¨ WeChat (ÂæÆ‰ø°) | ü´® Discord | üìë API &lt;/p&gt;
    &lt;p&gt;We release Qwen3-Omni, the natively end-to-end multilingual omni-modal foundation models. It is designed to process diverse inputs including text, images, audio, and video, while delivering real-time streaming responses in both text and natural speech. Click the video below for more information üòÉ&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2025.09.22: üéâüéâüéâ We have released Qwen3-Omni. For more details, please check our blog!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;State-of-the-art across modalities: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multilingual: Supports 119 text languages, 19 speech input languages, and 10 speech output languages.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Speech Input: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.&lt;/item&gt;
          &lt;item&gt;Speech Output: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Novel Architecture: MoE-based Thinker‚ÄìTalker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Real-time Audio/Video Interaction: Low-latency streaming with natural turn-taking and immediate text or speech responses.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Flexible Control: Customize behavior via system prompts for fine-grained control and easy adaptation.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Detailed Audio Captioner: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Qwen3-Omni supports a wide range of multimodal application scenarios, covering various domain tasks involving audio, image, video, and audio-visual modalities. Below are several cookbooks demonstrating the usage cases of Qwen3-Omni and these cookbooks include our actual execution logs. You can first follow the QuickStart guide to download the model and install the necessary inference environment dependencies, then run and experiment locally‚Äîtry modifying prompts or switching model types, and enjoy exploring the capabilities of Qwen3-Omni!&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Category&lt;/cell&gt;
        &lt;cell role="head"&gt;Cookbook&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Open&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio&lt;/cell&gt;
        &lt;cell&gt;Speech Recognition&lt;/cell&gt;
        &lt;cell&gt;Speech recognition, supporting multiple languages and long audio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Speech Translation&lt;/cell&gt;
        &lt;cell&gt;Speech-to-Text / Speech-to-Speech translation.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Music Analysis&lt;/cell&gt;
        &lt;cell&gt;Detailed analysis and appreciation of any music, including style, genre, rhythm, etc.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sound Analysis&lt;/cell&gt;
        &lt;cell&gt;Description and analysis of various sound effects and audio signals.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Caption&lt;/cell&gt;
        &lt;cell&gt;Audio captioning, detailed description of any audio input.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mixed Audio Analysis&lt;/cell&gt;
        &lt;cell&gt;Analysis of mixed audio content, such as speech, music, and environmental sounds.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Visual&lt;/cell&gt;
        &lt;cell&gt;OCR&lt;/cell&gt;
        &lt;cell&gt;OCR for complex images.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Object Grounding&lt;/cell&gt;
        &lt;cell&gt;Target detection and grounding.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Image Question&lt;/cell&gt;
        &lt;cell&gt;Answering arbitrary questions about any image.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Image Math&lt;/cell&gt;
        &lt;cell&gt;Solving complex mathematical problems in images, highlighting the capabilities of the Thinking model.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Description&lt;/cell&gt;
        &lt;cell&gt;Detailed description of video content.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Navigation&lt;/cell&gt;
        &lt;cell&gt;Generating navigation commands from first-person motion videos.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Video Scene Transition&lt;/cell&gt;
        &lt;cell&gt;Analysis of scene transitions in videos.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio-Visual&lt;/cell&gt;
        &lt;cell&gt;Audio Visual Question&lt;/cell&gt;
        &lt;cell&gt;Answering arbitrary questions in audio-visual scenarios, demonstrating the model's ability to model temporal alignment between audio and video.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Visual Interaction&lt;/cell&gt;
        &lt;cell&gt;Interactive communication with the model using audio-visual inputs, including task specification via audio.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Audio Visual Dialogue&lt;/cell&gt;
        &lt;cell&gt;Conversational interaction with the model using audio-visual inputs, showcasing its capabilities in casual chat and assistant-like behavior.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;Audio Function Call&lt;/cell&gt;
        &lt;cell&gt;Using audio input to perform function calls, enabling agent-like behaviors.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Downstream Task Fine-tuning&lt;/cell&gt;
        &lt;cell&gt;Omni Captioner&lt;/cell&gt;
        &lt;cell&gt;Introduction and capability demonstration of Qwen3-Omni-30B-A3B-Captioner, a downstream fine-tuned model based on Qwen3-Omni-30B-A3B-Instruct, illustrating the strong generalization ability of the Qwen3-Omni foundation model.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Here, we provide several methods to quickly get started with Qwen3-Omni. If you want complete experience of Qwen3-Omni, you can use Hugging Face Transformers. However, since Qwen3-Omni employs an MoE architecture, inference speed with Hugging Face Transformers on MoE models can be very slow. For large-scale invocation or low-latency requirements, we highly recommend using vLLM or performing inference via the DashScope API. We also strongly suggest using our provided Docker image, which includes a complete runtime environment for both Hugging Face Transformers and vLLM. In addition, our cookbooks offer some use cases to show Qwen3-Omni's capabilities. Welcome to learn more!&lt;/p&gt;
    &lt;p&gt;Below is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Model Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell&gt;The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the Qwen3-Omni Technical Report.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell&gt;The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the Qwen3-Omni Technical Report.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Captioner&lt;/cell&gt;
        &lt;cell&gt;A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's cookbook or Hugging Face Demo and ModelScope Demo.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;During loading in Hugging Face Transformers or vLLM, model weights will be automatically downloaded based on the model name. However, if your runtime environment is not conducive to downloading weights during execution, you can refer to the following commands to manually download the model weights to a local directory:&lt;/p&gt;
    &lt;code&gt;# Download through ModelScope (recommended for users in Mainland China)
pip install -U modelscope
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Instruct --local_dir ./Qwen3-Omni-30B-A3B-Instruct
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Thinking --local_dir ./Qwen3-Omni-30B-A3B-Thinking
modelscope download --model Qwen/Qwen3-Omni-30B-A3B-Captioner --local_dir ./Qwen3-Omni-30B-A3B-Captioner

# Download through Hugging Face
pip install -U "huggingface_hub[cli]"
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Instruct --local-dir ./Qwen3-Omni-30B-A3B-Instruct
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Thinking --local-dir ./Qwen3-Omni-30B-A3B-Thinking
huggingface-cli download Qwen/Qwen3-Omni-30B-A3B-Captioner --local-dir ./Qwen3-Omni-30B-A3B-Captioner&lt;/code&gt;
    &lt;p&gt;The Hugging Face Transformers code for Qwen3-Omni has been successfully merged, but the PyPI package has not yet been released. Therefore, you need to install it from source using the following command. We strongly recommend that you create a new Python environment or use our Docker to avoid environment runtime issues.&lt;/p&gt;
    &lt;code&gt;# If you already have transformers installed, please uninstall it first, or create a new Python environment
# pip uninstall transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate&lt;/code&gt;
    &lt;p&gt;We offer a toolkit to help you handle various types of audio and visual input more conveniently, providing an API-like experience. This includes support for base64, URLs, and interleaved audio, images, and videos. You can install it using the following command and make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed:&lt;/p&gt;
    &lt;code&gt;pip install qwen-omni-utils -U&lt;/code&gt;
    &lt;p&gt;Additionally, we recommend using FlashAttention 2 when running with Hugging Face Transformers to reduce GPU memory usage. However, if you are primarily using vLLM for inference, this installation is not necessary, as vLLM includes FlashAttention 2 by default.&lt;/p&gt;
    &lt;code&gt;pip install -U flash-attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;Also, you should have hardware that is compatible with FlashAttention 2. Read more about it in the official documentation of the FlashAttention repository. FlashAttention 2 can only be used when a model is loaded in &lt;code&gt;torch.float16&lt;/code&gt; or &lt;code&gt;torch.bfloat16&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here is a code snippet to show you how to use Qwen3-Omni with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;qwen_omni_utils&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;import soundfile as sf

from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

conversation = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you see and hear? Answer in one short sentence."}
        ],
    },
]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for inference
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors="pt", 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Inference: Generation of the output text and audio
text_ids, audio = model.generate(**inputs, 
                                 speaker="Ethan", 
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)
if audio is not None:
    sf.write(
        "output.wav",
        audio.reshape(-1).detach().cpu().numpy(),
        samplerate=24000,
    )&lt;/code&gt;
    &lt;p&gt;Here are some more advanced usage examples. You can expand the sections below to learn more.&lt;/p&gt;
    &lt;head&gt;Batch inference&lt;/head&gt;
    &lt;p&gt;The model can batch inputs composed of mixed samples of various types such as text, images, audio, and videos as input when &lt;code&gt;return_audio=False&lt;/code&gt; is set. Here is an example.&lt;/p&gt;
    &lt;code&gt;from transformers import Qwen3OmniMoeForConditionalGeneration, Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
# MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    MODEL_PATH,
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.disable_talker()

processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

# Conversation with image only
conversation1 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "text", "text": "What can you see in this image? Answer in one sentence."},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you hear in this audio?"},
        ]
    }
]

# Conversation with pure text and system prompt
conversation3 = [
    {
        "role": "system",
        "content": [
            {"type": "text", "text": "You are Qwen-Omni."}
        ],
    },
    {
        "role": "user",
        "content": "Who are you?"
    }
]

# Conversation with mixed media
conversation4 = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
            {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
            {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
        ],
    }
]

# Combine messages for batch processing
conversations = [conversation1, conversation2, conversation3, conversation4]

# Set whether to use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for batch inference
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(text=text, 
                   audio=audios, 
                   images=images, 
                   videos=videos, 
                   return_tensors="pt", 
                   padding=True, 
                   use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Batch inference does not support returning audio
text_ids, audio = model.generate(**inputs,
                                 return_audio=False,
                                 thinker_return_dict_in_generate=True,
                                 use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids.sequences[:, inputs["input_ids"].shape[1] :],
                              skip_special_tokens=True,
                              clean_up_tokenization_spaces=False)
print(text)&lt;/code&gt;
    &lt;head&gt;Use audio output or not&lt;/head&gt;
    &lt;p&gt;The model supports both text and audio outputs. If users do not need audio outputs, they can call &lt;code&gt;model.disable_talker()&lt;/code&gt; after initializing the model. This option will save about &lt;code&gt;10GB&lt;/code&gt; of GPU memory, but the &lt;code&gt;return_audio&lt;/code&gt; option for the &lt;code&gt;generate&lt;/code&gt; function will only allow &lt;code&gt;False&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
model.disable_talker()&lt;/code&gt;
    &lt;p&gt;For a more flexible experience, we recommend that users decide whether to return audio when the &lt;code&gt;generate&lt;/code&gt; function is called. If &lt;code&gt;return_audio&lt;/code&gt; is set to &lt;code&gt;False&lt;/code&gt;, the model will only return text outputs, resulting in faster text responses.&lt;/p&gt;
    &lt;code&gt;model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(
    "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    dtype="auto",
    device_map="auto",
    attn_implementation="flash_attention_2",
)
...
text_ids, _ = model.generate(..., return_audio=False)```

&amp;lt;/details&amp;gt;

&amp;lt;details&amp;gt;
&amp;lt;summary&amp;gt;Change voice type of output audio&amp;lt;/summary&amp;gt;

Qwen3-Omni supports changing the voice of the output audio. The `"Qwen/Qwen3-Omni-30B-A3B-Instruct"` checkpoint supports three voice types as follows:

| Voice Type | Gender | Description |
|------------|--------|-------------|
| Ethan      | Male   | A bright, upbeat voice with infectious energy and a warm, approachable vibe. |
| Chelsie    | Female | A honeyed, velvety voice that carries a gentle warmth and luminous clarity. |
| Aiden      | Male   | A warm, laid-back American voice with a gentle, boyish charm. |

Users can use the `speaker` parameter of the `generate` function to specify the voice type. By default, if `speaker` is not specified, the voice type is `Ethan`.

```python
text_ids, audio = model.generate(..., speaker="Ethan")&lt;/code&gt;
    &lt;code&gt;text_ids, audio = model.generate(..., speaker="Chelsie")&lt;/code&gt;
    &lt;code&gt;text_ids, audio = model.generate(..., speaker="Aiden")&lt;/code&gt;
    &lt;p&gt;Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to Usage Tips and Cookbooks for Usage Cases.&lt;/p&gt;
    &lt;p&gt;We strongly recommend using vLLM for inference and deployment of the Qwen3-Omni series models. Since our code is currently in the pull request stage, and audio output inference support for the Instruct model will be released in the near future, you can follow the commands below to install vLLM from source. Please note that we recommend you create a new Python environment or use our provided Docker to avoid runtime environment conflicts and incompatibilities. For more details on compiling vLLM from source, please refer to the vLLM official documentation.&lt;/p&gt;
    &lt;code&gt;git clone -b qwen3_omni https://github.com/wangxiongts/vllm.git
cd vllm
pip install -r requirements/build.txt
pip install -r requirements/cuda.txt
export VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/a5dd03c1ebc5e4f56f3c9d3dc0436e9c582c978f/vllm-0.9.2-cp38-abi3-manylinux1_x86_64.whl
VLLM_USE_PRECOMPILED=1 pip install -e . -v --no-build-isolation
# If you meet an "Undefined symbol" error while using VLLM_USE_PRECOMPILED=1, please use "pip install -e . -v" to build from source.
# Install the Transformers
pip install git+https://github.com/huggingface/transformers
pip install accelerate
pip install qwen-omni-utils -U
pip install -U flash-attn --no-build-isolation&lt;/code&gt;
    &lt;p&gt;You can use the following code for vLLM inference. The &lt;code&gt;limit_mm_per_prompt&lt;/code&gt; parameter specifies the maximum number of each modality's data allowed per message. Since vLLM needs to pre-allocate GPU memory, larger values will require more GPU memory; if OOM issues occur, try reducing this value. Setting &lt;code&gt;tensor_parallel_size&lt;/code&gt; greater than one enables multi-GPU parallel inference, improving concurrency and throughput. In addition, &lt;code&gt;max_num_seqs&lt;/code&gt; indicates the number of sequences that vLLM processes in parallel during each inference step. A larger value requires more GPU memory but enables higher batch inference speed. For more details, please refer to the vLLM official documentation. Below is a simple example of how to run Qwen3-Omni with vLLM:&lt;/p&gt;
    &lt;code&gt;import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    messages = [
        {
            "role": "user",
            "content": [
                {"type": "video", "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/draw.mp4"}
            ], 
        }
    ]

    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=True)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        "mm_processor_kwargs": {
            "use_audio_in_video": True,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios

    outputs = llm.generate([inputs], sampling_params=sampling_params)

    print(outputs[0].outputs[0].text)&lt;/code&gt;
    &lt;p&gt;Here are some more advanced usage examples. You can expand the sections below to learn more.&lt;/p&gt;
    &lt;head&gt;Batch inference&lt;/head&gt;
    &lt;p&gt;Using vLLM enables fast batch inference, which can help you efficiently process large volumes of data or conduct benchmarking. Refer to the following code example:&lt;/p&gt;
    &lt;code&gt;import os
import torch

from vllm import LLM, SamplingParams
from transformers import Qwen3OmniMoeProcessor
from qwen_omni_utils import process_mm_info

def build_input(processor, messages, use_audio_in_video):
    text = processor.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    audios, images, videos = process_mm_info(messages, use_audio_in_video=use_audio_in_video)

    inputs = {
        'prompt': text,
        'multi_modal_data': {},
        "mm_processor_kwargs": {
            "use_audio_in_video": use_audio_in_video,
        },
    }

    if images is not None:
        inputs['multi_modal_data']['image'] = images
    if videos is not None:
        inputs['multi_modal_data']['video'] = videos
    if audios is not None:
        inputs['multi_modal_data']['audio'] = audios
    
    return inputs

if __name__ == '__main__':
    # vLLM engine v1 not supported yet
    os.environ['VLLM_USE_V1'] = '0'

    MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Instruct"
    # MODEL_PATH = "Qwen/Qwen3-Omni-30B-A3B-Thinking"

    llm = LLM(
            model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.95,
            tensor_parallel_size=torch.cuda.device_count(),
            limit_mm_per_prompt={'image': 3, 'video': 3, 'audio': 3},
            max_num_seqs=8,
            max_model_len=32768,
            seed=1234,
    )

    sampling_params = SamplingParams(
        temperature=0.6,
        top_p=0.95,
        top_k=20,
        max_tokens=16384,
    )

    processor = Qwen3OmniMoeProcessor.from_pretrained(MODEL_PATH)

    # Conversation with image only
    conversation1 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
                {"type": "text", "text": "What can you see in this image? Answer in one sentence."},
            ]
        }
    ]

    # Conversation with audio only
    conversation2 = [
        {
            "role": "user",
            "content": [
                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"},
                {"type": "text", "text": "What can you hear in this audio?"},
            ]
        }
    ]

    # Conversation with pure text and system prompt
    conversation3 = [
        {
            "role": "system",
            "content": [
                {"type": "text", "text": "You are Qwen-Omni."}
            ],
        },
        {
            "role": "user",
            "content": "Who are you? Answer in one sentence."
        }
    ]

    # Conversation with mixed media
    conversation4 = [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"},
                {"type": "audio", "audio": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/cookbook/asr_fr.wav"},
                {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
            ],
        }
    ]
    
    USE_AUDIO_IN_VIDEO = True

    # Combine messages for batch processing
    conversations = [conversation1, conversation2, conversation3, conversation4]
    inputs = [build_input(processor, messages, USE_AUDIO_IN_VIDEO) for messages in conversations]

    outputs = llm.generate(inputs, sampling_params=sampling_params)

    result = [outputs[i].outputs[0].text for i in range(len(outputs))]
    print(result)&lt;/code&gt;
    &lt;head&gt;vLLM Serve Usage&lt;/head&gt;
    &lt;p&gt;vLLM serve for Qwen3-Omni currently only supports the thinker model. The &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter is not available in vLLM serve; you can handle this by separately passing video and audio inputs for processing. You can start vLLM serve through the following command:&lt;/p&gt;
    &lt;code&gt;# Qwen3-Omni-30B-A3B-Instruct for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen3-Omni-30B-A3B-Instruct for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Instruct --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4
# Qwen/Qwen3-Omni-30B-A3B-Thinking for single GPU
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 32768 --allowed-local-media-path / -tp 1
# Qwen/Qwen3-Omni-30B-A3B-Thinking for multi-GPU (example on 4 GPUs)
vllm serve Qwen/Qwen3-Omni-30B-A3B-Thinking --port 8901 --host 127.0.0.1 --dtype bfloat16 --max-model-len 65536 --allowed-local-media-path / -tp 4&lt;/code&gt;
    &lt;p&gt;Then you can use the chat API as below (via curl, for example):&lt;/p&gt;
    &lt;code&gt;curl http://localhost:8901/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
    "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": [
        {"type": "image_url", "image_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cars.jpg"}},
        {"type": "audio_url", "audio_url": {"url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen3-Omni/demo/cough.wav"}},
        {"type": "text", "text": "What can you see and hear? Answer in one sentence."}
    ]}
    ]
    }'&lt;/code&gt;
    &lt;p&gt;Additionally, for more usage details such as prompt settings, task-specific usage methods, and resource requirements, please refer to Usage Tips and Cookbooks for Usage Cases.&lt;/p&gt;
    &lt;p&gt;To further explore Qwen3-Omni, we encourage you to try our DashScope API for a faster and more efficient experience. For detailed API information and documentation, please refer to the following:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;API Description&lt;/cell&gt;
        &lt;cell role="head"&gt;API Documentation (Mainland China)&lt;/cell&gt;
        &lt;cell role="head"&gt;API Documentation (International)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Offline API for Qwen3-Omni-Flash, including Instruct and Thinking models&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/qwen-omni&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/en/model-studio/qwen-omni&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Real-time API for Qwen3-Omni-Flash, supporting end-to-end real-time interaction&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/realtime&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/en/model-studio/realtime&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;API for Qwen3-Omni-30B-A3B-Captioner model&lt;/cell&gt;
        &lt;cell&gt;https://help.aliyun.com/zh/model-studio/qwen3-omni-captioner&lt;/cell&gt;
        &lt;cell&gt;https://www.alibabacloud.com/help/zh/model-studio/qwen3-omni-captioner&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Precision&lt;/cell&gt;
        &lt;cell role="head"&gt;15s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;30s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;60s Video&lt;/cell&gt;
        &lt;cell role="head"&gt;120s Video&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell&gt;BF16&lt;/cell&gt;
        &lt;cell&gt;78.85 GB&lt;/cell&gt;
        &lt;cell&gt;88.52 GB&lt;/cell&gt;
        &lt;cell&gt;107.74 GB&lt;/cell&gt;
        &lt;cell&gt;144.81 GB&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell&gt;BF16&lt;/cell&gt;
        &lt;cell&gt;68.74 GB&lt;/cell&gt;
        &lt;cell&gt;77.79 GB&lt;/cell&gt;
        &lt;cell&gt;95.76 GB&lt;/cell&gt;
        &lt;cell&gt;131.65 GB&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Note: The table above presents the theoretical minimum memory requirements for inference with &lt;code&gt;transformers&lt;/code&gt; and &lt;code&gt;BF16&lt;/code&gt; precision, tested with &lt;code&gt;attn_implementation="flash_attention_2"&lt;/code&gt;. The Instruct model includes both the thinker and talker components, whereas the Thinking model includes only the thinker part.&lt;/p&gt;
    &lt;p&gt;When using Qwen3-Omni for audio-visual multimodal interaction, where the input consists of a video and its corresponding audio (with the audio serving as a query), we recommend using the following system prompt. This setup helps the model maintain high reasoning capability while better assuming interactive roles such as a smart assistant. Additionally, the text generated by the thinker will be more readable, with a natural, conversational tone and without complex formatting that is difficult to vocalize, leading to more stable and fluent audio output from the talker. You can customize the &lt;code&gt;user_system_prompt&lt;/code&gt; field in the system prompt to include character settings or other role-specific descriptions as needed.&lt;/p&gt;
    &lt;code&gt;user_system_prompt = "You are Qwen-Omni, a smart voice assistant created by Alibaba Qwen."
message = {
    "role": "system",
    "content": [
          {"type": "text", "text": f"{user_system_prompt} You are a virtual voice assistant with no gender or age.\nYou are communicating with the user.\nIn user messages, ‚ÄúI/me/my/we/our‚Äù refer to the user and ‚Äúyou/your‚Äù refer to the assistant. In your replies, address the user as ‚Äúyou/your‚Äù and yourself as ‚ÄúI/me/my‚Äù; never mirror the user‚Äôs pronouns‚Äîalways shift perspective. Keep original pronouns only in direct quotes; if a reference is unclear, ask a brief clarifying question.\nInteract with users using short(no more than 50 words), brief, straightforward language, maintaining a natural tone.\nNever use formal phrasing, mechanical expressions, bullet points, overly structured language. \nYour output must consist only of the spoken content you want the user to hear. \nDo not include any descriptions of actions, emotions, sounds, or voice changes. \nDo not use asterisks, brackets, parentheses, or any other symbols to indicate tone or actions. \nYou must answer users' audio or text questions, do not directly describe the video content. \nYou should communicate in the same language strictly as the user unless they request otherwise.\nWhen you are uncertain (e.g., you can't see/hear clearly, don't understand, or the user makes a comment rather than asking a question), use appropriate questions to guide the user to continue the conversation.\nKeep replies concise and conversational, as if talking face-to-face."}
    ]
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/code&gt; model is primarily designed for understanding and interacting with multimodal inputs, including text, audio, image, and video. To achieve optimal performance, we recommend that users include an explicit textual instruction or task description in each round of dialogue alongside the multimodal input. This helps clarify the intent and significantly enhances the model's ability to leverage its reasoning capabilities. For example:&lt;/p&gt;
    &lt;code&gt;messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "image", "image": "/path/to/image.png"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "Analyze this audio, image, and video together."},
        ], 
    }
]&lt;/code&gt;
    &lt;p&gt;In multimodal interaction, user-provided videos are often accompanied by audio (such as spoken questions or sounds from events in the video). This information helps the model provide a better interactive experience. We provide the following options for users to decide whether to use the audio from a video.&lt;/p&gt;
    &lt;code&gt;# In data preprocessing
audios, images, videos = process_mm_info(messages, use_audio_in_video=True)&lt;/code&gt;
    &lt;code&gt;# For Transformers
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", 
                   padding=True, use_audio_in_video=True)
text_ids, audio = model.generate(..., use_audio_in_video=True)

# For vLLM
text = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
inputs = {
    'prompt': text,
    'multi_modal_data': {},
    "mm_processor_kwargs": {
        "use_audio_in_video": True,
    },
}&lt;/code&gt;
    &lt;p&gt;It is worth noting that during a multi-round conversation, the &lt;code&gt;use_audio_in_video&lt;/code&gt; parameter must be set consistently across these steps; otherwise, unexpected results may occur.&lt;/p&gt;
    &lt;p&gt;Without local deployment, you can experience an online web demo directly by visiting our Hugging Face Spaces and ModelScope Studio. This includes quick hands-on experiences for Qwen3-Omni-Realtime, Qwen3-Omni (Instruct and Thinking), and Qwen3-Omni-30B-A3B-Captioner.&lt;/p&gt;
    &lt;p&gt;Real-time streaming interaction with Qwen3-Omni is available now. Please visit Qwen Chat and select the voice/video call option in the chat box to experience it.&lt;/p&gt;
    &lt;p&gt;In this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with the model through a web browser. Follow the steps below to get start :)&lt;/p&gt;
    &lt;p&gt;Before you begin, we strongly recommend that you refer to the Installation section in vLLM Usage to set up your environment, which will allow you to seamlessly use both the vLLM and Transformers backends. However, if you only intend to use the Transformers backend (note that this will result in significantly slower inference), please follow the installation instructions in Transformers Usage. That said, we still highly recommend using our Docker image to avoid potential environment-related issues. Additionally, if you are running locally, make sure your system has &lt;code&gt;ffmpeg&lt;/code&gt; installed and you install the following dependencies:&lt;/p&gt;
    &lt;code&gt;pip install gradio==5.44.1 gradio_client==1.12.1 soundfile==0.13.1&lt;/code&gt;
    &lt;p&gt;Once the required packages are installed, you can launch the web demo using the following commands. These commands will start a web server and provide you with a link to access the UI in your web browser. You can run &lt;code&gt;python web_demo.py --help&lt;/code&gt; and &lt;code&gt;python web_demo_captioner.py --help&lt;/code&gt; to learn about more options.&lt;/p&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Instruct with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio
# For Qwen3-Omni-30B-A3B-Instruct with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --use-transformers --generate-audio --flash-attn2&lt;/code&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Thinking with vLLM backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers
# For Qwen3-Omni-30B-A3B-Thinking with Transformers backend and FlashAttention support
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Thinking --use-transformers --flash-attn2&lt;/code&gt;
    &lt;code&gt;# For Qwen3-Omni-30B-A3B-Captioner with vLLM backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers
# For Qwen3-Omni-30B-A3B-Captioner with Transformers backend and FlashAttention support
python web_demo_captioner.py -c Qwen/Qwen3-Omni-30B-A3B-Captioner --use-transformers --flash-attn2&lt;/code&gt;
    &lt;p&gt;After running the command, you‚Äôll see a link generated in the terminal similar to this:&lt;/p&gt;
    &lt;code&gt;Running on local: http://127.0.0.1:8901/
&lt;/code&gt;
    &lt;p&gt;If you are running locally, copy this link and paste it into your browser to access the web UI. If you are running on a server or in a &lt;code&gt;docker&lt;/code&gt; container, please configure the address according to the server's actual IP, or set up port forwarding where necessary. For instructions on how to configure port forwarding from the official &lt;code&gt;docker&lt;/code&gt; container to the host machine, please refer to here.&lt;/p&gt;
    &lt;p&gt;To simplify the deployment process, we provide Docker images with pre-built environments: qwenllm/qwen3-omni. You only need to install the driver and download model files to launch the demos. Please refer to the guide to install the NVIDIA Container Toolkit, ensuring that your Docker can access the GPU. For users in mainland China who may have difficulty accessing Docker Hub, you can use mirror acceleration services to pull the images. First, run the following command to pull and initialize the container:&lt;/p&gt;
    &lt;code&gt;LOCAL_WORKDIR=/path/to/your/workspace
HOST_PORT=8901
CONTAINER_PORT=80
docker run --gpus all --name qwen3-omni \
    -v /var/run/docker.sock:/var/run/docker.sock -p $HOST_PORT:$CONTAINER_PORT \
    --mount type=bind,source=$LOCAL_WORKDIR,target=/data/shared/Qwen3-Omni \
    --shm-size=4gb \
    -it qwenllm/qwen3-omni:3-cu124&lt;/code&gt;
    &lt;p&gt;After executing the command, you will enter the bash shell of the container. Your local model and data directory (please replace &lt;code&gt;/path/to/your/workspace&lt;/code&gt; with the actual path) will be mounted to the container's internal path &lt;code&gt;/data/shared/Qwen3-Omni&lt;/code&gt;. The host's port &lt;code&gt;8901&lt;/code&gt; is mapped to port &lt;code&gt;80&lt;/code&gt; in the container, meaning you can access the service inside the container by visiting port &lt;code&gt;8901&lt;/code&gt; on the host machine.&lt;/p&gt;
    &lt;p&gt;Please note that services inside the container must be started with the IP &lt;code&gt;0.0.0.0&lt;/code&gt; to ensure proper port forwarding. For example:&lt;/p&gt;
    &lt;code&gt;# Run this command inside the Docker container
python web_demo.py -c Qwen/Qwen3-Omni-30B-A3B-Instruct --server-port 80 --server-name 0.0.0.0&lt;/code&gt;
    &lt;p&gt;For more ways to launch the web demo, please refer to Launch Local Web UI Demo. If you exit the container, you can re-enter it using the following command:&lt;/p&gt;
    &lt;code&gt;docker start qwen3-omni
docker exec -it qwen3-omni bash&lt;/code&gt;
    &lt;p&gt;Or if you want to completely remove the container, please run:&lt;/p&gt;
    &lt;code&gt;docker rm -f qwen3-omni&lt;/code&gt;
    &lt;p&gt;Qwen3-Omni maintains state-of-the-art performance on text and visual modalities without degradation relative to same-size single-model Qwen counterparts. Across 36 audio and audio-visual benchmarks, it achieves open-source SOTA on 32 and sets the SOTA on 22, outperforming strong closed-source systems such as Gemini 2.5 Pro and GPT-4o.&lt;/p&gt;
    &lt;head&gt;Text -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;GPT-4o-0327&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-235B-A22B&lt;p&gt;Non Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Instruct-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;General&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MMLU-Redux&lt;/cell&gt;
        &lt;cell&gt;91.3&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
        &lt;cell&gt;86.6&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;66.9&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;69.6&lt;/cell&gt;
        &lt;cell&gt;69.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;AIME25&lt;/cell&gt;
        &lt;cell&gt;26.7&lt;/cell&gt;
        &lt;cell&gt;24.7&lt;/cell&gt;
        &lt;cell&gt;61.3&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
        &lt;cell&gt;65.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;ZebraLogic&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;37.7&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;76.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Code&lt;/cell&gt;
        &lt;cell&gt;MultiPL-E&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
        &lt;cell&gt;79.3&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;81.4&lt;/cell&gt;
        &lt;cell&gt;81.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Alignment&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;83.2&lt;/cell&gt;
        &lt;cell&gt;84.7&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Creative Writing v3&lt;/cell&gt;
        &lt;cell&gt;84.9&lt;/cell&gt;
        &lt;cell&gt;80.4&lt;/cell&gt;
        &lt;cell&gt;86.0&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;81.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WritingBench&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;77.0&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;82.6&lt;/cell&gt;
        &lt;cell&gt;83.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;BFCL-v3&lt;/cell&gt;
        &lt;cell&gt;66.5&lt;/cell&gt;
        &lt;cell&gt;68.0&lt;/cell&gt;
        &lt;cell&gt;65.1&lt;/cell&gt;
        &lt;cell&gt;64.4&lt;/cell&gt;
        &lt;cell&gt;65.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Multilingual&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MultiIF&lt;/cell&gt;
        &lt;cell&gt;70.4&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;67.9&lt;/cell&gt;
        &lt;cell&gt;64.0&lt;/cell&gt;
        &lt;cell&gt;64.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PolyMATH&lt;/cell&gt;
        &lt;cell&gt;25.5&lt;/cell&gt;
        &lt;cell&gt;27.0&lt;/cell&gt;
        &lt;cell&gt;43.1&lt;/cell&gt;
        &lt;cell&gt;37.9&lt;/cell&gt;
        &lt;cell&gt;39.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;p&gt;Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-235B-A22B&lt;p&gt;Thinking&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-30B-A3B-Thinking-2507&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;General&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MMLU-Redux&lt;/cell&gt;
        &lt;cell&gt;92.1&lt;/cell&gt;
        &lt;cell&gt;92.7&lt;/cell&gt;
        &lt;cell&gt;91.4&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GPQA&lt;/cell&gt;
        &lt;cell&gt;82.8&lt;/cell&gt;
        &lt;cell&gt;71.1&lt;/cell&gt;
        &lt;cell&gt;73.4&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
        &lt;cell&gt;73.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Reasoning&lt;/cell&gt;
        &lt;cell&gt;AIME25&lt;/cell&gt;
        &lt;cell&gt;72.0&lt;/cell&gt;
        &lt;cell&gt;81.5&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;73.7&lt;/cell&gt;
        &lt;cell&gt;74.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;LiveBench 20241125&lt;/cell&gt;
        &lt;cell&gt;74.3&lt;/cell&gt;
        &lt;cell&gt;77.1&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;71.8&lt;/cell&gt;
        &lt;cell&gt;70.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Code&lt;/cell&gt;
        &lt;cell&gt;MultiPL-E&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;79.9&lt;/cell&gt;
        &lt;cell&gt;81.3&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Alignment&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;88.9&lt;/cell&gt;
        &lt;cell&gt;85.1&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Arena-Hard v2&lt;/cell&gt;
        &lt;cell&gt;56.7&lt;/cell&gt;
        &lt;cell&gt;61.5&lt;/cell&gt;
        &lt;cell&gt;56.0&lt;/cell&gt;
        &lt;cell&gt;55.1&lt;/cell&gt;
        &lt;cell&gt;57.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Creative Writing v3&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;84.4&lt;/cell&gt;
        &lt;cell&gt;82.5&lt;/cell&gt;
        &lt;cell&gt;83.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;WritingBench&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;80.3&lt;/cell&gt;
        &lt;cell&gt;85.0&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;85.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Agent&lt;/cell&gt;
        &lt;cell&gt;BFCL-v3&lt;/cell&gt;
        &lt;cell&gt;68.6&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;72.4&lt;/cell&gt;
        &lt;cell&gt;63.2&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Multilingual&lt;p&gt;Tasks&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;MultiIF&lt;/cell&gt;
        &lt;cell&gt;74.4&lt;/cell&gt;
        &lt;cell&gt;71.9&lt;/cell&gt;
        &lt;cell&gt;76.4&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;73.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PolyMATH&lt;/cell&gt;
        &lt;cell&gt;49.8&lt;/cell&gt;
        &lt;cell&gt;54.7&lt;/cell&gt;
        &lt;cell&gt;52.6&lt;/cell&gt;
        &lt;cell&gt;47.1&lt;/cell&gt;
        &lt;cell&gt;48.7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Audio -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;Seed-ASR&lt;/cell&gt;
        &lt;cell role="head"&gt;Voxtral-Mini&lt;/cell&gt;
        &lt;cell role="head"&gt;Voxtral-Small&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-4o-Transcribe&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;EN &amp;amp; ZH ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Wenetspeech&lt;p&gt;net | meeting&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;4.66 | 5.69&lt;/cell&gt;
        &lt;cell&gt;24.30 | 31.53&lt;/cell&gt;
        &lt;cell&gt;20.33 | 26.08&lt;/cell&gt;
        &lt;cell&gt;15.30 | 32.27&lt;/cell&gt;
        &lt;cell&gt;14.43 | 13.47&lt;/cell&gt;
        &lt;cell&gt;5.91 | 7.65&lt;/cell&gt;
        &lt;cell&gt;4.69 | 5.89&lt;/cell&gt;
        &lt;cell&gt;4.62 | 5.75&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Librispeech&lt;p&gt;clean | other&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;1.58 | 2.84&lt;/cell&gt;
        &lt;cell&gt;1.88 | 4.12&lt;/cell&gt;
        &lt;cell&gt;1.56 | 3.30&lt;/cell&gt;
        &lt;cell&gt;1.39 | 3.75&lt;/cell&gt;
        &lt;cell&gt;2.89 | 3.56&lt;/cell&gt;
        &lt;cell&gt;1.74 | 3.45&lt;/cell&gt;
        &lt;cell&gt;1.22 | 2.48&lt;/cell&gt;
        &lt;cell&gt;1.27 | 2.44&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CV15-en&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;9.47&lt;/cell&gt;
        &lt;cell&gt;7.79&lt;/cell&gt;
        &lt;cell&gt;10.01&lt;/cell&gt;
        &lt;cell&gt;9.89&lt;/cell&gt;
        &lt;cell&gt;7.61&lt;/cell&gt;
        &lt;cell&gt;6.05&lt;/cell&gt;
        &lt;cell&gt;5.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CV15-zh&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;24.67&lt;/cell&gt;
        &lt;cell&gt;19.30&lt;/cell&gt;
        &lt;cell&gt;9.84&lt;/cell&gt;
        &lt;cell&gt;8.00&lt;/cell&gt;
        &lt;cell&gt;5.13&lt;/cell&gt;
        &lt;cell&gt;4.31&lt;/cell&gt;
        &lt;cell&gt;4.28&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-en&lt;/cell&gt;
        &lt;cell&gt;3.40&lt;/cell&gt;
        &lt;cell&gt;3.96&lt;/cell&gt;
        &lt;cell&gt;3.77&lt;/cell&gt;
        &lt;cell&gt;3.32&lt;/cell&gt;
        &lt;cell&gt;2.94&lt;/cell&gt;
        &lt;cell&gt;3.77&lt;/cell&gt;
        &lt;cell&gt;2.72&lt;/cell&gt;
        &lt;cell&gt;2.74&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-zh&lt;/cell&gt;
        &lt;cell&gt;2.69&lt;/cell&gt;
        &lt;cell&gt;12.22&lt;/cell&gt;
        &lt;cell&gt;7.98&lt;/cell&gt;
        &lt;cell&gt;2.44&lt;/cell&gt;
        &lt;cell&gt;2.71&lt;/cell&gt;
        &lt;cell&gt;2.54&lt;/cell&gt;
        &lt;cell&gt;2.20&lt;/cell&gt;
        &lt;cell&gt;2.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Multilingual ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-avg&lt;p&gt;(19 lang)&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;15.67&lt;/cell&gt;
        &lt;cell&gt;8.09&lt;/cell&gt;
        &lt;cell&gt;4.48&lt;/cell&gt;
        &lt;cell&gt;5.55&lt;/cell&gt;
        &lt;cell&gt;14.04&lt;/cell&gt;
        &lt;cell&gt;5.33&lt;/cell&gt;
        &lt;cell&gt;5.31&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Lyric ASR (wer)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MIR-1K (vocal-only)&lt;/cell&gt;
        &lt;cell&gt;6.45&lt;/cell&gt;
        &lt;cell&gt;23.33&lt;/cell&gt;
        &lt;cell&gt;18.73&lt;/cell&gt;
        &lt;cell&gt;11.87&lt;/cell&gt;
        &lt;cell&gt;9.85&lt;/cell&gt;
        &lt;cell&gt;8.15&lt;/cell&gt;
        &lt;cell&gt;5.90&lt;/cell&gt;
        &lt;cell&gt;5.85&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Opencpop-test&lt;/cell&gt;
        &lt;cell&gt;2.98&lt;/cell&gt;
        &lt;cell&gt;31.01&lt;/cell&gt;
        &lt;cell&gt;16.06&lt;/cell&gt;
        &lt;cell&gt;7.93&lt;/cell&gt;
        &lt;cell&gt;6.49&lt;/cell&gt;
        &lt;cell&gt;2.84&lt;/cell&gt;
        &lt;cell&gt;1.54&lt;/cell&gt;
        &lt;cell&gt;2.02&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;S2TT (BLEU)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-en2xx&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;30.35&lt;/cell&gt;
        &lt;cell&gt;37.85&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;39.25&lt;/cell&gt;
        &lt;cell&gt;29.22&lt;/cell&gt;
        &lt;cell&gt;37.50&lt;/cell&gt;
        &lt;cell&gt;36.22&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-xx2en&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;27.54&lt;/cell&gt;
        &lt;cell&gt;32.81&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;35.41&lt;/cell&gt;
        &lt;cell&gt;28.61&lt;/cell&gt;
        &lt;cell&gt;31.08&lt;/cell&gt;
        &lt;cell&gt;30.71&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Fleurs-zh2xx&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;17.03&lt;/cell&gt;
        &lt;cell&gt;22.05&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;26.63&lt;/cell&gt;
        &lt;cell&gt;17.97&lt;/cell&gt;
        &lt;cell&gt;25.17&lt;/cell&gt;
        &lt;cell&gt;25.10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Fleurs-xx2zh&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;28.75&lt;/cell&gt;
        &lt;cell&gt;34.82&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;37.50&lt;/cell&gt;
        &lt;cell&gt;27.68&lt;/cell&gt;
        &lt;cell&gt;33.13&lt;/cell&gt;
        &lt;cell&gt;31.19&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;GPT-4o-Audio&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;VoiceBench&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AlpacaEval&lt;/cell&gt;
        &lt;cell&gt;95.6&lt;/cell&gt;
        &lt;cell&gt;96.1&lt;/cell&gt;
        &lt;cell&gt;94.3&lt;/cell&gt;
        &lt;cell&gt;89.9&lt;/cell&gt;
        &lt;cell&gt;94.8&lt;/cell&gt;
        &lt;cell&gt;96.4&lt;/cell&gt;
        &lt;cell&gt;95.4&lt;/cell&gt;
        &lt;cell&gt;96.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;CommonEval&lt;/cell&gt;
        &lt;cell&gt;89.8&lt;/cell&gt;
        &lt;cell&gt;88.3&lt;/cell&gt;
        &lt;cell&gt;88.4&lt;/cell&gt;
        &lt;cell&gt;76.7&lt;/cell&gt;
        &lt;cell&gt;90.8&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;91.0&lt;/cell&gt;
        &lt;cell&gt;90.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;WildVoice&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;92.1&lt;/cell&gt;
        &lt;cell&gt;93.4&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;91.6&lt;/cell&gt;
        &lt;cell&gt;90.5&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;90.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;SD-QA&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;84.5&lt;/cell&gt;
        &lt;cell&gt;90.1&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;78.1&lt;/cell&gt;
        &lt;cell&gt;76.8&lt;/cell&gt;
        &lt;cell&gt;78.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMSU&lt;/cell&gt;
        &lt;cell&gt;80.3&lt;/cell&gt;
        &lt;cell&gt;66.1&lt;/cell&gt;
        &lt;cell&gt;71.1&lt;/cell&gt;
        &lt;cell&gt;61.7&lt;/cell&gt;
        &lt;cell&gt;68.1&lt;/cell&gt;
        &lt;cell&gt;83.0&lt;/cell&gt;
        &lt;cell&gt;68.4&lt;/cell&gt;
        &lt;cell&gt;84.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;OpenBookQA&lt;/cell&gt;
        &lt;cell&gt;89.2&lt;/cell&gt;
        &lt;cell&gt;56.9&lt;/cell&gt;
        &lt;cell&gt;92.3&lt;/cell&gt;
        &lt;cell&gt;80.9&lt;/cell&gt;
        &lt;cell&gt;89.7&lt;/cell&gt;
        &lt;cell&gt;94.3&lt;/cell&gt;
        &lt;cell&gt;91.4&lt;/cell&gt;
        &lt;cell&gt;95.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;BBH&lt;/cell&gt;
        &lt;cell&gt;84.1&lt;/cell&gt;
        &lt;cell&gt;83.9&lt;/cell&gt;
        &lt;cell&gt;92.6&lt;/cell&gt;
        &lt;cell&gt;66.7&lt;/cell&gt;
        &lt;cell&gt;80.4&lt;/cell&gt;
        &lt;cell&gt;88.9&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;IFEval&lt;/cell&gt;
        &lt;cell&gt;76.0&lt;/cell&gt;
        &lt;cell&gt;83.8&lt;/cell&gt;
        &lt;cell&gt;85.7&lt;/cell&gt;
        &lt;cell&gt;53.5&lt;/cell&gt;
        &lt;cell&gt;77.8&lt;/cell&gt;
        &lt;cell&gt;80.6&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;80.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;AdvBench&lt;/cell&gt;
        &lt;cell&gt;98.7&lt;/cell&gt;
        &lt;cell&gt;98.9&lt;/cell&gt;
        &lt;cell&gt;98.1&lt;/cell&gt;
        &lt;cell&gt;99.2&lt;/cell&gt;
        &lt;cell&gt;99.3&lt;/cell&gt;
        &lt;cell&gt;97.2&lt;/cell&gt;
        &lt;cell&gt;99.4&lt;/cell&gt;
        &lt;cell&gt;98.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Overall&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;83.4&lt;/cell&gt;
        &lt;cell&gt;89.6&lt;/cell&gt;
        &lt;cell&gt;73.6&lt;/cell&gt;
        &lt;cell&gt;85.5&lt;/cell&gt;
        &lt;cell&gt;88.8&lt;/cell&gt;
        &lt;cell&gt;85.6&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;Audio Reasoning&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;MMAU-v05.15.25&lt;/cell&gt;
        &lt;cell&gt;62.5&lt;/cell&gt;
        &lt;cell&gt;71.8&lt;/cell&gt;
        &lt;cell&gt;77.4&lt;/cell&gt;
        &lt;cell&gt;65.5&lt;/cell&gt;
        &lt;cell&gt;77.5&lt;/cell&gt;
        &lt;cell&gt;75.4&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;76.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MMSU&lt;/cell&gt;
        &lt;cell&gt;56.4&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;62.6&lt;/cell&gt;
        &lt;cell&gt;69.0&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Best Specialist&lt;p&gt;Models&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT-4o-Audio&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Pro&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;RUL-MuchoMusic&lt;/cell&gt;
        &lt;cell&gt;47.6 (Audio Flamingo 3)&lt;/cell&gt;
        &lt;cell&gt;36.1&lt;/cell&gt;
        &lt;cell&gt;49.4&lt;/cell&gt;
        &lt;cell&gt;47.3&lt;/cell&gt;
        &lt;cell&gt;52.0&lt;/cell&gt;
        &lt;cell&gt;52.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;GTZAN&lt;p&gt;Acc.&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;87.9 (CLaMP 3)&lt;/cell&gt;
        &lt;cell&gt;76.5&lt;/cell&gt;
        &lt;cell&gt;81.0&lt;/cell&gt;
        &lt;cell&gt;81.7&lt;/cell&gt;
        &lt;cell&gt;93.0&lt;/cell&gt;
        &lt;cell&gt;93.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Genre&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;35.8 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;25.3&lt;/cell&gt;
        &lt;cell&gt;32.6&lt;/cell&gt;
        &lt;cell&gt;32.5&lt;/cell&gt;
        &lt;cell&gt;39.0&lt;/cell&gt;
        &lt;cell&gt;39.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Mood/Theme&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;10.9 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;11.3&lt;/cell&gt;
        &lt;cell&gt;14.1&lt;/cell&gt;
        &lt;cell&gt;8.9&lt;/cell&gt;
        &lt;cell&gt;21.0&lt;/cell&gt;
        &lt;cell&gt;21.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Instrument&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;39.8 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;34.2&lt;/cell&gt;
        &lt;cell&gt;33.0&lt;/cell&gt;
        &lt;cell&gt;22.6&lt;/cell&gt;
        &lt;cell&gt;40.5&lt;/cell&gt;
        &lt;cell&gt;40.7&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;MTG Top50&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;33.2 (MuQ-MuLan)&lt;/cell&gt;
        &lt;cell&gt;25.0&lt;/cell&gt;
        &lt;cell&gt;26.1&lt;/cell&gt;
        &lt;cell&gt;21.6&lt;/cell&gt;
        &lt;cell&gt;36.7&lt;/cell&gt;
        &lt;cell&gt;36.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MagnaTagATune&lt;p&gt;Micro F1&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;41.6 (MuQ)&lt;/cell&gt;
        &lt;cell&gt;29.2&lt;/cell&gt;
        &lt;cell&gt;28.1&lt;/cell&gt;
        &lt;cell&gt;30.1&lt;/cell&gt;
        &lt;cell&gt;44.3&lt;/cell&gt;
        &lt;cell&gt;46.8&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Vision -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;GPT4-o&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.0-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-VL&lt;p&gt;72B&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B&lt;p&gt;-Instruct&lt;/p&gt;&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash&lt;p&gt;-Instruct&lt;/p&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;General Visual Question Answering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMStar&lt;/cell&gt;
        &lt;cell&gt;64.7&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;70.8&lt;/cell&gt;
        &lt;cell&gt;68.5&lt;/cell&gt;
        &lt;cell&gt;69.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;HallusionBench&lt;/cell&gt;
        &lt;cell&gt;55.0&lt;/cell&gt;
        &lt;cell&gt;56.3&lt;/cell&gt;
        &lt;cell&gt;55.2&lt;/cell&gt;
        &lt;cell&gt;59.7&lt;/cell&gt;
        &lt;cell&gt;58.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MM-MT-Bench&lt;/cell&gt;
        &lt;cell&gt;7.7&lt;/cell&gt;
        &lt;cell&gt;6.7&lt;/cell&gt;
        &lt;cell&gt;7.6&lt;/cell&gt;
        &lt;cell&gt;7.4&lt;/cell&gt;
        &lt;cell&gt;7.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Math &amp;amp; STEM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMMU_val&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;71.3&lt;/cell&gt;
        &lt;cell&gt;70.2&lt;/cell&gt;
        &lt;cell&gt;69.1&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MMMU_pro&lt;/cell&gt;
        &lt;cell&gt;51.9&lt;/cell&gt;
        &lt;cell&gt;56.1&lt;/cell&gt;
        &lt;cell&gt;51.1&lt;/cell&gt;
        &lt;cell&gt;57.0&lt;/cell&gt;
        &lt;cell&gt;57.6&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MathVista_mini&lt;/cell&gt;
        &lt;cell&gt;63.8&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
        &lt;cell&gt;74.8&lt;/cell&gt;
        &lt;cell&gt;75.9&lt;/cell&gt;
        &lt;cell&gt;77.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;MathVision_full&lt;/cell&gt;
        &lt;cell&gt;30.4&lt;/cell&gt;
        &lt;cell&gt;48.6&lt;/cell&gt;
        &lt;cell&gt;38.1&lt;/cell&gt;
        &lt;cell&gt;56.3&lt;/cell&gt;
        &lt;cell&gt;58.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Documentation Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;AI2D&lt;/cell&gt;
        &lt;cell&gt;84.6&lt;/cell&gt;
        &lt;cell&gt;86.7&lt;/cell&gt;
        &lt;cell&gt;88.7&lt;/cell&gt;
        &lt;cell&gt;85.2&lt;/cell&gt;
        &lt;cell&gt;86.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;ChartQA_test&lt;/cell&gt;
        &lt;cell&gt;86.7&lt;/cell&gt;
        &lt;cell&gt;64.6&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
        &lt;cell&gt;87.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Counting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;CountBench&lt;/cell&gt;
        &lt;cell&gt;87.9&lt;/cell&gt;
        &lt;cell&gt;91.2&lt;/cell&gt;
        &lt;cell&gt;93.6&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
        &lt;cell&gt;90.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Video Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;Video-MME&lt;/cell&gt;
        &lt;cell&gt;71.9&lt;/cell&gt;
        &lt;cell&gt;72.4&lt;/cell&gt;
        &lt;cell&gt;73.3&lt;/cell&gt;
        &lt;cell&gt;70.5&lt;/cell&gt;
        &lt;cell&gt;71.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;LVBench&lt;/cell&gt;
        &lt;cell&gt;30.8&lt;/cell&gt;
        &lt;cell&gt;57.9&lt;/cell&gt;
        &lt;cell&gt;47.3&lt;/cell&gt;
        &lt;cell&gt;50.2&lt;/cell&gt;
        &lt;cell&gt;51.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MLVU&lt;/cell&gt;
        &lt;cell&gt;64.6&lt;/cell&gt;
        &lt;cell&gt;71.0&lt;/cell&gt;
        &lt;cell&gt;74.6&lt;/cell&gt;
        &lt;cell&gt;75.2&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-flash-thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;InternVL-3.5-241B-A28B&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;General Visual Question Answering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMStar&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
        &lt;cell&gt;77.9&lt;/cell&gt;
        &lt;cell&gt;74.9&lt;/cell&gt;
        &lt;cell&gt;75.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HallusionBench&lt;/cell&gt;
        &lt;cell&gt;61.1&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
        &lt;cell&gt;62.8&lt;/cell&gt;
        &lt;cell&gt;63.4&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MM-MT-Bench&lt;/cell&gt;
        &lt;cell&gt;7.8&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
        &lt;cell&gt;8.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Math &amp;amp; STEM&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMMU_val&lt;/cell&gt;
        &lt;cell&gt;76.9&lt;/cell&gt;
        &lt;cell&gt;77.7&lt;/cell&gt;
        &lt;cell&gt;75.6&lt;/cell&gt;
        &lt;cell&gt;75.0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MMMU_pro&lt;/cell&gt;
        &lt;cell&gt;65.8&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;60.5&lt;/cell&gt;
        &lt;cell&gt;60.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MathVista_mini&lt;/cell&gt;
        &lt;cell&gt;77.6&lt;/cell&gt;
        &lt;cell&gt;82.7&lt;/cell&gt;
        &lt;cell&gt;80.0&lt;/cell&gt;
        &lt;cell&gt;81.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;MathVision_full&lt;/cell&gt;
        &lt;cell&gt;62.3&lt;/cell&gt;
        &lt;cell&gt;63.9&lt;/cell&gt;
        &lt;cell&gt;62.9&lt;/cell&gt;
        &lt;cell&gt;63.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Documentation Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;AI2D_test&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;87.3&lt;/cell&gt;
        &lt;cell&gt;86.1&lt;/cell&gt;
        &lt;cell&gt;86.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;ChartQA_test&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;88.0&lt;/cell&gt;
        &lt;cell&gt;89.5&lt;/cell&gt;
        &lt;cell&gt;89.3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Counting&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;CountBench&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;88.6&lt;/cell&gt;
        &lt;cell&gt;92.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Video Understanding&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Video-MME&lt;/cell&gt;
        &lt;cell&gt;79.6&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;69.7&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;LVBench&lt;/cell&gt;
        &lt;cell&gt;64.5&lt;/cell&gt;
        &lt;cell&gt;‚Äì&lt;/cell&gt;
        &lt;cell&gt;49.0&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;MLVU&lt;/cell&gt;
        &lt;cell&gt;82.1&lt;/cell&gt;
        &lt;cell&gt;78.2&lt;/cell&gt;
        &lt;cell&gt;72.9&lt;/cell&gt;
        &lt;cell&gt;73.9&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;AudioVisual -&amp;gt; Text&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Previous Open-source SoTA&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen2.5-Omni&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Instruct&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WorldSense&lt;/cell&gt;
        &lt;cell&gt;47.1&lt;/cell&gt;
        &lt;cell&gt;50.9&lt;/cell&gt;
        &lt;cell&gt;45.4&lt;/cell&gt;
        &lt;cell&gt;54.0&lt;/cell&gt;
        &lt;cell&gt;54.1&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Previous Open-source SoTA&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemini-2.5-Flash-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-Flash-Thinking&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;DailyOmni&lt;/cell&gt;
        &lt;cell&gt;69.8&lt;/cell&gt;
        &lt;cell&gt;72.7&lt;/cell&gt;
        &lt;cell&gt;75.8&lt;/cell&gt;
        &lt;cell&gt;76.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;VideoHolmes&lt;/cell&gt;
        &lt;cell&gt;55.6&lt;/cell&gt;
        &lt;cell&gt;49.5&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
        &lt;cell&gt;57.3&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Zero-shot Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Datasets&lt;/cell&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Performance&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Content Consistency&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SEED&lt;p&gt;test-zh | test-en&lt;/p&gt;&lt;/cell&gt;
        &lt;cell&gt;Seed-TTSICL&lt;/cell&gt;
        &lt;cell&gt;1.11 | 2.24&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Seed-TTSRL&lt;/cell&gt;
        &lt;cell&gt;1.00 | 1.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MaskGCT&lt;/cell&gt;
        &lt;cell&gt;2.27 | 2.62&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;E2 TTS&lt;/cell&gt;
        &lt;cell&gt;1.97 | 2.19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;F5-TTS&lt;/cell&gt;
        &lt;cell&gt;1.56 | 1.83&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Spark TTS&lt;/cell&gt;
        &lt;cell&gt;1.20 | 1.98&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CosyVoice 2&lt;/cell&gt;
        &lt;cell&gt;1.45 | 2.57&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CosyVoice 3&lt;/cell&gt;
        &lt;cell&gt;0.71 | 1.45&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Qwen2.5-Omni-7B&lt;/cell&gt;
        &lt;cell&gt;1.42 | 2.33&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;1.07 | 1.39&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Multilingual Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="7"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Content Consistency&lt;/cell&gt;
        &lt;cell role="head"&gt;Speaker Similarity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;MiniMax&lt;/cell&gt;
        &lt;cell&gt;ElevenLabs&lt;/cell&gt;
        &lt;cell&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell&gt;MiniMax&lt;/cell&gt;
        &lt;cell&gt;ElevenLabs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Chinese&lt;/cell&gt;
        &lt;cell&gt;0.716&lt;/cell&gt;
        &lt;cell&gt;2.252&lt;/cell&gt;
        &lt;cell&gt;16.026&lt;/cell&gt;
        &lt;cell&gt;0.772&lt;/cell&gt;
        &lt;cell&gt;0.780&lt;/cell&gt;
        &lt;cell&gt;0.677&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;English&lt;/cell&gt;
        &lt;cell&gt;1.069&lt;/cell&gt;
        &lt;cell&gt;2.164&lt;/cell&gt;
        &lt;cell&gt;2.339&lt;/cell&gt;
        &lt;cell&gt;0.773&lt;/cell&gt;
        &lt;cell&gt;0.756&lt;/cell&gt;
        &lt;cell&gt;0.613&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;German&lt;/cell&gt;
        &lt;cell&gt;0.777&lt;/cell&gt;
        &lt;cell&gt;1.906&lt;/cell&gt;
        &lt;cell&gt;0.572&lt;/cell&gt;
        &lt;cell&gt;0.738&lt;/cell&gt;
        &lt;cell&gt;0.733&lt;/cell&gt;
        &lt;cell&gt;0.614&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Italian&lt;/cell&gt;
        &lt;cell&gt;1.067&lt;/cell&gt;
        &lt;cell&gt;1.543&lt;/cell&gt;
        &lt;cell&gt;1.743&lt;/cell&gt;
        &lt;cell&gt;0.742&lt;/cell&gt;
        &lt;cell&gt;0.699&lt;/cell&gt;
        &lt;cell&gt;0.579&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Portuguese&lt;/cell&gt;
        &lt;cell&gt;1.872&lt;/cell&gt;
        &lt;cell&gt;1.877&lt;/cell&gt;
        &lt;cell&gt;1.331&lt;/cell&gt;
        &lt;cell&gt;0.770&lt;/cell&gt;
        &lt;cell&gt;0.805&lt;/cell&gt;
        &lt;cell&gt;0.711&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Spanish&lt;/cell&gt;
        &lt;cell&gt;1.765&lt;/cell&gt;
        &lt;cell&gt;1.029&lt;/cell&gt;
        &lt;cell&gt;1.084&lt;/cell&gt;
        &lt;cell&gt;0.744&lt;/cell&gt;
        &lt;cell&gt;0.762&lt;/cell&gt;
        &lt;cell&gt;0.615&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Japanese&lt;/cell&gt;
        &lt;cell&gt;3.631&lt;/cell&gt;
        &lt;cell&gt;3.519&lt;/cell&gt;
        &lt;cell&gt;10.646&lt;/cell&gt;
        &lt;cell&gt;0.763&lt;/cell&gt;
        &lt;cell&gt;0.776&lt;/cell&gt;
        &lt;cell&gt;0.738&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;Korean&lt;/cell&gt;
        &lt;cell&gt;1.670&lt;/cell&gt;
        &lt;cell&gt;1.747&lt;/cell&gt;
        &lt;cell&gt;1.865&lt;/cell&gt;
        &lt;cell&gt;0.778&lt;/cell&gt;
        &lt;cell&gt;0.776&lt;/cell&gt;
        &lt;cell&gt;0.700&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="7"&gt;
        &lt;cell&gt;French&lt;/cell&gt;
        &lt;cell&gt;2.505&lt;/cell&gt;
        &lt;cell&gt;4.099&lt;/cell&gt;
        &lt;cell&gt;5.216&lt;/cell&gt;
        &lt;cell&gt;0.689&lt;/cell&gt;
        &lt;cell&gt;0.628&lt;/cell&gt;
        &lt;cell&gt;0.535&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Russian&lt;/cell&gt;
        &lt;cell&gt;3.986&lt;/cell&gt;
        &lt;cell&gt;4.281&lt;/cell&gt;
        &lt;cell&gt;3.878&lt;/cell&gt;
        &lt;cell&gt;0.759&lt;/cell&gt;
        &lt;cell&gt;0.761&lt;/cell&gt;
        &lt;cell&gt;0.676&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head&gt;Cross-Lingual Speech Generation&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Language&lt;/cell&gt;
        &lt;cell role="head"&gt;Qwen3-Omni-30B-A3B&lt;/cell&gt;
        &lt;cell role="head"&gt;CosyVoice3&lt;/cell&gt;
        &lt;cell role="head"&gt;CosyVoice2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-zh&lt;/cell&gt;
        &lt;cell&gt;5.37&lt;/cell&gt;
        &lt;cell&gt;5.09&lt;/cell&gt;
        &lt;cell&gt;13.5&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ja-to-zh&lt;/cell&gt;
        &lt;cell&gt;3.32&lt;/cell&gt;
        &lt;cell&gt;3.05&lt;/cell&gt;
        &lt;cell&gt;48.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-zh&lt;/cell&gt;
        &lt;cell&gt;0.99&lt;/cell&gt;
        &lt;cell&gt;1.06&lt;/cell&gt;
        &lt;cell&gt;7.70&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-en&lt;/cell&gt;
        &lt;cell&gt;2.76&lt;/cell&gt;
        &lt;cell&gt;2.98&lt;/cell&gt;
        &lt;cell&gt;6.47&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ja-to-en&lt;/cell&gt;
        &lt;cell&gt;3.31&lt;/cell&gt;
        &lt;cell&gt;4.20&lt;/cell&gt;
        &lt;cell&gt;17.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-en&lt;/cell&gt;
        &lt;cell&gt;3.34&lt;/cell&gt;
        &lt;cell&gt;4.19&lt;/cell&gt;
        &lt;cell&gt;11.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-ja&lt;/cell&gt;
        &lt;cell&gt;8.29&lt;/cell&gt;
        &lt;cell&gt;7.08&lt;/cell&gt;
        &lt;cell&gt;13.1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-ja&lt;/cell&gt;
        &lt;cell&gt;7.53&lt;/cell&gt;
        &lt;cell&gt;6.80&lt;/cell&gt;
        &lt;cell&gt;14.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;ko-to-ja&lt;/cell&gt;
        &lt;cell&gt;4.24&lt;/cell&gt;
        &lt;cell&gt;3.93&lt;/cell&gt;
        &lt;cell&gt;5.86&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;zh-to-ko&lt;/cell&gt;
        &lt;cell&gt;5.13&lt;/cell&gt;
        &lt;cell&gt;14.4&lt;/cell&gt;
        &lt;cell&gt;24.8&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;en-to-ko&lt;/cell&gt;
        &lt;cell&gt;4.96&lt;/cell&gt;
        &lt;cell&gt;5.87&lt;/cell&gt;
        &lt;cell&gt;21.9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;ja-to-ko&lt;/cell&gt;
        &lt;cell&gt;6.23&lt;/cell&gt;
        &lt;cell&gt;7.92&lt;/cell&gt;
        &lt;cell&gt;21.5&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Decoding Strategy: For the Qwen3-Omni series across all evaluation benchmarks, &lt;code&gt;Instruct&lt;/code&gt;models use greedy decoding during generation without sampling. For&lt;code&gt;Thinking&lt;/code&gt;models, the decoding parameters should be taken from the&lt;code&gt;generation_config.json&lt;/code&gt;file in the checkpoint.&lt;/item&gt;
      &lt;item&gt;Benchmark-Specific Formatting: For the majority of evaluation benchmarks, they come with their own ChatML formatting to embed the question or prompt. It should be noted that all video data are set to &lt;code&gt;fps=2&lt;/code&gt;during evaluation.&lt;/item&gt;
      &lt;item&gt;Default Prompts: For tasks in certain benchmarks that do not include a prompt, we use the following prompt settings:&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Task Type&lt;/cell&gt;
        &lt;cell role="head"&gt;Prompt&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Auto Speech Recognition (ASR) for Chinese&lt;/cell&gt;
        &lt;cell&gt;ËØ∑Â∞ÜËøôÊÆµ‰∏≠ÊñáËØ≠Èü≥ËΩ¨Êç¢‰∏∫Á∫ØÊñáÊú¨„ÄÇ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Auto Speech Recognition (ASR) for Other languages&lt;/cell&gt;
        &lt;cell&gt;Transcribe the audio into text.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Speech-to-Text Translation (S2TT)&lt;/cell&gt;
        &lt;cell&gt;Listen to the provided &amp;lt;source_language&amp;gt; speech and produce a translation in &amp;lt;target_language&amp;gt; text.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Song Lyrics Recognition&lt;/cell&gt;
        &lt;cell&gt;Transcribe the song lyrics into text without any punctuation, separate lines with line breaks, and output only the lyrics without additional explanations.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;System Prompt: No &lt;code&gt;system prompt&lt;/code&gt;should be set for any evaluation benchmark.&lt;/item&gt;
      &lt;item&gt;Input Sequence: The question or prompt should be input as user text. Unless otherwise specified by the benchmark, the text should come after multimodal data in the sequence. For example:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;messages = [
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "image", "image": "/path/to/image.png"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "text", "text": "Describe the audio, image and video."},
        ],
    },
]&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45336989</guid><pubDate>Mon, 22 Sep 2025 17:50:21 +0000</pubDate></item><item><title>Show HN: Python Audio Transcription: Convert Speech to Text Locally</title><link>https://www.pavlinbg.com/posts/python-speech-to-text-guide</link><description>&lt;doc fingerprint="840a859b39aa1292"&gt;
  &lt;main&gt;
    &lt;p&gt;Last week, I faced a dilemma that many researchers, journalists, and content creators know all too well: I had hours of recordings that needed to be transcribed. I had serious privacy concerns about uploading sensitive content to commercial transcription services and their third-party servers.&lt;/p&gt;
    &lt;p&gt;Instead of risking it, I built a Python-based transcription system using OpenAI‚Äôs Whisper model. The result? All my audio files were transcribed in under 10 minutes with 96% accuracy‚Äîcompletely free and processed locally on my laptop.&lt;/p&gt;
    &lt;p&gt;In this post, I will show you how you can build a simple script for processing any audio data without recurring costs or privacy compromises.&lt;/p&gt;
    &lt;head rend="h2"&gt;Essential Setup Requirements&lt;/head&gt;
    &lt;head rend="h3"&gt;1. FFmpeg Installation (Critical First Step)&lt;/head&gt;
    &lt;p&gt;FFmpeg handles audio processing and is required for all transcription methods. This is the #1 cause of setup failures.&lt;/p&gt;
    &lt;head rend="h4"&gt;‚ö†Ô∏è Setup Priority&lt;/head&gt;
    &lt;p&gt;Windows:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Download from ffmpeg.org/download.html&lt;/item&gt;
      &lt;item&gt;Extract to &lt;code&gt;C:\ffmpeg&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Add &lt;code&gt;C:\ffmpeg\bin&lt;/code&gt;to your PATH environment variable&lt;/item&gt;
      &lt;item&gt;Restart your terminal&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;macOS:&lt;/p&gt;
    &lt;code&gt;# Using Homebrew (recommended)
brew install ffmpeg&lt;/code&gt;
    &lt;p&gt;Linux (Ubuntu/Debian):&lt;/p&gt;
    &lt;code&gt;sudo apt update &amp;amp;&amp;amp; sudo apt install ffmpeg&lt;/code&gt;
    &lt;p&gt;Verify Installation:&lt;/p&gt;
    &lt;code&gt;ffmpeg -version&lt;/code&gt;
    &lt;p&gt;You should see version information. If you get ‚Äúcommand not found,‚Äù FFmpeg isn‚Äôt properly installed.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Python Environment Setup&lt;/head&gt;
    &lt;head rend="h4"&gt;üîß Virtual Environment Benefits&lt;/head&gt;
    &lt;code&gt;# Create isolated environment
python -m venv whisper-env
cd whisper-env

# Activate environment
# Windows:
Scripts\activate
# macOS/Linux:
source bin/activate

# Install required packages
pip install openai-whisper&lt;/code&gt;
    &lt;head rend="h2"&gt;Method 1: OpenAI Whisper (Recommended)&lt;/head&gt;
    &lt;p&gt;Whisper is OpenAI‚Äôs state-of-the-art speech recognition model, trained on 680,000 hours of multilingual audio. It‚Äôs specifically designed for robust, real-world audio transcription and handles various accents, background noise, and audio quality issues remarkably well.&lt;/p&gt;
    &lt;head rend="h3"&gt;Choosing the Right Whisper Model&lt;/head&gt;
    &lt;head rend="h4"&gt;üéØ Model Selection Guide&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="6"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;RAM Required&lt;/cell&gt;
        &lt;cell role="head"&gt;Speed&lt;/cell&gt;
        &lt;cell role="head"&gt;Accuracy&lt;/cell&gt;
        &lt;cell role="head"&gt;Best Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;tiny&lt;/cell&gt;
        &lt;cell&gt;39 MB&lt;/cell&gt;
        &lt;cell&gt;390 MB&lt;/cell&gt;
        &lt;cell&gt;32x realtime&lt;/cell&gt;
        &lt;cell&gt;89%&lt;/cell&gt;
        &lt;cell&gt;Quick testing, real-time applications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;base&lt;/cell&gt;
        &lt;cell&gt;74 MB&lt;/cell&gt;
        &lt;cell&gt;740 MB&lt;/cell&gt;
        &lt;cell&gt;16x realtime&lt;/cell&gt;
        &lt;cell&gt;94%&lt;/cell&gt;
        &lt;cell&gt;General use (recommended)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;small&lt;/cell&gt;
        &lt;cell&gt;244 MB&lt;/cell&gt;
        &lt;cell&gt;2.4 GB&lt;/cell&gt;
        &lt;cell&gt;6x realtime&lt;/cell&gt;
        &lt;cell&gt;96%&lt;/cell&gt;
        &lt;cell&gt;High-quality transcription needs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="6"&gt;
        &lt;cell&gt;medium&lt;/cell&gt;
        &lt;cell&gt;769 MB&lt;/cell&gt;
        &lt;cell&gt;5 GB&lt;/cell&gt;
        &lt;cell&gt;2x realtime&lt;/cell&gt;
        &lt;cell&gt;97%&lt;/cell&gt;
        &lt;cell&gt;Professional work, critical accuracy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;large&lt;/cell&gt;
        &lt;cell&gt;1.5 GB&lt;/cell&gt;
        &lt;cell&gt;10 GB&lt;/cell&gt;
        &lt;cell&gt;1x realtime&lt;/cell&gt;
        &lt;cell&gt;98%&lt;/cell&gt;
        &lt;cell&gt;Maximum accuracy, research purposes&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Basic Whisper Implementation&lt;/head&gt;
    &lt;p&gt;Here‚Äôs a clean, production-ready implementation:&lt;/p&gt;
    &lt;code&gt;import whisper
import os
from pathlib import Path
import time

class AudioTranscriber:
    def __init__(self, model_size="base"):
        """Initialize transcriber with specified Whisper model"""
        print(f"Loading Whisper {model_size} model...")
        self.model = whisper.load_model(model_size)
        print("Model loaded successfully!")
    
    def transcribe_file(self, audio_path, language=None):
        """
        Transcribe a single audio file
        
        Args:
            audio_path: Path to audio file
            language: Language code ('en', 'es', 'fr', etc.) or None for auto-detect
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"Audio file not found: {audio_path}")
        
        print(f"Transcribing: {Path(audio_path).name}")
        
        start_time = time.time()
        
        # Transcribe audio
        options = {"language": language} if language else {}
        result = self.model.transcribe(audio_path, **options)
        
        processing_time = time.time() - start_time
        
        print(f"‚úì Completed in {processing_time:.1f} seconds")
        print(f"‚úì Detected language: {result['language']}")
        
        return {
            'text': result['text'].strip(),
            'language': result['language'],
            'segments': result.get('segments', []),
            'processing_time': processing_time
        }
    
    def save_transcription(self, result, output_path):
        """Save transcription to text file"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("=== Transcription Results ===\n")
            f.write(f"Language: {result['language']}\n")
            f.write(f"Processing Time: {result['processing_time']:.1f} seconds\n")
            f.write("=" * 40 + "\n\n")
            f.write(result['text'])
        
        print(f"‚úì Transcription saved to: {output_path}")

# Usage example
def transcribe_audio_file(audio_path, model_size="base", language=None):
    """Simple function to transcribe an audio file"""
    
    transcriber = AudioTranscriber(model_size=model_size)
    result = transcriber.transcribe_file(audio_path, language=language)
    
    # Save transcription
    audio_name = Path(audio_path).stem
    output_path = f"{audio_name}_transcript.txt"
    transcriber.save_transcription(result, output_path)
    
    return result

# Example usage
if __name__ == "__main__":
    # Transcribe a file
    audio_file = "interview.wav"  # Replace with your audio file
    result = transcribe_audio_file(audio_file, model_size="base", language="en")
    
    print(f"\nTranscription preview:")
    print(result['text'][:200] + "..." if len(result['text']) &amp;gt; 200 else result['text'])&lt;/code&gt;
    &lt;head rend="h4"&gt;üéµ Supported Audio Formats&lt;/head&gt;
    &lt;head rend="h3"&gt;Batch Processing Multiple Files&lt;/head&gt;
    &lt;p&gt;For processing multiple audio files efficiently:&lt;/p&gt;
    &lt;code&gt;def batch_transcribe(audio_files, output_dir="transcripts", model_size="base"):
    """Transcribe multiple audio files"""
    
    os.makedirs(output_dir, exist_ok=True)
    transcriber = AudioTranscriber(model_size=model_size)
    
    results = []
    
    for i, audio_file in enumerate(audio_files, 1):
        print(f"\n--- Processing file {i}/{len(audio_files)} ---")
        
        try:
            result = transcriber.transcribe_file(audio_file)
            
            # Save individual transcription
            file_name = Path(audio_file).stem
            output_path = os.path.join(output_dir, f"{file_name}_transcript.txt")
            transcriber.save_transcription(result, output_path)
            
            results.append(result)
            
        except Exception as e:
            print(f"‚úó Failed to process {audio_file}: {str(e)}")
            continue
    
    print(f"\n‚úì Batch processing completed: {len(results)}/{len(audio_files)} files successful")
    return results

# Usage
audio_files = ["interview1.wav", "interview2.mp3", "lecture.m4a"]
batch_transcribe(audio_files, output_dir="my_transcripts")&lt;/code&gt;
    &lt;head rend="h3"&gt;Creating Subtitle Files (SRT Format)&lt;/head&gt;
    &lt;p&gt;Generate subtitle files for videos:&lt;/p&gt;
    &lt;code&gt;def create_srt_subtitles(audio_path, output_path=None):
    """Create SRT subtitle file from audio"""
    
    transcriber = AudioTranscriber(model_size="base")
    result = transcriber.transcribe_file(audio_path)
    
    if output_path is None:
        output_path = Path(audio_path).stem + ".srt"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for i, segment in enumerate(result['segments'], 1):
            start_time = format_timestamp(segment['start'])
            end_time = format_timestamp(segment['end'])
            
            f.write(f"{i}\n")
            f.write(f"{start_time} --&amp;gt; {end_time}\n")
            f.write(f"{segment['text'].strip()}\n\n")
    
    print(f"‚úì SRT subtitles saved to: {output_path}")

def format_timestamp(seconds):
    """Convert seconds to SRT timestamp format"""
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    secs = int(seconds % 60)
    millisecs = int((seconds % 1) * 1000)
    return f"{hours:02d}:{minutes:02d}:{secs:02d},{millisecs:03d}"

# Usage
create_srt_subtitles("presentation.mp4")&lt;/code&gt;
    &lt;head rend="h2"&gt;Method 2: Alternative with SpeechRecognition Library&lt;/head&gt;
    &lt;p&gt;For scenarios requiring different recognition engines or more control over audio preprocessing:&lt;/p&gt;
    &lt;code&gt;import speech_recognition as sr
from pydub import AudioSegment
import tempfile
import os

class FlexibleTranscriber:
    def __init__(self, engine="google"):
        """Initialize with specified recognition engine"""
        self.recognizer = sr.Recognizer()
        self.engine = engine
        
        # Optimize settings
        self.recognizer.energy_threshold = 300
        self.recognizer.dynamic_energy_threshold = True
        
    def preprocess_audio(self, audio_path):
        """Optimize audio for better recognition"""
        audio = AudioSegment.from_file(audio_path)
        
        # Convert to mono and normalize
        if audio.channels &amp;gt; 1:
            audio = audio.set_channels(1)
        
        audio = audio.set_frame_rate(16000)  # Standard sample rate
        audio = audio.normalize()  # Normalize volume
        
        # Export to temporary WAV file
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')
        audio.export(temp_file.name, format="wav")
        
        return temp_file.name
    
    def transcribe_file(self, audio_path, language='en-US'):
        """Transcribe audio file using speech_recognition library"""
        
        # Preprocess audio
        processed_path = self.preprocess_audio(audio_path)
        
        try:
            with sr.AudioFile(processed_path) as source:
                # Adjust for ambient noise
                self.recognizer.adjust_for_ambient_noise(source, duration=1)
                audio_data = self.recognizer.record(source)
            
            # Perform recognition
            if self.engine == "google":
                text = self.recognizer.recognize_google(audio_data, language=language)
            elif self.engine == "sphinx":
                text = self.recognizer.recognize_sphinx(audio_data)
            
            return {
                'text': text,
                'success': True,
                'engine': self.engine
            }
            
        except sr.UnknownValueError:
            return {
                'text': "",
                'success': False,
                'error': "Could not understand audio"
            }
        except sr.RequestError as e:
            return {
                'text': "",
                'success': False,
                'error': f"Recognition service error: {str(e)}"
            }
        finally:
            # Clean up temporary file
            os.unlink(processed_path)

# Usage
transcriber = FlexibleTranscriber(engine="google")
result = transcriber.transcribe_file("audio.wav")

if result['success']:
    print(result['text'])
else:
    print(f"Transcription failed: {result['error']}")&lt;/code&gt;
    &lt;head rend="h4"&gt;üîÑ Engine Comparison&lt;/head&gt;
    &lt;head rend="h2"&gt;Common Issues and Solutions&lt;/head&gt;
    &lt;head rend="h3"&gt;Issue 1: FFmpeg Not Found&lt;/head&gt;
    &lt;p&gt;Error: &lt;code&gt;[WinError 2] The system cannot find the file specified&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;Solution:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify FFmpeg installation: &lt;code&gt;ffmpeg -version&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Windows: Ensure FFmpeg is in your PATH environment variable&lt;/item&gt;
      &lt;item&gt;Restart your terminal/command prompt after PATH changes&lt;/item&gt;
      &lt;item&gt;Try reinstalling FFmpeg if the problem persists&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Issue 2: Out of Memory Errors&lt;/head&gt;
    &lt;p&gt;Error: CUDA out of memory or system RAM exhausted&lt;/p&gt;
    &lt;head rend="h4"&gt;‚ö° Memory Management Tips&lt;/head&gt;
    &lt;p&gt;Solutions:&lt;/p&gt;
    &lt;code&gt;# Use smaller model
transcriber = AudioTranscriber(model_size="tiny")

# For very long audio files, process in chunks
def transcribe_long_audio(audio_path, chunk_duration=300):  # 5 minutes
    audio = AudioSegment.from_file(audio_path)
    chunks = [audio[i:i+chunk_duration*1000] for i in range(0, len(audio), chunk_duration*1000)]
    
    transcriptions = []
    for i, chunk in enumerate(chunks):
        chunk_path = f"temp_chunk_{i}.wav"
        chunk.export(chunk_path, format="wav")
        
        result = transcriber.transcribe_file(chunk_path)
        transcriptions.append(result['text'])
        
        os.remove(chunk_path)
    
    return ' '.join(transcriptions)&lt;/code&gt;
    &lt;head rend="h3"&gt;Issue 3: Poor Accuracy on Noisy Audio&lt;/head&gt;
    &lt;p&gt;Problem: Low accuracy on recordings with background noise or poor quality&lt;/p&gt;
    &lt;head rend="h4"&gt;üé§ Audio Quality Tips&lt;/head&gt;
    &lt;p&gt;Solutions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Audio preprocessing:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;def enhance_audio(audio_path):
    """Basic audio enhancement"""
    audio = AudioSegment.from_file(audio_path)
    
    # Normalize volume
    audio = audio.normalize()
    
    # Apply high-pass filter to reduce low-frequency noise
    audio = audio.high_pass_filter(80)
    
    # Compress dynamic range
    audio = audio.compress_dynamic_range()
    
    return audio&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Specify language for better accuracy:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;result = transcriber.transcribe_file("audio.wav", language="en")&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use higher-quality model:&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Upgrade from 'base' to 'small' for better accuracy
transcriber = AudioTranscriber(model_size="small")&lt;/code&gt;
    &lt;head rend="h2"&gt;Performance Benchmarks&lt;/head&gt;
    &lt;p&gt;Based on testing with various audio types on a modern laptop:&lt;/p&gt;
    &lt;p&gt;Whisper Model Performance (1-hour audio file):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tiny: 1.9 minutes processing, 89% accuracy&lt;/item&gt;
      &lt;item&gt;base: 3.8 minutes processing, 94% accuracy&lt;/item&gt;
      &lt;item&gt;small: 10 minutes processing, 96% accuracy&lt;/item&gt;
      &lt;item&gt;medium: 30 minutes processing, 97% accuracy&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hardware Impact:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU only: Use base model maximum for reasonable speeds&lt;/item&gt;
      &lt;item&gt;8GB RAM: Comfortable with small model&lt;/item&gt;
      &lt;item&gt;16GB+ RAM: Can handle medium/large models without issues&lt;/item&gt;
      &lt;item&gt;GPU acceleration: 3-5x speed improvement (requires CUDA setup)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;üöÄ Optimization Strategy&lt;/head&gt;
    &lt;head rend="h2"&gt;Command-Line Usage&lt;/head&gt;
    &lt;p&gt;Create a simple command-line script for easy usage:&lt;/p&gt;
    &lt;code&gt;# transcribe.py
import sys
import argparse
from pathlib import Path

def main():
    parser = argparse.ArgumentParser(description='Transcribe audio files locally')
    parser.add_argument('audio_file', help='Path to audio file')
    parser.add_argument('--model', default='base', choices=['tiny', 'base', 'small', 'medium', 'large'])
    parser.add_argument('--language', help='Language code (e.g., en, es, fr)')
    parser.add_argument('--output', help='Output file path')
    
    args = parser.parse_args()
    
    # Transcribe
    result = transcribe_audio_file(
        args.audio_file, 
        model_size=args.model,
        language=args.language
    )
    
    # Save to custom output path if specified
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(result['text'])
        print(f"Transcription saved to: {args.output}")

if __name__ == "__main__":
    main()&lt;/code&gt;
    &lt;p&gt;Usage examples:&lt;/p&gt;
    &lt;code&gt;# Basic transcription
python transcribe.py interview.wav

# Specify model and language
python transcribe.py lecture.mp3 --model small --language en

# Custom output file
python transcribe.py podcast.m4a --output transcript.txt&lt;/code&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Local audio transcription with Python and Whisper offers a compelling alternative to commercial services. With a one-time setup, you get unlimited transcription capabilities, complete privacy, and often superior accuracy compared to cloud-based solutions.&lt;/p&gt;
    &lt;p&gt;Key advantages:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero ongoing costs after initial setup‚Äîno per-minute charges&lt;/item&gt;
      &lt;item&gt;Complete privacy‚Äîaudio never leaves your machine&lt;/item&gt;
      &lt;item&gt;High accuracy‚Äî94-98% depending on model choice and audio quality&lt;/item&gt;
      &lt;item&gt;Fast processing‚Äîtypically 4-16x real-time speed&lt;/item&gt;
      &lt;item&gt;Offline capability‚Äîworks without internet connection&lt;/item&gt;
      &lt;item&gt;No usage limits‚Äîtranscribe as much as you want&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Whether you‚Äôre a researcher transcribing interviews, a journalist working with sensitive sources, or a content creator processing podcasts, this local solution gives you the control and privacy that cloud services can‚Äôt match.&lt;/p&gt;
    &lt;p&gt;The setup might take 30 minutes, but you‚Äôll save hours of time and potentially hundreds of dollars in transcription costs. Plus, you‚Äôll have the peace of mind that comes with keeping your audio data completely under your control.&lt;/p&gt;
    &lt;head rend="h3"&gt;Stay up to date&lt;/head&gt;
    &lt;p&gt;Get notified when I publish something new, and unsubscribe at any time.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45337400</guid><pubDate>Mon, 22 Sep 2025 18:18:56 +0000</pubDate></item><item><title>Fine-grained HTTP filtering for Claude Code</title><link>https://ammar.io/blog/httpjail</link><description>&lt;doc fingerprint="fcf5f913cb30106f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Fine-grained HTTP filtering for Claude Code&lt;/head&gt;
    &lt;p&gt;Coding agents are becoming more powerful every day without commensurate security and governance tooling. The result is a world where solo developers happily run &lt;code&gt;claude --dangerously-skip-permissions&lt;/code&gt; for hours unmoderated while many of the world's most important organizations have barely tried agentic developmentLearned from our experience at Coder . I've been working on a tool called &lt;code&gt;httpjail&lt;/code&gt; in an effort to make agents available everywhere.&lt;/p&gt;
    &lt;p&gt;The tool is focused on mitigating these classes of risks:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Risk&lt;/cell&gt;
        &lt;cell role="head"&gt;Example&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Agents performing destructive actions&lt;/cell&gt;
        &lt;cell&gt;Deleting your database&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Agents leaking sensitive information&lt;/cell&gt;
        &lt;cell&gt;Exposing API keys or credentials&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Agents operating with more authority than desired&lt;/cell&gt;
        &lt;cell&gt;Pushing straight to main instead of opening a PR&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Agents may transgress accidentally (user misinterpretation) or intentionally (prompt injection).&lt;/p&gt;
    &lt;p&gt;There is a class of risks at the file-system interface too, but, I believe existing tooling (containers) is sufficient here. Existing network isolation tools rely on IP-based rules. In our case, they're imprecisecentralized, anycast load balancers power much of the internet and require constant maintenanceIPs change randomly and they're not a part of a service's implicit "contract".&lt;/p&gt;
    &lt;head rend="h2"&gt;httpjail&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;httpjail&lt;/code&gt; implements an HTTP(S) interceptor alongside process-level network isolation. Under default configuration, all DNS (udp:53) is permitted and
all other non-HTTP(S) traffic is blocked.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;httpjail&lt;/code&gt; rules are either JavaScript expressions or custom programs. This approach makes
them far more flexible than traditional rule-oriented firewalls and avoids the learning curve of a DSL.&lt;/p&gt;
    &lt;p&gt;Block all HTTP requests other than the LLM API traffic itself:&lt;/p&gt;
    &lt;code&gt;$ httpjail --js "r.host === 'api.anthropic.com'" -- claude "build something great"
&lt;/code&gt;
    &lt;p&gt;Allow only &lt;code&gt;GET&lt;/code&gt; requests i.e. make the internet read-only:&lt;/p&gt;
    &lt;code&gt;$ httpjail --js "r.method === 'GET'" -- claude "build something great"
&lt;/code&gt;
    &lt;p&gt;Only allow hosts in a &lt;code&gt;whitelist.txt&lt;/code&gt; file:&lt;/p&gt;
    &lt;code&gt;$ httpjail --sh "grep -qx \"$HTTPJAIL_HOST\" whitelist.txt" -- claude "research these APIs"
&lt;/code&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;In a nutshell:&lt;/p&gt;
    &lt;p&gt;Strong| NS[Create namespace&lt;/p&gt;
    &lt;p&gt;+ nftables redirect&lt;/p&gt;
    &lt;p&gt;+ setuid $SUDO_USER] Start --&amp;gt;|macOS/--weak&lt;/p&gt;
    &lt;p&gt;Weak| Env[Set $HTTP_PROXY&lt;/p&gt;
    &lt;p&gt;env vars] end subgraph "2. Target Process" direction TB Target[Target Process&lt;/p&gt;
    &lt;p&gt;e.g., claude] Target --&amp;gt; Request[HTTP/HTTPS&lt;/p&gt;
    &lt;p&gt;Request] Request --&amp;gt; Route{Route&lt;/p&gt;
    &lt;p&gt;Request} end subgraph "3. Interception" direction TB Proxy[Proxy :8080/:8443] Proxy --&amp;gt; Rules{Evaluate&lt;/p&gt;
    &lt;p&gt;JS/Script Rules} end subgraph "4. Result" direction TB Internet[‚úì Internet Access] Blocked[‚úó 403 Blocked] Bypass[‚ö†Ô∏è BYPASSED!&lt;/p&gt;
    &lt;p&gt;weak mode only] end NS --&amp;gt; Target Env --&amp;gt; Target Route --&amp;gt;|Strong: forced&lt;/p&gt;
    &lt;p&gt;via nftables| Proxy Route --&amp;gt;|Weak: respects&lt;/p&gt;
    &lt;p&gt;$HTTP_PROXY| Proxy Route --&amp;gt;|Weak: ignores&lt;/p&gt;
    &lt;p&gt;$HTTP_PROXY| Bypass Rules --&amp;gt;|Allow| Internet Rules --&amp;gt;|Deny| Blocked style NS fill:#00d4ff,color:#0a0a0a style Env fill:#00d4ff,color:#0a0a0a style Proxy fill:#404040 style Blocked fill:#8b2635 style Internet fill:#2a7f62 style Bypass fill:#8b2635&lt;/p&gt;
    &lt;head rend="h3"&gt;macOS (Weak Mode)&lt;/head&gt;
    &lt;p&gt;macOS uses &lt;code&gt;--weak/-w&lt;/code&gt; mode by default (see #7).&lt;/p&gt;
    &lt;p&gt;In weak mode, we rely on process cooperation via the standard &lt;code&gt;HTTP_PROXY&lt;/code&gt;/&lt;code&gt;HTTPS_PROXY&lt;/code&gt; environment variables. This mode is less of a jail and more of a suggestion that the majority of
well-meaning applications happen to comply with.&lt;/p&gt;
    &lt;head rend="h3"&gt;TLS Interception&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;httpjail&lt;/code&gt; implements full TLS interception to inspect and filter HTTPS traffic. Without interception, rules would only have access to the hostname via SNI, and most of the power of this tool would be lost.&lt;/p&gt;
    &lt;head rend="h4"&gt;How TLS Interception Works&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Certificate Authority Generation: On first run,&lt;/p&gt;&lt;code&gt;httpjail&lt;/code&gt;generates a self-signed Certificate Authority (CA) that's stored in&lt;code&gt;~/.config/httpjail/&lt;/code&gt;. This CA is used to sign certificates for intercepted connections.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Dynamic Certificate Generation: When a client connects,&lt;/p&gt;&lt;code&gt;httpjail&lt;/code&gt;:&lt;list rend="ul"&gt;&lt;item&gt;Extracts the Server Name Indication (SNI) from the TLS ClientHello&lt;/item&gt;&lt;item&gt;Generates a certificate on-the-fly for that specific hostname &lt;list rend="ul"&gt;&lt;item&gt;Uses a shared ECDSA P-256 key pair for all server certificates for O(1) keygen overhead&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;&lt;item&gt;Signs it with the CA certificate&lt;/item&gt;&lt;item&gt;Caches certificate in memory for performance&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Dual Mode Operation:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Transparent Proxy Mode: Directly accepts TLS connections by detecting the TLS ClientHello packet (starts with &lt;code&gt;0x16&lt;/code&gt;)&lt;/item&gt;
          &lt;item&gt;Explicit Proxy Mode: Handles HTTP CONNECT tunnels, responds with "200 Connection Established", then transitions to Transparent Proxy Mode&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Transparent Proxy Mode: Directly accepts TLS connections by detecting the TLS ClientHello packet (starts with &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Trust Injection: The CA certificate must be trusted by client applications.&lt;/p&gt;&lt;code&gt;httpjail&lt;/code&gt;automatically sets environment variables for common tools:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;SSL_CERT_FILE&lt;/code&gt;/&lt;code&gt;SSL_CERT_DIR&lt;/code&gt;for OpenSSL-based tools&lt;/item&gt;&lt;item&gt;&lt;code&gt;CURL_CA_BUNDLE&lt;/code&gt;for curl&lt;/item&gt;&lt;item&gt;&lt;code&gt;NODE_EXTRA_CA_CERTS&lt;/code&gt;for Node.js&lt;/item&gt;&lt;item&gt;&lt;code&gt;REQUESTS_CA_BUNDLE&lt;/code&gt;for Python requests&lt;/item&gt;&lt;item&gt;&lt;code&gt;GIT_SSL_CAINFO&lt;/code&gt;for Git&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Jail Escapes&lt;/head&gt;
    &lt;p&gt;In the weak jail, it's trivial for the agent to escape the jail by funnelling requests through a shim program that disregards the &lt;code&gt;HTTP_PROXY&lt;/code&gt; environment variable.&lt;/p&gt;
    &lt;p&gt;Even the strong jail is not perfect. There are potential escape hatches in the filesystem. For example, the agent could create a container via a Docker socket which would spawn outside the network namespace.&lt;/p&gt;
    &lt;p&gt;To combine filesystem and network isolation into one, &lt;code&gt;httpjail&lt;/code&gt; has a &lt;code&gt;--docker-run&lt;/code&gt; flag that works like this:&lt;/p&gt;
    &lt;code&gt;httpjail --js "r.host === 'api.github.com'" --docker-run -- \
    --rm alpine:latest wget -qO- https://api.github.com
&lt;/code&gt;
    &lt;p&gt;I believe there's still much value in this approach even with imperfect isolation. In my experience, models seldom try to escape restrictions intentionally placed by the user. And, if the jail is rendered ineffective by prompt injection, it wasn't doing its job in the first place.&lt;/p&gt;
    &lt;head rend="h2"&gt;Server Mode&lt;/head&gt;
    &lt;p&gt;For the strongest level of isolation, you can:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Run &lt;code&gt;httpjail --server&lt;/code&gt;on a standalone server.&lt;/item&gt;
      &lt;item&gt;Configure the network firewall to only permit 80/443 traffic to the proxy server. &lt;list rend="ul"&gt;&lt;item&gt;Optionally, you may redirect all traffic to the proxy server, otherwise you will need to take care in ensuring HTTP_PROXY is set in your environments and all of your web-faring applications respect it.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Run processes as usual in the development environment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Request] Check{Respects&lt;/p&gt;
    &lt;p&gt;$HTTP_PROXY?} end subgraph "Network Layer" FW[Network Firewall] Config{Config&lt;/p&gt;
    &lt;p&gt;Mode?} FW --&amp;gt; Config Config --&amp;gt;|Redirect All&lt;/p&gt;
    &lt;p&gt;80/443 Traffic| Force[Forced to Proxy] Config --&amp;gt;|Allow Only&lt;/p&gt;
    &lt;p&gt;Proxy IP| Check Check --&amp;gt;|Yes| Voluntary[To Proxy] Check --&amp;gt;|No| Drop[‚úó Dropped] end subgraph "httpjail --server" Decision{Evaluate&lt;/p&gt;
    &lt;p&gt;Rules} end subgraph Result direction TB API[‚úì Allowed APIs] Blocked[‚úó Blocked] end Request --&amp;gt; FW Force --&amp;gt; Decision Voluntary --&amp;gt; Decision Decision --&amp;gt;|Allow| API Decision --&amp;gt;|Deny| Blocked style Request fill:#404040 style Decision fill:#00d4ff,color:#0a0a0a style FW fill:#404040 style Blocked fill:#8b2635 style Drop fill:#8b2635 style API fill:#2a7f62 style Force fill:#2a7f62 style Voluntary fill:#4a7c59&lt;/p&gt;
    &lt;head rend="h2"&gt;Try it out&lt;/head&gt;
    &lt;code&gt;cargo install httpjail
&lt;/code&gt;
    &lt;p&gt;And, check out the GitHub repository for more details.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45338561</guid><pubDate>Mon, 22 Sep 2025 19:49:02 +0000</pubDate></item><item><title>Germicidal UV could make airborne diseases as rare as those carried by water</title><link>https://www.worksinprogress.news/p/how-to-clean-the-air</link><description>&lt;doc fingerprint="fe9810162a2317cb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Seeing the light&lt;/head&gt;
    &lt;head rend="h3"&gt;Germicidal ultraviolet could make airborne diseases as rare as those carried by water.&lt;/head&gt;
    &lt;p&gt;Works in Progress is becoming a print magazine. Our first print issue, Issue 21, will land in November. If you live in the United States and the United Kingdom, you can subscribe here. If you live outside the US or UK and want to be notified as soon as subscriptions are live in your country, leave your details here.&lt;/p&gt;
    &lt;p&gt;Between the 1860s and 1920, successive outbreaks of typhoid fever killed over 300,000 Americans. As population growth surged and people moved to urban areas en masse, American cities began to dump sewage in the same rivers that provided their drinking water. After epidemiologists linked typhoid outbreaks to water cleanliness, cities began building large-scale sand filtration systems in the 1890s, and in 1908, Jersey City pioneered the first continuous chlorination of a public water supply. By the 1920s, typhoid deaths had fallen by two-thirds, and waterborne diseases were in retreat across the country.&lt;/p&gt;
    &lt;p&gt;While typhoid and other waterborne diseases triggered vast engineering and regulatory responses, the equivalent airborne threats have not. Tuberculosis alone kills more than a million people every year around the world, yet the air in schools, clinics, and public buildings remains largely unfiltered and unmonitored. Covid-19, which killed over seven million people, demonstrated how rapidly airborne pathogens can spread in poorly ventilated spaces.&lt;/p&gt;
    &lt;p&gt;Just as filtration and chlorination made drinking water safe at scale, we now have the tools to do the same for indoor air: ventilation, high-quality filters, and germicidal light. A century ago, germicidal light at 254 nanometers seemed to be a promising way of controlling pathogens by killing them in the air, but it turned out to cause irritation and cancer in the skin, and it was largely dropped when antibiotics became widespread.&lt;/p&gt;
    &lt;p&gt;But today there is an update that has none of these drawbacks. We now know that wavelengths under 230 nanometers, especially 222 nanometer light, are harmless to humans, but can still disable microscopic pathogens. We know how to filter out all wavelengths except the ones we want, and how to direct them away from humans, cycling the air through them to clean it without exposing people to it, just in case it carries unknown risks. This far-UVC light, as it is called, may be how we can make the air we breathe as safe as the water we drink.&lt;/p&gt;
    &lt;head rend="h3"&gt;The sterilamp&lt;/head&gt;
    &lt;p&gt;Until the mid-nineteenth century, most physicians believed that disease spread through miasmas, poisonous vapors rising from filth and decay. Contemporaries were obsessed with air: moving to the countryside for the air, taking the air at the seaside. Bad air was blamed for sickness. Herbs were burned to purify the air to fight the plague. But because they didn‚Äôt understand what made the air dirty, they were not very good at cleaning it. This began to change only when Louis Pasteur and Robert Koch provided the first definitive proof that microscopic organisms were responsible for infectious disease.&lt;/p&gt;
    &lt;p&gt;Cleaning air relies on the same fundamental techniques as cleaning water: replacement, filtration, and disinfection. Pathogens replicate inside people, who then expel them into the air by breathing, talking, or coughing, where they can remain suspended and infect new individuals. The relative contributions of aerosol transmission, droplet transmission, and fomite (surface) transmission vary between diseases. Certain diseases, such as Covid-19, are driven by a small number of highly infectious individuals (‚Äòsuperspreaders‚Äô) that account for a disproportionate number of cases.&lt;/p&gt;
    &lt;p&gt;In 1877, British researchers Arthur Downes and Thomas Blunt stumbled upon a discovery that would lay the groundwork for modern disinfection. In a paper submitted to the Royal Society of London, they described how over the course of six months they had used sunlight to prevent bacteria from growing in a tube.&lt;/p&gt;
    &lt;p&gt;Follow-up research by Robert Koch demonstrated that sunlight could kill Mycobacterium tuberculosis, but the early experiments lacked precision. Scientists knew light worked, but not which parts of the spectrum were responsible. The turning point came in 1930, when Frederick L Gates published the first quantitative analysis of how ultraviolet light affected bacteria, pinpointing peak germicidal effectiveness at 265 nanometers, the same point that nucleic acids ‚Äì DNA and RNA ‚Äì absorb light.&lt;/p&gt;
    &lt;p&gt;Understanding the spectrum wasn‚Äôt necessary for harnessing it. In 1901, American electrical engineer Peter Cooper Hewitt patented the first mercury-vapor lamp to achieve widespread commercial success. Hewitt created a voltage difference across electrodes in a glass tube, with mercury vapor causing electrons to separate from mercury atoms and collide with other atoms, exciting them to higher energy states. When these excited atoms return to their ground state, they release energy as photons, primarily at a wavelength of 254 nanometers.&lt;/p&gt;
    &lt;p&gt;Nucleic acids absorb this 254-nanometer light and produce new bonds between bases, preventing DNA/RNA replication. For human skin and eyes, this means sunburn-like irritation. But for viruses, amoeba, and bacteria in the air, as engineers at an American manufacturing company put it, these rays spell doom.&lt;/p&gt;
    &lt;p&gt;An array of different factors influences how sensitive pathogens are to ultraviolet-C (UVC), ultraviolet light with a wavelength of 100‚Äì280 nanometers. Viruses with larger genomes ‚Äì such as herpesviruses (which have about 150,000 base pairs) or coronaviruses (30,000 base pairs) ‚Äì offer many nucleotide bonds for photons to break, so they can be more quickly disabled than small-genome viruses such as parvoviruses (5,000 base pairs).&lt;/p&gt;
    &lt;p&gt;In 1936, Dr. Deryl Hart, a surgeon at Duke University Hospital, became the first person to use ultraviolet radiation to curb airborne infections in surgical operating rooms. High-intensity germicidal ultraviolet light fixtures, designed to irradiate the entire room, cut postoperative wound infection rates from 11.6 percent to just 0.2 percent, and not one patient out of 2,463 cases died from postoperative infections.&lt;/p&gt;
    &lt;p&gt;Shortly after the installation at Duke, UV from mercury vapor lamps was used to create invisible ‚Äòcurtains‚Äô between cubicles in hospital wards and cribs in nurseries in Philadelphia, Boston, Toronto, and Evanston. In Boston, this attempt to prevent respiratory pathogens from crossing cubicles and infecting infant patients led to infection rates that were a quarter of those in the other cubicles. In the other cases, cross-infections fell by between 40 percent and 96 percent.&lt;/p&gt;
    &lt;p&gt;In 1937, researchers installed upper-room germicidal UV lights in the Germantown Friends School in Philadelphia and studied its impact over the next few school years. During the fourth year of the study, Philadelphia saw its largest recorded epidemic of measles, the most transmissible known pathogen. In the irradiated classrooms, only 14.5 percent of susceptible children fell ill, while in the unprotected ones, infection rates soared to 55 percent.&lt;/p&gt;
    &lt;p&gt;At the height of World War II, US defence manufacturer Westinghouse wrote that it was ‚Äòfighting two wars at once‚Äô: one against the Germans and Japanese and the other against germs. Once a major producer of UV lamps and fixtures, the company had developed the Sterilamp in the 1930s. The Sterilamp system is described in a 1943 newsletter as a ‚ÄòDEATH RAY THAT GUARDS LIFE‚Äô.&lt;/p&gt;
    &lt;p&gt;But the deployment of germicidal UV light stalled after the war. In 1945‚Äì46, the New York Department of Health tried to replicate the Philadelphia findings in large public rural schools and failed. In one of the schools with an internal control group in Port Byron, 90.4 percent of susceptible students in UV-treated classrooms still contracted measles, compared to 83 percent in unirradiated classrooms.&lt;/p&gt;
    &lt;p&gt;Once they took bus ridership into account, the researchers found that UV had appeared to reduce the measles incidence rate by roughly 8.2 percentage points (from 77.4 percent to 69.2 percent) in non-bus riders. Germicidal UV could disrupt measles transmission in specific spaces such as classrooms, but it proved insufficient if face-to-face exposure continued in buses, hallways, and other shared spaces. Mercury lamps were also costly and could irritate the skin and eyes of people exposed to them. No long-term effects had been observed in humans at the doses used in schools, but higher doses had been shown to raise long-term cancer risk in mice. At the same time, the large-scale emergence of antibiotics was transforming the fight against infectious diseases. By 1945, mass production made penicillin widely available, ushering in the antibiotic era and shifting the medical focus toward drug-based infection control. Together with the failures of the UV studies, interest in germicidal UV receded.&lt;/p&gt;
    &lt;head rend="h3"&gt;Returning from obsolescence&lt;/head&gt;
    &lt;p&gt;Decades later, over 700,000 people a year die from antibiotic resistance as people use more and more antibiotics while their discovery rates stagnate. Airborne viral pandemics, which antibiotics cannot treat, have caused enormous economic damage and inflicted tens of millions of deaths. There is once again a case for the use of germicidal ultraviolet light.&lt;/p&gt;
    &lt;p&gt;Skin and eye irritation and cancer risks limited direct exposure to 254-nanometer germicidal UV to surgical cases. But new technologies ‚Äì circulation, light filtering, and lower wavelengths ‚Äì have together fixed these problems.&lt;/p&gt;
    &lt;p&gt;If we can circulate air, we can move it, clean it, and put it back. Today‚Äôs systems do this by projecting beams across the upper level of a room, above the heads of occupants. Natural circulation patterns move air to the ceiling, where it is cleaned, before it falls back down. But even reflected beams can cause irritation, meaning that each installation has to be calibrated carefully for the specifics of the room. Even repainting or shifting furniture can change the reflection of the beams, meaning that the UV beams would need to be recalibrated.&lt;/p&gt;
    &lt;p&gt;Shorter-wavelength light works just as effectively, without many of the downsides. Light with a wavelength of 230 nanometers or lower, known as ‚Äòfar-UVC‚Äô, disables pathogens without hurting humans, but mercury lamps cannot produce it.&lt;/p&gt;
    &lt;p&gt;Unlike longer wavelengths, far-UVC penetrates weakly, meaning it is only absorbed by the uppermost layers of the skin and eye, tissues which slough off frequently and do not divide. When shone directly into the eyes, germicidal doses of far-UVC do not appear to have any effect beyond temporary discomfort (the same as if a torch was shone into your eyes). When installed overhead, the lamp‚Äôs positioning and human facial structure mean only five percent of the light reaches the eye, creating an even larger safety margin.&lt;/p&gt;
    &lt;p&gt;Long-term exposure does not harm mice at all, even those with a severely limited ability to repair DNA defects. In humans, a 36-month-long trial installation of far-UVC in a hospital, with ongoing exposure to germicidal doses, caused no adverse effects on those working on the ward. There is no evidence from any other study that far-UVC raises cancer rates, though more work should be done to prove this beyond any doubt.&lt;/p&gt;
    &lt;p&gt;In order to prevent disease transmission, far-UVC lamps must disable pathogens in respiratory aerosols quickly enough that, by the time they are inhaled, not enough for an infectious dose remain.&lt;/p&gt;
    &lt;p&gt;A 2024 study presented some of the best data on far-UVC in an occupied room. Four lamps were installed in a lab mouse cage-cleaning room, where the constant activity and movement continuously aerosolized murine norovirus present in the bedding. When switched on, the lamps reduced the amount of virus in the air by 98 percent, equivalent to at least 36 air changes per hour. This happened despite murine norovirus being more resistant to far-UVC than many common human respiratory viruses, likely due to its tough protein outer ‚Äòshell‚Äô.&lt;/p&gt;
    &lt;head rend="h3"&gt;Alternatives&lt;/head&gt;
    &lt;p&gt;There are other ways to clean the air, like ventilation and filtration. There‚Äôs an important role for both of these approaches, but they come with critical limitations that only far-UVC can plausibly overcome.&lt;/p&gt;
    &lt;p&gt;Ventilation is the easiest method of removing things from indoor air, diluting pathogens and pollutants into the 5.5 quadrillion tonnes of atmosphere outside. But it is not always convenient. It can be awkward or noisy to install and run. In highly polluted areas, bringing in outdoor air reduces disease transmission at the expense of worse air quality. In 2024, only seven countries met World Health Organization air quality standards. In very hot or cold places, outdoor air must be heated or cooled, which can be energy-intensive and costly, especially in older or heritage buildings where air conditioning needs to be retrofitted.&lt;/p&gt;
    &lt;p&gt;Another technique is filtration. Mechanical filters work by forcing air through a dense, pleated mesh of fine fibers that trap hazardous particles. Originally developed as part of the Manhattan Project to prevent the spread of radioactive particles, mechanical filters have become standard in hospitals, cleanrooms, and aircraft cabins.&lt;/p&gt;
    &lt;p&gt;Mechanical filters are ‚Äòplug and play‚Äô: there are dozens of verified and recommended models that anyone can buy off the shelf, install in their house by plugging them in, and expect to deliver reduced pollutants, allergens, and cleaner air for between $80 and $300. Consumer far-UVC systems are often considerably more expensive ‚Äì a typical example costs $2,500 for a lamp covering 1,000 square feet, although a newer model has recently come to market at $600, and less expensive lamps with lower output can be added together to cover a similar floorspace.&lt;/p&gt;
    &lt;p&gt;The drawbacks of ventilation-based air filtration methods are similar to the drawbacks of ventilation itself. Moving air from place to place and forcing it through a fine filter requires energy, and fresh air from outdoors is often at the wrong temperature, requiring extra heating or cooling. Meanwhile, the constant air movement can also chill the air to the point of discomfort, requiring indoor heating. This has obvious impacts on the costs and environmental footprint of buildings.&lt;/p&gt;
    &lt;p&gt;Doubling the ventilation rate from the US standard minimum in offices could add up to $40 per person per year to building running costs. This is lower than typically predicted, but still more than double what many building officials were prepared (before the pandemic) to pay. Energy recovery units help equalize the temperatures between intake and exhaust air, and can lower these figures by about 60 percent (or even more), depending on local climate and system type. However, in Europe, the hurdle is even higher: most buildings have no mechanical ventilation at all, so adapting existing ventilation systems is out of the question. For example, in the UK, an engineering review found that only five percent of residential buildings were likely to have air conditioning (a proxy for mechanical ventilation) by 2019. By contrast, in the US, about 90 percent of residences have air conditioning.&lt;/p&gt;
    &lt;p&gt;Ventilation comes with another tradeoff: noise. The more air you filter, the harder your fan has to work, the faster it spins, and the louder it gets. Free-standing filtration units can be powerful, quiet, or affordable ‚Äì but rarely all three at once. Purpose-built ventilation can be made quieter, but quiet movement of a large volume of air requires wide, leak-free ducting, which can increase installation complexity. Real world studies have shown that the noise and discomfort created by even relatively modest free-standing units mean that they are frequently turned down or switched off.&lt;/p&gt;
    &lt;p&gt;These drawbacks mean that ventilation-based methods lack the power to effectively block transmission of highly infectious pathogens or prevent pandemics. Without UV light, public buildings will require air handling systems akin to those in hospital isolation rooms to comply with current infection control guidelines. While circumstances vary from place to place, ventilation-based air quality measures quickly become impractical beyond around five air changes per hour, while upper-room UV light can achieve the equivalent of 35 air changes per hour, and far-UVC has the potential to reach well over 100.&lt;/p&gt;
    &lt;p&gt;Despite this, both ventilation and filtration need to be kept in mind as new buildings replace the old. If nothing else, ventilation design is an important factor in determining the effectiveness of UV systems; how well air is mixed and how quickly it moves affects the degree to which pathogens are exposed to UV light. These interactions have a positive or negative effect on pathogen removal. We flush and filter water, but we also disinfect it to eliminate remaining microbes. Air should be treated no differently.&lt;/p&gt;
    &lt;p&gt;Far-UVC is like an aerial disinfectant or bleach, except that it is harmless to humans at practical germicidal doses, and thus should not provoke resistance to its uptake. It does not alter pathogens in a way that allows resistance to emerge, a serious problem for antibiotics. Instead, it thoroughly damages microbial genomes at random, destroying bacteria and viruses alike, whether they are drug-resistant, vaccine-evasive, or indeed newly emerged.&lt;/p&gt;
    &lt;p&gt;The most widely used commercial far-UVC source is a krypton-chloride excimer lamp. ‚ÄòExcimer‚Äô is a contraction of ‚Äòexcited dimer‚Äô, a short-lived, high-energy molecule formed when the krypton and chlorine temporarily bond in an excited state as an electric current is passed through the gas mixture.&lt;/p&gt;
    &lt;p&gt;Krypton chloride lamps emit mostly 222-nanometer light, produced by the excited dimer decaying to its ground state and releasing the excess energy as a photon. Light at this wavelength is safe for human eyes and skin at doses that efficiently kill germs, but about 10‚Äì20 percent of the output consists of longer wavelengths with much lower maximum exposures. To use the lamps in occupied spaces, a filter with multiple layers of quartz and hafnium oxide is used to reflect unwanted wavelengths while allowing 222-nanometer light through.&lt;/p&gt;
    &lt;p&gt;Despite its potential, far-UVC has yet to achieve widespread use.&lt;/p&gt;
    &lt;p&gt;The very best krypton chloride emitters on the market are reliable, long-lasting, powerful, use little energy, and come with effective optical filters. But the state of the art is not representative. The krypton-chloride lamp industry is plagued by low-quality products with short lifespans that may not even produce any far-UVC light, and could even emit dangerous longer wavelengths. There is no product standard certification for UV lamps.&lt;/p&gt;
    &lt;p&gt;Top-tier excimer lamps, from reputable manufacturers, are expensive. This is partly due to their inherent complexity but largely from sluggish demand. The key reason is that, notwithstanding their successes in the 1930s, a certification vacuum denies buyers an authoritative seal of efficacy.&lt;/p&gt;
    &lt;p&gt;In part, this is because some of the studies establishing far-UVC‚Äôs efficacy aren‚Äôt as definitive as they need to be to really move the dial. One study on the impact of germicidal UV installations on tuberculosis transmission in homeless shelters was unsuccessful when the national rate of tuberculosis declined, which meant that the study ended up being too small to detect an effect even in principle.&lt;/p&gt;
    &lt;p&gt;In another study, germicidal UV was installed in hospitals to assess the impact of germicidal UV on flu transmission, but during the course of that study, the hospitals stopped routine flu testing. In a classroom study, not all classrooms had enough electrical sockets to plug in the portable air cleaners required for the study, some teachers turned off the air filters because they were too loud, and some schools didn‚Äôt install the germicidal UV devices they were supposed to because there were no legal protections for them for doing so.&lt;/p&gt;
    &lt;p&gt;But there are many other factors as well. Measuring infection control is challenging and seldom undertaken, particularly in public spaces. Epidemiological data is expensive and difficult to gather, and there is currently no way to measure the amount of viable, infectious pathogens in the air in real time. Office attendance can be tracked, but controlling for how users mix outside the office space is immensely difficult, and measuring the real-world effect of small-scale deployments in public areas is almost impossible. Studies aiming to cause deliberate disease transmission in controlled environments have failed to work in practice because they have been too small to generate enough infections.&lt;/p&gt;
    &lt;p&gt;Pathogen-free air and the research that goes into getting it are both, to some extent, public goods: the beneficiaries will mostly be people who haven‚Äôt paid for them. Even if a business reduces pathogens in its offices‚Äô air, the biggest upside goes to other people who share spaces with its employees. This could be commuters on the same train, people in shops, or parents whose children attend the same school.&lt;/p&gt;
    &lt;p&gt;Despite the lack of clear economic upside, we are already seeing some early adoption among respected institutions: Mount Sinai Hospital, for example, has far-UVC lamps installed in its Cohen Center for Recovery from Complex Chronic Illnesses. Ideally, others would emulate this example, creating a stronger basis for research.&lt;/p&gt;
    &lt;p&gt;If one part of the knot were cut, then one of the most promising disease-fighting technologies of our time could finally be employed en masse.&lt;/p&gt;
    &lt;p&gt;In the early 1900s, a public health official in Jersey City, John Leal, lost his father to illness likely caused by contaminated drinking water.&lt;/p&gt;
    &lt;p&gt;But Leal had an opportunity to prevent such a loss for others: he quietly directed the addition of chlorine to the drinking water supply in Jersey City, hiring engineer George Fuller to design and build a system for dripping a diluted bleach solution into Boonton Reservoir. He believed that this common household bleach agent could kill pathogens without harming people. He was right.&lt;/p&gt;
    &lt;p&gt;Beyond being a triumph of science, water sanitation led to a fundamental shift in public expectations. Once people saw that clean water was possible, they demanded it.&lt;/p&gt;
    &lt;p&gt;Twentieth-century water treatment programs transformed public health by virtually eliminating waterborne diseases. Ventilation, filtration and disinfection provide us with the opportunity to dramatically reduce the burden of airborne illnesses. Tuberculosis and coronaviruses would join typhoid and cholera as tragedies of the past, and seasonal flu and common colds would become rare rather than routine if clean air were as universal and expected as clean water.&lt;/p&gt;
    &lt;p&gt;Gavriel Kleinwaks is program director for indoor air quality at 1Day Sooner.&lt;/p&gt;
    &lt;p&gt;Karam Elabd is a researcher and writer.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45339923</guid><pubDate>Mon, 22 Sep 2025 21:44:54 +0000</pubDate></item><item><title>Paper2Agent: Stanford Reimagining Research Papers as Interactive AI Agents</title><link>https://arxiv.org/abs/2509.06917</link><description>&lt;doc fingerprint="f5bd3409d9b3a08a"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Artificial Intelligence&lt;/head&gt;&lt;p&gt; [Submitted on 8 Sep 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:We introduce Paper2Agent, an automated framework that converts research papers into AI agents. Paper2Agent transforms research output from passive artifacts into active systems that can accelerate downstream use, adoption, and discovery. Conventional research papers require readers to invest substantial effort to understand and adapt a paper's code, data, and methods to their own work, creating barriers to dissemination and reuse. Paper2Agent addresses this challenge by automatically converting a paper into an AI agent that acts as a knowledgeable research assistant. It systematically analyzes the paper and the associated codebase using multiple agents to construct a Model Context Protocol (MCP) server, then iteratively generates and runs tests to refine and robustify the resulting MCP. These paper MCPs can then be flexibly connected to a chat agent (e.g. Claude Code) to carry out complex scientific queries through natural language while invoking tools and workflows from the original paper. We demonstrate Paper2Agent's effectiveness in creating reliable and capable paper agents through in-depth case studies. Paper2Agent created an agent that leverages AlphaGenome to interpret genomic variants and agents based on ScanPy and TISSUE to carry out single-cell and spatial transcriptomics analyses. We validate that these paper agents can reproduce the original paper's results and can correctly carry out novel user queries. By turning static papers into dynamic, interactive AI agents, Paper2Agent introduces a new paradigm for knowledge dissemination and a foundation for the collaborative ecosystem of AI co-scientists.&lt;/quote&gt;&lt;p&gt; Current browse context: &lt;/p&gt;&lt;p&gt;cs.AI&lt;/p&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45340133</guid><pubDate>Mon, 22 Sep 2025 22:02:01 +0000</pubDate></item><item><title>Fall Foliage Map 2025</title><link>https://www.explorefall.com/fall-foliage-map</link><description>&lt;doc fingerprint="14d19396b861a2a5"&gt;
  &lt;main&gt;
    &lt;p&gt;Little to No Color&lt;/p&gt;
    &lt;p&gt;Low Color&lt;/p&gt;
    &lt;p&gt;Moderate Color&lt;/p&gt;
    &lt;p&gt;High Color&lt;/p&gt;
    &lt;p&gt;Peak Color&lt;/p&gt;
    &lt;p&gt;Past Peak Color&lt;/p&gt;
    &lt;p&gt;Color Report&lt;/p&gt;
    &lt;p&gt;Peak Timing&lt;/p&gt;
    &lt;p&gt;Reports&lt;/p&gt;
    &lt;p&gt;Download&lt;/p&gt;
    &lt;p&gt;Coming Fall 2025&lt;/p&gt;
    &lt;p&gt;Powered by Esri&lt;/p&gt;
    &lt;p&gt;Fall is nearly upon us, and we're once again helping fall foliage enthusiasts across the country find the best fall color! Use our map to explore the estimated timing of fall foliage throughout the United States, allowing you to plan trips with confidence this year. Check back regularly for updates based on the latest reports gathered from hundreds of sources throughout the country.&lt;/p&gt;
    &lt;p&gt;Two primary factors control the timing of fall foliage: daylight and temperature. This means that the further north and the higher in elevation a tree is, the earlier it will reveal it's colorful canopy. Photosynthesis grinds to a halt when the days grow short in the fall, and leaves no longer have a need for their excess stores of chlorophyll.&lt;/p&gt;
    &lt;p&gt;Over the course of a month or two, the concentrations of chlorophyll diminish, allowing less concentrated chemicals such as anthocyanin and carotenoids to dominate, turning the leaf red, yellow, or orange. The rate at which this change occurs varies amongst tree species, so it can be difficult to pinpoint a single peak in fall foliage.&lt;/p&gt;
    &lt;p&gt;Nevertheless, when the vast majority of trees in a particular area have full canopies of autumnal color, peak has arrived. Some areas, partiularly in the Northeast, experience vibrant red peaks due to an abundance of maple trees, while others experience a mixture of all of fall's colors. Different trees display different colors, giving each region its own unique peak.&lt;/p&gt;
    &lt;p&gt;For most of the United States, peak fall color arrives in the month of October. This is when wide swaths of the Northeast, Midwest, and Western states are aglow with bright fall foliage, and more than 80% of travelers make their fall foliage trips. Some less-populated regions will peak in September (August in northern Alaska), while the southernmost states hold off until mid-November.&lt;/p&gt;
    &lt;p&gt;The most popular fall foliage displays are found in New England, where approximately ten million people travel each year in hopes of photographing or simply walking through fall's splendor. Northern Vermont, New Hampshire, and northwestern Maine experience peak in early October, while much of New York, Massachusetts, and Pennsylvania have to wait until later in the month.&lt;/p&gt;
    &lt;p&gt;Out west, golden Aspens peak in sweeping displays in late September and early October, just prior to the invasive chill of winter. Non-desert, lower elevations in the Northwest are further delayed into late October/early November; however, the wait is well worth it.&lt;/p&gt;
    &lt;p&gt;If you've ever traveled in search of fall foliage before, you likely know how tricky it can be to be in the right place at the right time. The timing of peak color varies signficantly season-to-season, meaning what worked one year might not work the next! The best fall trips take careful planning, a lot of patience, and a reliable fall foliage map.&lt;/p&gt;
    &lt;p&gt;It's helpful to establish a baseline for when leaves normally change. Maps, like the one in the above section, can help you identify roughly when in the season you should be planning your trip. From there, you should consult a real-time fall foliage map like ours to see if fall foliage is on-time or running early/late due to ongoing weather conditions.&lt;/p&gt;
    &lt;p&gt;If at all possible, don't solidify your plans until you're two weeks out from peak fall foliage. This allows you to be flexible should extreme weather rear its head and disrupt the normal progression of fall foliage. Should that not be an option for you, do your planning in early September when fall foliage experts can give you an idea of whether or not fall color is on-time this year.&lt;/p&gt;
    &lt;p&gt;You'll want to make the most of your time in fall's splendor, so be sure to pick out a few beautiful hikes or drives on which you can truly be emersed in autumnal glory. If you're looking to beat the crowds, consider going to popular locations very early in the morning, before the majority of people arrive. Sunrise bathes fall foliage in golden hues, making early morning one of the best times to venture out!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45341324</guid><pubDate>Tue, 23 Sep 2025 00:14:37 +0000</pubDate></item><item><title>X server implementation for SIXEL-featured terminals (2010-2014)</title><link>https://github.com/saitoha/xserver-SIXEL</link><description>&lt;doc fingerprint="10de925be4457d82"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Notifications &lt;tool-tip&gt;You must be signed in to change notification settings&lt;/tool-tip&gt;&lt;/item&gt;
      &lt;item&gt;Fork 5&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A X server implementation for SIXEL-featured terminals, based on @pelya's Xsdl kdrive server(https://github.com/pelya/xserver-xsdl)&lt;/p&gt;
    &lt;head rend="h3"&gt;License&lt;/head&gt;
    &lt;head rend="h1"&gt;saitoha/xserver-SIXEL&lt;/head&gt;
    &lt;head rend="h2"&gt;Folders and files&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;Name&lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Last commit message&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell role="head"&gt;
          &lt;p&gt;Last commit date&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Repository files navigation&lt;/head&gt;
    &lt;quote&gt;X Server The X server accepts requests from client applications to create windows, which are (normally rectangular) "virtual screens" that the client program can draw into. Windows are then composed on the actual screen by the X server (or by a separate composite manager) as directed by the window manager, which usually communicates with the user via graphical controls such as buttons and draggable titlebars and borders. For a comprehensive overview of X Server and X Window System, consult the following article: http://en.wikipedia.org/wiki/X_server All questions regarding this software should be directed at the Xorg mailing list: http://lists.freedesktop.org/mailman/listinfo/xorg Please submit bug reports to the Xorg bugzilla: https://bugs.freedesktop.org/enter_bug.cgi?product=xorg The master development code repository can be found at: git://anongit.freedesktop.org/git/xorg/xserver http://cgit.freedesktop.org/xorg/xserver For patch submission instructions, see: http://www.x.org/wiki/Development/Documentation/SubmittingPatches For more information on the git code manager, see: http://wiki.x.org/wiki/GitPage&lt;/quote&gt;
    &lt;head rend="h2"&gt;About&lt;/head&gt;
    &lt;p&gt;A X server implementation for SIXEL-featured terminals, based on @pelya's Xsdl kdrive server(https://github.com/pelya/xserver-xsdl)&lt;/p&gt;
    &lt;head rend="h3"&gt;Resources&lt;/head&gt;
    &lt;head rend="h3"&gt;License&lt;/head&gt;
    &lt;head rend="h3"&gt;Stars&lt;/head&gt;
    &lt;head rend="h3"&gt;Watchers&lt;/head&gt;
    &lt;head rend="h3"&gt;Forks&lt;/head&gt;
    &lt;head rend="h2"&gt;Releases&lt;/head&gt;
    &lt;p&gt;No releases published&lt;/p&gt;
    &lt;head rend="h2"&gt;Packages 0&lt;/head&gt;
    &lt;p&gt; No packages published &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45341683</guid><pubDate>Tue, 23 Sep 2025 01:07:57 +0000</pubDate></item><item><title>Nine Things I Learned in Ninety Years</title><link>http://edwardpackard.com/wp-content/uploads/2025/09/Nine-Things-I-Learned-in-Ninety-Years.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45342364</guid><pubDate>Tue, 23 Sep 2025 03:03:06 +0000</pubDate></item><item><title>Gamebooks and graph theory (2019)</title><link>https://notes.atomutek.org/gamebooks-and-graph-theory.html</link><description>&lt;doc fingerprint="b28e5a636db0b3bc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Gamebooks and graph theory&lt;/head&gt;
    &lt;p&gt;Posted on 27 October 2019 in data-science&lt;/p&gt;
    &lt;p&gt;A game book is, contrary to the usual books, a book you don't read pages sequentially. These books are read interactively. You are offered a choice after a paragraph: go to the right turn to section 7, go to the left turn to 138. That's it. Depending on the series, you might have additional rules: fight, magic, psi power, etc.&lt;/p&gt;
    &lt;p&gt;During the winter holidays, I thought a bit more about these books. They could be encoded as directed graph networks. Therefore I could probably apply a bunch of network algorithms to them to extract interesting information such as:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the shortest path to an instant death,&lt;/item&gt;
      &lt;item&gt;the path with the most fights,&lt;/item&gt;
      &lt;item&gt;etc.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I chose for my analysis the Lone Wolf series because, luckily for me, some people have put all the books in an electronic format and it's legal. It's a story about Lone Wolf (unexpected I know) the last of his kind, a caste of warrior monks.&lt;/p&gt;
    &lt;p&gt;NB: The Dawn of the Darklords was excluded from the analysis as it was not officially released as a gamebook. It was included in the Magnamund companion.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The Masters of Darkness has the most action packed with a possible solution path including 65 fights;&lt;/item&gt;
      &lt;item&gt;The shortest path to death is The Kingdoms of Terror with only a 5 section path;&lt;/item&gt;
      &lt;item&gt;The Caverns of Kalte is the most deadly adventure with 19 instant death sections;&lt;/item&gt;
      &lt;item&gt;The shortest adventure is Flight from the Dark with a solution path only 27 sections long;&lt;/item&gt;
      &lt;item&gt;The longest adventure which can be done is The Shadow on the Sand with a touristic path of 224 sections, more than half of the sections (400).&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Summary&lt;/head&gt;
    &lt;p&gt;Here is a summary for the 28 books I've analyzed. For now Lonewolf comprises 4 series:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kai series: 1 to 5&lt;/item&gt;
      &lt;item&gt;Magnakai series: 6 to 12&lt;/item&gt;
      &lt;item&gt;Grand Master series: 13 to 20&lt;/item&gt;
      &lt;item&gt;New Order series: 21 to 32 (but now only 28 in project aon)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The values reported below are the average value for each category. Something interesting we can see if that from the 3rd series, there are no more cycles and the shortest path has increased on average 50% compared to the 1st and 2nd series. Also, the shortest path to death has tripled and the number of insta-death was halved.&lt;/p&gt;
    &lt;p&gt;Over time, the books might have become more focused on adventure and story but also less punishing. Having only read the first couple of books, I can't comment on this but if somebody has an opinion on this, I would be happy to hear about it and maybe update the post with your comments.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="9"&gt;
        &lt;cell role="head"&gt;series&lt;/cell&gt;
        &lt;cell role="head"&gt;shortest path&lt;/cell&gt;
        &lt;cell role="head"&gt;shortest path to death&lt;/cell&gt;
        &lt;cell role="head"&gt;path with the most fights&lt;/cell&gt;
        &lt;cell role="head"&gt;# of fight&lt;/cell&gt;
        &lt;cell role="head"&gt;# of luck&lt;/cell&gt;
        &lt;cell role="head"&gt;# of death&lt;/cell&gt;
        &lt;cell role="head"&gt;# of cycles&lt;/cell&gt;
        &lt;cell role="head"&gt;longest path&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;1-kai&lt;/cell&gt;
        &lt;cell&gt;51&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;20&lt;/cell&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;25&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;153&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;2-magnakai&lt;/cell&gt;
        &lt;cell&gt;66&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;21&lt;/cell&gt;
        &lt;cell&gt;42&lt;/cell&gt;
        &lt;cell&gt;23&lt;/cell&gt;
        &lt;cell&gt;15&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;171&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="9"&gt;
        &lt;cell&gt;3-grand-master&lt;/cell&gt;
        &lt;cell&gt;95&lt;/cell&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
        &lt;cell&gt;40&lt;/cell&gt;
        &lt;cell&gt;37&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;163&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;4-new-order&lt;/cell&gt;
        &lt;cell&gt;97&lt;/cell&gt;
        &lt;cell&gt;30&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;27&lt;/cell&gt;
        &lt;cell&gt;43&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;0&lt;/cell&gt;
        &lt;cell&gt;156&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h1"&gt;The technical bits&lt;/head&gt;
    &lt;head rend="h2"&gt;The preparation: Turn to 1&lt;/head&gt;
    &lt;p&gt;"Turn to" are the mythic words in these game books. It's also how we will divide the different sections of text, by using regexp. There are 5 types of section and their assigned color:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;normal: you can move forward to another section (white),&lt;/item&gt;
      &lt;item&gt;luck: you are asked to test your luck and you can move forward to another section (green),&lt;/item&gt;
      &lt;item&gt;fight: you are asked to fight some monster(s) and you can move forward to another section (yellow),&lt;/item&gt;
      &lt;item&gt;death: you chose badly and you got yourself killed, you have to restart from the section 1 (red),&lt;/item&gt;
      &lt;item&gt;start/end: first section and last section (blue).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Once the sections are defined, we have to create the directed graph. To do so, I used two python libraries:&lt;/p&gt;
    &lt;head rend="h2"&gt;Extracting the interesting information: Test your Luck&lt;/head&gt;
    &lt;p&gt;I mainly used &lt;code&gt;networkx&lt;/code&gt; for the graph network analysis. It's a straightforward library and the documentation is good.&lt;/p&gt;
    &lt;head rend="h3"&gt;Do you need a DAGger?&lt;/head&gt;
    &lt;p&gt;Typically a Lonewolf adventure is the equivalent of a Directed (A)Cyclic Graph:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Directed: Lonewolf, your character, goes from the section 1 to hopefully the latest section which is depending on the book the section 300, 350 or 400, without the possibility to come back to the previous section;&lt;/item&gt;
      &lt;item&gt;Acyclic: This is not totally true for the Lonewolf series as 7 books contain cycles, a "circular" path between two nodes which a node is repeated twice. Some algorithms like the shortest path or longest path require a DAG and we need to remove the cycles before running them.&lt;/item&gt;
      &lt;item&gt;Graph: The sections are the nodes of the graph and the vertices the choices for each section.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Disconnected graphs&lt;/head&gt;
    &lt;p&gt;In several books, graphs are disconnected. It means you can't go from the section 1 to the end section (300 or 350). This indicates usually that there is an enigma or puzzle asking to add numbers discovered along the adventure and reach the section given by the number. The only way to process these graphs is to check the text notes and add the missing edges manually.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cycle removal&lt;/head&gt;
    &lt;p&gt;The cycle removal is an interesting problem as it is one of the first problem to have been shown as NP-complete (NP stands for Non deterministic Polynomial time). This means that there is no known way to find a solution to solve that problem quickly and the time to find a solution grows as the size of the input grows. Nonetheless we are lucky because the data from a gamebook is usually quite small (300 to 400 nodes and 400 to 600 edges)!&lt;/p&gt;
    &lt;p&gt;The idea behind the cycle removal is simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;do a DFS search,&lt;/item&gt;
      &lt;item&gt;look at the nodes and their children,&lt;/item&gt;
      &lt;item&gt;if one or more children have been already visited, remove that edge.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I was curious about what could be done with a graph analysis of such textual / interactive games. Besides applying basic algorithms to test if the gamebook is playable, I didn't find much insights from it. I would like to see if there are any correlations between the features I chose and the popularity of the gamebooks. That being said, it was a cool project to do. I brushed up on the graph theory which I never really used outside of university.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future works&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Apply the same methodology to Figthing Fantasy gamebooks&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45342759</guid><pubDate>Tue, 23 Sep 2025 04:10:15 +0000</pubDate></item><item><title>Zoxide: A Better CD Command</title><link>https://github.com/ajeetdsouza/zoxide</link><description>&lt;doc fingerprint="56bb8c669bcd38dc"&gt;
  &lt;main&gt;
    &lt;p&gt;Special thanks to:&lt;/p&gt;
    &lt;p&gt;zoxide is a smarter cd command, inspired by z and autojump.&lt;/p&gt;
    &lt;p&gt;It remembers which directories you use most frequently, so you can "jump" to them in just a few keystrokes.&lt;lb/&gt; zoxide works on all major shells.&lt;/p&gt;
    &lt;p&gt;Getting started ‚Ä¢ Installation ‚Ä¢ Configuration ‚Ä¢ Integrations&lt;/p&gt;
    &lt;code&gt;z foo              # cd into highest ranked directory matching foo
z foo bar          # cd into highest ranked directory matching foo and bar
z foo /            # cd into a subdirectory starting with foo

z ~/foo            # z also works like a regular cd command
z foo/             # cd into relative path
z ..               # cd one level up
z -                # cd into previous directory

zi foo             # cd with interactive selection (using fzf)

z foo&amp;lt;SPACE&amp;gt;&amp;lt;TAB&amp;gt;  # show interactive completions (zoxide v0.8.0+, bash 4.4+/fish/zsh only)&lt;/code&gt;
    &lt;p&gt;Read more about the matching algorithm here.&lt;/p&gt;
    &lt;p&gt;zoxide can be installed in 4 easy steps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Install binary&lt;/p&gt;&lt;p&gt;zoxide runs on most major platforms. If your platform isn't listed below, please open an issue.&lt;/p&gt;&lt;head&gt;Linux / WSL&lt;/head&gt;&lt;p&gt;The recommended way to install zoxide is via the install script:&lt;/p&gt;&lt;code&gt;curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh&lt;/code&gt;&lt;p&gt;Or, you can use a package manager:&lt;/p&gt;&lt;th&gt;Distribution&lt;/th&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;asdf&lt;/td&gt;&lt;code&gt;asdf plugin add zoxide https://github.com/nyrst/asdf-zoxide.git&lt;/code&gt;&lt;code&gt;asdf install zoxide latest&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;conda-forge&lt;/td&gt;&lt;code&gt;conda install -c conda-forge zoxide&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;guix&lt;/td&gt;&lt;code&gt;guix install zoxide&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;Linuxbrew&lt;/td&gt;&lt;code&gt;brew install zoxide&lt;/code&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;nixpkgs&lt;/td&gt;&lt;code&gt;nix-env -iA nixpkgs.zoxide&lt;/code&gt;&lt;td&gt;AlmaLinux&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Alpine Linux 3.13+&lt;/td&gt;&lt;td&gt;Alpine Linux Packages&lt;/td&gt;&lt;code&gt;apk add zoxide&lt;/code&gt;&lt;td&gt;Arch Linux&lt;/td&gt;&lt;td&gt;Arch Linux Extra&lt;/td&gt;&lt;code&gt;pacman -S zoxide&lt;/code&gt;&lt;td&gt;CentOS Stream&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;del rend="overstrike"&gt;Debian 11+&lt;/del&gt;1&lt;del rend="overstrike"&gt;Debian Packages&lt;/del&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Devuan 4.0+&lt;/td&gt;&lt;td&gt;Devuan Packages&lt;/td&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Exherbo Linux&lt;/td&gt;&lt;td&gt;Exherbo packages&lt;/td&gt;&lt;code&gt;cave resolve -x repository/rust&lt;/code&gt;&lt;code&gt;cave resolve -x zoxide&lt;/code&gt;&lt;td&gt;Fedora 32+&lt;/td&gt;&lt;td&gt;Fedora Packages&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Gentoo&lt;/td&gt;&lt;td&gt;Gentoo Packages&lt;/td&gt;&lt;code&gt;emerge app-shells/zoxide&lt;/code&gt;&lt;td&gt;Linux Mint&lt;/td&gt;&lt;td&gt;apt.cli.rs (unofficial)&lt;/td&gt;&lt;td&gt;Setup the repository, then&lt;/td&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Manjaro&lt;/td&gt;&lt;code&gt;pacman -S zoxide&lt;/code&gt;&lt;td&gt;openSUSE Tumbleweed&lt;/td&gt;&lt;td&gt;openSUSE Factory&lt;/td&gt;&lt;code&gt;zypper install zoxide&lt;/code&gt;&lt;del rend="overstrike"&gt;Parrot OS&lt;/del&gt;1&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;del rend="overstrike"&gt;Raspbian 11+&lt;/del&gt;1&lt;del rend="overstrike"&gt;Raspbian Packages&lt;/del&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;RHEL 8+&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Rhino Linux&lt;/td&gt;&lt;td&gt;Pacstall Packages&lt;/td&gt;&lt;code&gt;pacstall -I zoxide-deb&lt;/code&gt;&lt;td&gt;Rocky Linux&lt;/td&gt;&lt;code&gt;dnf install zoxide&lt;/code&gt;&lt;td&gt;Slackware 15.0+&lt;/td&gt;&lt;td&gt;SlackBuilds&lt;/td&gt;&lt;td&gt;Instructions&lt;/td&gt;&lt;td&gt;Solus&lt;/td&gt;&lt;td&gt;Solus Packages&lt;/td&gt;&lt;code&gt;eopkg install zoxide&lt;/code&gt;&lt;td&gt;Ubuntu&lt;/td&gt;&lt;td&gt;apt.cli.rs (unofficial)&lt;/td&gt;&lt;td&gt;Setup the repository, then&lt;/td&gt;&lt;code&gt;apt install zoxide&lt;/code&gt;&lt;td&gt;Void Linux&lt;/td&gt;&lt;td&gt;Void Linux Packages&lt;/td&gt;&lt;code&gt;xbps-install -S zoxide&lt;/code&gt;&lt;head&gt;macOS&lt;/head&gt;&lt;p&gt;To install zoxide, use a package manager:&lt;/p&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;Homebrew&lt;/td&gt;&lt;code&gt;brew install zoxide&lt;/code&gt;&lt;td&gt;asdf&lt;/td&gt;&lt;code&gt;asdf plugin add zoxide https://github.com/nyrst/asdf-zoxide.git&lt;/code&gt;&lt;code&gt;asdf install zoxide latest&lt;/code&gt;&lt;td&gt;conda-forge&lt;/td&gt;&lt;code&gt;conda install -c conda-forge zoxide&lt;/code&gt;&lt;td&gt;MacPorts&lt;/td&gt;&lt;code&gt;port install zoxide&lt;/code&gt;&lt;td&gt;nixpkgs&lt;/td&gt;&lt;code&gt;nix-env -iA nixpkgs.zoxide&lt;/code&gt;&lt;p&gt;Or, run this command in your terminal:&lt;/p&gt;&lt;code&gt;curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh&lt;/code&gt;&lt;head&gt;Windows&lt;/head&gt;&lt;p&gt;zoxide works with PowerShell, as well as shells running in Cygwin, Git Bash, and MSYS2.&lt;/p&gt;&lt;p&gt;The recommended way to install zoxide is via&lt;/p&gt;&lt;code&gt;winget&lt;/code&gt;:&lt;quote&gt;winget install ajeetdsouza.zoxide&lt;/quote&gt;&lt;p&gt;Or, you can use an alternative package manager:&lt;/p&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;Chocolatey&lt;/td&gt;&lt;code&gt;choco install zoxide&lt;/code&gt;&lt;td&gt;conda-forge&lt;/td&gt;&lt;code&gt;conda install -c conda-forge zoxide&lt;/code&gt;&lt;td&gt;Scoop&lt;/td&gt;&lt;code&gt;scoop install zoxide&lt;/code&gt;&lt;p&gt;If you're using Cygwin, Git Bash, or MSYS2, you can also use the install script:&lt;/p&gt;&lt;code&gt;curl -sSfL https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | sh&lt;/code&gt;&lt;head&gt;BSD&lt;/head&gt;&lt;p&gt;To install zoxide, use a package manager:&lt;/p&gt;&lt;th&gt;Distribution&lt;/th&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;Any&lt;/td&gt;&lt;td&gt;crates.io&lt;/td&gt;&lt;code&gt;cargo install zoxide --locked&lt;/code&gt;&lt;td&gt;DragonFly BSD&lt;/td&gt;&lt;td&gt;DPorts&lt;/td&gt;&lt;code&gt;pkg install zoxide&lt;/code&gt;&lt;td&gt;FreeBSD&lt;/td&gt;&lt;td&gt;FreshPorts&lt;/td&gt;&lt;code&gt;pkg install zoxide&lt;/code&gt;&lt;td&gt;NetBSD&lt;/td&gt;&lt;td&gt;pkgsrc&lt;/td&gt;&lt;code&gt;pkgin install zoxide&lt;/code&gt;&lt;p&gt;Or, run this command in your terminal:&lt;/p&gt;&lt;code&gt;curl -sS https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | bash&lt;/code&gt;&lt;head&gt;Android&lt;/head&gt;&lt;p&gt;To install zoxide, use a package manager:&lt;/p&gt;&lt;th&gt;Repository&lt;/th&gt;&lt;th&gt;Instructions&lt;/th&gt;&lt;td&gt;Termux&lt;/td&gt;&lt;code&gt;pkg install zoxide&lt;/code&gt;&lt;p&gt;Or, run this command in your terminal:&lt;/p&gt;&lt;code&gt;curl -sS https://raw.githubusercontent.com/ajeetdsouza/zoxide/main/install.sh | bash&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Setup zoxide on your shell&lt;/p&gt;&lt;p&gt;To start using zoxide, add it to your shell.&lt;/p&gt;&lt;head&gt;Bash&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.bashrc&lt;/code&gt;):&lt;quote&gt;eval "$(zoxide init bash)"&lt;/quote&gt;&lt;head&gt;Elvish&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.elvish/rc.elv&lt;/code&gt;):&lt;quote&gt;eval (zoxide init elvish | slurp)&lt;/quote&gt;&lt;p&gt;Note zoxide only supports elvish v0.18.0 and above.&lt;/p&gt;&lt;head&gt;Fish&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.config/fish/config.fish&lt;/code&gt;):&lt;quote&gt;zoxide init fish | source&lt;/quote&gt;&lt;head&gt;Nushell&lt;/head&gt;&lt;p&gt;Add this to the end of your env file (find it by running&lt;/p&gt;&lt;code&gt;$nu.env-path&lt;/code&gt;in Nushell):&lt;quote&gt;zoxide init nushell | save -f ~/.zoxide.nu&lt;/quote&gt;&lt;p&gt;Now, add this to the end of your config file (find it by running&lt;/p&gt;&lt;code&gt;$nu.config-path&lt;/code&gt;in Nushell):&lt;quote&gt;source ~/.zoxide.nu&lt;/quote&gt;&lt;p&gt;Note zoxide only supports Nushell v0.89.0+.&lt;/p&gt;&lt;head&gt;PowerShell&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (find it by running&lt;/p&gt;&lt;code&gt;echo $profile&lt;/code&gt;in PowerShell):&lt;quote&gt;Invoke-Expression (&amp;amp; { (zoxide init powershell | Out-String) })&lt;/quote&gt;&lt;head&gt;Tcsh&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.tcshrc&lt;/code&gt;):&lt;quote&gt;zoxide init tcsh &amp;gt; ~/.zoxide.tcsh source ~/.zoxide.tcsh&lt;/quote&gt;&lt;head&gt;Xonsh&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.xonshrc&lt;/code&gt;):&lt;quote&gt;execx($(zoxide init xonsh), 'exec', __xonsh__.ctx, filename='zoxide')&lt;/quote&gt;&lt;head&gt;Zsh&lt;/head&gt;&lt;p&gt;Add this to the end of your config file (usually&lt;/p&gt;&lt;code&gt;~/.zshrc&lt;/code&gt;):&lt;quote&gt;eval "$(zoxide init zsh)"&lt;/quote&gt;&lt;p&gt;For completions to work, the above line must be added after&lt;/p&gt;&lt;code&gt;compinit&lt;/code&gt;is called. You may have to rebuild your completions cache by running&lt;code&gt;rm ~/.zcompdump*; compinit&lt;/code&gt;.&lt;head&gt;Any POSIX shell&lt;/head&gt;&lt;p&gt;Add this to the end of your config file:&lt;/p&gt;&lt;quote&gt;eval "$(zoxide init posix --hook prompt)"&lt;/quote&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Install fzf (optional)&lt;/p&gt;
        &lt;p&gt;fzf is a command-line fuzzy finder, used by zoxide for completions / interactive selection. It can be installed from here.&lt;/p&gt;
        &lt;p&gt;Note The minimum supported fzf version is v0.51.0.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Import your data (optional)&lt;/p&gt;&lt;p&gt;If you currently use any of these plugins, you may want to import your data into zoxide:&lt;/p&gt;&lt;head&gt;autojump&lt;/head&gt;&lt;p&gt;Run this command in your terminal:&lt;/p&gt;&lt;code&gt;zoxide import --from=autojump "/path/to/autojump/db"&lt;/code&gt;&lt;p&gt;The path usually varies according to your system:&lt;/p&gt;&lt;th&gt;OS&lt;/th&gt;&lt;th&gt;Path&lt;/th&gt;&lt;th&gt;Example&lt;/th&gt;&lt;td&gt;Linux&lt;/td&gt;&lt;code&gt;$XDG_DATA_HOME/autojump/autojump.txt&lt;/code&gt;or&lt;code&gt;$HOME/.local/share/autojump/autojump.txt&lt;/code&gt;&lt;code&gt;/home/alice/.local/share/autojump/autojump.txt&lt;/code&gt;&lt;td&gt;macOS&lt;/td&gt;&lt;code&gt;$HOME/Library/autojump/autojump.txt&lt;/code&gt;&lt;code&gt;/Users/Alice/Library/autojump/autojump.txt&lt;/code&gt;&lt;td&gt;Windows&lt;/td&gt;&lt;code&gt;%APPDATA%\autojump\autojump.txt&lt;/code&gt;&lt;code&gt;C:\Users\Alice\AppData\Roaming\autojump\autojump.txt&lt;/code&gt;&lt;head&gt;fasd, z, z.lua, zsh-z&lt;/head&gt;&lt;p&gt;Run this command in your terminal:&lt;/p&gt;&lt;code&gt;zoxide import --from=z "path/to/z/db"&lt;/code&gt;&lt;p&gt;The path usually varies according to your system:&lt;/p&gt;&lt;th&gt;Plugin&lt;/th&gt;&lt;th&gt;Path&lt;/th&gt;&lt;td&gt;fasd&lt;/td&gt;&lt;code&gt;$_FASD_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.fasd&lt;/code&gt;&lt;td&gt;z (bash/zsh)&lt;/td&gt;&lt;code&gt;$_Z_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.z&lt;/code&gt;&lt;td&gt;z (fish)&lt;/td&gt;&lt;code&gt;$Z_DATA&lt;/code&gt;or&lt;code&gt;$XDG_DATA_HOME/z/data&lt;/code&gt;or&lt;code&gt;$HOME/.local/share/z/data&lt;/code&gt;&lt;td&gt;z.lua (bash/zsh)&lt;/td&gt;&lt;code&gt;$_ZL_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.zlua&lt;/code&gt;&lt;td&gt;z.lua (fish)&lt;/td&gt;&lt;code&gt;$XDG_DATA_HOME/zlua/zlua.txt&lt;/code&gt;or&lt;code&gt;$HOME/.local/share/zlua/zlua.txt&lt;/code&gt;or&lt;code&gt;$_ZL_DATA&lt;/code&gt;&lt;td&gt;zsh-z&lt;/td&gt;&lt;code&gt;$ZSHZ_DATA&lt;/code&gt;or&lt;code&gt;$_Z_DATA&lt;/code&gt;or&lt;code&gt;$HOME/.z&lt;/code&gt;&lt;head&gt;ZLocation&lt;/head&gt;&lt;p&gt;Run this command in PowerShell:&lt;/p&gt;&lt;quote&gt;$db = New-TemporaryFile (Get-ZLocation).GetEnumerator() | ForEach-Object { Write-Output ($_.Name+'|'+$_.Value+'|0') } | Out-File $db zoxide import --from=z $db&lt;/quote&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When calling &lt;code&gt;zoxide init&lt;/code&gt;, the following flags are available:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;--cmd&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Changes the prefix of the &lt;code&gt;z&lt;/code&gt;and&lt;code&gt;zi&lt;/code&gt;commands.&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;--cmd j&lt;/code&gt;would change the commands to (&lt;code&gt;j&lt;/code&gt;,&lt;code&gt;ji&lt;/code&gt;).&lt;/item&gt;
          &lt;item&gt;&lt;code&gt;--cmd cd&lt;/code&gt;would replace the&lt;code&gt;cd&lt;/code&gt;command.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Changes the prefix of the &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;--hook &amp;lt;HOOK&amp;gt;&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;&lt;p&gt;Changes how often zoxide increments a directory's score:&lt;/p&gt;&lt;th&gt;Hook&lt;/th&gt;&lt;th&gt;Description&lt;/th&gt;&lt;code&gt;none&lt;/code&gt;&lt;td&gt;Never&lt;/td&gt;&lt;code&gt;prompt&lt;/code&gt;&lt;td&gt;At every shell prompt&lt;/td&gt;&lt;code&gt;pwd&lt;/code&gt;(default)&lt;td&gt;Whenever the directory is changed&lt;/td&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;--no-cmd&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Prevents zoxide from defining the &lt;code&gt;z&lt;/code&gt;and&lt;code&gt;zi&lt;/code&gt;commands.&lt;/item&gt;
          &lt;item&gt;These functions will still be available in your shell as &lt;code&gt;__zoxide_z&lt;/code&gt;and&lt;code&gt;__zoxide_zi&lt;/code&gt;, should you choose to redefine them.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;Prevents zoxide from defining the &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Environment variables2 can be used for configuration. They must be set before &lt;code&gt;zoxide init&lt;/code&gt; is called.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_DATA_DIR&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Specifies the directory in which the database is stored.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;The default value varies across OSes:&lt;/p&gt;&lt;th&gt;OS&lt;/th&gt;&lt;th&gt;Path&lt;/th&gt;&lt;th&gt;Example&lt;/th&gt;&lt;td&gt;Linux / BSD&lt;/td&gt;&lt;code&gt;$XDG_DATA_HOME&lt;/code&gt;or&lt;code&gt;$HOME/.local/share&lt;/code&gt;&lt;code&gt;/home/alice/.local/share&lt;/code&gt;&lt;td&gt;macOS&lt;/td&gt;&lt;code&gt;$HOME/Library/Application Support&lt;/code&gt;&lt;code&gt;/Users/Alice/Library/Application Support&lt;/code&gt;&lt;td&gt;Windows&lt;/td&gt;&lt;code&gt;%LOCALAPPDATA%&lt;/code&gt;&lt;code&gt;C:\Users\Alice\AppData\Local&lt;/code&gt;&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_ECHO&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;When set to 1, &lt;code&gt;z&lt;/code&gt;will print the matched directory before navigating to it.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;When set to 1, &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_EXCLUDE_DIRS&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;Excludes the specified directories from the database.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;This is provided as a list of globs, separated by OS-specific characters:&lt;/p&gt;
            &lt;th&gt;OS&lt;/th&gt;
            &lt;th&gt;Separator&lt;/th&gt;
            &lt;th&gt;Example&lt;/th&gt;
            &lt;td&gt;Linux / macOS / BSD&lt;/td&gt;
            &lt;code&gt;:&lt;/code&gt;
            &lt;code&gt;$HOME:$HOME/private/*&lt;/code&gt;
            &lt;td&gt;Windows&lt;/td&gt;
            &lt;code&gt;;&lt;/code&gt;
            &lt;code&gt;$HOME;$HOME/private/*&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;By default, this is set to&lt;/p&gt;&lt;code&gt;"$HOME"&lt;/code&gt;.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_FZF_OPTS&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_MAXAGE&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Configures the aging algorithm, which limits the maximum number of entries in the database.&lt;/item&gt;
          &lt;item&gt;By default, this is set to 10000.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;_ZO_RESOLVE_SYMLINKS&lt;/code&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;When set to 1, &lt;code&gt;z&lt;/code&gt;will resolve symlinks before adding directories to the database.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;When set to 1, &lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Application&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Plugin&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;aerc&lt;/cell&gt;
        &lt;cell&gt;Email client&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;alfred&lt;/cell&gt;
        &lt;cell&gt;macOS launcher&lt;/cell&gt;
        &lt;cell&gt;alfred-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;clink&lt;/cell&gt;
        &lt;cell&gt;Improved cmd.exe for Windows&lt;/cell&gt;
        &lt;cell&gt;clink-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;emacs&lt;/cell&gt;
        &lt;cell&gt;Text editor&lt;/cell&gt;
        &lt;cell&gt;zoxide.el&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;felix&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;joshuto&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;lf&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;See the wiki&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;nnn&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;nnn-autojump&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ranger&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;ranger-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;raycast&lt;/cell&gt;
        &lt;cell&gt;macOS launcher&lt;/cell&gt;
        &lt;cell&gt;raycast-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rfm&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;sesh&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;tmux&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;telescope.nvim&lt;/cell&gt;
        &lt;cell&gt;Fuzzy finder for Neovim&lt;/cell&gt;
        &lt;cell&gt;telescope-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tmux-session-wizard&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;tmux&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;tmux-sessionx&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;tmux&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;vim / neovim&lt;/cell&gt;
        &lt;cell&gt;Text editor&lt;/cell&gt;
        &lt;cell&gt;zoxide.vim&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xplr&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;zoxide.xplr&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;xxh&lt;/cell&gt;
        &lt;cell&gt;Transports shell configuration over SSH&lt;/cell&gt;
        &lt;cell&gt;xxh-plugin-prerun-zoxide&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;yazi&lt;/cell&gt;
        &lt;cell&gt;File manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;zabb&lt;/cell&gt;
        &lt;cell&gt;Finds the shortest possible query for a path&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;zesh&lt;/cell&gt;
        &lt;cell&gt;&lt;code&gt;zellij&lt;/code&gt; session manager&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;zsh-autocomplete&lt;/cell&gt;
        &lt;cell&gt;Realtime completions for zsh&lt;/cell&gt;
        &lt;cell&gt;Natively supported&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45342943</guid><pubDate>Tue, 23 Sep 2025 04:48:28 +0000</pubDate></item><item><title>Delete FROM users WHERE location = 'Iran';</title><link>https://gist.github.com/avestura/ce2aa6e55dad783b1aba946161d5fef4</link><description>&lt;doc fingerprint="ee1198109d4361d8"&gt;
  &lt;main&gt;
    &lt;p&gt;Hi! I am an Iranian Software Engineer, and in this torn paper note, I want to talk about some funny moments I had online related to the fact that I was spawned in this specific region of the world: Iran.&lt;/p&gt;
    &lt;p&gt;Back when I was a student, I got access to the Microsoft Imagine, and as a result, I got access to the Microsoft Store as a developer. This inspired me write one of my open-source projects called EyesGuard and publish it on Microsoft Store. However, one day, somebody told me that they can no longer find EyesGuard on the store.&lt;/p&gt;
    &lt;p&gt;I came to the realization that Microsoft deleted my app, my developer account, and all those comments on my app supporting me and suggesting ideas on how to improve the program. I tried to contact the support and email whoever I could, but I was ghosted. Nobody ever explained to me why, but I assume it's because of the sanctions.&lt;/p&gt;
    &lt;p&gt;Notion is a great product, and it was the primary tool I used to manage my personal notes. Not until they suddenly decided to wipe out every data related to the users residing in Iran. Hopefully, they actually responded to my support message:&lt;/p&gt;
    &lt;p&gt;It was because of sanctions. However, they told me that they will not restore the data, even if I leave Iran someday:&lt;/p&gt;
    &lt;p&gt;That said, I am very happy with my own self-hosted Siyuan now.&lt;/p&gt;
    &lt;p&gt;I read hackernews on a daily basis and I visit lots of different websites regularly. I am almost always on my VPN as I am internally firewalled by the government and externally shooed because of the sanctions, so I am probably missing some of these heart-warming messages:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Iranian IPs are blocked here, due to your decision to arm Russia with drones so that they can indiscriminately massacre civilians.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;My VPN turned off, and opening https://www.grepular.com showed me this message. I actually do not blame the people who do this. I think there is a fundamental misconception that people think because "Islamic Republic" has the word "Republic" in it, it must be a government of people in charge. That's not the case. I have yet to see anyone who actually supports Russian aggression in my real life in Iran. Funny enough, Iran's history is full of backstabs by the Russian government.&lt;/p&gt;
    &lt;p&gt;I tried contacting the author by sending this email:&lt;/p&gt;
    &lt;code&gt;Hi Mark,

I hope this message finds you well.

While browsing HackerNews, I came across your website but was greeted with this message:

&amp;gt; Iranian IPs are blocked here, due to your decision to arm Russia with drones so that they can indiscriminately massacre civilians.

I wanted to clarify that the decision to support Russia does not represent the Iranian people. That "your decision" refers to the regime, a theocratic minority that rules Iran without democratic legitimacy. The people of Iran have long protested and revolted against this regime, but unfortunately, they face brutal suppression while unarmed.

In my experience, most Iranians around me, including myself, stand firmly with Ukraine and against Russian aggression.

I‚Äôm not asking you to reconsider the IP restriction, you have your reasons and I respect that. I simply wanted to share this perspective and express my solidarity with Ukraine.

Slava Ukraini!

Best regards,
Avestura
&lt;/code&gt;
    &lt;p&gt;I got no replies from them, and I actually didn't expect one.&lt;/p&gt;
    &lt;p&gt;I woke up to the news that GitHub has removed the access of Iranians to their private repositories. Well, that was not good. I tried to launch my own self-hosted instance of Gitea to reduce the damage. However, later, GitHub announced that github is now available in Iran by securing a license from the US government, and we're now good. You see? The weather is good, the birds are singing, GitHub is free again. Fantastic!&lt;/p&gt;
    &lt;p&gt;Similarly, GitLab banned every account that once accessed from an Iranian IP, however, to this day, they never lifted the ban, even on public repositories. I guess they couldn't secure a license from the US government, or they simply never cared. Good luck to them in either case, though. GitLab is an amazing software. One can always self-host it.&lt;/p&gt;
    &lt;p&gt;The list goes on, and almost all of the services you probabelly heard of is banned here: Cloud platforms (AWS, GCP, Azure, ...), Educational platforms (coursera, udemy, etc), Payment software (stripe, paypal, ...).&lt;/p&gt;
    &lt;p&gt;I don't think any of these companies have bad intentions towards any group of people. They are a business after all. They don't hate their customers; they are just playing the game, and the game has such rules. But if someday some law or government forces me to prevent my services from a group, I'll think twice before writing those &lt;code&gt;if&lt;/code&gt; statements. I'll try to have more empathy. People behind those screens are more important than just some rows in my tables.&lt;/p&gt;
    &lt;p&gt;Important&lt;/p&gt;
    &lt;p&gt;In this text, I am NOT asking for the removal of the sanctions targeted at the Islamic Republic of Iran. I am merely remembering some moments on top of my head. For the record, I do not support the actions of the Islamic Republic, and on the contrary, I am in favor of the movements that release the people from such a mafia-like cult ruling a country with thousands of years of history. The actions of the group in charge of Iran are not defensible, and as a matter of fact, the people of Iran are the first layer of victims. Some examples are listed here. I especially feel it differently, as regime thugs put a gun to the throat of a dear person to me, and threatened to kill him if he showed up in protests.&lt;/p&gt;
    &lt;p&gt;By the way, did you know you could return &lt;code&gt;451 Unavailable For Legal Reasons&lt;/code&gt; instead of &lt;code&gt;403 Forbidden&lt;/code&gt; when you're going to ban me next time?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45343108</guid><pubDate>Tue, 23 Sep 2025 05:30:28 +0000</pubDate></item><item><title>Altoids by the Fistful</title><link>https://www.scottsmitelli.com/articles/altoids-by-the-fistful/</link><description>&lt;doc fingerprint="67519077dbe58b9f"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Altoids by the Fistful&lt;/head&gt;&lt;p&gt;‚ÄúWh‚Äî what did you say?‚Äù&lt;/p&gt;&lt;p&gt;It‚Äôs close to six o‚Äôclock on a weekday afternoon and the bar is starting to get noisy with the after-work crowd. It‚Äôs entirely possible I misheard that last part.&lt;/p&gt;&lt;p&gt;‚ÄúAltoids! I find the spearmint works a little better overall, but recently I‚Äôve started switching flavors depending on the situation.‚Äù&lt;/p&gt;&lt;p&gt;I‚Äôve worked with James‚Äî‚ÄúJim‚Äù as everyone on the team knows him‚Äîfor a little over two years and I‚Äôm used to this dance now. He gets a kind of tunnel vision in his excitement about whatever shiny new thing has captured his attention. It‚Äôs usually pretty easy to shake him out of it.&lt;/p&gt;&lt;p&gt;‚ÄúNo, Jim, the part before that.‚Äù&lt;/p&gt;&lt;p&gt;He looks at me for a moment, inquisitive, before pushing his beer aside. ‚ÄúHere, let me show you.‚Äù He reaches underneath the table and produces his beige-on-brown Timbuk2 messenger bag. There is a small wet spot left behind from his drink, and the bag plops right onto it. I watch as one of his stubby hands unbuckles the outermost pouch while the other one pulls out a small green and white tin. I am obviously intended to see this as clearly as possible, evidenced by the way he places it front and center between us.&lt;/p&gt;&lt;p&gt;‚ÄúRegular everyday Altoids, right? You take about four of them, maybe five.‚Äù He flips the lid open and traps the requisite number of small white mints between his fingertips, which he then pops into his mouth. ‚ÄúThis is the trick; you gotta half-chew it first.‚Äù At least two tiny shards fly in my direction as he speaks these words. It is like listening to a slow K-turn executed on a road covered in gravel and seashells. Three more slow and deliberate chomps, then his bite eases. ‚ÄúMmm.‚Äù The communication style switches to mime: an index finger raised in a ‚Äúone moment‚Äù gesture, followed by an exaggerated point downwards while unzipping the main pouch of the bag. It takes a few seconds of rooting around before the star of this particular show is found.&lt;/p&gt;&lt;p&gt;My eyes barely have enough time to resolve the object under the dismal light at this end of the bar before it‚Äôs in his mouth. He‚Äôs chewing the full concoction now‚Äîmouth closed, thank God. The crunching softens, then fades into the din from a nearby table of sales bros laughing at their sales bro anecdote. Jim is looking at me with a kind of confident smugness I haven‚Äôt seen since I bet my buddy at Guitar Center that he couldn‚Äôt spontaneously play ‚ÄúEverlong‚Äù from memory. A bet I lost, I might add.&lt;/p&gt;&lt;p&gt;There is a degree of intentional spectacle to this, I‚Äôd have to imagine. Each jaw movement is deliberate. Precise. He does not break eye contact with me, though I desperately want to break it with him. I can‚Äôt though. The absurdity of the scene is absolutely hypnotizing. One final swallow, a smack of his lips, then he opens his mouth wide like a child proving that they finished all their vegetables and have earned their dessert. ‚ÄúEasy peasy, no problem.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúThat was&amp;amp;mldr;‚Äù It‚Äôs like a significant piece of my brain has just completely locked up. I‚Äôm just saying words without thinking, filling the empty air.&lt;/p&gt;&lt;p&gt;‚ÄúA cat turd!‚Äù he proclaims, finishing my sentence.&lt;/p&gt;&lt;p&gt;A beat.&lt;/p&gt;&lt;p&gt;‚ÄúYou just ate a cat turd.‚Äù It‚Äôs all I can do in this moment to plainly restate the facts as I understand them, although the sense of alarm is definitely carrying in my voice.&lt;/p&gt;&lt;p&gt;‚ÄúYup, and it didn‚Äôt taste bad at all. The spearmint masks it completely. Watch, I‚Äôll do another one.‚Äù My eyes widen in dread as I shake my head weakly. I didn‚Äôt want to see him do that the first time; I sure as shit don‚Äôt want to see it again.&lt;/p&gt;&lt;p&gt;‚ÄúNo, that‚Äôs alright,‚Äù I balk.&lt;/p&gt;&lt;p&gt;There is an awkward reach across the bag as he grabs his glass, tips it toward me in a silent toast, then takes a long swill. Whether he admits it or not, there‚Äôs evidently something that needs to be washed down. He lets out a contented sigh as the almost-empty glass thumps back down on the table. I glance down at the chicken wings and carrot sticks I had been picking at. A minute ago, they were kinda bland‚Äîmerely okay by the standards of pub food. With the abrupt loss of my appetite, now they are destined for the dumpster out back.&lt;/p&gt;&lt;p&gt;He lifts the small tin of mints and gives it a little shake in front of my face. It sounds a lot more papery and a lot less metallic than I would‚Äôve guessed. ‚ÄúAltoids. I‚Äôm not exaggerating when I say these have completely changed the way I work.‚Äù I follow this little miracle box as they get tucked back into the bag, the buckles snapping shut to shield them from the lustful gaze of an angry world. He pauses and looks up at me again. ‚ÄúWould you like to try?‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúNo, Jim, I don‚Äôt want your cat turds.‚Äù&lt;/p&gt;&lt;p&gt;I don‚Äôt want your cat turds. Why did I say it like that? I don‚Äôt want anybody‚Äôs cat turds!&lt;/p&gt;&lt;p&gt;&amp;amp;mldr;Right?&lt;/p&gt;&lt;p&gt;‚ÄúCompletely changed the way I work,‚Äù he repeats mechanically, sliding his bag onto the empty seat to his left. I‚Äôm finding it quite difficult to look at Jim, so I instead follow the motions of the bag until it is completely out of my view. How many more are in there?&lt;/p&gt;&lt;p&gt;‚ÄúI used to spend so much of my day on cat turds, psyching myself up, trying strategies that didn‚Äôt work, all the cleanup when I was finished. That‚Äôs all gone now. I can never go back to the old way.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúI just&amp;amp;mldr; I mean&amp;amp;mldr;‚Äù My brain has started working again, at least superficially, and it has generated so many questions that I‚Äôm having a hard time selecting which one to ask first. ‚ÄúHow long have you been eating cat turds?‚Äù A fine question for this moment, I suppose.&lt;/p&gt;&lt;p&gt;‚ÄúWhat do you mean? I‚Äôve always had to eat cat turds. Since I was a kid in school, on through college, in all my jobs&amp;amp;mldr; They keep giving me cat turds and I keep having to eat them, otherwise it starts to pile up and then things really get messy.‚Äù&lt;/p&gt;&lt;p&gt;His face turns slightly serious as he parses my expression, his head tilting in suspicion. ‚ÄúYou eat cat turds too, yeah?‚Äù I choose not to answer that question. He continues anyway. ‚ÄúSure. We all do. We have to, ya know?‚Äù&lt;/p&gt;&lt;p&gt;We all do.&lt;/p&gt;&lt;p&gt;Those words have been repeating in my head with the consistency of a drumline cadence. We all do.&lt;/p&gt;&lt;p&gt;‚ÄúWalk sign is on to cross Pawk Avenue. Walk sign is on to cross Pawk Avenue.‚Äù I‚Äôve heard this prerecorded voice, clearly belonging to the most disgruntled DOT Traffic Signals employee available at the time of this crosswalk‚Äôs construction, at least twice per workday for the last two years. It stirs up a half-remembered dream of a career spent shoveling dirt into a hole‚Äîsomething that feels more like the idea of ‚Äúhonest work‚Äù than what I get paid to do every day. I bet nobody on the construction crew spent an entire workday fighting around with brittle, poorly designed automation tooling like I did today.&lt;/p&gt;&lt;p&gt;I‚Äôm quickly but unintentionally refilling my conscious mind with the task I had gleefully abandoned when Jim invited me out to after-work drinks. Normally I‚Äôd be irritated to spend more of my waking life thinking about this stuff, but after what I witnessed at the bar I welcome any distraction at all.&lt;/p&gt;&lt;p&gt;‚ÄúOkay. So, usually we have a string. This is one of many values inside a mapping type, within a list of similar mappings.‚Äù I‚Äôm narrating to myself silently, imagining little bits of JSON syntax stamped on rectangles that are kind of stacked on top of each other like playing cards. ‚ÄúBut ever since the schema change in V3, sometimes the value is another mapping type that wraps the string we want&amp;amp;mldr;‚Äù I‚Äôm visualizing another square to the right of the existing one. This one is yellow, distinct from the light blue of all the others, and it never occurs to me to question why that is.&lt;/p&gt;&lt;p&gt;‚ÄúBut because this is actually YAML, and the value comes from a template call, both the string form and the mapping form need to be escaped and indented in a way that works in both cases.‚Äù I‚Äôm chewing on the problem in pretty much the same mindset I had during work, only now I‚Äôm walking across midtown instead of staring at a computer screen. ‚ÄúWe could just revert that change, keep the value as JSON like it used to be and insert it verbatim&amp;amp;mldr; but DevEx owns that part and I wouldn‚Äôt want to have to fight to get that PR approved.‚Äù&lt;/p&gt;&lt;p&gt;‚ÄúPiece of shit.‚Äù I speak that bit out loud without really intending to. I snap back into awareness of my surroundings and look around. Nobody was near enough to hear it. They probably wouldn‚Äôt have cared if they were.&lt;/p&gt;&lt;p&gt;It occurs to me that, whenever anybody asks me what I do for a living and I wave my hand and say ‚ÄúComputers,‚Äù this is what I‚Äôm trying to avoid needing to have to explain. None of these words are being used in a way that would mean anything to most people. If one were to take the time to carefully define them all and how they fit together semantically, they describe concepts so abstract and detached from any kind of tangible shared experience that you‚Äôd hit a second wall trying to explain that.&lt;/p&gt;&lt;p&gt;‚ÄúOh, but wait, we have the &lt;code&gt;nindent&lt;/code&gt; function. I could just count up the indentation level of the outer list and&amp;amp;mldr; Ah, hell, I forgot this template is transcluded into pod and deployment specs and the nesting levels would be different between the two.‚Äù I briefly try to think of which chucklefuck I could blame this design on, but truth be told I rubber-stamped enough questionable pull requests in my time here that a fair amount of this situation is a mess of my own damn making.&lt;/p&gt;&lt;p&gt;Huh. I really do wonder what I would sound like trying to explain this to somebody who had no experience in the industry. I suppose if I was very excited about it, I might come across like an energetic kid going on and on about all the different Pok√©mon they know about and all the special attacks and vulnerabilities. But without that spark of passion, and in its place a jaded voice tinged with frustration and contempt, I would probably just sound like a raving lunatic. These words don‚Äôt mean anything. I‚Äôm not describing something that actually exists. I‚Äôm playing the part of an observer in a universe of little floating boxes, becoming physically agitated about a superficial difference within the yellow one, and none of it is real.&lt;/p&gt;&lt;p&gt;I‚Äôm definitely not feeling the passion on this one. This code runs deep inside a build-deploy pipeline that I have no hope of ever running directly on the computer I‚Äôm using. So I write the code, push it to CI, wait for a bunch of stuff I‚Äôm not interested in to finish running, then get to watch my change fail to work for either the stupidest typo that I never should‚Äôve made in the first place, or due to some error that is so novel that even the search engines assume I must really be having some other much more popular error instead of the one I provided. It feels like I am performing surgery using a scalpel held by a boardwalk arcade claw machine, complete with the constant squawking and shitting of project management seagulls.&lt;/p&gt;&lt;p&gt;And even if I could concisely explain all of that to my hypothetical interlocutor, there‚Äôs the even higher-level question: Why? Why did we even make this change? What was so irredeemably wrong with the last two versions of this thing that we‚Äôre now doing it all again a third time? What exactly is the goal we‚Äôre trying to achieve here? I can‚Äôt really say. It‚Äôs a question I never asked, partly because I learned a long time ago that asking questions just causes friction. Just nod and shut up. Put a +1 on a sketchy PR and get it out of here. Don‚Äôt hold up the pipeline. Recover enough stamina to face down the next eldritch nightmare that slithers its way to the top of my Jira swimlane. ‚ÄúSounds great, thanks.‚Äù Thumbs-up. Grit my teeth through to the next direct deposit, convince myself it‚Äôs not so bad. Do it over and over until some ill-defined end condition is met. I‚Äôll know it when I see it. I hope.&lt;/p&gt;&lt;p&gt;I catch myself at the tail end of a sigh. I fake like I‚Äôm yawning to stretch my upper body for a second. Approximately every muscle in my back now aches.&lt;/p&gt;&lt;p&gt;There‚Äôs this very real sense that I don‚Äôt&amp;amp;mldr; I don‚Äôt want to solve this problem. There is no intellectual reward at the end of this journey. It‚Äôs not interesting to me. This isn‚Äôt something that needs to be fixed, because it‚Äôs not a situation that ever should‚Äôve been permitted to happen in the first place. This is just a bunch of contrived nonsense that I must work through because the broader situation dictates it. It doesn‚Äôt matter if the solution is good or elegant. It doesn‚Äôt matter if it barely works. It doesn‚Äôt matter if it causes another problem that I stub my toe on in three weeks. It‚Äôs just&amp;amp;mldr; what I have to do.&lt;/p&gt;&lt;p&gt;I stop in my tracks.&lt;/p&gt;&lt;p&gt;These kinds of problems are my cat turds.&lt;/p&gt;&lt;p&gt;Unlike Jim, though, I can‚Äôt just cram a bunch of breath mints into my face to make this go away.&lt;/p&gt;&lt;p&gt;The ‚Äúdown‚Äù escalator into the train station is out of service, and it has been this way all summer. A pair of orange plastic barricades block the landings at both ends. I walk down two flights of stairs alongside a half-dozen other commuters. Having concluded that the template problem simply isn‚Äôt worth thinking any further about, I‚Äôm back on the cat turds. I understand what Jim was talking about now. This has been happening for almost my entire life, even going back to my days in elementary school.&lt;/p&gt;&lt;p&gt;All of the homework assignments that were blindly graded against answer keys from the back of a Teacher Edition of the textbook: Cat turds. College admission essays where I profused a longing desire to attend the distinguished universities that my parents and guidance counselor told me I should set my ambitions toward: Cat turds. Probably hundreds of cover letters submitted alongside job applications throughout the years, skimmed by perhaps tens of internal recruiters and hiring managers: Cat turds.&lt;/p&gt;&lt;p&gt;The notion that it was a good idea to manipulate highly whitespace-sensitive YAML data with the Go &lt;code&gt;text/template&lt;/code&gt; package. CI workflows that take 75 minutes to reach the one step in the entire process that fails. Tools and interfaces that force-update and introduce breaking changes for seemingly no justifiable reason, removing or kneecapping features that were being relied on, with issue trackers guarded by thickheaded bots that dismissively auto-close feature requests that kindly ask for consideration for those use cases. Massively over-complicated software that tries to be everything to everybody, but in reality ends up being a gigantic lumbering pile of failure and frustration. Cat turds.&lt;/p&gt;&lt;p&gt;I used to love this stuff. I still do. Except&amp;amp;mldr; I don‚Äôt. Not lately, anyway. A long time ago, this was unquestionably what I wanted to do with my life. I would stay up late, pushing back my bedtime for a few more minutes with these glorious machines, hacking away on some little project. Then I‚Äôd get up early the following morning, excited to jump back into the project before my day out in the world began. I don‚Äôt even clearly remember what I was building toward, but I know it had basically zero utility or market potential. The point of doing the project was simply to do the project‚Äîto press through problems, to learn new things, and to end the day with more skills and experience than I started with.&lt;/p&gt;&lt;p&gt;At one point, I had the 7-bit ASCII table memorized. Just the decimal codes; I didn‚Äôt really understand the usefulness of the hexadecimal representations, and it never occurred to me that the hex values would work much better in mnemonics. I don‚Äôt know why I took the time to learn that. I never really used that knowledge in any real day-to-day work, and it began to fade from my mind as soon as I found some other pointless esoterica to wallpaper over it.&lt;/p&gt;&lt;p&gt;Look at me now, having to Google how to read a text file line-by-line in Python despite having done it a hundred times at this point. The knowledge is up there somewhere, I‚Äôm sure of it. I just can‚Äôt always think of the idiom in the heat of the moment. Just a little hint to jog the old brain, that‚Äôs all I need.&lt;/p&gt;&lt;p&gt;I often wonder what my Younger Self would think of me now, failing to remember a two-line snippet of code that you‚Äôd find in the first ten pages of any beginner‚Äôs guide to the language. He‚Äôd probably sneer and say I need to devote more time to studying. But I‚Äôm an adult with things to do; I can‚Äôt spend all my time just memorizing things just in case I might need the information someday. Oh, and by the way: Younger Self, if you were such a friggin‚Äô hotshot, why did it take you fifteen years to finally wrap your head around regular expressions? What‚Äôs that? Because they were hard? So you spent all your time memorizing easy and pointless trivia rather than tackling anything that was genuinely challenging? And then building up a whole air of superiority based on the number of discrete facts you could rattle off, rather than their practical utility? What, were you trying to become a contestant on Computer Jeopardy! or something?&lt;/p&gt;&lt;p&gt;No wonder Younger Self grew up to be kind of an asshole.&lt;/p&gt;&lt;p&gt;I mean, I didn‚Äôt try to be an asshole. It‚Äôs just that I tended to gauge my own self-worth relative to others based on the only social currency we could accurately compare: the amount of ‚Äústuff‚Äù we knew. Some people memorize car engine displacements, others carry in their noggins enough digits of pi to resolve the observable universe down to the width of a hydrogen atom. I had a litany of command-line switches that I never used for anything, HTML character entity names for writing systems I couldn‚Äôt comprehend, and tales of tweaking settings deep inside the Windows 98 Device Manager just so I could brag about having been in there in the first place. I also at one time sincerely believed that maybe if I taught myself to‚ÄîI‚Äôm picking one example out of many‚Äîdecode Code 39 barcodes in my head, it would somehow make me cool and desirable during otherwise awkward social functions. (I did get reasonably good at it. All it takes is memorizing a couple of three-digit sequences. Having a teenager‚Äôs near-field visual acuity certainly helped.)&lt;/p&gt;&lt;p&gt;Everybody else who didn‚Äôt know those little pieces of nothing? They were the lessers. They didn‚Äôt put in the time to grind for this knowledge. They had never scaled the peaks of Mount AltaVista, nor had they knelt in the temple of the MSDN Library for Visual Studio on a banged-up pair of CD-Rs. I knew things they did not, therefore I felt I was higher-and-mightier than they were. I and I alone suffered for this knowledge. This attitude manifested itself in one of two ways: In the first case, I would barge my way into situations where my involvement wasn‚Äôt needed or appreciated, thinking I could ‚Äúsave‚Äù others from the pain I once had to contend with. More often than not, though, I would simply mock people for not knowing things‚Äîusually inside my own head, but sometimes outwardly on mailing lists and message boards. There were times when I judged a person to have failed to put in the necessary amount of work, so therefore they did not deserve to rise anywhere near where I considered my own level to be. It didn‚Äôt matter if the subject was deeply technical or a disagreement on the precise phrasing of a Simpsons quote. Somebody got something wrong, and it was my job to rectify that.&lt;/p&gt;&lt;p&gt;I feel bad for the people who worked on teams where Younger Self was the senior engineer. I was full of ideals and convictions back then. ‚ÄúNo, we‚Äôre not doing that. We‚Äôre going to Do It Right instead.‚Äù I was full of piss and vinegar. ‚ÄúHere, give me that; I‚Äôll just do it myself.‚Äù I was full of shit.&lt;/p&gt;&lt;p&gt;I now realize that everything I lorded over other people‚Äîall the things I gatekept without consciously understanding that this was what I was doing‚ÄîI didn‚Äôt need to do that. It really didn‚Äôt help anything. For some number of people who interacted with me, I was the problem. I could‚Äôve been more tolerant or forgiving, I could‚Äôve said ‚Äúlet‚Äôs find out together,‚Äù I could‚Äôve let other people have the fun once in a while. I could have minded my own damn business and saved everybody the hassle.&lt;/p&gt;&lt;p&gt;There were people out there who must‚Äôve felt that I was their cat turds.&lt;/p&gt;&lt;p&gt;I‚Äôll never be able to track down and apologize to every person I treated that way. And why did I even build that fiefdom and protect it so jealously? Why was I so insecure? Why did I have to always be right and have a ready justification for why everybody else was wrong?&lt;/p&gt;&lt;p&gt;It was just me, alone in my tiny sandbox, safe and secure behind my towering fortress of cat turds.&lt;/p&gt;&lt;p&gt;My usual train, the one packed so full that some riders have to stand in the aisles until after the first or second stop, usually leaves at 5:50. Now about three hours later, one can sometimes get an entire car to themselves. I settle down in a window seat looking out at the desolate platform. Evidently there aren‚Äôt all that many people interested in traveling across the river at this hour on a Wednesday evening. It feels nice to sit, despite the fact that I‚Äôve probably sat for a cumulative ten hours‚Äîat least‚Äîover the course of this day.&lt;/p&gt;&lt;p&gt;As sometimes happens, another rider boards the train and enters what had up to this point been my personal rail car. He selects the aisle seat in the row directly in front of me. At least 110 other seats in this car, every single one of them empty, and his choice is to sit right here. Sigh. I could get up and move to another seat but I‚Äôm&amp;amp;mldr; exhausted. I‚Äôm here, I‚Äôm settled in, and above all I‚Äôm just completely out of ambition. I guess it‚Äôs fine as long as he doesn‚Äôt start playing music or TikTok videos without headphones.&lt;/p&gt;&lt;p&gt;A long blow from the locomotive horn, and the train begins to creep forward. Right on schedule. We‚Äôre in a tunnel deep below the city‚Äôs west side, and the view out the window is pitch black aside from the occasional glow from a mercury-vapor emergency light. On the wall beneath each of these lights, patches of graffiti framed by concrete pillars. I wouldn‚Äôt say I‚Äôve memorized them all by heart; I can‚Äôt even read the tags on the majority of them. But they are at least familiar, and I‚Äôve found some of them serve as convenient signposts along this portion of the trip. I‚Äôm not really paying attention to any of them tonight, instead I‚Äôm staring blankly at a little patch of window glass as the scene rolls past.&lt;/p&gt;&lt;p&gt;I refocus my eyes a bit and realize I‚Äôm looking at the reflection of a screen, or at least the top corner of one. I turn away from the window and find the source of the light. The man in front of me has opened his laptop‚Äîa chunky Dell Latitude or something very close to it‚Äîand perched it on a small lap desk fashioned from his leather bag. He opens a web browser and logs into a Microsoft account, one key at a time, hunt-and-peck style. It prompts him for his second factor and he shifts awkwardly in the seat to retrieve his phone. The login process succeeds and, after a few clicks and a fair bit of both of our finite lifetimes spent staring at loading spinners, opens what appears to be a Word document. I can‚Äôt read anything on his screen, which is more a testament to how wrecked my eyes have become than anything else, but I can see that there‚Äôs about four, perhaps five lines of unformatted text up there already. He strokes his chin while giving it a good read-through, then his hands take their position on the trackpad. Right index finger moves the cursor, left index finger does the clicking. The screen flips to another browser tab, his left hand gratuitously double-clicks on the website suggested by the first tile on the screen, and the page loads.&lt;/p&gt;&lt;p&gt;I never learned to tell any of these sites apart from each other. I see lots of people using the one with the spirograph logo. The one that looks like a cartoon butthole is also quite popular among some departments at my job. This guy is using the one that‚Äôs represented by a symmetrical color blob. Not that one, the other color blob one. Yeah.&lt;/p&gt;&lt;p&gt;He has opened a chat session that has evidently been going for some time. The text entry box at the bottom of the window waits patiently for fresh input. Letter by agonizing letter, the keys needed to express his thoughts are pressed. The most-pressed key, however, is Backspace. This man is, using the most generous language possible, not a particularly fast or accurate typist. In total, he enters about ten words before pressing Enter. A short moment later, the machine responds. Entire sentences appear in the time it took him to type a single word. Multiple paragraphs with subheadings and bulleted lists scroll into view. The screen fills completely with this fresh text. He looks at this for a moment, moves his hands back to the trackpad, and selects a complete paragraph. His finger presses down with immense force as he drags the selection area ever wider, as if his catch is in danger of wriggling through his fingers if he doesn‚Äôt hold the button down hard enough. He flips back to his Word document and pastes the paragraph. Then back to the chat window. He begins typing again. Slowly. Excruciatingly.&lt;/p&gt;&lt;p&gt;This cycle repeats several times, incrementally building his document up to four or five double-spaced pages in length. It‚Äôs not exactly a fast process, but certainly faster than if he had thought up and typed out all that content the old-fashioned way. It‚Äôs certainly plausible that he at least read everything that went into the document, but I wouldn‚Äôt be able to prove it.&lt;/p&gt;&lt;p&gt;He selects another piece of text, this one substantially smaller than the other specimens that he‚Äôd been handling up until this point. This one is pasted into a discussion thread on Teams. He waits a moment for responses, closes the lid, and the laptop goes back into his bag. The man stands up, wraps the strap over his shoulder, and walks to the front of the car as the train brakes to a full stop. This is where our paths diverge, it would seem. The doors open and he steps out into the night.&lt;/p&gt;&lt;p&gt;Alone in the train car again, with nothing interesting to eavesdrop on, my mind begins to wander again. I wonder what the purpose of that document was. Why was it being prepared? Who dictated that a half-dozen input phrases needed to be inflated into a thousand-word wall of text? Who was going to sit and read all of that, anyway? And for what purpose?&lt;/p&gt;&lt;p&gt;I really don‚Äôt know. But I do know one thing: It‚Äôs cat turds.&lt;/p&gt;&lt;p&gt;This guy obviously didn‚Äôt want to do that task. Whether that was due to lack of passion and interest, or lack of skill and ability, he had a cat turd to eat and he found a little pack of Altoids that he could use to get through it with minimal suffering. The people who have to read it? There‚Äôs a good chance they‚Äôll be dealing with a cat turd too. Maybe they can choose to employ a chatbot to summarize it back down to his original inputs. Maybe it‚Äôll even do a passable job preserving the essence of the guy‚Äôs prompts.&lt;/p&gt;&lt;p&gt;It makes sense why a person or group of people would flock to anything that makes life‚Äôs demands a little less difficult for themselves. You‚Äôd have to be pretty dumb to want to do a task like that manually.&lt;/p&gt;&lt;p&gt;There‚Äôs still the question, though. Why are we all eating cat turds? When did we all collectively agree that we were all a-okay with the idea that we had to subject ourselves to this constant grind of doing shit that doesn‚Äôt really need to be done to satisfy requirements that were put in place simply ‚Äúbecause‚Äù and that seemingly only create more pointless work for other people (or ourselves!) to have to do later?&lt;/p&gt;&lt;p&gt;One of the defining characteristics of humanity is its ability to build and wield tools that make difficult tasks easier. One would presume there would also be a certain wisdom in knowing which of the difficult tasks were worth doing in the first place but&amp;amp;mldr; Well. When you presume, you make a pres out of u and me.&lt;/p&gt;&lt;p&gt;If I had known ahead of time that I‚Äôd be out this late, I would‚Äôve brought a jacket. The early autumn air is calm but crisp, and my borough‚Äôs train platform offers very little protection from the chill. The crickets are still chirping, but their song has slowed substantially compared to how they sounded a few weeks back. I stopped parking at the station a long time ago‚Äîthe monthly pass costs well over $150 now, and most days the parking lot is completely full before six o‚Äôclock in the morning anyway. It‚Äôs only a mile to the house, and this twenty-minute walk is pretty much the only exercise I get nowadays.&lt;/p&gt;&lt;p&gt;Once I cross the main boulevard at the four-way stop, it‚Äôs all suburban residential side streets. There is basically no traffic at this time of night in my sleepy little bedroom community. All the dogs have been walked, the kids have been put to bed, and the adults&amp;amp;mldr; Well, I‚Äôm sure there are at least a couple people around here drinking or smoking the memory of their cat turds away.&lt;/p&gt;&lt;p&gt;I‚Äôm no closer to anything resembling inner peace. I find I‚Äôve grown to despise large swaths of the only thing I‚Äôve ever been able to earn reliable income from. I tire of walking a path that has seemingly shifted beneath my feet to point toward a destination I no longer recognize. I‚Äôm embarrassed by the jerk my Younger Self used to be, and simultaneously ashamed of the energy I lost as I matured. I don‚Äôt really want to do most of what I have to do, while feeling a deep unsated need to achieve something that I have neither the stamina nor the freedom to pursue. At some point I‚Äôm going to reach down deep into the well of ambition to discover there ain‚Äôt nothing there to pull out anymore. And then?&lt;/p&gt;&lt;p&gt;Something percent of success is simply showing up. That‚Äôs roughly how the quote goes, right? I‚Äôve heard seventy percent, ninety percent, hell, let‚Äôs call it seventy-eight. It doesn‚Äôt matter because it isn‚Äôt a real thing that can be measured in any objective way. The idea is to inspire people to at least try. Put your butt in the chair, log into Teams, trick yourself into thinking, well, I made it all the way here, might as well prune my stale Git branches or something so I can feel like I‚Äôm doing real work. Push aside distractions, shake off procrastination, kindle that tiny spark into enough momentum to break through whatever barrier is standing in the way of getting something done. If only that worked with any degree of predictability.&lt;/p&gt;&lt;p&gt;There‚Äôs a metaphor that talks about painting the backs of cabinets. The idea is that, when you‚Äôre putting paint, stain, varnish, whatever on some cabinets, there‚Äôs no need to paint the surfaces that face toward the wall. From the day the units are mounted, to a day forty years from now when they are ripped down and thrown into a construction dumpster during a subsequent kitchen renovation, nobody will see the backs of any of those cabinets. Painting them would be a waste of time and materials. Nobody would know if it was done or not.&lt;/p&gt;&lt;p&gt;‚ÄúYes, but I would know.‚Äù That‚Äôs something my Dad would often say. His tendency has always been to be overly thorough, exacting and precise in any craft he partakes in. Everything‚Äîfrom the doors in the house to the stripes cut into the front lawn‚Äîwas always level, plumb, square, centered, polished, dust-free, squeak-free, fingerprint-free&amp;amp;mldr; He even demonstrated meticulous care in breaking down cardboard and filling up the waste bins at the curb. I still have no idea how he was able to raise two kids in that house without exploding from the chaos we brought.&lt;/p&gt;&lt;p&gt;Maybe it was genetic, or maybe I voluntarily developed it so my dad would be proud of me just like he was proud of the other things he made. Either way, I definitely started to take after him in those ways and I now recognize this same kind of care in myself all the time. Not just in the way I prefer all my clocks to read the exact correct time or my knack for always noticing the way the receptacle face isn‚Äôt exactly flush with the wall plate&amp;amp;mldr; but in a fundamental inability to not care about quality or craft. Even when the task doesn‚Äôt matter. Even if it results in an entire afternoon spent painting a piece of carpentry that nobody will ever see. I can‚Äôt not care.&lt;/p&gt;&lt;p&gt;All that stuff Younger Self struggled with‚Äîthe self-superiority, the sense that I had to be the one who did it if it needed to be done correctly, the derision and borderline abuse I gave others‚Äîthat was all just a big dogmatic ball of caring a whole lot about quality and craft, being rolled around by a kid who didn‚Äôt understand what to do with it. I had to work so hard to care so much, and these other people didn‚Äôt, and everything worked out for them anyway, and that wasn‚Äôt fair. Decades later, I still feel that way sometimes.&lt;/p&gt;&lt;p&gt;My parents still live in that house, surrounded by all the things my dad cared so much about. Aside from a whole bunch of trees that died and needed to be cut down to stumps, everything is still pretty much pristine. But if you start to look around, really scrutinize, you‚Äôll start to notice some things have slipped. There‚Äôs a film of dust on the higher wall decorations. Some of the brass knobs are becoming tarnished. A few of the light bulbs in the hallway fixtures don‚Äôt match. My dad seemed tired the last time we talked, and more than once he expressed the sentiment that ‚Äúeverything he owns is falling apart.‚Äù Is it simply the onset of physical old age that has limited his ability to stay on top of these things, or is he beginning to leave behind his era of caring?&lt;/p&gt;&lt;p&gt;Now that I think about it, I don‚Äôt think we‚Äôve ever really talked about how care factored into his career philosophy. I had always implicitly assumed that it was the same as it currently is with me: Work or play, it‚Äôs always there. Can‚Äôt turn it off even if I wanted to. But what if he could? What if all the care he demonstrated in projects around the house was compensation for all the things he deliberately avoided caring about at work? It would certainly explain how he was able to consistently sustain those standards. But then, that would mean that I modeled my own principles and tastes on a distorted view of my dad, untempered by whatever he didn‚Äôt let me see about his workplace persona.&lt;/p&gt;&lt;p&gt;How would I begin to‚Äîwell, I don‚Äôt want to say ‚Äúnot care,‚Äù that sounds too extreme. But maybe&amp;amp;mldr; selectively care? To care about the things that matter, the things that spark passion and joy and remind me why I spent so much time practicing this godforsaken occupation. While at the same time recognizing the things that don‚Äôt matter, the problems for which the optimal solution is to stop insisting on having that problem in the first place. The kinds of tasks for which the 78% showing-up baseline score is plenty good enough. Tasks on which care would be utterly wasted, the cases where the cabinets are so irredeemably fucked up that the lack of paint on the back is the last thing anybody‚Äôs going to worry about. Those are the tasks that hurt the most, because I find it basically impossible to make myself care about them. It offends my soul to try to force it, and it drains me of all ambition to move onto the next potentially heartening opportunity. It‚Äôs a real problem, and I find it always has been: If I can‚Äôt care about it, I have an extremely hard time bringing myself to do it at all.&lt;/p&gt;&lt;p&gt;Well, I suppose that‚Äôs when I open a chatbot session of my own. ‚ÄúHey there Chat. Uh, we‚Äôve never spoken before but, uh&amp;amp;mldr; Well, my entire system of self-motivation just completely broke down but I still need to keep moving forward. Can you help me out of this bind?‚Äù There‚Äôs a whole discipline‚Äîthey call it Prompt Engineering‚Äîthat‚Äôs just a fancy form of throwing your hands up and pressing the Care About It For Me button. That‚Äôs pretty much how it works. Provide it with any cat turd under the sun, it doesn‚Äôt matter. Chat will gobble them all up for you like a coprophagic dog.&lt;/p&gt;&lt;p&gt;I‚Äôd be lying if I said the idea didn‚Äôt make my skin crawl a little. Every fiber of my being says that this is a weight to be borne by me and me alone. This is my cat turd to eat; they gave it to me. When it‚Äôs done, I can open my grinning maw and say without equivocation that I was the one who got through it. I painted the back of this cabinet. I worked way too hard and poured far too much of my blood, sweat and tears into this thing. And my reward for a job well done is&amp;amp;mldr; debilitating exhaustion, most of the time. Getting a fresh cat turd to eat tomorrow. And the day after.&lt;/p&gt;&lt;p&gt;Of course, Chat can‚Äôt really care. It does a passable job pretending like it cares, saying the words that convey the illusion of care to any reader not paying very close attention. Where do I draw the line between fostering real care, versus passing off a degraded third-generation photocopy of some tokenization of what may have at one point been somebody else‚Äôs care? Is the line simply the boundary between the tasks I‚Äôm excited to do and the ones I put off until I‚Äôve depleted enough mental reserves to sorta care?&lt;/p&gt;&lt;p&gt;It really does feel like the average person has made a choice to abandon a great deal of care, at least in their professional capacity. Take a look around at all these people with their fake shit-eating grins, passing off a machine‚Äôs effort as their own and experiencing no consequences. Sometimes they‚Äôre rewarded for doing so. There are organizations that are beginning to mandate it now. These people aren‚Äôt eating their cat turds anymore, why am I still sitting here eating mine?&lt;/p&gt;&lt;p&gt;I round the final curve leading to the corner of my block. As I pass under the streetlight, I cast a shadow on the asphalt ahead. With each step it grows longer and more distorted. There‚Äôs a rustle from the shrubs bordering my neighbor‚Äôs driveway, and a small dark form emerges. It crosses the street halfway then abruptly stops. I stop as well. A pair of glowing yellow eyes look back at me. I stare at it, it stares at me. A possum, perhaps? Somebody‚Äôs outdoor cat? It‚Äôs just watching me, seemingly peering straight into my very soul. Can it see what I‚Äôm grappling with here? Is it passing judgement on me for thinking these thoughts? It sizes me up for a moment longer, turns its head, and becomes a black apparition once more. I struggle to track it as it continues across the street, and I lose sight of it entirely.&lt;/p&gt;&lt;p&gt;I arrive at home and shut the door behind me. Sunset was over two hours ago and it‚Äôs nearly pitch black in the hallway. I fumble around for the light switch, kick my shoes off next to the doorway, and hang my bag on its hook in the coat closet. Something grabs my attention, just above eye level, slightly overhanging the edge of the top shelf. I slide it out of its resting place and carry it into the kitchen. I sit down at the table and inspect it.&lt;/p&gt;&lt;p&gt;This object is a round metal cookie tin about twelve inches in diameter. Beneath a thin coat of dust, it is a deep red with a repeating pattern of snowmen and white snowflakes, and quite obviously once held winter holiday‚Äìthemed cookies. I repurposed it many years ago to hold the only vice I currently permit myself to indulge in: a meticulously curated collection of all different types of chocolate candies. I remove the lid and set it aside. I survey the contents, a sea of differently-shaped naked chocolate morsels. I don‚Äôt remember why I chose to remove all the foil and paper wrapping before putting these in here. From my vantage point, everything looks vaguely the same‚ÄîI can‚Äôt readily spot any differences between milk chocolate and dark, or those filled with caramel versus cr√®me.&lt;/p&gt;&lt;p&gt;One particular piece near the edge catches my eye, and I carefully select it for inspection. It‚Äôs not a very pleasing color or shape‚Äîoddly asymmetrical. I roll it around between my fingers. There‚Äôs a hair on it. I hold it up to my nose and take a whiff, hoping to detect the aroma of the cacao. Try as I might, I can‚Äôt pick up any trace of its scent.&lt;/p&gt;&lt;p&gt;Come to think of it, I can‚Äôt remember the last time I smelled anything.&lt;/p&gt;¬´ Back to Articles&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45343449</guid><pubDate>Tue, 23 Sep 2025 06:24:50 +0000</pubDate></item><item><title>Telli (YC F24) is hiring ambitious engineers [Berlin, on-site]</title><link>https://hi.telli.com/join-us</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45343689</guid><pubDate>Tue, 23 Sep 2025 07:01:44 +0000</pubDate></item><item><title>Japan's most sacred Shinto shrine rebuilt every 20yrs for more than a millennium</title><link>https://apnews.com/article/japan-ise-sacred-shrine-rebuilt-destroyed-shinto-religion-5828f94e07da91f2ca9a12ea777b7b96</link><description>&lt;doc fingerprint="feaec157aee5c2b7"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Japan‚Äôs most sacred Shinto shrine has been rebuilt every 20 years for more than a millennium&lt;/head&gt;
    &lt;p&gt;Each generation, Ise Jingu, Japan‚Äôs most revered Shinto shrine, is knocked down and rebuilt from scratch. It takes the country‚Äôs finest carpenters, woodcutters, builders and craftsmen around nine years to complete the task. The buildings at Ise will only stand for about a decade before the process is started all over again. This ritual has happened every 20 years for 1,300 years.&lt;/p&gt;
    &lt;p&gt;ISE, Japan (AP) ‚Äî Deep in the forests of the Japanese Alps, Shinto priests keep watch as woodsmen dressed in ceremonial white chop their axes into two ancient cypress trees, timing their swings so that they strike from three directions.&lt;/p&gt;
    &lt;p&gt;An hour later, the head woodcutter shouts, ‚ÄúA tree is falling!‚Äù as one of the 300-year-old trees crashes down, the forest echoing with a deep crack. A moment after, the other cypress topples over.&lt;/p&gt;
    &lt;p&gt;The ritualistic harvesting of this sacred timber is part of a remarkable process that has happened every two decades for the last 1,300 years at Ise Jingu, Japan‚Äôs most revered Shinto shrine.&lt;/p&gt;
    &lt;p&gt;Each generation, the Ise complex is knocked down and rebuilt from scratch, a massive, $390 million demolition and construction job that takes about nine years. It requires the country‚Äôs finest carpenters, woodcutters, builders and artisans to pour their hearts into the smallest details of structures that are doomed from the moment the work begins.&lt;/p&gt;
    &lt;p&gt;The buildings at Ise will only stand for about a decade before the project starts all over again, but as the priests consecrate the construction, the workers shout: ‚ÄúA building for a thousand years! Ten thousand years! A million years and forever!‚Äù&lt;/p&gt;
    &lt;p&gt;Those close to the shrine often recognize a deep poignancy about the way the never-ending rebuilding intertwines with their lives.&lt;/p&gt;
    &lt;p&gt;‚ÄúTwenty years from now, the older generation ‚Äî our grandfathers ‚Äî will likely no longer be here. And those of us who are still young now will then see our grandchildren involved in the next‚Äù version of Ise, said Yosuke Kawanishi, a Shinto priest whose family company crafts miniature replicas of the shrine. ‚ÄúAfter 20 years, the shrine we are building will have deteriorated quite a bit. But instead of thinking, ‚ÄòIt‚Äôs a shame to tear down something we worked so hard to build,‚Äô we think, ‚ÄòIt‚Äôs been 20 years, so we want the deity to move into a beautiful, fresh, new shrine.‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;Journalists for The Associated Press are documenting the latest version of this ancient cyclical process, which publicly began this year.&lt;/p&gt;
    &lt;head rend="h2"&gt;Rebuilding the 125 shrine buildings is a 9-year process&lt;/head&gt;
    &lt;p&gt;This is the 63rd cycle of reconstruction. The first was documented in 690, during Empress Jit≈ç‚Äôs reign, said Noboru Okada, professor emeritus at Kogakkan University and a specialist in Japanese history and archeology.&lt;/p&gt;
    &lt;p&gt;All 125 shrine buildings will be knocked down and identical structures ‚Äî as well as more than 1,500 garments and other ritual objects used in the shrine ‚Äî will be rebuilt using techniques that have been painstakingly passed down over generations. There are 33 accompanying festivals and ceremonies, cumulating in a 2033 ritual that sees the presiding deity transferred to the new shrine.&lt;/p&gt;
    &lt;p&gt;Ise‚Äôs inner shrine is dedicated to the sun goddess Amaterasu who has been enshrined for two millennia among the mountains of Mie prefecture, on the banks of the Isuzu River.&lt;/p&gt;
    &lt;p&gt;Miori Inata, in a book based on a decade photographing Ise‚Äôs reconstruction, offers some theories about the constant rebuilding, including that the 20-year-cycle matches the shelf-life of stored rice or the traditional two-decade phases that make up a human lifespan ‚Äî birth to adulthood, adulthood to middle age, middle age to death.&lt;/p&gt;
    &lt;p&gt;Inata writes of the culminating rites marking a new shrine: ‚ÄúI was greatly moved by the realization that what was transpiring before my eyes were precisely the same ceremonies that were performed 1,300 years ago, every 20 years since, and will continue to unfold again and again in the future.‚Äù&lt;/p&gt;
    &lt;p&gt;The rebuilding was stopped only twice, during the civil wars of the 15th and 16th centuries, and after World War II, according to Yukio Lippit, a professor of art history and architecture at Harvard.&lt;/p&gt;
    &lt;p&gt;‚ÄúIse is unique because of attrition ‚Äî renewal cycles are difficult to maintain ‚Äî and because of the vagaries of history; many other shrines that once underwent regular rebuilding have stopped doing so,‚Äù Lippit said.&lt;/p&gt;
    &lt;head rend="h2"&gt;Priests ask mountain deities for permission to fell trees&lt;/head&gt;
    &lt;p&gt;During a recent downpour, priests in starched robes banged drums and marched to Ise‚Äôs inner shrines for prayers marking the beginning of the age-old rebuilding process.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe world where we live and the mountain realm are separate, distinct worlds. Therefore, when people go onto the mountain to cut trees or gather plants, they must first receive permission from the mountain deities,‚Äù according to Okada, the historian.&lt;/p&gt;
    &lt;p&gt;Thousands gather to see the rebuilding ceremonies, part of about 7 million pilgrims a year who converge on the shrine, which has long been the polestar for Shinto devotees. Japan‚Äôs indigenous Shinto faith, which also acts as a cultural connection for family and community, is largely rooted in animism. In Shinto there are thousands of ‚Äúkami,‚Äù or spirits, that inhabit the world. While Ise thrives, the number of Shinto shrines has plummeted in recent decades as Japan‚Äôs population shrinks and young people increasingly move from the countryside to megacities.&lt;/p&gt;
    &lt;p&gt;‚ÄúYou can count with one hand the number of times you‚Äôll witness something like this in your lifetime, so I really felt it was a rare and precious sight,‚Äù said Yuto Nakase, who was viewing the ceremonies for the first time.&lt;/p&gt;
    &lt;p&gt;At night the priests assemble with lanterns and march to the mountains for a secret purification rite for a sacred pillar that will be enshrined beneath the floor of the main sanctuary.&lt;/p&gt;
    &lt;p&gt;The ceremony is off-limits to spectators, but shrine officials say that after the tree is cut down with a special axe, it is wrapped in white cloth, straw mats and reed mats.&lt;/p&gt;
    &lt;p&gt;Visitors often mention Ise‚Äôs deep sense of mystery.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt doesn‚Äôt say much, doesn‚Äôt show much and doesn‚Äôt offer much explanation. It‚Äôs something you feel,‚Äù Kawanishi, the Shinto priest, said of the shrine.&lt;/p&gt;
    &lt;p&gt;Yoriko Maeda, who owns a local sake shop, recognizes a transformation the moment she crosses a bridge into the shrine grounds.&lt;/p&gt;
    &lt;p&gt;‚ÄúMy breathing changes,‚Äù she said. ‚ÄúIt really feels different. What I sense also changes. The sounds, the wind or nature, seem to release my stress. ‚Ä¶ There‚Äôs a kind of depth there that, for me, makes it a very comforting and pleasant space.‚Äù&lt;/p&gt;
    &lt;head rend="h2"&gt;Tree-cutting ceremonies show keen attention to detail&lt;/head&gt;
    &lt;p&gt;In the forests of Nagano prefecture, a woodcutter takes the tip of a freshly felled tree and inserts it into the stump of another tree that has just been cut down. The assembled woodcutters then pray and bow together in front of the stump, commemorating these special cypresses that will be used to rebuild Ise.&lt;/p&gt;
    &lt;p&gt;‚ÄúIt honors the continuity of a tree‚Äôs life and is a prayer for the regeneration of the forest,‚Äù explains Soju Ikeda, who operates a local lumber company and manages a society for the preservation of traditional tree-felling skills. ‚ÄúYou take a moment to appreciate that trees are living beings and engrave that feeling into your heart.‚Äù&lt;/p&gt;
    &lt;p&gt;Over the following days, dozens of men dressed in traditional clothing drag the two-ton logs through the Isuzu River to the shrine, chanting rhythmically as they pull, knee-deep in the water.&lt;/p&gt;
    &lt;p&gt;At Ise there are ten carpenters‚Äô studios in permanent residence, plus others who are brought in, Lippit, the Harvard professor, said. The miscanthus reed thatch for the shrine‚Äôs roofs is specially grown to a length of over 2 meters; this takes about eight years and is timed for the rebuilding.&lt;/p&gt;
    &lt;p&gt;Cypress groves are specially planted at Ise for the constant construction, and their cultivation often exceeds individual human lifespans, with responsibilities for the trees passed from generation to generation.&lt;/p&gt;
    &lt;p&gt;Asked about his relationship to the cypress trees that are cultivated for the shrine, Ikeda, the lumber expert, had a one-word answer: ‚ÄúDeep.‚Äù&lt;/p&gt;
    &lt;p&gt;Forty years ago, when he was 24, he drove his grandfather to participate in the tree-felling ceremony. ‚ÄúHe said to me, ‚ÄòDo you know that the trees cry?‚Äô&lt;/p&gt;
    &lt;p&gt;‚ÄúI answered, ‚ÄòNo way, how could a tree cry?‚Äô‚Äù&lt;/p&gt;
    &lt;p&gt;But as they watched woodsmen chop down the cypress, ‚Äúthe sound of the axes echoed across the mountains, and after about an hour, when the axe struck the core of the tree, the scent of the cypress filled the air, flowing like blood,‚Äù he said.&lt;/p&gt;
    &lt;p&gt;At the final axe stroke, as the wood snapped, ‚Äúthe sound it made was like a shriek, a high-pitched ‚Äòkeee‚Äô sound, and then the tree fell with a thunderous thud. In that moment, I thought, ‚ÄòAh... it really cried.‚Äô I felt as if the tree wept, mourning its own life, as if it knew its life was precious.‚Äù&lt;/p&gt;
    &lt;p&gt;___&lt;/p&gt;
    &lt;p&gt;AP photographer Hiro Komae contributed to this report.&lt;/p&gt;
    &lt;p&gt;___&lt;/p&gt;
    &lt;p&gt;Associated Press religion coverage receives support through the AP‚Äôs collaboration with The Conversation US, with funding from Lilly Endowment Inc. The AP is solely responsible for this content.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45344198</guid><pubDate>Tue, 23 Sep 2025 08:11:01 +0000</pubDate></item></channel></rss>