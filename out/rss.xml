<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 31 Jan 2026 15:12:27 +0000</lastBuildDate><item><title>Ashcan Comic</title><link>https://en.wikipedia.org/wiki/Ashcan_comic</link><description>&lt;doc fingerprint="a0693c125a527c1"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Ashcan comic&lt;/head&gt;&lt;p&gt;An ashcan comic is a form of the American comic book created solely to establish trademarks on potential titles and not intended for sale. The practice was common in the 1930s and 1940s when the comic book industry was in its infancy, but was phased out after updates to US trademark law. The term was revived in the 1980s by Bob Burden, who applied it to prototypes of his self-published comic book. Since the 1990s, the term has been used to describe promotional materials produced in large print runs and made available for mass consumption. In the film and television industries, the term "ashcan copy" has been adopted for low-grade material created to preserve a claim to licensed property rights.&lt;/p&gt;&lt;head rend="h2"&gt;Original use&lt;/head&gt;[edit]&lt;p&gt;The modern comic book was created in the 1930s, and grew rapidly in popularity.[2] In the competition to secure trademarks on titles intended to sound thrilling, publishers including All-American Publications and Fawcett Comics developed the ashcan edition,[3] which was the same size as regular comics and usually had a black and white cover.[3] Typically, cover art was recycled from previous publications with a new title pasted to it.[4] Interior artwork ranged from previously published material in full color[3][4] to unfinished pencils without word balloons.[5] Some ashcans were only covers with no interior pages.[6] Production quality on these works range from being hand-stapled with untrimmed pages to machine-stapled and machine trimmed.[6] Once the practice was established, DC Comics used ashcans more frequently than any other publisher.[6] Not all the titles secured through ashcan editions were actually used for regular publications.[7]&lt;/p&gt;&lt;p&gt;The purpose of the ashcan editions was to fool the US Patent &amp;amp; Trademark Office into believing the book had actually been published.[8] Clerks at the office would accept the hastily produced material as legitimate, granting the submitting publisher a trademark to the title.[9][10] Since the ashcans had no other use, publishers printed as few as two copies; one was sent to the Trademark Office, the other was kept for their files.[11] Occasionally, publishers would send copies to distributors or wholesalers by registered mail to further establish publication dates,[4] but nearly all ashcan comic editions were limited to five copies or fewer.[3]&lt;/p&gt;&lt;p&gt;At the time, garbage cans were commonly called "ash cans" because they were used to hold ashes and soot from wood and coal heating systems.[5] The term was applied to these editions of comics because they had no value and were meant to be thrown away after being accepted by the Trademark Office.[5][8] Some spare copies were given to editors, employees, and visitors to keep as souvenirs.[6][8] Changes to the United States trademark law in 1946 allowed publishers to register a trademark with an intent to use instead of a finished product,[6] and the practice of creating and submitting ashcans was abandoned when publishers began to consider it an unnecessary effort lawyers used to justify a fee.[7] Because of their rarity, ashcans from this era are desired by collectors and often fetch a high price.[5] In April 2021, an ashcan copy of Action Comics #1 sold for US$204,000.[12]&lt;/p&gt;&lt;head rend="h2"&gt;Later use&lt;/head&gt;[edit]&lt;p&gt;In 1984, Golden Age comic book collector and dealer Bob Burden created Flaming Carrot Comics, published by Aardvark-Vanaheim.[3][8][13] For each issue, Burden had magazine-sized prototype editions printed and shared them with friends and people who assisted with production.[3] Some were also sent to retailers as advance previews to generate interest in his comic.[8] Fewer than forty copies of each prototype were made, and Burden described them as ashcans.[3]&lt;/p&gt;&lt;p&gt;In 1992, comic creator Rob Liefeld applied the term to two digest-sized prototype versions of Youngblood #1, but this ashcan was created for mass release. Instead of denoting the material as worthless, Liefeld's usage implied rarity and collectability.[3] This ashcan was the first publication from Image Comics, a publisher started by popular artists during a boom period for comic books. The sales success of the Youngblood ashcans prompted imitation, and for the next year nearly every new Image series had a corresponding ashcan.[8] Typical print run for Image ashcans was between 500 and 5,000. Soon, other publishers began releasing ashcans in a variety of sizes and formats.[3] In 1993, Triumphant Comics advertised ashcan editions of new properties with print runs of 50,000.[14]&lt;/p&gt;&lt;p&gt;Following the collapse of the speculation market in comics in the mid-1990s, the term has been used by publishers to describe booklets promoting upcoming comics.[9] Established publishers such as Dark Horse Comics, IDW Publishing, and DC Comics continue to use ashcan copies as part of their marketing plans for new titles.[15][16][17] Aspiring creators also apply the term to hand-stapled photocopied books they use to demonstrate their abilities to hiring editors at comic book conventions or as part of a submissions package.[18]&lt;/p&gt;&lt;head rend="h2"&gt;Film and television&lt;/head&gt;[edit]&lt;p&gt;The term has been appropriated by the film and television industries to refer to low-quality material made specifically to preserve rights to a licensed character, which often expire if unused for a set period of time. One of the earliest examples of this practice is the 1967 animated adaptation of The Hobbit.[19] Other prominent examples include the unreleased Fantastic Four film from 1994,[20] the 2004 direct-to-video film My Name is Modesty,[21] the low-budget sequels Porky's Pimpin' Pee Wee (2009) and Hellraiser: Revelations (2011), and a TV pilot adaptation of The Wheel of Time (2015).[19]&lt;/p&gt;&lt;head rend="h2"&gt;See also&lt;/head&gt;[edit]&lt;list rend="ul"&gt;&lt;item&gt;Burning off, the airing of otherwise-abandoned television programs in less desirable time slots or on sister networks, often for contractual or legal reasons.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;References&lt;/head&gt;[edit]&lt;list rend="ol"&gt;&lt;item&gt;^ Hembeck, Fred (June 18, 2003). "Johnny Thunder and Shazam!". The Hembeck Files. Retrieved June 22, 2005.&lt;/item&gt;&lt;item&gt;^ Ramsey, Taylor (February 5, 2013). "The History of Comics: Decade by decade". The Artifice. Retrieved June 23, 2018.&lt;/item&gt;&lt;item&gt;^ a b c d e f g h i Christensen, William A; Seifert, Mark (February 1994). "Evolution of the Ashcan". Wizard. No. 30. New York City: Wizard Entertainment. p. 89.&lt;/item&gt;&lt;item&gt;^ a b c Seifert, Mark (August 1, 2014). "75 Years Ago Today: DC Comics Wins The Race For Flash". Bleeding Cool. Avatar Press. Retrieved June 19, 2018.&lt;/item&gt;&lt;item&gt;^ a b c d Malito, Alessandra (October 9, 2017). "As fans throng to New York Comic Con, these comic books sell for millions of dollars". MarketWatch. Retrieved December 11, 2017.&lt;/item&gt;&lt;item&gt;^ a b c d e Colabuono, Gary (September 1999). "Absolutely Amazing DC Ashcans". Comic Book Marketplace. Vol. 2, no. 71. Coronado, CA: Gemstone Publishing, Inc. p. 24.&lt;/item&gt;&lt;item&gt;^ a b Hamerlinck, Paul; Daigh, Ralph (2001). Fawcett Companion: The Best of FCA. Raleigh, NC: TwoMorrows Publishing. p. 107. ISBN 9781893905108.&lt;/item&gt;&lt;item&gt;^ a b c d e f Colabuono, Gary (July 1993). "Ashcan Comics". Hero Illustrated. No. 1. Lombard, Illinois: Warrior Publications. p. 56.&lt;/item&gt;&lt;item&gt;^ a b "Historic DC Ashcan Comic Books in Heritage New York Auction". CGC Comics. January 31, 2012. Archived from the original on June 26, 2018. Retrieved June 19, 2018.&lt;/item&gt;&lt;item&gt;^ "Historic DC Ashcan comic books in Heritage Auctions February 22 New York event". Art Daily. Retrieved June 15, 2019.&lt;/item&gt;&lt;item&gt;^ Cox, Brian L (August 13, 2011). "Rare 'ashcan' comic books on display at Chicago Comic Con". Chicago Tribune. Retrieved December 11, 2017.&lt;/item&gt;&lt;item&gt;^ Action Comics #1 Ashcan Sells For $204,000 At Auction at Bleeding Cool. April 19, 2021 by Rich Johnston.&lt;/item&gt;&lt;item&gt;^ Mallette, Jack (November 1986). "Bob Burden (part 1)". Comics Interview. No. 40. Fictioneer Books. pp. 22‚Äì41.&lt;/item&gt;&lt;item&gt;^ "Advertisement". Hero Illustrated. No. 2. Lombard, Illinois: Warrior Publications. August 1993. p. 125.&lt;/item&gt;&lt;item&gt;^ Dietsch, TJ (April 12, 2012). "C2E2: Bobby Curnow Unleashes "Battle Beasts" at IDW". Comic Book Resources. Retrieved July 2, 2018.&lt;/item&gt;&lt;item&gt;^ Johnston, Rich (April 20, 2017). "Let's All Read The Dark Matter/Master Class Ashcan From DC Comics". Bleeding Cool. Avatar Press. Retrieved July 2, 2018.&lt;/item&gt;&lt;item&gt;^ Johnston, Rich (March 12, 2015). "Dark Horse Sends Fight Club 2, Rebels And Archie Vs. Predator". Bleeding Cool. Avatar Press. Retrieved June 25, 2018.&lt;/item&gt;&lt;item&gt;^ Hart, Christopher (2014). Drawing Cutting Edge Anatomy. Potter/Ten Speed/Harmony/Rodale. ISBN 9780770434861.&lt;/item&gt;&lt;item&gt;^ a b Wirestone, Clay (March 13, 2015). "The Weird History of the "Ashcan Copy"". Mental Floss. Retrieved June 19, 2018.&lt;/item&gt;&lt;item&gt;^ Ito, Robert (March 2005). "Fantastic Faux!". Los Angeles. p. 108. Retrieved January 1, 2012.&lt;/item&gt;&lt;item&gt;^ Sherman, Dale (2015). Quentin Tarantino FAQ: Everything Left to Know About the Original Reservoir Dog. Rowman &amp;amp; Littlefield Publishers. ISBN 9781495025976.&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46821488</guid><pubDate>Fri, 30 Jan 2026 07:26:20 +0000</pubDate></item><item><title>Surely the crash of the US economy has to be soon</title><link>https://wilsoniumite.com/2026/01/27/surely-it-has-to-be-soon/</link><description>&lt;doc fingerprint="76ec1011ecb30ded"&gt;
  &lt;main&gt;
    &lt;p&gt;Last year I predicted there would be a significant (2008+) economic crash that year. The year is now 2026 and I was wrong. At the time my main argument was essentially this:&lt;/p&gt;
    &lt;p&gt;The unemployment rate just follows these smooth curves, covid was an exception, and it was due to jump again. Not very scientific I know. There was another important graph of course:&lt;/p&gt;
    &lt;p&gt;Where classically an inverted yield curve has been a recession predictor. This one is a little more involved, but essentially the US government borrows money and normally what makes sense is that the government needs to pay more money every year in interest if it wants to borrow the money for longer. If, for some reason, the market says ‚Äúno actually we will take a lower fee if you take our money for longer‚Äù that is an inverted yield curve, and here that is shown by the difference between the interest on a 10 year loan and a 2 year loan being negative. Why this would predict market crashes is a complex topic and I encourage you to read around about it. It isn‚Äôt perfect of course, and one ‚Äúfeature‚Äù is that it isn‚Äôt ‚Äúwrong‚Äù yet, at least unless we don‚Äôt get a crash within the next few years.&lt;/p&gt;
    &lt;p&gt;But come on.&lt;/p&gt;
    &lt;p&gt;It surely will be this year.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs the current price of silver. Gold looks kinda similar (but smoother, I chose silver because it looks dramatic, but maybe it got you to read this further so that‚Äôs a win in my book). People buy precious metals when they might be worried about the value of fiat currencies, like, I don‚Äôt know, the dollar. Are people worried about the dollar?&lt;/p&gt;
    &lt;p&gt;To be honest I‚Äôm glad we are the ones getting out of that market first. Why might people be worried about the dollar?&lt;/p&gt;
    &lt;p&gt;Eh, I don‚Äôt know, there are lots of possible reasons and maybe you can think of some. The actual point is this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;US government debt has been a worry for a while. That worry doesn‚Äôt matter so long as people have faith in it, but it does matter insofar as it makes a possible debt crisis deeper. The bigger they are, the harder they fall.&lt;/item&gt;
      &lt;item&gt;There are one or more bubbles in the stock market. Almost everyone agrees that AI is a bubble. It funds itself in a circular fashion, and capex cannot be recovered with profits any time soon, even with optimistic outlooks. Other stocks may also be well overvalued, with sky high PEs and nonsensical business models (meme stocks are just the worst offenders)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It feels as though all we need is a spark. And yet, many sparks seem to have come and gone. Big market moves, in stocks or yields, that have recovered. Tariff and invasion threats, protests, you name it, they might move the needle but it always seems to move back. So, perhaps we won? Perhaps we built our markets so stable that they are these days impervious? That sounds silly on its face, and the two reasons I‚Äôd actually give are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Markets are just slower moving than ever before, big players just like to sit on their big piles of money, and it‚Äôs much easier to just assume the needle will go back and then everyone pats you on the back when it does. No client likes a skittish fund manager that ends up always being wrong&lt;/item&gt;
      &lt;item&gt;This is the 11th time that tariffs have happened, and it just isn‚Äôt surprising anymore.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Which is to say that no individual decision make want‚Äôs to be the first mover, so the market does not move.&lt;/p&gt;
    &lt;p&gt;A year ago there were a few signs. Right now, it feels like everything is primed to blow. Is that new? Do I always just feel that way? Am I just a broken clock that‚Äôs going to be right today? Maybe, but I damn well intend to be right at some point.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46822630</guid><pubDate>Fri, 30 Jan 2026 10:14:23 +0000</pubDate></item><item><title>BoldVoice (YC S21) Is Hiring Fullstack and Machine Learning Engineers</title><link>https://boldvoice.notion.site/careers-page?p=2e871a9bf729806c81f6e47f32e32622&amp;pm=s</link><description>&lt;doc fingerprint="10f452a104a33a8"&gt;
  &lt;main&gt;
    &lt;p&gt;JavaScript must be enabled in order to use Notion. Please enable JavaScript to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823430</guid><pubDate>Fri, 30 Jan 2026 12:00:12 +0000</pubDate></item><item><title>Pangolin (YC S25) is hiring software engineers (open-source, Go, networking)</title><link>https://docs.pangolin.net/careers/join-us</link><description>&lt;doc fingerprint="578d4934f0eed0f"&gt;
  &lt;main&gt;
    &lt;div&gt;Skip to main content&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;&lt;div&gt;We are looking for talented engineers to join our team and help build secure remote access. If you‚Äôre passionate about open-source software, networking, and security, we‚Äôd love to hear from you.&lt;head rend="h2"&gt;About Pangolin&lt;/head&gt; Pangolin delivers identity-aware remote access to internal apps and services. Our platform replaces legacy VPNs and simplifies secure access to infrastructure, applications, and developer environments. We build in the open and are self‚Äëhosted by default so teams retain control over data and infrastructure. The system is policy‚Äëdriven, integrates with standard IdPs, exposes clear observability and health, and provides an API for automation. If you‚Äôre interested in open-source auth and networking infrastructure, we‚Äôd love to chat. &lt;head rend="h2"&gt;Open Roles&lt;/head&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46823544</guid><pubDate>Fri, 30 Jan 2026 12:11:49 +0000</pubDate></item><item><title>HTTP Cats</title><link>https://http.cat/</link><description>&lt;doc fingerprint="edf6e74fda190558"&gt;
  &lt;main&gt;
    &lt;p&gt;https://http.cat/[status_code]&lt;/p&gt;
    &lt;p&gt;Note: If you need an extension at the end of the URL just add .jpg.&lt;/p&gt;
    &lt;p&gt;.jpg&lt;/p&gt;
    &lt;p&gt;Continue&lt;/p&gt;
    &lt;p&gt;Switching Protocols&lt;/p&gt;
    &lt;p&gt;Processing&lt;/p&gt;
    &lt;p&gt;Early Hints&lt;/p&gt;
    &lt;p&gt;OK&lt;/p&gt;
    &lt;p&gt;Created&lt;/p&gt;
    &lt;p&gt;Accepted&lt;/p&gt;
    &lt;p&gt;Non-Authoritative Information&lt;/p&gt;
    &lt;p&gt;No Content&lt;/p&gt;
    &lt;p&gt;Reset Content&lt;/p&gt;
    &lt;p&gt;Partial Content&lt;/p&gt;
    &lt;p&gt;Multi-Status&lt;/p&gt;
    &lt;p&gt;Already Reported&lt;/p&gt;
    &lt;p&gt;Transformation Applied&lt;/p&gt;
    &lt;p&gt;IM Used&lt;/p&gt;
    &lt;p&gt;Multiple Choices&lt;/p&gt;
    &lt;p&gt;Moved Permanently&lt;/p&gt;
    &lt;p&gt;Found&lt;/p&gt;
    &lt;p&gt;See Other&lt;/p&gt;
    &lt;p&gt;Not Modified&lt;/p&gt;
    &lt;p&gt;Use Proxy&lt;/p&gt;
    &lt;p&gt;Temporary Redirect&lt;/p&gt;
    &lt;p&gt;Permanent Redirect&lt;/p&gt;
    &lt;p&gt;Bad Request&lt;/p&gt;
    &lt;p&gt;Unauthorized&lt;/p&gt;
    &lt;p&gt;Payment Required&lt;/p&gt;
    &lt;p&gt;Forbidden&lt;/p&gt;
    &lt;p&gt;Not Found&lt;/p&gt;
    &lt;p&gt;Method Not Allowed&lt;/p&gt;
    &lt;p&gt;Not Acceptable&lt;/p&gt;
    &lt;p&gt;Proxy Authentication Required&lt;/p&gt;
    &lt;p&gt;Request Timeout&lt;/p&gt;
    &lt;p&gt;Conflict&lt;/p&gt;
    &lt;p&gt;Gone&lt;/p&gt;
    &lt;p&gt;Length Required&lt;/p&gt;
    &lt;p&gt;Precondition Failed&lt;/p&gt;
    &lt;p&gt;Payload Too Large&lt;/p&gt;
    &lt;p&gt;Request-URI Too Long&lt;/p&gt;
    &lt;p&gt;Unsupported Media Type&lt;/p&gt;
    &lt;p&gt;Request Range Not Satisfiable&lt;/p&gt;
    &lt;p&gt;Expectation Failed&lt;/p&gt;
    &lt;p&gt;I√¢m a teapot&lt;/p&gt;
    &lt;p&gt;Page Expired&lt;/p&gt;
    &lt;p&gt;Enhance Your Calm&lt;/p&gt;
    &lt;p&gt;Misdirected Request&lt;/p&gt;
    &lt;p&gt;Unprocessable Entity&lt;/p&gt;
    &lt;p&gt;Locked&lt;/p&gt;
    &lt;p&gt;Failed Dependency&lt;/p&gt;
    &lt;p&gt;Too Early&lt;/p&gt;
    &lt;p&gt;Upgrade Required&lt;/p&gt;
    &lt;p&gt;Precondition Required&lt;/p&gt;
    &lt;p&gt;Too Many Requests&lt;/p&gt;
    &lt;p&gt;Request Header Fields Too Large&lt;/p&gt;
    &lt;p&gt;No Response&lt;/p&gt;
    &lt;p&gt;Blocked by Windows Parental Controls&lt;/p&gt;
    &lt;p&gt;Unavailable For Legal Reasons&lt;/p&gt;
    &lt;p&gt;SSL Certificate Error&lt;/p&gt;
    &lt;p&gt;SSL Certificate Required&lt;/p&gt;
    &lt;p&gt;HTTP Request Sent to HTTPS Port&lt;/p&gt;
    &lt;p&gt;Token expired/invalid&lt;/p&gt;
    &lt;p&gt;Client Closed Request&lt;/p&gt;
    &lt;p&gt;Internal Server Error&lt;/p&gt;
    &lt;p&gt;Not Implemented&lt;/p&gt;
    &lt;p&gt;Bad Gateway&lt;/p&gt;
    &lt;p&gt;Service Unavailable&lt;/p&gt;
    &lt;p&gt;Gateway Timeout&lt;/p&gt;
    &lt;p&gt;Variant Also Negotiates&lt;/p&gt;
    &lt;p&gt;Insufficient Storage&lt;/p&gt;
    &lt;p&gt;Loop Detected&lt;/p&gt;
    &lt;p&gt;Bandwidth Limit Exceeded&lt;/p&gt;
    &lt;p&gt;Not Extended&lt;/p&gt;
    &lt;p&gt;Network Authentication Required&lt;/p&gt;
    &lt;p&gt;Web Server Is Down&lt;/p&gt;
    &lt;p&gt;Connection Timed Out&lt;/p&gt;
    &lt;p&gt;Origin Is Unreachable&lt;/p&gt;
    &lt;p&gt;SSL Handshake Failed&lt;/p&gt;
    &lt;p&gt;Site Frozen&lt;/p&gt;
    &lt;p&gt;Network Connect Timeout Error&lt;/p&gt;
    &lt;p&gt;Developed by @rogeriopvl&lt;/p&gt;
    &lt;p&gt;Original Images by Tomomi Imura (@girlie_mac)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46824422</guid><pubDate>Fri, 30 Jan 2026 13:56:51 +0000</pubDate></item><item><title>Kimi K2.5 Technical Report [pdf]</title><link>https://github.com/MoonshotAI/Kimi-K2.5/blob/master/tech_report.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46826597</guid><pubDate>Fri, 30 Jan 2026 16:43:50 +0000</pubDate></item><item><title>Antirender: remove the glossy shine on architectural renderings</title><link>https://antirender.com/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829147</guid><pubDate>Fri, 30 Jan 2026 20:05:24 +0000</pubDate></item><item><title>Peerweb: Decentralized website hosting via WebTorrent</title><link>https://peerweb.lol/</link><description>&lt;doc fingerprint="868e3ff18d2cd634"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ü™ê PeerWeb&lt;/head&gt;
    &lt;head rend="h2"&gt;Decentralized Website Hosting via WebTorrent&lt;/head&gt;
    &lt;head rend="h3"&gt;ü§î What is PeerWeb?&lt;/head&gt;
    &lt;p&gt;PeerWeb is a revolutionary way to host and share websites using WebTorrent technology. Instead of relying on centralized servers, websites are distributed across a peer-to-peer network, making them censorship-resistant and always available. üåç‚ú®&lt;/p&gt;
    &lt;head rend="h3"&gt;üì§ Quick Upload&lt;/head&gt;
    &lt;head rend="h4"&gt;Drag &amp;amp; Drop Your Website&lt;/head&gt;
    &lt;p&gt;Drop a folder with your website files&lt;/p&gt;
    &lt;head rend="h3"&gt;üìö How to Use PeerWeb&lt;/head&gt;
    &lt;head rend="h3"&gt;üí° Load Existing Site&lt;/head&gt;
    &lt;p&gt;To load a website from a torrent hash, enter it below:&lt;/p&gt;
    &lt;p&gt;üéØ Just the hash! PeerWeb automatically adds the magnet link prefix and trackers.&lt;/p&gt;
    &lt;head rend="h3"&gt;üß™ Demos&lt;/head&gt;
    &lt;p&gt; Functionality test page: &lt;lb/&gt;https://peerweb.lol/?orc=90c020bd252639622a14895a0fad713b91e0130c &lt;/p&gt;
    &lt;p&gt; SomaFM on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=908d19242ae1461f333a516d1f8b89c13ef2d259 &lt;/p&gt;
    &lt;p&gt; Chess on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=1e14b1ba7fcd03e5f165d53ed8223a333349db04 &lt;/p&gt;
    &lt;p&gt; Text Editor app on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=4e5f1204dcec68195bfcc89f9410a0b70a0ddfac &lt;/p&gt;
    &lt;head rend="h3"&gt;üêõ Debug Mode&lt;/head&gt;
    &lt;p&gt;For developers and troubleshooting, add &amp;amp;debug=true to see detailed progress:&lt;/p&gt;
    &lt;code&gt;https://peerweb.lol?orc=ABC123DEF456...&amp;amp;debug=true&lt;/code&gt;
    &lt;head rend="h3"&gt;üöÄ Advanced Options&lt;/head&gt;
    &lt;head rend="h3"&gt;üíæ Smart Caching&lt;/head&gt;
    &lt;p&gt;PeerWeb caches visited sites for lightning-fast loading! üöÄ&lt;/p&gt;
    &lt;head rend="h3"&gt;üõ°Ô∏è Security Features&lt;/head&gt;
    &lt;p&gt;Enhanced security with DOMPurify integration! üîí&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829582</guid><pubDate>Fri, 30 Jan 2026 20:40:00 +0000</pubDate></item><item><title>Stonebraker on CAP theorem and Databases (2010)</title><link>https://perspectives.mvdirona.com/2010/04/stonebraker-on-cap-theorem-and-databases/</link><description>&lt;doc fingerprint="d53793bb302c2f86"&gt;
  &lt;main&gt;
    &lt;p&gt;Mike Stonebraker published an excellent blog posting yesterday at the CACM site: Errors in Database Systems, Eventual Consistency, and the CAP Theorem. In this article, Mike challenges the application of Eric Brewer‚Äôs CAP Theorem by the NoSQL database community. Many of the high-scale NoSQL system implementers have argued that the CAP theorem forces them to go with an eventual consistent model. &lt;/p&gt;
    &lt;p&gt;Mike challenges this assertion pointing that some common database errors are not avoided by eventual consistency and CAP really doesn‚Äôt apply in these cases. If you have an application error, administrative error, or database implementation bug that losses data, then it is simply gone unless you have an offline copy. This, by the way, is why I‚Äôm a big fan of deferred delete. This is a technique where deleted items are marked as deleted but not garbage collected until some days or preferably weeks later. Deferred delete is not full protection but it has saves my butt more than once and I‚Äôm a believer. See On Designing and Deploying Internet-Scale Services for more detail.&lt;/p&gt;
    &lt;p&gt;CAP and the application of eventual consistency doesn‚Äôt directly protect us against application or database implementation errors. And, in the case of a large scale disaster where the cluster is lost entirely, again, neither eventual consistency nor CAP offer a solution. Mike also notes that network partitions are fairly rare. I could quibble a bit on this one. Network partitions should be rare but net gear continues to cause more issues than it should. Networking configuration errors, black holes, dropped packets, and brownouts, remain a popular discussion point in post mortems industry-wide. I see this improving over the next 5 years but we have a long way to go. In Networking: the Last Bastion of Mainframe Computing, I argue that net gear is still operating on the mainframe business model: large, vertically integrated and expensive equipment, deployed in pairs. When it comes to redundancy at scale, 2 is a poor choice.&lt;/p&gt;
    &lt;p&gt;Mike‚Äôs article questions whether eventual consistency is really the right answer for these workloads. I made some similar points in ‚ÄúI love eventual consistency but‚Ä¶‚Äù In that posting, I argued that many applications are much easier to implement with full consistency and full consistency can be practically implemented at high scale. In fact, Amazon SimpleDB recently announced support for full consistency. Apps needed full consistency are now easier to write and, where only eventual consistency is needed, its available as well.&lt;/p&gt;
    &lt;p&gt;Don‚Äôt throw full consistency out too early. For many applications, it is both affordable and helps reduce application implementation errors.&lt;/p&gt;
    &lt;p&gt;‚Äìjrh&lt;/p&gt;
    &lt;p&gt;Thanks to Deepak Singh for pointing me to this article.&lt;/p&gt;
    &lt;p&gt;b: http://blog.mvdirona.com / http://perspectives.mvdirona.com &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46831592</guid><pubDate>Fri, 30 Jan 2026 23:47:28 +0000</pubDate></item><item><title>Show HN: I trained a 9M speech model to fix my Mandarin tones</title><link>https://simedw.com/2026/01/31/ear-pronunication-via-ctc/</link><description>&lt;doc fingerprint="2d64271217290110"&gt;
  &lt;main&gt;
    &lt;p&gt;TL;DR: Mandarin pronunciation has been hard for me, so I took ~300 hours of transcribed speech and trained a small CTC model to grade my pronunciation. You can try it here.&lt;/p&gt;
    &lt;p&gt;In my previous post about Langseed, I introduced a platform for defining words using only vocabulary I had already mastered. My vocabulary has grown since then, but unfortunately, people still struggle to understand what I'm saying.&lt;/p&gt;
    &lt;p&gt;Part of the problem is tones. They're fairly foreign to me, and I'm bad at hearing my own mistakes, which is deeply frustrating when you don‚Äôt have a teacher.&lt;/p&gt;
    &lt;head rend="h2"&gt;First attempt: pitch visualisation&lt;/head&gt;
    &lt;p&gt;My initial plan was to build a pitch visualiser: split incoming audio into small chunks, run an FFT, extract the dominant pitch over time, and map it using an energy-based heuristic, loosely inspired by Praat.&lt;/p&gt;
    &lt;p&gt;But this approach quickly became brittle. There were endless special cases: background noise, coarticulation, speaker variation, voicing transitions, and so on.&lt;/p&gt;
    &lt;p&gt;And if there‚Äôs one thing we‚Äôve learned over the last decade, it‚Äôs the bitter lesson: when you have enough data and compute, learned representations usually beat carefully hand-tuned systems.&lt;/p&gt;
    &lt;p&gt;So instead, I decided to build a deep learning‚Äìbased Computer-Assisted Pronunciation Training (CAPT) system that could run entirely on-device. There are already commercial APIs that do this, but hey, where‚Äôs the fun in that?&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;I treated this as a specialised Automatic Speech Recognition (ASR) task. Instead of just transcribing text, the model needs to be pedantic about how something was said.&lt;/p&gt;
    &lt;p&gt;I settled on a Conformer encoder trained with CTC (Connectionist Temporal Classification) loss.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Conformer?&lt;/head&gt;
    &lt;p&gt;Speech is weird: you need to catch both local and global patterns:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Local interactions&lt;/p&gt;&lt;lb/&gt;The difference between a retroflex zh and an alveolar z happens in a split second. CNNs are excellent at capturing these short-range spectral features.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Global interactions&lt;/p&gt;&lt;lb/&gt;Mandarin tones are relative (a "high" pitch for me might be low for a child) and context-dependent (tone sandhi)1. Transformers excel at modeling this longer-range context.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Conformers combine both: convolution for local detail, attention for global structure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why CTC?&lt;/head&gt;
    &lt;p&gt;Most modern ASR models (e.g. Whisper) are sequence-to-sequence: they turn audio into the most likely text. The downside is they'll happily auto-correct you.&lt;/p&gt;
    &lt;p&gt;That‚Äôs a feature for transcription, but it‚Äôs a bug for language learning. If my tone is wrong, I don‚Äôt want the model to guess what I meant. I want it to tell me what I actually said.&lt;/p&gt;
    &lt;p&gt;CTC works differently. It outputs a probability distribution for every frame of audio (roughly every 40 ms). To handle alignment, it introduces a special &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt; token.&lt;/p&gt;
    &lt;p&gt;If the audio is "hello", the raw output might look like:&lt;/p&gt;
    &lt;code&gt;h h h &amp;lt;blank&amp;gt; e e &amp;lt;blank&amp;gt; l l l l &amp;lt;blank&amp;gt; l l o o o
&lt;/code&gt;
    &lt;p&gt;Collapsing repeats and removing blanks gives &lt;code&gt;hello&lt;/code&gt;. This forces the model has to deal with what I actually said, frame by frame.&lt;/p&gt;
    &lt;head rend="h2"&gt;Forced alignment: knowing when you said it&lt;/head&gt;
    &lt;p&gt;CTC tells us what was said, but not exactly when.&lt;/p&gt;
    &lt;p&gt;For a 3-second clip, the model might output a matrix with ~150 time steps (columns), each containing probabilities over all tokens (rows). Most of that matrix is just &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If the user reads "N«ê h«éo" (ni3, hao3), we expect two regions of high probability: one for &lt;code&gt;ni3&lt;/code&gt;, one for &lt;code&gt;hao3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We need to find a single, optimal path through this matrix that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Starts at the beginning&lt;/item&gt;
      &lt;item&gt;Ends at the end&lt;/item&gt;
      &lt;item&gt;Passes through &lt;code&gt;ni3&lt;/code&gt;‚Üí&lt;code&gt;hao3&lt;/code&gt;in order&lt;/item&gt;
      &lt;item&gt;Maximises total probability&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is exactly what the Viterbi algorithm computes, using dynamic programming.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tokenisation: Pinyin + tone as first-class tokens&lt;/head&gt;
    &lt;p&gt;Most Mandarin ASR systems output Hanzi. That hides pronunciation errors, because the writing system encodes meaning rather than pronunciation.&lt;/p&gt;
    &lt;p&gt;Instead, I created a token for every Pinyin syllable + tone:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;zhong1&lt;/code&gt;is one token&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;zhong4&lt;/code&gt;is a completely different token&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If I say the wrong tone, the model explicitly predicts the wrong token ID.&lt;/p&gt;
    &lt;p&gt;I also normalised the neutral tone by forcing it to be tone 5 (&lt;code&gt;ma5&lt;/code&gt;). This resulted in a vocabulary of 1,254 tokens, plus &lt;code&gt;&amp;lt;unk&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Training&lt;/head&gt;
    &lt;p&gt;I combined the AISHELL-1 and Primewords datasets (~300 hours total), augmented by SpecAugment (time/frequency masking). On 4√ó NVIDIA GeForce RTX 4090s, training took about 8 hours. Instead of obsessing over loss, I mostly focused on these metrics:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;TER (Token Error Rate): overall accuracy.&lt;/item&gt;
      &lt;item&gt;Tone Accuracy: accuracy over tones 1-5.&lt;/item&gt;
      &lt;item&gt;Confusion Groups: errors between difficult initial pairs like zh/ch/sh vs z/c/s.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Honey, I shrank the model&lt;/head&gt;
    &lt;p&gt;I started with a "medium" model (~75M parameters). It worked well, but I wanted something that could run in a browser or on a phone without killing the battery.&lt;/p&gt;
    &lt;p&gt;So I kept shrinking it, and I was honestly surprised by how little accuracy I lost:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;# Parameters&lt;/cell&gt;
        &lt;cell role="head"&gt;TER&lt;/cell&gt;
        &lt;cell role="head"&gt;Tone accuracy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;75M&lt;/cell&gt;
        &lt;cell&gt;4.83%&lt;/cell&gt;
        &lt;cell&gt;98.47%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;35M&lt;/cell&gt;
        &lt;cell&gt;5.16%&lt;/cell&gt;
        &lt;cell&gt;98.36%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;9M&lt;/cell&gt;
        &lt;cell&gt;5.27%&lt;/cell&gt;
        &lt;cell&gt;98.29%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The 9M-parameter model was barely worse. This strongly suggests the task is data-bound, not compute-bound.&lt;/p&gt;
    &lt;p&gt;The FP32 model was ~37 MB. After INT8 quantisation, it shrank to ~11 MB with a negligible accuracy drop (+0.0003 TER). Small enough to load instantly via &lt;code&gt;onnxruntime-web&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Alignment bug: silence ruins everything&lt;/head&gt;
    &lt;p&gt;To highlight mistakes, we need forced alignment. But I hit a nasty bug with leading silence.&lt;/p&gt;
    &lt;p&gt;I recorded myself saying "ÊàëÂñúÊ¨¢‚Ä¶" and paused for a second before speaking. The model confidently told me my first syllable was wrong. Confidence score: 0.0.&lt;/p&gt;
    &lt;p&gt;Why?&lt;/p&gt;
    &lt;p&gt;The alignment assigned the silent frames to &lt;code&gt;wo3&lt;/code&gt;. When I averaged probabilities over that span, the overwhelming &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt; probability completely drowned out &lt;code&gt;wo3&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;The fix&lt;/head&gt;
    &lt;p&gt;I decoupled UI spans (what gets highlighted) from scoring frames (what contributes to confidence).&lt;/p&gt;
    &lt;p&gt;We simply ignore frames where the model is confident it‚Äôs seeing silence:&lt;/p&gt;
    &lt;code&gt;def _filter_nonblank_frames(span_logp: torch.Tensor, blank_id: int = 0, thr: float = 0.7):
    """
    Only keep frames where the probability of &amp;lt;blank&amp;gt; is below a threshold.
    If we filter everything (total silence), we fall back to scoring the whole span.
    """
    p_blank = span_logp[:, blank_id].exp()
    keep = p_blank &amp;lt; thr
    if keep.any():
        return span_logp[keep]
    return span_logp  # Fallback
&lt;/code&gt;
    &lt;p&gt;This single change moved my confidence score for the first syllable from 0.0 ‚Üí 0.99.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I can already feel my pronunciation improving while beta testing this. It‚Äôs strict and unforgiving, exactly what I needed.&lt;/p&gt;
    &lt;p&gt;Native speakers, interestingly, complained that they had to over-enunciate to get marked correct. That‚Äôs likely a domain-shift issue: AISHELL is mostly read speech, while casual speech is faster and more slurred. Kids do poorly too: their pitch is higher, and they're basically absent from the training data. Adding conversational datasets like Common Voice feels like the obvious next step.&lt;/p&gt;
    &lt;p&gt;You can try the live demo here. It runs entirely in your browser. The download is ~13MB, still smaller than most websites today.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;For example, Tone 3 followed by Tone 3 is pronounced as Tone 2 followed by Tone 3 (‰Ω†Â•Ω ‚Üí n√≠ h«éo). ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46832074</guid><pubDate>Sat, 31 Jan 2026 00:51:27 +0000</pubDate></item><item><title>Naples' 1790s civil war was intensified by moral panic over Real Analysis (2023)</title><link>https://lareviewofbooks.org/article/foundational-anxieties-modern-mathematics-and-the-political-imagination/</link><description>&lt;doc fingerprint="af48931038ab4553"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Foundational Anxieties, Modern Mathematics, and the Political Imagination&lt;/head&gt;
    &lt;head rend="h2"&gt;Massimo Mazzotti uses a forgotten episode in revolutionary Naples to demonstrate the entanglement of mathematics and politics.&lt;/head&gt;
    &lt;head rend="h3"&gt;By Massimo MazzottiJune 2, 2023&lt;/head&gt;
    &lt;head rend="h4"&gt;Did you know LARB is a reader-supported nonprofit?&lt;/head&gt;
    &lt;p&gt;LARB publishes daily without a paywall as part of our mission to make rigorous, incisive, and engaging writing on every aspect of literature, culture, and the arts freely accessible to the public. Help us continue this work with your tax-deductible donation today!&lt;/p&gt;
    &lt;p&gt;This essay is adapted from Massimo Mazzotti‚Äôs 2023 book Reactionary Mathematics: A Genealogy of Purity, available now from the University of Chicago Press.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;A FORGOTTEN EPISODE in French-occupied Naples in the years around 1800‚Äîjust after the French Revolution‚Äîillustrates why it makes sense to see mathematics and politics as entangled. The protagonists of this story were gravely concerned about how mainstream mathematical methods were transforming their world‚Äîsomewhat akin to our current-day concerns about how digital algorithms are transforming ours. But a key difference was their straightforward moral and political reading of those mathematical methods. By contrast, in our own era we seem to think that mathematics offers entirely neutral tools for ordering and reordering the world‚Äîwe have, in other words, forgotten something that was obvious to them.&lt;/p&gt;
    &lt;p&gt;In this essay, I‚Äôll use the case of revolutionary Naples to argue that the rise of a new and allegedly neutral mathematics‚Äîcharacterized by rigor and voluntary restriction‚Äîwas a mathematical response to pressing political problems. Specifically, it was a response to the question of how to stabilize social order after the turbulence of the French Revolution. Mathematics, I argue, provided the logical infrastructure for the return to order. This episode, then, shows how and why mathematical concepts and methods are anything but timeless or neutral; they define what ‚Äúreason‚Äù is, and what it is not, and thus the concrete possibilities of political action. The technical and political are two sides of the same coin‚Äîand changes in notions like mathematical rigor, provability, and necessity simultaneously constitute changes in our political imagination.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;In 1806, the Kingdom of Naples was occupied by a French army and integrated into Napoleon‚Äôs imperial system. The French and their local supporters had a clear agenda: they wanted to transform the semifeudal society into a centralized administrative monarchy with a liberal economy. This ambitious plan, however, soon ran up against obdurate realities like muddy roads, brigandage, popular insurgencies, and the thinly disguised hostility of powerful local elites. There was another problem, too. Open a Neapolitan university textbook of the time and you will see that the French had to fight their battles in a land where their mathematics was wrong.&lt;/p&gt;
    &lt;p&gt;While armed and cultural resistance against the French invaders‚Äô imperial ambitions happened in other parts of Europe, the Neapolitan case is particularly interesting because it includes a mathematical resistance. This resistance took the form of a distinctive mathematical culture that was hegemonic in that kingdom for several decades‚Äîfrom the late 1790s to the 1830s. Contemporaries called it the Neapolitan synthetic school. The name referred to synthetic (or pure) geometry, a geometry that does not use coordinates and algebraic formulas to study figures and solve problems. Leading Neapolitan mathematicians embraced it as the veritable foundation of all mathematics. Only its methods and assumptions, they believed, could be trusted.&lt;/p&gt;
    &lt;p&gt;What the Neapolitans most adamantly did not trust was what they called, not without irony, the ‚Äúvery modern mathematics.‚Äù This body of knowledge, associated mainly with France, was characterized by the rapid advancements of an algebraized form of infinitesimal calculus and by its stunning and far-reaching practical applications. It had severed its connections with Euclidean geometry, and was referred to as ‚Äúanalysis‚Äù‚Äîa term that, in this context, meant a vast array of algebraic methods and algorithmic procedures that could be used to represent how things change, whether those things were, say, the trajectory of a cannonball or agricultural productivity.&lt;/p&gt;
    &lt;p&gt;Driven by eminently practical goals, analysis had become highly abstract: a versatile tool that could be applied to describe and control natural and social phenomena. The Neapolitans were quite sure it was morally suspect‚Äîa degenerate form of knowledge, and dangerous for the stability of society. Its proliferation across Europe and globally was, to them, an unmitigated disaster. While these allegations sound extravagant to our ears, some of their concerns resonate with ones in this century‚Äîe.g., about how even programmers who fashion certain complex algorithms do not understand their inner workings or why they come to the conclusions they do. Synthetics, for instance, pointed out that the analysts prioritized practical success over understanding: they aimed to model phenomena using algebraic tools that they could not fully justify, neither through some form of intellectual intuition nor through logic. By contrast, synthetic geometers clarified and grounded every single step of their procedures. They often used metaphors of sight to make this point: synthetic geometry allowed practitioners to see with clarity, and this is why their results could be trusted; analysts were blind when they manipulated their formulas. Using another set of metaphors: Analysts followed the fast flights of their feverish and uncontrolled imagination, while synthetics kept their feet on the ground. Their procedures were slow but safe.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;The mathematical 19th century was suffused with a distinctive foundational anxiety. We can see an early and radical manifestation of this anxiety in revolutionary Naples‚Äîin its bizarre and apparently backward attempt to return to a Greek-like pure geometry. The champion of this new old mathematics was Nicola Fergola (1753‚Äì1824), the charismatic and mystically inclined leader of a group of mathematicians and scientists who understood themselves as the last heirs of an ancient tradition, a tradition that was now under attack and needed to be defended.&lt;/p&gt;
    &lt;p&gt;Skeptics, however, understood the synthetic school‚Äôs mathematical resistance as backwardness, the rearguard action of a group of isolated practitioners. And yet, Fergola‚Äôs puzzling quest for purity was something more. For one thing, there was no established tradition of synthetic geometry worth defending in Naples. The tradition Fergola invoked was largely an invention‚Äîan imaginary mathematical lineage that ran through ancient Greece, late antiquity, and Christian Europe, all the way down to these self-proclaimed final paladins. In fact, Fergola, who was well aware of recent mathematical developments, breathed new life into forgotten mathematical techniques. Neapolitan synthetic mathematics, in other words, was not a remnant of the past, but a new way of understanding mathematics, characterized by new canons of rigor and founded on a core of ‚Äúpure‚Äù mathematics. In the name of a mythical tradition, it imposed a new discipline on its practitioners by emphasizing self-restraint as the key epistemic virtue. Fergola, his admirers reported, was a champion of self-control: he controlled his body, passions, and imagination, and knew well when to stop trusting his own mathematical techniques.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;The Neapolitans did not reject modern analysis simply because they considered it French. What triggered their anxiety were its technical features‚Äîthe way it worked. Following the example of mathematicians like Condorcet, analysts were aiming to create a repertoire of finite and infinite algebraic methods that were abstract and general enough to apply to any kind of problem, be it in geometry, physics, economics, or even politics. This zealous quest for universal problem-solving algorithms is precisely what made the synthetics uneasy. Interestingly, the analysts themselves knew well that the manipulation of these algorithms‚Äîthe way they produced results‚Äîwas not grounded in either geometric intuition or logical arguments. What warranted their use, they believed, was that algebraic procedures mirrored the fundamental workings of the human mind. If, as the analysts believed, the mind was an analytic machine, then analysis was the quintessential expression of human reason‚Äîand, as such, isomorphic to nature: analysis was effective because it mirrored the deep structures of reality. This was a mathematics essentially interwoven with the world of experience‚Äîone that, as d‚ÄôAlembert had written, ‚Äúgives us the most perfect examples of the manner in which one should use the art of reasoning.‚Äù&lt;/p&gt;
    &lt;p&gt;When asked to solve a geometric problem, the analysts would find an appropriate system of coordinates that would allow them to turn figures into algebraic formulas, then would manipulate these formulas to obtain the ‚Äúsolving equation,‚Äù as they called it. They would interpret the solutions as solutions to the original geometric problem. Their operations had thus shifted from geometry to algebra. Was this a legitimate move? The synthetics would say that it was legitimate only when they could see the geometry behind the formulas. But for complex problems this was not always possible, and in these instances algebra was blind; there was no way to reconstruct the geometrical meaning of the algebraic operations that led to the solution. It followed that the nature of the problem had changed. For the analysts, this was irrelevant: algebra captured the essential relations expressed by the terms of the problem, which then served to guide the mathematician toward the solution. For the synthetics, by contrast, a solution to the original geometrical problem could only be geometrical in nature; and so, what the analysts were offering were not solutions but meaningless numbers.&lt;/p&gt;
    &lt;p&gt;While the analysts strove for maximum generality, the synthetics argued for the specificity and locality of all mathematical methods. They saw this as a question of jurisdiction: there are many different ways of reasoning and many different methods, and they all have their legitimate function and scope. It would be illusory‚Äîand deceitful‚Äîto try to solve a geometrical problem using purely algebraic methods, or a political problem using the methods of geometry. Even more misleading would be to believe that there is a single universal method that can be applied to all kinds of problems. The synthetics‚Äô world was, so to speak, epistemologically stratified. They recognized many kinds of truth, and thought it essential to keep them separated from one another. The truth of the geometer, they claimed, has nothing to do with the truths of the theologian, historian, or politician.&lt;/p&gt;
    &lt;p&gt;These two mathematical cultures differed sharply in the way they conceived mathematical reasoning. For the synthetics, mathematical knowledge was the product of a process of recognition, the imperfect representation of metaphysical states of affairs that the gifted mathematician would be able to glimpse. Their teaching reflected this view: a close-knit school with an inner circle of students who worked with their maestro, engaging in an endless reflection on geometrical problems received from antiquity. For the analysts, mathematical reasoning was just a particular case of analytic reasoning‚Äîcalculus, especially, was where analytic reason could be best seen in action. They saw themselves as the standard bearers of modernization and the promoters of rational action across both scientific and social life. For them, mathematical training was a matter of learning to frame problems analytically and then solving them following a set of standard procedures. It was not a matter of intellectual intuition, gift, and genius. On the contrary, anyone, with proper training, could become an effective problem-solver.&lt;/p&gt;
    &lt;p&gt;Analysts enthusiastically compared their method to the clunky workings of a machine: to them, the machine was an emblem of rational thinking. It was also a way of arguing for the algorithmic nature and therefore accessibility of the method, as its standardized procedures could be easily learned, and deployed across different contexts. At the core of their science was not the ‚Äúpure and simple knowledge of truths,‚Äù they argued, but the knowledge of methods and their relative ‚Äústrength‚Äù in getting useful results, including approximate ones, through the sheer power of calculation. The synthetics countered that results needed always to be precise and perfectly interpretable. The analysts were teaching their students blind methods that deformed their young minds; they were turning them into automata‚Äîsoulless, machinelike number-crunchers‚Äîwho ignored at their peril the meaning of the formulas they were manipulating. Well before the dawn of digital computing, the questions of the meaning of formal procedures and of the social implications of their extensive use were at the core of a debate that, using the language of morals, addressed basic questions of social order.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;Why did some mathematicians perceive logical gaps in analysis in the years around 1800, and why did they consider them unbearable? It turns out that anxieties about the foundations of analysis were growing even among its supporters. By the 1820s, these anxieties were spreading across Europe. Consider Augustin-Louis Cauchy (1789‚Äì1857), who played a key role in the creation of modern mathematics as we know it. His much-celebrated revolution had less to do with his specific contributions and more with his overall transformation of mathematics as a discipline.&lt;/p&gt;
    &lt;p&gt;He brought order to the world of mathematics. He modernized it by imposing rigor. Mathematicians in the 18th century had achieved stunning results in algebra and infinitesimal calculus, but to Cauchy‚Äôs eyes, they had been too casual in how they defined their concepts and devised and applied their methods. This blithe attitude needed to end, he declared, and a new Euclidean spirit‚Äîa spirit of rigor‚Äîneeded to replace it. Cauchy was not interested in bringing back synthetic geometry. Rather, he aimed to reinterpret analysis within a new logical framework in which every concept and procedure would be logically justified. Cauchy‚Äôs program of rigorization redefined the meaning of mathematical techniques, providing precise definitions and limits for the application of each method. He set boundaries, in other words, within which certain techniques could be legitimately deployed. The modern mathematicians were those who, following Cauchy, could discipline themselves through a new kind of technical precision.&lt;/p&gt;
    &lt;p&gt;Cauchy‚Äôs rigorous analysis seems distant from Fergola‚Äôs Greek-like geometry. But it was shaped by the same foundational anxiety and urgency to restore order to a mathematical world that‚Äîin the views of both men‚Äîhad gone badly astray. What we learn from the comparison is that the fundamental opposition was not between geometry and algebra but between mathematics as a pure, rigorous, self-contained, and reliable body of knowledge and mathematics as a set of highly general and universally applicable algorithmic procedures expressing an all-encompassing analytic rationality. The fact that Fergola tried literally to reinstate geometry at the core of mathematics while Cauchy injected a rigorous Euclidean spirit within analysis was more about their local conditions than anything else. What really mattered is that both programs promised a return to mathematical order.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;Fergola and his students were initially marginal to Neapolitan scientific life. Their geometrical program was perceived as outdated, while the world of the salons scoffed at their baroque religious devotion and ascetic lifestyle. But this changed dramatically after the storming of the Bastille, when they quickly acquired an unprecedented cultural relevance. The Neapolitan government would take a decisive anti-French stance, reorient its entire cultural politics accordingly, and align itself with the Catholic Church. Within the Church itself, enlightened reformism gave way to a Jesuitic religiosity that reactivated baroque devotions and mobilized popular piety to support the alliance of throne and altar.&lt;/p&gt;
    &lt;p&gt;In 1794, the discovery of a Jacobin conspiracy to overthrow the monarchy sent the court into a state of panic. The Jacobins would succeed five years later, in 1799, when Naples became a republic. The leading revolutionaries were mathematicians. The chief conspirator of 1794, and the first president of the republic, Carlo Lauberg (1752‚Äì1834), was a teacher of chemistry and mathematics. It is no accident that almost every noteworthy figure in Neapolitan Jacobinism received some mathematical training: a basic understanding of analysis was an essential part of their worldview, as were republicanism, egalitarianism, and anticlericalism. The very structure of their secret society‚Äîa network of Jacobin clubs‚Äîwas a working model of how analysis could be deployed in matters of social organization.&lt;/p&gt;
    &lt;p&gt;Neapolitan Jacobins aimed to find universal methods to address pressing social and political problems‚Äîabove all, the problems of political representation and wealth redistribution. By the mid-1790s, they had become convinced that their vision of a just and equal society could be realized only through the universal implementation of analysis, which they understood as a revolutionary mathematics. Analysis was already being applied to the natural sciences and now, they said, it was time to apply it to the science of society as well, and to political matters. The analytic method would turn the art of politics into a science, replacing tradition, prejudice, and private interests with rational decision-making. To apply analysis to politics meant to reduce it to its elementary components, study their relations, and use algebraic procedures to intervene. This would detach politics from its metaphysical assumptions, turning it into a matter of rational and transparent administration. The analytic revolution could now be expected to transform society by making it possible to operationalize ‚Äúthe will of the people.‚Äù&lt;/p&gt;
    &lt;p&gt;Neapolitan Jacobins thus took the tools and basic assumptions of analysis and turned them into a militant mathematics‚Äîthe veritable ‚Äúbackbone of society.‚Äù A programmatically impure mathematics, it was a universal language and reasoning style that could be applied across disciplinary boundaries to bring about immediate social change. In fact, mathematics and politics merged seamlessly in the lived experience of these Jacobins, who saw themselves as agents of change, able to escape the logic of reform and the apparent fatalities of history.&lt;/p&gt;
    &lt;p&gt;The counterrevolutionaries reacted by turning these analytic features into the ‚ÄúJacobin machine,‚Äù a deadly device for the control of public opinion, political life, and the state. The metaphor emphasized discipline, organization, and the capacity for control, but also gestured toward the extraneous and polluting character of what was described as a set of manipulation techniques. In Naples, the Jacobin machine was viewed as foreign, disconnected from local political traditions. But in France too, its effect was seen as one of contamination, this time from the inside. In both cases, the purity of the body politic had to be defended from a malignant mechanical-analytic threat.&lt;/p&gt;
    &lt;p&gt;The breathtaking adventure of the Jacobin Republic, characterized by sweeping plans for popular education and redistribution of wealth, ended abruptly five months after it started, in June 1799, when British, Russian, and Turkish forces joined a local counterrevolutionary army and stormed the walls of Naples. In its aftermath, about 120 prominent Jacobins were put on show trials and executed. Fergola had moved to the countryside during these months. A biographer reported that he could not stand ‚Äúthe noise‚Äù of the city. When he returned, he and his students were asked to reorganize scientific life in the university and in the entire system of public education. Education, which had been ‚Äúinfected‚Äù by the Jacobins, a royal dispatch read, now needed to be brought back to its ancient order.&lt;/p&gt;
    &lt;p&gt;Fergola, a celibate vegetarian who found the presence of women extremely unpleasant, was a tormented man. He was not someone who could easily fit into the salons of Enlightenment Naples. His religiosity was deliberately untimely and baroque. He chose the most anti-modern and anti-rationalist religiosity, the popular piety of the Neapolitan crowds, when this religiosity was under attack. A spate of crying and bleeding Madonnas signaled the crisis of a subaltern agrarian world that would soon explode in massive counterrevolutionary insurgencies. Embracing popular religion in the 1790s meant embracing it as resistance. In general terms, it was a resistance against the modern state‚Äôs secularized and rational principles of organization. In this sense, Fergola‚Äôs public display of his rosary and bloodied scourge was a politicized act of resistance. And in this sense, his mathematics, too, was a politicized act of resistance.&lt;/p&gt;
    &lt;p&gt;Fergola suffered numerous and often inexplicable ‚Äúorganic‚Äù and ‚Äúmoral‚Äù ailments that progressively hampered his activity. Yet, we are told, he gazed serenely at his sore body as if that flesh was not his own: his entire life could be recounted as a triumph of spirit over matter. Exhausted by mysterious convulsions and prostrated by horrific demonic visions, Fergola felt that his faith was constantly put to the test. But even as his own health failed, his school prospered. To the end, he railed about the degeneration of learning and the ‚Äúsacrilegious horde,‚Äù which included Freemasons, Jacobins, liberals, and even some of his legitimist colleagues.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;With the French occupation of 1806, the synthetic school had to face, once again, the threat of analysis. The world, however, had changed. Most surviving Jacobins had come to terms with the Napoleonic normalization. The synthetics controlled the university, but outside of it, analysis thrived, constituting the core training in the new schools of engineering fostered by the French. The continuity, however, was mostly apparent. Many former revolutionaries, in France as in Naples, had turned the question of modernization into a technical problem, and had refashioned their personas and social function in terms of scientific neutrality and technocratic efficiency. Historian Ken Alder has aptly labeled them ‚Äútechno-Jacobins.‚Äù In this normalized context, mathematics was a neutral tool, the distinctive expertise of technical elites who served the state. The direct connection between mathematics, egalitarianism, and republicanism, built through the notion of a universal analytic reason, had been severed, and with it vanished the very possibility of a revolutionary mathematics. But reframing modernization as a technical rather than a political problem meant detaching analysis‚Äôs formal tools from their original source of legitimation. Intriguingly, the political choice of reframing modernization in exclusively technical terms had produced a profound and pervasive mathematical problem.&lt;/p&gt;
    &lt;p&gt;Led by Fergola‚Äôs students, the synthetic school fought against the technical elites of the modern state, mostly civil engineers and statisticians, for scientific hegemony. The old regime had long been imploding in Naples, but a new order struggled to consolidate itself. The French had arrived in Naples with a promise of order through modernization, and had found receptive interlocutors in the landed elites who could most benefit from the abolition of the feudal-communal system. The new technical experts had been charged with changing the kingdom‚Äôs physical and social landscape accordingly. Technical disciplines such as statistics or topography became key sites for negotiation, collaboration, and conflict between landed elites and the central government. On this technical terrain, the new experts would continuously clash with the synthetics.&lt;/p&gt;
    &lt;p&gt;It is not a coincidence that the synthetic school faded into oblivion when the reactionary position lost ground as a viable political option. From the 1820s onward, what was really at stake was the form of the new relations between the centralized administrative state, the landed elites, and the largely dispossessed peasant masses. Analysis had morphed into a set of allegedly neutral administrative tools, and the controversy between analytics and synthetics, which had long defined Neapolitan academic life, became increasingly meaningless. The technicians who supported the state‚Äôs modernizing action now argued for a mathematical reconciliation. What the two groups were defending, it was now believed, were simply two different ways of looking at mathematics, which should not be seen as opposed to each other but rather as complementary. The synthetics approach was useful for didactic purposes, while the analytic one was best suited for research and the discovery of new mathematical truths. This compromise was an elegant way of disposing of what, at that point, was an embarrassing anomaly for Neapolitan science. This normalized reconstruction eliminated revolutionary and reactionary scientific aberrations, emphasized continuity in the history of mathematics, and aligned with the political life of Restoration-age Naples, which was hegemonized by new landed elites and their liberal and constitutional ambitions. Emblematic of this cultural climate was the success of philosophical positions grounded on consciousness, which insisted that, within certain limits, individual reason was autonomous and legislative, and that fighting for liberty of conscience coincided with fighting for political and economic liberty.&lt;/p&gt;
    &lt;p&gt;The mathematical controversy between synthetics and analytics had been a controversy about the nature of reason all along. At stake was reason‚Äôs nature and limits. The Jacobin‚Äôs analytic reason was universal, active, calculative, individual, a priori, and ahistorical; it was a completely autonomous reason that, when not obstructed, could truthfully describe and legitimately change the world‚Äîthrough revolutionary action, if necessary. The reason of the synthetic, by contrast, was local, passive, intuitive, collective, a posteriori, and eminently suited to historical thinking; it was a dependent reason, whose outcomes needed to be warranted by external sources of legitimation like tradition, custom, experience, religion, and metaphysical principles. It was, as such, a reactionary reason that envisioned the return to order as a return to hierarchy‚Äîorder produced by subordination. This reactionary reason was a militant reason, its arguments forged in battle.&lt;/p&gt;
    &lt;p&gt;It is only by contrast to an abhorred revolutionary reason, political theorist Corey Robin reminds us, that the invocation of ancient forms of wisdom can captivate the modern mind. The image of reason that emerged with the consolidation of the modern liberal state valued individual reason while acknowledging its clear limits. The principles of a liberal economy and the new relationships of subordination between landed elites and peasant masses were not to be questioned. The autonomy of individual reason was celebrated against prerevolutionary obscurantism and absolutism, but it was only a relative autonomy, to be exercised within the boundaries of postfeudal order. The newly rigorous mathematics and the constitutional project set those boundaries with precision. If absolutism was a thing of the past, so should be revolutionary anarchy.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;Logical inference, Wittgenstein quipped, is how we refer to what we do not intend to question. The case of the Neapolitan mathematical resistance should not be seen as one in which mathematics was temporarily distorted by politics. When we craft logico-mathematical concepts and techniques, we design ways of ordering the natural and social world. These ways of ordering the world open up certain possibilities for action‚Äîincluding political action‚Äîwhile closing down others. They discriminate between what is visible, plausible, and logical, and what is none of those things. Jacobin mathematics was deployed to critique and radically transform the existing social order, empowering traditionally subordinate social groups and bringing them into the space of politics as legitimate autonomous agents. The mathematics of the synthetics was designed to deny this possibility, to turn it, in fact, into a logical impossibility‚Äîhence it was, strictly speaking, a reactionary mathematics.&lt;/p&gt;
    &lt;p&gt;Modern mathematics, as it took shape with Cauchy and those who continued his program, was constitutive of the postrevolutionary political normalization. It was the logico-mathematical infrastructure of the new moderate liberal discourse. It retained analysis‚Äôs operative orientation while embracing the synthetics‚Äô quest for a foundational core of mathematical knowledge. The image of reason it embodied was bounded and self-disciplined, and while it legitimated a neutral, instrumental technical dimension, it confined the truth of mathematics to the ethereal and otherworldly realm of pure mathematics. Fergola‚Äôs mathematics, it turns out, was modern.&lt;/p&gt;
    &lt;p&gt;¬§&lt;/p&gt;
    &lt;p&gt;Massimo Mazzotti is a professor at UC Berkeley, where he holds the Thomas M. Siebel Presidential Chair in the History of Science. He is the author of Reactionary Mathematics: A Genealogy of Purity (2023).&lt;/p&gt;
    &lt;p&gt;LARB Contributor&lt;/p&gt;
    &lt;head rend="h4"&gt;Massimo Mazzotti is a professor at UC Berkeley, where he holds the Thomas M. Siebel Presidential Chair in the History of Science. He is the co-editor of Algorithmic Modernity: Mechanizing Thought and Action, 1500‚Äì2000 (2023), and the author of Reactionary Mathematics: A Genealogy of Purity (2023).&lt;/head&gt;
    &lt;head rend="h3"&gt;LARB Staff Recommendations&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h2"&gt;Weird Science&lt;/head&gt;
        &lt;p&gt;From anti-vaxxers to Flat Earthers, the public‚Äôs (and scholars‚Äô) perception of science shifted sometime between 1990-2010, writes Michael Gordin.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h2"&gt;‚ÄúI Don‚Äôt Really Care. Do You?‚Äù: Scientists in the Grey Zone in 1930s Italy&lt;/head&gt;
        &lt;p&gt;Massimo Mazzotti reflects on how Italian scientists failed as a bulwark against fascist politics in the 1930s.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Did you know LARB is a reader-supported nonprofit?&lt;/head&gt;
    &lt;p&gt;LARB publishes daily without a paywall as part of our mission to make rigorous, incisive, and engaging writing on every aspect of literature, culture, and the arts freely accessible to the public. Help us continue this work with your tax-deductible donation today!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46833254</guid><pubDate>Sat, 31 Jan 2026 03:53:06 +0000</pubDate></item><item><title>Show HN: Phage Explorer</title><link>https://phage-explorer.org/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46833754</guid><pubDate>Sat, 31 Jan 2026 05:22:03 +0000</pubDate></item><item><title>Sumerian Star Map Recorded the Impact of an Asteroid (2024)</title><link>https://archaeologyworlds.com/5500-year-old-sumerian-star-map-recorded/</link><description>&lt;doc fingerprint="31845f72a1c499a"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;5,500-Year-Old Sumerian Star Map Recorded the Impact of a Massive Asteroid&lt;/head&gt;
    &lt;p&gt;For more than 150 years scientists have tried to solve the mystery of a notorious cuneiform clay tablet that reveals that in the past the impact case of so-called K√∂fel was detected. The circular stone-cast tablet was discovered in the late 1800s from the 650 BC King Ashurbanipal ‚Äòs underground library in Nineveh, Iraq.&lt;/p&gt;
    &lt;p&gt;Data processing, which was long believed to be an Assyrian tablet, mirrored the sky over Mesopotamia in 3300 BC and proved to be much more ancient Sumerian origin.&lt;/p&gt;
    &lt;p&gt;The tablet is the first astronomical instrument, the ‚ÄúAstrolabe.‚Äù It consists of a segmented, disk-shaped star chart with marked units of angle measure inscribed upon the rim.&lt;/p&gt;
    &lt;p&gt;Unfortunately considerable parts of the planisphere on this tablet are missing (approximately 40%), damage which dates to the sacking of Nineveh. The reverse of the tablet is not inscribed.&lt;/p&gt;
    &lt;p&gt;Still under study by modern scholars, the cuneiform tablet in the British Museum collection No K8538 (known as ‚Äúthe Planisphere‚Äù) provides extraordinary proof for the existence of sophisticated Sumerian astronomy.&lt;/p&gt;
    &lt;p&gt;In 2008 two authors, Alan Bond and Mark Hempsell published a book about the tablet called ‚ÄúA Sumerian Observation of the Kofels‚Äô Impact Event‚Äù.&lt;/p&gt;
    &lt;p&gt;Raising a storm in archaeological circles, they re-translated the cuneiform text and asserted the tablet records an ancient asteroid strike, the K√∂fels‚Äô Impact, which struck Austria sometime around 3100 BC.&lt;/p&gt;
    &lt;p&gt;The giant landslide centred at K√∂fels in Austria is 500m thick and five kilometres in diameter and has long been a mystery since geologists first looked at it in the 19th century.&lt;/p&gt;
    &lt;p&gt;The conclusion drawn by research in the middle 20th century was that it must be due to a very large meteor impact because of the evidence of crushing pressures and explosions. But this view lost favor as a much better understanding of impact sites developed in the late 20th century.&lt;/p&gt;
    &lt;p&gt;In the case of K√∂fels there is no crater, so to modern eyes it does not look as an impact site should look. However, the evidence that puzzled the earlier researchers remains unexplained by the view that it is just another landslide.&lt;/p&gt;
    &lt;p&gt;So what is the connection between the sophisticated Sumerian star chart discovered in the underground library in Nineveh and mysterious impact that took place in Austria?&lt;/p&gt;
    &lt;p&gt;Examination of the clay tablet reveals that it is an astronomical work as it has drawings of constellations on it and the text has known constellation names. It has attracted a lot of attention but in over a hundred years nobody has come up with a convincing explanation as to what it is.&lt;/p&gt;
    &lt;p&gt;With modern computer programs that can simulate trajectories and reconstruct the night sky thousands of years ago the researchers have established what the Planisphere tablet refers to. It is a copy of the night notebook of a Sumerian astronomer as he records the events in the sky before dawn on 29 June 3123 BC (Julian calendar).&lt;/p&gt;
    &lt;p&gt;Half the tablet records planet positions and cloud cover, the same as any other night, but the other half of the tablet records an object large enough for its shape to be noted even though it is still in space.&lt;/p&gt;
    &lt;p&gt;The astronomers made an accurate note of its trajectory relative to the stars, which to an error better than one degree is consistent with an impact at K√∂fels.&lt;/p&gt;
    &lt;p&gt;The observation suggests the asteroid is over a kilometer in diameter and the original orbit about the Sun was an Aten type, a class of asteroids that orbit close to the Earth, that are resonant with the Earth‚Äôs orbit.&lt;/p&gt;
    &lt;p&gt;This trajectory explains why there is no crater at K√∂fels. The incoming angle was very low (six degrees) and means the asteroid clipped a mountain called Gamskogel above the town of L√§ngenfeld, 11 kilometers from K√∂fels, and this caused the asteroid to explode before it reached its final impact point. As it traveled down the valley it became a fireball, around five kilometers in diameter (the size of the landslide).&lt;/p&gt;
    &lt;p&gt;When it hit K√∂fels it created enormous pressures that pulverized the rock and caused the landslide but because it was no longer a solid object it did not create a classic impact crater.&lt;/p&gt;
    &lt;p&gt;Mark Hempsell, discussing the K√∂fels event, said: ‚ÄúAnother conclusion can be made from the trajectory. The back plume from the explosion (the mushroom cloud) would be bent over the Mediterranean Sea re-entering the atmosphere over the Levant, Sinai, and Northern Egypt.&lt;/p&gt;
    &lt;p&gt;‚ÄúThe ground heating though very short would be enough to ignite any flammable material ‚Äì including human hair and clothes. It is probable more people died under the plume than in the Alps due to the impact blast.‚Äù&lt;/p&gt;
    &lt;p&gt;In other words, the remarkable ancient star map shows that the Sumerians made an observation of an Aten asteroid over a kilometer in diameter that impacted K√∂fels in Austria in the early morning of 29th June 3123 BC.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46834313</guid><pubDate>Sat, 31 Jan 2026 07:32:51 +0000</pubDate></item><item><title>We have ipinfo at home or how to geolocate IPs in your CLI using latency</title><link>https://blog.globalping.io/we-have-ipinfo-at-home-or-how-to-geolocate-ips-in-your-cli-using-latency/</link><description>&lt;doc fingerprint="2ca4589a35236a90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We have ipinfo at home or how to geolocate IPs in your CLI using latency&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;TLDR: I made a CLI tool that can resolve an IP address to a country, US state and even a city. https://github.com/jimaek/geolocation-tool&lt;/p&gt;&lt;lb/&gt;It works well and confirms ipinfo's findings.&lt;/quote&gt;
    &lt;p&gt;Recently, I read how ipinfo finally proved what most technical people assumed: VPN providers don't actually maintain a crazy amount of infrastructure in hundreds of countries. They simply fake the IP geolocation by intentionally providing wrong location data to ARIN, RIPE, and Geo DB providers via geofeeds.&lt;/p&gt;
    &lt;p&gt;They achieved their results using a novel approach compared to other geo IP providers. Based on their blog and HackerNews comments, they built a large probe network and used it to trace and ping every (or most) IP addresses on the internet.&lt;/p&gt;
    &lt;p&gt;This latency and hop data, most likely along with advanced algorithms and data cross-reference, provides a reliable way of correctly detecting the physical geolocation of an IP address, without relying on faked data available in public sources.&lt;/p&gt;
    &lt;p&gt;This is a very interesting approach that makes total sense, and I'm sure their clients appreciate it and heavily rely on it.&lt;/p&gt;
    &lt;p&gt;While I can't ping every single IP address on the internet from hundreds of locations just yet, I can do it to a limited subset using Globalping. So I decided to try it out and see if I can replicate their results and build a small tool to allow anyone to do the same.&lt;/p&gt;
    &lt;p&gt;Globalping is an open-source, community-powered project that allows users to self-host container-based probes. These probes then become part of our public network, which allows anyone to use them to run network testing tools such as ping and traceroute.&lt;/p&gt;
    &lt;p&gt;At the moment, the network has more than 3000 probes, which in theory should be plenty to geolocate almost any IP address down to a country and even a US state level.&lt;/p&gt;
    &lt;p&gt;To automate and simplify this process, I made a little CLI tool using the globalping-ts library. My original idea was simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Accept a single IP as input&lt;/item&gt;
      &lt;item&gt;Ping it a few times per continent to select the continent&lt;/item&gt;
      &lt;item&gt;Then ping the IP from many different probes on that continent&lt;/item&gt;
      &lt;item&gt;Group and sort the results; the country with the lowest latency should be the correct one&lt;/item&gt;
      &lt;item&gt;And as a bonus, repeat the same process for USA states if the winning country was the US&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Essentially, what I had to do was simply create a few measurements and pass the location I needed using Globalping‚Äôs magic field, which would automatically figure out what I was looking for and select a few pseudo-random probes that fit the location and limit.&lt;/p&gt;
    &lt;p&gt;Now initially, I used &lt;code&gt;ping&lt;/code&gt; with 2 packets to run all measurements as quickly as possible, but I quickly realized it wasn‚Äôt a good idea as most networks block ICMP traffic. Next, I tried switching to TCP-based &lt;code&gt;ping&lt;/code&gt;, which required trying a few popular ports to get it to work. I quickly realized this was too complicated and unreliable and switched to &lt;code&gt;traceroute&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It worked perfectly. Even though &lt;code&gt;traceroute&lt;/code&gt; uses ICMP by default, it did not matter to me if the target IP‚Äôs network allowed ICMP or not, I simply analyzed the latency of the last available hop. Even if you block ICMP, your upstream most likely allows it, and in most cases, it‚Äôs located in the same country.&lt;/p&gt;
    &lt;p&gt;Of course, this means the resulting data is not 100% perfect. A better approach would be to analyze each IP using different methods, including TCP and UDP-based &lt;code&gt;traceroute&lt;/code&gt; on different ports, and expand to the last few hops instead of just one. Maybe even try to figure out the location of the registered ASNs and use a weights system in combination with public whois info in order to ‚Äúvote‚Äù for the right location based on different inputs. Probably even mark low certainty IPs to be retested with a double amount of probes. (end of rant)&lt;/p&gt;
    &lt;p&gt;But that‚Äôs something for a commercial provider to figure out, which it seems they did.&lt;/p&gt;
    &lt;p&gt;For continent detection, I decided to use just 5 probes per continent; the results were extremely accurate. Although for IPs just on the "border" of continents it might be ineffective, a higher amount of probes would generate better results. For this use case, it was good enough.&lt;/p&gt;
    &lt;p&gt;My home IP in central Europe was too easy to detect:&lt;/p&gt;
    &lt;code&gt;Phase 1: Detecting continent...
  North America: 137.18 ms
  Europe: 32.39 ms
  Asia: 174.54 ms
  South America: 215.08 ms
  Oceania: 244.15 ms
  Africa: 156.83 ms
&lt;/code&gt;
    &lt;p&gt;In phase 2, all we need to do is run a single measurement with the winning continent as the location and a higher limit. Initially, I started with 250 probes with great accuracy.&lt;/p&gt;
    &lt;p&gt;Eventually, I decided to drop down to 50 as the default. Based on my tests, the results continued to look really good, and it would allow the tool to be run even without authentication, as the Globalping API allows 250 tests per hour per IP and 50 probes per measurement.&lt;/p&gt;
    &lt;p&gt;Although I recommend registering for a free account at https://dash.globalping.io/ and authenticating with a token to get up to 500 tests per hour and run more tests.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: If you need more tests than that, you can either host a probe to generate passive credits to be used as tests, or donate via GitHub Sponsors. We will automatically detect it and credit your account.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;Phase 2: Detecting country...
  Measuring from 50 probes...

  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0%   50/50 - Best: PL (7.29 ms)                    

Top 3 Locations:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1.. Poland, EU                               7.29 ms
  2.. Germany, EU                              13.42 ms
  3.. Lithuania, EU                            17.65 ms

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                      SUMMARY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Location: Poland, EU
  Minimum Latency: 7.29 ms
  Confidence: Medium
&lt;/code&gt;
    &lt;p&gt;Great, now we have a basic IP-to-country resolver that only takes a few seconds to provide a response, and I didn‚Äôt even have to understand or write any complicated math. Although I‚Äôm sure someone smarter could use a formula to geolocate IPs with even fewer probes and higher accuracy.&lt;/p&gt;
    &lt;p&gt;For phase 3, we want to resolve the US to a specific state or territory, just like ipinfo did, and luckily they even provided a few sample IPs and locations to benchmark against during testing.&lt;/p&gt;
    &lt;p&gt;Again, this was as simple as creating a new measurement with the USA as the location. I used 50 probes as the default limit and tested the NordVPN IP advertised as Bahamas but resolved to Miami by ipinfo.&lt;/p&gt;
    &lt;code&gt;Phase 3: Detecting US state...
  Measuring from 50 probes...

  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0%   50/50 - Best: FL (0.45 ms)                    

Top 3 Locations:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1. Florida, USA                             0.45 ms
  2. South Carolina, USA                      12.23 ms
  3. Georgia, USA                             15.01 ms

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                      SUMMARY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Location: Florida, United States
  Minimum Latency: 0.45 ms
  Confidence: Very High
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
&lt;/code&gt;
    &lt;p&gt;The tool agrees, Florida is the correct location. But how accurate can this system be? Can we expand it to show the city too?&lt;/p&gt;
    &lt;p&gt;Let's make a new phase, which again, will simply set the resulting country or state as the location and extract the city of the probe with the lowest latency. Here, since there are too many possible cities and towns per state and country, I expect the accuracy to be low and only point to the closest major hub. But in theory, this should be more than enough for use cases like routing or performance debugging.&lt;/p&gt;
    &lt;p&gt;And here we go, the same result ipinfo got&lt;/p&gt;
    &lt;code&gt;Phase 4: Detecting city...
  Measuring from 36 probes...

  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100.0%   36/36 - Best: Miami (0.00 ms)                 

Top 3 Locations:
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  1. Miami, Florida, USA                      0.00 ms
  2. West Palm Beach, Florida, USA            4.36 ms
  3. Tampa, Florida, USA                      5.85 ms

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
                      SUMMARY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
  Location: Miami, Florida, United States
  Minimum Latency: 0.00 ms
  Confidence: Very High
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
&lt;/code&gt;
    &lt;p&gt;The current results are good but could be better. The main problem is with how the magic field works: when setting, for example, 'Europe' as the location, it tries to spread the tests across all European probes but does not guarantee that every single country is going to be included.&lt;/p&gt;
    &lt;p&gt;This results in inconsistencies where a probe in the same country as the target IP was not selected, and so the tool assumes the IP is located in a different neighbouring country.&lt;/p&gt;
    &lt;p&gt;To fix this and make the results more consistent, you would need to change the selection logic and manually set every country per continent and US state. By passing the full list of countries/states to the Globalping API, you ensure that at least one probe in that location is going to be selected. Additionally, you fully control the number of probes per location, which is very important to control the accuracy.&lt;/p&gt;
    &lt;p&gt;For example, North America technically contains 43 countries and territories. This means you can't just set a limit of one probe per country, it is not enough to properly understand the latency to the target IP from the disproportionately larger USA. A better limit would be around 200 probes for the USA, 20 for Canada, and 10 for Mexico.&lt;/p&gt;
    &lt;p&gt;But the goal of this tool was to use a minimum amount of probes to allow unauthenticated users to test it out. The current approach works great, it is simple to implement and it is very easy to control the accuracy by simply setting a higher limit of probes.&lt;/p&gt;
    &lt;p&gt;Overall, latency-based geolocation detection seems to be a great way to verify the location of any IP as long as you have enough vantage points. It will most likely fall apart in regions with minimal or no coverage.&lt;/p&gt;
    &lt;p&gt;The tool itself is open source and you can run it like this:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;geolocate $IP&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;You can also use the ‚Äìlimit parameter to use more probes per phase. But be careful as it applies the set value to all phases and this will very quickly eat through your limit. Check the full docs in GitHub.&lt;/p&gt;
    &lt;p&gt;Pull requests with improvements are welcome!&lt;/p&gt;
    &lt;p&gt;Feel free to email me if you need some free credits to play around with d@globalping.io&lt;/p&gt;
    &lt;p&gt;And of course consider hosting a probe, it‚Äôs as simple as running a container https://github.com/jsdelivr/globalping-probe&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46834953</guid><pubDate>Sat, 31 Jan 2026 09:30:05 +0000</pubDate></item><item><title>CERN accepts $1B in private cash towards Future Circular Collider</title><link>https://physicsworld.com/a/cern-accepts-1bn-in-private-cash-towards-future-circular-collider/</link><description>&lt;doc fingerprint="f44d155c29849e32"&gt;
  &lt;main&gt;
    &lt;p&gt;Mark Thomson takes the reins at the CERN particle-physics lab, which recently received $1bn in private donations for its next collider project, as Michael Banks reports&lt;/p&gt;
    &lt;p&gt;The CERN particle-physics lab near Geneva has received $1bn from private donors towards the construction of the Future Circular Collider (FCC). The cash marks the first time in the lab‚Äôs 72-year history that individuals and philanthropic foundations have agreed to support a major CERN project. If built, the FCC would be the successor to the Large Hadron Collider (LHC), where the Higgs boson was discovered.&lt;/p&gt;
    &lt;p&gt;CERN originally released a four-volume conceptual design report for the FCC in early 2019, with more detail included in a three-volume feasibility study that came out last year. It calls for a giant tunnel some 90.7 km in circumference ‚Äì roughly three times as long as the LHC ‚Äì that would be built about 200 m underground on average.&lt;/p&gt;
    &lt;p&gt;The FCC has been recommended as the preferred option for the next flagship collider at CERN in the ongoing process to update the European Strategy for Particle Physics, which will be passed over to the CERN Council in May 2026.If the plans are given the green light by CERN Council in 2028, construction on the FCC electron-positron machine, dubbed FCC-ee, would begin in 2030. It would start operations in 2047, a few years after the High Luminosity LHC (HL-LHC) closes down, and run for about 15 years until the early 2060s.&lt;/p&gt;
    &lt;p&gt;The FCC-ee would focus on creating a million Higgs particles in total to allow physicists to study its properties with an accuracy an order of magnitude better that possible with the LHC. The FCC feasibility study then calls for a hadron machine, dubbed FCC-hh, to replace the FCC-ee in the existing 91 km tunnel. It would be a ‚Äúdiscovery machine‚Äù, smashing together protons at high energy ‚Äì about 85 TeV ‚Äì with the aim of creating new particles. If built, the FCC-hh will begin operation in 2073 and run to the end of the century.&lt;/p&gt;
    &lt;p&gt;The funding model for the FCC-ee, which is expected to have a price tag of about $18bn, is still a work in progress. But it is estimated that at least two-thirds of the construction costs will come from CERN‚Äôs 24 member states with the rest needing to be found elsewhere. One option to plug that gap is private donations and in late December CERN received a significant boost from several organizations including the Breakthrough Prize Foundation, the Eric and Wendy Schmidt Fund for Strategic Innovation, and the entrepreneurs John Elkann and Xavier Niel. Together, they pledged a total of $1bn towards the FCC-ee.&lt;/p&gt;
    &lt;p&gt;Costas Fountas, president of the CERN Council, says CERN is ‚Äúextremely grateful‚Äù for the interest. ‚ÄúThis once again demonstrates CERN‚Äôs relevance and positive impact on society, and the strong interest in CERN‚Äôs future that exists well beyond our own particle physics community,‚Äù he notes.&lt;/p&gt;
    &lt;p&gt;Eric Schmidt, who founded Google, claims that he and Wendy Schmidt were ‚Äúinspired by the ambition of this project and by what it could mean for the future of humanity‚Äù. The FCC, he believes, is an instrument that ‚Äúcould push the boundaries of human knowledge and deepen our understanding of the fundamental laws of the Universe‚Äù and could lead to technologies that could benefit society ‚Äúin profound ways‚Äù from medicine to computing to sustainable energy.&lt;/p&gt;
    &lt;p&gt;The cash promised has been welcomed by outgoing CERN director-general Fabiola Gianotti. ‚ÄúIt‚Äôs the first time in history that private donors wish to partner with CERN to build an extraordinary research instrument that will allow humanity to take major steps forward in our understanding of fundamental physics and the universe,‚Äù she said. ‚ÄúI am profoundly grateful to them for their generosity, vision, and unwavering commitment to knowledge and exploration.‚Äù&lt;/p&gt;
    &lt;head rend="h3"&gt;Further boost&lt;/head&gt;
    &lt;p&gt;The cash comes a few months after the Circular Electron‚ÄìPositron Collider (CEPC) ‚Äì a rival collider to the FCC-ee that also involves building a huge 100 km tunnel to study the Higgs in unprecedented detail ‚Äì was not considered for inclusion in China‚Äôs next five-year plan, which runs from 2026 to 2030. There has been much discussion in China about whether the CEPC is the right project for the country, with the collider facing criticism from particle physicist and Nobel laureate Chen-Ning Yang, before he died last year.&lt;/p&gt;
    &lt;p&gt;Wang Yifang of the Institute of High Energy Physics (IHEP) in Beijing says they will submit the CEPC for consideration again in 2030 unless FCC is officially approved before then. But for particle theorist John Ellis from Kings College London, China‚Äôs decision to effectively put the CEPC on the back burner ‚Äúcertainly simplifies the FCC discussion‚Äù. ‚ÄúHowever, an opportunity for growing the world particle physics community has been lost, or at least deferred [by the decision],‚Äù Ellis told Physics World.&lt;/p&gt;
    &lt;p&gt;Ellis adds, however, that he would welcome China‚Äôs participation in the FCC. ‚ÄúTheir accelerator and detector [technical design reviews] show that they could bring a lot to the table, if the political obstacles can be overcome,‚Äù he says. CERN releases plans for the ‚Äòmost extraordinary instrument ever built‚Äô&lt;/p&gt;
    &lt;p&gt;However, if the FCC-ee goes ahead China could perhaps make significant ‚Äúin-kind‚Äù contributions rather like those that occur with the ITER experimental fusion reactor, which is currently being built in France. In this case, instead of cash payments, the countries provide components, equipment and other materials.&lt;/p&gt;
    &lt;p&gt;Those considerations and more will now fall to the British physicist Mark Thomson, who took over from Gianotti as CERN director-general on 1 January for a five-year term. As well as working on funding requirements for the FCC-ee, top of his in-tray will actually be shutting down the LHC in June to make way for further work on the HL-LHC, which involves installing powerful new superconducting magnets and improving the detection.&lt;/p&gt;
    &lt;p&gt;About 90% of the 27 km LHC accelerator will be affected by the upgrade with a major part being to replace the magnets in the final focus systems of the two large experiments, ATLAS and CMS. These magnets will take the incoming beams and then focus them down to less than 10 ¬µm in cross section. The upgrade includes the installation of brand new state-of-the-art niobium-tin (Nb3Sn) superconducting focusing magnets.&lt;/p&gt;
    &lt;p&gt;The HL-LHC will probably not turn on until 2030, at which time Thomson‚Äôs term will nearly be over, but that doesn‚Äôt deter him from leading the world‚Äôs foremost particle-physics lab. ‚ÄúIt‚Äôs an incredibly exciting project,‚Äù Thomson told the Guardian. ‚ÄúIt‚Äôs more interesting than just sitting here with the machine hammering away.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835124</guid><pubDate>Sat, 31 Jan 2026 09:58:39 +0000</pubDate></item><item><title>Automatic Programming</title><link>https://antirez.com/news/159</link><description>&lt;doc fingerprint="6f109d7f48a74099"&gt;
  &lt;main&gt;
    &lt;quote&gt;In my YouTube channel, for some time now I started to refer to the process of writing software using AI assistance (soon to become just "the process of writing software", I believe) with the term "Automatic Programming". In case you didn't notice, automatic programming produces vastly different results with the same LLMs depending on the human that is guiding the process with their intuition, design, continuous steering and idea of software. Please, stop saying "Claude vibe coded this software for me". Vibe coding is the process of generating software using AI without being part of the process at all. You describe what you want in very general terms, and the LLM will produce whatever happens to be the first idea/design/code it would spontaneously, given the training, the specific sampling that happened to dominate in that run, and so forth. The vibe coder will, at most, report things not working or not in line with what they expected. When the process is actual software production where you know what is going on, remember: it is the software *you* are producing. Moreover remember that the pre-training data, while not the only part where the LLM learns (RL has its big weight) was produced by humans, so we are not appropriating something else. We can pretend AI generated code is "ours", we have the right to do so. Pre-training is, actually, our collective gift that allows many individuals to do things they could otherwise never do, like if we are now linked in a collective mind, in a certain way. That said, if vibe coding is the process of producing software without much understanding of what is going on (which has a place, and democratizes software production, so it is totally ok with me), automatic programming is the process of producing software that attempts to be high quality and strictly following the producer's vision of the software (this vision is multi-level: can go from how to do, exactly, certain things, at a higher level, to stepping in and tell the AI how to write a certain function), with the help of AI assistance. Also a fundamental part of the process is, of course, *what* to do. I'm a programmer, and I use automatic programming. The code I generate in this way is mine. My code, my output, my production. I, and you, can be proud. If you are not completely convinced, think to Redis. In Redis there is not much technical novelty, especially at its start it was just a sum of basic data structures and networking code that every competent system programmer could write. So, why it became a very useful piece of software? Because of the ideas and visions it contained. Programming is now automatic, vision is not (yet).&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835208</guid><pubDate>Sat, 31 Jan 2026 10:11:25 +0000</pubDate></item><item><title>Euro firms must ditch Uncle Sam's clouds and go EU-native</title><link>https://www.theregister.com/2026/01/30/euro_firms_must_ditch_us/</link><description>&lt;doc fingerprint="bf389010bb04ad4c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Euro firms must ditch Uncle Sam's clouds and go EU-native&lt;/head&gt;&lt;head rend="h2"&gt;Just because you're paranoid about digital sovereignty doesn't mean they're not after you&lt;/head&gt;&lt;p&gt;Opinion I'm an eighth-generation American, and let me tell you, I wouldn't trust my data, secrets, or services to a US company these days for love or money. Under our current government, we're simply not trustworthy.&lt;/p&gt;&lt;p&gt;In the Trump‚Äëredux era of 2026, European enterprises are finally taking data seriously, and that means packing up from Redmond-by-Seattle and moving their most sensitive workloads home. This isn't just compliance theater; it's a straight‚Äëup national economic security play.&lt;/p&gt;&lt;head rend="h2"&gt;Open source's new mission: Rebuild a continent's tech stack&lt;/head&gt;READ MORE&lt;p&gt;Europe's digital sovereignty paranoia, long waved off as regulatory chatter, is now feeding directly into procurement decisions. Gartner told The Reg last year that IT spending in Europe is set to grow by 11 percent in 2026, hitting $1.4 trillion, with a big chunk rolling into "sovereign cloud" options and on‚Äëprem/edge architectures.&lt;/p&gt;&lt;p&gt;The kicker? Fully 61 percent of European CIOs and tech leaders say they want to increase their use of local cloud providers. More than half say geopolitics will prevent them from leaning further on US‚Äëbased hyperscalers.&lt;/p&gt;&lt;p&gt;The American hypercloud vendors have figured this out. AWS recently made its European Sovereign Cloud available. This AWS cloud, Amazon claims, is "entirely located within the EU, and physically and logically separate from other AWS Regions." On top of that, EU residents will "independently operate it" and "be backed by strong technical controls, sovereign assurances, and legal protections designed to meet the needs of European governments and enterprises for sensitive data."&lt;/p&gt;&lt;p&gt;Many EU-based companies aren't pleased with this Euro-washing of American hypercloud services. The Cloud Infrastructure Service Providers in Europe (CISPE) trade association accuses the EU Cloud Sovereignty Framework of being set up to favor the incumbent (American) hypercloud providers.&lt;/p&gt;&lt;p&gt;They're not wrong.&lt;/p&gt;&lt;p&gt;You don't need a DEA warrant or a Justice Department subpoena to see the trend: Europe's 90‚Äëplus‚Äëpercent dependency on US cloud infrastructure, as former European Commission advisor Cristina Caffarra put it, is a single‚Äëshock‚Äëevent security nightmare waiting to rupture the EU's digital stability.&lt;/p&gt;&lt;p&gt;Seriously. What will you do if Washington decides to unplug you? Say Trump gets up on the wrong side of the bed and decides to invade Greenland. There goes NATO, and in all the saber-rattling leading up to the 10th Mountain Division being shipped to Nuuk, he orders American companies to cut their services to all EU countries and the UK.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;When AI 'builds a browser,' check the repo before believing the hype&lt;/item&gt;&lt;item&gt;Just because Linus Torvalds vibe codes doesn't mean it's a good idea&lt;/item&gt;&lt;item&gt;The most durable tech is boring, old, and everywhere&lt;/item&gt;&lt;item&gt;What the Linux desktop really needs to challenge Windows&lt;/item&gt;&lt;/list&gt;&lt;p&gt;With the way things are going, they're not going to say no. I mean, CEOs Tim Cook of Apple, Eric Yuan of Zoom, Lisa Su of AMD, and ‚Äì pay attention ‚Äì Amazon's Andy Jassy all went obediently to watch a feature-length White House screening of Melania, the universally-loathed, 104‚Äëminute Amazon‚Äëproduced documentary about First Lady Melania Trump.&lt;/p&gt;&lt;head rend="h2"&gt;Europe's cloud challenge: Building an Airbus for the digital age&lt;/head&gt;READ MORE&lt;p&gt;Sure, that's a silly example, but for American companies to do business today, they're kowtowing to Trump. Or, take a far more serious example, when Minnesota company CEOs called for "de-escalation" in the state, there was not one word about ICE or the government's role in the bloodshed. It was the corporate equivalent of the mealy-mouthed "thoughts and prayers" American right-wingers always say after a US school shooting.&lt;/p&gt;&lt;p&gt;Some companies have already figured out which way the wind is blowing. Airbus, the European aerospace titan, has put out a ‚Ç¨50 million, decade‚Äëlong tender to migrate its mission‚Äëcritical applications to a "sovereign European cloud." Airbus wants its whole stack ‚Äì data at rest, data in transit, logging, IAM, and security‚Äëmonitoring infrastructure ‚Äì all rooted in EU law and overseen by EU operators. As Catherine Jestin, Airbus's executive vice president of digital, told The Register: "We want to ensure this information remains under European control."&lt;/p&gt;&lt;p&gt;Who can blame them? Thanks to the American CLOUD Act and related US surveillance statutes, US‚Äëheadquartered providers must hand over European data regardless of where the bytes sit. Exhibit A is that Microsoft has already conceded that it cannot guarantee data independence from US law enforcement. Airbus is betting that "data residency on paper" from AWS‚Äëstyled "EU sections" is not enough. Real sovereignty demands EU‚Äëowned and run operations with full contractual and legal firewalls. Sure, your data may live in Frankfurt, but your fate still rests in Seattle, Redmond, or Mountain View if an American company owns your cloud provider.&lt;/p&gt;&lt;p&gt;Besides, do you really want some Trump apparatchik getting their hands on your data? I mean, this is a government where Madhu Gottumukkala, the acting director of the US Cybersecurity and Infrastructure Security Agency, uploaded sensitive data into ChatGPT!&lt;/p&gt;&lt;head rend="h2"&gt;UK urged to unplug from US tech giants as digital sovereignty fears grow&lt;/head&gt;READ MORE&lt;p&gt;In response, Brussels is pushing an open source‚Äëled exit from hyperscaler lock‚Äëin. Ministries are standardizing on Nextcloud‚Äëstyle collaboration stacks instead of Microsoft 365 to fund Euro‚Äënative clouds via the European Cloud Alliance. Some countries, like France, are already shoving Zoom, Teams, and other US videoconferencing platforms out the door in favor of a local service.&lt;/p&gt;&lt;p&gt;If you're running an EU‚Äëbased firm in 2026, the takeaway isn't that AWS‚Äëin‚ÄëFrankfurt is evil; it's that for certain workloads, especially national security, industrial IP, or high‚Äëprofile consumer data franchises, EU‚Äënative cloud and services are no longer a nice‚Äëto‚Äëhave but a business continuity plan requirement.&lt;/p&gt;&lt;p&gt;It's time to get serious about digital sovereignty. The clock is ticking, and there's no telling when Trump will go off. ¬Æ&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835336</guid><pubDate>Sat, 31 Jan 2026 10:34:07 +0000</pubDate></item><item><title>Guix System First Impressions as a Nix User</title><link>https://nemin.hu/guix.html</link><description>&lt;doc fingerprint="b71891772da12e9f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Guix System First Impressions as a Nix User&lt;/head&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;head rend="h2"&gt;1. My Journey to Guix System&lt;/head&gt;
    &lt;p&gt;Feel free to skip this section if you don't really care about backstories. I just figured it makes sense to recap how and why one might start having an interest in declarative distros before tackling the main topic.&lt;/p&gt;
    &lt;p&gt;I've been a Linux-only1 user for about ten years now and, like many others, I too embarked on the arduous journey of distro-hopping. I started with Mint and when that felt too slow, I switched to Ubuntu. When Ubuntu felt too handholdy2, I switched to Arch, which proved to be my main driver for well over five or so years. And when I couldn't resist the Siren's call, I moved on to Gentoo, thinking surely "harder is better". Which resulted in severe burnout in a few months, so I capitulated and switched to Fedora, which was very stable and honestly an all around excellent system. But once more, my interest was piqued, and (before today's adventure) I finally switched to NixOS.&lt;/p&gt;
    &lt;p&gt;I've always had a passing interest towards Nix ever since I've first heard about it, but until fairly recently, I always dismissed it as a tool for DevOps guys. The syntax was weird, the need for reproducible environments seemingly irrelevant, and stuff like the oft-recommended Nix Pills seemed anything but newbie-friendly.&lt;/p&gt;
    &lt;p&gt;So then why would someone like me, who's so adamant about not needing Nix eventually choose to go all-in? I guess it was at first less about Nix being better and just the rest being worse.&lt;/p&gt;
    &lt;p&gt; Of the two big reasons for the switch, one was that I realized that having per-directory environments for your projects is actually a very handy thing to do when you like to toy around with many technologies. I used to generate my other blog using Jekyll and, no matter which distro I used, it was always a pain in the neck to have a good Ruby environment set up. &lt;code&gt;bundler install&lt;/code&gt; didn't really want to work without privileges and I wasn't really a fan of unleashing &lt;code&gt;sudo&lt;/code&gt; on it, but usually that was the only way I could get things to work.
&lt;/p&gt;
    &lt;p&gt; With Nix, however, it was a matter of just describing a few packages in a shell and boom, Ruby in one folder, no Ruby (and thus no mess) everywhere else. I was hooked! I started adding &lt;code&gt;shell.nix&lt;/code&gt; files to all my little projects, hell, I started planning projects by first adding a &lt;code&gt;shell.nix&lt;/code&gt; with all the dependencies I would reasonably need.
&lt;/p&gt;
    &lt;p&gt;The other reason, which ultimately cemented that I need to commit, was that I was getting tired of my installed packages slowly drifting out of control. Sure, every package manager has some method of listing what's installed, but these are usually cumbersome and completely ephemeral (in the sense that any listing becomes invalid the moment you change anything).&lt;/p&gt;
    &lt;p&gt;With NixOS, the equation is flipped on its head: No longer did I query the system to tell me what's installed and what's not, it was now the system that worked based on files that I edit. The difference sounds small on paper, but for me it was an extremely liberating feeling to know that I could edit my system configuration in a versionable, explicit, and centralized way.&lt;/p&gt;
    &lt;p&gt;But NixOS isn't the only declarative distro out there. In fact GNU forked Nix fairly early and made their own spin called Guix, whose big innovation is that, instead of using the unwieldy Nix-language, it uses Scheme. Specifically Guile Scheme, GNU's sanctioned configuration language. I've been following Guix for a bit, but it never felt quite ready to me with stuff like KDE being only barely supported and a lot of hardware not working out of the box.&lt;/p&gt;
    &lt;p&gt;However, now that (after three years) Guix announced its 1.5.0 release with a lot of stuff stabilized and KDE finally a first-party citizen, I figured now is the best time to give it a fresh shot. This post captures my experiences from installation to the first 3-4 days.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Installer Impressions&lt;/head&gt;
    &lt;p&gt; Plug your USB in, &lt;code&gt;dd&lt;/code&gt; the file onto the drive, reboot, nothing unusual. If you've ever installed a Linux system, it's more of the same.
&lt;/p&gt;
    &lt;p&gt;After selecting the pendrive in my BIOS settings, the monitor began to glow in a deep, radiant blue as the Guix System logo appeared on my screen‚Ä¶ only to suddenly switch to a menacing red: My CPU's integrated GPU is not supported by free firmware. A helpful popup gave me a gentle nudge about picking free hardware next time (buddy, have you seen the PC part prices these days?) and off I went into the installer proper.&lt;/p&gt;
    &lt;p&gt;Figure 1: Picture of the installer graciously borrowed from the Guix installer manual.&lt;/p&gt;
    &lt;p&gt;The installer itself is refreshingly barebones and I mean this in a positive way. It asks all the necessary questions and provides a nice basic configuration file, all done in a retro Ncurses-based TUI. I was really happy to see that, unlike my last attempt at using Guix System in the early 2020-s, KDE Plasma is now a first-party choice during installation. I never really vibed too much with GNOME and the other options didn't appeal either, so the choice was obvious.&lt;/p&gt;
    &lt;p&gt;Now, I'm not sure if I just picked the worst possible time or if the Guix servers were facing unusual load or whatever may have happened, but after such a breeze of a setup, the moment I pressed install, my PC became unusable for the next 2.5 hours. Which is unacceptable for an installation process these days in my opinion. I am lucky enough to live in a household with fiber-optic internet, that merely shrugs at bandwidth of up to a gigabyte per second and yet nearly all packages downloaded with a whopping 50 kilobytes per second, meaning even small-ish 5-10 megabyte packages took long minutes to download.3&lt;/p&gt;
    &lt;p&gt;A reboot later my issues only got worse.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. I Can't Find my Way-land&lt;/head&gt;
    &lt;p&gt;I was assuming I'd get SDDM after having chosen KDE Plasma, but (what a later, closer read of the manual made me realize is the expected outcome for a default config) it was GDM that loaded in. I entered my name and password, and I was greeted with the familiar Plasma 6 spinner. The first hint that something might be off was that it loaded a bit longer than usual, but I was not going to get mad at waiting 10 seconds instead of 3. After all, I did just wait magnitudes longer to get here.&lt;/p&gt;
    &lt;p&gt;With practically nothing installed beyond the very basics, I clicked on Konsole, hoping to start prodding around my config and add some of my day to day apps. To my horror, it opened in the top left corner, without a titlebar and without any borders. What's more, no matter what I did, I couldn't move it. It also didn't show up on the menu bar, despite the application launcher still being completely usable. At this point I was fairly exhausted by these antics, but I figured,&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Well, it's a brand new release, perhaps this just snuck in. Let's give updating a shot and see if that helps.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt; So I issued &lt;code&gt;guix pull&lt;/code&gt;‚Ä¶ The download whizzed by with speed quite unexpected after what I experienced with the installer‚Ä¶ Only to crash into the brick wall that's indexing. Okay, whatever, another 10-12 minutes down the drain, at least now I have newest version.
&lt;/p&gt;
    &lt;p&gt;Figure 2: Better than before download speeds&lt;/p&gt;
    &lt;p&gt; Except I didn't. Because, unlike Nix, the &lt;code&gt;guix&lt;/code&gt; executable is not an omnipresent, unique thing that anyone and everyone uses on your PC. Not only does every user have their own instance, if you don't issue a certain set of commands, you won't start using the new version, despite updating it.
&lt;/p&gt;
    &lt;p&gt;To Guix's credit, the CLI does scream at you to update your environment or else you'll keep using the old version, but I still find this system very disorientating compared to Nix. I'm certain experienced Guixheads are long past being tripped up by this sort of stuff and might even struggle to remember that there was a time they had to do these special steps too, but as a new user it felt a bit rough, especially consdering this is Guix System, i.e. the system whose whole purpose is to be integrate Guix as much as it can.&lt;/p&gt;
    &lt;p&gt; Back to our issue at hand. I issued &lt;code&gt;sudo -s&lt;/code&gt; and &lt;code&gt;guix pull&lt;/code&gt;-ed again. Once more 10-12 minutes passed indexing. But at least I could finally call &lt;code&gt;guix system reconfigure /etc/config.scm&lt;/code&gt;. Interestingly things are much faster this time around, I saw speeds up to 30-50 Mbps. Before long the system was updated to the newest commit and I rebooted with high hopes.
&lt;/p&gt;
    &lt;p&gt;High hopes, that were immediately dashed when Plasma loaded in the same messed up way. At this point I started to suspect this might be an issue with the GPU driver, so I enabled the LXQT desktop environment and rebooted once more. Thankfully that one worked like a charm and I was able to boot up both Emacs (editing Scheme with GNU Nano is a pain I do not wish on anyone) and LibreWolf (Firefox's de-Mozilla-d variant).&lt;/p&gt;
    &lt;p&gt; Not having found anything too useful in the docs, I decided to make my problem someone else's so I fired up ERC4 and connected to Libera.chat's &lt;code&gt;#guix&lt;/code&gt; channel. After around half an hour of wait, a user by the name of Rutherther stepped up and offered me some help. We were able to figure it out that Nouveau wasn't able to drive my GPU (an RTX 5070), so his recommendation was that I should try booting with &lt;code&gt;nomodeset&lt;/code&gt;. I did, but it sadly didn't help much either.
&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Sympathy for the Devil&lt;/head&gt;
    &lt;p&gt;At this point I was out of ideas. Ideas of solving this using pure-Guix System, that is. There was still one option I wanted to avoid as long as I could, but alas, it seemed like the only option, that still had a realistic chance of working.&lt;/p&gt;
    &lt;p&gt;Figure 3: Nonguix's official logo, self-described to be "dark and evil".&lt;/p&gt;
    &lt;p&gt;Enter Nonguix, the Mr. Hyde to Guix's Dr. Jekyll, the shady guy who offers you a hit and first time's for free, the‚Ä¶ Erm, in a nutshell, it's the repository for non-free applications and drivers packages for Guix System, basically. Interestingly enough, by Guix's own findings about 64% of users utilize the Nonguix channel, which is perhaps not "literally everyone", but it does paint a picture that there is still stuff out there that you simply cannot replace with FOSS software yet.&lt;/p&gt;
    &lt;quote&gt;1: (cons* (channel 2: (name 'nonguix) 3: (url "https://gitlab.com/nonguix/nonguix") 4: ;; Enable signature verification: 5: (introduction 6: (make-channel-introduction 7: "897c1a470da759236cc11798f4e0a5f7d4d59fbc" 8: (openpgp-fingerprint 9: "2A39 3FFF 68F4 EF7A 3D29 12AF 6F51 20A0 22FB B2D5")))) 10: %default-channels)&lt;/quote&gt;
    &lt;p&gt; Enabling the repo wasn't exactly difficult. You just paste the short excerpt from above (also found in the README) into your &lt;code&gt;~/.config/guix/channels.scm&lt;/code&gt; and &lt;code&gt;/etc/guix/channels.scm&lt;/code&gt; files, &lt;code&gt;guix pull&lt;/code&gt;, let it index to its heart's content again, and then you have access to all that is nasty (yet occasionally useful) in the world.
&lt;/p&gt;
    &lt;p&gt;I figured perhaps if Linux-libre and its free firmware couldn't deal with my GPU, then surely Linux proper with its binary blobs could. Hell, for good measure I threw in the NVIDIA transform, which is supposed to automagically translate all dependencies to use the proprietary drivers.&lt;/p&gt;
    &lt;p&gt;Figure 4: What haste and half-reading manuals gets you‚Ä¶&lt;/p&gt;
    &lt;p&gt;Turns out my eagerness was a mistake. Not only did the process take yet another half an hour (if not more, I stopped counting), upon reboot all I was met with was a kernel panic about the driver not being able to cope with the GPU it found and a massive spew of FSCK logs.&lt;/p&gt;
    &lt;p&gt;Figure 5: 'FSCK' was indeed very close to the first words that came to my mind at this moment.&lt;/p&gt;
    &lt;p&gt;With no better ideas in mind, I took out my pendrive again and burned Nonguix's own pre-built ISO on it using my partner's PC. While it ultimately did get me a working system, this version has three unfortunate hindrances:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It was built in 2022, far before Guix's migration to Codeberg, meaning it still attempts to pull content from the unfathomably slow GNU Savannah mirror. I had to manually override my &lt;code&gt;channels.scm&lt;/code&gt;to point at the Codeberg repo instead, but with no easy means of finding its "channel introduction"5, I had to pass in&lt;code&gt;--disable-authentication&lt;/code&gt;to Guix when updating my system. A bit scary, but I trust the Codeberg repo.&lt;/item&gt;
      &lt;item&gt;Because of its age, I got a lot of somewhat intimidating errors about hardware not being recognized and other stuff I couldn't even decipher, but ultimately the system booted to the installer without issue.&lt;/item&gt;
      &lt;item&gt;For some reason while the installer itself does include Nonguix stuff, it actually does not include the repo in the resulting channels files, nor the substitution server for the project. The README has a warning about this, but if you happen to miss it, you could accidentally install a non-Nonguix Guix System (say that three times fast).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; None of these were particularly hard to fix, however, and soon enough I was back where I started. That is to say, in a &lt;code&gt;nomodeset&lt;/code&gt; X11 session, except this time running i3, as LXQT wasn't an available option on an installer this old. There was certainly a bit of a hacker-ish vibe to messing with code files in an environment like that, but I was honestly much more looking forward to finally having a usable desktop.
&lt;/p&gt;
    &lt;p&gt; Having learned from my hastiness, this time I was smarter. I only enabled the full kernel and firmware blobs, without going anywhere near the NVIDIA transform. I issued another &lt;code&gt;guix system reconfigure&lt;/code&gt; and, after having time for another tea session, my update was finally finished.
&lt;/p&gt;
    &lt;p&gt;I rebooted with tentative nervousness and‚Ä¶ Success? Huh.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Goals&lt;/head&gt;
    &lt;p&gt;Obviously there is little point in throwing Guix System on my PC and declaring success. I wanted to be able to at least reproduce the kind of workflow I'm used to using NixOS. For that, I need the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A browser: preferably Firefox, as I'm not a huge fan of Chrome / Chromium,&lt;/item&gt;
      &lt;item&gt;An E-mail client: preferably Thunderbird,&lt;/item&gt;
      &lt;item&gt;A basic office suite: preferably LibreOffice,&lt;/item&gt;
      &lt;item&gt;Dev environments: for Rust, Zig, Scheme, and TypeScript (with the option for more, if possible),&lt;/item&gt;
      &lt;item&gt;Emacs: I do almost all my text editing in it these days, falling back to Neovim for quick tasks,&lt;/item&gt;
      &lt;item&gt;Discord: for chatting with friends,&lt;/item&gt;
      &lt;item&gt;Telegram: for chatting with family,&lt;/item&gt;
      &lt;item&gt;Steam: for the very rare occasions I want to game,&lt;/item&gt;
      &lt;item&gt;NVIDIA drivers: I prefer to offload day-to-day usage to my CPU's integrated GPU, as it cuts my energy usage in half.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of these it was obvious that two would be relatively hard and one "outright impossible". The two being Steam and the drivers (as both are non-free and thus not in Guix's default repos) and the "impossible" one being Discord (which not even the non-free repo has packaged). But I was ready to compromise a little bit since I am requesting stuff that's explicitly against Guix's goals.&lt;/p&gt;
    &lt;head rend="h2"&gt;6. Results&lt;/head&gt;
    &lt;p&gt;Figure 6: My desktop running Wezterm packaged by me and Emacs.&lt;/p&gt;
    &lt;p&gt;While there has been occasional bumps and hitches along the ride, I must say I'm very impressed with Guix System so far. Let's go through this list in order:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browser: So far I'm really enjoying LibreWolf. It feels a lot snappier than Firefox and I'm really baffled how much speed I was apparently missing out on.&lt;/item&gt;
      &lt;item&gt;E-mails: I installed Icedove, which is basically just Thunderbird without Mozilla branding. It works as expected.&lt;/item&gt;
      &lt;item&gt;Office suite: LibreOffice is available as expected. Not much to say about it. I guess it's interesting that Guix isn't following the usual &lt;code&gt;-stale&lt;/code&gt;/&lt;code&gt;-fresh&lt;/code&gt;packaging schema, but I don't really mind not having cutting edge versions of an office suite :)&lt;/item&gt;
      &lt;item&gt;Dev environments: I've only briefly toyed with development environments so far, but to me it seems like for simple use-cases it might be even easier to use than &lt;code&gt;shell.nix&lt;/code&gt;(you don't need any sort of ceremony, just a&lt;code&gt;manifest.scm&lt;/code&gt;file with a&lt;code&gt;(specifications-&amp;gt;manifest &amp;lt;list of packages&amp;gt;)&lt;/code&gt;form inside and you have a dev env ready to go.)&lt;/item&gt;
      &lt;item&gt;Emacs: Installed just fine. I had to install &lt;code&gt;emacs-vterm&lt;/code&gt;to make Vterm work, but all that took was the very simple process of adding the library to my home configuration and then referencing it in my Emacs config as per this Reddit post.&lt;/item&gt;
      &lt;item&gt;Discord: I decided to just use Discord's browser version, which works just as fine (if not better). It's trading a tiny bit of convenience in return for not having to figure out how to manually add a package for it from some random third-party source. From what I've read elsewhere Flatpak is also an option, but I prefer having just one package manager at a time.&lt;/item&gt;
      &lt;item&gt;Steam: Installed shockingly easily. I have to really give props to the Nonguix team. I tested Portal 2 with the Nouveau driver, it is a little disheartening to see a 15 years old game6 lag, but I understand the people's hands are tied when it comes to the free drivers. After I managed to install the proprietary drivers, I was able to play even Portal RTX, which is something I never managed to get to work using NixOS.&lt;/item&gt;
      &lt;item&gt;NVIDIA drivers: This time I actually read the docs properly and it didn't take long for me to realize the initial problem that caused my previous install to be unbootable was of course found between the chair and keyboard. This time, after making sure I enabled the open drivers and kernel mode-setting, I crossed my fingers, issued a reconfigure and it works beautifully!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;6.1. The Good&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Helpful community: While I do feel like Guix's community could be much larger (see below), the one that exists is very helpful and nice from my limited experience. In all places I've looked so far (Libera's&lt;/p&gt;&lt;code&gt;#guix&lt;/code&gt;, /r/Guix, and the guix/guix Codeberg repository) I was met with genuinely kind and helpful people.&lt;p&gt;That is not to say I haven't seen some bad eggs, especially in posts from years ago, but I don't think there is any community without those, so I'm not going to cite this as a negative.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Home configuration: Having &lt;code&gt;guix home&lt;/code&gt;be a built-in, first class citizen, instead of a community made "extension" is excellent. Instead of needing to consult a third-party resource like Home Manager's documentation you can simply use what you already know about Guix and, if you happen to hit a wall, you can just read the official handbook which is guaranteed to always stay up to date with the rest of the system.&lt;/item&gt;
      &lt;item&gt;Package availability: As long as you largely use FOSS stuff (which is much easier than one might think), the amount of choice is awesome. I could basically just copy over the list of packages from my Nix config and practically everything had an equivalent.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scheme: I'm not really a seasoned Schemer, but I have dabbled in the language previously and it feels so much better to me than Nix (the language) ever did. One great benefit of this is that it's a lot easier to start digging into package definitions to figure things out for yourself.&lt;/p&gt;
        &lt;p&gt;This is "Freedom 1" of GNU's Four Essential Freedoms in effect. Since the code is pretty much just Scheme and the different mechanisms available are fairly well documented (see caveat below), the barrier to entry is much lower than with Nix in my opinion.7&lt;/p&gt;
        &lt;p&gt;Another nice benefit of this is that you can use Emacs' extensive Scheme support to help your configuration. Tools like Geiser can plug right into Guix and help you find package and function names and, once you're experienced enough, debug your config/packages on the fly. I personally haven't yet achieved mastery of such level yet, but having the REPL confirm if I've entered names in correctly before running the code is already a boon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ease of hacking: In the "to tinker on" sense, rather than "being insecure". With Nix, merely pulling in Nixpkgs is an effort, due to the repository being massive. My otherwise beefy machine struggled to switch between branches and make commits, which doesn't exactly inspire confidence in contributing, even though it was otherwise something I was excited to do. Meanwhile, with Guix I was able to get a fully functioning development environment in 15 minutes tops, which includes cloning the repo, authenticating all commits, generating bytecode for the entire repository, and getting Emacs set up to work nice with the codebase.&lt;/p&gt;&lt;p&gt;Not to mention, at the time of writing my Nixpkgs PR of guile-colorized is still not accepted, despite being open since October, 2025. Which is kind of disheartening, when the package is really trivial and has a very low blast-radius. With Guix I got an answer to an extremely noobish question on my first PR in mere hours.&lt;/p&gt;&lt;p&gt;On a separate, but related note, I also found it a lot easier to test my package in a "live" environment as&lt;/p&gt;&lt;code&gt;guix pull&lt;/code&gt;supports a parameter called&lt;code&gt;--url&lt;/code&gt;which you can easily point to a folder on your own PC. So once I was confident my code should work, I could just "check out" my local repository clone and build it like I was an end user. This let me make sure it really does work.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;6.2. The Ambiguous&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Search:&lt;/p&gt;&lt;code&gt;guix search&lt;/code&gt;not taking an extra parameter like&lt;code&gt;nix search&lt;/code&gt;is both very convenient and a bit of a bummer.&lt;p&gt;Its absence is not a deal breaker, but I really loved how with Nix, you could search in anything, that has a flake. Be that Nixpkgs, a repo you downloaded, a repo that's on a git forge, etc. I remember being awestruck that I could just do&lt;/p&gt;&lt;code&gt;nix search github:mozilla/nixpkgs-mozilla&lt;/code&gt;and search for their builds of Firefox without having to manually check out anything.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The documentation: Oof, this one is a bit hard to pass definite judgment on.&lt;/p&gt;
        &lt;p&gt;On one hand I love the thoroughness of it all. You can get a fairly decent idea of what Guix, what it can do for your, how to use it, and how to extend it, just by reading the manual. It is evident that the Guix team and GNU in general takes its mission to educate using free software very seriously. Stuff like the Packaging tutorial make it very easy for complete beginners to hack together package definitions without needing to consult any other resource.&lt;/p&gt;
        &lt;p&gt;On the other hand, it really is just a manual, not a tutorial. What I mean by this is that concepts that could belong together aren't placed near each other. A simple example would be services and customizing them. Assuming, you're in one of the sub-pages of Services and you suddenly realize you want to replace/modify one of the services, you are left completely clueless how that works. You have to go to a completely different chapter and find one particular function's description and then apply what you learn there. The Guix Cookbook has some examples, but you have to know about the cookbook in the first place.&lt;/p&gt;
        &lt;p&gt;And before anyone misunderstands me, I'm fine with RTFM, but in my opinion one of the preconditions of mass-appeal is having "pre-chewed" solutions for common problems, that don't require perusing multiple chapters.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;6.3. The Bad&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Substitute server stability: I imagine this is an issue that only a massive bag of money could fix, but the CI/CD servers could definitely use some more processing power. It's really annoying when you're trying to test something and you're suddenly forced to wait 10-15 minutes because the server can only spare 50-100 kbps for you.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Content out there: Clearly this isn't the Guix team's fault (and it's something I'm trying to lessen with this post, even if just a tiny bit), but it's really hard to find good quality material when it comes to Guix.&lt;/p&gt;
        &lt;p&gt;I mean, sure, there is the excellent System Crafters tutorial series, and the odd gems like DThompson's dev env tutorial, but as a whole you're largely left to your own to trawl through the manual, IRC logs, Reddit threads, Codeberg and the previous issue tracker, etc. It's not an impossible task, especially if you're used to doing Linux things "the hard way", but it's certainly a far cry from such one-stop shops as the Nix Flakes book or Wombat's Book of Nix.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;Guix's own build speed: Nix excels in speed, so I was hoping Guix would be the same. Yet stuff like &lt;code&gt;guix pull&lt;/code&gt;really bog things down. Doubly so, if you want to update not just your own&lt;code&gt;guix&lt;/code&gt;instance, but also root's.&lt;/item&gt;
      &lt;item&gt;Clarity of commands: The fact that all concerns are lumped together (unlike Nix's many utilities) means that to the new user the many commands such as &lt;code&gt;guix pull&lt;/code&gt;,&lt;code&gt;guix {system, home} reconfigure&lt;/code&gt;,&lt;code&gt;guix update&lt;/code&gt;can easily feel overwhelming and unclear what's updating/changing what. With time I'm sure you obtain a sort of mental muscle memory and you never think about it again, but starting out it's definitely a confusing part.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;7. Overall&lt;/head&gt;
    &lt;quote&gt;1: (define-module (guix-home-config) 2: #:use-module (nongnu packages) 3: #:use-module (gnu packages) 4: #:use-module (gnu home) 5: #:use-module (gnu home services) 6: #:use-module (gnu home services shells) 7: #:use-module (gnu services) 8: #:use-module (gnu system shadow) 9: #:use-module (guix gexp)) 10: 11: (define %packages 12: (list "git" "openssh" "librewolf" "ripgrep" 13: "bat" "eza" "fd" "zoxide" "bc" "gimp" 14: "libreoffice" "jujutsu" "starship" "direnv" 15: "okular" "gwenview" "bitwarden-desktop" 16: "icedove-wayland" "telegram-desktop" 17: "emacs-vterm" "ispell" "hunspell" "wezterm")) 18: 19: (define %nonfree-packages 20: (list "steam-nvidia" 21: "mpv-nvidia")) 22: 23: (define home-config 24: (home-environment 25: (packages (specifications-&amp;gt;packages (append %nonfree-packages %packages))) 26: (services 27: (append 28: (list 29: (service home-bash-service-type 30: (home-bash-configuration 31: (aliases '(("ls" . "eza"))) 32: (bashrc (list (local-file "./bashrc.sh"))))) 33: 34: (service home-files-service-type 35: `((".guile" ,%default-dotguile) 36: (".Xdefaults" ,%default-xdefaults))) 37: 38: (service home-xdg-configuration-files-service-type 39: `(("gdb/gdbinit" ,%default-gdbinit) 40: ("nano/nanorc" ,%default-nanorc)))) 41: 42: %base-home-services)))) 43: 44: home-config&lt;/quote&gt;
    &lt;p&gt;In a nutshell I'm very positively surprised by Guix System. After struggling so much with it years ago, this time everything just clicked after a much shorter battle. So much so that I'm happy to make it my daily driver for the foreseeable future. Beyond the slightly slower execution speed, I'm getting a comparable experience to NixOS, with all the usual pros a declarative environment brings and without having to put up with Nixlang.&lt;/p&gt;
    &lt;p&gt; My only recurring issues so far are the occasional slow download speeds and that I have to start my kernel in &lt;code&gt;nomodeset&lt;/code&gt; because otherwise the graphical environment crashes without me being able to switch to a TTY. It's a bummer, but honestly, I'm not too bothered by it so far. I'm trusting a driver update will fix it soon enough and, if not, it's not exactly difficult to throw in a kernel parameter into your config.
&lt;/p&gt;
    &lt;p&gt;I'm hoping to do a followup post about packaging in Guix, because I've been dipping my toes into it by trying to package Wezterm and the journey there was similarly arduous as installing the system itself.&lt;/p&gt;
    &lt;p&gt;Till then, thank you for reading and see you next time!&lt;/p&gt;
    &lt;head rend="h2"&gt;8. Notes&lt;/head&gt;
    &lt;p&gt;The stuff you see below are all I managed to write down mid-process. Some of these I threw it into the file from Nano, some from half-broken X11 sessions. Because of this, it's not exactly well-edited, but I hope it might provide a glimpse into my mind at the time.&lt;/p&gt;
    &lt;quote&gt;&lt;item&gt;The installer is decently simple&lt;/item&gt;&lt;item&gt;I appreciate the warning about incompatible hardware&lt;/item&gt;&lt;item&gt;2.5 hours at least to install (mirrors throttle connection to 50kbps)&lt;/item&gt;&lt;item&gt;KDE is simply not working out of the box (titlebars are missing)&lt;/item&gt;&lt;item&gt;It seems to also default to X11, when I'm looking for Wayland&lt;/item&gt;&lt;item&gt;The first&lt;/item&gt;&lt;code&gt;guix pull&lt;/code&gt;is horrendously slow&lt;item&gt;Wayland continues to elude me, seems to be an Nvidia issue&lt;/item&gt;&lt;item&gt;IRC recommends&lt;/item&gt;&lt;code&gt;nomodeset&lt;/code&gt;, doesn't help&lt;item&gt;Try enabling Nonguix, system no longer boots&lt;/item&gt;&lt;item&gt;Try installing using the Nonguix ISO&lt;/item&gt;&lt;item&gt;Lots of errors, terribly old release&lt;/item&gt;&lt;item&gt;Having to&lt;/item&gt;&lt;code&gt;guix pull&lt;/code&gt;myself to the present day again&lt;item&gt;Also I'm missing the introduction, so I have to run it using&lt;/item&gt;&lt;code&gt;--disable-authentication&lt;/code&gt;, not great, but I trust the Codeberg repo&lt;item&gt;At least the download speed seems to have normalized&lt;/item&gt;&lt;item&gt;It isn't entirely clear when you have to use&lt;/item&gt;&lt;code&gt;sudo&lt;/code&gt;&lt;item&gt;Running&lt;/item&gt;&lt;code&gt;i3&lt;/code&gt;on a shitty low-res has a certain vibe to it, but I'd prefer a system working out of the box&lt;/quote&gt;
    &lt;head rend="h2"&gt;Footnotes:&lt;/head&gt;
    &lt;p&gt;Well, if only life was so easy. What I mean here is that on my personal computer, I've not had Windows since about 2015. For work purposes my hands are currently chained to MacOS (though even there I use a Debian-based container).&lt;/p&gt;
    &lt;p&gt;No disrespect to Ubuntu-users, past and present! My opinion at the time was quite ignorant and nowadays I far more appreciate an easy to maintain system as you'll see from the rest of this post.&lt;/p&gt;
    &lt;p&gt;It's merely a hunch, but it feels to me that the servers are far slower during the (Central-European) night. During midday, I get really good download speeds, but after around 8 PM, it slows to a crawl.&lt;/p&gt;
    &lt;p&gt;Which, for the uninitiated, is an IRC client built into Emacs. This editor continues to wow me every day.&lt;/p&gt;
    &lt;p&gt;I probably could have figured it out in time. But at this point I was a bit exasperated and I really didn't want to type in an 10x4 character hexadecimal code by hand.&lt;/p&gt;
    &lt;p&gt;Goodness gracious, Portal 2 is almost 15 years old‚Ä¶&lt;/p&gt;
    &lt;p&gt;That being said, my Nix experience was still very much helpful here. Understanding stuff such as build phases, why packages need to be patched and how this usually works, and what the different build flags mean is pretty much a must if you want to attain an understanding deeper than just "kinda getting it."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835612</guid><pubDate>Sat, 31 Jan 2026 11:22:24 +0000</pubDate></item><item><title>Insane Growth Goldbridge (YC F25) Is Hiring a Forward Deployed Engineer</title><link>https://www.ycombinator.com/companies/goldbridge/jobs/78gGEHh-forward-deployed-engineer</link><description>&lt;doc fingerprint="5ff0179310e1c520"&gt;
  &lt;main&gt;
    &lt;p&gt;Ramp for Real Estate&lt;/p&gt;
    &lt;p&gt;Goldbridge is building the financial operating system for the largest asset class in the world ‚Äì real estate. More than $1T in rent flows through landlord bank accounts annually, with roughly a quarter locked in idle reserves and security deposits ‚Äì and billions more leaking from unnecessary property expenses. And with $2.5T in real estate loans about to mature in 2027/28, property owners are desperate to boost their income ASAP. Goldbridge solves this problem by creating the first AI-powered banking platform for real estate owners. We are backed by Y Combinator and other world-class investors, and our CEO is a 2x YC founder, former White House advisor, and 100-unit real estate owner/operator who understands this industry deeply. See full job description here: https://www.goldbridgebanking.com/careers/forward-deployed-engineer&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835834</guid><pubDate>Sat, 31 Jan 2026 12:00:22 +0000</pubDate></item><item><title>US reportedly investigate claims that Meta can read encrypted WhatsApp messages</title><link>https://www.theguardian.com/technology/2026/jan/31/us-authorities-reportedly-investigate-claims-that-meta-can-read-encrypted-whatsapp-messages</link><description>&lt;doc fingerprint="c4981111303790cc"&gt;
  &lt;main&gt;
    &lt;p&gt;US authorities have reportedly investigated claims that Meta can read users‚Äô encrypted chats on the WhatsApp messaging platform, which it owns.&lt;/p&gt;
    &lt;p&gt;The reports follow a lawsuit filed last week, which claimed Meta ‚Äúcan access virtually all of WhatsApp users‚Äô purportedly ‚Äòprivate‚Äô communications‚Äù.&lt;/p&gt;
    &lt;p&gt;Meta has denied the allegation, reported by Bloomberg, calling the lawsuit‚Äôs claim ‚Äúcategorically false and absurd‚Äù. It suggested the claim was a tactic to support the NSO Group, an Israeli firm that develops spyware used against activists and journalists, and which recently lost a lawsuit brought by WhatsApp.&lt;/p&gt;
    &lt;p&gt;The firm that filed last week‚Äôs lawsuit against Meta, Quinn Emanuel Urquhart &amp;amp; Sullivan, attributes the allegation to unnamed ‚Äúcourageous‚Äù whistleblowers from Australia, Brazil, India, Mexico and South Africa.&lt;/p&gt;
    &lt;p&gt;Quinn Emanuel is, in a separate case, helping to represent the NSO Group in its appeal against a judgment from a US federal court last year, which ordered it to pay $167m to WhatsApp for violating its terms of service in its deployment of Pegasus spyware against more than 1,400 users.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe‚Äôre pursuing sanctions against Quinn Emanuel for filing a meritless lawsuit that was designed purely to grab headlines,‚Äù said Carl Woog, a Meta spokesperson, in a statement. ‚ÄúThis is the same firm that is trying to help NSO overturn an injunction that barred their operations for targeting journalists and government officials with spyware.‚Äù&lt;/p&gt;
    &lt;p&gt;Adam Wolfson, a partner at Quinn Emanuel said: ‚ÄúOur colleagues‚Äô defence of NSO on appeal has nothing to do with the facts disclosed to us and which form the basis of the lawsuit we brought for worldwide WhatsApp users.&lt;/p&gt;
    &lt;p&gt;‚ÄúWe look forward to moving forward with those claims and note WhatsApp‚Äôs denials have all been carefully worded in a way that stops short of denying the central allegation in the complaint ‚Äì that Meta has the ability to read WhatsApp messages, regardless of its claims about end-to-end encryption.‚Äù&lt;/p&gt;
    &lt;p&gt;Steven Murdoch, professor of security engineering at UCL, said the lawsuit was ‚Äúa bit strange‚Äù. ‚ÄúIt seems to be going mostly on whistleblowers, and we don‚Äôt know much about them or their credibility,‚Äù he said. ‚ÄúI would be very surprised if what they are claiming is actually true.‚Äù&lt;/p&gt;
    &lt;p&gt;If WhatsApp were, indeed, reading users‚Äô messages, this was likely to have been discovered by staff and would end the business, he said. ‚ÄúIt‚Äôs very hard to keep secrets inside a company. If there was something as scandalous as this going on, I think it‚Äôs very likely that it would have leaked out from someone within WhatsApp.‚Äù&lt;/p&gt;
    &lt;p&gt;The Bloomberg article cites reports and interviews from officials within the US Department of Commerce in claiming that the US has investigated whether Meta could read WhatsApp messages. However, a spokesperson for the department called these assertions ‚Äúunsubstantiated‚Äù.&lt;/p&gt;
    &lt;p&gt;WhatsApp bills itself as an end-to-end encrypted platform, which means that messages can be read only by their sender and recipient, and are not decoded by a server in the middle.&lt;/p&gt;
    &lt;p&gt;This contrasts with some other messaging apps, such as Telegram, which encrypt messages between a sender and its own servers, preventing third parties from reading the messages, but allowing them ‚Äì in theory ‚Äì to be decoded and read by Telegram itself.&lt;/p&gt;
    &lt;p&gt;A senior executive in the technology sector told the Guardian that WhatsApp‚Äôs vaunted privacy ‚Äúleaves much to be desired‚Äù, given the platform‚Äôs willingness to collect metadata on its users, such as their profile information, their contact lists, and who they speak to and when.&lt;/p&gt;
    &lt;p&gt;However, the ‚Äúidea that WhatsApp can selectively and retroactively access the content of [end-to-end encrypted] individual chats is a mathematical impossibility‚Äù, he said.&lt;/p&gt;
    &lt;p&gt;Woog, of Meta, said: ‚ÄúWe‚Äôre pursuing sanctions against Quinn Emanuel for filing a meritless lawsuit that was designed purely to grab headlines. WhatsApp‚Äôs encryption remains secure and we‚Äôll continue to stand up against those trying to deny people‚Äôs right to private communication.‚Äù&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46836487</guid><pubDate>Sat, 31 Jan 2026 13:27:23 +0000</pubDate></item></channel></rss>