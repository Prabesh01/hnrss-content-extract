<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sat, 31 Jan 2026 21:11:42 +0000</lastBuildDate><item><title>Peerweb: Decentralized website hosting via WebTorrent</title><link>https://peerweb.lol/</link><description>&lt;doc fingerprint="868e3ff18d2cd634"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;ğŸª PeerWeb&lt;/head&gt;
    &lt;head rend="h2"&gt;Decentralized Website Hosting via WebTorrent&lt;/head&gt;
    &lt;head rend="h3"&gt;ğŸ¤” What is PeerWeb?&lt;/head&gt;
    &lt;p&gt;PeerWeb is a revolutionary way to host and share websites using WebTorrent technology. Instead of relying on centralized servers, websites are distributed across a peer-to-peer network, making them censorship-resistant and always available. ğŸŒâœ¨&lt;/p&gt;
    &lt;head rend="h3"&gt;ğŸ“¤ Quick Upload&lt;/head&gt;
    &lt;head rend="h4"&gt;Drag &amp;amp; Drop Your Website&lt;/head&gt;
    &lt;p&gt;Drop a folder with your website files&lt;/p&gt;
    &lt;head rend="h3"&gt;ğŸ“š How to Use PeerWeb&lt;/head&gt;
    &lt;head rend="h3"&gt;ğŸ’¡ Load Existing Site&lt;/head&gt;
    &lt;p&gt;To load a website from a torrent hash, enter it below:&lt;/p&gt;
    &lt;p&gt;ğŸ¯ Just the hash! PeerWeb automatically adds the magnet link prefix and trackers.&lt;/p&gt;
    &lt;head rend="h3"&gt;ğŸ§ª Demos&lt;/head&gt;
    &lt;p&gt; Functionality test page: &lt;lb/&gt;https://peerweb.lol/?orc=90c020bd252639622a14895a0fad713b91e0130c &lt;/p&gt;
    &lt;p&gt; SomaFM on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=908d19242ae1461f333a516d1f8b89c13ef2d259 &lt;/p&gt;
    &lt;p&gt; Chess on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=1e14b1ba7fcd03e5f165d53ed8223a333349db04 &lt;/p&gt;
    &lt;p&gt; Text Editor app on PeerWeb:&lt;lb/&gt;https://peerweb.lol/?orc=4e5f1204dcec68195bfcc89f9410a0b70a0ddfac &lt;/p&gt;
    &lt;head rend="h3"&gt;ğŸ› Debug Mode&lt;/head&gt;
    &lt;p&gt;For developers and troubleshooting, add &amp;amp;debug=true to see detailed progress:&lt;/p&gt;
    &lt;code&gt;https://peerweb.lol?orc=ABC123DEF456...&amp;amp;debug=true&lt;/code&gt;
    &lt;head rend="h3"&gt;ğŸš€ Advanced Options&lt;/head&gt;
    &lt;head rend="h3"&gt;ğŸ’¾ Smart Caching&lt;/head&gt;
    &lt;p&gt;PeerWeb caches visited sites for lightning-fast loading! ğŸš€&lt;/p&gt;
    &lt;head rend="h3"&gt;ğŸ›¡ï¸ Security Features&lt;/head&gt;
    &lt;p&gt;Enhanced security with DOMPurify integration! ğŸ”’&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46829582</guid><pubDate>Fri, 30 Jan 2026 20:40:00 +0000</pubDate></item><item><title>Show HN: I trained a 9M speech model to fix my Mandarin tones</title><link>https://simedw.com/2026/01/31/ear-pronunication-via-ctc/</link><description>&lt;doc fingerprint="2d64271217290110"&gt;
  &lt;main&gt;
    &lt;p&gt;TL;DR: Mandarin pronunciation has been hard for me, so I took ~300 hours of transcribed speech and trained a small CTC model to grade my pronunciation. You can try it here.&lt;/p&gt;
    &lt;p&gt;In my previous post about Langseed, I introduced a platform for defining words using only vocabulary I had already mastered. My vocabulary has grown since then, but unfortunately, people still struggle to understand what I'm saying.&lt;/p&gt;
    &lt;p&gt;Part of the problem is tones. They're fairly foreign to me, and I'm bad at hearing my own mistakes, which is deeply frustrating when you donâ€™t have a teacher.&lt;/p&gt;
    &lt;head rend="h2"&gt;First attempt: pitch visualisation&lt;/head&gt;
    &lt;p&gt;My initial plan was to build a pitch visualiser: split incoming audio into small chunks, run an FFT, extract the dominant pitch over time, and map it using an energy-based heuristic, loosely inspired by Praat.&lt;/p&gt;
    &lt;p&gt;But this approach quickly became brittle. There were endless special cases: background noise, coarticulation, speaker variation, voicing transitions, and so on.&lt;/p&gt;
    &lt;p&gt;And if thereâ€™s one thing weâ€™ve learned over the last decade, itâ€™s the bitter lesson: when you have enough data and compute, learned representations usually beat carefully hand-tuned systems.&lt;/p&gt;
    &lt;p&gt;So instead, I decided to build a deep learningâ€“based Computer-Assisted Pronunciation Training (CAPT) system that could run entirely on-device. There are already commercial APIs that do this, but hey, whereâ€™s the fun in that?&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;I treated this as a specialised Automatic Speech Recognition (ASR) task. Instead of just transcribing text, the model needs to be pedantic about how something was said.&lt;/p&gt;
    &lt;p&gt;I settled on a Conformer encoder trained with CTC (Connectionist Temporal Classification) loss.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Conformer?&lt;/head&gt;
    &lt;p&gt;Speech is weird: you need to catch both local and global patterns:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Local interactions&lt;/p&gt;&lt;lb/&gt;The difference between a retroflex zh and an alveolar z happens in a split second. CNNs are excellent at capturing these short-range spectral features.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Global interactions&lt;/p&gt;&lt;lb/&gt;Mandarin tones are relative (a "high" pitch for me might be low for a child) and context-dependent (tone sandhi)1. Transformers excel at modeling this longer-range context.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Conformers combine both: convolution for local detail, attention for global structure.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why CTC?&lt;/head&gt;
    &lt;p&gt;Most modern ASR models (e.g. Whisper) are sequence-to-sequence: they turn audio into the most likely text. The downside is they'll happily auto-correct you.&lt;/p&gt;
    &lt;p&gt;Thatâ€™s a feature for transcription, but itâ€™s a bug for language learning. If my tone is wrong, I donâ€™t want the model to guess what I meant. I want it to tell me what I actually said.&lt;/p&gt;
    &lt;p&gt;CTC works differently. It outputs a probability distribution for every frame of audio (roughly every 40 ms). To handle alignment, it introduces a special &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt; token.&lt;/p&gt;
    &lt;p&gt;If the audio is "hello", the raw output might look like:&lt;/p&gt;
    &lt;code&gt;h h h &amp;lt;blank&amp;gt; e e &amp;lt;blank&amp;gt; l l l l &amp;lt;blank&amp;gt; l l o o o
&lt;/code&gt;
    &lt;p&gt;Collapsing repeats and removing blanks gives &lt;code&gt;hello&lt;/code&gt;. This forces the model has to deal with what I actually said, frame by frame.&lt;/p&gt;
    &lt;head rend="h2"&gt;Forced alignment: knowing when you said it&lt;/head&gt;
    &lt;p&gt;CTC tells us what was said, but not exactly when.&lt;/p&gt;
    &lt;p&gt;For a 3-second clip, the model might output a matrix with ~150 time steps (columns), each containing probabilities over all tokens (rows). Most of that matrix is just &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If the user reads "NÇ hÇo" (ni3, hao3), we expect two regions of high probability: one for &lt;code&gt;ni3&lt;/code&gt;, one for &lt;code&gt;hao3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We need to find a single, optimal path through this matrix that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Starts at the beginning&lt;/item&gt;
      &lt;item&gt;Ends at the end&lt;/item&gt;
      &lt;item&gt;Passes through &lt;code&gt;ni3&lt;/code&gt;â†’&lt;code&gt;hao3&lt;/code&gt;in order&lt;/item&gt;
      &lt;item&gt;Maximises total probability&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This is exactly what the Viterbi algorithm computes, using dynamic programming.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tokenisation: Pinyin + tone as first-class tokens&lt;/head&gt;
    &lt;p&gt;Most Mandarin ASR systems output Hanzi. That hides pronunciation errors, because the writing system encodes meaning rather than pronunciation.&lt;/p&gt;
    &lt;p&gt;Instead, I created a token for every Pinyin syllable + tone:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;zhong1&lt;/code&gt;is one token&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;zhong4&lt;/code&gt;is a completely different token&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If I say the wrong tone, the model explicitly predicts the wrong token ID.&lt;/p&gt;
    &lt;p&gt;I also normalised the neutral tone by forcing it to be tone 5 (&lt;code&gt;ma5&lt;/code&gt;). This resulted in a vocabulary of 1,254 tokens, plus &lt;code&gt;&amp;lt;unk&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Training&lt;/head&gt;
    &lt;p&gt;I combined the AISHELL-1 and Primewords datasets (~300 hours total), augmented by SpecAugment (time/frequency masking). On 4Ã— NVIDIA GeForce RTX 4090s, training took about 8 hours. Instead of obsessing over loss, I mostly focused on these metrics:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;TER (Token Error Rate): overall accuracy.&lt;/item&gt;
      &lt;item&gt;Tone Accuracy: accuracy over tones 1-5.&lt;/item&gt;
      &lt;item&gt;Confusion Groups: errors between difficult initial pairs like zh/ch/sh vs z/c/s.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Honey, I shrank the model&lt;/head&gt;
    &lt;p&gt;I started with a "medium" model (~75M parameters). It worked well, but I wanted something that could run in a browser or on a phone without killing the battery.&lt;/p&gt;
    &lt;p&gt;So I kept shrinking it, and I was honestly surprised by how little accuracy I lost:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;# Parameters&lt;/cell&gt;
        &lt;cell role="head"&gt;TER&lt;/cell&gt;
        &lt;cell role="head"&gt;Tone accuracy&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;75M&lt;/cell&gt;
        &lt;cell&gt;4.83%&lt;/cell&gt;
        &lt;cell&gt;98.47%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;35M&lt;/cell&gt;
        &lt;cell&gt;5.16%&lt;/cell&gt;
        &lt;cell&gt;98.36%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;9M&lt;/cell&gt;
        &lt;cell&gt;5.27%&lt;/cell&gt;
        &lt;cell&gt;98.29%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The 9M-parameter model was barely worse. This strongly suggests the task is data-bound, not compute-bound.&lt;/p&gt;
    &lt;p&gt;The FP32 model was ~37 MB. After INT8 quantisation, it shrank to ~11 MB with a negligible accuracy drop (+0.0003 TER). Small enough to load instantly via &lt;code&gt;onnxruntime-web&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Alignment bug: silence ruins everything&lt;/head&gt;
    &lt;p&gt;To highlight mistakes, we need forced alignment. But I hit a nasty bug with leading silence.&lt;/p&gt;
    &lt;p&gt;I recorded myself saying "æˆ‘å–œæ¬¢â€¦" and paused for a second before speaking. The model confidently told me my first syllable was wrong. Confidence score: 0.0.&lt;/p&gt;
    &lt;p&gt;Why?&lt;/p&gt;
    &lt;p&gt;The alignment assigned the silent frames to &lt;code&gt;wo3&lt;/code&gt;. When I averaged probabilities over that span, the overwhelming &lt;code&gt;&amp;lt;blank&amp;gt;&lt;/code&gt; probability completely drowned out &lt;code&gt;wo3&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;The fix&lt;/head&gt;
    &lt;p&gt;I decoupled UI spans (what gets highlighted) from scoring frames (what contributes to confidence).&lt;/p&gt;
    &lt;p&gt;We simply ignore frames where the model is confident itâ€™s seeing silence:&lt;/p&gt;
    &lt;code&gt;def _filter_nonblank_frames(span_logp: torch.Tensor, blank_id: int = 0, thr: float = 0.7):
    """
    Only keep frames where the probability of &amp;lt;blank&amp;gt; is below a threshold.
    If we filter everything (total silence), we fall back to scoring the whole span.
    """
    p_blank = span_logp[:, blank_id].exp()
    keep = p_blank &amp;lt; thr
    if keep.any():
        return span_logp[keep]
    return span_logp  # Fallback
&lt;/code&gt;
    &lt;p&gt;This single change moved my confidence score for the first syllable from 0.0 â†’ 0.99.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I can already feel my pronunciation improving while beta testing this. Itâ€™s strict and unforgiving, exactly what I needed.&lt;/p&gt;
    &lt;p&gt;Native speakers, interestingly, complained that they had to over-enunciate to get marked correct. Thatâ€™s likely a domain-shift issue: AISHELL is mostly read speech, while casual speech is faster and more slurred. Kids do poorly too: their pitch is higher, and they're basically absent from the training data. Adding conversational datasets like Common Voice feels like the obvious next step.&lt;/p&gt;
    &lt;p&gt;You can try the live demo here. It runs entirely in your browser. The download is ~13MB, still smaller than most websites today.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;For example, Tone 3 followed by Tone 3 is pronounced as Tone 2 followed by Tone 3 (ä½ å¥½ â†’ nÃ­ hÇo). â†©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46832074</guid><pubDate>Sat, 31 Jan 2026 00:51:27 +0000</pubDate></item><item><title>Sumerian Star Map Recorded the Impact of an Asteroid (2024)</title><link>https://archaeologyworlds.com/5500-year-old-sumerian-star-map-recorded/</link><description>&lt;doc fingerprint="31845f72a1c499a"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;5,500-Year-Old Sumerian Star Map Recorded the Impact of a Massive Asteroid&lt;/head&gt;
    &lt;p&gt;For more than 150 years scientists have tried to solve the mystery of a notorious cuneiform clay tablet that reveals that in the past the impact case of so-called KÃ¶fel was detected. The circular stone-cast tablet was discovered in the late 1800s from the 650 BC King Ashurbanipal â€˜s underground library in Nineveh, Iraq.&lt;/p&gt;
    &lt;p&gt;Data processing, which was long believed to be an Assyrian tablet, mirrored the sky over Mesopotamia in 3300 BC and proved to be much more ancient Sumerian origin.&lt;/p&gt;
    &lt;p&gt;The tablet is the first astronomical instrument, the â€œAstrolabe.â€ It consists of a segmented, disk-shaped star chart with marked units of angle measure inscribed upon the rim.&lt;/p&gt;
    &lt;p&gt;Unfortunately considerable parts of the planisphere on this tablet are missing (approximately 40%), damage which dates to the sacking of Nineveh. The reverse of the tablet is not inscribed.&lt;/p&gt;
    &lt;p&gt;Still under study by modern scholars, the cuneiform tablet in the British Museum collection No K8538 (known as â€œthe Planisphereâ€) provides extraordinary proof for the existence of sophisticated Sumerian astronomy.&lt;/p&gt;
    &lt;p&gt;In 2008 two authors, Alan Bond and Mark Hempsell published a book about the tablet called â€œA Sumerian Observation of the Kofelsâ€™ Impact Eventâ€.&lt;/p&gt;
    &lt;p&gt;Raising a storm in archaeological circles, they re-translated the cuneiform text and asserted the tablet records an ancient asteroid strike, the KÃ¶felsâ€™ Impact, which struck Austria sometime around 3100 BC.&lt;/p&gt;
    &lt;p&gt;The giant landslide centred at KÃ¶fels in Austria is 500m thick and five kilometres in diameter and has long been a mystery since geologists first looked at it in the 19th century.&lt;/p&gt;
    &lt;p&gt;The conclusion drawn by research in the middle 20th century was that it must be due to a very large meteor impact because of the evidence of crushing pressures and explosions. But this view lost favor as a much better understanding of impact sites developed in the late 20th century.&lt;/p&gt;
    &lt;p&gt;In the case of KÃ¶fels there is no crater, so to modern eyes it does not look as an impact site should look. However, the evidence that puzzled the earlier researchers remains unexplained by the view that it is just another landslide.&lt;/p&gt;
    &lt;p&gt;So what is the connection between the sophisticated Sumerian star chart discovered in the underground library in Nineveh and mysterious impact that took place in Austria?&lt;/p&gt;
    &lt;p&gt;Examination of the clay tablet reveals that it is an astronomical work as it has drawings of constellations on it and the text has known constellation names. It has attracted a lot of attention but in over a hundred years nobody has come up with a convincing explanation as to what it is.&lt;/p&gt;
    &lt;p&gt;With modern computer programs that can simulate trajectories and reconstruct the night sky thousands of years ago the researchers have established what the Planisphere tablet refers to. It is a copy of the night notebook of a Sumerian astronomer as he records the events in the sky before dawn on 29 June 3123 BC (Julian calendar).&lt;/p&gt;
    &lt;p&gt;Half the tablet records planet positions and cloud cover, the same as any other night, but the other half of the tablet records an object large enough for its shape to be noted even though it is still in space.&lt;/p&gt;
    &lt;p&gt;The astronomers made an accurate note of its trajectory relative to the stars, which to an error better than one degree is consistent with an impact at KÃ¶fels.&lt;/p&gt;
    &lt;p&gt;The observation suggests the asteroid is over a kilometer in diameter and the original orbit about the Sun was an Aten type, a class of asteroids that orbit close to the Earth, that are resonant with the Earthâ€™s orbit.&lt;/p&gt;
    &lt;p&gt;This trajectory explains why there is no crater at KÃ¶fels. The incoming angle was very low (six degrees) and means the asteroid clipped a mountain called Gamskogel above the town of LÃ¤ngenfeld, 11 kilometers from KÃ¶fels, and this caused the asteroid to explode before it reached its final impact point. As it traveled down the valley it became a fireball, around five kilometers in diameter (the size of the landslide).&lt;/p&gt;
    &lt;p&gt;When it hit KÃ¶fels it created enormous pressures that pulverized the rock and caused the landslide but because it was no longer a solid object it did not create a classic impact crater.&lt;/p&gt;
    &lt;p&gt;Mark Hempsell, discussing the KÃ¶fels event, said: â€œAnother conclusion can be made from the trajectory. The back plume from the explosion (the mushroom cloud) would be bent over the Mediterranean Sea re-entering the atmosphere over the Levant, Sinai, and Northern Egypt.&lt;/p&gt;
    &lt;p&gt;â€œThe ground heating though very short would be enough to ignite any flammable material â€“ including human hair and clothes. It is probable more people died under the plume than in the Alps due to the impact blast.â€&lt;/p&gt;
    &lt;p&gt;In other words, the remarkable ancient star map shows that the Sumerians made an observation of an Aten asteroid over a kilometer in diameter that impacted KÃ¶fels in Austria in the early morning of 29th June 3123 BC.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46834313</guid><pubDate>Sat, 31 Jan 2026 07:32:51 +0000</pubDate></item><item><title>We have ipinfo at home or how to geolocate IPs in your CLI using latency</title><link>https://blog.globalping.io/we-have-ipinfo-at-home-or-how-to-geolocate-ips-in-your-cli-using-latency/</link><description>&lt;doc fingerprint="2ca4589a35236a90"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We have ipinfo at home or how to geolocate IPs in your CLI using latency&lt;/head&gt;
    &lt;quote&gt;&lt;p&gt;TLDR: I made a CLI tool that can resolve an IP address to a country, US state and even a city. https://github.com/jimaek/geolocation-tool&lt;/p&gt;&lt;lb/&gt;It works well and confirms ipinfo's findings.&lt;/quote&gt;
    &lt;p&gt;Recently, I read how ipinfo finally proved what most technical people assumed: VPN providers don't actually maintain a crazy amount of infrastructure in hundreds of countries. They simply fake the IP geolocation by intentionally providing wrong location data to ARIN, RIPE, and Geo DB providers via geofeeds.&lt;/p&gt;
    &lt;p&gt;They achieved their results using a novel approach compared to other geo IP providers. Based on their blog and HackerNews comments, they built a large probe network and used it to trace and ping every (or most) IP addresses on the internet.&lt;/p&gt;
    &lt;p&gt;This latency and hop data, most likely along with advanced algorithms and data cross-reference, provides a reliable way of correctly detecting the physical geolocation of an IP address, without relying on faked data available in public sources.&lt;/p&gt;
    &lt;p&gt;This is a very interesting approach that makes total sense, and I'm sure their clients appreciate it and heavily rely on it.&lt;/p&gt;
    &lt;p&gt;While I can't ping every single IP address on the internet from hundreds of locations just yet, I can do it to a limited subset using Globalping. So I decided to try it out and see if I can replicate their results and build a small tool to allow anyone to do the same.&lt;/p&gt;
    &lt;p&gt;Globalping is an open-source, community-powered project that allows users to self-host container-based probes. These probes then become part of our public network, which allows anyone to use them to run network testing tools such as ping and traceroute.&lt;/p&gt;
    &lt;p&gt;At the moment, the network has more than 3000 probes, which in theory should be plenty to geolocate almost any IP address down to a country and even a US state level.&lt;/p&gt;
    &lt;p&gt;To automate and simplify this process, I made a little CLI tool using the globalping-ts library. My original idea was simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Accept a single IP as input&lt;/item&gt;
      &lt;item&gt;Ping it a few times per continent to select the continent&lt;/item&gt;
      &lt;item&gt;Then ping the IP from many different probes on that continent&lt;/item&gt;
      &lt;item&gt;Group and sort the results; the country with the lowest latency should be the correct one&lt;/item&gt;
      &lt;item&gt;And as a bonus, repeat the same process for USA states if the winning country was the US&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Essentially, what I had to do was simply create a few measurements and pass the location I needed using Globalpingâ€™s magic field, which would automatically figure out what I was looking for and select a few pseudo-random probes that fit the location and limit.&lt;/p&gt;
    &lt;p&gt;Now initially, I used &lt;code&gt;ping&lt;/code&gt; with 2 packets to run all measurements as quickly as possible, but I quickly realized it wasnâ€™t a good idea as most networks block ICMP traffic. Next, I tried switching to TCP-based &lt;code&gt;ping&lt;/code&gt;, which required trying a few popular ports to get it to work. I quickly realized this was too complicated and unreliable and switched to &lt;code&gt;traceroute&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;It worked perfectly. Even though &lt;code&gt;traceroute&lt;/code&gt; uses ICMP by default, it did not matter to me if the target IPâ€™s network allowed ICMP or not, I simply analyzed the latency of the last available hop. Even if you block ICMP, your upstream most likely allows it, and in most cases, itâ€™s located in the same country.&lt;/p&gt;
    &lt;p&gt;Of course, this means the resulting data is not 100% perfect. A better approach would be to analyze each IP using different methods, including TCP and UDP-based &lt;code&gt;traceroute&lt;/code&gt; on different ports, and expand to the last few hops instead of just one. Maybe even try to figure out the location of the registered ASNs and use a weights system in combination with public whois info in order to â€œvoteâ€ for the right location based on different inputs. Probably even mark low certainty IPs to be retested with a double amount of probes. (end of rant)&lt;/p&gt;
    &lt;p&gt;But thatâ€™s something for a commercial provider to figure out, which it seems they did.&lt;/p&gt;
    &lt;p&gt;For continent detection, I decided to use just 5 probes per continent; the results were extremely accurate. Although for IPs just on the "border" of continents it might be ineffective, a higher amount of probes would generate better results. For this use case, it was good enough.&lt;/p&gt;
    &lt;p&gt;My home IP in central Europe was too easy to detect:&lt;/p&gt;
    &lt;code&gt;Phase 1: Detecting continent...
  North America: 137.18 ms
  Europe: 32.39 ms
  Asia: 174.54 ms
  South America: 215.08 ms
  Oceania: 244.15 ms
  Africa: 156.83 ms
&lt;/code&gt;
    &lt;p&gt;In phase 2, all we need to do is run a single measurement with the winning continent as the location and a higher limit. Initially, I started with 250 probes with great accuracy.&lt;/p&gt;
    &lt;p&gt;Eventually, I decided to drop down to 50 as the default. Based on my tests, the results continued to look really good, and it would allow the tool to be run even without authentication, as the Globalping API allows 250 tests per hour per IP and 50 probes per measurement.&lt;/p&gt;
    &lt;p&gt;Although I recommend registering for a free account at https://dash.globalping.io/ and authenticating with a token to get up to 500 tests per hour and run more tests.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: If you need more tests than that, you can either host a probe to generate passive credits to be used as tests, or donate via GitHub Sponsors. We will automatically detect it and credit your account.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;Phase 2: Detecting country...
  Measuring from 50 probes...

  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100.0%   50/50 - Best: PL (7.29 ms)                    

Top 3 Locations:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1.. Poland, EU                               7.29 ms
  2.. Germany, EU                              13.42 ms
  3.. Lithuania, EU                            17.65 ms

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                      SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Location: Poland, EU
  Minimum Latency: 7.29 ms
  Confidence: Medium
&lt;/code&gt;
    &lt;p&gt;Great, now we have a basic IP-to-country resolver that only takes a few seconds to provide a response, and I didnâ€™t even have to understand or write any complicated math. Although Iâ€™m sure someone smarter could use a formula to geolocate IPs with even fewer probes and higher accuracy.&lt;/p&gt;
    &lt;p&gt;For phase 3, we want to resolve the US to a specific state or territory, just like ipinfo did, and luckily they even provided a few sample IPs and locations to benchmark against during testing.&lt;/p&gt;
    &lt;p&gt;Again, this was as simple as creating a new measurement with the USA as the location. I used 50 probes as the default limit and tested the NordVPN IP advertised as Bahamas but resolved to Miami by ipinfo.&lt;/p&gt;
    &lt;code&gt;Phase 3: Detecting US state...
  Measuring from 50 probes...

  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100.0%   50/50 - Best: FL (0.45 ms)                    

Top 3 Locations:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. Florida, USA                             0.45 ms
  2. South Carolina, USA                      12.23 ms
  3. Georgia, USA                             15.01 ms

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                      SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Location: Florida, United States
  Minimum Latency: 0.45 ms
  Confidence: Very High
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
&lt;/code&gt;
    &lt;p&gt;The tool agrees, Florida is the correct location. But how accurate can this system be? Can we expand it to show the city too?&lt;/p&gt;
    &lt;p&gt;Let's make a new phase, which again, will simply set the resulting country or state as the location and extract the city of the probe with the lowest latency. Here, since there are too many possible cities and towns per state and country, I expect the accuracy to be low and only point to the closest major hub. But in theory, this should be more than enough for use cases like routing or performance debugging.&lt;/p&gt;
    &lt;p&gt;And here we go, the same result ipinfo got&lt;/p&gt;
    &lt;code&gt;Phase 4: Detecting city...
  Measuring from 36 probes...

  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 100.0%   36/36 - Best: Miami (0.00 ms)                 

Top 3 Locations:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1. Miami, Florida, USA                      0.00 ms
  2. West Palm Beach, Florida, USA            4.36 ms
  3. Tampa, Florida, USA                      5.85 ms

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                      SUMMARY
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  Location: Miami, Florida, United States
  Minimum Latency: 0.00 ms
  Confidence: Very High
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
&lt;/code&gt;
    &lt;p&gt;The current results are good but could be better. The main problem is with how the magic field works: when setting, for example, 'Europe' as the location, it tries to spread the tests across all European probes but does not guarantee that every single country is going to be included.&lt;/p&gt;
    &lt;p&gt;This results in inconsistencies where a probe in the same country as the target IP was not selected, and so the tool assumes the IP is located in a different neighbouring country.&lt;/p&gt;
    &lt;p&gt;To fix this and make the results more consistent, you would need to change the selection logic and manually set every country per continent and US state. By passing the full list of countries/states to the Globalping API, you ensure that at least one probe in that location is going to be selected. Additionally, you fully control the number of probes per location, which is very important to control the accuracy.&lt;/p&gt;
    &lt;p&gt;For example, North America technically contains 43 countries and territories. This means you can't just set a limit of one probe per country, it is not enough to properly understand the latency to the target IP from the disproportionately larger USA. A better limit would be around 200 probes for the USA, 20 for Canada, and 10 for Mexico.&lt;/p&gt;
    &lt;p&gt;But the goal of this tool was to use a minimum amount of probes to allow unauthenticated users to test it out. The current approach works great, it is simple to implement and it is very easy to control the accuracy by simply setting a higher limit of probes.&lt;/p&gt;
    &lt;p&gt;Overall, latency-based geolocation detection seems to be a great way to verify the location of any IP as long as you have enough vantage points. It will most likely fall apart in regions with minimal or no coverage.&lt;/p&gt;
    &lt;p&gt;The tool itself is open source and you can run it like this:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;geolocate $IP&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;You can also use the â€“limit parameter to use more probes per phase. But be careful as it applies the set value to all phases and this will very quickly eat through your limit. Check the full docs in GitHub.&lt;/p&gt;
    &lt;p&gt;Pull requests with improvements are welcome!&lt;/p&gt;
    &lt;p&gt;Feel free to email me if you need some free credits to play around with d@globalping.io&lt;/p&gt;
    &lt;p&gt;And of course consider hosting a probe, itâ€™s as simple as running a container https://github.com/jsdelivr/globalping-probe&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46834953</guid><pubDate>Sat, 31 Jan 2026 09:30:05 +0000</pubDate></item><item><title>Euro firms must ditch Uncle Sam's clouds and go EU-native</title><link>https://www.theregister.com/2026/01/30/euro_firms_must_ditch_us/</link><description>&lt;doc fingerprint="bf389010bb04ad4c"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Euro firms must ditch Uncle Sam's clouds and go EU-native&lt;/head&gt;&lt;head rend="h2"&gt;Just because you're paranoid about digital sovereignty doesn't mean they're not after you&lt;/head&gt;&lt;p&gt;Opinion I'm an eighth-generation American, and let me tell you, I wouldn't trust my data, secrets, or services to a US company these days for love or money. Under our current government, we're simply not trustworthy.&lt;/p&gt;&lt;p&gt;In the Trumpâ€‘redux era of 2026, European enterprises are finally taking data seriously, and that means packing up from Redmond-by-Seattle and moving their most sensitive workloads home. This isn't just compliance theater; it's a straightâ€‘up national economic security play.&lt;/p&gt;&lt;head rend="h2"&gt;Open source's new mission: Rebuild a continent's tech stack&lt;/head&gt;READ MORE&lt;p&gt;Europe's digital sovereignty paranoia, long waved off as regulatory chatter, is now feeding directly into procurement decisions. Gartner told The Reg last year that IT spending in Europe is set to grow by 11 percent in 2026, hitting $1.4 trillion, with a big chunk rolling into "sovereign cloud" options and onâ€‘prem/edge architectures.&lt;/p&gt;&lt;p&gt;The kicker? Fully 61 percent of European CIOs and tech leaders say they want to increase their use of local cloud providers. More than half say geopolitics will prevent them from leaning further on USâ€‘based hyperscalers.&lt;/p&gt;&lt;p&gt;The American hypercloud vendors have figured this out. AWS recently made its European Sovereign Cloud available. This AWS cloud, Amazon claims, is "entirely located within the EU, and physically and logically separate from other AWS Regions." On top of that, EU residents will "independently operate it" and "be backed by strong technical controls, sovereign assurances, and legal protections designed to meet the needs of European governments and enterprises for sensitive data."&lt;/p&gt;&lt;p&gt;Many EU-based companies aren't pleased with this Euro-washing of American hypercloud services. The Cloud Infrastructure Service Providers in Europe (CISPE) trade association accuses the EU Cloud Sovereignty Framework of being set up to favor the incumbent (American) hypercloud providers.&lt;/p&gt;&lt;p&gt;They're not wrong.&lt;/p&gt;&lt;p&gt;You don't need a DEA warrant or a Justice Department subpoena to see the trend: Europe's 90â€‘plusâ€‘percent dependency on US cloud infrastructure, as former European Commission advisor Cristina Caffarra put it, is a singleâ€‘shockâ€‘event security nightmare waiting to rupture the EU's digital stability.&lt;/p&gt;&lt;p&gt;Seriously. What will you do if Washington decides to unplug you? Say Trump gets up on the wrong side of the bed and decides to invade Greenland. There goes NATO, and in all the saber-rattling leading up to the 10th Mountain Division being shipped to Nuuk, he orders American companies to cut their services to all EU countries and the UK.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;When AI 'builds a browser,' check the repo before believing the hype&lt;/item&gt;&lt;item&gt;Just because Linus Torvalds vibe codes doesn't mean it's a good idea&lt;/item&gt;&lt;item&gt;The most durable tech is boring, old, and everywhere&lt;/item&gt;&lt;item&gt;What the Linux desktop really needs to challenge Windows&lt;/item&gt;&lt;/list&gt;&lt;p&gt;With the way things are going, they're not going to say no. I mean, CEOs Tim Cook of Apple, Eric Yuan of Zoom, Lisa Su of AMD, and â€“ pay attention â€“ Amazon's Andy Jassy all went obediently to watch a feature-length White House screening of Melania, the universally-loathed, 104â€‘minute Amazonâ€‘produced documentary about First Lady Melania Trump.&lt;/p&gt;&lt;head rend="h2"&gt;Europe's cloud challenge: Building an Airbus for the digital age&lt;/head&gt;READ MORE&lt;p&gt;Sure, that's a silly example, but for American companies to do business today, they're kowtowing to Trump. Or, take a far more serious example, when Minnesota company CEOs called for "de-escalation" in the state, there was not one word about ICE or the government's role in the bloodshed. It was the corporate equivalent of the mealy-mouthed "thoughts and prayers" American right-wingers always say after a US school shooting.&lt;/p&gt;&lt;p&gt;Some companies have already figured out which way the wind is blowing. Airbus, the European aerospace titan, has put out a â‚¬50 million, decadeâ€‘long tender to migrate its missionâ€‘critical applications to a "sovereign European cloud." Airbus wants its whole stack â€“ data at rest, data in transit, logging, IAM, and securityâ€‘monitoring infrastructure â€“ all rooted in EU law and overseen by EU operators. As Catherine Jestin, Airbus's executive vice president of digital, told The Register: "We want to ensure this information remains under European control."&lt;/p&gt;&lt;p&gt;Who can blame them? Thanks to the American CLOUD Act and related US surveillance statutes, USâ€‘headquartered providers must hand over European data regardless of where the bytes sit. Exhibit A is that Microsoft has already conceded that it cannot guarantee data independence from US law enforcement. Airbus is betting that "data residency on paper" from AWSâ€‘styled "EU sections" is not enough. Real sovereignty demands EUâ€‘owned and run operations with full contractual and legal firewalls. Sure, your data may live in Frankfurt, but your fate still rests in Seattle, Redmond, or Mountain View if an American company owns your cloud provider.&lt;/p&gt;&lt;p&gt;Besides, do you really want some Trump apparatchik getting their hands on your data? I mean, this is a government where Madhu Gottumukkala, the acting director of the US Cybersecurity and Infrastructure Security Agency, uploaded sensitive data into ChatGPT!&lt;/p&gt;&lt;head rend="h2"&gt;UK urged to unplug from US tech giants as digital sovereignty fears grow&lt;/head&gt;READ MORE&lt;p&gt;In response, Brussels is pushing an open sourceâ€‘led exit from hyperscaler lockâ€‘in. Ministries are standardizing on Nextcloudâ€‘style collaboration stacks instead of Microsoft 365 to fund Euroâ€‘native clouds via the European Cloud Alliance. Some countries, like France, are already shoving Zoom, Teams, and other US videoconferencing platforms out the door in favor of a local service.&lt;/p&gt;&lt;p&gt;If you're running an EUâ€‘based firm in 2026, the takeaway isn't that AWSâ€‘inâ€‘Frankfurt is evil; it's that for certain workloads, especially national security, industrial IP, or highâ€‘profile consumer data franchises, EUâ€‘native cloud and services are no longer a niceâ€‘toâ€‘have but a business continuity plan requirement.&lt;/p&gt;&lt;p&gt;It's time to get serious about digital sovereignty. The clock is ticking, and there's no telling when Trump will go off. Â®&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835336</guid><pubDate>Sat, 31 Jan 2026 10:34:07 +0000</pubDate></item><item><title>"Giving up upstream-ing my patches &amp; feel free to pick them up"</title><link>https://mail.openjdk.org/pipermail/hotspot-dev/2026-January/118080.html</link><description>&lt;doc fingerprint="7e5d043dbf9822fa"&gt;
  &lt;main&gt;
    &lt;p&gt;Hi, About one year ago, in Jan. 2025, I began my adventure of the OpenJDK codebase. Later I attempted to make some patches into the repository. I checked the documentation and learned that I have to sign an Oracle Contributor Agreement before submitting patches to OpenJDK. At that time, I dreamed that it was just a pretty normal CLA, like the ones I signed for other projects and shall just take at most several days. A few days later, I received an email asking me to update some information in the agreement. I did. After that, I have sent 5 emails to opensource_ww_grp at oracle.com asking if there was anything wrong (once a month from January to May). For each of my emails, I got a reply, saying that they "sincerely apologize" and "@Dalibor Topic Can you please review...", with no actual progress being made. Now it has been (more than) one year since I submitted my first OCA submission. And I have been tired of "/touch"-ing my PR once a month. I wonder if there is a reason for not reviewing my OCA submission. I do live in Chinese Mainland but I have no contractual or subordinate or teacher- student relationship with any entities that are restricted by the US import/ export control laws (according to OpenSanctions). If you think that I have such a relationship or should be rejected for any other reasons, please simply reject my OCA submission, instead of hanging it for months. As I no longer have enough interest and spare time to work on OpenJDK, I decided to give up upstreaming those patches. If anyone is interested in them, please feel free to pick up and submit these patches, most of which are small but I believe they are useful. As OCA requires that "each contribution that you submit is and shall be an original work of authorship", you may rewrite my patches from scratch so it is an original work, and you don't need to sign my name or ping me. I would like to give a list of the patches that I wanted to upstream but failed: - Checks if "llvm-config" is broken: https://github.com/AOSC-Tracking/jdk/commit/ 6a8b12b1ad700d994a2803de593ca06e698ef1a9 - Extend default thread stack size for zero: This addresses the stack overflow exception in javac when building JDK 24 with zero variants. https://github.com/AOSC-Tracking/jdk/commit/ 4534fcaafc149f649105dc9914c7cf4aaf8c802c https://www.mail-archive.com/build-dev@openjdk.org/msg14818.html Some patches that are not for the upstream OpenJDK but Loongson's fork of JDK and were also blocked by OCA: https://github.com/loongson/jdk/pull/134https://github.com/loongson/jdk/pull/126https://github.com/loongson/jdk/pull/125https://github.com/loongson/jdk/pull/135https://github.com/loongson/jdk/pull/136https://github.com/AOSC-Tracking/jdk/commit/ 913dcb2b2759437876ae3a40a1b074eeb1bfe09f https://github.com/AOSC-Tracking/jdk/commit/ caba8e6de73fd9ffa078d6c257d6be8500b9d16a Best wishes, Bye. -- Bingwu Zhang (a.k.a. xtex) @ Sat, 31 Jan 2026 08:42:31 +0000&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835454</guid><pubDate>Sat, 31 Jan 2026 10:53:38 +0000</pubDate></item><item><title>Guix System First Impressions as a Nix User</title><link>https://nemin.hu/guix.html</link><description>&lt;doc fingerprint="b71891772da12e9f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Guix System First Impressions as a Nix User&lt;/head&gt;
    &lt;head rend="h2"&gt;Table of Contents&lt;/head&gt;
    &lt;head rend="h2"&gt;1. My Journey to Guix System&lt;/head&gt;
    &lt;p&gt;Feel free to skip this section if you don't really care about backstories. I just figured it makes sense to recap how and why one might start having an interest in declarative distros before tackling the main topic.&lt;/p&gt;
    &lt;p&gt;I've been a Linux-only1 user for about ten years now and, like many others, I too embarked on the arduous journey of distro-hopping. I started with Mint and when that felt too slow, I switched to Ubuntu. When Ubuntu felt too handholdy2, I switched to Arch, which proved to be my main driver for well over five or so years. And when I couldn't resist the Siren's call, I moved on to Gentoo, thinking surely "harder is better". Which resulted in severe burnout in a few months, so I capitulated and switched to Fedora, which was very stable and honestly an all around excellent system. But once more, my interest was piqued, and (before today's adventure) I finally switched to NixOS.&lt;/p&gt;
    &lt;p&gt;I've always had a passing interest towards Nix ever since I've first heard about it, but until fairly recently, I always dismissed it as a tool for DevOps guys. The syntax was weird, the need for reproducible environments seemingly irrelevant, and stuff like the oft-recommended Nix Pills seemed anything but newbie-friendly.&lt;/p&gt;
    &lt;p&gt;So then why would someone like me, who's so adamant about not needing Nix eventually choose to go all-in? I guess it was at first less about Nix being better and just the rest being worse.&lt;/p&gt;
    &lt;p&gt; Of the two big reasons for the switch, one was that I realized that having per-directory environments for your projects is actually a very handy thing to do when you like to toy around with many technologies. I used to generate my other blog using Jekyll and, no matter which distro I used, it was always a pain in the neck to have a good Ruby environment set up. &lt;code&gt;bundler install&lt;/code&gt; didn't really want to work without privileges and I wasn't really a fan of unleashing &lt;code&gt;sudo&lt;/code&gt; on it, but usually that was the only way I could get things to work.
&lt;/p&gt;
    &lt;p&gt; With Nix, however, it was a matter of just describing a few packages in a shell and boom, Ruby in one folder, no Ruby (and thus no mess) everywhere else. I was hooked! I started adding &lt;code&gt;shell.nix&lt;/code&gt; files to all my little projects, hell, I started planning projects by first adding a &lt;code&gt;shell.nix&lt;/code&gt; with all the dependencies I would reasonably need.
&lt;/p&gt;
    &lt;p&gt;The other reason, which ultimately cemented that I need to commit, was that I was getting tired of my installed packages slowly drifting out of control. Sure, every package manager has some method of listing what's installed, but these are usually cumbersome and completely ephemeral (in the sense that any listing becomes invalid the moment you change anything).&lt;/p&gt;
    &lt;p&gt;With NixOS, the equation is flipped on its head: No longer did I query the system to tell me what's installed and what's not, it was now the system that worked based on files that I edit. The difference sounds small on paper, but for me it was an extremely liberating feeling to know that I could edit my system configuration in a versionable, explicit, and centralized way.&lt;/p&gt;
    &lt;p&gt;But NixOS isn't the only declarative distro out there. In fact GNU forked Nix fairly early and made their own spin called Guix, whose big innovation is that, instead of using the unwieldy Nix-language, it uses Scheme. Specifically Guile Scheme, GNU's sanctioned configuration language. I've been following Guix for a bit, but it never felt quite ready to me with stuff like KDE being only barely supported and a lot of hardware not working out of the box.&lt;/p&gt;
    &lt;p&gt;However, now that (after three years) Guix announced its 1.5.0 release with a lot of stuff stabilized and KDE finally a first-party citizen, I figured now is the best time to give it a fresh shot. This post captures my experiences from installation to the first 3-4 days.&lt;/p&gt;
    &lt;head rend="h2"&gt;2. Installer Impressions&lt;/head&gt;
    &lt;p&gt; Plug your USB in, &lt;code&gt;dd&lt;/code&gt; the file onto the drive, reboot, nothing unusual. If you've ever installed a Linux system, it's more of the same.
&lt;/p&gt;
    &lt;p&gt;After selecting the pendrive in my BIOS settings, the monitor began to glow in a deep, radiant blue as the Guix System logo appeared on my screenâ€¦ only to suddenly switch to a menacing red: My CPU's integrated GPU is not supported by free firmware. A helpful popup gave me a gentle nudge about picking free hardware next time (buddy, have you seen the PC part prices these days?) and off I went into the installer proper.&lt;/p&gt;
    &lt;p&gt;Figure 1: Picture of the installer graciously borrowed from the Guix installer manual.&lt;/p&gt;
    &lt;p&gt;The installer itself is refreshingly barebones and I mean this in a positive way. It asks all the necessary questions and provides a nice basic configuration file, all done in a retro Ncurses-based TUI. I was really happy to see that, unlike my last attempt at using Guix System in the early 2020-s, KDE Plasma is now a first-party choice during installation. I never really vibed too much with GNOME and the other options didn't appeal either, so the choice was obvious.&lt;/p&gt;
    &lt;p&gt;Now, I'm not sure if I just picked the worst possible time or if the Guix servers were facing unusual load or whatever may have happened, but after such a breeze of a setup, the moment I pressed install, my PC became unusable for the next 2.5 hours. Which is unacceptable for an installation process these days in my opinion. I am lucky enough to live in a household with fiber-optic internet, that merely shrugs at bandwidth of up to a gigabyte per second and yet nearly all packages downloaded with a whopping 50 kilobytes per second, meaning even small-ish 5-10 megabyte packages took long minutes to download.3&lt;/p&gt;
    &lt;p&gt;A reboot later my issues only got worse.&lt;/p&gt;
    &lt;head rend="h2"&gt;3. I Can't Find my Way-land&lt;/head&gt;
    &lt;p&gt;I was assuming I'd get SDDM after having chosen KDE Plasma, but (what a later, closer read of the manual made me realize is the expected outcome for a default config) it was GDM that loaded in. I entered my name and password, and I was greeted with the familiar Plasma 6 spinner. The first hint that something might be off was that it loaded a bit longer than usual, but I was not going to get mad at waiting 10 seconds instead of 3. After all, I did just wait magnitudes longer to get here.&lt;/p&gt;
    &lt;p&gt;With practically nothing installed beyond the very basics, I clicked on Konsole, hoping to start prodding around my config and add some of my day to day apps. To my horror, it opened in the top left corner, without a titlebar and without any borders. What's more, no matter what I did, I couldn't move it. It also didn't show up on the menu bar, despite the application launcher still being completely usable. At this point I was fairly exhausted by these antics, but I figured,&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Well, it's a brand new release, perhaps this just snuck in. Let's give updating a shot and see if that helps.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt; So I issued &lt;code&gt;guix pull&lt;/code&gt;â€¦ The download whizzed by with speed quite unexpected after what I experienced with the installerâ€¦ Only to crash into the brick wall that's indexing. Okay, whatever, another 10-12 minutes down the drain, at least now I have newest version.
&lt;/p&gt;
    &lt;p&gt;Figure 2: Better than before download speeds&lt;/p&gt;
    &lt;p&gt; Except I didn't. Because, unlike Nix, the &lt;code&gt;guix&lt;/code&gt; executable is not an omnipresent, unique thing that anyone and everyone uses on your PC. Not only does every user have their own instance, if you don't issue a certain set of commands, you won't start using the new version, despite updating it.
&lt;/p&gt;
    &lt;p&gt;To Guix's credit, the CLI does scream at you to update your environment or else you'll keep using the old version, but I still find this system very disorientating compared to Nix. I'm certain experienced Guixheads are long past being tripped up by this sort of stuff and might even struggle to remember that there was a time they had to do these special steps too, but as a new user it felt a bit rough, especially consdering this is Guix System, i.e. the system whose whole purpose is to be integrate Guix as much as it can.&lt;/p&gt;
    &lt;p&gt; Back to our issue at hand. I issued &lt;code&gt;sudo -s&lt;/code&gt; and &lt;code&gt;guix pull&lt;/code&gt;-ed again. Once more 10-12 minutes passed indexing. But at least I could finally call &lt;code&gt;guix system reconfigure /etc/config.scm&lt;/code&gt;. Interestingly things are much faster this time around, I saw speeds up to 30-50 Mbps. Before long the system was updated to the newest commit and I rebooted with high hopes.
&lt;/p&gt;
    &lt;p&gt;High hopes, that were immediately dashed when Plasma loaded in the same messed up way. At this point I started to suspect this might be an issue with the GPU driver, so I enabled the LXQT desktop environment and rebooted once more. Thankfully that one worked like a charm and I was able to boot up both Emacs (editing Scheme with GNU Nano is a pain I do not wish on anyone) and LibreWolf (Firefox's de-Mozilla-d variant).&lt;/p&gt;
    &lt;p&gt; Not having found anything too useful in the docs, I decided to make my problem someone else's so I fired up ERC4 and connected to Libera.chat's &lt;code&gt;#guix&lt;/code&gt; channel. After around half an hour of wait, a user by the name of Rutherther stepped up and offered me some help. We were able to figure it out that Nouveau wasn't able to drive my GPU (an RTX 5070), so his recommendation was that I should try booting with &lt;code&gt;nomodeset&lt;/code&gt;. I did, but it sadly didn't help much either.
&lt;/p&gt;
    &lt;head rend="h2"&gt;4. Sympathy for the Devil&lt;/head&gt;
    &lt;p&gt;At this point I was out of ideas. Ideas of solving this using pure-Guix System, that is. There was still one option I wanted to avoid as long as I could, but alas, it seemed like the only option, that still had a realistic chance of working.&lt;/p&gt;
    &lt;p&gt;Figure 3: Nonguix's official logo, self-described to be "dark and evil".&lt;/p&gt;
    &lt;p&gt;Enter Nonguix, the Mr. Hyde to Guix's Dr. Jekyll, the shady guy who offers you a hit and first time's for free, theâ€¦ Erm, in a nutshell, it's the repository for non-free applications and drivers packages for Guix System, basically. Interestingly enough, by Guix's own findings about 64% of users utilize the Nonguix channel, which is perhaps not "literally everyone", but it does paint a picture that there is still stuff out there that you simply cannot replace with FOSS software yet.&lt;/p&gt;
    &lt;quote&gt;1: (cons* (channel 2: (name 'nonguix) 3: (url "https://gitlab.com/nonguix/nonguix") 4: ;; Enable signature verification: 5: (introduction 6: (make-channel-introduction 7: "897c1a470da759236cc11798f4e0a5f7d4d59fbc" 8: (openpgp-fingerprint 9: "2A39 3FFF 68F4 EF7A 3D29 12AF 6F51 20A0 22FB B2D5")))) 10: %default-channels)&lt;/quote&gt;
    &lt;p&gt; Enabling the repo wasn't exactly difficult. You just paste the short excerpt from above (also found in the README) into your &lt;code&gt;~/.config/guix/channels.scm&lt;/code&gt; and &lt;code&gt;/etc/guix/channels.scm&lt;/code&gt; files, &lt;code&gt;guix pull&lt;/code&gt;, let it index to its heart's content again, and then you have access to all that is nasty (yet occasionally useful) in the world.
&lt;/p&gt;
    &lt;p&gt;I figured perhaps if Linux-libre and its free firmware couldn't deal with my GPU, then surely Linux proper with its binary blobs could. Hell, for good measure I threw in the NVIDIA transform, which is supposed to automagically translate all dependencies to use the proprietary drivers.&lt;/p&gt;
    &lt;p&gt;Figure 4: What haste and half-reading manuals gets youâ€¦&lt;/p&gt;
    &lt;p&gt;Turns out my eagerness was a mistake. Not only did the process take yet another half an hour (if not more, I stopped counting), upon reboot all I was met with was a kernel panic about the driver not being able to cope with the GPU it found and a massive spew of FSCK logs.&lt;/p&gt;
    &lt;p&gt;Figure 5: 'FSCK' was indeed very close to the first words that came to my mind at this moment.&lt;/p&gt;
    &lt;p&gt;With no better ideas in mind, I took out my pendrive again and burned Nonguix's own pre-built ISO on it using my partner's PC. While it ultimately did get me a working system, this version has three unfortunate hindrances:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;It was built in 2022, far before Guix's migration to Codeberg, meaning it still attempts to pull content from the unfathomably slow GNU Savannah mirror. I had to manually override my &lt;code&gt;channels.scm&lt;/code&gt;to point at the Codeberg repo instead, but with no easy means of finding its "channel introduction"5, I had to pass in&lt;code&gt;--disable-authentication&lt;/code&gt;to Guix when updating my system. A bit scary, but I trust the Codeberg repo.&lt;/item&gt;
      &lt;item&gt;Because of its age, I got a lot of somewhat intimidating errors about hardware not being recognized and other stuff I couldn't even decipher, but ultimately the system booted to the installer without issue.&lt;/item&gt;
      &lt;item&gt;For some reason while the installer itself does include Nonguix stuff, it actually does not include the repo in the resulting channels files, nor the substitution server for the project. The README has a warning about this, but if you happen to miss it, you could accidentally install a non-Nonguix Guix System (say that three times fast).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; None of these were particularly hard to fix, however, and soon enough I was back where I started. That is to say, in a &lt;code&gt;nomodeset&lt;/code&gt; X11 session, except this time running i3, as LXQT wasn't an available option on an installer this old. There was certainly a bit of a hacker-ish vibe to messing with code files in an environment like that, but I was honestly much more looking forward to finally having a usable desktop.
&lt;/p&gt;
    &lt;p&gt; Having learned from my hastiness, this time I was smarter. I only enabled the full kernel and firmware blobs, without going anywhere near the NVIDIA transform. I issued another &lt;code&gt;guix system reconfigure&lt;/code&gt; and, after having time for another tea session, my update was finally finished.
&lt;/p&gt;
    &lt;p&gt;I rebooted with tentative nervousness andâ€¦ Success? Huh.&lt;/p&gt;
    &lt;head rend="h2"&gt;5. Goals&lt;/head&gt;
    &lt;p&gt;Obviously there is little point in throwing Guix System on my PC and declaring success. I wanted to be able to at least reproduce the kind of workflow I'm used to using NixOS. For that, I need the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A browser: preferably Firefox, as I'm not a huge fan of Chrome / Chromium,&lt;/item&gt;
      &lt;item&gt;An E-mail client: preferably Thunderbird,&lt;/item&gt;
      &lt;item&gt;A basic office suite: preferably LibreOffice,&lt;/item&gt;
      &lt;item&gt;Dev environments: for Rust, Zig, Scheme, and TypeScript (with the option for more, if possible),&lt;/item&gt;
      &lt;item&gt;Emacs: I do almost all my text editing in it these days, falling back to Neovim for quick tasks,&lt;/item&gt;
      &lt;item&gt;Discord: for chatting with friends,&lt;/item&gt;
      &lt;item&gt;Telegram: for chatting with family,&lt;/item&gt;
      &lt;item&gt;Steam: for the very rare occasions I want to game,&lt;/item&gt;
      &lt;item&gt;NVIDIA drivers: I prefer to offload day-to-day usage to my CPU's integrated GPU, as it cuts my energy usage in half.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Of these it was obvious that two would be relatively hard and one "outright impossible". The two being Steam and the drivers (as both are non-free and thus not in Guix's default repos) and the "impossible" one being Discord (which not even the non-free repo has packaged). But I was ready to compromise a little bit since I am requesting stuff that's explicitly against Guix's goals.&lt;/p&gt;
    &lt;head rend="h2"&gt;6. Results&lt;/head&gt;
    &lt;p&gt;Figure 6: My desktop running Wezterm packaged by me and Emacs.&lt;/p&gt;
    &lt;p&gt;While there has been occasional bumps and hitches along the ride, I must say I'm very impressed with Guix System so far. Let's go through this list in order:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Browser: So far I'm really enjoying LibreWolf. It feels a lot snappier than Firefox and I'm really baffled how much speed I was apparently missing out on.&lt;/item&gt;
      &lt;item&gt;E-mails: I installed Icedove, which is basically just Thunderbird without Mozilla branding. It works as expected.&lt;/item&gt;
      &lt;item&gt;Office suite: LibreOffice is available as expected. Not much to say about it. I guess it's interesting that Guix isn't following the usual &lt;code&gt;-stale&lt;/code&gt;/&lt;code&gt;-fresh&lt;/code&gt;packaging schema, but I don't really mind not having cutting edge versions of an office suite :)&lt;/item&gt;
      &lt;item&gt;Dev environments: I've only briefly toyed with development environments so far, but to me it seems like for simple use-cases it might be even easier to use than &lt;code&gt;shell.nix&lt;/code&gt;(you don't need any sort of ceremony, just a&lt;code&gt;manifest.scm&lt;/code&gt;file with a&lt;code&gt;(specifications-&amp;gt;manifest &amp;lt;list of packages&amp;gt;)&lt;/code&gt;form inside and you have a dev env ready to go.)&lt;/item&gt;
      &lt;item&gt;Emacs: Installed just fine. I had to install &lt;code&gt;emacs-vterm&lt;/code&gt;to make Vterm work, but all that took was the very simple process of adding the library to my home configuration and then referencing it in my Emacs config as per this Reddit post.&lt;/item&gt;
      &lt;item&gt;Discord: I decided to just use Discord's browser version, which works just as fine (if not better). It's trading a tiny bit of convenience in return for not having to figure out how to manually add a package for it from some random third-party source. From what I've read elsewhere Flatpak is also an option, but I prefer having just one package manager at a time.&lt;/item&gt;
      &lt;item&gt;Steam: Installed shockingly easily. I have to really give props to the Nonguix team. I tested Portal 2 with the Nouveau driver, it is a little disheartening to see a 15 years old game6 lag, but I understand the people's hands are tied when it comes to the free drivers. After I managed to install the proprietary drivers, I was able to play even Portal RTX, which is something I never managed to get to work using NixOS.&lt;/item&gt;
      &lt;item&gt;NVIDIA drivers: This time I actually read the docs properly and it didn't take long for me to realize the initial problem that caused my previous install to be unbootable was of course found between the chair and keyboard. This time, after making sure I enabled the open drivers and kernel mode-setting, I crossed my fingers, issued a reconfigure and it works beautifully!&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;6.1. The Good&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Helpful community: While I do feel like Guix's community could be much larger (see below), the one that exists is very helpful and nice from my limited experience. In all places I've looked so far (Libera's&lt;/p&gt;&lt;code&gt;#guix&lt;/code&gt;, /r/Guix, and the guix/guix Codeberg repository) I was met with genuinely kind and helpful people.&lt;p&gt;That is not to say I haven't seen some bad eggs, especially in posts from years ago, but I don't think there is any community without those, so I'm not going to cite this as a negative.&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Home configuration: Having &lt;code&gt;guix home&lt;/code&gt;be a built-in, first class citizen, instead of a community made "extension" is excellent. Instead of needing to consult a third-party resource like Home Manager's documentation you can simply use what you already know about Guix and, if you happen to hit a wall, you can just read the official handbook which is guaranteed to always stay up to date with the rest of the system.&lt;/item&gt;
      &lt;item&gt;Package availability: As long as you largely use FOSS stuff (which is much easier than one might think), the amount of choice is awesome. I could basically just copy over the list of packages from my Nix config and practically everything had an equivalent.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scheme: I'm not really a seasoned Schemer, but I have dabbled in the language previously and it feels so much better to me than Nix (the language) ever did. One great benefit of this is that it's a lot easier to start digging into package definitions to figure things out for yourself.&lt;/p&gt;
        &lt;p&gt;This is "Freedom 1" of GNU's Four Essential Freedoms in effect. Since the code is pretty much just Scheme and the different mechanisms available are fairly well documented (see caveat below), the barrier to entry is much lower than with Nix in my opinion.7&lt;/p&gt;
        &lt;p&gt;Another nice benefit of this is that you can use Emacs' extensive Scheme support to help your configuration. Tools like Geiser can plug right into Guix and help you find package and function names and, once you're experienced enough, debug your config/packages on the fly. I personally haven't yet achieved mastery of such level yet, but having the REPL confirm if I've entered names in correctly before running the code is already a boon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Ease of hacking: In the "to tinker on" sense, rather than "being insecure". With Nix, merely pulling in Nixpkgs is an effort, due to the repository being massive. My otherwise beefy machine struggled to switch between branches and make commits, which doesn't exactly inspire confidence in contributing, even though it was otherwise something I was excited to do. Meanwhile, with Guix I was able to get a fully functioning development environment in 15 minutes tops, which includes cloning the repo, authenticating all commits, generating bytecode for the entire repository, and getting Emacs set up to work nice with the codebase.&lt;/p&gt;&lt;p&gt;Not to mention, at the time of writing my Nixpkgs PR of guile-colorized is still not accepted, despite being open since October, 2025. Which is kind of disheartening, when the package is really trivial and has a very low blast-radius. With Guix I got an answer to an extremely noobish question on my first PR in mere hours.&lt;/p&gt;&lt;p&gt;On a separate, but related note, I also found it a lot easier to test my package in a "live" environment as&lt;/p&gt;&lt;code&gt;guix pull&lt;/code&gt;supports a parameter called&lt;code&gt;--url&lt;/code&gt;which you can easily point to a folder on your own PC. So once I was confident my code should work, I could just "check out" my local repository clone and build it like I was an end user. This let me make sure it really does work.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;6.2. The Ambiguous&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Search:&lt;/p&gt;&lt;code&gt;guix search&lt;/code&gt;not taking an extra parameter like&lt;code&gt;nix search&lt;/code&gt;is both very convenient and a bit of a bummer.&lt;p&gt;Its absence is not a deal breaker, but I really loved how with Nix, you could search in anything, that has a flake. Be that Nixpkgs, a repo you downloaded, a repo that's on a git forge, etc. I remember being awestruck that I could just do&lt;/p&gt;&lt;code&gt;nix search github:mozilla/nixpkgs-mozilla&lt;/code&gt;and search for their builds of Firefox without having to manually check out anything.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The documentation: Oof, this one is a bit hard to pass definite judgment on.&lt;/p&gt;
        &lt;p&gt;On one hand I love the thoroughness of it all. You can get a fairly decent idea of what Guix, what it can do for your, how to use it, and how to extend it, just by reading the manual. It is evident that the Guix team and GNU in general takes its mission to educate using free software very seriously. Stuff like the Packaging tutorial make it very easy for complete beginners to hack together package definitions without needing to consult any other resource.&lt;/p&gt;
        &lt;p&gt;On the other hand, it really is just a manual, not a tutorial. What I mean by this is that concepts that could belong together aren't placed near each other. A simple example would be services and customizing them. Assuming, you're in one of the sub-pages of Services and you suddenly realize you want to replace/modify one of the services, you are left completely clueless how that works. You have to go to a completely different chapter and find one particular function's description and then apply what you learn there. The Guix Cookbook has some examples, but you have to know about the cookbook in the first place.&lt;/p&gt;
        &lt;p&gt;And before anyone misunderstands me, I'm fine with RTFM, but in my opinion one of the preconditions of mass-appeal is having "pre-chewed" solutions for common problems, that don't require perusing multiple chapters.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;6.3. The Bad&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Substitute server stability: I imagine this is an issue that only a massive bag of money could fix, but the CI/CD servers could definitely use some more processing power. It's really annoying when you're trying to test something and you're suddenly forced to wait 10-15 minutes because the server can only spare 50-100 kbps for you.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Content out there: Clearly this isn't the Guix team's fault (and it's something I'm trying to lessen with this post, even if just a tiny bit), but it's really hard to find good quality material when it comes to Guix.&lt;/p&gt;
        &lt;p&gt;I mean, sure, there is the excellent System Crafters tutorial series, and the odd gems like DThompson's dev env tutorial, but as a whole you're largely left to your own to trawl through the manual, IRC logs, Reddit threads, Codeberg and the previous issue tracker, etc. It's not an impossible task, especially if you're used to doing Linux things "the hard way", but it's certainly a far cry from such one-stop shops as the Nix Flakes book or Wombat's Book of Nix.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;Guix's own build speed: Nix excels in speed, so I was hoping Guix would be the same. Yet stuff like &lt;code&gt;guix pull&lt;/code&gt;really bog things down. Doubly so, if you want to update not just your own&lt;code&gt;guix&lt;/code&gt;instance, but also root's.&lt;/item&gt;
      &lt;item&gt;Clarity of commands: The fact that all concerns are lumped together (unlike Nix's many utilities) means that to the new user the many commands such as &lt;code&gt;guix pull&lt;/code&gt;,&lt;code&gt;guix {system, home} reconfigure&lt;/code&gt;,&lt;code&gt;guix update&lt;/code&gt;can easily feel overwhelming and unclear what's updating/changing what. With time I'm sure you obtain a sort of mental muscle memory and you never think about it again, but starting out it's definitely a confusing part.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;7. Overall&lt;/head&gt;
    &lt;quote&gt;1: (define-module (guix-home-config) 2: #:use-module (nongnu packages) 3: #:use-module (gnu packages) 4: #:use-module (gnu home) 5: #:use-module (gnu home services) 6: #:use-module (gnu home services shells) 7: #:use-module (gnu services) 8: #:use-module (gnu system shadow) 9: #:use-module (guix gexp)) 10: 11: (define %packages 12: (list "git" "openssh" "librewolf" "ripgrep" 13: "bat" "eza" "fd" "zoxide" "bc" "gimp" 14: "libreoffice" "jujutsu" "starship" "direnv" 15: "okular" "gwenview" "bitwarden-desktop" 16: "icedove-wayland" "telegram-desktop" 17: "emacs-vterm" "ispell" "hunspell" "wezterm")) 18: 19: (define %nonfree-packages 20: (list "steam-nvidia" 21: "mpv-nvidia")) 22: 23: (define home-config 24: (home-environment 25: (packages (specifications-&amp;gt;packages (append %nonfree-packages %packages))) 26: (services 27: (append 28: (list 29: (service home-bash-service-type 30: (home-bash-configuration 31: (aliases '(("ls" . "eza"))) 32: (bashrc (list (local-file "./bashrc.sh"))))) 33: 34: (service home-files-service-type 35: `((".guile" ,%default-dotguile) 36: (".Xdefaults" ,%default-xdefaults))) 37: 38: (service home-xdg-configuration-files-service-type 39: `(("gdb/gdbinit" ,%default-gdbinit) 40: ("nano/nanorc" ,%default-nanorc)))) 41: 42: %base-home-services)))) 43: 44: home-config&lt;/quote&gt;
    &lt;p&gt;In a nutshell I'm very positively surprised by Guix System. After struggling so much with it years ago, this time everything just clicked after a much shorter battle. So much so that I'm happy to make it my daily driver for the foreseeable future. Beyond the slightly slower execution speed, I'm getting a comparable experience to NixOS, with all the usual pros a declarative environment brings and without having to put up with Nixlang.&lt;/p&gt;
    &lt;p&gt; My only recurring issues so far are the occasional slow download speeds and that I have to start my kernel in &lt;code&gt;nomodeset&lt;/code&gt; because otherwise the graphical environment crashes without me being able to switch to a TTY. It's a bummer, but honestly, I'm not too bothered by it so far. I'm trusting a driver update will fix it soon enough and, if not, it's not exactly difficult to throw in a kernel parameter into your config.
&lt;/p&gt;
    &lt;p&gt;I'm hoping to do a followup post about packaging in Guix, because I've been dipping my toes into it by trying to package Wezterm and the journey there was similarly arduous as installing the system itself.&lt;/p&gt;
    &lt;p&gt;Till then, thank you for reading and see you next time!&lt;/p&gt;
    &lt;head rend="h2"&gt;8. Notes&lt;/head&gt;
    &lt;p&gt;The stuff you see below are all I managed to write down mid-process. Some of these I threw it into the file from Nano, some from half-broken X11 sessions. Because of this, it's not exactly well-edited, but I hope it might provide a glimpse into my mind at the time.&lt;/p&gt;
    &lt;quote&gt;&lt;item&gt;The installer is decently simple&lt;/item&gt;&lt;item&gt;I appreciate the warning about incompatible hardware&lt;/item&gt;&lt;item&gt;2.5 hours at least to install (mirrors throttle connection to 50kbps)&lt;/item&gt;&lt;item&gt;KDE is simply not working out of the box (titlebars are missing)&lt;/item&gt;&lt;item&gt;It seems to also default to X11, when I'm looking for Wayland&lt;/item&gt;&lt;item&gt;The first&lt;/item&gt;&lt;code&gt;guix pull&lt;/code&gt;is horrendously slow&lt;item&gt;Wayland continues to elude me, seems to be an Nvidia issue&lt;/item&gt;&lt;item&gt;IRC recommends&lt;/item&gt;&lt;code&gt;nomodeset&lt;/code&gt;, doesn't help&lt;item&gt;Try enabling Nonguix, system no longer boots&lt;/item&gt;&lt;item&gt;Try installing using the Nonguix ISO&lt;/item&gt;&lt;item&gt;Lots of errors, terribly old release&lt;/item&gt;&lt;item&gt;Having to&lt;/item&gt;&lt;code&gt;guix pull&lt;/code&gt;myself to the present day again&lt;item&gt;Also I'm missing the introduction, so I have to run it using&lt;/item&gt;&lt;code&gt;--disable-authentication&lt;/code&gt;, not great, but I trust the Codeberg repo&lt;item&gt;At least the download speed seems to have normalized&lt;/item&gt;&lt;item&gt;It isn't entirely clear when you have to use&lt;/item&gt;&lt;code&gt;sudo&lt;/code&gt;&lt;item&gt;Running&lt;/item&gt;&lt;code&gt;i3&lt;/code&gt;on a shitty low-res has a certain vibe to it, but I'd prefer a system working out of the box&lt;/quote&gt;
    &lt;head rend="h2"&gt;Footnotes:&lt;/head&gt;
    &lt;p&gt;Well, if only life was so easy. What I mean here is that on my personal computer, I've not had Windows since about 2015. For work purposes my hands are currently chained to MacOS (though even there I use a Debian-based container).&lt;/p&gt;
    &lt;p&gt;No disrespect to Ubuntu-users, past and present! My opinion at the time was quite ignorant and nowadays I far more appreciate an easy to maintain system as you'll see from the rest of this post.&lt;/p&gt;
    &lt;p&gt;It's merely a hunch, but it feels to me that the servers are far slower during the (Central-European) night. During midday, I get really good download speeds, but after around 8 PM, it slows to a crawl.&lt;/p&gt;
    &lt;p&gt;Which, for the uninitiated, is an IRC client built into Emacs. This editor continues to wow me every day.&lt;/p&gt;
    &lt;p&gt;I probably could have figured it out in time. But at this point I was a bit exasperated and I really didn't want to type in an 10x4 character hexadecimal code by hand.&lt;/p&gt;
    &lt;p&gt;Goodness gracious, Portal 2 is almost 15 years oldâ€¦&lt;/p&gt;
    &lt;p&gt;That being said, my Nix experience was still very much helpful here. Understanding stuff such as build phases, why packages need to be patched and how this usually works, and what the different build flags mean is pretty much a must if you want to attain an understanding deeper than just "kinda getting it."&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835612</guid><pubDate>Sat, 31 Jan 2026 11:22:24 +0000</pubDate></item><item><title>Insane Growth Goldbridge (YC F25) Is Hiring a Forward Deployed Engineer</title><link>https://www.ycombinator.com/companies/goldbridge/jobs/78gGEHh-forward-deployed-engineer</link><description>&lt;doc fingerprint="5ff0179310e1c520"&gt;
  &lt;main&gt;
    &lt;p&gt;Ramp for Real Estate&lt;/p&gt;
    &lt;p&gt;Goldbridge is building the financial operating system for the largest asset class in the world â€“ real estate. More than $1T in rent flows through landlord bank accounts annually, with roughly a quarter locked in idle reserves and security deposits â€“ and billions more leaking from unnecessary property expenses. And with $2.5T in real estate loans about to mature in 2027/28, property owners are desperate to boost their income ASAP. Goldbridge solves this problem by creating the first AI-powered banking platform for real estate owners. We are backed by Y Combinator and other world-class investors, and our CEO is a 2x YC founder, former White House advisor, and 100-unit real estate owner/operator who understands this industry deeply. See full job description here: https://www.goldbridgebanking.com/careers/forward-deployed-engineer&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46835834</guid><pubDate>Sat, 31 Jan 2026 12:00:22 +0000</pubDate></item><item><title>Film students who can no longer sit through films</title><link>https://www.theatlantic.com/ideas/2026/01/college-students-movies-attention-span/685812/</link><description>&lt;doc fingerprint="1668410428cb269c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Film Students Who Can No Longer Sit Through Films&lt;/head&gt;
    &lt;p&gt;The attention-span crisis goes to the movies.&lt;/p&gt;
    &lt;p&gt;Everyone knows itâ€™s hard to get college students to do the readingâ€”remember books? But the attention-span crisis is not limited to the written word. Professors are now finding that they canâ€™t even get film studentsâ€”film studentsâ€”to sit through movies. â€œI used to think, If homework is watching a movie, that is the best homework ever,â€ Craig Erpelding, a film professor at the University of Wisconsin at Madison, told me. â€œBut students will not do it.â€&lt;/p&gt;
    &lt;p&gt;I heard similar observations from 20 film-studies professors around the country. They told me that over the past decade, and particularly since the pandemic, students have struggled to pay attention to feature-length films. Malcolm Turvey, the founding director of Tufts Universityâ€™s Film and Media Studies Program, officially bans electronics during film screenings. Enforcing the ban is another matter: About half the class ends up looking furtively at their phones.&lt;/p&gt;
    &lt;p&gt;A handful of professors told me they hadnâ€™t noticed any change. Some students have always found old movies to be slow, Lynn Spigel, a professor of screen cultures at Northwestern University, told me. â€œBut the ones who are really dedicated to learning film always were into it, and they still are.â€&lt;/p&gt;
    &lt;p&gt;Most of the instructors I spoke with, however, feel that something is different now. And the problem is not limited to large introductory courses. Akira Mizuta Lippit, a cinema and media-studies professor at the University of Southern Californiaâ€”home to perhaps the top film program in the countryâ€”said that his students remind him of nicotine addicts going through withdrawal during screenings: The longer they go without checking their phone, the more they fidget. Eventually, they give in. He recently screened the 1974 Francis Ford Coppola classic The Conversation. At the outset, he told students that even if they ignored parts of the film, they needed to watch the famously essential and prophetic final scene. Even that request proved too much for some of the class. When the scene played, Lippit noticed that several students were staring at their phones, he told me. â€œYou do have to just pay attention at the very end, and I just canâ€™t get everybody to do that,â€ he said.&lt;/p&gt;
    &lt;p&gt;Many students are resisting the idea of in-person screenings altogether. Given the ease of streaming assignments from their dorm rooms, they see gathering in a campus theater as an imposition. Professors whose syllabi require in-person screenings outside of class time might see their enrollment drop, Meredith Ward, director of the Program in Film and Media Studies at Johns Hopkins University, told me. Accordingly, many professors now allow students to stream movies on their own time.&lt;/p&gt;
    &lt;p&gt;You can imagine how that turns out. At Indiana University, where Erpelding worked until 2024, professors could track whether students watched films on the campusâ€™s internal streaming platform. Fewer than 50 percent would even start the movies, he said, and only about 20 percent made it to the end. (Recall that these are students who chose to take a film class.) Even when students stream the entire film, itâ€™s not clear how closely they watch it. Some are surely folding laundry or scrolling Instagram, or both, while the movie plays.&lt;/p&gt;
    &lt;p&gt;The students I spoke with admitted to their own inattentiveness. They even felt bad about it. But that wasnâ€™t enough to make them sit through the assigned movies. Mridula Natarajan, a freshman at the University of Texas at Austin, took a world-cinema class this past fall. â€œThere were some movies that were extremely slow-paced, and ironically, that was the point of the movie,â€ she told me. â€œBut I guess impatience made me skip through stuff or watch it on two-times speed.â€&lt;/p&gt;
    &lt;p&gt;After watching movies distractedlyâ€”if they watch them at allâ€”students unsurprisingly canâ€™t answer basic questions about what they saw. In a multiple-choice question on a recent final exam, Jeff Smith, a film professor at UW Madison, asked what happens at the end of the Truffaut film Jules and Jim. More than half of the class picked one of the wrong options, saying that characters hide from the Nazis (the film takes place during World War I) or get drunk with Ernest Hemingway (who does not appear in the movie). Smith has administered similar exams for almost two decades; he had to grade his most recent exam on a curve to keep studentsâ€™ marks within a normal range.&lt;/p&gt;
    &lt;p&gt;The professors I spoke with didnâ€™t blame students for their shortcomings; they focused instead on how media diets have changed. From 1997 to 2014, screen time for children under age 2 doubled. And the screen in question, once a television, is now more likely to be a tablet or a smartphone. Students arriving in college today have no memory of a world before the infinite scroll. As teenagers, they spent nearly five hours a day on social media, with much of that time used for flicking from one short-form video to the next. An analysis of peopleâ€™s attention while working on a computer found that they now switch between tabs or apps every 47 seconds, down from once every two and a half minutes in 2004. â€œI can imagine that if your body and your psychology are not trained for the duration of a feature-length film, it will just feel excruciatingly long,â€ USCâ€™s Lippit said. (He also hypothesized that, because every movie is available on demand, students feel that they can always rewatch should they miss somethingâ€”even if they rarely take advantage of that option.)&lt;/p&gt;
    &lt;p&gt;Kyle Stine, a film and media-studies professor at Johns Hopkins, usually begins his course with an icebreaker: Whatâ€™s a movie you watched recently? In the past few years, some students have struggled to name any film. Kristen Warner, a performing- and media-arts professor at Cornell University, has noticed a similar trend. Some of her students arrive having seen only Disney movies. Erpelding, at UW Madison, said he tries to find a movie that everyone in his class has seen, to serve as a shared reference point they can talk about. Lately, thatâ€™s become impossible. Even students who are interested in going into filmmaking donâ€™t necessarily love watching films. â€œThe disconnect is that 10 years ago, people who wanted to go study film and media creation were cinephiles themselves,â€ Erpelding told me. â€œNowadays, theyâ€™re people that consume the same thing everyone else consumes, which is social media.â€&lt;/p&gt;
    &lt;p&gt;Of course, young people havenâ€™t given up on movies altogether. But the feature films that they do watch now tend to be engineered to cater to their attentional deficit. In a recent appearance on The Joe Rogan Experience, Matt Damon, the star of many movies that college students may not have seen, said that Netflix has started encouraging filmmakers to put action sequences in the first five minutes of a film to get viewers hooked. And just because young people are streaming movies, it doesnâ€™t mean theyâ€™re paying attention. When they sit down to watch, many are browsing social media on a second screen. Netflix has accordingly advised directors to have characters repeat the plot three or four times so that multitasking audiences can keep up with whatâ€™s happening, Damon said.&lt;/p&gt;
    &lt;p&gt;Some professors are treating wilting attention spans as a problem to be solved, not a reality to accept. Stine, at Johns Hopkins, is piloting a course on â€œslow cinemaâ€â€”minimalist films with almost no narrative thrustâ€”with the goal of helping students redevelop long modes of attention. Rick Warner, the director of film studies at the University of North Carolina, deliberately selects films with slow pacing and subtle details, such as Chantal Akermanâ€™s Jeanne Dielman, 23 quai du Commerce, 1080 Bruxelles, a three-hour movie that mostly follows a woman doing chores in her apartment. â€œI try to teach films that put their habits of viewing under strain,â€ Warner told me. â€œIâ€™m trying to sell them on the idea that a film watched properly can actually help them retrain their perception and can teach them how to concentrate again.â€ Once they get used to it, students enjoy the challenge, he said.&lt;/p&gt;
    &lt;p&gt;But other professors, perhaps concluding that resistance is futile, are adjusting to the media their students grew up on. Some show shorter films or have students watch movies over multiple sittings. Erpelding, who primarily teaches filmmaking courses, has moved from teaching traditional production methods to explaining how to maximize audience engagement. He now asks students to make three- or four-minute films, similar to the social-media edits they see online. After all, that seems to be the only type of video many young people want to watch.&lt;/p&gt;
    &lt;p&gt;By the way, the last scene of The Conversation has the paranoid Gene Hackman destroying his apartment in a desperate and futile search for listening devices. He eventually gives up, and mournfully plays the saxophone amid the wreckage. Itâ€™s a brilliant scene, and worth the wait.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46838026</guid><pubDate>Sat, 31 Jan 2026 16:26:35 +0000</pubDate></item><item><title>Finland to end "uncontrolled human experiment" with ban on youth social media</title><link>https://yle.fi/a/74-20207494</link><description>&lt;doc fingerprint="3cd6d75008609f9d"&gt;
  &lt;main&gt;
    &lt;p&gt;Lunch break at the Finnish International School of Tampere (FISTA) is a boisterous time.&lt;/p&gt;
    &lt;p&gt;The yard is filled with children â€” ranging from grades 1 to 9, or ages 6 to 16 â€” running around, shouting, playing football, shooting basketball hoops, doing what kids do.&lt;/p&gt;
    &lt;p&gt;And there's not a single screen in sight.&lt;/p&gt;
    &lt;p&gt;FISTA has taken advantage of the law change, brought in last August, which allows schools to restrict or completely ban the use of mobile phones during school hours. At FISTA, this means no phones at all unless specifically used for learning in the classroom.&lt;/p&gt;
    &lt;p&gt;"We've seen that cutting down on the possibilities for students to use their phones, during the breaks for instance, has spurred a lot of creativity," FISTA vice principal Antti Koivisto notes.&lt;/p&gt;
    &lt;p&gt;"They're more active, doing more physical things like playing games outdoors or taking part in the organised break activities or just socialising with each other."&lt;/p&gt;
    &lt;p&gt;With the smartphone restriction in schools widely considered to have been a success, Finland's government has now set its sights on social media platforms.&lt;/p&gt;
    &lt;p&gt;Prime Minister Petteri Orpo (NCP) said earlier this month that he supports banning the use of social media by children under the age of 15.&lt;/p&gt;
    &lt;p&gt;"I am deeply concerned about the lack of physical activity among children and young people, and the fact that it is increasing," Orpo said at the time.&lt;/p&gt;
    &lt;p&gt;And there is a growing groundswell of support for Finland introducing such a ban. Two-thirds of respondents to a survey published earlier this week said they back a ban on social media for under-15s. This is a near 10 percentage point jump compared to a similar survey carried out just last summer.&lt;/p&gt;
    &lt;head rend="h2"&gt;"Uncontrolled human experiment"&lt;/head&gt;
    &lt;p&gt;The concerns over social media, and in particular the effects on children, have been well-documented â€” but Finnish researcher Silja Kosola's recent description of the phenomenon as an "uncontrolled human experiment" has grabbed people's attention once again.&lt;/p&gt;
    &lt;p&gt;Kosola, an associate professor in adolescent medicine, has researched the impact of social media on young people, and tells Yle News that the consequences are not very well understood.&lt;/p&gt;
    &lt;p&gt;"We see a rise in self-harm and especially eating disorders. We see a big separation in the values of young girls and boys, which is also a big problem in society," Kosola explains.&lt;/p&gt;
    &lt;p&gt;In the video below, Silja Kosola explains the detrimental effects that excessive use of social media can have on young people.&lt;/p&gt;
    &lt;p&gt;She further notes that certain aspects of Finnish culture â€” such as the independence and freedom granted to children from a young age â€” have unwittingly exacerbated the ill effects of social media use.&lt;/p&gt;
    &lt;p&gt;"We have given smartphones to younger people more than anywhere else in the world. Just a couple of years ago, about 95 percent of first graders had their own smartphone, and that hasn't happened anywhere else," she says.&lt;/p&gt;
    &lt;head rend="h2"&gt;All eyes on Australia&lt;/head&gt;
    &lt;p&gt;Since 10 December last year, children under the age of 16 in Australia have been banned from using social media platforms such as TikTok, Snapchat, Facebook, Instagram and YouTube.&lt;/p&gt;
    &lt;p&gt;Prime Minister Anthony Albanese began drafting the legislation after he received a heartfelt letter from a grieving mother who lost her 12-year-old daughter to suicide.&lt;/p&gt;
    &lt;p&gt;Although Albanese has never revealed the details of the letter, he told public broadcaster ABC that it was "obvious social media had played a key role" in the young girl's death.&lt;/p&gt;
    &lt;p&gt;The legislation aims to shift the burden away from parents and children and onto the social media companies, who face fines of up to 49.5 million Australian dollars (29 million euros) if they consistently fail to keep kids off their platforms.&lt;/p&gt;
    &lt;p&gt;Clare Armstrong, ABC's chief digital political correspondent, told Yle News that the initial reaction to the roll-out has been some confusion but no little "relief".&lt;/p&gt;
    &lt;p&gt;"The government often talks about this law as being a tool to help parents and other institutions enforce and start conversations about tech and social media in ways that before, they couldn't," she says.&lt;/p&gt;
    &lt;p&gt;Although it is still early days, as the ban has only been in force for about six weeks, Armstrong adds that the early indicators have been good.&lt;/p&gt;
    &lt;p&gt;ABC journalist Clare Armstrong explains in the video below how children in Australia have been spending their time since the social media ban was introduced.&lt;/p&gt;
    &lt;p&gt;However, she adds a note of caution to any countries â€” such as Finland â€” looking to emulate the Australian model, noting that communication is key.&lt;/p&gt;
    &lt;p&gt;"Because you can write a very good law, but if the public doesn't understand it, and if it can't be enforced at that household level easily, then it's bound to fail," Armstrong says.&lt;/p&gt;
    &lt;head rend="h2"&gt;Playing to Finland's strengths&lt;/head&gt;
    &lt;p&gt;Seona Candy, an Australian living in Helsinki for over eight years, has been keenly following the events in her homeland since the social media ban came into effect in December.&lt;/p&gt;
    &lt;p&gt;She has heard anecdotally that if kids find themselves blocked from one platform, they just set up an account on another, "ones that maybe their parents don't even know exist".&lt;/p&gt;
    &lt;p&gt;"And this is then much, much harder, because those platforms don't have parental controls, so they don't have those things already designed into them that the more mainstream platforms do," Candy says.&lt;/p&gt;
    &lt;p&gt;Because of this issue, and others she has heard about, she warns against Finland introducing like-for-like legislation based around Australia's "reactive, knee-jerk" law change.&lt;/p&gt;
    &lt;p&gt;"I think the Finnish government should really invest in digital education, and digital literacy, and teach kids about digital safety. Finland is world-famous for education, and for media literacy. Play to your strengths, right?"&lt;/p&gt;
    &lt;p&gt;The All Points North podcast asked if Finland should introduce a similar ban on social media as in Australia. You can listen to the episode via this embedded player, on Yle Areena, via Apple, Spotify or wherever you get your podcasts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46838417</guid><pubDate>Sat, 31 Jan 2026 17:06:22 +0000</pubDate></item><item><title>Mobile carriers can get your GPS location</title><link>https://an.dywa.ng/carrier-gnss.html</link><description>&lt;doc fingerprint="ae2c9b9741e393f0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Mobile carriers can get your GPS location&lt;/head&gt;
    &lt;p&gt;In iOS 26.3, Apple introduced a new privacy feature which limits â€œprecise locationâ€ data made available to cellular networks via cell towers. The feature is only available to devices with Appleâ€™s in-house modem introduced in 2025. The announcement1 says&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Cellular networks can determine your location based on which cell towers your device connects to.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is well-known. I have served on a jury where the prosecution obtained location data from cell towers. Since cell towers are sparse (especially before 5G), the accuracy is in the range of tens to hundreds of metres2.&lt;/p&gt;
    &lt;p&gt;But this is not the whole truth, because cellular standards have built-in protocols that make your device silently send GNSS (i.e. GPS, GLONASS, Galileo, BeiDou) location to the carrier. This would have the same precision as what you see in your Map apps, in single-digit metres.&lt;/p&gt;
    &lt;p&gt;In 2G and 3G this is called Radio Resources LCS Protocol (RRLP)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;So the network simply asks â€œtell me your GPS coordinates if you know themâ€ and the phone will respond3.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In 4G and 5G this is called LTE Positioning Protocol (LPP)&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;RRLP, RRC, and LPP are natively control-plane positioning protocols. This means that they are transported in the inner workings of cellular networks and are practically invisible to end users4.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Itâ€™s worth noting that GNSS location is never meant to leave your device. GNSS coordinates are calculated entirely passively, your device doesnâ€™t need to send a single bit of information. Using GNSS is like finding out where you are by reading a road sign: you donâ€™t have to tell anyone else you read a road sign, anyone can read a road sign, and the people who put up road signs donâ€™t know who read which road sign when.&lt;/p&gt;
    &lt;p&gt;These capabilities are not secrets but somehow they have mostly slid under the radar of the public consciousness. They have been used in the wild for a long time, such as by the DEA in the US in 200656:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;[T]he DEA agents procured a court order (but not a search warrant) to obtain GPS coordinates from the courierâ€™s phone via a ping, or signal requesting those coordinates, sent by the phone company to the phone.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And by Shin Bet in Israel, which tracks everyone everywhere all the time7:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The GSS Tool was based on centralized cellular tracking operated by Israelâ€™s General Security Services (GSS). The technology was based on a framework that tracks all the cellular phones running in Israel through the cellular companiesâ€™ data centers. According to news sources, it routinely collects information from cellular companies and identifies the location of all phones through cellular antenna triangulation and GPS data7.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Notably, the Israeli government started using the data for contact tracing in March 202078, only a few weeks after the first Israeli COVID-19 case. An individual would be sent an SMS message informing them of close contact with a COVID patient and required to quarantine. This is good evidence that the location data Israeli carriers are collecting are far more precise than what cell towers alone can achieve.&lt;/p&gt;
    &lt;p&gt;A major caveat is that I donâ€™t know if RRLP and LPP are the exact techniques, and the only techniques, used by DEA, Shin Bet, and possibly others to collect GNSS data; there could be other protocols or backdoors weâ€™re not privy to.&lt;/p&gt;
    &lt;p&gt;Another unknown is whether these protocols can be exploited remotely by a foreign carrier. Saudi Arabia has abused SS7 to spy on people in the US9, but as far as I know this only locates a device to the coverage area of a Mobile Switching Center, which is less precise than cell tower data. Nonetheless, given the abysmal culture, competency, and integrity in the telecom industry, I would not be shocked if itâ€™s possible for a state actor to obtain the precise GNSS coordinates of anyone on earth using a phone number/IMEI.&lt;/p&gt;
    &lt;p&gt;Apple made a good step in iOS 26.3 to limit at least one vector of mass surveillance, enabled by having full control of the modem silicon and firmware. They must now allow users to disable GNSS location responses to mobile carriers, and notify the user when such attempts are made to their device.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;https://transition.fcc.gov/pshs/911/Apps Wrkshp 2015/911_Help_SMS_WhitePaper0515.pdf â†©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://laforge.gnumonks.org/blog/20101217-learning_about_gps/ â†©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Comment on United States v. Skinner, 690 F.3d 772 (6th Cir. 2012) https://harvardlawreview.org/print/vol-126/sixth-circuit-holds-that-pinging-a-targets-cell-phone-to-obtain-gps-data-is-not-a-search-subject-to-warrant-requirement-ae-united-states-v-skinner-690-f-3d-772-6th-cir-2012-rehae/ â†©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://www.cato.org/blog/skinning-fourth-amendment-sixth-circuits-awful-gps-tracking-decision â†©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://www.ericsson.com/en/blog/2020/12/5g-positioning--what-you-need-to-know â†©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Eran Toch and Oshrat Ayalon. 2023. How Mass surveillance Crowds Out Installations of COVID-19 Contact Tracing Applications. https://doi.org/10.1145/3579491 â†© â†©2 â†©3&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://www.nytimes.com/2020/03/16/world/middleeast/israel-coronavirus-cellphone-tracking.html â†©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;https://www.theguardian.com/world/2020/mar/29/revealed-saudis-suspected-of-phone-spying-campaign-in-us â†©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46838597</guid><pubDate>Sat, 31 Jan 2026 17:21:34 +0000</pubDate></item><item><title>US has investigated claims WhatsApp chats aren't private</title><link>https://www.bloomberg.com/news/articles/2026-01-29/us-has-investigated-claims-that-whatsapp-chats-aren-t-private</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46838635</guid><pubDate>Sat, 31 Jan 2026 17:25:30 +0000</pubDate></item><item><title>Genode OS is a tool kit for building highly secure special-purpose OS</title><link>https://genode.org/about/index</link><description>&lt;doc fingerprint="b4d81b96c2cf87f9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;About Genode&lt;/head&gt;
    &lt;p&gt;The Genode OS Framework is a tool kit for building highly secure special-purpose operating systems. It scales from embedded systems with as little as 4 MB of memory to highly dynamic general-purpose workloads.&lt;/p&gt;
    &lt;p&gt;Genode is based on a recursive system structure. Each program runs in a dedicated sandbox and gets granted only those access rights and resources that are needed for its specific purpose. Programs can create and manage sub-sandboxes out of their own resources, thereby forming hierarchies where policies can be applied at each level. The framework provides mechanisms to let programs communicate with each other and trade their resources, but only in strictly-defined manners. Thanks to this rigid regime, the attack surface of security-critical functions can be reduced by orders of magnitude compared to contemporary operating systems.&lt;/p&gt;
    &lt;p&gt;The framework aligns the construction principles of L4 with Unix philosophy. In line with Unix philosophy, Genode is a collection of small building blocks, out of which sophisticated systems can be composed. But unlike Unix, those building blocks include not only applications but also all classical OS functionalities including kernels, device drivers, file systems, and protocol stacks.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Features&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;CPU architectures: x86 (32 and 64 bit), ARM (32 and 64 bit), RISC-V&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kernels: most members of the L4 family (NOVA, seL4, Fiasco.OC, OKL4 v2.1, L4ka::Pistachio, L4/Fiasco), Linux, and a custom kernel.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Virtualization: VirtualBox (on NOVA), a custom virtual machine monitor for ARM, and a custom runtime for Unix software&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Over 100 ready-to-use components&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Genode is open source and commercially supported by Genode Labs.&lt;/p&gt;
    &lt;list rend="dl"&gt;
      &lt;item rend="dt-1"&gt;Road map&lt;/item&gt;
      &lt;item rend="dd-1"&gt;
        &lt;p&gt;The direction where the project is currently heading&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-2"&gt;Challenges&lt;/item&gt;
      &lt;item rend="dd-2"&gt;
        &lt;p&gt;A collection of project ideas, giving a glimpse on possible future directions&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-3"&gt;Publications&lt;/item&gt;
      &lt;item rend="dd-3"&gt;
        &lt;p&gt;Publications related to Genode&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-4"&gt;Licensing&lt;/item&gt;
      &lt;item rend="dd-4"&gt;
        &lt;p&gt;Open-Source and commercial licensing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item rend="dt-5"&gt;Screenshots&lt;/item&gt;
      &lt;item rend="dd-5"&gt;
        &lt;p&gt;Screenshots of Genode-based system scenarios&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46838981</guid><pubDate>Sat, 31 Jan 2026 18:03:32 +0000</pubDate></item><item><title>Nintendo DS code editor and scriptable game engine</title><link>https://crl.io/ds-game-engine/</link><description>&lt;doc fingerprint="ae9ef93a050ef3bf"&gt;
  &lt;main&gt;&lt;p&gt;2026&lt;/p&gt;&lt;p&gt;TL;DR&lt;/p&gt;&lt;p&gt;I built a scriptable 3D game engine for the Nintendo DS so you can write and run games directly on the console itself. Written in C using libnds, it compiles to a ~100KB .nds ROM that runs at 60 FPS. Features a touch-based code editor on the bottom screen and real-time 3D rendering on the top screen. Ships with a working 3D pong game as the default script.&lt;/p&gt;&lt;p&gt;I felt nostalgic for when I made my first games on an old TI-82 graphing calculator. So I tried bringing that whole experience to my Nintendo DS. A complete programming environment you can hold in your hands.&lt;/p&gt;&lt;p&gt;What you see is a scriptable game engine with a custom programming language featuring variables, loops, and conditionals. You write code using the bottom touchscreen, click play, and the game will execute in real-time on the top screen with full 3D rendering.&lt;/p&gt;&lt;p&gt;At a high level, the engine breaks down into three parts:&lt;/p&gt;&lt;p&gt;Uses the DS's 3D hardware to render colored cubes at 60 FPS. Each model has position (X, Y, Z), rotation angle, and color. The camera is fully controllable with position and yaw/pitch angles.&lt;/p&gt;&lt;quote&gt;// DS 3D rendering code (C + libnds) glMatrixMode(GL_MODELVIEW); glLoadIdentity(); gluLookAt(camX, camY, camZ, // camera position camX + lookX, camY + lookY, camZ + lookZ, // look target 0, 1, 0); // up vector&lt;/quote&gt;&lt;p&gt;Each model is drawn with a transform (position + Y-axis rotation), then the cube geometry: one color, six quads (24 vertices).&lt;/p&gt;&lt;quote&gt;// Per-model draw calls (from main.c) for (i = 0; i &amp;lt; MAX_MODELS; i++) { if (!modelActive[i]) continue; glPushMatrix(); glTranslatef(modelX[i], modelY[i], modelZ[i]); glRotatef(modelAngle[i], 0, 1, 0); drawCube(CUBE_COLORS[modelColorIndex[i]]); drawWireframeCube(); glPopMatrix(1); } // Cube geometry: RGB15 color -&amp;gt; glColor3b, then 6 faces as GL_QUADS glColor3b(r * 255/31, g * 255/31, b * 255/31); glBegin(GL_QUADS); /* +Z face */ glVertex3f(-1.0f, 1.0f, 1.0f); glVertex3f( 1.0f, 1.0f, 1.0f); glVertex3f( 1.0f, -1.0f, 1.0f); glVertex3f(-1.0f, -1.0f, 1.0f); /* -Z, +Y, -Y, +X, -X ... (24 vertices total) */ glEnd();&lt;/quote&gt;&lt;p&gt;A touch-based code editor with a custom UI drawn pixel-by-pixel to a 256x192 bitmap. Features include:&lt;/p&gt;&lt;quote&gt;// Software rendering to bottom screen u16 *subBuffer = (u16*)BG_BMP_RAM_SUB(0); // 256x192 framebuffer subBuffer[y * 256 + x] = RGB15(31, 31, 31); // white pixel&lt;/quote&gt;&lt;p&gt;Executes one line of script per frame (~60 lines/sec). Scripts can use 26 variables (A-Z) plus 9 read-only registers for input (D-pad, buttons) and system state (elapsed time, camera direction).&lt;/p&gt;&lt;quote&gt;// Script execution (simplified) if (tokenEquals(script[scriptIP], "add")) { int r = scriptReg[scriptIP]; // which register (A-Z) registers[r] += getNumberParamValue(scriptIP, 0); scriptIP++; // next line }&lt;/quote&gt;&lt;p&gt;Scripts are built from tokens (commands) with numeric parameters. Each line executes instantly, with no parsing overhead, just a series of if-checks against token names.&lt;/p&gt;&lt;p&gt;Variables &amp;amp; Math&lt;/p&gt;&lt;code&gt;SET A 5&lt;/code&gt; â€” set register A to 5&lt;code&gt;ADD A 1&lt;/code&gt; â€” add 1 to A&lt;code&gt;SUBTRACT A 2&lt;/code&gt; â€” subtract 2 from A&lt;code&gt;MULTIPLY B -1&lt;/code&gt; â€” multiply B by -1&lt;p&gt;Control Flow&lt;/p&gt;&lt;code&gt;LOOP&lt;/code&gt; / &lt;code&gt;END_LOOP&lt;/code&gt; â€” infinite loop&lt;code&gt;IF_GT A 10&lt;/code&gt; â€” if A &amp;gt; 10&lt;code&gt;IF_LT A 0&lt;/code&gt; â€” if A &amp;lt; 0&lt;code&gt;IF_TRUE kA&lt;/code&gt; â€” if A button pressed&lt;code&gt;END_IF&lt;/code&gt; â€” close conditional&lt;p&gt;3D Objects&lt;/p&gt;&lt;code&gt;MODEL 0&lt;/code&gt; â€” create model at index 0&lt;code&gt;POSITION 0 X Y Z&lt;/code&gt; â€” set position&lt;code&gt;ANGLE 0 45&lt;/code&gt; â€” set rotation angle&lt;code&gt;NEXT_COLOR 0&lt;/code&gt; â€” cycle color&lt;p&gt;Camera &amp;amp; Rendering&lt;/p&gt;&lt;code&gt;CAM_POS X Y Z&lt;/code&gt; â€” set camera position&lt;code&gt;CAM_ANGLE yaw pitch&lt;/code&gt; â€” set look direction&lt;code&gt;BACKGROUND 2&lt;/code&gt; â€” set bg color (0-3)&lt;code&gt;BEEP&lt;/code&gt; â€” play 0.1s sound&lt;code&gt;SLEEP 0.016&lt;/code&gt; â€” pause (60 FPS = 0.016s/frame)&lt;code&gt;LEFT, UP, RGT, DN&lt;/code&gt;: D-pad (1.0 when held, 0.0 when released)
&lt;code&gt;KA, KB&lt;/code&gt;: A and B buttons&lt;code&gt;TIME&lt;/code&gt;: elapsed seconds since script started&lt;code&gt;LOOKX, LOOKZ&lt;/code&gt;: camera forward direction (normalized X and Z)
&lt;p&gt;The engine ships with a playable pong game. Here's a simplified excerpt:&lt;/p&gt;&lt;quote&gt;MODEL 0 ; create ball MODEL 1 ; create paddle CAM_POS 0 8 18 ; position camera SET A 0 ; ball X position SET B 1 ; ball velocity SET C 0 ; paddle Z position LOOP ADD A B ; move ball IF_GT A 10 ; hit right wall? MULTIPLY B -1 ; reverse velocity END_IF IF_TRUE Up ; up button pressed? ADD C -0.5 ; move paddle up END_IF POSITION 0 A 0 0 ; update ball position POSITION 1 -13 0 C ; update paddle position SLEEP 0.016 ; ~60 FPS END_LOOP&lt;/quote&gt;&lt;p&gt;The full script includes collision detection, game-over logic, and beep sounds on miss, all done with simple register math and conditionals.&lt;/p&gt;&lt;code&gt;make&lt;/code&gt; in the project directory
&lt;code&gt;program.nds&lt;/code&gt; (~100 KB ROM file)
&lt;p&gt;You need a flashcart (e.g. R4, DSTT, Acekard) with a microSD card:&lt;/p&gt;&lt;code&gt;program.nds&lt;/code&gt; to the microSD card
&lt;p&gt;Note: I got my R4 cart + SD card from a friend years ago, so I don't have detailed setup instructions for the cart itself. Most modern flashcarts just need you to copy their firmware to the SD root, then add ROMs in a folder.&lt;/p&gt;&lt;p&gt; You can test the DS game engine build directly below. The emulator loads &lt;code&gt;ds-game-engine.nds&lt;/code&gt;. Loads a more basic pong game than the one in the video.
&lt;/p&gt;&lt;p&gt;Nintendo DS emulator (Desmond). If the game doesnâ€™t start, ensure JavaScript is enabled and the page has finished loading.&lt;/p&gt;&lt;p&gt;Compiled ROM (ds-game-engine.nds)&lt;/p&gt;&lt;p&gt;Feel free to ask or discuss in this Reddit thread&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46839215</guid><pubDate>Sat, 31 Jan 2026 18:27:36 +0000</pubDate></item><item><title>Google Cloud suspended my account for 2 years, only automated replies</title><link>https://news.ycombinator.com/item?id=46839375</link><description>&lt;doc fingerprint="6fcb82bf4eb65263"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;My Google account has been suspended from GCP since March 2024.&lt;/p&gt;
      &lt;p&gt;I have submitted multiple appeals through ts-consult@google.com over 2 years. Every time I get the same automated template asking me to explain, I reply with details, then nothing. No human ever responds.&lt;/p&gt;
      &lt;p&gt;Case: #1-8622000037271&lt;/p&gt;
      &lt;p&gt;Timeline: - March 2024: Suspended, appeal submitted - April 2024: Automated requests for info, I replied - Nov 2024: More automated emails, I replied again - Dec 2024 - now: Complete silence&lt;/p&gt;
      &lt;p&gt;I am a CS researcher at UC Berkeley. This has seriously impacted my work.&lt;/p&gt;
      &lt;p&gt;Has anyone successfully gotten Google to review a GCP suspension appeal? How do you reach a human?&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46839375</guid><pubDate>Sat, 31 Jan 2026 18:41:36 +0000</pubDate></item><item><title>Death Note: L, Anonymity and Eluding Entropy (2011)</title><link>https://gwern.net/death-note-anonymity</link><description>&lt;doc fingerprint="e74289562965a4bb"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Death Note: L, Anonymity &amp;amp; Eluding Entropy&lt;/head&gt;
    &lt;p&gt;Applied Computer Science: On Murder Considered As STEM Fieldâ€”using information theory to quantify the magnitude of Light Yagamiâ€™s mistakes in Death Note and considering fixes&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In the manga Death Note, the protagonist Light Yagami is given the supernatural weapon â€œDeath Noteâ€ which can kill anyone on demand, and begins using it to reshape the world. The genius detective L attempts to track him down with analysis and trickery, and ultimately succeeds. Death Note is almost a thought-experiment-given the perfect murder weapon, how can you screw up anyway? I consider the various steps of Lâ€™s process from the perspective of computer security, cryptography, and information theory, to quantify Lightâ€™s initial anonymity and how L gradually de-anonymizes him, and consider which mistake was the largest as follows:&lt;/p&gt;
      &lt;p&gt;Lightâ€™s fundamental mistake is to kill in ways unrelated to his goal.&lt;/p&gt;
      &lt;p&gt;Killing through heart attacks does not just make him visible early on, but the deaths reveals that his assassination method is impossibly precise and something profoundly anomalous is going on. L has been tipped off that Kira exists. Whatever the bogus justification may be, this is a major victory for his opponents. (To deter criminals and villains, it is not necessary for there to be a globally-known single anomalous or supernatural killer, when it would be equally effective to arrange for all the killings to be done naturalistically by ordinary mechanisms such as third parties/police/judiciary or used indirectly as parallel construction to crack cases.)&lt;/p&gt;
      &lt;p&gt;Worse, the deaths are non-random in other waysâ€”they tend to occur at particular times!&lt;/p&gt;
      &lt;p&gt;Just the scheduling of deaths cost Light 6 bits of anonymity&lt;/p&gt;
      &lt;p&gt;Lightâ€™s third mistake was reacting to the blatant provocation of Lind L. Tailor.&lt;/p&gt;
      &lt;p&gt;Taking the bait let L narrow his target down to 1â„3 the original Japanese population, for a gain of ~1.6 bits.&lt;/p&gt;
      &lt;p&gt;Lightâ€™s fourth mistake was to use confidential police information stolen using his policeman fatherâ€™s credentials.&lt;/p&gt;
      &lt;p&gt;This mistake was the largest in bits lost. This mistake cost him 11 bits of anonymity; in other words, this mistake cost him twice what his scheduling cost him and almost 8 times the murder of Tailor!&lt;/p&gt;
      &lt;p&gt;Killing Ray Penbar and the FBI team.&lt;/p&gt;
      &lt;p&gt;If we assume Penbar was tasked 200 leads out of the 10,000, then murdering him and the fiancee dropped Light just 6 bits or a little over half the fourth mistake and comparable to the original scheduling mistake.&lt;/p&gt;
      &lt;p&gt;Endgame: At this point in the plot, L resorts to direct measures and enters Lightâ€™s life directly, enrolling at the university, with Light unable to perfectly play the role of innocent under intense in-person surveillance.&lt;/p&gt;
      &lt;p&gt;From that point on, Light is screwed as he is now playing a deadly game of â€œMafiaâ€ with L &amp;amp; the investigative team. He frittered away &amp;gt;25 bits of anonymity and then L intuited the rest and suspected him all along.&lt;/p&gt;
      &lt;p&gt;Finally, I suggest how Light could have most effectively employed the Death Note and limited his loss of anonymity. In an appendix, I discuss the maximum amount of information leakage possible from using a Death Note as a communication device.&lt;/p&gt;
      &lt;p&gt;(Note: This essay assumes a familiarity with the early plot of Death Note and Light Yagami. If you are unfamiliar with DN, see my Death Note Ending essay or consult Wikipedia or read the DN rules.)&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I have called the protagonist of Death Note, Light Yagami, â€œhubristicâ€ and said he made big mistakes. So I ought to explain what he did wrong and how he could do better.&lt;/p&gt;
    &lt;p&gt;While Light starts scheming and taking serious risks as early as the arrival of the FBI team in Japan, he has fundamentally already screwed up. L should never have gotten that close to Light. The Death Note kills flawlessly without forensic trace and over arbitrary distances; Death Note is almost a thought-experimentâ€”given the perfect murder weapon, how can you screw up anyway?&lt;/p&gt;
    &lt;p&gt;Some of the other Death Note users highlight the problem. The user in the Yotsuba Group carries out the normal executions, but also kills a number of prominent competitors. The killings directly point to the Yotsuba Group and eventually the userâ€™s death. The moral of the story is that indirect relationships can be fatal in narrowing down the possibilities from â€˜everyoneâ€™ to â€˜these 8 menâ€™.&lt;/p&gt;
    &lt;head rend="h1"&gt;Detective Stories As Optimization Problems&lt;/head&gt;
    &lt;p&gt;In Lightâ€™s case, L starts with the worldâ€™s entire population of 7 billion people and needs to narrow it down to 1 person. Itâ€™s a search problem. It maps fairly directly onto basic information theory, in fact. (See also Simulation inferences, The 3 Grenades, and for case studies in applied deanonymization, Tor DNM-related arrests, 2011â€“4201511ya.) To uniquely specify one item out of 7 billion, you need 33 bits of information because log2(7000000000) â‰ˆ 32.7; to use an analogy, your 32-bit computer can only address one unique location in memory out of 4 billion locations, and adding another bit doubles the capacity to &amp;gt;8 billion. Is 33 bits of information a lot?&lt;/p&gt;
    &lt;p&gt;Not really. L could get one bit just by looking at history or crime statistics, and noting that mass murderers are, to an astonishing degree, male1, thereby ruling out half the world population and actually starting L off with a requirement to obtain only 32 bits to break Lightâ€™s anonymity.2 If Death Note users were sufficiently rational &amp;amp; knowledgeable, they could draw on concepts like superrationality to acausally cooperate3 to avoid this information leakageâ€¦ by arranging to pass on Death Notes to females4 to restore a 50:50 gender ratioâ€”for example, if for every female who obtained a Death note there were 3 males with Death Notes, then all users could roll a 1d3 dice and if 1 keep it and if 2 or 3 pass it on to someone of the opposite gender.&lt;/p&gt;
    &lt;p&gt;We should first point out that Light is always going to leak some bits. The only way he could remain perfectly hidden is to not use the Death Note at all. If you change the world in even the slightest way, then you have leaked information about yourself in principle. Everything is connected in some sense; you cannot magically wave away the existence of fire without creating a cascade of consequences that result in every living thing dying. For example, the fundamental point of Light executing criminals is to shorten their lifespanâ€”thereâ€™s no way to hide that. You canâ€™t both shorten their lives and not shorten their lives. He is going to reveal himself this way, at the least, to the actuaries and statisticians.&lt;/p&gt;
    &lt;p&gt;More historically, this has been a challenge for cryptographers, like in WWII: how did they exploit the Enigma &amp;amp; other communications without revealing they had done so? Their solution was misdirection: constantly arranging for plausible alternatives, like search planes that â€˜just happenedâ€™ to find German submarines or leaks to controlled known German agents about there being undiscovered spies. (However, the famous story that Winston Churchill allowed the town of Coventry to be bombed rather than risk the secret of Ultra has since been put into question.) This worked in part because of German overconfidence, because the war did not last too long, and in part because each cover story was plausible on its own and no one was, in the chaos of war, able to see the whole picture and realize that there were too many lucky search planes and too many undiscoverable moles; eventually, however, someone would realize, and apparently some Germans did conclude that Enigma had to have been broken (but much too late). Itâ€™s not clear to me what would be the best misdirection for Light to mask his normal killingsâ€”use the Death Noteâ€™s control features to invent an anti-criminal terrorist organization?&lt;/p&gt;
    &lt;p&gt;So there is a real challenge here: one party is trying to infer as much as possible from observed effects, and the other is trying to minimize how much the former can observe while not stopping entirely. How well does Light balance the competing demands?&lt;/p&gt;
    &lt;head rend="h1"&gt;Mistakes&lt;/head&gt;
    &lt;head rend="h2"&gt;Mistake 1&lt;/head&gt;
    &lt;p&gt;However, he can try to reduce the leakage and make his anonymity set as large as possible. For example, killing every criminal with a heart attack is a dead give-away. Criminals do not die of heart attacks that often. (The point is more dramatic if you replace â€˜heart attackâ€™ with â€˜lupusâ€™; as we all know, in real life itâ€™s never lupus.) Heart attacks are a subset of all deaths, and by restricting himself, Light makes it easier to detect his activities. 1,000 deaths of lupus are a blaring red alarm; 1,000 deaths of heart attacks are an oddity; and 1,000 deaths distributed over the statistically likely suspects of cancer and heart disease etc. are almost invisible (but still noticeable in principle).&lt;/p&gt;
    &lt;p&gt;So, Lightâ€™s fundamental mistake is to kill in ways unrelated to his goal. Killing through heart attacks does not just make him visible early on, but the deaths reveals that his assassination method is supernaturally precise. L has been tipped off that Kira exists. Whatever the bogus justification may be, this is a major victory for his opponents.&lt;/p&gt;
    &lt;p&gt;First mistake, and a classic one of serial killers (eg. the BTK killerâ€™s vaunting was less anonymous than he believed): delusions of grandeur and the desire to taunt, play with, and control their victims and demonstrate their power over the general population. From a literary perspective, this similarity is clearly not an accident, as we are meant to read Light as the Sociopath Hero archetype (akin to Grand Admiral Thrawn): his ultimate downfall is the consequence of his fatal personality flaw, hubris, particularly in the original sadistic sense. Light cannot help but self-sabotage like this.&lt;/p&gt;
    &lt;p&gt;(This is also deeply problematic from the point of carrying out Lightâ€™s theory of deterrence: to deter criminals and villains, it is not necessary for there to be a globally-known single supernatural killer, when it would be equally effective to arrange for all the killings to be done naturalistically by third parties/police/judiciary or used indirectly to crack cases. Arguably the deterrence would be more effective the more diffused itâ€™s believed to beâ€”since a single killer has a finite lifespan, finite knowledge, fallibility, and idiosyncratic preferences which reduce the threat and connection to criminality, while if all the deaths were ascribed to unusually effective police or detectives, this would be inferred as a general increase in all kinds of police competence, one which will not instantly disappear when one person gets bored or hit by a bus.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Mistake 2&lt;/head&gt;
    &lt;p&gt;Worse, the deaths are non-random in other waysâ€”they tend to occur at particular times! Graphed, daily patterns jump out.&lt;/p&gt;
    &lt;p&gt;L was able to narrow down the active times of the presumable student or worker to a particular range of longitude, say 125â€“150Â° out of 180Â°; and what country is most prominent in that range? Japan. So that cut down the 7 billion people to around 0.128 billion; 0.128 billion requires 27 bits (log2 (128000000) â‰ˆ 26.93) so just the scheduling of deaths cost Light 6 bits of anonymity!&lt;/p&gt;
    &lt;head rend="h3"&gt;De-Anonymization&lt;/head&gt;
    &lt;p&gt;On a side-note, some might be skeptical that one can infer much of anything from the graph and that Death Note was just glossing over this part. â€œHow can anyone infer that it was someone living in Japan just from 2 clumpy lines at morning and evening in Japan?â€ But actually, such a graph is surprisingly precise. I learned this years before I watched Death Note, when I was heavily active on Wikipedia; often I would wonder if two editors were the same person or roughly where an editor lived. What I would do if their edits or user page did not reveal anything useful is I would go to â€œKateâ€™s edit counterâ€ and I would examine the times of day all their hundreds or thousands of edits were made at. Typically, what one would see was ~4 hours where there were no edits whatsoever, then ~4 hours with moderate to high activity, a trough, then another gradual rise to 8 hours later and a further decline down to the first 4 hours of no activity. These periods quite clearly corresponded to sleep (pretty much everyone is asleep at 4 AM), morning, lunch &amp;amp; work hours, evening, and then night with people occasionally staying up late and editing5. There was noise, of course, from people staying up especially late or getting in a bunch of editing during their workday or occasionally traveling, but the overall patterns were clearâ€”never did I discover that someone was actually a nightwatchman and my guess was an entire hemisphere off. (Academic estimates based on user editing patterns correlate well with what is predicted by on the basis of the geography of IP edits.6)&lt;/p&gt;
    &lt;p&gt;Computer security research offers more scary results. Perhaps because â€œeverything is correlatedâ€, there are an amazing number of ways to break someoneâ€™s privacy and de-anonymize them (background; there is also financial incentive to do so in order to advertise &amp;amp; price discriminate):&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;small errors in their computerâ€™s clockâ€™s time (even over Tor)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Web browsing history7 or just the version and plugins8; and this is when random Firefox or Google Docs or Facebook bugs donâ€™t leak your identity&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Timing attacks based on how slow pages load9 (how many cache misses there are; timing attacks can also be used to learn website usernames or # of private photos)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Knowledge of what â€˜groupsâ€™ a person was in could uniquely identify 42%10 of people on social networking site XING, and possibly Facebook &amp;amp; 6 others&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Similarly, knowing just a few movies someone has watched11, popular or obscure, through Netflix often grants access to the rest of their profile if it was included in the Netflix Prize. (This was more dramatic than the AOL search data scandal because AOL searches had a great deal of personal information embedded in the search queries, but in contrast, the Netflix data seems impossibly impoverishedâ€”thereâ€™s nothing obviously identifying about what anime one has watched unless one watches obscure ones.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The researchers generalized their Netflix work to find isomorphisms between arbitrary graphs12 (such as social networks stripped of any and all data except for the graph structure), for example Flickr and Twitter, and give many examples of public datasets that could be de-anonymized13â€”such as your Amazon purchases ( et al 2011; blog). These attacks are on just the data that is left after attempts to anonymize data; they donâ€™t exploit the observation that the choice of what data to remove is as interesting as what is left, what Julian Sanchez calls â€œThe Redactorâ€™s Dilemmaâ€.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Usernames hardly bear discussing&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your hospital records can be de-anonymized just by looking at public voting rolls14 That researcher later went on to run â€œexperiments on the identifiability of de-identified survey data [cite], pharmacy data [cite], clinical trial data [cite], criminal data [State of Delaware v. Gannett Publishing], DNA [cite, cite, cite], tax data, public health registries [cite (sealed by court), etc.], web logs, and partial Social Security numbers [cite].â€ (Whew.)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your typing is surprisingly unique and the sounds of typing and arm movements can identify you or be used snoop on input &amp;amp; steal passwords&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Knowing your morning commute as loosely as to the individual blocks (or less granular) uniquely identifies (2009) you; knowing your commute to the zip code/census tract uniquely identifies 5% of people&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Your handwriting is fairly unique, sureâ€”but so is how you fill in bubbles on tests15&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Speaking of handwriting, your writing style can be pretty unique too&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the unnoticeable background electrical hum may uniquely date audio recordings. Unnoticeable sounds can also be used to persistently track devices/people, exfiltrate information across air gaps, and can be used to monitor room presence/activity, and even monitor finger movements or tapping noises to help break passphrases or copy physical keys&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;you may have heard of laser microphones for eavesdroppingâ€¦ but what about eavesdropping via video recording of potato chip bags, candy wrappers, hanging light bulbs, or power LEDs? (press release), or cellphone gyroscopes? Lasers are good for detecting your heartbeat as well, which isâ€”of courseâ€”uniquely identifying And hard drives can be turned into microphones. Soon even Lightâ€™s potato chips will no longer be safeâ€¦&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;steering &amp;amp; driving patterns are sufficiently unique as to allow identification of drivers from as little as 1 turn in some cases: et al 2017. These attacks also work on smartphones for time zone, barometric pressure, public transportation timing, IP address, &amp;amp; pattern of connecting to WiFi or cellular networks ( et al 2017), or accelerometers&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;smartphones can be IDed by the pattern of pixel noise, due to sensor noise such as small imperfections in the CCD sensors and lenses (and Facebook has even patented this)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;smartphone usage patterns, such as app preferences, app switching rates, consistency of commute patterns, overall geographic mobility, slower or less driving have been correlated with Alzheimerâ€™s disease ( et al 2019) and personality ( et al 2019).16&lt;/p&gt;
        &lt;p&gt;Eye tracking is also interesting.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;voices correlate with not just age/gender/ethnicity, butâ€¦ overall facial appearance?&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(The only surprising thing about DNA-related privacy breaks is how long they have taken to show up.)&lt;/p&gt;
    &lt;p&gt;To summarize: differential privacy is almost impossible17 and privacy is dead18. (See also â€œBroken Promises of Privacy: Responding to the Surprising Failure of Anonymizationâ€.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Mistake 3&lt;/head&gt;
    &lt;p&gt;Lightâ€™s third mistake was reacting to the canary trap provocation of the Lind L. Tailor broadcast, criticizing Kira, and Light lashing out to use the clearly-visible name &amp;amp; face to kill Lind L. Tailor. The live broadcast was a blatant attempt to provoke a reactionâ€”any reactionâ€”from a surprised &amp;amp; unprepared Light, and that alone should have been sufficient reason to simply ignore it (even if Light could not have reasonably known exactly how it was a trap): one should never do what an enemy wants one to do on ground &amp;amp; terms &amp;amp; timing prepared by the enemy. (Light had the option to use the Death Note at any time in the future, and that would have been almost as good a demonstration of his power as doing so during a live broadcast.)&lt;/p&gt;
    &lt;p&gt;Running the broadcast in 1 region was also a gamble &amp;amp; a potential mistake on Lâ€™s part; he had no real reason to think Light was in Kanto (or if he did already have priors/information to that effect, he shouldâ€™ve been bisecting Kanto) and should have arranged for it to be broadcast to exactly half of Japanâ€™s population, obtaining an expected maximum of 1 bit. But it was one that paid off; he narrowed his target down to 1â„3 the original Japanese population, for a gain of ~1.6 bits. (You can see it was a gamble by considering if Light had been outside Kanto; since he would not see it live, he would not have reacted, and all L would learn is that his suspect was in that other 2â„3 of the population, for a gain of only ~0.3 bits.)&lt;/p&gt;
    &lt;p&gt;But even this wasnâ€™t a huge mistake. He lost 6 bits to his schedule of killing, and lost another 1.6 bits to temperamentally killing Lind L. Tailor, but since the male population of Kanto is 21.5 million (43 million total), he still has ~24 bits of anonymity left (log2 (21500000) â‰ˆ 24.36). Thatâ€™s not too terrible, and the loss is mitigated even further by other details of this mistake, as pointed out by Zmflavius; specifically, that unlike â€œbeing maleâ€ or â€œbeing Japaneseâ€, the information about being in Kanto is subject to decay, since people move around all the time for all sorts of reasons:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;â€¦quite possibly Lightâ€™s biggest mistake was inadvertently revealing his connection to the police hierarchy by hacking his dadâ€™s computer. Whereas even the Lind L. Taylor debacle only revealed his killing mechanics and narrowed him down to â€œsomeone in the Kanto regionâ€ (which is, while an impressive accomplishment based on the information he had, entirely meaningless for actually finding a suspect), there were perhaps a few hundred people who had access to the information Lightâ€™s dad had. Thereâ€™s also the fact that L knew that Light was probably someone in their late teens, meaning that there was an extremely high chance that at the end of the school year, even that coup of his would expire, thanks to students heading off to university all over Japan (of course, Light went to Toudai, and a student of his caliber not attending such a university would be suspicious, but L had no way of knowing that then). I mean, perhaps L had hoped that Kira would reveal himself by suddenly moving away from the Kanto region, but come the next May, he would have no way of monitoring unusual movements among late teenagers, because a large percentage of them would be moving for legitimate reasons.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;(One could still run the inference â€œbackwardsâ€ on any particular person to verify they were in Kanto in the right time period, but as time passes, it becomes less possible to run the inference â€œforwardsâ€ and only examine people in Kanto.)&lt;/p&gt;
    &lt;p&gt;This mistake also shows us that the important thing that information theory buys us, really, is not the bit (we could be using log10 rather than log2, and compares â€œditsâ€ rather than â€œbitsâ€) so much as comparing events in the plot on a logarithmic scale. If we simply looked at how the absolute number of how many people were ruled out at each step, weâ€™d conclude that the first mistake by Light was a debacle without compare since it let L rule out &amp;gt;6 billion people, approximately 60Ã— more people than all the other mistakes put together would let L rule out. Mistakes are relative to each other, not absolutes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mistake 4&lt;/head&gt;
    &lt;p&gt;Lightâ€™s fourth mistake was to use confidential police information stolen using his policeman fatherâ€™s credentials. This was unnecessary as there are countless criminals he could still execute using public information (face+name is not typically difficult to get), and if for some reason he needed a specific criminal, he could either restrict use of secret information to a few high-priority victimsâ€”if only to avoid suspicions of hacking &amp;amp; subsequent security upgrades costing him access!â€”or manufacture, using the Death Noteâ€™s coercive powers or Kiraâ€™s public support, a way to release information such as a â€˜leakâ€™ or passing public transparency laws.&lt;/p&gt;
    &lt;p&gt;This mistake was the largest in bits lost. But interestingly, many or even most Death Note fans do not seem to regard this as his largest mistake, instead pointing to his killing Lind L. Tailor or perhaps relying too much on Mikami. The information theoretical perspective strongly disagrees, and lets us quantify how large this mistake was.&lt;/p&gt;
    &lt;p&gt;When he acts on the secret police information, he instantly cuts down his possible identity to one out of a few thousand people connected to the police. Letâ€™s be generous and say 10,000. It takes 14 bits to specify 1 person out of 10,000 (log2 (1,0000) â‰ˆ 13.29)â€”as compared to the 24â€“25 bits to specify a Kanto dweller.&lt;/p&gt;
    &lt;p&gt;This mistake cost him 11 bits of anonymity; in other words, this mistake cost him twice what his scheduling cost him and almost 8 times the murder of Tailor!&lt;/p&gt;
    &lt;head rend="h2"&gt;Mistake 5&lt;/head&gt;
    &lt;p&gt;In comparison, the fifth mistake, murdering Ray Penbarâ€™s fiancee and focusing Lâ€™s suspicion on Penbarâ€™s assigned targets was positively cheap. If we assume Penbar was tasked 200 leads out of the 10,000, then murdering him and the fiancee dropped Light from 14 bits to 8 bits (log2 (200) â‰ˆ 7.64) or just 6 bits or a little over half the fourth mistake and comparable to the original scheduling mistake.&lt;/p&gt;
    &lt;head rend="h2"&gt;Endgame&lt;/head&gt;
    &lt;p&gt;At this point in the plot, L resorts to direct measures and enters Lightâ€™s life directly, enrolling at the university. From this point on, Light is screwed as he is now playing a deadly game of Mafia with L &amp;amp; the investigative team. He frittered away &amp;gt;25 bits of anonymity and then L intuited the rest and suspected him all along. (We could justify L skipping over the remaining 8 bits by pointing out that L can analyze the deaths and infer psychological characteristics like arrogance, puzzle-solving, and great intelligence, which combined with heuristically searching the remaining candidates, could lead him to zero in on Light.)&lt;/p&gt;
    &lt;p&gt;From the theoretical point of view, the game was over at that point. The challenge for L then became proving it to Lâ€™s satisfaction under his self-imposed moral constraints.19&lt;/p&gt;
    &lt;head rend="h1"&gt;Security Is Hard (Letâ€™s Go Shopping)&lt;/head&gt;
    &lt;p&gt;What should Light have done? Thatâ€™s easy to answer, but tricky to implement.&lt;/p&gt;
    &lt;p&gt;One could try to manufacture disinformation. Terence Tao rehearses many of the above points about information theory &amp;amp; anonymity, and goes on to loosely discuss the possible benefits of faking information:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;â€¦one additional way to gain more anonymity is through deliberate disinformation. For instance, suppose that one reveals 100 independent bits of information about oneself. Ordinarily, this would cost 100 bits of anonymity (assuming that each bit was a priori equally likely to be true or false), by cutting the number of possibilities down by a factor of 2100; but if 5 of these 100 bits (chosen randomly and not revealed in advance) are deliberately falsified, then the number of possibilities increases again by a factor of (100&lt;/p&gt;&lt;code&gt;choose&lt;/code&gt;5) ~ 226, recovering about 26 bits of anonymity. In practice one gains even more anonymity than this, because to dispel the disinformation one needs to solve a satisfiability problem, which can be notoriously intractable computationally, although this additional protection may dissipate with time as algorithms improve (eg. by incorporating ideas from compressed sensing).&lt;/quote&gt;
    &lt;head rend="h2"&gt;Randomizing&lt;/head&gt;
    &lt;p&gt;The difficulty with suggesting that Light shouldâ€”or couldâ€”have used disinformation on the timing of deaths is that we are, in effect, engaging in a sort of hindsight bias.&lt;/p&gt;
    &lt;p&gt;How exactly is Light or anyone supposed to know that L could deduce his timezone from his killings? I mentioned an example of using Wikipedia edits to localize editors, but that technique was unique to me among WP editors20 and no doubt there are many other forms of information leakage I have never heard of despite compiling a list; if I were Light, even if I remembered my Wikipedia technique, I might not bother evenly distributing my killing over the clock or adopting a deceptive pattern (eg. suggesting I was in Europe rather than Japan).&lt;/p&gt;
    &lt;p&gt;If Light had known he was leaking timing information but didnâ€™t know that someone out there was clever enough to use it (a â€œknown unknownâ€), then we might blame him; but how is Light supposed to know these â€œunknown unknownsâ€?&lt;/p&gt;
    &lt;p&gt;Randomization is the answer. Randomization and encryption scramble the correlations between input and output, and they would serve as well in Death Note as they do in cryptography &amp;amp; statistics in the real world, at the cost of some efficiency. The point of randomization, both in cryptography and in statistical experiments, is to not just prevent the leaked information or confounders (respectively) you do know about but also the ones you do not yet know about.&lt;/p&gt;
    &lt;p&gt;To steal &amp;amp; paraphrase an example from Jim Manziâ€™s Uncontrolled: youâ€™re running a weight-loss experiment. You know that the effectiveness might vary with each subjectâ€™s pre-existing weight, but you donâ€™t believe in randomization (youâ€™re a practical man! only prissy statisticians worry about randomization!); so you split the subjects by weight, and for convenience you allocate them by when they show up to your experimentâ€”in the end, there are exactly 10 experimental subjects over 150 pounds and 10 controls over 150 pounds, and so on and so forth. Unfortunately, it turns out that unbeknownst to you, a genetic variant controls weight gain and a whole extended family showed up at your experiment early on and they all got allocated to â€˜experimentalâ€™ and none of them to â€˜controlâ€™ (since you didnâ€™t need to randomize, right? you were making sure the groups were matched on weight!). Your experiment is now bogus and misleading. Of course, you could run a second experiment where you make sure the experimental and control groups are matched on weight and also now matched on that genetic variantâ€¦ but now thereâ€™s the potential for some third confounder to hit you. If only you had used randomizationâ€”then you would probably have put some of the variants into the other group as well and your results wouldnâ€™tâ€™ve been bogus!&lt;/p&gt;
    &lt;p&gt;So to deal with Lightâ€™s first mistake, simply scheduling every death on the hour will not work because the sleep-wake cycle is still present. If he set up a list and wrote down n criminals for each hour to eliminate the peak-troughs rather than randomizing, could that still go wrong? Maybe: we donâ€™t know what information might be left in the data which an L or Turing could decipher. I can speculate about one possibilityâ€”the allocation of each kind of criminal to each hour. If one were to draw up lists and go in order (hey, one doesnâ€™t need randomization, right?), then the order might go â€˜criminals in the morning newspaper, criminals on TV, criminals whose details were not immediately given but were available online, criminals from years ago, historical criminals etc.â€™; if the morning-newspaper-criminals start at say 6 AM Japan timeâ€¦ And allocating evenly might be hard, since thereâ€™s naturally going to be shortfalls when there just arenâ€™t many criminals that day or the newspapers arenâ€™t publishing (holidays?) etc., so the shortfall periods will pinpoint what the Kira considers â€˜end of the dayâ€™.&lt;/p&gt;
    &lt;p&gt;A much safer procedure is thorough-going randomization applied to timing, subjects, and manner of death. Even if we assume that Light was bound and determined to reveal the existence of Kira and gain publicity and international notoriety (a major character flaw in its own right; accomplishing things, taking creditâ€”choose one), he still did not have to reduce his anonymity much past 32 bits.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Each executionâ€™s time could be determined by a random dice roll (say, a 24-sided dice for hours and a 60-sided dice for minutes).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Selecting method of death could be done similarly based on easily researched demographic data, although perhaps irrelevant (serving mostly to conceal that a killing has taken place).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Selecting criminals could be based on internationally accessible periodicals that plausibly every human has access to, such as the New York Times, and deaths could be delayed by months or years to broaden the possibilities as to where the Kira learned of the victim (TV? books? the Internet?) and avoiding issues like killing a criminal only publicized on one obscure Japanese public television channel. And so on.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Letâ€™s remember that all this is predicated on anonymity, and on Light using low-tech strategies; as one person asked me, â€œwhy doesnâ€™t Light set up an cryptographic assassination market or just take over the world? He would win without all this cleverness.â€ Well, then it would not be Death Note.&lt;/p&gt;
    &lt;head rend="h1"&gt;See Also&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;â€œWho wrote the Death Note script?â€ (statistical analysis of authorship)&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;External Links&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Discussion:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Translation: Russian (RU)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;â€œOn Murder Considered as one of the Fine Artsâ€, Thomas De Quincey&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;â€œStakeout: how the FBI tracked and busted a Chicago Anon; Continuous surveillance, informants, trap-and-trace gear-the FBI spared no â€¦â€ (deanonymizing Jeremy Hammond)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;â€œToxic pairs, re-identification, and information theory: Nationality &amp;amp; Religionâ€&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;â€œHow I targeted the Reddit CEO with Facebook ads to get a job interview at Redditâ€ (HN)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;â€œâ€˜Shatteredâ€™: Inside the secret battle to save Americaâ€™s undercover spies in the digital ageâ€&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;â€œThe signal quality of earnings announcements: evidence from an informed trading cartelâ€, 2020&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Appendices&lt;/head&gt;
    &lt;head rend="h2"&gt;Communicating With a Death Note&lt;/head&gt;
    &lt;p&gt;One might wonder how much information one could send intentionally with a Death Note, as opposed to inadvertently leak bits about oneâ€™s identity. As deaths are by and large publicly known information, weâ€™ll assume the sender and recipient have some sort of pre-arranged key or one-time pad (although one would wonder why theyâ€™d use such an immoral and clumsy system as opposed to steganography or messages online).&lt;/p&gt;
    &lt;p&gt;A death inflicted by a Death Note has 3 main distinguishing traits which one can controlâ€”who, when, and how:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;the person&lt;/p&gt;
        &lt;p&gt;The â€˜who?â€™ is already calculated for us: if it takes 33 bits to specify a unique human, then a particular human can convey 33 bits. Concerns about learnability (how would you learn of an Amazon tribesmanâ€™s death?) imply that itâ€™s really &amp;lt;33 bits.&lt;/p&gt;
        &lt;p&gt;If you try some scheme to encode more bits into the choice of assassination, you either wind up with 33 bits or you wind up unable to convey certain combinations of bits and effectively 33 bits anywayâ€”your scheme will tell you that to convey your desperately important message X of 50 bits telling all about Lâ€™s true identity and how you discovered it, you need to kill an Olafur Jacobs of Tanzania who weighs more than 200 pounds and is from Taiwan, but alas! Jacobs doesnâ€™t exist for you to kill.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the time&lt;/p&gt;
        &lt;p&gt;The â€˜whenâ€™ is handled by similar reasoning. There is a certain granularity to Death Note kills: even if it is capable of timing deaths down to the nanosecond, one canâ€™t actually witness this or receive records of this. Doctors may note time of death down to the minute, but no finer (and how do you get such precise medical records anyway?). News reports may be even less accurate, noting merely that it happened in the morning or in the late evening. In rare cases like live broadcasts, one may be able to do a little better, but even they tend to be delayed by a few seconds or minutes to allow for buffering, technical glitches be fixed, the stenographers produce the closed captioning, or simply to guard against embarrassing events (like Janet Jacksonâ€™s nipple-slip). So weâ€™ll not assume the timing can be more accurate than the minute. But which minutes does a Death Note user have to choose from? Inasmuch as the Death Note is apparently incapable of influencing the past or causing Pratchettian21 superluminal effects, the past is off-limits; but messages also have to be sent in time for whatever they are supposed to influence, so one cannot afford to have a window of a century. If the message needs to affect something within the day, then the user has a window of only 60 Â· 24 = 1,440 minutes, which is log2(1,440) = 10.49 bits; if the user has a window of a year, thatâ€™s slightly better, as a deathâ€™s timing down to the minute could embody as much as log2(60 Â· 24 Â· 365) = 19 bits. (Over a decade then is 22.3 bits, etc.) If we allow timing down to the second, then a year would be 24.9 bits. In any case, itâ€™s clear that weâ€™re not going to get more than 33 bits from the date. On the plus side, an â€˜IP over Deathâ€™ protocol would be superior to some other protocolsâ€”here, the worse your latency, the more bits you could extract from the packetâ€™s timestamp! Dinosaur Comics on compression schemes:&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the circumstances (such as the place)&lt;/p&gt;
        &lt;p&gt;The â€˜howâ€™â€¦ has many more degrees of freedom. The circumstances is much more difficult to calculate. We can subdivide it in a lot of ways; hereâ€™s one:&lt;/p&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Location (eg. latitude/longitude)&lt;/p&gt;
            &lt;p&gt;Earth has ~510,072,000,000 square meters of surface area; most of it is entirely useless from our perspectiveâ€”if someone is in an airplane and dies, how on earth does one figure out the exact square meter he was above? Or on the oceans? Earth has ~148,940,000,000 square meters of land, which is more usable: the usual calculations gives us log2(148940000000) = 37.12 bits. (Surprised at how similar to the â€˜who?â€™ bit calculation this is? But 37.12 - 33 = 4.12 and 24.12 = 17.4. The SF classic Stand on Zanzibar drew its name from the observation that the 7 billion people alive in 201016ya would fit in Zanzibar only if they stood shoulder to shoulderâ€”spread them out, and multiply that area by ~18â€¦) This raises an issue that affects all 3: how much can the Death Note control? Can it move victims to arbitrary points in, say, Siberia? Or is it limited to within driving distance? etc. Any of those issues could shrink the 37 bits by a great deal.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Cause Of Death&lt;/p&gt;
            &lt;p&gt;The International Classification of Diseases lists upwards of 20,000 diseases, and we can imagine thousands of possible accidental or deliberate deaths. But what matters is what gets communicated: if there are 500 distinct brain cancers but the death is only reported as â€˜brain cancerâ€™, the 500 count as 1 for our purposes. But weâ€™ll be generous and go with 20,000 for reported diseases plus accidents, which is log2(20000) = 14.3 bits.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Action Prior To Death&lt;/p&gt;
            &lt;p&gt;Actions prior to death overlaps with accidental causes; here the series doesnâ€™t help us. Lightâ€™s early experiments culminating in the â€œL, do you know death gods love apples?â€ seem to imply that actions are limited in entropy as each word took a death (assuming the ordinary English vocabulary of 50,000 words, 16 bits), but other plot events imply that humans can undertake long complex plans at the order of Death Notes (like Mikami bringing the fake Death Note to the final confrontation with Near). Actions before death could be reported in great detail, or they could be hidden under official secrecy like the aforementioned death gods mentioned (Light uniquely privileged in learning it succeeded as part of L testing him). I canâ€™t begin to guess how many distinct narratives would survive transmission or what limits the Note would set. We must leave this one undefined: itâ€™s almost surely more than 10 bits, but how many?&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Summing, we get &amp;lt;33 + &amp;lt;19 + 17 + &amp;lt;37 + 14 + ? = 120? bits per death.&lt;/p&gt;
    &lt;head rend="h2"&gt;â€œBayesian Jurisprudenceâ€&lt;/head&gt;
    &lt;p&gt;E.T. Jaynes in his posthumous Probability Theory: The Logic of Science (on Bayesian statistics) includes a chapter 5 on â€œQueer Uses For Probability Theoryâ€, discussing such topics as ESP; miracles; heuristics &amp;amp; biases; how visual perception is theory-laden; philosophy of science with regard to Newtonian mechanics and the famed discovery of Neptune; horse-racing &amp;amp; weather forecasting; and finallyâ€”section 5.8, â€œBayesian jurisprudenceâ€. Jaynesâ€™s analysis is somewhat similar in spirit to my above analysis, although mine is not explicitly Bayesian except perhaps in the discussion of gender as eliminating one necessary bit.&lt;/p&gt;
    &lt;p&gt;The following is an excerpt; see also â€œBayesian Justiceâ€.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;It is interesting to apply probability theory in various situations in which we canâ€™t always reduce it to numbers very well, but still it shows automatically what kind of information would be relevant to help us do plausible reasoning. Suppose someone in New York City has committed a murder, and you donâ€™t know at first who it is, but you know that there are 10 million people in New York City. On the basis of no knowledge but this, e(Guilty|X) = âˆ’70 db is the plausibility that any particular person is the guilty one.&lt;/p&gt;
      &lt;p&gt;How much positive evidence for guilt is necessary before we decide that some man should be put away? Perhaps +40 db, although your reaction may be that this is not safe enough, and the number ought to be higher. If we raise this number we give increased protection to the innocent, but at the cost of making it more difficult to convict the guilty; and at some point the interests of society as a whole cannot be ignored.&lt;/p&gt;
      &lt;p&gt;For example, if 1,000 guilty men are set free, we know from only too much experience that 200 or 300 of them will proceed immediately to inflict still more crimes upon society, and their escaping justice will encourage 100 more to take up crime. So it is clear that the damage to society as a whole caused by allowing 1,000 guilty men to go free, is far greater than that caused by falsely convicting one innocent man.&lt;/p&gt;
      &lt;p&gt;If you have an emotional reaction against this statement, I ask you to think: if you were a judge, would you rather face one man whom you had convicted falsely; or 100 victims of crimes that you could have prevented? Setting the threshold at +40 db will mean, crudely, that on the average not more than one conviction in 10,000 will be in error; a judge who required juries to follow this rule would probably not make one false conviction in a working lifetime on the bench.&lt;/p&gt;
      &lt;p&gt;In any event, if we took +40 db starting out from âˆ’70 db, this means that in order to ensure a conviction you would have to produce about 110 db of evidence for the guilt of this particular person. Suppose now we learn that this person had a motive. What does that do to the plausibility for his guilt? Probability theory says&lt;/p&gt;
      &lt;p&gt;(5-38)&lt;/p&gt;
      &lt;p&gt;since , i.e. we consider it quite unlikely that the crime had no motive at all. Thus, the [importance] of learning that the person had a motive depends almost entirely on the probability that an innocent person would also have a motive.&lt;/p&gt;
      &lt;p&gt;This evidently agrees with our common sense, if we ponder it for a moment. If the deceased were kind and loved by all, hardly anyone would have a motive to do him in. Learning that, nevertheless, our suspect did have a motive, would then be very [important] information. If the victim had been an unsavory character, who took great delight in all sorts of foul deeds, then a great many people would have a motive, and learning that our suspect was one of them is not so [important]. The point of this is that we donâ€™t know what to make of the information that our suspect had a motive, unless we also know something about the character of the deceased. But how many members of juries would realize that, unless it was pointed out to them?&lt;/p&gt;
      &lt;p&gt;Suppose that a very enlightened judge, with powers not given to judges under present law, had perceived this fact and, when testimony about the motive was introduced, he directed his assistants to determine for the jury the number of people in New York City who had a motive. If this number is then&lt;/p&gt;
      &lt;p&gt;and equation (5-38) reduces, for all practical purposes, to&lt;/p&gt;
      &lt;p&gt;(5-39)&lt;/p&gt;
      &lt;p&gt;You see that the population of New York has canceled out of the equation; as soon as we know the number of people who had a motive, then it doesnâ€™t matter any more how large the city was. Note that (5-39) continues to say the right thing even when is only 1 or 2.&lt;/p&gt;
      &lt;p&gt;You can go on this way for a long time, and we think you will find it both enlightening and entertaining to do so. For example, we now learn that the suspect was seen near the scene of the crime shortly before. From Bayesâ€™ theorem, the [importance] of this depends almost entirely on how many innocent persons were also in the vicinity. If you have ever been told not to trust Bayesâ€™ theorem, you should follow a few examples like this a good deal further, and see how infallibly it tells you what information would be relevant, what irrelevant, in plausible reasoning.22&lt;/p&gt;
      &lt;p&gt;In recent years there has grown up a considerable literature on Bayesian jurisprudence; for a review with many references, see 1996 [This is apparently Interpreting Evidence: Evaluating Forensic Science in the Courtroom â€“Editor].&lt;/p&gt;
      &lt;p&gt;Even in situations where we would be quite unable to say that numerical values should be used, Bayesâ€™ theorem still reproduces qualitatively just what your common sense (after perhaps some meditation) tells you. This is the fact that George PÃ³lya demonstrated in such exhaustive detail that the present writer was convinced that the connection must be more than qualitative.&lt;/p&gt;
    &lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46839743</guid><pubDate>Sat, 31 Jan 2026 19:11:34 +0000</pubDate></item><item><title>Berlin: Record harvest sparks mass giveaway of free potatoes</title><link>https://www.theguardian.com/world/2026/jan/31/record-harvest-berlin-giveaway-potatoes</link><description>&lt;doc fingerprint="e6cd836109ba3396"&gt;
  &lt;main&gt;
    &lt;p&gt;Germans love their potatoes. They eat on average 63kg a person every year, according to official statistics.&lt;/p&gt;
    &lt;p&gt;But the exceptional glut of potatoes produced by farmers during the last harvest has overwhelmed even the hardiest of fans.&lt;/p&gt;
    &lt;p&gt;Named the Kartoffel-Flut (potato flood), after the highest yield in 25 years, the bumper crop has inspired one farmer to organise a potato dump on Berlin, with appeals going out around the German capital for people to come to various hotspots and pick them up for free.&lt;/p&gt;
    &lt;p&gt;Soup kitchens, homeless shelters, kindergartens, schools, churches and non-profit organisations are among those to have taken their fill. Even Berlin zoo has participated in the â€œrescue missionâ€, taking tonnes of potatoes that would otherwise have gone to landfill, or to produce biogas, to feed its animals. Two lorry loads have been sent to Ukraine.&lt;/p&gt;
    &lt;p&gt;Ordinary city residents, many feeling the squeeze over the rise in the cost of living, have arrived at pre-announced potato dump locations, filling up anything from sacks and buckets to handcarts.&lt;/p&gt;
    &lt;p&gt;Astrid Marz queued recently in Kaulsdorf, on the eastern edge of Berlin, one of 174 distribution points spontaneously set up around the city, to stuff an old rucksack with spuds. â€œI stopped counting at 150. I think Iâ€™ve got enough to keep me and my neighbours going until the end of the year,â€ she said.&lt;/p&gt;
    &lt;p&gt;The operation, called 4000 Tonnes after the surplus a single potato farmer near Leipzig offered in December after a sale fell through at the last minute, was organised by a Berlin newspaper with the Berlin-based eco-friendly not-for-profit search engine Ecosia.&lt;/p&gt;
    &lt;p&gt;â€œAt first I thought it was some AI-generated fake news when I saw it on social media,â€ Marz, a teacher, said. â€œThere were pictures of huge mountains of â€˜earth applesâ€™,â€ she recalled, using the word ErdÃ¤pfel, an affectionate term for the potato sometimes used by Berliners, â€œwith the instruction to come and get them for free!â€&lt;/p&gt;
    &lt;p&gt;The excitement has lifted spirits at a time when arctic cold has Berlin in its grip, hampering travel, grinding public transport to a halt and leaving pavements hazardously icy.&lt;/p&gt;
    &lt;p&gt;â€œThere was a really party-like atmosphere,â€ said Ronald, describing how people cheerily helped one other with heavy loads and swapped culinary tips when he recently picked up potatoes for his family at the Tempelhofer Feld.&lt;/p&gt;
    &lt;p&gt;As a result of the buzz, the potato is receiving something of a new lease of life.&lt;/p&gt;
    &lt;p&gt;It has helped resurrect stories about how the humble tuber first became popular in Germany, after Prussiaâ€™s Frederick II issued an order for its cultivation in the 18th century, known as the Kartoffelbefehl (potato decree), establishing it as a staple food despite reported initial scepticism over its strange texture and form.&lt;/p&gt;
    &lt;p&gt;Recipes galore are being shared online as those who have scooped up the spuds try to work out what to do with the surfeit.&lt;/p&gt;
    &lt;p&gt;Although the potato has sometimes been spurned in recent years as some fitness gurus have recommended avoiding carbohydrates, experts have highlighted its nutritional properties, such as vitamin C and potassium.&lt;/p&gt;
    &lt;p&gt;Celebrity Berlin chef Marco MÃ¼ller of the Rutz restaurant has said now is the ideal moment to give the potato the Michelin-star treatment. He uses an innovative technique to make a rich broth from roasted potato peelings and a sought-after potato vinaigrette.&lt;/p&gt;
    &lt;p&gt;Another of the recipes doing the rounds is Angela Merkelâ€™s Kartoffelsuppe (potato soup), which the former German chancellor first shared with voters in the run-up to 2017â€™s general election in an interview with a celebrity magazine.&lt;/p&gt;
    &lt;p&gt;Her hot pot tip? To give it the necessary lumpy texture, she revealed: â€œI always pound the potatoes myself with a potato masher, rather than using a food mixer.â€&lt;/p&gt;
    &lt;p&gt;Criticism has come from farmers in the region, who say the market in Berlin is even more saturated and their crop has been devalued further still by the vast giveaway.&lt;/p&gt;
    &lt;p&gt;More widely, environmental lobbyists have said the glut in part stems from a warped and out-of-control food industry, and that the mountains of potatoes pictured in storage facilities across the region is reminiscent of the notorious butter mountains and milk lakes of the 1970s, when farmers were overly incentivised to produce food owing to the European Economic Communityâ€™s guarantee to buy up surplus products at high prices.&lt;/p&gt;
    &lt;p&gt;While itâ€™s the potatoâ€™s turn this year, last year hops were in surplus and next year, it is predicted, it will be milk.&lt;/p&gt;
    &lt;p&gt;A last hoorah for the intervention is expected in the coming days, and those keen to participate in the potato party are urged to keep a close eye on the organisersâ€™ website for the next drops.&lt;/p&gt;
    &lt;p&gt;There are, in theory, about 3,200 tonnes (3,200,000kg or 7,056,000lbs) still up for grabs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46839784</guid><pubDate>Sat, 31 Jan 2026 19:15:52 +0000</pubDate></item><item><title>Show HN: Minimal â€“ Open-Source Community driven Hardened Container Images</title><link>https://github.com/rtvkiz/minimal</link><description>&lt;doc fingerprint="84dbd39055d7784"&gt;
  &lt;main&gt;
    &lt;p&gt;A collection of production-ready container images with minimal CVEs, rebuilt daily using Chainguard's apko and Wolfi packages. By including only required packages, these images maintain a reduced attack surface and typically have zero or near-zero known vulnerabilities.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Image&lt;/cell&gt;
        &lt;cell role="head"&gt;Pull Command&lt;/cell&gt;
        &lt;cell role="head"&gt;Shell&lt;/cell&gt;
        &lt;cell role="head"&gt;Use Case&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-python:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Python apps, microservices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Node.js&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-node:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;Node.js apps, JavaScript&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Bun&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-bun:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Fast JavaScript/TypeScript runtime&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Go&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-go:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Go development, CGO builds&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Nginx&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-nginx:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Reverse proxy, static files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;HTTPD&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-httpd:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Maybe*&lt;/cell&gt;
        &lt;cell&gt;Apache web server&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Jenkins&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-jenkins:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Yes&lt;/cell&gt;
        &lt;cell&gt;CI/CD automation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Redis-slim&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-redis-slim:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;In-memory data store&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PostgreSQL-slim&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;docker pull ghcr.io/rtvkiz/minimal-postgres-slim:latest&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;No&lt;/cell&gt;
        &lt;cell&gt;Relational database&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;*HTTPD, Jenkins,Node.js may include shell(sh,busybox) via transitive Wolfi dependencies. CI treats shell presence as informational.&lt;/p&gt;
    &lt;p&gt;Container vulnerabilities are a top attack vector. Most base images ship with dozens of known CVEs that take weeks or months to patch:&lt;/p&gt;
    &lt;code&gt;Traditional images:     Your containers:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ debian:latest    â”‚    â”‚ minimal-python   â”‚
â”‚ 127 CVEs         â”‚    â”‚ 0-5 CVEs         â”‚
â”‚ Patched: ~30 daysâ”‚    â”‚ Patched: &amp;lt;48 hrs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;p&gt;Impact:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Pass security audits and compliance requirements (SOC2, FedRAMP, PCI-DSS)&lt;/item&gt;
      &lt;item&gt;Reduce attack surface with minimal, distroless images&lt;/item&gt;
      &lt;item&gt;Get CVE patches within 24-48 hours of disclosure (vs weeks for Debian/Ubuntu)&lt;/item&gt;
      &lt;item&gt;Cryptographically signed images with full SBOM for supply chain security&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Python - run your app
docker run --rm -v $(pwd):/app ghcr.io/rtvkiz/minimal-python:latest /app/main.py

# Node.js - run your app
docker run --rm -v $(pwd):/app -w /app ghcr.io/rtvkiz/minimal-node:latest index.js

# Bun - fast JavaScript runtime
docker run --rm ghcr.io/rtvkiz/minimal-bun:latest --version

# Go - build your app
docker run --rm -v $(pwd):/app -w /app ghcr.io/rtvkiz/minimal-go:latest build -o /tmp/app .

# Nginx - reverse proxy
docker run -d -p 8080:80 ghcr.io/rtvkiz/minimal-nginx:latest

# HTTPD - serve static content
docker run -d -p 8080:80 ghcr.io/rtvkiz/minimal-httpd:latest

# Jenkins - CI/CD controller
docker run -d -p 8080:8080 -v jenkins_home:/var/jenkins_home ghcr.io/rtvkiz/minimal-jenkins:latest

# Redis - in-memory data store
docker run -d -p 6379:6379 ghcr.io/rtvkiz/minimal-redis-slim:latest

# PostgreSQL - relational database
docker run -d -p 5432:5432 -v pgdata:/var/lib/postgresql/data ghcr.io/rtvkiz/minimal-postgres-slim:latest&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Image&lt;/cell&gt;
        &lt;cell role="head"&gt;Version&lt;/cell&gt;
        &lt;cell role="head"&gt;User&lt;/cell&gt;
        &lt;cell role="head"&gt;Entrypoint&lt;/cell&gt;
        &lt;cell role="head"&gt;Workdir&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Python&lt;/cell&gt;
        &lt;cell&gt;3.13.x&lt;/cell&gt;
        &lt;cell&gt;nonroot (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/python3&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/app&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Node.js&lt;/cell&gt;
        &lt;cell&gt;22.x LTS&lt;/cell&gt;
        &lt;cell&gt;nonroot (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/dumb-init -- /usr/bin/node&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/app&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Bun&lt;/cell&gt;
        &lt;cell&gt;latest&lt;/cell&gt;
        &lt;cell&gt;nonroot (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/bun&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/app&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Go&lt;/cell&gt;
        &lt;cell&gt;1.25.x&lt;/cell&gt;
        &lt;cell&gt;nonroot (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/go&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/app&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Nginx&lt;/cell&gt;
        &lt;cell&gt;mainline&lt;/cell&gt;
        &lt;cell&gt;nginx (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/sbin/nginx -g "daemon off;"&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;HTTPD&lt;/cell&gt;
        &lt;cell&gt;2.4.x&lt;/cell&gt;
        &lt;cell&gt;www-data (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/sbin/httpd -DFOREGROUND&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/var/www/localhost/htdocs&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Jenkins&lt;/cell&gt;
        &lt;cell&gt;2.541.x LTS&lt;/cell&gt;
        &lt;cell&gt;jenkins (1000)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;tini -- java -jar jenkins.war&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/var/jenkins_home&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Redis&lt;/cell&gt;
        &lt;cell&gt;8.4.x&lt;/cell&gt;
        &lt;cell&gt;redis (65532)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/redis-server&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;PostgreSQL&lt;/cell&gt;
        &lt;cell&gt;18.x&lt;/cell&gt;
        &lt;cell&gt;postgres (70)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/usr/bin/postgres&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;/&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         BUILD PIPELINE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Package Source            Image Assembly           Verification    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Wolfi     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚    apko    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Trivy    â”‚   â”‚
â”‚  â”‚ (pre-built) â”‚  install â”‚ (OCI image)â”‚  scan    â”‚ (CVE gate) â”‚   â”‚
â”‚  â”‚ Python, Go, â”‚          â”‚            â”‚          â”‚            â”‚   â”‚
â”‚  â”‚ Node, etc.  â”‚          â”‚            â”‚          â”‚            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                 â”‚                       â”‚          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚                       â–¼          â”‚
â”‚  â”‚   melange   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ (Jenkins,   â”‚  build from                   â”‚ cosign + SBOM  â”‚  â”‚
â”‚  â”‚  Redis)     â”‚  source                       â”‚ (sign &amp;amp; publishâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Trigger&lt;/cell&gt;
        &lt;cell role="head"&gt;When&lt;/cell&gt;
        &lt;cell role="head"&gt;Purpose&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scheduled&lt;/cell&gt;
        &lt;cell&gt;Daily at 2:00 AM UTC&lt;/cell&gt;
        &lt;cell&gt;Pick up latest CVE patches from Wolfi&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Push&lt;/cell&gt;
        &lt;cell&gt;On merge to &lt;code&gt;main&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;Deploy configuration changes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Manual&lt;/cell&gt;
        &lt;cell&gt;Workflow dispatch&lt;/cell&gt;
        &lt;cell&gt;Emergency rebuilds&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;All builds must pass a CVE gate (no CRITICAL/HIGH severity vulnerabilities) before publishing.&lt;/p&gt;
    &lt;code&gt;# Prerequisites
go install chainguard.dev/apko@latest
go install chainguard.dev/melange@latest  # needed for Jenkins, Redis
brew install trivy  # or: apt install trivy

# Build all images
make build

# Build specific image
make python
make node
make bun
make go
make nginx
make httpd
make jenkins
make redis-slim
make postgres-slim

# Scan for CVEs
make scan

# Run tests
make test&lt;/code&gt;
    &lt;code&gt;minimal/
â”œâ”€â”€ python/apko/python.yaml       # Python image (Wolfi pkg)
â”œâ”€â”€ node/apko/node.yaml           # Node.js image (Wolfi pkg)
â”œâ”€â”€ bun/apko/bun.yaml             # Bun image (Wolfi pkg)
â”œâ”€â”€ go/apko/go.yaml               # Go image (Wolfi pkg)
â”œâ”€â”€ nginx/apko/nginx.yaml         # Nginx image (Wolfi pkg)
â”œâ”€â”€ httpd/apko/httpd.yaml         # HTTPD image (Wolfi pkg)
â”œâ”€â”€ jenkins/
â”‚   â”œâ”€â”€ apko/jenkins.yaml         # Jenkins image
â”‚   â””â”€â”€ melange.yaml              # jlink JRE build
â”œâ”€â”€ redis-slim/
â”‚   â”œâ”€â”€ apko/redis.yaml           # Redis image
â”‚   â””â”€â”€ melange.yaml              # Redis source build
â”œâ”€â”€ postgres-slim/apko/postgres.yaml  # PostgreSQL image (Wolfi pkg)
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ build.yml                 # Daily CI pipeline
â”‚   â”œâ”€â”€ update-jenkins.yml        # Jenkins version updates
â”‚   â”œâ”€â”€ update-redis.yml          # Redis version updates
â”‚   â””â”€â”€ update-wolfi-packages.yml # Wolfi package updates
â”œâ”€â”€ Makefile
â””â”€â”€ LICENSE
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CVE gate - Builds fail if any CRITICAL/HIGH vulnerabilities detected&lt;/item&gt;
      &lt;item&gt;Signed images - All images signed with cosign keyless signing&lt;/item&gt;
      &lt;item&gt;SBOM generation - Full software bill of materials in SPDX format&lt;/item&gt;
      &lt;item&gt;Non-root users - All images run as non-root by default&lt;/item&gt;
      &lt;item&gt;Minimal attack surface - Only essential packages included&lt;/item&gt;
      &lt;item&gt;Shell-less images - Most images have no shell&lt;/item&gt;
      &lt;item&gt;Reproducible builds - Declarative apko configurations&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All images are signed with cosign keyless signing via Sigstore. To verify:&lt;/p&gt;
    &lt;code&gt;cosign verify \
  --certificate-oidc-issuer https://token.actions.githubusercontent.com \
  --certificate-identity-regexp https://github.com/rtvkiz/minimal/ \
  ghcr.io/rtvkiz/minimal-python:latest&lt;/code&gt;
    &lt;p&gt;Replace &lt;code&gt;minimal-python&lt;/code&gt; with any image name. A successful output confirms the image was built by this repository's CI pipeline and hasn't been tampered with.&lt;/p&gt;
    &lt;p&gt;This project is licensed under the MIT License - see the LICENSE file for details.&lt;/p&gt;
    &lt;p&gt;Container images include packages from Wolfi and other sources, each with their own licenses (Apache-2.0, MIT, GPL, LGPL, BSD, etc.). Full license information is included in each image's SBOM:&lt;/p&gt;
    &lt;code&gt;# View package licenses in an image
cosign download sbom ghcr.io/rtvkiz/minimal-python:latest | jq '.packages[].licenseConcluded'&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46840178</guid><pubDate>Sat, 31 Jan 2026 19:58:00 +0000</pubDate></item><item><title>The Saddest Moment (2013) [pdf]</title><link>https://www.usenix.org/system/files/login-logout_1305_mickens.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46840219</guid><pubDate>Sat, 31 Jan 2026 20:02:36 +0000</pubDate></item><item><title>Demystifying ARM SME to Optimize General Matrix Multiplications</title><link>https://arxiv.org/abs/2512.21473</link><description>&lt;doc fingerprint="a49c340ad1b790ae"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Computer Science &amp;gt; Distributed, Parallel, and Cluster Computing&lt;/head&gt;&lt;p&gt; [Submitted on 25 Dec 2025]&lt;/p&gt;&lt;head rend="h1"&gt;Title:Demystifying ARM SME to Optimize General Matrix Multiplications&lt;/head&gt;View PDF HTML (experimental)&lt;quote&gt;Abstract:General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives.&lt;/quote&gt;&lt;head rend="h3"&gt;References &amp;amp; Citations&lt;/head&gt;&lt;p&gt; export BibTeX citation Loading... &lt;/p&gt;&lt;head rend="h1"&gt;Bibliographic and Citation Tools&lt;/head&gt;&lt;p&gt; Bibliographic Explorer (What is the Explorer?) &lt;/p&gt;&lt;p&gt; Connected Papers (What is Connected Papers?) &lt;/p&gt;&lt;p&gt; Litmaps (What is Litmaps?) &lt;/p&gt;&lt;p&gt; scite Smart Citations (What are Smart Citations?) &lt;/p&gt;&lt;head rend="h1"&gt;Code, Data and Media Associated with this Article&lt;/head&gt;&lt;p&gt; alphaXiv (What is alphaXiv?) &lt;/p&gt;&lt;p&gt; CatalyzeX Code Finder for Papers (What is CatalyzeX?) &lt;/p&gt;&lt;p&gt; DagsHub (What is DagsHub?) &lt;/p&gt;&lt;p&gt; Gotit.pub (What is GotitPub?) &lt;/p&gt;&lt;p&gt; Hugging Face (What is Huggingface?) &lt;/p&gt;&lt;p&gt; Papers with Code (What is Papers with Code?) &lt;/p&gt;&lt;p&gt; ScienceCast (What is ScienceCast?) &lt;/p&gt;&lt;head rend="h1"&gt;Demos&lt;/head&gt;&lt;head rend="h1"&gt;Recommenders and Search Tools&lt;/head&gt;&lt;p&gt; Influence Flower (What are Influence Flowers?) &lt;/p&gt;&lt;p&gt; CORE Recommender (What is CORE?) &lt;/p&gt;&lt;head rend="h1"&gt;arXivLabs: experimental projects with community collaborators&lt;/head&gt;&lt;p&gt;arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.&lt;/p&gt;&lt;p&gt;Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.&lt;/p&gt;&lt;p&gt;Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46840252</guid><pubDate>Sat, 31 Jan 2026 20:05:22 +0000</pubDate></item></channel></rss>