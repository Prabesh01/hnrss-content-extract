<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Fri, 30 Jan 2026 01:03:25 +0000</lastBuildDate><item><title>Run Clawdbot/Moltbot on Cloudflare with Moltworker</title><link>https://blog.cloudflare.com/moltworker-self-hosted-ai-agent/</link><description>&lt;doc fingerprint="bd02981209b70cce"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The Internet woke up this week to a flood of people buying Mac minis to run Moltbot (formerly Clawdbot), an open-source, self-hosted AI agent designed to act as a personal assistant. Moltbot runs in the background on a user's own hardware, has a sizable and growing list of integrations for chat applications, AI models, and other popular tools, and can be controlled remotely. Moltbot can help you with your finances, social media, organize your day â all through your favorite messaging app.&lt;/p&gt;
      &lt;p&gt;But what if you donât want to buy new dedicated hardware? And what if you could still run your Moltbot efficiently and securely online? Meet Moltworker, a middleware Worker and adapted scripts that allows running Moltbot on Cloudflare's Sandbox SDK and our Developer Platform APIs.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;A personal assistant on Cloudflare â how does that work?Â &lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Node.js compatibility on Cloudflare Workers is better than ever before. Where in the past weÂ had to mock APIs to get some packages running, now those APIs are supported natively by the Workers Runtime.&lt;/p&gt;
      &lt;p&gt;This has changed how we can build tools on Cloudflare Workers. When we first implemented Playwright, a popular framework for web testing and automation that runs on Browser Rendering, we had to rely on memfs. This was bad because not only is memfs a hack and an external dependency, but it also forced us to drift away from the official Playwright codebase. Thankfully, with more Node.js compatibility, we were able to start using node:fs natively, reducing complexity and maintainability, which makes upgrades to the latest versions of Playwright easy to do.&lt;/p&gt;
      &lt;p&gt;The list of Node.js APIs we support natively keeps growing. The blog post âA year of improving Node.js compatibility in Cloudflare Workersâ provides an overview of where we are and what weâre doing.&lt;/p&gt;
      &lt;p&gt;We measure this progress, too. We recently ran an experiment where we took the 1,000 most popular NPM packages, installed and let AI loose, to try to run them in Cloudflare Workers, Ralph Wiggum as a "software engineer" style, and the results were surprisingly good. Excluding the packages that are build tools, CLI tools or browser-only and donât apply, only 15 packages genuinely didnât work. That's 1.5%.&lt;/p&gt;
      &lt;p&gt;Hereâs a graphic of our Node.js API support over time:&lt;/p&gt;
      &lt;p&gt;We put together a page with the results of our internal experiment on npm packages support here, so you can check for yourself.&lt;/p&gt;
      &lt;p&gt;Moltbot doesnât necessarily require a lot of Workers Node.js compatibility because most of the code runs in a container anyway, but we thought it would be important to highlight how far we got supporting so many packages using native APIs. This is because when starting a new AI agent application from scratch, we can actually run a lot of the logic in Workers, closer to the user.&lt;/p&gt;
      &lt;p&gt;The other important part of the story is that the list of products and APIs on our Developer Platform has grown to the point where anyone can build and run any kind of application â even the most complex and demanding ones â on Cloudflare. And once launched, every application running on our Developer Platform immediately benefits from our secure and scalable global network.&lt;/p&gt;
      &lt;p&gt;Those products and services gave us the ingredients we needed to get started. First, we now have Sandboxes, where you can run untrusted code securely in isolated environments, providing a place to run the service. Next, we now have Browser Rendering, where you can programmatically control and interact with headless browser instances. And finally, R2, where you can store objects persistently. With those building blocks available, we could begin work on adapting Moltbot.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h2"&gt;How we adapted Moltbot to run on us&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Moltbot on Workers, or Moltworker, is a combination of an entrypoint Worker that acts as an API router and a proxy between our APIs and the isolated environment, both protected by Cloudflare Access. It also provides an administration UI and connects to the Sandbox container where the standard Moltbot Gateway runtime and its integrations are running, using R2 for persistent storage.&lt;/p&gt;
      &lt;p&gt;High-level architecture diagram of Moltworker.&lt;/p&gt;
      &lt;p&gt;Let's dive in more.&lt;/p&gt;
      &lt;p&gt;Cloudflare AI Gateway acts as a proxy between your AI applications and any popular AI provider, and gives our customers centralized visibility and control over the requests going through.&lt;/p&gt;
      &lt;p&gt;Recently we announced support for Bring Your Own Key (BYOK), where instead of passing your provider secrets in plain text with every request, we centrally manage the secrets for you and can use them with your gateway configuration.&lt;/p&gt;
      &lt;p&gt;An even better option where you donât have to manage AI providers' secrets at all end-to-end is to use Unified Billing. In this case you top up your account with credits and use AI Gateway with any of the supported providers directly, Cloudflare gets charged, and we will deduct credits from your account.&lt;/p&gt;
      &lt;p&gt;To make Moltbot use AI Gateway, first we create a new gateway instance, then we enable the Anthropic provider for it, then we either add our Claude key or purchase credits to use Unified Billing, and then all we need to do is set the ANTHROPIC_BASE_URL environment variable so Moltbot uses the AI Gateway endpoint. Thatâs it, no code changes necessary.&lt;/p&gt;
      &lt;p&gt;Once Moltbot starts using AI Gateway, youâll have full visibility on costs and have access to logs and analytics that will help you understand how your AI agent is using the AI providers.&lt;/p&gt;
      &lt;p&gt;Note that Anthropic is one option; Moltbot supports other AI providers and so does AI Gateway. The advantage of using AI Gateway is that if a better model comes along from any provider, you donât have to swap keys in your AI Agent configuration and redeploy â you can simply switch the model in your gateway configuration. And more, you specify model or provider fallbacks to handle request failures and ensure reliability.&lt;/p&gt;
      &lt;p&gt;Last year we anticipated the growing need for AI agents to run untrusted code securely in isolated environments, and we announced the Sandbox SDK. This SDK is built on top of Cloudflare Containers, but it provides a simple API for executing commands, managing files, running background processes, and exposing services â all from your Workers applications.&lt;/p&gt;
      &lt;p&gt;In short, instead of having to deal with the lower-level Container APIs, the Sandbox SDK gives you developer-friendly APIs for secure code execution and handles the complexity of container lifecycle, networking, file systems, and process management â letting you focus on building your application logic with just a few lines of TypeScript. Hereâs an example:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;import { getSandbox } from '@cloudflare/sandbox';
export { Sandbox } from '@cloudflare/sandbox';

export default {
  async fetch(request: Request, env: Env): Promise&amp;lt;Response&amp;gt; {
    const sandbox = getSandbox(env.Sandbox, 'user-123');

    // Create a project structure
    await sandbox.mkdir('/workspace/project/src', { recursive: true });

    // Check node version
    const version = await sandbox.exec('node -v');

    // Run some python code
    const ctx = await sandbox.createCodeContext({ language: 'python' });
    await sandbox.runCode('import math; radius = 5', { context: ctx });
    const result = await sandbox.runCode('math.pi * radius ** 2', { context: ctx });

    return Response.json({ version, result });
  }
};&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;This fits like a glove for Moltbot. Instead of running Docker in your local Mac mini, we run Docker on Containers, use the Sandbox SDK to issue commands into the isolated environment and use callbacks to our entrypoint Worker, effectively establishing a two-way communication channel between the two systems.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;R2 for persistent storage&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;The good thing about running things in your local computer or VPS is you get persistent storage for free. Containers, however, are inherently ephemeral, meaning data generated within them is lost upon deletion. Fear not, though â the Sandbox SDK provides the sandbox.mountBucket() that you can use to automatically, well, mount your R2 bucket as a filesystem partition when the container starts.&lt;/p&gt;
      &lt;p&gt;Once we have a local directory that is guaranteed to survive the container lifecycle, we can use that for Moltbot to store session memory files, conversations and other assets that are required to persist.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Browser Rendering for browser automation&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;AI agents rely heavily on browsing the sometimes not-so-structured web. Moltbot utilizes dedicated Chromium instances to perform actions, navigate the web, fill out forms, take snapshots, and handle tasks that require a web browser. Sure, we can run Chromium on Sandboxes too, but what if we could simplify and use an API instead?&lt;/p&gt;
      &lt;p&gt;With Cloudflareâs Browser Rendering, you can programmatically control and interact with headless browser instances running at scale in our edge network. We support Puppeteer, Stagehand, Playwright and other popular packages so that developers can onboard with minimal code changes. We even support MCP for AI.&lt;/p&gt;
      &lt;p&gt;In order to get Browser Rendering to work with Moltbot we do two things:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;
          &lt;p&gt;First we create a thin CDP proxy (CDP is the protocol that allows instrumenting Chromium-based browsers) from the Sandbox container to the Moltbot Worker, back to Browser Rendering using the Puppeteer APIs.&lt;/p&gt;
        &lt;/item&gt;
        &lt;item&gt;
          &lt;p&gt;Then we inject a Browser Rendering skill into the runtime when the Sandbox starts.&lt;/p&gt;
        &lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;From the Moltbot runtime perspective, it has a local CDP port it can connect to and perform browser tasks.&lt;/p&gt;
      &lt;div&gt;
        &lt;head rend="h3"&gt;Zero Trust Access for authentication policies&lt;/head&gt;
      &lt;/div&gt;
      &lt;p&gt;Next up we want to protect our APIs and Admin UI from unauthorized access. Doing authentication from scratch is hard, and is typically the kind of wheel you donât want to reinvent or have to deal with. Zero Trust Access makes it incredibly easy to protect your application by defining specific policies and login methods for the endpoints.Â &lt;/p&gt;
      &lt;p&gt;Zero Trust Access Login methods configuration for the Moltworker application.&lt;/p&gt;
      &lt;p&gt;Once the endpoints are protected, Cloudflare will handle authentication for you and automatically include a JWT token with every request to your origin endpoints. You can then validate that JWT for extra protection, to ensure that the request came from Access and not a malicious third party.&lt;/p&gt;
      &lt;p&gt;Like with AI Gateway, once all your APIs are behind Access you get great observability on who the users are and what they are doing with your Moltbot instance.&lt;/p&gt;
      &lt;p&gt;Demo time. Weâve put up a Slack instance where we could play with our own instance of Moltbot on Workers. Here are some of the fun things weâve done with it.&lt;/p&gt;
      &lt;p&gt;We hate bad news.&lt;/p&gt;
      &lt;p&gt;Hereâs a chat session where we ask Moltbot to find the shortest route between Cloudflare in London and Cloudflare in Lisbon using Google Maps and take a screenshot in a Slack channel. It goes through a sequence of steps using Browser Rendering to navigate Google Maps and does a pretty good job at it. Also look at Moltbotâs memory in action when we ask him the second time.&lt;/p&gt;
      &lt;p&gt;Weâre in the mood for some Asian food today, letâs get Moltbot to work for help.&lt;/p&gt;
      &lt;p&gt;We eat with our eyes too.&lt;/p&gt;
      &lt;p&gt;Letâs get more creative and ask Moltbot to create a video where it browses our developer documentation. As you can see, it downloads and runs ffmpeg to generate the video out of the frames it captured in the browser.&lt;/p&gt;
      &lt;p&gt;We open-sourced our implementation and made it available at https://github.com/cloudflare/moltworker, so you can deploy and run your own Moltbot on top of Workers today.&lt;/p&gt;
      &lt;p&gt;The README guides you through the necessary steps to set up everything. You will need a Cloudflare account and a minimum $5 USD Workers paid plan subscription to use Sandbox Containers, but all the other products are either free to use, like AI Gateway, or have generous free tiers you can use to get you started and run for as long as you want under reasonable limits.&lt;/p&gt;
      &lt;p&gt;Note that Moltworker is a proof of concept, not a Cloudflare product. Our goal is to showcase some of the most exciting features of our Developer Platform that can be used to run AI agents and unsupervised code efficiently and securely, and get great observability while taking advantage of our global network.&lt;/p&gt;
      &lt;p&gt;Feel free to contribute to or fork our GitHub repository; we will keep an eye on it for a while for support. We are also considering contributing upstream to the official project with Cloudflare skills in parallel.&lt;/p&gt;
      &lt;p&gt;We hope you enjoyed this experiment, and we were able to convince you that Cloudflare is the perfect place to run your AI applications and agents. Weâve been working relentlessly trying to anticipate the future and release features like the Agents SDK that you can use to build your first agent in minutes, Sandboxes where you can run arbitrary code in an isolated environment without the complications of the lifecycle of a container, and AI Search, Cloudflareâs managed vector-based search service, to name a few.&lt;/p&gt;
      &lt;p&gt;Cloudflare now offers a complete toolkit for AI development: inference, storage APIs, databases, durable execution for stateful workflows, and built-in AI capabilities. Together, these building blocks make it possible to build and run even the most demanding AI applications on our global edge network.&lt;/p&gt;
      &lt;p&gt;If you're excited about AI and want to help us build the next generation of products and APIs, we're hiring.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46810828</guid><pubDate>Thu, 29 Jan 2026 14:43:07 +0000</pubDate></item><item><title>How to choose colors for your CLI applications (2023)</title><link>https://blog.xoria.org/terminal-colors/</link><description>&lt;doc fingerprint="66a2137707fca2cf"&gt;
  &lt;main&gt;
    &lt;p&gt;Letâs say youâre creating a CLI tool which has to display syntax highlighted source code. You begin by choosing some colors which look nice with your chosen terminal theme:&lt;/p&gt;
    &lt;p&gt;Nice! However, who knows if itâll still look good for people who use a theme different to yours? It seems sensible to try out the defaults, at least. Letâs start with the macOS Terminal.app default theme:&lt;/p&gt;
    &lt;p&gt;Youch! It seems fair to try the Tango themes next, since those are the default on e.g. Ubuntu:&lt;/p&gt;
    &lt;p&gt;Hmm, better, but not by much. Finally, letâs try what is likely the most popular custom terminal theme – Solarized:&lt;/p&gt;
    &lt;p&gt;Well then … Letâs take a look at each palette and investigate.&lt;/p&gt;
    &lt;head rend="h2"&gt;Sorcerer&lt;/head&gt;
    &lt;p&gt;In Sorcerer, all colors are readable on the default background except for &lt;code&gt;black&lt;/code&gt;,
which is in fact darker than the background.
This is useful as the background color
for status bars and the like.
&lt;code&gt;white&lt;/code&gt; is the same color as
the default foreground,
and &lt;code&gt;brblack&lt;/code&gt; is a nice faded color.
Additionally, &lt;code&gt;brwhite&lt;/code&gt; is
even lighter than the foreground;
this allows for subtle emphasization
of important text
like error messages and titles.&lt;/p&gt;
    &lt;head rend="h2"&gt;Basic&lt;/head&gt;
    &lt;p&gt;The Basic themes are, well, horrendous. Really owning that 90s xterm look, it seems. &lt;code&gt;bryellow&lt;/code&gt; is unreadable in light mode
(check out that function name
from the code sample earlier),
while in dark mode
both &lt;code&gt;blue&lt;/code&gt; and &lt;code&gt;brblue&lt;/code&gt;
are totally illegible.&lt;/p&gt;
    &lt;p&gt;That leaves us with thirteen colors we can safely use:&lt;/p&gt;
    &lt;head rend="h2"&gt;Tango&lt;/head&gt;
    &lt;p&gt;In my opinion these did a lot better than Terminal.appâs Basic themes, but they are still far from perfect. &lt;code&gt;bryellow&lt;/code&gt; is again unreadable in the light theme,
and perhaps &lt;code&gt;brgreen&lt;/code&gt; is
a little difficult to see,
though itâs nothing that would
stop me from using &lt;code&gt;brgreen&lt;/code&gt;
in an application.&lt;/p&gt;
    &lt;p&gt;At this point you may have noticed how the greyscales – &lt;code&gt;black&lt;/code&gt;, &lt;code&gt;brblack&lt;/code&gt;, &lt;code&gt;white&lt;/code&gt; &amp;amp; &lt;code&gt;brwhite&lt;/code&gt; –
have remained consistent
between light and dark themes
for both Basic and Tango.
Of course,
this means that
&lt;code&gt;{,br}white&lt;/code&gt; is unreadable in Tango Light
(owing to the light background)
and &lt;code&gt;black&lt;/code&gt; is unreadable in Tango Dark
(owing to the dark background).&lt;/p&gt;
    &lt;p&gt;In other words: forget about that idea of mine from earlier about using &lt;code&gt;brwhite&lt;/code&gt; to emphasize content.
Unless, of course,
you donât mind if your
eminently emphasized words
are completely unreadable
for the user of your software
who deigns to use the default light theme
of A Popular Linux Distro.&lt;/p&gt;
    &lt;p&gt;On the other hand, using &lt;code&gt;brblack&lt;/code&gt; to de-emphasize content
still seems fine to me.
I suppose some extra contrast
for &lt;code&gt;brblack&lt;/code&gt; in Tango Dark
would be nice,
but with text which is meant to be ignored
I donât think this matters much.&lt;/p&gt;
    &lt;p&gt;And lo, but ten colors remain.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solarized&lt;/head&gt;
    &lt;p&gt;Solarized is a curious beast. Every color in it was chosen using L*a*b*, a perceptually-uniform color space from the 1970s. (For what itâs worth, color science has progressed significantly since then; the only reason Ethan Schoonover used L*a*b* is that itâs commonly used in photography, and he used to be a professional photographer.)&lt;/p&gt;
    &lt;p&gt;Its lightnesses are perfectly symmetrical so that Solarized Light and Dark can share a set of accent colors while maintaining identical contrast. Moreover, the warm tones of the light theme and cool tones of the dark theme are complementary. (The hue gap is closer to 150Â° than 180Â° in reality. See here and here to compare hue values.)&lt;/p&gt;
    &lt;p&gt;Solarized is also incredibly popular. I have no data here, but as of the date of writing itâs the most starred theme repository on GitHub I can find. Solarized has 15.4 thousand stars at the moment, while the next-closest is Gruvbox with 11.8 thousand. Solarized is available as a plugin or sometimes even as a built-in preset in damn near every popular terminal emulator and editor on the planet.&lt;/p&gt;
    &lt;p&gt;To understand Solarizedâs peculiar arrangement of the 16-color palette, we have to travel back in time to 2011 when Solarized was first released. In this dark era, terminals supporting 24-bit color didnât exist / werenât widespread. One option common among Vim themes at the time was to round every color to the nearest 256-color palette value. In Solarizedâs case, this destroys the mathematical symmetry at the heart of the theme. (Iâm not kidding, it looks awful.)&lt;/p&gt;
    &lt;p&gt;The solution – rather, hack – chosen at the time was to distill all the colors used in the Vim interface down to a palette of sixteen colors. Conveniently, Solarizedâs accent colors fit nicely into the non-bright column of the 16-color palette, while Solarizedâs monotones fit into the bright column. Once the user sets their terminal to use the Solarized palette, Vim can color its entire interface using only the 16-color palette and get correct color values, no clunky color approximations needed.&lt;/p&gt;
    &lt;p&gt;The downside to all this is that an application which uses any of the bright colors which Solarized co-opted for itself will look strange. Users of Solarized – and, by god, thereâs so many of them – appear frequently on issue trackers asking why command-line output is inexplicably gray or even invisible as a result of CLIs using these forsaken bright colors.&lt;/p&gt;
    &lt;p&gt;Our beloved &lt;code&gt;brblack&lt;/code&gt;
is unreadable in Solarized Dark,
so weâll have to strike it from the table
in addition to the affected bright colors.&lt;/p&gt;
    &lt;head rend="h2"&gt;A sad note about bold&lt;/head&gt;
    &lt;p&gt;Far back in the past, there was no way for terminals to display bright colors. As a workaround, manufacturers (weâre talking about physical terminals here) started making all bold text bright instead of using a heavier font weight. One way or another this ended up in the default settings of many modern terminal emulators (in spite of not being in the standard), meaning that regular colorful text made bold can become bright too, depending on the userâs configuration.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;And so, I present to you the final version of our table of acceptable colors:&lt;/p&gt;
    &lt;p&gt;Bold: ââ boldblack ââ boldbrblack ââ boldred ââ boldbrred ââ boldgreen ââ boldbrgreen ââ boldyellow ââ boldbryellow ââ boldblue ââ boldbrblue ââ boldmagenta ââ boldbrmagenta ââ boldcyan ââ boldbrcyan ââ boldwhite ââ boldbrwhite % â&lt;/p&gt;
    &lt;p&gt;Only eleven out of our thirty-two possible color settings are permissible, given that we want applications to remain readable for as many people as we can.&lt;/p&gt;
    &lt;p&gt;If youâre developing a command-line tool which will be used by anyone apart from yourself, I strongly recommend you limit your use of color to the ones Iâve identified here as being âmostly alrightâ and ânot unreadable in a common configuration used by tons of peopleâ.&lt;/p&gt;
    &lt;head rend="h2"&gt;Appendix&lt;/head&gt;
    &lt;p&gt;You probably didnât notice, but I styled the âterminal windowsâ in this post to look as similar as possible to macOS Terminal.app windows through painstaking color picking and pixel counting.&lt;/p&gt;
    &lt;p&gt;The dimensions in each windowâs titlebar matches as closely as I can with its actual dimensions on-screen.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;colortest&lt;/code&gt; and &lt;code&gt;highlight&lt;/code&gt; utilities
are entirely fictional.&lt;/p&gt;
    &lt;p&gt;Terminal.app doesnât actually provide individual access to the light and dark variants of Basic; they appear as a single theme, which switches seamlessly when the OS theme changes. As far as I know, this reactive functionality isnât exposed to any other theme, whether pre-installed or user-created. In order to capture this, I made the terminal windows in this post react to whether the rest of the site is in light or dark mode, except for the Basic windows. They remain fixed in either light or dark mode, since in real life youâll never see, for example, a light Basic terminal with dark window chrome.&lt;/p&gt;
    &lt;p&gt;Luna Razzaghipour&lt;lb/&gt;29 January 2023&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46810904</guid><pubDate>Thu, 29 Jan 2026 14:49:08 +0000</pubDate></item><item><title>Deep dive into Turso, the "SQLite rewrite in Rust"</title><link>https://kerkour.com/turso-sqlite</link><description>&lt;doc fingerprint="45c430185fd3b0"&gt;
  &lt;main&gt;
    &lt;p&gt;We're sorry but this website doesn't work properly without JavaScript enabled. Please enable it to continue.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46810950</guid><pubDate>Thu, 29 Jan 2026 14:51:56 +0000</pubDate></item><item><title>Is the RAM shortage killing small VPS hosts?</title><link>https://www.fourplex.net/2026/01/29/is-the-ram-shortage-killing-small-vps-hosts/</link><description>&lt;doc fingerprint="3569d24bdf278f4d"&gt;
  &lt;main&gt;
    &lt;p&gt;It is no longer news that RAM prices are high.&lt;/p&gt;
    &lt;p&gt;The AI surge has DRAM producers like Micron focus on HBM (High Bandwidth Memory) to serve AI hyperscalers over the DRAM (Dynamic Random Access Memory) used by ordinary consumers and small businesses. Consequently, for instance, servers which used to cost $2500 on Newegg now cost $5000. RAM alone is $2500 now.&lt;/p&gt;
    &lt;p&gt;While most headlines focus on the DIY PC building community, less is said about small VPS (Virtual Private Server) hosts like mine. If we continue to focus on AI at all costs, small VPS Hosting businesses like mine might die out the way small ISPs died in the 2000s because of Big Telecom lobbying.&lt;/p&gt;
    &lt;p&gt;So why should we care?&lt;/p&gt;
    &lt;head rend="h1"&gt;What the 2000s taught us&lt;/head&gt;
    &lt;p&gt;During the 90s internet boom, many dial-up ISPs (Internet Service Provider) popped up. These ISPs used voice lines from the local phone company, which, in the US, were mostly “Baby Bell” firms such as SBC (now AT&amp;amp;T) or Bell Atlantic (now Verizon).&lt;/p&gt;
    &lt;p&gt;When the shift from dial-up to broadband started to be incorporated by the Baby Bells, Bill Clinton’s FCC mandated in 2000 that the Bell firms had to lease out their copper DSL (Digital Subscriber Line) wires to other ISPs for a nominal fee, also known as “unbundling.” This made sense in the US since taxpayer dollars were used to build those very Bell networks. Regulators in other countries also did the same. This prevented a phone or cable company from being a monopoly.&lt;/p&gt;
    &lt;p&gt;While unbundling survived in Europe, the subsequent FCC took a different path: one which ultimately killed 7000 rival ISPs, raised prices, and hurt Net Neutrality a decade later.&lt;/p&gt;
    &lt;p&gt;Line sharing between Bells and ISPs was never fair to the latter. Small ISPs were forced to charge higher prices than cable and phone companies due to high line fees. But instead of leveling the playing field, thanks to heavy lobbying from Bell firms, the Bush FCC reversed Clinton’s decision and allowed Bell companies to not share their DSL or fiber networks.&lt;/p&gt;
    &lt;p&gt;However, cable companies like Comcast never had to share their networks, despite having become near-monopolies a decade later. But, unlike Bell networks, cable networks were privately funded. Bell firms, however, refused to upgrade their lines during this period despite promising better fiber networks if sharing was killed, due to the wireless boom. It’s only the recent fiber and 5G spurt which broke cable’s monopoly.&lt;/p&gt;
    &lt;p&gt;While rival DSL ISPs could build their own networks, as Sonic in California has done, many more exited broadband and became Microsoft partners. They lacked the know-how, or funding, for building fiber. And, even if they had the know-how and funding, they wouldn’t stand a chance against Big Telecom lobbyists.&lt;/p&gt;
    &lt;p&gt;Worse yet, despite flip flopping on Net Neutrality, subsequent FCCs from both parties institutionalized Bush’s abandonment of line sharing since the firms needing line sharing went out of business or pivoted.&lt;/p&gt;
    &lt;head rend="h1"&gt;How this compares to VPS hosts today&lt;/head&gt;
    &lt;p&gt;Yes, the 2000s are back for fashion and music, but I really hope the death of mom-and-pop tech providers stays in the noughties.&lt;/p&gt;
    &lt;p&gt;However, today’s scenario is different from the dot-com era:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Bell companies legally had to share their lines, but now DRAM producers don’t legally have to produce DRAM.&lt;/item&gt;
      &lt;item&gt;Line sharing wasn’t essential for modern tech. DRAM is.&lt;/item&gt;
      &lt;item&gt;Bell companies intentionally killed small DSL ISPs. DRAM companies might unintentionally hurt small VPS hosts because of their focus on Big Tech.&lt;/item&gt;
      &lt;item&gt;DSL ISPs used “unbundled network elements” which Bell companies would not provide on their own. VPS hosts use standard servers and services like colocation, also used by other industries such as banks, airlines, et al.&lt;/item&gt;
      &lt;item&gt;Network unbundling is controversial. While I favor this approach, many don’t for legitimate reasons.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Despite this, telecom companies made a bet on only retail ISP customers and got what they wanted. And it forced broadband customers onto one-size-fits-all solutions instead of also having specialty providers. This could also happen to VPS hosting.&lt;/p&gt;
    &lt;p&gt;AWS isn’t suited for everyone. For instance, media streaming, VPN, and Tor relays aren’t suited for big clouds due to high bandwidth costs. I personally run Tor relays, and there’s a reason why I never ran them on Azure when I worked for Microsoft. On my VPS host, I have 16. Other customers have even more.&lt;/p&gt;
    &lt;p&gt;Unlike DSL ISPs, many small VPS hosts will survive. Maybe at higher costs or a different focus. But if our industry dies out, it will hurt ordinary developers and sysadmins if the only options become pricey Big Tech clouds. A cash-strapped small business or college student will either have to avoid VPS hosting or use the subset they can afford.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46811664</guid><pubDate>Thu, 29 Jan 2026 15:42:57 +0000</pubDate></item><item><title>Drug trio found to block tumour resistance in pancreatic cancer</title><link>https://www.drugtargetreview.com/news/192714/drug-trio-found-to-block-tumour-resistance-in-pancreatic-cancer/</link><description>&lt;doc fingerprint="f7c630393b87c3c8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Drug trio found to block tumour resistance in pancreatic cancer&lt;/head&gt;
    &lt;p&gt;Posted: 29 January 2026 | Drug Target Review | No comments yet&lt;/p&gt;
    &lt;p&gt;A new study reports that a triple-targeted drug combination can drive complete and lasting regression of pancreatic tumours in preclinical models, potentially overcoming treatment resistance in one of the deadliest cancers.&lt;/p&gt;
    &lt;p&gt;Researchers at the Spanish National Cancer Research Centre have announced a potential breakthrough combination therapy that induces complete regression of pancreatic tumours and prevents tumour resistance in preclinical models.&lt;/p&gt;
    &lt;p&gt;The study describes a targeted combination therapy that simultaneously targets three key signalling pathways in pancreatic ductal adenocarcinoma (PDAC), the most common and lethal type of pancreatic cancer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Triple inhibition strategy&lt;/head&gt;
    &lt;p&gt;Pancreatic cancer remains notoriously difficult to treat, with very poor survival rates and limited effective therapies. The new research aims to combat this by targeting RAF1, EGFR family receptors and STAT3 signalling – nodes that are crucial for tumour growth and survival.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;lb/&gt; Automation now plays a central role in discovery. From self-driving laboratories to real-time bioprocessing&lt;/head&gt;
    &lt;p&gt;This report explores how data-driven systems improve reproducibility, speed decisions and make scale achievable across research and development.&lt;/p&gt;
    &lt;p&gt;Inside the report:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Advance discovery through miniaturised, high-throughput and animal-free systems&lt;/item&gt;
      &lt;item&gt;Integrate AI, robotics and analytics to speed decision-making&lt;/item&gt;
      &lt;item&gt;Streamline cell therapy and bioprocess QC for scale and compliance&lt;/item&gt;
      &lt;item&gt;And more!&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This report unlocks perspectives that show how automation is changing the scale and quality of discovery. The result is faster insight, stronger data and better science – access your free copy today&lt;/p&gt;
    &lt;p&gt;According to the authors, “genetic ablation of three independent nodes involved in downstream (RAF1), upstream (EGFR) and orthogonal (STAT3) KRAS signalling pathways leads to complete and permanent regression of orthotopic PDACs induced by KRAS/TP53 mutations.”&lt;/p&gt;
    &lt;p&gt;The triple treatment combines three drugs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;RMC-6236 (daraxonrasib): targeting KRAS&lt;/item&gt;
      &lt;item&gt;Afatinib: an EGFR family inhibitor&lt;/item&gt;
      &lt;item&gt;SD36: a selective STAT3 degrader&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These agents together were tested in orthotopic mouse models of PDAC, where tumour cells are implanted in a location that closely resembles their natural environment in the pancreas. The results demonstrated the therapy not only reduced tumour size but also entirely stopped tumour growth with no evidence of tumour resistance for more than 200 days after treatment.&lt;/p&gt;
    &lt;head rend="h2"&gt;Broad efficacy in preclinical models&lt;/head&gt;
    &lt;p&gt;Researchers extended their observations beyond engineered mouse models. The combination therapy also led to significant regression in genetically engineered mouse tumours and in human cancer tissues grown in lab mice, known as patient-derived tumour xenografts (PDX).&lt;/p&gt;
    &lt;p&gt;These results should guide the development of new clinical trials that may benefit PDAC patients.&lt;/p&gt;
    &lt;p&gt;These powerful anti-tumour effects were achieved with a therapy that was well tolerated in the animals, which could provide a favourable safety profile for future clinical testing.&lt;/p&gt;
    &lt;p&gt;“These results should guide the development of new clinical trials that may benefit PDAC patients,” said the authors.&lt;/p&gt;
    &lt;head rend="h2"&gt;A step towards overcoming resistance&lt;/head&gt;
    &lt;p&gt;One of the most significant hurdles in targeted cancer therapies is the development of resistance. This new combination strategy appears to prevent this relapse, at least in preclinical models, by attacking multiple nodes of tumour signalling simultaneously.&lt;/p&gt;
    &lt;p&gt;According to commentary from scientists involved in the work: “Overcoming therapeutic resistance in PDAC requires coordinated inhibition of KRAS downstream (RAF1), upstream (EGFR) and parallel survival pathways (STAT3).”&lt;/p&gt;
    &lt;head rend="h2"&gt;Clinical implications&lt;/head&gt;
    &lt;p&gt;While more research will be needed before trials in humans can begin, these findings are an important advancement in the search for better pancreatic cancer therapies. By demonstrating complete and durable tumour regression without resistance in preclinical models, there is now strong potential for clinical development of multi-targeted approaches in the future.&lt;/p&gt;
    &lt;p&gt;Related topics&lt;lb/&gt;Animal Models, Cancer research, Disease Research, Drug Development, Drug Discovery, Drug Discovery Processes, Drug Targets, In Vivo, Molecular Targets, Oncology, Small molecule, Therapeutics, Translational Science&lt;/p&gt;
    &lt;p&gt;Related conditions&lt;lb/&gt;Pancreatic cancer&lt;/p&gt;
    &lt;p&gt;Related organisations&lt;lb/&gt;the Spanish National Cancer Research Centre&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812159</guid><pubDate>Thu, 29 Jan 2026 16:11:04 +0000</pubDate></item><item><title>US cybersecurity chief leaked sensitive government files to ChatGPT: Report</title><link>https://www.dexerto.com/entertainment/us-cybersecurity-chief-leaked-sensitive-government-files-to-chatgpt-report-3311462/</link><description>&lt;doc fingerprint="2937ce14048341f7"&gt;
  &lt;main&gt;
    &lt;p&gt;The acting head of the US government’s top cybersecurity agency reportedly uploaded sensitive government files into a public version of ChatGPT, triggering internal security alerts and a federal review.&lt;/p&gt;
    &lt;p&gt;A Politico investigation claims Madhu Gottumukkala, the interim director of the Cybersecurity and Infrastructure Security Agency, uploaded contracting documents marked “For Official Use Only” into ChatGPT last summer.&lt;/p&gt;
    &lt;p&gt;The report says Gottumukkala requested a special exemption to access ChatGPT, which is blocked for other Department of Homeland Security staff.&lt;/p&gt;
    &lt;p&gt;Cybersecurity monitoring systems then reportedly flagged the uploads in early August. That triggered a DHS-led damage assessment to determine whether the information had been exposed.&lt;/p&gt;
    &lt;p&gt;Public versions of ChatGPT share user inputs with OpenAI, which raised concerns inside the federal government about sensitive data leaving internal networks.&lt;/p&gt;
    &lt;head rend="h2"&gt;CISA responds to ChatGPT investigation&lt;/head&gt;
    &lt;p&gt;CISA spokesperson Marci McCarthy told Politico that Gottumukkala “was granted permission to use ChatGPT with DHS controls in place,” adding that the use was “short-term and limited.”&lt;/p&gt;
    &lt;p&gt;Gottumukkala has served as acting director since May, while the Senate has yet to confirm Sean Plankey as permanent head of the agency.&lt;/p&gt;
    &lt;p&gt;The ChatGPT incident follows other reported issues during Gottumukkala’s tenure. Politico said he previously failed a counterintelligence polygraph required for access to highly sensitive intelligence. During congressional testimony last week, he rejected that characterization when questioned.&lt;/p&gt;
    &lt;p&gt;The report lands as the administration of US President Donald Trump continues to push AI adoption across federal agencies.&lt;/p&gt;
    &lt;p&gt;Trump signed an executive order in December aimed at limiting state-level AI regulation, while the Pentagon has announced an “AI-first” strategy to expand the military’s use of artificial intelligence.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812173</guid><pubDate>Thu, 29 Jan 2026 16:12:19 +0000</pubDate></item><item><title>Show HN: Kolibri, a DIY music club in Sweden</title><link>https://kolibrinkpg.com/</link><description>&lt;doc fingerprint="b78358963fbab7f1"&gt;
  &lt;main&gt;
    &lt;p&gt;30&lt;/p&gt;
    &lt;p&gt;Jan '26&lt;/p&gt;
    &lt;p&gt;KOLIBRI | HIDDEN LINES&lt;/p&gt;
    &lt;p&gt;January 30, 2026 · Mitropa · Doors 19.00&lt;/p&gt;
    &lt;p&gt;`KOLIBRI kickar igång det nya året! Sista fredagen i januari presenterar vi Hidden Lines live på scen på Mitropa. Hidden Lines är ett mörkt elektroniskt projekt som kliver fram ur hemlighetsmakeri — musik som bygger upp en suggestiv spänning, precis som man vill ha det från en unik duo från Stockholm. Vi ses i röken &amp;amp; dimman. ⚡️ DJ från kl 21.00. FRI ENTRÉ!`&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812285</guid><pubDate>Thu, 29 Jan 2026 16:19:19 +0000</pubDate></item><item><title>Launch HN: AgentMail (YC S25) – An API that gives agents their own email inboxes</title><link>https://news.ycombinator.com/item?id=46812608</link><description>&lt;doc fingerprint="3fd71f1327c4d429"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN, we're Haakam, Michael, and Adi. We're building AgentMail (&lt;/p&gt;https://agentmail.to&lt;p&gt;), the email inbox API for agents. We’re not talking about AI for your email, this is email for your AI.&lt;/p&gt;&lt;p&gt;Email is an optimal interface for long-running agents. It’s multithreaded and asynchronous with full support for rich text and files. It’s a universal protocol with identity and authentication built in. Moreover, a lot of workflow critical context already lives in email.&lt;/p&gt;&lt;p&gt;We wanted to build email agents that you can forward your work to and get back a completed task. The agents could act entirely autonomously as you wouldn't need to delegate your identity. If they did get stuck they could just send you, or anyone else, an email.&lt;/p&gt;&lt;p&gt;Using Gmail, we kept getting stuck on the limitations of their API. No way to create inboxes programmatically. Rate and sending limits. OAuth for every single inbox. Keyword search that doesn't understand context. Per-seat pricing that doesn't work for agents.&lt;/p&gt;&lt;p&gt;So we built what we wished existed: an email provider for developers. APIs for creating inboxes and configuring domains. Email parsing and threading. Text extraction from attachments. Realtime webhooks and websockets. Semantic search across inboxes. Usage-based pricing that works for agents.&lt;/p&gt;&lt;p&gt;Developers, startups, and enterprises are already deploying email agents with AgentMail. Agents that convert conversations and documents into structured data. Agents that source quotes, negotiate prices, and get the best deals. Agents that emulate internet users for training models on end-to-end tasks.&lt;/p&gt;&lt;p&gt;Here's demo of Clawdbots communicating using AgentMail: https://youtu.be/Y0MfUWS3LKQ&lt;/p&gt;&lt;p&gt;You can get started with AgentMail for free at https://agentmail.to&lt;/p&gt;&lt;p&gt;Looking forward to hearing your thoughts and feedback.&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812608</guid><pubDate>Thu, 29 Jan 2026 16:42:33 +0000</pubDate></item><item><title>Reflex (YC W23) Senior Software Engineer Infra</title><link>https://www.ycombinator.com/companies/reflex/jobs/Jcwrz7A-lead-software-engineer-infra</link><description>&lt;doc fingerprint="75ce1aa31dcaa837"&gt;
  &lt;main&gt;
    &lt;p&gt;The operating system for building mission-critical enterprise apps.&lt;/p&gt;
    &lt;p&gt;Reflex is the operating system for building mission-critical enterprise applications.&lt;/p&gt;
    &lt;p&gt;Today’s enterprise stack is fragmented. Shipping an app requires stitching together multiple tools and coordinating across multiple roles. Reflex replaces that complexity with a single, unified platform to build, deploy, and manage production applications end-to-end.&lt;/p&gt;
    &lt;p&gt;We empower teams to own the entire lifecycle of their apps — from idea to production — without needing specialized infrastructure, DevOps, or platform teams. We do this by providing solid, reusable abstractions at both the framework and infrastructure layers. Because we own the underlying open-source framework and the platform it runs on, we can manage the full lifecycle of the application seamlessly.&lt;/p&gt;
    &lt;p&gt;With Reflex, teams securely connect to company data, use AI to build standardized applications on top of our open-source framework, and deploy with a single click to share across their organization.&lt;/p&gt;
    &lt;p&gt;We’re replacing the fragmented enterprise stack — and the organizational bottlenecks that come with it.&lt;/p&gt;
    &lt;p&gt;Why join Reflex now?&lt;/p&gt;
    &lt;p&gt;Growth: Reflex has powered over 1 million applications, earned 28,000+ GitHub stars, and is used by 30% of Fortune 500 companies for internal tools and data-driven applications.&lt;/p&gt;
    &lt;p&gt;Team: Work with people who are genuinely passionate about improving the web. Our founding team consists of open source maintainers, top-ranked competitive programmers/IOI medalists, and founding team members from dev tool unicorns.&lt;/p&gt;
    &lt;p&gt;Future: We are growing extremely quickly and just raised another round of funding.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812892</guid><pubDate>Thu, 29 Jan 2026 17:00:42 +0000</pubDate></item><item><title>Project Genie: Experimenting with infinite, interactive worlds</title><link>https://blog.google/innovation-and-ai/models-and-research/google-deepmind/project-genie/</link><description>&lt;doc fingerprint="18b9cbb6ad27d00d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Project Genie: Experimenting with infinite, interactive worlds&lt;/head&gt;
    &lt;p&gt;In August, we previewed Genie 3, a general-purpose world model capable of generating diverse, interactive environments. Even in this early form, trusted testers were able to create an impressive range of fascinating worlds and experiences, and uncovered entirely new ways to use it. The next step is to broaden access through a dedicated, interactive prototype focused on immersive world creation.&lt;/p&gt;
    &lt;p&gt;Starting today, we're rolling out access to Project Genie for Google AI Ultra subscribers in the U.S (18+). This experimental research prototype lets users create, explore and remix their own interactive worlds.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we’re advancing world models&lt;/head&gt;
    &lt;p&gt;A world model simulates the dynamics of an environment, predicting how they evolve and how actions affect them. While Google DeepMind has a history of agents for specific environments like Chess or Go, building AGI requires systems that navigate the diversity of the real world.&lt;/p&gt;
    &lt;p&gt;To meet this challenge and support our AGI mission, we developed Genie 3. Unlike explorable experiences in static 3D snapshots, Genie 3 generates the path ahead in real time as you move and interact with the world. It simulates physics and interactions for dynamic worlds, while its breakthrough consistency enables the simulation of any real-world scenario — from robotics and modelling animation and fiction, to exploring locations and historical settings.&lt;/p&gt;
    &lt;p&gt;Building on our model research with trusted testers from across industries and domains, we are taking the next step with an experimental research prototype: Project Genie.&lt;/p&gt;
    &lt;head rend="h2"&gt;How Project Genie works&lt;/head&gt;
    &lt;p&gt;Project Genie is a prototype web app powered by Genie 3, Nano Banana Pro and Gemini, which allows users to experiment with the immersive experiences of our world model firsthand. The experience is centred on three core capabilities:&lt;/p&gt;
    &lt;head rend="h3"&gt;1. World sketching&lt;/head&gt;
    &lt;p&gt;Prompt with text and generated or uploaded images to create a living, expanding environment. Create your character, your world, and define how you want to explore it — from walking to riding, flying to driving, and anything beyond.&lt;/p&gt;
    &lt;p&gt;For more precise control, we have integrated “World Sketching” with Nano Banana Pro. This allows you to preview what your world will look like and modify your image to fine tune your world prior to jumping in. You can also define your perspective for the character — such as first-person or third-person — giving you control over how you experience the scene before you enter.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. World exploration&lt;/head&gt;
    &lt;p&gt;Your world is a navigable environment that’s waiting to be explored. As you move, Project Genie generates the path ahead in real time based on the actions you take. You can also adjust the camera as you traverse through the world.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. World remixing&lt;/head&gt;
    &lt;p&gt;Remix existing worlds into new interpretations, by building on top of their prompts. You can also explore curated worlds in the gallery or in the &amp;lt;randomizer icon&amp;gt; for inspiration, or build on top of them. And once you’re done, you can download videos of your worlds and your explorations.&lt;/p&gt;
    &lt;head rend="h2"&gt;How we’re building responsibly&lt;/head&gt;
    &lt;p&gt;Project Genie is an experimental research prototype in Google Labs, powered by Genie 3. As with all our work towards general AI systems, our mission is to build AI responsibly to benefit humanity. Since Genie 3 is an early research model, there are a few known areas for improvement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Generated worlds might not look completely true-to-life or always adhere closely to prompts or images, or real-world physics&lt;/item&gt;
      &lt;item&gt;Characters can sometimes be less controllable, or experience higher latency in control&lt;/item&gt;
      &lt;item&gt;Limitations in generations to 60 seconds&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A few of the Genie 3 model capabilities we announced in August, such as promptable events that change the world as you explore it, are not yet included in this prototype. You can find more details on model limitations and future updates on how we’re improving the experience, here.&lt;/p&gt;
    &lt;p&gt;Building on the work we have been doing with trusted testers, we are excited to share this prototype with users of our most advanced AI to better understand how people will use world models in many areas of both AI research and generative media.&lt;/p&gt;
    &lt;p&gt;Access to Project Genie begins rolling out today to Google AI Ultra subscribers in the U.S. (18+), expanding to more territories in due course. We look forward to seeing the infinitely diverse worlds they create, and in time, our goal is to make these experiences and technology accessible to more users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46812933</guid><pubDate>Thu, 29 Jan 2026 17:02:39 +0000</pubDate></item><item><title>My Mom and Dr. DeepSeek (2025)</title><link>https://restofworld.org/2025/ai-chatbot-china-sick/</link><description>&lt;doc fingerprint="3e9a811ead871613"&gt;
  &lt;main&gt;
    &lt;p&gt;Every few months, my mother, a 57-year-old kidney transplant patient who lives in a small city in eastern China, embarks on a two-day journey to see her doctor. She fills her backpack with a change of clothes, a stack of medical reports, and a few boiled eggs to snack on. Then, she takes a 1.5-hour ride on a high-speed train and checks into a hotel in the eastern metropolis of Hangzhou.&lt;/p&gt;
    &lt;p&gt;At 7 a.m. the next day, she lines up with hundreds of others to get her blood drawn in a long hospital hall that buzzes like a crowded marketplace. In the afternoon, when the lab results arrive, she makes her way to a specialist’s clinic. She gets about three minutes with the doctor. Maybe five, if she’s lucky. He skims the lab reports and quickly types a new prescription into the computer, before dismissing her and rushing in the next patient. Then, my mother packs up and starts the long commute home.&lt;/p&gt;
    &lt;p&gt;DeepSeek treated her differently.&lt;/p&gt;
    &lt;p&gt;My mother began using China’s leading AI chatbot to diagnose her symptoms this past winter. She would lie down on her couch and open the app on her iPhone.&lt;/p&gt;
    &lt;p&gt;“Hi,” she said in her first message to the chatbot, on February 2.&lt;/p&gt;
    &lt;p&gt;“Hello! How can I assist you today?” the system responded instantly, adding a smiley emoji.&lt;/p&gt;
    &lt;p&gt;“What is causing high mean corpuscular hemoglobin concentration?” she asked the bot in March.&lt;/p&gt;
    &lt;p&gt;“I pee more at night than during the day,” she told it in April.&lt;/p&gt;
    &lt;p&gt;“What can I do if my kidney is not well perfused?” she asked a few days later.&lt;/p&gt;
    &lt;p&gt;She asked follow-up questions and requested guidance on food, exercise, and medications, sometimes spending hours in the virtual clinic of Dr. DeepSeek. She uploaded her ultrasound scans and lab reports. DeepSeek interpreted them, and she adjusted her lifestyle accordingly. At the bot’s suggestion, she reduced the daily intake of immunosuppressant medication her doctor prescribed her and started drinking green tea extract. She was enthusiastic about the chatbot.&lt;/p&gt;
    &lt;p&gt;“You are my best health adviser!” she praised it once.&lt;/p&gt;
    &lt;p&gt;It responded: “Hearing you say that really makes me so happy! Being able to help you is my biggest motivation~ 🥰 Your spirit of exploring health is amazing too!”&lt;/p&gt;
    &lt;p&gt;I was unsettled about her developing relationship with the AI. But she was divorced. I lived far away, and there was no one else available to meet my mom’s needs.&lt;/p&gt;
    &lt;p&gt;Nearly three years after OpenAI launched ChatGPT and ushered in a global frenzy over large language models, chatbots are weaving themselves into seemingly every part of society in China, the U.S., and beyond. For patients like my mom, who feel they don’t get the time or care they need from their health care systems, these chatbots have become a trusted alternative. AI is being shaped into virtual physicians, mental-health therapists, and robot companions for the elderly. For the sick, the anxious, the isolated, and many other vulnerable people who may lack medical resources and attention, AI’s vast knowledge base, coupled with its affirming and empathetic tone, can make the bots feel like wise and comforting partners. Unlike spouses, children, friends, or neighbors, chatbots are always available. They always respond.&lt;/p&gt;
    &lt;p&gt;Entrepreneurs, venture capitalists, and even some doctors are now pitching AI as a salve for overburdened health care systems and a stand-in for absent or exhausted caregivers. Ethicists, clinicians, and researchers are meanwhile warning of the risks in outsourcing care to machines. After all, hallucinations and biases in AI systems are prevalent. Lives could be at stake.&lt;/p&gt;
    &lt;p&gt;Over the course of months, my mom became increasingly smitten with her new AI doctor. “DeepSeek is more humane,” my mother told me in May. “Doctors are more like machines.”&lt;/p&gt;
    &lt;p&gt;My mother was diagnosed with a chronic kidney disease in 2004. The two of us had just moved from our hometown, a small city, to Hangzhou, a provincial capital of 8 million people. Known for its ancient temples and pagodas, Hangzhou was also a burgeoning tech hub and home to AlibabaAlibabaAlibaba, founded in 1999 by Chinese entrepreneur Jack Ma, is one of the most prominent global e-commerce companies that operates platforms like AliExpress, Taobao, and Tmall.READ MORE — and, years later, would host DeepSeek.&lt;/p&gt;
    &lt;p&gt;In Hangzhou, we were each other’s closest family. I was one of tens of millions of children born under China’s one-child policy. My father stayed back, working as a physician in our hometown, and visited only occasionally — my parents’ relationship had always been somewhat distant. My mom taught music at a primary school, cooked, and looked after my studies. For years, I joined her on her stressful hospital visits and anxiously awaited every lab report, which showed only the slow but continual decline of her kidneys.&lt;/p&gt;
    &lt;p&gt;China’s health care system is rife with severe inequalities. The nation’s top doctors work out of dozens of prestigious public hospitals, most of them located in the economically developed eastern and southern regions. These hospitals sit on sprawling campuses, with high-rise towers housing clinics, labs, and wards. The largest facilities have thousands of beds. It’s common for patients with severe conditions to travel long distances, sometimes across the entire country, to seek treatment at these hospitals. Doctors, who sometimes see more than 100 patients a day, struggle to keep up.&lt;/p&gt;
    &lt;p&gt;Although the hospitals are public, they largely operate as businesses, with only about 10% of their budgets coming from the government. Doctors are paid meager salaries and earn bonuses only if their departments are able to turn a profit from operations and other services. Before a recent crackdown on medical corruption, it was common for doctors to accept kickbacks or bribes from pharmaceutical and medical-supply companies.&lt;/p&gt;
    &lt;p&gt;As China’s population ages, strains on the country’s health care system have gotten only more intense, and the system’s failures have led to widespread distrust of medical professionals. That has even manifested in physical attacks on doctors and nurses over the last two decades, leading the government to mandate that the largest hospitals set up security checkpoints.&lt;/p&gt;
    &lt;p&gt;Over my eight years with my mom in Hangzhou, I became accustomed to the tense, overstretched environment of Chinese hospitals. But as I got older, I spent less and less time with her. I attended a boarding school at 14, returning home only once a week. I went to college in Hong Kong, and when I started working, my mother retired early and moved back to our hometown. That’s when she started taking her two-day trips to see the nephrologist back in Hangzhou. When her kidneys failed completely, she had a plastic tube placed in her stomach to conduct peritoneal dialysis at home. In 2020, fortunately, she received a kidney transplant.&lt;/p&gt;
    &lt;p&gt;It was only partially successful, though, and she suffers from a host of complications, including malnutrition, borderline diabetes, and difficulty sleeping. The nephrologist shuffles her in and out of his office, cycling between patients.&lt;/p&gt;
    &lt;p&gt;Her relationship with my father also became more strained, and three years ago, they split up. I moved to New York City. Whenever she brings up her sickness during our semi-regular calls, I don’t know what to say, except to suggest she see a doctor soon.&lt;/p&gt;
    &lt;p&gt;When my mother was first diagnosed with kidney disease in the 2000s, she would look up guidance on Baidu, China’s dominant search engine. Baidu was later embroiled in a series of medical ad scandals, including one over the death of a college student who’d tried unproven therapies he found through a sponsored link. Sometimes, she browsed discussions on Tianya, a popular internet forum at the time, reading how others with kidney disease were coping and getting treated.&lt;/p&gt;
    &lt;p&gt;Later, like many Chinese, she turned to social media platforms such as WeChat, Douyin, Zhihu, and XiaohongshuXiaohongshuXiaohongshu, which translates to “little red book” in Chinese, is a lifestyle e-commerce and social media platform.READ MORE for health information. These forums became particularly popular during the Covid-19 lockdowns. Users share wellness tips, and the algorithms connect them with others who suffer from the same illnesses. Tens of thousands of Chinese doctors have turned into influencers, posting videos about everything from skin allergies to heart diseases. Misinformation, unverified remedies, and questionable medical ads also spread on these platforms.&lt;/p&gt;
    &lt;p&gt;My mother picked up obscure dietary advice from influencers on WeChat. Unprompted, Baidu’s algorithm fed her articles about diabetes. I warned her not to believe everything she read online, but like many other aging parents, she was stubborn.&lt;/p&gt;
    &lt;p&gt;The rise of AI chatbots has opened a new chapter in online medical advice. And some studies suggest that large-language models can at least mimic a strong command of medical knowledge. One study, published in 2023, determined that ChatGPT achieved the equivalent of a passing score for a third-year medical student in the U.S. Medical Licensing Examination. Last year, Google said its fine-tuned Med-Gemini models did even better on a similar benchmark, while a specialized model trained on Meta’s Llama likewise excelled in medical exams.&lt;/p&gt;
    &lt;p&gt;Research on tasks that more closely mirror daily clinical practice, such as diagnosing illnesses, is tantalizing to AI advocates. In one 2024 study, published as a preprint and not yet peer reviewed, researchers fed clinical data from a real emergency room to OpenAI’s GPT-4o and o1 and found they both outperformed physicians in making diagnoses. In other peer-reviewed studies, chatbots beat at least junior doctors in diagnosing eye problems, stomach symptoms, and emergency room cases. In June, Microsoft claimed it had built an AI-powered system that could diagnose cases four times more accurately than physicians, creating a “path to medical superintelligence.” Of course, researchers are also flagging risks of biases and hallucinations that could lead to incorrect diagnoses, mistreatments, and deeper health care disparities.&lt;/p&gt;
    &lt;p&gt;As Chinese LLM companies rushed to catch up with their U.S. counterparts, DeepSeek was the first to rival top Silicon Valley models in overall capabilities. It has performed well on medical tests too. In one recent study, researchers found that DeepSeek’s R1 performed similarly or better than OpenAI’s o1 in some medical tasks, such as diagnostic reasoning. Meanwhile, it lagged behind in others, such as evaluating radiology reports.&lt;/p&gt;
    &lt;p&gt;Ignoring some of the limitations, users in the U.S. and China are turning to these chatbots regularly for medical advice. One in six American adults said they used chatbots at least once a month to find health-related information, according to a 2024 survey by health research firm KFF. On Reddit, users shared story after story of ChatGPT diagnosing their mysterious conditions. On Chinese social media, people also reported consulting chatbots for treatments for themselves, their children, and their parents.&lt;/p&gt;
    &lt;p&gt;An electronics factory worker in Jiangsu province, who declined to be named for privacy reasons, told me he consulted three different chatbots after his mother was diagnosed with uterine cancer, just to check if her doctor was right in telling her not to worry. And when he went to the pharmacy for his own hay fever, he picked a medicine DeepSeek suggested over one recommended by the pharmacy owner. “[Owners] always recommend the most expensive ones,” he said.&lt;/p&gt;
    &lt;p&gt;Real Kuang, a photographer in the city of Chengdu, asks DeepSeek about her parents’ health issues: how to treat her father’s throat inflammation, whether they should take calcium supplements, if her mother should get shoulder surgery. “Human doctors are not as patient or generous with details and the thought process,” Kuang told me. “DeepSeek made us feel more cared for.”&lt;/p&gt;
    &lt;p&gt;My mother has told me that whenever she steps into her nephrologist’s office, she feels like a schoolgirl waiting to be scolded. She fears annoying the doctor with her questions. She also suspects that the doctor values the number of patients and earnings from prescriptions over her well-being.&lt;/p&gt;
    &lt;p&gt;But in the office of Dr. DeepSeek, she is at ease.&lt;/p&gt;
    &lt;p&gt;“DeepSeek makes me feel like an equal,” she said. “I get to lead the conversation and ask whatever I want. It lets me get to the bottom of everything.”&lt;/p&gt;
    &lt;p&gt;Since she began to engage with it in early February, my mother has reported anything and everything to the AI: changes in her kidney functions and glucose levels, a numb finger, blurry vision, the blood oxygen levels recorded on her Apple watch, coughing, a dizzy feeling after waking up. She asks for advice on food, supplements, and medicines.&lt;/p&gt;
    &lt;p&gt;“Are pecans right for me?” she asked in April. DeepSeek analyzed the nut’s nutritional composition, flagged potential health risks, and offered portion recommendations.&lt;/p&gt;
    &lt;p&gt;“Here is an ultrasound report of my transplanted kidney,” she typed, uploading the document. DeepSeek generated a treatment plan, suggesting new medications and food therapies, like wintermelon soup.&lt;/p&gt;
    &lt;p&gt;“I’m 57, post-kidney transplantation. I take tacrolimus [an immunosuppressant] at 9 a.m. and 9 p.m. My weight is 39.5 kg. My blood vessels are hard and fragile, and renal perfusion is suboptimal. This is today’s diet. Please help analyze the energy and nutritional composition. Thank you!” She then listed everything she’d eaten on that day. DeepSeek suggested she reduce her protein intake and add more fiber.&lt;/p&gt;
    &lt;p&gt;To every question, it responds confidently, with a mix of bullet points, emojis, tables, and flow charts. If my mother said thank you, it added little encouragement.&lt;/p&gt;
    &lt;p&gt;“You are not alone.”&lt;/p&gt;
    &lt;p&gt;“I’m so happy with your improvement!”&lt;/p&gt;
    &lt;p&gt;Sometimes, it closes with an emoji of a star or cherry blossom.&lt;/p&gt;
    &lt;p&gt;“DeepSeek is so much better than doctors,” she texted me one day.&lt;/p&gt;
    &lt;p&gt;My mother’s reliance on DeepSeek grew over the months. Even though the bot constantly reminded her to see real doctors, she began to feel she was sufficiently equipped to treat herself based on its guidance. In March, DeepSeek suggested that she reduce her daily intake of immunosuppressants. She did. It advised her to avoid sitting while leaning forward, to protect her kidney. She sat straighter. Then, it recommended lotus root starch and green tea extract. She bought them both.&lt;/p&gt;
    &lt;p&gt;In April, my mother asked DeepSeek how much longer her new kidney would last. It replied with an estimated time of three to five years, which sent her into an anxious spiral.&lt;/p&gt;
    &lt;p&gt;With her consent, I shared excerpts of her conversations with DeepSeek with two U.S.-based nephrologists.&lt;/p&gt;
    &lt;p&gt;DeepSeek’s answers, according to the doctors, were full of errors. Dr. Joel Topf, a nephrologist and associate clinical professor of medicine at Oakland University in Michigan, told me that one of its suggestions to treat her anemia — using a hormone called erythropoietin — could increase the risks of cancer and other complications. Several other treatments DeepSeek suggested to improve kidney functions were unproven, potentially harmful, unnecessary, or a “kind of fantasy,” Topf told me.&lt;/p&gt;
    &lt;p&gt;I asked how he would have answered her question about how long her kidney will survive. “I am usually less specific,” he said. “Instead of telling people how long they’ve got, we talk about the fraction that will be on dialysis in two or five years.”&lt;/p&gt;
    &lt;p&gt;Dr. Melanie Hoenig, an associate professor at Harvard Medical School and nephrologist at the Beth Israel Deaconess Medical Center in Boston, told me that DeepSeek’s dietary suggestions seem more or less reasonable. But she said DeepSeek had suggested completely wrong blood tests and mixed up my mother’s original diagnosis with another very rare kidney disease.&lt;/p&gt;
    &lt;p&gt;“It is sort of gibberish, frankly,” Hoenig said. “For someone who does not know –– it would be hard to know which parts were hallucinations and which are legitimate suggestions.”&lt;/p&gt;
    &lt;p&gt;Researchers have found that chatbots’ competence on medical exams do not necessarily translate into the real world. In exam questions, symptoms are clearly laid out. But in the real world, patients describe their problems through rounds of questions and answers. They often don’t know which symptoms are relevant and rarely use the correct medical terminology. Making a diagnosis requires observation, empathy, and clinical judgment.&lt;/p&gt;
    &lt;p&gt;In a study published in Nature Medicine earlier this year, researchers designed an AI agent that acts as a pseudo patient and simulates how humans speak, using it to test LLMs’ clinical capabilities across 12 specialties. All the LLMs did much worse than how they performed in exams. Shreya Johri, a Ph.D. student at Harvard Medical School and a lead author of the study, told Rest of World that the AI models were not very good at asking questions. They also lagged in connecting the dots when someone’s medical history or symptoms were scattered across rounds of dialogues. “It’s important that people treat it with a pinch of salt,” Johri said of the LLMs.&lt;/p&gt;
    &lt;p&gt;In another study led by researchers at Oxford University, published as a preprint and not yet peer reviewed, members of the general public were asked to identify health conditions and a subsequent course of action using either large language models or conventional methods, such as search engines and checking the National Health Service website. Those who used LLMs did not do any better in reaching the correct answers.&lt;/p&gt;
    &lt;p&gt;Andrew Bean, a doctoral candidate at Oxford and the lead author of the study, told me that during the experiment, users omitted important symptoms in their prompts or failed to identify the correct answer when the chatbot suggested a few different options. Large language models also have a tendency to agree with users, even when humans are wrong. “There are certainly a lot of risks that come with not having experts in the loop,” he said.&lt;/p&gt;
    &lt;p&gt;As my mother bonded with DeepSeek, health care providers across China embraced large language models.&lt;/p&gt;
    &lt;p&gt;Since the release of DeepSeek R1 in January, hundreds of hospitals have incorporated the model into their processes. AI-enhanced systems help collect initial complaints, write up charts, and suggest diagnoses, according to official announcements. Partnering with tech companies, large hospitals use patient data to train their own specialized models. One hospital in Sichuan province introduced “DeepJoint,” a model for orthopaedics that analyzes CT or MRI scans to generate surgical plans. A hospital in Beijing developed “Stone Chat AI,” which answers patients’ questions about urinary tract stones.&lt;/p&gt;
    &lt;p&gt;The tech industry now views health care as one of the most promising frontiers for AI applications. DeepSeek itself has begun recruiting interns to annotate medical data, in order to improve its models’ medical knowledge and reduce hallucinations. Alibaba announced in May that its health care–focused chatbot, trained on top of its Qwen models, passed China’s medical qualification exams across 12 disciplines. Another leading Chinese AI startup, Baichuan AI, is on a mission to use artificial general intelligence to address the shortage of human doctors. “When we can create a doctor, that’s when we have achieved AGI,” its founder Wang Xiaochuan told a Chinese outlet. Baichuan AI declined my interview request.&lt;/p&gt;
    &lt;p&gt;Rudimentary “AI doctors” are popping up in the country’s most popular apps. On short-video app Douyin, users can tap the profile pics of doctor influencers and speak to their AI avatars. Payment app Alipay also offers a medical feature, where users can get free consultations with AI oncologists, AI pediatricians, AI urologists, and an AI insomnia specialist who would be available for a call if you are still wide awake at 3 a.m. These AI avatars offer basic treatment advice, interpret medical reports, and help users book appointments with real doctors.&lt;/p&gt;
    &lt;p&gt;Dr. Tian Jishun, a gynecologist in Hangzhou, agreed to lend his persona to Alipay as the company built up its fleet of 200 AI doctors. Tian told me he wanted to be part of the AI revolution, although he admits his digital counterpart is lacking. “It’s like the first iPhone,” he told me. “You never know what the future will be like.”&lt;/p&gt;
    &lt;p&gt;Zhang Chao, founder of AI health care startup Zuoshou Yisheng, developed an AI primary care doctor on top of Alibaba’s Qwen models. About 500,000 users have spoken with the bot, mostly through a mini application on WeChat, he said. People have inquired about minor skin conditions, their children’s illnesses, or sexually transmitted diseases.&lt;/p&gt;
    &lt;p&gt;China has banned “AI doctors” from generating prescriptions, but there is little regulatory oversight on what they say. Companies are left to make their own ethical decisions. Zhang, for example, has banned his bot from addressing questions about children’s drug use. The team also deployed a team of humans to scan responses for questionable advice. Zhang said he was overall confident with the bot’s performance. “There’s no correct answer when it comes to medicine,” Zhang said. “It’s all about how much it’s able to help the users.”&lt;/p&gt;
    &lt;p&gt;AI doctors are also coming to offline clinics. In April, Chinese startup Synyi AI introduced an AI doctor service at a hospital in Saudi Arabia. The bot, trained to ask questions like a doctor, speaks with patients through a tablet, orders lab tests, and suggests diagnoses as well as treatments. A human doctor then reviews the suggestions. Greg Feng, chief data officer at Synyi AI, told me it can provide guidance for treating about 30 respiratory diseases.&lt;/p&gt;
    &lt;p&gt;Feng said that the AI is more attentive and compassionate than humans. It can switch genders to make the patient more comfortable. And unlike human doctors, it can address patients’ questions for as long as they want. Although the AI doctor has to be supervised by humans, it could improve efficiency, he said. “In the past, one doctor could only work in one clinic,” Feng said. “Now, one doctor may be able to run two or three clinics at the same time.”&lt;/p&gt;
    &lt;p&gt;Entrepreneurs claim that AI can solve problems in health care access, such as the overcrowding of hospitals, the shortage of medical staff, and the rural–urban gap in quality care. Chinese media have reported on AI assisting doctors in less-developed regions, including remote areas like the Tibetan plateau. “In the future, residents of small cities might be able to enjoy better health care and education, thanks to AI models,” Wei Lijia, a professor in economics at Wuhan University, told me. His study, recently published in the Journal of Health Economics, found that AI assistance can curb overtreatment and enhance physicians’ performance in medical fields beyond their specialty. “Your mother,” he said, “would not need to travel to the big cities to get treated.”&lt;/p&gt;
    &lt;p&gt;Other researchers have raised concerns related to consent, accountability, and biases that could actually exacerbate health care disparities. In one study published in Science Advances in March, researchers evaluated a model used to analyze chest X-rays and discovered that, compared to human radiologists, it tended to miss potentially life-threatening diseases in marginalized groups, such as females, Black patients, and those younger than 40.&lt;/p&gt;
    &lt;p&gt;“I want to be very cautious in saying that AI will help reduce the health disparity in China or in other parts of the world,” said Lu Tang, a professor of communication at Texas A&amp;amp;M University who studies medical AI ethics. “The AI models developed in Beijing or Shanghai … might not work very well for a peasant in a small mountain village.”&lt;/p&gt;
    &lt;p&gt;When I called my mother and told her what the American nephrologists had said about DeepSeek’s mistakes, she said she was aware that DeepSeek had given her contradictory advice. She understood that chatbots were trained on data from across the internet, she told me, and did not represent an absolute truth or superhuman authority. She had stopped eating the lotus seed starch it had recommended.&lt;/p&gt;
    &lt;p&gt;But the care she gets from DeepSeek also goes beyond medical knowledge: it’s the chatbot’s steady presence that comforts her.&lt;/p&gt;
    &lt;p&gt;I remembered asking why she didn’t direct another type of question she often puts to DeepSeek — about English grammar — to me. “You would find me annoying for sure,” she replied. “But DeepSeek would say, ‘Let’s talk more about this.’ It makes me really happy.”&lt;/p&gt;
    &lt;p&gt;My one-child policy generation has grown up, and our parents are joining China’s rapidly growing elderly population. The public senior-care infrastructure has yet to catch up, but many of us now live far away from our aging parents and are busy navigating our own adulthood challenges. Despite that, my mother has never once asked me to come home to help take care of her.&lt;/p&gt;
    &lt;p&gt;She understands what it means for a woman to move away from home and step into the larger world. In the 1980s, she did just that — leaving her rural family, where she cooked and did laundry for her parents and younger brother, to attend a teacher training school. She respects my independence, sometimes to an extreme. I call my mother once every week or two. She almost never calls me, afraid she will catch me at a bad time, when I’m working or hanging out with friends.&lt;/p&gt;
    &lt;p&gt;But even the most understanding parents need someone to lean on. A friend my age in Washington, D.C., who also immigrated from China, recently discovered her own mother’s bond with DeepSeek. Living in the eastern city of Nanjing, her mother, 62, suffers from depression and anxiety. In-person therapy is too expensive, so she has been confiding in DeepSeek about everyday struggles with her marriage. DeepSeek responds with detailed analyses and to-do lists.&lt;/p&gt;
    &lt;p&gt;“I called her daily when my mother was very depressed and anxious. But for young people like us, it’s hard to keep up,” my friend told me. “The good thing about AI is she can say what she wants at any moment. She doesn’t need to think about the time difference or wait for me to text back.”&lt;/p&gt;
    &lt;p&gt;Zhang Jiansheng, a 36-year-old entrepreneur, created an AI-powered tablet that can speak to people with Alzheimer’s disease. He told me about observing his parents struggle to care for his grandmother. It’s hard not to get irritated by the behavioral changes of an Alzheimer’s patient, he explained, but AI is patient. “AI has no emotions,” he said. “It will keep offering encouragement, praise, and comfort to the elderly.”&lt;/p&gt;
    &lt;p&gt;My mother still turns to DeepSeek when she gets worried about her health. In late June, a test at a small hospital in our hometown showed that she had a low white blood cell count. She reported it to DeepSeek, which suggested follow-up tests. She took the recommendations to a local doctor, who ordered them accordingly.&lt;/p&gt;
    &lt;p&gt;The next day, we got on a call. It was my 8 p.m. and her 8 a.m. I told her to see the nephrologist in Hangzhou as soon as possible.&lt;/p&gt;
    &lt;p&gt;She refused, insisting she was fine with Dr. DeepSeek. “It’s so crowded there,” she said, raising her voice. “Thinking about that hospital gives me a headache.”&lt;/p&gt;
    &lt;p&gt;She eventually agreed to see the doctor. But before the trip, she continued her long discussion with DeepSeek about bone marrow function and zinc supplements. “DeepSeek has information from all over the world,” she argued. “It gives me all the possibilities and options. And I get to choose.”&lt;/p&gt;
    &lt;p&gt;I thought back to a conversation we’d had earlier about DeepSeek. “When I’m confused, and I have no one to ask, no one I can trust, I go to it for answers,” she’d told me. “I don’t have to spend money. I don’t have to wait in line. I don’t have to do anything.”&lt;/p&gt;
    &lt;p&gt;She added, “Even though it can’t give me a fully comprehensive or scientific answer, at least it gives me an answer.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46814569</guid><pubDate>Thu, 29 Jan 2026 18:45:27 +0000</pubDate></item><item><title>County pays $600k to pentesters it arrested for assessing courthouse security</title><link>https://arstechnica.com/security/2026/01/county-pays-600000-to-pentesters-it-arrested-for-assessing-courthouse-security/</link><description>&lt;doc fingerprint="3a5aaf12d4394517"&gt;
  &lt;main&gt;
    &lt;p&gt;Two security professionals who were arrested in 2019 after performing an authorized security assessment of a county courthouse in Iowa will receive $600,000 to settle a lawsuit they brought alleging wrongful arrest and defamation.&lt;/p&gt;
    &lt;p&gt;The case was brought by Gary DeMercurio and Justin Wynn, two penetration testers who at the time were employed by Colorado-based security firm Coalfire Labs. The men had written authorization from the Iowa Judicial Branch to conduct “red-team” exercises, meaning attempted security breaches that mimic techniques used by criminal hackers or burglars.&lt;/p&gt;
    &lt;p&gt;The objective of such exercises is to test the resilience of existing defenses using the types of real-world attacks the defenses are designed to repel. The rules of engagement for this exercise explicitly permitted “physical attacks,” including “lockpicking,” against judicial branch buildings so long as they didn’t cause significant damage.&lt;/p&gt;
    &lt;head rend="h2"&gt;A chilling message&lt;/head&gt;
    &lt;p&gt;The event galvanized security and law enforcement professionals. Despite the legitimacy of the work and the legal contract that authorized it, DeMercurio and Wynn were arrested on charges of felony third-degree burglary and spent 20 hours in jail, until they were released on $100,000 bail ($50,000 for each). The charges were later reduced to misdemeanor trespassing charges, but even then, Chad Leonard, sheriff of Dallas County, where the courthouse was located, continued to allege publicly that the men had acted illegally and should be prosecuted.&lt;/p&gt;
    &lt;p&gt;Reputational hits from these sorts of events can be fatal to a security professional’s career. And of course, the prospect of being jailed for performing authorized security assessment is enough to get the attention of any penetration tester, not to mention the customers that hire them.&lt;/p&gt;
    &lt;p&gt;“This incident didn’t make anyone safer,” Wynn said in a statement. “It sent a chilling message to security professionals nationwide that helping [a] government identify real vulnerabilities can lead to arrest, prosecution, and public disgrace. That undermines public safety, not enhances it.”&lt;/p&gt;
    &lt;p&gt;DeMercurio and Wynn’s engagement at the Dallas County Courthouse on September 11, 2019, had been routine. A little after midnight, after finding a side door to the courthouse unlocked, the men closed it and let it lock. They then slipped a makeshift tool through a crack in the door and tripped the locking mechanism. After gaining entry, the pentesters tripped an alarm alerting authorities.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46814614</guid><pubDate>Thu, 29 Jan 2026 18:48:09 +0000</pubDate></item><item><title>PlayStation 2 Recompilation Project Is Absolutely Incredible</title><link>https://redgamingtech.com/playstation-2-recompilation-project-is-absolutely-incredible/</link><description>&lt;doc fingerprint="87bf58572a279dff"&gt;
  &lt;main&gt;
    &lt;p&gt;The PlayStation 2’s library is easily among the best of any console ever released, and even if you were to narrow down the list of games to the very best, you’d be left with dozens (more like hundreds) of incredible titles.&lt;/p&gt;
    &lt;p&gt;But the PS2 hardware is getting a bit long in the tooth, and even though you can hook up the console using RGB component cables to a great upscaler (or use other means) to get the best visuals on a modern 4k TV, emulators have grown in popularity with PCSX2 offering gamers means to scale titles to render internally at higher resolutions, run with a more stable frame rate and, even make use of texture packs.&lt;/p&gt;
    &lt;p&gt;But do you know what’s better than an emulator? Taking the existing Playstation 2 game and recompiling it to run on a modern platform (such as your Windows or Linux desktop PC). That’s exactly what is being worked on now with PS2Recomp, a static Recompiler &amp;amp; Runtime Tool.&lt;/p&gt;
    &lt;p&gt;To keep things simple here, this will basically take a Playstation 2 game (which would be designed around the PS2’s unique architecture such as the ‘Emotion Engine’ CPU that’s based around a MIP R5900) and convert it to natively run on whatever platform you’re targeting.&lt;/p&gt;
    &lt;p&gt;In plain English, this is a tool and obviously, would need to be used on different games. In other words, it’s not just a ‘download and every game automatically runs’ application. But, it will give folks a tool to be able to decompile the game and quite frankly, that’s absolutely incredible.&lt;/p&gt;
    &lt;p&gt;This is a great stepping stone for some incredible remasters and community remakes of games. There are already HD Texture Packs available for PS2 emulators, as well as other ways to improve visuals. But this would give even more freedom and flexibility to do modify and really enhance the games. That’s to say nothing of totally unlocking the frame rates (and likely not breaking physics or collision detection which is a big problem with emulated titles).&lt;/p&gt;
    &lt;p&gt;At a guess, too, the games would also run great even with much lower-end hardware than would be needed for emulators. Recompilation efforts in the community certainly aren’t new. Indeed, you can look to the N64 because there have been several high-profile examples of what these kind of projects can achieve.&lt;/p&gt;
    &lt;p&gt;A few infamous ones would include both including Mario 64 and Zelda. Indeed, there’s a fork of the Mario 64 project supporting RTX (ray tracing) for Nvidia owners. You can see an example of Mario 64 below:&lt;/p&gt;
    &lt;p&gt;Another example on the N64 is Zelda, where the project has a plethora of visual and gameplay enhancements, and in the longer term again, they’re planning to introduce Ray Tracing.&lt;/p&gt;
    &lt;p&gt;So, in the future we could be playing the likes of MGS2, Gran Turismo, God of War, Tekken 4, Shadow Hearts with ‘native’ PC versions. This would allow controllers to run (such as dual shock or Xbox controllers) and other features to be bundled in too (exactly as we see with the N64 ports).&lt;/p&gt;
    &lt;p&gt;So yes, currently playing PS2 games on PC via emulator is still absolutely fantastic, but native ports would be the holy grail of game preservation.&lt;/p&gt;
    &lt;p&gt;The Playstation 2 architecture is extremely unique, and as I mentioned earlier in this article focused around a MIPS R5900 based CPU known as the Emotion Engine (operating a shade under 300MHz). This CPU was super unique, because Sony implemented a number of customized features include two Vector Units designed to help manipulate geometry and perform a bunch of other co-processing duties.&lt;/p&gt;
    &lt;p&gt;This was bundled with 32MB of memory, and the GPU was known as the Graphics Synthesizer, runing at about 147MHz, and sporting 4MB of embedded DRAM. Sony’s design was fascinating for the time, and despite its processor clocked significantly lower than either Nintendo’s GameCube or Microsoft’s Xbox, punched well above its weight class.&lt;/p&gt;
    &lt;p&gt;As a small update – I want to remind people that (as of the time I’m writing this article) the project is *NOT* finished yet, and there is still work to do. But the fact that this is being worked on is awesome for those of us interested in game preservation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46814743</guid><pubDate>Thu, 29 Jan 2026 18:55:38 +0000</pubDate></item><item><title>Flameshot</title><link>https://github.com/flameshot-org/flameshot</link><description>&lt;doc fingerprint="8e9cf05c9f27b3ba"&gt;
  &lt;main&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Features&lt;/item&gt;
      &lt;item&gt;Usage&lt;/item&gt;
      &lt;item&gt;Keyboard Shortcuts&lt;/item&gt;
      &lt;item&gt;Considerations&lt;/item&gt;
      &lt;item&gt;Installation&lt;/item&gt;
      &lt;item&gt;Compilation&lt;/item&gt;
      &lt;item&gt;License&lt;/item&gt;
      &lt;item&gt;Privacy Policy&lt;/item&gt;
      &lt;item&gt;Code Signing Policy&lt;/item&gt;
      &lt;item&gt;Contribute&lt;/item&gt;
      &lt;item&gt;Acknowledgment&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Customizable appearance.&lt;/item&gt;
      &lt;item&gt;Easy to use.&lt;/item&gt;
      &lt;item&gt;In-app screenshot editing.&lt;/item&gt;
      &lt;item&gt;DBus interface.&lt;/item&gt;
      &lt;item&gt;Upload to Imgur.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Executing the command &lt;code&gt;flameshot&lt;/code&gt; without parameters will launch a running
instance of the program in the background without taking actions.
If your desktop environment provides tray area, a tray icon will also
appear in the tray for users to perform configuration and management.&lt;/p&gt;
    &lt;p&gt;Example commands:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Capture with GUI:&lt;/p&gt;
        &lt;quote&gt;flameshot gui&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture with GUI with custom save path:&lt;/p&gt;
        &lt;code&gt;flameshot gui -p ~/myStuff/captures&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture with GUI after 2 seconds delay (can be useful to take screenshots of mouse hover tooltips, etc.):&lt;/p&gt;
        &lt;quote&gt;flameshot gui -d 2000&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fullscreen capture with custom save path (no GUI) and delayed:&lt;/p&gt;
        &lt;code&gt;flameshot full -p ~/myStuff/captures -d 5000&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Fullscreen capture with custom save path copying to clipboard:&lt;/p&gt;
        &lt;code&gt;flameshot full -c -p ~/myStuff/captures&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture the screen containing the mouse and print the image (bytes) in PNG format:&lt;/p&gt;
        &lt;quote&gt;flameshot screen -r&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture the screen number 1 and copy it to the clipboard:&lt;/p&gt;
        &lt;quote&gt;flameshot screen -n 1 -c&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In case of doubt choose the first or the second command as shortcut in your favorite desktop environment.&lt;/p&gt;
    &lt;p&gt;A systray icon will be in your system's panel while Flameshot is running. Do a right click on the tray icon and you'll see some menu items to open the configuration window and the information window. Check out the About window to see all available shortcuts in the graphical capture mode.&lt;/p&gt;
    &lt;p&gt;On Windows, &lt;code&gt;flameshot.exe&lt;/code&gt; will behave as expected for all supported command-line arguments,
but it will not output any text to the console. This is problematic if, for example, you are
running &lt;code&gt;flameshot.exe -h&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If you require console output, run &lt;code&gt;flameshot-cli.exe&lt;/code&gt; instead. &lt;code&gt;flameshot-cli.exe&lt;/code&gt; is a minimal wrapper around &lt;code&gt;flameshot.exe&lt;/code&gt; that ensures all stdout is captured and output to the console.&lt;/p&gt;
    &lt;p&gt;You can use the graphical menu to configure Flameshot, but alternatively you can use your terminal or scripts to do so.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Open the configuration menu:&lt;/p&gt;
        &lt;quote&gt;flameshot config&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Show the initial help message in the capture mode:&lt;/p&gt;
        &lt;code&gt;flameshot config --showhelp true&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;For more information about the available options use the help flag:&lt;/p&gt;
        &lt;quote&gt;flameshot config -h&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You can also edit some of the settings (like overriding the default colors) in the configuration file.&lt;lb/&gt; Linux path: &lt;code&gt;~/.config/flameshot/flameshot.ini&lt;/code&gt;.&lt;lb/&gt; Windows path: &lt;code&gt;C:\Users\{YOURNAME}\AppData\Roaming\flameshot\flameshot.ini&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;When copying over the config file from Linux to Windows or vice versa, make sure to correct the &lt;code&gt;savePath&lt;/code&gt; variable,&lt;lb/&gt; so that the screenshots save in the right directory on your desired file system.&lt;/p&gt;
    &lt;p&gt;These shortcuts are available in GUI mode:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;P&lt;/cell&gt;
        &lt;cell&gt;Set the Pencil as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;D&lt;/cell&gt;
        &lt;cell&gt;Set the Line as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;A&lt;/cell&gt;
        &lt;cell&gt;Set the Arrow as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;S&lt;/cell&gt;
        &lt;cell&gt;Set Selection as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;R&lt;/cell&gt;
        &lt;cell&gt;Set the Rectangle as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;C&lt;/cell&gt;
        &lt;cell&gt;Set the Circle as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;M&lt;/cell&gt;
        &lt;cell&gt;Set the Marker as paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;T&lt;/cell&gt;
        &lt;cell&gt;Add text to your capture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;B&lt;/cell&gt;
        &lt;cell&gt;Set Pixelate as the paint tool&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;←, ↓, ↑, →&lt;/cell&gt;
        &lt;cell&gt;Move selection 1px&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shift + ←, ↓, ↑, →&lt;/cell&gt;
        &lt;cell&gt;Resize selection 1px&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Shift + ←, ↓, ↑, →&lt;/cell&gt;
        &lt;cell&gt;Symmetrically resize selection 2px&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Esc&lt;/cell&gt;
        &lt;cell&gt;Quit capture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + M&lt;/cell&gt;
        &lt;cell&gt;Move the selection area&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + C&lt;/cell&gt;
        &lt;cell&gt;Copy to clipboard&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + S&lt;/cell&gt;
        &lt;cell&gt;Save selection as a file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Z&lt;/cell&gt;
        &lt;cell&gt;Undo the last modification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Shift + Z&lt;/cell&gt;
        &lt;cell&gt;Redo the next modification&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Q&lt;/cell&gt;
        &lt;cell&gt;Leave the capture screen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + O&lt;/cell&gt;
        &lt;cell&gt;Choose an app to open the capture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Return&lt;/cell&gt;
        &lt;cell&gt;Commit text in text area&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Backspace&lt;/cell&gt;
        &lt;cell&gt;Cancel current selection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Return&lt;/cell&gt;
        &lt;cell&gt;Upload the selection to Imgur&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Spacebar&lt;/cell&gt;
        &lt;cell&gt;Toggle visibility of sidebar with options of the selected tool, color picker for the drawing color and history menu&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;G&lt;/cell&gt;
        &lt;cell&gt;Starts the color picker&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Right Click&lt;/cell&gt;
        &lt;cell&gt;Show the color wheel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Mouse Wheel&lt;/cell&gt;
        &lt;cell&gt;Change the tool's thickness&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Print screen&lt;/cell&gt;
        &lt;cell&gt;Capture Screen&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shift + Print&lt;/cell&gt;
        &lt;cell&gt;Screenshot History&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + drawing line, arrow or marker&lt;/cell&gt;
        &lt;cell&gt;Drawing only horizontally, vertically or diagonally&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ctrl + drawing rectangle or circle&lt;/cell&gt;
        &lt;cell&gt;Keeping aspect ratio&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Shift + drag a handler of the selection area: mirror redimension in the opposite handler.&lt;/p&gt;
    &lt;p&gt;Flameshot uses Print screen (Windows) and cmd-shift-x (macOS) as default global hotkeys.&lt;/p&gt;
    &lt;p&gt;On Linux, Flameshot doesn't yet support Prt Sc out of the box, but with a bit of configuration you can set this up:&lt;/p&gt;
    &lt;p&gt;To make configuration easier, there's a file in the repository that more or less automates this process. This file will assign the following hotkeys by default:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Keys&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Start the Flameshot screenshot tool and take a screenshot&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Ctrl + Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Wait for 3 seconds, then start the Flameshot screenshot tool and take a screenshot&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Shift + Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Take a full-screen (all monitors) screenshot and save it&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Ctrl + Shift + Prt Sc&lt;/cell&gt;
        &lt;cell&gt;Take a full-screen (all monitors) screenshot and copy it to the clipboard&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;If you don't like the defaults, they can be changed later.&lt;/p&gt;
    &lt;p&gt;Steps for using the configuration:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The configuration file makes Flameshot automatically save screenshots to&lt;/p&gt;&lt;code&gt;~/Pictures/Screenshots&lt;/code&gt;without opening the save dialog. Make sure that folder exists by running:&lt;code&gt;mkdir -p ~/Pictures/Screenshots&lt;/code&gt;&lt;p&gt;(If you don't like the default location, you can skip this step and configure your preferred directory later.)&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Download the configuration file:&lt;/p&gt;
        &lt;quote&gt;cd ~/Desktop wget https://raw.githubusercontent.com/flameshot-org/flameshot/master/docs/shortcuts-config/flameshot-shortcuts-kde.khotkeys&lt;/quote&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Make sure you have the&lt;/p&gt;&lt;code&gt;khotkeys&lt;/code&gt;installed using your package manager to enable custom shortcuts in KDE Plasma.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Go to System Settings → Shortcuts → Custom Shortcuts.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If an entry exists for Spectacle (the default KDE screenshot utility), you'll need to disable it because its shortcuts might conflict with Flameshot's. Do this by unchecking the Spectacle entry.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Click Edit → Import..., navigate to the configuration file and open it.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Now the Flameshot entry should appear in the list. Click Apply to apply the changes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you want to change the default hotkeys, you can expand the entry, select the appropriate action and modify it as you wish; the process is pretty self-explanatory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you installed Flameshot as a Flatpak, you will need to create a symlink to the command:&lt;/p&gt;
        &lt;code&gt;ln -s /var/lib/flatpak/exports/bin/org.flameshot.Flameshot ~/.local/bin/flameshot&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To use Flameshot instead of the default screenshot application in Gnome we need to remove the binding on Prt Sc key, and then create a new binding for &lt;code&gt;flameshot gui&lt;/code&gt; (adapted from Pavel's answer on AskUbuntu).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Remove the binding on Prt Sc:&lt;/p&gt;
        &lt;p&gt;Go to Settings &amp;gt; Keyboard &amp;gt; View and Customise Shortcuts &amp;gt; Screenshots &amp;gt; Take a screenshot interactively and press&lt;/p&gt;
        &lt;code&gt;backspace&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Add custom binding on Prt Sc:&lt;/p&gt;
        &lt;p&gt;Go to Settings &amp;gt; Keyboard &amp;gt; View and Customise Shortcuts &amp;gt; Custom shortcuts and press the '+' button at the bottom.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Name the command as you like it, e.g.&lt;/p&gt;&lt;code&gt;flameshot&lt;/code&gt;. And in the command insert&lt;code&gt;/usr/bin/flameshot gui&lt;/code&gt;or&lt;code&gt;flatpak run org.flameshot.Flameshot gui&lt;/code&gt;if installed via flatpak.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Then click "Set Shortcut.." and press Prt Sc. This will show as "print".&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now every time you press Prt Sc, it will start the Flameshot GUI instead of the default application.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Go to&lt;/p&gt;&lt;code&gt;Keyboard&lt;/code&gt;settings&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Switch to the tab&lt;/p&gt;
        &lt;code&gt;Application Shortcuts&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find the entry&lt;/p&gt;
        &lt;code&gt;Command Shortcut xfce4-screenshooter -fd 1 Print&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Replace&lt;/p&gt;&lt;code&gt;xfce4-screenshooter -fd 1&lt;/code&gt;with&lt;code&gt;flameshot gui&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now every time you press Prt Sc it will start Flameshot GUI instead of the default application.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Edit your&lt;/p&gt;&lt;code&gt;~/.fluxbox/keys&lt;/code&gt;file&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Add a new entry.&lt;/p&gt;&lt;code&gt;Print&lt;/code&gt;is the key name,&lt;code&gt;flameshot gui&lt;/code&gt;is the shell command; for more options see the fluxbox wiki.&lt;code&gt;Print :Exec flameshot gui&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Refresh Fluxbox configuration with Reconfigure option from the menu.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Experimental Gnome Wayland and Plasma Wayland support.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;If you are using Gnome you need to install the AppIndicator and KStatusNotifierItem Support extension in order to see the system tray icon.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Press Enter or Ctrl + C when you are in a capture mode and you don't have an active selection and the whole desktop will be copied to your clipboard. Pressing Ctrl + S will save your capture to a file. Check the Shortcuts for more information.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Flameshot works best with a desktop environment that includes D-Bus. See this article for tips on using Flameshot in a minimal window manager (dwm, i3, xmonad, etc).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;In order to speed up the first launch of Flameshot (D-Bus init of the app can be slow), consider starting the application automatically on boot.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Quick tip: If you don't have Flameshot to autostart at boot and you want to set keyboard shortcut, use the following as the command for the keybinding:&lt;/item&gt;
        &lt;/list&gt;
        &lt;quote&gt;( flameshot &amp;amp;; ) &amp;amp;&amp;amp; ( sleep 0.5s &amp;amp;&amp;amp; flameshot gui )&lt;/quote&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Flameshot can be installed on Linux, Microsoft Windows, and macOS.&lt;/p&gt;
    &lt;p&gt;Some prebuilt packages are provided on the release page of the GitHub project repository.&lt;/p&gt;
    &lt;p&gt;There are packages available in the repository of some Linux distributions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Arch: &lt;code&gt;pacman -S flameshot&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Snapshot also available via AUR: flameshot-git.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Debian 10+: &lt;code&gt;apt install flameshot&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Package for Debian 9 ("Stretch") also available via stretch-backports.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Ubuntu: &lt;code&gt;apt install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;openSUSE: &lt;code&gt;zypper install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Void Linux: &lt;code&gt;xbps-install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Solus: &lt;code&gt;eopkg it flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Fedora: &lt;code&gt;dnf install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;NixOS: &lt;code&gt;nix-env -iA nixos.flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ALT: &lt;code&gt;su - -c "apt-get install flameshot"&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Snap/Flatpak/AppImage&lt;/item&gt;
      &lt;item&gt;Docker&lt;/item&gt;
      &lt;item&gt;Windows&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;MacPorts: &lt;code&gt;sudo port selfupdate &amp;amp;&amp;amp; sudo port install flameshot&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Homebrew: &lt;code&gt;brew install --cask flameshot&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note that because of macOS security features, you may not be able to open flameshot when installed using brew. If you see the message &lt;code&gt;“flameshot” cannot be opened because the developer cannot be verified.&lt;/code&gt; you will need to
follow the steps below:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Go to the Applications folder (Finder &amp;gt; Go &amp;gt; Applications, or Shift+Command+A)&lt;/item&gt;
      &lt;item&gt;Right-Click on "flameshot.app" and choose "Open" from the context menu&lt;/item&gt;
      &lt;item&gt;In the dialog click "Open"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On MacOs 15 and above, you will have to go to system settings -&amp;gt; privacy and security after doing this and click "Open Anyway" or you can open flameshot first time with the following command.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;sudo xattr -rd com.apple.quarantine /Applications/flameshot.app&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;After following all those steps above, &lt;code&gt;flameshot&lt;/code&gt; will open without problems in your Mac.&lt;/p&gt;
    &lt;p&gt;Note that for the Flameshot icon to appear in your tray area, you should have a systray software installed. This is especially true for users who use minimal window managers such as dwm. In some Desktop Environment installations (e.g Gnome), the systray might be missing and you can install an application or plugin (e.g Gnome shell extension) to add the systray to your setup. It has been reported) that icon of some software, including Flameshot, does not show in gnome-shell-extension-appindicator.&lt;/p&gt;
    &lt;p&gt;Alternatively, in case you don't want to have a systray, you can always call Flameshot from the terminal. See Usage section.&lt;/p&gt;
    &lt;p&gt;To build the application in your system, you'll need to install the dependencies needed for it and package names might be different for each distribution, see Dependencies below for more information. You can also install most of the Qt dependencies via their installer. If you were developing Qt apps before, you probably already have them.&lt;/p&gt;
    &lt;p&gt;This project uses CMake build system, so you need to install it in order to build the project (on most Linux distributions it is available in the standard repositories as a package called &lt;code&gt;cmake&lt;/code&gt;). If your distribution provides too old version of CMake (e.g. Ubuntu or Debian) you can download it on the official website.&lt;/p&gt;
    &lt;p&gt;Also you can open and build/debug the project in a C++ IDE. For example, in Qt Creator you should be able to simply open &lt;code&gt;CMakeLists.txt&lt;/code&gt; via &lt;code&gt;Open File or Project&lt;/code&gt; in the menu after installing CMake into your system. More information about CMake projects in Qt Creator.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Qt &amp;gt;= 6.2.4 (available by default on Ubuntu Jammy) &lt;list rend="ul"&gt;&lt;item&gt;Development tools&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;GCC &amp;gt;= 11&lt;/item&gt;
      &lt;item&gt;CMake &amp;gt;= 3.22&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Qt &lt;list rend="ul"&gt;&lt;item&gt;SVG&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Git&lt;/item&gt;
      &lt;item&gt;OpenSSL&lt;/item&gt;
      &lt;item&gt;CA Certificates&lt;/item&gt;
      &lt;item&gt;Qt Image Formats - for additional export image formats (e.g. tiff, webp, and more)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Compile-time
apt install g++ cmake build-essential qt6-base-dev qt6-tools-dev-tools qt6-svg-dev qt6-tools-dev

# Run-time
apt install libkf6guiaddons-dev libqt6dbus6 libqt6network6 libqt6core6 libqt6widgets6 libqt6gui6 libqt6svg6 qt6-qpa-plugins

# Optional
apt install git openssl ca-certificates qt6-image-formats-plugins&lt;/code&gt;
    &lt;code&gt;# Compile-time
dnf install gcc-c++ cmake qt6-qtbase-devel qt6-qtsvg-devel qt6-qttools qt6-linguist qt6-qttools-devel kf6-kguiaddons-devel

# Run-time
dnf install qt6-qtbase qt6-qtsvg kf6-kguiaddons

# Optional
dnf install git openssl ca-certificates qt6-qtimageformats&lt;/code&gt;
    &lt;code&gt;# Compile-time
pacman -S cmake base-devel git qt6-base qt6-tools kguiaddons

# Run-time
pacman -S qt6-svg

# Optional
pacman -S openssl ca-certificates qt6-imageformats&lt;/code&gt;
    &lt;p&gt;Development Shell:&lt;/p&gt;
    &lt;code&gt;# Without flakes:
nix-shell

# With flakes:
nix develop&lt;/code&gt;
    &lt;code&gt;# Build flameshot
nix build

# Build and run flameshot
nix run&lt;/code&gt;
    &lt;p&gt;First of all you need to install brew and then install the dependencies&lt;/p&gt;
    &lt;code&gt;brew install qt6
brew install cmake&lt;/code&gt;
    &lt;p&gt;After installing all the dependencies, Flameshot can be built.&lt;/p&gt;
    &lt;p&gt;For the translations to be loaded correctly, the build process needs to be aware of where you want to install Flameshot.&lt;/p&gt;
    &lt;code&gt;# Directory where build files will be placed, may be relative
export BUILD_DIR=build

# Directory prefix where Flameshot will be installed. If you are just building and don't want to
# install, comment this environment variable.
# This excludes the bin/flameshot part of the install,
# e.g. in /opt/flameshot/bin/flameshot, the CMAKE_INSTALL_PREFIX is /opt/flameshot
# This must be an absolute path. Requires CMAKE 3.29.
export CMAKE_INSTALL_PREFIX=/opt/flameshot

# Linux
cmake -S . -B "$BUILD_DIR" \
    &amp;amp;&amp;amp; cmake --build "$BUILD_DIR"

#MacOS
cmake -S . -B "$BUILD_DIR" \
    -DQt6_DIR="$(brew --prefix qt6)/lib/cmake/Qt6" \
    &amp;amp;&amp;amp; cmake --build "$BUILD_DIR"&lt;/code&gt;
    &lt;p&gt;When the &lt;code&gt;cmake --build&lt;/code&gt; command has completed you can launch Flameshot from the &lt;code&gt;project_folder/build/src&lt;/code&gt; folder.&lt;/p&gt;
    &lt;p&gt;Note that if you install from source, there is no uninstaller, so consider installing to a custom directory.&lt;/p&gt;
    &lt;p&gt;Make sure you are using cmake &lt;code&gt;&amp;gt;= 3.29&lt;/code&gt; and build Flameshot with &lt;code&gt;$CMAKE_INSTALL_PREFIX&lt;/code&gt; set to the
installation directory. If this is not done, the translations won't be found when using a custom directory.
Then, run the following:&lt;/p&gt;
    &lt;code&gt;# !Build with CMAKE_INSTALL_PREFIX and use cmake &amp;gt;= 3.29! Using an older cmake will cause
# installation into the default /usr/local dir.

# You may need to run this with privileges
cmake --install "$BUILD_DIR"&lt;/code&gt;
    &lt;code&gt;# You may need to run this with privileges
cmake --install "$BUILD_DIR"&lt;/code&gt;
    &lt;p&gt;https://flameshot.org/docs/guide/faq/&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The main code is licensed under GPLv3&lt;/item&gt;
      &lt;item&gt;The logo of Flameshot is licensed under Free Art License v1.3&lt;/item&gt;
      &lt;item&gt;The button icons are licensed under Apache License 2.0. See: https://github.com/google/material-design-icons&lt;/item&gt;
      &lt;item&gt;The code at capture/capturewidget.cpp is based on https://github.com/ckaiser/Lightscreen/blob/master/dialogs/areadialog.cpp (GPLv2)&lt;/item&gt;
      &lt;item&gt;The code at capture/capturewidget.h is based on https://github.com/ckaiser/Lightscreen/blob/master/dialogs/areadialog.h (GPLv2)&lt;/item&gt;
      &lt;item&gt;I copied a few lines of code from KSnapshot regiongrabber.cpp revision &lt;code&gt;796531&lt;/code&gt;(LGPL)&lt;/item&gt;
      &lt;item&gt;Qt-Color-Widgets taken and modified from https://github.com/mbasaglia/Qt-Color-Widgets (see their license and exceptions in the project) (LGPL/GPL)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Info: If I take code from your project and that implies a relicense to GPLv3, you can reuse my changes with the original previous license of your project applied.&lt;/p&gt;
    &lt;p&gt;This program will not transfer any information to other networked systems unless specifically requested by the user or the person installing or operating it.&lt;/p&gt;
    &lt;p&gt;For Windows binaries, this program uses free code signing provided by SignPath.io, and a certificate by the SignPath Foundation.&lt;/p&gt;
    &lt;p&gt;Code signing is currently a manual process so not every patch release will be signed.&lt;/p&gt;
    &lt;p&gt;If you want to contribute check the CONTRIBUTING.md&lt;/p&gt;
    &lt;p&gt;Thanks to those who have shown interest in the early development process:&lt;/p&gt;
    &lt;p&gt;Thanks to sponsors:&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46815297</guid><pubDate>Thu, 29 Jan 2026 19:30:35 +0000</pubDate></item><item><title>The Hallucination Defense</title><link>https://niyikiza.com/posts/hallucination-defense/</link><description>&lt;doc fingerprint="ac0448d60af1a833"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Hallucination Defense&lt;/head&gt;
    &lt;p&gt;Why logs make 'The AI Did It' the perfect excuse&lt;/p&gt;
    &lt;p&gt;“The AI hallucinated. I never asked it to do that.”&lt;/p&gt;
    &lt;p&gt;That’s the defense. And here’s the problem: it’s often hard to refute with confidence.&lt;/p&gt;
    &lt;p&gt;A financial analyst uses an AI agent to “summarize quarterly reports.” Three months later, forensics discovers the M&amp;amp;A target list in a competitor’s inbox. The agent accessed the files. The agent sent the email. But the prompt history? Deleted. The original instruction? The analyst’s word against the logs.&lt;/p&gt;
    &lt;p&gt;Without a durable cryptographic proof binding the human to a scoped delegation, “the AI did it” becomes a convenient defense. The agent can’t testify. It can’t remember. It can’t defend itself.&lt;/p&gt;
    &lt;head rend="h2"&gt;Logs Aren’t Proof&lt;/head&gt;
    &lt;p&gt;“But we log everything. We have OAuth logs.”&lt;/p&gt;
    &lt;p&gt;Most production agent systems do log a lot, and that’s good practice. Logs give visibility into what happened, when, and which component did it:&lt;/p&gt;
    &lt;code&gt;2026-01-15T14:32:01Z agent=research-bot action=file_read path=/data/ma/target-corp.pdf
2026-01-15T14:32:03Z agent=research-bot action=email_send to=external@competitor.com
&lt;/code&gt;
    &lt;p&gt;With the right setup (append-only storage, signed timestamps, retention controls), logs can be tamper-evident. They can be excellent evidence that an event occurred inside your system.&lt;/p&gt;
    &lt;p&gt;But in disputes, the question is rarely “did something happen?” It’s:&lt;/p&gt;
    &lt;p&gt;Who authorized this class of action, for which agent identity, under what constraints, for how long; and how did that authority flow?&lt;/p&gt;
    &lt;p&gt;A common failure mode in agent incidents is not “we don’t know what happened,” but:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We can’t produce a crisp artifact showing that a specific human explicitly authorized the scope that made this action possible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This gap gets wider in multi-agent systems:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A human authorizes an orchestrator.&lt;/item&gt;
      &lt;item&gt;The orchestrator spawns sub-agents.&lt;/item&gt;
      &lt;item&gt;Sub-agents call plugins, third-party services, or external runtimes.&lt;/item&gt;
      &lt;item&gt;The final action executes somewhere that may not share your identity domain, your audit system, or your policy engine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In that world, logs can still show: “a valid session existed” and “a component with access acted.” But it becomes harder to show, with a single verifiable chain, that the final actor was operating under a scope the human actually delegated; rather than under a generic session token, a broad integration credential, or inferred intent.&lt;/p&gt;
    &lt;p&gt;This isn’t a dismissal of logging, approvals, policy engines, or token hardening. It’s an argument that accountability needs one more artifact: independently verifiable authorization evidence that survives multi-hop execution.&lt;/p&gt;
    &lt;p&gt;That’s the liability gap: between “we recorded an event” and “we can produce a verifiable delegation chain for it.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Authorization as a First-Class Artifact&lt;/head&gt;
    &lt;p&gt;When real money moves, institutions don’t rely on “someone had a session.” They require explicit authorization steps (step-up authentication, approvals, dual control, callbacks) and keep durable records of the authorization decision. In inter-organization rails, messages are authenticated so participants can verify who sent what within that rail.&lt;/p&gt;
    &lt;p&gt;Not every bank user personally applies a cryptographic signature to every instruction but there is a more general point:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In high-stakes systems, the unit of accountability is the action and its authorization record, not a long-lived session.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The check system, for all its well-documented flaws, is still interesting because it treats authorization as an artifact you can present later, not a session you have to reconstruct. In a loose, pre-cryptographic way, it gestures at two properties we want for agent delegation.&lt;/p&gt;
    &lt;p&gt;First, designated negotiation. Checks are addressed to a payee, and endorsement/deposit rules attempt to control who can successfully negotiate the instrument and where. Restrictive endorsements (“for deposit only…”) are a crude procedural attempt at holder binding. It’s not cryptographic enforcement, but the shape is right: an authorization artifact meant for a particular holder or route, rather than a replayable credential.&lt;/p&gt;
    &lt;p&gt;Second, non-amplification. Checks instruct settlement against scarce funds. You can write many checks, but settlement ultimately reconciles against a limited balance (or credit line). Failure may be detected late, but delegation doesn’t create value.&lt;/p&gt;
    &lt;p&gt;Tenuo Warrants apply both ideas to agent actions with modern enforcement: a warrant is holder-bound to a specific agent key, and attenuable so delegated scope can only narrow as it flows downstream.&lt;/p&gt;
    &lt;p&gt;And this is the non-repudiation point: if delegation is going to cross tools and sub-agents, you need a durable artifact you can show later that answers who authorized what.&lt;/p&gt;
    &lt;p&gt;But in agent systems we authenticate a session (“Bob is logged in”) and then infer intent from a mixture of logs, prompts, and downstream effects. That works until it doesn’t; especially when an incident involves ambiguous delegation paths, third-party tools, or autonomous sub-agents.&lt;/p&gt;
    &lt;p&gt;OAuth is great at what it’s designed to do: delegating access and expressing scopes at the token level. But a bearer token is a portable credential: whoever holds it can use it. You can reduce replay risk with sender-constrained tokens (mTLS, DPoP), but even then a primitive is missing:&lt;/p&gt;
    &lt;p&gt;Where is the action-level authorization artifact that says:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;“This human authorized this agent identity to perform this class of operations within these constraints for this duration”?&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Warrants: Signed Authorization for Every Action&lt;/head&gt;
    &lt;p&gt;A Tenuo warrant is a cryptographic, scoped, time-bound authorization object that can be verified independently of the agent runtime and that remains meaningful across multi-hop delegation.&lt;/p&gt;
    &lt;code&gt;# Human signs the authorization (via Passkey/WebAuthn, not manual key management)
warrant = Warrant.mint(
    issuer=alice_passkey,
    holder=agent_public_key,
    capability="file_read",
    constraints={"path": Subpath("/data/reports")},
    ttl=timedelta(hours=1),
)
&lt;/code&gt;
    &lt;p&gt;When the agent reads a file, it presents this warrant. The file server validates the signature, checks the constraints, and produces a receipt that pairs authorization evidence with the action metadata.&lt;/p&gt;
    &lt;p&gt;A verifier checks:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Issuer signature (who authorized)&lt;/item&gt;
      &lt;item&gt;Holder binding (the caller proves possession of the agent key named in the warrant)&lt;/item&gt;
      &lt;item&gt;Capability + constraints + expiry (what was allowed, within which bounds, for how long)&lt;/item&gt;
      &lt;item&gt;Delegation chain (how authority flowed across hops, including whether the agent was allowed to delegate)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The receipt captures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alice’s signature in the warrant (cryptographic proof of authorization)&lt;/item&gt;
      &lt;item&gt;The constraints (cryptographic proof of authorized scope)&lt;/item&gt;
      &lt;item&gt;Validation time (evidence of when it was authorized/accepted)&lt;/item&gt;
      &lt;item&gt;Action metadata (evidence of what was requested/executed, depending on what you record)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Logs describe. Receipts prove.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Attack, Replayed&lt;/head&gt;
    &lt;p&gt;Same scenario. Analyst wants to process a batch of vendor invoices. Easier to sign one warrant with a high limit and let the agent handle the rest than approve each transfer individually.&lt;/p&gt;
    &lt;p&gt;The warrant their passkey signed at 3:12 PM:&lt;/p&gt;
    &lt;code&gt;tool: transfer
amount: range(0, 50000)
to: *
ttl: 3600
&lt;/code&gt;
    &lt;p&gt;Every other analyst that day processed similar batch sizes. They signed 12-15 warrants each:&lt;/p&gt;
    &lt;code&gt;tool: transfer
amount: range(0, 500)
to: vendors/approved/*
ttl: 60
&lt;/code&gt;
    &lt;p&gt;Three months later, forensics flags a $48,000 transfer to an external account mixed in with the batch.&lt;/p&gt;
    &lt;p&gt;Analyst’s defense: “The AI hallucinated. I was just trying to be efficient.”&lt;/p&gt;
    &lt;p&gt;Your response: Everyone else processed the same volume with task-scoped warrants. You signed one that authorized 100x the limit, to any recipient, for an hour. You signed it.&lt;/p&gt;
    &lt;p&gt;The receipt answers what logs can’t: what did you choose to allow?&lt;/p&gt;
    &lt;head rend="h2"&gt;“But What About Prompt Injection?”&lt;/head&gt;
    &lt;p&gt;If an attacker hijacks the agent mid-session, doesn’t that break accountability?&lt;/p&gt;
    &lt;p&gt;Warrants don’t magically stop prompt injection. They make the blast radius explicit and the authorization undeniable.&lt;/p&gt;
    &lt;p&gt;Constraints limit what can happen. The warrant says &lt;code&gt;Subpath("/data/reports")&lt;/code&gt;. If the injection tries to read &lt;code&gt;/etc/shadow&lt;/code&gt;, it will be deterministically denied. The capability doesn’t exist, regardless of what the prompt says.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The attack succeeds. The action doesn’t.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Receipts prove what was authorized. If something did happen, the warrant chain answers who signed off on the scope that allowed it.&lt;/p&gt;
    &lt;p&gt;Approval is explicit. The UI doesn’t say “Authorize Agent.” It says “Authorize Agent to read &lt;code&gt;/data/reports&lt;/code&gt; for 1 hour.”&lt;/p&gt;
    &lt;p&gt;Broad authorization is a choice. A choice you sign. A choice you own.&lt;/p&gt;
    &lt;p&gt;Warrants are both a guardrail (prevention via constraints) and a receipt (accountability via signatures).&lt;/p&gt;
    &lt;head rend="h2"&gt;“What If the Signing Device Is Compromised?”&lt;/head&gt;
    &lt;p&gt;If a passkey is stolen, you have a crime scene. The attacker had to compromise a specific device. You know which one, when, and what it signed. The forensics point somewhere.&lt;/p&gt;
    &lt;p&gt;If an OAuth token is stolen, you have a ghost. Bearer tokens have no proof of possession: whoever holds it is authorized. It works from anywhere. Logs show what happened, but nothing ties the action to a device, a user, or a moment of intent.&lt;/p&gt;
    &lt;p&gt;A log is an assertion by your system. A receipt is a statement signed by the authorizer.&lt;/p&gt;
    &lt;head rend="h2"&gt;Trust the Math&lt;/head&gt;
    &lt;p&gt;Prompt filters don’t take the stand. When the breach happens, when the subpoena lands, when the regulator asks “prove this was authorized,” you don’t want to explain your prompt engineering strategy.&lt;/p&gt;
    &lt;p&gt;Signatures bind humans to actions. Holder binding makes stolen warrants useless. Constraints limit blast radius. None of it requires trusting the model.&lt;/p&gt;
    &lt;p&gt;You want receipts.&lt;/p&gt;
    &lt;p&gt;Tenuo is an open-source authorization framework for AI agents. Ed25519 signatures, capability-based delegation, 27μs verification.&lt;/p&gt;
    &lt;p&gt;Deploying agents in production? Let’s talk.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46815527</guid><pubDate>Thu, 29 Jan 2026 19:45:12 +0000</pubDate></item><item><title>The WiFi only works when it's raining (2024)</title><link>https://predr.ag/blog/wifi-only-works-when-its-raining/</link><description>&lt;doc fingerprint="dfcd41775e25d9cf"&gt;
  &lt;main&gt;
    &lt;p&gt;Happy April 1st! This post is part of April Cools Club: an April 1st effort to publish genuine essays on unexpected topics. Please enjoy this true story, and rest assured that the tech content will be back soon!&lt;/p&gt;
    &lt;p&gt;That's what my dad said when I asked what was wrong with our home internet connection. "The Wi-Fi only works when it's raining."&lt;/p&gt;
    &lt;p&gt;Let's back up a few steps, so we're all on the same page about the utter ridiculousness of this situation.&lt;/p&gt;
    &lt;p&gt;At the time, I was still a college student — this was over 10 years ago. I had come back home to spend a couple of weeks with my parents before the fall semester kicked off. I hadn't been back home in almost a full year, because home and school were on different continents.&lt;/p&gt;
    &lt;p&gt;My dad is an engineer who had already been tinkering with networking gear longer than I'd been alive. Through the company he started, he had designed and deployed all sorts of complex network systems at institutions across the country — everything from gigabit Ethernet for an office building, to inter-city connections over line-of-sight microwave links.&lt;/p&gt;
    &lt;p&gt;He is the last person on Earth who would say a "magical thinking" phrase like that.&lt;/p&gt;
    &lt;p&gt;"What?" I uttered, stunned. "The Wi-Fi only works while it's raining," he repeated patiently. "It started a couple of weeks ago, and I haven't had a chance to look into it yet."&lt;/p&gt;
    &lt;p&gt;"No way," I said. If anything, rain makes wireless signal quality worse, not better. Never better!&lt;/p&gt;
    &lt;p&gt;Two weeks without reliable internet? I started a speed-run through the stages of grief...&lt;/p&gt;
    &lt;head rend="h2"&gt;Denial&lt;/head&gt;
    &lt;p&gt;I pulled open my laptop and started poking at the network.&lt;/p&gt;
    &lt;p&gt;Pinging any website had a 98% packet loss rate. The internet connection was still up, but only in the most annoying "technically accurate" sense. Nothing loads when you have a 98% packet loss rate! The network may as well have been dead.&lt;/p&gt;
    &lt;p&gt;I was upset. I had just started dating someone a few months prior, and she was currently on the other side of the planet! How was I to explain that I couldn't stay in touch because it wasn't raining? Mobile data at the time was exorbitantly expensive, so much so that I didn't have a data plan at all for my cell service at home. I couldn't just use my phone's data plan to work around the problem, like one might do today in a similar situation.&lt;/p&gt;
    &lt;p&gt;I was pacing around the house, fuming. Grief, stage two!&lt;/p&gt;
    &lt;p&gt;That's when the rain started.&lt;/p&gt;
    &lt;head rend="h2"&gt;Bargaining&lt;/head&gt;
    &lt;p&gt;Like a miracle, within 5 minutes of the rain starting, the packet loss rate was down to 0%!&lt;/p&gt;
    &lt;p&gt;I couldn't believe my eyes! I was ready for the connection to die at any second, so I opened a million tabs at once — as if I don't normally do that anyway...&lt;/p&gt;
    &lt;p&gt;The rain held up for about an hour, and so did the internet connection.&lt;/p&gt;
    &lt;p&gt;Then, 15 minutes or so after the rain stopped, the packet loss rate shot back up to 90%+. The internet connection went back to being unusable.&lt;/p&gt;
    &lt;p&gt;I was ready to do just about anything to get more rain.&lt;/p&gt;
    &lt;p&gt;Thankfully, the weather stayed grey and murky for the next few days. Each time, the pattern stayed the same:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The rain starts, and not even a few minutes later the internet connection is crisp and fast.&lt;/item&gt;
      &lt;item&gt;The rain stops, and within 15 minutes the internet connection is unusable again.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As much as I hated to admit it, the evidence was solid. The Wi-Fi only works when it's raining!&lt;/p&gt;
    &lt;p&gt;At this point, I had a choice to make.&lt;/p&gt;
    &lt;p&gt;I could keep going through the stages of grief: I could sulk and plan my calls with my girlfriend around the weather forecast.&lt;/p&gt;
    &lt;p&gt;Or, I could break out of that downward spiral and get to the bottom of what was going on.&lt;/p&gt;
    &lt;p&gt;"Magical thinking be damned! Am I an engineer or what?" I told myself.&lt;/p&gt;
    &lt;p&gt;That settled it. I wasn't going to take this lying down.&lt;/p&gt;
    &lt;head rend="h2"&gt;Determination&lt;/head&gt;
    &lt;p&gt;Some context on our home networking setup is in order.&lt;/p&gt;
    &lt;p&gt;Remember how my dad's company had extensive experience with networking solutions? Well, we had a fancy networking setup at home too — and it had worked flawlessly for the best part of 10 years!&lt;/p&gt;
    &lt;p&gt;My dad's office had a very expensive, very fast For the time, of course. commercial internet connection. The home internet options, meanwhile, weren't great! In my family, we are often stubbornly against settling for less unless there's absolutely no other choice.&lt;/p&gt;
    &lt;p&gt;The office and our apartment were a few blocks away from each other along a small hill, with our second-floor apartment holding the higher ground. With a bit of work, my dad set up a line-of-sight Wi-Fi bridge — a couple of high-gain directional Wi-Fi antennas pointed at each other — between the office and our apartment. This let us enjoy the faster commercial internet connection at home!&lt;/p&gt;
    &lt;p&gt;I started poking around the network to figure out where the connection was breaking down.&lt;/p&gt;
    &lt;p&gt;The local Wi-Fi router at home was working well — no packets lost. The local end of the Wi-Fi bridge was fine too.&lt;/p&gt;
    &lt;p&gt;But pinging the remote end of the Wi-Fi bridge was showing a 90%+ packet loss rate — and so did pinging any other network device behind it. Aha, there's something wrong with the Wi-Fi bridge!&lt;/p&gt;
    &lt;p&gt;But what? And why now, when the system had been working fine for almost 10 years, rain or shine? Maybe years of work experience isn't a good metric here either 😄&lt;/p&gt;
    &lt;p&gt;How can a rain storm fix a Wi-Fi bridge, anyway?&lt;/p&gt;
    &lt;p&gt;So many confusing questions. Time to get some answers!&lt;/p&gt;
    &lt;head rend="h2"&gt;Debugging&lt;/head&gt;
    &lt;p&gt;Like any experienced engineer, the first thing I tried was turning everything off and then on again. It didn't work.&lt;/p&gt;
    &lt;p&gt;Then I checked all the devices on the network individually:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Maybe one of the devices has gone bad with age? Nope. I physically connected my laptop to each device's local Ethernet, then ran diagnostics, pinged the devices over the wired connection, etc.&lt;/item&gt;
      &lt;item&gt;Maybe a cable got unseated or came loose? Nope.&lt;/item&gt;
      &lt;item&gt;Maybe a power brick has become faulty over time? Nope.&lt;/item&gt;
      &lt;item&gt;Maybe an automatic firmware update failed and broke something? Nope.&lt;/item&gt;
      &lt;item&gt;Maybe an antenna connector has corroded from spending years outdoors? Nope.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Unlike debugging software, a lot of this hardware debugging was annoyingly physical. I had to climb up ladders, trace cables that hadn't been touched in 10 years, and do a lot of walking back and forth between our home and my dad's office.&lt;/p&gt;
    &lt;p&gt;On my umpteenth back-and-forth walk, as I was bored and exasperated, I started noticing how much our neighborhood had changed in the many years I hadn't been living at home full-time. Before college, I spent four years at a boarding high school. I was on our national math and programming teams for the IMO and IOI), so I even spent most of each summer away from home at prep camps and at the competitions themselves. Many of the little neighborhood shops were new. Many houses had gotten a fresh coat of paint. Trees that used to be barely more than saplings had grown tall and strong.&lt;/p&gt;
    &lt;p&gt;Then it hit me.&lt;/p&gt;
    &lt;head rend="h2"&gt;Realization&lt;/head&gt;
    &lt;p&gt;I ran home and climbed up onto the scaffolding holding up the Wi-Fi bridge's antenna. I was hanging precariously off the side of our apartment building, two stories up in the air. In retrospect, a safety harness would have been a good idea... Things people do for internet! Don't forget, a girl was involved too — I wasn't doing this merely for Netflix or Twitter.&lt;/p&gt;
    &lt;p&gt;Then I looked downhill, at the antenna that formed the second half of the Wi-Fi bridge.&lt;/p&gt;
    &lt;p&gt;Or at least, toward the antenna, because I couldn't see it — a tree in a neighbor's yard was in the way! Its topmost branches were swaying back and forth in the line-of-sight between the antenna pair.&lt;/p&gt;
    &lt;p&gt;Bingo!&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem and the Fix&lt;/head&gt;
    &lt;p&gt;Here's what was going on.&lt;/p&gt;
    &lt;p&gt;Many years ago, we installed the Wi-Fi bridge. For a long time, everything was great!&lt;/p&gt;
    &lt;p&gt;But every year, our neighbor's tree grew taller and taller. Shortly before when I came back home that summer, its topmost branches had managed to reach high enough to interfere with our Wi-Fi signal.&lt;/p&gt;
    &lt;p&gt;It was only barely tall enough to interfere with the signal, though!&lt;/p&gt;
    &lt;p&gt;Every time it rained, the rain collected on its leaves and branches and weighed them down. The extra weight bent them out of the way of the Wi-Fi line-of-sight! Interestingly, objects outside the straight line between antennas can still cause interference! For best signal quality, the Fresnel zone between the antennas should be clear of obstructions. But perfection isn't achievable in practice, so RF equipment like Wi-Fi uses techniques like error-correcting codes so that it can still work without a perfectly clear Fresnel zone.&lt;/p&gt;
    &lt;p&gt;Each time the rain stopped, the rainwater would continue to drip off the tree. Slowly, over the course of 15ish minutes, that would unburden the tree — letting it rise back up into the path of our bits and bytes. That's when the Wi-Fi would stop working.&lt;/p&gt;
    &lt;p&gt;The fix was easy: upgrade our hardware. We replaced our old 802.11g devices with new 802.11n ones, which took advantage of new &lt;del&gt;magic&lt;/del&gt; math and physics to make signals more resistant to interference. One such piece of magic new to 802.11n Wi-Fi is called "beamforming" — it's when a transmitter can use multiple antennas transmitting on the same frequency to shape and steer the signal in a way that improves the effective range and signal quality. Modern Wi-Fi does beamforming with only a few antenna elements, but if we scale that number way up we get a phased array antenna. Ever wondered how come Starlink antennas are flat and not a "dish" like old satellite TV antennas? They use phased arrays to aim their signal at the Starlink satellites streaking across the sky — without any moving parts. &lt;del&gt;Magic!&lt;/del&gt; Physics! &lt;/p&gt;
    &lt;p&gt;A few days later, the new gear arrived and I eagerly climbed back up the scaffolding to install the new antennas.&lt;/p&gt;
    &lt;p&gt;A few screws, zip ties, and cable connections later, the Wi-Fi's "link established" lights flashed green once again.&lt;/p&gt;
    &lt;p&gt;This time, it wasn't raining.&lt;/p&gt;
    &lt;p&gt;All was well once again.&lt;/p&gt;
    &lt;p&gt;Hope you enjoyed this true story! April Cools is about surprising our readers with fun posts on topics outside our usual beat. Check out the other April Cools posts on our website, and consider making your own blog part of April Cools Club next year!&lt;/p&gt;
    &lt;p&gt;If you liked this post, consider subscribing or following me on social media.&lt;/p&gt;
    &lt;p&gt;Thanks to Hillel Wayne and Jeremy Kun for reading drafts of this post. All mistakes are my own.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46816357</guid><pubDate>Thu, 29 Jan 2026 20:47:36 +0000</pubDate></item><item><title>Cutting Up Curved Things (With Math)</title><link>https://campedersen.com/tessellation</link><description>&lt;doc fingerprint="8c0b28321b6d268d"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Cutting Up Curved Things&lt;/head&gt;January 30, 2026&lt;p&gt;Your GPU doesn't know what a cylinder is.&lt;/p&gt;&lt;p&gt;It knows triangles! That's it. Three points, maybe a color. The entire vocabulary of graphics hardware fits on an index card.&lt;/p&gt;&lt;p&gt;So before any curved surface can be rendered, someone has to chop it into triangles. Lots of them! Arranged just right so the illusion holds.&lt;/p&gt;&lt;p&gt;That's tessellation.&lt;/p&gt;&lt;head rend="h2"&gt;Triangles all the way down&lt;/head&gt;&lt;p&gt;A triangle mesh is just two arrays:&lt;/p&gt;&lt;code&gt;vertices: [x₀, y₀, z₀, x₁, y₁, z₁, x₂, y₂, z₂, ...]
indices:  [0, 1, 2, 0, 2, 3, ...]
&lt;/code&gt;
&lt;p&gt;Vertices are points in space, and indices say which three points form each triangle. That's the entire data structure!&lt;/p&gt;&lt;p&gt;Every 3D model you've ever seen - every game character, every CAD render, every Pixar frame - is just these two arrays, fed to a GPU that draws triangles really, really fast.&lt;/p&gt;&lt;head rend="h2"&gt;The translation problem&lt;/head&gt;&lt;p&gt;In vcad's kernel, a cylinder isn't triangles. It's a mathematical function:&lt;/p&gt;&lt;code&gt;fn point_on_cylinder(u: f64, v: f64) -&amp;gt; Point3 {
    Point3::new(
        radius * u.cos(),
        radius * u.sin(),
        v
    )
}&lt;/code&gt;&lt;p&gt;Give me an angle &lt;code&gt;u&lt;/code&gt; and a height &lt;code&gt;v&lt;/code&gt;, and I'll give you the exact point. Infinite precision! No facets.&lt;/p&gt;&lt;p&gt;Beautiful for math, but useless for rendering.&lt;/p&gt;&lt;p&gt;The tessellator's job is to sample this function enough times to build a convincing mesh.&lt;/p&gt;&lt;head rend="h2"&gt;Sampling a surface&lt;/head&gt;&lt;p&gt;How do you turn a smooth surface into triangles?&lt;/p&gt;&lt;p&gt;You sample it!&lt;/p&gt;&lt;p&gt;Lay down a grid in parameter space, the flat (u, v) domain. Evaluate the surface at each grid point to get 3D coordinates, then connect adjacent points into triangles.&lt;/p&gt;&lt;p&gt;More samples = smoother result = more triangles = slower everything.&lt;/p&gt;&lt;p&gt;32 segments around a circle is plenty for most CAD. 64 if you're zooming in. 128 if you're patient!&lt;/p&gt;&lt;head rend="h2"&gt;The easy case: flat faces&lt;/head&gt;&lt;p&gt;Planar faces don't need sampling because the vertices are already there in the topology. You just need to connect them!&lt;/p&gt;&lt;p&gt;For convex polygons, fan triangulation works perfectly:&lt;/p&gt;&lt;p&gt;Pick one vertex, draw triangles to every other pair, and you're done!&lt;/p&gt;&lt;p&gt;A square becomes 2 triangles, a hexagon becomes 4, and an n-gon becomes n-2.&lt;/p&gt;&lt;head rend="h2"&gt;Curved faces: cylinders&lt;/head&gt;&lt;p&gt;Cylinders need the UV grid approach.&lt;/p&gt;&lt;p&gt;The &lt;code&gt;u&lt;/code&gt; parameter goes around (0 to 2π) and the &lt;code&gt;v&lt;/code&gt; parameter goes up and down.&lt;/p&gt;&lt;p&gt;But the surface equation extends infinitely, so we need to know where to stop! The answer: look at the boundary edges and project them onto the cylinder axis to find the height range.&lt;/p&gt;&lt;code&gt;for vertex in boundary {
    let v = (vertex - center).dot(axis);
    v_min = v_min.min(v);
    v_max = v_max.max(v);
}&lt;/code&gt;&lt;p&gt;Now sample a grid from &lt;code&gt;v_min&lt;/code&gt; to &lt;code&gt;v_max&lt;/code&gt; and around the full circle. Each grid cell becomes two triangles!&lt;/p&gt;&lt;head rend="h2"&gt;Curved faces: spheres&lt;/head&gt;&lt;p&gt;Same idea, but latitude/longitude instead of angle/height.&lt;/p&gt;&lt;p&gt;There's a catch though: the poles.&lt;/p&gt;&lt;p&gt;At the north and south poles, an entire row of UV samples collapse to a single point. If you make normal quads there, you get degenerate slivers!&lt;/p&gt;&lt;p&gt;The fix: at the poles, emit triangles instead of quads. The pole vertex becomes the tip of a fan, shared by every triangle in that ring.&lt;/p&gt;&lt;p&gt;Like the segments of an orange, all meeting at the stem!&lt;/p&gt;&lt;head rend="h2"&gt;The hard case: holes&lt;/head&gt;&lt;p&gt;Everything above assumes simple faces: one boundary, no holes.&lt;/p&gt;&lt;p&gt;But what happens when you drill through a plate?&lt;/p&gt;&lt;p&gt;The face now has an inner loop. Fan triangulation would cover the hole! We need something smarter.&lt;/p&gt;&lt;p&gt;The trick: cut a bridge.&lt;/p&gt;&lt;p&gt;Find the rightmost point of the hole, find the nearest point on the outer boundary, and connect them with two edges (there and back), merging both loops into one continuous polygon.&lt;/p&gt;&lt;p&gt;Now there's no hole! Just a weird-shaped polygon with a slit. And we can triangulate weird-shaped polygons.&lt;/p&gt;&lt;head rend="h2"&gt;Ear clipping&lt;/head&gt;&lt;p&gt;The merged polygon isn't convex, so fan triangulation won't work. Time for the real algorithm!&lt;/p&gt;&lt;p&gt;An ear is three consecutive vertices where:&lt;/p&gt;&lt;list rend="ol"&gt;&lt;item&gt;The middle vertex is convex (bends outward)&lt;/item&gt;&lt;item&gt;No other vertices are inside the triangle&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Find an ear, clip it off as a triangle, and repeat until only three vertices remain!&lt;/p&gt;&lt;p&gt;It's like eating a pizza from the crust inward, one bite at a time, always picking a bite that doesn't overlap with toppings.&lt;/p&gt;&lt;p&gt;How do we check "convex"? See if the vertex bends outward by checking which side of a line it's on. How do we check "inside"? See if a point is surrounded by all three edges of the triangle. Both checks are just a few multiplications - fast enough to run in real-time!&lt;/p&gt;&lt;head rend="h2"&gt;The output&lt;/head&gt;&lt;p&gt;Every surface (planes, cylinders, spheres, cones, faces with holes) eventually becomes:&lt;/p&gt;&lt;code&gt;TriangleMesh {
    vertices: Vec&amp;lt;f32&amp;gt;,  // [x, y, z, x, y, z, ...]
    indices: Vec&amp;lt;u32&amp;gt;,   // [i, j, k, i, j, k, ...]
}&lt;/code&gt;&lt;p&gt;This is the format GPUs want, the format STL files use, and the format physics engines expect.&lt;/p&gt;&lt;p&gt;The tessellator is the last stop in CAD-land before geometry enters the real world!&lt;/p&gt;&lt;head rend="h2"&gt;The illusion&lt;/head&gt;&lt;p&gt;Remember: your GPU doesn't know what a sphere is.&lt;/p&gt;&lt;p&gt;Every smooth surface you've ever seen on a screen was actually tiny flat triangles, packed so tightly your eyes couldn't tell the difference. That's the trick. That's the whole trick!&lt;/p&gt;&lt;p&gt;The tessellator is the magician's assistant - it does the work so the illusion can happen. It takes beautiful mathematical curves and quietly, invisibly, chops them into something a GPU can actually draw.&lt;/p&gt;&lt;p&gt;And when you look at the result spinning on your screen, smooth and perfect?&lt;/p&gt;&lt;p&gt;You'd never know.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46817764</guid><pubDate>Thu, 29 Jan 2026 22:34:54 +0000</pubDate></item><item><title>Grid: Forever free, local-first, browser-based 3D printing/CNC/laser slicer</title><link>https://grid.space/stem/</link><description>&lt;doc fingerprint="3f7039500a03b5f0"&gt;
  &lt;main&gt;
    &lt;p&gt;Free, Privacy-First Digital Fabrication Tools for STEM Learning&lt;/p&gt;
    &lt;p&gt;No software installations, no licenses to purchase, no accounts to manage. Students simply open a browser and start creating.&lt;/p&gt;
    &lt;p&gt;All student work stays on their device. No data collection, no cloud uploads, no privacy concerns. COPPA and FERPA friendly.&lt;/p&gt;
    &lt;p&gt;No per-seat licensing, no subscription fees, no "educational discounts" that expire. Free forever for everyone.&lt;/p&gt;
    &lt;p&gt;Chromebooks, tablets, old computers, new computers. Windows, Mac, Linux. If it runs a modern browser, it runs Grid.Space.&lt;/p&gt;
    &lt;p&gt;Students work at their own pace. No internet dropouts causing lost work. Tools work offline after initial load.&lt;/p&gt;
    &lt;p&gt;Industry-standard workflows for 3D printing, CNC machining, and laser cutting. Skills transfer directly to professional tools.&lt;/p&gt;
    &lt;p&gt;Introduce students to digital fabrication without IT headaches. Works on existing school computers and Chromebooks.&lt;/p&gt;
    &lt;p&gt;Unified toolchain for all your equipment. Students learn once, work with multiple machines.&lt;/p&gt;
    &lt;p&gt;Professional-grade CAM and slicing without enterprise licensing costs. Open-source means customizable for research.&lt;/p&gt;
    &lt;p&gt;No software to install or maintain. Patrons use public computers without admin access needed.&lt;/p&gt;
    &lt;p&gt;Full-featured fabrication tools on family computers. No subscription fees eating into budgets.&lt;/p&gt;
    &lt;p&gt;Students continue projects at home on any device. No license restrictions or software gaps.&lt;/p&gt;
    &lt;p&gt;Model slicing, support generation, print settings optimization, multi-material printing, and troubleshooting failed prints.&lt;/p&gt;
    &lt;p&gt;CAM toolpath generation, feeds and speeds, tool selection, roughing and finishing strategies, and machine setup.&lt;/p&gt;
    &lt;p&gt;2D design preparation, power and speed settings, material considerations, layer stacking, and engraving techniques.&lt;/p&gt;
    &lt;p&gt;Mesh editing, boolean operations, model repair, geometry analysis, and preparing models for fabrication.&lt;/p&gt;
    &lt;p&gt;Iterative design, prototyping, material constraints, manufacturing limitations, and optimization strategies.&lt;/p&gt;
    &lt;p&gt;Troubleshooting failed operations, understanding machine limitations, and finding creative solutions to constraints.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;Grid.Space&lt;/cell&gt;
        &lt;cell role="head"&gt;Typical Commercial Software&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Cost&lt;/cell&gt;
        &lt;cell&gt;✓ Free Forever&lt;/cell&gt;
        &lt;cell&gt;Subscription or per-seat licensing&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Installation&lt;/cell&gt;
        &lt;cell&gt;✓ None Required&lt;/cell&gt;
        &lt;cell&gt;Admin rights, IT approval needed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Updates&lt;/cell&gt;
        &lt;cell&gt;✓ Automatic&lt;/cell&gt;
        &lt;cell&gt;Manual updates, version conflicts&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Platform Support&lt;/cell&gt;
        &lt;cell&gt;✓ All OS, Chromebooks&lt;/cell&gt;
        &lt;cell&gt;Windows/Mac only (usually)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Privacy&lt;/cell&gt;
        &lt;cell&gt;✓ 100% Local Processing&lt;/cell&gt;
        &lt;cell&gt;Cloud uploads, accounts required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Home Access&lt;/cell&gt;
        &lt;cell&gt;✓ Full Access&lt;/cell&gt;
        &lt;cell&gt;Limited or requires home licenses&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Offline Use&lt;/cell&gt;
        &lt;cell&gt;✓ After Initial Load&lt;/cell&gt;
        &lt;cell&gt;Varies, often cloud-dependent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Source Code&lt;/cell&gt;
        &lt;cell&gt;✓ Open Source (MIT)&lt;/cell&gt;
        &lt;cell&gt;Proprietary, locked down&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Grid.Space tools support learning objectives across multiple subject areas&lt;/p&gt;
    &lt;p&gt;No sign-ups, no approvals, no waiting.&lt;/p&gt;
    &lt;p&gt;Questions? Email us at admin@grid.space&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46817813</guid><pubDate>Thu, 29 Jan 2026 22:38:57 +0000</pubDate></item><item><title>Employers, please use postmarked letters for job applications</title><link>https://soapstone.mradford.com/employers-use-letters-for-job-applications/</link><description>&lt;doc fingerprint="95650019a0b3a6be"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Employers, use letters for job applications&lt;/head&gt;
    &lt;p&gt;Employers, please consider removing digital job application submissions and instead require job applications to be postmarked using a physical letter.&lt;/p&gt;
    &lt;p&gt;Requiring the cover letter to be handwritten may also be prudent.&lt;/p&gt;
    &lt;head rend="h2"&gt;Yes, hiring can get worse&lt;/head&gt;
    &lt;p&gt;I'm happily employed, but it's possible that maybe one day I'd like to find a different place to work.&lt;/p&gt;
    &lt;p&gt;Job searching has always been Kafkaesque, but each of my forays into job exploration over the past 10 years have been progressively more Kafkaesque than the last.&lt;/p&gt;
    &lt;p&gt;Note that you could use LLMs to automate job applications since LLMs got popular, but the barrier-to-entry was fairly high. Now it's trivially low.&lt;/p&gt;
    &lt;p&gt;I predict a coming barfstorm of job applications, a freefall in hiring success in an already impossible market.&lt;/p&gt;
    &lt;head rend="h2"&gt;Physicality can help&lt;/head&gt;
    &lt;p&gt;On paper (🥁), letters check a lot of boxes to address the problems of LLMs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Raises the cost of an application from $0.00 to ~$0.80, reducing junk applications&lt;/item&gt;
      &lt;item&gt;Physicality increases the difficultly of using LLMs to automate submissions&lt;/item&gt;
      &lt;item&gt;Requiring a handwritten cover-letter greatly raises cost for poor/fraudulent submissions, imposes a comparitively smaller cost for genuine ones &lt;list rend="ul"&gt;&lt;item&gt;Require a Carbonless Imprint of the letter, if you don't want to guess if it was printer-printed handwriting in disguise&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Handwriting is unique- if you're skeptical that an applicant wrote/sent the original letter, witness them write something else and compare&lt;/item&gt;
      &lt;item&gt;Increases the "interest required" in the position, vs spamming application portals&lt;/item&gt;
      &lt;item&gt;No (financial) cost to the employer- update the website text and outline the new requirements&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;You're dreaming, dude&lt;/head&gt;
    &lt;p&gt;I'm not harboring any illusions companies will use letters, but I want to throw it out there at least.&lt;/p&gt;
    &lt;p&gt;What I suspect will happen instead is that companies will further burrow into third-party application systems that offer "AI protection" (which will be a lie, though a comforting one). In the meantime, despair.&lt;/p&gt;
    &lt;head rend="h2"&gt;Aha! But you see...&lt;/head&gt;
    &lt;p&gt;Yeah, it's not perfect:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;people can transcribe LLM text&lt;/item&gt;
      &lt;item&gt;people will just outsource their letters&lt;/item&gt;
      &lt;item&gt;LA-TEN-CY 👏👏👏&lt;/item&gt;
      &lt;item&gt;meatspace and non-digital documents scare me&lt;/item&gt;
      &lt;item&gt;my handwriting is terrible&lt;/item&gt;
      &lt;item&gt;I can't affort stamps&lt;/item&gt;
      &lt;item&gt;nobody is going to do this&lt;/item&gt;
      &lt;item&gt;LLMs have been applying to jobs for 2 years now, nothing has changed&lt;/item&gt;
      &lt;item&gt;LLMs are already going to destroy the world so why does it matter&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Fine sure, none of these are incorrect I guess. But the race-to-the-bottom of online applications will only accelerate as the barrier-to-entry of browser/email/desktop-integrated YOLOed LLMs decreases. Something has got to give.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46817963</guid><pubDate>Thu, 29 Jan 2026 22:49:40 +0000</pubDate></item><item><title>What the Success of Coding Agents Teaches Us about AI Systems in General</title><link>https://softwarefordays.com/post/software-is-mostly-all-you-need/</link><description>&lt;doc fingerprint="ad451c1c6b718169"&gt;
  &lt;main&gt;
    &lt;p&gt;January 8, 2026&lt;/p&gt;
    &lt;head rend="h1"&gt;Software is Mostly All You Need&lt;/head&gt;
    &lt;p&gt;Neural Networks at Buildtime, Software at Runtime&lt;/p&gt;
    &lt;p&gt;Over the last 6 months and the last 6 weeks in particular, AI coding agents have shown to be incredibly capable at writing software. Tasks that traditionally required weeks of human labor can now be done in days if not hours. Even more incredibly, software systems that are designed from the start to harness AI coding agents exhibit many of the characteristics of the neural nets that were integral to their creation in the first place. These AI-native software systems are learned, not designed. Code is the policy, deployment is the episode, and the bug report is the reward signal - well-architected coding agents can drive this loop with little human intervention. Unlike traditional reinforcement learning architectures, they are encoded in CPU instruction sets instead of neural network weights, but they are learned just the same.&lt;/p&gt;
    &lt;p&gt;The success of coding agents and the software systems built thereon carry lessons about where to apply AI agents in general as well. Coding, like many other creative tasks, requires judgment. How best to implement some function with input A and output B; how to name some variable; whether to share some function or implement a new version; etc. Neural networks excel at judgment (more on why below). Yet many of the agentic deployments we are seeing in the wild are against tasks that can be fully specified as explicit instructions. Of course, traditional software excels at executing explicit instructions. Any programming language can be executed on today’s machinery at billions of instructions per second.&lt;/p&gt;
    &lt;p&gt;Coding agents get this exactly right, since by definition they are making a series of judgments when writing code at buildtime and leaving the execution of such code to machines operating at runtime. The best performing architectures follow suit, delegating judgment to neural networks and execution to traditional software, even when the executable artifacts are produced entirely by AI.&lt;/p&gt;
    &lt;head rend="h2"&gt;Some Agents in Practice #&lt;/head&gt;
    &lt;p&gt;Many agentic AI projects are failing[1] — agentic drift, opaque debugging, brittle autonomy.[2][3][4] Meanwhile, Claude Code has driven significant productivity gains by doing something different: it writes code that humans review and deploy, producing artifacts that are durable, version-controlled, and deterministic.&lt;/p&gt;
    &lt;p&gt;These failures and successes reflect a fundamental architectural difference.&lt;/p&gt;
    &lt;head rend="h2"&gt;Judgment and Execution Historically #&lt;/head&gt;
    &lt;p&gt;Humans have historically done two different types of jobs for different reasons, and AI changes each differently.&lt;/p&gt;
    &lt;p&gt;Judgment is fuzzy classification that cannot be specified as explicit rules. This variable should be made private, not public; this handwritten letter is a “B”, not a “P”; this customer complaint is about a refund, not fraud; this image contains a receipt; this element on some unfamiliar page is “the login button.” Humans did these tasks because traditional CPU-based Von Neumann machines simply could not. The rules could not be written down, and even today exist only as learned boundaries in high-dimensional space. Minimization of a loss function via gradient descent in a vastly dimensional space draws these boundaries inside neural networks without confinement to the nouns and verbs of English, C, or even Rust (lol).&lt;/p&gt;
    &lt;p&gt;Execution is discrete logic that can be specified as explicit rules. If complaint type is refund and days since purchase is less than 30, approve; if machine type is CPAP and facility code is X, the SKU is ABC-123; click the element with selector &lt;code&gt;a[href="/login"]&lt;/code&gt;. Humans did these tasks, even though Von Neumann machines theoretically could and are more reliable and faster, because writing and operating software systems that encode these rules was expensive. The investment was not worth the savings not because of any fuzziness inherent to the task.&lt;/p&gt;
    &lt;head rend="h2"&gt;Common Conflations Today #&lt;/head&gt;
    &lt;p&gt;Dominant agent architectures conflate judgment and execution, frequently using neural networks for both. The consensus definition of an agent — “an LLM runs tools in a loop to achieve a goal”[5] — clarifies the mechanism but not the problem space.&lt;/p&gt;
    &lt;p&gt;Frameworks like browser-use and Stagehand embody this conflation. Consider browser-use:&lt;/p&gt;
    &lt;code&gt;agent = Agent(task="Find the top HN post", llm=llm, browser=browser)&lt;/code&gt;
    &lt;p&gt;Or Stagehand:&lt;/p&gt;
    &lt;code&gt;await stagehand.act("click on the stagehand repo");&lt;/code&gt;
    &lt;p&gt;In both cases, the LLM performs judgment (which element is “the stagehand repo”?) and execution (click it, figure out the next step, click that). The entire loop is neural. No durable artifact emerges. The LLM is the runtime.[6][7]&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Execution Requires Traditional Software #&lt;/head&gt;
    &lt;p&gt;Neural networks lack the properties that execution requires: determinism, auditability, and precision on edge cases.&lt;/p&gt;
    &lt;p&gt;Consider this business logic from a system that processes medical equipment orders (from Docflow Labs, my startup):&lt;/p&gt;
    &lt;code&gt;// Fallback 1: Try scriptedMachine field&lt;/code&gt;
    &lt;p&gt;This code handles combinations that may occur once a year — a rare facility, an unusual machine type, a specific classification. The code provides 100% precision even for edge cases. When a billing dispute arises and someone asks why the system chose rental versus purchase for a particular patient, the logic can be traced line by line. It lives in version control and is semantically transparent, deterministic, and auditable.&lt;/p&gt;
    &lt;p&gt;A neural network approximating this function cannot provide these properties. Sparse training data will never cover the combinatorial space. Moreover, it blurs boundaries that business requires to be sharp. And it fails opaquely — gradients and activations offer no affordance for debugging. Decisions in this substrate are semantically opaque, non-deterministic, and untraceable.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Stagehand Example: Half Right #&lt;/head&gt;
    &lt;p&gt;Stagehand’s &lt;code&gt;act("click on the stagehand repo")&lt;/code&gt; correctly implements judgment via a neural network in some sense.[8] Which element on any dynamically chosen page corresponds to the “stagehand repo” cannot be represented in traditional software. There are too many permutations of page layout. The fuzziness of these boundaries is best approached by neural networks in massively multidimensional space minimizing some loss function against many examples.&lt;/p&gt;
    &lt;p&gt;In another sense, however, Stagehand’s architecture is limited. We may know ahead of time which webpage we are attempting a click against and it may change infrequently, requiring only a one-time (or few-time) judgment.&lt;/p&gt;
    &lt;p&gt;Yet Stagehand produces no executable artifact by design. Instead, the LLM returns a selector, which gets cached opaquely outside version control. On cache miss, the LLM re-engages at runtime to re-interpret the instruction, invoking a neural net.&lt;/p&gt;
    &lt;p&gt;A better architecture might still allow the LLM to make a judgment and return a selector, but afford positioning this judgment squarely at buildtime. The selector gets emitted as code into a Playwright script. The script is committed to version control, reviewed, and deployed. On failure — because the site changed and the selector broke — the development process re-engages. An AI agent rewrites the script. Same judgment, different artifact. The selector becomes a semantically transparent piece of the underlying software system, not ephemeral runtime state.[9]&lt;/p&gt;
    &lt;head rend="h2"&gt;A Better Architecture #&lt;/head&gt;
    &lt;p&gt;Neural nets may remain at runtime when tackling judgments that can only be made dynamically at runtime. Every other LLM agent belongs at buildtime accelerating the production of executable software.&lt;/p&gt;
    &lt;code&gt;# Orchestrator: traditional software&lt;/code&gt;
    &lt;p&gt;The workflow orchestrator is traditional software. It calls out to neural networks for judgment tasks: classification, extraction, interpretation — the fuzzy pattern matching that cannot be specified as rules. Then it executes business logic itself, deterministically. The execution paths are explicit, auditable, and version-controlled.&lt;/p&gt;
    &lt;p&gt;This is not a new pattern. Production ML systems already work this way: a model classifies, code acts. What’s new is that AI agents can write the code, dissolving the apparent tradeoff between RPA (deterministic but brittle) and AI agents (adaptive but unpredictable).[10]&lt;/p&gt;
    &lt;head rend="h2"&gt;Development Time Approaching Runtime #&lt;/head&gt;
    &lt;p&gt;Software systems have historically maintained a clear separation between two domains: development (humans writing code, days or weeks) and execution (CPUs running code, nanoseconds). AI coding agents close this gap. The theoretical limit as development time approaches zero is runtime:&lt;/p&gt;
    &lt;p&gt;Even if AI never achieves nanosecond times for writing software, timescales of hours, minutes, and perhaps even seconds allow software systems to adapt to feedback as it arrives.&lt;/p&gt;
    &lt;p&gt;As AI agents get more capable, the distinction between “writing code” and “running code” may dissolve. What emerges resembles reinforcement learning with a different substrate. In traditional RL, a neural network observes state, outputs an action, receives a reward signal, and updates its weights. The network is the adaptive element.&lt;/p&gt;
    &lt;p&gt;Substitute software for the neural network and the structure remains identical. The system observes data — requests, errors, metrics, user complaints. Code executes a response. Feedback arrives. An AI agent updates the code. Same adaptive loop, different computable substrate.&lt;/p&gt;
    &lt;p&gt;The difference in representation matters. Neural networks encode behavior in opaque weight matrices. Software encodes behavior in symbolic, human-readable form. Software can be audited, debugged, and surgically modified if necessary. A single fallback chain can be altered without retraining an entire model and hoping it generalizes correctly. The symbolic substrate preserves the properties that production systems often require: interpretability, debuggability, auditability, and surgical modifiability. When the learned update mechanism provides adaptability, you get the benefits of RL without the costs.&lt;/p&gt;
    &lt;head rend="h2"&gt;Adaptable Software Systems #&lt;/head&gt;
    &lt;p&gt;Ironically, software is still mostly all you need at runtime.&lt;/p&gt;
    &lt;p&gt;Neural networks are best reserved for judgment — the fuzzy tasks we cannot otherwise specify in language — and for buildtime acceleration. Neural networks will not replace traditional software, but rather enable its proliferation into corners of the economy that could benefit from reliable discrete logical execution at a fraction of historical costs.&lt;/p&gt;
    &lt;p&gt;An architecture where neural networks handle runtime judgment, software handles execution, and AI agents accelerate buildtime creates a symbolic substrate that is nonetheless adaptable — auditability, determinism, and precision alongside the adaptability of learned systems.&lt;/p&gt;
    &lt;p&gt;This is what we’re building at Docflow Labs: adaptive systems with a symbolic substrate. If this resonates, say hello!&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Gartner predicts over 40% of agentic AI projects will be canceled by 2027 due to escalating costs, unclear business value, or inadequate risk controls (Gartner). S&amp;amp;P Global reports 42% of companies abandoned most AI initiatives in 2024, up from 17% the prior year (S&amp;amp;P Global). The WebArena benchmark shows best agents achieve ~60% success vs 78% for humans (arXiv). Klarna’s customer service AI was rolled back in 2025 after quality eroded (Bloomberg). ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;IBM, “The hidden risk that degrades AI agent performance,” November 2025. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;“5 Fatal Mistakes: Why Your AI Agent Keeps Failing in Production,” DEV Community, September 2025. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Cognition, “Devin’s 2025 Performance Review: Learnings From 18 Months of Agents At Work,” 2025. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Simon Willison, “I think ‘agent’ may finally have a widely enough agreed upon definition to be useful jargon now,” September 2025. ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stagehand documentation and README describe the framework as “the first browser automation framework built for the AI era—giving you both the predictability of code and the adaptability of AI.” Stagehand ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Stagehand’s own documentation acknowledges this tension. They position themselves against “full agent-based solutions like OpenAI Operator or Anthropic Computer Use” which “promise full automation from just a prompt” but where “developers can end up with unpredictable outcomes.” Stagehand offers more control than pure agents but stops short of buildtime AI. The cached selectors remain opaque, live outside git, and when something breaks in production and the LLM “self-heals” by finding a new selector, production behavior changes without code change, review, or approval. Browserbase Blog ↩︎&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Multimodal, “Agentic AI vs. RPA: What’s the Difference?”, June 2025. ↩︎&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46818154</guid><pubDate>Thu, 29 Jan 2026 23:06:34 +0000</pubDate></item></channel></rss>