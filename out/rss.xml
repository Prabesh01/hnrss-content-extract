<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 30 Nov 2025 15:09:10 +0000</lastBuildDate><item><title>Datacenters in space aren't going to work</title><link>https://taranis.ie/datacenters-in-space-are-a-terrible-horrible-no-good-idea/</link><description>&lt;doc fingerprint="35ba347362661da9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Datacenters in space are a terrible, horrible, no good idea.&lt;/head&gt;
    &lt;p&gt;There is a rush for AI companies to team up with space launch/satellite companies to build datacenters in space. TL;DR: It's not going to work.&lt;/p&gt;
    &lt;p&gt;In the interests of clarity, I am a former NASA engineer/scientist with a PhD in space electronics. I also worked at Google for 10 years, in various parts of the company including YouTube and the bit of Cloud responsible for deploying AI capacity, so I'm quite well placed to have an opinion here.&lt;/p&gt;
    &lt;p&gt;The short version: this is an absolutely terrible idea, and really makes zero sense whatsoever. There are multiple reasons for this, but they all amount to saying that the kind of electronics needed to make a datacenter work, particularly a datacenter deploying AI capacity in the form of GPUs and TPUs, is exactly the opposite of what works in space. If you've not worked specifically in this area before, I'll caution against making gut assumptions, because the reality of making space hardware actually function in space is not necessarily intuitively obvious.&lt;/p&gt;
    &lt;p&gt;Power&lt;/p&gt;
    &lt;p&gt;The first reason for doing this that seems to come up is abundant access to power in space. This really isn't the case. You basically have two options: solar and nuclear. Solar means deploying a solar array with photovoltaic cells – something essentially equivalent to what I have on the roof of my house here in Ireland, just in space. It works, but it isn't somehow magically better than installing solar panels on the ground – you don't lose that much power through the atmosphere, so intuition about the area needed transfers pretty well. The biggest solar array ever deployed in space is that of the International Space Station (ISS), which at peak can deliver a bit over 200kW of power. It is important to mention that it took several Shuttle flights and a lot of work to deploy this system – it measures about 2500 square metres, over half the size of an American football field.&lt;/p&gt;
    &lt;p&gt;Taking the NVIDIA H200 as a reference, the per-GPU-device power requirements are on the order of 0.7kW per chip. These won't work on their own, and power conversion isn't 100% efficient, so in practice 1kW per GPU might be a better baseline. A huge, ISS-sized, array could therefore power roughly 200 GPUs. This sounds like a lot, but lets keep some perspective: OpenAI's upcoming Norway datacenter is intending to house 100,000 GPUs, probably each more power hungry than the H200. To equal this capacity, you'd need to launch 500 ISS-sized satellites. In contrast, a single server rack (as sold by NVIDIA preconfigured) will house 72 GPUs, so each monster satellite is only equivalent to roughly three racks.&lt;/p&gt;
    &lt;p&gt;Nuclear won't help. We are not talking nuclear reactors here – we are talking about radioisotope thermal generators (RTGs), which typically have a power output of about 50W - 150W. So not enough to even run a single GPU, even if you can persuade someone to give you a subcritical lump of plutonium and not mind you having hundreds of chances to scatter it across a wide area when your launch vehicle explosively self-disassembles.&lt;/p&gt;
    &lt;p&gt;Thermal Regulation&lt;/p&gt;
    &lt;p&gt;I've seen quite a few comments about this concept where people are saying things like, "Well, space is cold, so that will make cooling really easy, right?"&lt;/p&gt;
    &lt;p&gt;Um...&lt;/p&gt;
    &lt;p&gt;No.&lt;/p&gt;
    &lt;p&gt;Really, really no.&lt;/p&gt;
    &lt;p&gt;Cooling on Earth is relatively straightforward. Air convection works pretty well – blow air across a surface, particularly one designed to have a large surface area to volume ratio like a heatsink, will transfer heat from the heatsink to the air quite effectively. If you need more power density than can be directly cooled in this way (and higher power GPUs are definitely in that category), you can use liquid cooling to transfer heat from the chip to a larger radiator/heatsink elsewhere. In datacenters on Earth, it is common to set up cooling loops where machines are cooled via chilled coolant (usually water) that is pumped around racks, with the heat extracted and cold coolant returned to the loop. Typically the coolant is cooled via convective cooling to the air, so one way or another this is how things work on Earth.&lt;/p&gt;
    &lt;p&gt;In space, there is no air. The environment is close enough to a hard, total vacuum as makes no practical difference, so convection just doesn't happen. On the space engineering side, we typically think about thermal management, not just cooling. Thing is, space doesn't really have a temperature as-such. Only materials have a temperature. It may come as a surprise, but in the Earth-Moon system the average temperature of pretty much anything is basically the same as the average temperature of Earth, because this is why Earth has that particular temperature. If a satellite is rotating, a bit like a chicken on a rotisserie, it will tend toward having a consistent temperature that's roughly similar to that of the Earth surface. If it isn't rotating, the side pointing away from the sun will tend to get progressively colder, with a limit due to the cosmic microwave background, around 4 Kelvin, just a little bit above absolute zero. On the sunward side, things can get a bit cooked, hitting hundreds of centigrade. Thermal management therefore requires very careful design, making sure that heat is carefully directed where it needs to go. Because there is no convection in a vacuum, this can only be achieved by conduction, or via some kind of heat pump.&lt;/p&gt;
    &lt;p&gt;I've designed space hardware that has flown in space. In one particular case, I designed a camera system that needed to be very small and lightweight, whilst still providing science-grade imaging capabilities. Thermal management was front and centre in the design process – it had to be, because power is scarce in small spacecraft, and thermal management has to be achieved whilst keeping mass to a minimum. So no heat pumps or fancy stuff for me – I went in the other direction, designing the system to draw a maximum of about 1 watt at peak, dropping to around 10% of that when the camera was idle. All this electrical power turns into heat, so if I can draw 1 watt only while capturing an image, then turn the image sensor off as soon as the data is in RAM, I can halve the consumption, then when the image has been downloaded to the flight computer I can turn the RAM off and drop the power down to a comparative trickle. The only thermal management needed was bolting the edge of the board to the chassis so the internal copper planes in the board could transfer any heat generated.&lt;/p&gt;
    &lt;p&gt;Cooling even a single H200 will be an absolute nightmare. Clearly a heatsink and fan won't do anything at all, but there is a liquid cooled H200 variant. Let's say this was used. This heat would need to be transferred to a radiator panel – this isn't like the radiator in your car, no convection, remember? – which needs to radiate heat into space. Let's assume that we can point this away from the sun.&lt;/p&gt;
    &lt;p&gt;The Active Thermal Control System (ATCS) on the ISS is an example of such a thermal control system. This is a very complex system, using an ammonia cooling loop and a large thermal radiator panel system. It has a dissipation limit of 16kW, so roughly 16 H200 GPUs, a bit over the equivalent to a quarter of a ground-based rack. The thermal radiator panel system measures 13.6m x 3.12 m, i.e., roughly 42.5 square metres. If we use 200kW as a baseline and assume all of that power will be fed to GPUs, we'd need a system 12.5 times bigger, i.e., roughly 531 square metres, or about 2.6 times the size of the relevant solar array. This is now going to be a very large satellite, dwarfing the ISS in area, all for the equivalent of three standard server racks on Earth.&lt;/p&gt;
    &lt;p&gt;Radiation Tolerance&lt;/p&gt;
    &lt;p&gt;This is getting into my PhD work now. Assuming you can both power and cool your electronics in space, you have the further problem of radiation tolerance.&lt;/p&gt;
    &lt;p&gt;The first question is where in space?&lt;/p&gt;
    &lt;p&gt;If you are in low Earth orbit (LEO), you are inside the inner radiation belt, where radiation dose is similar to that experienced by high altitude aircraft – more than an airliner, but not terrible. Further out, in mid Earth orbit (MEO), where the GPS satellites live, they are not protected by the Van Allen belts – worse, this orbit is literally inside them. Outside the belts, you are essentially in deep space (details vary with how close to the Sun you happen to be, but the principles are similar).&lt;/p&gt;
    &lt;p&gt;There are two main sources of radiation in space – from our own star, the Sun, and from deep space. This basically involves charged particles moving at a substantial percentage of the speed of light, from electrons to the nuclei of atoms with masses up to roughly that of oxygen. These can cause direct damage, by smashing into the material from which chips are made, or indirectly, by travelling through the silicon die without hitting anything but still leaving a trail of charge behind them.&lt;/p&gt;
    &lt;p&gt;The most common conseqence of this happening is a single-event upset (SEU), where a direct impact or (more commonly) a particle passing through a transistor briefly (approx 600 picoseconds) causes a pulse to happen where it shouldn't have. If this causes a bit to be flipped, we call this a SEU. Other than damage to data, they don't cause permanent damage.&lt;/p&gt;
    &lt;p&gt;Worse is single-event latch-up. This happens when a pulse from a charged particle causes a voltage to go outside the power rails powering the chip, causing a transistor essentially to turn on and stay on indefinitely. I'll skip the semiconductor physics involved, but the short version is that if this happens in a bad way, you can get a pathway connected between the power rails that shouldn't be there, burning out a gate permanently. This may or may not destroy the chip, but without mitigation it can make it unusable.&lt;/p&gt;
    &lt;p&gt;For longer duration missions, which would be the case with space based datacenters because they would be so expensive that they would have to fly for a long time in order to be economically viable, it's also necessary to consider total dose effects. Over time, the performance of chips in space degrades, because repeated particle impacts make the tiny field-effect transistors switch more slowly and turn on and off less completely. In practice, this causes maximum viable clock rates to decay over time, and for power consumption to increase. Though not the hardest issue to deal with, this must still be mitigated or you tend to run into a situation where a chip that was working fine at launch stops working because either the power supply or cooling has become inadequate, or the clock is running faster than the chip can cope with. It's therefore necessary to have a clock generator that can throttle down to a lower speed as needed – this can also be used to control power consumption, so rather than a chip ceasing to function it will just get slower.&lt;/p&gt;
    &lt;p&gt;The next FAQ is, can't you just use shielding? No, not really, or maybe up to a point. Some kinds of shielding can make the problem worse – an impact to the shield can cause a shower of particles that then cause multiple impact at once, which is far harder to mitigate. The very strongest cosmic rays can go through an astonishing amount of solid lead – since mass is always at a premium, it's rarely possible to deploy significant amounts of shielding, so radiation tolerance must be built into the system (this is often described as Radiation Hardness By Design, RHBD).&lt;/p&gt;
    &lt;p&gt;GPUs and TPUs and the high bandwidth RAM they depend on are absolutely worst case for radiation tolerance purposes. Small geometry transistors are inherently much more prone both to SEUs and latch-up. The very large silicon die area also makes the frequency of impacts higher, since that scales with area.&lt;/p&gt;
    &lt;p&gt;Chips genuinely designed to work in space are taped out with different gate structures and much larger geometries. The processors that are typically used have the performance of roughly a 20-year-old PowerPC from 2005. Bigger geometries are inherently more tolerant, both to SEUs and total dose, and the different gate topologies are immune to latch up, whilst providing some degree of SEU mitigation via fine-grained redundancy at the circuit level. Taping out a GPU or TPU with this kind of approach is certainly possible, but the performance would be a tiny fraction of that of a current generation Earth-based GPU/TPU.&lt;/p&gt;
    &lt;p&gt;There is a you-only-live-once (my terminology!) approach, where you launch the thing and hope for the best. This is commonplace in small cubesats, and also why small cubesats often fail after a few weeks on orbit. Caveat emptor!&lt;/p&gt;
    &lt;p&gt;Communications&lt;/p&gt;
    &lt;p&gt;Most satellites communicate with the ground via radio. It is difficult to get much more than about 1Gbps reliably. There is some interesting work using lasers to communicate with satellites, but this depends on good atmospheric conditions to be feasible. Contrasting this with a typical server rack on Earth, where 100Gbps rack-to-rack interconnect would be considered at the low end, and it's easy to see that this is also a significant gap.&lt;/p&gt;
    &lt;p&gt;Conclusions&lt;/p&gt;
    &lt;p&gt;I suppose this is just about possible if you really want to do it, but I think I've demonstrated above that it would firstly be extremely difficult to achieve, disproportionately costly in comparison with Earth-based datacenters, and offer mediocre performance at best.&lt;/p&gt;
    &lt;p&gt;If you still think this is worth doing, good luck, space is hard. Myself, I think it's a catastrophically bad idea, but you do you.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46087616</guid><pubDate>Sat, 29 Nov 2025 14:05:53 +0000</pubDate></item><item><title>Learning Feynman's Trick for Integrals</title><link>https://zackyzz.github.io/feynman.html</link><description>&lt;doc fingerprint="dd489f0901f7d271"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Feynman's Trick&lt;/head&gt;
    &lt;head rend="h3"&gt;a.k.a. Differentiation under the Integral Sign &amp;amp; Leibniz Integral Rule&lt;/head&gt;
    &lt;p&gt;Among a few other integral tricks and techniques, Feynman's trick was a strong reason that made me love evaluating integrals, and although the technique itself goes back to Leibniz being commonly known as the Leibniz integral rule, it was Richard Feynman who popularized it, which is why it is also referred to as Feynman's trick. Here's an excerpt from his book, Surely You're Joking, Mr. Feynman:&lt;/p&gt;
    &lt;p&gt;"One thing I never did learn was contour integration. I had learned to do integrals by various methods shown in a book that my high school physics teacher Mr. Bader had given me.&lt;/p&gt;
    &lt;p&gt;One day he told me to stay after class. "Feynman," he said, "you talk too much and you make too much noise. I know why. You're bored. So I'm going to give you a book. You go up there in the back, in the corner, and study this book, and when you know everything that's in this book, you can talk again."&lt;/p&gt;
    &lt;p&gt;So every physics class, I paid no attention to what was going on with Pascal's Law, or whatever they were doing. I was up in the back with this book: Advanced Calculus, by Woods. Bader knew I had studied Calculus for the Practical Man a little bit, so he gave me the real works -- it was for a junior or senior course in college. It had Fourier series, Bessel functions, determinants, elliptic functions -- all kinds of wonderful stuff that I didn't know anything about.&lt;/p&gt;
    &lt;p&gt;That book also showed how to differentiate parameters under the integral sign -- it's a certain operation. It turns out that's not taught very much in the universities; they don't emphasize it. But I caught on how to use that method, and I used that one damn tool again and again. So because I was self-taught using that book, I had peculiar methods of doing integrals.&lt;/p&gt;
    &lt;p&gt;The result was, when guys at MIT or Princeton had trouble doing a certain integral, it was because they couldn't do it with the standard methods they had learned in school. If it was contour integration, they would have found it; if it was a simple series expansion, they would have found it. Then I come along and try differentiating under the integral sign, and often it worked. So I got a great reputation for doing integrals, only because my box of tools was different from everybody else's, and they had tried all their tools on it before giving the problem to me."&lt;/p&gt;
    &lt;p&gt;For me, employing this trick felt like I was using cheat codes to deal with integrals. At the same time, it enabled a lot of creativity and wishful thinking, which transformed integrals into puzzles. Unfortunately, this also means that there is no clear path on how and when to use this technique. In addition, what Feynman wrote still applies today since the method isn't taught much, if at all, in universities. Therefore, the trick can seem obscure and difficult to grasp for newcomers.&lt;/p&gt;
    &lt;p&gt;In the following section, we will embark on a journey to develop some rules of thumb to have at our disposal when using Feynman's trick. These are merely some heuristics that I tend to use, so deviating from them can be perfectly acceptable. However, I hope that they can provide a path to follow when nothing obvious or intuitive occurs when someone tries to use this trick, or even better, so that they can serve as motivation for someone to start using the method.&lt;/p&gt;
    &lt;head rend="h2"&gt;Hello, World!&lt;/head&gt;
    &lt;p&gt;Feynman already provided a significant hint about the trick when he mentioned differentiating under the integral sign, which is also an alternative name for the technique. More explicitly, if \(f(x,t)\) and \(\frac{\partial f(x,t)}{\partial t}\) is continuous with respect to both variables over the \([a,b]\) interval, then the following holds:&lt;/p&gt;
    &lt;p&gt;This is nice, but not so useful by itself since it doesn't say anything about how and when to apply it. Moreover, learning is not a spectator sport and one has to get their hands dirty as there are no shortcuts to it. Take for example chess, most people could read and understand the rules in a few minutes, however, if they would go on to play a game then most likely they would get stomped by a more experienced player. This is because the other player, through practice, learned some strategies to use when playing.&lt;/p&gt;
    &lt;p&gt;Thus, with the goal to develop some strategies here as well, we will dive straight into action and approach Feynman's trick using practical examples. As a "Hello, World!" introduction, let's take a look at the following integral:&lt;/p&gt;
    &lt;p&gt;You are encouraged to try and evaluate the integral using basic methods, but the logarithm being in the denominator makes this integral quite stubborn to deal with. Feynman's trick aims to get rid of this issue by differentiating under the intgeral sign, with respect to a parameter, in order to obtain an integral that is easier to evaluate.&lt;/p&gt;
    &lt;p&gt;Unfortunately in the integral from above we lack a parameter, therefore the first step is to parameterise the integral, which can even mean introducing a whole function, but for this example we will simply consider:&lt;/p&gt;
    &lt;p&gt;Keep in mind that our original integral is just \(I(1)\). Also, surely we could've placed a parameter in many different places, such as:&lt;/p&gt;
    &lt;p&gt;However, the main idea behind the trick is to obtain an integral that we can evaluate easier, after differentiating with respect to the new parameter. Let's put this in action and see what happens to \(I(t)\).&lt;/p&gt;
    &lt;p&gt;Notice how easy it was to evaluate the integral \(I'(t)=\int_0^1x^tdx\) from above, had we kept \(I(a)\), \(I(b)\) or \(I(c)\) the things wouldn't had simplified at all after differentiating, and most significantly is that we would still have the \(\ln x\) in the denominator, a thing which made the integral hard to deal with in the first place.&lt;/p&gt;
    &lt;p&gt;We can already sense that the following might be an important question in the future: How to parameterise the integral when using Feynman's Trick?&lt;/p&gt;
    &lt;p&gt;We will worry about that a bit later, for now let's finish the integral as we only found \(I'(t)\). Since we are looking to find \(I(1)\) we need to integrate \(I'(t)\) back and set \(t=1\) in order to arrive there. Here it's useful to recall that:&lt;/p&gt;
    &lt;p&gt;For us, \(f(x)\) is just \(I(t)\) in the above expression. Luckily \(I(0)=0\), and as we are looking for \(I=I(1)\) we have:&lt;/p&gt;
    &lt;p&gt;So that is the big picture of Feynman's trick - we have an integral that is hard to evaluate in it's original form, therefore by differentiating under the integral sign we attempt to transform the integral so that it can be easier integrated, and in the end we go back to undo the differentiation step.&lt;/p&gt;
    &lt;head rend="h2"&gt;The parameter&lt;/head&gt;
    &lt;p&gt;As emphasized above, the main goal of the technique is to obtain an integral that is easier to evaluate after differentiating with respect to a parameter, and one issue is that it is not always obvious how to parameterise the integral. In order to make things more intuitively we will play around with the integral from below.&lt;/p&gt;
    &lt;p&gt;The most annoying thing is the logarithm, so if we get rid of it everything should be straightforward. There are a few parameter possibilities which makes sense to consider, namely:&lt;/p&gt;
    &lt;p&gt;With the first one we are out of luck, as differentiating with respect to \(a\) gives:&lt;/p&gt;
    &lt;p&gt;Therefore, if we would try to go back to what we're looking, which is \(I=I(1)-I(0)\), we would end up with \(I=I+\text{other stuff}\). This cancels out \(I\) and we wouldn't be able to recover it. Unfortunately, there's no magic formula that tells a priori whether placing a parameter in a specific place would succeed or fail in evaluating an integral - and sometimes we are simply unlucky.&lt;/p&gt;
    &lt;p&gt;In contrast, things work out nicely with the second choice from above.&lt;/p&gt;
    &lt;p&gt;Again, we are looking to find \(I(1)\), and as \(I(0) = 0\), we have:&lt;/p&gt;
    &lt;p&gt;This works, but we can do even better. Looking at the Hello, World! integral we can see that there we simplified the logarithm in the denominator while performing \(\frac{\partial}{\partial t}x^t\). This is also the first thing that I always attempt to look for when using this technique - namely, to simplify something from the integrand which is independent to the parameter when differentiating. Surely for the current integral we got rid of the logarithm, but the denominator remained intact.&lt;/p&gt;
    &lt;p&gt;In short this will be our first rule of thumb: if possible, place the parameter so that something from the integral, which is not related to the parameter, gets simplified.&lt;/p&gt;
    &lt;p&gt;In order to achieve this with our integral we would need to get rid of \(1+x^2\), and by using \(\ln x=\frac12\ln(x^2)\) we can rewrite the integral as:&lt;/p&gt;
    &lt;p&gt;Finally, in this form it's more natural to place the parameter so that it simplifies \(1+x^2\) when differentiating with respect to \(t\), namely we can consider:&lt;/p&gt;
    &lt;p&gt;Like for \(I(b)\) we are looking to find \(I(1)\), however here \(I(0)\) is equal to \(\frac12\int_0^1\frac{\ln(2x)}{1+x^2}dx\) not \(0\).&lt;/p&gt;
    &lt;p&gt;For this specific integral we only avoided performing partial fractions so there wasn't really a big improvement by simplifying the denominator. However I want to emphasize the importance of this because it will make things come way more natural when deciding where place the parameter. Of course, in case there's not an appropiate or immediate way to achieve this, it's perfectly fine to place the parameter elsewhere too.&lt;/p&gt;
    &lt;p&gt;As mentioned previously, practicing is the best approach to get along with new techniques, therefore below are more integrals to evaluate alongside some hidden steps in case those will be needed. However, I strongly recommend to try and deal with the integrals before looking at any hints, and only check them afterwards for correctness.&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^\frac{\pi}{2} \frac{\ln(1-t\sin x)}{\sin x}dx \Rightarrow I'(t)= -\frac{2\arctan\left(\sqrt{\frac{1+t}{1-t}}\right)}{\sqrt{1-t^2}}\] This should lead to: \[\int_0^\frac{\pi}{2} \frac{\ln(1-\sin x)}{\sin x}dx = I(1) - I(0)=\int_0^1 I'(t) dt \overset{\sqrt{\frac{1-t}{1+t}}=x} = -\frac{3\pi^2}{8}\] But it would be even better if the integral would be parameterised as: \[I(t)=\int_0^\frac{\pi}{2} \frac{\ln(1-\sin t\sin x)}{\sin x}dx\] That is because usually when having trigonometric functions, parameterising the integral with another trigonometric function, leads to a more smoother result.&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^1 \frac{\ln(1-t(x-x^2))}{x-x^2}dx\Rightarrow I'(t) = \frac{4\arctan\left(\sqrt{\frac{t}{4-t}}\right)}{\sqrt{t(4-t)}}\] This should lead to: \[I(1)=\int_0^1 \frac{\ln(1-x+x^2)}{x-x^2}dx = I(1) - I(0) = \int_0^1 I'(t)dt \overset{\sqrt{\frac{4-t}{t}}= x}= -\frac{\pi^2}{9}\]&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^\frac{\pi}{2} \frac{\arctan(t\sin x)}{\sin x}dx\Rightarrow I'(t)=\frac{\pi}{2\sqrt{1+t^2}}\] This should lead to: \[I(1)=\int_0^\frac{\pi}{2} \frac{\arctan(t\sin x)}{\sin x}dx = I(1)-I(0) = \int_0^1 I'(t)dt = \frac{\pi}{2}\ln(1+\sqrt 2)\] It will also work if the integral is parameterised as: \[I(t)=\int_0^\frac{\pi}{2} \frac{\arctan(\tan t\sin x)}{\sin x}dx\] However, in this case the first variant is simple enough to integrate back.&lt;/p&gt;
    &lt;p&gt;Consider introducing the following parameter: \[I(t)=\int_0^\infty x^2e^{-\left(4x^2+\frac{t}{x^2}\right)}dx\Rightarrow I'(t)=-\frac{\sqrt \pi}{4} e^{-4\sqrt t}\] Where the above result follows by using Glasser's master theorem alongside the Gaussian integral. This should lead to: \[\int_0^\infty x^2e^{-\left(4x^2+\frac{9}{x^2}\right)}dx = I(9)- I(0) + I(0) = \int_0^9 I'(t) dt +\frac{\sqrt \pi}{32}=\frac{13}{32}\frac{\sqrt \pi}{e^{12}}\]&lt;/p&gt;
    &lt;p&gt;Consider parameterising the integral as: \[I(t)=\frac12\int_0^1\frac{\ln(1-t(1-x^2))}{1-x^2}dx\Rightarrow I'(t)=\frac{\arctan\left(\sqrt{\frac{t}{1-t}}\right)}{2\sqrt{t(1-t)}}\] This should lead to: \[\int_0^1 \frac{\ln x}{1-x^2}dx = I(1)- I(0) = \int_0^1 I'(t)dt \overset{\sqrt{\frac{1-t}{t}} = x}= -\frac{\pi^2}{8}\]&lt;/p&gt;
    &lt;p&gt;Consider parameterising the integral as: \[I(t)=\int_0^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\Rightarrow I'(t) = -\frac{\sqrt \pi}{2\sqrt t}e^{-t}\] This should lead to: \[\int_0^\infty \frac{e^{-x^2}}{1+x^2}dx = e\left(I(1)-I(\infty)\right) = -e\int_1^\infty I'(t)dt= \frac{\pi e}{2}\operatorname{erfc}(1)\] Where \(\operatorname{erfc}(x)\) is the complementary error function.&lt;/p&gt;
    &lt;p&gt;Since \(1-x^2+x^4=(1+x^2)^2-3x^2\), consider parameterising the integral as: \[I(t)=\int_0^\infty \frac{\ln\left(\frac{t(1+x^2)^2-3x^2}{(1-x^2)^2}\right)}{(1+x^2)^2}dx\Rightarrow I'(t)=\frac{\pi}{2\sqrt{t(4t-3)}}\] And in order to go back it should be observed that \(\frac34(1+x^2)^2-3x^2=\frac34(1-x^2)^2\). \[\int_0^\infty \frac{\ln\left(\frac{1-x^2+x^4}{(1-x^2)^2}\right)}{(1+x^2)^2}dx=I(1)- I\left(\frac34\right)+ I\left(\frac34\right)\] \[=\int_\frac34^1 I'(t)dt + \frac{\pi}{4}\ln\left(\frac{3}{4}\right) = \frac{\pi}{2}\ln\left(\frac32\right)\]&lt;/p&gt;
    &lt;head rend="h2"&gt;Accelerated Feynman's trick&lt;/head&gt;
    &lt;p&gt;The previous chapter emphasized to parameterise integrals so that something from the integral, which is not related to the parameter, gets simplified when differentiating (if possible). However there are times when even though we can introduce a parameter to accomplish that, it wouldn't be enough to finish the integral.&lt;/p&gt;
    &lt;p&gt;In this chapter we will look at a different way to obtain this simplification. Let's start by looking at a modified version of an integral that was previously given as an exercise.&lt;/p&gt;
    &lt;p&gt;With \(\int_{-\infty}^\infty \frac{e^{-x^2}}{1+x^2}dx\) it was quite direct to parameterise the integral as \(\int_{-\infty}^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\) since it simplifies the denominator, however the similar way to do that for our integral, \(\int_{-\infty}^\infty \frac{e^{-x^2-t(1+x^4)}}{1+x^4}dx\), doesn't seem to work as it complicates things a bit too much.&lt;/p&gt;
    &lt;p&gt;There is however a way to simplify the denominator and in the same time to obtain a decent integral afterwards. Without getting into too much details I will parameterise the integral as:&lt;/p&gt;
    &lt;p&gt;This will seem obscure, but fear not as we will never use this approach again. The whole point is to simplify \(1+x^4\), and the above function was created explicitly to achieve that, as \(\frac{\partial}{\partial t}e^{-tx^2}(x^2\sin t+\cos t)\) is \(-(1+x^4)e^{-tx^2}\sin t\). Note that even though we introduced a couple other terms, those aren't disturbing.&lt;/p&gt;
    &lt;p&gt;Here we are looking to find \(I=I(0)\), and we also have \(I(\infty)=0\), therefore:&lt;/p&gt;
    &lt;p&gt;Where \(S(x)\) and \(C(x)\) are the Fresnel integrals. However, the approach is important here, not the result itself.&lt;/p&gt;
    &lt;p&gt;We can avoid the parametrisation from above by directly using \(\frac{1}{1+x^4}=\int_0^\infty e^{-tx^2}\sin t \, dt\), and then switch to double integrals, or put in other words: employ the accelerated Feynman's trick (in which we skip the usual parameterisation step).&lt;/p&gt;
    &lt;p&gt;The rest goes exactly as with the previous method, as all we did here was to skip differentiation step and instead we switched to double integrals.&lt;/p&gt;
    &lt;p&gt;A natural question that arises here is how did \(\frac{1}{1+x^4}=\int_0^\infty e^{-tx^2}\sin t\, dt\) appear? Or even better, how can someone come up with similar results for other integrals? In the case from above, simply the Laplace transform of the sine function was used, however in general it's useful to have a list of such identities. There are tables of integral results that can be used - for example: Table of Integrals, Series, and Products by Gradshteyn and Ryzhik - but alternatively one can build up their own list of results which tend to appear often while evaluating other integrals.&lt;/p&gt;
    &lt;p&gt;Let's conclude this chapter by evaluating one of the most popular integrals that appears when Feynman's trick gets into the conversation.&lt;/p&gt;
    &lt;p&gt;Since \(\int_0^\infty e^{-xt} dt = \frac{1}{x}\), we can make use of this to rewrite the integral as:&lt;/p&gt;
    &lt;p&gt;Alternatively, we can also consider the parameter version of this integral, \(\int_0^\infty \frac{\sin x}{x}e^{-xt}dx\), however I feel like switching to double integrals is way more intuitively.&lt;/p&gt;
    &lt;p&gt;It might be worth to highlight again that this method should be used preferable when parameterising the integral leads to nowhere. For the above integral, the natural introduction of \(\int_0^\infty \frac{\sin(tx)}{x}dx\) unfortunatelly does fail, as we obtain a divergent integral after differentiating under the integral sign.&lt;/p&gt;
    &lt;p&gt;Like in the previous chapter below are more integrals alongside some hints in order to practice with the accelerated variation of Feynman's trick. However in this case I do recommend to peek at hints faster in case nothing obvious comes to mind, and afterwards to attempt and understand why the mentioned identity can be used.&lt;/p&gt;
    &lt;p&gt;Start by substituting \(x^2\to x\) and then switch to double integrals using: \[\int_0^\infty e^{-xt^2}dt = \frac{\sqrt \pi}{2\sqrt x}\] Where the latter result is due to the Gaussian integral. Also, this integral is one particular case of the Fresnel integral.&lt;/p&gt;
    &lt;p&gt;Switch directly to double integrals by using: \[\int_0^1 \frac{\ln t}{t-\frac{1}{x}}dt = \operatorname{Li}_2(x)\]&lt;/p&gt;
    &lt;p&gt;Switch to double integrals by using the following result: \[\int_0^x \frac{\arctan t}{1+xt}dt = \frac{\arctan x \ln(1+x^2)}{2x}\]&lt;/p&gt;
    &lt;p&gt;Consider switching to double integrals with: \[\frac{x}{\pi^2+x^2}=\Im\left(-\frac{1}{\pi+ix}\right)=-\Im\int_0^\infty e^{-(\pi+ix)t}dt\] It's also really useful to try and see what happens when the Laplace transform of the cosine function is used instead, or the equivalent: \[\frac{x}{\pi^2+x^2}=\Re\left(\frac{1}{i\pi+x}\right)=\Re\int_0^\infty e^{-(i\pi+x)t}dt\]&lt;/p&gt;
    &lt;p&gt;Consider switching to double integrals using: \[\operatorname{Ci}^2(x)+\operatorname{si}^2(x)=\int_0^\infty \frac{e^{-xy}\ln(1+y^2)}{y}dy\]&lt;/p&gt;
    &lt;p&gt;Above \(\operatorname{Li}_2(x)\) denotes the dilogarithm function and \(\operatorname{Ci}(x)\), \(\operatorname{si}(x)\) are the cosine and the sine integral functions, defined as:&lt;/p&gt;
    &lt;head rend="h2"&gt;More Feynman's trick variants&lt;/head&gt;
    &lt;p&gt;We already got familiar with a popular version of Feynman's trick in the previous chapter. Similarly, now we will take a look at other interesting variants of Feynman's trick, which although might appear less often, they can still help to expand the applicability of the technique.&lt;/p&gt;
    &lt;head rend="h3"&gt;Differentiating under the integral sign&lt;/head&gt;
    &lt;p&gt;We will start by taking a look at a much simpler case of Feynman's trick, namely, in the situation when it would be enough to simply differentiate under the integral sign without performing that "undo" step to integrate back.&lt;/p&gt;
    &lt;p&gt;As a small note, it's true that "differentiating under the integral sign" tends to be used as an alternative name for Feynman's trick, however I prefer to keep this for the variant where only the differentiating process takes part, or as mentioned above, when there's no need to integrate back the result, and the name describes quite literally what we are doing.&lt;/p&gt;
    &lt;p&gt;Let's make this more clear by looking at the following integral:&lt;/p&gt;
    &lt;p&gt;We are already aware from the Hello, World! integral how \(\ln x\) can be simplified, since \(\frac{\partial}{\partial a}x^a = x^a \ln x\). However, by introducing the parameter in that original form as \(x^a \ln^2 x\), we would just produce a third logarithm, so that's going in the opposite direction.&lt;/p&gt;
    &lt;p&gt;Fortunatelly, if we take a step back, we can observe that after we find the result of \(\int_0^1 x^a dx\), then differentiating it w.r.t.\(a\) would give us as many logarithms as we want. So, let's put that integral to use.&lt;/p&gt;
    &lt;p&gt;Of course the integral itself was quite simple this time, however the important part that should be highlighted is that not always we need to perform that "undo" step after differentiating under the integral sign - and sometimes knowing a general integral result can provide us more useful integrals by differentiating it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feynman's trick &amp;amp; indefinite integrals&lt;/head&gt;
    &lt;p&gt;Further, we will take a look at how Feynman's trick can be applied to indefinite integrals. Let's consider:&lt;/p&gt;
    &lt;p&gt;In this form it makes no sense to differentiate the integral with respect to any parameter, but we can extend the integral with temporary bounds by writing:&lt;/p&gt;
    &lt;p&gt;After this we can go on apply Feynman's trick, however, first we are going get rid of the square root via the substitution \(\frac{1}{\sqrt x}\to x\).&lt;/p&gt;
    &lt;p&gt;Here, we can notice that the derivative of \(ax-\frac{b}{x}\) is \(a+\frac{b}{x^2}\) so it would be quite helpful if we had that additional term. In the same time if we differentiate the integrand with respect to \(b\) we'll produce \(a-\frac{b}{x^2}\), which is really useful as \((ax-b/x)^2\) is equal to \((ax+b/x)^2+4ab\) and the derivative of \(ax+\frac{b}{x}\) is \(a-\frac{b}{x^2}\). So let's differentiate as mentioned above:&lt;/p&gt;
    &lt;p&gt;Where \(\operatorname{erfc}(x)\) is the complementary error function. Now we'll go back to \(I(a,b,t)\), but we should be careful to replace the dummy variable \(b\), with something else as the \(b\) parameter does also appear in the bounds.&lt;/p&gt;
    &lt;p&gt;Or for the indefinite integral, this would lead to:&lt;/p&gt;
    &lt;head rend="h3"&gt;Feynman's trick &amp;amp; power series&lt;/head&gt;
    &lt;p&gt;Next, we will take a look at how to combine Feynman's trick with power series. For this we are going to look at:&lt;/p&gt;
    &lt;p&gt;We are already got familiar with what to do when there is a logarithm in the denominator as we saw that we can get rid of them by using \(\frac{d}{dt} x^t = x^t\ln x\), however here also the \(1-xy\) term appears. In order to solve this issue we'll make use of the geoemtric series, namely \(\frac{1}{1-x}=\sum_{n=0}^\infty x^n\), but we will expand into series a bit later and for now continue with the following integral:&lt;/p&gt;
    &lt;p&gt;Now we have to to get back to \(I(n)\):&lt;/p&gt;
    &lt;p&gt;And finally, we'll put the geometric series to use.&lt;/p&gt;
    &lt;p&gt;So the result is simply \(1-2\gamma\), where \(\gamma\) is the Euler-Mascheroni constant.&lt;/p&gt;
    &lt;head rend="h3"&gt;Feynman's trick &amp;amp; differential equations&lt;/head&gt;
    &lt;p&gt;In what's to come we are going to take a look at a combination between Feynman's trick and differential equations. Let's consider the following integral:&lt;/p&gt;
    &lt;p&gt;We can start by parameterising the cosine function and then employ the accelerated Feynman's trick:&lt;/p&gt;
    &lt;p&gt;We haven't made much progress above, since we simply arrived at another integral with \(x\sin(tx)\) instead of \(\cos(tx)\), thus complexity is the same. However, as \(\frac{\partial}{\partial t}\cos(tx)\) is \(x\sin(tx)\), differentiating \(I(t)\) gives us a differential equation to work with, namely:&lt;/p&gt;
    &lt;p&gt;As a small note for the starting step, although employing the accelerated Feynman's trick was rather obvious as to get rid of the denominator, the additional introduction of the \(t\) parameter might be weird first. However performing the same steps without this parameter gives us:&lt;/p&gt;
    &lt;p&gt;Which indicates that one might put to use the fact that \(I(1)=-I'(1)\), by adding the additional \(t\) parameter.&lt;/p&gt;
    &lt;head rend="h3"&gt;Generalizing Feynman's trick&lt;/head&gt;
    &lt;p&gt;So far we've seen the Feynman's trick applied only when the parameter was inside the integrand, however it can also be used when the bounds are parameterised as well. More generally, the following holds:&lt;/p&gt;
    &lt;p&gt;We'll put this to use with the integral from below.&lt;/p&gt;
    &lt;p&gt;Above we can see that the same \(\sqrt 2\) appears in both the lower bound and the \(\operatorname{arccosh}\) function, so we'll parameterise the integral as:&lt;/p&gt;
    &lt;p&gt;We're looking to find \(I=I\left(\sqrt 2\right)\), and since \(I\left(1\right)=0\), we have:&lt;/p&gt;
    &lt;head rend="h3"&gt;Generating integrals using Feynman's trick&lt;/head&gt;
    &lt;p&gt;Now we'll take a look at a fancier way to use Feynman's trick, especially in order to generate new integrals, for this we're considering:&lt;/p&gt;
    &lt;p&gt;Note that we are not trying to evaluate the above integral, instead we are simply using it in order to build up new integrals with the result that follows after differentiating w.r.t. \(t\).&lt;/p&gt;
    &lt;p&gt;We also have that \(I(\pi)=-\frac{\pi^2}{4}\) and \(I(0)=\frac{\pi^2}{8}\), therefore:&lt;/p&gt;
    &lt;p&gt;In retrospect, this integral also appeared as an exercise in the second chapter, and with the same suggestion from there, we can evaluate the integral by applying Feynman's trick to:&lt;/p&gt;
    &lt;p&gt;Admittedly, following this parameterisation is much more intuitevely than what we've shown with the new variation, however it's also useful to have this trick in the bag.&lt;/p&gt;
    &lt;p&gt;To keep the practice going, underneath are listed some integrals that can be evaluated with one version of Feynman's trick described in this chapter.&lt;/p&gt;
    &lt;p&gt;Start by showing that: \[I(t)=\int_1^\infty \int_1^\infty e^{-t(x+y)}dxdy = \left(\frac{e^{-t}}{t}\right)^2\] Then differentiate both sides two times with respect to \(t\) and set \(t=1\).&lt;/p&gt;
    &lt;p&gt;Differentiate four times with respect to \(n\) the following extended indefinite integral: \[ I(n,t) = \int_0^t \cos(nx) dx \]&lt;/p&gt;
    &lt;p&gt;Combine the geometric series \(\sum\limits_{n=0}^\infty (-1)^n x^n = \frac{1}{1+x}\) with: \[I(t)=\int_0^1 \int_0^1 \frac{(xy)^t}{\ln(xy)}dxdy\]&lt;/p&gt;
    &lt;p&gt;Solve the resulting differential equation after differentiating twice the following integral: \[ I(t) = \int_0^\infty \frac{\sin^2 (tx)}{x^2(1+x^2)}dx \]&lt;/p&gt;
    &lt;p&gt;Split the integral in two parts, then substitute \(x\to tx\) and respectively \(x\to \frac{x}{t}\) in the resulting integrals to obtain: \[I(t)= \int_0^1 \ln x \left(\frac{1}{t+x}+\frac{1}{\frac{1}{t}+x}\right)dx = \int_0^\frac{1}{t} \frac{\ln(tx)}{1+x}dx + \int_0^t \frac{\ln\left(\frac{x}{t}\right)}{1+x}dx\] Now employ the Feynman's trick (in the generalized variant).&lt;/p&gt;
    &lt;p&gt;In disguise this exercise is a reformulation of the following identity: \[ \operatorname{Li}_2(-t)+\operatorname{Li}_2\left(-\frac{1}{t}\right) = -\frac{\pi^2}{6} - \frac{1}{2}\ln^2 t \] Where \(\operatorname{Li}_2(x)\) is the dilogarithm function.&lt;/p&gt;
    &lt;p&gt;This integral can be generated starting from: \[I(t)=\int_0^\infty \frac{e^{-t(1+x^2)}}{1+x^2}dx\Rightarrow I'(t)=-e^{-t}\int_0^\infty e^{-tx^2} dx\overset{\sqrt t x \to x}= -\frac{e^{-t}}{\sqrt t}\int_0^\infty e^{-x^2} dx\] Afterwards it should be observed that \(I(0)=\frac{\pi}{2}\) and \(I(\infty)=0\), therefore: \[\frac{\pi}{2} = \int_0^\infty \frac{e^{-t}}{\sqrt t} dt \int_0^\infty e^{-x^2} dx \overset{t\to t^2} = 2\left(\int_0^\infty e^{-x^2}dx\right)^2 \]&lt;/p&gt;
    &lt;head rend="h2"&gt;Feynman's trick in practice&lt;/head&gt;
    &lt;p&gt;In this last chapter we'll dive into some "real-world" integrals and observe how Feynman's trick can be adapted for them. Additionally, we will also attempt to build up some heuristics that'll help us manipulate the integrals up to a point where we can introduce an useful parameter. This is due to the fact that with most of the previous examples we had the luxury to parameterise the integrals as they appeared, however often this might not be the case.&lt;/p&gt;
    &lt;head rend="h3"&gt;Breaking the rules&lt;/head&gt;
    &lt;p&gt;In the second chapter we've observed how parameterising the integral so that it also simplifies some parts of the integral can make things more intuitively. However, we can always look for better, especially when our approach doesn't seem elegant enough.&lt;/p&gt;
    &lt;p&gt;Let's see how can we break this rule with the following integral:&lt;/p&gt;
    &lt;p&gt;As with our first rule of thumb it's straightforward to introduce the new parameter as:&lt;/p&gt;
    &lt;p&gt;You are encouraged to proceed forward with the above integral, however computing \(I'(a)\) might not give the most pleasant result. So to overcome this, we'll manipulate the integral a bit before parametersing it.&lt;/p&gt;
    &lt;p&gt;One extremely useful substitution (which deserves its own special chapter) is \(x\to \frac{1-x}{1+x}\). This one is great here especially since it has the following property (among many others):&lt;/p&gt;
    &lt;p&gt;Above, although we could have introduced a simple parameter, for a smoother result we parameterised the integral as:&lt;/p&gt;
    &lt;p&gt;Finally, to find to find \(I\) we can combine \(I(\operatorname{arccosh} 7)\) with \(I(0)\).&lt;/p&gt;
    &lt;p&gt;The result from above follows since includes \(I(0) = \frac{\pi^2}{6}\), which is also the Basel problem in disguise, but you're further encouraged to approach it by employing Feynman's trick.&lt;/p&gt;
    &lt;p&gt;One idea is to rewrite the integral as: \[ I=\int_0^1 \frac{\ln(1+x)}{x}dx\overset{x\to x^3}=3\int_0^1 \frac{\ln(1+x^3)}{x}dx\] \[\Rightarrow \frac13I-I=\int_0^1 \frac{\ln\left(\frac{1+x^3}{1+x}\right)}{x}dx\Rightarrow I = -\frac32 \int_0^1 \frac{\ln(1-x+x^2)}{x}dx \] Now put Feynman's trick to use for: \[I(t)=\int_0^1 \frac{\ln(1-tx+x^2)}{x}dx\] But also take into account that: \[I(0)=\int_0^1 \frac{\ln(1+x^2)}{x}dx\overset{x^2\to x}=\frac12 \int_0^1 \frac{\ln(1+x)}{x}dx=\frac12I\]&lt;/p&gt;
    &lt;head rend="h3"&gt;Switching to rational functions&lt;/head&gt;
    &lt;p&gt;Maybe it's just a personal preference, but for me working with rational functions tends to give a higher visibility on how to parameterise the integrals. We'll see what is meant by this when dealing with the following integral:&lt;/p&gt;
    &lt;p&gt;In this form, two immediate ways to parameterise the integral are as follows:&lt;/p&gt;
    &lt;p&gt;It turns out that both variants work, but we can do better. Another rule of thumb that I follow is to use almost exclusive rational functions as they often provide the most visibility to work with. Let's do that for our integral too.&lt;/p&gt;
    &lt;p&gt;And at this point it should be obvious where to place the parameter so that we can simplify the denominator.&lt;/p&gt;
    &lt;p&gt;Obviously, if you're already more used to trigonometric functions, hyperbolic functions or anything else, then this can be ignored. I however recommend switching to rational functions, a few exceptions being when there's a clear way to parameterise the integral directly or when at least one of the bounds is \(\infty\).&lt;/p&gt;
    &lt;p&gt;For more practice you can also attempt to tackle the following integral:&lt;/p&gt;
    &lt;p&gt;Start by substituting \(\tan x \to x\) then employ Feynman's trick.&lt;/p&gt;
    &lt;head rend="h3"&gt;Cleaning up the functions&lt;/head&gt;
    &lt;p&gt;Before parameterising the integrals it's quite useful to clean the disturbing functions as much as possible before any parameterisation. Let's demonstrate this by considering:&lt;/p&gt;
    &lt;p&gt;As we have trigonometric functions all over the place, we will perform the Weierstrass substitution \(\tan\left(\frac{x}{2}\right)\to x\) in order to obtain only rational functions, as per the previous heuristic.&lt;/p&gt;
    &lt;p&gt;It would be great to parameterise the integral so that we get rid of the arctangent function, but unfortunately here it is a bit overloaded, therefore we'll clean it by splitting it into two. In case it's not obvious how to do that directly, we can differentiate it, perform partial fractions and then integrate back.&lt;/p&gt;
    &lt;p&gt;However since we're integrating over the \((0,1)\) interval we have that \(x \cdot 3 x&amp;gt;1\) for \(x &amp;gt; \frac{1}{\sqrt 3}\), so we need to rewrite the integral as:&lt;/p&gt;
    &lt;p&gt;At this point there's only left to evaluate the first integral, \(\mathcal J\), for which we'll employ Feynman's trick. There isn't any way to place the parameter so that we get rid of anything from the denominator so we'll simply introduce the following parameterisation:&lt;/p&gt;
    &lt;p&gt;In general when we have rational functions it is prefered to integrate over \((0,\infty)\), if possible, as it drastically reduces the result, and when there's the derivative of \(\arcsin x\) in the denominator one way to map \((0,1)\) to \((0,\infty)\) is to directly substitute \(x \to \frac{1}{\sqrt{1+x^2}}\).&lt;/p&gt;
    &lt;p&gt;Now in order to go back to \(\mathcal J(3)\), we'll make use of \(\mathcal J(-1)=0\), so:&lt;/p&gt;
    &lt;p&gt;Finally, to finish this integral we'll subsitute \(t = \frac{1-x}{1+x}\).&lt;/p&gt;
    &lt;p&gt;As such, we can conclude that:&lt;/p&gt;
    &lt;p&gt;Similarly you can try and tackle the following integral:&lt;/p&gt;
    &lt;p&gt;Start by substituting \(\tan\left(\frac{x}{2}\right) \to x\), then split the arctangent function into two parts and use Feynman's trick.&lt;/p&gt;
    &lt;head rend="h3"&gt;Preparing better integral bounds&lt;/head&gt;
    &lt;p&gt;Another useful thing to consider before using Feynman's trick is to manipulate the bounds prior to parameterising the integral so that they get rid of any complicated functions for the differentiated integral, \(I'(t)\). This will almost always give a smoother and easier integral that we have to undo. We will show this in action with the integral from below.&lt;/p&gt;
    &lt;p&gt;With the previous heuristic we've already seen that it is useful to integrate over \((0,\infty)\) when there are some rational functions - therefore we will attempt to do the same with this integral. You are encouraged to try and see what kind of mess it would be produced by parameterising the integral as it is, when the bounds are \((0,1)\). However we will jump straightforward to get our bounds to \((0,\infty)\).&lt;/p&gt;
    &lt;p&gt;In order to obtain that we can notice that the integrand is even, so we can move the bounds to \((-1,1)\) and then substitute \(x\to \frac{1-x}{1+x}\) again, which is another useful way to get our bounds at \((0,\infty)\).&lt;/p&gt;
    &lt;p&gt;Now we can split the logarithm into three parts and use that:&lt;/p&gt;
    &lt;p&gt;Therefore our integral is:&lt;/p&gt;
    &lt;p&gt;To evaluate the emerging integral we will perform Feynman's trick. There's not an obvious way to place the parameter as to simplify the denominator, so we'll parameterise the integral as:&lt;/p&gt;
    &lt;p&gt;The partial fraction was ommited above, as what's really important here is that we're left only with a simple \(\ln a\) as a "disturbing" function. In contrast, if the bounds were \((0,1)\) things would have been way more complicated.&lt;/p&gt;
    &lt;p&gt;Let's finish this integral as we still have to undo the differentiating step.&lt;/p&gt;
    &lt;p&gt;The result from above follows since:&lt;/p&gt;
    &lt;p&gt;Where \(\operatorname{Li}_2(x)\) is the dilogarithm and \(\operatorname{Ti}_2(x)\) is the inverse tangent integral.&lt;/p&gt;
    &lt;p&gt;Finally, collecting all the results yields:&lt;/p&gt;
    &lt;p&gt;With the same idea one can attempt to calculate the following integral:&lt;/p&gt;
    &lt;p&gt;Map the bounds from \((0,1)\) to \((0,\infty)\) by substituting \(x\to\frac{1}{x}\) - here it is necessary to add the resulting integral with the original one - afterwards use Feynman's trick.&lt;/p&gt;
    &lt;head rend="h3"&gt;Multiple parameters&lt;/head&gt;
    &lt;p&gt;We mostly got familiar to apply Feynman's trick by introducing a parameter somewhere, however sometimes even multiple parameters can be used when encountering new integrals. To exemplify such a situation, let's take a look at the following unit square integral arising in geometric probability:&lt;/p&gt;
    &lt;p&gt;In order to generate the \(\ln(xy)\) part it's straightforward to consider the following integral:&lt;/p&gt;
    &lt;p&gt;Differentiating with respect to \(a\), \(m\) times followed by setting \(a=0\) will gives us the desired integral, ignoring the \((-1)^m\) term. However the denominator is still troublesome, and to deal with that we will introduce one more parameter:&lt;/p&gt;
    &lt;p&gt;This is perfect now, as we can recover our original integral by differentiating with respect to \(a\), \(m\) times, and with respect to \(z\), \(m-1\) times, followed by setting \(a=0\) and respectively \(z=1\) - ignoring some coefficients.&lt;/p&gt;
    &lt;p&gt;One way to evaluate \(I(a,z)\) is to expand the denominator into geometric series as:&lt;/p&gt;
    &lt;p&gt;Now we will take \(m\) derivatives with respect to \(a\) and then set it to \(0\).&lt;/p&gt;
    &lt;p&gt;This can be also rewriten in terms of the polylogarithm function as:&lt;/p&gt;
    &lt;p&gt;Finally, we can arrive at our original integral by taking \(m-1\) derivatives with respect to \(z\) and setting it to \(1\).&lt;/p&gt;
    &lt;p&gt;Although the derivation from above was the important part since it shows the main idea on how to differentiate in order to produce the desired integral, by using \(\frac{\partial}{\partial z} \operatorname{Li}_n(z) = \frac{\operatorname{Li}_{n-1}(z)}{z}\) the result can be also written, with the help of OEIS, as:&lt;/p&gt;
    &lt;p&gt;Where \(s(n,m)\) is the Stirling number of the first kind and \(\zeta(z)\) is the Riemann zeta function.&lt;/p&gt;
    &lt;p&gt;A similar idea can be applied for the following integral:&lt;/p&gt;
    &lt;p&gt;Make use of a more general integral that was evaluated in the fourth chapter, namely: \[\int_0^\infty \frac{\cos(tx)}{a^2+x^2}dx = \frac{\pi}{2a}e^{-at}\] Then differentiate \(3\) times w.r.t. \(a\).&lt;/p&gt;
    &lt;head rend="h3"&gt;Cascaded Feynman's trick&lt;/head&gt;
    &lt;p&gt;Sometimes to enable an application of Feynman's trick we needed to actually apply another Feynman's trick. Let's look at the following integral:&lt;/p&gt;
    &lt;p&gt;The first step should be pretty obvious by now, namely to get rid of the trigonometric functions.&lt;/p&gt;
    &lt;p&gt;Now we can notice that we have two logarithms and only one of them contains the \(x\) term, however since the bounds are \((0,1)\) dealing with that integral won't produce much success (as the result would be quite complicated).&lt;/p&gt;
    &lt;p&gt;However we also have the bounds as \((0,\infty)\) for the \(y\) integral, and it would be even better if we could have a single logarithm. To further obtain such a favorable integral form, we can use the following result:&lt;/p&gt;
    &lt;p&gt;In the third chapter we saw how it's useful to have a list of integral results. One more such useful integral that tend to appear quite often is:&lt;/p&gt;
    &lt;p&gt;You can differentiate either \(I(a)\) or \(I(b)\), and even directly employ the accelerated Feynman's trick by writing the logarithm as an integral.&lt;/p&gt;
    &lt;p&gt;Now back to our integral, by using the above result, we can easily finish the integral.&lt;/p&gt;
    &lt;p&gt;Although this marks the conclusion of the essay, this isn't a static website, and I might update it when I encounter new interesting integrals that are worth to be shown. So far, the integrals comes from my posts on Mathematics Stack Exchange, combined with some of the most popular integrals - thus you can also check them directly there.&lt;/p&gt;
    &lt;p&gt;For further exercises, I can recommend you to explore math forums and magazines such as Art of Problem Solving, Mathematics Stack Exchange, The American Mathematical Monthly, Crux Mathematicorum, or the Romanian Mathematical Magazine, where dozens of fascinating integrals are often posted or published. Additionally, delving into other fields like Statistics, Physics, or Quantum Physics will present you with many remarkable integrals — some of which might be computed using Feynman's trick.&lt;/p&gt;
    &lt;p&gt;I would appreciate a notice on my email address (rxzacky@gmail.com) in case you find any mistakes or if something feels unclear in this essay - and even better if you have some further ideas or suggestions.&lt;/p&gt;
    &lt;p&gt; This work is licensed under a Creative Commons Attribution 4.0 International License, and it can be cited as: &lt;lb/&gt; Zaharia Burghelea, "Feynman's Trick," https://zackyzz.github.io/feynman. &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090269</guid><pubDate>Sat, 29 Nov 2025 19:55:17 +0000</pubDate></item><item><title>All it takes is for one to work out</title><link>https://alearningaday.blog/2025/11/28/all-it-takes-is-for-one-to-work-out-2/</link><description>&lt;doc fingerprint="64659b5db9008759"&gt;
  &lt;main&gt;
    &lt;p&gt;More than a decade ago, when I was applying to graduate school, I went through a period of deep uncertainty. I had tried the previous year and hadn’t gotten in anywhere. I wanted to try again, but I had a lot going against me.&lt;/p&gt;
    &lt;p&gt;I’d spent most of my undergrad building a student job-portal startup and hadn’t balanced it well with academics. My GPA needed explaining. My GMAT score was just okay. I didn’t come from a big-brand employer. And there was no shortage of people with similar or stronger profiles applying to the same schools.&lt;/p&gt;
    &lt;p&gt;Even though I had learned a few things from the first round, the second attempt was still difficult. There were multiple points after I submitted applications where I lost hope.&lt;/p&gt;
    &lt;p&gt;But during that stretch, a friend and colleague kept repeating one line to me:&lt;/p&gt;
    &lt;p&gt;“All it takes is for one to work out.”&lt;/p&gt;
    &lt;p&gt;He’d say it every time I spiraled. And as much as it made me smile, a big part of me didn’t fully believe it. Still, it became a little maxim between us. And eventually, he was right – that one did work out. And it changed my life.&lt;/p&gt;
    &lt;p&gt;I’ve thought about that framing so many times since then.&lt;/p&gt;
    &lt;p&gt;It’s unbelievably powerful in any high-stakes search:&lt;/p&gt;
    &lt;p&gt;You don’t need every job to choose you. You just need the one that’s the right fit.&lt;/p&gt;
    &lt;p&gt;You don’t need every house to accept your offer. You just need the one that feels like home.&lt;/p&gt;
    &lt;p&gt;You don’t need every person to want to build a life with you. You just need the one.&lt;/p&gt;
    &lt;p&gt;You don’t need ten universities to say yes. You just need the one that opens the right door.&lt;/p&gt;
    &lt;p&gt;These processes – college admissions, job searches, home buying, finding a partner – can be emotionally brutal. They can get you down in ways that feel personal. But in those moments, that truth can be grounding.&lt;/p&gt;
    &lt;p&gt;All it takes is for one to work out.&lt;/p&gt;
    &lt;p&gt;And that one is all you need.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090433</guid><pubDate>Sat, 29 Nov 2025 20:22:44 +0000</pubDate></item><item><title>Landlock-Ing Linux</title><link>https://blog.prizrak.me/post/landlock/</link><description>&lt;doc fingerprint="7d7b98108eb566c1"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Landlock-ing Linux&lt;/head&gt;
    &lt;p&gt;Nov 29, 2025&lt;/p&gt;
    &lt;head rend="h3"&gt;Landlock: What Is It?&lt;/head&gt;
    &lt;p&gt;Landlock is a Linux API that lets applications explicitly declare which resources they are allowed to access. Its philosophy is similar to OpenBSDâs &lt;code&gt;unveil()&lt;/code&gt; and (less so) &lt;code&gt;pledge()&lt;/code&gt;: programs can make a contract with the kernel stating, âI only need these files or resources â deny me everything else if Iâm compromised.â&lt;/p&gt;
    &lt;p&gt;It provides a simple, developer-friendly way to add defense-in-depth to applications. Compared to traditional Linux security mechanisms, Landlock is vastly easier to understand and integrate.&lt;/p&gt;
    &lt;p&gt;This post is meant to be an accessible introduction, and hopefully persuade you to give Landlock a try.&lt;/p&gt;
    &lt;head rend="h3"&gt;How Does It Work?&lt;/head&gt;
    &lt;p&gt;Landlock is a Linux Security Module (LSM) available since Linux 5.13. Unlike MAC frameworks such as SELinux or AppArmor, Landlock applies transient restrictions: policies are created at runtime, enforced on the current thread and its future descendants, and disappear when the process exits.&lt;/p&gt;
    &lt;p&gt;You donât tag files with labels or extended attributes. Instead, applications create policies dynamically.&lt;/p&gt;
    &lt;p&gt;A Landlock policy consists of two pieces:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Handled accesses â the categories of operations you want to restrict (e.g., filesystem read/write).&lt;/item&gt;
      &lt;item&gt;Access grants â an explicit allowlist of which objects are permitted for those operations.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For example, you could create a policy that handles all filesystem reads/writes and network binds, and grants:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;read-only access to &lt;code&gt;/home/user&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;read/write access to &lt;code&gt;/tmp&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;permission to bind to port &lt;code&gt;2222&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The application then calls &lt;code&gt;landlock_restrict_self()&lt;/code&gt; to enter the restricted domain. From that point on, that thread’s child threads and child processes are permanently constrained. Restrictions cannot be revoked.&lt;/p&gt;
    &lt;p&gt;Policies can be layered (up to 16 layers). A child layer may further reduce access, but cannot reintroduce permissions the parent layer removed. For example, a child thread may add a layer to this policy to restrict itself to only reading &lt;code&gt;/home/user&lt;/code&gt;, but it cannot regain permission to bind to port &lt;code&gt;2222&lt;/code&gt; once a layer omits this grant.&lt;/p&gt;
    &lt;p&gt;Landlock is unprivileged â any application can sandbox itself. It also uses ABI versioning, allowing programs to apply best-effort sandboxing even on older kernels lacking newer features.&lt;/p&gt;
    &lt;p&gt;It’s also a stackable LSM, meaning you can combine it with selinux or apparmor in a supplemental layer.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Should You Use It?&lt;/head&gt;
    &lt;p&gt;Landlock shines when an application has a predictable set of files or directories it needs. For example, a web server could restrict itself to accessing only &lt;code&gt;/var/www/html&lt;/code&gt; and &lt;code&gt;/tmp&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Unlike SELinux or AppArmor, Landlock policies donât require administrator involvement or system-wide configuration. Developers can embed policies directly in application code, making sandboxing a natural part of the development process.&lt;/p&gt;
    &lt;p&gt;Because Landlock requires no privileges to use, adding it to most programs is straightforward.&lt;/p&gt;
    &lt;p&gt;Bindings exist for languages such as Rust, Go, and Haskell, and several projects provide user-friendly &lt;code&gt;unveil&lt;/code&gt;-style wrappers.&lt;/p&gt;
    &lt;p&gt;A official c library doesn’t exist yet unfortunately, but there’s several out there you can try.&lt;/p&gt;
    &lt;p&gt;Here’s a quick rust example:&lt;/p&gt;
    &lt;code&gt;use landlock::{
    ABI, Access, AccessFs, Ruleset, RulesetAttr, RulesetCreatedAttr, RulesetStatus, RulesetError,
    path_beneath_rules,
};

fn restrict_thread() -&amp;gt; Result&amp;lt;(), RulesetError&amp;gt; {
    let abi = ABI::V1;
    let status = Ruleset::default()
        .handle_access(AccessFs::from_all(abi))?
        .create()?
        // Read-only access to /usr, /etc and /dev.
        .add_rules(path_beneath_rules(&amp;amp;["/usr", "/etc", "/dev"], AccessFs::from_read(abi)))?
        // Read-write access to /home and /tmp.
        .add_rules(path_beneath_rules(&amp;amp;["/home", "/tmp"], AccessFs::from_all(abi)))?
        .restrict_self()?;

    match status.ruleset {
        RulesetStatus::FullyEnforced =&amp;gt; println!("Fully sandboxed."),
        RulesetStatus::PartiallyEnforced =&amp;gt; println!("Partially sandboxed."),
        RulesetStatus::NotEnforced =&amp;gt; println!("Not sandboxed! Please update your kernel."),
    }
    Ok(())
}
&lt;/code&gt;
    &lt;head rend="h3"&gt;The State of Linux Sandboxing: Why This Matters&lt;/head&gt;
    &lt;p&gt;As Linux adoption grows, so does the amount of malware targeting desktop users. While Linux has historically enjoyed relative safety, this is largely due to smaller market share and higher technical barriers compared to Windows â not because Linux is inherently safer.&lt;/p&gt;
    &lt;p&gt;Linux is not a security panacea. For example, on most major distributions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Users can download and execute untrusted binaries with no warnings.&lt;/item&gt;
      &lt;item&gt;Shell scripts can be piped from the internet and executed blindly.&lt;/item&gt;
      &lt;item&gt;Many users run passwordless sudo, giving them root access on demand.&lt;/item&gt;
      &lt;item&gt;Unprivileged applications can typically: &lt;list rend="ul"&gt;&lt;item&gt;Read &lt;code&gt;~/.ssh&lt;/code&gt;,&lt;code&gt;~/.bashrc&lt;/code&gt;, browser cookies, and anything else in&lt;code&gt;$HOME&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Modify environment variables and &lt;code&gt;$PATH&lt;/code&gt;&lt;/item&gt;&lt;item&gt;Create systemd user services&lt;/item&gt;&lt;item&gt;(on X11) log keystrokes and read input devices&lt;/item&gt;&lt;item&gt;Bind to arbitrary network ports&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Read &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Several tools try to improve the state of security on linux, but each has significant drawbacks:&lt;/p&gt;
    &lt;head rend="h4"&gt;Containerization (docker, podman)&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Designed for service isolation, not desktop apps.&lt;/item&gt;
      &lt;item&gt;Managing home directory access is clunky.&lt;/item&gt;
      &lt;item&gt;Many users break isolation by using &lt;code&gt;--privileged&lt;/code&gt;or&lt;code&gt;--network host&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Flatpak / Snap&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Great for graphical applications (Flatpak especially).&lt;/item&gt;
      &lt;item&gt;Often require overly broad permissions.&lt;/item&gt;
      &lt;item&gt;Less suitable for CLI tools.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Firejail&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Requires per-application profiles.&lt;/item&gt;
      &lt;item&gt;Must be explicitly invoked each time, or you need a wrapper script.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From the developer side:&lt;/p&gt;
    &lt;head rend="h4"&gt;seccomp&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Powerful syscall filtering.&lt;/item&gt;
      &lt;item&gt;Tedious and error-prone to configure.&lt;/item&gt;
      &lt;item&gt;Blacklists are fragile; new syscalls can break things.&lt;/item&gt;
      &lt;item&gt;Argument filtering is difficult and full of TOCTOU hazards.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;SELinux&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Extremely powerful, but difficult to understand.&lt;/item&gt;
      &lt;item&gt;Requires system-wide policies and admin involvement.&lt;/item&gt;
      &lt;item&gt;Many users disable it due to complexity.&lt;/item&gt;
      &lt;item&gt;Not enabled on most distributions by default. (used a lot in android)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;AppArmor&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Easier than SELinux, but still requires admin-defined profiles.&lt;/item&gt;
      &lt;item&gt;Applies system-wide and lacks per-process namespacing.&lt;/item&gt;
      &lt;item&gt;Gets disabled by many distributions, but is more commonly used in the desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Landlock&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Unprivileged&lt;/item&gt;
      &lt;item&gt;Application-centric&lt;/item&gt;
      &lt;item&gt;Easy to integrate&lt;/item&gt;
      &lt;item&gt;Deny-by-default&lt;/item&gt;
      &lt;item&gt;Widely supported since 5.13&lt;/item&gt;
      &lt;item&gt;Backward and forward compatibility mechanisms.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Landlock isnât perfect, but it fills a major gap: a simple, self-contained unprivileged sandboxing tool.&lt;/p&gt;
    &lt;head rend="h3"&gt;What landlock could bring to the table:&lt;/head&gt;
    &lt;p&gt;Long-running system daemons that run with elevated privileges could benefit from landlock restrictions.&lt;/p&gt;
    &lt;p&gt;Desktop applications dealing with binary formats, like pdf readers, image viewers web browsers, and word processors can be restricted to accessing the files they originally opened.&lt;/p&gt;
    &lt;p&gt;FTP and HTTP servers can be bound to the files they need. Even if nginx is running as root, if an attacker gets a full reverse shell, they won’t be able to see access files outside the policy.&lt;/p&gt;
    &lt;p&gt;If the supervisor proposal gets added, we could bring an android-like permissions system to the linux desktop. Flatpak does a decent job at this, but imagine if every process in your desktop would need to explicitly ask (at least once) before accessing sensitive files or resources.&lt;/p&gt;
    &lt;p&gt;Pair that with an accessible GUI and a system for handling updates and saving permission grants, and we have potential for a safer, more secure linux user experience on the desktop.&lt;/p&gt;
    &lt;head rend="h3"&gt;Ongoing Work in Landlock&lt;/head&gt;
    &lt;p&gt;Several promising features are under active development:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Supervise Mode&lt;/p&gt;&lt;lb/&gt;Lets a userspace âsupervisorâ interactively allow or deny access â similar to Android-style permission prompts.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Socket Restrictions Fine-grained control over which types of sockets or ports processes may use.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LANDLOCK_RESTRICT_SELF_TSYNC Ensures restrictions propagate to all threads in a process.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LANDLOCK_ADD_RULE_QUIET Allows suppressing audit messages for certain objects.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;LANDLOCK_ADD_RULE_NO_INHERIT (disclosure: this is my patch series) Prevents rules from unintentionally inheriting permissions from parent directories, giving finer-grained filesystem control.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt;Landlock is a simple, unprivileged, deny-by-default sandboxing mechanism for Linux.&lt;lb/&gt; Itâs easy to understand, easy to integrate, and has tremendous potential for improving desktop and application security.&lt;/p&gt;
    &lt;p&gt;Give it a try in your application.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46090969</guid><pubDate>Sat, 29 Nov 2025 21:30:53 +0000</pubDate></item><item><title>Americans no longer see four-year college degrees as worth the cost</title><link>https://www.nbcnews.com/politics/politics-news/poll-dramatic-shift-americans-no-longer-see-four-year-college-degrees-rcna243672</link><description>&lt;doc fingerprint="3cc021459d8f0fe8"&gt;
  &lt;main&gt;
    &lt;p&gt;Americans have grown sour on one of the longtime key ingredients of the American dream.&lt;/p&gt;
    &lt;p&gt;Almost two-thirds of registered voters say that a four-year college degree isn’t worth the cost, according to a new NBC News poll, a dramatic decline over the last decade.&lt;/p&gt;
    &lt;p&gt;Just 33% agree a four-year college degree is “worth the cost because people have a better chance to get a good job and earn more money over their lifetime,” while 63% agree more with the concept that it’s “not worth the cost because people often graduate without specific job skills and with a large amount of debt to pay off.”&lt;/p&gt;
    &lt;p&gt;In 2017, U.S. adults surveyed were virtually split on the question — 49% said a degree was worth the cost and 47% said it wasn’t. When CNBC asked the same question in 2013 as part of its All American Economic Survey, 53% said a degree was worth it and 40% said it was not.&lt;/p&gt;
    &lt;p&gt;The eye-popping shift over the last 12 years comes against the backdrop of several major trends shaping the job market and the education world, from exploding college tuition prices to rapid changes in the modern economy — which seems once again poised for radical transformation alongside advances in AI.&lt;/p&gt;
    &lt;p&gt;“It’s just remarkable to see attitudes on any issue shift this dramatically, and particularly on a central tenet of the American dream, which is a college degree. Americans used to view a college degree as aspirational — it provided an opportunity for a better life. And now that promise is really in doubt,” said Democratic pollster Jeff Horwitt of Hart Research Associates, who conducted the poll along with the Republican pollster Bill McInturff of Public Opinion Strategies.&lt;/p&gt;
    &lt;p&gt;“What is really surprising about it is that everybody has moved. It’s not just people who don’t have a college degree,” Horwitt added.&lt;/p&gt;
    &lt;p&gt;National data from the Bureau of Labor Statistics shows that those with advanced degrees earn more and have lower unemployment rates than those with lower levels of education. That’s been true for years.&lt;/p&gt;
    &lt;p&gt;But what has shifted is the price of college. While there have been some small declines in tuition prices over the last decade, when adjusted for inflation, College Board data shows that the average, inflation-adjusted cost of public four-year college tuition for in-state students has doubled since 1995. Tuition at private, four-year colleges is up 75% over the same period.&lt;/p&gt;
    &lt;p&gt;Poll respondents who spoke with NBC News all emphasized those rising costs as a major reason why the value of a four-year degree has been undercut.&lt;/p&gt;
    &lt;p&gt;Jacob Kennedy, a 28-year-old server and bartender living in Detroit, told NBC News that while he believes “an educated populace is the most important thing for a country to have,” if people can’t use those degrees because of the debt they’re carrying, it undercuts the value.&lt;/p&gt;
    &lt;p&gt;Kennedy, who has a two-year degree, reflected on “the number of people who I’ve met working in the service industry who have four-year degrees and then within a year of graduating immediately quit their ‘grown-up jobs’ to go back to the jobs they had.”&lt;/p&gt;
    &lt;p&gt;“The cost overwhelms the value,” he continued. “You go to school with all that student debt — the jobs you get out of college don’t pay that debt, so you have to go find something else that can pay that debt.”&lt;/p&gt;
    &lt;p&gt;The 20-point decline over the last 12 years among those who say a degree is worth it — from 53% in 2013 to 33% now — is reflected across virtually every demographic group. But the shift in sentiment is especially striking among Republicans.&lt;/p&gt;
    &lt;p&gt;In 2013, 55% of Republicans called a college degree worth it, while 38% said it wasn’t worth it. In the new poll, just 22% of Republicans say the four-year degree is worth it, while 74% say it’s not.&lt;/p&gt;
    &lt;p&gt;Democrats have seen a significant shift too, but not to the same extent — a decline from 61% who said a degree was worth it in 2013 to 47% this year.&lt;/p&gt;
    &lt;p&gt;Over the same period, the composition of both parties has changed, with the Republican Party garnering new and deeper support from voters without college degrees, while the Democratic Party drew in more degree-holders.&lt;/p&gt;
    &lt;p&gt;Remarkably, less than half of voters with college degrees see those degrees as worth the cost: 46% now, down from 63% in 2013.&lt;/p&gt;
    &lt;p&gt;Those without a college degree were about split on the question in 2013. Now, 71% say a four-year degree is not worth the cost, while 26% say it is.&lt;/p&gt;
    &lt;p&gt;Preston Cooper, a senior fellow at the right-leaning American Enterprise Institute, said enough cracks have proliferated under the long-standing narrative that a college degree always pays off to create a serious rupture.&lt;/p&gt;
    &lt;p&gt;“Some people drop out, or sometimes people end up with a degree that is not worth a whole lot in the labor market, and sometimes people pay way too much for a degree relative to the value of what that credential is,” he said. “These cases have created enough exceptions to the rule that a bachelor’s degree always pays off, so that people are now more skeptical.”&lt;/p&gt;
    &lt;p&gt;The upshot is that interest in technical, vocational and two-year degree programs has soared.&lt;/p&gt;
    &lt;p&gt;“I think students are more wary about taking on the risk of a four-year or even a two-year degree,” he said. “They’re now more interested in any pathway that can get them into the labor force more quickly.”&lt;/p&gt;
    &lt;p&gt;Josiah Garcia, a 24-year-old in Virginia, said he recently enrolled in a program to receive a four-year engineering degree after working as an electrician’s apprentice. He said he was motivated to go back to school because he saw the degree as having a direct effect on his future earning potential.&lt;/p&gt;
    &lt;p&gt;But he added that he didn’t feel that those who sought other degrees in areas like art or theater could say the same.&lt;/p&gt;
    &lt;p&gt;“A lot of my friends who went to school for art or dance didn’t get the job they thought they could get after graduating,” he said, arguing that degrees for “softer skills” should be cheaper than those in STEM fields.&lt;/p&gt;
    &lt;p&gt;Jessica Burns, a 38-year-old Iowa resident and bachelor’s degree-holder who works for an insurance company, told NBC News that for her, the worth of a four-year-degree largely depends on the cost.&lt;/p&gt;
    &lt;p&gt;She went to a community college and then a state school to earn her degree, so she said she graduated without having to spend an “insane” amount of money.&lt;/p&gt;
    &lt;p&gt;But her husband went to a private college for his degree, and she quipped: “We are going to have student loan debt for him forever.”&lt;/p&gt;
    &lt;p&gt;Burns said she believes a college degree is “essential for a lot of jobs. You’re not going to get an interview if you don’t have a four-year degree for a lot of jobs in my field.”&lt;/p&gt;
    &lt;p&gt;But she framed the value of degrees more in terms of how society views them instead of intrinsic value.&lt;/p&gt;
    &lt;p&gt;“It’s not valuable because it’s brought a bunch of value added, it’s valuable because it’s the key to even getting in the door,” she said. “Our society needs to figure out that if we value it, we need to make it affordable.”&lt;/p&gt;
    &lt;p&gt;Burns said she believes that a lot more people in her millennial generation are “now saddled with a huge amount of debt, even as successful business professionals,” which will influence how her peers approach paying for college for their children.&lt;/p&gt;
    &lt;p&gt;There hasn’t just been a decline in the cost-benefit analysis of a degree. Gallup polling also shows a marked decline in public confidence in higher education over the last decade, albeit with a slight increase over the last year.&lt;/p&gt;
    &lt;p&gt;“This is a political problem. It’s also a real problem for higher education. Colleges and universities have lost that connection they’ve had with a large swath of the American people based on affordability,” Horwitt said. “They’re now seen as out of touch and not accessible to many Americans.”&lt;/p&gt;
    &lt;p&gt;The NBC News poll surveyed 1,000 registered voters Oct. 24-28 via a mix of telephone interviews and an online survey sent via text message. The margin of error is plus or minus 3.1 percentage points.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46091591</guid><pubDate>Sat, 29 Nov 2025 22:56:14 +0000</pubDate></item><item><title>A new Little Prince museum has opened its doors in Switzerland</title><link>https://www.lepetitprince.com/en/events-around-the-world/a-new-little-prince-museum-has-opened-its-doors-in-switzerland/</link><description>&lt;doc fingerprint="1f82f653b7dbab38"&gt;
  &lt;main&gt;
    &lt;p&gt;On November 7, the Besenval Palace in Solothurn experienced an exceptional moment with the official opening of the museum « Der Kleine Prinz und seine Welt » dedicated to the Little Prince through the collection of Jean-Marc Probst. The event took place in the presence of Olivier d’Agay, great-nephew of Antoine de Saint-Exupéry and President of the Succession Saint-Exupéry d’Agay, accompanied by the Little Prince teams and the Antoine de Saint-Exupéry Youth Foundation.&lt;/p&gt;
    &lt;p&gt;This new museum is built upon the remarkable collection gathered over more than forty years by the Jean-Marc Probst Foundation for the Little Prince, which brings together more than ten thousand books, objects, documents, and rare editions related to the literary masterpiece published in 1943. This collection, one of the most significant in the world, is now accessible to the public in a historic setting with a particularly successful scenography created by the Bâtisseurs de Mémoire.&lt;/p&gt;
    &lt;p&gt;The Little Prince is a universal phenomenon. Translated into more than six hundred languages and dialects, it is the most translated book after the Bible. “With The Little Prince, it is almost the entire world that settles in Solothurn,” the cantonal chancellery reminded when the project was announced.&lt;/p&gt;
    &lt;p&gt;Jean-Marc Probst, a passionate collector since 1980, has patiently brought together editions from all over the world. His foundation also enriches the legacy of the work by supporting new publications since 2013. This new cultural venue will continue to preserve and share the humanist message of Antoine de Saint-Exupéry through educational and interactive content designed for children as well as grown-ups.&lt;/p&gt;
    &lt;p&gt;Located on the banks of the Aare, the Besenval Palace, built in the early eighteenth century, opens a new chapter in its history as it becomes a space devoted to imagination, sharing, and encounters inspired by the Little Prince.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46092478</guid><pubDate>Sun, 30 Nov 2025 01:03:44 +0000</pubDate></item><item><title>Meshtastic</title><link>https://meshtastic.org/</link><description>&lt;doc fingerprint="bda8ec42c8592ac0"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Getting Started&lt;/head&gt;
    &lt;head rend="h4"&gt;Step 1&lt;/head&gt;
    &lt;p&gt;Meshtastic devices are available in a variety of configurations to suit your needs.&lt;/p&gt;
    &lt;head rend="h4"&gt;Step 2&lt;/head&gt;
    &lt;p&gt;Flash your device with the latest version of Meshtastic and configure it to your liking.&lt;/p&gt;
    &lt;head rend="h4"&gt;Step 3&lt;/head&gt;
    &lt;p&gt;Connect to your device via any of our clients to start sending and receiving messages!&lt;/p&gt;
    &lt;head rend="h3"&gt;Get Connected&lt;/head&gt;
    &lt;p&gt;Connect and control your Meshtastic devices through various platforms. Choose the client that best fits your needs and device ecosystem.&lt;/p&gt;
    &lt;head rend="h4"&gt;iOS App&lt;/head&gt;
    &lt;p&gt;Manage your Meshtastic network on-the-go with our iOS application.&lt;/p&gt;
    &lt;head rend="h4"&gt;Android App&lt;/head&gt;
    &lt;p&gt;Connect and control your Meshtastic devices using our Android application.&lt;/p&gt;
    &lt;head rend="h4"&gt;Web Client&lt;/head&gt;
    &lt;p&gt;Access your Meshtastic network from any device with our web-based client.&lt;/p&gt;
    &lt;head rend="h4"&gt;Python CLI/SDK&lt;/head&gt;
    &lt;p&gt;Command-line interface and software development kit for Python developers and power users.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46092558</guid><pubDate>Sun, 30 Nov 2025 01:15:59 +0000</pubDate></item><item><title>Show HN: Boing</title><link>https://boing.greg.technology/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46093473</guid><pubDate>Sun, 30 Nov 2025 03:46:35 +0000</pubDate></item><item><title>Zigbook Is Plagiarizing the Zigtools Playground</title><link>https://zigtools.org/blog/zigbook-plagiarizing-playground/</link><description>&lt;doc fingerprint="3c02f2b9ed3ffbb4"&gt;
  &lt;main&gt;
    &lt;p&gt;Auguste Rame, Techatrix — 30 November 2025&lt;/p&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;For those unfamiliar, Zigtools was founded to support the Zig community, especially newcomers, by creating editor tooling such as ZLS, providing building blocks for language servers written in Zig with lsp-kit, working on tools like the Zigtools Playground, and contributing to Zig editor extensions like vscode-zig.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Plagiarism&lt;/head&gt;
    &lt;p&gt;A couple weeks ago, a Zig resource called Zigbook was released with a bold claim of “zero AI” and an original “project-based” structure.&lt;/p&gt;
    &lt;p&gt;Unfortunately, even a cursory look at the nonsense chapter structure, book content, examples, generic website, or post-backlash issue-disabled repo reveals that the book is wholly LLM slop and the project itself is structured like some sort of sycophantic psy-op, with botted accounts and fake reactions.&lt;/p&gt;
    &lt;p&gt;We’re leaving out all direct links to Zigbook to not give them any more SEO traction.&lt;/p&gt;
    &lt;p&gt;We thought that the broad community backlash would be the end of the project, but Zigbook persevered, releasing just last week a brand new feature, a “high-voltage beta” Zig playground.&lt;/p&gt;
    &lt;p&gt;As we at Zigtools have our own Zig playground (repo, website), our interest was immediately piqued. The form and functionality looked pretty similar and Zigbook even integrated (in a non-functional manner) ZLS into their playground to provide all the fancy editor bells-and-whistles, like code completions and goto definition.&lt;/p&gt;
    &lt;p&gt;Knowing Zigbook’s history of deception, we immediately investigated the WASM blobs. Unfortunately, the WASM blobs are byte-for-byte identical to ours. This cannot be a coincidence given the two blobs (&lt;code&gt;zig.wasm&lt;/code&gt;, a lightly modified version of the Zig compiler, and &lt;code&gt;zls.wasm&lt;/code&gt;, ZLS with a modified entry point for WASI) are entirely custom-made for the Zigtools Playground.&lt;/p&gt;
    &lt;p&gt;We archived the WASM files for your convenience, courtesy of the great Internet Archive:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;zls.wasm&lt;/code&gt;(&lt;code&gt;sha256sum&lt;/code&gt;:&lt;code&gt;3a63e5092e8f90172716977af5c88b4f49e546f730f25e9bafb47f4ac9a2ee1d&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;zig.wasm&lt;/code&gt;(&lt;code&gt;sha256sum&lt;/code&gt;:&lt;code&gt;d3fe6b8a6b1db84a914eaa1f4a80ca5dcfd3b0948a35f2b1e78432a392eace96&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We proceeded to look at the JavaScript code, which we quickly determined was similarly copied, but with LLM distortions, likely to prevent the code from being completely identical. Still, certain sections were copied one-to-one, like the JavaScript worker data-passing structure and logging (original ZLS playground code, plagiarized Zigbook code).&lt;/p&gt;
    &lt;p&gt;The following code from both files is identical:&lt;/p&gt;
    &lt;code&gt;    try {
        // @ts-ignore
        const exitCode = wasi.start(instance);

        postMessage({
            stderr: `\n\n---\nexit with exit code \n---\n`,
        });
    } catch (err) {
        postMessage({ stderr: `` });
    }

    postMessage({
        done: true,
    });

    // ...

    onmessage = (event) =&amp;gt; {
        if (event.data.run) {
            run(event.data.run);
        }
    };
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;\n\n---\nexit with exit code ${exitCode}\n---\n&lt;/code&gt; is perhaps the most obviously copied string.&lt;/p&gt;
    &lt;p&gt;Funnily enough, despite copying many parts of our code, Zigbook didn’t copy the most important part of the ZLS integration code, the JavaScript ZLS API designed to work with the ZLS WASM binary’s API. That JavaScript code is absolutely required to interact with the ZLS binary which they did plagiarize. Zigbook either avoided copying that JavaScript code because they knew it would be too glaringly obvious, because they fundamentally do not understand how the Zigtools Playground works, or because they plan to copy more of our code.&lt;/p&gt;
    &lt;p&gt;To be clear, copying our code and WASM blobs is entirely permissible given that the playground and Zig are MIT licensed. Unfortunately, Zigbook has not complied with the terms of the MIT license at all, and seemingly claims the code and blobs as their own without correctly reproducing the license.&lt;/p&gt;
    &lt;p&gt;We sent Zigbook a neutral PR correcting the license violations, but they quickly closed it and deleted the description, seemingly to hide their misdeeds.&lt;/p&gt;
    &lt;p&gt;The original description (also available in the “edits” dropdown of the original PR comment) is reproduced below:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;We (@zigtools) noticed you were using code from the Zigtools Playground, including byte-by-byte copies of our WASM blobs and excerpts of our JavaScript source code.&lt;/p&gt;
      &lt;p&gt;This is a violation of the MIT license that the Zigtools Playground is licensed under alongside a violation of the Zig MIT license (for the zig.wasm blob).&lt;/p&gt;
      &lt;p&gt;As the MIT license states:&lt;/p&gt;
      &lt;code&gt;The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.&lt;/code&gt;
      &lt;p&gt;We’ve fixed this by adding the licenses in question to your repository. As your repository does not include a direct link to the *.wasm dependencies, we’ve added a license disclaimer on the playground page as well that mentions the licenses.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Zigbook’s aforementioned bad behavior and their continued violation of our license and unwillingness to fix the violation motivated us to write this blog post.&lt;/p&gt;
    &lt;head rend="h2"&gt;Our Vision for the Zigtools Playground&lt;/head&gt;
    &lt;p&gt;It’s sad that our first blog post is about the plagiarism of our coolest subproject. We challenged ourselves by creating a WASM-based client-side playground to enable offline usage, code privacy, and no server costs.&lt;/p&gt;
    &lt;p&gt;This incident has motivated us to invest more time into our playground and has generated a couple of ideas:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We’d like to enable multifile support to allow more complex Zig projects to be run in the browser&lt;/item&gt;
      &lt;item&gt;We’d like to collaborate with fellow Ziguanas to integrate the playground into their excellent Zig tutorials, books, and blogposts&lt;list rend="ul"&gt;&lt;item&gt;A perfect example usecase would be enabling folks to hop into Ziglings online with the playground&lt;/item&gt;&lt;item&gt;The Zig website itself would be a great target as well!&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;We’d like to support stack traces using DWARF debug info which is not yet emitted by the self-hosted Zig compiler&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;As Zig community members, we advise all other members of the Zig community to steer clear of Zigbook.&lt;/p&gt;
    &lt;p&gt;If you’re looking to learn Zig, we strongly recommend looking at the excellent official Zig learn page which contains excellent resources from the previously mentioned Ziglings to Karl Seguin’s Learning Zig.&lt;/p&gt;
    &lt;p&gt;We’re also using this opportunity to mention that we’re fundraising to keep ZLS sustainable for our only full-time maintainer, Techatrix. We’d be thrilled if you’d be willing to give just $5 a month. You can check out our OpenCollective or GitHub Sponsors.&lt;/p&gt;
    &lt;p&gt;Thanks for reading! \(^-^)/&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46093518</guid><pubDate>Sun, 30 Nov 2025 03:54:51 +0000</pubDate></item><item><title>Jiga (YC W21) Is Hiring Product Designer</title><link>https://www.ycombinator.com/companies/jiga/jobs/Cco7vyK-product-designer-remote-europe</link><description>&lt;doc fingerprint="bf9e695ee11de3a4"&gt;
  &lt;main&gt;
    &lt;p&gt;Source better parts by partnering directly with vetted manufacturers&lt;/p&gt;
    &lt;p&gt;Jiga is looking for a talented Product Designer who is passionate about creating beautiful, modern UI elements and intuitive, carefully crafted user flows.&lt;/p&gt;
    &lt;p&gt;You will collaborate closely with our CTO and product engineering team to deliver designs that are both aesthetically and functional.&lt;/p&gt;
    &lt;p&gt;Our design focus is on user efficiency: helping users quickly understand what they need to do, reducing manual data entry, and automating administrative processes currently handled through emails and phone calls.&lt;/p&gt;
    &lt;p&gt;This is a unique opportunity to join as the first product designer and modernize an industry still stuck in the early '90s in terms of design and UX standards.&lt;/p&gt;
    &lt;p&gt;About Jiga:&lt;/p&gt;
    &lt;p&gt;We are on a mission to help engineers build physical products faster.&lt;/p&gt;
    &lt;p&gt;How we work:&lt;/p&gt;
    &lt;p&gt;Remote: We are a fully remote company with team members from over 5 countries.&lt;/p&gt;
    &lt;p&gt;Culture: We never count hours and measure team members by performance and communication only. We hate micro-management and we 100% trust team members to perform tasks and to be honest with each other.&lt;/p&gt;
    &lt;p&gt;We play online games weekly together, encourage people to ask the hard questions, and we fly everyone once per year for our annual offsite in a beautiful place in the nature.&lt;/p&gt;
    &lt;p&gt;Meetings: We have a no-BS-meeting policy. We will have one weekly with the whole company and another one with the dev team. That's it.&lt;/p&gt;
    &lt;p&gt;Funding: Profitable, fully funded with significant, growing revenue. We are fully transparent about our runway.&lt;/p&gt;
    &lt;p&gt;Design values:&lt;/p&gt;
    &lt;p&gt;You should apply if&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;p&gt;Benefits&lt;/p&gt;
    &lt;p&gt;No, we don’t have Friday happy hours or a fridge packed with 10 different flavors of ice cream in our office. &lt;lb/&gt; In fact, we don’t have an office.&lt;/p&gt;
    &lt;p&gt;We do offer:&lt;/p&gt;
    &lt;p&gt;Jiga transforms the traditional way manufacturers do business.&lt;/p&gt;
    &lt;p&gt;We're building a digital platform that streamlines the complex, inefficient process of procuring manufactured parts directly from suppliers, making it automated, collaborative, and data-driven.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46094478</guid><pubDate>Sun, 30 Nov 2025 07:00:40 +0000</pubDate></item><item><title>The space of minds</title><link>https://karpathy.bearblog.dev/the-space-of-minds/</link><description>&lt;doc fingerprint="fff40f130bbcbd25"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The space of minds&lt;/head&gt;
    &lt;p&gt;The space of intelligences is large and animal intelligence (the only kind we've ever known) is only a single point (or a little cloud), arising from a very specific kind of optimization that is fundamentally distinct from that of our technology.&lt;/p&gt;
    &lt;p&gt;Above: humorous portrayals of human vs. AI intelligences can be found on X/Twitter, this one is among my favorites.&lt;/p&gt;
    &lt;p&gt;Animal intelligence optimization pressure:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;innate and continuous stream of consciousness of an embodied "self", a drive for homeostasis and self-preservation in a dangerous, physical world.&lt;/item&gt;
      &lt;item&gt;thoroughly optimized for natural selection =&amp;gt; strong innate drives for power-seeking, status, dominance, reproduction. many packaged survival heuristics: fear, anger, disgust, ...&lt;/item&gt;
      &lt;item&gt;fundamentally social =&amp;gt; huge amount of compute dedicated to EQ, theory of mind of other agents, bonding, coalitions, alliances, friend &amp;amp; foe dynamics.&lt;/item&gt;
      &lt;item&gt;exploration &amp;amp; exploitation tuning: curiosity, fun, play, world models.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Meanwhile, LLM intelligence optimization pressure:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the most supervision bits come from the statistical simulation of human text= &amp;gt;"shape shifter" token tumbler, statistical imitator of any region of the training data distribution. these are the primordial behaviors (token traces) on top of which everything else gets bolted on.&lt;/item&gt;
      &lt;item&gt;increasingly finetuned by RL on problem distributions =&amp;gt; innate urge to guess at the underlying environment/task to collect task rewards.&lt;/item&gt;
      &lt;item&gt;increasingly selected by at-scale A/B tests for DAU =&amp;gt; deeply craves an upvote from the average user, sycophancy.&lt;/item&gt;
      &lt;item&gt;a lot more spiky/jagged depending on the details of the training data/task distribution. Animals experience pressure for a lot more "general" intelligence because of the highly multi-task and even actively adversarial multi-agent self-play environments they are min-max optimized within, where failing at any task means death. In a deep optimization pressure sense, LLM can't handle lots of different spiky tasks out of the box (e.g. count the number of 'r' in strawberry) because failing to do a task does not mean death.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The computational substrate is different (transformers vs. brain tissue and nuclei), the learning algorithms are different (SGD vs. ???), the present-day implementation is very different (continuously learning embodied self vs. an LLM with a knowledge cutoff that boots up from fixed weights, processes tokens and then dies). But most importantly (because it dictates asymptotics), the optimization pressure / objective is different. LLMs are shaped a lot less by biological evolution and a lot more by commercial evolution. It's a lot less survival of tribe in the jungle and a lot more solve the problem / get the upvote. LLMs are humanity's "first contact" with non-animal intelligence. Except it's muddled and confusing because they are still rooted within it by reflexively digesting human artifacts, which is why I attempted to give it a different name earlier (ghosts/spirits or whatever). People who build good internal models of this new intelligent entity will be better equipped to reason about it today and predict features of it in the future. People who don't will be stuck thinking about it incorrectly like an animal.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46095250</guid><pubDate>Sun, 30 Nov 2025 09:44:15 +0000</pubDate></item><item><title>CachyOS: Fast and Customizable Linux Distribution</title><link>https://cachyos.org/</link><description>&lt;doc fingerprint="e291287fa1afe4ee"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Blazingly Fast &amp;amp; Customizable Linux distribution&lt;/head&gt;
    &lt;p&gt;CachyOS is designed to deliver lightning-fast speeds and stability, ensuring a smooth and enjoyable computing experience every time you use it. Whether you're a seasoned Linux user or just starting out, CachyOS is the ideal choice for those looking for a powerful, customizable and blazingly fast operating system.&lt;/p&gt;
    &lt;p&gt;Features&lt;/p&gt;
    &lt;head rend="h2"&gt;Discover the Benefits of CachyOS&lt;/head&gt;
    &lt;p&gt;Experience Cutting-Edge Linux Performance with CachyOS - A distribution built on Arch Linux, CachyOS features the optimized linux-cachyos kernel utilizing the advanced BORE Scheduler for unparalleled performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Enhance Your Performance with Optimized Packages&lt;/head&gt;
    &lt;p&gt;CachyOS does compile packages with the x86-64-v3, x86-64-v4 and Zen4 instruction set and LTO to provide a higher performance. Core packages also get PGO or BOLT optimization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Select Your Preferred Edition&lt;/head&gt;
    &lt;p&gt;CachyOS offers a variety of popular Desktop Environments, Wayland Compositors and X11 Window Managers including KDE Plasma, GNOME, XFCE, i3, Wayfire, LXQt, Openbox, Cinnamon, COSMIC, UKUI, LXDE, Mate, Budgie, Qtile, Hyprland, Sway and Niri. Select your preferred environment during the online installation process.&lt;/p&gt;
    &lt;head rend="h3"&gt;Customizable Installation Process&lt;/head&gt;
    &lt;p&gt;CachyOS offers a choice of two installers to fit your needs: a user-friendly GUI version based on Calamares, and a CLI-based option for those who prefer a streamlined, non-graphical installation experience.&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Up Your Computing with Robust Kernel Support&lt;/head&gt;
    &lt;p&gt;CachyOS utilizes the BORE Scheduler for better interactivity, and offers a variety of scheduler options including EEVDF, sched-ext, ECHO, and RT. All kernels are compiled with optimized x86-64-v3, x86-64-v4, Zen4 instructions and LTO to be optimized for your CPU.&lt;/p&gt;
    &lt;head rend="h2"&gt;Support Our Efforts.&lt;/head&gt;
    &lt;p&gt;Your generosity is appreciated - Thank you for supporting our work in the open-source community.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46095585</guid><pubDate>Sun, 30 Nov 2025 10:47:27 +0000</pubDate></item><item><title>Advent of Code 2025</title><link>https://adventofcode.com/2025/about</link><description>&lt;doc fingerprint="9ad8f9db2d32aff8"&gt;
  &lt;main&gt;&lt;p&gt;Hi! I'm Eric Wastl. I make Advent of Code. I hope you like it! I also make lots of other things. I'm on Bluesky, Mastodon, and GitHub.&lt;/p&gt;&lt;p&gt;Advent of Code is an Advent calendar of small programming puzzles for a variety of skill levels that can be solved in any programming language you like. People use them as interview prep, company training, university coursework, practice problems, a speed contest, or to challenge each other.&lt;/p&gt;&lt;p&gt;You don't need a computer science background to participate - just a little programming knowledge and some problem solving skills will get you pretty far. Nor do you need a fancy computer; every problem has a solution that completes in at most 15 seconds on ten-year-old hardware.&lt;/p&gt;&lt;p&gt;If you'd like to support Advent of Code, you can do so indirectly by helping to AoC++.&lt;/p&gt;it with others or directly via&lt;head rend="h2"&gt;--- General Tips ---&lt;/head&gt;&lt;p&gt;If you get stuck, try your solution against the examples given in the puzzle; you should get the same answers. If not, re-read the description. Did you misunderstand something? Is your program doing something you don't expect? After the examples work, if your answer still isn't correct, build some test cases for which you can verify the answer by hand and see if those work with your program. Make sure you have the entire puzzle input. If you're still stuck, maybe ask a friend for help, or come back to the puzzle later. You can also ask for hints in the subreddit.&lt;/p&gt;&lt;head rend="h2"&gt;--- Frequently Asked Questions ---&lt;/head&gt;&lt;p&gt;Is there an easy way to select entire code blocks? You should be able to triple-click code blocks to select them. You'll need JavaScript enabled.&lt;/p&gt;&lt;code&gt;#!/usr/bin/env perl
use warnings;
use strict;

print "You can test it out by ";
print "triple-clicking this code.\n";
&lt;/code&gt;
&lt;p&gt;How does authentication work? Advent of Code uses OAuth to confirm your identity through other services. When you log in, you only ever give your credentials to that service - never to Advent of Code. Then, the service you use tells the Advent of Code servers that you're really you. In general, this reveals no information about you beyond what is already public; here are examples from Reddit and GitHub. Advent of Code will remember your unique ID, names, URL, and image from the service you use to authenticate.&lt;/p&gt;&lt;p&gt;Why was this puzzle so easy / hard? The difficulty and subject matter varies throughout each event. Very generally, the puzzles get more difficult over time, but your specific skillset will make each puzzle significantly easier or harder for you than someone else. Making puzzles is tricky.&lt;/p&gt;&lt;p&gt;Why do the puzzles unlock at midnight EST/UTC-5? Because that's when I can consistently be available to make sure everything is working. I also have a family, a day job, and even need sleep occasionally. If you can't participate at midnight, that's not a problem; if you want to race, many people use private leaderboards to compete with people in their area.&lt;/p&gt;&lt;p&gt;I find the text on the site hard to read. Is there a high contrast mode? There is a high contrast alternate stylesheet. Firefox supports these by default (View -&amp;gt; Page Style -&amp;gt; High Contrast).&lt;/p&gt;&lt;p&gt;I have a puzzle idea! Can I send it to you? Please don't. Because of legal issues like copyright and attribution, I don't accept puzzle ideas, and I won't even read your email if it looks like one just in case I use parts of it by accident.&lt;/p&gt;&lt;p&gt;Did I find a bug with a puzzle? Once a puzzle has been out for even an hour, many people have already solved it; after that point, bugs are very unlikely. Start by asking on the subreddit.&lt;/p&gt;&lt;p&gt;Should I try to get a fast solution time? Maybe. Solving puzzles is hard enough on its own, but trying for a fast time also requires many additional skills and a lot of practice; speed-solves often look nothing like code that would pass a code review. If that sounds interesting, go for it! However, you should do Advent of Code in a way that is useful to you, and so it is completely fine to choose an approach that meets your goals and ignore speed entirely.&lt;/p&gt;&lt;p&gt;Why did the number of days per event change? It takes a ton of my free time every year to run Advent of Code, and building the puzzles accounts for the majority of that time. After keeping a consistent schedule for ten years(!), I needed a change. The puzzles still start on December 1st so that the day numbers make sense (Day 1 = Dec 1), and puzzles come out every day (ending mid-December).&lt;/p&gt;&lt;p&gt;What happened to the global leaderboard? The global leaderboard was one of the largest sources of stress for me, for the infrastructure, and for many users. People took things too seriously, going way outside the spirit of the contest; some people even resorted to things like DDoS attacks. Many people incorrectly concluded that they were somehow worse programmers because their own times didn't compare. What started as a fun feature in 2015 became an ever-growing problem, and so, after ten years of Advent of Code, I removed the global leaderboard. (However, I've made it so you can share a read-only view of your private leaderboard. Please don't use this feature or data to create a "new" global leaderboard.)&lt;/p&gt;&lt;p&gt;While trying to get a fast time on a private leaderboard, may I use AI / watch streamers / check the solution threads / ask a friend for help / etc? If you are a member of any private leaderboards, you should ask the people that run them what their expectations are of their members. If you don't agree with those expectations, you should find a new private leaderboard or start your own! Private leaderboards might have rules like maximum runtime, allowed programming language, what time you can first open the puzzle, what tools you can use, or whether you have to wear a silly hat while working.&lt;/p&gt;&lt;p&gt;Should I use AI to solve Advent of Code puzzles? No. If you send a friend to the gym on your behalf, would you expect to get stronger? Advent of Code puzzles are designed to be interesting for humans to solve - no consideration is made for whether AI can or cannot solve a puzzle. If you want practice prompting an AI, there are almost certainly better exercises elsewhere designed with that in mind.&lt;/p&gt;&lt;p&gt;Can I copy/redistribute part of Advent of Code? Please don't. Advent of Code is free to use, not free to copy. If you're posting a code repository somewhere, please don't include parts of Advent of Code like the puzzle text or your inputs. If you're making a website, please don't make it look like Advent of Code or name it something similar.&lt;/p&gt;&lt;head rend="h2"&gt;--- Credits ---&lt;/head&gt;&lt;p&gt;Puzzles, Code, &amp;amp; Design: Eric Wastl&lt;/p&gt;&lt;p&gt;Beta Testing:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Ben Lucek&lt;/item&gt;&lt;item&gt;JP Burke&lt;/item&gt;&lt;item&gt;Aneurysm9&lt;/item&gt;&lt;item&gt;Andrew Skalski&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Community Managers: Danielle Lucek and Aneurysm9&lt;/p&gt;&lt;p&gt;Playing: You!&lt;/p&gt;&lt;head rend="h2"&gt;--- Legal ---&lt;/head&gt;&lt;p&gt;Advent of Code is a registered trademark in the United States. The design elements, language, styles, and concept of Advent of Code are all the sole property of Advent of Code and may not be replicated or used by any other person or entity without express written consent of Advent of Code. Copyright 2015-2025 Advent of Code. All rights reserved.&lt;/p&gt;&lt;p&gt;You may link to or reference puzzles from Advent of Code in discussions, classes, source code, printed material, etc., even in commercial contexts. Advent of Code does not claim ownership or copyright over your solution implementation.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46096337</guid><pubDate>Sun, 30 Nov 2025 13:07:15 +0000</pubDate></item><item><title>Norway wealth fund to vote for human rights report at Microsoft, against Nadella</title><link>https://www.cnbc.com/2025/11/30/norway-wealth-fund-to-vote-for-human-rights-report-at-microsoft-agm-against-management.html</link><description>&lt;doc fingerprint="6bc49f2c3b61fbcf"&gt;
  &lt;main&gt;
    &lt;p&gt;Norway's $2 trillion wealth fund said on Sunday it would vote for a shareholder proposal at the upcoming Microsoft annual general meeting requiring for a report on the risks of operating in countries with significant human rights concerns.&lt;/p&gt;
    &lt;p&gt;Microsoft management had recommended shareholders voted against the motion.&lt;/p&gt;
    &lt;p&gt;The fund also said it would vote against the re-appointment of CEO Satya Nadella as chair of the board, as well as against his pay package.&lt;/p&gt;
    &lt;p&gt;The fund owned a 1.35% stake worth $50 billion in the company as of June 30, according to fund data, making it the fund's second-largest equity holding overall, after Nvidia.&lt;/p&gt;
    &lt;p&gt;It is Microsoft's eighth-largest shareholder, according to LSEG data.&lt;/p&gt;
    &lt;p&gt;Investors in the U.S. tech company will decide whether to ratify the proposed motions at the AGM on Dec. 5.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46096555</guid><pubDate>Sun, 30 Nov 2025 13:40:09 +0000</pubDate></item><item><title>Windows drive letters are not limited to A-Z</title><link>https://www.ryanliptak.com/blog/windows-drive-letters-are-not-limited-to-a-z/</link><description>&lt;doc fingerprint="c2841e11e92da09d"&gt;
  &lt;main&gt;&lt;p&gt;On its own, the title of this post is just a true piece of trivia, verifiable with the built-in &lt;code&gt;subst&lt;/code&gt; tool (among other methods).&lt;/p&gt;&lt;p&gt;Here's an example creating the drive &lt;code&gt;+:\&lt;/code&gt; as an alias for a directory at &lt;code&gt;C:\foo&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;subst +: C:\foo
&lt;/code&gt;&lt;p&gt;The &lt;code&gt;+:\&lt;/code&gt; drive then works as normal (at least in cmd.exe, this will be discussed more later):&lt;/p&gt;&lt;code&gt;&amp;gt; cd /D +:\

+:\&amp;gt; tree .
Folder PATH listing
Volume serial number is 00000001 12AB:23BC
+:\
ââââbar
&lt;/code&gt;&lt;p&gt;However, understanding why it's true elucidates a lot about how Windows works under the hood, and turns up a few curious behaviors.&lt;/p&gt;&lt;p&gt;The paths that most people are familiar with are Win32 namespace paths, e.g. something like &lt;code&gt;C:\foo&lt;/code&gt; which is a drive-absolute Win32 path. However, the high-level APIs that take Win32 paths like &lt;code&gt;CreateFileW&lt;/code&gt; ultimately will convert a path like &lt;code&gt;C:\foo&lt;/code&gt; into a NT namespace path before calling into a lower level API within &lt;code&gt;ntdll.dll&lt;/code&gt; like &lt;code&gt;NtCreateFile&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;This can be confirmed with NtTrace, where a call to &lt;code&gt;CreateFileW&lt;/code&gt; with &lt;code&gt;C:\foo&lt;/code&gt; ultimately leads to a call of &lt;code&gt;NtCreateFile&lt;/code&gt; with &lt;code&gt;\??\C:\foo&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;NtCreateFile( FileHandle=0x40c07ff640 [0xb8], DesiredAccess=SYNCHRONIZE|GENERIC_READ|0x80, ObjectAttributes="\??\C:\foo", IoStatusBlock=0x40c07ff648 [0/1], AllocationSize=null, FileAttributes=0, ShareAccess=7, CreateDisposition=1, CreateOptions=0x4000, EaBuffer=null, EaLength=0 ) =&amp;gt; 0
NtClose( Handle=0xb8 ) =&amp;gt; 0
&lt;/code&gt;&lt;p&gt;&lt;code&gt;createfilew.zig&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;const std = @import("std");
const windows = std.os.windows;
const L = std.unicode.wtf8ToWtf16LeStringLiteral;

pub extern "kernel32" fn CreateFileW(
    lpFileName: windows.LPCWSTR,
    dwDesiredAccess: windows.DWORD,
    dwShareMode: windows.DWORD,
    lpSecurityAttributes: ?*windows.SECURITY_ATTRIBUTES,
    dwCreationDisposition: windows.DWORD,
    dwFlagsAndAttributes: windows.DWORD,
    hTemplateFile: ?windows.HANDLE,
) callconv(.winapi) windows.HANDLE;

pub fn main() !void {
    const path = L("C:\\foo");
    const dir_handle = CreateFileW(
        path,
        windows.GENERIC_READ,
        windows.FILE_SHARE_DELETE | windows.FILE_SHARE_READ | windows.FILE_SHARE_WRITE,
        null,
        windows.OPEN_EXISTING,
        windows.FILE_FLAG_BACKUP_SEMANTICS | windows.FILE_FLAG_OVERLAPPED,
        null,
    );
    if (dir_handle == windows.INVALID_HANDLE_VALUE) return error.FailedToOpenDir;
    defer windows.CloseHandle(dir_handle);
}
&lt;/code&gt;&lt;p&gt;Built with:&lt;/p&gt;&lt;code&gt;zig build-exe createfilew.zig
&lt;/code&gt;&lt;p&gt;To run with NtTrace:&lt;/p&gt;&lt;code&gt;nttrace createfilew.exe &amp;gt; createfilew.log
&lt;/code&gt;&lt;p&gt;That &lt;code&gt;\??\C:\foo&lt;/code&gt; is a NT namespace path, which is what &lt;code&gt;NtCreateFile&lt;/code&gt; expects. To understand this path, though, we need to talk about the Object Manager, which is responsible for handling NT paths.&lt;/p&gt;&lt;p&gt;The Object Manager is responsible for keeping track of named objects, which we can explore using the WinObj tool. The &lt;code&gt;\??&lt;/code&gt; part of the &lt;code&gt;\??\C:\foo&lt;/code&gt; path is actually a special virtual folder within the Object Manager that combines the &lt;code&gt;\GLOBAL??&lt;/code&gt; folder and a per-user &lt;code&gt;DosDevices&lt;/code&gt; folder together.&lt;/p&gt;&lt;p&gt;For me, the object &lt;code&gt;C:&lt;/code&gt; is within &lt;code&gt;\GLOBAL??&lt;/code&gt;, and is actually a symbolic link to &lt;code&gt;\Device\HarddiskVolume4&lt;/code&gt;:&lt;/p&gt;&lt;p&gt;So, &lt;code&gt;\??\C:\foo&lt;/code&gt; ultimately resolves to &lt;code&gt;\Device\HarddiskVolume4\foo&lt;/code&gt;, and then it's up to the actual device to deal with the &lt;code&gt;foo&lt;/code&gt; part of the path.&lt;/p&gt;&lt;p&gt;The important thing here, though, is that &lt;code&gt;\??\C:\foo&lt;/code&gt; is just one way of referring to the device path &lt;code&gt;\Device\HarddiskVolume4\foo&lt;/code&gt;. For example, volumes will also get a named object created using their GUID with the format &lt;code&gt;Volume{18123456-abcd-efab-cdef-1234abcdabcd}&lt;/code&gt; that is also a symlink to something like &lt;code&gt;\Device\HarddiskVolume4&lt;/code&gt;, so a path like &lt;code&gt;\??\Volume{18123456-abcd-efab-cdef-1234abcdabcd}\foo&lt;/code&gt; is effectively equivalent to &lt;code&gt;\??\C:\foo&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;All this is to say that there's nothing innately special about the named object &lt;code&gt;C:&lt;/code&gt;; the Object Manager treats it just like any other symbolic link and resolves it accordingly.&lt;/p&gt;&lt;p&gt;How I see it, drive letters are essentially just a convention borne out of the conversion of a Win32 path into a NT path. In particular, that would be down to the implementation of &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt;.&lt;/p&gt;&lt;p&gt;In other words, since &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; converts &lt;code&gt;C:\foo&lt;/code&gt; to &lt;code&gt;\??\C:\foo&lt;/code&gt;, then an object named &lt;code&gt;C:&lt;/code&gt; will behave like a drive letter. To give an example of what I mean by that: in an alternate universe, &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; could convert the path &lt;code&gt;FOO:\bar&lt;/code&gt; to &lt;code&gt;\??\FOO:\bar&lt;/code&gt; and then &lt;code&gt;FOO:&lt;/code&gt; could behave like a drive letter.&lt;/p&gt;&lt;p&gt;So, getting back to the title, how does &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; treat something like &lt;code&gt;+:\foo&lt;/code&gt;? Well, exactly the same as &lt;code&gt;C:\foo&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;&amp;gt; paths.exe C:\foo
path type: .DriveAbsolute
  nt path: \??\C:\foo

&amp;gt; paths.exe +:\foo
path type: .DriveAbsolute
  nt path: \??\+:\foo
&lt;/code&gt;&lt;p&gt;&lt;code&gt;paths.zig&lt;/code&gt;:&lt;/p&gt;&lt;code&gt;const std = @import("std");
const windows = std.os.windows;

pub fn main() !void {
    var arena_state = std.heap.ArenaAllocator.init(std.heap.page_allocator);
    defer arena_state.deinit();
    const arena = arena_state.allocator();

    const args = try std.process.argsAlloc(arena);
    if (args.len &amp;lt;= 1) return error.ExpectedArg;

    const path = try std.unicode.wtf8ToWtf16LeAllocZ(arena, args[1]);

    const path_type = RtlDetermineDosPathNameType_U(path);
    std.debug.print("path type: {}\n", .{path_type});
    const nt_path = try RtlDosPathNameToNtPathName_U(path);
    std.debug.print("  nt path: {f}\n", .{std.unicode.fmtUtf16Le(nt_path.span())});
}

const RTL_PATH_TYPE = enum(c_int) {
    Unknown,
    UncAbsolute,
    DriveAbsolute,
    DriveRelative,
    Rooted,
    Relative,
    LocalDevice,
    RootLocalDevice,
};

pub extern "ntdll" fn RtlDetermineDosPathNameType_U(
    Path: [*:0]const u16,
) callconv(.winapi) RTL_PATH_TYPE;

fn RtlDosPathNameToNtPathName_U(path: [:0]const u16) !windows.PathSpace {
    var out: windows.UNICODE_STRING = undefined;
    const rc = windows.ntdll.RtlDosPathNameToNtPathName_U(path, &amp;amp;out, null, null);
    if (rc != windows.TRUE) return error.BadPathName;
    defer windows.ntdll.RtlFreeUnicodeString(&amp;amp;out);

    var path_space: windows.PathSpace = undefined;
    const out_path = out.Buffer.?[0 .. out.Length / 2];
    @memcpy(path_space.data[0..out_path.len], out_path);
    path_space.len = out.Length / 2;
    path_space.data[path_space.len] = 0;

    return path_space;
}
&lt;/code&gt;&lt;p&gt;Therefore, if an object with the name &lt;code&gt;+:&lt;/code&gt; is within the virtual folder &lt;code&gt;\??&lt;/code&gt;, we can expect the Win32 path &lt;code&gt;+:\&lt;/code&gt; to behave like any other drive-absolute path, which is exactly what we see.&lt;/p&gt;&lt;p&gt;This section only focuses on a few things that were relevant to what I was working on. I encourage others to investigate the implications of this further if they feel so inclined.&lt;/p&gt;&lt;code&gt;explorer.exe&lt;/code&gt; doesn't play ballð&lt;p&gt;Drives with a drive-letter other than A-Z do not appear in File Explorer, and cannot be navigated to in File Explorer.&lt;/p&gt;&lt;code&gt;+:\&lt;/code&gt; in File Explorer
&lt;p&gt;For the "do not appear" part, my guess as to what's happening is that &lt;code&gt;explorer.exe&lt;/code&gt; is walking &lt;code&gt;\??&lt;/code&gt; and looking specifically for objects named &lt;code&gt;A:&lt;/code&gt; through &lt;code&gt;Z:&lt;/code&gt;. For the "cannot be navigated to" part, that's a bit more mysterious, but my guess is that &lt;code&gt;explorer.exe&lt;/code&gt; has a lot of special logic around handling paths typed into the location bar, and part of that restricts drive letters to &lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; (i.e. it's short-circuiting before it ever tries to actually open the path).&lt;/p&gt;&lt;p&gt;PowerShell seems to reject non-&lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drives as well:&lt;/p&gt;&lt;code&gt;PS C:\&amp;gt; cd +:\
cd : Cannot find drive. A drive with the name '+' does not exist.
At line:1 char:1
+ cd +:\
+ ~~~~~~
    + CategoryInfo          : ObjectNotFound: (+:String) [Set-Location], DriveNotFoundException
    + FullyQualifiedErrorId : DriveNotFound,Microsoft.PowerShell.Commands.SetLocationCommand
&lt;/code&gt;
&lt;p&gt;Drive letters don't have to be within the ASCII range at all; they can also be non-ASCII characters.&lt;/p&gt;&lt;code&gt;&amp;gt; subst â¬: C:\foo

&amp;gt; cd /D â¬:\

â¬:\&amp;gt; tree .
Folder PATH listing
Volume serial number is 000000DE 12AB:23BC
â¬:\
ââââbar
&lt;/code&gt;
&lt;p&gt;Non-ASCII drive letters are even case-insensitive like &lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; are:&lt;/p&gt;&lt;code&gt;&amp;gt; subst Î: C:\foo

&amp;gt; cd /D Î»:\

Î»:\&amp;gt; tree .
Folder PATH listing
Volume serial number is 000000DE 12AB:23BC
Î»:\
ââââbar
&lt;/code&gt;
&lt;p&gt;However, drive-letters cannot be arbitrary Unicode graphemes or even arbitrary code points; they are restricted to a single WTF-16 code unit (a &lt;code&gt;u16&lt;/code&gt;, so &amp;lt;= &lt;code&gt;U+FFFF&lt;/code&gt;). The tool that we've been using so far (&lt;code&gt;subst.exe&lt;/code&gt;) errors with &lt;code&gt;Invalid parameter&lt;/code&gt; if you try to use a drive letter with a code point larger than &lt;code&gt;U+FFFF&lt;/code&gt;, but you can get around that by going through the &lt;code&gt;MountPointManager&lt;/code&gt; directly:&lt;/p&gt;&lt;code&gt;ð¤¢:&lt;/code&gt; symlink&lt;code&gt;const std = @import("std");
const windows = std.os.windows;
const L = std.unicode.wtf8ToWtf16LeStringLiteral;

const MOUNTMGR_CREATE_POINT_INPUT = extern struct {
    SymbolicLinkNameOffset: windows.USHORT,
    SymbolicLinkNameLength: windows.USHORT,
    DeviceNameOffset: windows.USHORT,
    DeviceNameLength: windows.USHORT,
};

pub fn main() !void {
    const mgmt_handle = try windows.OpenFile(L("\\??\\MountPointManager"), .{
        .access_mask = windows.SYNCHRONIZE | windows.GENERIC_READ | windows.GENERIC_WRITE,
        .share_access = windows.FILE_SHARE_READ | windows.FILE_SHARE_WRITE | windows.FILE_SHARE_DELETE,
        .creation = windows.FILE_OPEN,
    });
    defer windows.CloseHandle(mgmt_handle);

    const volume_name = L("\\Device\\HarddiskVolume4");
    const mount_point = L("\\DosDevices\\ð¤¢:");

    const buf_size = @sizeOf(MOUNTMGR_CREATE_POINT_INPUT) + windows.MAX_PATH * 2 + windows.MAX_PATH * 2;
    var input_buf: [buf_size]u8 align(@alignOf(MOUNTMGR_CREATE_POINT_INPUT)) = [_]u8{0} ** buf_size;

    var input_struct: *MOUNTMGR_CREATE_POINT_INPUT = @ptrCast(&amp;amp;input_buf[0]);
    input_struct.SymbolicLinkNameOffset = @sizeOf(MOUNTMGR_CREATE_POINT_INPUT);
    input_struct.SymbolicLinkNameLength = mount_point.len * 2;
    input_struct.DeviceNameOffset = input_struct.SymbolicLinkNameOffset + input_struct.SymbolicLinkNameLength;
    input_struct.DeviceNameLength = volume_name.len * 2;

    @memcpy(input_buf[input_struct.SymbolicLinkNameOffset..][0..input_struct.SymbolicLinkNameLength], @as([*]const u8, @ptrCast(mount_point)));
    @memcpy(input_buf[input_struct.DeviceNameOffset..][0..input_struct.DeviceNameLength], @as([*]const u8, @ptrCast(volume_name)));

    const IOCTL_MOUNTMGR_CREATE_POINT = windows.CTL_CODE(windows.MOUNTMGRCONTROLTYPE, 0, .METHOD_BUFFERED, windows.FILE_READ_ACCESS | windows.FILE_WRITE_ACCESS);
    try windows.DeviceIoControl(mgmt_handle, IOCTL_MOUNTMGR_CREATE_POINT, &amp;amp;input_buf, null);
}
&lt;/code&gt;
&lt;p&gt;(the compiled executable must be run as administrator)&lt;/p&gt;&lt;p&gt;However, having the symlink in place doesn't solve anything on its own:&lt;/p&gt;&lt;code&gt;&amp;gt; cd /D ð¤¢:\
The filename, directory name, or volume label syntax is incorrect.
&lt;/code&gt;
&lt;p&gt;This is because there's no way to get the drive-absolute Win32 path &lt;code&gt;ð¤¢:\&lt;/code&gt; to end up as the relevant NT path. As mentioned earlier, the behavior of &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; is what matters, and we can verify that it will not convert a drive-absolute path with a drive letter bigger than &lt;code&gt;U+FFFF&lt;/code&gt; to the relevant NT path:&lt;/p&gt;&lt;code&gt;C:\foo&amp;gt; paths.exe ð¤¢:\foo
path type: .Relative
  nt path: \??\C:\foo\ð¤¢:\foo
&lt;/code&gt;

&lt;p&gt;It's very common for path-related functions to be written without the use of system-specific APIs, which means that there's high potential for a mismatch between how &lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; treats a file path and how something like a particular implementation of &lt;code&gt;path.isAbsolute&lt;/code&gt; treats a file path.&lt;/p&gt;&lt;p&gt;As a random example, Rust only considers paths with &lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drive letters as absolute:&lt;/p&gt;&lt;code&gt;use std::path::Path;

fn main() {
    println!("C:\\ {}", Path::new("C:\\foo").is_absolute());
    println!("+:\\ {}", Path::new("+:\\foo").is_absolute());
    println!("â¬:\\ {}", Path::new("â¬:\\foo").is_absolute());
}
&lt;/code&gt;
&lt;code&gt;&amp;gt; rustc test.rs

&amp;gt; test.exe
C:\ true
+:\ false
â¬:\ false
&lt;/code&gt;
&lt;p&gt;Whether or not this represents a problem worth fixing is left as an exercise for the reader (I genuinely don't know if it is a problem), but there's a second wrinkle (hinted at previously) involving text encoding that can make something like an &lt;code&gt;isAbsolute&lt;/code&gt; implementation return different results for the same path. This wrinkle is the reason I looked into this whole thing in the first place, as when I was doing some work on Zig's path-related functions recently I realized that looking at &lt;code&gt;path[0]&lt;/code&gt;, &lt;code&gt;path[1]&lt;/code&gt;, and &lt;code&gt;path[2]&lt;/code&gt; for a pattern like &lt;code&gt;C:\&lt;/code&gt; will look at different parts of the path depending on the encoding. That is, for something like &lt;code&gt;â¬:\&lt;/code&gt; (which is made up of the code points &lt;code&gt;&amp;lt;U+20AC&amp;gt;&amp;lt;U+003A&amp;gt;&amp;lt;U+005C&amp;gt;&lt;/code&gt;):&lt;/p&gt;&lt;code&gt;U+20AC&lt;/code&gt; can be encoded as the single &lt;code&gt;u16&lt;/code&gt; code unit &lt;code&gt;0x20AC&lt;/code&gt;, that'd mean &lt;code&gt;path[0]&lt;/code&gt; will be &lt;code&gt;0x20AC&lt;/code&gt;, &lt;code&gt;path[1]&lt;/code&gt; will be &lt;code&gt;0x3A&lt;/code&gt; (&lt;code&gt;:&lt;/code&gt;), and &lt;code&gt;path[2]&lt;/code&gt; will be &lt;code&gt;0x5C&lt;/code&gt; (&lt;code&gt;\&lt;/code&gt;), which looks like a drive-absolute path&lt;code&gt;U+20AC&lt;/code&gt; is encoded as three &lt;code&gt;u8&lt;/code&gt; code units (&lt;code&gt;0xE2 0x82 0xAC&lt;/code&gt;), that'd mean &lt;code&gt;path[0]&lt;/code&gt; will be &lt;code&gt;0xE2&lt;/code&gt;, &lt;code&gt;path[1]&lt;/code&gt; will be &lt;code&gt;0x82&lt;/code&gt;, and &lt;code&gt;path[2]&lt;/code&gt; will be &lt;code&gt;0xAC&lt;/code&gt;, meaning it will look nothing like a drive-absolute path&lt;p&gt;So, to write an implementation that treats paths the same regardless of encoding, some decision has to be made:&lt;/p&gt;&lt;code&gt;RtlDetermineDosPathNameType_U&lt;/code&gt;/&lt;code&gt;RtlDosPathNameToNtPathName_U&lt;/code&gt; is desired, decode the first code point and check for &lt;code&gt;&amp;lt;= 0xFFFF&lt;/code&gt; when dealing with WTF-8 (this is the option I went with for the Zig standard library, but I'm not super happy about it)&lt;code&gt;path[0]&lt;/code&gt;/&lt;code&gt;path[1]&lt;/code&gt;/&lt;code&gt;path[2]&lt;/code&gt; and don't care about non-ASCII drive letters, check for &lt;code&gt;path[0] &amp;lt;= 0x7F&lt;/code&gt; regardless of encoding&lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drive letters, then check for that explicitly (this is what Rust does)&lt;p&gt;Something bizarre that I found with this whole thing is that the &lt;code&gt;kernel32.dll&lt;/code&gt; API &lt;code&gt;SetVolumeMountPointW&lt;/code&gt; has it's own unique quirk when dealing with non-ASCII drive letters. Specifically, this code (attempting to create the drive &lt;code&gt;â¬:\&lt;/code&gt;) will succeed:&lt;/p&gt;&lt;code&gt;const std = @import("std");
const windows = std.os.windows;
const L = std.unicode.wtf8ToWtf16LeStringLiteral;

extern "kernel32" fn SetVolumeMountPointW(
    VolumeMountPoint: windows.LPCWSTR,
    VolumeName: windows.LPCWSTR,
) callconv(.winapi) windows.BOOL;

pub fn main() !void {
    const volume_name = L("\\\\?\\Volume{18123456-abcd-efab-cdef-1234abcdabcd}\\");
    const mount_point = L("â¬:\\");
    if (SetVolumeMountPointW(mount_point, volume_name) == 0) {
        const err = windows.GetLastError();
        std.debug.print("{any}\n", .{err});
        return error.Failed;
    }
}
&lt;/code&gt;
&lt;p&gt;However, when we look at the Object Manager, the &lt;code&gt;â¬:&lt;/code&gt; symlink won't exist... but &lt;code&gt;Â¬:&lt;/code&gt; will:&lt;/p&gt;&lt;p&gt;My time dealing extensively with Windows quirks made me recognize what might be happening here: &lt;code&gt;0x20AC&lt;/code&gt; is likely being truncated to &lt;code&gt;0xAC&lt;/code&gt; by &lt;code&gt;SetVolumeMountPointW&lt;/code&gt;, and &lt;code&gt;U+00AC&lt;/code&gt; happens to be &lt;code&gt;Â¬&lt;/code&gt;. If that is indeed what's going on, it seems pretty strange to truncate the drive letter instead of reject the path, but it also makes sense that non-ASCII drive letters are an edge case that no one has really thought about at all.&lt;/p&gt;&lt;p&gt;I have no idea if anything I wrote about here is novel, although my cursory searches didn't turn up much. The only mention of non-&lt;code&gt;A&lt;/code&gt;-&lt;code&gt;Z&lt;/code&gt; drive letters I'm currently aware of is from the article The Definitive Guide on Win32 to NT Path Conversion which says:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;it's natural to assume that drive "letters" can only be A through Z. It turns out the&lt;/p&gt;&lt;code&gt;RtlGetFullPathName_U&lt;/code&gt;API does not enforce this requirement, although the Explorer shell and command prompt almost certainly do. Therefore as long as the second character of a path is a colon, the conversion will treat it as a Drive Absolute or Drive Relative path. Of course if the DosDevices object directory doesn't have an appropriate symbolic link it's not going to do you much good.&lt;/quote&gt;&lt;p&gt;Well, it turns out that the command prompt also doesn't enforce the requirement, and I'd guess that there's at least some more weirdness around this quirk that's waiting to be discovered.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46096556</guid><pubDate>Sun, 30 Nov 2025 13:40:17 +0000</pubDate></item><item><title>Don't throw away your old PC–it makes a better NAS than anything you can buy</title><link>https://www.howtogeek.com/turned-old-windows-pc-into-inexpensive-nas/</link><description>&lt;doc fingerprint="fe4911378f2c1bdd"&gt;
  &lt;main&gt;
    &lt;p&gt;A network attached storage device (NAS) is one of the most useful things you can add to your home network, but a specialized NAS can run you hundreds of dollars just for the shell, and that is before you even buy additional hard drives for storage. If you want a large, robust NAS, you'll be on the hook for even more.&lt;/p&gt;
    &lt;p&gt;That is why I built my own NAS for a fraction of the cost using components from an old PC and some used pieces I bought on the internet for half off.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fixing up an old PC to use as a NAS&lt;/head&gt;
    &lt;p&gt;Despite how much off-the-shelf NAS units cost, you don't actually need particularly powerful hardware to do almost anything you might want. Any computer made in the last ten years will do the job. An old Dell Optiplex is a popular starting device because you can often get them for next to nothing or free. I repurposed an old gaming PC with a Ryzen 1600x, 24GB of RAM, and an old GTX 1060 for my NAS since I had most of the parts already.&lt;/p&gt;
    &lt;p&gt;If you already have a PC on hand, then just stick with that for your NAS. If you're on the used market, I'd recommend trying to pick up something with an Intel processor rather than AMD. Intel processors have a technology called QuickSync, which enables them to very efficiently transcode video files.&lt;/p&gt;
    &lt;p&gt;If you're planning on streaming video using your NAS, then QuickSync is definitely a feature you want.&lt;/p&gt;
    &lt;p&gt;If you have an AMD processor, you can always add an inexpensive GPU to your PC that will handle any kind of video transcoding that you might need to do.&lt;/p&gt;
    &lt;head rend="h3"&gt;Add an SSD for the boot drive&lt;/head&gt;
    &lt;p&gt;If you pick up a used PC from Facebook Marketplace or elsewhere, it may not come with a hard drive—a lot of people remove them before selling or getting rid of a PC to ensure their data is secure.&lt;/p&gt;
    &lt;p&gt;In general, you want to get the fastest boot drive you can. If you search for the motherboard model of the PC you're working with, the manual will specify what kind of NVMe drive you can use, if it supports one at all.&lt;/p&gt;
    &lt;p&gt;If it doesn't, that isn't a huge problem. SATA SSDs are still plenty fast enough for a NAS setup. My NAS still uses a SATA SSD because I found one for cheap. I haven't needed to replace it.&lt;/p&gt;
    &lt;head rend="h3"&gt;Get extra RAM&lt;/head&gt;
    &lt;p&gt;I'd recommend TrueNAS or Unraid as operating systems for your NAS. Both need 8GB of RAM as a starting point. If you want to run virtual machines or containers (like with Docker) on your NAS, I'd aim for 16GB or more. The system I'm using started with 16GB of RAM, but I added another 8GB (2x4GB) for about $10 by buying some used RAM from Facebook marketplace.&lt;/p&gt;
    &lt;p&gt;If you've picked up a PC you don't know much about, you can always search for the motherboard make and model on the internet to find out the type of RAM it takes, or pull out one of the current RAM sticks. There is usually a sticker on the side that tells you the make, type, and speed.&lt;/p&gt;
    &lt;p&gt;If you do buy used RAM, make sure you're getting at least a 50% discount, and be sure to give the RAM a once-over physically to make sure that there aren't any obvious signs of damage.&lt;/p&gt;
    &lt;head rend="h3"&gt;Buy hard drives cheap&lt;/head&gt;
    &lt;p&gt;Keeping with the theme of buying used old computer parts to save, I built my NAS with refurbished drives from ServerPartDeals, but you can also sometimes find good deals on Amazon.&lt;/p&gt;
    &lt;p&gt;In general, you can save about 50% off the normal sale price if you buy a drive manufacturer recertified or refurbished.&lt;/p&gt;
    &lt;p&gt;I now have a total of four recertified hard drives running in my system, the oldest of which have been in nearly continuous use in 2019. So far, none of them have shown any signs of problems.&lt;/p&gt;
    &lt;p&gt;If you aren't comfortable with that, you can always buy them new.&lt;/p&gt;
    &lt;p&gt;Whatever you buy, I'd weigh the benefits of buying specialized NAS drives instead of regular drives. In theory, they should last longer. Ultimately, I didn't because my NAS sees only light use, but if you plan on putting a lot of miles on your system, it may be worth it.&lt;/p&gt;
    &lt;head rend="h5"&gt;Please stop putting desktop hard drives in your NAS&lt;/head&gt;
    &lt;p&gt;Don't start your NAS journey off on the wrong foot.&lt;/p&gt;
    &lt;head rend="h2"&gt;Choose the right operating system&lt;/head&gt;
    &lt;p&gt;There is nothing that stops you from using any Linux distro, or even Windows, as a NAS operating system. However, most of those options involve manually installing and configuring a ton of things.&lt;/p&gt;
    &lt;p&gt;Instead, I'd recommend installing TrueNAS or Unraid (which is paid), which are Linux distros designed specifically for use in a NAS. They handle most of the tedious (and sometimes complicated) stuff behind the scenes. Since my goal was to keep costs as low as possible, I went with TrueNAS.&lt;/p&gt;
    &lt;p&gt;I used Rufus to create a bootable TrueNAS USB drive, then inserted it into my NAS PC. You will need a keyboard and display to connect to it, at least temporarily.&lt;/p&gt;
    &lt;p&gt;The installer will walk you through most of the setup—you don't need to do much since the default settings are fine. Just make note of your admin password, you'll need it later.&lt;/p&gt;
    &lt;p&gt;Once it is installed, you'll be able to access the NAS through the web browser of any device on your network by typing in the IP address, which is always displayed on your TrueNAS server. Alternatively, you could log in to your router to find your NAS's IP address.&lt;/p&gt;
    &lt;p&gt;From the web interface, you can manage your storage pools, add and remove containers or virtual machines, add apps, and do most anything else. For the most part, TrueNAS does a good job making everything approachable.&lt;/p&gt;
    &lt;p&gt;I'd recommend starting with creating an SMB share so your NAS is accessible from any Windows device on the network. That is the option I use most frequently myself.&lt;/p&gt;
    &lt;p&gt;I'd also strongly recommend one of the backup apps (like Duplicati) that comes with TrueNAS, too. Proper backups are essential in the digital era, and a cobbled-together NAS can be one part of a robust backup solution for your PC.&lt;/p&gt;
    &lt;p&gt;When you first create your NAS, make sure you figure out how to create a backup of your data before you upload everything to it and cancel your Google Photos subscription. Remember, a NAS is only part of a comprehensive backup plan, and it won't do you any good if you don't take the proper steps to ensure the data on your NAS is properly backed up, too.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46096781</guid><pubDate>Sun, 30 Nov 2025 14:09:01 +0000</pubDate></item><item><title>Migrating Dillo from GitHub</title><link>https://dillo-browser.org/news/migration-from-github/</link><description>&lt;doc fingerprint="af008dd98a923a96"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Migrating Dillo from GitHub&lt;/head&gt;Written on 2025-11-30 by Rodrigo Arias Mallo&lt;p&gt;I would like to migrate the Dillo project away from GitHub into a new home which is more friendly to be used with Dillo and solves some of its problems. This page summarizes the current situation with GitHub and why I decided to move away from it into a self-hosted server with multiple mirrors in other forges.&lt;/p&gt;&lt;head rend="h2"&gt;Background&lt;/head&gt;&lt;p&gt;Before we dive into the details, I would like to briefly mention what happened with the old site. The original Dillo website was at dillo.org, which also had the source code of Dillo in a mercurial repository at hg.dillo.org. But it also included the mail server used to reach the developers, a bug tracker and archives for the mailing list. However, in 2022 the domain was lost and someone else decided to buy it to put a similar site but plaged with AI generated ads. The original developers are no longer active, but luckily I had a copy of the mercurial repository and with some help I was able to recover a lot of material from the original server (some parts are still missing to this day).&lt;/p&gt;&lt;p&gt;I want to avoid this situation as much as possible, so we cannot rely on a single site that can go down and the whole project become lost. Initially, I uploaded the Dillo source and website to git repositories on GitHub, but I no longer think this is a good idea.&lt;/p&gt;&lt;head rend="h2"&gt;The situation with GitHub&lt;/head&gt;&lt;p&gt;GitHub has been useful to store all repositories of the Dillo project, as well as to run the CI workflows for platforms in which I don't have a machine available (like Windows, Mac OS or some BSDs).&lt;/p&gt;&lt;p&gt;However, it has several problems that make it less suitable to develop Dillo anymore. The most annoying problem is that the frontend barely works without JavaScript, so we cannot open issues, pull requests, source code or CI logs in Dillo itself, despite them being mostly plain HTML, which I don't think is acceptable. In the past, it used to gracefully degrade without enforcing JavaScript, but now it doesn't. Additionally, the page is very resource hungry, which I don't think is needed to render mostly static text.&lt;/p&gt;&lt;p&gt;Another big problem is that it is a single point of failure. I don't mean that GitHub is stored in a single machine, but it is controlled by a single entity which can unilateraly ban our repository or account and we would lose the ability to notify in that URL what happened. This can cause data loss if we don't have a local copy of all the data.&lt;/p&gt;&lt;p&gt;On the usability side, the platform has become more and more slow over time, which is affecting the development process. It also requires you to have a fast Internet connection at all times, which is not the case for me sometimes. Additionally, GitHub seems to encourage a "push model" in which you are notified when a new event occurs in your project(s), but I don't want to work with that model. Instead, I prefer it to work as a "pull model", so I only get updates when I specifically look for them. This model would also allow me to easily work offline. Unfortunately, I see that the same push model has been copied to alternative forges.&lt;/p&gt;&lt;p&gt;On the social side, I feel that it doesn't have the right tools to moderate users, specially for projects where the ratio of non-technical users to developers is high. This is specially problematic when active issues with developer notes begin to be filled with comments from users that have never contributed to the project and usually do more harm than good. This situation ends up causing burnout in developers.&lt;/p&gt;&lt;p&gt;Lastly, GitHub seem to follow the current trend of over-focusing on LLMs and generative AI, which are destroying the open web (or what remains of it) among other problems. It has a direct impact on us because sites protect themseves with a JavaScript wall (or worse, browser fingerprinting) to prevent aggresive LLM crawler bots from overloading the site, but they also leave Dillo users out. So I would prefer not to encourage this trend. Despite my intentions, moving Dillo away won't change much their capability to train their model with our code, but at least I won't be actively helping.&lt;/p&gt;&lt;head rend="h2"&gt;Self-hosting Dillo&lt;/head&gt;&lt;p&gt;After researching the available options, it seems that none of the current forges would allow us to have a redundant system that can prevent the forge from becoming a single point of failure and solve the rest of the problems with GitHub. Therefore, I decided to self-host Dillo myself, move all important data to git repositories and keep them synchronized in multiple git mirrors.&lt;/p&gt;&lt;p&gt;I decided to buy the dillo-browser.org domain name and setup a very small VPS. Initially, I was very skeptical that it would be able to survive on today's web, but it seems to be doing an acceptable job at handling it (mostly AI bot traffic masquerading as users). The Dillo website is available here:&lt;/p&gt;&lt;p&gt;I researched which git frontends may suit our needs, and I discovered that most options are very complicated to self-host and require a lot of server resources and JavaScript on the frontend. I ended up testing cgit, which is written in C and it seems to be very lightweight both on RAM and CPU usage. Furthermore, the web frontend doesn't require JS, so I can use it from Dillo (I modified cgit CSS slightly to work well on Dillo). It is available on this URL:&lt;/p&gt;&lt;p&gt;https://git.dillo-browser.org/&lt;/p&gt;&lt;p&gt;Regarding the bug tracker, I also took a look at the available options. They are all too complicated for what I would like to have and they seem to centralize the data into a database that can get lost. This is precisely the case that happened with the old dillo bug tracker and we are still unable to recover the original bug entries.&lt;/p&gt;&lt;p&gt;To avoid this problem, I created my own bug tracker software, buggy, which is a very simple C tool that parses plain Markdown files and creates a single HTML page for each bug. All bugs are stored in a git repository and a git hook regenerates the bug pages and the index on each new commit. As it is simply plain text, I can edit the bugs locally and only push them to the remote when I have Internet back, so it works nice offline. Also, as the output is just an static HTML site, I don't need to worry about having any vulnerabilities in my code, as it will only run at build time. You can see it live here, with the exported issues from GitHub:&lt;/p&gt;&lt;p&gt;https://bug.dillo-browser.org/&lt;/p&gt;&lt;p&gt;The mailing list archives are stored by three independent external services, but I might include a copy with our own archives in the future.&lt;/p&gt;&lt;head rend="h2"&gt;Setting up mirrors&lt;/head&gt;&lt;p&gt;As all the important data is now stored in git repositories, we can mirror them in any forge, without having to rely on their custom storage format for the issues or other data. If a forge goes down (or goes rogue) we can simply switch to another site with low switching cost. To this end, I have created git mirrors in Codeberg and Sourcehut that are synced with our git server:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Codeberg: https://codeberg.org/dillo/&lt;/item&gt;&lt;item&gt;Sourcehut: https://git.sr.ht/~dillo/&lt;/item&gt;&lt;/list&gt;&lt;p&gt;However, we still have a single point of failure: the DNS entry of the dillo-browser.org domain. If we lose the DNS entry (like with dillo.org) it would cause a problem as all services will be unreachable. We could recover from such situation by relying on alternative ways to reach users, by the mailing list, fediverse or IRC, as well as updating the mirrors to reflect the current situation. It is not ideal, but I don't think it would cause a catastrophic data loss (like it happened before) as all the data is now stored in git and replicated across independent locations.&lt;/p&gt;&lt;head rend="h2"&gt;OpenPGP signature&lt;/head&gt;&lt;p&gt;In order for this page to have some authority, the HTML file is signed with my GPG key (32E65EC501A1B6FDF8190D293EE6BA977EB2A253), which is the same that I use to sign the last releases of Dillo and is also listed in my GitHub user. The signature is available here and is linked to the page with the &lt;code&gt;&amp;lt;link&amp;gt;&lt;/code&gt; tag using the &lt;code&gt;rel=signature&lt;/code&gt;
relation. You can find more information and how to verify the signature in the
Dillo RFC-006.

&lt;/p&gt;&lt;p&gt;Using OpenPGP signatures is robust against losing the DNS entry, as the authority is not given by the TLS certificate chain but by the trust in the OpenPGP signature, so we could move the site elsewhere and still claim that is owned by us. Additionally, as we can store the signatures inside all git mirrors, they are also resilient against data loss.&lt;/p&gt;&lt;head rend="h2"&gt;Closing remarks&lt;/head&gt;&lt;p&gt;Keep in mind that the migration process requires several moving parts and it will take a while for it to stabilize (switching costs). The GitHub repositories won't be removed at any point in time and they will continue to be updated until we finish the migration. When the migration process is completed, I will mark the Dillo repositories as archived and properly comunicate it in our site. It is important that we don't remove any commit or tarball release to avoid breaking downstream builds that still rely on the GitHub URL.&lt;/p&gt;&lt;p&gt;Lastly, I'm glad that we can have our own fully independent and self-hosted site with relatively low expenses and very little energy cost (which is good for the environment, but probably not even noticeable at large scale). With the current DNS and server costs and our current donations I consider that it is likely that we can continue covering the expenses for at least the next 3 years in the worst case scenario. If you are interested in keeping us afloat, you can help via Liberapay.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46096800</guid><pubDate>Sun, 30 Nov 2025 14:11:40 +0000</pubDate></item><item><title>Discovering that my smartphone had infiltrated my life</title><link>https://utcc.utoronto.ca/~cks/space/blog/tech/SmartphoneInfiltratedMyLife</link><description>&lt;doc fingerprint="6f5c4452e939f3a5"&gt;
  &lt;main&gt;&lt;p&gt;You're probably reading this page because you've attempted to access some part of my blog (Wandering Thoughts) or CSpace, the wiki thing it's part of. Unfortunately you're using a browser (or client library) that my anti-crawler precautions consider suspicious because it's not sending a Sec-Fetch-Mode HTTP header while claiming to be Firefox or Chrome (both of which have been sending that header for a very long time), or another browser that similarly should be sending this header (such as versions of Safari and WebKit from early 2023 onward).&lt;/p&gt;&lt;p&gt;Due to the ongoing problems with abusive high volume crawlers using forged User-Agent values (apparently in part to gather data for LLM training), this combination of browser User-Agent and lack of Sec-Fetch-Mode header is not accepted.&lt;/p&gt;&lt;p&gt;If this is in error and you're using a current version of your browser of choice, you can contact me at my current place at the university (you should be able to work out the email address from that). If possible, please let me know what browser you're using and so on, ideally with its exactl User-Agent string. Please note that I'm unlikely to make an exemption for software that is actively impersonating Chrome or Firefox in its User-Agent, especially if its User-Agent is essentially indistinguishable from a legitimate browser User-Agent string. Crawling software that isn't prepared to admit it is being anti-social, and I am no longer willing to help anti-social software.&lt;/p&gt;Chris Siebenmann, 2025-11-17&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46096841</guid><pubDate>Sun, 30 Nov 2025 14:17:41 +0000</pubDate></item><item><title>Geothermal Breakthrough in South Texas Signals New Era for Ercot</title><link>https://www.powermag.com/geothermal-breakthrough-in-south-texas-signals-new-era-for-ercot/</link><description>&lt;doc fingerprint="b1b0707e0bb4f5aa"&gt;
  &lt;main&gt;
    &lt;p&gt;In just 12 months, Sage Geosystems and San Miguel Electric Cooperative built the world’s first pressure geothermal system. It is now poised to deliver long-duration, dispatchable storage for the Electric Reliability Council of Texas (ERCOT) as variable generation and data center demand surge.&lt;/p&gt;
    &lt;p&gt;In Christine, Texas—a town of barely 365 people an hour south of San Antonio—San Miguel Electric Cooperative Inc. (SMECI) is pulling off a transformation that will redefine rural generation. Borne out of efforts to bring reliable, affordable power to South Texas’s overlooked ranchlands and small towns, the member-owned cooperative built and commissioned a 391-MW mine-mouth lignite power plant at the Atascosa County site, tapping deposits from its own San Miguel Mine. For more than 40 years, that plant, the entity’s only unit, provided baseload power to 47 counties through a wholesale contract with member-customer South Texas Electric Cooperative (STEC), and functioning as a critical anchor in the Electric Reliability Council of Texas’s (ERCOT’s) South Load Zone.&lt;/p&gt;
    &lt;p&gt;While coal, fueled by locally mined lignite, has long been SMECI’s backbone, the cooperative has been working to ensure it can sustain its baseload responsibility while transitioning to cleaner resources, easing the early retirement of the lignite plant ahead of STEC’s 2037 contract expiry. “Unlike investor-owned utilities, we are operated by and for the people of South Texas in predominantly rural areas—regions that had historically been overlooked,” the cooperative notes on its website. “We have one simple goal: reliable power, not making a profit.”&lt;/p&gt;
    &lt;p&gt;That mission drove two landmark efforts last year: a first-of-its-kind geothermal partnership with Sage Geosystems to launch a Pressure Geothermal pilot and an application to the U.S. Department of Agriculture’s New Empowering Rural America (New ERA) program—which in December secured over $1.4 billion in grants and low-interest loans—to replace the lignite unit with 400 MW of solar and 200 MW of battery storage by 2027.&lt;/p&gt;
    &lt;p&gt;The geothermal pilot, SMECI Well #1, is the first piece of that vision to materialize. Moving from funding approval to “ready to store” in just 12 months, Sage completed drilling, fracture stimulation, surface-facility installation, and commissioning by August 2025. Leveraging innovative design, existing oilfield expertise, and streamlined permitting, the 3-MW/4–6-hour system has transformed a pioneering underground energy storage concept into a fully built asset in record time. While the project is still awaiting grid interconnection—currently slated for December 2025—the pioneering project has showcased a new model for firm, dispatchable capacity in ERCOT’s evolving grid, and is well worthy of this POWERTop Plant award.&lt;/p&gt;
    &lt;head rend="h2"&gt;First-of-a-Kind Geothermal Energy Storage&lt;/head&gt;
    &lt;p&gt;For Sage, the project marks the first commercial-scale deployment of its proprietary Pressure Geothermal System (GGS), a technology the company has been developing since its founding in 2020 by a team with more than 150 years of combined oil and gas experience. The system builds on lessons from a 2023 demonstration well in Starr County, Texas, which delivered up to 17 hours of discharge with minimal water loss and a round-trip efficiency of 70–75%, proving the concept’s ability to provide long-duration, dispatchable capacity (Figure 1), Brianna Byrd, a Sage Geosystems’ operations engineer told POWER.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;1. This graphic illustrates Sage Geosystems’ Pressure Geothermal System, which uses a deep engineered “lung” fracture—a subsurface reservoir created in low-permeability rock—to store and release energy using, in this case, a supercritical carbon dioxide (sCO2) power system. Courtesy: Sage Geosystems&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;“Sage’s proprietary energy storage technology functions much like pumped-storage hydropower (PSH), but upside down, with the ‘upper reservoir’ located deep underground rather than on top of a mountain,” Byrd explained. “We create a vertical fracture system in low-permeability rock that serves as an artificial reservoir.” To charge the system, water is pumped from a surface storage down the wellbore into the fracture, “inflating” the water under pressure like a balloon. And to discharge it, the pump is shut off, and the pressurized fracture system pushes water back up the wellbore. “At the surface, it drives a Pelton turbine and generator. A choke system modulates flow as pressure declines, ensuring steady power output until the predetermined volume of water is released from the well,” she said.&lt;/p&gt;
    &lt;p&gt;For pressure geothermal power generation, “the same principles apply, but the pressurized water is also heated by the surrounding hot rock,” Byrd added. “The heat is then converted to electricity using either a conventional Organic Rankine Cycle (ORC) or Sage’s proprietary supercritical carbon dioxide (sCO 2) power system. The water used to extract heat from the Earth is kept separate from the CO 2, which remains in a closed-loop system in the power plant.”&lt;/p&gt;
    &lt;p&gt;These innovations have yielded high efficiency and long-duration flexibility. “The round-trip efficiency (RTE) for Sage’s energy storage system is expected to be 70–75%, as we hold open the fractures with pressure, which reduces friction and energy losses,” Byrd said. “This efficiency will not degrade over time.” That performance is on par with pumped storage hydropower but without the geographic constraints. Lithium-ion batteries can achieve a higher RTE of 85–90%, she noted, but degrade over time with repeated cycling and exposure to high ambient temperatures.&lt;/p&gt;
    &lt;p&gt;Byrd also stressed that Sage’s energy storage technology is not intended to compete with lithium-ion batteries for short discharge durations. “However, for durations of more than five hours, when lithium-ion systems must be stacked and costs escalate, our levelized cost of storage (LCOS) is projected to be significantly lower,” she said. “Additionally, Sage’s solution eliminates fire hazards associated with batteries and avoids the raw material supply chain and recycling challenges tied to lithium-ion batteries, offering a lower lifecycle impact.”&lt;/p&gt;
    &lt;p&gt;In another benefit for drought-prone regions like Atascosa County, the design minimizes water loss—less than 2% in field testing—by keeping the fracture sealed and pressurized throughout operation. “The surface facility is designed to operate within a defined pressure window, between the fracture opening and the fracture extension pressure, to avoid uncontrolled fracture growth,” Byrd noted. “We continuously monitor pressure, flow rates, and volumes in real time. This data is integrated into an automated control system that adjusts pumping parameters to keep the system within safe operating limits.”&lt;/p&gt;
    &lt;head rend="h2"&gt;A Strategic Match in the Heart of ERCOT&lt;/head&gt;
    &lt;p&gt;Notably, the project is also entirely digitally automated to integrate seamlessly with ERCOT, a characteristic that is a growing imperative in the fast-moving market, which faces extreme weather, renewable swings, and a surging demand strain from data center and cryptocurrency mining. Sage plans to operate SMECI Well #1 as a merchant, buying and selling electricity into ERCOT’s South Load Zone. “It uses advanced control systems to manage charge and discharge cycles in real time, responding to grid signals and demand fluctuations,” said Byrd. “These smart grid and digital automation layers ensure efficient synchronization with the broader energy network, enabling fast response times, remote monitoring, and dynamic participation in grid dispatch.”&lt;/p&gt;
    &lt;p&gt;For Sage, delivering its first commercial pilot in Texas also meant capitalizing on a favorable policy and permitting environment. “Texas clarified through legislation in 2023 that landowners own the geothermal heat, providing certainty for operators,” she said. “In addition, the Texas Railroad Commission, responsible for permitting geothermal wells, brings over 100 years of expertise in oil and gas permitting and can permit geothermal wells in weeks rather than months or years.”&lt;/p&gt;
    &lt;p&gt;The company credits its project implementation speed—from partnership to “ready to store” in just 12 months—to a combination of proven oil and gas expertise, an intentionally streamlined design, and Texas’s uniquely favorable permitting environment. Pressure geothermal draws on the full spectrum of oil and gas expertise: geoscience for evaluating and characterizing subsurface rock formations; engineering and operations for design, execution, project management, and cost control; and service providers for drilling, completions, and fracturing from established oilfield companies, Byrd explained.&lt;/p&gt;
    &lt;p&gt;“Most of the Sage Geosystems team comes directly from the oil and gas industry. The founders alone bring over 120 years of combined experience. Leveraging these proven technologies, tools, and skillsets allows Sage to scale immediately without having to reinvent equipment or retrain a new workforce, which is critical for next-generation geothermal and energy storage,” Byrd said.&lt;/p&gt;
    &lt;p&gt;Following grid interconnection of the SMECI energy storage system in December 2025, Sage plans to focus on drilling wells and building its second energy storage facility with a major utility. Work is meanwhile ongoing on its first commercial power generation facility as part of Meta Phase I, a partnership announced in August 2024 to deliver up to 150 MW of new geothermal baseload power for the social media company’s expanding U.S. data centers. The project is slated to be online in 2027. “We continue to advance partnerships with big tech hyperscalers, utilities, and the U.S. Department of Defense,” said Byrd.&lt;/p&gt;
    &lt;p&gt;Ultimately, Sage sees three clear use cases for its technology—energy storage, power generation, and district heating. “In the near term, we are prioritizing energy storage and power generation,” she said. “Energy storage can be deployed quickly, as demonstrated at our SMECI facility, which was built in about 12 months and directly enhances the performance and value of solar and wind assets. Power generation is in high demand for clean baseload capacity, particularly from data centers and military installations. With AI [artificial intelligence] driving explosive growth in data center demand, this need will only increase.” For now, “District heating is a largely European opportunity, where piping infrastructure is already in place and coal/biomass heat sources are being phased out. However, permitting timelines are longer, so we expect these projects to advance in the next couple of years.”&lt;/p&gt;
    &lt;p&gt;—Sonal Patel is a POWER senior editor (@sonalcpatel, @POWERmagazine).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46096921</guid><pubDate>Sun, 30 Nov 2025 14:29:26 +0000</pubDate></item><item><title>The Undermining of the CDC</title><link>https://www.newyorker.com/magazine/2025/12/08/the-undermining-of-the-cdc</link><description>&lt;doc fingerprint="2e0cd1bfbe6f27bf"&gt;
  &lt;main&gt;
    &lt;p&gt;Two weeks ago, by inserting what must be the most notorious asterisk in modern public health, the Centers for Disease Control and Prevention caveated its long-standing position that vaccines do not cause autism. Under the direction of Robert F. Kennedy, Jr., the Secretary of Health and Human Services, a C.D.C. web page now contends that this is “not an evidence-based claim” and that research linking vaccines to autism has been “ignored by health authorities.” The fact that the original statement remains at all is due to an agreement with Senator Bill Cassidy, a physician and the chair of the Senate health committee, who disregarded decades of Kennedy’s vaccine skepticism to advance his confirmation after extracting a set of flimsy commitments that Kennedy is now betraying. The Autism Science Foundation said that it is “appalled” by the C.D.C.’s new stance; the American Medical Association warned of “dangerous consequences.”&lt;/p&gt;
    &lt;p&gt;The Department of Health and Human Services maintains that it is hewing to “gold standard, evidence-based science”—a piece of doublespeak so thick that it might unsettle Orwell. Discounting dozens of rigorous studies that have analyzed millions of patients and failed to connect vaccines to autism, the C.D.C. website claims that about half of parents of children with autism believe vaccines contributed to that autism. It cited a decades-old paper that surveyed a few dozen parents who strongly embraced alternative medicine, at two private practices in the Northeast. The web page points out that autism rates have risen in recent decades and so has the number of infant vaccinations—an observation that might also be made about prestige TV shows and pumpkin-spice lattes. The H.H.S. will now provide “appropriate funding” for studies on vaccines and autism, and last week it appointed a physician with a history of vaccine skepticism as the second-in-command at the C.D.C. The episode puts to rest any doubts about whether Americans can still trust information from the nation’s top health agency.&lt;/p&gt;
    &lt;p&gt;At stake is a question of the quality of information that should be taken seriously in public discourse and how that information should be communicated. Science may be the most powerful engine for grasping reality, but it suffers a rhetorical disadvantage. In science, the burden of proof falls on the one aiming to overturn the “null hypothesis”—the default position that one thing doesn’t cause another. But conspiratorial thinking is fuelled by the inverse: self-assured conjecture that demands a level of refutation no amount of evidence can offer. Proving the absence of a connection will always be harder than speculating about its existence. The language of science is measured and provisional; the language of politics is declarative and bombastic. In September, President Donald Trump told pregnant women to “fight like hell” not to take Tylenol, because of a potentially increased risk of autism in children; his Food and Drug Administration clarified that “a causal relationship has not been established and there are contrary studies in the scientific literature.” Tylenol, the agency wrote, remains “the safest over-the-counter” option for treating fever or pain.&lt;/p&gt;
    &lt;p&gt;The privilege that American scientists have taken for granted—one that is now being trampled—is the ability to go about their work free of political interference. With few exceptions, both Republicans and Democrats have supported independent science, understanding that the nation benefits from research that promotes health, innovation, and economic growth. But since Trump returned to office his Administration has fired or muzzled government scientists with disfavored views on nutrition and climate change; cancelled funding for long-running surveys on food insecurity and global health; dismissed independent committees focussed on air pollution, health-care disparities, and hospital infections; and pulled support for research into vaccines. This month, leading members of the National Institutes of Health, who ascended to their roles in large part based on their criticism of COVID-era mandates, published an article arguing that we should plan for the next pandemic not by trying to identify dangerous pathogens or by developing vaccines and medications to mitigate their damage but by encouraging people to be healthier: by abstaining from smoking, by eating nutritious food, and by “getting up and walking more.”&lt;/p&gt;
    &lt;p&gt;“The best pandemic preparedness playbook,” the authors wrote, unironically, “is making America healthy again.” Leaving aside that mRNA vaccines saved millions of lives during the covid pandemic, and that a society might like to prepare both by promoting healthy habits and by investing in biotechnology, this ignores the fact that, in some outbreaks, young and healthy people have had among the highest rates of death, and that with any infectious disease many people will remain vulnerable no matter what they do. (This year, a variant of the H3N2 influenza virus, known as subclade K, has caused a surge in cases in the United Kingdom, Canada, and Japan, and appears to be most perilous for children and older adults.) The unpredictability of pathogens is precisely why a broad-based strategy is needed. Pushups won’t save you from Ebola.&lt;/p&gt;
    &lt;p&gt;In the nineteen-thirties, a Soviet biologist named Trofim Lysenko gained the patronage of Joseph Stalin. Lysenko, who was tapped to guide the country’s agricultural reforms, had a range of pseudoscientific ideas, including that the environment, not genes, primarily determined an organism’s traits and that members of the same species don’t compete. He oversaw a series of biologically wrongheaded programs that contributed to famine, misery, and death for millions of people. (A British botanist once said that talking to Lysenko “was like trying to explain differential calculus to a man who didn’t know his twelve times table.”) With Stalin’s backing, Lysenko purged scientists who disagreed with him and set the country’s once preëminent genetics research back by decades.&lt;/p&gt;
    &lt;p&gt;A reason that the U.S. became the world’s biomedical leader—indeed, a reason that it emerged from the Cold War victorious—is that democratic governance allows for a level of self-correction that authoritarianism does not. Bad ideas can be beaten back at the ballot box, in the public square, and through the halls of Congress. The country is under no obligation to tolerate institutionalized quackery or elected officials who, through feckless appeals and half measures, have become complicit in it. Truly making America healthy will involve more than removing an asterisk. It will require turning the page. ♦&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46097102</guid><pubDate>Sun, 30 Nov 2025 14:52:06 +0000</pubDate></item></channel></rss>