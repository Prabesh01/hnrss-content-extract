<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 24 Sep 2025 12:19:31 +0000</lastBuildDate><item><title>Getting AI to work in complex codebases</title><link>https://github.com/humanlayer/advanced-context-engineering-for-coding-agents/blob/main/ace-fca.md</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45347532</guid><pubDate>Tue, 23 Sep 2025 14:27:36 +0000</pubDate></item><item><title>Always Invite Anna</title><link>https://sharif.io/anna-alexei</link><description>&lt;doc fingerprint="fedd98054dfc58ec"&gt;
  &lt;main&gt;
    &lt;p&gt;I was lucky enough to make a few friends my first semester of college. We ended up hanging out quite a bit during those early months.&lt;/p&gt;
    &lt;p&gt;We’d all get excited for the weekends because Friday nights meant going out to party. Everyone except for Anna, that is.&lt;/p&gt;
    &lt;p&gt;Anna was quiet, shy, and a definitely a goody-two-shoes. She was from Alabama and spoke with a pronounced southern drawl I’d rarely heard in Maryland. She was reserved but friendly once you got to know her. Anna cared about school a lot. She was almost always studying whenever I saw her.&lt;/p&gt;
    &lt;p&gt;Every Friday night we’d make plans to go out together and party. But Anna would always refuse to come. She’d say something along the lines of “I have to study” or “I just don’t feel like it tonight.”&lt;/p&gt;
    &lt;p&gt;Eventually, we stopped inviting Anna out. Everyone except Alexei.&lt;/p&gt;
    &lt;p&gt;I liked Alexei the most in our friend group. He was valedictorian of his high school, played tennis at a competitive level, and was remarkably smart. If anyone deserved to have an ego, it was Alexei. Yet somehow he managed to be the kindest person I’d ever known. But my absolute favorite thing about Alexei was that he always invited Anna to come party with us.&lt;/p&gt;
    &lt;p&gt;One Friday night as we were all about to leave the dorms for a house party, Alexei stopped us. “Hold on, let’s invite Anna.” We headed over to her dorm and invited her to come with us. She said “Sorry, I have to study for my Arabic exam next week, but you guys have fun.”&lt;/p&gt;
    &lt;p&gt;Alexei continued to invite Anna every time we went out for the rest of the semester. And Anna said no every single time.&lt;/p&gt;
    &lt;p&gt;Curious about his persistence, I asked him “Why do you keep inviting Anna out when she’ll just say no?”&lt;/p&gt;
    &lt;p&gt;I’ll never forget what he told me: “I know she’s always going to say no, but that’s not the point. I invite her out so she’ll always feel included in the group.”&lt;/p&gt;
    &lt;p&gt;After that first semester, the friend group disbanded and we all went our separate ways. Many years later I ran into Anna and we ended up catching up. She told me how difficult her first semester of college had been. She was very close with her mom and sister and missed them them terribly.&lt;/p&gt;
    &lt;p&gt;But then she said something that stayed with me: She was grateful. She was grateful to be part of that brief friend group because she felt like she had a family away from home. And that even though she never partied with us, she always felt included because we would stop by her room and invite her anyway.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45348495</guid><pubDate>Tue, 23 Sep 2025 15:33:23 +0000</pubDate></item><item><title>Find SF parking cops</title><link>https://walzr.com/sf-parking/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45350690</guid><pubDate>Tue, 23 Sep 2025 18:06:07 +0000</pubDate></item><item><title>How to draw construction equipment for kids</title><link>https://alyssarosenberg.substack.com/p/how-to-draw-construction-equipment</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45351410</guid><pubDate>Tue, 23 Sep 2025 19:09:50 +0000</pubDate></item><item><title>Apple A19 SoC die shot</title><link>https://chipwise.tech/our-portfolio/apple-a19-dieshot/</link><description>&lt;doc fingerprint="229e687d1b3cd1b2"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Apple A19 SoC die shot&lt;/head&gt;
    &lt;p&gt;These images represent the first high-resolution microscopy of Apple’s A19 chip, extracted from the iPhone 17, revealing its full complexity under the hood. Built on TSMC’s third-generation 3 nm process node—dubbed N3P—the A19 marks a refinement over the earlier N3E technology used in the A18 series, offering higher transistor density, better energy efficiency, and modest performance gains. On the CPU side, the chip retains a hybrid core design (performance plus efficiency cores), while upgrades to the GPU include more cores on the Pro models. Key supporting blocks—image signal processor, display engine, Neural Engine—also see enhancements, enabling better on-device AI, imaging, and power management. Taken together, the die shots not only visualize the physical layout—logic blocks, cache banks, interconnects—but also reflect Apple’s continuous push in process technology and architectural refinement.&lt;/p&gt;
    &lt;head rend="h2"&gt;High Resolution Floorplan images available here&lt;/head&gt;
    &lt;p&gt;+31537113618&lt;/p&gt;
    &lt;p&gt;info@chipwise.tech&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45351437</guid><pubDate>Tue, 23 Sep 2025 19:12:08 +0000</pubDate></item><item><title>Is Fortran better than Python for teaching basics of numerical linear algebra?</title><link>https://loiseaujc.github.io/posts/blog-title/fortran_vs_python.html</link><description>&lt;doc fingerprint="4718593732e41c11"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Fortran better than Python for teaching the basics of numerical linear algebra?&lt;/head&gt;
    &lt;p&gt;Disclaimer – This is not a post about which language is the most elegant or which implementation is the fastest (we all know it’s &lt;code&gt;Fortran&lt;/code&gt;). It’s about teaching the basics of scientific computing to engineering students with a limited programming experience. Yes, the &lt;code&gt;Numpy&lt;/code&gt;/&lt;code&gt;Scipy&lt;/code&gt;/&lt;code&gt;matplotlib&lt;/code&gt; stack is awesome. Yes, you can use &lt;code&gt;numba&lt;/code&gt; or &lt;code&gt;jax&lt;/code&gt; to speed up your code, or &lt;code&gt;Cython&lt;/code&gt;, or even &lt;code&gt;Mojo&lt;/code&gt; the latest kid in the block. Or you know what? Use &lt;code&gt;Julia&lt;/code&gt; or &lt;code&gt;Rust&lt;/code&gt; instead. But that’s not the basics and it’s beyond the point.&lt;/p&gt;
    &lt;p&gt;I’ve been teaching an Intro to Scientific Computing class for nearly 10+ years. This class is intended for second year engineering students and, as such, places a large emphasis on numerical linear algebra. Like the rest of Academia, I’m using a combination of &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;numpy&lt;/code&gt; arrays for this. Yet, after all these years, I start to believe it ain’t necessarily the right choice for a first encounter with numerical linear algebra. Obvisouly everything is not black and white and I’ll try to be nuanced. But, in my opinion, a strongly typed language such as &lt;code&gt;Fortran&lt;/code&gt; might lead to an overall better learning experience. And that’s what it’s all about when you start Uni: learning the principles of scientific programming, not the quirks of a particular language (unless you’re a CS student, which is a different crowd).&lt;/p&gt;
    &lt;p&gt;Don’t get me wrong though. Being proficient with &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt; and &lt;code&gt;matplotlib&lt;/code&gt; is an absolute necessity for STEM students today, and that’s a good thing. Even from an educational perspective, the scientific &lt;code&gt;Python&lt;/code&gt; ecosystem enables students to do really cool projects, putting the fun back in learning. It would be completely non-sensical to deny this. But using &lt;code&gt;x = np.linalg.solve(A, b)&lt;/code&gt; ain’t the same thing as having a basic understanding of how these algorithms work. And to be clear: the goal of these classes is not to transform a student into a numerical linear algebra expert who could write the next generation LAPACK. It is to teach them just enough of numerical computing so that, when they’ll transition to an engineering position, they’ll be able to make an informed decision regarding which solver or algorithm to use when writing a simulation or data analysis tool to tackle whatever business problem they’re working on.&lt;/p&gt;
    &lt;p&gt;If you liked and aced your numerical methods class, then what I’ll discuss might not necessary be relatable. You’re one of a kind. More often than not, students struggle with such courses. This could be due to genuine comprehension difficulties, or lazyness and lack of motivation simply because they don’t see the point. While both issues are equally important to address, I’ll focus on the first one: students who are willing to put the effort into learning the subject but have difficulties transforming the mathematical algorithm into an actionnable piece of code. Note however that initially motivated but struggling students might easily drift to the second type, hence my focus there first.&lt;/p&gt;
    &lt;p&gt;In the rest of this post, I’ll go through two examples. For each, I’ll show a typical &lt;code&gt;Python&lt;/code&gt; code such a student might write and discuss all of the classical problems they’ve encountered to get there. A large part of these are syntax issues or result from the permissiveness of an interpreted language like &lt;code&gt;Python&lt;/code&gt; which is a double edged sword. Then I’ll show an equivalent &lt;code&gt;Fortran&lt;/code&gt; implementation and explain why I believe it can solve part of these problems. But first, I need to address the two elephants in the room:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;My research is on applied mathematics and numerical linear algebra for the physical sciences. I am not doing research on Education. Everything that follows comes from my reflection about my interactions with students I taught to or mentored. If you have scientific evidence (pertaining to scientific computing in particular) proving me wrong, please tell me.&lt;/item&gt;
      &lt;item&gt;When I write &lt;code&gt;Fortran&lt;/code&gt;, what I really mean is modern&lt;code&gt;Fortran&lt;/code&gt;, not&lt;code&gt;FORTRAN&lt;/code&gt;. Anything pre-dating the&lt;code&gt;Fortran 90&lt;/code&gt;standard (or even better, the&lt;code&gt;Fortran 2018&lt;/code&gt;one) is not even an option (yes, I’m looking at you&lt;code&gt;FORTRAN 77&lt;/code&gt;and your incomprehensible&lt;code&gt;goto&lt;/code&gt;, error-prone&lt;code&gt;common&lt;/code&gt;, artithmetic&lt;code&gt;if&lt;/code&gt;and what not).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;With that being said, let’s get started with a concrete, yet classical, example to illustrate my point.&lt;/p&gt;
    &lt;head rend="h2"&gt;The &lt;code&gt;Hello World&lt;/code&gt; of iterative solvers&lt;/head&gt;
    &lt;p&gt;You’ve started University a year ago and are taking your first class on scientific computing. Maybe you already went through the hassle of Gaussian elimination and the LU factorization. During the last class, Professor X discussed about iterative solvers for linear systems. It is now the hands-on session and today’s goal is to implement the Jacobi method. Why Jacobi? Because it is simple enough to implement in an hour or so.&lt;/p&gt;
    &lt;p&gt;The exact problem you’re given is the following:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider the Poisson equation with homogeneous Dirichlet boundary conditions on the unit-square. Assume the Laplace operator has been discretized using a second-order accurate central finite-difference scheme. The discretized equation reads \[\dfrac{u_{i+1, j} - 2u_{i, j} + u_{i-1, j}}{\Delta x^2} + \dfrac{u_{i, j+1} - 2u_{i, j} + u_{i, j-1}}{\Delta y^2} = b_{i, j}.\] For the sake of simplicity, take \(\Delta x = \Delta y\). Write a function implementing the Jacobi method to solve the resulting linear system to a user-prescribed tolerance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;We can all agree this is a simple enough yet somewhat realistic example. More importantly, it is sufficient to illustrate my point. Here is what the average student might write in &lt;code&gt;Python&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;import numpy as np

def jacobi(b , dx, tol, maxiter):
    # Initialize variables.
    nx, ny = b.shape
    residual = 1.0
    u = np.zeros((nx, ny))
    tmp = np.zeros((nx, ny))

    # Jacobi solver.
    for iteration in range(maxiter):
        # Jacobi iteration.
        for i in range(1, nx-1):
            for j in range(1, ny-1):
                tmp[i, j] = 0.25*(b[i, j]*dx**2 - u[i+1, j] - u[i-1, j] 
                                                - u[i, j+1] - u[i, j-1])

        # Compute residual
        residual = np.linalg.norm(u-tmp)
        # Update solution.
        u = tmp
        # If converged, exit the loop.
        if residual &amp;lt;= tol:
            break

    return u&lt;/code&gt;
    &lt;p&gt;Yes, you shouldn’t do &lt;code&gt;for&lt;/code&gt; loops in &lt;code&gt;Python&lt;/code&gt;. But remember, you are not a seasoned programmer. You’re taking your first class on scientific computing and that’s how the Jacobi method is typically presented. Be forgiving.&lt;/p&gt;
    &lt;head rend="h3"&gt;Where do students struggle?&lt;/head&gt;
    &lt;p&gt;Admittidely, the code is quite readable and look very similar to the pseudocode you’d use to describe the Jacobi method. But if you’re reading this blog post, there probably are a handful of things you’ve internalized and don’t even think about anymore (true for both &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;Fortran&lt;/code&gt;). And that’s precisely what the students (at least mine) struggle with, starting with the very first line.&lt;/p&gt;
    &lt;p&gt;What the hell is &lt;code&gt;numpy&lt;/code&gt; and why do I need it? Also, why import it as &lt;code&gt;np&lt;/code&gt;? – These questions come back every year. Yet, I don’t have satisfying answers. I always hesitate between&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Trust me kid, you don’t want to use nested lists in&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;to do any serious numerical computing.&lt;/quote&gt;
    &lt;p&gt;which naturally begs the question of why, or&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;When I said we’ll use&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;for this scientific computing class, what I really meant is we’ll use&lt;code&gt;numpy&lt;/code&gt;which is a package written for numerical computing because&lt;code&gt;Python&lt;/code&gt;doesn’t naturally have good capabilities for number crunching. As for the import as&lt;code&gt;np&lt;/code&gt;, that’s just a convention.&lt;/quote&gt;
    &lt;p&gt;And this naturally leads to the question of “why Python in the first place then?” for which the only valid answer I have is&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Well, because&lt;/p&gt;&lt;code&gt;Python&lt;/code&gt;is supposed to be easy to learn and everybody uses it.&lt;/quote&gt;
    &lt;p&gt;Clearly, &lt;code&gt;import numpy as np&lt;/code&gt; is an innocent-looking line of code. It has nothing to do with the subject being taught though, and everything with the choice of the language, only diverting the students from the learning process.&lt;/p&gt;
    &lt;p&gt;I coded everything correctly, 100% sure, but I get this weird error message about indentation – Oh boy! What a classic! The error message varies between&lt;/p&gt;
    &lt;code&gt;IndentationError: expected an indented block&lt;/code&gt;
    &lt;p&gt;and&lt;/p&gt;
    &lt;code&gt;TabError: inconsistent use of tabs and spaces in indentation&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;&amp;lt;TAB&amp;gt;&lt;/code&gt; versus &lt;code&gt;SPACE&lt;/code&gt; is a surprisingly hot topic in programming which I don’t want to engage in. A seasoned programmer might say “simply configure your IDE properly” which is fair. But we’re talking about your average student (who’s not a CS one remember) and they might use IDLE or even just notepad. As for the &lt;code&gt;IndentationError&lt;/code&gt;, it is a relatively easy error to catch. Yet, the fact that &lt;code&gt;for&lt;/code&gt;, &lt;code&gt;if&lt;/code&gt; or &lt;code&gt;while&lt;/code&gt; constructs are not clearly delineated in &lt;code&gt;Python&lt;/code&gt; other than visually is surprisingly hard for students. I find that it puts an additional cognitive burden on top of a subject which is already demanding enough.&lt;/p&gt;
    &lt;p&gt;It could also be more subtle. The code might run but the results are garbage because the student wrote something like&lt;/p&gt;
    &lt;code&gt;    for iteration in range(maxiter):
    # Jacobi iteration.
    for i in range(1, nx-1):
    for j in range(1, ny-1):
    tmp[i, j] = 0.25*(b[i, j]*dx**2 - u[i+1, j] - u[i-1, j] 
                                                - u[i, j+1] - u[i, j-1])&lt;/code&gt;
    &lt;p&gt;You might argue that this perfectly understandable, though if you want to be picky, there is no dealineation of where the different loops end. Which the whole point of indentation in &lt;code&gt;Python&lt;/code&gt;. But students do not necessarily get that.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;range(1, nx-1)&lt;/code&gt; and not &lt;code&gt;range(2, nx-1)&lt;/code&gt;? The first column/row is my boundary. – Another classic related to 0-based vs 1-based indexing. And another very hot debate I don’t want to engage in. The fact however is that linear algebra (and a lot of scientific computing for that matter) use 1-based indexing. Think about vectors or matrices. Almost every single maths books write them as&lt;/p&gt;
    &lt;p&gt;\[ \begin{bmatrix} a_{11} &amp;amp; a_{12} &amp;amp; a_{13} \\ a_{21} &amp;amp; a_{22} &amp;amp; a_{23} \\ a_{31} &amp;amp; a_{32} &amp;amp; a_{33} \end{bmatrix}. \]&lt;/p&gt;
    &lt;p&gt;The upper left element has the (1, 1) index, not (0, 0). Why use a language with 0-based indexing for linear algebra other than putting an additional cognitive burden on the students learning the subject? This is a recipe for the nefarious off-by-one error. And these errors are sneaky. The code might run but produce incorrect results and it’s a nightmare for the students (or the poor TA helping them) to figure out why.&lt;/p&gt;
    &lt;p&gt;Why &lt;code&gt;np.linalg.norm&lt;/code&gt; and not just &lt;code&gt;norm&lt;/code&gt; or &lt;code&gt;np.norm&lt;/code&gt;? – This is one is related to my first point. When you’re used to it, you no longer question it. But you don’t know students then and, once more, I don’t have a really clear answer other than&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Well,&lt;/p&gt;&lt;code&gt;linalg&lt;/code&gt;stand for linear algebra, and&lt;code&gt;np.linalg&lt;/code&gt;is a collection of linear algebra related function. It is a submodule of&lt;code&gt;numpy&lt;/code&gt;, the package I told you about before.&lt;/quote&gt;
    &lt;p&gt;Grouping like-minded functionalities into a dedicated submodule is definitely good practice, no question there. Discussing the architecture of &lt;code&gt;numpy&lt;/code&gt; makes a lot of sense when students have to do a big project involving numerical computing but not strictly speaking about numerical computing. On the other hand, when it is their first numerical computing class (and possibly first with &lt;code&gt;Python&lt;/code&gt;) I find it distracting. Again, it’s not a big thing really but still. And then you have to explain why &lt;code&gt;np.det&lt;/code&gt; and &lt;code&gt;np.trace&lt;/code&gt; are not part of &lt;code&gt;np.linalg&lt;/code&gt;…&lt;/p&gt;
    &lt;p&gt;Other common problems – There are other very common problems like using the wrong function or inconsistent use of lower- or upper-case for variables. Once you know &lt;code&gt;Python&lt;/code&gt; is case-sensitive, this is mainly a concentration problem. No big deal there. But there is one last thing that tends to cause problems to distracted students and that has to do with the dynamic nature of &lt;code&gt;Python&lt;/code&gt;. Nowhere in the code snippet is it clearly specified that &lt;code&gt;b&lt;/code&gt; needs to be a two-dimensional &lt;code&gt;np.array&lt;/code&gt; of real numbers nor that it shouldn’t be modified by the function. It is only implicit. And that can be a big problem for students when working with marginally more complicated algorithms. Sure enough, type annotation is a thing now in &lt;code&gt;Python&lt;/code&gt;, but it still is pretty new and comparatively few people actually use them.&lt;/p&gt;
    &lt;head rend="h3"&gt;What about &lt;code&gt;Fortran&lt;/code&gt;?&lt;/head&gt;
    &lt;p&gt;Alright, I’ve spent the last five minutes talking shit about &lt;code&gt;Python&lt;/code&gt; but how does &lt;code&gt;Fortran&lt;/code&gt; compare with it? Here is a typical implementation of the same function. I’ve actually digged it from my own set of archived homeworks I did 15+ years ago and hardly modified it.&lt;/p&gt;
    &lt;code&gt;function jacobi(b, dx, tol, maxiter) result(u)
    implicit none
    real, dimension(:, :), intent(in) :: b
    real, intent(in) :: dx, tol
    integer, intent(in) :: maxiter
    real, dimension(:, :), allocatable :: u
    ! Internal variables.
    real, dimension(:, :), allocatable :: tmp
    integer :: nx, ny, i, j, iteration

    ! Initialize variables.
    nx = size(b, 1) ; ny = size(b, 2)
    allocate(u(nx, ny), source = 0.0)
    residual = 1.0

    ! Jacobi solver.
    do iteration = 1, maxiter
        ! Jacobi iteration.
        do j = 2, ny-1
            do i = 2, nx-1
                tmp(i, j) = 0.25*(b(i, j)*dx**2 - u(i+1, j) - u(i-1, j) &amp;amp;
                                                - u(i, j+1) - u(i, j-1))
            enddo
        enddo

        ! Compute residual.
        residual = norm2(u - tmp)
        ! Update solution.
        u = tmp
        ! If convered, exit the loop.
        if (residual &amp;lt;= tol) exit
    enddo

end function&lt;/code&gt;
    &lt;p&gt;No surprise there. The task is sufficiently simple that both implementations are equally readable. If anything, the &lt;code&gt;Fortran&lt;/code&gt; one is a bit more verbose. But in view of what I’ve just said about the &lt;code&gt;Python&lt;/code&gt; code, I think it actually a good thing. Let me explain.&lt;/p&gt;
    &lt;p&gt;Definition of the variables – &lt;code&gt;Fortran&lt;/code&gt; is a strongly typed language. Lines 2 to 8 are nothing but the definitions of the different variables used in the routine. While you might argue it’s a pain in the a** to write these, I think it can actually be very beneficial for students. Before even implementing the method, they have to clearly think about which variables are input, which are ouput, what are their types and dimensions. And to do so, they have to have at least a minimal understanding of the algorithm itself. Once it’s done, there are no more surprises (hopefully), and the contract between the code and the user is crystal clear. And more importantly, the effort put in clearly identifying the input and output of numerical algorithm usually pays off and leads to less error-prone process.&lt;/p&gt;
    &lt;p&gt;Begining and end of the constructs – &lt;code&gt;Fortran&lt;/code&gt; uses the &lt;code&gt;do&lt;/code&gt;/&lt;code&gt;end do&lt;/code&gt; (or &lt;code&gt;enddo&lt;/code&gt;) construct, clearly specifying where the loop starts where it ends. The indentation used in the code snippet really is just a matter of style. In constrast to &lt;code&gt;Python&lt;/code&gt;, writing&lt;/p&gt;
    &lt;code&gt;    do j = 2, ny-1
    do i = 2, nx-1
    tmp(i, j) = 0.25*(b(i, j)*dx**2 - u(i+1, j) - u(i-1, j) &amp;amp;
                                    - u(i, j+1) - u(i, j-1))
    enddo
    enddo&lt;/code&gt;
    &lt;p&gt;does not make the code any less readable and wouldn’t change a dime in terms of computations. It’s a minor thing, fair enough. But it instantly get rid of the &lt;code&gt;IndentationError&lt;/code&gt; or &lt;code&gt;TabError&lt;/code&gt; which are puzzling students. I may be wrong, but I believe it actually reduces the cognitive load associated with the programming language and let the students focus on the actual numerical linear algebra task.&lt;/p&gt;
    &lt;p&gt;No off-by-one error – By default, &lt;code&gt;Fortran&lt;/code&gt; uses a 1-based indexing. No off-by-one errors, period.&lt;/p&gt;
    &lt;p&gt;Intrinsic functions for basic scientific computations – While you have to use &lt;code&gt;np.linalg.norm&lt;/code&gt; in &lt;code&gt;Python&lt;/code&gt; to compute the norm of a vector, &lt;code&gt;Fortran&lt;/code&gt; natively has the &lt;code&gt;norm2&lt;/code&gt; function for that. No external library required. If you want to be picky, you may say that &lt;code&gt;norm2&lt;/code&gt; is a weird name and that &lt;code&gt;norm&lt;/code&gt; might be just fine.&lt;/p&gt;
    &lt;p&gt;Some quirks of &lt;code&gt;Fortran&lt;/code&gt; – All is not perfect though, starting with Line 2 and the &lt;code&gt;implicit none&lt;/code&gt; statement. This is a historical remnant which is considered good practice by modern &lt;code&gt;Fortran&lt;/code&gt; standards but not actually needed. Students being students, they will more likely than not ask questions about it although it has nothing to do with the subject of the class itself. Admittidely, it can be a bit cumbersome to explicitely define all the integers you use even if it’s just for a one-time loop. Likewise, there is the question of &lt;code&gt;real&lt;/code&gt; vs &lt;code&gt;double precision&lt;/code&gt; vs &lt;code&gt;real(wp)&lt;/code&gt; (where &lt;code&gt;wp&lt;/code&gt; is yet another variable you’ve defined somewhere). I don’t think it matters too much though when learning the basics of numerical linear algebra algorithms, although it certainly does when you start discussing about precision and performances.&lt;/p&gt;
    &lt;head rend="h2"&gt;Linear least-squares, your first step into Machine Learning&lt;/head&gt;
    &lt;p&gt;Alright, let’s look at another example. Same class, later in the semester. Professor X now discusses over-determined linear systems and how it relates to least-squares, regression and basic machine learning applications. During the hands-on session, you’re given the following problem&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Consider the following unconstrained quadratic program \[\mathrm{minimize} \quad \| Ax - b \|_2^2.\] Write a least-squares solver based on the QR factorization of the matrix \(A\). You can safely assume that \(A\) is a tall matrix (i.e. \(m &amp;gt; n\)).&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Here is what the typical &lt;code&gt;Python&lt;/code&gt; code written by the students might look like.&lt;/p&gt;
    &lt;code&gt;import numpy as np

def qr(A):
    # Initialize variables.
    m, n = A.shape
    Q = np.zeros((m, n))
    R = np.zeros((n, n))

    # QR factorization based on the Gram-Schmidt orthogonalization process.
    for i in range(n):
        q = A[:, i]
        # Orthogonalization w.r.t. to the previous basis vectors.
        for j in range(i):
            R[j, i] = np.vdot(q, Q[:, j])
            q = q - R[j, i]*Q[:, j]

        # Normalize and store the new vector.
        R[i, i] = np.linalg.norm(q)
        Q[:, i] = q / R[i, i]

    return Q, R

def upper_triangular_solve(R, b):
    # Initialize variables.
    n = R.shape[0]
    x = np.zeros((n))

    # Backsubstitution.
    for i in range(n-1, -1, -1):
        x[i] = b[i]
        for j in range(n-1, i, -1):
            x[i] = x[i] - R[i, j]*x[j]
        x[i] = x[i] / R[i, i]

    return x

def lstsq(A, b):
    # QR factorization.
    Q, R = qr(A)
    # Solve R @ x = Q.T @ b.
    x = upper_triangular_solve(R, Q.T @ b)
    return x&lt;/code&gt;
    &lt;p&gt;This one was adapted from an exercise I gave last year. In reality, students lumped everything into one big function unless told otherwise, but nevermind. For comparison, here is the equivalent &lt;code&gt;Fortran&lt;/code&gt; code.&lt;/p&gt;
    &lt;code&gt;subroutine qr(A, Q, R)
    implicit none
    real, dimension(:, :), intent(in) :: A
    real, dimension(:, :), allocatable, intent(out) :: Q, R
    ! Internal variables.
    integer :: i, j, m, n
    real, dimension(:), allocatable :: q_hat

    ! Initialize variables.
    m = size(A, 1); n = size(A, 2)
    allocate(Q(m, n), source=0.0)
    allocate(R(n, n), source=0.0)
    
    ! QR factorization based on the Gram-Schmidt orthogonalization process.
    do i = 1, n
        q_hat = A(:, i)
        ! Orthogonalize w.r.t. the previous basis vectors.
        do j = 1, i-1
            R(j, i) = dot_product(q_hat, Q(:, j))
            q_hat = q_hat - R(j, i)*Q(:, j)
        end do

        ! Normalize and store the new vector.
        R(i, i) = norm2(q_hat)
        Q(:, i) = q_hat / R(i, i)
    end do
end subroutine

function upper_triangular_solve(R, b) result(x)
    implicit none
    real, dimension(:, :), intent(in) :: R
    real, dimension(:), intent(in) :: b
    real, dimension(:), allocatable :: x
    ! Internal variables.
    integer :: n, i, j

    ! Initialize variables.
    n = size(R, 1)
    allocate(x(n), source=0.0)

    ! Backsubstitution.
    do i = n, 1, -1
        x(i) = b(i)
        do j = n-1, i, -1
            x(i) = x(i) - R(i, j)*x(j)
        enddo
        x(i) = x(i) / R(i, i)
    end do
end function

function lstsq(A, b) result(x)
    implicit none
    real, dimension(:, :), intent(in) :: A
    real, dimension(:), intent(in) :: b
    real, dimension(:), allocatable :: x
    ! Internal variables.
    real, dimension(:, :), allocatable :: Q, R

    ! QR factorization.
    call qr(A, Q, R)
    ! Solve R @ x = Q.T @ b.
    x = upper_triangular_solve(R, matmul(transpose(Q), b))
end function&lt;/code&gt;
    &lt;p&gt;Just like the Jacobi example, both implementations are equally readable. At this point in the semester, the students got somewhat more comfortable with &lt;code&gt;Python&lt;/code&gt;. The classical indentation problems were not so much of a problem anymore. The off-by-one errors due to 0-based indexing for the Gram-Schmidt orthogonalization in &lt;code&gt;qr&lt;/code&gt; or in the backsubstitution algorithm on the other hand… That was painful. In a 90-minutes class, it took almost a whole hour simply for them to debug these errors.&lt;/p&gt;
    &lt;p&gt;But there was another thing that confused students. A lot. And that has to do with computing dot products in &lt;code&gt;numpy&lt;/code&gt;. There’s so many different ways: &lt;code&gt;np.vdot(x, y)&lt;/code&gt;, &lt;code&gt;np.dot(x.T, y)&lt;/code&gt;, &lt;code&gt;np.dot(np.transpose(x), y)&lt;/code&gt;, or &lt;code&gt;x.transpose().dot(y)&lt;/code&gt; to list just the ones I have seen in their codes. Again, this has nothing to do with linear algebra, but everything with the language. Not only do they need to learn the math, but they simultaneously need to learn the not-quite-necessarily-math-standard syntax used in the language (yes, I’m looking at you &lt;code&gt;@&lt;/code&gt;). It’s just a question of habits, sure enough, but again it can be impeding the learning process.&lt;/p&gt;
    &lt;p&gt;On the other hand, the &lt;code&gt;Fortran&lt;/code&gt; implementation is even closer to the standard mathematical description of the algorithm: 1-based indexing, intrinsic &lt;code&gt;dot_product&lt;/code&gt; function, etc. But beside the &lt;code&gt;implicit none&lt;/code&gt;, there is the need to use a &lt;code&gt;subroutine&lt;/code&gt; rather than a &lt;code&gt;function&lt;/code&gt; construct for the QR decomposition because it has two output variables. Not a big deal again, but to be fair, it does add another minor layer of abstraction due to the language semantics rather than that of the subject being studied.&lt;/p&gt;
    &lt;head rend="h2"&gt;&lt;code&gt;Fortran&lt;/code&gt; may have a slight edge, but I swept some things under the rug…&lt;/head&gt;
    &lt;p&gt;In the end, when it comes to teaching the basics of numerical linear algebra, &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;Fortran&lt;/code&gt; are not that different. And in that regard, neither is &lt;code&gt;Julia&lt;/code&gt; which I really like as well. The main advantages I see of using &lt;code&gt;Fortran&lt;/code&gt; for this task however are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;1-based indexing : in my experience, the 0-based indexing in &lt;code&gt;Python&lt;/code&gt;leads to so many off-by-one erros driving the students crazy. Because linear algebra textbooks naturally use 1-based indexing, having to translate everything in your head to 0-based indices is a huge cognitive burden on top of a subject already demanding enough. You might get used to it eventually, but it’s a painful process impeding the learning outcomes.&lt;/item&gt;
      &lt;item&gt;Strong typing : combined with &lt;code&gt;implicit none&lt;/code&gt;, having to declare the type, dimension and input or output nature of every variable you use might seem cumbersome at first. But it forces students to pause and ponder to identify which is which. Sure this is an effort, but it is worth it. Learning is not effortless and this effort forces you to have a somewhat better understanding of a numerical algorithm before even starting to implement it. Which I think is a good thing.&lt;/item&gt;
      &lt;item&gt;Clear delineation of the constructs : at least during the first few weeks, having to rely only on visual clues to identify where does a loop ends in &lt;code&gt;Python&lt;/code&gt;seems to be quite complicated for a non-negligible fraction of the students I have. In that respect, the&lt;code&gt;do&lt;/code&gt;/&lt;code&gt;enddo&lt;/code&gt;construct in&lt;code&gt;Fortran&lt;/code&gt;is much more explicit and probably easier to grasp.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Obvisouly, I’m not expecting educators worldwide to switch back to &lt;code&gt;Fortran&lt;/code&gt; overnight, nor is it necessarily desirable. The advantages I see are non-negligible from my perspective but certainly not enough by themselves. There are many other things that need to be taken into account. &lt;code&gt;Python&lt;/code&gt; is a very generalist language. You can do so much more than just numerical computing so it makes complete sense to have it in the classroom. The ecosystem is incredibly vast and the interactive nature definitely has its pros. Notebooks such as &lt;code&gt;Jupyter&lt;/code&gt; can be incredible teaching tools (although they come with their own problems in term good coding practices). So are the &lt;code&gt;Pluto&lt;/code&gt; notebooks in &lt;code&gt;Julia&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;Fortran&lt;/code&gt; is good at one thing: enabling computational scientists and engineers to write high-performing mathematical models without all the intricacies of equally peformant but more CS-oriented languages such as &lt;code&gt;C&lt;/code&gt; or &lt;code&gt;C++&lt;/code&gt;. Sure enough, the modern &lt;code&gt;Fortran&lt;/code&gt; ecosystem is orders of magnitude smaller than &lt;code&gt;Python&lt;/code&gt;, and targetted toward numerical computing almost exclusively. And the &lt;code&gt;Julia&lt;/code&gt; one is fairly impressive. But the community is working on it (see the fortran-lang website or the Fortran discourse if you don’t trust me). The bad rep of &lt;code&gt;Fortran&lt;/code&gt; is unjustified, particularly for teaching purposes. Many of its detractors have hardly been exposed to anything else than &lt;code&gt;FORTRAN 77&lt;/code&gt;. And it’s true that, by current standards, most of &lt;code&gt;FORTRAN 77&lt;/code&gt; codes are terrible sphagetti codes making extensive use of implicit typing and incomprehensible &lt;code&gt;goto&lt;/code&gt; statements. Even I, as a &lt;code&gt;Fortran&lt;/code&gt; programmer, acknowledge it. But that’s no longer what &lt;code&gt;Fortran&lt;/code&gt; is since the 1990’s, and certainly not today!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45351624</guid><pubDate>Tue, 23 Sep 2025 19:29:26 +0000</pubDate></item><item><title>Podman Desktop celebrates 3M downloads</title><link>https://podman-desktop.io/blog/3-million</link><description>&lt;doc fingerprint="7cfef03e2d35f7a8"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;3,000,000 downloads. Thank you&lt;/head&gt;
    &lt;head rend="h2"&gt;Wooohooo!!&lt;/head&gt;
    &lt;p&gt;We are extremely excited to share that Podman Desktop just crossed 3,000,000 downloads! This is a huge step for the project and we are incredibly thankful for how each of you has helped! This milestone belongs to you. You file issues, suggest features, build extensions, teach teammates, and nudge us to make the day-to-day better. Thank you for helping turn an idea into a tool people rely on.&lt;/p&gt;
    &lt;p&gt;To celebrate this milestone, and thank you, we built a small surprise: https://3m.podman-desktop.io&lt;/p&gt;
    &lt;p&gt;We are grateful for all the feedback we have been receiving, here is just a short collection:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;“Lovely to have all containers in one tool. Thanks!” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;"Podman Desktop is a total win." - balancedchaos Reddit (r/podman)&lt;/item&gt;
      &lt;item&gt;“Great project! Small improvements each time make it strong long-term.” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;"The experience has been nice, and the ability to run containers under user without going root is definitely nice." - ajyotirmay Hacker News&lt;/item&gt;
      &lt;item&gt;“You are doing a great job! Thanks to you I always recommend podman whenever 'docker' comes out in conversations” - anonymous user feedback&lt;/item&gt;
      &lt;item&gt;“OMG this tool is amazing. Tutorial was great. Much easier than minikube.” - anonymous user feedback&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We read every comment. Yes, even the spicy ones. That feedback shapes our roadmap and helps us focus on the work that makes the biggest difference.&lt;/p&gt;
    &lt;p&gt;Here are other noteworthy milestones we’ve reached in our quest to help developers work with containers and Kubernetes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Podman Desktop is now an official CNCF Sandbox Project&lt;/head&gt;
    &lt;p&gt;Last year, we proudly contributed Podman Desktop to the Cloud Native Computing Foundation (CNCF), and we were accepted into the CNCF Sandbox on January 21, 2025. 🎉&lt;/p&gt;
    &lt;p&gt;This milestone highlights our commitment to building open, community-driven tools that empower developers to seamlessly work with containers and Kubernetes. Joining the CNCF Sandbox is just the beginning. Reaching this 3 million downloads milestone shows the need to build a vibrant cloud‑native ecosystem and collaborate with the community to take Podman Desktop even further.&lt;/p&gt;
    &lt;head rend="h2"&gt;Highlights from the past year&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Smoother Kubernetes workflows: Easier context and namespace switching, a powerful dashboard for your cluster, and less jumping to the terminal when you want to apply YAML or peek at events and logs.&lt;/item&gt;
      &lt;item&gt;Better Docker compatibility: Clearer setup and diagnostics, improved socket handling, and fewer surprises when you bridge Docker and Podman workflows.&lt;/item&gt;
      &lt;item&gt;Everyday quality of life: Bulk actions for containers, better notifications, clearer status in the UI, and lots of fit and finish fixes that make everything feel calmer.&lt;/item&gt;
      &lt;item&gt;AI on your laptop, without drama: Podman AI Lab is easier to set up, with a curated model catalog, simple playgrounds, and an OpenAI-compatible API you can call from your apps.&lt;/item&gt;
      &lt;item&gt;Extensions, everywhere: More community-built extensions, plus tooling that makes it easier to develop and test your own. If you are extending Podman Desktop for your team, thank you. You are shaping where we take the platform.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Enterprise adoption of Podman Desktop&lt;/head&gt;
    &lt;p&gt;In recent months, we’ve seen more and more enterprises adopting Podman Desktop and making it part of critical developer workflows. To highlight this, we wanted to share a recent note we received:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;In 2023, our company studied the possible solutions to run containers on our engineers’ laptop in the most efficient way. We judged that our best bet was to migrate our thousands of engineers to Podman Desktop. That was a brave move but we believed Podman Desktop was the most promising solution. We did not know how quickly it would become the best solution of all and how right that decision would be!&lt;/p&gt;
      &lt;p&gt;We migrated most engineers in 2023 and did the last mile at the beginning of 2024. Podman Desktop evolved at an insane pace. It improved release after release. And it still does. It quickly became a rock solid solution with more and more useful features to discover every month!&lt;/p&gt;
      &lt;p&gt;On top of that, Podman Desktop is a Community solution which allows us to have a very healthy relationship with the contributors of the project.&lt;/p&gt;
      &lt;p&gt;I am happy to hear that Podman Desktop reached 3M downloads. This means more and more people realise how good this software is. Thank you Podman Desktop. Special thanks to all the project’s contributors!&lt;/p&gt;
      &lt;p&gt;Fabrice Pipart, Amadeus&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;New here? Grab the latest build&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Download Podman Desktop for Windows, macOS, and Linux: https://podman-desktop.io/downloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;From all of us on the Podman Desktop team, thank you for trusting us with your workflow and for helping us get better with every release. If you haven't tried Podman Desktop in a while, grab the latest build and let us know what you think. If you are already a daily user, we would love to hear what is working and what is not, so we can make the next million downloads even more useful.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45352460</guid><pubDate>Tue, 23 Sep 2025 20:40:21 +0000</pubDate></item><item><title>Is life a form of computation?</title><link>https://thereader.mitpress.mit.edu/is-life-a-form-of-computation/</link><description>&lt;doc fingerprint="b2025f96f139f3ba"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Life a Form of Computation?&lt;/head&gt;
    &lt;p&gt;In 1994, a strange, pixelated machine came to life on a computer screen. It read a string of instructions, copied them, and built a clone of itself — just as the Hungarian-American Polymath John von Neumann had predicted half a century earlier. It was a striking demonstration of a profound idea: that life, at its core, might be computational.&lt;/p&gt;
    &lt;p&gt;Although this is seldom fully appreciated, von Neumann was one of the first to establish a deep link between life and computation. Reproduction, like computation, he showed, could be carried out by machines following coded instructions. In his model, based on Alan Turing’s Universal Machine, self-replicating systems read and execute instructions much like DNA does: “if the next instruction is the codon CGA, then add an arginine to the protein under construction.” It’s not a metaphor to call DNA a “program” — that is literally the case.&lt;/p&gt;
    &lt;p&gt;Of course, there are meaningful differences between biological computing and the kind of digital computing done by a personal computer or your smartphone. DNA is subtle and multilayered, including phenomena like epigenetics and gene proximity effects. Cellular DNA is nowhere near the whole story, either. Our bodies contain (and continually swap) countless bacteria and viruses, each running their own code.&lt;/p&gt;
    &lt;p&gt;Biological computing is “massively parallel,” decentralized, and noisy. Your cells have somewhere in the neighborhood of 300 quintillion ribosomes, all working at the same time. Each of these exquisitely complex floating protein factories is, in effect, a tiny computer — albeit a stochastic one, meaning not entirely predictable. The movements of hinged components, the capture and release of smaller molecules, and the manipulation of chemical bonds are all individually random, reversible, and inexact, driven this way and that by constant thermal buffeting. Only a statistical asymmetry favors one direction over another, with clever origami moves tending to “lock in” certain steps such that a next step becomes likely to happen.&lt;/p&gt;
    &lt;p&gt;This differs greatly from the operation of “logic gates” in a computer, basic components that process binary inputs into outputs using fixed rules. They are irreversible and engineered to be 99.99 percent reliable and reproducible.&lt;/p&gt;
    &lt;p&gt;Biological computing is computing, nonetheless. And its use of randomness is a feature, not a bug. In fact, many classic algorithms in computer science also require randomness (albeit for different reasons), which may explain why Turing insisted that the Ferranti Mark I, an early computer he helped to design in 1951, include a random number instruction. Randomness is thus a small but important conceptual extension to the original Turing Machine, though any computer can simulate it by calculating deterministic but random-looking or “pseudorandom” numbers.&lt;/p&gt;
    &lt;p&gt;Parallelism, too, is increasingly fundamental to computing today. Modern AI, for instance, depends on both massive parallelism and randomness — as in the parallelized “stochastic gradient descent” (SGD) algorithm, used for training most of today’s neural nets, the “temperature” setting used in chatbots to introduce a degree of randomness into their output, and the parallelism of Graphics Processing Units (GPUs), which power most AI in data centers.&lt;/p&gt;
    &lt;p&gt;Traditional digital computing, which relies on the centralized, sequential execution of instructions, was a product of technological constraints. The first computers needed to carry out long calculations using as few parts as possible. Originally, those parts were flaky, expensive vacuum tubes, which had a tendency to burn out and needed frequent replacement by hand. The natural design, then, was a minimal “Central Processing Unit” (CPU) operating on sequences of bits ferried back and forth from an external memory. This has come to be known as the “von Neumann architecture.”&lt;/p&gt;
    &lt;p&gt;Turing and von Neumann were both aware that computing could be done by other means, though. Turing, near the end of his life, explored how biological patterns like leopard spots could arise from simple chemical rules, in a field he called morphogenesis. Turing’s model of morphogenesis was a biologically inspired form of massively parallel, distributed computation. So was his earlier concept of an “unorganized machine,” a randomly connected neural net modeled after an infant’s brain.&lt;/p&gt;
    &lt;p&gt;These were visions of what computing without a central processor could look like — and what it does look like, in living systems.&lt;/p&gt;
    &lt;p&gt;Von Neumann also began exploring massively parallel approaches to computation as far back as the 1940s. In discussions with Polish mathematician Stanisław Ulam at Los Alamos, he conceived the idea of “cellular automata,” pixel-like grids of simple computational units, all obeying the same rule, and all altering their states simultaneously by communicating only with their immediate neighbors. With characteristic bravura, von Neumann went so far as to design, on paper, the key components of a self-reproducing cellular automaton, including a horizontal “tape” of cells containing instructions and blocks of cellular “circuitry” for reading, copying, and executing them.&lt;/p&gt;
    &lt;p&gt;Designing a cellular automaton is far harder than ordinary programming, because every cell or “pixel” is simultaneously altering its own state and its environment. Add randomness and subtle feedback effects, as in biology, and it becomes even harder to reason about, “program,” or “debug.”&lt;/p&gt;
    &lt;p&gt;Nonetheless, Turing and von Neumann grasped something fundamental: Computation doesn’t require a central processor, logic gates, binary arithmetic, or sequential programs. There are infinite ways to compute, and, crucially, they are all equivalent. This insight is one of the greatest accomplishments of theoretical computer science.&lt;/p&gt;
    &lt;p&gt;This “platform independence” or “multiple realizability” means that any computer can emulate any other one. If the computers are of different designs, though, the emulation may be glacially slow. For that reason, von Neumann’s self-reproducing cellular automaton has never been physically built — though that would be fun to see!&lt;/p&gt;
    &lt;p&gt;That demonstration in 1994 — the first successful emulation of von Neumann’s self-reproducing automation — couldn’t have happened much earlier. A serial computer requires serious processing power to loop through the automaton’s 6,329 cells over the 63 billion time steps required for the automaton to complete its reproductive cycle. Onscreen, it worked as advertised: a pixelated two-dimensional Rube Goldberg machine, squatting astride a 145,315-cell–long instruction tape trailing off to the right, pumping information out of the tape and reaching out with a “writing arm” to slowly print a working clone of itself just above and to the right of the original.&lt;/p&gt;
    &lt;p&gt;It’s similarly inefficient for a serial computer to emulate a parallel neural network, heir to Turing’s “unorganized machine.” Consequently, running big neural nets like those in Transformer-based chatbots has only recently become practical, thanks to ongoing progress in the miniaturization, speed, and parallelism of digital computers.&lt;/p&gt;
    &lt;p&gt;In 2020, my colleague Alex Mordvintsev combined modern neural nets, Turing’s morphogenesis, and von Neumann’s cellular automata into the “neural cellular automaton” (NCA), replacing the simple per-pixel rule of a classic cellular automaton with a neural net. This net, capable of sensing and affecting a few values representing local morphogen concentrations, can be trained to “grow” any desired pattern or image, not just zebra stripes or leopard spots.&lt;/p&gt;
    &lt;p&gt;Real cells don’t literally have neural nets inside them, but they do run highly evolved, nonlinear, and purposive “programs” to decide on the actions they will take in the world, given external stimulus and an internal state. NCAs offer a general way to model the range of possible behaviors of cells whose actions don’t involve movement, but only changes of state (here, represented as color) and the absorption or release of chemicals.&lt;/p&gt;
    &lt;p&gt;The first NCA Alex showed me was of a lizard emoji, which could regenerate not only its tail, but also its limbs and head! It was a powerful demonstration of how complex multicellular life can “think locally” yet “act globally,” even when each cell (or pixel) is running the same program — just as each of your cells is running the same DNA. Simulations like these show how computation can produce lifelike behavior across scales. Building on von Neumann’s designs and extending into modern neural cellular automata, they offer a glimpse into the computational underpinnings of living systems.&lt;/p&gt;
    &lt;p&gt;Blaise Agüera y Arcas is a VP/Fellow at Google, where he is the CTO of Technology &amp;amp; Society, and the founder of Paradigms of Intelligence, an organization dedicated to fundamental AI research. He is the author of “What Is Intelligence?,” from which this article is adapted.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45352533</guid><pubDate>Tue, 23 Sep 2025 20:46:24 +0000</pubDate></item><item><title>Qwen3-VL</title><link>https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;from=research.latest-advancements-list</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45352672</guid><pubDate>Tue, 23 Sep 2025 20:59:17 +0000</pubDate></item><item><title>From Rust to reality: The hidden journey of fetch_max</title><link>https://questdb.com/blog/rust-fetch-max-compiler-journey/</link><description>&lt;doc fingerprint="1c018251a0ff3b2c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;From Rust to Reality: The Hidden Journey of fetch_max&lt;/head&gt;
    &lt;head rend="h2"&gt;How a Job Interview Sent Me Down a Compiler Rabbit Hole&lt;/head&gt;
    &lt;p&gt;I occasionally interview candidates for engineering roles. We need people who understand concurrent programming. One of our favorite questions involves keeping track of a maximum value across multiple producer threads - a classic pattern that appears in many real-world systems.&lt;/p&gt;
    &lt;p&gt;Candidates can use any language they want. In Java (the language I know best), you might write a CAS loop, or if you're feeling functional, use &lt;code&gt;updateAndGet()&lt;/code&gt; with a lambda:&lt;/p&gt;
    &lt;quote&gt;AtomicLong highScore = new AtomicLong(100);[...]highScore.updateAndGet(current -&amp;gt; Math.max(current, newScore));&lt;/quote&gt;
    &lt;p&gt;But that lambda is doing work - it's still looping under the hood, retrying if another thread interferes. You can see the loop right in AtomicLong's source code.&lt;/p&gt;
    &lt;p&gt;Then one candidate chose Rust.&lt;/p&gt;
    &lt;p&gt;I was following along as he started typing, expecting to see either an explicit CAS loop or some functional wrapper around one. But instead, he just wrote:&lt;/p&gt;
    &lt;quote&gt;high_score.fetch_max(new_score, Ordering::Relaxed);&lt;/quote&gt;
    &lt;p&gt;"Rust has fetch_max built in," he explained casually, moving on to the next part of the problem.&lt;/p&gt;
    &lt;p&gt;Hold on. This wasn't a wrapper around a loop pattern - this was a first-class atomic operation, sitting right there next to &lt;code&gt;fetch_add&lt;/code&gt; and &lt;code&gt;fetch_or&lt;/code&gt;. Java
doesn't have this. C++ doesn't have this. How could Rust just... have this?&lt;/p&gt;
    &lt;p&gt;After the interview, curiosity got the better of me. Why would Rust provide &lt;code&gt;fetch_max&lt;/code&gt; as a built-in intrinsic? Intrinsics usually exist to leverage
specific hardware instructions. But x86-64 doesn't have an &lt;code&gt;atomic max&lt;/code&gt;
instruction. So there had to be a CAS loop somewhere in the pipeline. Unless...
maybe some architectures do have this instruction natively? And if so, how
does the same Rust code work on both?&lt;/p&gt;
    &lt;p&gt;I had to find out. Was the loop in Rust's standard library? Was it in LLVM? Was it generated during code generation for x86-64?&lt;/p&gt;
    &lt;p&gt;So I started digging. What I found was a fascinating journey through five distinct layers of compiler transformations, each one peeling back another level of abstraction, until I found exactly where that loop materialized. Let me share what I discovered.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 1: The Rust Code&lt;/head&gt;
    &lt;p&gt;Let's start with what that candidate wrote - a simple high score tracker that can be safely updated from multiple threads:&lt;/p&gt;
    &lt;quote&gt;use std::sync::atomic::{AtomicU64, Ordering};fn main() {let high_score = AtomicU64::new(100);// [...]// Another thread reports a new score of 200let _old_score = high_score.fetch_max(200, Ordering::Relaxed);// [...]}// Save this snippet as `main.rs` we are going to use it later.&lt;/quote&gt;
    &lt;p&gt;This single line does exactly what it promises: atomically fetches the current value, compares it with the new one, updates it if the new value is greater, and returns the old value. It's safe, concise, and impossible to mess up. No explicit loops, no retry logic visible anywhere. But how does it actually work under the hood?&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 2: The Macro Expansion&lt;/head&gt;
    &lt;p&gt;Before our &lt;code&gt;fetch_max&lt;/code&gt; call even reaches anywhere close to machine code generation,
there's another layer of abstraction at work. The &lt;code&gt;fetch_max&lt;/code&gt; method isn't hand-written
for each atomic type - it's generated by a Rust macro called &lt;code&gt;atomic_int!&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;If we peek into Rust's standard library source code, we find that &lt;code&gt;AtomicU64&lt;/code&gt;
and all its methods are actually created by
this macro:&lt;/p&gt;
    &lt;quote&gt;atomic_int! {cfg(target_has_atomic = "64"),// ... various configuration attributes ...atomic_umin, atomic_umax, // The intrinsics to use8, // Alignmentu64 AtomicU64 // The type to generate}&lt;/quote&gt;
    &lt;p&gt;Inside this macro, &lt;code&gt;fetch_max&lt;/code&gt; is defined as a
template
that works for any integer type:&lt;/p&gt;
    &lt;quote&gt;pub fn fetch_max(&amp;amp;self, val: $int_type, order: Ordering) -&amp;gt; $int_type {// SAFETY: data races are prevented by atomic intrinsics.unsafe { $max_fn(self.v.get(), val, order) }}&lt;/quote&gt;
    &lt;p&gt;The &lt;code&gt;$max_fn&lt;/code&gt; placeholder gets replaced with &lt;code&gt;atomic_umax&lt;/code&gt; for unsigned types
and &lt;code&gt;atomic_max&lt;/code&gt; for signed types. This single macro definition generates
&lt;code&gt;fetch_max&lt;/code&gt; methods for &lt;code&gt;AtomicI8&lt;/code&gt;, &lt;code&gt;AtomicU8&lt;/code&gt;, &lt;code&gt;AtomicI16&lt;/code&gt;, &lt;code&gt;AtomicU16&lt;/code&gt;, and so
on - all the way up to &lt;code&gt;AtomicU128&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;So our simple &lt;code&gt;fetch_max&lt;/code&gt; call is actually invoking generated code. But what
does the &lt;code&gt;atomic_umax&lt;/code&gt; function actually do? To answer that, we need
to see what the Rust compiler produces next.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 3: LLVM IR&lt;/head&gt;
    &lt;p&gt;Now that we know &lt;code&gt;fetch_max&lt;/code&gt; is macro-generated code calling &lt;code&gt;atomic_umax&lt;/code&gt;,
let's see what happens when the Rust compiler processes it. The compiler
doesn't go straight to assembly. First, it translates the code into an
intermediate representation. Rust uses the LLVM compiler project, so it
generates LLVM Intermediate Representation (IR).&lt;/p&gt;
    &lt;p&gt;If we peek at the LLVM IR for our &lt;code&gt;fetch_max&lt;/code&gt; call, we see something like this:&lt;/p&gt;
    &lt;quote&gt;; Before the transformationbb7:%0 = atomicrmw umax ptr %self, i64 %val monotonic, align 8...&lt;/quote&gt;
    &lt;p&gt;This is LLVM's language for saying: "I need an atomic read-modify-write operation. The modification I want to perform is an unsigned maximum."&lt;/p&gt;
    &lt;p&gt;This is a powerful, high-level instruction within the compiler itself. But it poses a critical question: does the CPU actually have a single instruction called &lt;code&gt;umax&lt;/code&gt;? For most architectures, the answer is no. So how does the
compiler bridge this gap?&lt;/p&gt;
    &lt;head rend="h3"&gt;How to See This Yourself&lt;/head&gt;
    &lt;p&gt;My goal is not to merely describe what is happening, but to give you the tools to see it for yourself. You can trace this transformation step-by-step on your own machine.&lt;/p&gt;
    &lt;p&gt;First, tell the Rust compiler to stop after generating the LLVM IR:&lt;/p&gt;
    &lt;quote&gt;rustc --emit=llvm-ir main.rs&lt;/quote&gt;
    &lt;p&gt;This creates a &lt;code&gt;main.ll&lt;/code&gt; file. This file contains the LLVM IR
representation of your Rust code, including our &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction.
Keep the file around; we'll use it in the next steps.&lt;/p&gt;
    &lt;head rend="h2"&gt;Interlude: Compiler Intrinsics&lt;/head&gt;
    &lt;p&gt;We're missing something important. How does the Rust function &lt;code&gt;atomic_umax&lt;/code&gt;
actually become the LLVM instruction &lt;code&gt;atomicrmw umax&lt;/code&gt;? This is where compiler
intrinsics come into play.&lt;/p&gt;
    &lt;p&gt;If you dig into Rust's source code, you'll find that &lt;code&gt;atomic_umax&lt;/code&gt; is
defined like this:&lt;/p&gt;
    &lt;quote&gt;/// Updates `*dst` to the max value of `val` and the old value (unsigned comparison)#[inline]#[cfg(target_has_atomic)]#[cfg_attr(miri, track_caller)] // even without panics, this helps for Miri backtracesunsafe fn atomic_umax&amp;lt;T: Copy&amp;gt;(dst: *mut T, val: T, order: Ordering) -&amp;gt; T {// SAFETY: the caller must uphold the safety contract for `atomic_umax`unsafe {match order {Relaxed =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Relaxed }&amp;gt;(dst, val),Acquire =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Acquire }&amp;gt;(dst, val),Release =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::Release }&amp;gt;(dst, val),AcqRel =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::AcqRel }&amp;gt;(dst, val),SeqCst =&amp;gt; intrinsics::atomic_umax::&amp;lt;T, { AO::SeqCst }&amp;gt;(dst, val),}}}&lt;/quote&gt;
    &lt;p&gt;But what is this &lt;code&gt;intrinsics::atomic_umax&lt;/code&gt; function? If you look at its
definition,
you find something slightly unusual:&lt;/p&gt;
    &lt;quote&gt;/// Maximum with the current value using an unsigned comparison./// `T` must be an unsigned integer type.////// The stabilized version of this intrinsic is available on the/// [`atomic`] unsigned integer types via the `fetch_max` method. For example, [`AtomicU32::fetch_max`].#[rustc_intrinsic]#[rustc_nounwind]pub unsafe fn atomic_umax&amp;lt;T: Copy, const ORD: AtomicOrdering&amp;gt;(dst: *mut T, src: T) -&amp;gt; T;&lt;/quote&gt;
    &lt;p&gt;There is no body. This is a declaration, not a definition. The &lt;code&gt;#[rustc_intrinsic]&lt;/code&gt; attribute tells the Rust compiler that this function
maps directly to a low-level operation understood by the compiler
itself. When the Rust compiler sees a call to &lt;code&gt;intrinsics::atomic_umax&lt;/code&gt;, it
knows to
replace it
with the corresponding
LLVM intrinsic function.&lt;/p&gt;
    &lt;p&gt;So our journey actually looks like this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;fetch_max&lt;/code&gt;method (user-facing API)&lt;/item&gt;
      &lt;item&gt;Macro expands to call &lt;code&gt;atomic_umax&lt;/code&gt;function&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;atomic_umax&lt;/code&gt;is a compiler intrinsic&lt;/item&gt;
      &lt;item&gt;Rustc replaces the intrinsic with LLVM's &lt;code&gt;atomicrmw umax&lt;/code&gt;← We are here&lt;/item&gt;
      &lt;item&gt;LLVM processes this instruction...&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Layer 4: The Transformation&lt;/head&gt;
    &lt;p&gt;LLVM runs a series of "passes" that analyze and transform the code. The one we're interested in is called the &lt;code&gt;AtomicExpandPass&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Its job is to look at high-level atomic operations like &lt;code&gt;atomicrmw umax&lt;/code&gt; and ask
the target architecture, "Can you do this natively?"&lt;/p&gt;
    &lt;p&gt;When the &lt;code&gt;x86-64&lt;/code&gt; backend says "No, I can't," this pass expands the single
instruction into a sequence of more fundamental ones that the CPU does
understand. The result is a
compare-and-swap (CAS) loop.&lt;/p&gt;
    &lt;p&gt;We can see this transformation in action by asking LLVM to emit the intermediate representation before and after this pass. To see the IR before the &lt;code&gt;AtomicExpandPass&lt;/code&gt;, run:&lt;/p&gt;
    &lt;quote&gt;llc -print-before=atomic-expand main.ll -o /dev/null&lt;/quote&gt;
    &lt;quote&gt;&lt;p&gt;Tip: If you do not have&lt;/p&gt;&lt;code&gt;llc&lt;/code&gt;installed, you can ask&lt;code&gt;rustc&lt;/code&gt;to run the pass for you directly.&lt;code&gt;rustc -C llvm-args="-print-before=atomic-expand -print-after=atomic-expand" main.rs&lt;/code&gt;&lt;/quote&gt;
    &lt;p&gt;The code will be printed to your terminal. The function containing our atomic max looks like this:&lt;/p&gt;
    &lt;quote&gt;*** IR Dump Before Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:%_0 = alloca [8 x i8], align 8%order = alloca [1 x i8], align 1store i8 %0, ptr %order, align 1%1 = load i8, ptr %order, align 1%_7 = zext i8 %1 to i64switch i64 %_7, label %bb2 [i64 0, label %bb7i64 1, label %bb5i64 2, label %bb6i64 3, label %bb4i64 4, label %bb3]bb2: ; preds = %startunreachablebb7: ; preds = %start%2 = atomicrmw umax ptr %self, i64 %val monotonic, align 8store i64 %2, ptr %_0, align 8br label %bb1bb5: ; preds = %start%3 = atomicrmw umax ptr %self, i64 %val release, align 8store i64 %3, ptr %_0, align 8br label %bb1bb6: ; preds = %start%4 = atomicrmw umax ptr %self, i64 %val acquire, align 8store i64 %4, ptr %_0, align 8br label %bb1bb4: ; preds = %start%5 = atomicrmw umax ptr %self, i64 %val acq_rel, align 8store i64 %5, ptr %_0, align 8br label %bb1bb3: ; preds = %start%6 = atomicrmw umax ptr %self, i64 %val seq_cst, align 8store i64 %6, ptr %_0, align 8br label %bb1bb1: ; preds = %bb3, %bb4, %bb6, %bb5, %bb7%7 = load i64, ptr %_0, align 8ret i64 %7}&lt;/quote&gt;
    &lt;p&gt;You can see the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction in multiple places, depending on
the memory ordering specified. This is the high-level atomic operation that the
compiler backend understands, but the CPU does not.&lt;/p&gt;
    &lt;quote&gt;llc -print-after=atomic-expand main.ll -o /dev/null&lt;/quote&gt;
    &lt;p&gt;This is the relevant part of the output:&lt;/p&gt;
    &lt;quote&gt;*** IR Dump After Expand Atomic instructions (atomic-expand) ***; Function Attrs: inlinehint nonlazybind uwtabledefine internal i64 @_ZN4core4sync6atomic9AtomicU649fetch_max17h6c42d6f2fc1a6124E(ptr align 8 %self, i64 %val, i8 %0) unnamed_addr #1 {start:%_0 = alloca [8 x i8], align 8%order = alloca [1 x i8], align 1store i8 %0, ptr %order, align 1%1 = load i8, ptr %order, align 1%_7 = zext i8 %1 to i64switch i64 %_7, label %bb2 [i64 0, label %bb7i64 1, label %bb5i64 2, label %bb6i64 3, label %bb4i64 4, label %bb3]bb2: ; preds = %startunreachablebb7: ; preds = %start%2 = load i64, ptr %self, align 8 ; seed expected valuebr label %atomicrmw.start ; enter CAS loopatomicrmw.start: ; preds = %atomicrmw.start, %bb7%loaded = phi i64 [ %2, %bb7 ], [ %newloaded, %atomicrmw.start ] ; on first iteration: use %2, on retries: use value observed by last cmpxchg%3 = icmp ugt i64 %loaded, %val ; unsigned compare (umax semantics)%new = select i1 %3, i64 %loaded, i64 %val ; desired = max(loaded, val)%4 = cmpxchg ptr %self, i64 %loaded, i64 %new monotonic monotonic, align 8 ; CAS: if *self==loaded, store new%success = extractvalue { i64, i1 } %4, 1 ; boolean: whether the swap happened%newloaded = extractvalue { i64, i1 } %4, 0 ; value seen in memory before the CASbr i1 %success, label %atomicrmw.end, label %atomicrmw.start ; loop until CAS succeedsatomicrmw.end: ; preds = %atomicrmw.startstore i64 %newloaded, ptr %_0, align 8br label %bb1[... MORE OF THE SAME, JUST FOR DIFFERENT ORDERING..]bb1: ; preds = %bb3, %bb4, %bb6, %bb5, %bb7%7 = load i64, ptr %_0, align 8ret i64 %7}&lt;/quote&gt;
    &lt;p&gt;We can see the pass did not change the first part - it still has the code to dispatch based on the memory ordering. But in the &lt;code&gt;bb7&lt;/code&gt; block, where we originally had the
&lt;code&gt;atomicrmw umax&lt;/code&gt; LLVM instruction, we now see a full compare-and-swap loop.
A compiler engineer would say that the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction has been
"lowered" into a sequence of more primitive operations, that are closer to what
the hardware can actually execute.&lt;/p&gt;
    &lt;p&gt;Here's the simplified logic:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read (seed): grab the current value (&lt;code&gt;expected&lt;/code&gt;).&lt;/item&gt;
      &lt;item&gt;Compute: &lt;code&gt;desired = umax(expected, val)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;Attempt: &lt;code&gt;observed, success = cmpxchg(ptr, expected, desired, [...])&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;If success, return &lt;code&gt;observed&lt;/code&gt;(the old value). Otherwise&lt;code&gt;set expected = observed&lt;/code&gt;and loop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This CAS loop is a fundamental pattern in lock-free programming. The compiler just built it for us automatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Layer 5: The Final Product (x86-64 Assembly)&lt;/head&gt;
    &lt;p&gt;We're at the final step. To see the final machine code, you can tell &lt;code&gt;rustc&lt;/code&gt; to
emit the assembly directly:&lt;/p&gt;
    &lt;quote&gt;rustc --emit=asm main.rs&lt;/quote&gt;
    &lt;p&gt;This will produce a &lt;code&gt;main.s&lt;/code&gt; file containing the final assembly code.
Inside, you'll find the result of the &lt;code&gt;cmpxchg&lt;/code&gt; loop:&lt;/p&gt;
    &lt;quote&gt;.LBB8_2:movq -32(%rsp), %rax # rax = &amp;amp;selfmovq (%rax), %rax # rax = *self (seed 'expected')movq %rax, -48(%rsp) # spill expected to stack.LBB8_3: # loop headmovq -48(%rsp), %rax # rax = expectedmovq -32(%rsp), %rcx # rcx = &amp;amp;selfmovq -40(%rsp), %rdx # rdx = valmovq %rax, %rsi # rsi = expected (scratch)subq %rdx, %rsi # set flags for unsigned compare: expected - valcmovaq %rax, %rdx # if (expected &amp;gt; val) rdx = expected; else rdx = val (compute max)lock cmpxchgq %rdx, (%rcx)# CAS: if *rcx==rax then *rcx=rdx; rax &amp;lt;- old *rcx; ZF=successsete %cl # cl = successmovq %rax, -56(%rsp) # spill observed to stacktestb $1, %cl # branch on successmovq %rax, -48(%rsp) # expected = observed (for retry)jne .LBB8_4 # success -&amp;gt; exitjmp .LBB8_3 # failure → retry&lt;/quote&gt;
    &lt;p&gt;The syntax might look a bit different from what you're used to, that's because it's in AT&amp;amp;T syntax, which is the default for &lt;code&gt;rustc&lt;/code&gt;. If you prefer Intel syntax, you can
use &lt;code&gt;rustc --emit=asm main.rs -C "llvm-args=-x86-asm-syntax=intel"&lt;/code&gt; to get that.&lt;/p&gt;
    &lt;p&gt;I'm not an assembly expert, but you can see the key parts of the CAS loop here:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Seed read (first iteration): Load &lt;code&gt;*self&lt;/code&gt;once to initialize the expected value.&lt;/item&gt;
      &lt;item&gt;Compute umax without branching: The pair &lt;code&gt;sub&lt;/code&gt;+&lt;code&gt;cmova&lt;/code&gt;implements&lt;code&gt;desired = max_u(expected, val)&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;CAS operation: On x86-64, &lt;code&gt;cmpxchg&lt;/code&gt;uses&lt;code&gt;RAX&lt;/code&gt;as the expected value and returns the observed value in&lt;code&gt;RAX&lt;/code&gt;;&lt;code&gt;ZF&lt;/code&gt;encodes success.&lt;/item&gt;
      &lt;item&gt;Retry or finish: If &lt;code&gt;ZF&lt;/code&gt;is clear, we failed and need to retry. Otherwise, we are done.&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;&lt;p&gt;Note we did not ask&lt;/p&gt;&lt;code&gt;rustc&lt;/code&gt;to optimize the code. If we did, the compiler would generate more efficient assembly: No spills to the stack, fewer jumps, no dispatch on memory ordering, etc. But I wanted to keep the output as close to the original IR as possible to make it easier to follow.&lt;/quote&gt;
    &lt;head rend="h2"&gt;The Beauty of Abstraction&lt;/head&gt;
    &lt;p&gt;And there we have it. Our journey is complete. We started with a safe, clear, single line of Rust and ended with a CAS loop written in assembly language.&lt;/p&gt;
    &lt;p&gt;Rust &lt;code&gt;fetch_max&lt;/code&gt; → Macro-generated &lt;code&gt;atomic_umax&lt;/code&gt; → LLVM
&lt;code&gt;atomicrmw umax&lt;/code&gt; → LLVM &lt;code&gt;cmpxchg&lt;/code&gt; loop → Assembly &lt;code&gt;lock cmpxchg&lt;/code&gt; loop&lt;/p&gt;
    &lt;p&gt;This journey is a perfect example of the power of modern compilers. We get to work at a high level of abstraction, focusing on safety and logic, while the compiler handles the messy, error-prone, and incredibly complex task of generating correct and efficient code for the hardware.&lt;/p&gt;
    &lt;p&gt;So, next time you use an atomic, take a moment to appreciate the incredible, hidden journey your code is about to take.&lt;/p&gt;
    &lt;p&gt;PS: After conducting this journey I learned that C++26 adds &lt;code&gt;fetch_max&lt;/code&gt;
too!&lt;/p&gt;
    &lt;p&gt;PPS: We are hiring!&lt;/p&gt;
    &lt;head rend="h2"&gt;Bonus: Apple Silicon (AArch64)&lt;/head&gt;
    &lt;p&gt;Out of curiosity, I also checked how this looks on Apple Silicon (AArch64). This architecture does have a native &lt;code&gt;atomic max&lt;/code&gt; instruction, so the
&lt;code&gt;AtomicExpandPass&lt;/code&gt; does not need to lower it into a CAS loop. The LLVM code before and after
the pass is identical, still containing the &lt;code&gt;atomicrmw umax&lt;/code&gt; instruction.&lt;/p&gt;
    &lt;p&gt;The final assembly contains a variant of the &lt;code&gt;LDUMAX&lt;/code&gt; instruction. This is the relevant part of the assembly:&lt;/p&gt;
    &lt;quote&gt;ldr x8, [sp, #16] # x8 = value to compare withldr x9, [sp, #8] # x9 = pointer to the atomic variableldumax x8, x8, [x9] # atomic unsigned max (relaxed), [x9] = max(x8, [x9]), x8 = old valuestr x8, [sp, #40] # Store old valueb LBB8_11&lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Note that AArch64 uses Unified Assembler Language, when reading the snippet above, it's important to remember that the destination register comes first.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;And that's really it. We could continue to dig into the microarchitecture, to see how instructions are executed at the hardware level, what are the effects of the &lt;code&gt;LOCK&lt;/code&gt; prefix, dive into differences in memory ordering, etc.
But we'll leave that for another day.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Alice: "Would you tell me, please, which way I ought to go from here?"&lt;/p&gt;&lt;lb/&gt;The Cat: "That depends a good deal on where you want to get to."&lt;lb/&gt;Alice: "I don't much care where."&lt;lb/&gt;The Cat: "Then it doesn't much matter which way you go."&lt;lb/&gt;Alice: "...So long as I get somewhere."&lt;lb/&gt;The Cat: "Oh, you're sure to do that, if only you walk long enough."&lt;p&gt;- Lewis Carroll, Alice's Adventures in Wonderland&lt;/p&gt;&lt;/quote&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45352944</guid><pubDate>Tue, 23 Sep 2025 21:24:45 +0000</pubDate></item><item><title>Top Programming Languages 2025</title><link>https://spectrum.ieee.org/top-programming-languages-2025</link><description>&lt;doc fingerprint="ff481adb873c88f9"&gt;
  &lt;main&gt;&lt;p&gt;Since 2013, we’ve been metaphorically peering over the shoulders of programmers to create our annual interactive rankings of the most popular programming languages. But fundamental shifts in how people are coding may not just make it harder to measure popularity, but could even make the concept itself irrelevant. And then things might get really weird. To see why, let’s start with this year’s rankings and a quick refresher of how we put this thing together.&lt;/p&gt;&lt;p&gt;In the “Spectrum” default ranking, which is weighted with the interests of IEEE members in mind, we see that once again Python has the top spot, with the biggest change in the top five being JavaScript’s drop from third place last year to sixth place this year. As JavaScript is often used to create web pages, and vibe coding is often used to create websites, this drop in the apparent popularity may be due to the effects of AI that we’ll dig into in a moment. But first to finish up with this year’s scores, in the “Jobs” ranking, which looks exclusively at what skills employers are looking for, we see that Python has also taken 1st place, up from second place last year, though SQL expertise remains an incredibly valuable skill to have on your resume.&lt;/p&gt;&lt;p&gt;Because we can’t literally look over the shoulders of everyone who codes, including kids hacking on Minecraft servers or academic researchers developing new architectures, we rely on proxies to measure popularity. We detail our methodology here, but the upshot is that we merge metrics from multiple sources to create our rankings. The metrics we choose publicly signal interest across a wide range of languages—Google search traffic, questions asked on Stack Exchange, mentions in research papers, activity on the GitHub open source code repository, and so on.&lt;/p&gt;&lt;p&gt;But programmers are turning away from many of these public expressions of interest. Rather than page through a book or search a website like Stack Exchange for answers to their questions, they’ll chat with an LLM like Claude or ChatGPT in a private conversation. And with an AI assistant like Cursor helping to write code, the need to pose questions in the first place is significantly decreased. For example, across the total set of languages evaluated in the TPL, the number of questions we saw posted per week on Stack Exchange in 2025 was just 22 percent of what it was in 2024.&lt;/p&gt;&lt;p&gt;With less signal in publicly available metrics, it becomes harder to track popularity across a broad range of languages. This existential problem for our rankings can be tackled by searching for new metrics, or trying to survey programmers—in all their variety—directly. However, an even more fundamental problem is looming in the wings.&lt;/p&gt;&lt;p&gt;Whether it’s a seasoned coder using an AI to handle the grunt work, or a neophyte vibe coding a complete web app, AI assistance means that programmers can concern themselves less and less with the particulars of any language. First details of syntax, then flow control and functions, and so on up the levels of how a program is put together—more and more is being left to the AI.&lt;/p&gt;&lt;p&gt;Although code-writing LLM’s are still very much a work in progress, as they take over an increasing share of the work, programmers inevitably shift from being the kind of people willing to fight religious wars over whether source code should be indented by typing tabs or spaces to people who care less and less about what language is used.&lt;/p&gt;&lt;p&gt;After all, the whole reason different computer languages exist is because given a particular challenge, it’s easier to express a solution in one language versus another. You wouldn’t control a washing machine using the R programming language, or conversely do a statistical analysis on large datasets using C.&lt;/p&gt;&lt;p&gt;But it is technically possible to do both. A human might tear their hair out doing it, but LLMs have about as much hair as they do sentience. As long as there’s enough training data, they’ll generate code for a given prompt in any language you want. In practical terms, this means using one—any one—of today’s most popular general purpose programming languages. In the same way most developers today don’t pay much attention to the instruction sets and other hardware idiosyncrasies of the CPUs that their code runs on, which language a program is vibe coded in ultimately becomes a minor detail.&lt;/p&gt;&lt;p&gt;Sure, there will always be some people who care, just as today there are nerds like me willing to debate the merits of writing for the Z80 versus the 6502 8-bit CPUs. But overall, the popularity of different computer languages could become as obscure a topic as the relative popularity of railway track gauges.&lt;/p&gt;&lt;p&gt;One obvious long-term consequence to this is that it will become harder for new languages to emerge. Previously, new languages could emerge from individuals or small teams evangelizing their approach to potential contributors and users. Presentations, papers, demos, sample code and tutorials seeded new developer ecosystems. A single well-written book, like Leo Brodie’s Starting Forth or Brian Kernighan and Dennis Ritchies’ The C Programming Language, could make an enormous difference to a language’s popularity.&lt;/p&gt;&lt;p&gt;But while a few samples and a tutorial can be enough material to jump-start adoption among programmers familiar with the ins and outs of hands-on coding, it’s not enough for today’s AIs. Humans build mental models that can extrapolate from relatively small amounts of data. LLMs rely on statistical probabilities, so the more data they can crunch, they better they are. Consequently programmers have noted that AIs give noticeably poorer results when trying to code in less-used languages.&lt;/p&gt;&lt;p&gt;There are research efforts to make LLMs more universal coders, but that doesn’t really help new languages get off the ground. Fundamentally new languages grow because they are scratching some itch a programmer has. That itch can be as small as being annoyed at semicolons having to be placed after every statement, or as large as a philosophical argument about the purpose of computation.&lt;/p&gt;&lt;p&gt;But if an AI is soothing our irritations with today’s languages, will any new ones ever reach the kind of critical mass needed to make an impact? Will the popularity of today’s languages remain frozen in time?&lt;/p&gt;&lt;head rend="h2"&gt;What’s the future of programming languages?&lt;/head&gt;&lt;p&gt;Before speculating further about the future, let’s touch base again where we are today. Modern high-level computer languages are really designed to do two things: create an abstraction layer that makes it easier to process data in a suitable fashion, and stop programmers from shooting themselves in the foot.&lt;/p&gt;&lt;p&gt;The first objective has been around since the days of Fortran and Cobol, aimed at processing scientific and business data respectively. The second objective emerged later, spurred in no small part by Edgar Dijkstra’s 1968 paper “Go To Statement Considered Harmful.” In this he argued for eliminating the ability for a programmer to make jumps to arbitrary points in their code. This restriction was to prevent so-called spaghetti code that makes it hard for a programmer to understand how a computer actually executes a given program. Instead, Dijkstra demanded that programmers bend to structural rules imposed by the language. Dijkstra’s argument ultimately won the day, and most modern languages do indeed minimize or eliminate Go Tos altogether in favor of structures like functions and other programmatic blocks.&lt;/p&gt;&lt;p&gt;These structures don’t exist at the level of the CPU. If you look at the instruction sets for Arm, x86, or RISC-V processors, the flow of a program is controlled by just three types of machine code instructions. These are conditional jumps, unconditional jumps, and jumps with a trace stored (so you can call a subroutine and return to where you started). In other words, it’s Go Tos all the way down. Similarly, strict data types designed to label and protect data from incorrect use dissolve into anonymous bits flowing in and out of memory.&lt;/p&gt;&lt;p&gt;So how much abstraction and anti-foot-shooting structure will a sufficiently-advanced coding AI really need? A hint comes from recent research in AI-assisted hardware design, such as Dall-EM, a generative AI developed at Princeton University used to create RF and electromagnetic filters. Designing these filters has always been something of a black art, involving the wrangling of complex electromagnetic fields as they swirl around little strips of metal. But Dall-EM can take in the desired inputs and outputs and spit out something that looks like a QR code. The results are something no human would ever design—but it works.&lt;/p&gt;&lt;p&gt;Similarly, could we get our AIs to go straight from prompt to an intermediate language that could be fed into the interpreter or compiler of our choice? Do we need high-level languages at all in that future? True, this would turn programs into inscrutable black boxes, but they could still be divided into modular testable units for sanity and quality checks. And instead of trying to read or maintain source code, programmers would just tweak their prompts and generate software afresh.&lt;/p&gt;&lt;p&gt;What’s the role of the programmer in a future without source code? Architecture design and algorithm selection would remain vital skills—for example, should a pathfinding program use a classic approach like the A* algorithm, or instead should it try to implement a new method? How should a piece of software be interfaced with a larger system? How should new hardware be exploited? In this scenario, computer science degrees, with their emphasis on fundamentals over the details of programming languages, rise in value over coding boot camps.&lt;/p&gt;Will there be a Top Programming Language in 2026? Right now, programming is going through the biggest transformation since compilers broke onto the scene in the early 1950s. Even if the predictions that much of AI is a bubble about to burst come true, the thing about tech bubbles is that there’s always some residual technology that survives. It’s likely that using LLMs to write and assist with code is something that’s going to stick. So we’re going to be spending the next 12 months figuring out what popularity means in this new age, and what metrics might be useful to measure. What do you think popularity should mean? What metrics do you think we should consider? Let us know in the comments below.&lt;list rend="ul"&gt;&lt;item&gt;AI Models Embrace Humanlike Reasoning ›&lt;/item&gt;&lt;item&gt;LLM Benchmarking Shows Capabilities Doubling Every 7 Months ›&lt;/item&gt;&lt;item&gt;Why Functional Programming Should Be the Future of Software Development ›&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Stephen Cass is the special projects editor at IEEE Spectrum. He currently helms Spectrum's Hands On column, and is also responsible for interactive projects such as the Top Programming Languages app. He has a bachelor's degree in experimental physics from Trinity College Dublin.&lt;/p&gt;&lt;p&gt;A programming language is a bridge between natural English and computer machine languages, allowing humans to tell the computer what to do.&lt;/p&gt;&lt;p&gt;The idea language will be human's natural languages, including English, Chinese and other popular human languages we learned after birth.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45354314</guid><pubDate>Tue, 23 Sep 2025 23:42:50 +0000</pubDate></item><item><title>Baldur's Gate 3 Steam Deck – Native Version</title><link>https://larian.com/support/faqs/steam-deck-native-version_121</link><description>&lt;doc fingerprint="bcd4a8bd8df387c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Steam Deck - Native Version&lt;/head&gt;
    &lt;p&gt;Upon release of Hotfix #34 on your Steam Deck, your device will install the Native version.&lt;/p&gt;
    &lt;p&gt;If you are unsure whether the build has been installed correctly, you can do the following:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the game’s Steam page. Click on the Settings button and select Properties.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p/&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once in the Properties page, go to the Compatibility tab.&lt;/item&gt;
      &lt;item&gt;Tick the box for “Force the use of a specific Steam Play compatibility tool”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select any version that has Linux Runtime.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow the game to update if an update appears.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;What’s the difference between the Steam Deck Native and Proton version?&lt;/p&gt;
    &lt;p&gt;Our Proton version runs on the Steam Deck via the Proton compatibility layer, which requires extra CPU processing power. Running the game natively on the Steam Deck requires less CPU usage and memory consumption overall!&lt;/p&gt;
    &lt;p&gt;Can I still switch back to the Proton version?&lt;/p&gt;
    &lt;p&gt;Yes. If you’re having issues with the Steam Deck Native build, you can revert to the Proton version. Take the following steps to do so:&lt;/p&gt;
    &lt;p&gt;Go to the game’s Steam page. Click on the Settings button and select Properties.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Once in the Properties page, go to the Compatibility tab.&lt;/item&gt;
      &lt;item&gt;Tick the box for “Force the use of a specific Steam Play compatibility tool”.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Select any Proton version 8 or higher.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Allow the game to update.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now that there is a Steam Deck Native build, is Baldur’s Gate 3 supported on Linux?&lt;/p&gt;
    &lt;p&gt;Larian does not provide support for the Linux platform. The Steam Deck Native build is only supported on Steam Deck.&lt;/p&gt;
    &lt;head rend="h2"&gt;Savegames&lt;/head&gt;
    &lt;p&gt;Where are my saves located currently (before using the Steam Deck Native version)?&lt;/p&gt;
    &lt;p&gt;Before the Steam Deck Native version becomes the primary version, your saves will be in the compatdata folder: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/p&gt;
    &lt;p&gt;Where are my saves located when I use the Steam Deck Native version?&lt;/p&gt;
    &lt;p&gt;After the Steam Deck Native version becomes the primary version, your saves will be in the following folder: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/p&gt;
    &lt;p&gt;Why are my saves in different folders?&lt;/p&gt;
    &lt;p&gt;When Baldur’s Gate 3 runs on the Proton compatibility layer, the Proton version will store the saves in the compatdata folder, which is a mirrored version of the Windows file storage system. On the Steam Deck Native version, the saves are stored natively on the SteamOS file storage system.&lt;/p&gt;
    &lt;p&gt;Will my savegames be transferred over to the new version when I use the Steam Deck Native version?&lt;/p&gt;
    &lt;p&gt;If your Steam Cloud saves are turned on, your most recent saves will be synced to the Steam Deck Native savegame folder automatically.&lt;/p&gt;
    &lt;p&gt;What if I don’t have Cloud saves turned on, or I want my older saves?&lt;/p&gt;
    &lt;p&gt;Your saves are still stored on the Steam Deck, but they will be stored in the compatdata folder.&lt;lb/&gt; You can manually transfer these files via the Desktop:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;First, switch to Desktop Mode by clicking on the Steam button and selecting Power. Then click on Switch to Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p/&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;If you have a mouse and keyboard to hand, plug them in to make your life a little easier, and click on the folder icon on the bar at the bottom.&lt;/item&gt;
      &lt;item&gt;In the explorer window, navigate to: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/item&gt;
      &lt;item&gt;Copy the Savegames folder.&lt;/item&gt;
      &lt;item&gt;Navigate to: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/PlayerProfiles/Public&lt;/item&gt;
      &lt;item&gt;Paste the copied folder in this location.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;lb/&gt; Will my old saves still take up storage space on my Steam Deck?&lt;/p&gt;
    &lt;p&gt;Yes, your old saves will still take up storage space. If you want to save some space and you don't plan on using the Proton version, you can delete the compatdata folder after you've copied over the folders.&lt;/p&gt;
    &lt;head rend="h2"&gt;Mods&lt;/head&gt;
    &lt;p&gt;Will my mods be transferred over automatically?&lt;/p&gt;
    &lt;p&gt;If you are logged into your Larian Account and have it connected to mod.io, all mods you are subscribed to will be downloaded when the transition to Steam Deck Native occurs.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt; What if I’m not logged into a Larian Account or connected to mod.io?&lt;/p&gt;
    &lt;p&gt;You can either manually download the mods from the Mod Manager or transfer them manually from the previous folder.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;To do so,switch to Desktop Mode by clicking on the Steam button and selecting Power. Then click on Switch to Desktop.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Click on the folder icon on the bar at the bottom.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;In the explorer window, navigate to: /home/deck/.local/share/Steam/steamapps/compatdata/1086940/pfx/drive_c/users/steamuser/AppData/Local/Larian Studios/Baldur's Gate 3&lt;/item&gt;
      &lt;item&gt;Copy the Mods folder.&lt;/item&gt;
      &lt;item&gt;Navigate to: /home/deck/.local/share/Larian Studios/Baldur's Gate 3/&lt;/item&gt;
      &lt;item&gt;Paste the copied folder in this location.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45354644</guid><pubDate>Wed, 24 Sep 2025 00:26:59 +0000</pubDate></item><item><title>Zutty: Zero-cost Unicode Teletype, high-end terminal for low-end systems</title><link>https://git.hq.sig7.se/zutty.git</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45355462</guid><pubDate>Wed, 24 Sep 2025 02:07:23 +0000</pubDate></item><item><title>New study shows plants and animals emit a visible light that expires at death</title><link>https://pubs.acs.org/doi/10.1021/acs.jpclett.4c03546</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45355965</guid><pubDate>Wed, 24 Sep 2025 03:27:41 +0000</pubDate></item><item><title>Greatest irony of the AI age: Humans hired to clean AI slop</title><link>https://www.sify.com/ai-analytics/greatest-irony-of-the-ai-age-humans-being-increasingly-hired-to-clean-ai-slop/</link><description>&lt;doc fingerprint="be4119bcbb886cbd"&gt;
  &lt;main&gt;
    &lt;p&gt;On one side is AI swallowing millions of jobs, and on the other is humans being hired to clean up the nonsense AI often generates, finds Satyen K. Bordoloi&lt;/p&gt;
    &lt;p&gt;This was early 2023, a few months after ChatGPT had just made the perfect superintelligence landing in our lives. A producer friend, who wanted a beat sheet of a series written into a synopsis, sent me a document he said he had gotten written.&lt;/p&gt;
    &lt;p&gt;A reading of its first paragraph was all it took to identify the writer: ChatGPT. The perfect robotic structure, excessive and often misplaced adverbs and adjectives, and the absence of indirect tense gave it away instantly. It was sloppy in its sterile perfection.&lt;/p&gt;
    &lt;p&gt;Yet, my friend asked me to take it as a base and improve it. Crunched for time, I did. I didn’t know then, but I had unwittingly participated in what would become one of the most in-demand gigs two years later: humans cleaning up AI slop.&lt;/p&gt;
    &lt;p&gt;This is the defining irony of the AI age. While AI is consuming millions of jobs, it is simultaneously creating a unique category of employment for hundreds of thousands of humans: cleaning up the mess AI makes. Designers, writers and digital artists are increasingly being hired not to create from scratch, but fix the mess AI invariably makes when tasked with complex work. What is doubly ironic is that these are often the same humans who would have been hired to create the original had AI not been brought to undercut them.&lt;/p&gt;
    &lt;head rend="h2"&gt;WHAT IS AI SLOP&lt;/head&gt;
    &lt;p&gt;Jack Izzo, in a Yahoo article, defines it better than any LLM can: “AI slop is the evolution of spam, in a way. Like spam, slop is low-quality content, but thanks to artificial intelligence (AI) tools like ChatGPT and Midjourney, it’s even easier to produce. Like spam, slop can grow like a weed if left unchecked, overwhelming social media feeds and leaving users unsure of what’s real and what’s not. Like spam, slop comes in many forms — posts on social media.. books on Amazon, music on Spotify, articles from less-than-reliable news outlets (and, unfortunately, some reliable outlets) and even occasionally in peer-reviewed scientific journals.”&lt;/p&gt;
    &lt;p&gt;It is the content equivalent of empty calories: visually or textually appealing, but devoid of substance, originality, or reliable meaning.&lt;/p&gt;
    &lt;p&gt;With video generation becoming as cheap and easy as creating images, the internet is being flooded with AI-generated video slop. A hyper-realistic video of a seagull staring down a French fry on a car dashboard before smashing the window to grab it generated over 140 million views. A CCTV-style video of rabbits jumping on a backyard trampoline has racked up over 200 million views on TikTok and X. So has another video of a bear doing the same. And unsurprisingly, even porn is now overflowing with AI-generated slop.&lt;/p&gt;
    &lt;p&gt;Now the bunny video had tell-tale glitches: a bunny with two heads, and another vanishing mid-bounce. These alerted the discerning viewers to its sloppy origin. But this raises a question: What if the creator had hired a VFX artist to correct the errors?&lt;/p&gt;
    &lt;head rend="h2"&gt;HARMS OF THE AI SLOPOCALYPSE&lt;/head&gt;
    &lt;p&gt;The dangers of AI slop are many. First and foremost, the well of misinformation that the internet has always been is now being industrialised by AI that can generate thousands of plausible-sounding articles, product reviews, or social media posts in the time it takes a person to write just one. This floods everything, burying good information under a mountain of convincing garbage. So far, we have seen the enshittification of online businesses.&lt;/p&gt;
    &lt;p&gt;However, with AI models remixing and regurgitating existing content, what we have is the enshittification of culture itself, as music playlists are already overflowing with AI-generated music, Amazon with AI-generated books, and TikTok and other social media platforms are slowly filling up with AI-made videos.&lt;/p&gt;
    &lt;p&gt;And let’s not forget that creating this garbage consumes staggering amounts of water and electricity, contributing to emissions that harm the planet. Then there are people hired to clean up AI nonsense who could have been artists in their own right, but are now relegated to digital janitorial duties, leading to frustration and burnout.&lt;/p&gt;
    &lt;head rend="h2"&gt;CLEANUP CREW TO THE RESCUE&lt;/head&gt;
    &lt;p&gt;And the ones saving us from the slopocalypse, irony be crucified, are now good old humans with analogue brains. The promised AI utopia of effortless creation is instead giving rise to an underclass of digital rescuers, whose job profiles are being rewritten as AI code and training changes. These roles for AI clean up specialists are cropping up across industries, especially in freelance and creative sectors where AI’s limitations are most glaring.&lt;/p&gt;
    &lt;p&gt;First and foremost are the AI content rewriters hired to rewrite AI-generated articles, blogs, and marketing content that lack nuance, emotional resonance and factual accuracy. Then there are the art fixers hired to redraw or retouch AI-generated logos, illustrations, and art. Most AI-generated images have wrapped text, symmetry that doesn’t match reality and can be pixelated. Actual graphic designers and AI artists work to restore clarity and scale. AI code debuggers are hired to patch buggy code written by the likes of GitHub Copilot or ChatGPT. These actual developers and freelance engineers are hired to test, fix and optimise AI-generated code.&lt;/p&gt;
    &lt;p&gt;AI-generated videos are glitchy and often get physics wrong, and generate random things inside frames. AI video polishers are typically VFX artists whose job is to enhance the visual coherence and thus the realism of the footage.&lt;/p&gt;
    &lt;p&gt;These roles are not about collaboration, but correction. And the cost-saving AI promised is a mirage that can’t be held without the hidden overhead of human quality control. This entire endeavour reeks of a bizarre inefficiency as machines create slop at scale, and humans are hired to clean it up at a premium.&lt;/p&gt;
    &lt;p&gt;Go to freelance platforms like Upwork, Fiverr, and Freelancer, and you’ll see a surging demand for human-led creativity, especially in writing, image creation, and design.&lt;/p&gt;
    &lt;head rend="h2"&gt;MOTHER OF ALL IRONIES&lt;/head&gt;
    &lt;p&gt;AI was supposed to replace humans. Instead, it is creating a parallel economy of human fixers: people who make synthetic content usable, relatable, real – make it feel more human. There’s another irony – AI is replacing humans in certain jobs, while also creating menial jobs for them. People who, before AI, would have become artists have been relegated to the job of cleaners, janitors, cleaning AI slop. Yes, AI is on one side revealing just how irreplaceable humans are when it comes to nuance, empathy, and storytelling, but at the cost of the humans who can do those.&lt;/p&gt;
    &lt;p&gt;The problem here, as often isn’t artificial intelligence, but natural human stupidity. AI creating slop and humans hired to clean it isn’t an inevitable tech progress outcome. No! It’s a choice arisen out of a gold rush mentality that prioritises speed, volume and cost-cutting over quality, authenticity, and truth.&lt;/p&gt;
    &lt;p&gt;The solution, hence, lies not in AI becoming more ‘intelligent’, but in humans becoming smarter and realising that humans should always be in the loop, not brought in at the end to clean up. The solution isn’t in abandoning AI, but in recalibrating our relationship with it. We must realise that AI isn’t a replacement for human creativity and judgment, but that it’s a tool, a powerful one at that, which, when guided by human empathy and art, will create beauty and heart.&lt;/p&gt;
    &lt;p&gt;The greatest irony of this AI age may be that humans are hired to clean up AI’s mess; the greatest tragedy, however, would be if we became so accustomed to that slop that we forgot what a clean, human-made world looks like. The cleanup crew is a temporary fix. The real work is in ensuring that our technological future is built not on a foundation of AI slop, but on a commitment to genuine human creativity and integrity.&lt;/p&gt;
    &lt;head rend="h3"&gt;In case you missed:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kodak Moment: How Apple, Amazon, Meta, Microsoft Missed the AI Boat, Playing Catch-Up&lt;/item&gt;
      &lt;item&gt;Rise of Generative AI in India: Trends &amp;amp; Opportunities&lt;/item&gt;
      &lt;item&gt;The End of SEO as We Know It: Welcome to the AIO Revolution&lt;/item&gt;
      &lt;item&gt;Google Falters Under AI Onslaught: Future of Search in Peril?&lt;/item&gt;
      &lt;item&gt;One Year of No-camera Filmmaking: How AI Rewrote Rules of Cinema Forever&lt;/item&gt;
      &lt;item&gt;Rise of the Robolympics: When R2-D2 Meets Rocky Balboa&lt;/item&gt;
      &lt;item&gt;Are Hallucinations Good For AI? Maybe Not, But They’re Great For Humans&lt;/item&gt;
      &lt;item&gt;AI Taken for Granted: Has the World Reached the Point of AI Fatigue?&lt;/item&gt;
      &lt;item&gt;Hey Marvel, Just Admit You’re Using AI – We All Are!&lt;/item&gt;
      &lt;item&gt;Anthropomorphisation of AI: Why Can’t We Stop Believing AI Will End the World?&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45356226</guid><pubDate>Wed, 24 Sep 2025 04:15:04 +0000</pubDate></item><item><title>You didn't see it coming</title><link>https://aishwaryagoel.com/you-didnt-see-it-coming/</link><description>&lt;doc fingerprint="188c151f1be4746f"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;You Didn't See It Coming&lt;/head&gt;
    &lt;p&gt;It started with the hiring posts.&lt;/p&gt;
    &lt;p&gt;A friend, call him Dev, began writing on LinkedIn and Twitter/X about his intentionally small team. How they moved faster, knew each other deeply, avoided the bureaucracy that kills startups. His posts got traction. The story felt authentic because, at the time, it was.&lt;/p&gt;
    &lt;p&gt;What I knew, and his followers didn't, was simpler. Dev couldn't hire. His runway was tight, his equity offers weren't competitive, and the candidates he wanted kept choosing better-funded competitors. The small team wasn't a philosophy, it was a constraint.&lt;/p&gt;
    &lt;p&gt;A year later, the layoff headlines started. Big tech cutting thousands, venture-backed startups imploding, bloated teams getting slashed. Suddenly, Dev's posts multiplied. Thoughtful threads about choosing culture over rapid growth, the wisdom of staying lean. He wrote about how he'd "always believed in sustainable team building and praised his foresight in avoiding the growth-at-all-costs trap.&lt;/p&gt;
    &lt;p&gt;I watched this transformation with fascination. Dev had become the prophet of his own past.&lt;/p&gt;
    &lt;p&gt;Source:Internet&lt;/p&gt;
    &lt;p&gt;I started noticing it everywhere. The founder who couldn't afford an office now posts about remote-first culture. The one who couldn't hire senior engineers writes threads about growing talent from within. The startup that stayed in their home market because international expansion was too expensive now shares insights about deep local expertise over global sprawl.&lt;/p&gt;
    &lt;p&gt;We've all become heroes of our carefully edited stories.&lt;/p&gt;
    &lt;p&gt;Nobody wants to admit their big decisions were driven by fear, circumstance, or luck. We want to feel like we have agency, like we're smart enough to see around corners. Social media gives us the perfect platform to curate evidence of our judgment, to turn our anxious stumbling into confident striding.&lt;/p&gt;
    &lt;p&gt;But something darker emerges. I think about founders reading Dev's small team philosophy, especially those facing similar hiring challenges.&lt;/p&gt;
    &lt;p&gt;What do they learn? That Doubt is failure. Feeds become echo chambers where everyone is always right, and the messy reality of decision making is sanitized into personal brand moments.&lt;/p&gt;
    &lt;p&gt;What do we need? Real guidance. That comes from honesty about constraints, the times when your "strategy" was actually just making the best of what you had. It demands admitting that many successful decisions weren't visionary choices but creative responses to circumstances beyond your control.&lt;/p&gt;
    &lt;p&gt;But that kind of honesty doesn't get shared or saved on LinkedIn and X/Twitter.&lt;/p&gt;
    &lt;p&gt;Still, I couldn't shake the feeling that something important was being lost in all this performed wisdom. Maybe the most valuable thing we could share wasn't evidence of our foresight, but honest accounts of how often we're all just figuring it out as we go.&lt;/p&gt;
    &lt;p&gt;Maybe the real prophet in my feed was the person brave enough to admit they didn't see it coming.&lt;/p&gt;
    &lt;head rend="h3"&gt;What I've been Learning:&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The $10 Trillion AI Revolution: Loved the "flops per knowledge worker" concept and how uncertainty can become competitive leverage. The idea of building investment thesis around what others see as risk resonated - turning ambiguity into advantage.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The Stubborn Genius of James Dyson: Dyson's obsession with doing things differently rather than following trends was fascinating. His point that breakthrough success isn't quantum leaps but persistent iteration that just looks sudden from the outside is a great learning.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note: Written from my own experience, with Claude helping me structure my rambling thoughts into something readable&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45356958</guid><pubDate>Wed, 24 Sep 2025 06:26:05 +0000</pubDate></item><item><title>That Secret Service SIM farm story is bogus</title><link>https://cybersect.substack.com/p/that-secret-service-sim-farm-story</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45357693</guid><pubDate>Wed, 24 Sep 2025 08:24:20 +0000</pubDate></item><item><title>S3 scales to petabytes a second on top of slow HDDs</title><link>https://bigdata.2minutestreaming.com/p/how-aws-s3-scales-with-tens-of-millions-of-hard-drives</link><description>&lt;doc fingerprint="365cf1b6358e8e8d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;how AWS S3 serves 1 petabyte per second on top of slow HDDs&lt;/head&gt;
    &lt;head rend="h3"&gt;Learn how Amazon built the backbone of the modern web that scales to 1 PB/s and 150M QPS on commodity hard drives&lt;/head&gt;
    &lt;p&gt;Everyone knows what AWS S3 is, but few comprehend the massive scale it operates at, nor what it took to get there.&lt;/p&gt;
    &lt;p&gt;In essence - it’s a scalable multi-tenant storage service with APIs to store and retrieve objects, offering extremely high availability1 and durability2 at a relatively low cost3.&lt;/p&gt;
    &lt;head rend="h1"&gt;Scale&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;400+ trillion4 objects&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;150 million requests a second (150,000,000/s)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;&amp;gt; 1 PB/s of peak traffic&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;tens of millions of disks&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Behind It All?&lt;/head&gt;
    &lt;p&gt;Hard drives.&lt;/p&gt;
    &lt;p&gt;How S3 achieves this scale is an engineering marvel. To understand and appreciate the system, we first must appreciate its core building block - the hard drive.&lt;/p&gt;
    &lt;p&gt;Hard Disk Drives (HDDs) are an old, somewhat out-of-favor technology largely superseded by SSDs. They are physically fragile, constrained for IOPS and high in latency.&lt;/p&gt;
    &lt;p&gt;But they nailed something flash still hasn’t: dirt cheap commodity economics:&lt;/p&gt;
    &lt;p&gt;Over their lifetime, HDDs have seen exponential improvement:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;price: 6,000,000,000x cheaper per byte (inflation-adjusted)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;capacity: increased 7,200,000x&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;size: decreased 5,000x&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;weight: decreased 1,235x&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But one issue has consistently persisted - they’re constrained for IOPS. They have been stuck at 120 IOPS for the last 30 years.&lt;lb/&gt;Latency also hasn’t kept up in the same pace as the rest.&lt;/p&gt;
    &lt;p&gt;This means that per byte, HDDs are becoming slower.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why are HDDs slow?&lt;/head&gt;
    &lt;p&gt;HDDs are slow because of physics.&lt;/p&gt;
    &lt;p&gt;They require real-world mechanical movement to read data. (unlike SSDs, which use electricity travelling at ~50% the speed of light). Here is a good visualization:&lt;/p&gt;
    &lt;p&gt;The platter spins around the spindle at about 7200 rounds per minute (RPM)5.&lt;/p&gt;
    &lt;p&gt;The mechanical arm (actuator) with its read/write head physically moves across the platter and waits for it to rotate until it gets to the precise LBA address where the data resides.&lt;/p&gt;
    &lt;p&gt;Accessing data from the disk therefore involves two mechanical operations and one electrical.&lt;/p&gt;
    &lt;p&gt;That physical movements are:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;seek - the act of the actuator moving left or right to the correct track on the platter&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;full-platter seek time: ~8ms&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;half-platter seek time (avg): ~4ms&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;rotation - waiting for the spindle to spin the disk until it matches the precise address on the platter’s track&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;full rotational latency: ~8.3ms6&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;half rotational latency (avg): ~4ms&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And then the electrical one:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;transfer rate - the act of the head shoving bits off the platter across the bus into memory (the drive’s internal cache)&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;reading 0.5MB: ~2.5ms on average7&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Sequential I/O&lt;/head&gt;
    &lt;p&gt;Hard Drives are optimized for sequential access patterns.&lt;/p&gt;
    &lt;p&gt;Reading/writing bytes that are laid out consecutively on the disk is fast. The natural rotation of the platter cycles through the block of bytes and no excessive seeks need to be performed (the actuator stays still).&lt;/p&gt;
    &lt;p&gt;The easiest and most popular data structure with sequential access patterns is the Log. Popular distributed systems like Apache Kafka are built on top of it and through sequential access patterns squeeze out great performance off cheap hardware.&lt;/p&gt;
    &lt;p&gt;It is no surprise that S3’s storage backend - ShardStore - is based on a log-structured merge tree (LSM) itself.&lt;/p&gt;
    &lt;p&gt;In essence, writes for S3 is easy. Because they write sequentially to the disk, they take advantage of the HDD’s performance. (similar to Kafka, I bet they batch pending PUTs so as to squeeze out more sequential throughput on disk via appends to the log)8&lt;/p&gt;
    &lt;p&gt;Reads, however, are trickier. AWS can’t control what files the user requests - so they have to jump around the drive when serving them.&lt;/p&gt;
    &lt;head rend="h3"&gt;Random I/O&lt;/head&gt;
    &lt;p&gt;In the average case, a read on a random part of the drive would involve half of the full physical movement.&lt;/p&gt;
    &lt;p&gt;The average read latency is the sum of both average physical movements plus the transfer rate. Overall, you’re looking at ~11ms on average to read 0.5 MB of random I/O from a drive. That’s very slow.&lt;/p&gt;
    &lt;p&gt;Since a second has 1000 milliseconds, you’d only achieve ~45MB/s of random I/O from a single drive.&lt;/p&gt;
    &lt;p&gt;Because physical movements are a bottleneck - disks have been stuck at this same random I/O latency for the better part of 30 years.&lt;/p&gt;
    &lt;p&gt;They are simply not efficient under random access patterns. That’s when you’d opt for SSDs. But if you have to store massive amounts of data - SSDs become unaffordable.9&lt;/p&gt;
    &lt;p&gt;This becomes a pickle when you are S3 - a random access system10 that also stores massive amounts of data.&lt;/p&gt;
    &lt;p&gt;Yet, S3 found a way to do it - it delivers tolerable latency11 and outstanding12 throughput while working around the physical limitations.&lt;/p&gt;
    &lt;head rend="h1"&gt;Need for Parallelism&lt;/head&gt;
    &lt;p&gt;S3 solves this problem through massive parallelism.&lt;/p&gt;
    &lt;p&gt;They spread the data out in many (many!13) hard drives so they can achieve massive read throughput by utilizing each drive in parallel.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Storing a 1 TB file in a single HDD means limits your reading rate by that single drive’s max throughput (~300 MB/s14).&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Splitting that same 1 TB file across 20,000 different HDDs means you can read it in parallel at the sum of all HDDs’ throughput (TB/s).&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They do this via Erasure Coding.&lt;/p&gt;
    &lt;head rend="h2"&gt;Erasure Coding&lt;/head&gt;
    &lt;p&gt;Redundancy schemes are common practice in storage systems.&lt;/p&gt;
    &lt;p&gt;They are most often associated with data durability - protecting against data loss when hardware fails.&lt;/p&gt;
    &lt;p&gt;S3 uses Erasure Coding (EC). It breaks data into K shards with M redundant “parity” shards. EC allows you to reconstruct the data from any K shards out of the total K+M shards.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The S3 team shares they use a 5-of-9 scheme. They shard each object into 9 pieces - 5 Regular Shards (K) and 4 Parity Shards (M)&lt;/p&gt;
      &lt;p&gt;This approach tolerates up to 4 losses. To access the object, they need 5/9 shards.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This scheme helps S3 find a middle balance - it doesn’t take much extra disk capacity yet still provides flexible I/O.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;EC makes them store 1.8x the original data.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;A naive alternative like 3-way replication would result in 3x the data. That extra 1.2x starts to matter when we’re talking hundreds of exabytes.&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;EC gives them 9 possible read sources - an ample hedge against bottlenecks&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;3-way replication would only give them 3 sources. If all 3 nodes are hot, performance would suffer.&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;9 read sources also offer much more burst demand I/O due to parallelism&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;An under-appreciated aspect of EC is precisely its ability to distribute load. Such schemes spread the hot spots of a system out and give it the flexibility to steer read traffic in a balanced way. And since shards are small, firing off hedge requests15 to dodge stragglers is far cheaper than with full replicas.&lt;/p&gt;
    &lt;head rend="h2"&gt;Parallelism in Action&lt;/head&gt;
    &lt;p&gt;S3 leverages parallelism in three main ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;From the user’s perspective - upload/download the file in chunks.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;From the client’s perspective - send requests to multiple different front-end servers.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;From the server’s perspective - store an object in multiple storage servers.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Any part of the end-to-end path can become a bottleneck, so it’s important to optimize everything.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Across Front-end Servers&lt;/head&gt;
    &lt;p&gt;Instead of requesting all the files through one connection to one S3 endpoint, users are encouraged to open as many connections as necessary. This happens behind the scenes in the library code through an internal HTTP connection pool.&lt;/p&gt;
    &lt;p&gt;This approach utilizes many different endpoints of the distributed system, ensuring no single point in the infrastructure becomes too hot (e.g. front-end proxies, caches, etc)&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Across Hard Drives&lt;/head&gt;
    &lt;p&gt;Instead of storing the data in a single hard-drive, the system breaks it into shards via EC and spreads it out across multiple storage back ends.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Across PUT/GET Operations&lt;/head&gt;
    &lt;p&gt;Instead of sending one request through a single thread and HTTP connection, the client chunks it into 10 parts and uploads each in parallel.16&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;PUT requests support multipart upload, which AWS recommends in order to maximize throughput by leveraging multiple threads.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;GET requests similarly support an HTTP header denoting you read only a particular range of the object (called byte-ranged GET). AWS again recommends this for achieving higher aggregate throughput instead of the single object read request.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Uploading 1 GB/s to a single server may be difficult, but uploading 100 chunks each at 10 MB/s chunks to 100 different servers is very practical.&lt;/p&gt;
    &lt;p&gt;This simple idea goes a long way.&lt;/p&gt;
    &lt;head rend="h1"&gt;Avoiding Hot Spots&lt;/head&gt;
    &lt;p&gt;S3 now finds itself with a difficult problem. They have tens of millions of drives, hundreds of millions of parallel requests per second and hundreds of millions of EC shards to persist per second.&lt;/p&gt;
    &lt;p&gt;How do they spread this load around effectively so as to avoid certain nodes/disks overheating?&lt;/p&gt;
    &lt;p&gt;As we said earlier - a single disk can do around ~45 MB/s of random IOs. It seems trivial to hit that bottleneck. Not to mention any additional system maintenance work like rebalancing data around for more efficient spreading would also take valuable IOs off the disk.&lt;/p&gt;
    &lt;p&gt;Forming hot spots in a distributed system is dangerous, because it can easily cause a domino-like spiral into system-wide degradation17.&lt;/p&gt;
    &lt;p&gt;Needless to say, S3 is very careful in trying to spread data around. Their solution is again deceptively simple:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;randomize where you place data on ingestion&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;continuously rebalance it&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;scale &amp;amp; chill&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Shuffle Sharding &amp;amp; Power of Two&lt;/head&gt;
    &lt;p&gt;Where you place data initially is key to performance. Moving it later is more expensive.&lt;/p&gt;
    &lt;p&gt;Unfortunately, at write time you have no good way of knowing whether the data you’re about to persist is going to be accessed frequently or not.&lt;/p&gt;
    &lt;p&gt;Knowing the perfect the least-loaded HDD to place new data in is also impossible at this scale. You can’t keep a synchronous globally-consistent view when you are serving hundreds of millions of requests per second across tens of millions of drives. This approach would also risk load correlation - placing similar workloads together and having them burst together at once.&lt;/p&gt;
    &lt;p&gt;A key realization is that picking at random works better in this scenario. 💡&lt;/p&gt;
    &lt;p&gt;It’s how AWS intentionally engineers decorrelation into their system:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;A given PUT picks a random set of drives&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The next PUT, even if it’s targetting the same key/bucket, picks a different set of near-random drives.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The way they do it is through the so-called Power of Two Random Choices:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Power of Two Random Choices: a well-studied phenomenon in load balancing that says choosing between the least-loaded of two completely random nodes yields much better results than choosing just one node at random.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Rebalancing&lt;/head&gt;
    &lt;p&gt;Another key realization is that newer data chunks are hotter than older ones. 💡&lt;/p&gt;
    &lt;p&gt;Fresh data is accessed more frequently. As it grows older, it gets accessed less.&lt;lb/&gt;All hard drives therefore eventually cool off in usage as they get filled with data and said data ages. The result is full storage capacity with ample I/O capacity.&lt;/p&gt;
    &lt;p&gt;AWS has to proactively rebalance the cold data out (so as to free up space) and rebalance cold data in (so as to make use of the free I/O).&lt;/p&gt;
    &lt;p&gt;Data rebalances are also needed when new racks of disks are added to S3. Each rack contains 20 PB of capacity18, and every disk in there is completely empty. The system needs to proactively spread the load around the new capacity.&lt;/p&gt;
    &lt;p&gt;Suffice to say - S3 constantly rebalances data around.&lt;/p&gt;
    &lt;head rend="h2"&gt;Chill@Scale&lt;/head&gt;
    &lt;p&gt;The last realization is perhaps the least intuitive: the larger the system becomes, the more predictable it is. 💡&lt;/p&gt;
    &lt;p&gt;AWS experienced so-called workload decorrelation as S3 grew. That is the phenomenon of seeing a smoothening of load once it’s aggregated on a large enough scale. While their peak demand is growing in size, their peak-to-mean delta is collapsing.&lt;/p&gt;
    &lt;p&gt;This is because storage workloads are inherently very bursty - they demand a lot at once, and then may remain idle for a long time (months).&lt;/p&gt;
    &lt;p&gt;Because independent workloads do not burst together, the more workloads you cram together - the more those idle spots get filled up and the more predictable the system becomes in aggregate. 💡&lt;/p&gt;
    &lt;p&gt;copyright: AWS; from this re:Invent presentation.&lt;/p&gt;
    &lt;head rend="h1"&gt;Summary&lt;/head&gt;
    &lt;p&gt;AWS S3 is a massively multi-tenant storage service. It’s a gigantic distributed system consisting of many individually slow nodes that on aggregate allow you to access data faster than any single node can provide. S3 achieves this through:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;massive parallelization across the end-to-end path (user, client, server)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;neat load-balancing tricks like the power of two random&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;spreading out data via erasure coding&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;lowering tail latency via hedge requests&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;the economies of multi-tenancy at world scale&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It started as a service optimized for backups, video and image storage for e-commerce websites - but eventually grew support being the main storage system used for analytics and machine learning on massive data lakes.&lt;/p&gt;
    &lt;p&gt;Nowadays, the growing trend is for entire data infrastructure projects to be based on top of S3. This gives them the benefits of stateless nodes (easy scaling, less management) while outsourcing difficult durability, replication and load-balancing problems to S3. And get this - it also reduces cloud costs.19&lt;/p&gt;
    &lt;p&gt;Subscribe for more interesting dives in big data distributed systems (or the occassional small data gem).&lt;/p&gt;
    &lt;head rend="h1"&gt;References&lt;/head&gt;
    &lt;p&gt;S3 has a lot of other goodies up its bag, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;shuffle sharding at the DNS level&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;client library hedging requests by cancelling slow requests that pass the p95 threshold and sending new ones to a different host&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;software updates done erasure-coding-style, including rolling out their brand-new ShardStore storage system without any impact to their fleet&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;conway’s law and how it shapes S3’s architecture (consisting of 300+ microservices)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;their durability culture, including continuous detection, durable chain of custody, a design process that includes durability threat modelling and formal verification&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These are generally shared in their annual S3 Deep Dive at re:Invent:&lt;/p&gt;
    &lt;p&gt;Thank you to the S3 team for sharing what they’ve built, and thank you for reading!&lt;/p&gt;
    &lt;head rend="h1"&gt;Side Quest Reads 👇&lt;/head&gt;
    &lt;p&gt;S3 has never been down for more than 5 hours in its entire existence. And that incident was 8 years ago, in just one region (out of 38) in AWS. It was considered one of AWS' most impactful outages of all time.&lt;/p&gt;
    &lt;p&gt;S3 markets itself as being designed for 11 nines of durability. Careful with the wording - they don’t legally promise 99.999999999% durability. In fact, Amazon does not legally provide any SLA for durability.&lt;/p&gt;
    &lt;p&gt;By relative low cost, I mean relative to the other storage you can buy on AWS. S3 is still ~$21.5-$23.5 per TB of storage. In fact, S3 hasn’t lowered its prices in 8 years despire HDD prices falling 60% since. When I ran back-of-the-napkin maths for what it’d cost for me to build my own S3 bare metal, the cost came out to $0.875 per TB of storage (25x cheaper). Alternatively, hosting it on Hetzner would be around $5.73 per TB.&lt;/p&gt;
    &lt;p&gt;400,000,000,000,000&lt;/p&gt;
    &lt;p&gt;The first 7200 rpm drive was the Seagate Barracuda released in 1992. Today, rpm has largely remained unchanged. Larger 15k-ish rpm drives exist, but aren’t super common.&lt;/p&gt;
    &lt;p&gt;7200 rotations per minute == 7200 rotations per 60000 milliseconds == 8.33ms per rotation&lt;/p&gt;
    &lt;p&gt;HDDs on average have ~170-200 MB/s transfer rate; 200mb / 1000ms == 0.2 mb/ms; 0.5 mb == ~2.5ms; They’re simply not optimized for random access like this.&lt;/p&gt;
    &lt;p&gt;Kafka loves batching entries. It batches on the client (by waiting), it batches in the protocol (by merging entries) and it batches on the server (by storing in page cache and utilizing the OS’ async flush). It’s such an obvious perf gain that S3 must do something similar in their back-end and storage system.&lt;/p&gt;
    &lt;p&gt;although this is slowly but surely beginning to change for certain data thresholds. SSDs have massively deflated in price in just the last 15 years.&lt;/p&gt;
    &lt;p&gt;In aggregate, S3 exhibits random access. You as a tenant can PUT/GET any blobs of any size. An average S3 disk would therefore consist of blobs from thousands of tenants. If they all attempt to access their data simultaneously, the drive simply cannot serve every request at once.&lt;/p&gt;
    &lt;p&gt;Not a lot of benchmark actually exist here. Testing 0.5MB files, I got writes at ~140ms p99 and 26ms p50, reads at 86ms p99. Larger files allegedly get larger p99, and they vary throughout the week.&lt;/p&gt;
    &lt;p&gt;At least one public number is Anthropic driving tens of terabytes per second. There are likely much larger single customer workloads out there - S3 does more than a petabyte a second!&lt;/p&gt;
    &lt;p&gt;AWS shares that tens of thousands of their customers have their data spread over 1,000,000 disks. This is a great example of how multi-tenancy at scale can convert the financially impossible into the affordable. It would be prohibitively expensive for any single tenant to deploy a million HDDs themselves, but when shared behind a multi-tenant system - it becomes surprisingly cheap.&lt;/p&gt;
    &lt;p&gt;e.g a modern cheap 20TB HDD maxes out at around 291 MB/s of data transfer: https://www.westerndigital.com/products/internal-drives/wd-gold-sata-hdd?sku=WD203KRYZ; note this is marketing numbers too&lt;/p&gt;
    &lt;p&gt;The concept of a hedge request was popularized by this Google paper “The Tail at Scale”. It essentially talks about how fanout requests (where a root request results in many sub-requests, e.g like S3’s GETs requesting multiple shards) can significantly reduce their tail latency by speculatively sending extra requests (i.e if you need 5 sub-requests to build an object - send 6). This extra request is sent only once one of the sub-requests surpasses the usual p95 latency. S3’s client libraries also utilize this concept.&lt;/p&gt;
    &lt;p&gt;An interesting detail is that each part of the multi-part upload must be getting Erasure Coded 5-of-9 too. So a single object uploaded through multipart upload can consist of hundreds of shards.&lt;/p&gt;
    &lt;p&gt;If too many requests hit the same disk at the same point in time, the disk starts to stall because its limited I/O is exhausted. This accumulates tail latency to requests that depend on the drive. This delay impacts other operations like writes. It also gets amplified up the stack in other components beyond the drive. If left unchecked, it can cause a cascade that significantly slows down the whole system.&lt;/p&gt;
    &lt;p&gt;As someone with no data center experience, I find it super cool when Amazon shares pictures of what the physical disks look like. Here is an example of one such rack of disks. It consists of 1000 drives - 20TB each. It’s said this rack weighs more than a car, and Amazon had to reinforce the flooring in their data centers to support it.&lt;/p&gt;
    &lt;p&gt;Apache Kafka (what I am most familiar with) has been seeing the so-called “Diskless” trend where the write path uses S3 instead of local disks. This trades off higher latency for lower costs (by 90% [!]). Similar projects exist - Turbopuffer (Vector DB built on S3), SlateDB (embedded LSM on S3), Nixiesearch (Lucene on S3). In general, every data infra project seems to be offloading as much as possible to object storage. (Clickhouse, OpenSearch, Elastic). Before Diskless, Kafka similarly used a two-tier approach where cold data was offloaded to S3 (for a 10x storage cost saving)&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45358280</guid><pubDate>Wed, 24 Sep 2025 10:05:29 +0000</pubDate></item><item><title>My game's server is blocked in Spain whenever there's a football match on</title><link>https://old.reddit.com/r/gamedev/comments/1np6kyn/my_games_server_is_blocked_in_spain_whenever/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45358433</guid><pubDate>Wed, 24 Sep 2025 10:26:23 +0000</pubDate></item><item><title>Huntington's disease treated for first time</title><link>https://www.bbc.com/news/articles/cevz13xkxpro</link><description>&lt;doc fingerprint="2e099265b8279da2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Huntington's disease successfully treated for first time&lt;/head&gt;
    &lt;p&gt;One of the cruellest and most devastating diseases – Huntington's – has been successfully treated for the first time, say doctors.&lt;/p&gt;
    &lt;p&gt;The disease runs through families, relentlessly kills brain cells and resembles a combination of dementia, Parkinson's and motor neurone disease.&lt;/p&gt;
    &lt;p&gt;An emotional research team became tearful as they described how data shows the disease was slowed by 75% in patients.&lt;/p&gt;
    &lt;p&gt;It means the decline you would normally expect in one year would take four years after treatment, giving patients decades of "good quality life", Prof Sarah Tabrizi told BBC News.&lt;/p&gt;
    &lt;p&gt;The new treatment is a type of gene therapy given during 12 to 18 hours of delicate brain surgery.&lt;/p&gt;
    &lt;p&gt;The first symptoms of Huntington's disease tend to appear in your 30s or 40s and is normally fatal within two decades – opening the possibility that earlier treatment could prevent symptoms from ever emerging.&lt;/p&gt;
    &lt;p&gt;Prof Tabrizi, director of the University College London Huntington's Disease Centre, described the results as "spectacular".&lt;/p&gt;
    &lt;p&gt;"We never in our wildest dreams would have expected a 75% slowing of clinical progression," she said.&lt;/p&gt;
    &lt;p&gt;None of the patients who have been treated are being identified, but one was medically retired and has returned to work. Others in the trial are still walking despite being expected to need a wheelchair.&lt;/p&gt;
    &lt;p&gt;Treatment is likely to be very expensive. However, this is a moment of real hope in a disease that hits people in their prime and devastates families.&lt;/p&gt;
    &lt;p&gt;Huntington's runs through Jack May-Davis' family. He has the faulty gene that causes the disease, as did his dad, Fred, and his grandmother, Joyce.&lt;/p&gt;
    &lt;p&gt;Jack said it was "really awful and horrible" watching his dad's inexorable decline.&lt;/p&gt;
    &lt;p&gt;The first symptoms appeared in Fred's late 30s, including changes in behaviour and the way he moved. He eventually needed 24/7 palliative care before he died at the age of 54, in 2016.&lt;/p&gt;
    &lt;p&gt;Jack is 30, a barrister's clerk, newly engaged to Chloe and has taken part in research at UCL to turn his diagnosis into a positive.&lt;/p&gt;
    &lt;p&gt;But he'd always known he was destined to share his father's fate, until today.&lt;/p&gt;
    &lt;p&gt;Now he says the "absolutely incredible" breakthrough has left him "overwhelmed" and able to look to a future that "seems a little bit brighter, it does allow me to think my life could be that much longer".&lt;/p&gt;
    &lt;p&gt;Huntington's disease is caused by an error in part of our DNA called the huntingtin gene.&lt;/p&gt;
    &lt;p&gt;If one of your parents has Huntington's disease, there's a 50% chance that you will inherit the altered gene and will eventually develop Huntington's too.&lt;/p&gt;
    &lt;p&gt;This mutation turns a normal protein needed in the brain – called the huntingtin protein – into a killer of neurons.&lt;/p&gt;
    &lt;p&gt;The goal of the treatment is to reduce levels of this toxic protein permanently, in a single dose.&lt;/p&gt;
    &lt;p&gt;The therapy uses cutting edge genetic medicine combining gene therapy and gene silencing technologies.&lt;/p&gt;
    &lt;p&gt;It starts with a safe virus that has been altered to contain a specially designed sequence of DNA.&lt;/p&gt;
    &lt;p&gt;This is infused deep into the brain using real-time MRI scanning to guide a microcatheter to two brain regions - the caudate nucleus and the putamen. This takes 12 to 18 hours of neurosurgery.&lt;/p&gt;
    &lt;p&gt;The virus then acts like a microscopic postman – delivering the new piece of DNA inside brain cells, where it becomes active.&lt;/p&gt;
    &lt;p&gt;This turns the neurons into a factory for making the therapy to avert their own death.&lt;/p&gt;
    &lt;p&gt;The cells produce a small fragment of genetic material (called microRNA) that is designed to intercept and disable the instructions (called messenger RNA) being sent from the cells' DNA for building mutant huntingtin.&lt;/p&gt;
    &lt;p&gt;This results in lower levels of mutant huntingtin in the brain.&lt;/p&gt;
    &lt;p&gt;Results from the trial - which involved 29 patients - have been released in a statement by the company uniQure, but have not yet been published in full for review by other specialists.&lt;/p&gt;
    &lt;p&gt;The data showed that three years after surgery there was an average 75% slowing of the disease based on a measure which combines cognition, motor function and the ability to manage in daily life.&lt;/p&gt;
    &lt;p&gt;The data also shows the treatment is saving brain cells. Levels of neurofilaments in spinal fluid – a clear sign of brain cells dying – should have increased by a third if the disease continued to progress, but was actually lower than at the start of the trial.&lt;/p&gt;
    &lt;p&gt;"This is the result we've been waiting for," said Prof Ed Wild, consultant neurologist at the National Hospital for Neurology and Neurosurgery at UCLH.&lt;/p&gt;
    &lt;p&gt;"There was every chance that we would never see a result like this, so to be living in a world where we know this is not only possible, but the actual magnitude of the effect is breathtaking, it's very difficult to fully encapsulate the emotion."&lt;/p&gt;
    &lt;p&gt;He said he was "a bit teary" thinking about the impact it could have on families.&lt;/p&gt;
    &lt;p&gt;The treatment was considered safe, although some patients did develop inflammation from the virus that caused headaches and confusion that either resolved or needed steroid treatment.&lt;/p&gt;
    &lt;p&gt;Prof Wild anticipates the therapy "should last for life" because brain cells are not replaced by the body in the same manner as blood, bone and skin are constantly renewed.&lt;/p&gt;
    &lt;p&gt;Approximately 75,000 people have Huntington's disease in the UK, US and Europe with hundreds of thousands carrying the mutation meaning they will develop the disease.&lt;/p&gt;
    &lt;p&gt;UniQure says it will apply for a licence in the US in the first quarter of 2026 with the aim of launching the drug later that year. Conversations with authorities in the UK and Europe will start next year, but the initial focus is on the US.&lt;/p&gt;
    &lt;p&gt;Dr Walid Abi-Saab, the chief medical officer at uniQure, said he was "incredibly excited" about what the results mean for families, and added that the treatment had "the potential to fundamentally transform" Huntington's disease.&lt;/p&gt;
    &lt;p&gt;However, the drug will not be available for everyone due to the highly complex surgery and the anticipated cost.&lt;/p&gt;
    &lt;p&gt;"It will be expensive for sure," says Prof Wild.&lt;/p&gt;
    &lt;p&gt;There isn't an official price for the drug. Gene therapies are often pricey, but their long-term impact means that can still be affordable. In the UK, the NHS does pay for a £2.6m-per-patient gene therapy for haemophilia B.&lt;/p&gt;
    &lt;p&gt;Prof Tabrizi says this gene therapy "is the beginning" and will open the gates for therapies that can reach more people.&lt;/p&gt;
    &lt;p&gt;She paid tribute to the "truly brave" volunteers who took part in the trial, saying she was "overjoyed for the patients and families".&lt;/p&gt;
    &lt;p&gt;She is already working with a group of young people who know they have the gene, but don't yet have symptoms – known as stage zero Huntington's – and is aiming to do the first prevention trial to see if the disease can be significantly delayed or even stopped completely.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45358940</guid><pubDate>Wed, 24 Sep 2025 11:37:30 +0000</pubDate></item></channel></rss>