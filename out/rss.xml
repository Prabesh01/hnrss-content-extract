<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 19 Jan 2026 07:46:27 +0000</lastBuildDate><item><title>Simulating the Ladybug Clock Puzzle</title><link>https://austinhenley.com/blog/ladybugclock.html</link><description>&lt;doc fingerprint="c6bca0b44d0196bf"&gt;
  &lt;main&gt;
    &lt;p&gt; Associate Teaching Professor&lt;lb/&gt; Carnegie Mellon University &lt;/p&gt;
    &lt;p&gt;A few days ago, 3Blue1Brown posted a 60-second video describing a puzzle...&lt;/p&gt;
    &lt;p&gt;Imagine that a ladybug lands on the 12 o'clock marker of a clock. It then proceeds to move either clockwise or counterclockwise to the adjacent hour marker, one at a time, and repeats until all hour markers have been visited at least once.&lt;/p&gt;
    &lt;p&gt;What is the probability that it ends on the 6?&lt;/p&gt;
    &lt;p&gt;These sort of puzzles always intrigue me. They're simple to describe and at first might even look easy to solve, but as I dig into them, my intuition leads me astray.&lt;/p&gt;
    &lt;p&gt;What a fun Saturday morning project! I whipped up a simulator to try it out. It works like this:&lt;/p&gt;
    &lt;quote&gt;let position = 0; // 0-11 hour markers let visited = new Set([0]); while (visited.size &amp;lt; 12) { const direction = Math.random() &amp;lt; 0.5 ? -1 : 1; position = (position + direction + 12) % 12; visited.add(position); } return position;&lt;/quote&gt;
    &lt;p&gt;See, it is simple. But before running the simulator, can you guess the probability of it ending on 6? What about 11 or 1? 3? The other numbers?&lt;/p&gt;
    &lt;p&gt;It stumped me. My guess was that 6 would be the most likely—it is the farthest away but it is also necessary to visit all other numbers first. The numbers closer to 12 would be gradually less likely. Am I right? It might remind you of other random walk problems.&lt;/p&gt;
    &lt;p&gt;So what is the answer? Well, I first ran the simulator 100 times and the results looked random. More runs! After ~1500 runs, all of the numbers were showing 8-10% likelihood with no discernable pattern. That isn't what I expected. After 5000 runs, they were all 8.4-9.7%. And then after 10,000 runs...&lt;/p&gt;
    &lt;p&gt;Based on the simulator, all numbers are equally likely with 9% probability, excluding 12 of course, since that is visited first and thus can't be last. The answer is 1⁄11.&lt;/p&gt;
    &lt;p&gt;An even more fun question? What is the average number of moves that the ladybug will make to visit all 12 numbers? I'd love to hear someone's explanation for that answer!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46661644</guid><pubDate>Sat, 17 Jan 2026 20:19:46 +0000</pubDate></item><item><title>A Social Filesystem</title><link>https://overreacted.io/a-social-filesystem/</link><description>&lt;doc fingerprint="37359953aaf486d5"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A Social Filesystem&lt;/head&gt;
    &lt;p&gt;January 18, 2026&lt;/p&gt;
    &lt;p&gt;Remember files?&lt;/p&gt;
    &lt;p&gt;You write a document, hit save, and the file is on your computer. It’s yours. You can inspect it, you can send it to a friend, and you can open it with other apps.&lt;/p&gt;
    &lt;p&gt;Files come from the paradigm of personal computing.&lt;/p&gt;
    &lt;p&gt;This post, however, isn’t about personal computing. What I want to talk about is social computing—apps like Instagram, Reddit, Tumblr, GitHub, and TikTok.&lt;/p&gt;
    &lt;p&gt;What do files have to do with social computing?&lt;/p&gt;
    &lt;p&gt;Historically, not a lot—until recently.&lt;/p&gt;
    &lt;p&gt;But first, a shoutout to files.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Files Are Awesome&lt;/head&gt;
    &lt;p&gt;Files, as originally invented, were not meant to live inside the apps.&lt;/p&gt;
    &lt;p&gt;Since files represent your creations, they should live somewhere that you control. Apps create and read your files on your behalf, but files don’t belong to the apps.&lt;/p&gt;
    &lt;p&gt;Files belong to you—the person using those apps.&lt;/p&gt;
    &lt;p&gt;Apps (and their developers) may not own your files, but they do need to be able to read and write them. To do that reliably, apps need your files to be structured. This is why app developers, as part of creating apps, may invent and evolve file formats.&lt;/p&gt;
    &lt;p&gt;A file format is like a language. An app might “speak” several formats. A single format can be understood by many apps. Apps and formats are many-to-many. File formats let different apps work together without knowing about each other.&lt;/p&gt;
    &lt;p&gt;Consider this &lt;code&gt;.svg&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;SVG is an open specification. This means that different developers agree on how to read and write SVG. I created this SVG file in Excalidraw, but I could have used Adobe Illustrator or Inkscape instead. Your browser already knew how to display this SVG. It didn’t need to hit any Excalidraw APIs or to ask permissions from Excalidraw to display this SVG. It doesn’t matter which app has created this SVG.&lt;/p&gt;
    &lt;p&gt;The file format is the API.&lt;/p&gt;
    &lt;p&gt;Of course, not all file formats are open or documented.&lt;/p&gt;
    &lt;p&gt;Some file formats are application-specific or even proprietary like &lt;code&gt;.doc&lt;/code&gt;. And yet, although &lt;code&gt;.doc&lt;/code&gt; was undocumented, it didn’t stop motivated developers from reverse-engineering it and creating more software that reads and writes &lt;code&gt;.doc&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Another win for the files paradigm.&lt;/p&gt;
    &lt;p&gt;The files paradigm captures a real-world intuition about tools: what we make with a tool does not belong to the tool. A manuscript doesn’t stay inside the typewriter, a photo doesn’t stay inside the camera, and a song doesn’t stay in the microphone.&lt;/p&gt;
    &lt;p&gt;Our memories, our thoughts, our designs should outlive the software we used to create them. An app-agnostic storage (the filesystem) enforces this separation.&lt;/p&gt;
    &lt;p&gt;A file has many lives.&lt;/p&gt;
    &lt;p&gt;You may create a file in one app, but someone else can read it using another app. You may switch the apps you use, or use them together. You may convert a file from one format to another. As long as two apps correctly “speak” the same file format, they can work in tandem even if their developers hate each others’ guts.&lt;/p&gt;
    &lt;p&gt;And if the app sucks?&lt;/p&gt;
    &lt;p&gt;Someone could always create “the next app” for the files you already have:&lt;/p&gt;
    &lt;p&gt;Apps may come and go, but files stay—at least, as long as our apps think in files.&lt;/p&gt;
    &lt;p&gt;See also: File over app&lt;/p&gt;
    &lt;head rend="h2"&gt;The Everything Folder&lt;/head&gt;
    &lt;p&gt;When you think of social apps—Instagram, Reddit, Tumblr, GitHub, TikTok—you probably don’t think about files. Files are for personal computing only, right?&lt;/p&gt;
    &lt;p&gt;A Tumblr post isn’t a file.&lt;/p&gt;
    &lt;p&gt;An Instagram follow isn’t a file.&lt;/p&gt;
    &lt;p&gt;A Hacker News upvote isn’t a file.&lt;/p&gt;
    &lt;p&gt;But what if they behaved as files—at least, in all the important ways? Suppose you had a folder that contained all of the things ever &lt;code&gt;POST&lt;/code&gt;ed by your online persona:&lt;/p&gt;
    &lt;p&gt;It would include everything you’ve created across different social apps—your posts, likes, scrobbles, recipes, etc. Maybe we can call it your “everything folder”.&lt;/p&gt;
    &lt;p&gt;Of course, closed apps like Instagram aren’t built this way. But imagine they were. In that world, a “Tumblr post” or an “Instagram follow” are social file formats:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;You posting on Tumblr would create a “Tumblr post” file in your folder.&lt;/item&gt;
      &lt;item&gt;You following on Instagram would put an “Instagram follow” file into your folder.&lt;/item&gt;
      &lt;item&gt;You upvoting on Hacker News would add an “HN upvote” file to your folder.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note this folder is not some kind of an archive. It’s where your data actually lives:&lt;/p&gt;
    &lt;p&gt;Files are the source of truth—the apps would reflect whatever’s in your folder.&lt;/p&gt;
    &lt;p&gt;Any writes to your folder would be synced to the interested apps. For example, deleting an “Instagram follow” file would work just as well as unfollowing through the app. Crossposting to three Tumblr communities could be done by creating three “Tumblr post” files. Under the hood, each app manages files in your folder.&lt;/p&gt;
    &lt;p&gt;In this paradigm, apps are reactive to files. Every app’s database mostly becomes derived data—an app-specific cached materialized view of everybody’s folders.&lt;/p&gt;
    &lt;head rend="h2"&gt;A Social Filesystem&lt;/head&gt;
    &lt;p&gt;This might sound very hypothetical, but it’s not. What I’ve described so far is the premise behind the AT protocol. It works in production at scale. Bluesky, Leaflet, Tangled, Semble, and Wisp are some of the new open social apps built this way.&lt;/p&gt;
    &lt;p&gt;It doesn’t feel different to use those apps. But by lifting user data out of the apps, we force the same separation as we’ve had in personal computing: apps don’t trap what you make with them. Someone can always make a new app for old data:&lt;/p&gt;
    &lt;p&gt;Like before, app developers evolve their file formats. However, they can’t gatekeep who reads and writes files in those formats. Which apps to use is up to you.&lt;/p&gt;
    &lt;p&gt;Together, everyone’s folders form something like a distributed social filesystem:&lt;/p&gt;
    &lt;p&gt;I’ve previously written about the AT protocol in Open Social, looking at its model from a web-centric perspective. But I think that looking at it from the filesystem perspective is just as intriguing, so I invite you to take a tour of how it works.&lt;/p&gt;
    &lt;p&gt;A personal filesystem starts with a file.&lt;/p&gt;
    &lt;p&gt;What does a social filesystem start with?&lt;/p&gt;
    &lt;head rend="h3"&gt;A Record&lt;/head&gt;
    &lt;p&gt;Here is a typical social media post:&lt;/p&gt;
    &lt;p&gt;How would you represent it as a file?&lt;/p&gt;
    &lt;p&gt;It’s natural to consider JSON as a format. After all, that’s what you’d return if you were building an API. So let’s fully describe this post as a piece of JSON:&lt;/p&gt;
    &lt;p&gt;However, if we want to store this post as a file, it doesn’t make sense to embed the author information there. After all, if the author later changes their display name or avatar, we wouldn’t want to go through their every post and change them there.&lt;/p&gt;
    &lt;p&gt;So let’s assume their avatar and name live somewhere else—perhaps, in another file. We could leave &lt;code&gt;author: 'dril'&lt;/code&gt; in the JSON but this is unnecessary too. Since this file lives inside the creator’s folder—it’s their post, after all—we can always figure out the author based on whose folder we’re currently looking at.&lt;/p&gt;
    &lt;p&gt;Let’s remove the &lt;code&gt;author&lt;/code&gt; field completely:&lt;/p&gt;
    &lt;p&gt;This seems like a good way to describe this post:&lt;/p&gt;
    &lt;p&gt;But wait, no, this is still wrong.&lt;/p&gt;
    &lt;p&gt;You see, &lt;code&gt;replyCount&lt;/code&gt;, &lt;code&gt;repostCount&lt;/code&gt;, and &lt;code&gt;likeCount&lt;/code&gt; are not really something that the post’s author has created. These values are derived from the data created by other people—their replies, their reposts, their likes. The app that displays this post will have to keep track of those somehow, but they aren’t this user’s data.&lt;/p&gt;
    &lt;p&gt;So really, we’re left with just this:&lt;/p&gt;
    &lt;p&gt;That’s our post as a file!&lt;/p&gt;
    &lt;p&gt;Notice how it took some trimming to identify which parts of the data actually belong in this file. This is something that you have to be intentional about when creating apps with the AT protocol. My mental model for this is to think about the &lt;code&gt;POST&lt;/code&gt; request. When the user created this thing, what data did they send? That’s likely close to what we’ll want to store. That’s the stuff the user has just created.&lt;/p&gt;
    &lt;p&gt;Our social filesystem will be structured more rigidly than a traditional filesystem. For example, it will only consist of JSON files. To make this more explicit, we’ll start introducing our new terminology. We’ll call this kind of file a record.&lt;/p&gt;
    &lt;head rend="h3"&gt;Record Keys&lt;/head&gt;
    &lt;p&gt;Now we need to give our record a name. There are no natural names for posts. Could we use sequential numbers? Our names need only be unique within a folder:&lt;/p&gt;
    &lt;p&gt;One downside is that we’d have to keep track of the latest one so there’s a risk of collisions when creating many files from different devices at the same time.&lt;/p&gt;
    &lt;p&gt;Instead, let’s use timestamps with some per-clock randomness mixed in:&lt;/p&gt;
    &lt;p&gt;This is nicer because these can be generated locally and will almost never collide.&lt;/p&gt;
    &lt;p&gt;We’ll use these names in URLs so let’s encode them more compactly. We’ll pick our encoding carefully so that sorting alphabetically goes in the chronological order:&lt;/p&gt;
    &lt;p&gt;Now &lt;code&gt;ls -r&lt;/code&gt; gives us a reverse chronological timeline of posts! That’s neat. Also, since we’re sticking with JSON as our lingua franca, we don’t need file extensions.&lt;/p&gt;
    &lt;p&gt;Not all records accumulate over time. For example, you can write many posts, but you only have one copy of profile information—your avatar and display name. For “singleton” records, it makes sense to use a predefined name, like &lt;code&gt;me&lt;/code&gt; or &lt;code&gt;self&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;By the way, let’s save this profile record to &lt;code&gt;profiles/self&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;Note how, taken together, &lt;code&gt;posts/34qye3wows2c5&lt;/code&gt; and &lt;code&gt;profiles/self&lt;/code&gt; let us reconstruct more of the UI we started with, although some parts are still missing:&lt;/p&gt;
    &lt;p&gt;Before we fill them in, though, we need to make our system sturdier.&lt;/p&gt;
    &lt;head rend="h3"&gt;Lexicons&lt;/head&gt;
    &lt;p&gt;This was the shape of our post record:&lt;/p&gt;
    &lt;p&gt;And this was the shape of our profile record:&lt;/p&gt;
    &lt;p&gt;Since these are stored as files, it’s important for the format not to drift.&lt;/p&gt;
    &lt;p&gt;Let’s write some type definitions:&lt;/p&gt;
    &lt;p&gt;TypeScript seems convenient for this but it isn’t sufficient. For example, we can’t express constraints like “the &lt;code&gt;text&lt;/code&gt; string should have at most 300 Unicode graphemes”, or “the &lt;code&gt;createdAt&lt;/code&gt; string should be formatted as datetime”.&lt;/p&gt;
    &lt;p&gt;We need a richer way to define social file formats.&lt;/p&gt;
    &lt;p&gt;We might shop around for existing options (RDF? JSON Schema?) but if nothing quite fits, we might as well design our own schema language explicitly geared towards the needs of our social filesystem. This is what our &lt;code&gt;Post&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;p&gt;We’ll call this the Post lexicon because it’s like a language our app wants to speak.&lt;/p&gt;
    &lt;p&gt;My first reaction was also “ouch” but it helped to think that conceptually it’s this:&lt;/p&gt;
    &lt;p&gt;I used to yearn for a better syntax but I’ve actually come around to hesitantly appreciate the JSON. It being trivial to parse makes it super easy to build tooling around it (more on that in the end). And of course, we can make bindings turning these into type definitions and validation code for any programming language.&lt;/p&gt;
    &lt;head rend="h3"&gt;Collections&lt;/head&gt;
    &lt;p&gt;Our social filesystem looks like this so far:&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;posts/&lt;/code&gt; folder has records that satisfy the Post lexicon, and the &lt;code&gt;profiles/&lt;/code&gt; folder contains records (a single record, really) that satisfy the Profile lexicon.&lt;/p&gt;
    &lt;p&gt;This can be made to work well for a single app. But here’s a problem. What if there’s another app with its own notion of “posts” and “profiles”?&lt;/p&gt;
    &lt;p&gt;Recall, each user has an “everything folder” with data from every app:&lt;/p&gt;
    &lt;p&gt;Different apps will likely disagree on what the format of a “post” is! For example, a microblog post might have a 300 character limit, but a proper blog post might not.&lt;/p&gt;
    &lt;p&gt;Can we get the apps to agree with each other?&lt;/p&gt;
    &lt;p&gt;We could try to put every app developer in the same room until they all agree on a perfect lexicon for a post. That would be an interesting use of everyone’s time.&lt;/p&gt;
    &lt;p&gt;For some use cases, like cross-site syndication, a standard-ish jointly governed lexicon makes sense. For other cases, you really want the app to be in charge. It’s actually good that different products can disagree about what a post is! Different products, different vibes. We’d want to support that, not to fight it.&lt;/p&gt;
    &lt;p&gt;Really, we’ve been asking the wrong question. We don’t need every app developer to agree on what a &lt;code&gt;post&lt;/code&gt; is; we just need to let anyone “define” their own &lt;code&gt;post&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We could try namespacing types of records by the app name:&lt;/p&gt;
    &lt;p&gt;But then, app names can also clash. Luckily, we already have a way to avoid conflicts—domain names. A domain name is unique and implies ownership.&lt;/p&gt;
    &lt;p&gt;Why don’t we take some inspiration from Java?&lt;/p&gt;
    &lt;p&gt;This gives us collections.&lt;/p&gt;
    &lt;p&gt;A collection is a folder with records of a certain lexicon type. Twitter’s lexicon for posts might differ from Tumblr’s, and that’s fine—they’re in separate collections. The collection is always named like &lt;code&gt;&amp;lt;whoever.designs.the.lexicon&amp;gt;.&amp;lt;name&amp;gt;&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;For example, you could imagine these collection names:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;com.instagram.follow&lt;/code&gt;for Instagram follows&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fm.last.scrobble&lt;/code&gt;for Last.fm scrobbles&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;io.letterboxd.review&lt;/code&gt;for Letterboxd reviews&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You could also imagine these slightly whackier collection names:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;com.ycombinator.news.vote&lt;/code&gt;(subdomains are ok)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;co.wint.shitpost&lt;/code&gt;(personal domains work too)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;org.schema.recipe&lt;/code&gt;(a shared standard someday?)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;fm.last.scrobble_v2&lt;/code&gt;(breaking changes = new lexicon, just like file formats)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It’s like having a dedicated folder for every file extension.&lt;/p&gt;
    &lt;p&gt;To see some real lexicon names, check out UFOs and Lexicon Garden.&lt;/p&gt;
    &lt;head rend="h3"&gt;There Is No Lexicon Police&lt;/head&gt;
    &lt;p&gt;If you’re an application author, you might be thinking:&lt;/p&gt;
    &lt;p&gt;Who enforces that the records match their lexicons? If any app can (with the user’s explicit consent) write into any other app’s collection, how do we not end up with a lot of invalid data? What if some other app puts junk into “my” collection?&lt;/p&gt;
    &lt;p&gt;The answer is that records could be junk, but it still works out anyway.&lt;/p&gt;
    &lt;p&gt;It helps to draw a parallel to file extensions. Nothing stops someone from renaming &lt;code&gt;cat.jpg&lt;/code&gt; to &lt;code&gt;cat.pdf&lt;/code&gt;. A PDF reader would just refuse to open it.&lt;/p&gt;
    &lt;p&gt;Lexicon validation works the same way. The &lt;code&gt;com.tumblr&lt;/code&gt; in &lt;code&gt;com.tumblr.post&lt;/code&gt; signals who designed the lexicon, but the records themselves could have been created by any app at all. This is why apps always treat records as untrusted input, similar to &lt;code&gt;POST&lt;/code&gt; request bodies. When you generate type definitions from a lexicon, you also get a function that will do the validation for you. If some record passes the check, great—you get a typed object. If not, fine, ignore that record.&lt;/p&gt;
    &lt;p&gt;So, validate on read, just like files.&lt;/p&gt;
    &lt;p&gt;Some care is required when evolving lexicons. From the moment some lexicon is used in the wild, you should never change which records it would consider valid. For example, you can add new optional fields, but you can’t change whether some field is optional. This ensures that the new code can still read old records and that the old code will be able to read any new records. There’s a linter to check for this. (For breaking changes, make a new lexicon, as you would do with a file format.)&lt;/p&gt;
    &lt;p&gt;Although this is not required, you can publish your lexicons for documentation and distribution. It’s like publishing type definitions. There’s no separate registry for those; you just put them into a &lt;code&gt;com.atproto.lexicon.schema&lt;/code&gt; collection of some account, and then prove the lexicon’s domain is owned by you. For example, if I wanted to publish an &lt;code&gt;io.overreacted.comment&lt;/code&gt; lexicon, I could place it here:&lt;/p&gt;
    &lt;p&gt;Then I’d need to do some DNS setup to prove &lt;code&gt;overreacted.io&lt;/code&gt; is mine. This would make my lexicon show up in pdsls, Lexicon Garden, and other tools.&lt;/p&gt;
    &lt;head rend="h3"&gt;Links&lt;/head&gt;
    &lt;p&gt;Let’s circle back to our post.&lt;/p&gt;
    &lt;p&gt;We’ve already decided that the profile should live in the &lt;code&gt;com.twitter.profile&lt;/code&gt; collection, and the post itself should live in the &lt;code&gt;com.twitter.post&lt;/code&gt; collection:&lt;/p&gt;
    &lt;p&gt;But what about the likes?&lt;/p&gt;
    &lt;p&gt;Actually, what is a like?&lt;/p&gt;
    &lt;p&gt;A like is something that the user creates, so it makes sense for each like to be a record. A like record doesn’t convey any data other than which post is being liked:&lt;/p&gt;
    &lt;p&gt;In TypeScript, we expressed this as a reference to the &lt;code&gt;Post&lt;/code&gt; type. Since lexicons are JSON files with globally unique names, here’s how we’ll say this in lexicon:&lt;/p&gt;
    &lt;p&gt;We’re saying: a Like is an object with a &lt;code&gt;subject&lt;/code&gt; field that refers to some Post.&lt;/p&gt;
    &lt;p&gt;However, “refers” is doing a lot of work here. What does a Like record actually look like? How do you actually refer from inside of one JSON file to another JSON file?&lt;/p&gt;
    &lt;p&gt;We could try to refer to the Post record by its path in our “everything folder”:&lt;/p&gt;
    &lt;p&gt;But this only uniquely identifies it within a single user’s “everything folder”. Recall that each user has their own, completely isolated folders with all of their stuff:&lt;/p&gt;
    &lt;p&gt;We need to find some way to refer to the users themselves:&lt;/p&gt;
    &lt;p&gt;How do we do it?&lt;/p&gt;
    &lt;head rend="h3"&gt;Identity&lt;/head&gt;
    &lt;p&gt;This is a difficult problem.&lt;/p&gt;
    &lt;p&gt;So far, we’ve been building up a kind of a filesystem for social apps. But the “social” part requires linking between users. We need a reliable way to refer to some user. The challenge is that we’re building a distributed filesystem where the “everything folders” of different users may be hosted on different computers, by different companies, communities or organizations, or be self-hosted.&lt;/p&gt;
    &lt;p&gt;What’s more, we don’t want anyone to be locked into their current hosting. The user should be able to change who hosts their “everything folder” at any point, and without breaking any existing links to their files. The main tension is that we want to preserve users’ ability to change their hosting, but we don’t want that to break any links. Additionally, we want to make sure that, although the system is distributed, we’re confident that each piece of data has not been tampered with.&lt;/p&gt;
    &lt;p&gt;For now, you can forget all about records, collections, and folders. We’ll focus on a single problem: links. More concretely, we need a design for permanent links that allow swappable hosting. If we don’t make this work, everything else falls apart.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 1: Host as Identity&lt;/head&gt;
    &lt;p&gt;Suppose dril’s content is hosted by &lt;code&gt;some-cool-free-hosting.com&lt;/code&gt;. The most intuitive way to link to his content is to use a normal HTTP link to his hosting:&lt;/p&gt;
    &lt;p&gt;This works, but then if dril wants to change his hosting, he’d break every link. So this is not a solution—it’s the exact problem that we’re trying to solve. We want the links to point at “wherever dril’s stuff will be”, not “where dril’s stuff is right now”.&lt;/p&gt;
    &lt;p&gt;We need some kind of an indirection.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 2: Handle as Identity&lt;/head&gt;
    &lt;p&gt;We could give dril some persistent identifier like &lt;code&gt;@dril&lt;/code&gt; and use that in links:&lt;/p&gt;
    &lt;p&gt;We could then run a registry that stores a JSON document like this for each user:&lt;/p&gt;
    &lt;p&gt;The idea is that this document tells us how to find &lt;code&gt;@dril&lt;/code&gt;’s actual hosting.&lt;/p&gt;
    &lt;p&gt;We’d also need to provide some way for dril to update this document.&lt;/p&gt;
    &lt;p&gt;Some version of this could work but it seems unfortunate to invent our own global namespace when one already exists on the internet. Let’s try a twist on this idea.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 3: Domain as Identity&lt;/head&gt;
    &lt;p&gt;There’s already a global namespace anyone can participate in: DNS. If dril owns &lt;code&gt;wint.co&lt;/code&gt;, maybe we could let him use that domain as his persistent identity:&lt;/p&gt;
    &lt;p&gt;This doesn’t mean that the actual content is hosted at &lt;code&gt;wint.co&lt;/code&gt;; it just means that &lt;code&gt;wint.co&lt;/code&gt; hosts the JSON document that says where the content currently is. For example, maybe the convention is to serve that document as &lt;code&gt;/document.json&lt;/code&gt;. Again, the document points us at the hosting. Obviously, dril can update his doc.&lt;/p&gt;
    &lt;p&gt;This is somewhat elegant but in practice the tradeoff isn’t great. Losing domains is pretty common, and most people wouldn’t want that to brick their accounts.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 4: Hash as Identity&lt;/head&gt;
    &lt;p&gt;The last two attempts share a flaw: they tie you to the same handle forever.&lt;/p&gt;
    &lt;p&gt;Whether it’s a handle like &lt;code&gt;@dril&lt;/code&gt; or a domain handle like &lt;code&gt;@wint.co&lt;/code&gt;, we want people to be able to change their handles at any time without breaking links.&lt;/p&gt;
    &lt;p&gt;Sounds familiar? We also want the same for hosting. So let’s keep the “domain handles” idea but store the current handle in JSON alongside the current hosting:&lt;/p&gt;
    &lt;p&gt;This JSON is turning into sort of a calling card for your identity. “Call me &lt;code&gt;@wint.co&lt;/code&gt;, my stuff is at &lt;code&gt;https://some-cool-free-hosting.com&lt;/code&gt;.”&lt;/p&gt;
    &lt;p&gt;Now we need somewhere to host this document, and some way for you to edit it.&lt;/p&gt;
    &lt;p&gt;Let’s revisit the “centralized registry” from approach #2. One problem with it was using handles as permanent identifiers. Also, centralized is bad, but why is it bad? It’s bad for many reasons, but usually it’s the risk of abuse of power or a single point of failure. Maybe we can, if not remove, then reduce some of those risks. For example, it would be nice if could make the registry’s output self-verifiable.&lt;/p&gt;
    &lt;p&gt;Let’s see if we can use mathematics to help with this.&lt;/p&gt;
    &lt;p&gt;When you create an account, we’ll generate a private and a public key. We then create a piece of JSON with your initial handle, hosting, and public key. We sign this “create account” operation with your private key. Then we hash the signed operation. That gives us a string of gibberish like &lt;code&gt;6wpkkitfdkgthatfvspcfmjo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The registry will store your operation under that hash. That hash becomes the permanent identifier for your account. We’ll use it in links to refer to you:&lt;/p&gt;
    &lt;p&gt;To resolve a link like this, we ask the registry for the document belonging to &lt;code&gt;6wpkkitfdkgthatfvspcfmjo&lt;/code&gt;. It returns current your hosting, handle, and public key. Then we fetch &lt;code&gt;com.twitter.post/34qye3wows2c5&lt;/code&gt; from your hosting.&lt;/p&gt;
    &lt;p&gt;Okay, but how do you update your handle or your hosting in this registry?&lt;/p&gt;
    &lt;p&gt;To update, you create a new operation with a &lt;code&gt;prev&lt;/code&gt; field set to the hash of your previous operation. You sign it and send it to the registry. The registry validates the signature, appends the operation to your log, and updates the document.&lt;/p&gt;
    &lt;p&gt;To prove that it doesn’t forge the served documents, the registry exposes an endpoint that lists past operations for an identifier. To verify an operation, you check that its signature is valid and that its &lt;code&gt;prev&lt;/code&gt; field matches the hash of the operation before it. This lets you verify the entire chain of updates down to the first operation. The hash of the first operation is the identifier, so you can verify that too. At that point, you know that every change was signed with the user’s key.&lt;/p&gt;
    &lt;p&gt;(More on the trust model in the PLC specification.)&lt;/p&gt;
    &lt;p&gt;With this approach, the registry is still centralized but it can’t forge anyone’s documents without the risk of that being detected. To further reduce the need to trust the registry, we make its entire operation log auditable. The registry would hold no private data and be entirely open source. Ideally, it would eventually be spun it out into an independent legal entity so that long-term it can be like ICANN.&lt;/p&gt;
    &lt;p&gt;Since most people wouldn’t want to do key management, it’s assumed the hosting would hold the keys on behalf of the user. The registry includes a way to register an overriding rotational key, which is helpful in case the hosting itself goes rogue. (I wish for a way to set this up with a good UX; most people don’t have this on.)&lt;/p&gt;
    &lt;p&gt;Finally, since the handle is now determined by the document held in the registry, we’ll need to add some way for a domain to signal that it agrees with being some identifier’s handle. This could be done via DNS, HTTPS, or a mix of both.&lt;/p&gt;
    &lt;p&gt;Phew! This is not perfect but it gets us surprisingly far.&lt;/p&gt;
    &lt;head rend="h4"&gt;Attempt 5: DID as Identity&lt;/head&gt;
    &lt;p&gt;From the end user perspective, attempt #4 (hash as identity) is the most friendly. It doesn’t use domains for identity (only as handles), so losing a domain is fine.&lt;/p&gt;
    &lt;p&gt;However, some find relying on a third-party registry, no matter how transparent, untenable. So it would be nice to support approach #3 (domain as identity) too.&lt;/p&gt;
    &lt;p&gt;We’ll use a flexible identifier standard called DID (decentralized identifier) which is essentially a way to namespace multiple unrelated identification methods:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;did:web:wint.co&lt;/code&gt;and such — domain-based (attempt #3)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;did:plc:6wpkkitfdkgthatfvspcfmjo&lt;/code&gt;and such — registry-based (attempt #4)&lt;/item&gt;
      &lt;item&gt;This also leaves us a room to add other methods in the future, like &lt;code&gt;did:bla:...&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This makes our Like record look like this:&lt;/p&gt;
    &lt;p&gt;This is going to be its final form. We write &lt;code&gt;at://&lt;/code&gt; here to remind ourselves that this isn’t an HTTP link, and that you need to follow the resolution procedure (get the document, get the hosting, then get the record) to actually get the result.&lt;/p&gt;
    &lt;p&gt;Now you can forget everything we just discussed and remember four things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;A DID is a string identifier that represents an account.&lt;/item&gt;
      &lt;item&gt;An account’s DID never changes.&lt;/item&gt;
      &lt;item&gt;Every DID points at a document with the current hosting, handle, and public key.&lt;/item&gt;
      &lt;item&gt;A handle needs to be verified in the other direction (the domain must agree).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The mental model is that there’s a function like this:&lt;/p&gt;
    &lt;p&gt;You give it a DID, and it returns where to find their stuff, their bidirectionally verified current handle, and their public key. You’ll want a &lt;code&gt;'use cache'&lt;/code&gt; on it.&lt;/p&gt;
    &lt;p&gt;Let’s now finish our social filesystem.&lt;/p&gt;
    &lt;head rend="h3"&gt;&lt;code&gt;at://&lt;/code&gt; URI&lt;/head&gt;
    &lt;p&gt;With a DID, we can finally construct a path that identifies every particular record:&lt;/p&gt;
    &lt;p&gt;An &lt;code&gt;at://&lt;/code&gt; URI is a link to a record that survives hosting and handle changes.&lt;/p&gt;
    &lt;p&gt;The mental model here is that you can always resolve it to a record:&lt;/p&gt;
    &lt;p&gt;If the hosting is down, it would temporarily not resolve, but if the user puts it up anywhere and points their DID there, it will start resolving again. The user can also delete the record, which would remove it from the user’s “everything folder”.&lt;/p&gt;
    &lt;p&gt;Another way to think about &lt;code&gt;at://&lt;/code&gt; URI is that it is as a unique identifier of every record in our filesystem, so it can serve as a key in a database or a cache.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hyperlinks for JSON&lt;/head&gt;
    &lt;p&gt;With links, we can finally represent relationships between records.&lt;/p&gt;
    &lt;p&gt;Let’s look at dril’s post again:&lt;/p&gt;
    &lt;p&gt;Where do the 125 thousand likes come from?&lt;/p&gt;
    &lt;p&gt;These are just 125 thousand &lt;code&gt;com.twitter.like&lt;/code&gt; records in different people’s “everything folders” that each link to dril’s &lt;code&gt;com.twitter.post&lt;/code&gt; record:&lt;/p&gt;
    &lt;p&gt;Where do the 56K reposts come from? Similarly, this means that there are 56K &lt;code&gt;com.twitter.repost&lt;/code&gt; records across our social filesystem linking to this post:&lt;/p&gt;
    &lt;p&gt;What about the replies?&lt;/p&gt;
    &lt;p&gt;A reply is just a post that has a parent post. In TypeScript, we’d write it like this:&lt;/p&gt;
    &lt;p&gt;In lexicon, we’d write it like this:&lt;/p&gt;
    &lt;p&gt;This says: the &lt;code&gt;parent&lt;/code&gt; field is a reference to another &lt;code&gt;com.twitter.post&lt;/code&gt; record.&lt;/p&gt;
    &lt;p&gt;Every reply to dril’s post will have dril’s post as their &lt;code&gt;parent&lt;/code&gt;:&lt;/p&gt;
    &lt;p&gt;So, to get the reply count, we just need to count every such post:&lt;/p&gt;
    &lt;p&gt;We’ve now explained how every piece of the original UI can be derived from files:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The display name and avi come from dril’s &lt;code&gt;com.twitter.profile/self&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The tweet text and date come from dril’s &lt;code&gt;com.twitter.post/34qye3wows2c5&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;The like count is aggregated from everyone’s &lt;code&gt;com.twitter.like&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;The repost count is aggregated from everyone’s &lt;code&gt;com.twitter.repost&lt;/code&gt;s.&lt;/item&gt;
      &lt;item&gt;The reply count is aggregated from everyone’s &lt;code&gt;com.twitter.post&lt;/code&gt;s.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The last finishing touch is the handle. Unfortunately, &lt;code&gt;@dril&lt;/code&gt; can no longer work as a handle since we’ve chosen to use domains as handles. As a consolation, dril would be able to use &lt;code&gt;@wint.co&lt;/code&gt; across every future social app if he would like to.&lt;/p&gt;
    &lt;head rend="h3"&gt;A Repository&lt;/head&gt;
    &lt;p&gt;It’s time to give our “everything folder” a proper name. We’ll call it a repository. A repository is identified by a DID. It contains collections, which contain records:&lt;/p&gt;
    &lt;p&gt;Each repository is a user’s little piece of the social filesystem. A repository can be hosted anywhere—a free provider, a paid service, or your own server. You can move your repository as many times as you’d like without breaking links.&lt;/p&gt;
    &lt;p&gt;One challenge with building a social filesystem in practice is that apps need to be able to compute derived data (e.g. like counts) with no extra overhead. Of course, it would be completely impractical to look for every &lt;code&gt;com.twitter.like&lt;/code&gt; record in every repo referencing a specific post when trying to serve the UI for that post.&lt;/p&gt;
    &lt;p&gt;This is why, in addition to treating a repository as a filesystem—you can list and read stuff—you can treat it as a stream, subscribing to it by a WebSocket. This lets anyone build a local app-specific cache with just the derived data that app needs. Over the stream, you receive each commit as an event, along with the tree delta.&lt;/p&gt;
    &lt;p&gt;For example, a Hacker News backend could listen to creates/updates/deletes of &lt;code&gt;com.ycombinator.news.*&lt;/code&gt; records in every known repository and save those records locally for fast querying. It could also track derived data like &lt;code&gt;vote_count&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Subscribing to every known repository from every app is inconvenient. It is nicer to use dedicated services called relays which retransmit all events. However, this raises the issue of trust: how do you know whether someone else’s relay is lying?&lt;/p&gt;
    &lt;p&gt;To solve this, let’s make the repository data self-certifying. We can structure the repository as a hash tree. Each write is a signed commit containing the new root hash. This makes it possible to verify records as they come in against their original authors’ public keys. As long as you subscribe to a relay that retransmits its proofs, you can check every proof to know the records are authentic.&lt;/p&gt;
    &lt;p&gt;Verifying authenticity of records does not require storing their content, which means that relays can act as simple retransmitters and are affordable to run.&lt;/p&gt;
    &lt;head rend="h2"&gt;Up in the Atmosphere&lt;/head&gt;
    &lt;p&gt;Open pdsls.&lt;/p&gt;
    &lt;p&gt;If you want to explore the Atmosphere (&lt;code&gt;at://&lt;/code&gt;-mosphere, get it?), pdsls is the best starting point. Given a DID or a handle, it shows a list of collections and their records. It’s really like an old school file manager, except for the social stuff.&lt;/p&gt;
    &lt;p&gt;Go to &lt;code&gt;at://danabra.mov&lt;/code&gt; if you want some random place to start. Notice that you understand 80% of what’s going on there—Collections, Identity, Records, etc.&lt;/p&gt;
    &lt;p&gt;Feel free to branch out. Records link to other records. There is no app-specific aggregation there so it feels a little “ungrounded” (e.g. there is no thread view like in Bluesky) but there are some interesting navigational features like Backlinks.&lt;/p&gt;
    &lt;p&gt;Watch me walk around the Atmosphere for a bit:&lt;/p&gt;
    &lt;p&gt;(Yeah, what was that lexicon?! I didn’t expect to run into this while recording.)&lt;/p&gt;
    &lt;p&gt;Anyway, my favorite demo is this.&lt;/p&gt;
    &lt;p&gt;Watch me create a Bluesky post by creating a record via pdsls:&lt;/p&gt;
    &lt;p&gt;This works with any AT app, there’s nothing special about Bluesky. In fact, every AT app that cares to listen to events about the Bluesky Post lexicon knows that this post has been created. Apps live downstream from everybody’s records.&lt;/p&gt;
    &lt;p&gt;A month ago, I’ve made a little app called Sidetrail (it’s open source) to practice full-stack development. It lets you create step-by-step walkthroughs and “walk” those. Here you can see I’m deleting an &lt;code&gt;app.sidetrail.walk&lt;/code&gt; record in pdsls, and the corresponding walk disappears from my Sidetrail “walking” tab:&lt;/p&gt;
    &lt;p&gt;I know exactly why it works, it’s not supposed to surprise me, but it does! My repo really is the source of truth. My data lives in the Atmosphere, and apps “react” to it.&lt;/p&gt;
    &lt;p&gt;It’s weird!!!&lt;/p&gt;
    &lt;p&gt;Here is the code of my ingester:&lt;/p&gt;
    &lt;p&gt;This syncs everyone’s repo changes to my database so I have a snapshot that’s easy to query. I’m sure I could write this more clearly, but conceptually, it’s like I’m re-rendering my database. It’s like I called a &lt;code&gt;setState&lt;/code&gt; “above” the internet, and now the new props flow down from files into apps, and my DB reacts to them.&lt;/p&gt;
    &lt;p&gt;I could delete those tables in production, and then use Tap to backfill my database from scratch. I’m just caching a slice of the global data. And everyone building AT apps also needs to cache some slices. Maybe different slices, but they overlap. So pooling resources becomes more useful. More of our tooling can be shared too.&lt;/p&gt;
    &lt;p&gt;Here’s another example that I really like.&lt;/p&gt;
    &lt;p&gt;This is a teal.fm Relay demo made by &lt;code&gt;@chadmiller.com&lt;/code&gt;. It shows the list of everyone’s recently played tracks, as well as some of the overall playing stats:&lt;/p&gt;
    &lt;p&gt;Now, you can see it says “678,850 scrobbles” at the top of the screen. You might think people have been scrobbling their plays to the teal.fm API for a while.&lt;/p&gt;
    &lt;p&gt;Well, not really.&lt;/p&gt;
    &lt;p&gt;The teal.fm API doesn’t actually exist. It’s not a thing. Moreover, the teal.fm product doesn’t actually exist either. I mean, I think it’s in development (this is a hobby project!), but at the time of writing, https://teal.fm/ is only a landing page.&lt;/p&gt;
    &lt;p&gt;But this doesn’t matter!&lt;/p&gt;
    &lt;p&gt;All you need to start scrobbling is to put records of the &lt;code&gt;fm.teal.alpha.feed.play&lt;/code&gt; lexicon into your repo.&lt;/p&gt;
    &lt;p&gt;Let’s see if anyone is doing this right now:&lt;/p&gt;
    &lt;p&gt;waiting to connect&lt;/p&gt;
    &lt;p&gt;The lexicon isn’t published as a record (yet?) but it’s easy to find on GitHub. So anyone can build a scrobbler that writes these. I’m using one of those scrobblers.&lt;/p&gt;
    &lt;p&gt;Here’s my scrobble showing up:&lt;/p&gt;
    &lt;p&gt;(It’s a bit slow but &lt;del&gt;I think&lt;/del&gt; the delay is on the Spotify/scrobbler integration side.)&lt;/p&gt;
    &lt;p&gt;To be clear, the person who made this demo doesn’t work on teal.fm either. It’s not an “official” demo or anything, and it’s also not using the “teal.fm database” or “teal.fm API” or anything like it. It just indexes &lt;code&gt;fm.teal.alpha.feed.play&lt;/code&gt;s.&lt;/p&gt;
    &lt;p&gt;The demo’s data layer is using the new &lt;code&gt;lex-gql&lt;/code&gt; package, which is another of &lt;code&gt;@chadtmiller.com&lt;/code&gt;’s experiments. You give it some lexicons, and it lets you run GraphQL on your backfilled snapshot of the relevant parts of the social filesystem.&lt;/p&gt;
    &lt;p&gt;If you have the world’s JSON, why not run joins over products?&lt;/p&gt;
    &lt;p&gt;There’s one last example that I wanted to share.&lt;/p&gt;
    &lt;p&gt;For months, I’ve been complaining about the Bluesky’s default Discover feed which, frankly, doesn’t work all that great for me. Then I heard people saying good things about &lt;code&gt;@spacecowboy17.bsky.social&lt;/code&gt;’s For You algorithm.&lt;/p&gt;
    &lt;p&gt;I’ve been giving it a try, and I really like it!&lt;/p&gt;
    &lt;p&gt;I ended up switching to it completely. It reminds me of the Twitter algo in 2017—the swings are a bit hard but it finds the stuff I wouldn’t want to miss. It’s also much more responsive to “Show Less”. Its core principle seems pretty simple.&lt;/p&gt;
    &lt;p&gt;How does a custom feed like this work? Well, a Bluesky feed is just an endpoint that returns a list of &lt;code&gt;at://&lt;/code&gt; URIs. That’s the contract. You know how this works.&lt;/p&gt;
    &lt;p&gt;Could there be feeds of things other than posts? Sure.&lt;/p&gt;
    &lt;p&gt;Funnily enough, &lt;code&gt;@spacecowboy17.bsky.social&lt;/code&gt; used to run For You from a home computer. He posts a lot of interesting stuff, like A/B tests of feed changes. Also, here’s a For You debugger for my account. “Switch perspectives” is cool.&lt;/p&gt;
    &lt;p&gt;There was a tweet a few weeks ago clowning on Bluesky for being so bad at algorithms that users have to install a third-party feed to get a good experience.&lt;/p&gt;
    &lt;p&gt;I agree with &lt;code&gt;@dame.is&lt;/code&gt; that this shows something important: Bluesky is a place where that can happen. Why? In the Atmosphere, third party is first party. We’re all building projections of the same data. It’s a feature that someone can do it better.&lt;/p&gt;
    &lt;p&gt;An everything app tries to do everything.&lt;/p&gt;
    &lt;p&gt;An everything ecosystem lets everything get done.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46665839</guid><pubDate>Sun, 18 Jan 2026 08:18:36 +0000</pubDate></item><item><title>Command-line Tools can be 235x Faster than your Hadoop Cluster (2014)</title><link>https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html</link><description>&lt;doc fingerprint="177a599341632ef6"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Command-line Tools can be 235x Faster than your Hadoop Cluster&lt;/head&gt;
    &lt;head rend="h3"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;As I was browsing the web and catching up on some sites I visit periodically, I found a cool article from Tom Hayden about using Amazon Elastic Map Reduce (EMR) and mrjob in order to compute some statistics on win/loss ratios for chess games he downloaded from the millionbase archive, and generally have fun with EMR. Since the data volume was only about 1.75GB containing around 2 million chess games, I was skeptical of using Hadoop for the task, but I can understand his goal of learning and having fun with mrjob and EMR. Since the problem is basically just to look at the result lines of each file and aggregate the different results, it seems ideally suited to stream processing with shell commands. I tried this out, and for the same amount of data I was able to use my laptop to get the results in about 12 seconds (processing speed of about 270MB/sec), while the Hadoop processing took about 26 minutes (processing speed of about 1.14MB/sec).&lt;/p&gt;
    &lt;p&gt;After reporting that the time required to process the data with 7 c1.medium machine in the cluster took 26 minutes, Tom remarks&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;This is probably better than it would take to run serially on my machine but probably not as good as if I did some kind of clever multi-threaded application locally.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This is absolutely correct, although even serial processing may beat 26 minutes. Although Tom was doing the project for fun, often people use Hadoop and other so-called Big Data (tm) tools for real-world processing and analysis jobs that can be done faster with simpler tools and different techniques.&lt;/p&gt;
    &lt;p&gt;One especially under-used approach for data processing is using standard shell tools and commands. The benefits of this approach can be massive, since creating a data pipeline out of shell commands means that all the processing steps can be done in parallel. This is basically like having your own Storm cluster on your local machine. Even the concepts of Spouts, Bolts, and Sinks transfer to shell pipes and the commands between them. You can pretty easily construct a stream processing pipeline with basic commands that will have extremely good performance compared to many modern Big Data (tm) tools.&lt;/p&gt;
    &lt;p&gt;An additional point is the batch versus streaming analysis approach. Tom mentions in the beginning of the piece that after loading 10000 games and doing the analysis locally, that he gets a bit short on memory. This is because all game data is loaded into RAM for the analysis. However, considering the problem for a bit, it can be easily solved with streaming analysis that requires basically no memory at all. The resulting stream processing pipeline we will create will be over 235 times faster than the Hadoop implementation and use virtually no memory.&lt;/p&gt;
    &lt;head rend="h3"&gt;Learn about the data&lt;/head&gt;
    &lt;p&gt;The first step in the pipeline is to get the data out of the PGN files. Since I had no idea what kind of format this was, I checked it out on Wikipedia.&lt;/p&gt;
    &lt;code&gt;[Event "F/S Return Match"]
[Site "Belgrade, Serbia Yugoslavia|JUG"]
[Date "1992.11.04"]
[Round "29"]
[White "Fischer, Robert J."]
[Black "Spassky, Boris V."]
[Result "1/2-1/2"]
(moves from the game follow...)
&lt;/code&gt;
    &lt;p&gt;We are only interested in the results of the game, which only have 3 real outcomes. The 1-0 case means that white won, the 0-1 case means that black won, and the 1/2-1/2 case means the game was a draw. There is also a - case meaning the game is ongoing or cannot be scored, but we ignore that for our purposes.&lt;/p&gt;
    &lt;head rend="h3"&gt;Acquire sample data&lt;/head&gt;
    &lt;p&gt;The first thing to do is get a lot of game data. This proved more difficult than I thought it would be, but after some looking around online I found a git repository on GitHub from rozim that had plenty of games. I used this to compile a set of 3.46GB of data, which is about twice what Tom used in his test. The next step is to get all that data into our pipeline.&lt;/p&gt;
    &lt;head rend="h3"&gt;Build a processing pipeline&lt;/head&gt;
    &lt;p&gt;If you are following along and timing your processing, don’t forget to clear your OS page cache as otherwise you won’t get valid processing times.&lt;/p&gt;
    &lt;p&gt;Shell commands are great for data processing pipelines because you get parallelism for free. For proof, try a simple example in your terminal.&lt;/p&gt;
    &lt;code&gt;sleep 3 | echo "Hello world."
&lt;/code&gt;
    &lt;p&gt;Intuitively it may seem that the above will sleep for 3 seconds and then print &lt;code&gt;Hello world&lt;/code&gt; but in fact both steps are done at the same time. This basic fact is what can offer such great speedups for simple non-IO-bound processing systems capable of running on a single machine.&lt;/p&gt;
    &lt;p&gt;Before starting the analysis pipeline, it is good to get a reference for how fast it could be and for this we can simply dump the data to &lt;code&gt;/dev/null&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;cat *.pgn &amp;gt; /dev/null
&lt;/code&gt;
    &lt;p&gt;In this case, it takes about 13 seconds to go through the 3.46GB, which is about 272MB/sec. This would be a kind of upper-bound on how quickly data could be processed on this system due to IO constraints.&lt;/p&gt;
    &lt;p&gt;Now we can start on the analysis pipeline, the first step of which is using &lt;code&gt;cat&lt;/code&gt; to generate the stream of data.&lt;/p&gt;
    &lt;code&gt;cat *.pgn
&lt;/code&gt;
    &lt;p&gt;Since only the result lines in the files are interesting, we can simply scan through all the data files, and pick out the lines containing ‘Results’ with &lt;code&gt;grep&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result"
&lt;/code&gt;
    &lt;p&gt;This will give us only the &lt;code&gt;Result&lt;/code&gt; lines from the files. Now if we want, we can simply use the &lt;code&gt;sort&lt;/code&gt; and &lt;code&gt;uniq&lt;/code&gt; commands in order to get a list of all the unique items in the file along with their counts.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result" | sort | uniq -c
&lt;/code&gt;
    &lt;p&gt;This is a very straightforward analysis pipeline, and gives us the results in about 70 seconds. While we can certainly do better, assuming linear scaling this would have taken the Hadoop cluster approximately 52 minutes to process.&lt;/p&gt;
    &lt;p&gt;In order to reduce the speed further, we can take out the &lt;code&gt;sort | uniq&lt;/code&gt; steps from the pipeline, and replace them with AWK, which is a wonderful tool/language for event-based data processing.&lt;/p&gt;
    &lt;code&gt;cat *.pgn | grep "Result" | awk '{ split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This will take each result record, split it on the hyphen, and take the character immediately to the left, which will be a 0 in the case of a win for black, a 1 in the case of a win for white, or a 2 in the case of a draw. Note that &lt;code&gt;$0&lt;/code&gt; is a built-in variable that represents the entire record.&lt;/p&gt;
    &lt;p&gt;This reduces the running time to approximately 65 seconds, and since we’re processing twice as much data this is a speedup of around 47 times.&lt;/p&gt;
    &lt;p&gt;So even at this point we already have a speedup of around 47 with a naive local solution. Additionally, the memory usage is effectively zero since the only data stored is the actual counts, and incrementing 3 integers is almost free in memory space terms. However, looking at &lt;code&gt;htop&lt;/code&gt; while this is running shows that &lt;code&gt;grep&lt;/code&gt; is currently the bottleneck with full usage of a single CPU core.&lt;/p&gt;
    &lt;head rend="h3"&gt;Parallelize the bottlenecks&lt;/head&gt;
    &lt;p&gt;This problem of unused cores can be fixed with the wonderful &lt;code&gt;xargs&lt;/code&gt; command, which will allow us to parallelize the &lt;code&gt;grep&lt;/code&gt;. Since &lt;code&gt;xargs&lt;/code&gt; expects input in a certain way, it is safer and easier to use &lt;code&gt;find&lt;/code&gt; with the &lt;code&gt;-print0&lt;/code&gt; argument in order to make sure that each file name being passed to &lt;code&gt;xargs&lt;/code&gt; is null-terminated. The corresponding &lt;code&gt;-0&lt;/code&gt; tells &lt;code&gt;xargs&lt;/code&gt; to expected null-terminated input. Additionally, the &lt;code&gt;-n&lt;/code&gt; how many inputs to give each process and the &lt;code&gt;-P&lt;/code&gt; indicates the number of processes to run in parallel. Also important to be aware of is that such a parallel pipeline doesn’t guarantee delivery order, but this isn’t a problem if you are used to dealing with distributed processing systems. The &lt;code&gt;-F&lt;/code&gt; for &lt;code&gt;grep&lt;/code&gt; indicates that we are only matching on fixed strings and not doing any fancy regex, and can offer a small speedup, which I did not notice in my testing.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n1 -P4 grep -F "Result" | gawk '{ split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print NR, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This results in a run time of about 38 seconds, which is an additional 40% or so reduction in processing time from parallelizing the &lt;code&gt;grep&lt;/code&gt; step in our pipeline. This gets us up to approximately 77 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;p&gt;Although we have improved the performance dramatically by parallelizing the &lt;code&gt;grep&lt;/code&gt; step in our pipeline, we can actually remove this entirely by having &lt;code&gt;awk&lt;/code&gt; filter the input records (lines in this case) and only operate on those containing the string “Result”.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n1 -P4 awk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;You may think that would be the correct solution, but this will output the results of each file individually, when we want to aggregate them all together. The resulting correct implementation is conceptually very similar to what the MapReduce implementation would be.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n4 -P4 awk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++ } END { print white+black+draw, white, black, draw }' | awk '{games += $1; white += $2; black += $3; draw += $4; } END { print games, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;By adding the second awk step at the end, we obtain the aggregated game information as desired.&lt;/p&gt;
    &lt;p&gt;This further improves the speed dramatically, achieving a running time of about 18 seconds, or about 174 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;p&gt;However, we can make it a bit faster still by using mawk, which is often a drop-in replacement for &lt;code&gt;gawk&lt;/code&gt; and can offer better performance.&lt;/p&gt;
    &lt;code&gt;find . -type f -name '*.pgn' -print0 | xargs -0 -n4 -P4 mawk '/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++ } END { print white+black+draw, white, black, draw }' | mawk '{games += $1; white += $2; black += $3; draw += $4; } END { print games, white, black, draw }'
&lt;/code&gt;
    &lt;p&gt;This &lt;code&gt;find | xargs mawk | mawk&lt;/code&gt; pipeline gets us down to a runtime of about 12 seconds, or about 270MB/sec, which is around 235 times faster than the Hadoop implementation.&lt;/p&gt;
    &lt;head rend="h3"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;Hopefully this has illustrated some points about using and abusing tools like Hadoop for data processing tasks that can better be accomplished on a single machine with simple shell commands and tools. If you have a huge amount of data or really need distributed processing, then tools like Hadoop may be required, but more often than not these days I see Hadoop used where a traditional relational database or other solutions would be far better in terms of performance, cost of implementation, and ongoing maintenance.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46666085</guid><pubDate>Sun, 18 Jan 2026 08:58:40 +0000</pubDate></item><item><title>Keystone (YC S25) Is Hiring</title><link>https://news.ycombinator.com/item?id=46667101</link><description>&lt;doc fingerprint="8d21d67548473058"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Keystone builds infrastructure for autonomous coding agents. We give agents sandboxed environments that mirror production, event-based triggers (Sentry, Linear, GitHub), and verification workflows so they can ship code end-to-end— not just write it. We're hiring a founding engineer to work directly with me (solo founder) on core product. Stack is TypeScript, React (Next.js), Python, Postgres, Redis, AWS.&lt;/p&gt;
      &lt;p&gt;In-person in SoMa. $150K-$350K + 0.5-3% equity.&lt;/p&gt;
      &lt;p&gt;https://www.workatastartup.com/jobs/88801&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46667101</guid><pubDate>Sun, 18 Jan 2026 12:00:10 +0000</pubDate></item><item><title>Predicting OpenAI's ad strategy</title><link>https://ossa-ma.github.io/blog/openads</link><description>&lt;doc fingerprint="281eb85881276e3e"&gt;
  &lt;main&gt;
    &lt;p&gt;The World is Ads&lt;/p&gt;
    &lt;p&gt;Here we go again, the tech press is having another AI doom cycle.&lt;/p&gt;
    &lt;p&gt;I've primarily written this as a response to an NYT analyst painting a completely unsubstantiated, baseless, speculative, outrageous, EGREGIOUS, preposterous "grim picture" on OpenAI going bust.&lt;/p&gt;
    &lt;p&gt;Mate come on. OpenAI is not dying, they're not running out of money. Yes, they're creating possibly the craziest circular economy and defying every economics law since Adam Smith published 'The Wealth of Nations'. $1T in commitments is genuinely insane. But I doubt they're looking to be acquired; honestly by who? you don't raise $40 BILLION at $260 BILLION VALUATION to get acquired. It's all for the $1T IPO.&lt;/p&gt;
    &lt;p&gt;But it seems that the pinnacle of human intelligence: the greatest, smartest, brightest minds have all come together to... build us another ad engine. What happened to superintelligence and AGI?&lt;/p&gt;
    &lt;p&gt;See if OpenAI was not a direct threat to the current ad giants would Google be advertising Gemini every chance they get? Don't forget they're also capitalising on their brand new high-intent ad funnel by launching ads on Gemini and AI overview.&lt;/p&gt;
    &lt;p&gt;Let's crunch the numbers.&lt;/p&gt;
    &lt;head rend="h2"&gt;Quick Recap of OpenAI's 2025&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;March: Closed $40B funding round at $260B valuation, the largest raise by a private tech company on record.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;June: Hit $10B ARR.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;July: First $1B revenue month, doubled from $500M monthly in January.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;November: Sam Altman says OpenAI expects $20B ARR for 2025.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Reached 800M WAU, ~190M DAU, 35M paying subscribers, 1M business customers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;January 2026: "Both our Weekly Active User (WAU) and Daily Active User (DAU) figures continue to produce all-time-highs (Jan 14 was the highest, Jan 13 was the second highest, etc.)"&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;January 16, 2026: Announced ads in ChatGPT free and Go tiers.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Yes, OpenAI is burning $8-12B in 2025. Compute infrastructure is obviously not cheap when serving 190M people daily.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting OpenAI's Ad Strategy&lt;/head&gt;
    &lt;p&gt;So let's try to model their expected ARPU (annual revenue per user) by understanding what OpenAI is actually building and how it compares to existing ad platforms.&lt;/p&gt;
    &lt;p&gt;The ad products they've confirmed thus far:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ads at bottom of answers when there's a relevant sponsored product or service based on your current conversation&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rollout:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Q1 2026: Limited beta with select advertisers&lt;/item&gt;
      &lt;item&gt;Q2-Q3 2026: Expanded to ChatGPT Search for free-tier users&lt;/item&gt;
      &lt;item&gt;Q4 2026: Sidebar sponsored content + affiliate features&lt;/item&gt;
      &lt;item&gt;2027: Full international expansion, self-serve platform&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Testing starts "in the coming weeks" for logged-in adults in the U.S. on free and Go tiers. Ads will be "clearly labeled and separated from the organic answer." Users can learn why they're seeing an ad or dismiss it.&lt;/p&gt;
    &lt;p&gt;Their principles:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Answer independence: Ads don't influence ChatGPT's answers&lt;/item&gt;
      &lt;item&gt;Conversation privacy: Conversations stay private from advertisers, data never sold&lt;/item&gt;
      &lt;item&gt;Choice and control: Users can turn off personalization and clear ad data&lt;/item&gt;
      &lt;item&gt;Plus, Pro, Business, and Enterprise tiers won't have ads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;They also mentioned a possibility of conversational ads where you can ask follow-up questions about products directly.&lt;/p&gt;
    &lt;p&gt;Revenue targets: Reports suggest OpenAI is targeting $1B in ad revenue for 2026, scaling to $25B by 2029, though OpenAI hasn't confirmed these numbers publicly. We can use these as the conservative benchmark, but knowing the sheer product talent at OpenAI, the funding and hunger. I think they're blow past this.&lt;/p&gt;
    &lt;p&gt;Personal speculations on integration:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Self-serve platform: Advertisers bid for placements, super super super likely, exactly what Google does, probably their biggest revenue stream.&lt;/item&gt;
      &lt;item&gt;Affiliate commissions: Built-in checkouts so users can buy products inside ChatGPT, OpenAI takes commission, similar to their Shopify collab.&lt;/item&gt;
      &lt;item&gt;Sidebar sponsored content: When users ask about topics with market potential, sponsored info appears in a sidebar marked "Sponsored"&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now let's compare this to existing ad platforms:&lt;/p&gt;
    &lt;head rend="h3"&gt;Google: Intent + Vertical Integration = Highest Revenue&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Auction-based system where advertisers bid on keywords. Ads appear in search results based on bid + quality score.&lt;/item&gt;
      &lt;item&gt;Why it works: High intent (search queries) + owns the entire vertical stack (ad tech, auction system, targeting, decades of optimization)&lt;/item&gt;
      &lt;item&gt;Ad revenue: [$212.4B in ad revenue in the first 3 quarters of 2025]https://www.demandsage.com/google-ads-statistics/ (8.4% growth from 2024's $273.4B)&lt;/item&gt;
      &lt;item&gt;Google doesn't report ARPU so we need to calculate it: ARPU = $296.2B (projected) ÷ 5.01B = $59.12 per user annually.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Meta: No Intent + Vertical Integration = High ARPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Newsfeed ads delivered via auction. Meta's Andromeda AI evaluates bid + predicted action rate + ad quality to determine placement.&lt;/item&gt;
      &lt;item&gt;Why it works: Passive scrolling = low purchase intent, but on a massive scale + owns targeting infrastructure + Andromeda AI&lt;/item&gt;
      &lt;item&gt;ARPU: $68.44 in North America, $49.63 globally (Q1 2025)&lt;/item&gt;
      &lt;item&gt;Revenue: $160B in 2024 (97.3% of total revenue)&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Twitter/X: Engagement + No Vertical Stack = Low ARPU&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;How it works: Auction-based promoted tweets in timeline. Advertisers only pay when users complete actions (click, follow, engage).&lt;/item&gt;
      &lt;item&gt;Why it works: Timeline engagement, CPC ~$0.18, but doesn't own vertical stack and does it on a smaller scale&lt;/item&gt;
      &lt;item&gt;ARPU: ~$5.54 ($2.3B revenue ÷ 415M MAU)&lt;/item&gt;
      &lt;item&gt;Revenue: ~$2.3B in 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;ChatGPT: High Intent + No Vertical Stack = Where Does It Sit?&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Intent level: High. 2.5B prompts daily includes product research, recommendations, comparisons. More intent than Meta's passive scrolling, comparable to Google search.&lt;/item&gt;
      &lt;item&gt;Vertical integration: None. Yet.&lt;/item&gt;
      &lt;item&gt;Scale: 1B WAU by Feb 2026, but free users only (~950M at 95% free tier).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So where should ChatGPT's ARPU sit?&lt;/p&gt;
    &lt;p&gt;It sits with Search, not Social.&lt;/p&gt;
    &lt;p&gt;Which puts it between X ($5.54) and Meta ($49.63). OpenAI has better intent than Meta but worse infrastructure. They have more scale than X but no vertical integration. When a user asks ChatGPT "Help me plan a 5-day trip to Kyoto" or "Best CRM for small business," that is High Intent. That is a Google-level query, not a Facebook-level scroll.&lt;/p&gt;
    &lt;p&gt;We already have a benchmark for this: Perplexity.&lt;/p&gt;
    &lt;p&gt;In late 2024/2025, reports confirmed Perplexity was charging CPMs exceeding $50. This is comparable to premium video or high-end search, and miles above the ~$2-6 CPMs seen on social feeds.&lt;/p&gt;
    &lt;p&gt;If Perplexity can command $50+ CPMs with a smaller user base, OpenAI’s "High Agency" product team will likely floor their pricing there.&lt;/p&gt;
    &lt;p&gt;Super Bullish Target ARPU Trajectory:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026: $5.50 (The "Perplexity Floor") - Even with a clumsy beta and low fill rate, high-intent queries command premium pricing. If they serve just one ad every 20 queries at a Perplexity-level CPM, they hit this number effortlessly.&lt;/item&gt;
      &lt;item&gt;2027: $18.00 - The launch of a self-serve ad manager (like Meta/Google) allows millions of SMBs to bid. Competition drives price.&lt;/item&gt;
      &lt;item&gt;2028: $30.00 - This is where "Ads" become "Actions." OpenAI won't just show an ad for a flight; they will book it. Taking a cut of the transaction (CPA model) yields 10x the revenue of showing a banner.&lt;/item&gt;
      &lt;item&gt;2029: $50.00 (Suuuuuuuper bullish case) - Approaching Google’s ~$60 ARPU. By now, the infrastructure is mature, and "Conversational Commerce" is the standard. This is what Softbank is praying will happen.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And we're forgetting that OpenAI have a serious serious product team, I don't doubt for once they'll be fully capable of building out the stack and integrating ads til they occupy your entire subconscious.&lt;/p&gt;
    &lt;p&gt;In fact they hired Fidji Simo as their "CEO of Applications", a newly created role that puts her in charge of their entire revenue engine. Fidji is a Meta powerhouse who spent a decade at Facebook working on the Facebook App and... ads:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Leading Monetization of the Facebook App, with a focus on mobile advertising that represents the vast majority of Facebook's revenue. Launched new ad products such as Video Ads, Lead Ads, Instant Experiences, Carousel ads, etc.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;Launched and grew video advertising to be a large portion of Facebook's revenue.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;head rend="h2"&gt;Being Realistic About Competition&lt;/head&gt;
    &lt;p&gt;ChatGPT will hit 1B WAU by February 2026.&lt;/p&gt;
    &lt;p&gt;But 1.5-1.8B free users by 2028? That assumes zero competition impact from anyone, certainly not the looming giant Gemini. Unrealistic.&lt;/p&gt;
    &lt;p&gt;Let's estimate growth super conservatively accounting for competition:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;2026: 950M free users (1B WAU × 95% free tier)&lt;/item&gt;
      &lt;item&gt;2027: 1.1B free users (slower growth as market saturates)&lt;/item&gt;
      &lt;item&gt;2028: 1.2-1.3B free users (competition from Google, Claude)&lt;/item&gt;
      &lt;item&gt;2029: 1.4B free users (mature market, multi-player landscape)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The main revenue growth comes from ARPU scaling not just user growth.&lt;/p&gt;
    &lt;head rend="h2"&gt;Predicting 2026&lt;/head&gt;
    &lt;p&gt;Crunching all the numbers from "High Intent" model, 2026 looks different.&lt;/p&gt;
    &lt;p&gt;Base revenue from subscriptions + enterprise + API: $25-30B&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;35M paying subscribers: $8.4B minimum (conservatively assuming all at $20/mo Plus tier)&lt;/item&gt;
      &lt;item&gt;Definitely higher with Pro ($200/mo) and Enterprise (custom pricing)&lt;/item&gt;
      &lt;item&gt;Enterprise/API: $2.3B in 2025 → $17.4B by mid-2027&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Ad revenue (year 1): ~$5.2B&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;950M free users x $5.50 ARPU&lt;/item&gt;
      &lt;item&gt;ChatGPT does 2.5B prompts daily this is what advertisers would class as both higher engagement and higher intent than passive scrolling (although you can fit more ads in a scroll than a chat)&lt;/item&gt;
      &lt;item&gt;Reality Check: This assumes they monetise typical search queries at rates Perplexity has already proven possible.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Total 2026 Revenue: ~$30-35B.&lt;/p&gt;
    &lt;head rend="h2"&gt;Projecting 2027-2029&lt;/head&gt;
    &lt;p&gt;These projections use futuresearch.ai's base forecast ($39B median for mid-2027, no ads) + advertising overlay from internal OpenAI docs + conservative user growth.&lt;/p&gt;
    &lt;p&gt;2027:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $39B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $19.8B (1.1B free users × $18 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $58.8B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2028:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $55-60B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $36-39B (1.2-1.3B free users × $30 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $91-99B&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;2029:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Base revenue (no ads): $70-80B&lt;/item&gt;
      &lt;item&gt;Ad revenue: $70B (1.4B free users × $50 ARPU)&lt;/item&gt;
      &lt;item&gt;Total: $140-150B&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;The World is Ads&lt;/head&gt;
    &lt;p&gt;Ads were the key to unlocking profitability, you must've seen it coming, thanks to you not skipping that 3 minute health insurance ad - you, yes you helped us achieve AGI!&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Mission alignment: Our mission is to ensure AGI benefits all of humanity; our pursuit of advertising is always in support of that mission and making AI more accessible.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The A in AGI stands for Ads! It's all ads!! Ads that you can't even block because they are BAKED into the streamed probabilistic word selector purposefully skewed to output the highest bidder's marketing copy.&lt;/p&gt;
    &lt;p&gt;Look on the bright side, if they're turning to ads it likely means AGI is not on the horizon. Your job is safe!&lt;/p&gt;
    &lt;p&gt;It's 4:41AM in London, I'm knackered. Idek if I'm gonna post this because I love AI and do agree that some things are a necessary evil to achieve a greater goal (AGI). Nevertheless, if you have any questions or comments, shout me -&amp;gt; ossamachaib.cs@gmail.com.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46668021</guid><pubDate>Sun, 18 Jan 2026 14:25:49 +0000</pubDate></item><item><title>Sins of the Children</title><link>https://asteriskmag.com/issues/07/sins-of-the-children</link><description>&lt;doc fingerprint="e67013d6963124f4"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;The thing that came down right beside us was three meters high with a massive articulated body. A bug, really, Chelicer style. Eight crooked legs out from a central hub like all the mobile life here had, but most of what we’d seen was gracile, delicate, and came up to your waist. Even the Farmers — which we’d pegged as the most advanced species around — were only a meter and a half tall, and most of that was stilting limbs. This thing was not gracile. Every segment and joint of it was ridgy, armored, and spiky. It was dun and khaki like the planet’s dust, but too big to have hidden anywhere nearby, towering over the scrub. There were spread vanes like sails projecting from its back, but it couldn’t have flown under organic power. It must have weighed five tons.&lt;/p&gt;
      &lt;p&gt;We just stared. In that moment, when we could have run or called for help, we goggled at it. The stalked globes of its eyes looked back, devoid of living connection. A vast armored monster, airdropped from nowhere.&lt;/p&gt;
      &lt;p&gt;I saw the motion, off on a neighboring hillside. There was a second monster out there, surprisingly hard to spot. It hunkered down, drawing its limbs in.&lt;/p&gt;
      &lt;p&gt;Chunk! That same sound. The thing on the hillside was gone.&lt;/p&gt;
      &lt;p&gt;A second later it was on us, coming down right in front of Merrit. I thought of mechanical advantage, the tricks you could do with a rigid exoskeleton. I thought of fleas, but on an absurd macro scale. It jumped and came down on eight legs that must have been shock absorbers par excellence.&lt;/p&gt;
      &lt;p&gt;Chelicer life doesn’t quite have a front or a back, built around that hub of legs. The mouth is on the underside and that’s what this thing tilted at Merrit. &lt;/p&gt;
      &lt;p&gt;I’d dissected some of the Farmers and they had an arrangement like eight knuckly stumps to mumble over their food with. These new arrivals had a setup like a sphincter made of scissor blades and nutcrackers, more an industrial process than biology. We’d seen what those tools had done to the weather station already. Right then we were more concerned with what it did to Merrit. &lt;/p&gt;
      &lt;p&gt;He was just crouched there, midway through sifting the wreckage. The monster took instant offense. Its mouthparts extended out and just … macerated him. Chopped and crushed so that in a heartbeat there was nothing left that looked remotely human, just a wadded bloody ball of flesh and splintered bone and rags of suit. &lt;/p&gt;
      &lt;p&gt;Greffin and I started shooting. Our guns were so badly printed you could see the mold lines. They chewed up their own mass for ammo in a spray of flechettes. True to our miserly resource budget, most of that barrage just slanted off the things’ carapaces, and I knew we were both going to follow Merrit into extinction, carved up and spat out with alien contempt. Except then Greffin hit a joint, and one monster was suddenly down a leg. That, apparently, was enough. We watched them ratchet down for takeoff, still shrugging off our fire, then ping upward. I recorded the flight of one, desperately trying to keep it in my field of vision. Without that, who’d believe us? Alien mega-fleas utilizing sheer mechanical tension to jump a half kilometer at a time.&lt;/p&gt;
      &lt;p&gt;We bagged what was left of Merrit. And I grabbed the leg when the skimmer came for evac. Because it was proof that here be monsters.&lt;/p&gt;
      &lt;p&gt;The Farmers had been the tipping point, the reason to establish a human presence planetside. Yes, 14d was a unique world, unknown alien ecosphere, all that. But if it hadn’t held anything useful then the Garveneer would have focused elsewhere in the Chelicer system. And if the world had only offered mineral wealth, we’d have a robot mining operation stripping the place instead. All that unique ecosphere would have been flensed from the planet’s surface as an incidental side effect of our efforts. But on this world, the valuable thing was the biology, which needed more finesse. A human presence on the ground. Meaning a whole team of us thawed off the shelves and given this chance to justify our existence on the payroll.&lt;/p&gt;
      &lt;p&gt;Which was now under threat, as were we. We evacuated back to the farms with our grisly souvenirs.&lt;/p&gt;
      &lt;p&gt;The Concerns that have spearheaded humanity’s expansion from star to star have refined an efficient system for exploiting exoplants. When a Concern builds farms, that means a continent’s span of identical fields, robot tended. Everything growing and being harvested at an accelerated rate, processed and dried for minimal weight in transit. Turned into the Ship’s Reconstitute we’re all thoroughly sick of eating. The stuff from the Chelicer farms can look mighty good in comparison, which is a shame because a bite would kill you stone dead. But then they’re not our farms. They’re a thing the locals were doing long before we arrived.&lt;/p&gt;
      &lt;p&gt;The locals — Species 11 — are like spiders only ganglier. Four stilty legs interspersed with four spindly arms, and a hub of a body in the middle, high enough to come up to your waist. We called them Farmers from the start because it’s what they do: tend great stretches of this one crop. Not even a very exciting-looking crop, sort of a warty purple potato-looking thing, except it turns out to be superefficient at concentrating the elements in the crappy soil they’ve got here. Many of which elements are useful to us, for our superconductors and our computational substructures and all that good stuff. When we discovered that, you can be damn sure we moved in and took possession double time. Built our processing plant and started making off with a big chunk of the crop. &lt;/p&gt;
      &lt;p&gt;What did the locals think of this? My professional xenobiologist’s opinion was they didn’t think a damn thing. They didn’t react at all. The whole farming schtick they had going was just instinct, like ants, only they didn’t even defend anything. When they got in the way of the machines, they got chewed up. We thought at the time they’d evolved with no natural predators. &lt;/p&gt;
      &lt;p&gt;We sure as hell were wrong about that.&lt;/p&gt;
      &lt;p&gt;Greffin and I made our reports. The dozen on-planet crew came to commiserate, meaning get the gory details. We told everyone to carry a gun and know the emergency drill. Chelicer had an apex predator we hadn’t known about. After which cautionary tales, I was left facing up to the mission’s biggest pain in my ass, namely FenJuan.&lt;/p&gt;
      &lt;p&gt;FenJuan had screwed up royally on some past previous assignments, was my guess. They’d been something senior, and something had gone south in expensive ways. Meaning FenJuan slumming it on our team was an invisible mark against every one of us, because their personnel file came with baggage. Worse, they were my immediate colleague in biosciences, the two of us responsible for figuring out the local biochemistry.&lt;/p&gt;
      &lt;p&gt;“Stort,” they addressed me, frosty as always.&lt;/p&gt;
      &lt;p&gt;“Fen,” I replied with just as much love.&lt;/p&gt;
      &lt;p&gt;“My samples?” they said. Because they didn’t do fieldwork, just like they didn’t do basic human interaction, just sat at base camp and bitched.&lt;/p&gt;
      &lt;p&gt;And I’d given them samples previously. I’d cut a chunk out of a dozen critters on four other excursions and brought them back. And I’d just seen a work colleague turned to paste by some local monster-bug neither my nor FenJuan’s science had accounted for. But in the Concerns you don’t get time off for inefficient foibles like grief or trauma, so I made do with snarling at FenJuan that they’d had all the damn samples they were getting from me and if that wasn’t good enough then maybe they were the problem.&lt;/p&gt;
      &lt;p&gt;“When I say, ‘Get me a selection so I can run comparative studies,’” they snapped, “I do not mean just go snip bits off the Farmers and call the job done. A man is dead because we don’t understand the world here.”&lt;/p&gt;
      &lt;p&gt;Which was turning it back on me, making it my fault. And which wasn’t true to boot. I told them that if they were having difficulty distinguishing between samples maybe they didn’t have the basic analytical skills required for the task. The structures that they’d pegged as the local equivalent of a genome were probably just some essential organelle that every damn beastie possessed, and the real genome-equivalent had gone completely under FenJuan’s radar. &lt;/p&gt;
      &lt;p&gt;“You want a sample?” I asked FenJuan. “For real? Cut your own out of this. You can be absolutely sure it doesn’t come from a Farmer.” And I pointed them at the leg, the one we’d shot off the big bouncing bastard.&lt;/p&gt;
      &lt;p&gt;Shouting at people works, when you’re not allowed time off to process death. Works remarkably well, if it’s the only outlet you’ve got. Just as well. There would be plenty of both shouting and death in everyone’s future.&lt;/p&gt;
    &lt;/div&gt;
    &lt;div&gt;
      &lt;p&gt;I liked to sit outside to complete my reports. Chelicer has good sun, if you’ve had the treatments to ward off skin damage. I wrote up my thoughts on our giant killer flea problem, watching the Farmers pick their way across the vast fields of “Species 13 Resource” as per official Concern designation, or the Chelicetato as our vulgar parlance had it. They groped over each tuber in turn, then pissed out the right chemicals to help the things grow. The Farmers were a remarkable find. We’d have gone way over budget making robot gardeners even half as efficient. Worth fighting off a few giant bugs for.&lt;/p&gt;
      &lt;p&gt;Past the processing plants, the elevator cable stretched into forever. Up there was the Garveneer, our home away from home, taking every processed tuber we could hoik out of the ground. And our little outpost here was just the beginning. There were tens of millions of Farmers all over the planet, wherever the conditions suited their crop, all ready to become part of the industrial agriculture of the Concerns. We’d struck the jackpot when we surveyed Chelicer 14d.&lt;/p&gt;
      &lt;p&gt;Greffin had been going through recent survey images, looking for monsters. She sent me what she found. Holes like burrows I could have driven a ground-car into, written off as geological because nothing we’d seen could have made them. Now we knew better. Maybe the monster fleas had emerged only recently. Maybe there was a cicada thing going on, killer flea season. We made some recommendations for the next security meeting, and I did a tour of the turret guns that had been put in with the processing plant and never needed since.&lt;/p&gt;
      &lt;p&gt;Doing that put me in FenJuan’s orbit and I braced myself for the sandpaper of their company. They were deep in analyzing the giant leg, though, or thin-sliced samples thereof. They had a few dead Farmers too — there were plenty of aimless ones not working, now we’d harvested their plots. &lt;/p&gt;
      &lt;p&gt;“Stort,” they said, not the usual bark, but thoughtful. On the screens was a variety of different views of microscopic-scale Chelicer cell structure. The spiral-walled cones that FenJuan reckoned were hereditary information, and that they’d been unspooling and trying to decode. Sections had been flagged up on each, identical one to another.&lt;/p&gt;
      &lt;p&gt;“Junk DNA,” I said, and waited for their usual invective. It didn’t come, though. FenJuan actually nodded a little, a tiny iota of acknowledgment I’d said something that wasn’t stupid. And Earth life accumulates a certain amount of genetic junk, right? Stuff in the genome that’s been switched off, acquired from bacteria, or from benign transcription errors carried on down through the generations. But FenJuan reckoned something like 90 percent of any given beastie’s hereditary was this unused junk. &lt;/p&gt;
      &lt;p&gt;I wanted to say they were imagining things. I wanted to say it was a crap planet with crap aliens who had crap hereditary code, and us coming along to exploit them was the best thing that could have happened. That was how my encounters with FenJuan generally went. It was basically entertainment for the rest of the team.&lt;/p&gt;
      &lt;p&gt;I didn’t say any of that. FenJuan and I looked at each other, not quite ready to bury the hatchet, but maybe agreeing there was a bigger problem out there to save that mutual hatchet for.&lt;/p&gt;
      &lt;p&gt;The attack came the next day, and we weren’t prepared.&lt;/p&gt;
      &lt;p&gt;I heard the sound, distant, echoing across flat farmland from the dry hills. Chunk. For two whole seconds I was thinking some piece of machinery had gone wrong and how that was someone else’s problem. And then the first of them came down, just like before. Crashing onto the roof of the processing plant hard enough to buckle the plastic composite. Leering over the edge like a gargoyle. I swear it was twice the size of the one that killed Merrit. &lt;/p&gt;
      &lt;p&gt;I was shouting. Most of us were shouting, but I still caught a rapid heavy drumroll underneath the human noise. Chunkchunkchunkchunkchunkchunkchunk…&lt;/p&gt;
      &lt;p&gt;They started dropping down all round us. We were running for the plant, because it was the most reinforced building and that was the emergency drill. Someone got word to the guns that their services were needed, and they started running friend-or-foe algorithms as a dozen human beings fled frantically into their arcs of fire. &lt;/p&gt;
      &lt;p&gt;One of the death-fleas crashed down in front of me, outspread sails barely slowing it. The articulation of its legs popped and twisted, absorbing the force of impact. A gun hammered chips out of its carapace. It lunged forward and snipped someone — one of the resources team I think — right in half with its scissor-blade face. I screamed and just about ducked through the shadow of its wings, not knowing if I’d get killed by its jaws or our own turrets.&lt;/p&gt;
      &lt;p&gt;Most of us got inside. They didn’t break in after us, but only because they didn’t try. Maybe object permanence isn’t a big thing on Chelicer: Once we were out of sight they seemed to forget us, through they chewed up all the guns.&lt;/p&gt;
      &lt;p&gt;Through our cameras, we got to see all the rest of what they did.&lt;/p&gt;
      &lt;p&gt;The Farmers, it turned out, had natural predators. Or they did in death-flea season. The monsters went to town, mostly on the Farmers that didn’t have anything left to farm, because they were just milling about. It was a massacre. And though they were weird alien spider guys, and you can’t really anthropomorphize that, we were all surprisingly cut up. It wasn’t that they were getting slaughtered out there. It was that they were ours. Our livelihood, our profit, the injection of resources that was earning us our wage-worth.&lt;/p&gt;
      &lt;p&gt;The massacre was monopolizing our attention, so the real damage went almost unnoticed until the earthquakelike convulsion that cracked every wall and trashed the processor floor. For a moment the problem was so big I couldn’t work out what had happened.&lt;/p&gt;
      &lt;p&gt;The elevator cable. Something about it — maybe just that it was the biggest thing around — had drawn their ire. A half dozen of the bastards had jumped to it, and those mouthparts had sawn through the supertensile material like it was string.&lt;/p&gt;
      &lt;p&gt;That took a long while to clear up. The actual cable was, after all, a long weighted strand that stretched a good way out of atmosphere and into space, and our actual ship was tethered at the halfway point. The Garveneer decoupled sharpish, you can be sure, and the vast length of the cable, cut free at its anchor, just vanished upward and sideways like the blade of God’s own scythe, on its way toward the outer reaches of the system. &lt;/p&gt;
      &lt;p&gt;We were stuck on-planet for some time, and we’d just had it demonstrated to us that the death-fleas were more than capable of carving their way into our compromised fortress if they wanted. Yes, our lords and masters in the Concern could shuttle us back to orbit, but that would require circumstances to fall into a very narrow gap indeed. That (1) it wasn’t worth continuing work on Chelicer 14d, and (2) it was actually worth retrieving us, rather than writing us off. &lt;/p&gt;
      &lt;p&gt;You can imagine the mood on the ground as we waited for their decision. We all gathered in the surviving common space and tried to convince ourselves we weren’t screwed. All except FenJuan, who didn’t do social graces, but just kept on studying the samples, which our remaining instruments couldn’t tell apart.&lt;/p&gt;
      &lt;p&gt;In the end, after they’d left us hanging for five days, there was a meeting. A handful of us on a staticky link to the chief director safe aboard the Garveneer. We were ready to be bawled out for a colossal loss of resources. That was the very best we thought we’d get. Instead, though, the Great Man was onside. The harvest from Chelicer had been very good indeed, solving a variety of rare elements shortages none of us knew the Concern had. This world we had worked on was the new hope of further human expansion. If only we could solve our little pest problem.&lt;/p&gt;
      &lt;p&gt;“We need to keep you folks safe,” said the director heartily. I looked over the recommendations. What they actually wanted to keep safe was the harvest, of course, which meant the Farmers. By then we had images from all over the planet of sporadic attacks on Farmer colonies. Death-fleas picking off the weak. Nothing as sustained as we’d seen at our base camp, but plainly a part of the circle of life in these parts.&lt;/p&gt;
      &lt;p&gt;“Our engineers up here are working on a new cable,” the Great Man told us. “But drones, too. Hunter drones. A whole fleet of them. We can justify the cost, given the potential resource revenue you’ve demonstrated. We’re proposing a global initiative to wipe out these things.”&lt;/p&gt;
      &lt;p&gt;“Wipe out the species, Director?” FenJuan clarified. &lt;/p&gt;
      &lt;p&gt;“Given the losses we’ve sustained and the clear threat to productivity, it’s the leading proposal. But I’m here for your thoughts.” That cheery smile of his. “Stort?”&lt;/p&gt;
      &lt;p&gt;“We’re obviously still adjusting our picture of the ecosphere to incorporate these things,” I said. “Given the low species count on-world, having an apex predator that only emerges sporadically makes some sense. What happens if we remove it? We can’t know. If this was a matter of wanting to preserve a working natural ecosystem I’d say there would be too many potential imbalances generated by cropping the top of the food chain. But.”&lt;/p&gt;
      &lt;p&gt;“But,” the director agreed. Because we were not, after all, interested in preserving the ecosystem. Just that part of it that worked for us.&lt;/p&gt;
      &lt;p&gt;FenJuan’s eyes were boring into me; I didn’t meet them. “Historically,” I said, “in a managed agricultural paradigm, removal of the top predators has been accomplished very profitably. Wolves, sheep, so on. It’s not as though we’re going to have a problem with some Farmer population explosion. If some other species booms, we can manage the consequences. I say do it.”&lt;/p&gt;
      &lt;p&gt;“Director,” FenJuan put in, unasked. “I have yet to come to any understanding of the biology or relationships involved here. There’s a commonality between species I can’t account for. This world plainly went through some severe ecological crisis that left a depauperate web of interdependence. We don’t know—”&lt;/p&gt;
      &lt;p&gt;On our screens, the director settled back in his big chair. “We know all we need to. What this world could be worth to us. How much damage those beasts are capable of doing. An elevator cable! We’ll conduct a localized culling in your region first. Barring any obvious consequence, we can roll it out to the rest of the world and follow up with plant and personnel wherever these Farmer creatures are to be found.” His smile was genuinely pleased, a man who’s going to see a nice bonus. “Well done, all. I know it’s been tough, but you’re heroes.”&lt;/p&gt;
      &lt;p&gt;The local cull, when it first happened, was something to watch. Drone footage wheeling and spinning as our machines found and chased the fleas. Killed them as they leapt through the air, as they landed thunderously on the ground, as they emerged from their burrows. Wiping them out within 200 klicks of the processing plant. &lt;/p&gt;
      &lt;p&gt;And nothing broke. The Farmers kept on farming. The crops grew. The crops that, at a cellular level, seemed weirdly indistinguishable from the things that tended them. FenJuan was raising issues every day, by then. Desperate to communicate how weird their results were. Not doing their job, because their job was solely and specifically to identify aspects of the local biochemistry that could be profitably exploited. Instead of which, they were going nuts about how every critter just seemed to have this enormous bolus of unused genetic-equivalent information, with a huge overlap between species. And I think they’d just about worked it out, except by then they’d made such a nuisance of themselves that FenJuan was the very last person our bosses up in orbit wanted to hear from. Did this discovery open up new vistas of planetary exploitation for our already profitable operation? No? Then pipe down and stop using up comms resources.&lt;/p&gt;
      &lt;p&gt;The people the director did want to hear from were designing and deploying the hunter-killers. Our expanded drone fleet was greenlit: hundreds of machines shipped downwell and let loose across the globe. Wherever they found the fleas, they destroyed them. We felt we were liberators. Whole populations of Farmers could live without those monstrous shadows falling on them. Yes, we were making a species extinct, but it wasn’t a nice species. We were already on the next phase of occupation, a 10-year building plan where we’d fill the planet with farms and processing plants, replicating our first outpost over and over until there wasn’t an inch of the world that wasn’t working for us.&lt;/p&gt;
      &lt;p&gt;A couple of years into our agricultural expansion, the cacti disappeared. Not cacti, obviously. Species 43 in the Concern bestiary, but cactus enough that the name had stuck. We had a look one morning and there just wasn’t any more of it left. I suggested maybe it had been living off some sort of death-flea by-products, though the timing seemed unusually lethargic for that kind of interaction. I ended up working alongside FenJuan, and we found drone footage of the cacti stuff getting up and running around, so that Species 43 turned out to be the larval-or-something form of Species 22, and we had to recalibrate the records. &lt;/p&gt;
      &lt;p&gt;At around the same time, the little hairy critters that were Species 38 rooted down and grew long spires with puffballs on them, making them actually Species 17. Half a year later our existing Species 11s lost their poles and became another sort of thing we’d already seen, and so on and so on. To most of us it was a curiosity. To FenJuan it was a crawling horror that I was starting to share. All their snapping, bitching at me for not seeing, and I’d just written it all off as someone pissed their Concern work record was full of demerits. Except they’d been right and I’d been wrong.&lt;/p&gt;
      &lt;p&gt;There were no more cacti. That was what scared FenJuan. We watched a wave of transformations. Each form turned into something else, but none of it turned into the cactuslike Species 43. Then it was something else, where our current batch just metamorphosed and there were no new ones. None at all, anywhere on Chelicer. The dry country became less and less inhabited as species after species vanished away.&lt;/p&gt;
      &lt;p&gt;Or not species. That was what FenJuan had been trying to understand. Developmental stages. Not a circle of life, but a life cycle.&lt;/p&gt;
      &lt;p&gt;Our prized cheliceratos, which had been putting out runners and new tubers happily for over a decade, were suddenly ambulatory one morning, sprouting a thicket of spindly legs and just giving up their life of being agricultural produce. That got people’s attention. Around the same time one weird round critter rooted down where the Farmers were and became the new Chelicetato crop, and the dumbest of our colleagues reckoned that was all OK then. FenJuan and I had stopped trying to raise the alarm, by then, because it obviously wasn’t going to help. Soon after, some buried fungal-looking thing we’d found no use for sprouted legs and became new Farmers. And the old farmers … died off. Wore out, natural causes. Leaving only the least dregs we’d left of their crop. From which a handful of stunted things crawled, devouring their own left-behind husks and the last corpses of their tenders. They were tiny, but we recognized them even as they began to wearily dig down into the parched, lifeless soil. Nascent fleas, entering that dormant part of their cycle from which they would emerge, at some future date, into a world devoid of anything that could sustain them. Behind them, the whole ecosystem of life stages had been rolled up. There was nothing left of it. They were the last.&lt;/p&gt;
      &lt;p&gt; As we had harvested and plundered, we had been watching a decade-long series of transformations. One that had definitively ended. Life on Chelicer vanished. Plant forms, bug forms, just about every macrobiological creature dying off one at a time and not being replaced by a new generation. As though death had asked them to form an orderly queue. &lt;/p&gt;
      &lt;p&gt;There had been a mass extinction in Chelicer’s past, FenJuan and I reckoned. Something that had killed off everything except a hardy species that inherited an utterly impoverished planetary biome. Colder at the poles, warmer at the equator, but barren, desperate. So, over the ages, that species had developed to exploit every last opportunity that the world had left to it, not through speciation but through adaption of its life cycle. Gathering the meager resources of the world, concentrating them in living forms that could be harvested in turn. Sedentary stages, mobile stages, squeezing every possible niche of everything that could be gained and then transforming into the next phase of its long and complex chain of shapes. A desperate ecosystem of one, harvesting and gathering and recycling, each stage into the next, surviving everything thrown at it. Except us, who came and severed a single link utterly and irrevocably. Cut one thread and watched the whole unravel over a mere decade.&lt;/p&gt;
      &lt;p&gt;FenJuan and I were last off the planet, on the final elevator car along with the last salvage from our farming operations. It was on us, we had been told. We were the biologists, and we should have seen it coming. And they were right; we should. But all that would have done was salve our professional pride. I don’t believe for a moment they’d have listened to us if we’d said Stop. Stop isn’t the way of the Concerns. Stop doesn’t meet quotas or hit targets.&lt;/p&gt;
      &lt;p&gt;We stepped into the elevator car, FenJuan and I. We looked back over a world unrelieved by messy, complicated stuff, such as life. A failed commercial opportunity, as the report would say. &lt;/p&gt;
      &lt;p&gt;I wanted to say something. Possibly You were right. But what good would it do? We were both going to be back on ice when we reached the ship, with personnel files so dire they’ll probably never thaw us out again. But, like the life of Chelicer, we’re not important, compared to the bigger picture of the Concerns and their expansion. We humans go on, world to world, star to star, making the universe our own. But on Chelicer there will only ever be dust.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46669663</guid><pubDate>Sun, 18 Jan 2026 17:08:58 +0000</pubDate></item><item><title>Gaussian Splatting – A$AP Rocky "Helicopter" music video</title><link>https://radiancefields.com/a-ap-rocky-releases-helicopter-music-video-featuring-gaussian-splatting</link><description>&lt;doc fingerprint="2ff1eb95032d70b0"&gt;
  &lt;main&gt;
    &lt;p&gt;Michael Rubloff&lt;/p&gt;
    &lt;p&gt;Jan 13, 2026&lt;/p&gt;
    &lt;p&gt;Believe it or not, A$AP Rocky is a huge fan of radiance fields.&lt;/p&gt;
    &lt;p&gt;Yesterday, when A$AP Rocky released the music video for Helicopter, many viewers focused on the chaos, the motion, and the unmistakable early MTV energy of the piece. Whatâs easier to miss, unless you know what youâre looking at, is that nearly every human performance in the video was captured volumetrically and rendered as dynamic splats.&lt;/p&gt;
    &lt;p&gt;I spoke with Evercoast, the team responsible for capturing the performances, as well as Chris Rutledge, the projectâs CG Supervisor at Grin Machine, and Wilfred Driscoll of WildCapture and FitsÅ«.ai, to understand how Helicopter came together and why this project represents one of the most ambitious real world deployments of dynamic gaussian splatting in a major music release to date.&lt;/p&gt;
    &lt;p&gt;The decision to shoot Helicopter volumetrically wasnât driven by technology for technologyâs sake. According to the team, the director Dan Strait approached the project in July with a clear creative goal to capture human performance in a way that would allow radical freedom in post-production. This would have been either impractical or prohibitively expensive using conventional filming and VFX pipelines.&lt;/p&gt;
    &lt;p&gt;Chris told me heâd been tracking volumetric performance capture for years, fascinated by emerging techniques that could enable visuals that simply werenât possible before. Two years ago, he began pitching the idea to directors in his circle, including Dan, as a âsomedayâ workflow. When Dan came back this summer and said he wanted to use volumetric capture for the entire video, the proliferation of gaussian splatting enabled them to take it on.&lt;/p&gt;
    &lt;p&gt;The aesthetic leans heavily into kinetic motion. Dancers colliding, bodies suspended in midair, chaotic fight scenes, and performers interacting with props that later dissolve into something else entirely. Every punch, slam, pull-up, and fall you see was physically performed and captured in 3D.&lt;/p&gt;
    &lt;p&gt;Almost every human figure in the video, including Rocky himself, was recorded volumetrically using Evercoastâs system. Itâs all real performance, preserved spatially.&lt;/p&gt;
    &lt;p&gt;This is not the first time that A$AP Rocky has featured a radiance field in one of his music videos. The 2023 music video for Shittinâ Me featured several NeRFs and even the GUI for Instant-NGP, which you can spot throughout the piece.&lt;/p&gt;
    &lt;p&gt;The primary shoot for Helicopter took place in August in Los Angeles. Evercoast deployed a 56 camera RGB-D array, synchronized across two Dell workstations. Performers were suspended from wires, hanging upside down, doing pull-ups on ceiling-mounted bars, swinging props, and performing stunts, all inside the capture volume.&lt;/p&gt;
    &lt;p&gt;Scenes that appear surreal in the final video were, in reality, grounded in very physical setups, such as wooden planks standing in for helicopter blades, real wire rigs, and real props. The volumetric data allowed those elements to be removed, recomposed, or entirely recontextualized later without losing the authenticity of the human motion.&lt;/p&gt;
    &lt;p&gt;Over the course of the shoot, Evercoast recorded more than 10 terabytes of raw data, ultimately rendering roughly 30 minutes of final splatted footage, exported as PLY sequences totaling around one terabyte.&lt;/p&gt;
    &lt;p&gt;That data was then brought into Houdini, where the post production team used CG Nomads GSOPs for manipulation and sequencing, and OTOYâs OctaneRender for final rendering. Thanks to this combination, the production team was also able to relight the splats.&lt;/p&gt;
    &lt;p&gt;One of the more powerful aspects of the workflow was Evercoastâs ability to preview volumetric captures at multiple stages. The director could see live spatial feedback on set, generate quick mesh based previews seconds after a take, and later review fully rendered splats through Evercoastâs web player before downloading massive PLY sequences for Houdini.&lt;/p&gt;
    &lt;p&gt;In practice, this meant creative decisions could be made rapidly and cheaply, without committing to heavy downstream processing until the team knew exactly what they wanted. Itâs a workflow that more closely resembles simulation than traditional filming.&lt;/p&gt;
    &lt;p&gt;Chris also discovered that Octaneâs Houdini integration had matured, and that Octaneâs early splat support was far enough along to enable relighting. According to the team, the ability to relight splats, introduce shadowing, and achieve a more dimensional â3D videoâ look was a major reason the final aesthetic lands the way it does.&lt;/p&gt;
    &lt;p&gt;The team also used Blender heavily for layout and previs, converting splat sequences into lightweight proxy caches for scene planning. Wilfred described how WildCaptureâs internal tooling was used selectively to introduce temporal consistency. In his words, the team derived primitive pose estimation skeletons that could be used to transfer motion, support collision setups, and allow Houdiniâs simulation toolset to handle rigid body, soft body, and more physically grounded interactions.&lt;/p&gt;
    &lt;p&gt;One recurring reaction to the video has been confusion. Viewers assume the imagery is AI-generated. According to Evercoast, that couldnât be further from the truth. Every stunt, every swing, every fall was physically performed and captured in real space. What makes it feel synthetic is the freedom volumetric capture affords. You arenât limited by the cameraâs composition. You have free rein to explore, reposition cameras after the fact, break spatial continuity, and recombine performances in ways that 2D simply canât.&lt;/p&gt;
    &lt;p&gt;In other words, radiance field technology isnât replacing reality. Itâs preserving everything.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46670024</guid><pubDate>Sun, 18 Jan 2026 17:40:55 +0000</pubDate></item><item><title>Show HN: Lume 0.2 – Build and Run macOS VMs with unattended setup</title><link>https://cua.ai/docs/lume/guide/getting-started/introduction</link><description>&lt;doc fingerprint="25b849d06d0791ad"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;What is Lume?&lt;/head&gt;
    &lt;p&gt;Introduction to Lume - the macOS VM CLI and framework&lt;/p&gt;
    &lt;p&gt;Lume is a VM runtime for building AI agents, running CI/CD pipelines, and automating macOS. It uses Apple's native Virtualization Framework to run macOS and Linux VMs at near-native speed on Apple Silicon.&lt;/p&gt;
    &lt;p&gt;MIT License&lt;/p&gt;
    &lt;p&gt;Lume is open-source and MIT licensed. If you find it useful, we'd appreciate a star on GitHub!&lt;/p&gt;
    &lt;p&gt;Cloud macOS Sandboxes&lt;/p&gt;
    &lt;p&gt;We're piloting a managed service for customers who want to run cloud macOS sandboxes for CI/CD and agent workloads. Book a demo if you're interested.&lt;/p&gt;
    &lt;p&gt;A single binary with an HTTP API. Create a VM, run it headlessly, control it programmatically.&lt;/p&gt;
    &lt;head rend="h2"&gt;Architecture&lt;/head&gt;
    &lt;p&gt;You can use Lume directly via CLI, or run &lt;code&gt;lume serve&lt;/code&gt; to expose an HTTP API for programmatic access. The Computer SDK uses this API to automate macOS interactions.&lt;/p&gt;
    &lt;head rend="h2"&gt;How it works&lt;/head&gt;
    &lt;p&gt;Lume is a thin layer over Apple's Virtualization Framework, which provides hardware-accelerated virtualization on Apple Silicon. This gives you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Native speed — CPU instructions execute directly via hardware virtualization&lt;/item&gt;
      &lt;item&gt;Paravirtualized graphics — Basic GPU support via Apple's virtualization layer (limited to GPU Family 5)&lt;/item&gt;
      &lt;item&gt;Efficient storage — Sparse disk files only consume actual usage, not allocated size&lt;/item&gt;
      &lt;item&gt;Rosetta 2 support — Run x86 Linux binaries in ARM Linux VMs&lt;/item&gt;
      &lt;item&gt;Automated golden images — Go from IPSW to fully configured macOS VM without manual intervention&lt;/item&gt;
      &lt;item&gt;Registry support — Pull and push VM images from GHCR or GCS registries&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;When to use Lume&lt;/head&gt;
    &lt;p&gt;Testing across macOS versions — Spin up a VM with a specific macOS version, test your software, tear it down. No need to maintain multiple physical machines.&lt;/p&gt;
    &lt;p&gt;Automating macOS tasks — Combine Lume with Unattended Setup to create pre-configured VMs. The setup automation uses VNC and OCR to click through the Setup Assistant without manual intervention.&lt;/p&gt;
    &lt;p&gt;Running CI/CD locally — Test your macOS builds in isolated VMs before pushing to remote CI. The &lt;code&gt;--no-display&lt;/code&gt; flag runs VMs headlessly.&lt;/p&gt;
    &lt;p&gt;Sandboxing risky operations — Need to test untrusted software or destructive scripts? Run them in a VM, then delete it. Clone a known-good VM to reset to a clean state instantly.&lt;/p&gt;
    &lt;p&gt;Building AI agents — Lume powers the Cua Computer SDK, providing VMs that AI models can interact with through screenshots and input simulation.&lt;/p&gt;
    &lt;p&gt;Used by Anthropic&lt;/p&gt;
    &lt;p&gt;Apple's Virtualization Framework—the same technology Lume is built on—powers Claude Cowork, Anthropic's sandboxed environment for Claude Code. It downloads a Linux root filesystem and boots it in an isolated VM where Claude can safely execute commands without access to your broader system.&lt;/p&gt;
    &lt;head rend="h2"&gt;What Lume doesn't do&lt;/head&gt;
    &lt;p&gt;Lume requires Apple Silicon—it won't work on Intel Macs or other platforms.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get started&lt;/head&gt;
    &lt;p&gt;Ready to try it? Install Lume and create your first VM in the Quickstart.&lt;/p&gt;
    &lt;p&gt;Was this page helpful?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46670181</guid><pubDate>Sun, 18 Jan 2026 17:53:21 +0000</pubDate></item><item><title>Flux 2 Klein pure C inference</title><link>https://github.com/antirez/flux2.c</link><description>&lt;doc fingerprint="2e5c8b38193e188c"&gt;
  &lt;main&gt;
    &lt;p&gt;This program generates images from text prompts (and optionally from other images) using the FLUX.2-klein-4B model from Black Forest Labs. It can be used as a library as well, and is implemented entirely in C, with zero external dependencies beyond the C standard library. MPS and BLAS acceleration are optional but recommended.&lt;/p&gt;
    &lt;p&gt;I (the human here, Salvatore) wanted to test code generation with a more ambitious task, over the weekend. This is the result. It is my first open source project where I wrote zero lines of code. I believe that inference systems not using the Python stack (which I do not appreciate) are a way to free open models usage and make AI more accessible. There is already a project doing the inference of diffusion models in C / C++ that supports multiple models, and is based on GGML. I wanted to see if, with the assistance of modern AI, I could reproduce this work in a more concise way, from scratch, in a weekend. Looks like it is possible.&lt;/p&gt;
    &lt;p&gt;This code base was written with Claude Code, using the Claude Max plan, the small one of ~80 euros per month. I almost reached the limits but this plan was definitely sufficient for such a large task, which was surprising. In order to simplify the usage of this software, no quantization is used, nor do you need to convert the model. It runs directly with the safetensors model as input, using floats.&lt;/p&gt;
    &lt;p&gt;Even if the code was generated using AI, my help in steering towards the right design, implementation choices, and correctness has been vital during the development. I learned quite a few things about working with non trivial projects and AI.&lt;/p&gt;
    &lt;code&gt;# Build (choose your backend)
make mps       # Apple Silicon (fastest)
# or: make blas    # Intel Mac / Linux with OpenBLAS
# or: make generic # Pure C, no dependencies

# Download the model (~16GB)
pip install huggingface_hub
python download_model.py

# Generate an image
./flux -d flux-klein-model -p "A woman wearing sunglasses" -o output.png&lt;/code&gt;
    &lt;p&gt;That's it. No Python runtime, no PyTorch, no CUDA toolkit required at inference time.&lt;/p&gt;
    &lt;p&gt;Generated with: &lt;code&gt;./flux -d flux-klein-model -p "A picture of a woman in 1960 America. Sunglasses. ASA 400 film. Black and White." -W 250 -H 250 -o /tmp/woman.png&lt;/code&gt;, and later processed with image to image generation via &lt;code&gt;./flux -d flux-klein-model -i /tmp/woman.png -o /tmp/woman2.png -p "oil painting of woman with sunglasses" -v -H 256 -W 256&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero dependencies: Pure C implementation, works standalone. BLAS optional for ~30x speedup (Apple Accelerate on macOS, OpenBLAS on Linux)&lt;/item&gt;
      &lt;item&gt;Metal GPU acceleration: Automatic on Apple Silicon Macs&lt;/item&gt;
      &lt;item&gt;Text-to-image: Generate images from text prompts&lt;/item&gt;
      &lt;item&gt;Image-to-image: Transform existing images guided by prompts&lt;/item&gt;
      &lt;item&gt;Integrated text encoder: Qwen3-4B encoder built-in, no external embedding computation needed&lt;/item&gt;
      &lt;item&gt;Memory efficient: Automatic encoder release after encoding (~8GB freed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;./flux -d flux-klein-model -p "A fluffy orange cat sitting on a windowsill" -o cat.png&lt;/code&gt;
    &lt;p&gt;Transform an existing image based on a prompt:&lt;/p&gt;
    &lt;code&gt;./flux -d flux-klein-model -p "oil painting style" -i photo.png -o painting.png -t 0.7&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;-t&lt;/code&gt; (strength) parameter controls how much the image changes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.0&lt;/code&gt;= no change (output equals input)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;1.0&lt;/code&gt;= full generation (input only provides composition hint)&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.7&lt;/code&gt;= good balance for style transfer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Required:&lt;/p&gt;
    &lt;code&gt;-d, --dir PATH        Path to model directory
-p, --prompt TEXT     Text prompt for generation
-o, --output PATH     Output image path (.png or .ppm)
&lt;/code&gt;
    &lt;p&gt;Generation options:&lt;/p&gt;
    &lt;code&gt;-W, --width N         Output width in pixels (default: 256)
-H, --height N        Output height in pixels (default: 256)
-s, --steps N         Sampling steps (default: 4)
-S, --seed N          Random seed for reproducibility
&lt;/code&gt;
    &lt;p&gt;Image-to-image options:&lt;/p&gt;
    &lt;code&gt;-i, --input PATH      Input image for img2img
-t, --strength N      How much to change the image, 0.0-1.0 (default: 0.75)
&lt;/code&gt;
    &lt;p&gt;Output options:&lt;/p&gt;
    &lt;code&gt;-q, --quiet           Silent mode, no output
-v, --verbose         Show detailed config and timing info
&lt;/code&gt;
    &lt;p&gt;Other options:&lt;/p&gt;
    &lt;code&gt;-e, --embeddings PATH Load pre-computed text embeddings (advanced)
-h, --help            Show help
&lt;/code&gt;
    &lt;p&gt;The seed is always printed to stderr, even when random:&lt;/p&gt;
    &lt;code&gt;$ ./flux -d flux-klein-model -p "a landscape" -o out.png
Seed: 1705612345
out.png
&lt;/code&gt;
    &lt;p&gt;To reproduce the same image, use the printed seed:&lt;/p&gt;
    &lt;code&gt;$ ./flux -d flux-klein-model -p "a landscape" -o out.png -S 1705612345
&lt;/code&gt;
    &lt;p&gt;Choose a backend when building:&lt;/p&gt;
    &lt;code&gt;make            # Show available backends
make generic    # Pure C, no dependencies (slow)
make blas       # BLAS acceleration (~30x faster)
make mps        # Apple Silicon Metal GPU (fastest, macOS only)&lt;/code&gt;
    &lt;p&gt;Recommended:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;macOS Apple Silicon: &lt;code&gt;make mps&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;macOS Intel: &lt;code&gt;make blas&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux with OpenBLAS: &lt;code&gt;make blas&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Linux without OpenBLAS: &lt;code&gt;make generic&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For &lt;code&gt;make blas&lt;/code&gt; on Linux, install OpenBLAS first:&lt;/p&gt;
    &lt;code&gt;# Ubuntu/Debian
sudo apt install libopenblas-dev

# Fedora
sudo dnf install openblas-devel&lt;/code&gt;
    &lt;p&gt;Other targets:&lt;/p&gt;
    &lt;code&gt;make clean      # Clean build artifacts
make info       # Show available backends for this platform
make test       # Run reference image test&lt;/code&gt;
    &lt;p&gt;The model weights are downloaded from HuggingFace:&lt;/p&gt;
    &lt;code&gt;pip install huggingface_hub
python download_model.py&lt;/code&gt;
    &lt;p&gt;This downloads approximately 16GB to &lt;code&gt;./flux-klein-model&lt;/code&gt;:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;VAE (~300MB)&lt;/item&gt;
      &lt;item&gt;Transformer (~4GB)&lt;/item&gt;
      &lt;item&gt;Qwen3-4B Text Encoder (~8GB)&lt;/item&gt;
      &lt;item&gt;Tokenizer&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FLUX.2-klein-4B is a rectified flow transformer optimized for fast inference:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Component&lt;/cell&gt;
        &lt;cell role="head"&gt;Architecture&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Transformer&lt;/cell&gt;
        &lt;cell&gt;5 double blocks + 20 single blocks, 3072 hidden dim, 24 attention heads&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;VAE&lt;/cell&gt;
        &lt;cell&gt;AutoencoderKL, 128 latent channels, 8x spatial compression&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Text Encoder&lt;/cell&gt;
        &lt;cell&gt;Qwen3-4B, 36 layers, 2560 hidden dim&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Inference steps: This is a distilled model that produces good results with exactly 4 sampling steps.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Phase&lt;/cell&gt;
        &lt;cell role="head"&gt;Memory&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Text encoding&lt;/cell&gt;
        &lt;cell&gt;~8GB (encoder weights)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Diffusion&lt;/cell&gt;
        &lt;cell&gt;~8GB (transformer ~4GB + VAE ~300MB + activations)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Peak&lt;/cell&gt;
        &lt;cell&gt;~16GB (if encoder not released)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The text encoder is automatically released after encoding, reducing peak memory during diffusion. If you generate multiple images with different prompts, the encoder reloads automatically.&lt;/p&gt;
    &lt;p&gt;Benchmarks on Apple M3 Max (128GB RAM), generating a 4-step image:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Size&lt;/cell&gt;
        &lt;cell role="head"&gt;C (MPS)&lt;/cell&gt;
        &lt;cell role="head"&gt;C (BLAS)&lt;/cell&gt;
        &lt;cell role="head"&gt;C (Generic)&lt;/cell&gt;
        &lt;cell role="head"&gt;PyTorch (MPS)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;512x512&lt;/cell&gt;
        &lt;cell&gt;49.6s&lt;/cell&gt;
        &lt;cell&gt;51.9s&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;5.4s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;256x256&lt;/cell&gt;
        &lt;cell&gt;32.4s&lt;/cell&gt;
        &lt;cell&gt;29.7s&lt;/cell&gt;
        &lt;cell&gt;-&lt;/cell&gt;
        &lt;cell&gt;3.0s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;64x64&lt;/cell&gt;
        &lt;cell&gt;25.0s&lt;/cell&gt;
        &lt;cell&gt;23.5s&lt;/cell&gt;
        &lt;cell&gt;605.6s&lt;/cell&gt;
        &lt;cell&gt;2.2s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The C implementation uses float32 throughout, while PyTorch uses bfloat16 with highly optimized MPS kernels. The next step of this project is likely to implement such an optimization, in order to reach similar speed, or at least try to approach it.&lt;/item&gt;
      &lt;item&gt;The generic (pure C) backend is extremely slow and only practical for testing at small sizes.&lt;/item&gt;
      &lt;item&gt;Times include text encoding, denoising (4 steps), and VAE decode.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Maximum resolution: 1024x1024 pixels. Higher resolutions require prohibitive memory for the attention mechanisms.&lt;/p&gt;
    &lt;p&gt;Minimum resolution: 64x64 pixels.&lt;/p&gt;
    &lt;p&gt;Dimensions should be multiples of 16 (the VAE downsampling factor).&lt;/p&gt;
    &lt;p&gt;The library can be integrated into your own C/C++ projects. Link against &lt;code&gt;libflux.a&lt;/code&gt; and include &lt;code&gt;flux.h&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Here's a complete program that generates an image from a text prompt:&lt;/p&gt;
    &lt;code&gt;#include "flux.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    /* Load the model. This loads VAE, transformer, and text encoder. */
    flux_ctx *ctx = flux_load_dir("flux-klein-model");
    if (!ctx) {
        fprintf(stderr, "Failed to load model: %s\n", flux_get_error());
        return 1;
    }

    /* Configure generation parameters. Start with defaults and customize. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.width = 512;
    params.height = 512;
    params.seed = 42;  /* Use -1 for random seed */

    /* Generate the image. This handles text encoding, diffusion, and VAE decode. */
    flux_image *img = flux_generate(ctx, "A fluffy orange cat in a sunbeam", &amp;amp;params);
    if (!img) {
        fprintf(stderr, "Generation failed: %s\n", flux_get_error());
        flux_free(ctx);
        return 1;
    }

    /* Save to file. Format is determined by extension (.png or .ppm). */
    flux_image_save(img, "cat.png");
    printf("Saved cat.png (%dx%d)\n", img-&amp;gt;width, img-&amp;gt;height);

    /* Clean up */
    flux_image_free(img);
    flux_free(ctx);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Compile with:&lt;/p&gt;
    &lt;code&gt;gcc -o myapp myapp.c -L. -lflux -lm -framework Accelerate  # macOS
gcc -o myapp myapp.c -L. -lflux -lm -lopenblas              # Linux&lt;/code&gt;
    &lt;p&gt;Transform an existing image guided by a text prompt. The &lt;code&gt;strength&lt;/code&gt; parameter controls how much the image changes:&lt;/p&gt;
    &lt;code&gt;#include "flux.h"
#include &amp;lt;stdio.h&amp;gt;

int main(void) {
    flux_ctx *ctx = flux_load_dir("flux-klein-model");
    if (!ctx) return 1;

    /* Load the input image */
    flux_image *photo = flux_image_load("photo.png");
    if (!photo) {
        fprintf(stderr, "Failed to load image\n");
        flux_free(ctx);
        return 1;
    }

    /* Set up parameters. Output size defaults to input size. */
    flux_params params = FLUX_PARAMS_DEFAULT;
    params.strength = 0.7;  /* 0.0 = no change, 1.0 = full regeneration */
    params.seed = 123;

    /* Transform the image */
    flux_image *painting = flux_img2img(ctx, "oil painting, impressionist style",
                                         photo, &amp;amp;params);
    flux_image_free(photo);  /* Done with input */

    if (!painting) {
        fprintf(stderr, "Transformation failed: %s\n", flux_get_error());
        flux_free(ctx);
        return 1;
    }

    flux_image_save(painting, "painting.png");
    printf("Saved painting.png\n");

    flux_image_free(painting);
    flux_free(ctx);
    return 0;
}&lt;/code&gt;
    &lt;p&gt;Strength values:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;0.3&lt;/code&gt;- Subtle style transfer, preserves most details&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.5&lt;/code&gt;- Moderate transformation&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.7&lt;/code&gt;- Strong transformation, good for style transfer&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;0.9&lt;/code&gt;- Almost complete regeneration, keeps only composition&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When generating multiple images with different seeds but the same prompt, you can avoid reloading the text encoder:&lt;/p&gt;
    &lt;code&gt;flux_ctx *ctx = flux_load_dir("flux-klein-model");
flux_params params = FLUX_PARAMS_DEFAULT;
params.width = 256;
params.height = 256;

/* Generate 5 variations with different seeds */
for (int i = 0; i &amp;lt; 5; i++) {
    flux_set_seed(1000 + i);

    flux_image *img = flux_generate(ctx, "A mountain landscape at sunset", &amp;amp;params);

    char filename[64];
    snprintf(filename, sizeof(filename), "landscape_%d.png", i);
    flux_image_save(img, filename);
    flux_image_free(img);
}

flux_free(ctx);&lt;/code&gt;
    &lt;p&gt;Note: The text encoder (~8GB) is automatically released after the first generation to save memory. It reloads automatically if you use a different prompt.&lt;/p&gt;
    &lt;p&gt;All functions that can fail return NULL on error. Use &lt;code&gt;flux_get_error()&lt;/code&gt; to get a description:&lt;/p&gt;
    &lt;code&gt;flux_ctx *ctx = flux_load_dir("nonexistent-model");
if (!ctx) {
    fprintf(stderr, "Error: %s\n", flux_get_error());
    /* Prints something like: "Failed to load VAE - cannot generate images" */
    return 1;
}&lt;/code&gt;
    &lt;p&gt;Core functions:&lt;/p&gt;
    &lt;code&gt;flux_ctx *flux_load_dir(const char *model_dir);   /* Load model, returns NULL on error */
void flux_free(flux_ctx *ctx);                     /* Free all resources */

flux_image *flux_generate(flux_ctx *ctx, const char *prompt, const flux_params *params);
flux_image *flux_img2img(flux_ctx *ctx, const char *prompt, const flux_image *input,
                          const flux_params *params);&lt;/code&gt;
    &lt;p&gt;Image handling:&lt;/p&gt;
    &lt;code&gt;flux_image *flux_image_load(const char *path);     /* Load PNG or PPM */
int flux_image_save(const flux_image *img, const char *path);  /* 0=success, -1=error */
flux_image *flux_image_resize(const flux_image *img, int new_w, int new_h);
void flux_image_free(flux_image *img);&lt;/code&gt;
    &lt;p&gt;Utilities:&lt;/p&gt;
    &lt;code&gt;void flux_set_seed(int64_t seed);                  /* Set RNG seed for reproducibility */
const char *flux_get_error(void);                  /* Get last error message */
void flux_release_text_encoder(flux_ctx *ctx);     /* Manually free ~8GB (optional) */&lt;/code&gt;
    &lt;code&gt;typedef struct {
    int width;              /* Output width in pixels (default: 256) */
    int height;             /* Output height in pixels (default: 256) */
    int num_steps;          /* Denoising steps, use 4 for klein (default: 4) */
    float guidance_scale;   /* CFG scale, use 1.0 for klein (default: 1.0) */
    int64_t seed;           /* Random seed, -1 for random (default: -1) */
    float strength;         /* img2img only: 0.0-1.0 (default: 0.75) */
} flux_params;

/* Initialize with sensible defaults */
#define FLUX_PARAMS_DEFAULT { 256, 256, 4, 1.0f, -1, 0.75f }&lt;/code&gt;
    &lt;p&gt;MIT&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46670279</guid><pubDate>Sun, 18 Jan 2026 18:01:58 +0000</pubDate></item><item><title>Dead Internet Theory</title><link>https://kudmitry.com/articles/dead-internet-theory/</link><description>&lt;doc fingerprint="32e0135d28e5b5bd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Dead Internet Theory&lt;/head&gt;
    &lt;p&gt;The other day I was browsing my one-and-only social network — which is not a social network, but I’m tired of arguing with people online about it — HackerNews. It’s like this dark corner of the internet, where anonymous tech-enthusiasts, scientists, entrepreneurs, and internet-trolls, like to lurk. I like HackerNews. It helps me stay up-to-date about recent tech news (like Cloudflare acquiring Astro which makes me happy for the Astro team, but also sad and worried since I really like Astro, and big-tech has a tendency to ruin things); it mostly avoids politics; and it’s not a social network.&lt;/p&gt;
    &lt;p&gt;And, in the fashion of HackerNews, I stumbled upon someone sharing their open-source project. It’s great to see people work on their projects and decide to show them to the world. I think people underestimate the fear of actually shipping stuff, which involves sharing it with the world.&lt;/p&gt;
    &lt;p&gt;Upon glancing at the comment section, I started to see other anonymous participants questioning the validity of said open-source project in terms of how much of it was AI-generated. I grabbed my popcorn, and started to follow this thread. More accusations started to appear: the commit timeline does not make sense; the code has AI-generated comments; etc. And at the same time, the author tried to reply to every comment claiming that they wrote this 100% without using AI.&lt;/p&gt;
    &lt;p&gt;I don’t mind people using AI to write code, even though I tried to resist it myself, until eventually succumbing to it. But I think it’s fair to disclose the use of AI, especially in open-source software. People on the internet are, mostly, anonymous, and it’s not always possible to verify the claims or expertise of particular individuals. But as the amount of code is growing, considering that everyone is using AI to generate whatever-app they want, it’s impossible to verify every piece of code we are going to use. So it’s fair to know, I think, if some project is AI generated and to what extent. In the end, LLMs are just probabilistic next-token generators. And while they are getting extremely good at most simple tasks, they have the potential to wreak havoc with harder problems or edge-cases (especially if there are no experienced engineers, with domain knowledge, to review the generated code).&lt;/p&gt;
    &lt;p&gt;As I was following this thread, I stared to see a pattern: the comments of the author looked AI generated too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;The use of em-dashes, which on most keyboard require a special key-combination that most people don’t know, and while in markdown two dashes will render as em-dash, this is not true of HackerNews (hence, you often see &lt;code&gt;--&lt;/code&gt;in HackerNews comments, where the author is probably used to Markdown renderer turning it into em-dash)&lt;/item&gt;
      &lt;item&gt;The notorious “you are absolutely right”, which no living human ever used before, at least not that I know of&lt;/item&gt;
      &lt;item&gt;The other notorious “let me know if you want to [do that thing] or [explore this other thing]” at the end of the sentence&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I was sitting there, refreshing the page, seeing the author being confronted with use of AI in both their code and their comments, while the author claiming to have not used AI at all. Honestly, I was thinking I was going insane. Am I wrong to suspect them? What if people DO USE em-dashes in real life? What if English is not their native language and in their native language it’s fine to use phrases like “you are absolutely right”? Is this even a real person? Are the people who are commenting real?&lt;/p&gt;
    &lt;p&gt;And then it hit me. We have reached the Dead Internet. The Dead Internet Theory claims that since around 2016 (a whooping 10 years already), the internet is mainly dead, i.e. most interactions are between bots, and most content is machine generated to either sell you stuff, or game the SEO game (in order to sell you stuff).&lt;/p&gt;
    &lt;p&gt;I’m &lt;del&gt;ashamed&lt;/del&gt; proud to say that I spent a good portion of my teenage years on the internet, chatting and learning from real people who knew more than me. Back in the early 2000s, there were barely bots on the internet. The average non-tech human didn’t know anything about phpBB forums, and the weird people with pseudonyms who hanged-out in there. I spent countless hours inside IRC channels, and on phpBB forums, learning things like network programming, OS-development, game-development, and of course web-development (which became my profession for almost two decades now). I’m basically a graduate of the Internet University. Back then, nobody had doubts that they were talking to a human-being. Sure, you could think that you spoke to a hot girl, who in reality was a fat guy, but hey, at least they were real!&lt;/p&gt;
    &lt;p&gt;But today, I no longer know what is real. I saw a picture on LinkedIn, from a real tech company, posting about their “office vibes” and their happy employees. And then I went to the comment section, and sure enough this picture is AI generated (mangled text that does not make sense, weird hand artifacts). It was posted by an employee of the company, it showed other employees of said company, and it was altered with AI to showcase a different reality. Hell, maybe the people on the picture do not even exist!&lt;/p&gt;
    &lt;p&gt;And these are mild examples. I don’t use social networks (and no, HackerNews is not a social network), but I hear horror stories about AI generated content on Facebook, Xitter, TikTok, ranging from photos of giants that built the pyramids in Egypt, all the way to short videos of pretty girls saying that the EU is bad for Poland.&lt;/p&gt;
    &lt;p&gt;I honestly got sad that day. Hopeless, if I could say. AI is easily available to the masses, which allow them to generate shitload of AI-slop. People no longer need to write comments or code, they can just feed this to AI agents who will generate the next “you are absolutely right” masterpiece.&lt;/p&gt;
    &lt;p&gt;I like technology. I like software engineering, and the concept of the internet where people could share knowledge and create communities. Were there malicious actors back then on the internet? For sure. But what I am seeing today, makes me question whether the future we are headed to is a future where technology is useful anymore. Or, rather, it’s a future where bots talk with bots, and human knowledge just gets recycled and repackaged into “10 step to fix your [daily problem] you are having” for the sake of selling you more stuff.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46671731</guid><pubDate>Sun, 18 Jan 2026 20:19:07 +0000</pubDate></item><item><title>Show HN: Dock – Slack minus the bloat, tax, and 90-day memory loss</title><link>https://getdock.io/</link><description>&lt;doc fingerprint="2c22189aa3f9fafa"&gt;
  &lt;main&gt;&lt;p&gt; Your time. Your decisions. Your sanity.&lt;lb/&gt; Team chat that just works. &lt;/p&gt;&lt;code&gt;main&lt;/code&gt;?&lt;p&gt;Async messages for deep work. Real-time chat when it matters. Work across timezones without the noise.&lt;/p&gt;&lt;p&gt;Decisions get lost in chat. Not here. One click to mark, instant recall months later from your Decisions inbox.&lt;/p&gt;&lt;p&gt;SOC 2 compliant infrastructure. Your data encrypted in transit and at rest. One-click data import/export. Your data stays yours.&lt;lb/&gt;Learn more →&lt;/p&gt;&lt;p&gt;The feature Slack charges $8.75/user for? Free on Dock.&lt;/p&gt;&lt;p&gt;Find any message, from any time. No 90-day cutoff. No upgrade required. Your entire team history, always at your fingertips — completely free.&lt;/p&gt;&lt;p&gt;No learning curve. No feature overwhelm. Create a channel, invite your team, start talking. It's chat — we didn't reinvent it.&lt;/p&gt;&lt;p&gt;90-day message limit. Hidden AI costs. Features you never asked for.&lt;/p&gt;&lt;p&gt;Just to search your old messages. The "Pro" tax for basic functionality.&lt;/p&gt;&lt;p&gt;Search everything. Forever. No 90-day limit. No upgrade wall. Just free.&lt;/p&gt;&lt;p&gt;You're paying for Slack AI whether you use it or not. It's in the price.&lt;/p&gt;&lt;p&gt;We don't bundle AI you didn't ask for. Chat is chat. Pay for what you use.&lt;/p&gt;&lt;p&gt;Workflows, canvases, clips, huddles, lists... When did chat get this complicated?&lt;/p&gt;&lt;p&gt;Channels. DMs. Threads. Files. That's it. Fast, focused, done.&lt;/p&gt;&lt;p&gt;No surprises when your team grows. One price for your whole team.&lt;/p&gt;&lt;p&gt;100+ members? Contact us&lt;/p&gt;&lt;p&gt;Have questions? Read our FAQ →&lt;/p&gt;&lt;p&gt;Join the waitlist. Team chat that respects your time and budget.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46671952</guid><pubDate>Sun, 18 Jan 2026 20:42:49 +0000</pubDate></item><item><title>Police Invested Millions in Shadowy Phone-Tracking Software Won't Say How Used</title><link>https://www.texasobserver.org/texas-police-invest-tangles-sheriff-surveillance/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46672150</guid><pubDate>Sun, 18 Jan 2026 21:05:14 +0000</pubDate></item><item><title>Show HN: Beats, a web-based drum machine</title><link>https://beats.lasagna.pizza</link><description>&lt;doc fingerprint="32e55a1cb8f697b3"&gt;
  &lt;main&gt;
    &lt;p&gt;Share your beat with this URL:&lt;/p&gt;
    &lt;p&gt;BEATS&lt;/p&gt;
    &lt;p&gt;A web-based drum machine inspired by the Teenage Engineering Pocket Operators.&lt;/p&gt;
    &lt;p&gt;CREDITS:&lt;/p&gt;
    &lt;p&gt;• Wrote by @kinduff&lt;/p&gt;
    &lt;p&gt;• Built with Tone.js and Stimulus.js&lt;/p&gt;
    &lt;p&gt;• With the awesome VT323 font&lt;/p&gt;
    &lt;p&gt;THANKS TO:&lt;/p&gt;
    &lt;p&gt;• andiam03 for transposing patterns and inspiring!&lt;/p&gt;
    &lt;p&gt;• ethanhein for the original idea!&lt;/p&gt;
    &lt;p&gt;• all beta reviewers!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46672181</guid><pubDate>Sun, 18 Jan 2026 21:10:08 +0000</pubDate></item><item><title>High-speed train collision in Spain kills at least 21</title><link>https://www.bbc.com/news/articles/cedw6ylpynyo</link><description>&lt;doc fingerprint="66f111daae52b2fd"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;High-speed train collision in Spain kills at least 39&lt;/head&gt;
    &lt;p&gt;At least 39 people have died in a train collision in southern Spain and dozens more have been injured in the country's worst rail crash in more than a decade, according to Spain's Civil Guard.&lt;/p&gt;
    &lt;p&gt;Carriages on a Madrid-bound train derailed and crossed over to the opposite tracks, colliding with an oncoming train in Adamuz, near the city of Córdoba.&lt;/p&gt;
    &lt;p&gt;Four hundred passengers and staff were onboard both trains, the rail networks said. At least 73 people were taken to hospital - 24 of them seriously injured, including four children - according to Andalusia's emergency services.&lt;/p&gt;
    &lt;p&gt;Spanish Transport Minister Óscar Puente described the incident as "extremely strange" as officials launched an investigation.&lt;/p&gt;
    &lt;p&gt;All the railway experts consulted by the government "are extremely baffled by the accident", Puente told reporters in Madrid.&lt;/p&gt;
    &lt;p&gt;Rail network operator Adif said the collision happened at 19:45 local time (18:45 GMT), about an hour after the train left Málaga heading to Madrid, when it derailed on a straight stretch of track.&lt;/p&gt;
    &lt;p&gt;The force of the crash pushed the carriages of the second train into an embankment, Puente said. He added that most of those killed and injured were in the front carriages of the second train, which was travelling from Madrid to Huelva.&lt;/p&gt;
    &lt;p&gt;The type of train involved in the crash was a Freccia 1000, which can reach top speeds of 400 km/h (250 mph), a spokesperson for the Italian rail company Ferrovie dello Stato told the Reuters news agency.&lt;/p&gt;
    &lt;p&gt;Rescue teams said the twisted wreckage of the trains made it difficult to recover people trapped inside the carriages.&lt;/p&gt;
    &lt;p&gt;Córdoba fire chief Francisco Carmona told Spanish public broadcaster RTVE: "We have even had to remove a dead person to be able to reach someone alive. It is hard, tricky work."&lt;/p&gt;
    &lt;p&gt;Salvador Jimenez, a journalist with RTVE who was on one of the trains, said the impact felt like an "earthquake".&lt;/p&gt;
    &lt;p&gt;"I was in the first carriage. There was a moment when it felt like an earthquake and the train had indeed derailed," Jimenez said.&lt;/p&gt;
    &lt;p&gt;Footage from the scene appears to show some train carriages had tipped over on their sides. Rescue workers can be seen scaling the train to pull people out of the lopsided train doors and windows.&lt;/p&gt;
    &lt;p&gt;A Madrid-bound passenger, José, told public broadcaster Canal Sur: "There were people and screaming, calling for doctors."&lt;/p&gt;
    &lt;p&gt;All rail services between Madrid and Andalusia were suspended following the accident and are expected to remain closed all day on Monday.&lt;/p&gt;
    &lt;p&gt;Iryo, a private rail company that operated the journey from Málaga, said around 300 passengers were on board the train that first derailed, while the other train – operated by the state-funded firm Renfe – had around 100 passengers.&lt;/p&gt;
    &lt;p&gt;The official cause is not yet known. An investigation is not expected to determine what happened for at least a month, according to the transport minister.&lt;/p&gt;
    &lt;p&gt;Spain's Prime Minister, Pedro Sánchez, said the country will endure a "night of deep pain".&lt;/p&gt;
    &lt;p&gt;The mayor of Adamuz, Rafael Moreno, was one of the first people on the scene of the accident, describing it as "a nightmare".&lt;/p&gt;
    &lt;p&gt;King Felipe VI and Queen Letizia said they were following news of the disaster "with great concern".&lt;/p&gt;
    &lt;p&gt;"We extend our most heartfelt condolences to the relatives and loved ones of the dead, as well as our love and wishes for a swift recovery to the injured," the royal palace said on X.&lt;/p&gt;
    &lt;p&gt;The emergency agency in the region of Andalusia urged any crash survivors to contact their families or post on social media that they are alive.&lt;/p&gt;
    &lt;p&gt;Advanced medical posts were set up for impacted passengers to be treated for injuries and transferred to hospital. Adif said it set up spaces for relatives of the victims at Atocha, Seville, Córdoba, Málaga and Huelva stations.&lt;/p&gt;
    &lt;p&gt;The Spanish Red Cross has deployed emergency support services to the scene, while also offering counselling to families nearby.&lt;/p&gt;
    &lt;p&gt;Miguel Ángel Rodríguez from the Red Cross told RNE radio: "The families are going through a situation of great anxiety due to the lack of information. These are very distressing moments."&lt;/p&gt;
    &lt;p&gt;French President Emmanuel Macron, Italian Prime Minister Giorgia Meloni and European Commission chief Ursula von der Leyen have published statements offering condolences.&lt;/p&gt;
    &lt;p&gt;"My thoughts are with the victims, their families and the entire Spanish people. France stands by your side," Macron wrote on social media.&lt;/p&gt;
    &lt;p&gt;In 2013, Spain suffered its worst high-speed train derailment in Galicia, north-west Spain, which left 80 people dead and 140 others injured.&lt;/p&gt;
    &lt;p&gt;Spain's high-speed rail network is the second largest in the world, behind China, connecting more than 50 cities across the country. Adif data shows the Spanish rail is more than 4,000km long (2,485 miles).&lt;/p&gt;
    &lt;p&gt;Get our flagship newsletter with all the headlines you need to start the day. Sign up here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46673453</guid><pubDate>Sun, 18 Jan 2026 23:54:43 +0000</pubDate></item><item><title>Show HN: I quit coding years ago. AI brought me back</title><link>https://calquio.com/finance/compound-interest</link><description>&lt;doc fingerprint="72dbddfd7c3dba78"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Compound Interest Calculator&lt;/head&gt;
    &lt;p&gt;Calculate how your investments grow over time with compound interest.&lt;/p&gt;
    &lt;head rend="h3"&gt;How much are you investing?&lt;/head&gt;
    &lt;head rend="h3"&gt;What return do you expect?&lt;/head&gt;
    &lt;head rend="h3"&gt;How long will you invest?&lt;/head&gt;
    &lt;head rend="h2"&gt;You May Also Like&lt;/head&gt;
    &lt;head rend="h2"&gt;Related Articles&lt;/head&gt;
    &lt;head rend="h3"&gt;Beyond the Nest Egg: Finding Your Financial 'Crossover Point' with Compound Interest&lt;/head&gt;
    &lt;p&gt;Discover the Crossover Point: the milestone where interest earnings exceed your contributions. A guide to compound interest for late-start investors.&lt;/p&gt;
    &lt;head rend="h3"&gt;The 'Wait Tax': Quantifying the Exact Cost of Delaying Your Investments&lt;/head&gt;
    &lt;p&gt;Stop waiting for the 'perfect time' to invest. Learn how to calculate your 'Wait Tax'—the massive financial penalty of delaying your portfolio by just 12–24 months.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Compound Interest?&lt;/head&gt;
    &lt;p&gt;Compound interest is interest calculated on both the initial principal and the accumulated interest from previous periods. Unlike simple interest, which only earns interest on the original amount, compound interest allows your money to grow exponentially over time.&lt;/p&gt;
    &lt;p&gt;Albert Einstein reportedly called compound interest "the eighth wonder of the world," saying: "He who understands it, earns it; he who doesn't, pays it."&lt;/p&gt;
    &lt;head rend="h2"&gt;The Compound Interest Formula&lt;/head&gt;
    &lt;p&gt;The basic formula for compound interest is:&lt;/p&gt;
    &lt;p&gt;Where:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A = Final amount (principal + interest)&lt;/item&gt;
      &lt;item&gt;P = Principal (initial investment)&lt;/item&gt;
      &lt;item&gt;r = Annual interest rate (as a decimal)&lt;/item&gt;
      &lt;item&gt;n = Number of times interest compounds per year&lt;/item&gt;
      &lt;item&gt;t = Time in years&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For continuous compounding, the formula becomes:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Rule of 72&lt;/head&gt;
    &lt;p&gt;A quick mental math trick to estimate how long it takes to double your money:&lt;/p&gt;
    &lt;p&gt;For example:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;At 6% interest: 72 ÷ 6 = 12 years to double&lt;/item&gt;
      &lt;item&gt;At 8% interest: 72 ÷ 8 = 9 years to double&lt;/item&gt;
      &lt;item&gt;At 12% interest: 72 ÷ 12 = 6 years to double&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Rule of 72 is a quick approximation. For more precise calculations, use the formula above or our calculator!&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Compound Frequency Matters&lt;/head&gt;
    &lt;p&gt;The more frequently interest compounds, the more you earn. Think of it as: how often the bank calculates and adds interest to your balance.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Annual compounding: Interest added once per year&lt;/item&gt;
      &lt;item&gt;Monthly compounding: Interest added 12 times per year&lt;/item&gt;
      &lt;item&gt;Daily compounding: Interest added 365 times per year&lt;/item&gt;
      &lt;item&gt;Continuous compounding: Interest added infinitely (theoretical maximum)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;At a 10% annual rate on $10,000 over 10 years:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Annual compounding: $25,937&lt;/item&gt;
      &lt;item&gt;Monthly compounding: $27,070&lt;/item&gt;
      &lt;item&gt;Daily compounding: $27,179&lt;/item&gt;
      &lt;item&gt;Continuous compounding: $27,183&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Tips for Maximizing Compound Interest&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Start early – Time is your greatest ally. Even small amounts grow significantly over decades.&lt;/item&gt;
      &lt;item&gt;Be consistent – Regular contributions amplify the effect of compounding.&lt;/item&gt;
      &lt;item&gt;Reinvest returns – Don't withdraw interest; let it compound.&lt;/item&gt;
      &lt;item&gt;Seek higher rates – Even a 1% difference compounds to significant amounts over time.&lt;/item&gt;
      &lt;item&gt;Minimize fees – High fees erode your compounding gains.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46673809</guid><pubDate>Mon, 19 Jan 2026 00:50:20 +0000</pubDate></item><item><title>The Code-Only Agent</title><link>https://rijnard.com/blog/the-code-only-agent</link><description>&lt;doc fingerprint="540ea85569ca9a17"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;The Code-Only Agent&lt;/head&gt;&lt;p&gt;When Code Execution Really is All You Need&lt;/p&gt;&lt;p&gt;If you're building an agent, you're probably overwhelmed. Tools. MCP. Subagents. Skills. The ecosystem pushes you toward complexity, toward "the right way" to do things. You should know: Concepts like "Skills" and "MCP" are actually outcomes of an ongoing learning process of humans figuring stuff out. The space is wide open for exploration. With this mindset I wanted to try something different. Simplify the assumptions.&lt;/p&gt;&lt;p&gt; What if the agent only had &lt;code&gt;one tool&lt;/code&gt;? Not just any tool, but the most powerful one. The
            &lt;code&gt;Turing-complete&lt;/code&gt; one: execute code.
          &lt;/p&gt;&lt;p&gt; Truly one tool means: no `bash`, no `ls`, no `grep`. Only &lt;code&gt;execute_code&lt;/code&gt;. And you enforce it.
          &lt;/p&gt;&lt;p&gt;When you watch an agent run, you might think: "I wonder what tools it'll use to figure this out. Oh look, it ran `ls`. That makes sense. Next, `grep`. Cool."&lt;/p&gt;&lt;p&gt;The simpler Code-Only paradigm makes that question irrelevant. The question shifts from "what tools?" to "what code will it produce?" And that's when things get interesting.&lt;/p&gt;&lt;head rend="h2"&gt;&lt;code&gt;execute_code&lt;/code&gt;: One Tool to Rule Them All&lt;/head&gt;&lt;p&gt;Traditional prompting works like this:&lt;/p&gt;&lt;p&gt; &amp;gt; Agent, do thing &lt;lb/&gt; &amp;gt; Agent responds with thing &lt;/p&gt;&lt;p&gt;Contrast with:&lt;/p&gt;&lt;p&gt; &amp;gt; Agent, do thing &lt;lb/&gt; &amp;gt; Agent creates and runs code to do thing &lt;/p&gt;&lt;p&gt; It does this every time. No, really, &lt;code&gt;every&lt;/code&gt;
            time. Pick a runtime for our Code-Only agent, say Python. It needs
            to find a file? It writes Python code to find the file and executes
            the code. Maybe it runs rglob. Maybe it does os.walk.
          &lt;/p&gt;&lt;p&gt;It needs to create a script that crawls a website? It doesn't write the script to your filesystem (reminder: there's no create_file tool to do that!). It writes code to output a script that crawls a website.1&lt;/p&gt;&lt;p&gt;We make it so that there is literally no way for the agent to do anything productive without writing code.&lt;/p&gt;&lt;p&gt;So what? Why do this? You're probably thinking, how is this useful? Just give it `bash` tool already man.&lt;/p&gt;&lt;p&gt;Let's think a bit more deeply what's happening. Traditional agents respond with something. Tell it to find some DNA pattern across 100 files. It might `ls` and `grep`, it might do that in some nondeterministic order, it'll figure out an answer and maybe you continue interacting because it missed a directory or you added more files. After some time, you end up with a conversation of tool calls, responses, and an answer.&lt;/p&gt;&lt;p&gt; At some point the agent might even write a Python script to do this DNA pattern finding. That would be a lucky happy path, because we could rerun that script or update it later... Wait, that's handy... actually, more than handy... isn't that &lt;code&gt;ideal&lt;/code&gt;? Wouldn't it be better if we told it to write a script at the
            start? You see, the Code-Only agent doesn't need to be told to write
            a script. It
            &lt;code&gt;has&lt;/code&gt;
            to, because that's literally the only way for it to do anything of
            substance.
          &lt;/p&gt;&lt;p&gt;The Code-Only agent produces something more precise than an answer in natural language. It produces a code witness of an answer. The answer is the output from running the code. The agent can interpret that output in natural language (or by writing code), but the "work" is codified in a very literal sense. The Code-Only agent doesn't respond with something. It produces a code witness that outputs something.&lt;/p&gt;&lt;p&gt;Try ❯❯ Code-Only plugin for Claude Code&lt;/p&gt;&lt;head rend="h2"&gt;Code witnesses are semantic guarantees&lt;/head&gt;&lt;p&gt;Let's follow the consequences. The code witness must abide by certain rules: The rules imposed by the language runtime semantics (e.g., of Python). That's not a "next token" process. That's not a "LLM figures out sequence of tool calls, no that's not what I wanted". It's piece of code. A piece of code! Our one-tool agent has a wonderful property: It went through latent space to produce something that has a defined semantics, repeatably runnable, and imminently comprehensible (for humans or agents alike to reason about). This is nondeterministic LLM token-generation projected into the space of Turing-complete code, an executable description of behavior as we best understand it.&lt;/p&gt;&lt;p&gt; Is a Code-Only agent really enough, or too extreme? I'll be frank: I pursued this extreme after two things (1) inspiration from articles in Further Reading below (2) being annoyed at agents for not comprehensively and exhaustively analyzing 1000s of files on my laptop. They would skip, take shortcuts, hallucinate. I knew how to solve part of that problem: create a &lt;code&gt;programmatic&lt;/code&gt;
            loop and try have fresh instances/prompts to do the work
            comprehensively. I can rely on the semantics of a loop written in
            Python. Take this idea further, and you realize that for anything
            long-running and computable (e.g., bash or some tool), you actually
            want the real McCoy: the full witness of code, a trace of why things
            work or don't work. The Code-Only agent
            &lt;code&gt;enforces&lt;/code&gt;
            that principle.
          &lt;/p&gt;&lt;p&gt; Code-Only agents are not too extreme. I think they're the only way forward for computable things. If you're writing travel blog posts, you accept the LLMs answer (and you don't need to run tools for that). When something is computable though, Code-Only is the only path to a &lt;code&gt;fully trustworthy&lt;/code&gt;
            way to make progress where you need guarantees (subject to
            the semantics that your language of choice guarantees, of course). When I say
            guarantees, I mean that in the looser sense, and also in a
            Formal
            sense. Which beckons: What happens when we use a language like
            Lean with some of the
            strongest guarantees? Did we not observe that
            programs are proofs?
          &lt;/p&gt;&lt;p&gt;This lens says the Code-Only agent is a producer of proofs, witnesses of computational behavior in the world of proofs-as-programs. An LLM in a loop forced to produce proofs, run proofs, interpret proof results. That's all.&lt;/p&gt;&lt;head rend="h2"&gt;Going Code-Only&lt;/head&gt;&lt;p&gt;So you want to go Code-Only. What happens? The paradigm is simple, but the design choices are surprising.&lt;/p&gt;&lt;p&gt;First, the harness. The LLM's output is code, and you execute that code. What should be communicated back? Exit code makes sense. What about output? What if the output is very large? Since you're running code, you can specify the result type that running the code should return.&lt;/p&gt;&lt;p&gt;I've personally, e.g., had the tool return results directly if under a certain threshold (1K bytes). This would go into the session context. Alternatively, write the results to a JSON file on disk if it exceeds the threshold. This avoids context blowup and the result tells the agent about the output file path written to disk. How best to pass results, persist them, and optimize for size and context fill are open questions. You also want to define a way to deal with `stdout` and `stderr`: Do you expose these to the agent? Do you summarize before exposing?&lt;/p&gt;&lt;p&gt;Next, enforcement. Let's say you're using Claude Code. It's not enough to persuade it to always create and run code. It turns out it's surprisingly twisty to force Claude Code into a single tool (maybe support for this will improve). The best plugin-based solution I found is a tool PreHook that catches banned tool uses. This wastes some iterations when Claude Code tries to use a tool that's not allowed, but it learns to stop attempting filesystem reads/writes. An initial prompt helps direct.&lt;/p&gt;&lt;p&gt;Next, the language runtime. Python, TypeScript, Rust, Bash. Any language capable of being executed is fair game, but you'll need to think through whether it works for your domain. Dynamic languages like Python are interesting because you can run code natively in the agent's own runtime, rather than through subprocess calls. Likewise TypeScript/JS can be injected into TypeScript-based agents (see Further Reading).&lt;/p&gt;&lt;p&gt;Once you get into the Code-Only mindset, you'll see the potential for composition and reuse. Claude Skills define reusable processes in natural language. What's the equivalent for a Code-Only agent? I'm not sure a Skills equivalent exists yet, but I anticipate it will take shape soon: code as building blocks for specific domains where Code-Only agents compose programmatic patterns. How is that different from calling APIs? APIs form part of the reusable blocks, but their composition (loops, parallelism, asynchrony) is what a Code-Only agent generates.&lt;/p&gt;&lt;p&gt;What about heterogeneous languages and runtimes for our `execute_tool`? I don't think we've thought that far yet.&lt;/p&gt;&lt;head rend="h2"&gt;Further Reading&lt;/head&gt;&lt;p&gt;The agent landscape is quickly evolving. My thoughts on how the Code-Only paradigm fits into inspiring articles and trends, from most recent and going back:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;prose.md (Jan 2026) — Code-Only reduces prompts to executable code (with loops and statement sequences). Prose expands prompts into natural language with program-like constructs (also loops, sequences, parallelism). The interplay of natural language for agent orchestration and rigid semantics for agent execution could be extremely powerful.&lt;/item&gt;&lt;item&gt;Welcome to Gas Town (Jan 2026) — Agent orchestration gone berserk. Tool running is the low-level operation at the bottom of the agent stack. Code-Only fits as the primitive: no matter how many agents you orchestrate, each one reduces to generating and executing code.&lt;/item&gt;&lt;item&gt;Anthropic Code Execution with MCP article (Nov 2025) — MCP-centric view of exposing MCP servers as code API and not tool calls. Code-Only is simpler and more general. It doesn't care about MCP, and casting the MCP interface as an API is a mechanical necessity that acknowledges the power of going Code-Only.&lt;/item&gt;&lt;item&gt;Anthropic Agent Skills article (Oct 2025) — Skills embody reusable processes framed in natural language. They can generate and run code, but that's not their only purpose. Code-Only is narrower (but computationally all-powerful): the reusable unit is always executable. The analog to Skills manifests as pluggable executable pieces: functions, loops, composable routines over APIs.&lt;/item&gt;&lt;item&gt;Cloudflare Code Mode article (Sep 2025) — Possibly the earliest concrete single-code-tool implementation. Code Mode converts MCP tools into a TypeScript API and gives the agent one tool: execute TypeScript. Their insight is pragmatic: LLMs write better code than tool calls because of training data. In its most general sense, going Code-Only doesn't need to rely on MCP or APIs, and encapsulates all code execution concerns.&lt;/item&gt;&lt;item&gt;Ralph Wiggum as a "software engineer" (Jul 2025) — A programmatic loop over agents (agent orchestration). Huntley describes it as "deterministically bad in a nondeterministic world". Code-Only inverts this a bit: projection of a nondeterministic model into deterministic execution. Agent orchestration on top of an agent's Code-Only inner-loop could be a powerful combination.&lt;/item&gt;&lt;item&gt;Tools: Code is All You Need (Jul 2025) — Raises code as a first-order concern for agents. Ronacher's observation: asking an LLM to write a script to transform markdown makes it possible to reason about and trust the process. The script is reviewable, repeatable, composable. Code-Only takes this further where every action becomes a script you can reason about.&lt;/item&gt;&lt;item&gt;How to Build an Agent (Apr 2025) — The cleanest way to achieve a Code-Only agent today may be to build it from scratch. Tweaking current agents like Claude Code to enforce a single tool means friction. Thorsten's article is a lucid account for building an agent loop with tool calls. If you want to enforce Code-Only, this makes it easy to do it yourself.&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;What's Next&lt;/head&gt;&lt;p&gt;Two directions feel inevitable. First, agent orchestration. Tools like prose.md let you compose agents in natural language with program-like constructs. What happens when those agents are Code-Only in their inner loop? You get natural language for coordination, rigid semantics for execution. The best of both.&lt;/p&gt;&lt;p&gt;Second, hybrid tooling. Skills work well for processes that live in natural language. Code-Only works well for processes that need guarantees. We'll see agents that fluidly mix both: Skills for orchestration and intent, Code-Only for computation and precision. The line between "prompting an agent" and "programming an agent" will blur until it disappears.&lt;/p&gt;&lt;p&gt;Try ❯❯ Code-Only plugin for Claude Code&lt;/p&gt;&lt;p&gt;1There is something beautifully quine-like about this agent. I've always loved quines.&lt;/p&gt;Timestamped 9 Jan 2026&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46674416</guid><pubDate>Mon, 19 Jan 2026 02:27:07 +0000</pubDate></item><item><title>My thoughts on Gas Town after 10k hours of Claude Code</title><link>https://simonhartcher.com/posts/2026-01-19-my-thoughts-on-gas-town-after-10000-hours-of-claude-code/</link><description>&lt;doc fingerprint="3e005b1a3bb7a76f"&gt;
  &lt;main&gt;
    &lt;p&gt;I know that there are not 10,000 hours in a year. I’ve been living inside Claude Code and it feels like a lifetime.&lt;/p&gt;
    &lt;p&gt;Over the weekend, I had a play with Steve Yegge’s Gas Town. If you don’t know what that is, you should go read this first. I’ve already encountered people in the community who are treating this as the second coming. Steve said it himself that it’s not for everyone, and he’s right.&lt;/p&gt;
    &lt;head rend="h2"&gt;My time with Claude Code&lt;/head&gt;
    &lt;p&gt;After almost a year since Claude Code was released, I have been using it daily as frequently as the limits allow. With nights and weekends, I surmise that I have spent over 10k hours with it. I recently upgraded from the $100 to $200 tier of Claude Max. My workflow, which I have touched on before, is focused on pair programming. Whether I am the driver or the observer can vary by the task. Ultimately I feel as though I have more agency when I work this way, and it is way more fun (for me) than letting a suite of agents do all the work.&lt;/p&gt;
    &lt;head rend="h2"&gt;Agents take my agency away&lt;/head&gt;
    &lt;p&gt;That’s where my issue with Gas Town lies. It feels like deferring everything to agents, and I get almost no visibility of what’s going on other than work was completed. Don’t get me wrong, I understand that I can ask the mayor to explain what was done, or link me to the code in question, though it feels very cumbersome.&lt;/p&gt;
    &lt;p&gt;Additionally, with the token speed of current agents (Claude Opus 4.5 in my case), unless you have many rigs operating at once, the whole process just seems really slow. Since I like to be more involved, it just feels like I’m sitting there forever waiting for things to happen.&lt;/p&gt;
    &lt;head rend="h2"&gt;A small note on beads&lt;/head&gt;
    &lt;p&gt;Prior to Gas Town’s release, I started using beads. Beads is at the heart of Gas Town. It’s what &lt;code&gt;gt&lt;/code&gt; (the Gas Town CLI) uses to keep track of what needs to be done, and what has been done, across all of its agents. I do really like beads, but I still want to look at pull-requests. Since beads stores all of its state in git, those changes pollute every PR you or an agent makes.&lt;/p&gt;
    &lt;p&gt;What beads illuminates is that agents need contracts. Humans do a pretty good job of intuitively determining the order in which work must happen. Agents don’t have a good grasp of this. Beads solves that by representing issues as a graph. If A blocks B, the agent can’t work on B until A is closed.&lt;/p&gt;
    &lt;p&gt;I would rather beads didn’t store all of its data in git alongside your work. Git might still be the perfect storage medium (I’m not convinced on that), but it should be separated from the code in my opinion.&lt;/p&gt;
    &lt;head rend="h2"&gt;The future&lt;/head&gt;
    &lt;p&gt;Gas Town is undoubtedly a look into the future of low touch agentic focused workflows for everyone. I don’t want to take away from what has been achieved here. I would credit Steve for the engineering, but as he admits, he has never seen the code.&lt;/p&gt;
    &lt;p&gt;Even if I am vibe engineering, I still care about the code. I still look at it. I think Gas Town is cool, but I don’t think it’s for me, yet.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46674976</guid><pubDate>Mon, 19 Jan 2026 04:11:28 +0000</pubDate></item><item><title>Show HN: AWS-doctor – A terminal-based AWS health check and cost optimizer in Go</title><link>https://github.com/elC0mpa/aws-doctor</link><description>&lt;doc fingerprint="361bd0d2cdef958f"&gt;
  &lt;main&gt;
    &lt;p&gt;A terminal-based tool that acts as a comprehensive health check for your AWS accounts. Built with Golang, aws-doctor diagnoses cost anomalies, detects idle resources, and provides a proactive analysis of your cloud infrastructure—effectively giving you the insights of AWS Trusted Advisor without the need for a Business or Enterprise support plan.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;📉 Cost Comparison: Compares costs between the current and previous month for the exact same period (e.g., comparing Jan 1–15 vs Feb 1–15) to give a fair assessment of spending velocity.&lt;/item&gt;
      &lt;item&gt;🏥 Waste Detection (The "Checkup"): Scans your account for "zombie" resources and inefficiencies that are silently inflating your bill.&lt;/item&gt;
      &lt;item&gt;📊 Trend Analysis: Visualizes cost history over the last 6 months to spot long-term anomalies.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As a Cloud Architect, I often need to check AWS costs and billing information. While the AWS Console provides raw data, it lacks the immediate context I need to answer the question: "Are we spending efficiently?"&lt;/p&gt;
    &lt;p&gt;I created aws-doctor to fill that gap. It doesn't just show you the bill; it acts as a diagnostic tool that helps you understand where the money is going and what can be cleaned up. It automates the routine checks I used to perform manually, serving as a free, open-source alternative to the paid recommendations found in AWS Trusted Advisor.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;go install github.com/elC0mpa/aws-doctor@latest&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;--profile&lt;/code&gt;: Specify the AWS profile to use (default is "").&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--region&lt;/code&gt;: Specify the AWS region to use (default is "us-east-1").&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--trend&lt;/code&gt;: Shows a trend analysis for the last 6 months.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;--waste&lt;/code&gt;: Makes an analysis of possible money waste you have in your AWS Account.&lt;list rend="ul"&gt;&lt;item&gt;Unused EBS Volumes (not attached to any instance).&lt;/item&gt;&lt;item&gt;EBS Volumes attached to stopped EC2 instances.&lt;/item&gt;&lt;item&gt;Unassociated Elastic IPs.&lt;/item&gt;&lt;item&gt;EC2 reserved instance that are scheduled to expire in the next 30 days or have expired in the preceding 30 days.&lt;/item&gt;&lt;item&gt;EC2 instance stopped for more than 30 days.&lt;/item&gt;&lt;item&gt;EC2 instances stopped for more than 30 days.&lt;/item&gt;&lt;item&gt;Load Balancers with no attached target groups.&lt;/item&gt;&lt;item&gt;Inactive VPC interface endpoints.&lt;/item&gt;&lt;item&gt;Inactive NAT Gateways.&lt;/item&gt;&lt;item&gt;Idle Load Balancers.&lt;/item&gt;&lt;item&gt;RDS Idle DB Instances.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Roadmap&lt;/p&gt;
    &lt;p&gt;[x] Add monthly trend analysis.&lt;/p&gt;
    &lt;p&gt;[x] Add waste/waste analysis logic.&lt;/p&gt;
    &lt;p&gt;[ ] Export reports to CSV and PDF formats (Medical records for your cloud).&lt;/p&gt;
    &lt;p&gt;[ ] Distribute the CLI in Fedora, Ubuntu, and macOS repositories.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46675092</guid><pubDate>Mon, 19 Jan 2026 04:35:05 +0000</pubDate></item><item><title>Show HN: Pdfwithlove – PDF tools that run 100% locally (no uploads, no back end)</title><link>https://pdfwithlove.netlify.app</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=46675231</guid><pubDate>Mon, 19 Jan 2026 05:04:07 +0000</pubDate></item><item><title>Show HN: Intent Layer: A context engineering skill for AI agents</title><link>https://www.railly.dev/blog/intent-layer/</link><description>&lt;doc fingerprint="17c4665ee7ef9f0e"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;Today I’m releasing &lt;code&gt;/intent-layer&lt;/code&gt;, the first skill from  Crafter Station .&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;npx skills add crafter-station/skills --skill intent-layer -g&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Works with Claude Code, Codex, Cursor, Copilot, and 10+ more agents .&lt;/p&gt;
      &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
      &lt;p&gt;I’ve been using Claude Code daily for months. Same model, same prompts, completely different results depending on the repo.&lt;/p&gt;
      &lt;p&gt;On a large codebase I watched Claude:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;Burn 40k tokens exploring dead ends&lt;/item&gt;
        &lt;item&gt;Find mocked tests, outdated docs, random utilities&lt;/item&gt;
        &lt;item&gt;Miss the config file with the actual bug&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Reasonable search. Wrong places. Bug still there.&lt;/p&gt;
      &lt;head rend="h2"&gt;Why This Happens&lt;/head&gt;
      &lt;p&gt;Your best engineers don’t grep randomly. They have a mental map:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;What each folder owns&lt;/item&gt;
        &lt;item&gt;What breaks if you touch it wrong&lt;/item&gt;
        &lt;item&gt;Where the real logic lives&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;That map took years to build. Your agents don’t have it.&lt;/p&gt;
      &lt;head rend="h2"&gt;The Solution: Context Engineering&lt;/head&gt;
      &lt;p&gt;Context engineering is designing the full information an agent needs to perform reliably:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;System prompts and instructions&lt;/item&gt;
        &lt;item&gt;Structured inputs and outputs&lt;/item&gt;
        &lt;item&gt;Tools and their definitions&lt;/item&gt;
        &lt;item&gt;RAG and memory systems&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Intent Layer solves the first piece: system prompt infrastructure.&lt;/p&gt;
      &lt;head rend="h2"&gt;What Intent Layer Does&lt;/head&gt;
      &lt;p&gt;The skill helps you set up &lt;code&gt;AGENTS.md&lt;/code&gt; files at folder boundaries. Simple markdown that gives agents the context they can’t get from code alone:&lt;/p&gt;
      &lt;quote&gt;
        &lt;code&gt;# Payment Service

## Purpose

Handles payment processing and settlement.
Does NOT own billing/invoicing (see billing-service).

## Contracts

- All processor calls go through src/clients/processor.ts
- Settlement config lives in ../platform-config/rules/

## Pitfalls

- src/legacy/ looks dead but handles pre-2023 accounts
- Test mode hits real sandbox. Charges appear then void.&lt;/code&gt;
      &lt;/quote&gt;
      &lt;p&gt;Run &lt;code&gt;/intent-layer&lt;/code&gt; on your project and it:&lt;/p&gt;
      &lt;list rend="ol"&gt;
        &lt;item&gt;Detects existing CLAUDE.md / AGENTS.md files&lt;/item&gt;
        &lt;item&gt;Analyzes your codebase structure&lt;/item&gt;
        &lt;item&gt;Suggests where to add context nodes&lt;/item&gt;
        &lt;item&gt;Asks what patterns and pitfalls to document&lt;/item&gt;
      &lt;/list&gt;
      &lt;p&gt;Run it again later to audit existing nodes or find new candidates as your codebase grows.&lt;/p&gt;
      &lt;head rend="h2"&gt;Results&lt;/head&gt;
      &lt;p&gt;Same bug, with AGENTS.md in place:&lt;/p&gt;
      &lt;list rend="ul"&gt;
        &lt;item&gt;16k tokens loaded (not 40k)&lt;/item&gt;
        &lt;item&gt;Went straight to the config file&lt;/item&gt;
        &lt;item&gt;Found it first try&lt;/item&gt;
      &lt;/list&gt;
      &lt;head rend="h2"&gt;Try It&lt;/head&gt;
      &lt;quote&gt;
        &lt;code&gt;npx skills add crafter-station/skills --skill intent-layer -g&lt;/code&gt;
      &lt;/quote&gt;
      &lt;head rend="h2"&gt;What’s Next&lt;/head&gt;
      &lt;p&gt;Intent Layer is the first of several context engineering skills I’m building. More coming soon.&lt;/p&gt;
      &lt;head rend="h2"&gt;Credits&lt;/head&gt;
      &lt;p&gt;Built on The Intent Layer by Tyler Brandt. His AI Adoption Roadmap maps the stages most teams are stuck at. Start there for the full methodology.&lt;/p&gt;
      &lt;p&gt;Context engineering framework from DAIR.AI and LangChain .&lt;/p&gt;
      &lt;p&gt;Also related to my earlier AI-First Manifesto where I proposed &lt;code&gt;LLMS.md&lt;/code&gt; files. Intent Layer is the evolved version of that idea.&lt;/p&gt;
      &lt;p&gt;Follow @RaillyHugo for more on context engineering.&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46675236</guid><pubDate>Mon, 19 Jan 2026 05:04:40 +0000</pubDate></item></channel></rss>