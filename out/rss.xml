<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Sun, 19 Oct 2025 02:41:08 +0000</lastBuildDate><item><title>Secret diplomatic message deciphered after 350 years</title><link>https://www.nationalarchives.gov.uk/explore-the-collection/the-collection-blog/secret-diplomatic-message-deciphered-after-350-years/</link><description>&lt;doc fingerprint="577d8f1d85143253"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôm excited to report that the challenge set in my blog post of 4 August 2025 has been met. The letter sent by William Perwich on 9 April 1670 from the court of Louis XIV in France has been successfully decrypted, not once but twice.&lt;/p&gt;
    &lt;p&gt;The collection&lt;/p&gt;
    &lt;head rend="h2"&gt;Hidden in plain sight: an undeciphered letter from Louis XIV‚Äôs France&lt;/head&gt;
    &lt;p&gt;Read Ruth's first blog post about the mysterious letter.&lt;/p&gt;
    &lt;p&gt;On a murky Monday in late September, I started work to find my inbox brightened by two emails with solutions which made sense, both in the context of a columnar transposition of the text and because they included gossip at the court also reported by the Venetian ambassador on the same day.&lt;/p&gt;
    &lt;p&gt;The cryptographers who cracked the cipher were Matthew Brown (working alone) and Dr George Lasry, Professor Norbert Biermann and Tomokiyo Satoshi (working together). The latter team are well known for deciphering a group of letters from Mary Queen of Scots, found in the National Library of France, in 2023.&lt;/p&gt;
    &lt;p&gt;The tricky aspects to the cipher were working out how many columns it was in (20) before randomly rearranging them until they formed recognisable words, and identifying all the letters which were ‚Äònulls‚Äô, i.e. should be discarded before attempting to decrypt (a number it is impossible to be certain about without Perwich‚Äôs original cipher key).&lt;/p&gt;
    &lt;p&gt;The letter frequencies pushed the cryptographers in the right direction. Most letters appeared a usual number of times for the English language but, exceptionally, there were 8 Qs. On noting that 6 of these were towards the right margin, they were made aware that each line was probably completed with nulls.&lt;/p&gt;
    &lt;p&gt;The solution was reached by using codebreaking software the team had developed along with extensive manual work, in part required because Perwich had mistakenly omitted a couple of letters in his ciphertext.&lt;/p&gt;
    &lt;p&gt;After finessing the results, the final decryption reads:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The souldiers grumble much that the king is of late growne cool towards them and gives them not the encouragement in their addresses as hee used. They complain hee is wholly given up to his mistresses who are no enemies to 97 nor peace and consequently dissuade the 60 96 pursuing the French manufactere with vigour knows that peace can onely advance his designs I heard a great man say that the 61 sayd to his brother he wisht him not to oppose madam's going for 40 becaus her journey was for the interest of his kingdome wherupon most do boast of an alliance [likelyhood]&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;You will note that some numerical codes, standing in for names and places, remain in the deciphered letter. These were a separate system, impossible to crack without the key, especially as there were two numbers for each entity, but possible to interpret with knowledge of the historical context.&lt;/p&gt;
    &lt;p&gt;The solution in modern English, with the numbers tentatively interpreted, reads:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The soldiers grumble much that the King is of late grown cool towards them and gives them not the encouragement in their addresses as he used to. They complain he is wholly given up to his mistresses, who are no enemies to [the Dutch] nor peace, and consequently dissuade the [King.]&lt;/p&gt;
      &lt;p&gt;[Colbert,] pursuing the French manufacture with vigour, knows that peace can only advance his designs.&lt;/p&gt;
      &lt;p&gt;I heard a great man say the [King] said to his brother he wished him not to oppose Madame‚Äôs going [to England] because her journey was for the interest of his kingdom, whereupon most do boast of the likelihood of an alliance.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Despite this court gossip relayed by Perwich, Louis XIV and his Comptroller General of Finance, Jean-Baptiste Colbert, did in fact remain belligerent towards the Dutch. The Secret Treaty of Dover was designed to break the Triple Alliance (between England, the Dutch Republic and Sweden) and to get England to attack the Dutch in partnership with France, which happened in 1672.&lt;/p&gt;
    &lt;p&gt;We also know that Louis managed to prevent his brother, Philippe, from jealously interfering in his wife‚Äôs (Madame‚Äôs) successful negotiations with her brother, Charles II.&lt;/p&gt;
    &lt;p&gt;So, the secret message was not about stale croissants, as I feared, but proves to have been worth the bother of being enciphered. Its comments about Louis and his influential mistresses, in particular, could well have offended Perwich‚Äôs French hosts. However, it does not suggest Perwich knew anything about the scandalous terms of the forthcoming Secret Treaty with Charles II.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45606986</guid><pubDate>Thu, 16 Oct 2025 15:56:05 +0000</pubDate></item><item><title>When you opened a screen shot of a video in Paint, the video was playing in it</title><link>https://devblogs.microsoft.com/oldnewthing/20251014-00/?p=111681</link><description>&lt;doc fingerprint="377fcdfb29c88adf"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;@ChenCravat In an old version of Windows (Windows 98 iirc) if you took screenshot of a video from media player and paste it into paint, and resume media player, video would play inside paint. Do you why it happened? It is still bugging me to this day.&lt;/p&gt;
      &lt;p&gt;‚Äî Yasar Arabaci @ysar.bsky.social (@y_arabaci) July 18, 2025&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;One of the tricks for video playback is to use a green screen, more technically known as color-keying or chroma-keying.&lt;/p&gt;
    &lt;p&gt;The media player program didn‚Äôt render the video pixels to the screen. Rather, it followed this recipe:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Draw solid green where you want the video to go.&lt;/item&gt;
      &lt;item&gt;Render the video pixels to a graphics surface shared with the graphics card.&lt;/item&gt;
      &lt;item&gt;Tell the graphics card that whenever it sees a green pixel about to be written to the screen, it should substitute a pixel from that shared graphics surface.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Surface&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;üèñÔ∏è&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;‚Üì&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;‚Üí&lt;/cell&gt;
        &lt;cell&gt;Graphics card&lt;/cell&gt;
        &lt;cell&gt;‚Üí&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;üèñÔ∏è&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Desktop&lt;/cell&gt;
        &lt;cell&gt;Monitor&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are a few advantages to this approach.&lt;/p&gt;
    &lt;p&gt;One is that the shared graphics surface need not have the same pixel format as the user‚Äôs main display. Therefore, you can specify that the shared graphics surface have a pixel format that matches that of the video, avoiding the need to do any pixel format conversions.&lt;/p&gt;
    &lt;p&gt;Another is that you can update the content without having to go through a full paint cycle. You just update the shared graphics surface, and the results are on the screen at the next frame. This lets you update the video at 60 frames per second from a background thread, which works even if the UI thread is busy or sluggish.&lt;/p&gt;
    &lt;p&gt;You can do even better if you create two shared graphics surfaces. The first one holds the contents of the video frame you want the user to see right now. And the second one is where you create the contents of the video frame you want the user to see next. And then at the vertical blank, you tell the video card to switch to the second shared graphics surface (known as ‚Äúflipping‚Äù), and the entire screen updates at once with no tearing. While the second surface is on the screen, you can render the next frame to the first surface, and then flip again at the next vertical blank. Repeat this process for each frame of the video.&lt;/p&gt;
    &lt;p&gt;A media player program of this era typically negotiated with the graphics card (via DirectDraw) to get one of these magic graphic surfaces and configure it to use it as replacement pixels. These special surfaces were called ‚Äúoverlays‚Äù because they appeared to overlay the desktop.&lt;/p&gt;
    &lt;p&gt;When you took a screen shot, you got the pixels that Windows gave to the video card as the contents of the desktop. If an overlay is active, then these are not the same pixels that came out of the video card and sent to your monitor. The computer never sees these monitor pixels; they are something generated on the fly by the graphics card and sent directly to the monitor. Your screen shot was a screen shot of the desktop screen, and it contains green pixels where the video would go.&lt;/p&gt;
    &lt;p&gt;Now, when you load the image into Paint or any other image viewer, Windows sends those green pixels to the video card, but if the media player is still running, then its overlay is still active, and if you put Paint in the same place that the media player window is, then the green pixels in Paint get changed into the pixels of the active video. The video card doesn‚Äôt know that the pixels came from Paint. Its job is to look for green pixels in a certain region of the screen and change them into the pixels from the shared surface.&lt;/p&gt;
    &lt;p&gt;If you move the Paint window to another position where it doesn‚Äôt overlap the media player, or if the media player isn‚Äôt playing a video, you will see the bitmap‚Äôs true nature: It‚Äôs just a bunch of green pixels.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs go back to the green screen analogy: Imagine you are visiting a television studio while the presenter is giving a weather report. The presenter is standing in front of a green screen, but the image that goes out to viewers contains an animating weather map where the green backdrop would normally appear. You take a picture of the green screen with your phone, and you hold up the phone to the television camera. What do the viewers at home see? Do they see a phone with a picture of a green screen? No, they see a phone with an animating weather map! And if you move your phone around, what the viewers at home see is different parts of the weather map being inserted into your phone. When you get home, your friends tell you, ‚ÄúWow, how did you do that? You took a still picture of a weather map, and when you held it up, it was animating!‚Äù&lt;/p&gt;
    &lt;p&gt;Now, while overlays are better than going through paint cycles, they still have their problems. For example, if a window moves over the media player, and it happens to have green pixels, then the video will play in that other window. If you move the media player window, it needs to move the overlay to match the media player‚Äôs new location, and in practice there is some lag to this tracking, causing it to look jerky. Also, there is a limit on the number of overlays supported by a graphics card, so if they‚Äôre all used up, then the media player has to go through the old software rendering path.&lt;/p&gt;
    &lt;p&gt;Nowadays, video rendering is no longer done with overlays. Instead, content is rendered to graphic surfaces that are associated with window. The desktop compositors takes the graphics content of all the windows, including their composition visuals, and combines them to form a full desktop image that is sent to the monitor. The desktop compositor understands window positions, so when you move the window, the composition visuals automatically move with them, so you don‚Äôt get the phenomenon of the overlay lagging the window position. The desktop compositor also understands visual transformations, so that when you hit Alt+Tab or hover over the taskbar button, the animating video is automatically resized and repositioned to match the preview thumbnail.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45609986</guid><pubDate>Thu, 16 Oct 2025 19:57:41 +0000</pubDate></item><item><title>K8s with 1M nodes</title><link>https://bchess.github.io/k8s-1m/</link><description>&lt;doc fingerprint="e798c941e1211d92"&gt;
  &lt;main&gt;
    &lt;p&gt;This is an effort to create a fully functional Kubernetes cluster with 1 million active nodes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why?&lt;/head&gt;
    &lt;p&gt;Several years ago at OpenAI I helped author Scaling Kubernetes to 7500 Nodes which remains one of the CNCF‚Äôs most popular blog posts. Alibaba made a post about running Kubernetes clusters with 10K nodes. Google made a post about 15K nodes with Bayer Crop Science. Fast forward to today, GKE supports running some clusters up to 65K nodes, and AWS recently announced support for clusters up to 100K nodes.&lt;/p&gt;
    &lt;p&gt;In online forums and in my own conversations with peers, I‚Äôve encountered a lot of debate about how big a Kubernetes cluster can get. What tends to be lacking from these discussions is hard data and evidence-backed justifications. I‚Äôve worked with engineers reluctant to push things beyond what they‚Äôve seen before because they‚Äôre fearful or uncertain of what may go wrong. Or when something does go wrong, the response is to scale down the cluster rather than understand and address the bottleneck.&lt;/p&gt;
    &lt;p&gt;The spirit of the k8s-1m project is to identify the hard blockers to scalability. What are the actual limits of how hard a particular system can be pushed, and why are those the limits? What would it take to overcome those limits to make the system even more scalable? What types of workloads can be run at greater scale than others?&lt;/p&gt;
    &lt;p&gt;Many limitations are imposed by software. As software engineers, it‚Äôs on us to work through those limitations. Once we‚Äôve done all we can on the software side and the remaining bottlenecks are in hardware, only then should we wait on next generation hardware to achieve our goals.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt have ambitions to commercialize this effort. Certainly not as-is. I‚Äôm fine making the code freely available. Realistically, while all of the claims and results in this project are true and verifiable, each large-scale Kubernetes cluster has its own unique workload demands that may or may not work well with this design. So I wouldn‚Äôt classify this as "production-ready" or recommend that anyone use it for mission-critical workloads.&lt;/p&gt;
    &lt;p&gt;At the same time, I hope that the proposed designs here inspire the Kubernetes community with what is possible and influence the future roadmap of the project.&lt;/p&gt;
    &lt;head rend="h2"&gt;Components&lt;/head&gt;
    &lt;p&gt;Early on in this project, I asked ChatGPT ‚ÄúI want to scale Kubernetes to 1 million nodes. What types of problems would I need to overcome?‚Äù&lt;/p&gt;
    &lt;p&gt;Here‚Äôs a portion of what I got:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Scaling Kubernetes to 1 million nodes is a formidable challenge and involves overcoming a variety of technical hurdles. Here are the key areas you would need to address:&lt;/p&gt;
      &lt;p&gt;etcd Scalability and Performance: etcd is the backbone of Kubernetes' storage, handling all API object data. With 1 million nodes, the volume of data managed by etcd will increase significantly. Optimizing etcd‚Äôs performance, including efficient data partitioning, tuning the database to handle high loads, and possibly looking into alternatives or supplements to etcd for certain types of data, is crucial.&lt;/p&gt;
      &lt;p&gt;kube-apiserver Performance: The Kubernetes kube-apiserver would face enormous loads in a cluster of this size. Optimizing its performance includes horizontal scaling (running multiple kube-apiserver instances), rate limiting, and efficient caching mechanisms. Also, minimizing the watch events and reducing the overall number of requests to the kube-apiserver is important.&lt;/p&gt;
      &lt;p&gt;Networking: Handling network traffic for 1 million nodes is extremely challenging. This includes the management of IP addresses, efficient routing, and possibly segmenting the network to prevent it from becoming a bottleneck. Solutions like Cilium, which uses eBPF technology for networking and security, might be necessary.&lt;/p&gt;
      &lt;p&gt;Scheduling and Resource Management: The scheduler will have increased latency as it tries to place pods optimally across a vast number of nodes. Enhancing the scheduler to handle large scale operations efficiently, possibly by implementing custom schedulers for specific tasks, is required.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;It goes on to list other challenges, including Logging &amp;amp; Monitoring, Security, and Storage management.&lt;/p&gt;
    &lt;p&gt;I think these first items are the most critical. There are a wide range of options for support for logging or PVCs, but if you can‚Äôt run etcd or schedule pods or have the pods talk to one another, you‚Äôre not getting anywhere.&lt;/p&gt;
    &lt;p&gt;Below I‚Äôm going to discuss the three major areas I tackled.&lt;/p&gt;
    &lt;head rend="h3"&gt;Networking&lt;/head&gt;
    &lt;p&gt;The common challenges with networking in Kubernetes are IP address space, service discovery, and network policies (aka firewalling). Compared to later challenges, configuring networking to support 1M nodes turned out to be relatively easy.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pod IPs&lt;/head&gt;
    &lt;p&gt;It can be a challenge to plan IP address space in large clusters. Efficient routing usually means routing a contiguous CIDR to each individual node, which means you‚Äôre pre-allocating how many IP addresses (and thus pods) each node can serve. Some nodes that are intended for lots of small workloads end up constrained by the number of available IP addresses before they run out of other hardware resources. A 10.0.0.0/8 has 16 million IPs. For a 1-million node cluster, that leaves just 15 pod IPs per node, which is likely not enough.&lt;/p&gt;
    &lt;p&gt;The answer is to use IPv6 thoroughly and exclusively. The enormous IPv6 address space means that there‚Äôs plenty of room for every pod to have its own globally accessible IP address.&lt;/p&gt;
    &lt;p&gt;Kubernetes has great support for IPv6, and it requires no code changes to create a fully functioning Kubernetes cluster that uses IPv6 exclusively.&lt;/p&gt;
    &lt;p&gt;My goal is for each node to have an IPv6 address prefix with a range large enough so each pod on that node can have its own IP out of that range. Plus, of course, at least one for the host itself.&lt;/p&gt;
    &lt;p&gt;Of course, using IPv6 requires support from your compute vendor. All of the major cloud providers (and even many less-major) support IPv6 to some degree or another. And with public IPv6, it‚Äôs trivial to create a single cluster that spans multiple clouds.&lt;/p&gt;
    &lt;p&gt;I primarily focused on AWS, GCP, and Vultr. (Scoff if you want at Vultr, but they have cheap compute and I am self-bootstrapping this project.) But each one has slightly different twists on its support for IPv6 addressing inside a VM. To give a sense of the range of situations, let me briefly describe each below:&lt;/p&gt;
    &lt;p&gt;Vultr: Each node gets a /64. The primary IP of the node is the ::1 of that range, and the server automatically receives all traffic for any IP in the full /64 range.&lt;/p&gt;
    &lt;p&gt;GCP: Each node gets a /96. The primary IP of the node is some random IP within that range. The server must send valid NDP RA packets for the IPs that it wants to receive traffic for.&lt;/p&gt;
    &lt;p&gt;AWS: Each node gets a /128. You can add a /80 prefix (that comes from a different range) via an API call to an existing NIC or VM. (The ‚ÄòCreate‚Äô API looks like it can support setting both an IPv6 address and an IPv6 range at creation time, but you‚Äôll get an error). The server must send valid NDP RA packets for the IPs it wants to receive traffic for, and all outgoing packets must use the one MAC address that matches the primary IP.&lt;/p&gt;
    &lt;p&gt;To satisfy the intersection of these requirements, particularly the requirement about MAC addresses, I create one bridge for all of the pods' interfaces to share. But leave the host interface separate, and enable forwarding to handle traffic between the bridge and the host interface. A host-local IPAM is set to a /96 IPv6 prefix of what I get from the provider. This gives us a full 2^32 IPs per node, plenty of space for pods.&lt;/p&gt;
    &lt;p&gt;Because these are global public IPv6 addresses, no special routing is necessary. No packet encapsulation or NAT is used. Traffic from each pod is correctly sourced from its true origin pod IP, regardless of destination.&lt;/p&gt;
    &lt;head rend="h4"&gt;IPv4-only external service dependencies&lt;/head&gt;
    &lt;p&gt;If you only have an IPv6 address, then you can only reach other IPv6 addresses on the internet. Anything that is IPv4-only isn‚Äôt directly accessible.&lt;/p&gt;
    &lt;p&gt;Most services I used in this project worked fine: Ubuntu packages, PyPi packages, the docker.io registry. The main exception was GitHub. Github.com remains stubbornly IPv4-only. Tsk tsk.&lt;/p&gt;
    &lt;p&gt;Many AWS services have dual-stack endpoints but notably for this project Elastic Container Registry (ECR) does not. Tsk tsk to them as well.&lt;/p&gt;
    &lt;p&gt;For IPv6 devices to reach IPv4 hosts, most cloud providers offer some sort of NAT64 gateway. You can also roll your own gateway on a Linux VM. I over-engineered this a bit with a custom WireGuard server. All VMs connect via WireGuard to this server and use it as an IPv4 gateway.&lt;/p&gt;
    &lt;head rend="h4"&gt;Network Policies&lt;/head&gt;
    &lt;p&gt;High-level, I hand-waved over this problem and did not use network policies between workloads.&lt;/p&gt;
    &lt;p&gt;1 million nodes would have 1 million separate IPv6 prefixes, which is far too many individual entries for any firewall solution to support. Security-minded folks: clutch your pearls when I say that I do not use extensive firewalling to prevent access into the cluster from the Internet. I do use firewall rules to limit to a select few number of ports that I know need to be reached, but otherwise we must rely on other techniques to safeguard unauthorized inbound access to servers and pods.&lt;/p&gt;
    &lt;p&gt;Thorough use of TLS covers most use cases for this project. The enormous size of the IPv6 address space also makes scanning impractical. Cilium, kube-proxy, or other network plugins could also limit which pods can reach which pods, but at significant cost of additional watches on the control plane.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre using one single vendor for all of your nodes, it may be plausible that all nodes still get ipv6 ranges out of 1 or a few larger spaces, a count low enough that could be reasonably installed as firewall rules.&lt;/p&gt;
    &lt;head rend="h4"&gt;Network flow needs (# of TCP connections)&lt;/head&gt;
    &lt;p&gt;Both kube-apiservers and etcd support both HTTP/2 and gRPC. Many individual requests and streams are multiplexed over a single TCP connection. Kubernetes sets a default HTTP/2 limit of 100 concurrent requests (or technically streams) per TCP connection. (HTTP/2 can support far more than that, but as you add more streams you run into performance problems like head-of-line blocking). So each kubelet needs at least 1 connection to the kube-apiserver control plane. And you can expect 1 more connection for kube-proxy, or any similar CNI like Cilium or Calico. With 1M nodes, that means each kube-apiserver is supporting at least 2 million TCP connections. With 8 kube-apiservers, each server would be supporting 250K connections to kubelets.&lt;/p&gt;
    &lt;p&gt;Linux itself can support this number of connections with some light tuning. And of course make sure you have allowed yourself enough file descriptors. Nevertheless it may be more than your network provider can support. Azure, for example, documents that it can support a maximum of 500k inbound and 500k outbound connections per VM. GCP and AWS do not publish limits, but there are limits in any system to both the total number of concurrent connections as well as the rate of new connections being made.&lt;/p&gt;
    &lt;head rend="h3"&gt;Managing state&lt;/head&gt;
    &lt;p&gt;When I talk about ‚Äúmanaging state,‚Äù I mean the API surface that Kubernetes exposes for interacting with resources. With careful tuning, the kube-apiserver can scale to sufficiently high levels of throughput. etcd, however, is the bottleneck. In this section, I‚Äôll outline why that is and describe a replacement implementation that can meet the demands of a million-node cluster.&lt;/p&gt;
    &lt;head rend="h4"&gt;kube-apiservers vs etcd&lt;/head&gt;
    &lt;p&gt;First a quick overview about the ways you work with state in Kubernetes. Any number of clients interact with kube-apiservers, which then in turn interact with etcd.&lt;/p&gt;
    &lt;p&gt;kube-apiservers are stateless. etcd is the persistent store for all of Kubernetes resources. All CRUD operations you send to a kube-apiserver are actually persisted by etcd.&lt;/p&gt;
    &lt;p&gt;kube-apiservers have seven common verbs for state:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;create&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;get&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;list&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;update&lt;/code&gt;(aka&lt;code&gt;replace&lt;/code&gt;)&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;patch&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;delete&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;watch&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;etcd has four:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;put&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;range&lt;/code&gt;- includes&lt;code&gt;get&lt;/code&gt;with a null&lt;code&gt;range_end&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;deleteRange&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;watch&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;kube-apiserver &lt;code&gt;create&lt;/code&gt;, &lt;code&gt;update&lt;/code&gt;, &lt;code&gt;patch&lt;/code&gt;, and &lt;code&gt;delete&lt;/code&gt; all result in an etcd &lt;code&gt;put&lt;/code&gt; operation. (A &lt;code&gt;delete&lt;/code&gt; is just a &lt;code&gt;put&lt;/code&gt; with a null value). etcd doesn‚Äôt support any partial updates of values, only putting the entire value. So all operations that involve modifying a resource result in a new etcd &lt;code&gt;put&lt;/code&gt; of the entire resource contents.&lt;/p&gt;
    &lt;p&gt;kube-apiserver &lt;code&gt;watch&lt;/code&gt; can, but often doesn‚Äôt, result in an etcd &lt;code&gt;watch&lt;/code&gt;. More on that below.&lt;/p&gt;
    &lt;head rend="h4"&gt;Meeting the QPS needs for a 1M node cluster&lt;/head&gt;
    &lt;p&gt;Kubelets interact with the kube-apiserver primarily through two resource types:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Node&lt;/code&gt;- the resource representing a server for running pods&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lease&lt;/code&gt;- a lightweight heartbeat object updated by&lt;code&gt;kubelet`&lt;/code&gt;to signal liveness&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Lease is critical: if it isn‚Äôt refreshed in time, the NodeController marks the node as &lt;code&gt;NotReady&lt;/code&gt;. By default, each kubelet updates its &lt;code&gt;Lease&lt;/code&gt; every 10 seconds. At a scale of 1 million nodes, that alone translates to 100K writes per second just to keep the nodes "alive."&lt;/p&gt;
    &lt;p&gt;Adding in the constant churn of other resources, the system needs to sustain on the order of many hundreds of thousands of writes per second, plus a significant volume of reads.&lt;/p&gt;
    &lt;p&gt;For kube-apiserver, this is manageable. It‚Äôs stateless, so QPS can be scaled out simply by running more replicas. If one instance can‚Äôt handle the load, more can be added, and traffic will spread across them.&lt;/p&gt;
    &lt;p&gt;For etcd, things are different. Etcd is stateful, which makes scaling QPS much harder.&lt;/p&gt;
    &lt;head rend="h4"&gt;etcd is too slow&lt;/head&gt;
    &lt;p&gt;Using the etcd-benchmark tool, I measured about 50K writes/sec out of a single etcd instance backed by NVMe storage. Importantly, adding replicas doesn‚Äôt help. Write throughput actually drops with more members since each write must be coordinated across a quorum of replicas to maintain consistency. So with the typical 3-replica setup, effective write QPS is even lower than the benchmarked 50K/s. That‚Äôs nowhere near what‚Äôs needed to support a 1M-node cluster.&lt;/p&gt;
    &lt;p&gt;At first glance, 50K QPS seems surprisingly low given modern hardware capabilities. A single NVMe drive can do over 1M 4K writes per second, and a single DDR5 DIMM can push 10x more than that. So why is etcd is far behind raw hardware limits?&lt;/p&gt;
    &lt;p&gt;The answer lies in etcd‚Äôs interface and guarantees. For one thing, etcd is ensuring that all writes are durable to disk. For every &lt;code&gt;put&lt;/code&gt; or &lt;code&gt;delete&lt;/code&gt; call, etcd ensures the change is written to disk via &lt;code&gt;fsync&lt;/code&gt; before acknowledging success. This helps ensure that there is never any data loss if the host crashes or loses power. But that durability drastically reduces the number of IOPS that a modern NVMe drive can support.&lt;/p&gt;
    &lt;p&gt;Plus, etcd has a pretty broad interface surface area:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;It is a key value store and so of course supports reads, writes, and deletes of single objects.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It can support querying a range of sorted keys.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It keeps history for all changes, so you can query for an older version of a particular key, or even a range of keys. Older changes eventually get ‚Äúcompacted‚Äù to reduce state size.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It has a notion of ‚Äúwatches‚Äù, meaning it can stream out all of the changes that affect a particular key or range of keys.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It also has a ‚Äúlease‚Äù API, where keys can be attached to a TTL that will cause them to expire if not renewed.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;It supports transactions, supporting an atomic If/Then/Else.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Implementing all of those interfaces can make for complex software. Beyond simple puts and deletes, etcd must support transactions, maintain multi-versioned history, and enforce Raft-based consensus across replicas.&lt;/p&gt;
    &lt;p&gt;These features are what give Kubernetes its consistency and reliability, but they also impose strict constraints on performance. Intuitively, strong consistency means more serialization: operations can‚Äôt always be parallelized freely. Writes often need to follow a carefully ordered path through Raft, the WAL, and compaction, ensuring that every replica agrees on state before acknowledging success.&lt;/p&gt;
    &lt;p&gt;The result is raw hardware capable of millions of writes per second, but etcd delivering orders of magnitude less due to the interfaces and guarantees it must uphold.&lt;/p&gt;
    &lt;p&gt;But do we need all of these things?&lt;/p&gt;
    &lt;head rend="h5"&gt;Reduce durability and eliminate replicas&lt;/head&gt;
    &lt;p&gt;Perhaps my spiciest take from this entire project: most clusters don‚Äôt actually need the level of reliability and durability that etcd provides.&lt;/p&gt;
    &lt;p&gt;As we‚Äôll see in the next section, the majority of writes in a Kubernetes cluster are for ephemeral resources.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Kubernetes&lt;/p&gt;&lt;code&gt;Events&lt;/code&gt;may only stick around for minutes.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Lease&lt;/code&gt;objects typically expire within tens of seconds.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If the cluster is disrupted, restoring these objects is almost never useful, and certainly not to the precision of their last few milliseconds of updates. Even for longer-lived objects, Kubernetes is designed to reconcile automatically:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Nodes&lt;/code&gt;continually refresh status via&lt;code&gt;kubelet&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Controllers will bring&lt;/p&gt;&lt;code&gt;DaemonSet&lt;/code&gt;and&lt;code&gt;Deployment&lt;/code&gt;status back in sync with actual&lt;code&gt;Pods&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If we stopped &lt;code&gt;fsync&lt;/code&gt;-ing these ephemeral writes, or even stopped writing them altogether and just relied on RAM, clusters could process far more operations and perform substantially better.&lt;/p&gt;
    &lt;p&gt;In fact, even full control plane data loss isn‚Äôt catastrophic in some environments. Many clusters are ephemeral themselves, with all configuration encoded in Terraform, Helm, or GitOps. In those cases, rebuilding is often easier than preserving every last write. Some organizations already treat Kubernetes clusters as cattle.&lt;/p&gt;
    &lt;p&gt;If you‚Äôre not mad yet, let me push you a little further: you probably don‚Äôt need etcd replicas at all.&lt;/p&gt;
    &lt;p&gt;In the 5 years I ran Kubernetes clusters at OpenAI, we never once had an unplanned VM outage on an etcd VM. etcd‚Äôs resource needs are tiny. The database is limited to 8GB. CPU is no more than 2-4 cores. Most cloud providers can do live migration on VMs this small. With network-attached storage like EBS, recovery is straightforward: spin up a replacement VM, attach the volume, and resume operation with zero data loss.&lt;/p&gt;
    &lt;p&gt;If you had just 1 etcd instance and that went down, your Kubernetes cluster control plane would go down. Pods would still stay running. Nodes would still be reachable. It‚Äôs possible that you could still serve traffic. If etcd used EBS, recovery would be the time to start a new VM and attach the volume, with no data loss.&lt;/p&gt;
    &lt;p&gt;Yes, running a single etcd instance is a single point of failure. But failures are rare and the practical impact is often negligible. Meanwhile, etcd replicas come with a significant performance cost. For many workloads, that tradeoff simply isn‚Äôt worth it.&lt;/p&gt;
    &lt;p&gt;Always stop writing &lt;code&gt;Event&lt;/code&gt; and &lt;code&gt;Lease&lt;/code&gt; to disk. Beyond that, you have some options:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;You don‚Äôt need durability: Run one replica, keep all state in memory.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You can tolerate losing a few ms of updates: Run a single replica with a network-attached disk, but without&lt;/p&gt;&lt;code&gt;fsync&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You‚Äôd rather avoid data loss: Run multiple replicas in case one goes down, but don‚Äôt bother writing changes to disk. Rely on the uptime of the other replicas to keep from losing data.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;You‚Äôre paranoid about data loss: Run a single replica with a network-attached disk, and enable&lt;/p&gt;&lt;code&gt;fsync&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h5"&gt;Reduce the interface&lt;/head&gt;
    &lt;p&gt;As I described, etcd has a pretty broad interface surface area. But does Kubernetes actually use all of those features?&lt;/p&gt;
    &lt;p&gt;To measure this, I wrote a small tool called etcd proxy, The proxy sits between Kubernetes and etcd, transparently forwarding all traffic while logging every request and response.&lt;/p&gt;
    &lt;p&gt;With that in place, I spun up a Kubernetes cluster and ran Sonobuoy, the standard conformance test suite of Kubernetes. Sonobuoy systematically exercises the full API surface of Kubernetes, ensuring compliance with upstream expectations. Running it through the proxy produced a complete, real-world trace of the requests and workloads that etcd must handle in a conforming cluster.&lt;/p&gt;
    &lt;p&gt;It turns out that Kubernetes actually uses just a small amount of the etcd interface.&lt;/p&gt;
    &lt;p&gt;There‚Äôs of course &lt;code&gt;read&lt;/code&gt;, &lt;code&gt;write&lt;/code&gt;, &lt;code&gt;range&lt;/code&gt;, and &lt;code&gt;watch&lt;/code&gt; queries, but they all follow a few patterns.&lt;/p&gt;
    &lt;head rend="h6"&gt;Txn-Put&lt;/head&gt;
    &lt;p&gt;Kubernetes does do Txn queries, but they‚Äôre always of this form:&lt;/p&gt;
    &lt;code&gt;{
  "method": "/etcdserverpb.KV/Txn",
  "request": {
    "compare": [
      {
        "key": "SOMEKEY",
        "modRevision": "SOMEREV",
        "target": "MOD"
      }
    ],
    "success": [
      {
        "requestPut": {
          "key": "SOMEKEY",
          "value": "..."
        }
      }
    ],
    "failure": [
      {
        "requestRange": {
          "key": "SOMEKEY"
        }
      }
    ]
  }
}&lt;/code&gt;
    &lt;p&gt;In other words, do a &lt;code&gt;put&lt;/code&gt; if the &lt;code&gt;modRev&lt;/code&gt; of this key is set to this particular value, otherwise just return me the current version. And this makes sense, because Kubernetes is often patching or updating existing resources but turning that into a &lt;code&gt;put&lt;/code&gt; of the full resource safely means that the underlying resource must not have changed in between.&lt;/p&gt;
    &lt;head rend="h6"&gt;Leases&lt;/head&gt;
    &lt;p&gt;Note that Kubernetes Leases are not the same as etcd Leases. Kubernetes leases are implemented as regular K/V‚Äôs in etcd. Kubernetes makes very few etcd Leases.&lt;/p&gt;
    &lt;p&gt;The main area where Kubernetes uses etcd leases is on Events objects, e.g.:&lt;/p&gt;
    &lt;code&gt;{
  "method": "/etcdserverpb.Lease/LeaseGrant",
  "request": {
    "TTL": "3660"
  },
  "response": {
    "ID": "7587883212297104637",
    "TTL": "3660"
  }
}
{
  "method": "/etcdserverpb.KV/Txn",
  "request": {
    "compare": [
      {
        "key": "/registry/events/NAMESPACE/SOMEEVENT",
        "modRevision": "205",
        "target": "MOD"
      }
    ],
    "failure": [
      {
        "requestRange": {
          "key": "/registry/events/NAMESPACE/SOMEEVENT",
        }
      }
    ],
    "success": [
      {
        "requestPut": {
          "key": "/registry/events/NAMESPACE/SOMEEVENT",
          "lease": "7587883212297104637",
          "value": "..."
        }
      }
    ]
  }
}&lt;/code&gt;
    &lt;p&gt;The purpose of this is to manage some sane TTL on events. It‚Äôs not critical to the consistency model of Kubernetes.&lt;/p&gt;
    &lt;head rend="h6"&gt;Ranges&lt;/head&gt;
    &lt;p&gt;etcd could be implemented as a simple hash-table with O(1) insertion time, if it weren‚Äôt for range queries. Range queries return a sorted list of keys within a given span, which requires storing data in a sorted structure. Inserting into a sorted list or B-Tree is O(log n). In my view, supporting Range is thus the most difficult constraint that etcd must implement to be Kubernetes compatible. Nevertheless, it is critical.&lt;/p&gt;
    &lt;p&gt;Fortunately, we can take advantage of the predictable structure of the keyspace:&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;/registry/[$APIGROUP/]$APIKIND/[$NAMESPACE/]$NAME&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Range queries are typically scoped to be either within a particular namespace, or across all namespaces for a given resource Kind. Kubernetes never performs a range query that spans across multiple resource Kinds (e.g., Pods and ConfigMaps together).&lt;/p&gt;
    &lt;p&gt;This introduces an opportunity: rather than one global B-tree for the entire keyspace, we can maintain separate B-trees per resource Kind. That shrinks the effective n in O(log n) to just the number of objects of a single kind, improving both inserts and queries.&lt;/p&gt;
    &lt;p&gt;Another wrinkle is the use of &lt;code&gt;limit&lt;/code&gt; on range queries. Kubernetes rarely needs to retrieve all objects at once; queries often return only 500, 1,000, or 10,000 results at a time. However, range responses are also expected to include a count field representing the total number of remaining objects. This undermines the benefit of &lt;code&gt;limit&lt;/code&gt;, since even merely counting all remaining keys can still be expensive.&lt;/p&gt;
    &lt;p&gt;In practice, though, Kubernetes doesn‚Äôt rely on &lt;code&gt;count&lt;/code&gt; being exact. It only needs to know that there are more than &lt;code&gt;limit&lt;/code&gt; results available. This looser requirement leaves room for approximation, and is one area where further optimizations are possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;mem_etcd: custom in-memory etcd&lt;/head&gt;
    &lt;p&gt;I built a new program called mem_etcd that implements the etcd interface but with the simplifications described above. Written in Rust, it provides fully correct semantics for the etcd APIs that Kubernetes depends on.&lt;/p&gt;
    &lt;p&gt;mem_etcd maintains two main data structures:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;A hash map storing the full keyspace&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;A B-tree indexing the keys within each prefix.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each value also stores the non-compacted revision history for that key. This design makes writes to existing keys O(1), while writes to new keys and range queries are O(log n) (where n is the number of resources of that &lt;code&gt;Kind&lt;/code&gt;). &lt;code&gt;Range&lt;/code&gt; queries also require additional linear work up to the query‚Äôs limit.&lt;/p&gt;
    &lt;p&gt;Despite its name, mem_etcd can provide durability by writing a write-ahead log (WAL) to disk. Each prefix of &lt;code&gt;/registry/[$APIGROUP/]$APIKIND/[$NAMESPACE/]&lt;/code&gt; is written to its own separate file. By default, files are written in &lt;code&gt;buffered&lt;/code&gt; mode, so &lt;code&gt;put&lt;/code&gt; calls can complete before the data is durably written to disk. This behavior can be changed with a CLI flag that enables &lt;code&gt;fsync&lt;/code&gt;, forcing all writes to be flushed to disk before the &lt;code&gt;put&lt;/code&gt; completes.  You can also configure some prefixes to not be written to disk at all.&lt;/p&gt;
    &lt;code&gt;% (cd /tmpfs ; etcd-3.5.16 --snapshot-count=9999999999 --quota-backend-bytes=9999999999) &amp;amp;
% parallel -j $X --results out_{#}.txt './benchmark put --total 10000000 --clients 1000 --conns 10 --key-space-size 10000000 --key-size=48 --val-size=1024' ::: {1..$X}&lt;/code&gt;
    &lt;p&gt;These tests were run on a pair of &lt;code&gt;c4d-standard-192-lssd&lt;/code&gt; instances, with one VM running mem_etcd and the other running the client benchmark. In these results, you can easily observe how badly enabling &lt;code&gt;fsync&lt;/code&gt; negatively impacts throughput and latency. Note that the baseline comparison of etcd is a single replica of etcd v3.5.16 running on a tmpfs (ram-based) disk. This should be an optimal environment for etcd as there is no actual disk involved and &lt;code&gt;fsync&lt;/code&gt;, while still being a syscall, is otherwise a no-op. mem_etcd is storing its WAL on a local NVMe, what GCE calls Titanium SSD. Though the instance type has 16 local disks, only 1 is used for this test.&lt;/p&gt;
    &lt;code&gt;% timeout 10 parallel -j $X --results out_{#}.txt   './etcd-lease-flood -num-keys 1000 -workers 100 -key-prefix {#}' ::: {1..$X}&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;etcd-lease-flood&lt;/code&gt; is a custom benchmark designed to simulate the dominant type of load in a large Kubernetes cluster. Each client creates 100 &lt;code&gt;Lease&lt;/code&gt; objects directly in etcd, using the same protobuf encoding as Kubernetes. For each &lt;code&gt;Lease&lt;/code&gt;, the client repeatedly issues &lt;code&gt;put&lt;/code&gt; updates in a tight loop, attempting to update the &lt;code&gt;Lease&lt;/code&gt; as quickly as possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Watch()&lt;/head&gt;
    &lt;p&gt;There are several different types of watches and each has different performance characteristics. Let‚Äôs unpack them.&lt;/p&gt;
    &lt;p&gt;https://kubernetes.io/docs/reference/using-api/api-concepts/#semantics-for-watch has some useful details about how the kube-apiserver handles the parameters of your watch:&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;resourceVersion unset: Get State and Start at Most Recent&lt;/p&gt;&lt;lb/&gt;Start a watch at the most recent resource version, which must be consistent (in detail: served from etcd via a quorum read). To establish initial state, the watch begins with synthetic "Added" events of all resources instances that exist at the starting resource version. All following watch events are for all changes that occurred after the resource version the watch started at.&lt;p&gt;resourceVersion=`"0`": Get State and Start at Any&lt;/p&gt;&lt;lb/&gt;Start a watch at any resource version; the most recent resource version available is preferred, but not required‚Ä¶. To establish initial state, the watch begins with synthetic "Added" events for all resource instances that exist at the starting resource version. All following watch events are for all changes that occurred after the resource version the watch started at.&lt;p&gt;resourceVersion=`"{value other than 0}`": Start at Exact&lt;/p&gt;&lt;lb/&gt;Start a watch at an exact resource version. The watch events are for all changes after the provided resource version. Unlike "Get State and Start at Most Recent" and "Get State and Start at Any", the watch is not started with synthetic "Added" events for the provided resource version.&lt;/quote&gt;
    &lt;p&gt;Let‚Äôs re-format the important points into a table:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;resourceVersion unset&lt;/cell&gt;
        &lt;cell role="head"&gt;resourceVersion=0&lt;/cell&gt;
        &lt;cell role="head"&gt;resourceVersion&amp;gt;0&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;p&gt;Served from kube-apiserver state (instead of etcd)&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚ùå&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚úÖ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚úÖ&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Includes an initial list&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚úÖ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚úÖ&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;‚ùå&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;So a &lt;code&gt;watch&lt;/code&gt; is often preceded by a &lt;code&gt;list&lt;/code&gt;. The &lt;code&gt;list&lt;/code&gt; provides a snapshot-in-time of a set of resources, marked with a revision number. Then you start a watch from that revision number, which will then stream to you all of the changes that have occurred since that revision.&lt;/p&gt;
    &lt;p&gt;When &lt;code&gt;resourceVersion&lt;/code&gt; is set, a watch against the kube-apiserver does not create a new watch against etcd. At startup time, a kube-apiserver creates &lt;code&gt;watch&lt;/code&gt; streams against etcd for each of the well-known standard resources. Any time a client creates a watch, the kube-apiserver handles that stream itself based on the one etcd watch stream it maintains. So while client watches can be expensive for kube-apiservers, it adds no additional load to etcd. You can horizontally scale more kube-apiservers.&lt;/p&gt;
    &lt;p&gt;Furthermore, watches are not really that bad for etcd. A watch has a beginning and an end range, and those ranges fit within the same prefixes as Range queries. With each Put we need to do a log(n) lookup in the list of watches to find watches that could match that key. But there are far far fewer watches than objects. The n is small and is done asynchronously after the write is committed anyway, so it does not affect the request time to complete a write.&lt;/p&gt;
    &lt;p&gt;Watches do create network amplification. For each write into etcd, there may be N corresponding watches for that object. That results in a lot of outbound network traffic from etcd. The kube-apiservers are on the receiving end of these watches. kube-apiservers are consolidating their own watches, but etcd is still sending a copy of the data to each kube-apiserver. While adding more kube-apiserver replicas does help with many Kubernetes scalability problems, each replica does put additional pressure on the etcd NIC. The network throughput of etcd is the most immediate hardware bottleneck of large-scale Kubernetes clusters. However, these demands are limited to just between etcd and the kube-apiservers. In a single datacenter with modern hardware there‚Äôs still plenty of potential interconnect that could be established amongst these servers.&lt;/p&gt;
    &lt;head rend="h5"&gt;Watches per node&lt;/head&gt;
    &lt;p&gt;By scaling up the number of nodes I was able to observe how many watches each node creates. Per kubelet + kube-proxy, I observe:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;4 watches of&lt;/p&gt;
        &lt;code&gt;configmaps&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;2 watches each of&lt;/p&gt;&lt;code&gt;pods&lt;/code&gt;,&lt;code&gt;secrets&lt;/code&gt;,&lt;code&gt;services&lt;/code&gt;,&lt;code&gt;nodes&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;1 watch each of&lt;/p&gt;&lt;code&gt;namespaces&lt;/code&gt;,&lt;code&gt;endpoints&lt;/code&gt;,&lt;code&gt;csidrivers&lt;/code&gt;,&lt;code&gt;runtimeclasses&lt;/code&gt;,&lt;code&gt;endpointslices&lt;/code&gt;,&lt;code&gt;networkpolicies&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That‚Äôs 18 watches per node, so 18M watches for 1M nodes. These are only against the kube-apiserver and do not passthrough to etcd directly. With enough kube-apiservers we should be fine.&lt;/p&gt;
    &lt;head rend="h4"&gt;Update()&lt;/head&gt;
    &lt;p&gt;Let‚Äôs revisit our 1M kubelet Lease requirement. Kubelet is issuing an &lt;code&gt;Update&lt;/code&gt; (aka &lt;code&gt;Replace&lt;/code&gt;, aka PUT) of its Lease resource every 10 seconds.&lt;/p&gt;
    &lt;p&gt;This is an old Lease:&lt;/p&gt;
    &lt;code&gt;apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  creationTimestamp: "2025-06-26T18:27:28Z"
  name: my-node
  namespace: kube-node-lease
  ownerReferences:
  - apiVersion: v1
    kind: Node
    name: my-node
    uid: ef4d9943-841b-49cc-9fc2-a5faab77e63f
  resourceVersion: "1556549"
  uid: 7e2ec4e2-263f-4350-9397-76f37ceb83cd
spec:
  holderIdentity: my-node
  leaseDurationSeconds: 40
  renewTime: "2025-07-01T21:41:50.646654Z"&lt;/code&gt;
    &lt;p&gt;This is the body of calling Update() when renewing that Lease:&lt;/p&gt;
    &lt;code&gt;apiVersion: coordination.k8s.io/v1
kind: Lease
metadata:
  creationTimestamp: "2025-06-26T18:27:28Z"
  name: my-node
  namespace: kube-node-lease
  ownerReferences:
  - apiVersion: v1
    kind: Node
    name: my-node
    uid: ef4d9943-841b-49cc-9fc2-a5faab77e63f
  resourceVersion: "1556549"
  uid: 7e2ec4e2-263f-4350-9397-76f37ceb83cd
spec:
  holderIdentity: my-node
  leaseDurationSeconds: 40
  renewTime: "2025-07-01T21:51:50.650000Z"&lt;/code&gt;
    &lt;p&gt;Note the &lt;code&gt;renewTime&lt;/code&gt; has been updated  to something 10 seconds later. (&lt;code&gt;renewTime&lt;/code&gt; is in fact always set to 40 seconds in the future, so we can tolerate some amount of failed or slow lease updates).&lt;/p&gt;
    &lt;p&gt;The other key field is the &lt;code&gt;resourceVersion&lt;/code&gt;. When a client sends an &lt;code&gt;Update()&lt;/code&gt; to a kube-apiserver, it includes the same &lt;code&gt;resourceVersion&lt;/code&gt; from the previous version of the resource it‚Äôs updating. This is for safety to ensure that no other client has updated the resource in-between.  Every time a resource is updated on the server, the server assigns the new resource a monotonically-increasing new resourceVersion.  An Update operation must include a resourceVersion that indicates what the old version of the resource it thinks it‚Äôs replacing. That way we‚Äôre not accidentally overwriting some other change that has happened in-between.&lt;/p&gt;
    &lt;p&gt;You‚Äôd think that kube-apiserver could simply convert this Update operation into a &lt;code&gt;Txn-Put&lt;/code&gt; operation in etcd, passing through this command in a straightforward and stateless way. Unfortunately kube-apiserver‚Äôs Update implementation also always needs to obtain the entire Old version of this resource. There‚Äôs a few reasons for this:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Server-side fields: some resources have fields such as ‚Äòstatus‚Äô and ‚ÄòmanagedFields‚Äô that are only ever updated by the server.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Admission checks: the Admission check interface takes both the old and the new resource.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So to keep Update calls performant, kube-apiserver will maintain Watch caches of most commonly-used resources. When an Update occurs, it‚Äôll pull the old version from its local watch cache. If for some reason the old version is not in the watch cache, then kube-apiserver will first issue a Range to etcd to get the old resource before calling &lt;code&gt;Txn-Put&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Having to do two synchronous calls to etcd for each Update would double our QPS needs and latency, so it‚Äôs much better if we can rely on an up-to-date watch cache.&lt;/p&gt;
    &lt;p&gt;However this introduces a new requirement and constraint for 1M nodes: kube-apiservers must be able to ‚ÄúWatch‚Äù at a rate of at least 100K events/sec.&lt;/p&gt;
    &lt;p&gt;In my testing this is where things get a little tight.&lt;/p&gt;
    &lt;head rend="h5"&gt;Caching and locking&lt;/head&gt;
    &lt;p&gt;kube-apiserver is deserializing (and, more critically, allocating memory for) 100K nested dictionaries per second. It stores these in a cache, backed by a B-Tree protected with a RWMutex. That RWMutex is under heavy contention:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Update()&lt;/code&gt;calls that are attempting to read the cache for the old objects.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Update()&lt;/code&gt;calls that complete (&lt;code&gt;GuaranteedUpdate()&lt;/code&gt;finalizer) are writing the new value into the cache&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Events from the etcd Watch stream is also writing new values into the cache&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Adding more kube-apiservers helps reduce the contention caused by Update, but it doesn‚Äôt reduce the watch load - each kube-apiserver still needs to be able to keep up with the full watch stream of every change that occurs. And adding more kube-apiserver replicas puts additional strain on etcd - most critically its ability to push copies of the watch stream out to the network to each kube-apiserver.&lt;/p&gt;
    &lt;p&gt;It‚Äôs a relatively recent change that the kube-apiserver cache is backed by a B-Tree. Previously it was backed by a hash map. This was enabled with feature flag &lt;code&gt;BtreeWatchCache&lt;/code&gt; which became &lt;code&gt;true&lt;/code&gt; by default in Kubernetes 1.32. As far as I can tell, the motivation to move to B-Tree was for faster &lt;code&gt;List()&lt;/code&gt; response. Remember that &lt;code&gt;List()&lt;/code&gt; needs to return items in sorted order, so keeping the items in a B-Tree will make that much faster. But &lt;code&gt;Get()&lt;/code&gt; and &lt;code&gt;Update()&lt;/code&gt; of existing items is now O(n log n) instead of O(1).&lt;/p&gt;
    &lt;p&gt;In my testing, I was unable to get the B-Tree-based cache to scale much more beyond 40K updates per second on a c4a-standard-72 GCP instance. The cache gets stale, unable to keep up with the stream of watch events, too much time being spent waiting for cache lock.&lt;/p&gt;
    &lt;p&gt;With the old hashmap-based cache and 11x kube-apiservers there‚Äôs enough replicas to handle the Update() load of 100K Lease updates per second.&lt;/p&gt;
    &lt;head rend="h5"&gt;Garbage collection&lt;/head&gt;
    &lt;p&gt;kube-apiservers parse and decode all resources into their individual fields. Resources with lots of fields thus create a lot of tiny objects in Go, and that puts pressure on garbage collection. Adding more kube-apiserver replicas won‚Äôt help if they all are watching the same resource event streams. There‚Äôs no real cure, but setting &lt;code&gt;GOMEMLIMIT&lt;/code&gt; and &lt;code&gt;GOGC&lt;/code&gt; can help.&lt;/p&gt;
    &lt;p&gt;I set &lt;code&gt;GOMEMLIMIT&lt;/code&gt; to a number 10-20% less than memory I have on-hand, and set &lt;code&gt;GOGC&lt;/code&gt; up to a few hundred.&lt;/p&gt;
    &lt;head rend="h3"&gt;Scheduler&lt;/head&gt;
    &lt;p&gt;It doesn‚Äôt do any good to have a 1-million node cluster if you can‚Äôt schedule pods on it. The Kubernetes scheduler is a pretty common bottleneck for large jobs. I ran a benchmark, scheduling 50K pods on 50K nodes, and it took about 4.5 minutes. That‚Äôs already uncomfortably long.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Warning&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;If you‚Äôre creating pods with some sort of replication controller like Deployment, DaemonSet, or StatefulSet, which can be a bottleneck even before the scheduler. DaemonSet creates a burst of 500 pods at a time and then waits for the Watch stream to show that those are created before proceeding (the rate depends on many factors but expect &amp;lt;5K/sec). The scheduler doesn‚Äôt even get a chance to run until those pods are created.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For this 1-million node cluster project, I set an ambitious goal of being able to schedule 1 million pods in 1 minute. Admittedly the number is somewhat arbitrary, but the symmetry with all those m's seemed nice.&lt;/p&gt;
    &lt;p&gt;I also wanted to keep full compatibility with the standard kube-scheduler. It would be far easier to write a simplified scheduler from scratch that scales impressively in narrow scenarios but then fails spectacularly in real-world use cases. There‚Äôs a lot of complexity in the existing scheduler that arises from being battle-tested across lots of different production environments. Stripping away those pesky features to make a ‚Äúfaster‚Äù scheduler would be misleading.&lt;/p&gt;
    &lt;p&gt;So, we‚Äôre going to preserve the functionality and implementation of the kube-scheduler as much as we can. What‚Äôs getting in our way to making it more scalable?&lt;/p&gt;
    &lt;p&gt;kube-scheduler works by keeping state of all nodes, and then has a O(n*p) loop, where for each pod it evaluates it against every node. First it filters out nodes that the pod wouldn‚Äôt fit at all. Then, for each remaining node, it calculates a score on how well that node would match the pod. The pod is then scheduled to the highest-scoring node, or a random choice among the highest-scoring nodes if there‚Äôs a tie.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;p&gt;Tip&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;The kube-scheduler has some techniques to improve performance:&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This is parallelizable. And to be fair, the scheduler does parallelize the filtering and generation of scores of nodes against a particular pod. But the scheduler is still burdened by having to do it for all nodes. This isn‚Äôt just parallelizable, this can also be distributable.&lt;/p&gt;
    &lt;head rend="h4"&gt;Basic design: shard on nodes&lt;/head&gt;
    &lt;p&gt;It‚Äôs akin to the classic scatter/gather design of a distributed search system. Think of each node as a document in the corpus, and each pod as a search query. The query is fanned out to many shards, each responsible for a fraction of the documents. Each shard selects its top candidate(s) and sends them back to a central gatherer to ultimately identify the overall top result.&lt;/p&gt;
    &lt;p&gt;Generic scatter-gather design pattern&lt;/p&gt;
    &lt;p&gt;The key difference is that in search, documents are read-only and thus queries can be evaluated in parallel without conflicts. In scheduling, however, executing a decision actually modifies the documents (i.e. allocates node resources). If two pods are scheduled in parallel to the same node, one may succeed while the other must fail due to insufficient resources.&lt;/p&gt;
    &lt;p&gt;Nevertheless, for a large cluster it‚Äôs reasonable to design for ‚Äúoptimistic concurrency‚Äù. That is, presume that multiple pods can be scheduled at the same time without conflicts. We still check to see if a conflict arises before committing. And if a conflict occurs, we still do the correct thing by ‚Äúrolling back‚Äù the other pod, e.g. it has to be re-scheduled. But the chance and resulting impact of this happening is low - low enough that you get much higher throughput by running in parallel and absorbing the wasted effort if it does happen.&lt;/p&gt;
    &lt;p&gt;So my initial architecture idea of the distributed scheduler:&lt;/p&gt;
    &lt;p&gt;Scatter-gather as a Kubernetes pod scheduler pattern&lt;/p&gt;
    &lt;p&gt;The Relay starts a watch on unscheduled pods against the kube-apiserver. As streams of pods come in, the Relay forwards them to different schedulers.&lt;lb/&gt; Each Scheduler is responsible for doing filtering and scoring against its own subset of the overall nodes, then sending back to Relay the top-winning node and score.&lt;/p&gt;
    &lt;p&gt;The Relay then aggregates these scores, picks the overall winner, sends a true/false back to the Scheduler, and that true/false dictates whether the scheduler should actually bind the pod to that node.&lt;/p&gt;
    &lt;p&gt;The scheduler here is thus a slightly modified version of the upstream kube-scheduler. It has a custom gRPC server endpoint for receiving the new pod. It has custom code to know which of the overall nodes it is responsible for. And it has a custom Permit extension point for sending the proposed node back to the Relay. The Permit extension point runs after the nodes are filtered and scored, but before the pod is bound. Permit extensions return ‚Äòtrue‚Äô or ‚Äòfalse‚Äô to approve whether or not the pod should be scheduled on the specified node.&lt;/p&gt;
    &lt;p&gt;This is the basic design and it works pretty well. It doesn‚Äôt quite work up to 1M node scale - we‚Äôll talk about that next - but it delivers a much more scalable scheduler solution than what exists today, while preserving all of the nuanced complex battle-tested logic of the current system.&lt;/p&gt;
    &lt;p&gt;Today‚Äôs scheduler is effectively O(n x p), where n is the number of nodes and p the number of pods. That complexity becomes untenable as n grows. The sharded approach helps counteract the scaling problem: if you have n nodes, then you can shard the work across r replicas where r is some factor of n, turning that large factor back into something more tractable.&lt;/p&gt;
    &lt;p&gt;I should mention there‚Äôs one fairly large exception, and that‚Äôs pod evictions. Pod eviction can occur when there‚Äôs a new pod to schedule, but there‚Äôs not enough available resources currently on the cluster to schedule that pod. When this occurs, the scheduler does a scan across all pods currently running in the cluster, trying to identify a set of lower-priority pods that, if they were to be killed, would leave enough space for this new pod. To be fair, I didn‚Äôt implement this. You could squint at the current approach and imagine how we could also distribute the work of eviction calculation, but I didn‚Äôt do it.&lt;/p&gt;
    &lt;head rend="h4"&gt;The painful long tail: running large distributed systems in reality&lt;/head&gt;
    &lt;p&gt;On my hardware, a single scheduler was able to filter and score a pod against 1K nodes in about 1ms. So we could do 1K pods on 1K nodes in 1s. Remember the goal was 1M pods on 1M nodes in 60s. Recall that the overall work is O(n x p) (each pod has to be evaluated against each node), so going from 1K pods and nodes to 1M pods &amp;amp; 1M nodes is not a factor of 1K more work, but 1K*1K, or 1 million times more work. Even allowing ourselves 60s instead of 1s, we‚Äôre going to need a lot more schedulers.&lt;/p&gt;
    &lt;head rend="h5"&gt;Add more relays and distribute the score gathering&lt;/head&gt;
    &lt;p&gt;In fact, we‚Äôre going to need so many schedulers that a single relay simply doesn‚Äôt have enough network bandwidth to send to all of them in enough time. We need multiple relays. In fact we actually need multiple levels of relays to reach all of the schedulers.&lt;/p&gt;
    &lt;p&gt;Similarly the gathering stage, of collecting all scores and determining a winner, can also be distributed. Each scheduler and relay has a Score &lt;code&gt;Gather`&lt;/code&gt; endpoint, and it‚Äôs determined via a hash of the pod name to determine which scheduler is responsible for gathering the scores of a particular pod.&lt;/p&gt;
    &lt;p&gt;Here is a simplified example of what more relays looks like. This is with a fanout of 3, while in reality I used a fanout of 10. I was aiming to maximize but not exceed the maximum transmit throughput of each NIC to transmit 1M * 4K of Pod data in 60 seconds.&lt;/p&gt;
    &lt;p&gt;Note that it‚Äôs packed. Not all schedulers are on the same level.&lt;/p&gt;
    &lt;head rend="h5"&gt;Fight long-tail latency&lt;/head&gt;
    &lt;p&gt;My goal was to see linear time reductions as I added more replicas. In reality, I started hitting a plateau, where no matter how many more replicas I added, things remained the same or even got worse. While on average, most of the schedulers were doing less work and thus finishing more quickly, it became more frequent to see one or two stragglers that were not faster at all. This was a problem because I needed all schedulers to report back their best node before we could pick a winner.&lt;/p&gt;
    &lt;p&gt;There‚Äôs a well-known Google paper by Jeff Dean called The Tail of Scale that talks about exactly this problem. Our servers aren‚Äôt running real-time OSes and software. They are busy with all sorts of miscellaneous background tasks; observability, upgrades, garbage collection. Garbage collection is a big problem in Golang if you‚Äôre trying to write tightly coordinated software. It interrupts a currently-running task or defers a queued one. Suddenly a task that usually takes 300 microseconds spikes to take 1 millisecond. With enough individual servers, inevitably someone is always taking that 1 millisecond. If you have tightly timed coordinated systems that rely on everyone to respond before proceeding, 99% of your servers are waiting for that long-tail 1% to finish.&lt;/p&gt;
    &lt;p&gt;So I implemented a few things to reduce this problem:&lt;/p&gt;
    &lt;p&gt;Use pinned CPUs. It‚Äôs a way to ensure that cpu cores are dedicated to one single container‚Äôs processes, not context switching between various random processes. In kubelet this is done via ‚ÄúCPU Manager Policy‚Äù. Just specifying this made my tasks much more consistently performing.&lt;/p&gt;
    &lt;p&gt;Tweak garbage collection. Increasing GOGC above 100 can reduce the amount of overall time spent in garbage collection at the expense of using more memory. Using an aggressive GOGC plus a GOMEMLIMIT near towards your actual memory limit is a fantastic way to ensure that you only do GC when you really need to. I use a GOGC value of 800 and a GOMEMLIMIT set to 90% of the container‚Äôs memory limit.&lt;/p&gt;
    &lt;p&gt;Give up on stragglers. Simply don‚Äôt wait for the last N% to respond. This can effectively cut off your long tail. Beware that if there‚Äôs one consistently slow node, then this can create a feedback loop, hammering that server with more and more requests will just make it slower until it totally melts down.&lt;/p&gt;
    &lt;p&gt;Really you should just go read the The Tail of Scale paper, it covers several other possible scenarios and fixes.&lt;/p&gt;
    &lt;p&gt;One thing that I did not do is overlap workloads across multiple servers. I could‚Äôve assigned each kube node to multiple shards, so that any one of them can calculate and score the pods on that node. I worried this would result in too many cases of data inconsistency, where the node became over-subscribed with pods because the various shards were not consistent with one-another about which pods had been scheduled on that node.&lt;/p&gt;
    &lt;head rend="h5"&gt;Replace watcher with AdmissionWebHook&lt;/head&gt;
    &lt;p&gt;This one remains a bit of a mystery. The kube-scheduler typically learns about pods to schedule by doing a watch with fieldSelector ‚Äòspec.nodeName=‚Äô (meaning, pods that have no current nodeName set). When creating a lot of pods quickly (over &amp;gt;5K/sec), the watch stream would frequently stall for tens of seconds at a time.&lt;/p&gt;
    &lt;p&gt;This was one particularly bad example:&lt;/p&gt;
    &lt;p&gt;Sometimes even though there‚Äôd be plenty of pods to schedule, the watch stream had stalled so badly that the scheduler would be starved of pods to process.&lt;/p&gt;
    &lt;p&gt;To overcome this, I made the scheduler take a rather extreme change of interface. Rather than creating a watch, I made the scheduler a ValidatingWebhook. This made it so the kube-apiserver would hit an HTTP endpoint on the scheduler with every new pod that was created, in line with the create request. Typical Validating Webhooks are used for security, to approve or deny some resource fields from being set by particular clients. In this case, the scheduler approved all pods. It was merely a way for it to learn about every new pod faster (and synchronously) than by using a watch stream.&lt;/p&gt;
    &lt;head rend="h4"&gt;Results&lt;/head&gt;
    &lt;p&gt;Created a 100K node cluster and then timed how long it takes to schedule 100K pods. The pods have no nodeSelector or affinity.&lt;/p&gt;
    &lt;p&gt;Each scheduler ran on a dedicated c4d-standard-32; that is 32 AMD Turin cores and 128GB of DDR5 RAM. Experiments where dist-schedulers that had more than 1 replica also had 1 dedicated dist-scheduler-relay VM.&lt;/p&gt;
    &lt;p&gt;Each dist-scheduler replica is configured to run 30 separate internal schedulers, each with a parallelism setting of ‚Äò2‚Äô.&lt;/p&gt;
    &lt;p&gt;The default-scheduler is kube-scheduler 1.32.3 with no modifications.&lt;/p&gt;
    &lt;p&gt;One puzzling result is how much better a 1x dist-scheduler performed than the default-scheduler. Adjusting the &lt;code&gt;parallelism&lt;/code&gt; setting had no impact on the performance nor the CPU-seconds, which seemed to peak at about 20 (leaving 12 cores free).&lt;/p&gt;
    &lt;p&gt;Note that adding replicas to dist-scheduler did result in a more-or-less linear time improvement. In other words, doubling the number of dist-scheduler replicas results in a halving of the time to completion. This trend continues to the 256x replica/1M pod scale as we‚Äôll see in the next section.&lt;/p&gt;
    &lt;head rend="h2"&gt;Experiments&lt;/head&gt;
    &lt;head rend="h3"&gt;1M nodes, 1M pods with kwok&lt;/head&gt;
    &lt;head rend="h4"&gt;Test setup:&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;kube-apiserver: 5x c4d-standard-192s running kube-apiservers via k3s v1.32.4+k3s1.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;kube-scheduler and kube-controller-manager v1.32.4 each run as a separate process on the same VMs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;code&gt;feature-gates=kube:BtreeWatchCache=false&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;No cloud-controller-manager, traefik, or servicelb&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;etcd: 1x c4d-highmem-16 running custom mem_etcd implementation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;kubelet:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;For dist-scheduler: 285x c4d-highcpu-32‚Äôs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;For kwok: 7x c4a-highcpu-32‚Äôs&lt;/p&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Both running kubelet via k3s v1.32.4+k3s1&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;kwok: 100x pods running a modified version of kwok v0.6.0&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Pod scheduler: 289 replicas (8670 AMD Turin cores) of custom distributed scheduler implementation, consisting of 256 schedulers and 29 relays.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Procedure&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Start all VMs. Wait for kwok and dist-scheduler to be fully running&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create 1M nodes via make_nodes&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wait for 1M nodes and 1M leases to be present in the etcd database&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Create 1M pods via create-pods&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Wait for all pods to have a spec.nodeName&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Results&lt;/head&gt;
    &lt;p&gt;In the below graphs, the green line annotation indicates when the first pod was created and the red line indicates when the millionth pod was scheduled.&lt;/p&gt;
    &lt;head rend="h5"&gt;etcd&lt;/head&gt;
    &lt;head rend="h5"&gt;kube-apiserver&lt;/head&gt;
    &lt;head rend="h5"&gt;scheduler&lt;/head&gt;
    &lt;head rend="h3"&gt;Kwok vs kubelet&lt;/head&gt;
    &lt;p&gt;So far all of these experiments are run using kwok instead of real kubelets. But how realistic is that? It‚Äôs possible kwok doesn‚Äôt generate nearly the same pattern of load as real kubelets, and so these experiments wouldn‚Äôt be representative of a real-life cluster with 1M nodes each running kubelet.&lt;/p&gt;
    &lt;p&gt;Unfortunately running 1M real kubelets is beyond my budget. But maybe we can run a smaller-scale experiment with real kubelets and measure how its workload compares to an equivalent-sized kwok cluster.&lt;/p&gt;
    &lt;p&gt;With some careful configuration, I can run a test of a 100K kubelet cluster. The trick is to run many kubelets at once all on the same VM. They each run in separate Linux namespaces off of the host. They each have their own IPv6 address and range from which they can allocate pod addresses. They each run their own copy of containerd with which to allocate nested pods.&lt;/p&gt;
    &lt;p&gt;Deploying and managing 100K kubelet containers across lots of VMs sounds difficult. If only there were software to orchestrate this‚Ä¶ Aha! We can create a Kubernetes Deployment of kubelets!&lt;/p&gt;
    &lt;p&gt;There are still some single points of contention because of the shared kernel. Kube-proxy by default uses iptables, and changes to iptables are done with a mutex. Nftables is more performant and more friendly to concurrency but nevertheless remains a bottleneck. So we will do better to have lots of small VMs rather than fewer big ones to spread out our concurrency constraints.&lt;/p&gt;
    &lt;p&gt;Additionally, for the IPv6 subnetworks of each kubelet to be reachable from the cloud provider, we need to propagate neighbor advertisement packets. I deploy ndppd on each VM (as a DaemonSet) to do this.&lt;/p&gt;
    &lt;head rend="h4"&gt;Test setup&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;kube-apiserver: 6x c4d-standard-192s running kube-apiservers via k3s v1.32.4+k3s1.&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;No cloud-controller-manager, traefik, or servicelb&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;etcd: 1x c4d-highmem-8 running custom mem_etcd implementation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Kubelet:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;
            &lt;p&gt;For kubelet-in-pod: 426x c4d-highmem-8‚Äôs (3408 AMD Turin cores and 27264GiB of RAM)&lt;/p&gt;
            &lt;list rend="ul"&gt;
              &lt;item&gt;
                &lt;p&gt;Running k3s v1.32.4+k3s1&lt;/p&gt;
              &lt;/item&gt;
            &lt;/list&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;For kwok: 2x c4a-highcpu-32‚Äôs&lt;/p&gt;
          &lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;kubelet-pod: Running a slightly modified version of the k3s v1.32.4+k3s1 image where&lt;/p&gt;&lt;code&gt;libjansson&lt;/code&gt;is installed, which adds json support to nftables, required for kubelet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Procedure&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Wait for cluster to boot and all VM nodes to go Ready&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Deploy Deployment of kubelet-as-pod.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Scale up to 100K replicas&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture graphs of kube-apiserver and etcd load&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tear down and re-create cluster&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Deploy kwok. Create 100K kwok nodes.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Capture graphs of kube-apiserver and etcd load&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Results&lt;/head&gt;
    &lt;head rend="h5"&gt;etcd&lt;/head&gt;
    &lt;head rend="h5"&gt;kube-apiserver&lt;/head&gt;
    &lt;head rend="h2"&gt;Conclusion: How large can a Kubernetes cluster be?&lt;/head&gt;
    &lt;p&gt;The truth is that cluster size matters far less than the rate of operations on any single resource Kind‚Äîespecially creates and updates. Operations on different Kinds are isolated: each runs in its own goroutine protected by its own mutex. You can even shard across multiple etcd clusters by resource kind, so cross-kind modifications scale relatively independently.&lt;/p&gt;
    &lt;p&gt;The biggest source of writes is ususally Lease updates that keep Nodes alive. That makes cluster size fundamentally constrained by how quickly the system can process those updates.&lt;/p&gt;
    &lt;p&gt;A standard etcd setup on modern hardware sustains roughly 50,000 modifications per second. With careful sharding (separate etcd clusters for Nodes, Leases, and Pods), you could likely support around 500,000 nodes with standard etcd.&lt;/p&gt;
    &lt;p&gt;Replacing etcd with a more scalable backend shifts the bottleneck to the kube-apiserver‚Äôs watch cache. Each resource Kind today is guarded by a single RWMutex over a B-tree. Replacing that with a hash map can likely support ~100,000 events/second, enough to support 1 million nodes on current hardware. To go beyond that, increase the Lease interval (e.g., &amp;gt;10s) to reduce modification rate.&lt;/p&gt;
    &lt;p&gt;At scale, the biggest aggregate limiter is Go‚Äôs garbage collector. The kube-apiserver creates and discards vast numbers of small objects when parsing and decoding resources, and this churn drives GC pressure. Adding more kube-apiserver replicas doesn‚Äôt help, since all of them are subscribed to the same event streams.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to run yourself&lt;/head&gt;
    &lt;p&gt;See the RUNNING file for instructions on how to run a cluster yourself.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45611252</guid><pubDate>Thu, 16 Oct 2025 22:04:57 +0000</pubDate></item><item><title>Titan submersible‚Äôs $62 SanDisk memory card found undamaged at wreckage site</title><link>https://www.tomshardware.com/pc-components/microsd-cards/tragic-oceangate-titan-submersibles-usd62-sandisk-memory-card-found-undamaged-at-wreckage-site-12-stills-and-nine-videos-have-been-recovered-but-none-from-the-fateful-implosion</link><description>&lt;doc fingerprint="a50f2c926b2b1f56"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Tragic Titan submersible‚Äôs $62 SanDisk memory card found undamaged at wreckage site ‚Äî 12 stills and nine videos have been recovered, but none from the fateful OceanGate implosion&lt;/head&gt;
    &lt;p&gt;The specialist camera was rated to 6,000m, but the lens and some of its components were probably damaged by the implosion.&lt;/p&gt;
    &lt;p&gt;Recovery teams working on the Titan submersible have found the vessel's specialist stills and video camera intact. Fascinatingly, while there was some damage to the camera‚Äôs housing and internal components, tech and science enthusiast Scott Manley reveals that the internal SD card was ‚Äúundamaged.‚Äù Contents of the memory card have since been investigated, and 12 stills and nine videos have been recovered.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The recovery teams found a hardened underwater camera in the wreckage of the Titan submersible, and inside the casing was an undamaged SD card. pic.twitter.com/QCOtdcS7dUOctober 15, 2025&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Click 'See more' for images.&lt;/p&gt;
    &lt;p&gt;In the images, you can see a SubC-branded Rayfin Mk2 Benthic Camera, recovered from the wreckage of the ill-fated submersible operated by OceanGate. This still and video camera is rated to withstand depths up to 6,000m (19,685 feet, 3,281 fathoms). The titanium and synthetic sapphire crystal constructed device features both onboard and expansion memory (the titular SD card).&lt;/p&gt;
    &lt;head rend="h2"&gt;Casing intact, lens shattered but remained in place&lt;/head&gt;
    &lt;p&gt;Inside the camera's tube-like form, it is observed that the PCBs had suffered some slight damage. For example, connectors between two boards were sheared off, and some surface mount components were similarly damaged.&lt;/p&gt;
    &lt;p&gt;In some images of the PCB that are shared, you will notice details are blurred at the request of the Canada-based underwater imaging specialist and Rayfin Mk2 Benthic Camera manufacturer. However, SubC‚Äôs requested trade secret obfuscation hasn‚Äôt stopped internet sleuths asserting that they know exactly what components have been redacted.&lt;/p&gt;
    &lt;p&gt;Picking through Manley's Tweet thread replies, the key PCBs in the camera were likely an Inforce 6601 System on Module (SoM) based on the Qualcomm SD820 processor, which comes with 4GB of RAM on board and 64GB of UFS storage. Another component is thought to be the Teensy 4.0 or 3.2, which acted as an MCU. Last but not least, the undamaged SD card is almost certainly a SanDisk Extreme Pro 512GB ($62.99 at the time of writing), though it was de-branded in the photos.&lt;/p&gt;
    &lt;head rend="h2"&gt;Data recovery process and results&lt;/head&gt;
    &lt;p&gt;With an undamaged SD card, of course, investigators (and others) were interested in what details of the tragedy may have been captured and stored by this camera system. The first step was to make ‚Äúan exact binary image of the SD card‚Äù so the original could be left untampered.&lt;/p&gt;
    &lt;p&gt;Get Tom's Hardware's best news and in-depth reviews, straight to your inbox.&lt;/p&gt;
    &lt;p&gt;Check out Manley‚Äôs Tweet thread, containing investigator report screengrabs, to take in the full gamut of back and forth between the data forensics investigator, Canada‚Äôs Transportation Safety Board of Canada (TSB), and SubC. To cut a long technical story short, though, the parties eventually met up at a lab/office in Newfoundland, where a recovered NVRAM chip and SD card image were interfaced with a ‚Äúsurrogate SoM board.‚Äù This did the trick, and 12 still and nine videos were recovered!&lt;/p&gt;
    &lt;p&gt;Recovered images were at a resolution of 4,056 x 3,040 pixels, implying a pretty common 12.3MP sensor was used by the SubC Rayfin Mk2 Benthic Camera. Videos were at a more standard 3,840 x 2,160 pixels ‚Äì commonly referred to as 4K Ultra HD (UHD) video.&lt;/p&gt;
    &lt;p&gt;Somewhat disappointingly, the images and videos shared in the report were taken in the vicinity of the ROV shop at the Marine Institute, also in Newfoundland. The location was the logistical base for Titanic dive missions. No deep-sea shenanigans around the Titanic wreck were revealed. Manley explains in his Twitter thread that ‚Äúthe camera had been configured to dump data onto an external storage device, so nothing was found from the accident dive.‚Äù Nothing particularly pertinent to the tragic accident, that is.&lt;/p&gt;
    &lt;p&gt;Follow Tom's Hardware on Google News, or add us as a preferred source, to get our latest news, analysis, &amp;amp; reviews in your feeds.&lt;/p&gt;
    &lt;p&gt;Mark Tyson is a news editor at Tom's Hardware. He enjoys covering the full breadth of PC tech; from business and semiconductor design to products approaching the edge of reason.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45613898</guid><pubDate>Fri, 17 Oct 2025 06:39:09 +0000</pubDate></item><item><title>Is Postgres read heavy or write heavy?</title><link>https://www.crunchydata.com/blog/is-postgres-read-heavy-or-write-heavy-and-why-should-you-care</link><description>&lt;doc fingerprint="ea4bf8ff29727bda"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Is Postgres Read Heavy or Write Heavy? (And Why You Should You Care)&lt;/head&gt;
    &lt;p&gt;10 min readMore by this author&lt;/p&gt;
    &lt;p&gt;When someone asks about Postgres tuning, I always say ‚Äúit depends‚Äù. What ‚Äúit‚Äù is can vary widely but one major factor is the read and write traffic of a Postgres database. Today let‚Äôs dig into knowing if your Postgres database is read heavy or write heavy.&lt;/p&gt;
    &lt;p&gt;Of course write heavy or read heavy can largely be inferred from your business logic. Social media app - read heavy. IoT logger - write heavy. But ‚Ä¶. Many of us have mixed use applications. Knowing your write and read load can help you make other decisions about tuning and architecture priorities with your Postgres fleet.&lt;/p&gt;
    &lt;p&gt;Understanding whether a Postgres database is read-heavy or write-heavy is paramount for effective database administration and performance tuning. For example, a read-heavy database might benefit more from extensive indexing, query caching, and read replicas, while a write-heavy database might require optimizations like faster storage, efficient WAL (Write-Ahead Log) management, table design considerations (such as fill factor and autovacuum tuning) and careful consideration of transaction isolation levels.&lt;/p&gt;
    &lt;p&gt;By reviewing a detailed read/write estimation, you can gain valuable insights into the underlying workload characteristics, enabling informed decisions for optimizing resource allocation and improving overall database performance.&lt;/p&gt;
    &lt;head rend="h3"&gt;Read and writes are not really equal&lt;/head&gt;
    &lt;p&gt;The challenge here in looking at Postgres like this is that reads and writes are not really equal.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Postgres reads data in whole 8kb units, called blocks on disk or pages once they‚Äôre part of the shared memory. The cost of reading is much lower than writing. Since the most frequently used data generally resides in the shared buffers or the OS cache, many queries never need additional physical IO and can return results just from memory.&lt;/item&gt;
      &lt;item&gt;Postgres writes by comparison are a little more complicated. When changing an individual tuple, Postgres needs to write data to WAL defining what happens. If this is the first write after a checkpoint, this could include a copy of the full data page. This also can involve writing additional data for any index changes, toast table changes, or toast table indexes. This is the direct write cost of a single database change, which is done before the commit is accepted. There is also the IO cost for writing out all dirty page buffers, but this is generally done in the background by the background writer. In addition to these write IO costs, the data pages need to be in memory in order to make changes, so every write operation also has potential read overhead as well.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That being said - I‚Äôve worked on a query using internal table statistics that loosely estimates read load and write load.&lt;/p&gt;
    &lt;head rend="h2"&gt;Query Postgres for read and write traffic&lt;/head&gt;
    &lt;p&gt;This query leverages Postgres‚Äô internal metadata to provide an estimation of the number of disk pages (or blocks) that have been directly affected by changes to a given number of tuples (rows). This estimation is crucial for understanding the read/write profile of a database, which in turn can inform optimization strategies (see below).&lt;/p&gt;
    &lt;p&gt;The query's logic is broken down into several Common Table Expressions (CTEs) to enhance readability and modularity:&lt;/p&gt;
    &lt;p&gt;ratio_target CTE:&lt;/p&gt;
    &lt;p&gt;This initial CTE is designed to establish a predefined threshold. It allows the user to specify a target ratio of read pages per write page. This ratio serves as the primary criteria for classifying a database or table as either read-heavy or write-heavy.&lt;/p&gt;
    &lt;p&gt;I‚Äôve set the ratio in the query to 5 reads : 1 write, which means that roughly 20% of the database activity would be writes in this case. This is a bit of a fudge factor number and the exact definition of what makes up a write-heavy database may differ. If you set to 100, it would consider 100 reads to be equivalent to 1 write, or 1%; this is to allow you to tweak the definitions here for the classifications.&lt;/p&gt;
    &lt;p&gt;By defining this threshold explicitly, the query provides a flexible mechanism for evaluating different performance characteristics based on specific application requirements. For instance, a higher ratio_target might indicate a preference for read-intensive operations, while a lower one might suggest a workload dominated by writes.&lt;/p&gt;
    &lt;p&gt;table_list CTE&lt;/p&gt;
    &lt;p&gt;This CTE is responsible for the core calculations necessary to determine the read and write page counts. It performs the following key functions:&lt;/p&gt;
    &lt;p&gt;Total read pages:&lt;/p&gt;
    &lt;p&gt;It calculates the total number of pages that are typically read for the tables under consideration. This metric is fundamental to assessing the read demand placed on the database.&lt;/p&gt;
    &lt;p&gt;Estimated changed pages for writes:&lt;/p&gt;
    &lt;p&gt;To estimate the number of pages affected by write operations, the table_list CTE utilizes the existing relpages (total pages) and reltuples (total tuples) statistics from the pg_class system catalog. By calculating the ratio of relpages to reltuples, the query derives an estimated density of tuples per page. This density is then applied to the observed number of tuple writes to project how many physical pages were likely impacted by these write operations. This approach provides a practical way to infer disk I/O related to writes without needing to track every individual page modification.&lt;/p&gt;
    &lt;p&gt;Final comparison and classification&lt;/p&gt;
    &lt;p&gt;After the table_list CTE has computed the estimated read pages and write-affected pages, the final stage of the query involves a comparative analysis. The calculated number of read pages is directly compared against the estimated number of write pages. Based on this comparison, and in conjunction with the ratio_target defined earlier, the query then classifies each table (or the database as a whole) into one of several categories. These categories typically include:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Read-heavy: This classification is applied when the proportion of read pages significantly outweighs the write pages, based on the defined ratio_target.&lt;/item&gt;
      &lt;item&gt;Write-heavy: Conversely, this classification indicates that write operations are more prevalent, with a higher number of write-affected pages relative to read pages.&lt;/item&gt;
      &lt;item&gt;Other scenarios: The query can also identify other scenarios, such as balanced workloads where read and write operations are roughly equivalent, or cases where the data volume is too low to make a definitive classification.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The read/write Postgres query:&lt;/p&gt;
    &lt;code&gt;WITH
ratio_target AS (SELECT 5 AS ratio),
table_list AS (SELECT
 s.schemaname,
 s.relname AS table_name,
 -- Sum of heap and index blocks read from disk (from pg_statio_user_tables)
 si.heap_blks_read + si.idx_blks_read AS blocks_read,
 -- Sum of all write operations (tuples) (from pg_stat_user_tables)
s.n_tup_ins + s.n_tup_upd + s.n_tup_del AS write_tuples,
relpages * (s.n_tup_ins + s.n_tup_upd + s.n_tup_del ) / (case when reltuples = 0 then 1 else reltuples end) as blocks_write
FROM
 -- Join the user tables statistics view with the I/O statistics view
 pg_stat_user_tables AS s
JOIN pg_statio_user_tables AS si ON s.relid = si.relid
JOIN pg_class c ON c.oid = s.relid
WHERE
 -- Filter to only show tables that have had some form of read or write activity
(s.n_tup_ins + s.n_tup_upd + s.n_tup_del) &amp;gt; 0
AND
 (si.heap_blks_read + si.idx_blks_read) &amp;gt; 0
 )
SELECT *,
 CASE
   -- Handle case with no activity
   WHEN blocks_read = 0 and blocks_write = 0 THEN
     'No Activity'
   -- Handle write-heavy tables
   WHEN blocks_write * ratio &amp;gt; blocks_read THEN
     CASE
       WHEN blocks_read = 0 THEN 'Write-Only'
       ELSE
         ROUND(blocks_write :: numeric / blocks_read :: numeric, 1)::text || ':1 (Write-Heavy)'
     END
   -- Handle read-heavy tables
   WHEN blocks_read &amp;gt; blocks_write * ratio THEN
     CASE
       WHEN blocks_write = 0 THEN 'Read-Only'
       ELSE
         '1:' || ROUND(blocks_read::numeric / blocks_write :: numeric, 1)::text || ' (Read-Heavy)'
     END
   -- Handle balanced tables
   ELSE
     '1:1 (Balanced)'
 END AS activity_ratio
FROM table_list, ratio_target
ORDER BY
 -- Order by the most active tables first (sum of all operations)
 (blocks_read + blocks_write) DESC;
&lt;/code&gt;
    &lt;p&gt;Results will look something like this:&lt;/p&gt;
    &lt;code&gt;schemaname |¬† table_name ¬† | blocks_read | write_tuples | blocks_write | ratio | ¬† ¬† activity_ratio

- -----------+---------------+-------------+--------------+--------------+-------+------------------------

public ¬† ¬† | audit_logs¬† ¬† | ¬† ¬† ¬† ¬† ¬† 2 |¬† ¬† ¬† 1500000 |¬† ¬† ¬† ¬† 18519 | ¬† ¬† 5 | 9259.5:1 (Write-Heavy)
public ¬† ¬† | orders¬† ¬† ¬† ¬† | ¬† ¬† ¬† ¬† ¬† 8 |¬† ¬† ¬† ¬† ¬† ¬† 4 | ¬† ¬† ¬† ¬† ¬† -0 | ¬† ¬† 5 | Read-Only
public ¬† ¬† | articles¬† ¬† ¬† | ¬† ¬† ¬† ¬† ¬† 2 | ¬† ¬† ¬† ¬† ¬† 10 |¬† ¬† ¬† ¬† ¬† ¬† 1 | ¬† ¬† 5 | 0.5:1 (Write-Heavy)
public ¬† ¬† | user_profiles | ¬† ¬† ¬† ¬† ¬† 1 |¬† ¬† ¬† ¬† ¬† ¬† 3 | ¬† ¬† ¬† ¬† ¬† -0 | ¬† ¬† 5 | Read-Only
&lt;/code&gt;
    &lt;head rend="h3"&gt;pg_stat_statements&lt;/head&gt;
    &lt;p&gt;Another way to look at read and write traffic is through the pg_stat_statements extension. It aggregates statistics for every unique query run on your database. It also will collect data about Postgres queries row by row.&lt;/p&gt;
    &lt;p&gt;While the above query accounts for a bit more distribution in workload, pg_stat_statements is also a good checkpoint for traffic volume.&lt;/p&gt;
    &lt;code&gt;SELECT
  SUM(CASE WHEN query ILIKE 'SELECT%' THEN rows ELSE 0 END) AS rows_read,
   SUM(CASE WHEN query ILIKE 'INSERT%' OR query ILIKE 'UPDATE%' OR query ILIKE 'DELETE%' THEN rows ELSE 0 END) AS rows_written
FROM pg_stat_statements;

 cache_hits | disk_reads | rows_read | rows_written
------------+------------+-----------+--------------
      27586 |        998 |    443628 |           30
(1 row)
&lt;/code&gt;
    &lt;head rend="h2"&gt;Performance Tuning for High Write Traffic in Postgres&lt;/head&gt;
    &lt;p&gt;For write-heavy systems, the bottleneck is often I/O and transaction throughput. You're constantly writing to the disk, which is slower than reading from memory.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Faster Storage: The most direct way to improve write performance is to use faster storage, such as NVMe SSDs, and provision more I/O operations per second (IOPS).&lt;/item&gt;
      &lt;item&gt;Postgres 18 now has asynchronous I/O which should be more performant than traditional methods.&lt;/item&gt;
      &lt;item&gt;More RAM: While reads benefit from RAM for caching too, writes also benefit from a larger shared_buffers pool, which can hold more dirty pages before they need to be flushed to disk.&lt;/item&gt;
      &lt;item&gt;I/O burst systems: Many cloud based systems come with extra I/O out of the box, so looking at these numbers may also be helpful.&lt;/item&gt;
      &lt;item&gt;Minimize Indexes: While essential for reads, every index needs to be updated during a write operation. Over-indexing can significantly slow down writes so remove unused indexes.&lt;/item&gt;
      &lt;item&gt;Utilizing HOT updates: Postgres has a performance improvement for frequently updated rows that are indexed, so adjusting fill factor to take advantage of this could be worth looking into.&lt;/item&gt;
      &lt;item&gt;Tune the WAL (Write-Ahead Log): The WAL is where every change is written before it's committed to the main database files. Tuning parameters like wal_buffers can reduce the number of disk flushes and improve write performance.&lt;/item&gt;
      &lt;item&gt;Optimize Checkpoints: Checkpoints sync the data from shared memory to disk. Frequent or large checkpoints can cause I/O spikes. Adjusting checkpoint_timeout and checkpoint_completion_target can smooth out these events.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Performance tuning for read traffic&lt;/head&gt;
    &lt;p&gt;For read-heavy systems, the primary goal is to get data to the user as quickly as possible and ideally have much data in the buffer cache so it is not reading from disk.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Effective Caching: Ensure your shared_buffers and effective_cache_size are configured to take advantage of available RAM. This lets Postgres keep frequently accessed data in memory, avoiding costly disk reads.&lt;/item&gt;
      &lt;item&gt;Optimize Queries and Indexes: Use EXPLAIN ANALYZE to pinpoint slow SELECT queries and add indexes on columns used in WHERE clauses, JOIN conditions, and ORDER BY statements. Remember, indexes speed up lookups at the cost of slower writes.&lt;/item&gt;
      &lt;item&gt;Scaling out with read replicas: A read replica is a copy of your primary database that's kept in sync asynchronously. All write operations go to the primary, but you can distribute read queries across one or more replicas. This distributes the read load, offloads traffic from your primary server, and can dramatically improve read throughput without impacting your write performance.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Most Postgres databases are read heavy&lt;/head&gt;
    &lt;p&gt;Most Postgres databases are going to be far more read heavy than write heavy. I estimate just based on experience that 10:1 reads to writes is probably something where it is starting to get write heavy. Of course, there are outliers to this.&lt;/p&gt;
    &lt;p&gt;The right scaling strategy depends entirely on your workload. By proactively monitoring your Postgres stats using internal statistics in the Postgres catalog, you can make informed decisions that will keep your database healthy and your application fast.&lt;/p&gt;
    &lt;p&gt;Co-authored with Elizabeth Christensen&lt;/p&gt;
    &lt;head rend="h2"&gt;Related Articles&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Is Postgres Read Heavy or Write Heavy? (And Why You Should You Care)&lt;p&gt;10 min read&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;PostGIS Performance: Indexing and EXPLAIN&lt;p&gt;3 min read&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Postgres Migrations Using Logical Replication&lt;p&gt;7 min read&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Postgres 18: OLD and NEW Rows in the RETURNING Clause&lt;p&gt;2 min read&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt;Postgres‚Äô Original Project Goals: The Creators Totally Nailed It&lt;p&gt;9 min read&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45619108</guid><pubDate>Fri, 17 Oct 2025 17:06:16 +0000</pubDate></item><item><title>Chen-Ning Yang, Nobel laureate, dies at 103</title><link>https://www.chinadaily.com.cn/a/202510/18/WS68f3170ea310f735438b5bf2.html</link><description>&lt;doc fingerprint="73451408afd08a0a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;China's first Nobel laureate, Yang Chen-Ning, dies, aged 103&lt;/head&gt;
    &lt;p&gt;Yang Chen-Ning, a world-renowned physicist and Nobel laureate, passed away in Beijing on Saturday at 103.&lt;/p&gt;
    &lt;p&gt;Yang, an academician of the Chinese Academy of Sciences, professor at Tsinghua University, and the honorary president of the Institute for Advanced Study at Tsinghua, died after an illness, the university said in an obituary, calling the late professor "immortal".&lt;/p&gt;
    &lt;p&gt;Together with his colleague Tsung-dao Lee, Yang was awarded the Nobel Prize in Physics in 1957 for their theory of parity non-conservation in weak interaction.&lt;/p&gt;
    &lt;p&gt;He was often ranked alongside Albert Einstein as one of the 20th century's greatest physicists.&lt;/p&gt;
    &lt;p&gt;Born in Hefei, Anhui province, in 1922, Yang moved with his family to Tsinghua in 1929. He enrolled at the National Southwestern Associated University in 1938 and later entered the graduate school of Tsinghua University in 1942, earning a master's degree in science in 1944. In 1945, he went to the United States for further studies as a Tsinghua University government-sponsored student, attending the University of Chicago, where he received his PhD in 1948 and remained for postdoctoral work.&lt;/p&gt;
    &lt;p&gt;He joined the Institute for Advanced Study in Princeton in 1949, becoming a permanent member in 1952 and a professor in 1955. In 1966, he was appointed as the Albert Einstein Professor of Physics at the State University of New York at Stony Brook, working there until 1999.&lt;/p&gt;
    &lt;p&gt;Since 1986, he had been a visiting professor at the Chinese University of Hong Kong. From 1997, he served as the honorary director of the newly established Center for Advanced Study ‚Äî now the Institute for Advanced Study ‚Äî at Tsinghua University and became a Tsinghua professor in 1999.&lt;/p&gt;
    &lt;p&gt;Yang, having made seminal contributions to modern physics, is recognized as one of the most eminent physicists of the 20th century. His work with Robert Mills on the "Yang-Mills theory" laid the foundation for the Standard Model of particle physics and is regarded as one of the cornerstones of modern physics, comparable in significance to Maxwell's equations and Einstein's theory of general relativity.&lt;/p&gt;
    &lt;p&gt;In collaboration with Tsung-Dao Lee, he proposed the non-conservation of parity in weak interactions, a revolutionary idea for which they were jointly awarded the Nobel Prize in Physics in 1957, becoming the first Chinese Nobel laureate.&lt;/p&gt;
    &lt;p&gt;Yang was a foreign member of more than ten academies of sciences worldwide and received honorary doctoral degrees from over twenty renowned universities.&lt;/p&gt;
    &lt;p&gt;Yang maintained a deep affinity for his homeland and made outstanding contributions to China's scientific and educational development. His first visit to the People's Republic of China in 1971 helped initiate a wave of visits by overseas Chinese scholars, earning him recognition as a pioneer in building academic bridges between China and the United States.&lt;/p&gt;
    &lt;p&gt;He later proposed the restoration and strengthening of basic scientific research to China's central leadership and personally raised funds to establish a committee for educational exchange with China ‚Äî sponsoring nearly a hundred Chinese scholars for further studies in the US. Many of those scholars later became key figures in China's scientific and technological advancement. Yang played a significant role in promoting domestic scientific exchange and progress, offering crucial advice on major national scientific projects and policies.&lt;/p&gt;
    &lt;p&gt;Upon his return to Tsinghua, he dedicated himself to the development of the Institute for Advanced Study, investing immense effort into the growth of basic disciplines like physics and the cultivation of talent at Tsinghua, significantly impacting the reform and development of China's higher education.&lt;/p&gt;
    &lt;p&gt;The life of Professor Yang was that of an immortal legend ‚Äî exploring the unknown with a timeless echo of a heart devoted to his nation, the obituary said.&lt;/p&gt;
    &lt;p&gt;Yang's century-long journey constitutes an eternal chapter shining among the stars of humanity, it said.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Musical medley: Super Bund Music Festival kicks off in Shanghai&lt;/item&gt;
      &lt;item&gt;Physicist Chen-Ning Yang dies at 103&lt;/item&gt;
      &lt;item&gt;China improves regulations on personal information outbound transfer&lt;/item&gt;
      &lt;item&gt;Generative AI users in China reach 515m&lt;/item&gt;
      &lt;item&gt;China Focus: China achieves numerous breakthroughs in space exploration quest&lt;/item&gt;
      &lt;item&gt;Rediscovering the magnetism of Yan'an for China's youth&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45625229</guid><pubDate>Sat, 18 Oct 2025 05:47:14 +0000</pubDate></item><item><title>./watch</title><link>https://dotslashwatch.com/</link><guid isPermaLink="false">https://news.ycombinator.com/item?id=45626130</guid><pubDate>Sat, 18 Oct 2025 09:55:06 +0000</pubDate></item><item><title>EQ: A video about all forms of equalizers</title><link>https://www.youtube.com/watch?v=CLAt95PrwL4</link><description>&lt;doc fingerprint="7055905545553646"&gt;
  &lt;main&gt;
    &lt;p&gt;About Press Copyright Contact us Creators Advertise Developers Terms Privacy Policy &amp;amp; Safety How YouTube works Test new features NFL Sunday Ticket ¬© 2025 Google LLC&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45626349</guid><pubDate>Sat, 18 Oct 2025 10:51:02 +0000</pubDate></item><item><title>Lux: A luxurious package manager for Lua</title><link>https://github.com/lumen-oss/lux</link><description>&lt;doc fingerprint="ac056e38d961f788"&gt;
  &lt;main&gt;
    &lt;head rend="h4"&gt;A luxurious package manager for Lua.&lt;/head&gt;
    &lt;p&gt;Key Features ‚Ä¢ How To Use ‚Ä¢ Comparison with Luarocks ‚Ä¢ Related Projects ‚Ä¢ Contributing&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Create and manage Lua projects &lt;list rend="ul"&gt;&lt;item&gt;Easily manage dependencies, build steps and more through the &lt;code&gt;lux.toml&lt;/code&gt;file.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Easily manage dependencies, build steps and more through the &lt;/item&gt;
      &lt;item&gt;Parallel builds and installs üöÄ&lt;/item&gt;
      &lt;item&gt;Add/remove dependencies with simple CLI commands&lt;/item&gt;
      &lt;item&gt;Automatic generation of rockspecs &lt;list rend="ul"&gt;&lt;item&gt;Say goodbye to managing 10 different rockspec files in your source code üéâ&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Integrated code formatting via &lt;code&gt;lx fmt&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Powered by &lt;code&gt;stylua&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Powered by &lt;/item&gt;
      &lt;item&gt;Easily specify compatible Lua versions &lt;list rend="ul"&gt;&lt;item&gt;Lux will take care of Lua header installation automatically&lt;/item&gt;&lt;item&gt;Forget about users complaining they have the wrong Lua headers installed on their system&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Automatic EmmyLua/LuaCATS based type checking via &lt;code&gt;lx check&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Powered by &lt;code&gt;emmylua-analyzer-rust&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Powered by &lt;/item&gt;
      &lt;item&gt;Automatic code linting via &lt;code&gt;lx lint&lt;/code&gt;&lt;list rend="ul"&gt;&lt;item&gt;Powered by &lt;code&gt;luacheck&lt;/code&gt;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Powered by &lt;/item&gt;
      &lt;item&gt;Powerful lockfile support &lt;list rend="ul"&gt;&lt;item&gt;Makes for fully reproducible developer environments.&lt;/item&gt;&lt;item&gt;Makes Lux easy to integrate with Nix!&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Fully compatible &lt;list rend="ul"&gt;&lt;item&gt;Works with existing luarocks packages.&lt;/item&gt;&lt;item&gt;Have a complex rockspec that you don't want to rewrite to TOML? No problem! Lux allows the creation of an &lt;code&gt;extra.rockspec&lt;/code&gt;file, everything just works.&lt;/item&gt;&lt;item&gt;Have a very complex build script? Lux can shell out to &lt;code&gt;luarocks&lt;/code&gt;if it knows it has to preserve maximum compatibility.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Automatically adds project dependencies to a &lt;code&gt;.luarc.json&lt;/code&gt;file so they can be picked up by&lt;code&gt;lua-language-server&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Warning&lt;/p&gt;
    &lt;p&gt;Lux, while generally functional, is a work in progress and does not have a &lt;code&gt;1.0&lt;/code&gt; release yet.&lt;/p&gt;
    &lt;p&gt;Feel free to consult the documentation on how to get started with Lux!&lt;/p&gt;
    &lt;p&gt;It features a tutorial and several guides to make you good at managing Lua projects.&lt;/p&gt;
    &lt;p&gt;As this project is still a work in progress, some luarocks features have not been (fully) implemented yet. On the other hand, lux has some features that are not present in luarocks.&lt;/p&gt;
    &lt;p&gt;The following table provides a brief comparison:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;lux&lt;/cell&gt;
        &lt;cell role="head"&gt;luarocks v3.12.2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;project format&lt;/cell&gt;
        &lt;cell&gt;TOML / Lua&lt;/cell&gt;
        &lt;cell&gt;Lua&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;add/remove dependencies&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;parallel builds/installs&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;proper lockfile support with integrity checks&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå (basic, dependency versions only)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;run tests with busted&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;linting with luacheck&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;code formatting with stylua&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;automatic lua detection/installation&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;default build specs&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;custom build backends&lt;/cell&gt;
        &lt;cell&gt;‚úÖ1&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;rust-mlua&lt;/code&gt; build spec&lt;/cell&gt;
        &lt;cell&gt;‚úÖ (builtin)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ (external build backend)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;&lt;code&gt;treesitter-parser&lt;/code&gt; build spec&lt;/cell&gt;
        &lt;cell&gt;‚úÖ (builtin)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ (external build backend)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;install pre-built binary rocks&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;install multiple packages with a single command&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;install packages using version constraints&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;auto-detect external dependencies and Lua headers with &lt;code&gt;pkg-config&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;resolve multiple versions of the same dependency at runtime&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;pack and upload pre-built binary rocks&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;luarocks.org manifest namespaces&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;luarocks.org dev packages&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;versioning&lt;/cell&gt;
        &lt;cell&gt;SemVer2&lt;/cell&gt;
        &lt;cell&gt;arbitrary&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;rockspecs with CVS/Mercurial/SVN/SSCM sources&lt;/cell&gt;
        &lt;cell&gt;‚ùå (YAGNI3)&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;static type checking&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;generate a &lt;code&gt;.luarc&lt;/code&gt; file with dependencies&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;git dependencies in local projects&lt;/cell&gt;
        &lt;cell&gt;‚úÖ&lt;/cell&gt;
        &lt;cell&gt;‚ùå&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Lux includes the following packages and libraries:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;lux-cli&lt;/code&gt;: The main CLI for interacting with projects and installing Lua packages from the command line.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua&lt;/code&gt;: The Lux Lua API, which provides:&lt;list rend="ul"&gt;&lt;item&gt;&lt;code&gt;lux.loader&lt;/code&gt;for resolving dependencies on&lt;code&gt;require&lt;/code&gt;at runtime.&lt;/item&gt;&lt;item&gt;A work-in-progress API for embedding Lux into Lua applications. We provide builds of &lt;code&gt;lux-lua&lt;/code&gt;for Lua 5.1, 5.2, 5.3, 5.4 and Luajit.&lt;code&gt;lux-cli&lt;/code&gt;uses&lt;code&gt;lux-lua&lt;/code&gt;for commands like&lt;code&gt;lx lua&lt;/code&gt;,&lt;code&gt;lx run&lt;/code&gt;and&lt;code&gt;lx path&lt;/code&gt;.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lib&lt;/code&gt;: The Lux library for Rust. A dependency of&lt;code&gt;lux-cli&lt;/code&gt;and&lt;code&gt;lux-lua&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note&lt;/p&gt;
    &lt;p&gt;We do not yet provide a way to install &lt;code&gt;lux-lua&lt;/code&gt; as a Lua library using Lux.
See #663.
Lux can detect a lux-lua installation using pkg-config
or via the &lt;code&gt;LUX_LIB_DIR&lt;/code&gt; environment variable.&lt;/p&gt;
    &lt;p&gt;Our pre-built binary release artifacts are bundled with &lt;code&gt;lux-lua&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Dependencies:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;gnupg&lt;/code&gt;,&lt;code&gt;libgpg-error&lt;/code&gt;and&lt;code&gt;gpgme&lt;/code&gt;(*nix only)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If building without the &lt;code&gt;vendored&lt;/code&gt; feature:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;lua&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;libgit2&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;openssl&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If building with the &lt;code&gt;vendored&lt;/code&gt; feature:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;perl&lt;/code&gt;and&lt;code&gt;perl-core&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;make&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To link &lt;code&gt;gpgme&lt;/code&gt; statically on Linux and macOS, set the environment variable
&lt;code&gt;SYSTEM_DEPS_LINK=static&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We usually recommend building with the &lt;code&gt;vendored&lt;/code&gt; feature enabled,
to statically link &lt;code&gt;lua&lt;/code&gt;, &lt;code&gt;libgit2&lt;/code&gt; and &lt;code&gt;openssl&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;SYSTEM_DEPS_LINK="static" cargo build --locked --profile release --features vendored&lt;/code&gt;
    &lt;p&gt;Or, to build with dynamically linked libraries:&lt;/p&gt;
    &lt;code&gt;cargo build --locked --profile release&lt;/code&gt;
    &lt;p&gt;On Windows/MSVC, you must disable the &lt;code&gt;gpgme&lt;/code&gt; feature:&lt;/p&gt;
    &lt;code&gt;cargo build --locked --profile release --no-default-features --features lua54,vendored&lt;/code&gt;
    &lt;p&gt;You can build &lt;code&gt;lux-lua&lt;/code&gt; for a given Lua version with:&lt;/p&gt;
    &lt;code&gt;cargo xtask51 dist-lua # lux-lua for Lua 5.1
cargo xtask52 dist-lua # for Lua 5.2
cargo xtask53 dist-lua # ...
cargo xtask54 dist-lua
cargo xtaskjit dist-lua&lt;/code&gt;
    &lt;p&gt;This will install &lt;code&gt;lux-lua&lt;/code&gt; to &lt;code&gt;target/dist/share/lux-lua/&amp;lt;lua&amp;gt;/lux.so&lt;/code&gt;
and a pkg-config &lt;code&gt;.pc&lt;/code&gt; file to &lt;code&gt;target/dist/lib/lux-lua*.pc&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;To build completions:&lt;/p&gt;
    &lt;code&gt;cargo xtask dist-completions&lt;/code&gt;
    &lt;p&gt;To build man pages:&lt;/p&gt;
    &lt;code&gt;cargo xtask dist-man&lt;/code&gt;
    &lt;p&gt;To build the binary distributions for your platform, bundled with completions, man pages and &lt;code&gt;lux-lua&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;cargo xtask dist-package&lt;/code&gt;
    &lt;p&gt;If you would like to use the latest version of lux with Nix, you can import our flake. It provides an overlay and packages for:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;lux-cli&lt;/code&gt;: The Lux CLI package.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua51&lt;/code&gt;The Lux Lua API for Lua 5.1&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua52&lt;/code&gt;The Lux Lua API for Lua 5.2&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua53&lt;/code&gt;The Lux Lua API for Lua 5.3&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-lua54&lt;/code&gt;The Lux Lua API for Lua 5.4&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;lux-luajit&lt;/code&gt;The Lux Lua API for Luajit&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you have a &lt;code&gt;lux-lua&lt;/code&gt; build and &lt;code&gt;pkg-config&lt;/code&gt; in a Nix devShell,
Lux will auto-detect &lt;code&gt;lux-lua&lt;/code&gt;.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;luarocks - The original Lua package manager&lt;/item&gt;
      &lt;item&gt;rocks.nvim - A Neovim plugin manager that uses &lt;code&gt;luarocks&lt;/code&gt;under the hood, and will soon be undergoing a rewrite to use Lux instead.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Credits go to the Luarocks team for maintaining luarocks and luarocks.org for as long as they have. Without their prior work Lux would not be possible.&lt;/p&gt;
    &lt;p&gt;Contributions are more than welcome! See CONTRIBUTING.md for a guide.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Lux is licensed under LGPL-3.0+.&lt;/item&gt;
      &lt;item&gt;The Lux logo ¬© 2025 by Kai Jakobi is licensed under CC BY-NC-SA 4.0.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Footnotes&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Supported via a compatibility layer that uses luarocks as a backend. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mostly compatible with the luarocks version parser, which allows an arbitrary number of version components. To comply with SemVer, we treat anything after the third version component (except for the specrev) as a prerelease/build version. ‚Ü©&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45626961</guid><pubDate>Sat, 18 Oct 2025 12:53:32 +0000</pubDate></item><item><title>SQL Anti-Patterns</title><link>https://datamethods.substack.com/p/sql-anti-patterns-you-should-avoid</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45626985</guid><pubDate>Sat, 18 Oct 2025 12:56:50 +0000</pubDate></item><item><title>Ripgrep 15.0</title><link>https://github.com/BurntSushi/ripgrep/releases/tag/15.0.0</link><description>&lt;doc fingerprint="3cab1bfb2c6333d9"&gt;
  &lt;main&gt;
    &lt;p&gt;ripgrep 15 is a new major version release of ripgrep that mostly has bug fixes,&lt;lb/&gt; some minor performance improvements and minor new features.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;In case you haven't heard of it before, ripgrep is a line-oriented search&lt;/p&gt;&lt;lb/&gt;tool that recursively searches the current directory for a regex pattern.&lt;lb/&gt;By default, ripgrep will respect gitignore rules and automatically skip&lt;lb/&gt;hidden files/directories and binary files.&lt;/quote&gt;
    &lt;p&gt;Here are some highlights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Several bugs around gitignore matching have been fixed. This includes&lt;lb/&gt;a commonly reported bug related to applying gitignore rules from parent&lt;lb/&gt;directories.&lt;/item&gt;
      &lt;item&gt;A memory usage regression when handling very large gitignore files has been&lt;lb/&gt;fixed.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;rg -vf file&lt;/code&gt;, where&lt;code&gt;file&lt;/code&gt;is empty, now matches everything.&lt;/item&gt;
      &lt;item&gt;The &lt;code&gt;-r/--replace&lt;/code&gt;flag now works with&lt;code&gt;--json&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;A subset of Jujutsu (&lt;code&gt;jj&lt;/code&gt;) repositories are now treated as if they were git&lt;lb/&gt;repositories. That is, ripgrep will respect&lt;code&gt;jj&lt;/code&gt;'s gitignores.&lt;/item&gt;
      &lt;item&gt;Globs can now use nested curly braces.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Platform support:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;aarch64&lt;/code&gt;for Windows now has release artifacts.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;powerpc64&lt;/code&gt;no longer has release artifacts generated for it. The CI&lt;lb/&gt;release workflow stopped working, and I didn't deem it worth my time to&lt;lb/&gt;debug it. If someone wants this and can test it, I'd be happy to add it&lt;lb/&gt;back.&lt;/item&gt;
      &lt;item&gt;ripgrep binaries are now compiled with full LTO enabled. You may notice&lt;lb/&gt;small performance improvements from this and a modest decrease in binary&lt;lb/&gt;size.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Performance improvements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;PERF #2111:&lt;lb/&gt;Don't resolve helper binaries on Windows when&lt;code&gt;-z/--search-zip&lt;/code&gt;isn't used.&lt;/item&gt;
      &lt;item&gt;PERF #2865:&lt;lb/&gt;Avoid using path canonicalization on Windows when emitting hyperlinks.&lt;/item&gt;
      &lt;item&gt;PERF #3184:&lt;lb/&gt;Improve performance of large values with&lt;code&gt;-A/--after-context&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bug fixes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;BUG #829,&lt;lb/&gt;BUG #2731,&lt;lb/&gt;BUG #2747,&lt;lb/&gt;BUG #2770,&lt;lb/&gt;BUG #2778,&lt;lb/&gt;BUG #2836,&lt;lb/&gt;BUG #2933,&lt;lb/&gt;BUG #3067:&lt;lb/&gt;Fix bug related to gitignores from parent directories.&lt;/item&gt;
      &lt;item&gt;BUG #1332,&lt;lb/&gt;BUG #3001:&lt;lb/&gt;Make&lt;code&gt;rg -vf file&lt;/code&gt;where&lt;code&gt;file&lt;/code&gt;is empty match everything.&lt;/item&gt;
      &lt;item&gt;BUG #2177:&lt;lb/&gt;Ignore a UTF-8 BOM marker at the start of&lt;code&gt;.gitignore&lt;/code&gt;(and similar files).&lt;/item&gt;
      &lt;item&gt;BUG #2750:&lt;lb/&gt;Fix memory usage regression for some truly large gitignore files.&lt;/item&gt;
      &lt;item&gt;BUG #2944:&lt;lb/&gt;Fix a bug where the "bytes searched" in&lt;code&gt;--stats&lt;/code&gt;output could be incorrect.&lt;/item&gt;
      &lt;item&gt;BUG #2990:&lt;lb/&gt;Fix a bug where ripgrep would mishandle globs that ended with a&lt;code&gt;.&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;BUG #2094,&lt;lb/&gt;BUG #3076:&lt;lb/&gt;Fix bug with&lt;code&gt;-m/--max-count&lt;/code&gt;and&lt;code&gt;-U/--multiline&lt;/code&gt;showing too many matches.&lt;/item&gt;
      &lt;item&gt;BUG #3100:&lt;lb/&gt;Preserve line terminators when using&lt;code&gt;-r/--replace&lt;/code&gt;flag.&lt;/item&gt;
      &lt;item&gt;BUG #3108:&lt;lb/&gt;Fix a bug where&lt;code&gt;-q --files-without-match&lt;/code&gt;inverted the exit code.&lt;/item&gt;
      &lt;item&gt;BUG #3131:&lt;lb/&gt;Document inconsistency between&lt;code&gt;-c/--count&lt;/code&gt;and&lt;code&gt;--files-with-matches&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;BUG #3135:&lt;lb/&gt;Fix rare panic for some classes of large regexes on large haystacks.&lt;/item&gt;
      &lt;item&gt;BUG #3140:&lt;lb/&gt;Ensure hyphens in flag names are escaped in the roff text for the man page.&lt;/item&gt;
      &lt;item&gt;BUG #3155:&lt;lb/&gt;Statically compile PCRE2 into macOS release artifacts on&lt;code&gt;aarch64&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;BUG #3173:&lt;lb/&gt;Fix ancestor ignore filter bug when searching whitelisted hidden files.&lt;/item&gt;
      &lt;item&gt;BUG #3178:&lt;lb/&gt;Fix bug causing incorrect summary statistics with&lt;code&gt;--json&lt;/code&gt;flag.&lt;/item&gt;
      &lt;item&gt;BUG #3179:&lt;lb/&gt;Fix gitignore bug when searching absolute paths with global gitignores.&lt;/item&gt;
      &lt;item&gt;BUG #3180:&lt;lb/&gt;Fix a panicking bug when using&lt;code&gt;-U/--multiline&lt;/code&gt;and&lt;code&gt;-r/--replace&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Feature enhancements:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many enhancements to the default set of file types available for filtering.&lt;/item&gt;
      &lt;item&gt;FEATURE #1872:&lt;lb/&gt;Make&lt;code&gt;-r/--replace&lt;/code&gt;work with&lt;code&gt;--json&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;FEATURE #2708:&lt;lb/&gt;Completions for the fish shell take ripgrep's config file into account.&lt;/item&gt;
      &lt;item&gt;FEATURE #2841:&lt;lb/&gt;Add&lt;code&gt;italic&lt;/code&gt;to the list of available style attributes in&lt;code&gt;--color&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;FEATURE #2842:&lt;lb/&gt;Directories containing&lt;code&gt;.jj&lt;/code&gt;are now treated as git repositories.&lt;/item&gt;
      &lt;item&gt;FEATURE #2849:&lt;lb/&gt;When using multithreading, schedule files to search in order given on CLI.&lt;/item&gt;
      &lt;item&gt;FEATURE #2943:&lt;lb/&gt;Add&lt;code&gt;aarch64&lt;/code&gt;release artifacts for Windows.&lt;/item&gt;
      &lt;item&gt;FEATURE #3024:&lt;lb/&gt;Add&lt;code&gt;highlight&lt;/code&gt;color type, for styling non-matching text in a matching line.&lt;/item&gt;
      &lt;item&gt;FEATURE #3048:&lt;lb/&gt;Globs in ripgrep (and the&lt;code&gt;globset&lt;/code&gt;crate) now support nested alternates.&lt;/item&gt;
      &lt;item&gt;FEATURE #3096:&lt;lb/&gt;Improve completions for&lt;code&gt;--hyperlink-format&lt;/code&gt;in bash and fish.&lt;/item&gt;
      &lt;item&gt;FEATURE #3102:&lt;lb/&gt;Improve completions for&lt;code&gt;--hyperlink-format&lt;/code&gt;in zsh.&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45627324</guid><pubDate>Sat, 18 Oct 2025 13:44:02 +0000</pubDate></item><item><title>Root System Drawings</title><link>https://images.wur.nl/digital/collection/coll13/search</link><description>&lt;doc fingerprint="fbf286432d583069"&gt;
  &lt;main&gt;
    &lt;p&gt;Javascript Required To experience full interactivity, please enable Javascript in your browser.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45627394</guid><pubDate>Sat, 18 Oct 2025 13:52:24 +0000</pubDate></item><item><title>Flowistry: An IDE plugin for Rust that focuses on relevant code</title><link>https://github.com/willcrichton/flowistry</link><description>&lt;doc fingerprint="b283f24420974767"&gt;
  &lt;main&gt;
    &lt;p&gt;Flowistry is a tool that analyzes the information flow of Rust programs. Flowistry understands whether it's possible for one piece of code to affect another. Flowistry integrates into the IDE to provide a "focus mode" which helps you focus on the code that's related to your current task.&lt;/p&gt;
    &lt;p&gt;For example, this GIF shows the focus mode when reading a function that unions two sets together:&lt;/p&gt;
    &lt;p&gt;When the user clicks a given variable or expression, Flowistry fades out all code that does not influence that code, and is not influenced by that code. For example, &lt;code&gt;orig_len&lt;/code&gt; is not influenced by the for-loop, while &lt;code&gt;set.len()&lt;/code&gt; is.&lt;/p&gt;
    &lt;p&gt;Flowistry can be helpful when you're reading a function with a lot of code. For example, this GIF shows a real function in the Rust compiler. If you want to understand the role of a specific argument to the function, then Flowistry can filter out most of the code as irrelevant:&lt;/p&gt;
    &lt;p&gt;The algorithm that powers Flowistry was published in the paper "Modular Information Flow through Ownership" at PLDI 2022.&lt;/p&gt;
    &lt;p&gt;Table of contents&lt;/p&gt;
    &lt;p&gt;Flowistry is available as a VSCode plugin. You can install Flowistry from the Visual Studio Marketplace or the Open VSX Registry. In VSCode:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Go to the Extensions pane by clicking this button in the left margin:&lt;/item&gt;
      &lt;item&gt;Search for "Flowistry" and then click "Install".&lt;/item&gt;
      &lt;item&gt;Open a Rust workspace and wait for the tool to finish installing.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Note on platform support: Flowistry does not yet support NixOS. Flowistry cannot provide pre-built binaries for ARM targets like M1 Macs, so Flowistry must be installed from scratch on these targets (this is done for you, but will take a few more minutes than usual).&lt;/p&gt;
    &lt;p&gt;Alternatively, you can install it from source:&lt;/p&gt;
    &lt;code&gt;# Install flowistry binaries
git clone https://github.com/willcrichton/flowistry
cd flowistry
cargo install --path crates/flowistry_ide

# Install vscode extension
cd ide
npm install
npm run build
ln -s $(pwd) ~/.vscode/extensions/flowistry
&lt;/code&gt;
    &lt;p&gt;If you are interested in the underlying analysis, you can use the &lt;code&gt;flowistry&lt;/code&gt; crate published to crates.io: https://crates.io/crates/flowistry&lt;/p&gt;
    &lt;p&gt;The documentation is published here: https://willcrichton.net/flowistry/flowistry/&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Docs.rs doesn't support documentation for crates that use&lt;/p&gt;&lt;code&gt;#![feature(rustc_private)]&lt;/code&gt;so we have to host it ourselves.&lt;/quote&gt;
    &lt;p&gt;Note that the latest Flowistry has a Maximum Supported Rust Version of Rust 1.73. Flowistry is not guaranteed to work with features implemented after 1.73.&lt;/p&gt;
    &lt;p&gt;Once you have installed Flowistry, open a Rust workspace in VSCode. You should see this icon in the bottom toolbar:&lt;/p&gt;
    &lt;p&gt;Flowistry starts up by type-checking your codebase. This may take a few minutes if you have many dependencies.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: Flowistry type-checking results are cached in the&lt;/p&gt;&lt;code&gt;target/flowistry&lt;/code&gt;directory. If you delete this folder, Flowistry will have to recompute types. Also for a large codebase this directory may take up a fair amount of disk space.&lt;/quote&gt;
    &lt;p&gt;Once Flowistry has booted up, the loading icon will disappear. Then you can enter focus mode by running the "Toggle focus mode" command. By default the keyboard shortcut is Ctrl+R Ctrl+A (‚åò+R ‚åò+A on Mac), or you can use the Flowistry context menu:&lt;/p&gt;
    &lt;p&gt;In focus mode, Flowistry will automatically compute the information flow within a given function once you put your cursor there. Once Flowistry has finished analysis, the status bar will look like this:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Note: Flowistry can be a bit slow for larger functions. It may take up to 15 seconds to finish the analysis.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Flowistry infers what you want to focus on based on your cursor. So if you click on a variable, you should see the focus region of that variable. Flowistry will highlight the focused code in gray, and then fade out code outside the focus region. For example, because the user's cursor is on &lt;code&gt;view_projection&lt;/code&gt;, that variable is highlighted in gray, and its focus region is shown.&lt;/p&gt;
    &lt;p&gt;Sometimes you want to keep the focus region where it is, and click on other code to inspect it without changing focus. For this purpose, Flowistry has a concept of a "mark". Once you have selected code to focus on, you can run the "Set mark" command (Ctrl+R Ctrl+S / ‚åò+R ‚åò+S). Then a mark is set at your cursor's current position, and the focus will stay there until you run the "Unset mark" command (Ctrl+R Ctrl+D / ‚åò+R ‚åò+D).&lt;/p&gt;
    &lt;p&gt;If you want to modify all the code in the focus region, e.g. to comment it out or copy it, then you can run the "Select focused region" command (Ctrl+R Ctrl+T / ‚åò+R ‚åò+T). This will add the entire focus region into your editor's selection.&lt;/p&gt;
    &lt;p&gt;Flowistry is an active research project into the applications of information flow analysis for Rust. It is continually evolving as we experiment with analysis techniques and interaction paradigms. So it's not quite as polished or efficient as tools like Rust Analyzer, but we hope you can still find it useful! Nevertheless, there are a number of important limitations you should understand when using Flowistry to avoid being surprised.&lt;/p&gt;
    &lt;p&gt;If you have questions or issues, please file a Github issue, join our Discord, or DM @wcrichton on Twitter.&lt;/p&gt;
    &lt;p&gt;When your code has references, Flowistry needs to understand what that reference points-to. Flowistry uses Rust's lifetime information to determine points-to information. However, data structures that use interior mutability such as &lt;code&gt;Arc&amp;lt;Mutex&amp;lt;T&amp;gt;&amp;gt;&lt;/code&gt; explicitly do not share lifetimes between pointers to the same data. For example, in this snippet:&lt;/p&gt;
    &lt;code&gt;let x = Arc::new(Mutex::new(0));
let y = x.clone();
*x.lock().unwrap() = 1;
println!("{}", y.lock().unwrap());&lt;/code&gt;
    &lt;p&gt;Flowistry can determine that &lt;code&gt;*x.lock().unwrap() = 1&lt;/code&gt; is a mutation to &lt;code&gt;x&lt;/code&gt;, but it can not determine that it is a mutation to &lt;code&gt;y&lt;/code&gt;. So if you focus on &lt;code&gt;y&lt;/code&gt;, the assignment to 1 would be faded out, even though it is relevant to the value of &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We are researching methods to overcome this limitation, but for now just be aware that this is the main case where Flowistry is known to provide an incorrect answer.&lt;/p&gt;
    &lt;p&gt;Flowistry's analysis tries to include all code that could have an influence on a focal point. This analysis makes a number of assumptions for both practical and fundamental reasons. For example, in this snippet:&lt;/p&gt;
    &lt;code&gt;let mut v = vec![1, 2, 3];
let x = v.get_mut(0);
println!("{:?} {}", v, x);&lt;/code&gt;
    &lt;p&gt;If you focus on &lt;code&gt;v&lt;/code&gt; on line 3, it will include &lt;code&gt;v.get_mut(0)&lt;/code&gt; as an operation that could have modified &lt;code&gt;v&lt;/code&gt;. The reason is that Flowistry does not actually analyze the bodies of called functions, but rather approximates based on their type signatures. Because &lt;code&gt;get_mut&lt;/code&gt; takes &lt;code&gt;&amp;amp;mut self&lt;/code&gt; as input, it assumes that the vector could be modified.&lt;/p&gt;
    &lt;p&gt;In general, you should use focus mode as a pruning tool. If code is faded out, then you don't have to read it (minus the limitation mentioned above!). If it isn't faded out, then it might be relevant to your task.&lt;/p&gt;
    &lt;p&gt;Flowistry works by analyzing the MIR graph for a given function using the Rust compiler's API. Then the IDE extension lifts the analysis results from the MIR level back to the source level. However, a lot of information about the program is lost in the journey from source code to MIR.&lt;/p&gt;
    &lt;p&gt;For example, if the source contains an expression &lt;code&gt;foo.whomp.bar().baz()&lt;/code&gt;, it's possible that a temporary variable is only generated for the expression &lt;code&gt;foo.whomp.bar()&lt;/code&gt;. So if the user selects &lt;code&gt;foo&lt;/code&gt;, Flowistry may not be able to determine that this corresponds to the MIR place that represents &lt;code&gt;foo&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;This is why the IDE extension highlights the focused code in gray, so you can understand what your cursor's selection actually maps to.&lt;/p&gt;
    &lt;p&gt;Flowistry analyzes a single function at a time. If a function contains other functions, e.g. &lt;code&gt;fn&lt;/code&gt; definitions, or closures, or implicitly via async, then Flowistry will only show you focus regions within the smallest function body containing your cursor. This is usually well defined for function definitions and closures, but may be confusing for async since that depends on how rustc decides to carve up your async function.&lt;/p&gt;
    &lt;p&gt;If rustup fails, especially with an error like "could not rename downloaded file", this is probably because Flowistry is running rustup concurrently with another tool (like rust-analyzer). Until rustup#988 is resolved, there is unfortunately no automated way around this.&lt;/p&gt;
    &lt;p&gt;To solve the issue, go to the command line and run:&lt;/p&gt;
    &lt;code&gt;rustup toolchain install nightly-2023-08-25 -c rust-src -c rustc-dev -c llvm-tools-preview
&lt;/code&gt;
    &lt;p&gt;Then go back to VSCode and click "Continue" to let Flowistry continue installing.&lt;/p&gt;
    &lt;p&gt;Rust Analyzer does not support MIR and the borrow checker, which are essential parts of Flowistry's analysis. That fact is unlikely to change for a long time, so Flowistry is a standalone tool.&lt;/p&gt;
    &lt;p&gt;See Limitations for known issues. If that doesn't explain what you're seeing, please post it in the unexpected highlights issue or ask on Discord.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45627692</guid><pubDate>Sat, 18 Oct 2025 14:33:22 +0000</pubDate></item><item><title>Picturing Mathematics</title><link>https://mathenchant.wordpress.com/2025/10/18/picturing-mathematics/</link><description>&lt;doc fingerprint="d0851df7d6d09a8"&gt;
  &lt;main&gt;
    &lt;p&gt;I‚Äôm a great believer in low-tech math. I don‚Äôt like to rely on things a computer tells me; what if there‚Äôs a bug in the code? I prefer trusting things that I can check for myself. At the same time, I‚Äôm keenly aware of the limits of my imagination even when it‚Äôs aided by paper and pencil. Sometimes I need a computer to show me things I can imagine myself imagining but don‚Äôt yet know how to imagine.&lt;/p&gt;
    &lt;p&gt;ILLUSTRATING MATH TOGETHER&lt;/p&gt;
    &lt;p&gt;In 2016, the Institute for Computational and Experimental Research in Mathematics (ICERM) in Providence, Rhode Island hosted a workshop called Illustrating Mathematics with the hope of bringing together researchers who, like me, study mathematical abstractions that can be brought to life by appropriate visuals. The workshop spawned a community that has held meetings at ICERM from time to time and has been running a webinar series since 2023.&lt;/p&gt;
    &lt;p&gt;I‚Äôve spoken twice at the webinar. Back in 2024, I gave a brief mathematical eulogy for the brilliant mathematical explorer Roger Antonsen, now sadly deceased (though you can‚Äôt tell that he‚Äôs deceased from his website), who had a unique knack for coming up with cool visuals related to every topic we ever discussed. The striking figure below, arising from a deterministic model of a one-dimensional gas I‚Äôd proposed, is just one instance among many dozens he created as part of our email conversations.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;On October 10th, 2025, I spoke at the webinar for a second time, even more briefly: I gave a five-minute ‚Äúshow-and-ask‚Äù pitch as a warmup-act for the phenomenal math explainer/animator Grant Sanderson (aka 3Blue1Brown). My lightning talk was entitled ‚ÄúEvolving cross sections of Ford spheres‚Äù, and it was my way of testing the waters of the webinar crowd. I wondered: if I described a compelling mathematical object that nobody has illustrated yet in a fully satisfying manner, or at least not in a way that I find fully satisfying, and I shared with other webinar attendees my vision of how one could make that mathematical object more available to the brain by way of the eye, then could I convince others, more skilled than I in the art of computer-assisted illustration, to bring my vision into reality?&lt;/p&gt;
    &lt;p&gt;The answer proved to be a resounding ‚ÄúYes!‚Äù Roice Nelson (with whom I‚Äôve corresponded in the past) was one of several people who expressed interest, and Roice and I have moved forward with this project. Arguably I shouldn‚Äôt be spending my time this way‚ÄîI don‚Äôt plan to write any research articles on the Ford spheres. I just think that they‚Äôre cool things that other people would find interesting if they were better publicized. And they got stuck in my head like a catchy tune.&lt;/p&gt;
    &lt;p&gt;AN 87-YEAR-OLD FRACTAL&lt;/p&gt;
    &lt;p&gt;I‚Äôm sure you‚Äôve heard of fractals‚Äîthey had a moment back in the 1980s that basically never ended, with fractals penetrating not just the sciences and geek culture but popular culture as well, culminating in a line about frozen fractals in a stirring power ballad in the 2013 Disney movie Frozen. The Ford spheres form a three-dimensional fractal that not enough mathematicians know about, even though Lester Ford described it in a charming article called, simply, ‚ÄúFractions‚Äù, back in 1938‚Äîthirty-seven years before Benoit Mandelbrot coined the term ‚Äúfractal‚Äù.&lt;/p&gt;
    &lt;p&gt;There are actually many arrangements that are called Ford sphere arrangements nowadays, but the one Ford himself described looks like this:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;This image is a still from a video made by Sam Wells and Aidan Donahue. The video gives some intuition for the fractal, but (to quote another Disney movie heroine) I want more.&lt;/p&gt;
    &lt;p&gt;What makes the Ford spheres worthy of study? From a research perspective, they‚Äôre descendants of a more famous two-dimensional fractal Ford wrote about in his 1938 article. The Ford circles are geometrical surrogates for the rational numbers, and the way the circles nestle against one another turns out to reflect important facts in number theory.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;It stands to reason that the analogous three-dimensional fractals would have secrets to teach us as well.&lt;/p&gt;
    &lt;p&gt;ALL OVER THE PLACE BUT ALMOST NOWHERE&lt;/p&gt;
    &lt;p&gt;Another thing that makes the Ford spheres worthy of illustration is the way they offer math-loving non-mathematicians the chance to have their minds blown by the counterintuitive behavior of countable dense sets. The primordial example of such a set is the set of rational numbers: as elements of the real line, the rational numbers are all over the place but they‚Äôre also almost nowhere. I‚Äôve chosen my phrasing to be provocative and a little paradoxical, but in a certain mathematical sense, it‚Äôs true: hardly any real numbers are rational, but no tiniest stretch of the real line is free of them. If you zoom in on (say) the square root of two, no matter how far in you zoom, you‚Äôll keep on seeing rational numbers with ever-bigger numerators and denominators. Ford circles give geometric meaning to that bigness: the bigger those numerators and denominators are, the tinier the corresponding circles are.&lt;/p&gt;
    &lt;p&gt;All the Ford circles are tangent to a single horizontal line. One way to think about the Ford circles is as what you get when you try to pack together as many circles as you can above that line. You start by drawing evenly-spaced circles tangent to the number line at the points . . . , ‚àí2, ‚àí1, 0, 1, 2, . . . I‚Äôll just show the two circles that touch the line at 0 and 1 and hereafter ignore all the circles to the left or right of them:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Then you add a circle to fill the gap between the 0-circle and the 1-circle, tangent to the line at 1/2):&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Then you add more circles to fill the new gaps with tangencies at 1/3 and 2/3:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Then you add even more circles to fill the newer gaps with tangencies at 1/4, 2/5, 3/5, and 3/4:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;If you continue this process, the circles you‚Äôll draw are precisely the Ford circles, all tangent to the line, and the points of tangency will be all the rational numbers and nothing else.&lt;/p&gt;
    &lt;p&gt;Now imagine that, having drawn in the Ford circles (or as many of them as you have the patience to draw), you add to your picture a horizontal line parallel to, but slightly above, the line we were talking about before. This new line will intersect some of the circles. If you move that new line downward slightly, it‚Äôll intersect more of the circles. As you continue to move the new line further downward, closer and closer to the original line (which I‚Äôll call the ‚Äúlimit line‚Äù), you start to intersect more and more circles.&lt;/p&gt;
    &lt;p&gt;FROM TWO TO THREE&lt;/p&gt;
    &lt;p&gt;Ford also described a similar fractal one dimension up. We have infinitely many spheres, all tangent to the x, y plane, and the points of tangency correspond exactly to the points (x, y) with x and y rational. Here‚Äôs Ford‚Äôs sketch showing four of the infinitely many spheres:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;(Yeah, four is a lot less than infinity, but cut Ford some slack: this was before computers.)&lt;/p&gt;
    &lt;p&gt;I want to picture this complicated three-dimensional object by way of its two-dimensonal cross-sections. Here‚Äôs one of the animations Roice sent me a few days ago, as part of our ongoing work:&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;It shows what you get when you intersect the Ford spheres with a moving plane that approaches, without ever reaching, the limit plane that all the Ford spheres touch (analogous to the limit line that all the Ford circles touch). As time passes in the video and the moving plane moves on, we see a mix of growing disks and shrinking disks; the shrinking disks are cross-sections of the spheres whose centers the plane has already passed through, while the growing disks are cross-sections of the spheres whose centers still lie just a bit ahead of us. The picture becomes frothier and frothier, approaching the limit of infinite fractal frothiness.&lt;/p&gt;
    &lt;p&gt;The video is just a rough cut, but already I can see features of the image that I didn‚Äôt expect: halos and solar arches, one might call them. Perhaps one of you will find a way to make rigorous mathematics of what your eyes are telling you, but even if not, I hope the animation gives you visual pleasure.&lt;/p&gt;
    &lt;p&gt;WHY I BOTHER&lt;/p&gt;
    &lt;p&gt;If this essay inspires any of you to drop by one of the monthly meetings of the Illustrating Math webinar, visit the webinar link. Or, if you‚Äôre feeling brave and want to pitch an idea or to give a five-minute presentation of any kind, go to the show-and-ask signup sheet. Or if you just want to see what other cool visuals Roice has created, check out his website.&lt;/p&gt;
    &lt;p&gt;I realized shortly before I published this essay that there is a connection to my research, though it‚Äôs not a direct link, and that it was probably subconsciously driving me to explore the Ford spheres. Two decades ago I was looking at ‚Äúrotor-router blobs‚Äù that gave rise to images like this one, generated by Tobias Friedrich and Lionel Levine:&lt;/p&gt;
    &lt;p&gt;If you‚Äôre like me, your eyes and brain see ghostly circles (or near-circles), forming bands separated by lines of orange fire. The trouble is, those near-circles are very much creations of your eyes and brain, intermediated by software called ImageMagick. The task of figuring out what details at the pixel-level create ghostly near-circles in my brain defeated me. Such are the frustrations of ‚Äúdigital pointillism‚Äù: when we zoom in, we tend to lose sight of what we are trying to understand! It‚Äôs the problem faced by creators of monumental paintings: you have to stand close to the canvas to paint your strokes or dots or whatever, but when you stand close it‚Äôs easy to literally lose sight of the big picture. I‚Äôd like to try looking at those blobs again sometime, once I have the tools and the skills to ‚Äúinterrogate‚Äù such pictures more effectively than I could in the past.&lt;/p&gt;
    &lt;p&gt;A smaller-scale version of this gap in my skill-set manifests itself for the Ford spheres. Those halos and solar arches exist in my brain (and I hope in yours), but what do they correspond to at the pixel level? I don‚Äôt know how to ask the picture to tell me, but I‚Äôm hoping I can learn.&lt;/p&gt;
    &lt;p&gt;I‚Äôll finish by mentioning one last reason for bringing the Ford spheres from the world of fancy to the world of the senses: videos like these can convey to non-mathematicians, in a way that words and symbols can‚Äôt, what makes math so addictive to those of us who love it.&lt;/p&gt;
    &lt;p&gt;Thanks to David Jacobi and Roice Nelson.&lt;/p&gt;
    &lt;p&gt;REFERENCES&lt;/p&gt;
    &lt;p&gt;L. R. Ford, Fractions, American Mathematical Monthly, 45, 586‚Äì601.&lt;/p&gt;
    &lt;p&gt;S. Northshield, Ford circles and spheres, 2015.&lt;/p&gt;
    &lt;p&gt;C. Pickover, Beauty and Gaussian Rational Numbers, Chapter 103 (pages 243-247) in: ‚ÄúWonders of Numbers: Adventures in Mathematics, Mind, and Meaning‚Äù, Oxford University Press, 2001.&lt;/p&gt;
    &lt;p&gt;S. Wells and A. Donahue, Ford spheres, 2021.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45628283</guid><pubDate>Sat, 18 Oct 2025 15:52:39 +0000</pubDate></item><item><title>Tinnitus Neuromodulator</title><link>https://mynoise.net/NoiseMachines/neuromodulationTonesGenerator.php</link><description>&lt;doc fingerprint="b64980b949699afa"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;User Stories&lt;/head&gt;
      &lt;p&gt;Write your own here. Click the blue bullets ‚óè to load associated settings.&lt;/p&gt;
      &lt;p&gt;‚óè Been loving this project, this website for years. The sounds, the stories, the community. &amp;lt;3 This particular "noise" for me is a real lifesaver. It always helps me with (tension-) headaches. Thank you so much.&lt;lb/&gt; ‚óè I couldn't think for two days straight; it was so bad and kept getting worse. This quieted the constant chatter and noise down to a mild hum.&lt;lb/&gt; ‚óè This and a special Dreaming Nautilus almost put me to sleep for some weird reason.&lt;lb/&gt; ‚óè Neural Hack sounds so funny, that one time I laughed so hard at night listening to this, that my mom came upstairs and whooped me. All jokes aside, if anyone has trouble managing their tinnitus, then this is the one for you! 8.7 out of 10.&lt;lb/&gt; ‚óè Yes, this is pretty wonderful. I only need 3 of the sliders for it to work, thank you. Why not make a standalone device for this?&lt;lb/&gt; ‚óè It takes a moment to adjust, but once you do, all your problems warble away.&lt;lb/&gt; ‚óè I feel like I'm on an alien planet!&lt;lb/&gt; ‚óè St√©phane and Steve have created a nearly magical tinnitus solution. Whenever my tinnitus starts acting up, I pop on these settings for a few minutes, and when I take the headphones off... Silence. Absolute silence. I have no idea how this works, but it's genius!&lt;lb/&gt; ‚óè An absolute godsend! It's just tickly enough to distract me from the noise in my ears, but it doesn't crowd out my actual thoughts.&lt;lb/&gt; ‚óè You wouldn't think such an exotic, high-pitched, and beepy sound generator would be calming, but when you find the right spot and let it go for a few minutes, then pause the audio, it's so much quieter‚Äînot gone, but still a relief compared to before. Even just letting it play and noticing your ear ringing contributing to the music instead of being a distracting annoyance is calming.&lt;lb/&gt; ‚óè Once properly calibrated, after a few minutes of listening, it helps reduce the perception of tinnitus‚Äîand even after removing the headphones, the sensation almost seems to vanish. myNoise is a great tool to use when needed.&lt;lb/&gt; ‚óè I have infrequent tinnitus in one ear that gets really annoying when I'm sick. I tried this on a whim, and I straight-up can't hear it anymore! What a godsend.&lt;lb/&gt; ‚óè I've been using this neuromodulator on a daily basis for many weeks and this is by far the best help I could ever find to release my tinnitus 24/7 head drill. It doesn‚Äôt get rid of it completely, but it tones it down massively - just leaves a soft background hum that lets you live your life. Honestly, it feels like magic. No idea how it works this well. Huge thanks to the creators - you guys rock!&lt;lb/&gt; ‚óè I use the tinnitus neuromodulator when things get rough. It doesn't just mask the tinnitus, it transports me into a space where I can focus again. For a while, I feel clear. What gets me is this: when I take off the headphones, the tinnitus is gone. Not reduced‚Äîgone. Then, slowly, it creeps back. That minute of silence is everything.&lt;lb/&gt; ‚óè If you have basic audio knowledge, this website is a much better option than commercial tinnitus therapies based on overpriced hearing aids. Thanks to this website, I‚Äôm reconfiguring my relationship with tinnitus in a much more organic and spontaneous way.&lt;lb/&gt; ‚óè My tinnitus is an infrequent problem. When it gets bad, though, it gets horrible. This just saved my whole morning.&lt;lb/&gt; ‚óè I don't have tinnitus, but I love the strangeness of this generator. It sounds really fascinating and seems to help me focus. By the way, I guess this is what an aural representation (a strange tone poem?) of a ketamine trip would sound like :D&lt;lb/&gt; ‚óè I use myNoise to go to sleep.&lt;lb/&gt; ‚óè This doesn't so much drown out my tinnitus as blend with it, but it's still a fantastic change from the usual! I've had tinnitus my entire life, and as a kid, I never understood why people liked silence so much since it was so loud! Weird alien beep-boops is the new silence.&lt;lb/&gt; ‚óè Wow! I don't have tinnitus, but this is stunning!&lt;lb/&gt; ‚óè This is so satisfying to listen to, and I don't even have tinnitus.&lt;lb/&gt; ‚óè It's the best website for listening to sounds while doing productive activities.&lt;lb/&gt; ‚óè I have found "trance" to work well with outside high-pitched noises like old TVs. Nice job.&lt;lb/&gt; ‚óè This is crazy, it actually works. Thankfully my tinnitus is not constant, but very annoying when it's present. It actually makes it disappear, even after I've closed the sound generator, for quite some time.&lt;lb/&gt; ‚óè This is the first time in a while that I've been able to just sit and be content without that infernal ringing hijacking my thoughts. Definitely not something I'd be able to listen to for longer periods, but it's amazing how well my tinnitus just blends in and becomes a part of the experience.&lt;lb/&gt; ‚óè This app and noise really help me in primary school, and since I am sometimes cleverer than my peers, this helps.&lt;lb/&gt; ‚óè Thanks so much for this site. My tinnitus fluctuates a lot, and when it's bad, I rely on sounds/maskers like these for work and at night.&lt;lb/&gt; ‚óè It helps me write an interesting Egyptian story and gives a nice effect.&lt;lb/&gt; ‚óè I was using this to help me focus because I have really bad tinnitus, and it makes it so hard to focus! As soon as it started playing, it drowned out my tinnitus and made me able to think way better! The sounds seem to go all around me, and it's just so helpful. I definitely recommend MyNoise!&lt;lb/&gt; ‚óè Against all odds, it apparently works. I have tested it a couple of times today by playing the audio on headphones for 5-10 minutes, and there is a significant relief in my tinnitus for shorter periods of time. I will try running the audio for a longer time to see if I can have a longer-lasting effect.&lt;lb/&gt; ‚óè It's tingling my brain.&lt;lb/&gt; ‚óè I must be doing it wrong. Unfortunately, the sound in my head got louder as I was adjusting the sliders.&lt;lb/&gt; ‚óè I needed this sound every day in 2022 when I was having major mental issues and anxiety, which created a feedback loop for my tinnitus (tinnitus increased anxiety, anxiety increased tinnitus.) with the help of this sound generator I managed to reduce my anxiety and tinnitus to the point where it's nothing more than a passing thought every now and then, that I come back here to get rid of. THANKS: )&lt;lb/&gt; ‚óè I love this soo much.&lt;lb/&gt; ‚óè I love this beyond belief! Thank you :) &lt;lb/&gt; ‚óè It's funny, the raw noises should really make me angry but they work. I don't get it. Why does it work better than any other tinnitus blocker.&lt;lb/&gt; ‚óè I don't have tinnitus; I have MS. Certain sounds have started to bother me a lot, even waking me up. I can't sleep for longer than 2 hours some nights. It's not fun. I usually have to turn some white noise on loud enough to mask the sounds but that hurts sometimes, and is hard to sleep with. This sound however is pretty weird. I don't have to crank it up loud, and it works. I'm actually sleepy.&lt;lb/&gt; ‚óè It helps me focus on reading. I like it ;)&lt;lb/&gt; ‚óè My form of tinnitus is masked by many of your noise generators. I prefer natural sounds, but this did remarkably let me forget the constant, piercing ring in my right ear rather than just mask it.&lt;lb/&gt; ‚óè Dude thank you, I've had musical ear syndrome since I was a kid, and a life of listening to heavy metal and rock and roll at high volume on headphones and going to shows has made tinnitus worse. Now I get musical ear syndrome all the time, it can be distracting as well a bit concerning but this soundscape cured it instantly. Thank you! This saved me from a mental breakdown.&lt;lb/&gt; ‚óè All points of connectivity reaching all other points.&lt;lb/&gt; ‚óè Works wonders to cancel out the Tinnitus on both of my ears. It seems to make my ringing go away for short periods of time also. &lt;lb/&gt; ‚óè Who else heard an iphone alarm and horror movie riffs? -TDR&lt;lb/&gt; ‚óè This is a god - send! My brain never quiets down, and this lovely soundscape is a perfect solution for letting my brain latch on to something while I do things I need to. Thank you so much!&lt;lb/&gt; ‚óè It might not relieve my tinnitus, but it definitely helps me focus on my homework!&lt;lb/&gt; ‚óè Works for my ADHD as well as tinnitus :3&lt;lb/&gt; ‚óè This is the ONLY thing I've found that actually helps with my tinnitus. 10-20 minutes of listening, and I'm free from it for an hour or even longer. Thank you so, so much for this.&lt;lb/&gt; ‚óè I'm still highly skeptical if this truly works but, with the noises I selected, I like it. &lt;lb/&gt; ‚óè I just recommended this to a friend with tinnitus who used to leave the TV on all night, and we found this present that I hope works better!&lt;lb/&gt; ‚óè This helps me break free from the world. Great noise.&lt;lb/&gt; ‚óè Relief from tinnitus, for the first time in years! I can't handle listening to it for much longer than twenty minutes (because I'm bored by generated noise after a while), but when I take my headphones off, my tinnitus is so massively diminished I can hardly believe it. Compared to everything else I've unsuccessfully tried for my tinnitus, this feels like actual magic--audiomancy?&lt;lb/&gt; ‚óè Wow, this actually works! I didn't expect it, but it does, and while not completely canceling out my tinnitus, works much better than songs or such.&lt;lb/&gt; ‚óè The absolute relief I feel right now has me reeling. My Jaw dropped instantly when I plugged this baby in. My head feels about half the weight it usually does just from honestly maybe a minute total of listening to this. I didn't think there would be such a solution to tinnitus but this might just really improve my quality of life, especially in the falling asleep department! Thank you, thank you!&lt;lb/&gt; ‚óè This sound helps my tinnitus a lot and it also kind of tickles my ears a bit!&lt;lb/&gt; ‚óè Suffering from tinitus for many years, this is helping me tremendously. My tinitus sounds resemble the high pitch noise you get from old TV sets. Listening to this setting for just a couple of minutes brings me relief for up to an hour afterwards, sometimes even longer because I forget to concentrate on it. Just amazing!&lt;lb/&gt; ‚óè I started using MyNoise's ocean sounds years ago to help me sleep due to severe misophonia, it blocks the noise from the street and I don't need to sleep with earplugs or headphones anymore. Recently a covid infection damaged my ear nerves and caused intolerable sound sensitivity accompanied of tinnitus and hyperacusis. If it wasn't for these sound generators I would have lost my sanity.&lt;lb/&gt; ‚óè I have autism and difficulty focusing. I've never liked listening to things like this generator, but for the first time I quite like this! I love the way that I can hear the sounds travel from each side of my brain. It's pretty awesome to listen to when you've got synesthesia. &lt;lb/&gt; ‚óè I have no tinnitus but "Neural Drops" + RPG Evil Charm on the "Monsters" preset + Canyon drone around 1 kHz is my best mix for listening to it before lucid dreaming with out-of-body experience.&lt;lb/&gt; ‚óè For a few years I've used myNoise to block out the sounds of busy city life and to get to sleep. More recently in late 2022 I contracted covid which unfortunately left me with hearing damage and significant tinnitus in one ear. The only thing that has been able to relieve the horrible ringing so far are some of the neuromodulator tracks here, and for that I am grateful. &lt;lb/&gt; ‚óè As an autistic person with pretty debilitating hyperacusis (I pretty much can't function without ear plugs) the onset of some mild tinnitus was distressing to say the least. White noise is usually the recommended go-to for tinnitus but it's painful to me. High pitched fuzzy or low constant humming sounds are literal agony. This is varied enough to ease and accommodate both issues. Miraculous!&lt;lb/&gt; ‚óè Well it just works. I get into music stuff lately after a long session of mixing, mastering or drum take your ear just start to deafen sometimes with headaches. I used this as a tool to "reset" my ear by listening to it at a barely heard loudness and turning it down even more after I can hear it clearly, just rinse and repeat!&lt;lb/&gt; ‚óè Just wanted to add my comment to say that yes, in my experience (loud ~9kHz in left ear) this is more than a placebo effect re: tinnitus relief. HOW exciting. Cheers! Breathing deeper and slower too.&lt;lb/&gt; ‚óè When getting the automation right, it takes ages for piano or guitar sounds to get repetitive, sounds of nature and human voices (radio chatter etc) sound completely natural, organic, and at home here without feeling repetitive at all. And sci-fi sounds, and drone like instruments can pretty much do their thing forever and play well off of each other while they do it.&lt;lb/&gt; ‚óè Suffering since 11 months from tinnitus. I destressed my life, did different types of therapy but nothing came close to the effect this neural symphony has. It actually gives me seconds or minutes as close to silence as I wasn't for 11 months. THANK YOU!&lt;lb/&gt; ‚óè Amazing, always helps when silence is a bit too noisy. My uncle who has bad tinnitus commented that it helped even after a few minutes without headphones... Amazingly helpful in this era of being used to hearing fans, fridges, pinging tech and then ringing noises when it's supposed to be quiet.&lt;lb/&gt; ‚óè Wow, so interesting. It fades away quickly. Greetings from Ecuador.&lt;lb/&gt; ‚óè It takes away my tinnitus if I listen to if for a few minutes. I genuinely cannot believe this. &lt;lb/&gt; ‚óè I have had tinnitus for several years and it recently got worse causing anxiety attacks. After discovering this site I found it really works on lessening the tinnitus and I feel so much better! I play it while working from home. Amazing! &lt;lb/&gt; ‚óè I developed a high pitched whistling tinnitus 10 years ago when I was 14 and I thought I lost the sound of silence forever, but listening to this for 20 minutes on shuffle makes my tinnitus disappear completely! Even if it's just for a little while, it feels wonderful to experience true silence again.&lt;lb/&gt; ‚óè Peaceful and relaxing!&lt;lb/&gt; ‚óè Strangely, this also blocks out when my ears begin to ring highly.&lt;lb/&gt; ‚óè I have tinnitus that sounds like a cat purring. The "purring" is annoying and it causes me to think a cat is near me, a car is starting, Etcetera. These settings work strangely well.&lt;lb/&gt; ‚óè I luckily don't have tinnitus but this sounds like I've been taken by aliens! &lt;lb/&gt; ‚óè Yes!! It helps!!&lt;lb/&gt; ‚óè It definitely assisted me with my tinnitus, I feel substantially more relaxed as well 10/10. &lt;lb/&gt; ‚óè Gone! My tinnitus sounds like a dentist's drill. Sometimes it's hard to blend it out, but with this setting it's gone. Thanx, Doc!&lt;lb/&gt; ‚óè Silence! Silence, thank god! I've never felt so much joy.&lt;lb/&gt; ‚óè I dont have tinnitus but this is still a cool sound to study to.&lt;lb/&gt; ‚óè The tinnitus neuromodulator sounds empty and sinister by itself. To make it less empty and sinister, the sinewave stereo slider can combine well with the take it easy generator on the bad trip preset.&lt;lb/&gt; ‚óè This helps with misophonia, too! I put it at a low volume and listen to it during class, it's super helpful.&lt;lb/&gt; ‚óè I was a bit skeptical, but after listening for 10 minutes on a moderately high volume with ear buds, I can say the tinnitus relief is real! I'm now trying to find out how long the relief will last, and how long I should listen to the modulation sounds... &lt;lb/&gt; ‚óè I don't even have tinnitus, but I like listening to this while I write my sci-fi novel!&lt;lb/&gt; ‚óè Playing with settings was a lot of fun! I masked my tinnitus with it!&lt;lb/&gt; ‚óè It was fun playing with the settings and I lowered some and raised some, slowed it down a bit, and it's really wonderful. I think it helps with my tinnitus but it's also just fun to listen to!&lt;lb/&gt; ‚óè Wow! Being able to hear pure silence even only for a few seconds is truly something. Thank you.&lt;lb/&gt; ‚óè Ahhh this is literally so amazing, it actually helped so so so so so so much!! It got so much quieter after I took my headphones off.&lt;lb/&gt; ‚óè This.... Is good. So good. &lt;lb/&gt; ‚óè Amazing, amazing, amazing for my tinnitus. It's not especially loud, but it's very constant when it's quiet, and having this setting is surprisingly relaxing for my mind to not hear it in the background.&lt;lb/&gt; ‚óè Doesn't get rid of the tinnitus, but masks it while I listen to it.&lt;lb/&gt; ‚óè Suddenly had an onset of tinnitus last night. This setting in particular seems to be holding it down for me while I wait for my trip to an ENT doctor.&lt;lb/&gt; ‚óè Tried a lot of relief sound from youtube, but most of them are too high pitched for me and are very painful. But this one is perfect, you saved my life thank you.&lt;lb/&gt; ‚óè I constantly hear ringing whenever it's silent, and it affects my day-to-day tasks. I used this for a while, and I went somewhere silent, and there you go, no more ringing! Mind blown!&lt;lb/&gt; ‚óè Recently I had been dealing with pretty bad tinnitus, it had gotten to the point where I couldn't even fall asleep some nights. After one day of using this, I have had more relief than I have had in weeks. I am very grateful for this program, I have already donated and plan to continue supporting this.&lt;lb/&gt; ‚óè I suffer from tinnitus due to infections, and this is wonderful! &lt;lb/&gt; ‚óè I don't even have tinnitus, I just find it a cool, emotionally neutral background to stay focused at work... It feels like I'm coding on an alien spaceship!&lt;lb/&gt; ‚óè I can't say if this will work for everyone's tinnitus, but it does for mine! After listening for 20 minutes or so, my tinnitus is much reduced and stays that way for up to two hours afterward. I am interested to see if there is any long-term effect from using this generator daily.&lt;lb/&gt; ‚óè Thank you so much! My tinnitus began after a concussion 6 years ago. I have no hearing problems, just issues caused by my brain. I found your website, turned on my computer speakers &amp;amp; immediately my neck &amp;amp; shoulders relaxed as the tinnitus decreased significantly. Initially I listened for about 1/2 hour &amp;amp; when I walked away it was much lower volume than before. Best for me on the default setting.&lt;lb/&gt; ‚óè This generator is completely amazing, if for no other reason it detracts attention away from my tinnitus, which is now chronic for years and years. The Neural Symphony functions are easy to use and adjust. I have discovered that for me I drop all pulses and warble to zero, and then use the automated slider animation. Wide range of high frequencies, ever changing. Bliss.&lt;lb/&gt; ‚óè ‚Üê Click here to hear some crazy hums and beeps! &lt;lb/&gt; ‚óè This was overwhelming for me at first, but I clicked Surprise! a few times and got settings that not only hide my tinnitus but also make a pleasing sound. I love it.&lt;lb/&gt; ‚óè I have tinnitus in scattered frequencies between 8-10k, and this setting on studio monitors (at a quiet volume) works very well to calm my mind, and get my focus off of audio based stimuli. &lt;lb/&gt; ‚óè It works o_0. Tinnitus is not big problem for me because I have it since I was kid, but it's nice feeling to not hear it :D&lt;lb/&gt; ‚óè I loved it, it's kind of magical too.&lt;lb/&gt; ‚óè It's like C3PO had too much sugar lolololololol!&lt;lb/&gt; ‚óè I don't have tinnitus. But this sounds crazy! I love it! 8D&lt;lb/&gt; ‚óè Neuromodulator and Summer Night are about the only things that got me through the worst bout of tinnitus. Thank you so much!&lt;lb/&gt; ‚óè Oh my goodness. You are an absolute godsend; I wish I could have found this sooner. Will def be using this whenever I need it. Thank you so so so so much! &amp;lt;3&lt;lb/&gt; ‚óè I use this daily to focus at work and to get my mind off of my tinnitus. Also, I use this to sleep on my phone, this is the best thing that I've had in a long time.&lt;lb/&gt; ‚óè Thanks -- this doesn't seem to "fix" anything but it does help concentrate with the tinnitus is bothering me. I have great hearing but tinnitus (with high-pitched sounds in one ear and a tiny bit in the other) started a couple weeks ago after my doctor put me on 150mg Effexor. We cut the medication after this happened but the tinnitus is still loud :( Anyway thanks for your work on this site! &lt;lb/&gt; ‚óè I fortunately don't have tinnitus but it helped with my daily earaches anyway.&lt;lb/&gt; ‚óè I thought people were overreacting, my God I was wrong. It truly works.&lt;lb/&gt; ‚óè I have used myNoise numerous times now and I find great relief from my very loud and annoying tinnitus. Waiting to see if any reversal becomes evident. Thank you for this wonderful tool! &lt;lb/&gt; ‚óè It was not working for me at first. The tinnitus would go away and come back after a few seconds. I tweaked the settings so the high pitched noises were louder and more frequent. A day after I am writing this with little to no tinnitus now. Idk if this really works or not but it's worth a try.&lt;lb/&gt; ‚óè I suffer from tinnitus in both the ears and have two distinct high and low frequency patterns. I listened this for about 45 mins, kept away my headphones and then went for a coffee... Guess what I could hear `silence'! Very promising!&lt;lb/&gt; ‚óè I found that all the sounds hurt to listen to and increased or battled with my tinnitus. Disappointing! &lt;lb/&gt; ‚óè This is certainly promising, and I did notice an affect pretty quickly. However, ACRN should be tunable to the frequency of of your tinnitus. The Steve Sequence is kind of all over the place, and mostly tuned lower than my own (6kHz, and pretty common). Would be nice to see something where the frequency can be tuned, and maybe change the waveform (sine, sawtooth, fuzz, etc). [Note from editor : try the Tape Speed Control feature]&lt;lb/&gt; ‚óè Gosh that's impressive, Thank you so much for your work&lt;lb/&gt; ‚óè I don't have tinnitus, but this is really relaxing with the right settings.&lt;lb/&gt; ‚óè I have suffered tinnitus for as long as I can remember, very few things have helped lessen its obnoxious effects, and those that did (like music) were very distracting (I have ADHD too), but this generator works perfectly for me, it helps my tinnitus to the point that I don't even notice it, and even better, its not distracting in any way! Focusing has never been easier.&lt;lb/&gt; ‚óè Try Lake Life (loon calls and lapping shore water) with this - pretty funky. Keep expecting a strange interstellar artifact to light in the depths of the lake and rise in the full moon before me. Fantastical.&lt;lb/&gt; ‚óè Love it with Hydrogen XII.&lt;lb/&gt; ‚óè OH MY GOSH BEAUTIFUL SILENCE&lt;lb/&gt; ‚óè This literally worked in 10 seconds... wow!&lt;lb/&gt; ‚óè This noise generator gave me nausea and vertigo and my tinnitus was unaffected. Am I using it wrong?&lt;lb/&gt; ‚óè I must say that I didn't really have faith in it working to dampen my tinnitus, but I had to eat my words back when I took my headphones off. I was close to shedding tears at finally being close to experiencing silence after all these years of constant noise. I want to thank you for creating this, and I am most definitely going to favorite this and listen to it as much as possible.&lt;lb/&gt; ‚óè While I may not have Tinnitus, this generator is still a very entertaining experience! I sometimes use it to calm myself or focus on writing, which was surprising as I thought it would have the opposite effect!&lt;lb/&gt; ‚óè Yes. Really does improve my tinnitus. Even more, I find it actually quite soothing as ambience for indoor work - less aural fatigue than white noise. For some reason, it even makes house-cleaning less tedious :-) Kudos!&lt;lb/&gt; ‚óè As a musician, I have to deal with tinnitus on the daily. It got bad this morning, and this helped me out a ton.&lt;lb/&gt; ‚óè The tinnitus neuromodulation noises have had a proufound effect on lessening my tinnitus and I thoroughly recommend it to anyone who is seeking relief. Short of a future breakthrough in medication or surgical treatments, this is the best solution I have come across and it makes life that bit more bearable. Many, many, many thanks!&lt;lb/&gt; ‚óè After listening to the Sinescape preset for this generator for only maybe 20 minutes, I found that my rather loud tinnitus because much quieter and almost non-existent immediately after removing my headphones. I'm curious to see how long this lasts, but it's quite a relief even for a little while!&lt;lb/&gt; ‚óè This generator is the first thing that helps with my tinnitus, lowering its pitch and volume until it's almost gone even after I took off the headphones. The relief only lasts for a few hours, but to someone who has a constant high pitched noise in their ears otherwise, those hours mean the world. I cannot thank you enough for your amazing work, Stphane. You're a true blessing to the world.&lt;lb/&gt; ‚óè After listening to this for awhile, my tinnitus was actually GONE after I took off my headphones.I don't know how long this will last but thank you. I will enjoy this sweet, sweet silence while it lasts. Donation coming your way as soon as I get home! Love this site.&lt;lb/&gt; ‚óè My tinnitus is quite high-pitched and fairly constant. If I play this sequence for a little while, it brings it back down under a low roar, if only for a few hours. &amp;lt;3&lt;lb/&gt; ‚óè I cannot express my thanks! Every sound has changed my life in a different way! Thank you SO MUCH! Thank You!!! These settings seem to be the ultimate cure to my T.&lt;lb/&gt; ‚óè ‚Üê extreme warbling&lt;lb/&gt; ‚óè Holy Crap! I was experiencing on of my usual Tinnitus spikes (and you know how annoying this can be). I tried this custom noise take from one of the testimonials and my tinnitus just went down from 10 to 4! Thanks!&lt;lb/&gt; ‚óè Brilliant site I could spend hours here. I have tinnitus a different high pitched frequency in each ear.Slightly spacey space-station? A little dreamy but a little sedating as well and kills the tinnitus.&lt;lb/&gt; ‚óè This works for me - a little dreamy but a little sedating as well.&lt;lb/&gt; ‚óè This is a very strange soundscape, but in an enjoyable way. I personally do not have tinnitus, but I think this is still fun to listen to. You just have to find the right sliders to make sense of this one, hahaha.&lt;lb/&gt; ‚óè That's artificial rain... trickle, and some wind. Very relaxing during computer work! Both stimulating and calming. :)&lt;lb/&gt; ‚óè I have Tinnitus. I also have perfect hearing and from years of being a musician one would anticipate hearing loss. None. I am also a Director for an electric and water utility and I have a lot of concentrating work (reports, analysis, etc). When My Tinnitus is in full flight, it is impossible to concentrate. This is a life saver. I can work all day and feel the calming of the tones. Amazing! &lt;lb/&gt; ‚óè I am sitting in the library trying to work and someone near me has the touch-tones turned all the way up on their phone. This is the only thing that drowns it out. Thank you!!!&lt;lb/&gt; ‚óè Back again, THANK GOD for this. When I'm having a rough day with my T, I just come home, throw this on and chill and just zone out and stuff I love it. Thank you!&lt;lb/&gt; ‚óè May be my imagination, listened for about 15 min, then when taking my speakers away from my ears, my pulsating tinnitus was quieter.&lt;lb/&gt; ‚óè First I found myself heaving a relieved sigh, and then started crying when I found the custom noise that masked my tinnitus. It has been five months of unrelenting high pitched whine, yet I refused to allow myself a pity party, concluding, "It is what it is; suck it up, girl, there is little you can do." But to have this respite is a great blessing. Thank you! &lt;lb/&gt; ‚óè Kudos to all who put this wonderful chaotic neural symphony together! As a tinnitus sufferer I found the sounds had a very soothing effect on me. I will be using this selection again for sure.&lt;lb/&gt; ‚óè I noticed listening to music with different frequencies helps soften my Tinnitus. I have a constant 24/7 14,000hz sawtooth sound in my head for the two years. This program is amazing! &lt;lb/&gt; ‚óè It is soothing but I hear my tinnitus while I am listening to it. I don't really think this would help me in any way, long term. It is soothing though, so thanks for that! &lt;lb/&gt; ‚óè Not for me. Sounds more like star wars. What really helps me is a soft noise plug. Stick in the ear canal and when the sounds starts bothering switch to other ear. This is the only thing that helps me.&lt;lb/&gt; ‚óè This is incredible. I've had a fairly mild, but annoying tinnitus accompanying me for some time now and this Neural Hack makes it seem magically inexistent. I am impressed (and very relaxed).&lt;lb/&gt; ‚óè I have been listening to music with MY Tinnitus frequency embedded into the music. I understand this works for some, not me. After over 6 months, no change. The past two days have been horrible... I am a musician and although I have been playing rock music for over 40 years, I have no hearing damage at all... 20 minutes on customizing my own pattern, I can actually concentrate on my work!&lt;lb/&gt; ‚óè Thank you so much for this incredible site! I'm quite happy to make a donation to you out of simple gratitude for all your efforts putting these sounds online for us, and keeping the site (and your iOS app, myNoise) working. I've experienced persistent tinnitus for a couple decades, now, with almost no respite except after one minutes or so of "Neural Hack" or "Neural Symphony!" Amazing!&lt;lb/&gt; ‚óè I have severe tinnitus and this noise generator may actually change my life.&lt;lb/&gt; ‚óè Thank you so much for this. It provides relief for my tinnitus when nothing else does.&lt;lb/&gt; ‚óè This site is amazing! Brilliant concept and execution! As a Tinnitus sufferer, it has become safe haven for me. Thank you so much, Stephane!&lt;lb/&gt; ‚óè I'm a fairly new comer to myNoise having serendipitously discovered the site only three weeks ago and a very long sufferer of Tinnitus. I have only discovered this particular generator only moments ago and jumped right in to listen. Ironically, I've been having episodes all week so I'll absolutely write the results of my experience after one week and again at two weeks. Today is 8 June '16 &lt;lb/&gt; ‚óè Something very weird happens if I listen to this for 15-20 minutes: in addition to the tinnitus masking, my mind actually "quiets down". The parts of my brain that would normally be wandering seem to be occupied chasing down the bells and warbles, allowing the rest of my attention to focus entirely on the task at hand. Awesome brain hack.&lt;lb/&gt; ‚óè I don't have tinnitus, but I find myself enjoying the bizarre dreamlike discord that using the animation sliders can provide. It's a rather lovely chiming chaos... that is weirdly calming to me. I'm not sure what that says :)&lt;lb/&gt; ‚óè My goodness, I've been waiting for something like this. God Bless!&lt;lb/&gt; ‚óè I guess I am fortunate that I only get ringing in my ears when I take certain prescription medications I can't believe my luck that a few hours after I took the medication I started to get the ringing and came here to find some rain sounds and discovered this new generator. What a gift this is! Thank you, thank you, thank you!&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45628391</guid><pubDate>Sat, 18 Oct 2025 16:08:10 +0000</pubDate></item><item><title>Most users cannot identify AI bias, even in training data</title><link>https://www.psu.edu/news/bellisario-college-communications/story/most-users-cannot-identify-ai-bias-even-training-data</link><description>&lt;doc fingerprint="bb98193601090b0f"&gt;
  &lt;main&gt;
    &lt;p&gt;UNIVERSITY PARK, Pa. ‚Äî When recognizing faces and emotions, artificial intelligence (AI) can be biased, like classifying white people as happier than people from other racial backgrounds. This happens because the data used to train the AI contained a disproportionate number of happy white faces, leading it to correlate race with emotional expression. In a recent study, published in Media Psychology, researchers asked users to assess such skewed training data, but most users didn‚Äôt notice the bias ‚Äî unless they were in the negatively portrayed group.&lt;/p&gt;
    &lt;p&gt;The study was designed to examine whether laypersons understand that unrepresentative data used to train AI systems can result in biased performance. The scholars, who have been studying this issue for five years, said AI systems should be trained so they ‚Äúwork for everyone,‚Äù and produce outcomes that are diverse and representative for all groups, not just one majority group. According to the researchers, that includes understanding what AI is learning from unanticipated correlations in the training data ‚Äî or the datasets fed into the system to teach it how it is expected to perform in the future.&lt;/p&gt;
    &lt;p&gt;‚ÄúIn the case of this study, AI seems to have learned that race is an important criterion for determining whether a face is happy or sad,‚Äù said senior author S. Shyam Sundar, Evan Pugh University Professor and director of the Center for Socially Responsible Artificial Intelligence at Penn State. ‚ÄúEven though we don't mean for it to learn that.‚Äù&lt;/p&gt;
    &lt;p&gt;The question is whether humans can recognize this bias in the training data. According to the researchers, most participants in their experiments only started to notice bias when the AI showed biased performance, such as misclassifying emotions for Black individuals but doing a good job of classifying the emotions expressed by white individuals. Black participants were more likely to suspect that there was an issue, especially when the training data over-represented their own group for representing negative emotion (sadness).&lt;/p&gt;
    &lt;p&gt;‚ÄúIn one of the experiment scenarios ‚Äî which featured racially biased AI performance ‚Äî the system failed to accurately classify the facial expression of the images from minority groups,‚Äù said lead author Cheng "Chris" Chen, an assistant professor of emerging media and technology at Oregon State University who earned her doctorate in mass communications from the Donald P. Bellisario College of Communications at Penn State. ‚ÄúThat is what we mean by biased performance in an AI system where the system favors the dominant group in its classification.‚Äù&lt;/p&gt;
    &lt;p&gt;Chen, Sundar and co-author Eunchae Jang, a doctoral student in mass communications at the Bellisario College, created 12 versions of a prototype AI system designed to detect users‚Äô facial expressions. With 769 participants across three experiments, the researchers tested how users might detect bias in different scenarios. The first two experiments included participants from a variety of racial backgrounds with white participants making up most of the sample. In the third experiment, the researchers intentionally recruited an equal number of Black and white participants.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45629299</guid><pubDate>Sat, 18 Oct 2025 18:13:27 +0000</pubDate></item><item><title>How to sequence your DNA for &lt;$2k</title><link>https://maxlangenkamp.substack.com/p/how-to-sequence-your-dna-for-2k</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45629970</guid><pubDate>Sat, 18 Oct 2025 19:58:25 +0000</pubDate></item><item><title>Bevy TLDR ‚Äì Game development with Bevy summarized</title><link>https://taintedcoders.com/bevy/tldr</link><description>&lt;doc fingerprint="a0988f7c21851b98"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Bevy TLDR&lt;/head&gt;
    &lt;p&gt;The goal of this document is to provide the maximum amount of important content in the minimum amount of words. It can be useful to give this to your language model to give it an up to date source of information about Bevy.&lt;/p&gt;
    &lt;p&gt;The full text of this document in markdown can be found on the Bevy starter&lt;/p&gt;
    &lt;p&gt;Bevy is an archetype Entity-Component-System (ECS) game engine built in Rust. It emphasizes modularity, performance, and ease of use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Entity and Components&lt;/head&gt;
    &lt;p&gt;An &lt;code&gt;Entity&lt;/code&gt; on its own holds no data or behavior. The actual &lt;code&gt;Entity&lt;/code&gt; is just an identifier to find associated components where the real data is stored.&lt;/p&gt;
    &lt;p&gt;Each &lt;code&gt;Entity&lt;/code&gt; can only have a single &lt;code&gt;Component&lt;/code&gt; of each type. These components can be added and removed dynamically over the course of the entity's lifetime. Everything is stored inside a &lt;code&gt;World&lt;/code&gt; and everything is managed by the &lt;code&gt;App&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;A good mental model to use is that entities represent a row in an in-memory database, while components are our columns.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Entities An identifier for a row&lt;/item&gt;
      &lt;item&gt;Components A column in a row&lt;/item&gt;
      &lt;item&gt;Systems All the behavior&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We define components by deriving the &lt;code&gt;Component&lt;/code&gt; trait:&lt;/p&gt;
    &lt;code&gt;#[derive(Component)]
struct Player;

#[derive(Component)]
enum Ship {
  Destroyer,
  Cruiser,
  Battleship,
}

#[derive(Component)]
struct Health(f32);

#[derive(Component)]
#[component(on_add = on_position_added)]
struct Position {
  x: i32,
  y: i32,
}
&lt;/code&gt;
    &lt;p&gt;Components have 5 different life-cycle hooks we can use to handle side effects that need to happen:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;code&gt;#[component(on_add = on_add_function)]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;#[component(on_insert = on_insert_function)]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;#[component(on_replace = on_replace_function)]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;#[component(on_remove = on_remove_function)]&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;#[component(on_despawn = on_despawn_function)]&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Components can also be required by other components&lt;/p&gt;
    &lt;code&gt;#[derive(Component)]
#[require(Position, Ship)]
struct Player;

fn spawn_player_with_required_components(
  mut commands: Commands
) {
  commands.spawn(Player);
}
&lt;/code&gt;
    &lt;p&gt;When a component is spawned, if it has any required components, it will automatically add them unless we override them. The only requirement is that each required component implements the &lt;code&gt;Default&lt;/code&gt; trait.&lt;/p&gt;
    &lt;p&gt;All these required calls are recursive. If a component you require has required components, they will also be added.&lt;/p&gt;
    &lt;p&gt;A &lt;code&gt;Resource&lt;/code&gt; is a special kind of component that has no &lt;code&gt;Entity&lt;/code&gt;. They have more convenient accessors for systems since there is only ever one of them.&lt;/p&gt;
    &lt;code&gt;#[derive(Resource)]
struct Score(usize);

fn main() {
  App::new()
    .add_plugins(DefaultPlugins)
    .init_resource::&amp;lt;Score&amp;gt;()
    .run();
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Systems&lt;/head&gt;
    &lt;p&gt;Systems are where we trigger side effects that change our game's state.&lt;/p&gt;
    &lt;p&gt;In Bevy, systems are simple rust functions with one rule: They can only have parameters that implement &lt;code&gt;SystemParam&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;fn spawn_player(mut commands: Commands) {
  // Spawns a single entity with multiple components
  commands.spawn((
      Player,
      Ship::Destroyer,
      Health(100.0),
      Position { x: 1, y: 2 }
  ));
}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;Commands&lt;/code&gt; are what we use to change the state of our &lt;code&gt;World&lt;/code&gt; in a way that is more performant than letting each system mutate the world directly.&lt;/p&gt;
    &lt;p&gt;When you use the system parameter &lt;code&gt;Commands&lt;/code&gt; you are enqueuing your commands to the &lt;code&gt;CommandQueue&lt;/code&gt; which runs when we transition to the next &lt;code&gt;Schedule&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h2"&gt;Apps&lt;/head&gt;
    &lt;p&gt;Everything is coordinated through an &lt;code&gt;App&lt;/code&gt; which schedules our systems to run at certain points in the game's loop:&lt;/p&gt;
    &lt;code&gt;use bevy::prelude::*;

fn main() {
   App::new()
     .add_systems(Startup, setup_everything)
     .add_systems(Update, process_input)
     .add_systems(FixedUpdate, move_player)
     .run();
}
&lt;/code&gt;
    &lt;p&gt;You will mostly be adding your logic to the three main schedule labels:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;Update&lt;/code&gt;runs once every loop&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;FixedUpdate&lt;/code&gt;runs once every fixed amount of time&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Startup&lt;/code&gt;runs once at startup&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Additionally there are other built-in schedule labels for more specific use:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;code&gt;PreStartup&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Startup&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;PostStartup&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;First&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;PreUpdate&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;StateTransition&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RunFixedUpdateLoop&lt;/code&gt;which runs&lt;code&gt;FixedUpdate&lt;/code&gt;conditionally&lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Update&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;PostUpdate&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;Last&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These types are each a &lt;code&gt;ScheduleLabel&lt;/code&gt;. Labels are used to identify a &lt;code&gt;Schedule&lt;/code&gt; which contains the metadata and executor needed to run them under certain conditions.&lt;/p&gt;
    &lt;p&gt;Bevy will try and run all systems in parallel as long as there are no mutable data access conflicts. Archetypes are used as a performance optimization for this process.&lt;/p&gt;
    &lt;p&gt;An &lt;code&gt;App&lt;/code&gt; can be given a state enum to manage different modes of operation:&lt;/p&gt;
    &lt;code&gt;#[derive(Debug, Clone, Eq, PartialEq, Hash, Default, States)]
enum AppState {
  #[default]
  MainMenu,
  InGame,
  Paused,
}

fn main() {
  App::new()
    // Add our state to our app definition
    .init_state::&amp;lt;AppState&amp;gt;()
    // We can add systems to trigger during transitions
    .add_systems(OnEnter(AppState::MainMenu), spawn_menu)
    // Or we can use run conditions
    .add_systems(Update, play_game.run_if(in_state(AppState::InGame)))
    .run();
}
&lt;/code&gt;
    &lt;p&gt;If we wanted to create explicit transitions we could implement the logic on our state:&lt;/p&gt;
    &lt;code&gt;impl AppState {
  fn next(&amp;amp;self) -&amp;gt; Self {
    match *self {
      AppState::MainMenu =&amp;gt; AppState::InGame,
      AppState::InGame =&amp;gt; AppState::Paused,
      AppState::Paused =&amp;gt; AppState::InGame,
    }
  }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Plugins&lt;/head&gt;
    &lt;p&gt;Almost every app will include the &lt;code&gt;DefaultPlugins&lt;/code&gt; plugin which groups together all the default functionality needed for a game.&lt;/p&gt;
    &lt;code&gt;fn main() {
  App::new()
    .add_plugins(DefaultPlugins)
    .run();
}
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;DefaultPlugins&lt;/code&gt; includes the following&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Plugin&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DiagnosticsPlugin&lt;/cell&gt;
        &lt;cell&gt;Adds core diagnostics&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;DlssInitPlugin&lt;/cell&gt;
        &lt;cell&gt;Initializes DLSS support if available&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;FrameCountPlugin&lt;/cell&gt;
        &lt;cell&gt;Adds frame counting functionality&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;HierarchyPlugin&lt;/cell&gt;
        &lt;cell&gt;Handles &lt;code&gt;Parent&lt;/code&gt; and &lt;code&gt;Children&lt;/code&gt; components&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;InputPlugin&lt;/cell&gt;
        &lt;cell&gt;Adds keyboard and mouse input&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;PanicHandlerPlugin&lt;/cell&gt;
        &lt;cell&gt;Adds sensible panic handling&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ScheduleRunnerPlugin&lt;/cell&gt;
        &lt;cell&gt;Configures an &lt;code&gt;App&lt;/code&gt; to run its &lt;code&gt;Schedule&lt;/code&gt; according to a given &lt;code&gt;RunMode&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TaskPoolPlugin&lt;/cell&gt;
        &lt;cell&gt;Setup of default task pools for multithreading&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;TimePlugin&lt;/cell&gt;
        &lt;cell&gt;Adds time functionality&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;TransformPlugin&lt;/cell&gt;
        &lt;cell&gt;Handles &lt;code&gt;Transform&lt;/code&gt; components&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Then additionally, depending on the features you enable, it will include:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Plugin&lt;/cell&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AccessibilityPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_window&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds non-GUI accessibility functionality&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AnimationPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_animation&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds animation support&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AntiAliasPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_anti_alias&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds multi-sample anti-aliasing (MSAA)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AssetPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_asset&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds asset server and resources to load assets&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AudioPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_audio&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds support for using sound assets&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DefaultPickingPlugins&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_picking&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds picking functionality&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;DevToolsPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_dev_tools&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enables developer tools in an &lt;code&gt;App&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CameraPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_camera&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds 2D and 3D camera components and systems&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CiTestingPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_ci_testing&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Helps instrument continuous integration&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;CorePipelinePlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_core_pipeline&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;The core rendering pipeline&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GltfPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_gltf&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds support for loading gltf models&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GilrsPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_gilrs&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds support for gamepad inputs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;GizmoPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_gizmos&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Provides an immediate mode drawing api for visual debugging&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;HotPatchPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;hotpatching&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enables hot-patching of assets&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ImagePlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_render&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds the &lt;code&gt;Image&lt;/code&gt; asset and prepares them to render on your GPU&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LightPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_light&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds light components and systems&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;LogPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_log&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds logging to apps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;MeshPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_mesh&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds the &lt;code&gt;Mesh&lt;/code&gt; asset and prepares them to render on the GPU&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PbrPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_pbr&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds physical based rendering with &lt;code&gt;StandardMaterial&lt;/code&gt; etc&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PostProcessingPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_post_process&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds post processing effects&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;PipelinedRenderingPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_render&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds pipelined rendering&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;RenderPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_render&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Sets up rendering backend powered by &lt;code&gt;wgpu&lt;/code&gt; crate&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ScenePlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_scene&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Loading and saving collections of entities and components to files&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;SpritePlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_sprite&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Handling of sprites (images on our entities)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;StatesPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_state&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds state management for Apps&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TerminalCtrlCHandlerPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;std&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Handles Ctrl-C signals in terminal applications&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;TextPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_text&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Supports loading fonts and rendering text&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;UiPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_ui&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds support for UI layouts (flex, grid, etc)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;UiRenderPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_ui_render&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Adds support for sending UI nodes to renderer&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;WindowPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_window&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Provides an interface to create and manage &lt;code&gt;Window&lt;/code&gt; components&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;WinitPlugin&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;bevy_winit&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Interface to create operating system windows (to actually display our game)&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Plugins are a way to group related functionality together. They receive a mutable reference to the &lt;code&gt;App&lt;/code&gt; and can add systems, resources, and other plugins. Plugins are run in the order they are added to the &lt;code&gt;App&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;fn plugin(app: &amp;amp;mut App) {
  app.add_system(some_plugin_system);
}

fn main() {
  App::new().add_plugins(plugin);
}
&lt;/code&gt;
    &lt;p&gt;If we need to manage the life-cycle of a plugin we can implement the &lt;code&gt;Plugin&lt;/code&gt; trait and hook into it.&lt;/p&gt;
    &lt;code&gt;pub struct CameraPlugin;

impl Plugin for CameraPlugin {
  fn cleanup(&amp;amp;self, _app: &amp;amp;App) -&amp;gt; bool {
    info!("Time to clean up")
    true
  }

  fn build(&amp;amp;self, app: &amp;amp;mut App) {
    app.add_systems(Startup, initialize_camera);
  }
}

fn initialize_camera(mut commands: Commands) {
  commands.spawn(Camera2d);
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Querying&lt;/head&gt;
    &lt;p&gt;To access the components of an entity inside our systems we can use the &lt;code&gt;Query&amp;lt;D, F&amp;gt;&lt;/code&gt; system parameter:&lt;/p&gt;
    &lt;code&gt;fn fetch_players(query: Query&amp;lt;&amp;amp;Player&amp;gt;) {
  for player in &amp;amp;query {
    info!("Player: {:?}", player);
  }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;Query&lt;/code&gt; system parameter lets us specify the data we want from each entity using the two generic parameters:&lt;/p&gt;
    &lt;code&gt;//     ------- the `QueryData`
//    |  ---- the `QueryFilter`
//    v  v
Query&amp;lt;D, F&amp;gt;

//      --------- Give us read-only access to all the `Transform` components
//     |     ---- Which have a `Player` component on the same entity
//     v     v
Query&amp;lt;&amp;amp;Ball, With&amp;lt;Player&amp;gt;&amp;gt;

//                     --- NOTE: Each parameter can be a tuple as well
//                    |
//                    v
Query&amp;lt;&amp;amp;mut Transform, (With&amp;lt;Player&amp;gt;, With&amp;lt;Living&amp;gt;)&amp;gt;
&lt;/code&gt;
    &lt;p&gt;When one of the generic parameters is a tuple then all the types in that tuple must be satisfied by that query.&lt;/p&gt;
    &lt;p&gt;There are convenient types that make expressing more complicated queries easier:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Option&amp;lt;T&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;a component but only if it exists, otherwise &lt;code&gt;None&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;AnyOf&amp;lt;T&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;fetches entities with any of the components in type T&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Ref&amp;lt;T&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;shared borrow of an entity's component &lt;code&gt;T&lt;/code&gt; with access to change detection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;Entity&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;returns the entity&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In addition to the &lt;code&gt;Query&lt;/code&gt; system parameter there are other sibling system parameters that also perform queries:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;System parameter&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Single&amp;lt;D, F&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Matches exactly one query item. Skips the system if more or none.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Option&amp;lt;Single&amp;lt;D, F&amp;gt;&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Matches zero or one query item. Skips the system if more.&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;Populated&amp;lt;D, F&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;matches at least one or more. Skips the system if none.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;&lt;code&gt;Single&lt;/code&gt; can be useful to reduce boilerplate when you know there is only ever a single entity with a particular component:&lt;/p&gt;
    &lt;code&gt;fn move_the_only_player(mut transform: Single&amp;lt;&amp;amp;mut Transform, With&amp;lt;Player&amp;gt;&amp;gt;) {
  transform.translation.x += 1.
}
&lt;/code&gt;
    &lt;p&gt;The second argument in your &lt;code&gt;Query&amp;lt;D, F&amp;gt;&lt;/code&gt; is the &lt;code&gt;QueryFilter&lt;/code&gt;. These filters are wrapped by a condition type:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;method&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;With&amp;lt;T&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;only items with a &lt;code&gt;T&lt;/code&gt; component&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Without&amp;lt;T&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;only items without a &lt;code&gt;T&lt;/code&gt; component&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Or&amp;lt;F&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;checks if all filters in the tuple &lt;code&gt;F&lt;/code&gt; apply&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Changed&amp;lt;T&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;only components of type &lt;code&gt;T&lt;/code&gt; that were changed this tick&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;Added&amp;lt;T&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;only components of type &lt;code&gt;T&lt;/code&gt; that were added this tick&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;To retrieve components from our ECS storage our &lt;code&gt;Query&lt;/code&gt; system parameter provides several methods:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;method&lt;/cell&gt;
        &lt;cell role="head"&gt;description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;iter&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;returns an iterator over all items&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;for_each&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;runs the given function in parallel for each item&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;iter_many&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;runs a given function for each item matching a list of entities&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;iter_combinations&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;returns an iterator over all combinations of a specified number of items&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;par_iter&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;returns a parallel iterator&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;get&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;returns a query item for a given entity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_component&amp;lt;T&amp;gt;&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;returns the component for a given entity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;many&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;returns a query item for a given list of entities&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;get_single&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;the safe version of &lt;code&gt;single&lt;/code&gt; which returns a &lt;code&gt;Result&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;single&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;returns the query item while panicking if there are others&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;is_empty&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;returns true if the query is empty&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;contains&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;returns true if query contains a given entity&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Each method also has a corresponding &lt;code&gt;*_mut&lt;/code&gt; variant which will return the components with mutable ownership. This lets us change their data, instead of just reading it.&lt;/p&gt;
    &lt;p&gt;In situations where we have a particular &lt;code&gt;Entity&lt;/code&gt; (which is basically an ID), we can use &lt;code&gt;get&lt;/code&gt; or &lt;code&gt;get_mut&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;#[derive(Resource)]
struct PlayerRef(Entity);

fn move_player_by_component(
  mut query: Query&amp;lt;&amp;amp;mut Transform&amp;gt;,
  player: Res&amp;lt;PlayerRef&amp;gt;
) {
  if let Ok(mut transform) = query.get_mut(player.0) {
    transform.translation.x += 1.;
  }
}
&lt;/code&gt;
    &lt;p&gt;In cases where we have a list of &lt;code&gt;Entity&lt;/code&gt; and we want to iterate over only those entity components we can use &lt;code&gt;iter_many&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;#[derive(Component)]
struct Health(pub f32);

#[derive(Resource)]
struct Selection {
  enemies: Vec&amp;lt;Entity&amp;gt;
}

const ATTACK_DAMAGE: f32 = 10.;

fn attack_selected_enemies(
  mut query: Query&amp;lt;&amp;amp;mut Health&amp;gt;,
  selected: Res&amp;lt;Selection&amp;gt;
) {
  let mut iter = query.iter_many_mut(&amp;amp;selected.enemies);
  while let Some(mut health) = iter.fetch_next() {
    health.0 -= ATTACK_DAMAGE;
  }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Assets&lt;/head&gt;
    &lt;p&gt;To load assets we use the &lt;code&gt;AssetServer&lt;/code&gt; which manages asynchronous loading assets from a particular &lt;code&gt;AssetSource&lt;/code&gt;, usually the filesystem.&lt;/p&gt;
    &lt;p&gt;All assets follow the same general process:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;We register a new &lt;code&gt;Asset&amp;lt;T&amp;gt;&lt;/code&gt;type if its custom&lt;/item&gt;
      &lt;item&gt;We Register an &lt;code&gt;AssetLoader&lt;/code&gt;for that asset if its custom&lt;/item&gt;
      &lt;item&gt;We add the asset to our &lt;code&gt;assets&lt;/code&gt;folder&lt;/item&gt;
      &lt;item&gt;Then we call &lt;code&gt;AssetServer::load&lt;/code&gt;to get a&lt;code&gt;Handle&amp;lt;T&amp;gt;&lt;/code&gt;to the asset&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;fn load_images(asset_server: Res&amp;lt;AssetServer&amp;gt;, mut commands: Commands) {
  // This will not block, the asset will be loaded in the background
  let image_handle: Handle&amp;lt;Image&amp;gt; = asset_server.load("images/bevy.png");

  commands.spawn(Sprite {
    image: image_handle,
    ..default()
  });
}
&lt;/code&gt;
    &lt;p&gt;By default it will expect our assets to be inside the &lt;code&gt;assets&lt;/code&gt; folder inside the root directory of our application controlled by the &lt;code&gt;BEVY_ASSET_ROOT&lt;/code&gt; environment variable.&lt;/p&gt;
    &lt;p&gt;Assets can be tracked one of two ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Through events like &lt;code&gt;AssetEvent::LoadedWithDependencies&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Or by querying the asset server with &lt;code&gt;AssetServer::get_load_state&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Messages and events&lt;/head&gt;
    &lt;p&gt;There are two kind of events in Bevy:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;Message&lt;/code&gt;for communication between systems&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Event&lt;/code&gt;and&lt;code&gt;EntityEvent&lt;/code&gt;for observers that trigger immediate behavior&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;&lt;code&gt;Messages&amp;lt;T&amp;gt;&lt;/code&gt; is a collection that acts as a double buffered queue. This is done to ensure each system has an opportunity to see each message. It is helping systems not have to care about the exact ordering within a frame.&lt;/p&gt;
    &lt;p&gt;Messages are defined by deriving the &lt;code&gt;Message&lt;/code&gt; trait:&lt;/p&gt;
    &lt;code&gt;// With a marker message
#[derive(Message)]
struct PlayerKilled;

// With a unit type
#[derive(Message)]
struct PlayerDetected(Entity);

// With fields
#[derive(Message)]
struct PlayerDamaged {
  entity: Entity,
  damage: f32,
}

fn main() {
  App::new()
    .add_message::&amp;lt;PlayerKilled&amp;gt;();
    .add_message::&amp;lt;PlayerDetected&amp;gt;();
    .add_message::&amp;lt;PlayerDamaged&amp;gt;();
}
&lt;/code&gt;
    &lt;p&gt;If your messages are not consumed by 2 frames from now then they will be cleaned up and dropped silently.&lt;/p&gt;
    &lt;p&gt;To write messages to a stream we use a &lt;code&gt;MessageWriter&amp;lt;T&amp;gt;&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;fn detect_player(
  mut messages: MessageWriter&amp;lt;PlayerDetected&amp;gt;,
  players: Query&amp;lt;(Entity, &amp;amp;Transform), With&amp;lt;Player&amp;gt;&amp;gt;,
) {
  for (entity, transform) in players {
    messages.write(PlayerDetected(entity));
  }
}
&lt;/code&gt;
    &lt;p&gt;We can read messages from our systems with an &lt;code&gt;MessageReader&amp;lt;T&amp;gt;&lt;/code&gt; that consumes messages from our buffers:&lt;/p&gt;
    &lt;code&gt;fn react_to_detection(mut messages: MessageReader&amp;lt;PlayerDetected&amp;gt;) {
  for message in messages.read() {
    // Do something with each event here
  }
}
&lt;/code&gt;
    &lt;p&gt;Events are the immediate version of messages. They come in two types:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;Event&lt;/code&gt;for global events defined with a&lt;code&gt;GlobalTrigger&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;EntityEvent&lt;/code&gt;for entity specific events defined with an&lt;code&gt;EntityTrigger&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These events are consumed by an &lt;code&gt;Observer&lt;/code&gt; which is a callback system that takes an &lt;code&gt;On&lt;/code&gt; system parameter:&lt;/p&gt;
    &lt;code&gt;fn on_respawn(
  event: On&amp;lt;Add, Enemy&amp;gt;,
  query: Query&amp;lt;(&amp;amp;Enemy, &amp;amp;Position)&amp;gt;,
) {
  let (enemy, position) = query.get(event.entity).unwrap();
  println!("Enemy was respawned at {:?}", position);
}
&lt;/code&gt;
    &lt;p&gt;Observers can be global by adding them to the &lt;code&gt;App&lt;/code&gt; definition:&lt;/p&gt;
    &lt;code&gt;fn main() {
  App::new().add_plugins(DefaultPlugins).add_observer(on_respawn);
}
&lt;/code&gt;
    &lt;p&gt;Or they can be local and only triggered for particular entities:&lt;/p&gt;
    &lt;code&gt;fn spawn_boss(mut commands: Commands) {
  let entity = commands.spawn((Enemy, Boss)).observe(on_boss_spawned).id();

  // Later, or potentially in another system
  commands.trigger(BossSpawned { entity });
}
&lt;/code&gt;
    &lt;p&gt;These entity events will bubble up a hierarchy of &lt;code&gt;ChildOf&lt;/code&gt; attached components.&lt;/p&gt;
    &lt;p&gt;This table summarizes the differences between events and messages:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Events&lt;/cell&gt;
        &lt;cell role="head"&gt;Messages&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Optimal event frequency&lt;/cell&gt;
        &lt;cell&gt;Infrequent&lt;/cell&gt;
        &lt;cell&gt;Frequent&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Handler&lt;/cell&gt;
        &lt;cell&gt;Only handles a single event&lt;/cell&gt;
        &lt;cell&gt;Can handle many messages together&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Latency&lt;/cell&gt;
        &lt;cell&gt;Immediate&lt;/cell&gt;
        &lt;cell&gt;Up to 1 frame&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Event propagation&lt;/cell&gt;
        &lt;cell&gt;Bubbling&lt;/cell&gt;
        &lt;cell&gt;None&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Scope&lt;/cell&gt;
        &lt;cell&gt;World or Entity&lt;/cell&gt;
        &lt;cell&gt;World&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Ordering&lt;/cell&gt;
        &lt;cell&gt;No explicit order&lt;/cell&gt;
        &lt;cell&gt;Ordered&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Coupling&lt;/cell&gt;
        &lt;cell&gt;High&lt;/cell&gt;
        &lt;cell&gt;Low&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h2"&gt;Relationships&lt;/head&gt;
    &lt;p&gt;Bevy has a built-in relationship it provides for parent/child relationships that is made up of two components:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;ChildOf&lt;/code&gt;: The&lt;code&gt;Relationship&lt;/code&gt;we attach to other entities&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;Children&lt;/code&gt;: The&lt;code&gt;RelationshipTarget&lt;/code&gt;that is kept in sync&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;These will propagate &lt;code&gt;Transform&lt;/code&gt; and &lt;code&gt;GlobalTransform&lt;/code&gt; of the parent to its children to keep them in sync.&lt;/p&gt;
    &lt;p&gt;When you despawn the parent (the entity holding the &lt;code&gt;Children&lt;/code&gt;) then all the &lt;code&gt;ChildOf&lt;/code&gt; components are removed automatically.&lt;/p&gt;
    &lt;code&gt;fn spawn_ship(mut commands: Commands) {
  let fleet = commands.spawn(Fleet).id();

  commands.spawn(Ship, ChildOf(fleet));
}
&lt;/code&gt;
    &lt;p&gt;We can spawn children from a parent with the &lt;code&gt;with_children&lt;/code&gt; method:&lt;/p&gt;
    &lt;code&gt;fn spawn_fleet(mut commands: Commands) {
  commands
    .spawn(Fleet)
    .with_children(|parent| {
      parent.spawn((Ship, Name::new("Ship 1")));
      parent.spawn((Ship, Name::new("Ship 2")));
    });
}
&lt;/code&gt;
    &lt;p&gt;Instead of the closure we can pass a bundle of children to the &lt;code&gt;children!&lt;/code&gt; macro.&lt;/p&gt;
    &lt;code&gt;fn spawn_fleet_with_sugar(mut commands: Commands) {
  commands.spawn((
    Fleet,
    children![
      (Ship, Name::new("Ship 3")),
      (Ship, Name::new("Ship 4")),
    ]
  ));
}
&lt;/code&gt;
    &lt;p&gt;The source of truth is the &lt;code&gt;Relationship&lt;/code&gt; component. This is the component we will be adding to other entities to specify the relationship. It must contain a reference to the entity we will be attaching ourselves to.&lt;/p&gt;
    &lt;code&gt;#[derive(Component)]
#[relationship(relationship_target = ShipAttachments)]
struct AttachedToShip(pub Entity);
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;RelationshipTarget&lt;/code&gt; is the component that will automatically be kept in sync with all our &lt;code&gt;AttachedToShip&lt;/code&gt; components. It must contain a list of entities to store them.&lt;/p&gt;
    &lt;code&gt;#[derive(Component)]
#[relationship_target(relationship = AttachedToShip, linked_spawn)]
struct ShipAttachments(Vec&amp;lt;Entity&amp;gt;);
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;linked_spawn&lt;/code&gt; will allow us to remove the &lt;code&gt;ShipAttachments&lt;/code&gt; and Bevy will automatically despawn any &lt;code&gt;AttachedToShip&lt;/code&gt; components on our other entities.&lt;/p&gt;
    &lt;p&gt;To create the relationship we can then spawn this &lt;code&gt;Relationship&lt;/code&gt; on other entities.&lt;/p&gt;
    &lt;code&gt;fn spawn_ship(mut commands: Commands) {
  // Spawn the parent Ship
  let ship = commands.spawn((Ship, Name::new("Ship"))).id();

  // Spawn a GunTurret and attach it to the Ship using the new Relationship
  // component
  commands.spawn((GunTurret, AttachedToShip(ship), Name::new("GunTurret 1")));
  commands.spawn((GunTurret, AttachedToShip(ship), Name::new("GunTurret 2")));
}
&lt;/code&gt;
    &lt;p&gt;This can be shortened by using the &lt;code&gt;related!&lt;/code&gt; macro to specify the relationships from the parent entity instead:&lt;/p&gt;
    &lt;code&gt;fn build_ship(mut commands: Commands) {
  // Spawn a Ship entity
  commands.spawn((
    Ship,
    Name::new("Ship A"),
    related!(ShipAttachments[
      // Attach GunTurrets to the Ship using the relationship
      (GunTurret, Name::new("GunTurret 1")),
      (GunTurret, Name::new("GunTurret 2")),
    ]),
  ));
}
&lt;/code&gt;
    &lt;p&gt;Relationships are stored as components so we can query them:&lt;/p&gt;
    &lt;code&gt;fn log_ship_report(
  ships: Query&amp;lt;(&amp;amp;Name, &amp;amp;ShipAttachments), With&amp;lt;Ship&amp;gt;&amp;gt;,
  turrents: Query&amp;lt;&amp;amp;Name, With&amp;lt;GunTurret&amp;gt;&amp;gt;,
) {
  for (ship_name, attachments) in &amp;amp;ships {
    info!("{} has the following attachments:", ship_name.as_str());

    for &amp;amp;attachment in &amp;amp;attachments.0 {
      if let Ok(child_name) = turrents.get(attachment) {
        info!(" - {}", child_name.as_str());
      }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;You can iterate the association from either side of the relationship:&lt;/p&gt;
    &lt;code&gt;fn iterate_from_turrets_to_ships(
  ships: Query&amp;lt;Entity, With&amp;lt;Ship&amp;gt;&amp;gt;,
  turrets: Query&amp;lt;Entity, With&amp;lt;AttachedToShip&amp;gt;&amp;gt;,
  attachments: Query&amp;lt;&amp;amp;AttachedToShip&amp;gt;,
) {
  for turret in &amp;amp;turrets {
    for attached in attachments.iter_ancestors(turret) {
      let ship = ships.get(attached);

      info!("Turret {:?} is attached to Ship {:?}", turret, ship);
    }
  }
}

fn iterate_from_ships_to_turrets(
  ships: Query&amp;lt;Entity, With&amp;lt;Ship&amp;gt;&amp;gt;,
  turrets: Query&amp;lt;Entity, With&amp;lt;GunTurret&amp;gt;&amp;gt;,
  ship_attachments: Query&amp;lt;&amp;amp;ShipAttachments&amp;gt;,
) {
  for ship in &amp;amp;ships {
    for attachment in ship_attachments.iter_descendants(ship) {
      let turret_entity = turrets.get(attachment);
      info!("Ship {:?} has Turret {:?}", ship, turret_entity);
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;You must be careful not to use this if your relationships contain loops as this will run infinitely.&lt;/p&gt;
    &lt;p&gt;Bevy does not currently have a native way of representing many-to-many relationships. &lt;code&gt;ChildOf&lt;/code&gt; can only point to a single entity.&lt;/p&gt;
    &lt;head rend="h2"&gt;Input&lt;/head&gt;
    &lt;p&gt;There are two ways to handle input in Bevy:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Reacting to the events emitted automatically by Bevy's input systems&lt;/item&gt;
      &lt;item&gt;Querying a resource like &lt;code&gt;ButtonInput&lt;/code&gt;,&lt;code&gt;Axis&lt;/code&gt;,&lt;code&gt;Touches&lt;/code&gt;or&lt;code&gt;Gamepads&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Bevy has a different resource for each type of input:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Resource&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Axis&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;stores the position data from certain input devices&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;ButtonInput&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;a "press-able" input&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GamepadAxis&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;An axis of a gamepad&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;GamepadButton&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;represents a single button of a gamepad just like a keyboard&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;Gamepads&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;represents a collection of connected game controllers&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;TouchInput&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;represents touch based input events&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;Touches&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;a collection of &lt;code&gt;Touch&lt;/code&gt;es that have happened&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;For example, to handle keyboard input we use the &lt;code&gt;ButtonInput&amp;lt;T&amp;gt;&lt;/code&gt; resource which has a set of convenient methods we can use to trigger behavior:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;pressed&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;will return &lt;code&gt;true&lt;/code&gt; between a press and release event&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;just_pressed&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;will return &lt;code&gt;true&lt;/code&gt; for one frame after a press event&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;just_released&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;will return &lt;code&gt;true&lt;/code&gt; for one frame after a release event&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We can read these events in general by listening to &lt;code&gt;KeyboardInput&lt;/code&gt; events:&lt;/p&gt;
    &lt;code&gt;/// Track keyboard inputs ‚Äî useful for debugging or keybinding tools
fn log_keyboard_input(mut keyboard_events: EventReader&amp;lt;KeyboardInput&amp;gt;) {
    for event in keyboard_events.read() {
        println!(
            "Key pressed: {:?}, logical key: {:?}",
            event.key_code, event.logical_key
        );
    }
}
&lt;/code&gt;
    &lt;p&gt;Or we can use the resource to check for a more specific state:&lt;/p&gt;
    &lt;code&gt;/// Handle player jump
fn jump_input_system(input: Res&amp;lt;ButtonInput&amp;lt;KeyCode&amp;gt;&amp;gt;) {
    if input.just_pressed(KeyCode::Space) {
        info!("Jump!");
    }
}

fn combo_key_system(input: Res&amp;lt;ButtonInput&amp;lt;KeyCode&amp;gt;&amp;gt;) {
    let shift = input.any_pressed([KeyCode::ShiftLeft, KeyCode::ShiftRight]);
    let ctrl = input.any_pressed([KeyCode::ControlLeft, KeyCode::ControlRight]);

    if ctrl &amp;amp;&amp;amp; shift &amp;amp;&amp;amp; input.just_pressed(KeyCode::KeyA) {
        info!("Special ability activated! (Ctrl + Shift + A)");
    }
}
&lt;/code&gt;
    &lt;p&gt;When you place your mouse on the screen it would two positions:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;On-screen coordinates (the position of the pixel on a screen)&lt;/item&gt;
      &lt;item&gt;World coordinates (the position of the mouse projected onto our game)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;RelativeCursorPosition&lt;/code&gt; component stores the cursor position relative to our node. If it is within the range of &lt;code&gt;(-0.5, -0.5)&lt;/code&gt; to &lt;code&gt;(0.5, 0.5)&lt;/code&gt; then the cursor is currently over the node, with &lt;code&gt;(0., 0.)&lt;/code&gt; being center. You can use the &lt;code&gt;cursor_over: bool&lt;/code&gt; field to figure this out.&lt;/p&gt;
    &lt;p&gt;If the cursor position is unknown (e.g we are alt+tabbed out of our game) then the position will be &lt;code&gt;None&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;use bevy::ui::RelativeCursorPosition;

fn relative_cursor_position(cursor_query: Query&amp;lt;&amp;amp;RelativeCursorPosition&amp;gt;) {
  if let Ok(cursor) = cursor_query.single() {
    if let Some(cursor) = cursor.normalized {
      info!("({:.1}, {:.1})", cursor.x, cursor.y)
    }
  }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Cameras&lt;/head&gt;
    &lt;p&gt;Each &lt;code&gt;Camera&lt;/code&gt; is responsible for 3 main things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;The render target which is the region of the screen to draw something&lt;/item&gt;
      &lt;item&gt;The projection which determines how to transform 3D into 2D (our screen)&lt;/item&gt;
      &lt;item&gt;The position of the view in our scene to capture and transform&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Each frame, Bevy will start by drawing the &lt;code&gt;ClearColor&lt;/code&gt; over the camera's viewport and then draw things from scratch on the screen.&lt;/p&gt;
    &lt;p&gt;The coordinate system in Bevy is right handed so:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;X increases going to the right&lt;/item&gt;
      &lt;item&gt;Y increases going up&lt;/item&gt;
      &lt;item&gt;Z increases coming towards the screen&lt;/item&gt;
      &lt;item&gt;The default center of the screen is (0, 0)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When we spawn a camera we use &lt;code&gt;Camera2d&lt;/code&gt; or &lt;code&gt;Camera3d&lt;/code&gt; depending on our game.&lt;/p&gt;
    &lt;code&gt;// Useful for marking the "main" camera if we have many
#[derive(Component)]
#[require(Camera2d)]
pub struct MainCamera;

fn initialize_camera(mut commands: Commands) {
  commands.spawn(MainCamera);
}

fn move_camera(
  mut camera: Single&amp;lt;&amp;amp;mut Transform, With&amp;lt;MainCamera&amp;gt;&amp;gt;,
  player: Single&amp;lt;&amp;amp;Transform, With&amp;lt;Player&amp;gt;&amp;gt;,
  time: Res&amp;lt;Time&amp;gt;,
) {
  let direction = Vec3::new(
    player.translation.x,
    player.translation.y,
    camera.translation.z,
  );

  camera.translation =
    camera.translation.lerp(direction, time.delta_secs() * 2.);
}

fn rotate_camera_to_mouse(
  time: Res&amp;lt;Time&amp;gt;,
  mut mouse_motion: MessageReader&amp;lt;MouseMotion&amp;gt;,
  mut transform: Single&amp;lt;&amp;amp;mut Transform, With&amp;lt;Camera&amp;gt;&amp;gt;,
) {
  let dt = time.delta_secs();
  // The factors are just arbitrary mouse sensitivity values.
  // It's often nicer to have a faster horizontal sensitivity than vertical.
  let mouse_sensitivity = Vec2::new(0.12, 0.10);

  for motion in mouse_motion.read() {
    let delta_yaw = -motion.delta.x * dt * mouse_sensitivity.x;
    let delta_pitch = -motion.delta.y * dt * mouse_sensitivity.y;

    // Add yaw which is turning left/right (global)
    transform.rotate_y(delta_yaw);

    // Add pitch which is looking up/down (local)
    const PITCH_LIMIT: f32 = std::f32::consts::FRAC_PI_2 - 0.01;
    let (yaw, pitch, roll) = transform.rotation.to_euler(EulerRot::YXZ);
    let pitch = (pitch + delta_pitch).clamp(-PITCH_LIMIT, PITCH_LIMIT);

    // Apply the rotation
    transform.rotation = Quat::from_euler(EulerRot::YXZ, yaw, pitch, roll);
  }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;UI&lt;/head&gt;
    &lt;p&gt;Bevy's UI system is also done through its ECS.&lt;/p&gt;
    &lt;p&gt;A &lt;code&gt;Node&lt;/code&gt; is a component that holds the layout and style properties. Nodes are laid out with either a flexbox or CSS grid layout.&lt;/p&gt;
    &lt;p&gt;This is what a &lt;code&gt;Node&lt;/code&gt; looks like:&lt;/p&gt;
    &lt;code&gt;impl Node {
  pub const DEFAULT: Self = Self {
    display: Display::DEFAULT,
    box_sizing: BoxSizing::DEFAULT,
    position_type: PositionType::DEFAULT,
    left: Val::Auto,
    right: Val::Auto,
    top: Val::Auto,
    bottom: Val::Auto,
    flex_direction: FlexDirection::DEFAULT,
    flex_wrap: FlexWrap::DEFAULT,
    align_items: AlignItems::DEFAULT,
    justify_items: JustifyItems::DEFAULT,
    align_self: AlignSelf::DEFAULT,
    justify_self: JustifySelf::DEFAULT,
    align_content: AlignContent::DEFAULT,
    justify_content: JustifyContent::DEFAULT,
    margin: UiRect::DEFAULT,
    padding: UiRect::DEFAULT,
    border: UiRect::DEFAULT,
    flex_grow: 0.0,
    flex_shrink: 1.0,
    flex_basis: Val::Auto,
    width: Val::Auto,
    height: Val::Auto,
    min_width: Val::Auto,
    min_height: Val::Auto,
    max_width: Val::Auto,
    max_height: Val::Auto,
    aspect_ratio: None,
    overflow: Overflow::DEFAULT,
    overflow_clip_margin: OverflowClipMargin::DEFAULT,
    row_gap: Val::ZERO,
    column_gap: Val::ZERO,
    grid_auto_flow: GridAutoFlow::DEFAULT,
    grid_template_rows: Vec::new(),
    grid_template_columns: Vec::new(),
    grid_auto_rows: Vec::new(),
    grid_auto_columns: Vec::new(),
    grid_column: GridPlacement::DEFAULT,
    grid_row: GridPlacement::DEFAULT,
  };
}
&lt;/code&gt;
    &lt;p&gt;Which we can use to spawn a simple UI box centered on the screen:&lt;/p&gt;
    &lt;code&gt;fn spawn_box(mut commands: Commands) {
  let container = Node {
    width: percent(100.0),
    height: percent(100.0),
    justify_content: JustifyContent::Center,
    ..default()
  };

  let square = (
    BackgroundColor(Color::srgb(0.65, 0.65, 0.65)),
    Node {
      width: px(200.),
      border: UiRect::all(px(2.)),
      ..default()
    },
  );

  commands.spawn((container, children![(square)]));
}
&lt;/code&gt;
    &lt;p&gt;All &lt;code&gt;Children&lt;/code&gt; of a node will set their position to be relative to their parent, so the &lt;code&gt;Node&lt;/code&gt; we spawned as a child will be placed in the center of its parent.&lt;/p&gt;
    &lt;p&gt;Text can be rendered in two separate ways:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;As part of our game with &lt;code&gt;Text2d&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;As part of our UI with &lt;code&gt;Text&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;fn spawn_text_in_ui(mut commands: Commands, assets: Res&amp;lt;AssetServer&amp;gt;) {
  commands.spawn((
    Node {
      position_type: PositionType::Absolute,
      bottom: px(5.0),
      right: px(5.0),
      ..default()
    },
    Text::new("Here is some text"),
    TextColor(Color::BLACK),
    TextLayout::new_with_justify(Justify::Center),
  ));
}

fn spawn_text_in_scene(
  asset_server: ResMut&amp;lt;AssetServer&amp;gt;,
  mut commands: Commands,
) {
  commands.spawn((
    TextFont {
      font: asset_server.load("fonts/FiraSans-Bold.ttf"),
      font_size: 100.0,
      ..default()
    },
    TextColor(Color::WHITE),
    Text2d::new("Hello, Bevy!"),
    TextLayout::new_with_justify(Justify::Center),
    Transform::from_xyz(0., 0., 0.),
  ));
}
&lt;/code&gt;
    &lt;p&gt;Adding interactivity happens through an &lt;code&gt;Interaction&lt;/code&gt; component.&lt;/p&gt;
    &lt;code&gt;fn button_system(
  mut interactions: Query&amp;lt;
    (
      &amp;amp;Interaction,
      &amp;amp;mut BackgroundColor,
      &amp;amp;mut BorderColor,
      &amp;amp;Children,
    ),
    (Changed&amp;lt;Interaction&amp;gt;, With&amp;lt;Button&amp;gt;),
  &amp;gt;,
  mut texts: Query&amp;lt;&amp;amp;mut Text&amp;gt;,
) {
  for (interaction, mut color, mut border_color, children) in &amp;amp;mut interactions
  {
    if let Ok(mut text) = texts.get_mut(children[0]) {
      match *interaction {
        Interaction::Pressed =&amp;gt; {
          text.0 = "Press".to_string();
          *color = PRESSED_BUTTON.into();
          border_color.set_all(BLUE);
        }
        Interaction::Hovered =&amp;gt; {
          text.0 = "Hover".to_string();
          *color = HOVERED_BUTTON.into();
          border_color.set_all(WHITE);
        }
        Interaction::None =&amp;gt; {
          text.0 = "Button".to_string();
          *color = NORMAL_BUTTON.into();
          border_color.set_all(BLACK);
        }
      }
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;UiStack&lt;/code&gt; orders the UI nodes so that we can have stacking windows. The first entry is the furthest node and the first to get rendered on the screen.&lt;/p&gt;
    &lt;p&gt;However the first node is also the last to receive any interactions so its actually the final node that would be interacted with.&lt;/p&gt;
    &lt;head rend="h2"&gt;Timers&lt;/head&gt;
    &lt;p&gt;Timers come in two modes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;TimerMode::Once&lt;/code&gt;which will tick down to 0 once, and only resets manually&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;TimerMode::Repeat&lt;/code&gt;which will tick down to 0 then reset itself automatically&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In Bevy, timers don't tick down from their initial value. Instead they tick up from zero until they reach their &lt;code&gt;Duration&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;We can then call &lt;code&gt;finished&lt;/code&gt; or &lt;code&gt;just_finished&lt;/code&gt; to switch behavior when they are done. The difference between them is that &lt;code&gt;just_finished&lt;/code&gt; only returns &lt;code&gt;true&lt;/code&gt; if the timer finished in the last tick.&lt;/p&gt;
    &lt;code&gt;#[derive(Resource, Default)]
pub struct MatchTime(Timer);

impl MatchTime {
  pub fn new() -&amp;gt; Self {
    Self(Timer::from_seconds(60.0, TimerMode::Once))
  }
}

fn countdown(time: Res&amp;lt;Time&amp;gt;, mut match_time: ResMut&amp;lt;MatchTime&amp;gt;) {
  match_time.0.tick(time.delta());
}

fn end_match(match_time: Res&amp;lt;MatchTime&amp;gt;) {
  if match_time.0.is_finished() {
    // Here we would rest our game
  }
}
&lt;/code&gt;
    &lt;p&gt;Bevy has a built in &lt;code&gt;Time&lt;/code&gt; resource we can use to get the &lt;code&gt;delta&lt;/code&gt; in seconds of the time between this tick and the last.&lt;/p&gt;
    &lt;code&gt;fn time_passed(time: Res&amp;lt;Time&amp;gt;) {
  info!("Duration passed: {:?}", time.delta());
  info!("Seconds passed: {:?}", time.delta_secs_f64());
  info!("Total time since startup: {:?}", time.elapsed());
}
&lt;/code&gt;
    &lt;p&gt;Now lets say we wanted to have a &lt;code&gt;Cooldown&lt;/code&gt; for one of our abilities. This wouldn't make sense as a &lt;code&gt;Resource&lt;/code&gt; because the cooldown would be specific to one of our players.&lt;/p&gt;
    &lt;code&gt;#[derive(Component)]
struct Cooldown(Timer);

#[derive(Component)]
struct Player;

fn cast_spell(
  mut commands: Commands,
  mut player_query: Query&amp;lt;Entity, With&amp;lt;Player&amp;gt;&amp;gt;,
  cooldowns: Query&amp;lt;&amp;amp;Cooldown, With&amp;lt;Player&amp;gt;&amp;gt;,
) {
  if let Ok(player) = player_query.single() {
    if let Ok(cooldown) = cooldowns.get(player) {
      info!(
        "You cannot cast yet. Your cooldown is {:0.0}% complete!",
        cooldown.0.fraction() * 100.0
      )
    } else {
      // Add an entity to the world with a timer
      commands
        .entity(player)
        .insert(Cooldown(Timer::from_seconds(5.0, TimerMode::Once)));

      // Cast the spell here
    }
  }
}

fn tick_cooldowns(
  mut commands: Commands,
  mut cooldowns: Query&amp;lt;(Entity, &amp;amp;mut Cooldown)&amp;gt;,
  time: Res&amp;lt;Time&amp;gt;,
) {
  for (entity, mut cooldown) in &amp;amp;mut cooldowns {
    cooldown.0.tick(time.delta());

    if cooldown.0.is_finished() {
      commands.entity(entity).remove::&amp;lt;Cooldown&amp;gt;();
    }
  }
}
&lt;/code&gt;
    &lt;p&gt;Sometimes you will want a timer that is only ever used in a single system. For these cases a &lt;code&gt;Resource&lt;/code&gt; would be too public and you might prefer a &lt;code&gt;Local&lt;/code&gt; system parameter instead:&lt;/p&gt;
    &lt;code&gt;fn local_timer(time: Res&amp;lt;Time&amp;gt;, mut timer: Local&amp;lt;Timer&amp;gt;) {
  timer.tick(time.delta());

  if timer.just_finished() {
    info!("The timer is finished");
  }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Audio&lt;/head&gt;
    &lt;p&gt;&lt;code&gt;AudioSource&lt;/code&gt; holds the audio data and is connected to an &lt;code&gt;AudioSink&lt;/code&gt; which is usually done by spawning an &lt;code&gt;AudioPlayer&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;The data must be one of the file formats supported by Bevy:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;code&gt;wav&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;ogg&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;flac&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;mp3&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;A sink is a destination for the sound data. This is the place where sources will send their data and will be emitted to the global listener.&lt;/p&gt;
    &lt;code&gt;fn play_pitch(
    mut pitch_assets: ResMut&amp;lt;Assets&amp;lt;Pitch&amp;gt;&amp;gt;,
    mut commands: Commands,
) {
    info!("playing pitch with frequency: {}", 220.0);
    commands.spawn((
        AudioPlayer(pitch_assets.add(Pitch::new(220.0, Duration::new(1, 0)))),
        PlaybackSettings::DESPAWN,
    ));
}
&lt;/code&gt;
    &lt;p&gt;There are a few different playback settings that are built in:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Setting&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;PlaybackSettings::ONCE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Will play the associated audio only once&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;PlaybackSettings::LOOP&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Will loop the audio&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;PlaybackSettings::DESPAWN&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Will play the audio once then despawn the entity&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;PlaybackSettings::REMOVE&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Will play the audio once then despawn the component&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We can trigger our sounds to play by spawning an &lt;code&gt;AudioPlayer&lt;/code&gt; on any entity.&lt;/p&gt;
    &lt;code&gt;fn play_background_audio(
  asset_server: Res&amp;lt;AssetServer&amp;gt;,
  mut commands: Commands,
) {
  let audio = asset_server.load("background_audio.ogg");

  // Create an entity dedicated to playing our background music
  commands.spawn((
    AudioPlayer::new(audio),
    PlaybackSettings::LOOP,
  ));

  // Spawn our listener
  commands.spawn((
    SpatialListener::new(100.), // Gap between the ears
    Transform::default(),
  ));
}
&lt;/code&gt;
    &lt;p&gt;Once the asset is loaded the music will start playing in a loop until this entity we spawned is despawned or the component is removed.&lt;/p&gt;
    &lt;p&gt;To control the playback of our &lt;code&gt;AudioPlayer&lt;/code&gt; we can use the &lt;code&gt;AudioSink&lt;/code&gt; which was added by the &lt;code&gt;AudioPlugin&lt;/code&gt; automatically when we spawned our entity:&lt;/p&gt;
    &lt;code&gt;fn pause(
  keyboard_input: Res&amp;lt;ButtonInput&amp;lt;KeyCode&amp;gt;&amp;gt;,
  music_controller: Query&amp;lt;&amp;amp;AudioSink, With&amp;lt;MusicBox&amp;gt;&amp;gt;,
) {
  let Ok(sink) = music_controller.single() else {
    return;
  };

  if keyboard_input.just_pressed(KeyCode::Space) {
    sink.toggle_playback();
  }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;AudioSink&lt;/code&gt; is our public API to:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Method&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;play&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Resumes playback&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;pause&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Pause playback&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;stop&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Stop the playback, cannot be restarted after&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;mute&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Mute the playback&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;unmute&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Unmute the playback&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;toggle_playback&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Toggle the playback&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;toggle_mute&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Toggle muting the playback&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;is_paused&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns true if the sink is paused&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;is_muted&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns true if the sink is muted&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;speed&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Get the speed of the sound&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;set_speed&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Control the speed of the playback&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;
          &lt;code&gt;empty&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Returns true if the sink has no more sounds to play&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;try_seek&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Seek to a certain point in the source sound&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;There are two separate sources of volume for our apps:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Global volume&lt;/item&gt;
      &lt;item&gt;Audio sink volume&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To change the global volume we modify the &lt;code&gt;GlobalVolume&lt;/code&gt; resource:&lt;/p&gt;
    &lt;code&gt;use bevy::audio::Volume;

fn change_global_volume(mut volume: ResMut&amp;lt;GlobalVolume&amp;gt;) {
  volume.volume = Volume::Linear(0.5);
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Scenes&lt;/head&gt;
    &lt;p&gt;Scenes can be serialized into file based representations. Everything is serialized into a file and then reinitialized when the scene is loaded.&lt;/p&gt;
    &lt;p&gt;Scenes are saved into a &lt;code&gt;.scn&lt;/code&gt; or &lt;code&gt;.scn.ron&lt;/code&gt;. The format of the file is based on Rusty Object Notation (RON).&lt;/p&gt;
    &lt;p&gt;We save scenes to a file by using the &lt;code&gt;DynamicScene::serialize&lt;/code&gt; method:&lt;/p&gt;
    &lt;code&gt;fn save_scene_system(world: &amp;amp;mut World) {
  let scene = DynamicScene::from_world(world);

  // Scenes can be serialized like this:
  let type_registry = world.resource::&amp;lt;AppTypeRegistry&amp;gt;();
  let type_registry = type_registry.read();
  let serialized_scene = scene.serialize(&amp;amp;type_registry).unwrap();

  // Showing the scene in the console
  info!("{}", serialized_scene);

  // Writing the scene to a new file. Using a task to avoid calling the
  // filesystem APIs in a system as they are blocking This can't work in WASM as
  // there is no filesystem access
  #[cfg(not(target_arch = "wasm32"))]
  IoTaskPool::get()
    .spawn(async move {
      // Write the scene RON data to file
      File::create(format!("assets/{NEW_SCENE_FILE_PATH}"))
        .and_then(|mut file| file.write(serialized_scene.as_bytes()))
        .expect("Error while writing scene to file");
    })
    .detach();
}
&lt;/code&gt;
    &lt;p&gt;When Bevy loads the scene file, it needs to deserialize it into actual components and entities that it loads into your world.&lt;/p&gt;
    &lt;p&gt;There are 3 ways to spawn scenes:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Using &lt;code&gt;SceneSpawner::spawn_dynamic&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Adding the &lt;code&gt;DynamicSceneRoot&lt;/code&gt;component to an entity&lt;/item&gt;
      &lt;item&gt;Using the &lt;code&gt;DynamicSceneBuilder&lt;/code&gt;to construct a&lt;code&gt;DynamicScene&lt;/code&gt;from a&lt;code&gt;World&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The easiest of these is simply spawning a &lt;code&gt;DynamicSceneRoot&lt;/code&gt;. It uses the &lt;code&gt;SceneLoader&lt;/code&gt; to deserialize everything:&lt;/p&gt;
    &lt;code&gt;const SCENE_FILE_PATH: &amp;amp;str = "scene.ron";

fn load_scene_system(mut commands: Commands, asset_server: Res&amp;lt;AssetServer&amp;gt;) {
  // "Spawning" a scene bundle creates a new entity and spawns new instances
  // of the given scene's entities as children of that entity.
  let scene = asset_server.load(SCENE_FILE_PATH);
  commands.spawn(DynamicSceneRoot(scene));
}
&lt;/code&gt;
    &lt;p&gt;Once the scene has been loaded, a &lt;code&gt;SceneInstance&lt;/code&gt; component is added to the component which can be used with the &lt;code&gt;SceneSpawner&lt;/code&gt; to interact with the scene.&lt;/p&gt;
    &lt;p&gt;For example we could despawn all our loaded scenes:&lt;/p&gt;
    &lt;code&gt;use bevy::scene::SceneInstance;

fn despawn_all_scenes(
  query: Query&amp;lt;&amp;amp;SceneInstance&amp;gt;,
  mut spawner: ResMut&amp;lt;SceneSpawner&amp;gt;,
  world: &amp;amp;mut World,
) {
  // Despawning the scene root entity will also despawn all of its children
  for instance in &amp;amp;query {
    spawner.despawn_instance_sync(world, instance);
  }
}
&lt;/code&gt;
    &lt;p&gt;The &lt;code&gt;FromWorld&lt;/code&gt; trait determines how your component is constructed when it loads into the &lt;code&gt;World&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Implementing &lt;code&gt;FromWorld&lt;/code&gt; on a component will let you customize initialization using the current Worlds resources:&lt;/p&gt;
    &lt;code&gt;impl FromWorld for ComponentB {
  fn from_world(world: &amp;amp;mut World) -&amp;gt; Self {
    let time = world.resource::&amp;lt;Time&amp;gt;();
    ComponentB {
      _time_since_startup: time.elapsed(),
      value: "Default Value".to_string(),
    }
  }
}
&lt;/code&gt;
    &lt;head rend="h2"&gt;Physics&lt;/head&gt;
    &lt;p&gt;Bevy does not have a built-in physics engine. The most native to Bevy is &lt;code&gt;avian&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;Your position, in the eyes of Bevy's renderer, is dictated by an entity's &lt;code&gt;Transform&lt;/code&gt; component.&lt;/p&gt;
    &lt;p&gt;In Avian we can use a somewhat more convenient &lt;code&gt;Position&lt;/code&gt; component. This is kept in sync with the &lt;code&gt;Transform&lt;/code&gt; automatically by Avian's &lt;code&gt;SyncPlugin&lt;/code&gt;.&lt;/p&gt;
    &lt;code&gt;fn move_things_with_position(mut query: Query&amp;lt;&amp;amp;mut Position&amp;gt;) {
  for mut position in &amp;amp;mut query {
    position.x += 1.;
  }
}
&lt;/code&gt;
    &lt;p&gt;Just like a &lt;code&gt;Position&lt;/code&gt; Avian provides a &lt;code&gt;Rotation&lt;/code&gt; component.&lt;/p&gt;
    &lt;code&gt;fn rotate_things(mut query: Query&amp;lt;&amp;amp;mut Rotation&amp;gt;) {
  for mut rotation in &amp;amp;mut query {
    *rotation = rotation.add_angle_fast(0.1);
  }
}
&lt;/code&gt;
    &lt;p&gt;Rigid bodies come in 3 different components, each specialized for something:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;code&gt;RigidBody::Dynamic&lt;/code&gt;are similar to real life objects and are affected by forces and contacts.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RigidBody::Kinematic&lt;/code&gt;can only be moved programmatically, which is useful for things like player character controllers and moving platforms.&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;RigidBody::Static&lt;/code&gt;can not move, so they can be good for objects in the environment like the ground and walls.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To move things we can control the &lt;code&gt;Position&lt;/code&gt; directly or use a &lt;code&gt;LinearVelocity&lt;/code&gt; component:&lt;/p&gt;
    &lt;code&gt;fn spawn_ball(mut commands: Commands) {
  commands.spawn((
    RigidBody::Dynamic,
    LinearVelocity(Vec2::new(0.0, 0.0)),
    Collider::circle(0.5),
    Mass(5.0),
    CenterOfMass::new(0.0, -0.5),
  ));
}
&lt;/code&gt;
    &lt;p&gt;Avian is going to use the size of this collider to determine how much mass your body has.&lt;/p&gt;
    &lt;p&gt;Adding a &lt;code&gt;Sensor&lt;/code&gt; component will let you detect collisions without affecting the entity's mass properties or interacting with other physical bodies.&lt;/p&gt;
    &lt;code&gt;fn spawn_sensor(mut commands: Commands) {
  commands.spawn((
    RigidBody::Dynamic,
    Collider::circle(0.5),
    Sensor,
  ));
}
&lt;/code&gt;
    &lt;p&gt;For processing a large number of collisions at once you would use the &lt;code&gt;MessageReader&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;fn react_to_collisions(mut collision_events: MessageReader&amp;lt;CollisionStart&amp;gt;) {
  for event in collision_events.read() {
    info!(
      "Collision started between {:?} and {:?}",
      event.collider1, event.collider2
    );
  }
}
&lt;/code&gt;
    &lt;p&gt;However if we want entity-specific collisions then we can use observers:&lt;/p&gt;
    &lt;code&gt;#[derive(Component)]
struct SecurityCamera;

#[derive(Component)]
struct Enemy;

fn setup_security_cameras(mut commands: Commands) {
  commands
    .spawn((
      SecurityCamera,
      Collider::circle(3.0), // Detection radius
      Sensor,
      CollisionEventsEnabled, // So we receive collision events
    ))
    .observe(|trigger: On&amp;lt;CollisionStart&amp;gt;, enemy_query: Query&amp;lt;&amp;amp;Enemy&amp;gt;| {
      let camera = trigger.collider1;
      let intruder = trigger.collider2;
      if enemy_query.contains(intruder) {
        println!("Security camera {camera} detected enemy {intruder}!");
      }
    });
}
&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45631212</guid><pubDate>Sat, 18 Oct 2025 23:59:40 +0000</pubDate></item><item><title>Using Pegs in Janet</title><link>https://articles.inqk.net/2020/09/19/how-to-use-pegs-in-janet.html</link><description>&lt;doc fingerprint="14c2e1706361f839"&gt;
  &lt;main&gt;
    &lt;p&gt;Instructions on how to use parser expression grammars in Janet.&lt;/p&gt;
    &lt;head rend="h1"&gt;How√¢To: Using PEGs in Janet&lt;/head&gt;
    &lt;p&gt;Janet is a small, Lisp-like language. Unlike most programming languages, it offers no support for regular expressions. Instead, Janet supports parser expression grammars, or PEGs.&lt;/p&gt;
    &lt;p&gt;A PEG in Janet is usually described by an associative data structure that lists a series of rules.1 For each rule, the key is the name of the rule and the value is a description of the string that the rule will match. What makes PEGs especially powerful is the ability for rules to refer to other rules (including recursive references) and for rules to run arbitrary functions.&lt;/p&gt;
    &lt;p&gt;Let√¢s see how we can use a PEG to parse a simplified subset of HTML. We√¢ll use sequences, choices, captures (both compiled and match-time), replacements, drops and back-references. It√¢s going to be fun.2&lt;/p&gt;
    &lt;head rend="h2"&gt;Steps&lt;/head&gt;
    &lt;head rend="h3"&gt;Step 1. Define &lt;code&gt;:main&lt;/code&gt; rule&lt;/head&gt;
    &lt;p&gt;Janet begins parsing using the &lt;code&gt;:main&lt;/code&gt; rule. So let√¢s start with that:&lt;/p&gt;
    &lt;code&gt;'{:main (* :tagged -1)}
&lt;/code&gt;
    &lt;p&gt;This rule defines a pattern consisting of a sequence (represented by &lt;code&gt;*&lt;/code&gt;)3 of
the rule &lt;code&gt;:tagged&lt;/code&gt; and the value &lt;code&gt;-1&lt;/code&gt;. This rule will match if the rule
&lt;code&gt;:tagged&lt;/code&gt; matches and then the string ends (the value &lt;code&gt;-1&lt;/code&gt; matches if we are at
the end of the string).&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 2. Define &lt;code&gt;:tagged&lt;/code&gt; rule&lt;/head&gt;
    &lt;p&gt;Now if we try to use this grammar, Janet will complain that the rule &lt;code&gt;:tagged&lt;/code&gt;
is not defined so let√¢s define that next:&lt;/p&gt;
    &lt;code&gt;'{:main (* :tagged -1)
  :tagged (* :open-tag :value :close-tag)}
&lt;/code&gt;
    &lt;p&gt;This is pretty straightforward. Our &lt;code&gt;:tagged&lt;/code&gt; rule consists of an opening tag,
a value of some kind and a closing tag.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 3. Define &lt;code&gt;:open-tag&lt;/code&gt; rule&lt;/head&gt;
    &lt;code&gt;'{:main (* :tagged -1)
  :tagged (* :open-tag :value :close-tag)
  :open-tag (* "&amp;lt;" (capture :w+ :tag-name) "&amp;gt;")}
&lt;/code&gt;
    &lt;p&gt;We name the capture so that we can use a reference to it in our closing tag rule. I went with &lt;code&gt;:tag-name&lt;/code&gt; but you can choose whatever you like.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 4. Define &lt;code&gt;:close-tag&lt;/code&gt; rule&lt;/head&gt;
    &lt;p&gt;Up to this point, we√¢ve been adding rules in the order that they√¢re processed. Let√¢s deviate from that here and instead define our next rule to match closing tags:&lt;/p&gt;
    &lt;code&gt;~{:main (* :tagged -1)
  :tagged (* :open-tag :value :close-tag)
  :open-tag (* "&amp;lt;" (capture :w+ :tag-name) "&amp;gt;")
  :close-tag (* "&amp;lt;/" (cmt (* (backref :tag-name) (capture :w+)) ,=) "&amp;gt;")}
&lt;/code&gt;
    &lt;p&gt;This rule can be a little difficult to follow. We√¢re doing a match-time capture4 here using the &lt;code&gt;cmt&lt;/code&gt; function.5 This will match if the result of
the provided function (in this case, &lt;code&gt;=&lt;/code&gt;) is truthy. If it is falsey, the match
will fail.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;=&lt;/code&gt; function is passed the values of any captures as separate arguments. We
have two captures in our pattern: the back-reference to the &lt;code&gt;:tag-name&lt;/code&gt; capture
and the value matched by &lt;code&gt;:w+&lt;/code&gt;. If the tag names match,6 the &lt;code&gt;:close-tag&lt;/code&gt;
rule will match.&lt;/p&gt;
    &lt;p&gt;The eagle-eyed amongst you might have noticed that the quoting character at the very beginning has changed from &lt;code&gt;'&lt;/code&gt; to &lt;code&gt;~&lt;/code&gt;. All but the simplest PEGs will
include references to various functions &lt;code&gt;sequence&lt;/code&gt;, &lt;code&gt;capture&lt;/code&gt;, etc that are run
by the PEG engine. To prevent these functions being called in our grammar
definition, we need to quote our data structure. However, when it comes to
passing the functions we want &lt;code&gt;cmt&lt;/code&gt; to call, we need to pass a reference to
these functions. The solution is quasi-quoting. We quasi-quote the data
structure and then unquote the function symbol.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 5. Define &lt;code&gt;:value&lt;/code&gt; rule&lt;/head&gt;
    &lt;p&gt;OK, now we√¢ll define the &lt;code&gt;:value&lt;/code&gt; rule:&lt;/p&gt;
    &lt;code&gt;~{:main (* :tagged -1)
  :tagged (* :open-tag :value :close-tag)
  :open-tag (* "&amp;lt;" (capture :w+ :tag-name) "&amp;gt;")
  :value (any (+ :tagged :untagged))
  :close-tag (* "&amp;lt;/" (cmt (* (backref :tag-name) (capture :w+)) ,=) "&amp;gt;")}
&lt;/code&gt;
    &lt;p&gt;The value in between two tags could be nothing, a tagged value, an untagged value or a combination of tagged and untagged values. We can match zero or more occurrences of the pattern using the &lt;code&gt;any&lt;/code&gt; function. The &lt;code&gt;+&lt;/code&gt; combinator7
tries the first pattern (&lt;code&gt;:tagged&lt;/code&gt;) and if that fails, tries the second pattern
(&lt;code&gt;:untagged&lt;/code&gt;).&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 6. Define &lt;code&gt;:untagged&lt;/code&gt; rule&lt;/head&gt;
    &lt;p&gt;Speaking of &lt;code&gt;:untagged&lt;/code&gt;, this is the last named rule we need:&lt;/p&gt;
    &lt;code&gt;~{:main (* :tagged -1)
  :tagged (* :open-tag :value :close-tag)
  :open-tag (* "&amp;lt;" (capture :w+ :tag-name) "&amp;gt;")
  :value (any (+ :tagged :untagged))
  :close-tag (* "&amp;lt;/" (cmt (* (backref :tag-name) (capture :w+)) ,=) "&amp;gt;")
  :untagged (some (if-not "&amp;lt;" 1))}
&lt;/code&gt;
    &lt;p&gt;This rule matches against one or more characters that are not &lt;code&gt;&amp;lt;&lt;/code&gt;.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 7. Drop back reference after use&lt;/head&gt;
    &lt;p&gt;We have a problem at the moment. Our grammar won√¢t match nested tags because the &lt;code&gt;:tag-name&lt;/code&gt; capture is never being removed from the capture stack. We can solve
this problem using &lt;code&gt;unref&lt;/code&gt;:8&lt;/p&gt;
    &lt;code&gt;~{:main (* :tagged -1)
  :tagged (unref (* :open-tag :value :close-tag))
  :open-tag (* "&amp;lt;" (capture :w+ :tag-name) "&amp;gt;")
  :value (any (+ :tagged :untagged))
  :close-tag (* "&amp;lt;/" (cmt (* (backref :tag-name) (capture :w+)) ,=) "&amp;gt;")
  :untagged (some (if-not "&amp;lt;" 1))}
&lt;/code&gt;
    &lt;p&gt;Now when we get to the end of a nested &lt;code&gt;:tagged&lt;/code&gt; pattern, we√¢ll drop the captures.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 8. Capture tag names and values&lt;/head&gt;
    &lt;p&gt;The above is all well and good for checking if a string matches the grammar but it√¢s usually much more helpful to return some structured data. Let√¢s do that:&lt;/p&gt;
    &lt;code&gt;~{:main (* :tagged -1)
  :tagged (unref (replace (* :open-tag :value :close-tag) ,struct))
  :open-tag (* (constant :tag) "&amp;lt;" (capture :w+ :tag-name) "&amp;gt;")
  :value (* (constant :value) (group (any (+ :tagged :untagged))))
  :close-tag (drop (* "&amp;lt;/" (cmt (* (backref :tag-name) (capture :w+)) ,=) "&amp;gt;"))
  :untagged (capture (some (if-not "&amp;lt;" 1)))}
&lt;/code&gt;
    &lt;p&gt;There√¢s quite a bit going on here so let√¢s look at each rule in turn.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;We√¢ve added a call to&lt;/p&gt;&lt;code&gt;replace&lt;/code&gt;. This will pass the values captured to the function&lt;code&gt;struct&lt;/code&gt;. We√¢ll see why in a second.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Next, in&lt;/p&gt;&lt;code&gt;:open-tag&lt;/code&gt;, we√¢re pushing the value&lt;code&gt;:tag&lt;/code&gt;onto the capture stack using&lt;code&gt;constant&lt;/code&gt;. Because the call to&lt;code&gt;constant&lt;/code&gt;is part of the sequence in this pattern, it only occurs if the entirety of the pattern matches.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;We use a similar trick in&lt;/p&gt;&lt;code&gt;:value&lt;/code&gt;. This time, instead of pushing&lt;code&gt;:tag&lt;/code&gt;, we push&lt;code&gt;:value&lt;/code&gt;.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Still in&lt;/p&gt;&lt;code&gt;:value&lt;/code&gt;, we use&lt;code&gt;group&lt;/code&gt;to collect all of the matches in the&lt;code&gt;(any (+ :tagged :untagged))&lt;/code&gt;pattern and put them inside an array that we push onto the capture stack.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Next, we√¢ve added a call to&lt;/p&gt;&lt;code&gt;drop&lt;/code&gt;in&lt;code&gt;:close-tag&lt;/code&gt;. This ensures that the value generated by the function&lt;code&gt;=&lt;/code&gt;(i.e.&lt;code&gt;true&lt;/code&gt;) is removed from the capture stack.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Finally, we need to capture the result of&lt;/p&gt;&lt;code&gt;:untagged&lt;/code&gt;. We do this with&lt;code&gt;capture&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Now the call to &lt;code&gt;struct&lt;/code&gt; hopefully makes sense. At the time this is called, the
pattern always returns four captures: (1) &lt;code&gt;:tag&lt;/code&gt;, (2) the tag name, (3)
&lt;code&gt;:value&lt;/code&gt; and (4) the tag√¢s value. These get pulled off the capture stack,
turned into a struct and pushed back on.&lt;/p&gt;
    &lt;head rend="h3"&gt;Bonus Step. Compile the grammar&lt;/head&gt;
    &lt;p&gt;If we intend to use the grammar repeatedly, we can compile it for a performance boost:&lt;/p&gt;
    &lt;code&gt;(peg/compile
  ~{:main (* :tagged -1)
    :tagged (unref (replace (* :open-tag :value :close-tag) ,struct))
    :open-tag (* (constant :tag) "&amp;lt;" (capture :w+ :tag-name) "&amp;gt;")
    :value (* (constant :value) (group (any (+ :tagged :untagged))))
    :close-tag (drop (* "&amp;lt;/" (cmt (* (backref :tag-name) (capture :w+)) ,=) "&amp;gt;"))
    :untagged (capture (some (if-not "&amp;lt;" 1)))})
&lt;/code&gt;
    &lt;head rend="h2"&gt;Example&lt;/head&gt;
    &lt;p&gt;Let√¢s see an example. Assuming this is our code:&lt;/p&gt;
    &lt;code&gt;(def grammar
  (peg/compile
    ~{:main (* :tagged -1)
      :tagged (unref (replace (* :open-tag :value :close-tag) ,struct))
      :open-tag (* (constant :tag) "&amp;lt;" (capture :w+ :tag-name) "&amp;gt;")
      :value (* (constant :value) (group (any (+ :tagged :untagged))))
      :close-tag (drop (* "&amp;lt;/" (cmt (* (backref :tag-name) (capture :w+)) ,=) "&amp;gt;"))
      :untagged (capture (some (if-not "&amp;lt;" 1)))}))
&lt;/code&gt;
    &lt;p&gt;Then:&lt;/p&gt;
    &lt;code&gt;(peg/match grammar "&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Hello&amp;lt;/em&amp;gt; &amp;lt;strong&amp;gt;world&amp;lt;/strong&amp;gt;!&amp;lt;/p&amp;gt;")

# =&amp;gt; @[{:tag "p"
#       :value @[{:tag "em" :value @["Hello"]}
#                " "
#                {:tag "strong" :value @["world"]}
#                "!"]}]

(peg/match grammar "&amp;lt;em&amp;gt;Hello&amp;lt;/em&amp;gt; &amp;lt;strong&amp;gt;world&amp;lt;/strong&amp;gt;!&amp;lt;/p&amp;gt;")

# =&amp;gt; nil
&lt;/code&gt;
    &lt;p&gt;The second match returns &lt;code&gt;nil&lt;/code&gt; because of the unmatched &lt;code&gt;&amp;lt;/p&amp;gt;&lt;/code&gt;. Neat!&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrap-Up&lt;/head&gt;
    &lt;p&gt;Building support for PEGs directly into the language is one of Janet√¢s best decisions. They might take a bit of time to get the hang of, but when you do, you√¢ll be able to parse data in a way that√¢s considerably more powerful than with regular expressions. ‚ú∫&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;It is possible to define a PEG using a string that contains a single (unnamed) rule but that√¢s not especially interesting so we√¢ll focus instead on the associative definition. √¢¬©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;I have barely left my house in the past six months. √¢¬©&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;&lt;p&gt;To make the other functions stand out, I√¢m using the alias&lt;/p&gt;&lt;code&gt;*&lt;/code&gt;rather than&lt;code&gt;sequence&lt;/code&gt;in this post. √¢¬©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;I√¢m using a match-time capture so I can explain how those work but a better solution for this specific problem would be to use the&lt;/p&gt;&lt;code&gt;backmatch&lt;/code&gt;function. If we did that, our rule could be&lt;code&gt;(* "&amp;lt;/" (backmatch :tag-name) "&amp;gt;")&lt;/code&gt;. √¢¬©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Match-time captures are created using the&lt;/p&gt;&lt;code&gt;cmt&lt;/code&gt;function. It√¢s unfortunate that this function looks like it√¢s related to commenting. Really, the&lt;code&gt;cmt&lt;/code&gt;function is a way of dynamically adjusting a pattern using captures. √¢¬©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;This comparison is case sensitive. If you want a case-insensitive match, you need a different function to&lt;/p&gt;&lt;code&gt;=&lt;/code&gt;. √¢¬©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;As with&lt;/p&gt;&lt;code&gt;*&lt;/code&gt;, we are using the alias&lt;code&gt;+&lt;/code&gt;rather than&lt;code&gt;choice&lt;/code&gt;. √¢¬©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;The&lt;/p&gt;&lt;code&gt;unref&lt;/code&gt;combinator was added in v1.15.3. √¢¬©&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45631328</guid><pubDate>Sun, 19 Oct 2025 00:33:24 +0000</pubDate></item><item><title>GoGoGrandparent (YC S16) Is Hiring Back End and Full-Stack Engineers</title><link>https://news.ycombinator.com/item?id=45631422</link><description>&lt;doc fingerprint="73bbe85d35405560"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;p&gt;About Us We're a digital caregiving platform dedicated to helping older and disabled adults live independently and thrive in their own homes‚Äîavoiding the need for retirement communities. We adapt on-demand APIs from companies like Uber, Lyft, DoorDash, and Instacart to meet the unique needs of individuals with cognitive, visual, mobility, and dexterity challenges. We're a highly profitable and fast-growing startup. Our team is fully remote, with a total engineering headcount of 12 (including this role).&lt;/p&gt;
      &lt;p&gt;FULLY REMOTE | US, UK, or able to work 4+ hours overlap with mainland US | $100k ‚Äì $160k (partially location-dependent)&lt;/p&gt;
      &lt;p&gt;Tech Stack Back-end heavy: Node.js, TypeScript, MySQL, REST + GraphQL Front-end: Vue.js (nice to have) Deployment: AWS, Docker/Kubernetes (nice to have)&lt;/p&gt;
      &lt;p&gt;Requirements 6+ years of professional experience (primarily in Node.js and Vue.js)&lt;/p&gt;
      &lt;p&gt;Interview Process 2-stage interview process.&lt;/p&gt;
      &lt;p&gt;If you're passionate about improving the lives of older adults and people with disabilities, send your LinkedIn or CV (keep it brief) to william@gogograndparent.com, or apply at https://www.ycombinator.com/companies/gogograndparent/jobs&lt;/p&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45631422</guid><pubDate>Sun, 19 Oct 2025 01:00:42 +0000</pubDate></item></channel></rss>