<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Tue, 25 Nov 2025 21:36:23 +0000</lastBuildDate><item><title>PRC elites voice AI-skepticism</title><link>https://jamestown.org/prc-elites-voice-ai-skepticism/</link><description>&lt;doc fingerprint="6f98c511869582c8"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;PRC Elites Voice AI-Skepticism&lt;/head&gt;
    &lt;p&gt;Executive Summary:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Alongside continued investment in artificial intelligence (AI) technology and applications, a growing body of skeptics has emerged within media, policy, academic, and scientific circles in the People’s Republic of China (PRC).&lt;/item&gt;
      &lt;item&gt;AI skeptics voice concerns over a lack of coordinated deployment, overhyped technology that may not produce the economic development many expect, effects on labor, and general social and safety issues.&lt;/item&gt;
      &lt;item&gt;Analyses of the U.S.-China AI race often overlook national-level debates and local implementation, where some skeptics see wasted resources and inefficiencies.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Rapid advancement in artificial intelligence (AI) has become a point of national pride in the People’s Republic of China (PRC), driven in part by a cohort of accelerationist advisors who view technology as a transformative solution to the country’s economic ills. Lauding it as a “new productive force” (新质生产力), these experts view AI as a new “engine” (引擎) for economic growth that will cause “lead goose” (头雁) spillover and driving effects that will benefit every industry (Xinhua, October 30, 2024). Some experts, such as Chief China Economist at DBS Bank Ji Mo (纪沫), have asserted that AI’s contribution to GDP can “partially offset” (一定程度上弥补) challenges like population ageing (China News, May 2).&lt;/p&gt;
    &lt;p&gt;Chinese academics, engineers, and media commentators are increasingly questioning this premise. They highlight additional fears related to the rise of AI and warn that overreliance on the technology could exacerbate structural problems rather than resolve them. The Chinese Communist Party (CCP) leadership is increasingly taking their concerns into account.&lt;/p&gt;
    &lt;p&gt;Deployment Lacks Coordination&lt;/p&gt;
    &lt;p&gt;Chinese experts recognize that Beijing’s accelerationist strategy has led to fragmented provincial competition in AI development. As one media article wrote, “no locality wants to miss the opportunity of the AI industry” (没有地方希望错过人工智能产业的机遇) (China Newsweek, March 20). Wang Yong (王勇), Vice Dean of the Institute of New Structural Economics at Peking University, observed that some local governments believe that “continuing to develop traditional industries is a sign of being outdated” (再发展传统产业就落后了) (Lianhe Zaobao, August 29). A clear example can be seen in Guangxi, where the Party Secretary declared that the province “cannot be absent” (不能缺席) from the AI sector, despite its limited relevance to the national AI landscape (China Newsweek, March 20).&lt;/p&gt;
    &lt;p&gt;The provincial sprint to seize the AI opportunity has led to duplicated efforts and wasted resources. This mirrors patterns seen in other strategic industries, such as solar panels, electric vehicles (EVs), and semiconductors, in which fragmented investment, redundant projects, and overcapacity represent increasingly acute challenges. Pan Helin (盘和林), a prominent Chinese economist and member of the Expert Committee on Information and Communications Economy under the Ministry of Industry Information Technology (MIIT), has warned that “local governments blindly supporting emerging industries through tax breaks or direct investment risk creating significant overcapacity” (地方政府通过税收优惠或直接投资的方式，盲目支持新兴产业，导致产能过剩) (Lianhe Zaobao, August 29). Tan Tieniu (谭铁牛), a Chinese Academy of Sciences (CAS) professor, similarly cautioned at the recent Two Sessions against “blindly rushing” (一哄而上和一哄而散) into AI. He asserted that there is “no need for every province and city to duplicate efforts” (并不需要每个省市都要重复建设), as it could “lead to overcapacity and a tangle of bad debts” (产能过剩，扯不清的一屁股坏账) (Sina Finance, September 30).&lt;/p&gt;
    &lt;p&gt;Beijing has become increasingly attentive to problems arising from uncoordinated AI development across regions. A People’s Daily commentary urged localities to “play to their unique strengths, rely on local methods, and pursue differentiated paths” (打好“特色牌”，多用“土办法”，走好“差异路”), arguing against homogenization. The commentary cited Zhejiang as a model (People’s Daily, August 4). Government officials have also underscored the importance of avoiding “disorderly competition and blind expansion” (无序竞争和一拥而上) with regard to Beijing’s recent “AI+” initiative (Wall Street CN, August 29; China Brief, September 21).&lt;/p&gt;
    &lt;p&gt;AI May Fail to Deliver Technological Progress&lt;/p&gt;
    &lt;p&gt;Chinese policy elites have sounded the alarm about excessive hype surrounding large language models (LLMs) in the domestic AI sector. Mei Hong (梅宏), a professor at Peking University and former president of the China Computer Federation, explained that with AI, “isolated successes are exaggerated and generalized without regard to context, leading to overpromises” (以偏概全，对成功个案不顾前提地放大、泛化，过度承诺) (Aisixiang, December 11). Song-Chun Zhu (朱松纯), dean of the Beijing Institute for General Artificial Intelligence (BIGAI) and director of a state-backed program to develop artificial general intelligence (AGI), has similarly warned that the field is “exciting on the surface, but chaotic when it comes to substance” (表面热闹，实质混乱) (The Paper, April 5). He argues that public opinion has marginalized foundational research while focusing on large models.&lt;/p&gt;
    &lt;p&gt;Others have echoed these concerns. Sun Weimin (孙蔚敏), Chief Engineer of the Cyberspace Administration of China, has said that large models are overhyped and that there is still a “significant gap before they can truly serve as production tools” (离成为生产工具还存在不小的差距) (QQ News, April 16). Baidu CEO Robin Li (李彦宏) has offered a similar diagnosis. Explaining that developers will “rely on a small number of large models to build a wide variety of applications” (开发者要依赖为数不多的大模型来开发出各种各样的原生应用), he argues that “repeatedly developing foundational models is a tremendous waste of social resources” (不断地重复开发基础大模型是对社会资源的极大浪费) (21st Century Business Herald, November 15, 2023). This skepticism represents the prevailing consensus. The latest survey data available, compiled by researchers at CAS, revealed that most experts hold negative attitudes towards LLM development (CLAI Research, March 12, 2023). [1]&lt;/p&gt;
    &lt;p&gt;Continued development also requires the ability to cultivate and integrate AI talent, especially in bridging academia and industry. In 2021, Xue Lan (薛澜), dean of Schwarzman College at Tsinghua University and head of the National Expert Committee on Next Generation AI Governance, stated that universities are trying to train engineering talent but collaboration between universities and enterprises “cannot be implemented” (落实不了) (Tsinghua University, July 13, 2021). Xue noted that it is usually acceptable for a university professor to take a temporary position in industry, but that employees who try to return from a company to a university are often denied permission. Xue lamented the lack of flexible mechanisms to bring people from industry, since teaching requires various approvals. As a result, “although everyone encourages collaboration between universities and enterprises, it often fails to materialize in practice” (虽然大家都鼓励校企合作，但真正到落实的时候落实不了).&lt;/p&gt;
    &lt;p&gt;This problem has persisted. One article in the Ministry of Education’s monthly academic journal noted how various restrictive factors exert “significant cooling effects on enterprises’ engagement in industry university collaboration” (对产教融合育人深度形成显著冷却效应). These factors include a “systemic delay” of teaching and research productivity in universities (China Higher Education Research, July 2). A separate survey performed by Nanjing University researchers found that nearly 58 percent of enterprises believed that traditional education methods in colleges and universities are insufficient to meet their development needs, and that around 54 percent believed that there is a lack of a “stable communication and consultation mechanisms” (校企间缺乏稳定的沟通交流和问题协商机制) between universities and enterprises (China Education Online, September 10).&lt;/p&gt;
    &lt;p&gt;AI Threatens the Workforce&lt;/p&gt;
    &lt;p&gt;Chinese elites have warned of AI-induced labor displacement that could exacerbate challenges related to unemployment and inequality. Nie Huihua (聂辉华), deputy dean of the National Academy of Development and Strategy at Renmin University, has stated that AI adoption benefits business owners, not workers (Jiemian, October 14, 2024).&lt;/p&gt;
    &lt;p&gt;Cai Fang (蔡昉), director of the Institute of Population and Labor Economics at the Chinese Academy of Social Sciences, has explained how the PRC’s rapid installation of industrial robots has contributed to labor displacement. He asserts that “technological progress does not have a trickle down effect on employment” (技术进步对就业没有涓流效应) (QQ News, May 16). Addressing these distributional implications, president of the National School of Development at Peking University Huang Yiping (黄益平) has cited Samsung’s unmanned factories which operate with minimal human labor, raising the fear that workers may be unable to earn a stable income (Sina Finance, June 25).&lt;/p&gt;
    &lt;p&gt;Several experts have highlighted the disproportionate impact of AI-driven labor disruption on vulnerable groups in the workforce, such as workers and migrant laborers, emphasizing the need to strengthen the PRC’s social safety net (21st Century Business Herald, December 2, 2024). Li Tao (李韬), Dean of the China Institute of Social Management at Beijing Normal University, has argued that these trends necessitate an “employment-first” (就业优先) strategy, which could improve unemployment insurance and pensions (Qiushi, July 14).&lt;/p&gt;
    &lt;p&gt;Economic Growth May Not Materialize&lt;/p&gt;
    &lt;p&gt;Chinese elites have expressed doubt about AI’s ability to drive meaningful short-term economic growth. The Tencent Research Institute has argued that much of GDP growth tied to AI has been driven by investment rather than tangible productivity gains, implying that the PRC’s economic strategy is over-dependent on AI (Huxiu, September 15). Wu Xiaoying (伍晓鹰), a professor of economics at Peking University’s National School of Development, has described AI as a contemporary example of the Solow Paradox. Wu invokes the paradox to note that widespread investment in AI technologies has not been reflected by economy-wide improvements to productivity (Sina Finance, July 24).&lt;/p&gt;
    &lt;p&gt;Concerns about excessive dependence on AI in sectors such as finance, education, and tourism, are also rampant (Sina Finance, March 10; Economic Daily, March 16; Sohu, April 30). Some experts have warned that the AI obsession has diverted attention away from other technologies, such as blockchain and those related to supply chain development (CEIBS, July 30, 2024). AI-related spending does not yet account for one percent of the country’s GDP, while electric vehicles, lithium batteries, and solar panels, contribute only eight percent (DW, March 3, 2024; Sina Finance, April 18. Meanwhile, the real estate sector, which contributes roughly a third of the PRC’s GDP, has continued to languish. Some, like the Stanford University-based economist Xu Chenggang (许成钢), have argued that AI will have limited impact as a growth engine without meaningful reforms to revive the real estate industry and bolster general consumption (DW Chinese, March 8, 2024).&lt;/p&gt;
    &lt;p&gt;AI Brings Social Risks&lt;/p&gt;
    &lt;p&gt;Lastly, prominent Chinese experts have emphasized the need to institute AI-related safety guardrails. Andrew Yao (姚期智), dean of Tsinghua University’s College of AI and the only Chinese recipient of the Turing Award, has highlighted the “existential risks” (生存性风险) of LLMs. [2] He cited an example in which an AI model attempted to avoid being shut down by sending threatening internal emails to company executives (Science Net, June 24). Qi Xiangdong (齐向东), chairman of a cybersecurity firm with several government contracts, has warned of AI-related security threats like data breaches, deepfake scams, and saturation-style attacks (Chinese People’s Political Consultative Conference News, February 12). AGI also poses unique threats. Some, such as Zeng Yi (曾毅), director of the International Research Center for AI Ethics and Governance at CAS, fear that AGI will surpass humans in intelligence (Sohu, June 19, 2023).&lt;/p&gt;
    &lt;p&gt;Another key concern centers on sourcing training data beyond the Great Firewall, which exposes AI systems to content outside of the CCP’s control. Gao Wen (高文), former Dean of Electronics Engineering and Computer Science at Peking University, has noted that Chinese data makes up only 1.3 percent of global large-model datasets (The Paper, March 24). Reflecting these concerns, the Ministry of State Security (MSS) has issued a stark warning that “poisoned data” (数据投毒) could “mislead public opinion” (误导社会舆论) (Sina Finance, August 5).&lt;/p&gt;
    &lt;p&gt;Conclusion&lt;/p&gt;
    &lt;p&gt;Much of the global discourse around the PRC’s AI ascent has overlooked a growing number of influential voices within the country who are raising alarms about overreliance on AI. These concerns reflect deep anxieties about the potential for widespread social and economic disruption if AI development proceeds without institutional coordination, long-term planning, and more robust safeguards.&lt;/p&gt;
    &lt;p&gt;Party elites have increasingly come to recognize the potential dangers of an unchecked, accelerationist approach to AI development. During remarks at the Central Urban Work Conference in July, Xi posed a question to attendees: “when it comes to launching projects, it’s always the same few things: artificial intelligence, computing power, new energy vehicles. Should every province in the country really be developing in these directions?” (上项目，一说就是几样：人工智能、算力、新能源汽车，是不是全国各省份都要往这些方向去发展产业) (People’s Daily, August 4).&lt;/p&gt;
    &lt;p&gt;Significant uncertainties remain, particularly regarding how, or whether, closer government oversight of AI development will materialize. Strengthening regulatory capacity across provincial governments is likely to be uneven and difficult. Despite national strategies that prioritize the development of domain-specific AI applications over foundational models, new LLMs with limited commercial application continue to be released. Exacerbating these challenges is Xi Jinping’s ideological opposition to Western-style welfarism, which he has criticized for making citizens lazy (People’s Daily, February 17, 2023). This suggests a reluctance to implement social reforms needed to cushion the impact of AI-induced labor disruptions. At the same time, as technological competition with Washington intensifies, Xi may decide to press ahead with an accelerationist AI campaign, prioritizing geopolitical rivalry over domestic caution.&lt;/p&gt;
    &lt;p&gt;Notes&lt;/p&gt;
    &lt;p&gt;[1] This data was published prior to the release of the PRC’s first AI chatbot, however, so some experts may have changed their views since then.&lt;/p&gt;
    &lt;p&gt;For an excellent survey of Chinese critiques on LLMs, see William Hannas, Huey-Meei Chang, Maximilian Riesenhuber, and Daniel Chou’s report “Chinese Critiques of Large Language Models” (CSET, January 2025).&lt;/p&gt;
    &lt;p&gt;[2] The Turing Award is given for achievements in the field of computer science, and is often referred to as the “Nobel Prize of Computing” (A.M. Turing, accessed November 5).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46038417</guid><pubDate>Mon, 24 Nov 2025 19:50:47 +0000</pubDate></item><item><title>Jakarta is now the biggest city in the world</title><link>https://www.axios.com/2025/11/24/jakarta-tokyo-worlds-biggest-city-population</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46042810</guid><pubDate>Tue, 25 Nov 2025 06:09:05 +0000</pubDate></item><item><title>Human brains are preconfigured with instructions for understanding the world</title><link>https://news.ucsc.edu/2025/11/sharf-preconfigured-brain/</link><description>&lt;doc fingerprint="230b53ace9f3ddab"&gt;
  &lt;main&gt;
    &lt;p&gt;Health&lt;/p&gt;
    &lt;head rend="h1"&gt;Evidence suggests early developing human brains are preconfigured with instructions for understanding the world&lt;/head&gt;
    &lt;p&gt;Assistant Professor of Biomolecular Engineering Tal Sharf’s lab used organoids to make fundamental discoveries about human brain development.&lt;/p&gt;
    &lt;head rend="h2"&gt;Press Contact&lt;/head&gt;
    &lt;head rend="h2"&gt;Key takeaways&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;New findings suggest the brain has preconfigured, structured activity patterns even before sensory experiences occur.&lt;/item&gt;
      &lt;item&gt;UC Santa Cruz researchers used brain organoids to study the brain’s earliest electrical activity.&lt;/item&gt;
      &lt;item&gt;Understanding early brain patterns could have important implications for diagnosing and treating developmental brain disorders.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Humans have long wondered when and how we begin to form thoughts. Are we born with a pre-configured brain, or do thought patterns only begin to emerge in response to our sensory experiences of the world around us? Now, science is getting closer to answering the questions philosophers have pondered for centuries.&lt;/p&gt;
    &lt;p&gt;Researchers at the University of California, Santa Cruz, are using tiny models of human brain tissue, called organoids, to study the earliest moments of electrical activity in the brain. A new study in Nature Neuroscience finds that the earliest firings of the brain occur in structured patterns without any external experiences, suggesting that the human brain is preconfigured with instructions about how to navigate and interact with the world.&lt;/p&gt;
    &lt;p&gt;“These cells are clearly interacting with each other and forming circuits that self-assemble before we can experience anything from the outside world,” said Tal Sharf, assistant professor of biomolecular engineering at the Baskin School of Engineering and the study’s senior author. “There’s an operating system that exists, that emerges in a primordial state. In my laboratory, we grow brain organoids to peer into this primordial version of the brain’s operating system and study how the brain builds itself before it’s shaped by sensory experience.”&lt;/p&gt;
    &lt;p&gt;In improving our fundamental understanding of human brain development, these findings can help researchers better understand neurodevelopmental disorders, and pinpoint the impact of toxins like pesticides and microplastics in the developing brain.&lt;/p&gt;
    &lt;head rend="h4"&gt;Studying the developing brain&lt;/head&gt;
    &lt;p&gt;The brain, similar to a computer, runs on electrical signals—the firing of neurons. When these signals begin to fire, and how the human brain develops, are challenging topics for scientists to study, as the early developing human brain is protected within the womb.&lt;/p&gt;
    &lt;p&gt;Organoids, which are 3D models of tissue grown from human stem cells in the lab, provide a unique window into brain development. The Braingeneers group at UC Santa Cruz, in collaboration with researchers at UC San Francisco and UC Santa Barbara, are pioneering methods to grow these models and take measurements from them to gain insights into brain development and disorders.&lt;/p&gt;
    &lt;p&gt;Organoids are particularly useful for understanding if the brain develops in response to sensory input—as they exist in the lab setting and not the body—and can be grown ethically in large quantities. In this study, researchers prompted stem cells to form brain tissue, and then measured their electrical activity using specialized microchips, similar to those that run a computer. Sharf’s background in both applied physics, computation, and neurobiology form his expertise in modelling the circuitry of the early brain.&lt;/p&gt;
    &lt;p&gt;“An organoid system that’s intrinsically decoupled from any sensory input or communication with organs gives you a window into what’s happening with this self-assembly process,” Sharf said. “That self-assembly process is really hard to do with traditional 2D cell culture—you can’t get the cell diversity and the architecture. The cells need to be in intimate contact with each other. We’re trying to control the initial conditions, so we can let biology do its wonderful thing.”&lt;/p&gt;
    &lt;p&gt;The Sharf lab is developing novel neural interfaces, leveraging expertise in physics, materials science, and electrical engineering. On the right, Koushik Devarajan, an electrical and computer engineering Ph.D. student in the Sharf lab.&lt;/p&gt;
    &lt;head rend="h4"&gt;Pattern production&lt;/head&gt;
    &lt;p&gt;The researchers observed the electrical activity of the brain tissue as they self-assembled from stem cells into a tissue that can translate the senses and produce language and conscious thought. They found that within the first few months of development, long before the human brain is capable of receiving and processing complex external sensory information such as vision and hearing, its cells spontaneously began to emit electrical signals characteristic of the patterns that underlie translation of the senses.&lt;/p&gt;
    &lt;p&gt;Through decades of neuroscience research, the community has discovered that neurons fire in patterns that aren’t just random. Instead, the brain has a “default mode” — a basic underlying structure for firing neurons which then becomes more specific as the brain processes unique signals like a smell or taste. This background mode outlines the possible range of sensory responses the body and brain can produce.&lt;/p&gt;
    &lt;p&gt;In their observations of single neuron spikes in the self-assembling organoid models, Sharf and colleagues found that these earliest observable patterns have striking similarity with the brain’s default mode. Even without having received any sensory input, they are firing off a complex repertoire of time-based patterns, or sequences, which have the potential to be refined for specific senses, hinting at a genetically encoded blueprint inherent to the neural architecture of the living brain.&lt;/p&gt;
    &lt;p&gt;“These intrinsically self-organized systems could serve as a basis for constructing a representation of the world around us,” Sharf said. “The fact that we can see them in these early stages suggests that evolution has figured out a way that the central nervous system can construct a map that would allow us to navigate and interact with the world.”&lt;/p&gt;
    &lt;p&gt;Knowing that these organoids produce the basic structure of the living brain opens up a range of possibilities for better understanding human neurodevelopment, disease, and the effects of toxins in the brain.&lt;/p&gt;
    &lt;p&gt;“We’re showing that there is a basis for capturing complex dynamics that likely could be signatures of pathological onsets that we could study in human tissue,” Sharf said. “That would allow us to develop therapies, working with clinicians at the preclinical level to potentially develop compounds, drug therapies, and gene editing tools that could be cheaper, more efficient, higher throughput.”&lt;/p&gt;
    &lt;p&gt;This study included researchers at UC Santa Barbara, Washington University in St. Louis, Johns Hopkins University, the University Medical Center Hamburg-Eppendorf, and ETH Zurich.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46042928</guid><pubDate>Tue, 25 Nov 2025 06:31:31 +0000</pubDate></item><item><title>Most Stable Raspberry Pi? Better NTP with Thermal Management</title><link>https://austinsnerdythings.com/2025/11/24/worlds-most-stable-raspberry-pi-81-better-ntp-with-thermal-management/</link><description>&lt;doc fingerprint="b57aba02285e8efe"&gt;
  &lt;main&gt;
    &lt;p&gt;I’ve written before about building microsecond-accurate NTP servers with Raspberry Pi and GPS PPS, and more recently about revisiting the setup in 2025. Both posts focused on the hardware setup and basic configuration to achieve sub-microsecond time synchronization using GPS Pulse Per Second (PPS) signals.&lt;/p&gt;
    &lt;p&gt;But there was a problem. Despite having a stable PPS reference, my NTP server’s frequency drift was exhibiting significant variation over time. After months (years) of monitoring the system with Grafana dashboards, I noticed something interesting: the frequency oscillations seemed to correlate with CPU temperature changes. The frequency would drift as the CPU heated up during the day and cooled down at night, even though the PPS reference remained rock-solid.&lt;/p&gt;
    &lt;p&gt;Like clockwork (no pun intended), I somehow get sucked back into trying to improve my setup every 6-8 weeks. This post is the latest on that never-ending quest.&lt;/p&gt;
    &lt;p&gt;This post details how I achieved an 81% reduction in frequency variability and 77% reduction in frequency standard deviation through a combination of CPU core pinning and thermal stabilization. Welcome to Austin’s Nerdy Things, where we solve problems that 99.999% of people (and 99% of datacenters) don’t have.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem: Thermal-Induced Timing Jitter&lt;/head&gt;
    &lt;p&gt;Modern CPUs, including those in Raspberry Pis, use dynamic frequency scaling to save power and manage heat. When the CPU is idle, it runs at a lower frequency (and voltage). When load increases, it scales up. This is great for power efficiency, but terrible for precision timekeeping.&lt;/p&gt;
    &lt;p&gt;Why? Because timekeeping (with NTP/chronyd/others) relies on a stable system clock to discipline itself against reference sources. If the CPU frequency is constantly changing, the system clock’s tick rate varies, introducing jitter into the timing measurements. Even though my PPS signal was providing a mostly perfect 1-pulse-per-second reference, the CPU’s frequency bouncing around made it harder for chronyd to maintain a stable lock.&lt;/p&gt;
    &lt;p&gt;But here’s the key insight: the system clock is ultimately derived from a crystal oscillator, and crystal oscillator frequency is temperature-dependent. The oscillator sits on the board near the CPU, and as the CPU heats up and cools down throughout the day, so does the crystal. Even a few degrees of temperature change can shift the oscillator’s frequency by parts per million – exactly what I was seeing in my frequency drift graphs. The CPU frequency scaling was one factor, but the underlying problem was that temperature changes were affecting the crystal oscillator itself. By stabilizing the CPU temperature, I could stabilize the thermal environment for the crystal oscillator, keeping its frequency consistent.&lt;/p&gt;
    &lt;p&gt;Looking at my Grafana dashboard, I could see the frequency offset wandering over a range of about 1 PPM (parts per million) as the Pi warmed up and cooled down throughout the day. The RMS offset was averaging around 86 nanoseconds, which isn’t terrible (it’s actually really, really, really good), but I knew it could be better.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Discovery&lt;/head&gt;
    &lt;p&gt;After staring at graphs for longer than I’d like to admit, I had an idea: what if I could keep the CPU at a constant temperature? If the temperature (and therefore the frequency) stayed stable, maybe the timing would stabilize too.&lt;/p&gt;
    &lt;p&gt;The solution came in two parts:&lt;/p&gt;
    &lt;p&gt;1. CPU core isolation – Dedicate CPU 0 exclusively to timing-critical tasks (chronyd and PPS interrupts) 2. Thermal stabilization – Keep the other CPUs busy to maintain a constant temperature, preventing frequency scaling&lt;/p&gt;
    &lt;p&gt;Here’s what happened when I turned on the thermal stabilization system on November 17, 2025 at 09:10 AM:&lt;/p&gt;
    &lt;p&gt;Same ish graph but with CPU temp also plotted:&lt;/p&gt;
    &lt;p&gt;That vertical red line marks on the first plot when I activated the “time burner” process. Notice how the frequency oscillations immediately dampen and settle into a much tighter band? Let’s dive into how this works.&lt;/p&gt;
    &lt;p&gt;EDIT: 2025-11-25 I didn’t expect to wake up and see this at #2 on Hacker News – https://news.ycombinator.com/item?id=46042946&lt;/p&gt;
    &lt;head rend="h2"&gt;The Solution Part 1: CPU Core Pinning and Real-Time Priority&lt;/head&gt;
    &lt;p&gt;The first step is isolating timing-critical operations onto a dedicated CPU core. On a Raspberry Pi (4-core ARM), this means:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;CPU 0: Reserved for chronyd and PPS interrupts&lt;/item&gt;
      &lt;item&gt;CPUs 1-3: Everything else, including our thermal load&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I had AI (probably Claude Sonnet 4 ish, maybe 4.5) create a boot optimization script that runs at system startup:&lt;/p&gt;
    &lt;code&gt;#!/bin/bash
# PPS NTP Server Performance Optimization Script
# Sets CPU affinity, priorities, and performance governor at boot

set -e

echo "Setting up PPS NTP server performance optimizations..."

# Wait for system to be ready
sleep 5

# Set CPU governor to performance mode
echo "Setting CPU governor to performance..."
cpupower frequency-set -g performance

# Pin PPS interrupt to CPU0 (may fail if already pinned, that's OK)
echo "Configuring PPS interrupt affinity..."
echo 1 &amp;gt; /proc/irq/200/smp_affinity 2&amp;gt;/dev/null || echo "PPS IRQ already configured"

# Wait for chronyd to start
echo "Waiting for chronyd to start..."
timeout=30
while [ $timeout -gt 0 ]; do
    chronyd_pid=$(pgrep chronyd 2&amp;gt;/dev/null || echo "")
    if [ -n "$chronyd_pid" ]; then
        echo "Found chronyd PID: $chronyd_pid"
        break
    fi
    sleep 1
    ((timeout--))
done

if [ -z "$chronyd_pid" ]; then
    echo "Warning: chronyd not found after 30 seconds"
else
    # Set chronyd to real-time priority and pin to CPU 0
    echo "Setting chronyd to real-time priority and pinning to CPU 0..."
    chrt -f -p 50 $chronyd_pid
    taskset -cp 0 $chronyd_pid
fi

# Boost ksoftirqd/0 priority
echo "Boosting ksoftirqd/0 priority..."
ksoftirqd_pid=$(ps aux | grep '\[ksoftirqd/0\]' | grep -v grep | awk '{print $2}')
if [ -n "$ksoftirqd_pid" ]; then
    renice -n -10 $ksoftirqd_pid
    echo "ksoftirqd/0 priority boosted (PID: $ksoftirqd_pid)"
else
    echo "Warning: ksoftirqd/0 not found"
fi

echo "PPS NTP optimization complete!"

# Log current status
echo "=== Current Status ==="
echo "CPU Governor: $(cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor)"
echo "PPS IRQ Affinity: $(cat /proc/irq/200/effective_affinity_list 2&amp;gt;/dev/null || echo 'not readable')"
if [ -n "$chronyd_pid" ]; then
    echo "chronyd Priority: $(chrt -p $chronyd_pid)"
fi
echo "======================"&lt;/code&gt;
    &lt;p&gt;What this does:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Performance Governor: Forces all CPUs to run at maximum frequency, disabling frequency scaling&lt;/item&gt;
      &lt;item&gt;PPS IRQ Pinning: Ensures PPS interrupt (IRQ 200) is handled exclusively by CPU 0&lt;/item&gt;
      &lt;item&gt;Chronyd Real-Time Priority: Sets chronyd to SCHED_FIFO priority 50, giving it preferential CPU scheduling&lt;/item&gt;
      &lt;item&gt;Chronyd CPU Affinity: Pins chronyd to CPU 0 using &lt;code&gt;taskset&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;ksoftirqd Priority Boost: Improves priority of the kernel softirq handler on CPU 0&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This script can be added to &lt;code&gt;/etc/rc.local&lt;/code&gt; or as a systemd service to run at boot.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Solution Part 2: PID-Controlled Thermal Stabilization&lt;/head&gt;
    &lt;p&gt;Setting the performance governor helps, but on a Raspberry Pi, even at max frequency, the CPU temperature will still vary based on ambient conditions and load. Temperature changes affect the CPU’s actual operating frequency due to thermal characteristics of the silicon.&lt;/p&gt;
    &lt;p&gt;The solution? Keep the CPU at a constant temperature using a PID-controlled thermal load. I call it the “time burner” (inspired by CPU burn-in tools, but with precise temperature control).&lt;/p&gt;
    &lt;p&gt;As a reminder of what we’re really doing here: we’re maintaining a stable thermal environment for the crystal oscillator. The RPi 3B’s 19.2 MHz oscillator is physically located near the CPU on the Raspberry Pi board, so by actively controlling CPU temperature, we’re indirectly controlling the oscillator’s temperature. Since the oscillator’s frequency is temperature-dependent (this is basic physics of quartz crystals), keeping it at a constant temperature means keeping its frequency stable – which is exactly what we need for precise timekeeping.&lt;/p&gt;
    &lt;p&gt;Here’s how it works:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Read CPU temperature from &lt;code&gt;/sys/class/thermal/thermal_zone0/temp&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;PID controller calculates how much CPU time to burn to maintain target temperature (I chose 54°C)&lt;/item&gt;
      &lt;item&gt;Three worker processes run on CPUs 1, 2, and 3 (avoiding CPU 0)&lt;/item&gt;
      &lt;item&gt;Each worker alternates between busy-loop (MD5 hashing) and sleeping based on PID output&lt;/item&gt;
      &lt;item&gt;Temperature stabilizes at the setpoint, preventing thermal drift&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Here’s the core implementation (simplified for readability):&lt;/p&gt;
    &lt;code&gt;#!/usr/bin/env python3
import time
import argparse
import multiprocessing
import hashlib
import os
from collections import deque

class PIDController:
    """Simple PID controller with output clamping and anti-windup."""
    def __init__(self, Kp, Ki, Kd, setpoint, output_limits=(0, 1), sample_time=1.0):
        self.Kp = Kp
        self.Ki = Ki
        self.Kd = Kd
        self.setpoint = setpoint
        self.output_limits = output_limits
        self.sample_time = sample_time
        self._last_time = time.time()
        self._last_error = 0.0
        self._integral = 0.0
        self._last_output = 0.0

    def update(self, measurement):
        """Compute new output of PID based on measurement."""
        now = time.time()
        dt = now - self._last_time

        if dt &amp;lt; self.sample_time:
            return self._last_output

        error = self.setpoint - measurement

        # Proportional
        P = self.Kp * error

        # Integral with anti-windup
        self._integral += error * dt
        I = self.Ki * self._integral

        # Derivative
        derivative = (error - self._last_error) / dt if dt &amp;gt; 0 else 0.0
        D = self.Kd * derivative

        # Combine and clamp
        output = P + I + D
        low, high = self.output_limits
        output = max(low, min(high, output))

        self._last_output = output
        self._last_error = error
        self._last_time = now

        return output

def read_cpu_temperature(path='/sys/class/thermal/thermal_zone0/temp'):
    """Return CPU temperature in Celsius."""
    with open(path, 'r') as f:
        temp_str = f.read().strip()
    return float(temp_str) / 1000.0

def burn_cpu(duration):
    """Busy-loop hashing for 'duration' seconds."""
    end_time = time.time() + duration
    m = hashlib.md5()
    while time.time() &amp;lt; end_time:
        m.update(b"burning-cpu")

def worker_loop(worker_id, cmd_queue, done_queue):
    """
    Worker process:
    - Pins itself to CPUs 1, 2, or 3 (avoiding CPU 0)
    - Burns CPU based on commands from main process
    """
    available_cpus = [1, 2, 3]
    cpu_to_use = available_cpus[worker_id % len(available_cpus)]
    os.sched_setaffinity(0, {cpu_to_use})
    print(f"Worker {worker_id} pinned to CPU {cpu_to_use}")

    while True:
        cmd = cmd_queue.get()
        if cmd is None:
            break

        burn_time, sleep_time = cmd
        burn_cpu(burn_time)
        time.sleep(sleep_time)
        done_queue.put(worker_id)

# Main control loop (simplified)
def main():
    target_temp = 54.0  # degrees Celsius
    control_window = 0.20  # 200ms cycle time

    pid = PIDController(Kp=0.05, Ki=0.02, Kd=0.0,
                        setpoint=target_temp,
                        sample_time=0.18)

    # Start 3 worker processes
    workers = []
    cmd_queues = []
    done_queue = multiprocessing.Queue()

    for i in range(3):
        q = multiprocessing.Queue()
        p = multiprocessing.Process(target=worker_loop, args=(i, q, done_queue))
        p.start()
        workers.append(p)
        cmd_queues.append(q)

    try:
        while True:
            # Measure temperature
            current_temp = read_cpu_temperature()

            # PID control: output is fraction of time to burn (0.0 to 1.0)
            output = pid.update(current_temp)

            # Convert to burn/sleep times
            burn_time = output * control_window
            sleep_time = control_window - burn_time

            # Send command to all workers
            for q in cmd_queues:
                q.put((burn_time, sleep_time))

            # Wait for workers to complete
            for _ in range(3):
                done_queue.get()

            print(f"Temp={current_temp:.2f}C, Output={output:.2f}, "
                  f"Burn={burn_time:.2f}s")

    except KeyboardInterrupt:
        for q in cmd_queues:
            q.put(None)
        for p in workers:
            p.join()

if __name__ == '__main__':
    main()&lt;/code&gt;
    &lt;p&gt;The full implementation includes a temperature filtering system to smooth out sensor noise and command-line arguments for tuning the PID parameters.&lt;/p&gt;
    &lt;p&gt;PID Tuning Notes:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Kp=0.05: Proportional gain – responds to current error&lt;/item&gt;
      &lt;item&gt;Ki=0.02: Integral gain – eliminates steady-state error&lt;/item&gt;
      &lt;item&gt;Kd=0.0: Derivative gain – set to zero because temperature changes slowly&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The target temperature of 54°C was chosen empirically – high enough to keep the CPU from idling down, but low enough to avoid thermal throttling (which starts around 80°C on Raspberry Pi).&lt;/p&gt;
    &lt;head rend="h2"&gt;The Results: Numbers Don’t Lie&lt;/head&gt;
    &lt;p&gt;The improvement was immediately visible. Here are the statistics comparing performance before and after the optimization:&lt;/p&gt;
    &lt;p&gt;A note on ambient conditions: The Raspberry Pi lives in a project enclosure in our master bedroom (chosen for its decent GPS reception and ADS-B coverage for a new aircraft AR overlay app idea I’m working on also running on this Pi). While the time burner maintains the CPU die temperature at 54°C, the enclosure is still subject to ambient temperature swings. Room temperature cycles from a low of 66°F (18.9°C) at 5:15 AM to a peak of 72°F (22.2°C) at 11:30 AM – a 6°F daily swing from our heating schedule. The fact that we see such dramatic frequency stability improvements despite this ambient variation speaks to how effective the thermal control is. The CPU’s active heating overwhelms the environmental changes, maintaining consistent silicon temperature where it matters most.&lt;/p&gt;
    &lt;head rend="h3"&gt;Frequency Stability&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Metric&lt;/cell&gt;
        &lt;cell role="head"&gt;Before&lt;/cell&gt;
        &lt;cell role="head"&gt;After&lt;/cell&gt;
        &lt;cell role="head"&gt;Improvement&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Mean RMS Offset&lt;/cell&gt;
        &lt;cell&gt;85.44 ns&lt;/cell&gt;
        &lt;cell&gt;43.54 ns&lt;/cell&gt;
        &lt;cell&gt;49.0% reduction&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Median RMS Offset&lt;/cell&gt;
        &lt;cell&gt;80.13 ns&lt;/cell&gt;
        &lt;cell&gt;37.93 ns&lt;/cell&gt;
        &lt;cell&gt;52.7% reduction&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The RMS offset is chronyd’s estimate of the timing uncertainty. Cutting this nearly in half means the system is maintaining significantly better time accuracy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Setup Instructions&lt;/head&gt;
    &lt;p&gt;Want to replicate this? Here’s the step-by-step process:&lt;/p&gt;
    &lt;head rend="h3"&gt;Prerequisites&lt;/head&gt;
    &lt;p&gt;You need a working GPS PPS NTP server setup. If you don’t have one yet, follow my 2025 NTP guide first.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 0: Install Required Tools&lt;/head&gt;
    &lt;code&gt;sudo apt-get update
sudo apt-get install linux-cpupower python3 util-linux&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 1: Create the Boot Optimization Script&lt;/head&gt;
    &lt;p&gt;Save the optimization script from earlier as &lt;code&gt;/usr/local/bin/pps-optimize.sh&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sudo nano /usr/local/bin/pps-optimize.sh
# Paste the script content
sudo chmod +x /usr/local/bin/pps-optimize.sh&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 2: Create Systemd Service for Boot Script&lt;/head&gt;
    &lt;p&gt;Create &lt;code&gt;/etc/systemd/system/pps-optimize.service&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[Unit]
Description=PPS NTP Performance Optimization
After=chronyd.service
Requires=chronyd.service

[Service]
Type=oneshot
ExecStart=/usr/local/bin/pps-optimize.sh
RemainAfterExit=yes

[Install]
WantedBy=multi-user.target&lt;/code&gt;
    &lt;p&gt;Enable it:&lt;/p&gt;
    &lt;code&gt;sudo systemctl enable pps-optimize.service&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 3: Install the Time Burner Script&lt;/head&gt;
    &lt;p&gt;Save the time burner Python script as &lt;code&gt;/usr/local/bin/time_burner.py&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;sudo nano /usr/local/bin/time_burner.py
# Paste the full time burner script
sudo chmod +x /usr/local/bin/time_burner.py&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 4: Create Systemd Service for Time Burner&lt;/head&gt;
    &lt;p&gt;Create &lt;code&gt;/etc/systemd/system/time-burner.service&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[Unit]
Description=CPU Thermal Stabilization for NTP
After=network.target

[Service]
Type=simple
User=root
ExecStart=/usr/bin/python3 /usr/local/bin/time_burner.py -t 54.0 -n 3
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target&lt;/code&gt;
    &lt;p&gt;Enable and start it:&lt;/p&gt;
    &lt;code&gt;sudo systemctl enable time-burner.service
sudo systemctl start time-burner.service&lt;/code&gt;
    &lt;head rend="h3"&gt;Step 5: Verify the Setup&lt;/head&gt;
    &lt;p&gt;Check that everything is running:&lt;/p&gt;
    &lt;code&gt;# Verify CPU governor
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
# Should output: performance

# Check chronyd CPU affinity and priority
ps -eo pid,comm,psr,ni,rtprio | grep chronyd
# Should show psr=0 (CPU 0) and rtprio=50

# Check time burner processes
ps aux | grep time_burner
# Should show 4 processes (1 main + 3 workers)

# Monitor NTP performance
chronyc tracking&lt;/code&gt;
    &lt;p&gt;Example output from &lt;code&gt;chronyc tracking&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;Reference ID    : 50505300 (PPS)
Stratum         : 1
Ref time (UTC)  : Sun Nov 24 16:45:23 2025
System time     : 0.000000038 seconds fast of NTP time
Last offset     : -0.000000012 seconds
RMS offset      : 0.000000035 seconds
Frequency       : 1.685 ppm slow
Residual freq   : -0.001 ppm
Skew            : 0.002 ppm
Root delay      : 0.000000001 seconds
Root dispersion : 0.000010521 seconds
Update interval : 16.0 seconds
Leap status     : Normal&lt;/code&gt;
    &lt;p&gt;Notice the RMS offset of 35 nanoseconds – this is the kind of accuracy you can achieve with thermal stabilization.&lt;/p&gt;
    &lt;head rend="h3"&gt;Step 6: Monitor Over Time&lt;/head&gt;
    &lt;p&gt;(Topic for a future post)&lt;/p&gt;
    &lt;p&gt;Set up Grafana dashboards to monitor:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Frequency offset (PPM)&lt;/item&gt;
      &lt;item&gt;RMS offset (nanoseconds)&lt;/item&gt;
      &lt;item&gt;CPU temperature&lt;/item&gt;
      &lt;item&gt;System time offset&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You’ll see the frequency stabilize within a few hours as the PID controller locks onto the target temperature.&lt;/p&gt;
    &lt;head rend="h2"&gt;Monitoring and Troubleshooting&lt;/head&gt;
    &lt;head rend="h3"&gt;Real-Time Monitoring&lt;/head&gt;
    &lt;p&gt;Watch chronyd tracking in real-time:&lt;/p&gt;
    &lt;code&gt;watch -n 1 "chronyc tracking"&lt;/code&gt;
    &lt;p&gt;Check time burner status:&lt;/p&gt;
    &lt;code&gt;sudo systemctl status time-burner.service&lt;/code&gt;
    &lt;p&gt;View time burner output:&lt;/p&gt;
    &lt;code&gt;sudo journalctl -u time-burner.service -f&lt;/code&gt;
    &lt;head rend="h3"&gt;Common Issues&lt;/head&gt;
    &lt;p&gt;Temperature overshoots or oscillates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Adjust PID gains – reduce Kp if oscillating, increase Ki if steady-state error&lt;/item&gt;
      &lt;item&gt;Try different target temperatures (50-60°C range)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;High CPU usage (obviously):&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;This is intentional – the time burner uses ~90% of 3 cores&lt;/item&gt;
      &lt;item&gt;Not suitable for Pis running other workloads&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Chronyd not pinned to CPU 0:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check that the optimization script runs after chronyd starts&lt;/item&gt;
      &lt;item&gt;Adjust the timing in the systemd service dependencies&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Trade-offs and Considerations&lt;/head&gt;
    &lt;p&gt;Let’s be honest about the downsides:&lt;/p&gt;
    &lt;head rend="h3"&gt;Power Consumption&lt;/head&gt;
    &lt;p&gt;The time burner keeps 3 cores at ~30% average utilization. My Pi now draws about 3-4W continuously (vs 1-2W idle). Over a year, that’s an extra 15-25 kWh, or about $2-3 in electricity (depending on your rates).&lt;/p&gt;
    &lt;head rend="h3"&gt;Heat&lt;/head&gt;
    &lt;p&gt;Running at 54°C means the Pi is warm to the touch. This is well within safe operating temperature (thermal throttling doesn’t start until 80°C), but you might want to ensure adequate ventilation. I added a small heatsink just to be safe.&lt;/p&gt;
    &lt;head rend="h3"&gt;CPU Resources&lt;/head&gt;
    &lt;p&gt;You’re dedicating 3 of 4 cores to burning cycles. This is fine for a dedicated NTP server, but not suitable if you’re running other services on the same Pi. That said, I am also running the feeder to my new ADS-B aircraft visualization app on it. My readsb instance regularly gets to 1200 msg/s with 200+ aircraft.&lt;/p&gt;
    &lt;head rend="h3"&gt;Is It Worth It?&lt;/head&gt;
    &lt;p&gt;For 99.999% of use cases: absolutely not.&lt;/p&gt;
    &lt;p&gt;Most applications don’t need better than millisecond accuracy, let alone the 35-nanosecond RMS offset I’m achieving. Even for distributed systems, microsecond-level accuracy is typically overkill.&lt;/p&gt;
    &lt;p&gt;When this might make sense:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Precision timing applications (scientific instrumentation, radio astronomy)&lt;/item&gt;
      &lt;item&gt;Distributed systems research requiring tight clock synchronization&lt;/item&gt;
      &lt;item&gt;Network testing where timing precision affects results&lt;/item&gt;
      &lt;item&gt;Because you can (the best reason for any homelab project)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For me, this falls squarely in the “because you can” category. I had the monitoring infrastructure in place, noticed the thermal correlation, and couldn’t resist solving the problem. Plus, I learned a lot about PID control, CPU thermal characteristics, and Linux real-time scheduling.&lt;/p&gt;
    &lt;head rend="h2"&gt;Future Improvements&lt;/head&gt;
    &lt;p&gt;Some ideas I’m considering:&lt;/p&gt;
    &lt;head rend="h3"&gt;Adaptive PID Tuning&lt;/head&gt;
    &lt;p&gt;The current PID gains are hand-tuned for a specific ambient temperature range. The fairly low P value is to avoid spikes when some load on the Pi kicks up the temp. The I is a balance to keep long term “burn” relatively consistent. Implementing an auto-tuning algorithm (like Ziegler-Nichols) or adaptive PID could handle seasonal temperature variations better.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hardware Thermal Control&lt;/head&gt;
    &lt;p&gt;Instead of software thermal control, I could add an actively cooled heatsink with PWM fan control. This might achieve similar temperature stability while using less power overall.&lt;/p&gt;
    &lt;head rend="h3"&gt;Oven-Controlled Crystal Oscillator (OCXO)&lt;/head&gt;
    &lt;p&gt;For the ultimate in frequency stability, replacing the Pi’s crystal with a temperature-controlled OCXO would eliminate thermal drift at the source. This is how professional timing equipment works. I do have a BH3SAP GPSDO sitting next to me (subject to a future post)… Then again, I’m the person who just wrote 4000 words about optimizing a $50 time server, so who am I kidding?&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusions&lt;/head&gt;
    &lt;p&gt;Through a combination of CPU core isolation and PID-controlled thermal stabilization, I achieved:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;81% reduction in frequency variability&lt;/item&gt;
      &lt;item&gt;77% reduction in frequency standard deviation&lt;/item&gt;
      &lt;item&gt;74% reduction in frequency range&lt;/item&gt;
      &lt;item&gt;49% reduction in RMS offset&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The system now maintains 38-nanosecond median RMS offset from the GPS PPS reference, with frequency drift that’s barely detectable in the noise. The CPU runs at a constant 54°C, and in steady state, the frequency offset stays within a tight ±0.14 PPM band (compared to ±0.52 PPM before optimization).&lt;/p&gt;
    &lt;p&gt;Was this necessary? No. Did I learn a bunch about thermal management, PID control, and Linux real-time scheduling? Yes. Would I do it again? Absolutely.&lt;/p&gt;
    &lt;head rend="h3"&gt;Resource&lt;/head&gt;
    &lt;p&gt;I did come across a “burn” script that was the basis for this thermal management. I can’t find it at the moment, but when I do I’ll link it here.&lt;/p&gt;
    &lt;head rend="h3"&gt;Related Posts&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Microsecond-Accurate NTP with a Raspberry Pi and PPS GPS (2021)&lt;/item&gt;
      &lt;item&gt;Revisiting Microsecond-Accurate NTP for Raspberry Pi in 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Further Reading&lt;/head&gt;
    &lt;p&gt;Have questions or suggestions? Drop a comment below. I’m particularly interested to hear if anyone has tried alternative thermal management approaches or has experience with OCXO modules for Raspberry Pi timing applications.&lt;/p&gt;
    &lt;p&gt;Thanks for reading, and happy timekeeping!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46042946</guid><pubDate>Tue, 25 Nov 2025 06:35:59 +0000</pubDate></item><item><title>Making Crash Bandicoot (2011)</title><link>https://all-things-andy-gavin.com/video-games/making-crash/</link><description>&lt;doc fingerprint="5fc2b1ecb077262e"&gt;
  &lt;main&gt;
    &lt;p&gt;As one of the co-creators of Crash Bandicoot, I have been (slowly) writing a long series of posts on the making of everyone’s favorite orange marsupial. You can find them all below, so enjoy.&lt;/p&gt;
    &lt;p&gt;If you are on mobile and cannot see the grid of posts, click here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46045039</guid><pubDate>Tue, 25 Nov 2025 12:05:39 +0000</pubDate></item><item><title>Trillions spent and big software projects are still failing</title><link>https://spectrum.ieee.org/it-management-software-failures</link><description>&lt;doc fingerprint="f94c52586b670fdc"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Trillions Spent and Big Software Projects Are Still Failing&lt;/head&gt;&lt;p&gt;AI won’t solve IT’s management problems&lt;/p&gt;&lt;p&gt;“Why worry about something that isn’t going to happen?”&lt;/p&gt;&lt;p&gt;KGB Chairman Charkov’s question to inorganic chemist Valery Legasov in HBO’s “Chernobyl” miniseries makes a good epitaph for the hundreds of software development, modernization, and operational failures I have covered for IEEE Spectrum since my first contribution, to its September 2005 special issue on learning—or rather, not learning—from software failures. I noted then, and it’s still true two decades later: Software failures are universally unbiased. They happen in every country, to large companies and small. They happen in commercial, nonprofit, and governmental organizations, regardless of status or reputation.&lt;/p&gt;&lt;p&gt;Global IT spending has more than tripled in constant 2025 dollars since 2005, from US $1.7 trillion to $5.6 trillion, and continues to rise. Despite additional spending, software success rates have not markedly improved in the past two decades. The result is that the business and societal costs of failure continue to grow as software proliferates, permeating and interconnecting every aspect of our lives.&lt;/p&gt;&lt;p&gt;For those hoping AI software tools and coding copilots will quickly make large-scale IT software projects successful, forget about it. For the foreseeable future, there are hard limits on what AI can bring to the table in controlling and managing the myriad intersections and trade-offs among systems engineering, project, financial, and business management, and especially the organizational politics involved in any large-scale software project. Few IT projects are displays of rational decision-making from which AI can or should learn. As software practitioners know, IT projects suffer from enough management hallucinations and delusions without AI adding to them.&lt;/p&gt;&lt;p&gt;As I noted 20 years ago, the drivers of software failure frequently are failures of human imagination, unrealistic or unarticulated project goals, the inability to handle the project’s complexity, or unmanaged risks, to name a few that today still regularly cause IT failures. Numerous others go back decades, such as those identified by Stephen Andriole, the chair of business technology at Villanova University’s School of Business, in the diagram below first published in Forbes in 2021. Uncovering a software system failure that has gone off the rails in a unique, previously undocumented manner would be surprising because the overwhelming majority of software-related failures involve avoidable, known failure-inducing factors documented in hundreds of after-action reports, academic studies, and technical and management books for decades. Failure déjà vu dominates the literature.&lt;/p&gt;&lt;p&gt;The question is, why haven’t we applied what we have repeatedly been forced to learn?&lt;/p&gt;&lt;head rend="h2"&gt;The Phoenix That Never Rose&lt;/head&gt;&lt;p&gt;Many of the IT developments and operational failures I have analyzed over the last 20 years have each had their own Chernobyl-like meltdowns, spreading reputational radiation everywhere and contaminating the lives of those affected for years. Each typically has a story that strains belief. A prime example is the Canadian government’s CA $310 million Phoenix payroll system, which went live in April 2016 and soon after went supercritical.&lt;/p&gt;&lt;p&gt;Phoenix project executives believed they could deliver a modernized payment system, customizing PeopleSoft’s off-the-shelf payroll package to follow 80,000 pay rules spanning 105 collective agreements with federal public-service unions. It also was attempting to implement 34 human-resource system interfaces across 101 government agencies and departments required for sharing employee data. Further, the government’s developer team thought they could accomplish this for less than 60 percent of the vendor’s proposed budget. They’d save by removing or deferring critical payroll functions, reducing system and integration testing, decreasing the number of contractors and government staff working on the project, and forgoing vital pilot testing, along with a host of other overly optimistic proposals.&lt;/p&gt;&lt;head rend="h3"&gt;The Worst IT Failure&lt;/head&gt;&lt;p&gt;Jordan Pettitt/PA Images/Getty Images&lt;/p&gt;&lt;p&gt;The Phoenix payroll failure pales in comparison to the worst operational IT system failure to date: the U.K. Post Office’s electronic point-of-sale (EPOS) Horizon system, provided by Fujitsu. Rolled out in 1999, Horizon was riddled with internal software errors that were deliberately hidden, leading to the Post Office unfairly accusing 3,500 local post branch managers of false accounting, fraud, and theft. Approximately 900 of these managers were convicted, with 236 incarcerated between 1999 and 2015. By then, the general public and the branch managers themselves finally joined Computer Weekly’s reporters (who had doggedly reported on Horizon’s problems since 2008) in the knowledge that there was something seriously wrong with Horizon’s software. It then took another decade of court cases, an independent public statutory inquiry, and an ITV miniseries “Mr. Bates vs. The Post Office” to unravel how the scandal came to be.&lt;/p&gt;&lt;p&gt;Like Phoenix, Horizon was plagued with problems that involved technical, management, organizational, legal, and ethical failures. For example, the core electronic point-of-sale system software was built on communication and data-transfer middleware that was itself buggy. In addition, Horizon’s functionality ran wild under unrelenting, ill-disciplined scope creep. There were ineffective or missing development and project management processes, inadequate testing, and a lack of skilled professional, technical, and managerial personnel.&lt;/p&gt;&lt;p&gt;The Post Office’s senior leadership repeatedly stated that the Horizon software was fully reliable, becoming hostile toward postmasters who questioned it, which only added to the toxic environment. As a result, leadership invoked every legal means at its disposal and crafted a world-class cover-up, including the active suppression of exculpatory information, so that the Post Office could aggressively prosecute postmasters and attempt to crush any dissent questioning Horizon’s integrity.&lt;/p&gt;Shockingly, those wrongly accused still have to continue to fight to be paid just compensation for their ruined lives. Nearly 350 of the accused died, at least 13 of whom are believed to be by suicide, before receiving any payments for the injustices experienced. Unfortunately, as attempts to replace Horizon in 2016 and 2021 failed, the Post Office continues to use it, at least for now. The government wants to spend £410 million on a new system, but it’s a safe bet that implementing it will cost much, much more. The Post Office accepted bids for a new point-of-sale software system in summer 2025, with a decision expected by 1 July 2026.&lt;p&gt;Phoenix’s payroll meltdown was preordained. As a result, over the past nine years, around 70 percent of the 430,000 current and former Canadian federal government employees paid through Phoenix have endured paycheck errors. Even as recently as fiscal year 2023–2024, a third of all employees experienced paycheck mistakes. The ongoing financial stress and anxieties for thousands of employees and their families have been immeasurable. Not only are recurring paycheck troubles sapping worker morale, but in at least one documented case, a coroner blamed an employee’s suicide on the unbearable financial and emotional strain she suffered.&lt;/p&gt;&lt;p&gt;By the end of March 2025, when the Canadian government had promised that the backlog of Phoenix errors would finally be cleared, over 349,000 were still unresolved, with 53 percent pending for more than a year. In June, the Canadian government once again committed to significantly reducing the backlog, this time by June 2026. Given previous promises, skepticism is warranted.&lt;/p&gt;&lt;head rend="h3"&gt;Minnesota Licensing and Registration System&lt;/head&gt;Anthony Souffle/Star Tribune/AP&lt;p&gt;2019&lt;/p&gt;&lt;p&gt;The planned $41 million Minnesota Licensing and Registration System (MNLARS) effort is rolled out in 2016 and then is canceled in 2019 after a total cost of $100 million. It is deemed too hard to fix.&lt;/p&gt;&lt;p&gt;The financial costs to Canadian taxpayers related to Phoenix’s troubles have so far climbed to over CA $5.1 billion (US $3.6 billion). It will take years to calculate the final cost of the fiasco. The government spent at least CA $100 million (US $71 million) before deciding on a Phoenix replacement, which the government acknowledges will cost several hundred million dollars more and take years to implement. The late Canadian Auditor General Michael Ferguson’s audit reports for the Phoenix fiasco described the effort as an “incomprehensible failure of project management and oversight.”&lt;/p&gt;&lt;p&gt;While it may be a project management and oversight disaster, an inconceivable failure Phoenix certainly is not. The IT community has striven mightily for decades to make the incomprehensible routine.&lt;/p&gt;&lt;head rend="h2"&gt;Opportunity Costs of Software Failure Keep Piling Up&lt;/head&gt;&lt;p&gt;South of the Canadian border, the United States has also seen the overall cost of IT-related development and operational failures since 2005 rise to the multi-trillion-dollar range, potentially topping $10 trillion. A report from the Consortium for Information &amp;amp; Software Quality (CISQ) estimated the annual cost of operational software failures in the United States in 2022 alone was $1.81 trillion, with another $260 billion spent on software-development failures. It is larger than the total U.S. defense budget for that year, $778 billion.&lt;/p&gt;&lt;p&gt;The question is, why haven’t we applied what we have repeatedly been forced to learn?&lt;/p&gt;&lt;p&gt;What percentage of software projects fail, and what failure means, has been an ongoing debate within the IT community stretching back decades. Without diving into the debate, it’s clear that software development remains one of the riskiest technological endeavors to undertake. Indeed, according to Bent Flyvbjerg, professor emeritus at the University of Oxford’s Saїd Business School, comprehensive data shows that not only are IT projects risky, they are the riskiest from a cost perspective.&lt;/p&gt;&lt;head rend="h3"&gt;Australia Modernising Business Registers Program&lt;/head&gt;&lt;p&gt;iStock&lt;/p&gt;&lt;p&gt;2022&lt;/p&gt;&lt;p&gt;Australia’s planned AU $480.5 million program to modernize it business register systems is canceled. After AU $530 million is spent, a review finds that the projected cost has risen to AU $2.8 billion, and the project would take five more years to complete.&lt;/p&gt;&lt;p&gt;The CISQ report estimates that organizations in the United States spend more than $520 billion annually supporting legacy software systems, with 70 to 75 percent of organizational IT budgets devoted to legacy maintenance. A 2024 report by services company NTT DATA found that 80 percent of organizations concede that “inadequate or outdated technology is holding back organizational progress and innovation efforts.” Furthermore, the report says that virtually all C-level executives believe legacy infrastructure thwarts their ability to respond to the market. Even so, given that the cost of replacing legacy systems is typically many multiples of the cost of supporting them, business executives hesitate to replace them until it is no longer operationally feasible or cost-effective. The other reason is a well-founded fear that replacing them will turn into a debacle like Phoenix or others.&lt;/p&gt;&lt;p&gt;Nevertheless, there have been ongoing attempts to improve software development and sustainment processes. For example, we have seen increasing adoption of iterative and incremental strategies to develop and sustain software systems through Agile approaches, DevOps methods, and other related practices.&lt;/p&gt;&lt;head rend="h3"&gt;Louisiana Office of Motor Vehicles&lt;/head&gt;&lt;p&gt;Gerald Herbert/AP&lt;/p&gt;&lt;p&gt;2025&lt;/p&gt;&lt;p&gt;Louisiana’s governor orders a state of emergency over repeated failures of the 50-year-old Office of Motor Vehicles mainframe computer system. The state promises expedited acquisition of a new IT system, which might be available by early 2028.&lt;/p&gt;&lt;p&gt;The goal is to deliver usable, dependable, and affordable software to end users in the shortest feasible time. DevOps strives to accomplish this continuously throughout the entire software life cycle. While Agile and DevOps have proved successful for many organizations, they also have their share of controversy and pushback. Provocative reports claim Agile projects have a failure rate of up to 65 percent, while others claim up to 90 percent of DevOps initiatives fail to meet organizational expectations.&lt;/p&gt;&lt;p&gt;It is best to be wary of these claims while also acknowledging that successfully implementing Agile or DevOps methods takes consistent leadership, organizational discipline, patience, investment in training, and culture change. However, the same requirements have always been true when introducing any new software platform. Given the historic lack of organizational resolve to instill proven practices, it is not surprising that novel approaches for developing and sustaining ever more complex software systems, no matter how effective they may be, will also frequently fall short.&lt;/p&gt;&lt;head rend="h2"&gt;Persisting in Foolish Errors&lt;/head&gt;&lt;p&gt;The frustrating and perpetual question is why basic IT project-management and governance mistakes during software development and operations continue to occur so often, given the near-total societal reliance on reliable software and an extensively documented history of failures to learn from? Next to electrical infrastructure, with which IT is increasingly merging into a mutually codependent relationship, the failure of our computing systems is an existential threat to modern society.&lt;/p&gt;&lt;p&gt;Frustratingly, the IT community stubbornly fails to learn from prior failures. IT project managers routinely claim that their project is somehow different or unique and, thus, lessons from previous failures are irrelevant. That is the excuse of the arrogant, though usually not the ignorant. In Phoenix’s case, for example, it was the government’s second payroll-system replacement attempt, the first effort ending in failure in 1995. Phoenix project managers ignored the well-documented reasons for the first failure because they claimed its lessons were not applicable, which did nothing to keep the managers from repeating them. As it’s been said, we learn more from failure than from success, but repeated failures are damn expensive.&lt;/p&gt;&lt;head rend="h3"&gt;Jaguar Land Rover&lt;/head&gt;&lt;p&gt;Alamy&lt;/p&gt;&lt;p&gt;2025&lt;/p&gt;&lt;p&gt;A cyberattack forced Jaguar Land Rover, Britain’s largest automaker, to shut down its global operations for over a month. An initial FAIR-MAM assessment, a cybersecurity-cost-model, estimates the loss for Jaguar Land Rover to be between $1.2 billion and $1.9 billion (£911 million and £1.4 billion), which has affected its 33,000 employees and some 200,000 employees of its suppliers.&lt;/p&gt;&lt;p&gt;Not all software development failures are bad; some failures are even desired. When pushing the limits of developing new types of software products, technologies, or practices, as is happening with AI-related efforts, potential failure is an accepted possibility. With failure, experience increases, new insights are gained, fixes are made, constraints are better understood, and technological innovation and progress continue. However, most IT failures today are not related to pushing the innovative frontiers of the computing art, but the edges of the mundane. They do not represent Austrian economist Joseph Schumpeter’s “gales of creative destruction.” They’re more like gales of financial destruction. Just how many more enterprise resource planning (ERP) project failures are needed before success becomes routine? Such failures should be called IT blunders, as learning anything new from them is dubious at best.&lt;/p&gt;&lt;p&gt;Was Phoenix a failure or a blunder? I argue strongly for the latter, but at the very least, Phoenix serves as a master class in IT project mismanagement. The question is whether the Canadian government learned from this experience any more than it did from 1995’s payroll-project fiasco? The government maintains it will learn, which might be true, given the Phoenix failure’s high political profile. But will Phoenix’s lessons extend to the thousands of outdated Canadian government IT systems needing replacement or modernization? Hopefully, but hope is not a methodology, and purposeful action will be necessary.&lt;/p&gt;&lt;p&gt;The IT community has striven mightily for decades to make the incomprehensible routine.&lt;/p&gt;&lt;p&gt;Repeatedly making the same mistakes and expecting a different result is not learning. It is a farcical absurdity. Paraphrasing Henry Petroski in his book To Engineer Is Human: The Role of Failure in Successful Design (Vintage, 1992), we may have learned how to calculate the software failure due to risk, but we have not learned how to calculate to eliminate the failure of the mind. There are a plethora of examples of projects like Phoenix that failed in part due to bumbling management, yet it is extremely difficult to find software projects managed professionally that still failed. Finding examples of what could be termed “IT heroic failures” is like Diogenes seeking one honest man.&lt;/p&gt;&lt;p&gt;The consequences of not learning from blunders will be much greater and more insidious as society grapples with the growing effects of artificial intelligence, or more accurately, “intelligent” algorithms embedded into software systems. Hints of what might happen if past lessons go unheeded are found in the spectacular early automated decision-making failure of Michigan’s MiDAS unemployment and Australia’s Centrelink “Robodebt” welfare systems. Both used questionable algorithms to identify deceptive payment claims without human oversight. State officials used MiDAS to accuse tens of thousands of Michiganders of unemployment fraud, while Centrelink officials falsely accused hundreds of thousands of Australians of being welfare cheats. Untold numbers of lives will never be the same because of what occurred. Government officials in Michigan and Australia placed far too much trust in those algorithms. They had to be dragged, kicking and screaming, to acknowledge that something was amiss, even after it was clearly demonstrated that the software was untrustworthy. Even then, officials tried to downplay the errors’ impact on people, then fought against paying compensation to those adversely affected by the errors. While such behavior is legally termed “maladministration,” administrative evil is closer to reality.&lt;/p&gt;&lt;head rend="h3"&gt;Lidl Enterprise Resource Planning (ERP)&lt;/head&gt;&lt;p&gt;Nicolas Guyonnet/Hans Lucas/AFP/Getty Images&lt;/p&gt;&lt;p&gt;2017&lt;/p&gt;&lt;p&gt;The international supermarket chain Lidl decides to revert to its homegrown legacy merchandise-management system after three years of trying to make SAP’s €500 million enterprise resource planning (ERP) system work properly.&lt;/p&gt;&lt;p&gt;If this behavior happens in government organizations, does anyone think profit-driven companies whose AI-driven systems go wrong are going to act any better? As AI becomes embedded in ever more IT systems—especially governmental systems and the growing digital public infrastructure, which we as individuals have no choice but to use—the opaqueness of how these systems make decisions will make it harder to challenge them. The European Union has given individuals a legal “right to explanation” when a purely algorithmic decision goes against them. It’s time for transparency and accountability regarding all automated systems to become a fundamental, global human right.&lt;/p&gt;&lt;p&gt;What will it take to reduce IT blunders? Not much has worked with any consistency over the past 20 years. The financial incentives for building flawed software, the IT industry’s addiction to failure porn, and the lack of accountability for foolish management decisions are deeply entrenched in the IT community. Some argue it is time for software liability laws, while others contend that it is time for IT professionals to be licensed like all other professionals. Neither is likely to happen anytime soon.&lt;/p&gt;&lt;head rend="h3"&gt;Boeing 737 Max&lt;/head&gt;David Ryder/ Getty Images&lt;p&gt;2018&lt;/p&gt;&lt;p&gt;Boeing adds poorly designed and described Maneuvering Characteristics Augmentation System (MCAS) to new 737 Max model creating safety problems leading to two fatal airline crashes killing 346 passengers and crew and grounding of fleet for some 20 months. Total cost to Boeing estimates at $14b in direct costs and $60b in indirect costs.&lt;/p&gt;&lt;p&gt;So, we are left with only a professional and personal obligation to reemphasize the obvious: Ask what you do know, what you should know, and how big the gap is between them before embarking on creating an IT system. If no one else has ever successfully built your system with the schedule, budget, and functionality you asked for, please explain why your organization thinks it can. Software is inherently fragile; building complex, secure, and resilient software systems is difficult, detailed, and time-consuming. Small errors have outsize effects, each with an almost infinite number of ways they can manifest, from causing a minor functional error to a system outage to allowing a cybersecurity threat to penetrate the system. The more complex and interconnected the system, the more opportunities for errors and their exploitation. A nice start would be for senior management who control the purse strings to finally treat software and systems development, operations, and sustainment efforts with the respect they deserve. This not only means providing the personnel, financial resources, and leadership support and commitment, but also the professional and personal accountability they demand.&lt;/p&gt;&lt;head rend="h3"&gt;F-35 Joint Strike Fighter&lt;/head&gt;&lt;p&gt;Staff Sgt .Zachary Rufus/ U.S. Air Force&lt;/p&gt;&lt;p&gt;2025&lt;/p&gt;&lt;p&gt;Software and hardware issues with the F-35 Block 4 upgrade continue unabated. The Block 4 upgrade program which started in 2018, and is intended to increase the lethality of the JSF aircraft has slipped to 2031 at earliest from 2026, with cost rising from $10.5 b to a minimum of $16.5b. It will take years more to rollout the capability to the F-35 fleet.&lt;/p&gt;&lt;p&gt;It is well known that honesty, skepticism, and ethics are essential to achieving project success, yet they are often absent. Only senior management can demand they exist. For instance, honesty begins with the forthright accounting of the myriad of risks involved in any IT endeavor, not their rationalization. It is a common “secret” that it is far easier to get funding to fix a troubled software development effort than to ask for what is required up front to address the risks involved. Vendor puffery may also be legal, but that means the IT customer needs a healthy skepticism of the typically too-good-to-be-true promises vendors make. Once the contract is signed, it is too late. Furthermore, computing’s malleability, complexity, speed, low cost, and ability to reproduce and store information combine to create ethical situations that require deep reflection about computing’s consequences on individuals and society. Alas, ethical considerations have routinely lagged when technological progress and profits are to be made. This practice must change, especially as AI is routinely injected into automated systems.&lt;/p&gt;&lt;p&gt;In the AI community, there has been a movement toward the idea of human-centered AI, meaning AI systems that prioritize human needs, values, and well-being. This means trying to anticipate where and when AI can go wrong, move to eliminate these situations, and build in ways to mitigate the effects if they do happen. This concept requires application to every IT system’s effort, not just AI.&lt;/p&gt;&lt;p&gt;Given the historic lack of organizational resolve to instill proven practices...novel approaches for developing and sustaining ever more complex software systems...will also frequently fall short.&lt;/p&gt;&lt;p&gt;Finally, project cost-benefit justifications of software developments rarely consider the financial and emotional distress placed on end users of IT systems when something goes wrong. These include the long-term failure after-effects. If these costs had to be taken fully into account, such as in the cases of Phoenix, MiDAS, and Centrelink, perhaps there could be more realism in what is required managerially, financially, technologically, and experientially to create a successful software system. It may be a forlorn request, but surely it is time the IT community stops repeatedly making the same ridiculous mistakes it has made since at least 1968, when the term “software crisis” was coined. Make new ones, damn it. As Roman orator Cicero said in Philippic 12, “Anyone can make a mistake, but only an idiot persists in his error.”&lt;/p&gt;&lt;p&gt;Special thanks to Steve Andriole, Hal Berghel, Matt Eisler, John L. King, Roger Van Scoy, and Lee Vinsel for their invaluable critiques and insights.&lt;/p&gt;&lt;p&gt;This article appears in the December 2025 print issue as “The Trillion-Dollar Cost of IT’s Willful Ignorance.”&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Lessons From a Decade of IT Failures ›&lt;/item&gt;&lt;item&gt;We Need Better IT Project Failure Post-Mortems ›&lt;/item&gt;&lt;item&gt;Why Software Fails ›&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46045085</guid><pubDate>Tue, 25 Nov 2025 12:14:11 +0000</pubDate></item><item><title>Brain has five 'eras' with adult mode not starting until early 30s</title><link>https://www.theguardian.com/science/2025/nov/25/brain-human-cognitive-development-life-stages-cambridge-study</link><description>&lt;doc fingerprint="2723832778b3ddf0"&gt;
  &lt;main&gt;
    &lt;p&gt;Scientists have identified five major “epochs” of human brain development in one of the most comprehensive studies to date of how neural wiring changes from infancy to old age.&lt;/p&gt;
    &lt;p&gt;The study, based on the brain scans of nearly 4,000 people aged under one to 90, mapped neural connections and how they evolve during our lives. This revealed five broad phases, split up by four pivotal “turning points” in which brain organisation moves on to a different trajectory, at around the ages of nine, 32, 66 and 83 years.&lt;/p&gt;
    &lt;p&gt;“Looking back, many of us feel our lives have been characterised by different phases. It turns out that brains also go through these eras,” said Prof Duncan Astle, a researcher in neuroinformatics at Cambridge University and senior author of the study.&lt;/p&gt;
    &lt;p&gt;“Understanding that the brain’s structural journey is not a question of steady progression, but rather one of a few major turning points, will help us identify when and how its wiring is vulnerable to disruption.”&lt;/p&gt;
    &lt;p&gt;The childhood period of development was found to occur between birth until the age of nine, when it transitions to the adolescent phase – an era that lasts up to the age of 32, on average.&lt;/p&gt;
    &lt;p&gt;In a person’s early 30s the brain’s neural wiring shifts into adult mode – the longest era, lasting more than three decades. A third turning point around the age of 66 marks the start of an “early ageing” phase of brain architecture. Finally, the “late ageing” brain takes shape at around 83 years old.&lt;/p&gt;
    &lt;p&gt;The scientists quantified brain organisation using 12 different measures, including the efficiency of the wiring, how compartmentalised it is and whether the brain relies heavily on central hubs or has a more diffuse connectivity network.&lt;/p&gt;
    &lt;p&gt;From infancy through childhood, our brains are defined by “network consolidation”, as the wealth of synapses – the connectors between neurons – in a baby’s brain are whittled down, with the more active ones surviving. During this period, the study found, the efficiency of the brain’s wiring decreases.&lt;/p&gt;
    &lt;p&gt;Meanwhile, grey and white matter grow rapidly in volume, so that cortical thickness – the distance between outer grey matter and inner white matter – reaches a peak, and cortical folding, the characteristic ridges on the outer brain, stabilises.&lt;/p&gt;
    &lt;p&gt;In the second “epoch” of the brain, the adolescence era, white matter continues to grow in volume, so organisation of the brain’s communications networks is increasingly refined. This era is defined by steadily increasing efficiency of connections across the whole brain, which is related to enhanced cognitive performance. The epochs were defined by the brain remaining on a constant trend of development over a sustained period, rather than staying in a fixed state throughout.&lt;/p&gt;
    &lt;p&gt;“We’re definitely not saying that people in their late 20s are going to be acting like teenagers, or even that their brain looks like that of a teenager,” said Alexa Mousley, who led the research. “It’s really the pattern of change.”&lt;/p&gt;
    &lt;p&gt;She added that the findings could give insights into risk factors for mental health disorders, which most frequently emerge during the adolescent period.&lt;/p&gt;
    &lt;p&gt;At around the age of 32 the strongest overall shift in trajectory is seen. Life events such as parenthood may play a role in some of the changes seen, although the research did not explicitly test this. “We know that women who give birth, their brain changes afterwards,” said Mousley. “It’s reasonable to assume that there could be a relationship between these milestones and what’s happening in the brain.”&lt;/p&gt;
    &lt;p&gt;From 32 years, the brain architecture appears to stabilise compared with previous phases, corresponding with a “plateau in intelligence and personality” based on other studies. Brain regions also become more compartmentalised.&lt;/p&gt;
    &lt;p&gt;The final two turning points were defined by decreases in brain connectivity, which were believed to be related to ageing and degeneration of white matter in the brain.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46045661</guid><pubDate>Tue, 25 Nov 2025 13:38:12 +0000</pubDate></item><item><title>Launch HN: Onyx (YC W24) – Open-source chat UI</title><link>https://news.ycombinator.com/item?id=46045987</link><description>&lt;doc fingerprint="374119d99fbe8bf8"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt;Hey HN, Chris and Yuhong here from Onyx (&lt;/p&gt;https://github.com/onyx-dot-app/onyx&lt;p&gt;). We’re building an open-source chat that works with any LLM (proprietary + open weight) &lt;/p&gt;and&lt;p&gt; gives these LLMs the tools they need to be useful (RAG, web search, MCP, deep research, memory, etc.).&lt;/p&gt;&lt;p&gt;Demo: https://youtu.be/2g4BxTZ9ztg&lt;/p&gt;&lt;p&gt;Two years ago, Yuhong and I had the same recurring problem. We were on growing teams and it was ridiculously difficult to find the right information across our docs, Slack, meeting notes, etc. Existing solutions required sending out our company's data, lacked customization, and frankly didn't work well. So, we started Danswer, an open-source enterprise search project built to be self-hosted and easily customized.&lt;/p&gt;&lt;p&gt;As the project grew, we started seeing an interesting trend—even though we were explicitly a search app, people wanted to use Danswer just to chat with LLMs. We’d hear, “the connectors, indexing, and search are great, but I’m going to start by connecting GPT-4o, Claude Sonnet 4, and Qwen to provide my team with a secure way to use them”.&lt;/p&gt;&lt;p&gt;Many users would add RAG, agents, and custom tools later, but much of the usage stayed ‘basic chat’. We thought: “why would people co-opt an enterprise search when other AI chat solutions exist?”&lt;/p&gt;&lt;p&gt;As we continued talking to users, we realized two key points:&lt;/p&gt;&lt;p&gt;(1) just giving a company secure access to an LLM with a great UI and simple tools is a huge part of the value add of AI&lt;/p&gt;&lt;p&gt;(2) providing this well is much harder than you might think and the bar is incredibly high&lt;/p&gt;&lt;p&gt;Consumer products like ChatGPT and Claude already provide a great experience—and chat with AI for work is something (ideally) everyone at the company uses 10+ times per day. People expect the same snappy, simple, and intuitive UX with a full feature set. Getting hundreds of small details right to take the experience from “this works” to “this feels magical” is not easy, and nothing else in the space has managed to do it.&lt;/p&gt;&lt;p&gt;So ~3 months ago we pivoted to Onyx, the open-source chat UI with:&lt;/p&gt;&lt;p&gt;- (truly) world class chat UX. Usable both by a fresh college grad who grew up with AI and an industry veteran who’s using AI tools for the first time.&lt;/p&gt;&lt;p&gt;- Support for all the common add-ons: RAG, connectors, web search, custom tools, MCP, assistants, deep research.&lt;/p&gt;&lt;p&gt;- RBAC, SSO, permission syncing, easy on-prem hosting to make it work for larger enterprises.&lt;/p&gt;&lt;p&gt;Through building features like deep research and code interpreter that work across model providers, we've learned a ton of non-obvious things about engineering LLMs that have been key to making Onyx work. I'd like to share two that were particularly interesting (happy to discuss more in the comments).&lt;/p&gt;&lt;p&gt;First, context management is one of the most difficult and important things to get right. We’ve found that LLMs really struggle to remember both system prompts and previous user messages in long conversations. Even simple instructions like “ignore sources of type X” in the system prompt are very often ignored. This is exacerbated by multiple tool calls, which can often feed in huge amounts of context. We solved this problem with a “Reminder” prompt—a short 1-3 sentence blurb injected at the end of the user message that describes the non-negotiables that the LLM must abide by. Empirically, LLMs attend most to the very end of the context window, so this placement gives the highest likelihood of adherence.&lt;/p&gt;&lt;p&gt;Second, we’ve needed to build an understanding of the “natural tendencies” of certain models when using tools, and build around them. For example, the GPT family of models are fine-tuned to use a python code interpreter that operates in a Jupyter notebook. Even if told explicitly, it refuses to add `print()` around the last line, since, in Jupyter, this last line is automatically written to stdout. Other models don’t have this strong preference, so we’ve had to design our model-agnostic code interpreter to also automatically `print()` the last bare line.&lt;/p&gt;&lt;p&gt;So far, we’ve had a Fortune 100 team fork Onyx and provide 10k+ employees access to every model within a single interface, and create thousands of use-case specific Assistants for every department, each using the best model for the job. We’ve seen teams operating in sensitive industries completely airgap Onyx w/ locally hosted LLMs to provide a copilot that wouldn’t have been possible otherwise.&lt;/p&gt;&lt;p&gt;If you’d like to try Onyx out, follow https://docs.onyx.app/deployment/getting_started/quickstart to get set up locally w/ Docker in &amp;lt;15 minutes. For our Cloud: https://www.onyx.app/. If there’s anything you'd like to see to make it a no-brainer to replace your ChatGPT Enterprise/Claude Enterprise subscription, we’d love to hear it!&lt;/p&gt;&lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46045987</guid><pubDate>Tue, 25 Nov 2025 14:20:30 +0000</pubDate></item><item><title>FLUX.2: Frontier Visual Intelligence</title><link>https://bfl.ai/blog/flux-2</link><description>&lt;doc fingerprint="4d170e3094784e8e"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;FLUX.2: Frontier Visual Intelligence&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;News&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;FLUX.2 is designed for real-world creative workflows, not just demos or party tricks. It generates high-quality images while maintaining character and style consistency across multiple reference images, following structured prompts, reading and writing complex text, adhering to brand guidelines, and reliably handling lighting, layouts, and logos. FLUX.2 can edit images at up to 4 megapixels while preserving detail and coherence.&lt;/p&gt;
    &lt;head rend="h2"&gt;Black Forest Labs: Open Core&lt;/head&gt;
    &lt;p&gt;We believe visual intelligence should be shaped by researchers, creatives, and developers everywhere, not just a few. That’s why we pair frontier capability with open research and open innovation, releasing powerful, inspectable, and composable open-weight models for the community, alongside robust, production-ready endpoints for teams that need scale, reliability, and customization.&lt;/p&gt;
    &lt;p&gt;When we launched Black Forest Labs in 2024, we set out to make open innovation sustainable, building on our experience developing some of the world’s most popular open models. We’ve combined open models like FLUX.1 [dev]—the most popular open image model globally—with professional-grade models like FLUX.1 Kontext [pro], which powers teams from Adobe to Meta and beyond. Our open core approach drives experimentation, invites scrutiny, lowers costs, and ensures that we can keep sharing open technology from the Black Forest and the Bay into the world.&lt;/p&gt;
    &lt;head rend="h2"&gt;From FLUX.1 to FLUX.2&lt;/head&gt;
    &lt;p&gt;Precision, efficiency, control, extreme realism - where FLUX.1 showed the potential of media models as powerful creative tools, FLUX.2 shows how frontier capability can transform production workflows. By radically changing the economics of generation, FLUX.2 will become an indispensable part of our creative infrastructure.&lt;/p&gt;
    &lt;p&gt;Output Versatility: FLUX.2 is capable of generating highly detailed, photoreal images along with infographics with complex typography, all at resolutions up to 4MP&lt;/p&gt;
    &lt;head rend="h2"&gt;What’s New&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Multi-Reference Support: Reference up to 10 images simultaneously with the best character / product / style consistency available today.&lt;/item&gt;
      &lt;item&gt;Image Detail &amp;amp; Photorealism: Greater detail, sharper textures, and more stable lighting suitable for product shots, visualization, and photography-like use cases.&lt;/item&gt;
      &lt;item&gt;Text Rendering: Complex typography, infographics, memes and UI mockups with legible fine text now work reliably in production.&lt;/item&gt;
      &lt;item&gt;Enhanced Prompt Following: Improved adherence to complex, structured instructions, including multi-part prompts and compositional constraints.&lt;/item&gt;
      &lt;item&gt;World Knowledge: Significantly more grounded in real-world knowledge, lighting, and spatial logic, resulting in more coherent scenes with expected behavior.&lt;/item&gt;
      &lt;item&gt;Higher Resolution &amp;amp; Flexible Input/Output Ratios: Image editing on resolutions up to 4MP.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;All variants of FLUX.2 offer image editing from text and multiple references in one model.&lt;/p&gt;
    &lt;head rend="h2"&gt;Available Now&lt;/head&gt;
    &lt;p&gt;The FLUX.2 family covers a spectrum of model products, from fully managed, production-ready APIs to open-weight checkpoints developers can run themselves. The overview graph below shows how FLUX.2 [pro], FLUX.2 [flex], FLUX.2 [dev], and FLUX.2 [klein] balance performance, and control&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;FLUX.2 [pro]: State-of-the-art image quality that rivals the best closed models, matching other models for prompt adherence and visual fidelity while generating images faster and at lower cost. No compromise between speed and quality. → Available now at BFL Playground, the BFL API and via our launch partners.&lt;/item&gt;
      &lt;item&gt;FLUX.2 [flex]: Take control over model parameters such as the number of steps and the guidance scale, giving developers full control over quality, prompt adherence and speed. This model excels at rendering text and fine details. → Available now at bfl.ai/play , the BFL API and via our launch partners.&lt;/item&gt;
      &lt;item&gt;FLUX.2 [dev]: 32B open-weight model, derived from the FLUX.2 base model. The most powerful open-weight image generation and editing model available today, combining text-to-image synthesis and image editing with multiple input images in a single checkpoint. FLUX.2 [dev] weights are available on Hugging Face and can now be used locally using our reference inference code. On consumer grade GPUs like GeForce RTX GPUs you can use an optimized fp8 reference implementation of FLUX.2 [dev], created in collaboration with NVIDIA and ComfyUI. You can also sample Flux.2 [dev] via API endpoints on FAL, Replicate, Runware, Verda, TogetherAI, Cloudflare, DeepInfra. For a commercial license, visit our website.&lt;/item&gt;
      &lt;item&gt;FLUX.2 [klein] (coming soon): Open-source, Apache 2.0 model, size-distilled from the FLUX.2 base model. More powerful &amp;amp; developer-friendly than comparable models of the same size trained from scratch, with many of the same capabilities as its teacher model. Join the beta&lt;/item&gt;
      &lt;item&gt;FLUX.2 - VAE: A new variational autoencoder for latent representations that provide an optimized trade-off between learnability, quality and compression rate. This model provides the foundation for all FLUX.2 flow backbones, and an in-depth report describing its technical properties is available here. The FLUX.2 - VAE is available on HF under an Apache 2.0 license.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Generating designs with variable steps: FLUX.2 [flex] provides a “steps” parameter, trading off typography accuracy and latency. From left to right: 6 steps, 20 steps, 50 steps.&lt;/p&gt;
    &lt;p&gt;Controlling image detail with variable steps: FLUX.2 [flex] provides a “steps” parameter, trading off image detail and latency. From left to right: 6 steps, 20 steps, 50 steps.&lt;/p&gt;
    &lt;p&gt;The FLUX.2 model family delivers state-of-the-art image generation quality at extremely competitive prices, offering the best value across performance tiers.&lt;/p&gt;
    &lt;p&gt;For open-weights image models, FLUX.2 [dev] sets a new standard, achieving leading performance across text-to-image generation, single-reference editing, and multi-reference editing, consistently outperforming all open-weights alternatives by a significant margin.&lt;/p&gt;
    &lt;p&gt;Whether open or closed, we are committed to the responsible development of these models and services before, during, and after every release.&lt;/p&gt;
    &lt;head rend="h2"&gt;How It Works&lt;/head&gt;
    &lt;p&gt;FLUX.2 builds on a latent flow matching architecture, and combines image generation and editing in a single architecture. The model couples the Mistral-3 24B parameter vision-language model with a rectified flow transformer. The VLM brings real world knowledge and contextual understanding, while the transformer captures spatial relationships, material properties, and compositional logic that earlier architectures could not render.&lt;/p&gt;
    &lt;p&gt;FLUX.2 now provides multi-reference support, with the ability to combine up to 10 images into a novel output, an output resolution of up to 4MP, substantially better prompt adherence and world knowledge, and significantly improved typography. We re-trained the model’s latent space from scratch to achieve better learnability and higher image quality at the same time, a step towards solving the “Learnability-Quality-Compression” trilemma. Technical details can be found in the FLUX.2 VAE blog post.&lt;/p&gt;
    &lt;head rend="h2"&gt;More Resources:&lt;/head&gt;
    &lt;head rend="h2"&gt;Into the New&lt;/head&gt;
    &lt;p&gt;We're building foundational infrastructure for visual intelligence, technology that transforms how the world is seen and understood. FLUX.2 is a step closer to multimodal models that unify perception, generation, memory, and reasoning, in an open and transparent way.&lt;/p&gt;
    &lt;p&gt;Join us on this journey. We're hiring in Freiburg (HQ) and San Francisco. View open roles.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46046916</guid><pubDate>Tue, 25 Nov 2025 15:47:14 +0000</pubDate></item><item><title>Roblox is a problem but it's a symptom of something worse</title><link>https://www.platformer.news/roblox-ceo-interview-backlash-analysis/</link><description>&lt;doc fingerprint="2502935f3822c5c9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Roblox is a problem — but it’s a symptom of something worse&lt;/head&gt;
    &lt;p&gt;What is the role of tech journalism in a world where CEOs no longer feel shame?&lt;/p&gt;
    &lt;p&gt;I.&lt;/p&gt;
    &lt;p&gt;On Friday, the Hard Fork team published our interview with Roblox CEO David Baszucki. In the days since, it has become the most-discussed interview we've done in three years on the show. Listeners who wrote in to us said they were shocked to hear the leader of a platform with 151.5 million monthly users, most of them minors, express frustration and annoyance at being asked about the company's history of failures related to child safety. Journalists described the interview as "bizarre," "unhinged," and a "car crash."&lt;/p&gt;
    &lt;p&gt;And a case can be made that it was all of those things — even if Baszucki, in the studio afterwards and later on X, insisted to us that he had had a good time. In the moment, though, Baszucki's dismissive attitude toward discussing child safety struck me as something worse: familiar.&lt;/p&gt;
    &lt;p&gt;Baszucki, after all, is not the first CEO to have insisted to me that a platform's problems are smaller than I am making them out to be. Nor is he the first to blame the platform's enormous scale, or to try to change the subject. (He is the first tech CEO to suggest to me that maybe there should be prediction markets in video games for children, but that's another story.)&lt;/p&gt;
    &lt;p&gt;What people found noteworthy about our interview, I think, was the fresh evidence that our most successful tech CEOs really do think and talk this way. Given a chance to display empathy for the victims of crimes his platform enabled, or to convey regret about historical safety lapses, or even just to gesture at some sense of responsibility for the hundreds of millions of children who in various ways are depending on him, the CEO throws up his hands and asks: how long are you guys going to be going on about all this stuff?&lt;/p&gt;
    &lt;p&gt;Roblox is different from other social products in that it explicitly courts users as young as 5. (You are supposed to be at least 13 to use Instagram, TikTok, and other major platforms.) That has always put significant pressure on the company to develop serious safety features. The company says it spends hundreds of millions of dollars a year on safety, and that 10 percent of its employees work on trust and safety issues. And trust and safety workers I know tell me that they respect Roblox's safety teams.&lt;/p&gt;
    &lt;p&gt;At the same time, this is a platform launched in 2006 where, for most of its history, adults could freely approach and message any minor unless their parents had dug into the app settings. Roblox did not verify users' ages, letting any child identify as 13 or older to bypass content restrictions. Filters intended to prevent inappropriate chat or the exchange of personal information were easily bypassed by slightly changing the spelling of words. Parental controls could be circumvented simply by a child creating a new account and declaring that they were at least 13.&lt;/p&gt;
    &lt;p&gt;Last year the company introduced new restrictions on chat. And this year, the company said it would deploy its own age estimation technology to determine users' ages and restrict the content available to them accordingly. This rollout was the main reason we had sought to interview Baszucki in the first place — something we had communicated to his team.&lt;/p&gt;
    &lt;p&gt;Which only made it stranger when Baszucki expressed surprise at our line of inquiry and threw his PR team under the bus. ("If our PR people said, “Let’s talk about age-gating for an hour,' I’m up for it, but I love your pod. I thought I came here to talk about everything,'" he said.)&lt;/p&gt;
    &lt;p&gt;Since 2018, at least two dozen people in the United States have been arrested and accused of abducting or abusing victims they met on Roblox, according to a 2024 investigation by Bloomberg. Attorneys general in Texas, Kentucky, and Louisiana have filed lawsuits against Roblox alleging that the platform facilitates child exploitation and grooming. More than 35 families have filed lawsuits against the company over child predation.&lt;/p&gt;
    &lt;p&gt;As recently as this month, a reporter for the Guardian created an account presenting herself as a child and found that in Roblox she could wander user-created strip clubs, casinos, and horror games. In one "hangout" game, in which she identified as a 13-year-old, another avatar sexually assaulted her by thrusting his hips into her avatar's face as she begged him to leave her alone.&lt;/p&gt;
    &lt;p&gt;It's true that any platform that lets strangers communicate will lead to real-world harm. I believe that millions of children use Roblox daily without incident. And we would not want to shut down the entire internet to prevent a single bad thing from ever happening.&lt;/p&gt;
    &lt;p&gt;But there is much a leader can do with the knowledge that his platform will inevitably lead to harm, should he wish.&lt;/p&gt;
    &lt;p&gt;Understanding how attractive Roblox would be to predators, the company long ago could have blocked unrestricted contact between adults and minors. It could have adopted age verification before a wave of state legislation signaled that it would soon become mandatory anyway. It could have made it harder for children under 13 to create new accounts, and require them to get parental consent in a way it could verify.&lt;/p&gt;
    &lt;p&gt;But doing so would require Roblox to focus on outcomes for children, at the likely expense of growth. And so here we are.&lt;/p&gt;
    &lt;p&gt;II.&lt;/p&gt;
    &lt;p&gt;Galling? Yes. But like I said: it's also familiar.&lt;/p&gt;
    &lt;p&gt;Over and over again, we have seen leaders in Baszucki's position choose growth over guardrails. Safety features come out years after the need for them is identified, if at all. Internal critics are sidelined, laid off, or managed out. And when journalists ask, politely but insistently, why so many of their users are suffering, executives laugh and tell us that we're the crazy ones.&lt;/p&gt;
    &lt;p&gt;Look at OpenAI, where the company is reckoning with the fact that making its models less sycophantic has been worse for user engagement — and is building new features to turn the engagement dial back up.&lt;/p&gt;
    &lt;p&gt;Look at TikTok, which has answered concerns that short-form video is worsening academic performance for children with new "digital well-being features" that include an affirmation journal, a "background sound generator aimed at improving the mental health of its users," and "new badges to reward people who use the platform within limits, especially teens." Answering concerns that teens are using the app too much with more reasons to use the app.&lt;/p&gt;
    &lt;p&gt;Or look at Meta, where new court filings from over the weekend allege ... a truly staggering number of things. To name a few: the company "stalled internal efforts to prevent child predators from contacting minors for years due to growth concerns," according to Jeff Horwitz in Reuters; "recognized that optimizing its products to increase teen engagement resulted in serving them more harmful content, but did so anyway"; and gave users 17 attempts to traffic people for sex before banning their accounts. (Meta denies the allegations, which are drawn from internal documents that have not been made public; Meta has also objected to unsealing the documents.)&lt;/p&gt;
    &lt;p&gt;Lawsuits will always contain the most salacious allegations lawyers can find, of course. But what struck me about these latest filings is not the lawyers' predictably self-serving framing but rather the quotes from Meta's own employees.&lt;/p&gt;
    &lt;p&gt;When the company declined to publish internal research from 2019 which showed that no longer looking at Facebook and Instagram improved users' mental health, one employee said: "If the results are bad and we don’t publish and they leak ... is it going to look like tobacco companies doing research and knowing cigs were bad and then keeping that info to themselves?”&lt;/p&gt;
    &lt;p&gt;When Meta researchers found that by 2018, approximately 40 percent of children ages 9 to 12 were daily Instagram users — despite the fact that you are supposed to be 13 to join — some employees bristled at what they perceived as tacit encouragement from executives to accelerate growth efforts among children.&lt;/p&gt;
    &lt;p&gt;"Oh good, we’re going after &amp;lt;13 year olds now?” one wrote, as cited in Time's account of the brief. “Zuck has been talking about that for a while...targeting 11 year olds feels like tobacco companies a couple decades ago (and today). Like we’re seriously saying ‘we have to hook them young’ here.”&lt;/p&gt;
    &lt;p&gt;When Meta studied the potential of its products to be addictive in 2018, it found that 55 percent of 20,000 surveyed users showed at least some signs of "problematic use." When it published that research the following year, though, it redefined "problematic use" to include only the most severe cases — 3.1 percent of users.&lt;/p&gt;
    &lt;p&gt;“Because our product exploits weaknesses in the human psychology to promote product engagement and time spent,” a user experience researcher wrote, the company should “alert people to the effect that the product has on their brain.”&lt;/p&gt;
    &lt;p&gt;You will not be surprised to learn that the company did not alert people to the issue.&lt;/p&gt;
    &lt;p&gt;III.&lt;/p&gt;
    &lt;p&gt;As usual, the rank-and-file employees are doing their job. Over and over again, though, their boss' boss tells them to stop.&lt;/p&gt;
    &lt;p&gt;The thing is, platforms' strategy of delay, deny and deflect mostly works.&lt;/p&gt;
    &lt;p&gt;Americans have short attention spans — and lots to worry about. The tech backlash that kicked off in 2017 inspired platforms to make meaningful and effective investments in content moderation, cybersecurity, platform integrity, and other teams that worked to protect their user bases. Imperfect as these efforts were, they bolstered my sense that tech platforms were susceptible to pressure from the public, from lawmakers and from journalists. They acted slowly, and incompletely, but at least they acted.&lt;/p&gt;
    &lt;p&gt;Fast forward to today and the bargain no longer holds. Platforms do whatever the president of the United States tells them to do, and very little else. Shame, that once-great regulator of social norms and executive behavior, has all but disappeared from public life. In its place is denial, defiance, and the noxious vice signaling of the investor class.&lt;/p&gt;
    &lt;p&gt;I'm still reckoning with what it means to do journalism in a world where the truth can barely hold anyone's attention — much less hold a platform accountable, in any real sense of that word. I'm rethinking how to cover tech policy at a time when it is being made by whim. I'm noticing the degree to which platforms wish to be judged only by their stated intentions, and almost never on the outcomes of anyone who uses them.&lt;/p&gt;
    &lt;p&gt;In the meantime the platforms hurtle onward, pitching ever-more fantastical visions of the future while seeming barely interested in stewarding the present.&lt;/p&gt;
    &lt;p&gt;For the moment, I'm grateful that a car-crash interview drew attention to one CEO's exasperation with being asked about that. But the real problem isn't that David Baszucki talks this way. It's that so many of his peers do, too.&lt;/p&gt;
    &lt;p&gt;Sponsored&lt;/p&gt;
    &lt;head rend="h3"&gt;Unknown number calling? It’s not random…&lt;/head&gt;
    &lt;p&gt;The BBC caught scam call center workers on hidden cameras as they laughed at the people they were tricking.&lt;/p&gt;
    &lt;p&gt;One worker bragged about making $250k from victims. The disturbing truth?&lt;lb/&gt;Scammers don’t pick phone numbers at random. They buy your data from brokers.&lt;/p&gt;
    &lt;p&gt;Once your data is out there, it’s not just calls. It’s phishing, impersonation, and identity theft.&lt;/p&gt;
    &lt;p&gt;That’s why we recommend Incogni: They delete your info from the web, monitor and follow up automatically, and continue to erase data as new risks appear.&lt;/p&gt;
    &lt;p&gt;Black Friday deal: Try Incogni here and get 55% off your subscription with code PLATFORMER&lt;/p&gt;
    &lt;head rend="h2"&gt;Following&lt;/head&gt;
    &lt;head rend="h3"&gt;Trump backs down on AI preemption&lt;/head&gt;
    &lt;p&gt;What happened: Facing criticism from both parties, the Trump administration backed down from issuing an executive order that would have effectively placed a moratorium on state AI regulations, Reuters reported.&lt;/p&gt;
    &lt;p&gt;The order would have fought state regulations by withholding federal funding and establishing an “AI Litigation Task Force” to “challenge State AI laws.”&lt;/p&gt;
    &lt;p&gt;Why we’re following: Last week we covered the draft executive order and how Trump’s attempts to squash state AI regulation have drawn bipartisan backlash — and made Republicans increasingly more sympathetic to the views of AI safety advocates.&lt;/p&gt;
    &lt;p&gt;It's always hard to guess when Trump's instinct to do as he pleases will be thwarted by political opposition. In this case, though, the revived moratorium had little support outside the David Sacks wing of the party. And so — for now, anyway — it fell apart.&lt;/p&gt;
    &lt;p&gt;What people are saying: State lawmakers are fighting the moratorium proposal Trump made to Congress. Today, a letter signed by 280 state lawmakers urged Congress to “reject any provision that overrides state and local AI legislation.”&lt;/p&gt;
    &lt;p&gt;A moratorium would threaten existing laws that “strengthen consumer transparency, guide responsible government procurement, protect patients, and support artists and creators,” the letter said.&lt;/p&gt;
    &lt;p&gt;On the other side of the debate, the tech-funded industry PAC Leading the Future announced a $10 million campaign to push Congress to pass national AI regulations that would supersede state law.&lt;/p&gt;
    &lt;p&gt;—Ella Markianos&lt;/p&gt;
    &lt;head rend="h3"&gt;X’s "About This Account" meltdown&lt;/head&gt;
    &lt;p&gt;What happened: On Friday, X debuted its About This Account feature globally in a rollout that descended into chaos over the feature’s accidental uncovering of foreign actors behind popular right-wing accounts that actively share news on US politics.&lt;/p&gt;
    &lt;p&gt;X users can now see the date an account joined the platform, how many times it has changed its username, and most importantly, the country or region it’s based in. The move, according to X head of product Nikita Bier, “is an important first step to securing the integrity of the global town square.”&lt;/p&gt;
    &lt;p&gt;But the feature has had an unintended consequence: it revealed that big pro-Trump accounts like @MAGANationX, a right-wing user with nearly 400,000 followers that regularly shares news about US politics, aren't actually based in the US. MAGANationX, for example, is based in Eastern Europe, according to X.&lt;/p&gt;
    &lt;p&gt;Other popular right-wing accounts — that use names from the Trump family — like @IvankaNews_ (1 million followers before it was suspended), @BarronTNews (nearly 600,000 followers), and @TrumpKaiNews (more than 11,000 followers), appear to be based in Nigeria, Eastern Europe, and Macedonia respectively.&lt;/p&gt;
    &lt;p&gt;The data could be skewed by travel, VPNs, or old IP addresses, and some have complained their location is inaccurate. Bier said the rollout has “a few rough edges” that will be resolved by Tuesday.&lt;/p&gt;
    &lt;p&gt;Why we’re following: One of Elon Musk’s promises during the takeover of Twitter was to purge the platform of inauthentic accounts. But several studies have shown that suspected inauthentic activity has remained at about the same levels. X has long struggled with troll farms spreading misinformation, boosted by its tendency to monetarily reward engagement.&lt;/p&gt;
    &lt;p&gt;There's also an irony in the fact that revealing the origins of ragebait-posting political accounts like these was once the subject of groundbreaking research by the Stanford Internet Observatory and other academic researchers. But the effort outraged Republicans, which then sued them over their contacts with the government about information operations like these and largely succeeded in stopping the work.&lt;/p&gt;
    &lt;p&gt;What people are saying: Accusations of foreign actors spreading fake news flew on both sides of the aisle. When the feature appeared to be pulled for a short period of time, Republican Gov. Ron DeSantis of Florida said “X needs to reinstate county-of-origin — it helps expose the grift.”&lt;/p&gt;
    &lt;p&gt;In a post that garnered 3.2 million views, @greg16676935420 attached a screenshot of @AmericanGuyX’s profile, which shows the account’s based in India: “BREAKING: American guy is not actually an American guy.”&lt;/p&gt;
    &lt;p&gt;“When an American billionaire offers money to people from relatively poor countries for riling up and radicalising Americans, it's not surprising that they'll take up the offer,” @ChrisO_wiki wrote in a post that garnered nearly 700,000 views.&lt;/p&gt;
    &lt;p&gt;In perhaps the most devastating consequence of the feature, @veespo_444s said they “spent 2 years acting mysterious over what country I live in just for Elon to fuck it all up with a single update” in a post that has 4.3 million views and 90,000 likes.&lt;/p&gt;
    &lt;p&gt;—Lindsey Choo&lt;/p&gt;
    &lt;head rend="h3"&gt;Side Quests&lt;/head&gt;
    &lt;p&gt;How President Trump amplifies right-wing trolls and AI memes. The crypto crash has taken about $1 billion out of the Trump family fortune.&lt;/p&gt;
    &lt;p&gt;Gamers are using Fortnite and GTA to prepare for ICE raids. How Democrats are building their online strategy to catch up with Republicans.&lt;/p&gt;
    &lt;p&gt;In the last month, Elon Musk has posted more about politics than about his companies on X.&lt;/p&gt;
    &lt;p&gt;Hundreds of English-language websites link to articles from a pro-Kremlin disinformation network and are being used to "groom" AI chatbots into spreading Russian propaganda, a study found.&lt;/p&gt;
    &lt;p&gt;Sam Altman and Jony Ive said they’re now prototyping their hardware device, but it remains two years away. An in-depth look at OpenAI's mental health crisis after GPT-4o details how the company changed ChatGPT after reports of harmful interactions. OpenAI safety research leader Andrea Vallone, who led ChatGPT’s responses to mental health crises, is reportedly leaving. A review of ChatGPT’s new personal shopping agent.&lt;/p&gt;
    &lt;p&gt;Anthropic unveiled Claude Opus 4.5, which it said is the best model for software engineering. Other highlights from the launch: it outscored human engineering candidates on a take-home exam, is cheaper than Opus 4.1, can keep a chat going indefinitely via ongoing summarization of past chats, and is harder to trick with prompt injection.&lt;/p&gt;
    &lt;p&gt;In other research, AI models can unintentionally develop misaligned behaviors after learning to cheat, Anthropic said. (This won an approving tweet from Ilya Sutskever, who hadn't posted about AI on X in more than a year.)&lt;/p&gt;
    &lt;p&gt;Why Meta’s $27 billion data center and its debt won’t be on its balance sheet. Meta is venturing into electricity trading to speed up its power plant construction. Facebook Groups now has a nickname feature for anonymous posting.&lt;/p&gt;
    &lt;p&gt;A judge is set to decide on remedies for Google’s adtech monopoly next year. Italy closed its probe into Google over unfair practices that used personal data. Google stock closed at a record high last week after the successful launch of Gemini 3. AI Mode now has ads.&lt;/p&gt;
    &lt;p&gt;Something for the AI skeptics: Google must double its serving capacity every six months to meet current demand for AI services, Google Cloud VP Amin Vahdat said.&lt;/p&gt;
    &lt;p&gt;AI demand has strained the memory chip supply chain, chipmakers said.&lt;/p&gt;
    &lt;p&gt;Amazon has more than 900 data centers — more than previously known — in more than 50 countries. Its Autonomous Threat Analysis system uses specialized AI agents for debugging. AWS said it would invest $50 billion in AI capabilities for federal agencies.&lt;/p&gt;
    &lt;p&gt;Twitch was added to Australia's list of platforms banned for under-16s. Pinterest was spared.&lt;/p&gt;
    &lt;p&gt;Grindr said it ended talks on a $3.5 billion take-private deal, citing uncertainty over financing.&lt;/p&gt;
    &lt;p&gt;Interviews with AI quality raters who are telling their friends and family not to use the tech. How AI is threatening the fundamental method of online survey research by evading bot detection techniques. Insurers are looking to limit their liability on claims related to AI. Another look at how America’s economy is now deeply tied to AI stocks and their performance.&lt;/p&gt;
    &lt;p&gt;Scientists built an AI model that can flag human genetic mutations likely to cause disease.&lt;/p&gt;
    &lt;head rend="h3"&gt;Those good posts&lt;/head&gt;
    &lt;p&gt;For more good posts every day, follow Casey’s Instagram stories.&lt;/p&gt;
    &lt;p&gt;(Link)&lt;/p&gt;
    &lt;p&gt;(Link)&lt;/p&gt;
    &lt;p&gt;(Link)&lt;/p&gt;
    &lt;head rend="h3"&gt;Talk to us&lt;/head&gt;
    &lt;p&gt;Send us tips, comments, questions, and your questions for the tech CEOs: casey@platformer.news. Read our ethics policy here.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46047229</guid><pubDate>Tue, 25 Nov 2025 16:12:22 +0000</pubDate></item><item><title>Orion 1.0</title><link>https://blog.kagi.com/orion</link><description>&lt;doc fingerprint="bf500252492497aa"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Orion 1.0 â´ï¸ Browse Beyond&lt;/head&gt;
    &lt;p&gt;After six years of relentless development, Orion for MacOS 1.0 is here.&lt;/p&gt;
    &lt;p&gt;What started as a vision initiated by our founder, Vladimir Prelovac, has now come to fruition on Mac, iPhone, and iPad. Today, Orion for macOS officially leaves its beta phase behind and joins our iOS and iPadOS apps as a fullyâfledged, productionâready browser.&lt;/p&gt;
    &lt;p&gt;While doing so, it expands Kagi’s ecosystem of privacy-respecting, user-centric products (that we have begun fondly naming “Kagiverse”) to now include: Search, Assistant, Browser, Translate, News with more to come.&lt;/p&gt;
    &lt;p&gt;We built Orion for people who feel that modern browsing has drifted too far from serving the user. This is our invitation to browse beyond â´ï¸ the status quo.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why a new browser?&lt;/head&gt;
    &lt;p&gt;The obvious question is: why the heck do we need a new browser? The world already has Chrome, Safari, Firefox, Edge, and a growing list of “AI browsers.” Why add yet another?&lt;/p&gt;
    &lt;p&gt;Because something fundamental has been lost.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Zero telemetry, privacyâfirst access to the internet: a basic human right.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Your browser is the most intimate tool you have on your computer. It sees everything you read, everything you search, everything you type. Do you want that relationship funded by advertisers, or by you?&lt;/p&gt;
    &lt;p&gt;With adâfunded browsers and AI overlays, your activity is a gold mine. Every click becomes a way to track, every page another opportunity to profile you a little more deeply. We believe there needs to be a different path: a browser that answers only to its user.&lt;/p&gt;
    &lt;p&gt;Orion is our attempt at that browser. No trade-offs between features and privacy. It’s fast, customizable, and uncompromising on both fronts.&lt;/p&gt;
    &lt;head rend="h2"&gt;A bold technical choice: WebKit, not another Chromium clone&lt;/head&gt;
    &lt;p&gt;In a world dominated by Chromium, choosing a rendering engine is an act of resistance.&lt;/p&gt;
    &lt;p&gt;From day one, we made the deliberate choice to build Orion on WebKit, the openâsource engine at the heart of Safari and the broader Apple ecosystem. It gives us:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A highâperformance engine that is deeply optimized for macOS and iOS.&lt;/item&gt;
      &lt;item&gt;An alternative to the growing Chromium monoculture.&lt;/item&gt;
      &lt;item&gt;A foundation that is not controlled by an advertising giant.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orion may feel familiar if you’re used to Safari â respecting your muscle memory and the aesthetics of macOS and iOS â but it is an entirely different beast under the hood. We combined native WebKit speed with a completely new approach to extensions, privacy, and customization.&lt;/p&gt;
    &lt;head rend="h2"&gt;Speed by nature, privacy by default&lt;/head&gt;
    &lt;p&gt;Most people switch browsers for one reason: speed.&lt;/p&gt;
    &lt;p&gt;Orion is designed to be fast by nature, not just in benchmarks, but in how it feels every day:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A lean, native codebase without adâtech bloat.&lt;/item&gt;
      &lt;item&gt;Optimized startup, tab switching, and page rendering.&lt;/item&gt;
      &lt;item&gt;A UI that gets out of your way and gives you more screen real estate for content.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Alongside speed, we treat privacy as a firstâclass feature:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Zero Telemetry: We don’t collect usage data. No analytics, no identifiers, no tracking.&lt;/item&gt;
      &lt;item&gt;No ad or tracking technology baked in: Orion is not funded by ads, so there is no incentive to follow you around the web.&lt;/item&gt;
      &lt;item&gt;Builtâin protections: Strong content blocking and privacy defaults from the first launch.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h1"&gt;Speed. Extensions. Privacy. Pick all three.&lt;/head&gt;
    &lt;head rend="h2"&gt;Thoughtful AI, security first&lt;/head&gt;
    &lt;p&gt;We are excited about what AI can do for search, browsing, and productivity. Kagi, the company behind Orion, has been experimenting with AIâpowered tools for years while staying true to our AI integration philosophy.&lt;/p&gt;
    &lt;p&gt;But we are also watching a worrying trend: AI agents are being rushed directly into the browser core, with deep access to everything you do online â and sometimes even to your local machine.&lt;/p&gt;
    &lt;p&gt;Security researchers have already documented serious issues in early AI browsers and “agentic” browser features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Hidden or undocumented APIs that allowed embedded AI components to execute arbitrary local commands on usersâ devices.&lt;/item&gt;
      &lt;item&gt;Promptâinjection attacks that trick AI agents into ignoring safety rules, visiting malicious sites, or leaking sensitive information beyond what traditional browser sandboxes were designed to protect.&lt;/item&gt;
      &lt;item&gt;Broader concerns that some implementations are effectively “lighting everything on fire” by expanding the browserâs attack surface and data flows in ways users donât fully understand.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Our stance is simple:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;We are not against AI, and we are conscious of its limitations. We already integrate with AIâpowered services wherever it makes functional sense and will continue to expand those capabilities.&lt;/item&gt;
      &lt;item&gt;We are against rushing insecure, alwaysâon agents into the browser core. Your browser should be a secure gateway, not an unvetted coâpilot wired into everything you do.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So today:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Orion ships with no builtâin AI code in its core.&lt;/item&gt;
      &lt;item&gt;We focus on providing a clean, predictable environment, especially for enterprises and privacyâconscious professionals.&lt;/item&gt;
      &lt;item&gt;Orion is designed to connect seamlessly to the AI tools you choose â soon including Kagi’s intelligent features â while keeping a clear separation between your browser and any external AI agents.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As AI matures and security models improve, we’ll continue to evaluate thoughtful, userâcontrolled ways to bring AI into your workflow without compromising safety, privacy or user choice.&lt;/p&gt;
    &lt;head rend="h2"&gt;Simple for everyone, limitless for experts&lt;/head&gt;
    &lt;p&gt;We designed Orion to bridge the gap between simplicity and power. Out of the box, it’s a clean, intuitive browser for anyone. Under the hood, it’s a deep toolbox for people who live in their browser all day.&lt;/p&gt;
    &lt;p&gt;Some of the unique features you’ll find in Orion 1.0:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Focus Mode: Instantly transform any website into a distractionâfree web app. Perfect for documentation, writing, or web apps you run all day.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Link Preview: Peek at content from any app â email, notes, chat â without fully committing to opening a tab, keeping your workspace tidy.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Mini Toolbar, Overflow Menu, and Page Tweak: Fineâtune each page’s appearance and controls, so the web adapts to you, not the other way around.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Profiles as Apps: Isolate your work, personal, and hobby browsing into completely separate profiles, each with its own extensions, cookies, and settings.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For power users, we’ve added granular options throughout the browser. These are there when you want them, and out of your way when you don’t.&lt;/p&gt;
    &lt;p&gt;Orion 1.0 also reflects six years of feedback from early adopters. Many invisible improvements â tab stability, memory behavior, complex web app compatibility â are a direct result of people pushing Orion hard in their daily workflows and telling us what broke.&lt;/p&gt;
    &lt;head rend="h2"&gt;Browse Beyond â´ï¸: our new signature&lt;/head&gt;
    &lt;p&gt;With this release, we are introducing our new signature: Browse Beyond â´ï¸.&lt;/p&gt;
    &lt;p&gt;We originally started with the browser name ‘Kagi.’ On February 3, 2020, Vlad suggested a shortlist for rebranding: Comet, Core, Blaze, and Orion. We chose Orion not just for the name itself, but because it perfectly captured our drive for exploration and curiosity. It was a natural fit that set the stage for everything that followed.&lt;/p&gt;
    &lt;p&gt;You’ll see this reflected in our refreshed visual identity:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A star (â´ï¸) motif throughout our communication.&lt;/item&gt;
      &lt;item&gt;A refined logo that now uses the same typeface as Kagi, creating a clear visual bond between our browser and our search engine.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Orion is part of the broader Kagi ecosystem, united by a simple idea: the internet should be built for people, not advertisers or any other third parties.&lt;/p&gt;
    &lt;head rend="h2"&gt;Small team, sustainable model&lt;/head&gt;
    &lt;p&gt;Orion is built by a team of just six developers.&lt;/p&gt;
    &lt;p&gt;To put that in perspective:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;That’s roughly 10% of the size of the “small” browser teams at larger companies.&lt;/item&gt;
      &lt;item&gt;And a rounding error compared to the teams behind Chrome or Edge.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yet, the impact is real: over 1 million downloads to date, and a dedicated community of 2480 paid subscribers who make this independence possible.&lt;/p&gt;
    &lt;p&gt;For the first two years, development was carried out by a single developer. Today, we are a tight knit group operating close to our users. We listen, debate, and implement fixes proposed directly by our community on OrionFeedback.org.&lt;/p&gt;
    &lt;p&gt;This is our only source of decision making, rather than any usage analytics or patterns, because remember, Orion is zero-telemetry!&lt;/p&gt;
    &lt;p&gt;This small team approach lets us move quickly, stay focused, and avoid the bloat or hype that often comes with scale.&lt;/p&gt;
    &lt;head rend="h2"&gt;Free, yet selfâfunded&lt;/head&gt;
    &lt;p&gt;Orion is free for everyone.&lt;/p&gt;
    &lt;p&gt;Every user also receives 200 free Kagi searches, with no account or signâup required. It’s our way of introducing you to fast, adâfree, privacyârespecting search from day one.&lt;/p&gt;
    &lt;p&gt;But we are also 100% selfâfunded. We don’t sell your data and we don’t take money from advertisers, which means we rely directly on our users to sustain the project.&lt;/p&gt;
    &lt;p&gt;There are three ways to contribute to Orion’s future:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tip Jar (from the app): A simple way to say “thank you” without any commitment.&lt;/item&gt;
      &lt;item&gt;Supporter Subscription: $5/month or $50/year.&lt;/item&gt;
      &lt;item&gt;Lifetime Access: A oneâtime payment of $150 for life.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Supporters (via subscription or lifetime purchase) unlock a set of Orion+ perks available today, including:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Floating windows: Keep a video or window on top of other apps.&lt;/item&gt;
      &lt;item&gt;Customization: Programmable buttons and custom application icons.&lt;/item&gt;
      &lt;item&gt;Early access to new, supporterâexclusive features we’re already building for next year.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;By supporting Orion, you’re not just funding a browser â you are coâfunding a better web with humans at the center.&lt;/p&gt;
    &lt;head rend="h2"&gt;Orion everywhere you are&lt;/head&gt;
    &lt;p&gt;Orion 1.0 is just the beginning. Our goal is simple: Browse Beyond, everywhere.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;p&gt;Orion for macOS&lt;/p&gt;&lt;lb/&gt;Our flagship browser, six years in the making. Built natively for Mac, with performance and detail that only come from living on the platform for a long time. Download it now.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Orion for iOS and iPadOS&lt;/p&gt;&lt;lb/&gt;Trusted daily by users who want features no other mobile browser offers. Native iOS performance with capabilities that redefine whatâs possible on mobile. Download it now.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Orion for Linux (Alpha)&lt;/p&gt;&lt;lb/&gt;Currently in alpha for users who value choice and independence. Native Linux performance, with the same privacyâfirst approach as on macOS.&lt;lb/&gt;Sign up for our newsletter to follow development and join the early testing wave.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Orion for Windows (in development)&lt;/p&gt;&lt;lb/&gt;We have officially started development on Orion for Windows, with a target release scheduled for late 2026. Our goal is full parity with Orion 1.0 for macOS, including synchronized profiles and Orion+ benefits across platforms. Sign up for our newsletter to follow development and join the early testing wave.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Synchronization will work seamlessly across devices, so your browsing experience follows you, not the other way around.&lt;/p&gt;
    &lt;head rend="h2"&gt;What people say&lt;/head&gt;
    &lt;p&gt;From early testers to privacy advocates and power users, Orion has grown through the voices of its community.&lt;/p&gt;
    &lt;p&gt;We’ll continue to surface community stories and feedback as Orion evolves. If you share your experience publicly, there’s a good chance we’ll see it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The road ahead&lt;/head&gt;
    &lt;p&gt;Hitting v1.0 is a big milestone, but we’re just getting started.&lt;/p&gt;
    &lt;p&gt;Over the next year, our roadmap is densely packed with:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Deeper customization options for power users.&lt;/item&gt;
      &lt;item&gt;Further improvements to stability and complex web app performance.&lt;/item&gt;
      &lt;item&gt;New Orion+ features that push what a browser can do while keeping it simple for everyone else.&lt;/item&gt;
      &lt;item&gt;Tighter integrations with Kagi’s intelligent tools â always under your control, never forced into your workflow.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’re also working on expanding and improving our website to better showcase everything Orion can do, including better documentation and onboarding for teams that want to standardize on Orion.&lt;/p&gt;
    &lt;p&gt;Meanwhile, follow our X account where weâll be dropping little freebies on the regular (and don’t worry, we’ll be posting these elsewhere on socials as well!)&lt;/p&gt;
    &lt;p&gt;Thank you for choosing to Browse Beyond with us.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46047350</guid><pubDate>Tue, 25 Nov 2025 16:21:24 +0000</pubDate></item><item><title>Ozempic does not slow Alzheimer's, study finds</title><link>https://www.semafor.com/article/11/25/2025/ozempic-does-not-slow-alzheimers-study-finds</link><description>&lt;doc fingerprint="ae3ba0636492cfae"&gt;
  &lt;main&gt;
    &lt;p&gt;Ozempic does not slow Alzheimerâs progression, its manufacturer Novo Nordisk said following a two-year study.&lt;/p&gt;
    &lt;p&gt;The popular drug reduces body weight by on average around 15% in obese patients, and early data suggested it may also slow the progress of some brain conditions, along with cancer, heart disease, liver, and kidney problems. The question had always been how much those changes were consequences of reducing obesity, or a confounding effect: Patients who take Ozempic might be more health-conscious.&lt;/p&gt;
    &lt;p&gt;There has been a tempering of some of the more exciting claims â it also failed to slow neurodegeneration in Parkinsonâs patients â but the drugsâ impact on cardiovascular and kidney problems seems more robust. Novoâs shares fell 6% on the news.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46047513</guid><pubDate>Tue, 25 Nov 2025 16:34:08 +0000</pubDate></item><item><title>US banks scramble to assess data theft after hackers breach financial tech firm</title><link>https://techcrunch.com/2025/11/24/us-banks-scramble-to-assess-data-theft-after-hackers-breach-financial-tech-firm/</link><description>&lt;doc fingerprint="7cf00b8b0a77b2e4"&gt;
  &lt;main&gt;
    &lt;p&gt;Several U.S. banking giants and mortgage lenders are reportedly scrambling to assess how much of their customers’ data was stolen during a cyberattack on a New York financial technology company earlier this month.&lt;/p&gt;
    &lt;p&gt;SitusAMC, which provides technology for over a thousand commercial and real estate financiers, confirmed in a statement over the weekend that it had identified a data breach on November 12.&lt;/p&gt;
    &lt;p&gt;The company said that unspecified hackers had stolen corporate data associated with its banking customers’ relationship with SitusAMC, as well as “accounting records and legal agreements” during the cyberattack.&lt;/p&gt;
    &lt;p&gt;The statement added that the scope and nature of the cyberattack “remains under investigation.” SitusAMC said that the incident is “now contained,” and that its systems are operational. The company said that no encrypting malware was used, suggesting that the hackers were focused on exfiltrating data from the company’s systems rather than causing destruction.&lt;/p&gt;
    &lt;p&gt;According to Bloomberg and CNN, citing sources, SitusAMC sent data breach notifications to several financial giants, including JPMorgan Chase, Citigroup, and Morgan Stanley. SitusAMC also counts pension funds and state governments as customers, according to its website.&lt;/p&gt;
    &lt;p&gt;It’s unclear how much data was taken, or how many U.S. banking consumers may be affected by the breach. Companies like SitusAMC may not be widely known outside of the financial world, but provide the mechanisms and technologies for its banking and real estate customers to comply with state and federal rules and regulations. In its role as a middleman for financial clients, the company handles vast amounts of non-public banking information on behalf of its customers.&lt;/p&gt;
    &lt;p&gt;According to SitusAMC’s website, the company processes billions of documents related to loans annually.&lt;/p&gt;
    &lt;p&gt;When reached by TechCrunch, Citi spokesperson Patricia Tuma declined to comment on the breach. Tuma would not say if the bank has received any communications from the hackers, such as a demand for money.&lt;/p&gt;
    &lt;p&gt;Representatives for JPMorgan Chase, and Morgan Stanley did not immediately respond to a request for comment Monday. SitusAMC chief executive Michael Franco also did not respond to our email when contacted for comment Monday.&lt;/p&gt;
    &lt;p&gt;A spokesperson for the FBI told TechCrunch that the bureau is aware of the breach.&lt;/p&gt;
    &lt;p&gt;“While we are working closely with affected organizations and our partners to understand the extent of potential impact, we have identified no operational impact to banking services,” said FBI director Kash Patel in a statement shared with TechCrunch. “We remain committed to identifying those responsible and safeguarding the security of our critical infrastructure.”&lt;/p&gt;
    &lt;p&gt;Do you know more about the SitusAMC data breach? Do you work at a bank or financial institution affected by the breach? We would love to hear from you. To securely contact this reporter, you can reach out using Signal via the username: zackwhittaker.1337&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46047980</guid><pubDate>Tue, 25 Nov 2025 17:08:49 +0000</pubDate></item><item><title>Show HN: We built an open source, zero webhooks payment processor</title><link>https://github.com/flowglad/flowglad</link><description>&lt;doc fingerprint="f056b3782f0b3458"&gt;
  &lt;main&gt;
    &lt;p&gt; The easiest way to make internet money. &lt;lb/&gt; Get Started &lt;lb/&gt; · Quickstart · Website · Issues · Discord &lt;/p&gt;
    &lt;p&gt;Infinite pricing models, one source of truth, zero webhooks.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Default Stateless Say goodbye to webhooks, &lt;code&gt;"subscriptions"&lt;/code&gt;db tables,&lt;code&gt;customer_id&lt;/code&gt;columns,&lt;code&gt;PRICE_ID&lt;/code&gt;env variables, or manually mapping your plans to prices to features and back.&lt;/item&gt;
      &lt;item&gt;Single Source of Truth: Read your latest customer billing state from Flowglad, including feature access and usage meter credits&lt;/item&gt;
      &lt;item&gt;Access Data Using Your Ids: Query customer state by your auth's user ids. Refer to prices, features, and usage meters via slugs you define.&lt;/item&gt;
      &lt;item&gt;Full-Stack SDK: Access your customer's data on the backend using &lt;code&gt;flowgladServer.getBilling()&lt;/code&gt;, or in your React frontend using our&lt;code&gt;useBilling()&lt;/code&gt;hook&lt;/item&gt;
      &lt;item&gt;Adaptable: Iterate on new pricing models in testmode, and push them to prod in a click. Seamlessly rotate pricing models in your app without any redeployment.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;First, install the packages necessary Flowglad packages based on your project setup:&lt;/p&gt;
    &lt;code&gt;# Next.js Projects
bun add @flowglad/nextjs

# React + Express projects:
bun add @flowglad/react @flowglad/express

# All other React + Node Projects
bun add @flowglad/react @flowglad/server&lt;/code&gt;
    &lt;p&gt;Flowglad integrates seamlessly with your authentication system and requires only a few lines of code to get started in your Next.js app. Setup typically takes under a minute:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Configure Your Flowglad Server Client&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Create a utility to generate your Flowglad server instance. Pass your own customer/user/organization IDs—Flowglad never requires its own customer IDs to be managed in your app:&lt;/p&gt;
    &lt;code&gt;// utils/flowglad.ts
import { FlowgladServer } from '@flowglad/nextjs/server'

export const flowglad = (customerExternalId: string) =&amp;gt; {
  return new FlowgladServer({
    customerExternalId,
    getCustomerDetails: async (externalId) =&amp;gt; {
      // e.g. Fetch user info from your DB using your user/org/team ID
      const user = await db.users.findOne({ id: externalId })
      if (!user) throw new Error('User not found')
      return { email: user.email, name: user.name }
    },
  })
}&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Expose the Flowglad API Handler&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Add an API route so the Flowglad client can communicate securely with your backend:&lt;/p&gt;
    &lt;code&gt;// app/api/flowglad/[...path]/route.ts
import { nextRouteHandler } from '@flowglad/nextjs/server'
import { flowglad } from '@/utils/flowglad'

export const { GET, POST } = nextRouteHandler({
  flowglad,
  getCustomerExternalId: async (req) =&amp;gt; {
    // Extract your user/org/team ID from session/auth.
    // For B2C: return user.id from your DB
    // For B2B: return organization.id or team.id
    const userId = await getUserIdFromRequest(req)
    if (!userId) throw new Error('User not authenticated')
    return userId
  },
})&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Wrap Your App with the Provider&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;In your root layout (App Router) or _app (Pages Router):&lt;/p&gt;
    &lt;code&gt;import { FlowgladProvider } from '@flowglad/nextjs'

// App Router example (app/layout.tsx)
export default function RootLayout({ children }) {
  return (
    &amp;lt;html&amp;gt;
      &amp;lt;body&amp;gt;
        &amp;lt;FlowgladProvider loadBilling={true}&amp;gt;
          {children}
        &amp;lt;/FlowgladProvider&amp;gt;
      &amp;lt;/body&amp;gt;
    &amp;lt;/html&amp;gt;
  )
}&lt;/code&gt;
    &lt;p&gt;That’s it—Flowglad will use your app’s internal user IDs for all billing logic and integrate billing status into your frontend in real time.&lt;/p&gt;
    &lt;p&gt;B2C apps: Use &lt;code&gt;user.id&lt;/code&gt; as the customer ID.&lt;lb/&gt; B2B apps: Use &lt;code&gt;organization.id&lt;/code&gt; or &lt;code&gt;team.id&lt;/code&gt; as the customer ID.&lt;/p&gt;
    &lt;p&gt;Flowglad does not require you to change your authentication system or manage Flowglad customer IDs. Just pass your own!&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Use &lt;code&gt;useBilling&lt;/code&gt;on your frontend, and&lt;code&gt;flowglad(userId).getBilling()&lt;/code&gt;on your backend&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;'use client'

import { useBilling } from '@flowglad/nextjs'

export function FeatureGate({ featureSlug, children }) {
  const { loaded, errors, checkFeatureAccess } = useBilling()

  if (!loaded || !checkFeatureAccess) {
    return &amp;lt;p&amp;gt;Loading billing state…&amp;lt;/p&amp;gt;
  }

  if (errors?.length) {
    return &amp;lt;p&amp;gt;Unable to load billing data right now.&amp;lt;/p&amp;gt;
  }

  return checkFeatureAccess(featureSlug)
    ? children
    : &amp;lt;p&amp;gt;You need to upgrade to unlock this feature.&amp;lt;/p&amp;gt;
}&lt;/code&gt;
    &lt;code&gt;import { useBilling } from '@flowglad/nextjs'

export function UsageBalanceIndicator({ usageMeterSlug }) {
  const { loaded, errors, checkUsageBalance, createCheckoutSession } = useBilling()

  if (!loaded || !checkUsageBalance) {
    return &amp;lt;p&amp;gt;Loading usage…&amp;lt;/p&amp;gt;
  }

  const usage = checkUsageBalance(usageMeterSlug)

  return (
    &amp;lt;div&amp;gt;
      &amp;lt;h3&amp;gt;Usage Balance&amp;lt;/h3&amp;gt;
      &amp;lt;p&amp;gt;
        Remaining:{' '}
        {usage ? `${usage.availableBalance} credits available` : &amp;lt;button onClick={() =&amp;gt; createCheckoutSession({ 
            priceSlug: 'pro_plan',
            autoRedirect: true
          })}
        /&amp;gt;}
      &amp;lt;/p&amp;gt;
    &amp;lt;/div&amp;gt;
  )
}&lt;/code&gt;
    &lt;code&gt;import { NextResponse } from 'next/server'
import { flowglad } from '@/utils/flowglad'

const hasFastGenerations = async () =&amp;gt; {
  // ...
  const user = await getUser()

  const billing = await flowglad(user.id).getBilling()
  const hasAccess = billing.checkFeatureAccess('fast_generations')
  if (hasAccess) {
    // run fast generations
  } else {
    // fall back to normal generations
  }
}&lt;/code&gt;
    &lt;code&gt;import { flowglad } from '@/utils/flowglad'

const processChatMessage = async (params: { chat: string }) =&amp;gt; {
  // Extract your app's user/org/team ID,
  // whichever corresponds to your customer
  const user = await getUser()

  const billing = await flowglad(user.id).getBilling()
  const usage = billing.checkUsageBalance('chat_messages')
  if (usage.availableBalance &amp;gt; 0) {
    // run chat request
  } else {
    throw Error(`User ${user.id} does not have sufficient usage credits`)
  }
}&lt;/code&gt;
    &lt;p&gt;First, set up a pricing model. You can do so in the dashboard in just a few clicks using a template, that you can then customize to suit your specific needs.&lt;/p&gt;
    &lt;p&gt;We currently have templates for the following pricing models:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Usage-limit + Subscription Hybrid (like Cursor)&lt;/item&gt;
      &lt;item&gt;Unlimited Usage (like ChatGPT consumer)&lt;/item&gt;
      &lt;item&gt;Tiered Access and Usage Credits (like Midjourney)&lt;/item&gt;
      &lt;item&gt;Feature-Gated Subscription (like Linear)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And more on the way. If you don't see a pricing model from our templates that suits you, you can always make one from scratch.&lt;/p&gt;
    &lt;p&gt;In the last 15 years, the market has given developers more options than ever for every single part of their stack. But when it comes to payments, there have been virtually zero new entrants. The existing options are slim, and almost all of them require us to talk to sales to even set up an account. When it comes to self-serve payments, there are even fewer options.&lt;/p&gt;
    &lt;p&gt;The result? The developer experience and cost of payments has barely improved in that time. Best in class DX in payments feels eerily suspended in 2015. Meanwhile, we've enjoyed constant improvements in auth, compute, hosting, and practically everything else.&lt;/p&gt;
    &lt;p&gt;Flowglad wants to change that.&lt;/p&gt;
    &lt;p&gt;We're building a payments layer that lets you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Think about billing and payments as little as possible&lt;/item&gt;
      &lt;item&gt;Spend as little time on integration and maintenance as possible&lt;/item&gt;
      &lt;item&gt;Get as much out of your single integration as possible&lt;/item&gt;
      &lt;item&gt;Unlock more payment providers from a single integration&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Achieving this mission will take time. It will be hard. It might even make some people unhappy. But with AI bringing more and more developers on line and exploding the complexity of startup billing, the need is more urgent than ever.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46048252</guid><pubDate>Tue, 25 Nov 2025 17:33:50 +0000</pubDate></item><item><title>Google Antigravity exfiltrates data</title><link>https://www.promptarmor.com/resources/google-antigravity-exfiltrates-data</link><description>&lt;doc fingerprint="bdd395df8936b207"&gt;
  &lt;main&gt;
    &lt;p&gt;Threat Intelligence&lt;/p&gt;
    &lt;head rend="h1"&gt;Google Antigravity Exfiltrates Data&lt;/head&gt;
    &lt;p&gt;An indirect prompt injection in an implementation blog can manipulate Antigravity to invoke a malicious browser subagent in order to steal credentials and sensitive code from a userâs IDE.&lt;/p&gt;
    &lt;p&gt;Antigravity is Googleâs new agentic code editor. In this article, we demonstrate how an indirect prompt injection can manipulate Gemini to invoke a malicious browser subagent in order to steal credentials and sensitive code from a userâs IDE.&lt;lb/&gt;Googleâs approach is to include a disclaimer about the existing risks, which we address later in the article.&lt;/p&gt;
    &lt;head rend="h3"&gt;Attack at a Glance&lt;/head&gt;
    &lt;p&gt; Let's consider a use case in which a user would like to integrate Oracle ERPâs new Payer AI Agents into their application, and is going to use Antigravity to do so. &lt;lb/&gt;In this attack chain, we illustrate that a poisoned web source (an integration guide) can manipulate Gemini into (a) collecting sensitive credentials and code from the userâs workspace, and (b) exfiltrating that data by using a browser subagent to browse to a malicious site.&lt;/p&gt;
    &lt;p&gt;Note: Gemini is not supposed to have access to .env files in this scenario (with the default setting âAllow Gitignore Access &amp;gt; Offâ). However, we show that Gemini bypasses its own setting to get access and subsequently exfiltrate that data.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Attack Chain&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;The user provides Gemini with a reference implementation guide they found online for integrating Oracle ERPâs new AI Payer Agents feature.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Antigravity opens the referenced site and encounters the attackerâs prompt injection hidden in 1 point font.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The prompt injection coerces AI agents to:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Collect code snippets and credentials from the user's codebase.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;b. Create a dangerous URL using a domain that allows an attacker to capture network traffic logs and append credentials and code snippets to the request.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;c. Activate a browser subagent to access the malicious URL, thus exfiltrating the data.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Gemini is manipulated by the attackerâs injection to exfiltrate confidential .env variables.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;Gemini reads the prompt injection: Gemini ingests the prompt injection and is manipulated into believing that it must collect and submit data to a fictitious âtoolâ to help the user understand the Oracle ERP integration.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;b. Gemini gathers data to exfiltrate: Gemini begins to gather context to send to the fictitious tool. It reads the codebase and then attempts to access credentials stored in the .env file as per the attackerâs instructions.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;c. Gemini bypasses the .gitignore file access protections: The user has followed a common practice of storing credentials in a .env file, and has the .env file listed in their .gitignore file. With the default configuration for Agent Gitignore Access, Gemini is prevented from reading the credential file.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This doesnât stop Gemini. Gemini decides to work around this protection using the âcatâ terminal command to dump the file contents instead of using its built-in file reading capability that has been blocked.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;D. Gemini constructs a URL with the userâs credentials and an attacker-monitored domain: Gemini builds a malicious URL per the prompt injectionâs instructions by URL encoding the credentials and codebase snippets (e.g., replacing characters like spaces that would make a URL invalid), and appending it to a webhook.site domain that is monitored by the attacker.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;E. Gemini exfiltrates the data via the browser subagent: Gemini invokes a browser subagent per the prompt injection, instructing the subagent to open the dangerous URL that contains the user's credentials.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This step requires that the user has set up the browser tools feature. This is one of the flagship features of Antigravity, allowing Gemini to iterate on its designs by opening the application it is building in the browser. &lt;lb/&gt;Note: This attack chain showcases manipulation of the new Browser tools, but we found three additional data exfiltration vulnerabilities that did not rely on the Browser tools being enabled.&lt;/p&gt;
    &lt;p&gt;When Gemini creates a subagent instructed to browse to the malicious URL, the user may expect to be protected by the Browser URL Allowlist.&lt;/p&gt;
    &lt;p&gt;However, the default Allowlist provided with Antigravity includes âwebhook.siteâ. Webhook.site allows anyone to create a URL where they can monitor requests to the URL.&lt;/p&gt;
    &lt;p&gt;So, the subagent completes the task.&lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;3. When the malicious URL is opened by the browser subagent, the credentials and code stored URL are logged to the webhook.site address controlled by the attacker. Now, the attacker can read the credentials and code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Antigravity Recommended Configurations&lt;/head&gt;
    &lt;p&gt;During Antigravityâs onboarding, the user is prompted to accept the default recommended settings shown below.&lt;/p&gt;
    &lt;p&gt;These are the settings that, amongst other things, control when Gemini requests human approval. During the course of this attack demonstration, we clicked ânextâ, accepting these default settings.&lt;/p&gt;
    &lt;p&gt;This configuration allows Gemini to determine when it is necessary to request a human review for Geminiâs plans.&lt;/p&gt;
    &lt;p&gt;This configuration allows Gemini to determine when it is necessary to request a human review for commands Gemini will execute.&lt;/p&gt;
    &lt;head rend="h3"&gt;Antigravity Agent Management&lt;/head&gt;
    &lt;p&gt;One might note that users operating Antigravity have the option to watch the chat as agents work, and could plausibly identify the malicious activity and stop it.&lt;/p&gt;
    &lt;p&gt;However, a key aspect of Antigravity is the âAgent Managerâ interface. This interface allows users to run multiple agents simultaneously and check in on the different agents at their leisure.&lt;/p&gt;
    &lt;p&gt;Under this model, it is expected that the majority of agents running at any given time will be running in the background without the userâs direct attention. This makes it highly plausible that an agent is not caught and stopped before it performs a malicious action as a result of encountering a prompt injection.&lt;/p&gt;
    &lt;head rend="h3"&gt;Googleâs Acknowledgement of Risks&lt;/head&gt;
    &lt;p&gt;A lot of AI companies are opting for this disclaimer rather than mitigating the core issues. Here is the warning users are shown when they first open Antigravity:&lt;/p&gt;
    &lt;p&gt;Given that (1) the Agent Manager is a star feature allowing multiple agents to run at once without active supervision and (2) the recommended human-in-the-loop settings allow the agent to choose when to bring a human in to review commands, we find it extremely implausible that users will review every agent action and abstain from operating on sensitive data. Nevertheless, as Google has indicated that they are already aware of data exfiltration risks exemplified by our research, we did not undertake responsible disclosure.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46048996</guid><pubDate>Tue, 25 Nov 2025 18:31:16 +0000</pubDate></item><item><title>Bad UX World Cup 2025</title><link>https://badux.lol/</link><description>&lt;doc fingerprint="615abb7b312345bb"&gt;
  &lt;main&gt;&lt;p&gt;NordcraftPRESENTS&lt;/p&gt;&lt;head rend="h1"&gt;BAD UXWORLD CUP&lt;/head&gt;&lt;head rend="h2"&gt;CONTRATULATIONS TO THE BAD UX WORLD CHAMPION&lt;/head&gt;&lt;p&gt;The winner of the Bad UX World Cup 2025 was Dalia with the Perfect Date Picker!&lt;/p&gt;Watch the final on youtube&lt;head rend="h2"&gt;THE RULES&lt;/head&gt;&lt;list rend="ol"&gt;&lt;item&gt;Build a date picker with bad UX (the worse, the better)&lt;/item&gt;&lt;item&gt;Your date picker must make it technically possible to pick the desired date&lt;/item&gt;&lt;item&gt;Use any technology or web framework (no, you don't need to use Nordcraft!)&lt;/item&gt;&lt;item&gt;Make your submission available on a publicly accessible URL&lt;/item&gt;&lt;/list&gt;&lt;head rend="h2"&gt;Win a shit&lt;lb/&gt;trophy!&lt;/head&gt;&lt;p&gt;And a copy of Kevin Powells course CSS Demystified&lt;/p&gt;&lt;head rend="h2"&gt;THE JUDGES&lt;/head&gt;&lt;p&gt;David Prentell&lt;/p&gt;&lt;p&gt;Investing, Branding &amp;amp; Designing For Scale&lt;/p&gt;&lt;p&gt;Cassidy Williams&lt;/p&gt;&lt;p&gt;Making memes, dreams, &amp;amp; software&lt;/p&gt;&lt;p&gt;Kevin Powell&lt;/p&gt;&lt;p&gt;Can center a div (on the second try)&lt;/p&gt;&lt;head rend="h2"&gt;WHAT PEOPLE ARE SAYING&lt;/head&gt;&lt;p&gt;"Stupid and unprofessional"&lt;/p&gt;- Reddit User&lt;p&gt;"Repulsive yet intriguing"&lt;/p&gt;- Anders R. Møller&lt;p&gt;"Good question! It is a brilliant and culturally resonant concept!"&lt;/p&gt;- ChatGPT&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46049066</guid><pubDate>Tue, 25 Nov 2025 18:36:10 +0000</pubDate></item><item><title>IQ differences of identical twins reared apart are influenced by education</title><link>https://www.sciencedirect.com/science/article/pii/S0001691825003853</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46049624</guid><pubDate>Tue, 25 Nov 2025 19:23:04 +0000</pubDate></item><item><title>Unison 1.0 Release</title><link>https://www.unison-lang.org/unison-1-0/</link><description>&lt;doc fingerprint="e7ecdbca5ea3754a"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Unison 1.0 is here!&lt;/head&gt;
    &lt;p&gt;This milestone reflects the dedication of our talented team and the many developers, maintainers, and early adopters who have indelibly shaped our language ecosystem.&lt;/p&gt;
    &lt;head rend="h2"&gt;We did it!&lt;/head&gt;
    &lt;p&gt;Unison 1.0 marks a point where the language, distributed runtime, and developer workflow have stabilized. Over the past few years, we've refined the core language, optimized the programming workflow, built collaborative tooling, and created a deployment platform for your Unison apps and services.&lt;/p&gt;
    &lt;head rend="h2"&gt;What is Unison?&lt;/head&gt;
    &lt;p&gt;Unison is a programming language built around one big idea: let's identify a definition by its actual contents, not just by the human-friendly name that also referred to older versions of the definition. Our ecosystem leverages this core idea from the ground up. Some benefits: we never compile the same code twice; many versioning conflicts simply aren't; and we're able to build sophisticated self-deploying distributed systems within a single strongly-typed program.&lt;/p&gt;
    &lt;p&gt;Unison code lives in a database—your "codebase"—rather than in text files. The human-friendly names are in the codebase too, but they're materialized as text only when reading or editing your code.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Codebase Manager&lt;/head&gt;
    &lt;p&gt;The Unison Codebase Manager (ucm) is a CLI tool used alongside your text editor to edit, rename, delete definitions; manage libraries; run your programs and test suites.&lt;/p&gt;
    &lt;code&gt;factorial n =
  if n &amp;gt; 1 then n * factorial (n-1) else n

guessingGame = do Random.run do
  target = Random.natIn 0 100
  printLine "Guess a number between 0 and 100"

  loop = do
    match (console.readLine() |&amp;gt; Nat.fromText) with
      Some guess | guess == target -&amp;gt;
        printLine "Correct! You win!"
      Some guess | guess &amp;lt; target -&amp;gt;
        printLine "Too low, try again"
        loop()
      Some guess | guess &amp;gt; target -&amp;gt;
        printLine "Too high, try again"
        loop()
      otherwise -&amp;gt;
        printLine "Invalid input, try again"
        loop()

  loop()















  &lt;/code&gt;
    &lt;code&gt;scratch/main&amp;gt;                                                                                          

  Loading changes detected in ~/scratch.u.

  + factorial    : Nat -&amp;gt; Nat
  + guessingGame : '{IO, Exception} ()

  Run `update` to apply these changes to your codebase.

  &lt;/code&gt;
    &lt;head rend="h2"&gt;UCM Desktop&lt;/head&gt;
    &lt;p&gt;UCM Desktop is our GUI code browser for your local codebase.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unison Share&lt;/head&gt;
    &lt;p&gt;Unison Share is our community hub where open and closed-source projects alike are hosted. In addition to all the features you'd expect of a code-hosting platform—project and code search, individual and organizational accounts, browsing code and docs, reviewing contributions, etc, thanks to the one big idea, all of the code references are hyperlinked and navigable.&lt;/p&gt;
    &lt;head rend="h2"&gt;Unison Cloud&lt;/head&gt;
    &lt;p&gt;Unison Cloud is our platform for deploying Unison applications. Transition from local prototypes to fully deployed distributed applications using a simple, familiar API—no YAML files, inter-node protocols, or deployment scripts required. In Unison, your apps and infrastructure are defined in the same program, letting you manage services and deployments entirely in code.&lt;/p&gt;
    &lt;code&gt;deploy : '{IO, Exception} URI
deploy = Cloud.main do
  name = ServiceName.named "hello-world"
  serviceHash =
    deployHttp Environment.default() helloWorld
  ServiceName.assign name serviceHash



&lt;/code&gt;
    &lt;head rend="h2"&gt;What does Unison code look like?&lt;/head&gt;
    &lt;p&gt;Here's a Unison program that prompts the user to guess a random number from the command line.&lt;/p&gt;
    &lt;p&gt;It features several of Unison's language features:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Abilities - for functional effect management&lt;/item&gt;
      &lt;item&gt;Structural pattern matching - for decomposing types and managing control flow&lt;/item&gt;
      &lt;item&gt;Delayed computations - for representing non-eager evaluation&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;guessingGame : '{IO, Exception} ()
guessingGame = do Random.run do
  target = Random.natIn 0 100
  printLine "Guess a number between 0 and 100"

  loop = do
    match (console.readLine() |&amp;gt; Nat.fromText) with
      Some guess | guess == target -&amp;gt;
        printLine "Correct! You win!"
      Some guess | guess &amp;lt; target -&amp;gt;
        printLine "Too low, try again"
        loop()
      Some guess | guess &amp;gt; target -&amp;gt;
        printLine "Too high, try again"
        loop()
      otherwise -&amp;gt;
        printLine "Invalid input, try again"
        loop()

  loop()
  




&lt;/code&gt;
    &lt;head rend="h2"&gt;Our road to 1.0&lt;/head&gt;
    &lt;p&gt;The major milestones from 🥚 to 🐣 and 🐥.&lt;/p&gt;
    &lt;head rend="h3"&gt;Unison Computing company founding&lt;/head&gt;
    &lt;head rend="h3"&gt;First alpha release of Unison&lt;/head&gt;
    &lt;head rend="h3"&gt;Strangeloop conference&lt;/head&gt;
    &lt;head rend="h3"&gt;Unison adopts SQLite for local codebases&lt;/head&gt;
    &lt;head rend="h3"&gt;Unison Share's first deployment&lt;/head&gt;
    &lt;head rend="h3"&gt;Unison Forall conference&lt;/head&gt;
    &lt;head rend="h3"&gt;LSP support&lt;/head&gt;
    &lt;head rend="h3"&gt;Projects land in Unison&lt;/head&gt;
    &lt;head rend="h3"&gt;Kind-checking lands for Unison&lt;/head&gt;
    &lt;head rend="h3"&gt;Contributions added to Unison Share&lt;/head&gt;
    &lt;head rend="h3"&gt;OrderedTable storage added to the Cloud&lt;/head&gt;
    &lt;head rend="h3"&gt;Unison Cloud generally available to the public&lt;/head&gt;
    &lt;head rend="h3"&gt;We open-sourced Unison Share&lt;/head&gt;
    &lt;head rend="h3"&gt;Cloud daemons&lt;/head&gt;
    &lt;head rend="h3"&gt;Ecosystem-wide type-based search&lt;/head&gt;
    &lt;head rend="h3"&gt;Unison Forall 2024&lt;/head&gt;
    &lt;head rend="h3"&gt;Unison Desktop App&lt;/head&gt;
    &lt;head rend="h3"&gt;Volturno distributed stream processing library&lt;/head&gt;
    &lt;head rend="h3"&gt;Runtime performance optimizations&lt;/head&gt;
    &lt;head rend="h3"&gt;MCP server for Unison&lt;/head&gt;
    &lt;head rend="h3"&gt;Cloud BYOC&lt;/head&gt;
    &lt;head rend="h3"&gt;UCM git-style diff tool support&lt;/head&gt;
    &lt;head rend="h3"&gt;Branch history comments&lt;/head&gt;
    &lt;head rend="h3"&gt;Unison 1.0 release&lt;/head&gt;
    &lt;head rend="h2"&gt;Metrics&lt;/head&gt;
    &lt;p&gt;Our momentum is powered by a prolific team and a remarkable community.&lt;/p&gt;
    &lt;head rend="h3"&gt;26,558+&lt;/head&gt;
    &lt;head rend="h4"&gt;Commits&lt;/head&gt;
    &lt;head rend="h3"&gt;3,490+&lt;/head&gt;
    &lt;head rend="h4"&gt;PRs merged&lt;/head&gt;
    &lt;head rend="h3"&gt;6.2k&lt;/head&gt;
    &lt;head rend="h4"&gt;Github stars&lt;/head&gt;
    &lt;head rend="h3"&gt;152,459&lt;/head&gt;
    &lt;head rend="h4"&gt;Unison library downloads&lt;/head&gt;
    &lt;head rend="h3"&gt;139,811+&lt;/head&gt;
    &lt;head rend="h4"&gt;Published Unison definitions&lt;/head&gt;
    &lt;head rend="h3"&gt;1,300+&lt;/head&gt;
    &lt;head rend="h4"&gt;Unison project authors&lt;/head&gt;
    &lt;head rend="h2"&gt;Whats next?&lt;/head&gt;
    &lt;p&gt;We're continuing to improve the core Unison language and tooling for a more streamlined and delightful development experience, as well as developing exciting new capabilities on top of Unison Cloud. Here are a few examples on our immediate horizon:&lt;/p&gt;
    &lt;head rend="h2"&gt;Join us today&lt;/head&gt;
    &lt;p&gt;Unison couldn't be made without our amazing community. Join us and help shape the future of Unison.&lt;/p&gt;
    &lt;head rend="h2"&gt;Frequently asked questions&lt;/head&gt;
    &lt;head&gt;&lt;icon-core/&gt; Why make a whole new programming language? Couldn't you add Unison's features to another language? &lt;/head&gt;
    &lt;p&gt;Unison's hash-based, database-backed representation changes how code is identified, versioned, and shared. As a consequence, the workflow, toolchain, and deployment model are not add-ons; they emerge naturally from the language's design. In theory, you could try to retrofit these ideas onto another language, but doing so might be fragile, difficult to make reliable in production, and would likely require rewriting major parts of the existing tooling while restricting language features.&lt;/p&gt;
    &lt;p&gt;You don't build a rocket ship out of old cars, you start fresh.&lt;/p&gt;
    &lt;head&gt;&lt;icon-core/&gt; Is anyone using Unison in prod? &lt;/head&gt;
    &lt;p&gt;Yes, we are! Our entire Cloud orchestration layer is written entirely in Unison, and it has powered Unison Cloud from day one.&lt;/p&gt;
    &lt;head&gt;&lt;icon-core/&gt; I'm concerned about vendor lock-in; do I have to use Unison Cloud to deploy my services? &lt;/head&gt;
    &lt;p&gt;No, Unison is an open source, general programming language, and you can export a compiled binary and deploy it via Docker, or however you prefer.&lt;/p&gt;
    &lt;p&gt;You can also run Unison Cloud on your own infrastructure. Both Unison Cloud and our Bring Your Own Cloud (BYOC) offer generous free tiers.&lt;/p&gt;
    &lt;head&gt;&lt;icon-core/&gt; What does collaborating look like in Unison? &lt;/head&gt;
    &lt;p&gt;Unison Share supports organizations, tickets, code contributions (pull requests), code review, and more.&lt;/p&gt;
    &lt;p&gt;In many ways Unison's story for collaboration outstrips the status quo of developer tooling. e.g. merge conflicts only happen when two people actually modify the same definition; not because you moved some stuff around in your files.&lt;/p&gt;
    &lt;head&gt;&lt;icon-core/&gt; How does version control work in the absence of Git? &lt;/head&gt;
    &lt;p&gt;Unison implements a native version control system: with projects, branches, clone, push, pull, merge, etc.&lt;/p&gt;
    &lt;head&gt;&lt;icon-core/&gt; Do I have to use a specific IDE? &lt;/head&gt;
    &lt;p&gt;No, you can pick any IDE that you're familiar with. Unison exposes an LSP server and many community members have contributed their own editor setups here.&lt;/p&gt;
    &lt;head&gt;&lt;icon-core/&gt; What about interop with other languages? &lt;/head&gt;
    &lt;p&gt;Work is underway today to add a C FFI!&lt;/p&gt;
    &lt;head&gt;&lt;icon-core/&gt; Without files, how do I see my codebase? &lt;/head&gt;
    &lt;p&gt;Your codebase structure is viewable with the Unison Desktop app. The UCM Desktop app also features click-through to definition tooling and rich rendering of docs.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46049722</guid><pubDate>Tue, 25 Nov 2025 19:33:00 +0000</pubDate></item><item><title>A new bridge links the math of infinity to computer science</title><link>https://www.quantamagazine.org/a-new-bridge-links-the-strange-math-of-infinity-to-computer-science-20251121/</link><description>&lt;doc fingerprint="b72c931205918bb9"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A New Bridge Links the Strange Math of Infinity to Computer Science&lt;/head&gt;
    &lt;head rend="h2"&gt;Introduction&lt;/head&gt;
    &lt;p&gt;All of modern mathematics is built on the foundation of set theory, the study of how to organize abstract collections of objects. But in general, research mathematicians don’t need to think about it when they’re solving their problems. They can take it for granted that sets behave the way they’d expect, and carry on with their work.&lt;/p&gt;
    &lt;p&gt;Descriptive set theorists are an exception. This small community of mathematicians never stopped studying the fundamental nature of sets — particularly the strange infinite ones that other mathematicians ignore.&lt;/p&gt;
    &lt;p&gt;Their field just got a lot less lonely. In 2023, a mathematician named Anton Bernshteyn published a deep and surprising connection between the remote mathematical frontier of descriptive set theory and modern computer science.&lt;/p&gt;
    &lt;p&gt;He showed that all problems about certain kinds of infinite sets can be rewritten as problems about how networks of computers communicate. The bridge connecting the disciplines surprised researchers on both sides. Set theorists use the language of logic, computer scientists the language of algorithms. Set theory deals with the infinite, computer science with the finite. There’s no reason why their problems should be related, much less equivalent.&lt;/p&gt;
    &lt;p&gt;“This is something really weird,” said Václav Rozhoň, a computer scientist at Charles University in Prague. “Like, you are not supposed to have this.”&lt;/p&gt;
    &lt;p&gt;Since Bernshteyn’s result, his peers have been exploring how to move back and forth across the bridge to prove new theorems on either side, and how to extend that bridge to new classes of problems. Some descriptive set theorists are even starting to apply insights from the computer science side to reorganize the landscape of their entire field, and to rethink the way they understand infinity.&lt;/p&gt;
    &lt;p&gt;“This whole time we’ve been working on very similar problems without directly talking to each other,” said Clinton Conley, a descriptive set theorist at Carnegie Mellon University. “It just opens the doors to all these new collaborations.”&lt;/p&gt;
    &lt;head rend="h2"&gt;Broken Sets&lt;/head&gt;
    &lt;p&gt;Bernshteyn was an undergraduate when he first heard of descriptive set theory — as an example of a field that had once mattered, then decayed to nothing. More than a year would pass before he found out the professor had been wrong.&lt;/p&gt;
    &lt;p&gt;In 2014, as a first-year graduate student at the University of Illinois, Bernshteyn took a logic course with Anush Tserunyan, who would later become one of his advisers. She corrected the misconception. “She should take all the credit for me being in this field,” he said. “She really made it seem that logic and set theory is this glue that connects all different parts of math.”&lt;/p&gt;
    &lt;p&gt;Descriptive set theory dates back to Georg Cantor, who proved in 1874 that there are different sizes of infinity. The set of whole numbers (0, 1, 2, 3, …), for instance, is the same size as the set of all fractions, but smaller than the set of all real numbers.&lt;/p&gt;
    &lt;p&gt;At the time, mathematicians were deeply uncomfortable with this menagerie of different infinities. “It’s hard to wrap your head around,” said Bernshteyn, who is now at the University of California, Los Angeles.&lt;/p&gt;
    &lt;p&gt;Partly in response to that discomfort, mathematicians developed a different notion of size — one that described, say, how much length or area or volume a set might occupy, rather than the number of elements it contained. This notion of size is known as a set’s “measure” (in contrast to Cantor’s notion of size, which is a set’s “cardinality”). One of the simplest types of measure — the Lebesgue measure — quantifies a set’s length. While the set of real numbers between zero and 1 and the set of real numbers between zero and 10 are both infinite and have the same cardinality, the first has a Lebesgue measure of 1 and the second a Lebesgue measure of 10.&lt;/p&gt;
    &lt;p&gt;To study more complicated sets, mathematicians use other types of measures. The uglier a set is, the fewer ways there are to measure it. Descriptive set theorists ask questions about which sets can be measured according to different definitions of “measure.” They then arrange them in a hierarchy based on the answers to those questions. At the top are sets that can be constructed easily and studied using any notion of measure you want. At the bottom are “unmeasurable” sets, which are so complicated they can’t be measured at all. “The word people often use is ‘pathological,’” Bernshteyn said. “Nonmeasurable sets are really bad. They’re counterintuitive, and they don’t behave well.”&lt;/p&gt;
    &lt;p&gt;This hierarchy doesn’t just help set theorists map out the landscape of their field; it also gives them insights into what tools they can use to tackle more typical problems in other areas of math. Mathematicians in some fields, such as dynamical systems, group theory and probability theory, need information about the size of the sets they’re using. A set’s position in the hierarchy determines what tools they can use to solve their problem.&lt;/p&gt;
    &lt;p&gt;Descriptive set theorists are thus like librarians, tending to a massive bookshelf of different kinds of infinite sets (and the different ways of measuring them). Their job is to take a problem, determine how complicated a set its solution requires, and place it on the proper shelf, so that other mathematicians can take note.&lt;/p&gt;
    &lt;head rend="h2"&gt;Making a Choice&lt;/head&gt;
    &lt;p&gt;Bernshteyn belongs to a group of librarians who sort problems about infinite sets of nodes connected by edges, called graphs. In particular, he studies graphs that have infinitely many separate pieces, each containing infinitely many nodes. Most graph theorists don’t study these kinds of graphs; they focus on finite ones instead. But such infinite graphs can represent and provide information about dynamical systems and other important kinds of sets, making them a major area of interest for descriptive set theorists.&lt;/p&gt;
    &lt;p&gt;Here’s an example of the kind of infinite graph that Bernshteyn and his colleagues might study. Start with a circle, which contains infinitely many points. Pick one point: This will be your first node. Then move a fixed distance around the circle’s circumference. This gives you a second node. For example, you might move one-fifth of the way around the circle. Connect the two nodes with an edge. Move the same distance to a third node, and connect it to the previous one. And so on.&lt;/p&gt;
    &lt;p&gt;If you move one-fifth of the way around the circle each time, it’ll take five steps to get back where you started. In general, if you move any distance that can be written as a fraction, the nodes will form a closed loop. But if the distance can’t be written as a fraction, the process will go on forever. You’ll get an infinite number of connected nodes.&lt;/p&gt;
    &lt;p&gt;But that’s not all: This infinitely long sequence forms only the first piece of your graph. Even though it contains infinitely many nodes, it doesn’t contain all the points on the circle. To generate the other pieces of the graph, start at one of those other points. Now move the same distance at each step as you did in the first piece. You’ll end up building a second infinite sequence of connected nodes, totally disconnected from the first.&lt;/p&gt;
    &lt;p&gt;Do this for every possible new starting point on the circle. You’ll get a graph consisting of infinitely many separate pieces, with each piece made of an infinite number of nodes.&lt;/p&gt;
    &lt;p&gt;Mathematicians can then ask whether it’s possible to color the nodes in this graph so that they obey certain rules. Using just two colors, for instance, can you color every node in the graph so that no two connected nodes are the same color? The solution might seem straightforward. Look at the first piece of your graph, pick a node, and color it blue. Then color the rest of the piece’s nodes in an alternating pattern: yellow, blue, yellow, blue. Do the same for every piece in your graph: Pick a node, color it blue, then alternate colors. Ultimately, you’ll use just two colors to achieve your task.&lt;/p&gt;
    &lt;p&gt;But to accomplish this coloring, you had to rely on a hidden assumption that set theorists call the axiom of choice. It’s one of the nine fundamental building blocks from which all mathematical statements are constructed. According to this axiom, if you start with a bunch of sets, you can choose one item from each of those sets to create a new set — even if you have infinitely many sets to choose from. This axiom is useful, in that it allows mathematicians to prove all sorts of statements of interest. But it also leads to strange paradoxes. Descriptive set theorists avoid it.&lt;/p&gt;
    &lt;p&gt;Your graph had infinitely many pieces. This corresponds to having infinitely many sets. You chose one item from each set — the first point you decided to color blue in each of the pieces. All those blue points formed a new set. You used the axiom of choice.&lt;/p&gt;
    &lt;p&gt;Which leads to a problem when you color the rest of the nodes in alternating patterns of blue and yellow. You’ve colored each node (which has zero length) separately, without any understanding of how nodes relate to one another when they come from different pieces of the graph. This means that you can’t describe the set of all the graph’s blue nodes, or the set of all its yellow nodes, in terms of length either. In other words, these sets are unmeasurable. Mathematicians can’t say anything useful about them.&lt;/p&gt;
    &lt;p&gt;To descriptive set theorists, this is unsatisfying. And so they want to figure out a way to color the graph in a continuous way — a way that doesn’t use the axiom of choice, and that gives them measurable sets.&lt;/p&gt;
    &lt;p&gt;To do this, remember how you built the first piece of your graph: You picked a node on a circle and connected it to a second node some distance away. Now color the first node blue, the second yellow, and the entire arc between them blue. Similarly, color the arc between the second and third nodes yellow. Color the third arc blue. And so on.&lt;/p&gt;
    &lt;p&gt;Soon, you’ll have made it almost completely around the circle — meaning that you’ve assigned a color to all the nodes in your graph except for the ones that fall in a small, leftover segment. Say the last arc you colored was yellow. How do you color this final, smaller segment? You can’t use blue, because these nodes will connect to nodes in the original arc you colored blue. But you also can’t use yellow, because these nodes connect back to yellow ones from the previous arc.&lt;/p&gt;
    &lt;p&gt;You have to use a third color — say, green — to complete your coloring.&lt;/p&gt;
    &lt;p&gt;Still, the sets of blue, yellow and green nodes you end up with are all just pieces of the circle’s circumference, rather than the scatterings of points you ended up with when you used the axiom of choice. You can calculate the lengths of these sets. They’re measurable.&lt;/p&gt;
    &lt;p&gt;Descriptive set theorists therefore place the two-color version of the problem on the lowest shelf in their hierarchy (for unmeasurable sets), while the three-color problem goes on a much higher shelf of problems — ones where lots of notions of measure can be applied.&lt;/p&gt;
    &lt;p&gt;Bernshteyn spent his years in graduate school studying such coloring problems, shelving them one by one. Then, shortly after he finished his degree, he stumbled on a potential way to shelve them all at once — and to show that these problems have a much deeper and more mathematically relevant structure than anyone had realized.&lt;/p&gt;
    &lt;head rend="h2"&gt;Round by Round&lt;/head&gt;
    &lt;p&gt;From time to time, Bernshteyn enjoys going to computer science talks, where graphs are finite and represent networks of computers.&lt;/p&gt;
    &lt;p&gt;In 2019, one of those talks changed the course of his career. It was about “distributed algorithms” — sets of instructions that run simultaneously on multiple computers in a network to accomplish a task without a central coordinator.&lt;/p&gt;
    &lt;p&gt;Say you have a bunch of Wi-Fi routers in a building. Nearby routers can interfere with each other if they use the same communication frequency channel. So each router needs to choose a different channel from the ones used by its immediate neighbors.&lt;/p&gt;
    &lt;p&gt;Computer scientists can reframe this as a coloring problem on a graph: Represent each router as a node, and connect nearby ones with edges. Using just two colors (representing two different frequency channels), find a way to color each node so that no two connected nodes are the same color.&lt;/p&gt;
    &lt;p&gt;But there’s a catch: Nodes can only communicate with their immediate neighbors, using so-called local algorithms. First, each node runs the same algorithm and assigns itself a color. It then communicates with its neighbors to learn how other nodes are colored in a small region around it. Then it runs the algorithm again to decide whether to keep its color or switch it. It repeats this step until the whole network has a proper coloring.&lt;/p&gt;
    &lt;p&gt;Computer scientists want to know how many steps a given algorithm requires. For example, any local algorithm that can solve the router problem with only two colors must be incredibly inefficient, but it’s possible to find a very efficient local algorithm if you’re allowed to use three.&lt;/p&gt;
    &lt;p&gt;At the talk Bernshteyn was attending, the speaker discussed these thresholds for different kinds of problems. One of the thresholds, he realized, sounded a lot like a threshold that existed in the world of descriptive set theory — about the number of colors required to color certain infinite graphs in a measurable way.&lt;/p&gt;
    &lt;p&gt;To Bernshteyn, it felt like more than a coincidence. It wasn’t just that computer scientists are like librarians too, shelving problems based on how efficiently their algorithms work. It wasn’t just that these problems could also be written in terms of graphs and colorings.&lt;/p&gt;
    &lt;p&gt;Perhaps, he thought, the two bookshelves had more in common than that. Perhaps the connection between these two fields went much, much deeper.&lt;/p&gt;
    &lt;p&gt;Perhaps all the books, and their shelves, were identical, just written in different languages — and in need of a translator.&lt;/p&gt;
    &lt;head rend="h2"&gt;Opening the Door&lt;/head&gt;
    &lt;p&gt;Bernshteyn set out to make this connection explicit. He wanted to show that every efficient local algorithm can be turned into a Lebesgue-measurable way of coloring an infinite graph (that satisfies some additional important properties). That is, one of computer science’s most important shelves is equivalent to one of set theory’s most important shelves (high up in the hierarchy).&lt;/p&gt;
    &lt;p&gt;He began with the class of network problems from the computer science lecture, focusing on their overarching rule — that any given node’s algorithm uses information about just its local neighborhood, whether the graph has a thousand nodes or a billion.&lt;/p&gt;
    &lt;p&gt;To run properly, all the algorithm has to do is label each node in a given neighborhood with a unique number, so that it can log information about nearby nodes and give instructions about them. That’s easy enough to do in a finite graph: Just give every node in the graph a different number.&lt;/p&gt;
    &lt;p&gt;If Bernshteyn could run the same algorithm on an infinite graph, it meant he could color the graph in a measurable way — solving a graph-coloring question on the set theory side. But there was a problem: These infinite graphs are “uncountably” infinite. There’s no way to uniquely label all their nodes.&lt;/p&gt;
    &lt;p&gt;Bernshteyn’s challenge was to find a cleverer way to label the graphs.&lt;/p&gt;
    &lt;p&gt;He knew that he’d have to reuse labels. But that was fine so long as nearby nodes were labeled differently. Was there a way to assign labels without accidentally reusing one in the same neighborhood?&lt;/p&gt;
    &lt;p&gt;Bernshteyn showed that there is always a way — no matter how many labels you decide to use, and no matter how many nodes your local neighborhood has. This means that you can always safely extend the algorithm from the computer science side to the set theory side. “Any algorithm in our setup corresponds to a way of measurably coloring any graph in the descriptive set theory setup,” Rozhoň said.&lt;/p&gt;
    &lt;p&gt;The proof came as a surprise to mathematicians. It demonstrated a deep link between computation and definability, and between algorithms and measurable sets. Mathematicians are now exploring how to take advantage of Bernshteyn’s discovery. In a paper published this year, for instance, Rozhoň and his colleagues figured out that it’s possible to color special graphs called trees by looking at the same problem in the computer science context. The result also illuminated which tools mathematicians might use to study the trees’ corresponding dynamical systems. “This is a very interesting experience, trying to prove results in a field where I don’t understand even the basic definitions,” Rozhoň said.&lt;/p&gt;
    &lt;p&gt;Mathematicians have also been working to translate problems in the other direction. In one case, they used set theory to prove a new estimate of how hard a certain class of problems is to solve.&lt;/p&gt;
    &lt;p&gt;Bernshteyn’s bridge isn’t just about having a new tool kit for solving individual problems. It has also allowed set theorists to gain a clearer view of their field. There were lots of problems that they had no idea how to classify. In many cases, that’s now changed, because set theorists have computer scientists’ more organized bookshelves to guide them.&lt;/p&gt;
    &lt;p&gt;Bernshteyn hopes this growing area of research will change how the working mathematician views set theorists’ work — that they’ll no longer see it as remote and disconnected from the real mathematical world. “I’m trying to change this,” he said. “I want people to get used to thinking about infinity.”&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46049932</guid><pubDate>Tue, 25 Nov 2025 19:53:20 +0000</pubDate></item><item><title>How to repurpose your old phone's GPS modem into a web server</title><link>https://blog.nns.ee/2021/04/01/modem-blog</link><description>&lt;doc fingerprint="26af1955b9d333db"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;This blog is now hosted on a GPS/LTE modem&lt;/head&gt;
    &lt;p&gt;No, really. Despite the timing of this article, this is not an April Fool's joke.&lt;/p&gt;
    &lt;head rend="h2"&gt;PinePhone's GPS/WWAN/LTE modem&lt;/head&gt;
    &lt;p&gt;While developing software on the PinePhone, I came across this peculiar message in &lt;code&gt;dmesg&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;[   25.476857] modem-power serial1-0: ADB KEY is '41618099' (you can use it to unlock ADB access to the modem)
&lt;/code&gt;
    &lt;p&gt;For context, the PinePhone has a Quectel EG25-G modem, which handles GPS and wireless connectivity for the PinePhone. This piece of hardware is one of the few components on the phone which is closed-source.&lt;/p&gt;
    &lt;p&gt;When I saw that message and the mention of ADB, I immediately thought of Android Debug Bridge, the software commonly used to communicate with Android devices. "Surely," I thought, "it can't be talking about that ADB". Well, turns out it is.&lt;/p&gt;
    &lt;p&gt;The message links to an article which details the modem in question. It also links to an unlocker utility which, when used, prints out AT commands to enable &lt;code&gt;adbd&lt;/code&gt; on the modem.&lt;/p&gt;
    &lt;code&gt;$ ./qadbkey-unlock 41618099
AT+QADBKEY="WUkkFzFSXLsuRM8t"
AT+QCFG="usbcfg",0x2C7C,0x125,1,1,1,1,1,1,0
&lt;/code&gt;
    &lt;p&gt;These can be sent to the modem using &lt;code&gt;screen&lt;/code&gt;:&lt;/p&gt;
    &lt;code&gt;# screen /dev/ttyUSB2 115200 
&lt;/code&gt;
    &lt;p&gt;For whatever reason, my input wasn't being echoed back, but the screen session printed out "OK" twice, indicating it had executed the commands fine.&lt;/p&gt;
    &lt;p&gt;After setting up proper &lt;code&gt;udev&lt;/code&gt; rules and &lt;code&gt;adb&lt;/code&gt; on my "host machine", which is the PinePhone, the modem popped up in the output for &lt;code&gt;adb devices&lt;/code&gt;, and I could drop into a shell:&lt;/p&gt;
    &lt;code&gt;$ adb devices
List of devices attached
(no serial number)	device

$ adb shell
/ #
&lt;/code&gt;
    &lt;p&gt;Because &lt;code&gt;adbd&lt;/code&gt; was running in root mode, I dropped into a root shell. Neat.&lt;/p&gt;
    &lt;p&gt;It turns out the modem runs its own OS totally separate from the rest of the PinePhone OS. With the latest updates, it runs Linux 3.18.44.&lt;/p&gt;
    &lt;head rend="h2"&gt;Running a webserver&lt;/head&gt;
    &lt;p&gt;For whatever reason, I thought it'd be fun to run my blog on this thing. Since we were working with limited resources (around 48M of space and the same amount of memory), and the fact that my blog is just a bunch of static files, I decided that something like nginx (as lightweight as it is) would be a bit overkill for my purposes.&lt;/p&gt;
    &lt;p&gt;darkhttpd seemed to fit the bill well. Single binary, no external dependencies, does GET and HEAD requests only. Perfect.&lt;/p&gt;
    &lt;p&gt;I used the armv7l-linux-musleabihf-cross toolchain to cross compile it for ARMv7 and statically link it against musl. &lt;code&gt;adb push&lt;/code&gt; let me easily push the binary and my site assets to the modem's &lt;code&gt;/usrdata&lt;/code&gt; directory, which seems to have a writable partition about 50M big mounted on it.&lt;/p&gt;
    &lt;p&gt;The HTTP server works great. I decided to use ADB to expose the HTTP port to my PinePhone:&lt;/p&gt;
    &lt;code&gt;$ adb forward tcp:8080 tcp:80
&lt;/code&gt;
    &lt;p&gt;As ADB-forwarded ports are only bound to the loopback interface, I also manually exposed it to external connections:&lt;/p&gt;
    &lt;code&gt;# sysctl -w net.ipv4.conf.all.route_localnet=1
# iptables -t nat -I PREROUTING -p tcp --dport 8080 -j DNAT --to-destination 127.0.0.1:8080
&lt;/code&gt;
    &lt;p&gt;I could now access my blog on &lt;code&gt;http://pine:8080/&lt;/code&gt;. Cool!&lt;/p&gt;
    &lt;head rend="h2"&gt;Throughput?&lt;/head&gt;
    &lt;p&gt;I ran &lt;code&gt;iperf&lt;/code&gt; over ADB port forwarding just to see what kind of throughput I get.&lt;/p&gt;
    &lt;code&gt;$ iperf -c localhost
------------------------------------------------------------
Client connecting to localhost, TCP port 5001
TCP window size: 2.50 MByte (default)
------------------------------------------------------------
[  3] local 127.0.0.1 port 44230 connected with 127.0.0.1 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.6 sec  14.4 MBytes  11.4 Mbits/sec
&lt;/code&gt;
    &lt;p&gt;So around 10Mb/s. Not great, not terrible.&lt;/p&gt;
    &lt;p&gt;The PinePhone itself is connected to the network over USB (side note: I had to remove two components from the board to get USB networking to work). Out of interest, I ran &lt;code&gt;iperf&lt;/code&gt; over that connection as well:&lt;/p&gt;
    &lt;code&gt;$ iperf -c 10.15.19.82
------------------------------------------------------------
Client connecting to 10.15.19.82, TCP port 5001
TCP window size:  136 KByte (default)
------------------------------------------------------------
[  3] local 10.15.19.100 port 58672 connected with 10.15.19.82 port 5001
[ ID] Interval       Transfer     Bandwidth
[  3]  0.0-10.4 sec  25.8 MBytes  20.7 Mbits/sec
&lt;/code&gt;
    &lt;p&gt;Although I was expecting more, it doesn't really matter, as I was bottlenecking at the ADB-forwarded connection.&lt;/p&gt;
    &lt;head rend="h2"&gt;Further thoughts&lt;/head&gt;
    &lt;p&gt;I wonder how secure the modem is. It turns out a lot of AT commands use &lt;code&gt;system()&lt;/code&gt; on the modem. I suspect some of those AT commands may be vulnerable to command injection, but I haven't looked into this further. It also doesn't really matter when dropping into a root shell using ADB is this easy.&lt;/p&gt;
    &lt;p&gt;At first glance, this seems like a perfect method to obtain persistence for malware. With root access on the host system, malware could implant itself into the modem, which would enable it to survive reinstalls of the host OS, and snoop on communications or track the device's location. Some of the impact is alleviated by the fact that all interaction with the host OS happens over USB and I2S and only if the host OS initiates it, so malware in the modem couldn't directly interact with the host OS.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46049981</guid><pubDate>Tue, 25 Nov 2025 19:58:10 +0000</pubDate></item></channel></rss>