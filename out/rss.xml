<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Mon, 08 Sep 2025 06:16:12 +0000</lastBuildDate><item><title>The Expression Problem and its solutions (2016)</title><link>https://eli.thegreenplace.net/2016/the-expression-problem-and-its-solutions/</link><description>&lt;doc fingerprint="7698b0504c1917f1"&gt;
  &lt;main&gt;
    &lt;p&gt;The craft of programming is almost universally concerned with different types of data and operations/algorithms that act on this data [1]. Therefore, it's hardly surprising that designing abstractions for data types and operations has been on the mind of software engineers and programming-language designers since... forever.&lt;/p&gt;
    &lt;p&gt;Yet I've only recently encountered a name for a software design problem which I ran into multiple times in my career. It's a problem so fundamental that I was quite surprised that I haven't seen it named before. Here is a quick problem statement.&lt;/p&gt;
    &lt;p&gt;Imagine that we have a set of data types and a set of operations that act on these types. Sometimes we need to add more operations and make sure they work properly on all types; sometimes we need to add more types and make sure all operations work properly on them. Sometimes, however, we need to add both - and herein lies the problem. Most of the mainstream programming languages don't provide good tools to add both new types and new operations to an existing system without having to change existing code. This is called the "expression problem". Studying the problem and its possible solutions gives great insight into the fundamental differences between object-oriented and functional programming and well as concepts like interfaces and multiple dispatch.&lt;/p&gt;
    &lt;head rend="h2"&gt;A motivating example&lt;/head&gt;
    &lt;p&gt;As is my wont, my example comes from the world of compilers and interpreters. To my defense, this is also the example used in some of the seminal historic sources on the expression problem, as the historical perspective section below details.&lt;/p&gt;
    &lt;p&gt;Imagine we're designing a simple expression evaluator. Following the standard interpreter design pattern, we have a tree structure consisting of expressions, with some operations we can do on such trees. In C++ we'd have an interface every node in the expression tree would have to implement:&lt;/p&gt;
    &lt;code&gt;class Expr {
public:
  virtual std::string ToString() const = 0;
  virtual double Eval() const = 0;
};
&lt;/code&gt;
    &lt;p&gt;This interface shows that we currently have two operations we can do on expression trees - evaluate them and query for their string representations. A typical leaf node expression:&lt;/p&gt;
    &lt;code&gt;class Constant : public Expr {
public:
  Constant(double value) : value_(value) {}

  std::string ToString() const {
    std::ostringstream ss;
    ss &amp;lt;&amp;lt; value_;
    return ss.str();
  }

  double Eval() const {
    return value_;
  }

private:
  double value_;
};
&lt;/code&gt;
    &lt;p&gt;And a typical composite expression:&lt;/p&gt;
    &lt;code&gt;class BinaryPlus : public Expr {
public:
  BinaryPlus(const Expr&amp;amp; lhs, const Expr&amp;amp; rhs) : lhs_(lhs), rhs_(rhs) {}

  std::string ToString() const {
    return lhs_.ToString() + " + " + rhs_.ToString();
  }

  double Eval() const {
    return lhs_.Eval() + rhs_.Eval();
  }

private:
  const Expr&amp;amp; lhs_;
  const Expr&amp;amp; rhs_;
};
&lt;/code&gt;
    &lt;p&gt;Until now, it's all fairly basic stuff. How extensible is this design? Let's see... if we want to add new expression types ("variable reference", "function call" etc.), that's pretty easy. We just define additional classes inheriting from Expr and implement the Expr interface (ToString and Eval).&lt;/p&gt;
    &lt;p&gt;However, what happens if we want to add new operations that can be applied to expression trees? Right now we have Eval and ToString, but we may want additional operations like "type check" or "serialize" or "compile to machine code" or whatever.&lt;/p&gt;
    &lt;p&gt;It turns out that adding new operations isn't as easy as adding new types. We'd have to change the Expr interface, and consequently change every existing expression type to support the new method(s). If we don't control the original code or it's hard to change it for other reasons, we're in trouble.&lt;/p&gt;
    &lt;p&gt;In other words, we'd have to violate the venerable open-closed principle, one of the main principles of object-oriented design, defined as:&lt;/p&gt;
    &lt;quote&gt;software entities (classes, modules, functions, etc.) should be open for extension, but closed for modification&lt;/quote&gt;
    &lt;p&gt;The problem we're hitting here is called the expression problem, and the example above shows how it applies to object-oriented programming.&lt;/p&gt;
    &lt;p&gt;Interestingly, the expression problem bites functional programming languages as well. Let's see how.&lt;/p&gt;
    &lt;head rend="h2"&gt;The expression problem in functional programming&lt;/head&gt;
    &lt;p&gt;Update 2018-02-05: a new post discusses the problem and its solutions in Haskell in more depth.&lt;/p&gt;
    &lt;p&gt;Object-oriented approaches tend to collect functionality in objects (types). Functional languages cut the cake from a different angle, usually preferring types as thin data containers, collecting most functionality in functions (operations) that act upon them. Functional languages don't escape the expression problem - it just manifests there in a different way.&lt;/p&gt;
    &lt;p&gt;To demonstrate this, let's see how the expression evaluator / stringifier looks in Haskell. Haskell is a good poster child for functional programming since its pattern matching on types makes such code especially succinct:&lt;/p&gt;
    &lt;code&gt;module Expressions where

data Expr = Constant Double
          | BinaryPlus Expr Expr

stringify :: Expr -&amp;gt; String
stringify (Constant c) = show c
stringify (BinaryPlus lhs rhs) = stringify lhs
                                ++ " + "
                                ++ stringify rhs

evaluate :: Expr -&amp;gt; Double
evaluate (Constant c) = c
evaluate (BinaryPlus lhs rhs) = evaluate lhs + evaluate rhs
&lt;/code&gt;
    &lt;p&gt;Now let's say we want to add a new operation - type checking. We simply have to add a new function typecheck and define how it behaves for all known kinds of expressions. No need to modify existing code.&lt;/p&gt;
    &lt;p&gt;On the other hand, if we want to add a new type (like "function call"), we get into trouble. We now have to modify all existing functions to handle this new type. So we hit exactly the same problem, albeit from a different angle.&lt;/p&gt;
    &lt;head rend="h2"&gt;The expression problem matrix&lt;/head&gt;
    &lt;p&gt;A visual representation of the expression problem can be helpful to appreciate how it applies to OOP and FP in different ways, and how a potential solution would look.&lt;/p&gt;
    &lt;p&gt;The following 2-D table (a "matrix") has types in its rows and operations in its columns. A matrix cell row, col is checked when the operation col is implemented for type row:&lt;/p&gt;
    &lt;p&gt;In object-oriented languages, it's easy to add new types but difficult to add new operations:&lt;/p&gt;
    &lt;p&gt;Whereas in functional languages, it's easy to add new operations but difficult to add new types:&lt;/p&gt;
    &lt;head rend="h2"&gt;A historical perspective&lt;/head&gt;
    &lt;p&gt;The expression problem isn't new, and has likely been with us since the early days; it pops its head as soon as programs reach some not-too-high level of complexity.&lt;/p&gt;
    &lt;p&gt;It's fairly certain that the name expression problem comes from an email sent by Philip Wadler to a mailing list deailing with adding generics to Java (this was back in the 1990s).&lt;/p&gt;
    &lt;p&gt;In that email, Wadler points to the paper "Synthesizing Object-Oriented and Functional Design to Promote Re-Use" by Krishnamurthi, Felleisen and Friedman as an earlier work describing the problem and proposed solutions. This is a great paper and I highly recommend reading it. Krishnamurthi et.al., in their references, point to papers from as early as 1975 describing variations of the problem in Algol.&lt;/p&gt;
    &lt;head rend="h2"&gt;Flipping the matrix with the visitor pattern&lt;/head&gt;
    &lt;p&gt;So far the article has focused on the expression problem, and I hope it's clear by now. However, the title also has the word solution in it, so let's turn to that.&lt;/p&gt;
    &lt;p&gt;It's possible to kinda solve (read on to understand why I say "kinda") the expression problem in object-oriented languages; first, we have to look at how we can flip the problem on its side using the visitor pattern. The visitor pattern is very common for this kind of problems, and for a good reason. It lets us reformulate our code in a way that makes it easier to change in some dimensions (though harder in others).&lt;/p&gt;
    &lt;p&gt;For the C++ sample shown above, rewriting it using the visitor pattern means adding a new "visitor" interface:&lt;/p&gt;
    &lt;code&gt;class ExprVisitor {
public:
  virtual void VisitConstant(const Constant&amp;amp; c) = 0;
  virtual void VisitBinaryPlus(const BinaryPlus&amp;amp; bp) = 0;
};
&lt;/code&gt;
    &lt;p&gt;And changing the Expr interface to be:&lt;/p&gt;
    &lt;code&gt;class Expr {
public:
  virtual void Accept(ExprVisitor* visitor) const = 0;
};
&lt;/code&gt;
    &lt;p&gt;Now expression types defer the actual computation to the visitor, as follows:&lt;/p&gt;
    &lt;code&gt;class Constant : public Expr {
public:
  Constant(double value) : value_(value) {}

  void Accept(ExprVisitor* visitor) const {
    visitor-&amp;gt;VisitConstant(*this);
  }

  double GetValue() const {
    return value_;
  }

private:
  double value_;
};

// ... similarly, BinaryPlus would have
//
//    void Accept(ExprVisitor* visitor) const {
//      visitor-&amp;gt;VisitBinaryPlus(*this);
//    }
//
// ... etc.
&lt;/code&gt;
    &lt;p&gt;A sample visitor for evaluation would be [2]:&lt;/p&gt;
    &lt;code&gt;class Evaluator : public ExprVisitor {
public:
  double GetValueForExpr(const Expr&amp;amp; e) {
    return value_map_[&amp;amp;e];
  }

  void VisitConstant(const Constant&amp;amp; c) {
    value_map_[&amp;amp;c] = c.GetValue();
  }

  void VisitBinaryPlus(const BinaryPlus&amp;amp; bp) {
    bp.GetLhs().Accept(this);
    bp.GetRhs().Accept(this);
    value_map_[&amp;amp;bp] = value_map_[&amp;amp;(bp.GetLhs())] + value_map_[&amp;amp;(bp.GetRhs())];
  }

private:
  std::map&amp;lt;const Expr*, double&amp;gt; value_map_;
};
&lt;/code&gt;
    &lt;p&gt;It should be obvious that for a given set of data types, adding new visitors is easy and doesn't require modifying any other code. On the other hand, adding new types is problematic since it means we have to update the ExprVisitor interface with a new abstract method, and consequently update all the visitors to implement it.&lt;/p&gt;
    &lt;p&gt;So it seems that we've just turned the expression problem on its side: we're using an OOP language, but now it's hard to add types and easy to add ops, just like in the functional approach. I find it extremely interesting that we can do this. In my eyes this highlights the power of different abstractions and paradigms, and how they enable us to rethink a problem in a completely different light.&lt;/p&gt;
    &lt;p&gt;So we haven't solved anything yet; we've just changed the nature of the problem we're facing. Worry not - this is just a stepping stone to an actual solution.&lt;/p&gt;
    &lt;head rend="h2"&gt;Extending the visitor pattern&lt;/head&gt;
    &lt;p&gt;The following is code excerpts from a C++ solution that follows the extended visitor pattern proposed by Krishnamurthi et. al. in their paper; I strongly suggest reading the paper (particularly section 3) if you want to understand this code on a deep level. A complete code sample in C++ that compiles and runs is available here.&lt;/p&gt;
    &lt;p&gt;Adding new visitors (ops) with the visitor pattern is easy. Our challenge is to add a new type without upheaving too much existing code. Let's see how it's done.&lt;/p&gt;
    &lt;p&gt;One small design change that we should make to the original visitor pattern is use virtual inheritance for Evaluator, for reasons that will soon become obvious:&lt;/p&gt;
    &lt;code&gt;class Evaluator : virtual public ExprVisitor {
  // .. the rest is the same
};
&lt;/code&gt;
    &lt;p&gt;Now we're going to add a new type - FunctionCall:&lt;/p&gt;
    &lt;code&gt;// This is the new ("extended") expression we're adding.
class FunctionCall : public Expr {
public:
  FunctionCall(const std::string&amp;amp; name, const Expr&amp;amp; argument)
      : name_(name), argument_(argument) {}

  void Accept(ExprVisitor* visitor) const {
    ExprVisitorWithFunctionCall* v =
        dynamic_cast&amp;lt;ExprVisitorWithFunctionCall*&amp;gt;(visitor);
    if (v == nullptr) {
      std::cerr &amp;lt;&amp;lt; "Fatal: visitor is not ExprVisitorWithFunctionCall\n";
      exit(1);
    }
    v-&amp;gt;VisitFunctionCall(*this);
  }

private:
  std::string name_;
  const Expr&amp;amp; argument_;
};
&lt;/code&gt;
    &lt;p&gt;Since we don't want to modify the existing visitors, we create a new one, extending Evaluator for function calls. But first, we need to extend the ExprVisitor interface to support the new type:&lt;/p&gt;
    &lt;code&gt;class ExprVisitorWithFunctionCall : virtual public ExprVisitor {
public:
  virtual void VisitFunctionCall(const FunctionCall&amp;amp; fc) = 0;
};
&lt;/code&gt;
    &lt;p&gt;Finally, we write the new evaluator, which extends Evaluator and supports the new type:&lt;/p&gt;
    &lt;code&gt;class EvaluatorWithFunctionCall : public ExprVisitorWithFunctionCall,
                                  public Evaluator {
public:
  void VisitFunctionCall(const FunctionCall&amp;amp; fc) {
    std::cout &amp;lt;&amp;lt; "Visiting FunctionCall!!\n";
  }
};
&lt;/code&gt;
    &lt;p&gt;Multiple inheritance, virtual inheritance, dynamic type checking... that's pretty hard-core C++ we have to use here, but there's no choice. Unfortunately, multiple inheritance is the only way C++ lets us express the idea that a class implements some interface while at the same time deriving functionality from another class. What we want to have here is an evaluator (EvaluatorWithFunctionCall) that inherits all functionality from Evaluator, and also implements the ExprVisitorWithFunctionCall interface. In Java, we could say something like:&lt;/p&gt;
    &lt;code&gt;class EvaluatorWithFunctionCall extends Evaluator implements ExprVisitor {
  // ...
}
&lt;/code&gt;
    &lt;p&gt;But in C++ virtual multiple inheritance is the tool we have. The virtual part of the inheritance is essential here for the compiler to figure out that the ExprVisitor base underlying both Evaluator and ExprVisitorWithFunctionCall is the same and should only appear once in EvaluatorWithFunctionCall. Without virtual, the compiler would complain that EvaluatorWithFunctionCall doesn't implement the ExprVisitor interface.&lt;/p&gt;
    &lt;p&gt;This is a solution, alright. We kinda added a new type FunctionCall and can now visit it without changing existing code (assuming the virtual inheritance was built into the design from the start to anticipate this approach). Here I am using this "kinda" word again... it's time to explain why.&lt;/p&gt;
    &lt;p&gt;This approach has multiple flaws, in my opinion:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Note the dynamic_cast in FunctionCall::Accept. It's fairly ugly that we're forced to mix in dynamic checks into this code, which should supposedly rely on static typing and the compiler. But it's just a sign of a larger problem.&lt;/item&gt;
      &lt;item&gt;If we have an instance of an Evaluator, it will no longer work on the whole extended expression tree since it has no understanding of FunctionCall. It's easy to say that all new evaluators should rather be EvaluatorWithFunctionCall, but we don't always control this. What about code that was already written? What about Evaluators created in third-party or library code which we have no control of?&lt;/item&gt;
      &lt;item&gt;The virtual inheritance is not the only provision we have to build into the design to support this pattern. Some visitors would need to create new, recursive visitors to process complex expressions. But we can't anticipate in advance which dynamic type of visitor needs to be created. Therefore, the visitor interface should also accept a "visitor factory" which extended visitors will supply. I know this sounds complicated, and I don't want to spend more time on this here - but the Krishnamurthi paper addresses this issue extensively in section 3.4&lt;/item&gt;
      &lt;item&gt;Finally, the solution is unwieldy for realistic applications. Adding one new type looks manageable; what about adding 15 new types, gradually over time? Imagine the horrible zoo of ExprVisitor extensions and dynamic checks this would lead to.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Yeah, programming is hard. I could go on and on about the limitations of classical OOP and how they surface in this example [3]. Instead, I'll just present how the expression problem can be solved in a language that supports multiple dispatch and separates the defintion of methods from the bodies of types they act upon.&lt;/p&gt;
    &lt;head rend="h2"&gt;Solving the expression problem in Clojure&lt;/head&gt;
    &lt;p&gt;There are a number of ways the expression problem as displayed in this article can be solved in Clojure using the language's built-in features. Let's start with the simplest one - multi-methods.&lt;/p&gt;
    &lt;p&gt;First we'll define the types as records:&lt;/p&gt;
    &lt;code&gt;(defrecord Constant [value])
(defrecord BinaryPlus [lhs rhs])
&lt;/code&gt;
    &lt;p&gt;Then, we'll define evaluate as a multimethod that dispatches upon the type of its argument, and add method implementations for Constant and BinaryPlus:&lt;/p&gt;
    &lt;code&gt;(defmulti evaluate class)

(defmethod evaluate Constant
  [c] (:value c))

(defmethod evaluate BinaryPlus
  [bp] (+ (evaluate (:lhs bp)) (evaluate (:rhs bp))))
&lt;/code&gt;
    &lt;p&gt;Now we can already evaluate expressions:&lt;/p&gt;
    &lt;code&gt;user=&amp;gt; (use 'expression.multimethod)
nil
user=&amp;gt; (evaluate (-&amp;gt;BinaryPlus (-&amp;gt;Constant 1.1) (-&amp;gt;Constant 2.2)))
3.3000000000000003
&lt;/code&gt;
    &lt;p&gt;Adding a new operation is easy. Let's add stringify:&lt;/p&gt;
    &lt;code&gt;(defmulti stringify class)

(defmethod stringify Constant
  [c] (str (:value c)))

(defmethod stringify BinaryPlus
  [bp]
  (clojure.string/join " + " [(stringify (:lhs bp))
                              (stringify (:rhs bp))]))
&lt;/code&gt;
    &lt;p&gt;Testing it:&lt;/p&gt;
    &lt;code&gt;user=&amp;gt; (stringify (-&amp;gt;BinaryPlus (-&amp;gt;Constant 1.1) (-&amp;gt;Constant 2.2)))
"1.1 + 2.2"
&lt;/code&gt;
    &lt;p&gt;How about adding new types? Suppose we want to add FunctionCall. First, we'll define the new type. For simplicity, the func field of FunctionCall is just a Clojure function. In real code it could be some sort of function object in the language we're interpreting:&lt;/p&gt;
    &lt;code&gt;(defrecord FunctionCall [func argument])
&lt;/code&gt;
    &lt;p&gt;And define how evaluate and stringify work for FunctionCall:&lt;/p&gt;
    &lt;code&gt;(defmethod evaluate FunctionCall
  [fc] ((:func fc) (evaluate (:argument fc))))

(defmethod stringify FunctionCall
  [fc] (str (clojure.repl/demunge (str (:func fc)))
            "("
            (stringify (:argument fc))
            ")"))
&lt;/code&gt;
    &lt;p&gt;Let's take it for a spin (the full code is here):&lt;/p&gt;
    &lt;code&gt;user=&amp;gt; (def callexpr (-&amp;gt;FunctionCall twice (-&amp;gt;BinaryPlus (-&amp;gt;Constant 1.1)
                                                         (-&amp;gt;Constant 2.2))))
#'user/callexpr
user=&amp;gt; (evaluate callexpr)
6.6000000000000005
user=&amp;gt; (stringify callexpr)
"expression.multimethod/twice@52e29c38(1.1 + 2.2)"
&lt;/code&gt;
    &lt;p&gt;It should be evident that the expression problem matrix for Clojure is:&lt;/p&gt;
    &lt;p&gt;We can add new ops without touching any existing code. We can also add new types without touching any existing code. The code we're adding is only the new code to handle the ops/types in question. The existing ops and types could come from a third-party library to which we don't have source access. We could still extend them for our new ops and types, without ever having to touch (or even see) the original source code [4].&lt;/p&gt;
    &lt;head rend="h2"&gt;Is multiple dispatch necessary to cleanly solve the expression problem?&lt;/head&gt;
    &lt;p&gt;I've written about multiple dispatch in Clojure before, and in the previous section we see another example of how to use the language's defmulti/defmethod constructs. But is it multiple dispatch at all? No! It's just single dispatch, really. Our ops (evaluate and stringify) dispatch on a single argument - the expression type) [5].&lt;/p&gt;
    &lt;p&gt;If we're not really using multiple dispatch, what is the secret sauce that lets us solve the expression problem so elegantly in Clojure? The answer is - open methods. Note a crucial difference between how methods are defined in C++/Java and in Clojure. In C++/Java, methods have to be part of a class and defined (or at least declared) in its body. You cannot add a method to a class without changing the class's source code.&lt;/p&gt;
    &lt;p&gt;In Clojure, you can. In fact, since data types and multimethods are orthogonal entities, this is by design. Methods simply live outside types - they are first class citizens, rather than properties of types. We don't add methods to a type, we add new methods that act upon the type. This doesn't require modifying the type's code in any way (or even having access to its code).&lt;/p&gt;
    &lt;p&gt;Some of the other popular programming languages take a middle way. In languages like Python, Ruby and JavaScript methods belong to types, but we can dynamically add, remove and replace methods in a class even after it was created. This technique is lovingly called monkey patching. While initially enticing, it can lead to big maintainability headaches in code unless we're very careful. Therefore, if I had to face the expression problem in Python I'd prefer to roll out some sort of multiple dispatch mechanism for my program rather than rely on monkey patching.&lt;/p&gt;
    &lt;head rend="h2"&gt;Another Clojure solution - using protocols&lt;/head&gt;
    &lt;p&gt;Clojure's multimethods are very general and powerful. So general, in fact, that their performance may not be optimal for the most common case - which is single dispatch based on the type of the sole method argument; note that this is exactly the kind of dispatch I'm using in this article. Therefore, starting with Clojure 1.2, user code gained the ability to define and use protocols - a language feature that was previously restricted only to built-in types.&lt;/p&gt;
    &lt;p&gt;Protocols leverage the host platform's (which in Clojure's case is mostly Java) ability to provide quick virtual dispatch, so using them is a very efficient way to implement runtime polymorphism. In addition, protocols retain enough of the flexibility of multimethods to elegantly solve the expression problem. Curiously, this was on the mind of Clojure's designers right from the start. The Clojure documentation page about protocols lists this as one of their capabilities:&lt;/p&gt;
    &lt;quote&gt;[...] Avoid the 'expression problem' by allowing independent extension of the set of types, protocols, and implementations of protocols on types, by different parties. [...] do so without wrappers/adapters&lt;/quote&gt;
    &lt;p&gt;Clojure protocols are an interesting topic, and while I'd like to spend some more time on them, this article is becoming too long as it is. So I'll leave a more thorough treatment for some later time and for now will just show how protocols can also be used to solve the expression problem we're discussing.&lt;/p&gt;
    &lt;p&gt;The type definitions remain the same:&lt;/p&gt;
    &lt;code&gt;(defrecord Constant [value])
(defrecord BinaryPlus [lhs rhs])
&lt;/code&gt;
    &lt;p&gt;However, instead of defining a multimethod for each operation, we now define a protocol. A protocol can be thought of as an interface in a language like Java, C++ or Go - a type implements an interface when it defines the set of methods declared by the interface. In this respect, Clojure's protocols are more like Go's interfaces than Java's, as we don't have to say a-priori which interfaces a type implements when we define it.&lt;/p&gt;
    &lt;p&gt;Let's start with the Evaluatable protocol, that consists of a single method - evaluate:&lt;/p&gt;
    &lt;code&gt;(defprotocol Evaluatable
  (evaluate [this]))
&lt;/code&gt;
    &lt;p&gt;Another protocol we'll define is Stringable:&lt;/p&gt;
    &lt;code&gt;(defprotocol Stringable
  (stringify [this]))
&lt;/code&gt;
    &lt;p&gt;Now we can make sure our types implement these protocols:&lt;/p&gt;
    &lt;code&gt;(extend-type Constant
  Evaluatable
    (evaluate [this] (:value this))
  Stringable
    (stringify [this] (str (:value this))))

(extend-type BinaryPlus
  Evaluatable
    (evaluate [this] (+ (evaluate (:lhs this)) (evaluate (:rhs this))))
  Stringable
    (stringify [this]
      (clojure.string/join " + " [(stringify (:lhs this))
                                  (stringify (:rhs this))])))
&lt;/code&gt;
    &lt;p&gt;The extend-type macro is a convenience wrapper around the more general extend - it lets us implement multiple protocols for a given type. A sibling macro named extend-protocol lets us implement the same protocol for multiple types in the same invocation [6].&lt;/p&gt;
    &lt;p&gt;It's fairly obvious that adding new data types is easy - just as we did above, we simply use extend-type for each new data type to implement our current protocols. But how do we add a new protocol and make sure all existing data types implement it? Once again, it's easy because we don't have to modify any existing code. Here's a new protocol:&lt;/p&gt;
    &lt;code&gt;(defprotocol Serializable
  (serialize [this]))
&lt;/code&gt;
    &lt;p&gt;And this is its implementation for the currently supported data types:&lt;/p&gt;
    &lt;code&gt;(extend-protocol Serializable
  Constant
    (serialize [this] [(type this) (:value this)])
  BinaryPlus
    (serialize [this] [(type this)
                       (serialize (:lhs this))
                       (serialize (:rhs this))]))
&lt;/code&gt;
    &lt;p&gt;This time, extending a single protocol for multiple data types - extend-protocol is the more convenient macro to use.&lt;/p&gt;
    &lt;head rend="h2"&gt;Small interfaces are extensibility-friendly&lt;/head&gt;
    &lt;p&gt;You may have noted that the protocols (interfaces) defined in the Clojure solution are very small - consisting of a single method. Since adding methods to an existing protocol is much more problematic (I'm not aware of a way to do this in Clojure), keeping protocols small is a good idea. This guideline comes up in other contexts as well; for example, it's good practice to keep interfaces in Go very minimal.&lt;/p&gt;
    &lt;p&gt;In our C++ solution, splitting the Expr interface could also be a good idea, but it wouldn't help us with the expression problem, since we can't modify which interfaces a class implements after we've defined it; in Clojure we can.&lt;/p&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[1]&lt;/cell&gt;
        &lt;cell&gt;"Types of data" and "operations" are two terms that should be fairly obvious to modern-day programmers. Philip Wadler, in his discussion of the expression problem (see the "historical perspective" section of the article) calls them "datatypes" and "functions". A famous quote from Fred Brooks's The Mythical Man Month (1975) is "Show me your flowcharts and conceal your tables, and I shall continue to be mystified. Show me your tables, and I won√¢t usually need your flowcharts; they√¢ll be obvious."&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[2]&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;Note the peculiar way in which data is passed between Visit* methods in a Expr* -&amp;gt; Value map kept in the visitor. This is due to our inability to make Visit* methods return different types in different visitors. For example, in Evaluator we'd want them to return double, but in Stringifier they'd probably return std::string. Unfortunately C++ won't let us easily mix templates and virtual functions, so we have to resort to either returning void* the C way or the method I'm using here.&lt;/p&gt;
          &lt;p&gt;Curiously, in their paper Krishnamurthi et.al. run into the same issue in the dialect of Java they're using, and propose some language extensions to solve it. Philip Wadler uses proposed Java generics in his approach.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[3]&lt;/cell&gt;
        &lt;cell&gt;I can't resist, so just in brief: IMHO inheritance is only good for a very narrow spectrum of uses, but languages like C++ hail it as the main extension mechanism of types. But inheritance is deeply flawed for many other use cases, such as implementations of interfaces. Java is a bit better in this regard, but in the end the primacy of classes and their "closed-ness" make a lot of tasks - like the expression problem - very difficult to express in a clean way.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[4]&lt;/cell&gt;
        &lt;cell&gt;In fact, there are plenty of examples in which the Clojure implementation and the standard library provide protocols that can be extended by the user for user-defined types. Extending user-written protocols and multimethods for built-in types is trivial. As an exercise, add an evaluate implementation for java.lang.Long, so that built-in integers could participate in our expression trees without requiring wrapping in a Constant.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[5]&lt;/cell&gt;
        &lt;cell&gt;FWIW, we can formulate a multiple dispatch solution to the expression problem in Clojure. The key idea is to dispatch on two things: type and operation. Just for fun, I coded a prototype that does this which you can see here. I think the approach presented in the article - each operation being its own multimethod - is preferable, though.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row&gt;
        &lt;cell&gt;[6]&lt;/cell&gt;
        &lt;cell&gt;The sharp-eyed reader will notice a cool connection to the expression problem matrix. extend-type can add a whole new row to the matrix, while extend-protocol adds a column. extend adds just a single cell.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45155877</guid></item><item><title>I am giving up on Intel and have bought an AMD Ryzen 9950X3D</title><link>https://michael.stapelberg.ch/posts/2025-09-07-bye-intel-hi-amd-9950x3d/</link><description>&lt;doc fingerprint="266d3453c9759674"&gt;
  &lt;main&gt;
    &lt;head&gt;Table of contents&lt;/head&gt;
    &lt;p&gt;The Intel 285K CPU in my high-end 2025 Linux PC died again! üò° Notably, this was the replacement CPU for the original 285K that died in March, and after reading through the reviews of Intel CPUs on my electronics store of choice, many of which (!) mention CPU replacements, I am getting the impression that Intel‚Äôs current CPUs just are not stable üòû. Therefore, I am giving up on Intel for the coming years and have bought an AMD Ryzen 9950X3D CPU instead.&lt;/p&gt;
    &lt;head rend="h2"&gt;What happened? Or: the batch job of death&lt;/head&gt;
    &lt;p&gt;On the 9th of July, I set out to experiment with layout-parser and tesseract in order to convert a collection of scanned paper documents from images into text.&lt;/p&gt;
    &lt;p&gt;I expected that offloading this task to the GPU would result in a drastic speed-up, so I attempted to build layout-parser with CUDA. Usually, it‚Äôs not required to compile software yourself on NixOS, but CUDA is non-free, so the default NixOS cache does not compile software with CUDA. (Tip: Enable the Nix Community Cache, which contains prebuilt CUDA packages, too!)&lt;/p&gt;
    &lt;p&gt;This lengthy compilation attempt failed with a weird symptom: I left for work, and after a while, my PC was no longer reachable over the network, but fans kept spinning at 100%! üò≥ At first, I suspected a Linux bug, but now I am thinking this was the first sign of the CPU being unreliable.&lt;/p&gt;
    &lt;p&gt;When the CUDA build failed, I ran the batch job without GPU offloading instead. It took about 4 hours and consumed roughly 300W constantly. You can see it on this CPU usage graph:&lt;/p&gt;
    &lt;p&gt;On the evening of the 9th, the computer still seemed to work fine.&lt;/p&gt;
    &lt;p&gt;But the next day, when I wanted to wake up my PC from suspend-to-RAM as usual, it wouldn‚Äôt wake up. Worse, even after removing the power cord and waiting a few seconds, there was no reaction to pressing the power button.&lt;/p&gt;
    &lt;p&gt;Later, I diagnosed the problem to either the mainboard and/or the CPU. The Power Supply, RAM and disk all work with different hardware. I ended up returning both the CPU and the mainboard, as I couldn‚Äôt further diagnose which of the two is broken.&lt;/p&gt;
    &lt;p&gt;To be clear: I am not saying the batch job killed the CPU. The computer was acting strangely in the morning already. But the batch job might have been what really sealed the deal.&lt;/p&gt;
    &lt;head rend="h2"&gt;No, it wasn‚Äôt the heat wave&lt;/head&gt;
    &lt;p&gt;Tom‚Äôs Hardware recently reported that ‚ÄúIntel Raptor Lake crashes are increasing with rising temperatures in record European heat wave‚Äù, which prompted some folks to blame Europe‚Äôs general lack of Air Conditioning.&lt;/p&gt;
    &lt;p&gt;But in this case, I actually did air-condition the room about half-way through the job (at about 16:00), when I noticed the room was getting hot. Here‚Äôs the temperature graph:&lt;/p&gt;
    &lt;p&gt;I would say that 25 to 28 degrees celsius are normal temperatures for computers.&lt;/p&gt;
    &lt;p&gt;I also double-checked if the CPU temperature of about 100 degrees celsius is too high, but no: this Tom‚Äôs Hardware article shows even higher temperatures, and Intel specifies a maximum of 110 degrees. So, running at ‚Äúonly‚Äù 100 degrees for a few hours should be fine.&lt;/p&gt;
    &lt;p&gt;Lastly, even if Intel CPUs were prone to crashing under high heat, they should never die.&lt;/p&gt;
    &lt;head rend="h2"&gt;Which AMD CPU to buy?&lt;/head&gt;
    &lt;p&gt;I wanted the fastest AMD CPU (for desktops, not for servers), which currently is the Ryzen 9 9950X, but there is also the Ryzen 9 9950X3D, a variant with 3D V-Cache. Depending on the use-case, the variant with or without 3D V-Cache is faster, see the comparison on Phoronix.&lt;/p&gt;
    &lt;p&gt;Ultimately, I decided for the 9950X3D model, not just because it performs better in many of the benchmarks, but also because Linux 6.13 and newer let you control whether to prefer the CPU cores with larger V-Cache or higher frequency, which sounds like an interesting capability: By changing this setting, maybe one can see how sensitive certain workloads are to extra cache.&lt;/p&gt;
    &lt;p&gt;Aside from the CPU, I also needed a new mainboard (for AMD‚Äôs socket AM5), but I kept all the other components. I ended up selecting the ASUS TUF X870+ mainboard. I usually look for low power usage in a mainboard, so I made sure to go with an X870 mainboard instead of an X870E one, because the X870E has two chipsets (both of which consume power and need cooling)! Given the context of this hardware replacement, I also like the TUF line‚Äôs focus on endurance‚Ä¶&lt;/p&gt;
    &lt;head rend="h2"&gt;Performance&lt;/head&gt;
    &lt;p&gt;The performance of the AMD 9950X3D seems to be slightly better than the Intel 285K:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Workload&lt;/cell&gt;
        &lt;cell role="head"&gt;12900K (2022)&lt;/cell&gt;
        &lt;cell role="head"&gt;285K (2025)&lt;/cell&gt;
        &lt;cell role="head"&gt;9950X3D (2025)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;build Go 1.24.3&lt;/cell&gt;
        &lt;cell&gt;‚âà35s&lt;/cell&gt;
        &lt;cell&gt;‚âà26s&lt;/cell&gt;
        &lt;cell&gt;‚âà24s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;gokrazy/rsync tests&lt;/cell&gt;
        &lt;cell&gt;‚âà0.5s&lt;/cell&gt;
        &lt;cell&gt;‚âà0.4s&lt;/cell&gt;
        &lt;cell&gt;‚âà0.5s&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;gokrazy Linux compile&lt;/cell&gt;
        &lt;cell&gt;3m 13s&lt;/cell&gt;
        &lt;cell&gt;2m 7s&lt;/cell&gt;
        &lt;cell&gt;1m 56s&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;In case you‚Äôre curious, the commands used for each workload are:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;code&gt;cd src; ./make.bash&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;make test&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;code&gt;gokr-rebuild-kernel -cross=arm64&lt;/code&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;(I have not included the gokrazy UEFI integration tests because I think there is an unrelated difference that prevents comparison of my old results with how the test runs currently.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Power consumption&lt;/head&gt;
    &lt;p&gt;In my high-end 2025 Linux PC I explained that I chose the Intel 285K CPU for its lower idle power consumption, and some folks were skeptical if AMD CPUs are really worse in that regard.&lt;/p&gt;
    &lt;p&gt;Having switched between 3 different PCs, but with identical peripherals, I can now answer the question of how the top CPUs differ in power consumption!&lt;/p&gt;
    &lt;p&gt;I picked a few representative point-in-time power values from a couple of days of usage:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;CPU&lt;/cell&gt;
        &lt;cell role="head"&gt;Mainboard&lt;/cell&gt;
        &lt;cell role="head"&gt;idle power&lt;/cell&gt;
        &lt;cell role="head"&gt;idle power with monitor&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Intel 12900k&lt;/cell&gt;
        &lt;cell&gt;ASUS PRIME Z690-A&lt;/cell&gt;
        &lt;cell&gt;40W&lt;/cell&gt;
        &lt;cell&gt;60W&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Intel 285k&lt;/cell&gt;
        &lt;cell&gt;ASUS PRIME Z890-P&lt;/cell&gt;
        &lt;cell&gt;46W&lt;/cell&gt;
        &lt;cell&gt;65W&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;AMD 9950X3D&lt;/cell&gt;
        &lt;cell&gt;ASUS TUF GAMING X870-PLUS WIFI&lt;/cell&gt;
        &lt;cell&gt;55W&lt;/cell&gt;
        &lt;cell&gt;80W&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Looking at two typical evenings, here is the power consumption of the Intel 285K:&lt;/p&gt;
    &lt;p&gt;‚Ä¶and here is the same PC setup, but with the AMD 9950X3D:&lt;/p&gt;
    &lt;p&gt;I get the general impression that the AMD CPU has higher power consumption in all regards: the baseline is higher, the spikes are higher (peak consumption) and it spikes more often / for longer.&lt;/p&gt;
    &lt;p&gt;Looking at my energy meter statistics, I usually ended up at about 9.x kWh per day for a two-person household, cooking with induction.&lt;/p&gt;
    &lt;p&gt;After switching my PC from Intel to AMD, I end up at 10-11 kWh per day.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I started buying Intel CPUs because they allowed me to build high-performance computers that ran Linux flawlessly and produced little noise. This formula worked for me over many years:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Back in 2008, I bought a mobile Intel CPU in a desktop case (article in German).&lt;/item&gt;
      &lt;item&gt;Then, in 2012, I could just buy a regular Intel CPU (i7-2600K) for my Linux PC, because they had gotten so much better in terms of power saving.&lt;/item&gt;
      &lt;item&gt;Over the years, I bought an i7-8700K, and later an i9-9900K.&lt;/item&gt;
      &lt;item&gt;The last time this formula worked out for me was with my 2022 high-end Linux PC.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;On the one hand, I‚Äôm a little sad that this era has ended. On the other hand, I have had a soft spot for AMD since I had one of their K6 CPUs in one of my early PCs and in fact, I have never stopped buying AMD CPUs (e.g. for my Ryzen 7-based Mini Server).&lt;/p&gt;
    &lt;p&gt;Maybe AMD could further improve their idle power usage in upcoming models? And, if Intel survives for long enough, maybe they succeed at stabilizing their CPU designs again? I certainly would love to see some competition in the CPU market.&lt;/p&gt;
    &lt;p&gt;Did you like this post? Subscribe to this blog‚Äôs RSS feed to not miss any new posts!&lt;/p&gt;
    &lt;p&gt;I run a blog since 2005, spreading knowledge and experience for over 20 years! :)&lt;/p&gt;
    &lt;p&gt;If you want to support my work, you can buy me a coffee.&lt;/p&gt;
    &lt;p&gt;Thank you for your support! ‚ù§Ô∏è&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45155986</guid></item><item><title>What is the origin of the private network address 192.168.*.*? (2009)</title><link>https://lists.ding.net/othersite/isoc-internet-history/2009/oct/msg00000.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45156826</guid></item><item><title>Show HN: I'm a dermatologist and I vibe coded a skin cancer learning app</title><link>https://molecheck.info/</link><description>&lt;doc fingerprint="7fb8a917f1b73ba5"&gt;
  &lt;main&gt;
    &lt;p&gt;For the best experience, please scan the QR code with your phone's camera to use the app on your mobile device.&lt;/p&gt;
    &lt;p&gt;Are you worried about this skin lesion?Swipe left (concerned) / right (not concerned) or use the buttons.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45157020</guid></item><item><title>SQLite's Use of Tcl (2017)</title><link>https://www.tcl-lang.org/community/tcl2017/assets/talk93/Paper.html</link><description>&lt;doc fingerprint="64a9edcfc230b6b"&gt;
  &lt;main&gt;
    &lt;p&gt;SQLite is a TCL extension that has escaped into the wild.&lt;/p&gt;
    &lt;p&gt;The design of SQLite was inspired by the design of TCL, both in the way it handles datatypes and in the formatting of its source code. The index use case for SQLite was in a Tcl/Tk application for an industrial company. From its inception, SQLite has always depended heavily on TCL. These days, SQLite no longer uses TCL internally and can be run separately from any TCL interpreter, and yet the SQLite development process still depends heavily on TCL.&lt;/p&gt;
    &lt;p&gt;SQLite is an SQL database engine, and the most widely used database engine in the world. SQLite is built into all cellphones as a core component and is the primary means of data persistence on phones. SQLite is also an integral part of most web browsers. SQLite is built into MacOS and is used by most of the default applications on that platform. Windows10 requires the C:\Windows\System32\winsqlite3.dll file in order to boot. Countless other popular applications like Skype and WhatsApp and iTunes depend on SQLite.&lt;/p&gt;
    &lt;p&gt;Because SQLite is open source and can be freely downloaded and duplicated, exact usage numbers are unavailable. But reasonable estimates are that there are more SQLite instances in operation today than there are people on earth. Most devices that use the SQLite database engine contain hundreds of separate databases, and there are billions of such devices. Hence, there are likely around one trillion SQLite databases in active use.&lt;/p&gt;
    &lt;p&gt;There are more copies of SQLite in use than there are copies of Linux. We know this because SQLite is used on almost all Linux systems, but SQLite is also used on many other non-linux systems such as Macs, iPhones, and Windows computers. By similar arguments, there are more copies of SQLite in use than there are Macs, or Windows PCs. There are probably more copies of SQLite in use than all other database engines combined. It seems likely that SQLite is the most widely used and deployed software component of any kind, with the possible exception of the zLib compression library.&lt;/p&gt;
    &lt;p&gt;SQLite is not written in TCL. Rather, SQLite is intended to be used by TCL. Like the TCL interpreter, SQLite is written in ANSI C.&lt;/p&gt;
    &lt;p&gt;The fact that SQLite was primarily intended to be used from TCL is evident in an number of ways.&lt;/p&gt;
    &lt;p&gt;All programming languages, other than C/C++, require some kind of adaptor in order to interface with the SQLite C implementation. Language adaptors for SQLite are widely available from third party programmers. The adaptors for PHP and Python are built into those languages, for example. A Java adaptor is baked into Android. And so forth. Only the TCL adaptor is included as part of the SQLite core. The source code file that implements the TCL adaptor for SQLite, "tclsqlite.c", was part of the very first check-in on the SQLite source repository on 2000-05-29. (See https://sqlite.org/src/ci/6f3655f79f9b6fc9.)&lt;/p&gt;
    &lt;p&gt;All modern SQL implementations provide a means to do late binding of parameter values to the SQL statements. Usually a naked "?" character, or a "?" followed by an integer is used. For example:&lt;/p&gt;
    &lt;quote&gt;SELECT passwd, photo FROM user WHERE uid=?1&lt;/quote&gt;
    &lt;p&gt;The "?1" token in the SQL above would be assigned a value at run-time in order to look up the password and photograph of a particular user.&lt;/p&gt;
    &lt;p&gt;SQLite supports this syntax. But because of its TCL heritage, SQLite also allows the parameter to take the form of a TCL variable. Hence:&lt;/p&gt;
    &lt;quote&gt;SELECT passwd, photo FROM user WHERE uid=$uid&lt;/quote&gt;
    &lt;p&gt;When a statement such as the above is run, the TCL language adaptor automatically binds the value of the $uid TCL variable to the SQL statement before it is evaluated, providing an intuitive and seamless interface between TCL and SQL. SQLite is the only database engine that behaves this way.&lt;/p&gt;
    &lt;p&gt;The TCL heritage of SQLite is visible in the type system of SQLite. Early versions of SQLite (prior to 2004) operated on the classic TCL principal that "everything is a string". Beginning with SQLite3 (2004-06-18), SQLite also supports binary data. However, types are still very flexible in SQLite, just as they are in TCL. SQLite treats the datatypes on column names in a CREATE TABLE statement as suggestions rather than hard requirements. SQLite is happy to store a 100KB string value in a column that is declared "SHORT INT", just as TCL is happy to store either a large string or a small integer in the same variable. There are some differences in how SQLite deals with datatypes, in comparison to TCL, due to the different nature of the SQL language. SQLite has the concept of "type affinity". If a column is declared "INT" and one inserts a string into that column that looks like an integer and can be safely converted into an integer without loss of information, then that conversion occurs automatically. This provides a measure of compatibility with the rigid type systems of other SQL database engines.&lt;/p&gt;
    &lt;p&gt;The flexible type system of SQLite seems natural and intuitive to programmers with prior experience programming in TCL. Curiously, though, it is a source of frustration and frequent complaining from programmers accustomed to the rigid and unforgiving type systems of languages like Java.&lt;/p&gt;
    &lt;p&gt;The similarities in the type systems of TCL and SQLite extends to more than just the interface. An important part of the C/C++ interface for SQLite is the "sqlite3_value" object (https://sqlite.org/c3ref/value.html) which is analogous to the Tcl_Obj object in TCL. Both TCL and SQLite use a dual-representation approach, where each value can be represented simultaneously as both a string and some other type.&lt;/p&gt;
    &lt;p&gt;SQLite began as a TCL extension, though these days most uses of SQLite are in applications written in languages other than TCL. Many programmers who use SQLite in their applications have no knowledge or awareness of TCL. The SQLite source code used by most developers is a single file of pure C code named "sqlite3.c" that contains no TCL code. This is what we mean when we say that SQLite as "escaped" into the wild. Deployed instances of SQLite no longer depends on TCL.&lt;/p&gt;
    &lt;p&gt;Nevertheless, SQLite is still heavily dependent upon TCL and the ongoing support, maintenance, and enhancement of SQLite would not be possible without TCL, and would be seriously inconvenienced without Tk.&lt;/p&gt;
    &lt;p&gt;The deliverable source code for SQLite is a single file named "sqlite3.c" and its companion header "sqlite3.h". Both files are 100% ANSI-C code. But developers do not edit these files directly. The sqlite3.c and sqlite3.h source files are build products, and the source tree used to build those files is over 50% TCL code. Figure 1 nearby shows the exact ratios.&lt;/p&gt;
    &lt;p&gt;Figure 1 is for the main SQLite source repository. Many of the test cases and much of the documentation is held in separate repositories, not included in Figure 1. The separate repositories also contain a great deal of TCL code.&lt;/p&gt;
    &lt;p&gt;Much of the TCL code in the main SQLite repository consists of test scripts. At this writing, the core repository contains 1153 separate test scripts totally about 389 KB of space. But this is not the only use of TCL in SQLite.&lt;/p&gt;
    &lt;p&gt;A non-trivial amount of the deliverable C code for SQLite is machine generated. Some of the machine generated code is created by C programs, such as LEMON which translates the SQL language grammar into C code to implement a push-down automaton to parse the SQL language. But much of the automatically generated code is created using TCL scripts. TCL is well suited for scanning source files to extract information to be merged with other files and for making mechanized edits. For example, the byte-code engine used to evaluate SQL statements inside of SQLite is implemented as a large "switch" statement inside a "for" loop, with a separate "case" for each opcode, all in the "vdbe.c" source file. At build-time, TCL scripts scan the vdbe.c source file looking for the appropriate "case" statements and then build header files that assign consecutive integers to each symbolic opcode name. (The opcodes used by the SQLite byte-code engine are not an API as they are in TCL and thus can change from one build to the next.) This mapping of symbolic opcode names into integers is not a simple as one might suppose. For reasons of optimization, there are many constraints on the specific values that are assigned to opcodes. For example, many opcodes such as OP_Add must have the same numeric value as the corresponding "+" token in the SQL language parser. Sometimes a group of related opcodes, such as the comparison operators OP_Eq, OP_Ne, OP_Lt, OP_Le, OP_Ge, and OP_Gt, need to be assigned consecutive integers in a specific order. These constraints are all handled effortlessly in TCL. Accomplishing the same with AWK would be rather more difficult.&lt;/p&gt;
    &lt;p&gt;Perhaps the most important task for TCL during the SQLite build process is constructing the SQLite amalgamation source code file. Recall that most developers use SQLite in the form of a single big file of C code named "sqlite3.c" and referred to as "the amalgamation". A TCL script named "mksqlite3c.tcl" runs in order to construct the amalgamation from over one hundred separate input files. Each of these inputs files must be added to the amalgamation in just the right order. Furthermore, the source files are edited as part of the amalgamation building process. When mksqlite3c.tcl encounters a "#include" for an SQLite header, it replaces the "#include" with a copy of that header file, taking care to make sure each header file is only included once. The mksqlite3.tcl script automatically adds the "static" keyword to internal SQLite APIs to give them file linkage, and makes other similar edits.&lt;/p&gt;
    &lt;p&gt;In addition to the core SQLite library, the SQLite source tree also contains code for several analysis and control programs. One of these programs is called "sqlite3_analyzer" (or "sqlite3_analyzer.exe" on Windows). The sqlite3_analyzer program examines an SQLite database and generates a detailed report on the disk usage by the various tables and indexes within that database. The sqlite3_analyzer program is very useful in understanding how an application is using disk space.&lt;/p&gt;
    &lt;p&gt;It turns out that sqlite3_analyzer, though disguised as an ordinary executable, is really a TCL application. The main source code file for this application is tool/spaceanal.tcl. During the build process, this script is converted into a C-language string constant (using another TCL script) and added to a very simple C-language wrapper than starts a TCL interpreter and then passes the application script to that interpreter.&lt;/p&gt;
    &lt;p&gt;The sqlite3_analyzer program could be rewritten in pure C. But that would be a lot of code. The TCL script that implements sqlite3_analyzer is less than 1000 lines long. The equivalent C program would surely be at least ten times larger.&lt;/p&gt;
    &lt;p&gt;Note that the sqlite3_analyzer utility program statically links a TCL interpreter and so does not require a TCL installation on the target computer to use. The sqlite3_analyzer utility program is used by tens of thousands of developers, most of whom do not realize that they are really running a TCL application.&lt;/p&gt;
    &lt;p&gt;One of the key features of SQLite is that it uses aviation-grade testing. The tests of SQLite, at a minimum, provide 100% modified condition/decision coverage (MC/DC) of the SQLite code, with independence. 100% MC/DC roughly means that every branch instruction at the machine code level is exercised at least once in each direction. The precise definition of MC/DC is slightly stricter than this, for example when comparing boolean vectors, but the 100% branch tests coverage definition is very close approximation. The "with independence" term means that SQLite is tested in multiple ways with test code being written and maintained by different individuals.&lt;/p&gt;
    &lt;p&gt;The amount of testing done on SQLite is fanatical. On the other hand, that level of testing is necessary for a fundamental low-level component, such as a database engine, that is used by billions of devices. If an ordinary application encounters an obscure bug, it can normally be rebooted to clear the problem. But the job of a database engine is to remember things, and so databases tend to remember their mistakes across reboots. For these reasons, it is important that SQLite have a very low bug density.&lt;/p&gt;
    &lt;p&gt;TCL is used in every aspect of SQLite testing. The test cases that are part of the primary SQLite source code repository are written in TCL. Other test cases such as TH3 and SQLLogicTest are written in C but still depend on TCL for operation.&lt;/p&gt;
    &lt;p&gt;The TH3 test suite is a set of proprietary tests for SQLite that form the primary means of achieving 100% MC/DC. TH3 is designed to run on embedded hardware without the support of a desktop operating system. TH3 consists of over 1350 test modules together with over 100 control files. The test modules are written in either C or SQL or a combination of both. The control files are text formatted in a way that easily parsed by TCL. To build a TH3 test, a TCL script is run that combines some subset of the test modules and control files into a single large C program that will automatically run the required tests. This C program is then linked against the "sqlite3.c" amalgamation and the resulting binary is moved to the target computer and executed. TCL scripts automate this entire process on all major host platforms.&lt;/p&gt;
    &lt;p&gt;To verify that the TH3 tests really do provide 100% MC/DC, special options are added to the TCL scripts that run the tests, causing the GCC coverage analysis tools to be invoked. The output of gcov is then postprocessed to reveal and report any branch instructions that were missed by the tests. The TH3 tests themselves are all implemented in C and SQL, but the operation and management of those tests is all done with TCL.&lt;/p&gt;
    &lt;p&gt;The extensive documentation for SQLite available on the SQLite website (https://sqlite.org/) is all generated by TCL. Many of the documents, such as the API reference documentation and the descriptions of the byte-code engine opcodes, are created by TCL scripts that scan C source code and extract the necessary information from the code and comments. Thus, the API documentation is largely derived from comments in the source code. Keeping the official documentation (in comments) and the source code close together helps ensure that they are in agreement.&lt;/p&gt;
    &lt;p&gt;Other whitepaper documents are generated from source files that look mostly like HTML but which contain additional TCL code embedded inside of &amp;lt;tcl&amp;gt;...&amp;lt;/tcl&amp;gt; markup. The added TCL code is used for advanced formatting techniques, for automatically creating cross-references and cross-links, and for constructing complex displays such as the popular "rail-road" syntax diagrams for SQL.&lt;/p&gt;
    &lt;p&gt;The text editor used by the primary author of SQLite is a custom editor with emacs-style key bindings that is built on top of the Tk Text widget. The "e" editor, as it is called, is cross-platform, which helps in the development of a cross-platform software library like SQLite. The "e" editor has been used for the entire 17-year history of SQLite. It has been enhanced over the years with various customizations created especially to help manage the SQLite source code.&lt;/p&gt;
    &lt;p&gt;The Fossil version control system used for the SQLite source code (and written specifically for that purpose) uses Tcl/Tk to show graphical side-by-side diffs in a separate window. When the "fossil diff --tk" command is run, Fossil generates a script to show the diff graphically and then kicks off a separate "wish" process to run that script and display the diff in a separate window. This graphical diff window has a "Save" button which will cause the Tcl/Tk code needed to reproduce itself to be written to a file. This file can be, in turn, sent to a collaborator for display. Passing around graphical diffs as ordinary text files is much simpler and easier than passing around JPEG images or text "context" diffs.&lt;/p&gt;
    &lt;p&gt;No two SQLite developers work in the same office. The team is geographically distributed. To help the team stay in touch, a custom chatroom has been created using a Tcl/Tk script. The same script works as both client and server. The chatroom is private and uses a proprietary protocol, so that developers are free to discuss sensitive matters without fear of eavesdropping. The chatroom is implemented as just over 1000 lines of Tk code, and is thus accessible and easy to customize. Among the customizations is the ability to send saved "fossil diff --tk" graphical diffs to collaborators and have the diff pop automatically on the collaborators screen. Small features like this seem trivial by themselves, but together than help the developers to work much more efficiently. These kinds of productivity-enhancing features are unavailable to users of commercial business collaboration packages such as HipChat.&lt;/p&gt;
    &lt;p&gt;SQLite is an indispensable element of most modern computer systems, and TCL is an indispensable tool used in the production and maintenance of SQLite. Thus, the computing infrastructure we enjoy today would not exist except for TCL.&lt;/p&gt;
    &lt;p&gt;As deployed, SQLite contains no TCL code. However, the design of SQLite is inspired by TCL. And TCL is used extensively in the code generation, testing, analysis, documentation, and development of SQLite. Without TCL, SQLite would not exist.&lt;/p&gt;
    &lt;p&gt;Every developer and every team has a finite number of "brain cycles" available to do their job. The fewer cycles spent messing with tools, the more cycles are available to devote towards solving the problem. So for maximum productivity, it is important to use tools that get the job done with a minimum of fuss and bother. Our 17-year experience using TCL in the SQLite project has convinced us that TCL is just such a tool. Tcl provides the most help per brain cycle of any similar technology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45158814</guid></item><item><title>The MacBook has a sensor that knows the exact angle of the screen hinge</title><link>https://twitter.com/samhenrigold/status/1964428927159382261</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We‚Äôve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info ¬© 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45158968</guid></item><item><title>Keeping secrets out of logs (2024)</title><link>https://allan.reyes.sh/posts/keeping-secrets-out-of-logs/</link><description>&lt;doc fingerprint="a622f1124d050d9c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Keeping Secrets Out of Logs&lt;/head&gt;
    &lt;p&gt;This post is about how to keep secrets out of logs, and my claim is that (like many things in security) there isn‚Äôt a singular action or silver bullet that lets you do this. I would go so far as to say that there‚Äôs not even an 80/20 rule, where one action fixes 80% of the problem. It‚Äôs not like preventing SQL injection with prepared statements or preventing buffer overflows by using memory-safe languages.&lt;/p&gt;
    &lt;p&gt;What I will offer instead, are lead bullets, of which there are many. I‚Äôm going to talk about 10 of them. They are imperfect and sometimes unreliable things that, if put in the right places and with defense-in-depth, can still give us a real good chance at succeeding. My hope is that by the end, you‚Äôll have a slightly better framework for how to reason about this problem and some new ideas to add to your kit.&lt;/p&gt;
    &lt;p&gt;Table of contents:&lt;/p&gt;
    &lt;head rend="h2"&gt;The Problem&lt;/head&gt;
    &lt;p&gt;With that, let‚Äôs dive in and set the table by talking about the problem with secrets in logs.&lt;/p&gt;
    &lt;p&gt;So, there are some problems that are annoying. And there are some problems that are difficult.&lt;/p&gt;
    &lt;p&gt;This is both. I‚Äôm gonna level with you: I absolutely hate this problem. But I‚Äôm not going to gaslight you and tell you that this is the most important thing to work on worry about, because it probably isn‚Äôt!&lt;/p&gt;
    &lt;p&gt;You have somewhere between 5 and 50 other problems in your backlog that seem more important, 1 of which you found out about this morning. But I think it‚Äôs likely that none of those problems are nearly as annoying. While researching this topic, I interviewed about a dozen other engineers and, on this point, they unanimously agreed! Nobody likes dealing with secrets in logs because it is extraordinarily annoying.&lt;/p&gt;
    &lt;p&gt;This is a problem that‚Äôs also difficult, but not even in the fun sense, like being technically complex or interesting. Once you catch sensitive data in logs, it‚Äôs usually pretty straightforward (at least in retrospect) to determine how they got there. But, it‚Äôs also surprisingly elusive to prevent, and it crops up in incredibly unexpected places and ways.&lt;/p&gt;
    &lt;p&gt;Secrets could mean lots of different things to lots of different teams, but I‚Äôll use it interchangeably with ‚Äúsensitive data‚Äù: stuff that you want to keep confidential. What‚Äôs so frustrating when breaching confidentiality in logs is the full spectrum of potential impact.&lt;/p&gt;
    &lt;p&gt;In the best case (left), you might log an isolated, internal credential, like an API key, which (kudos!) you rotate right after fixing the source of leak. The impact is minimal, and you just move on. Of course, all the way on the other end of the spectrum (right), you might log something that an attacker or inside threat could use to do some real harm.&lt;/p&gt;
    &lt;p&gt;And then somewhere in-between, where I suspect most of the incidents lie. You might log secrets that you unfortunately, can‚Äôt rotate yourself. Things like PII or your customer‚Äôs passwords, which are reused on other sites, because of course they are. And, depending on your policies, threat model, or regulations, you might choose to issue a disclosure or notification.&lt;/p&gt;
    &lt;p&gt;And it is painful.&lt;/p&gt;
    &lt;p&gt;You could be doing so many good data security practices, like secure-by-design frameworks, database and field-level encryption, zero-touch production, access control&amp;amp;mldr; but logging bypasses all of that&amp;amp;mldr; and ultimately degrades trust, in your systems and in your company. It feels unfair because it‚Äôs only a fraction of your security story.&lt;/p&gt;
    &lt;p&gt;And this is a problem that happens to companies of all sizes:&lt;/p&gt;
    &lt;p&gt;Something about ‚Äúplaintext‚Äù just kinda stings, especially as a security practitioner. It‚Äôs like&amp;amp;mldr; the most profane insult you can hurl at a security engineer. Imagine retorting with, ‚ÄúOh yea? Well, you store your passwords in plaintext!‚Äù&lt;/p&gt;
    &lt;p&gt;But logging passwords and storing them in plaintext are&amp;amp;mldr; kinda the same thing.&lt;/p&gt;
    &lt;p&gt;Because while logs are rarely or purposefully public, they‚Äôre typically afforded broader access than direct access to your databases.&lt;/p&gt;
    &lt;p&gt;Everyone knows by now that storing plaintext secrets in your database is a terrible idea. Logs, however, are still data-at-rest, and we should treat them with the same level of scrutiny.&lt;/p&gt;
    &lt;p&gt;I cherry picked those examples because they are established companies with very mature security programs. I‚Äôm not trying to throw shade; in fact, I deeply respect them for being public and transparent about this. I think this also hints that preventing secrets in logs is a deceptively difficult and frustrating problem.&lt;/p&gt;
    &lt;p&gt;If we can understand some causes, we might gain a deeper appreciation for these past occurrences, and stand a better chance at avoiding new incidents in the future.&lt;/p&gt;
    &lt;head rend="h2"&gt;Causes&lt;/head&gt;
    &lt;p&gt;This is certainly not comprehensive, but from my interviews and personal experience, here are six of the most common causes.&lt;/p&gt;
    &lt;head rend="h3"&gt;ü§¶ Direct logging&lt;/head&gt;
    &lt;code&gt;const temp = res.cookie["session"];

// TODO: remove after testing is done
Logger.info("session HERE", { temp });
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;Narrator: it was not removed after testing was done&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The first group is perhaps the most obvious and facepalm one: when sensitive data is directly logged. Sometimes it‚Äôs purely accidental, like the example above: someone wants to debug session cookies in their local environment and then&amp;amp;mldr; accidentally commits the code. Sometimes it comes from an uninformed position where the developer just doesn‚Äôt know any better.&lt;/p&gt;
    &lt;p&gt;These tend to be fairly easy to trace down the exact line of code or commit that introduces it. With this example, you can just grep the codebase for &lt;code&gt;session here&lt;/code&gt; and you‚Äôll find it instantly.&lt;/p&gt;
    &lt;head rend="h3"&gt;üö∞ Kitchen sinks&lt;/head&gt;
    &lt;code&gt;const client = googleSdk.admin(...);
try {
  const res = client.tokens.list(...);
} catch (e) {
  Logger.error("failed fetch", { e });
}
&lt;/code&gt;
    &lt;p&gt;I‚Äôm sure you‚Äôve seen or written code like this before. Here we have an API client or SDK that is used to fetch some data. Exceptions are caught, kind of, and then promptly logged so that on-call engineers can debug the errors.&lt;/p&gt;
    &lt;p&gt;What happens?&lt;/p&gt;
    &lt;p&gt;That error is decorated with a config object stuffed with secrets and the full response object, which is also stuffed with secrets, and now they‚Äôre both in your logs!&lt;/p&gt;
    &lt;code&gt;{
  e: {
    status: 400,
    ...
    config: { üí•‚ò†Ô∏èü™¶ },
    response: { üí£üò≠üò± },
  }
}
&lt;/code&gt;
    &lt;p&gt;I call these ‚Äúkitchen sinks,‚Äù objects that contain or hold secrets, often in opaque or unexpected ways. Think of an actual kitchen sink that‚Äôs filled to the brim with dirty dishes and you can‚Äôt easily tell what‚Äôs at the bottom without reaching into it. Maybe it‚Äôs a spoon, or maybe it‚Äôs knife and now you have to go to the hospital. What tends to happen is that the whole kitchen sink gets logged, and the logging library happily serializes it, including parts that were actually sensitive.&lt;/p&gt;
    &lt;p&gt;This seems to happen with code that attaches additional data onto errors, or code that logs full request and response objects. It‚Äôs typically a bit hard to catch in code review unless you know to look for them. If you are blessed with static types, seeing an &lt;code&gt;any&lt;/code&gt; type flow into logs can be a good hint that you‚Äôre
logging too much.&lt;/p&gt;
    &lt;head rend="h3"&gt;üîß Configuration changes&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;Narrator: it was not okay&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Next example: someone needs additional observability and changes a setting like the global log level. You know exactly what happens, here. This dev is about to have a bad time and find out that hope, in fact, is not a valid strategy.&lt;/p&gt;
    &lt;p&gt;We started with an observability problem. Now we also have security problem: brand new secrets are getting emitted into logs.&lt;/p&gt;
    &lt;p&gt;In that example (that totally never happened to me ever), developers built production around log levels set to &lt;code&gt;WARN&lt;/code&gt; and above, but once you flip it to
&lt;code&gt;DEBUG&lt;/code&gt;, all this new stuff comes out of the woodwork.&lt;/p&gt;
    &lt;p&gt;These type of configuration changes tend to involve a system that was built with one set of assumptions, but some kind of modification moves that system from a known state into a unknown state, introducing a new set of problems.&lt;/p&gt;
    &lt;p&gt;These often involve low-level or global utilities like logging config, HTTP middleware, or some central piece of infra like a load balancer. They tend to be singletons that are difficult or costly to test, or they crop up only at runtime. On the positive side, it‚Äôs usually loud and quick to patch, but cleanup can be kinda painful.&lt;/p&gt;
    &lt;head rend="h3"&gt;ü•ß Embedded secrets&lt;/head&gt;
    &lt;code&gt;app.get("/login/:slug", async (req, res) =&amp;gt; {
  const magicLink = req.params["slug"];
  await login({ magicLink });
});
&lt;/code&gt;
    &lt;p&gt;I completely made up this phrase, but the idea is that secrets are coupled to, embedded into, and baked into more general formats like URLs or remote procedure calls. The central idea is that it‚Äôs designed into the format and the system, and can‚Äôt easily be separated.&lt;/p&gt;
    &lt;p&gt;Say you have a magic login link handler (see above) where a user can click a link and sign into a web app. There‚Äôs nothing in that code that logs the link, but if you look at HTTP logs, it‚Äôs right there in plain view:&lt;/p&gt;
    &lt;code&gt;47.29.201.179 - - [17/Jul/2024:13:17:10 +0000] "GET /login/Uj79z1pe01...
&lt;/code&gt;
    &lt;p&gt;These types of leaks arise from fundamental designs that don‚Äôt take logging into consideration or incorrectly assume some end-to-end flow. The sensitivity gets lost out of context, and ends up getting logged in another layer, system, or service.&lt;/p&gt;
    &lt;head rend="h3"&gt;üì° Telemetry&lt;/head&gt;
    &lt;code&gt;try:
    db_name = os.getenv("DB_NAME")
    db_pass = os.getenv("DB_PASS") # ü§´ Secret!
    conn = db.connect(db_name, db_pass)
    ...
except Error as e:
    # Don't log e! Not today!!11
    Logger.error("failed to connect")
finally:
    conn.close()
&lt;/code&gt;
    &lt;p&gt;Next example: we have some Python code that‚Äôs connecting to a database, we‚Äôre specifically NOT logging the error object, and we want to ensure we always close out the connection.&lt;/p&gt;
    &lt;p&gt;How can &lt;code&gt;db_pass&lt;/code&gt; possibly make it into logs? Telemetry!&lt;/p&gt;
    &lt;p&gt;"Oops, that's a log, too!"&lt;/p&gt;
    &lt;p&gt;It turns out that things like error monitoring and analytics can totally be logs, too. I kind of cheated in the code example, because there‚Äôs no mention of telemetry in it at all, but it turns out that if you hook it up to error monitoring like Sentry (above), run-time errors send the local variable context right to the dashboard, and you can see the database password in plaintext.&lt;/p&gt;
    &lt;p&gt;These causes tend to bypass the central logging pipeline and become Yet Another Place to have to worry about secrets.&lt;/p&gt;
    &lt;head rend="h3"&gt;üï∫üèª User input&lt;/head&gt;
    &lt;p&gt;Alright, last example. Say there‚Äôs a sign in form and the entire dev team made super duper sure that the password field is totally locked down from logging, they read this super awesome post, and took care of all the causes we discussed.&lt;/p&gt;
    &lt;p&gt;What happens?&lt;/p&gt;
    &lt;p&gt;Users end up jamming passwords into the username field!&lt;/p&gt;
    &lt;p&gt;So if you ever looked at login alerts for AWS and saw usernames replaced with &lt;code&gt;HIDDEN_DUE_TO_SECURITY_REASONS&lt;/code&gt;, this is precisely why!&lt;/p&gt;
    &lt;p&gt;Everything that‚Äôs within proximity to sensitive user input tends to be radioactive. It could be a UI issue, but users are surprisingly determined to volunteer secrets in ways that you haven‚Äôt prepared for.&lt;/p&gt;
    &lt;p&gt;We‚Äôve touched on a half dozen causes, and the list of things goes on. We didn‚Äôt even talk about the wonder that is crashdumps. But, I think it‚Äôs important to zoom out and note that these are proximate causes.&lt;/p&gt;
    &lt;p&gt;I stand by my claim that there‚Äôs no silver bullet to take these all out. If we want to avoid playing whack-a-mole, we must bring out our lead bullets that address these issues at a deeper level, and prevent these kinds of things from happening.&lt;/p&gt;
    &lt;head rend="h2"&gt;Fixes (lead bullets)&lt;/head&gt;
    &lt;p&gt;So let‚Äôs dive in! We will survey 10 fixes, and the order we‚Äôll go in is somewhere between ‚Äúa dependency graph of things that build on each other‚Äù and ‚Äúfollowing the lifecycle of a secret.‚Äù Some of these are obvious or perhaps things you‚Äôre already doing, so I‚Äôll focus more on fixes that I think might be a bit newer. That said, it is worth starting with the basics.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;üìê Data architecture&lt;/item&gt;
      &lt;item&gt;üçû Data transformations&lt;/item&gt;
      &lt;item&gt;ü™® Domain primitives&lt;/item&gt;
      &lt;item&gt;üéÅ Read-once objects&lt;/item&gt;
      &lt;item&gt;üóÉÔ∏è Log formatters&lt;/item&gt;
      &lt;item&gt;üß™ Unit tests&lt;/item&gt;
      &lt;item&gt;üïµÔ∏è Sensitive data scanners&lt;/item&gt;
      &lt;item&gt;ü§ñ Log pre-processors&lt;/item&gt;
      &lt;item&gt;üîé Taint checking&lt;/item&gt;
      &lt;item&gt;ü¶∏ People&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;üìê Data architecture&lt;/head&gt;
    &lt;p&gt;Lead bullet #1 is the most basic and high-level: data architecture and understanding that this is primarily a data flow problem. And part of the solution is reducing the number of data flows and shrinking the problem space so you simply have less things to worry about and protect.&lt;/p&gt;
    &lt;p&gt;Instead of stray print statements or components that write directly to filesystem, you instead centralize all your data flows through a single stream. Make it so that there‚Äôs one and only one way to log something. If you can understand and control the data structures that enter that funnel, you can prohibit secrets from exiting it.&lt;/p&gt;
    &lt;p&gt;This has the allure of being a silver bullet, because of course if you can get to 100% of all the things we mentioned here, you‚Äôre golden! But in practice (and as we‚Äôve seen previously), that‚Äôs difficult because secrets find a way to sneak in or new outflows and side channels are created.&lt;/p&gt;
    &lt;head rend="h3"&gt;üçû Data transformations&lt;/head&gt;
    &lt;p&gt;The previous bullet was about controlling how data flows through your system, this is about transforming, slicing, and disarming that data into safer forms that can be logged. These are the data security fundamentals that you‚Äôre already familiar with and likely already doing. This is your bread and butter, so I‚Äôm not going to dive into every one. From top to bottom, this is generally arranged from awesome to meh&amp;amp;mldr; basically, by how much information is retained.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Transformation&lt;/cell&gt;
        &lt;cell role="head"&gt;Result&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Minimization&lt;/cell&gt;
        &lt;cell&gt;‚òÅ (nothing)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Redaction&lt;/cell&gt;
        &lt;cell&gt;[redacted]&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Tokenization&lt;/cell&gt;
        &lt;cell&gt;2706a40d-3d1d&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Hashing&lt;/cell&gt;
        &lt;cell&gt;daadfab322b59&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;Encryption&lt;/cell&gt;
        &lt;cell&gt;AzKt7vBE7qEuf&amp;amp;mldr;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Masking&lt;/cell&gt;
        &lt;cell&gt;¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑&lt;code&gt;5309&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;On the top, we have data minimization. The best way to not log secrets, is to not have secrets to begin with! This is everything from going passwordless to fetching only the data you need.&lt;/p&gt;
    &lt;p&gt;Redaction is the next best thing. Blanking out the secret parts and before you pass objects around in memory.&lt;/p&gt;
    &lt;p&gt;Tokenization, hashing, encryption: these all have their pros, cons, and caveats. Like&amp;amp;mldr; are you even doing it correctly?&lt;/p&gt;
    &lt;p&gt;Dead last is masking. You leave parts of the secret intact. Maybe this works for you. Maybe it doesn‚Äôt. Maybe you go straight to jail ü§∑&lt;/p&gt;
    &lt;p&gt;When these techniques work, they generally work well. But very often what happens is that they aren‚Äôt used or are used too late, after something is already logged. These have their places in our toolbox, but my claim again is one bullet isn‚Äôt enough.&lt;/p&gt;
    &lt;head rend="h3"&gt;ü™® Domain primitives&lt;/head&gt;
    &lt;p&gt;Let‚Äôs introduce lead bullet #3: domain primitives. Almost all the secrets you run across in codebases are encoded in-memory as string primitives, and I think that makes our jobs harder. Strings can be almost anything.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Strings: any sequence of bytes from&lt;/p&gt;&lt;code&gt;""&lt;/code&gt;to&lt;code&gt;"cÃ¥ÃûÃë≈•Ã∏ÕàÃòÃå hÃ∏Õù Ã≠ÃòÃä√ºÃ∂ÃúÃ´Ã¶Ã†ÕãÃÜÕ† ƒºÃµÃÆÃ§ÃüÃâÃÄÕÇ·ππÃ¥ÃùÃÇü§∑867-53-0999"&lt;/code&gt;&lt;/quote&gt;
    &lt;code&gt;const secret = "..."
&lt;/code&gt;
    &lt;p&gt;There‚Äôs very little about them‚Äî‚Äîat compile time or run-time‚Äî‚Äîthat lets you know that it‚Äôs sensitive, dangerous to log, or somehow different than any other vanilla string.&lt;/p&gt;
    &lt;p&gt;The alternative is a concept I learned from the book Secure by Design, and I think it‚Äôs one of the most powerful concepts you can add to your codebase, for logs or anything else where you want to layer in security at a fundamental level.&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;Domain primitives: ‚Äúcombines secure constructs and value objects to define the smallest building block of a domain‚Äù&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;const secret = new Secret("...")
&lt;/code&gt;
    &lt;p&gt;You use them as basic building blocks that hold secret values, and they provide security invariants and guarantees that basic string primitives simply cannot.&lt;/p&gt;
    &lt;p&gt;It‚Äôs one of the easiest things you can do. If you shift from ‚Äúany string can be a secret‚Äù to ‚Äúsecrets are secrets‚Äù, it makes things a lot easier to reason about and protect.&lt;/p&gt;
    &lt;head rend="h4"&gt;Compile-time&lt;/head&gt;
    &lt;p&gt;You can use these to great advantage at compile-time, giving developers immediate feedback right in their editors.&lt;/p&gt;
    &lt;p&gt;We can type a logging function (&lt;code&gt;log()&lt;/code&gt;) so that it never accepts secrets.
Then, we use some fetching function that returns secrets, typed as secrets (and
not as strings). If we try to log that secret, it will not compile. The type
system will not let you log this secret.&lt;/p&gt;
    &lt;code&gt;// Types
declare const brand: unique symbol;
type Secret = string &amp;amp; { [brand]: string }; // Branded type that extends string
type NotSecret&amp;lt;T&amp;gt; = T extends Secret ? never : T; // Type that excludes secrets

// Logging function
function log&amp;lt;T extends string&amp;gt;(message: NotSecret&amp;lt;T&amp;gt;) { ... };
&lt;/code&gt;
    &lt;code&gt;const message: string = "this is fine"; // üßµ string primitive
const secretz: Secret = getSecret();    // üëà domain primitive

log(message); // üëå compiles!
log(secretz); // üí• error!
&lt;/code&gt;
    &lt;quote&gt;
      &lt;p&gt;See this example in the TypeScript Playground.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I‚Äôm omitting and glossing over a ton of details here, because I don‚Äôt want you to focus on the implementation or even TypeScript, for that matter. The salient point here is that instead of tossing secret strings around, you brand them as secret types, providing useful context to both compiler and developer.&lt;/p&gt;
    &lt;head rend="h4"&gt;Run-time&lt;/head&gt;
    &lt;p&gt;It‚Äôs really easy to get started, even with code that is functionally a no-op. This is basically the simplest form I can think of‚Äîan almost empty class:&lt;/p&gt;
    &lt;code&gt;class OpenAIToken extends String { /* that could be it! */ }

const token = new OpenAIToken(...);
&lt;/code&gt;
    &lt;p&gt;It‚Äôs supposed to represent OpenAI credentials, but it‚Äôs just using and extending basic language primitives. You can introduce these objects where secrets originate, like password fields or anytime you decrypt sensitive data fetched from the database. And then layer in behaviors and invariants for where they tend to end up. You progressively start introducing these at both sources and sinks, allowing you to control where secrets should or shouldn‚Äôt go. You can embed these into data structures so you know what contains secrets. And along the way, you increase the clarity and safety of your codebase: not only can you prevent these tokens from going into logs, you can make sure you‚Äôre sending them only to OpenAI and not to some other API by accident.&lt;/p&gt;
    &lt;p&gt;I think in the long run, domain primitives are the most powerful control we have because it makes our code secure by design, but it does take some time to get there. These can easily address the direct logging cause we discussed earlier, and with some modifications can help with many more.&lt;/p&gt;
    &lt;head rend="h4"&gt;Run-time: part deux&lt;/head&gt;
    &lt;p&gt;We can extend this and make it so that the default serialization behavior is redaction.&lt;/p&gt;
    &lt;code&gt;class Secret extends String {
    toString() { return "[redacted]" } // Override!
}
&lt;/code&gt;
    &lt;code&gt;const secret = new Secret("shhh!");
console.log(secret);
&lt;/code&gt;
    &lt;code&gt;Secret: "[redacted]"
&lt;/code&gt;
    &lt;p&gt;If you try to stuff this into logs, into JSON, into kitchen sinks, into error monitoring, wherever, it‚Äôll always spit out the word ‚Äúredacted‚Äù. You have to intentionally reach for the value.&lt;/p&gt;
    &lt;p&gt;Let‚Äôs take it further. We can create a custom class with an explicit &lt;code&gt;unwrap()&lt;/code&gt;
function:&lt;/p&gt;
    &lt;code&gt;class Secret&amp;lt;T&amp;gt; {
    constructor(private readonly value: T) {}
    toString() { return "[redacted]" } // Override serialization
    unwrap() { return this.value }     // Explicit getter function
}
&lt;/code&gt;
    &lt;p&gt;There‚Äôs so many things you can do here, like maybe you want to encrypt or zero it out in memory, because that‚Äôs in your threat model. You can take this as far as you need to or are comfortable with. We‚Äôll take it just one step further.&lt;/p&gt;
    &lt;head rend="h3"&gt;üéÅ Read-once objects&lt;/head&gt;
    &lt;p&gt;There‚Äôs a bit to unpack here, but these build off domain primitives in a very powerful way.&lt;/p&gt;
    &lt;code&gt;class Secret&amp;lt;T&amp;gt; {
    private locked = false;
    constructor(private readonly value: T) {}
    toString() { return "[redacted]" }

    /* @returns the sensitive value (once and only once) */
    unwrap() {
        if (this.locked) { throw new Error("already read") }
        this.locked = true;
        return this.value;
    }
}
&lt;/code&gt;
    &lt;p&gt;These objects wrap and keep the secret safe, until you actually need it. The code in the &lt;code&gt;unwrap()&lt;/code&gt; function is the crux: there‚Äôs a latch or
lock that activates after the secret is retrieved the first time. It goes into a
‚Äúlocked‚Äù state, and any following reads result in an error that fails loudly.&lt;/p&gt;
    &lt;code&gt;const secret = getSecret();
const res = await authenticate(secret.unwrap()); // Proper usage

Logger.info(secret);          // [redacted]
Logger.info(secret.unwrap()); // üí• Error!
&lt;/code&gt;
    &lt;p&gt;Once you get a secret (from user input, database, decryption, etc.) you wrap it in a read-once object immediately and keep it wrapped for as long as you can. And for its single, intended purpose, like using it for some kind of API authentication, you unwrap the value, use it, and then the object stays locked for good. This is surprisingly effective at preventing and detecting unintentional use. It addresses and disarms many of the proximate causes that we discussed earlier.&lt;/p&gt;
    &lt;p&gt;This object pairs extremely well with static analysis. Tools like CodeQL or Semgrep can help ensure that developers aren‚Äôt bypassing any safety guarantees.&lt;/p&gt;
    &lt;p&gt;These are generally high signal, especially when you have good unit test coverage. One drawback is that read-once objects, if handled incorrectly but not necessarily unsafely, could cause errors at run-time. But I think the tradeoffs are usually worth it, especially if you complement it with testing, static analysis, and taint-checking. Speaking of which&amp;amp;mldr;&lt;/p&gt;
    &lt;head rend="h3"&gt;üîé Taint checking&lt;/head&gt;
    &lt;p&gt;I like to think of taint checking as static analysis with superpowers. I absolutely love it and the first time I used it, it was like someone just handed me a lightsaber. Quick review for the uninitiated: the general idea here is that you add taint to various sources (like database objects), and yell loudly if the data flows into certain sinks (like logs).&lt;/p&gt;
    &lt;p&gt;The red data flow trace on the right detects the secret flowing into logs. But the green path is fine, because the secret is tokenized. Let‚Äôs walk through a quick example: semgrep.dev/playground/s/4bq5L&lt;/p&gt;
    &lt;p&gt;On the left, we‚Äôve marked a couple sources like decrypt and a database fetcher. We‚Äôve also marked our logger as a sink, and the &lt;code&gt;tokenize()&lt;/code&gt; function as a
sanitizer.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;On the right in red, we can see that taint was created from the decrypt function, propagated through the &lt;code&gt;getSSN()&lt;/code&gt;function, and then flagged for going into the logs on line 18.&lt;/item&gt;
      &lt;item&gt;In blue, there‚Äôs a much shorter path where the user model from the database is tainted and then flagged for going into logs.&lt;/item&gt;
      &lt;item&gt;And then lastly, in green, we‚Äôre tokenizing the decrypted SSN, so it‚Äôs not flagging that it‚Äôs logged.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The idea that this is checking millions or more different data flows is the real magic part for me.&lt;/p&gt;
    &lt;head rend="h4"&gt;Awesome&lt;/head&gt;
    &lt;p&gt;Some of the strengths of taint analysis: obviously automation. Tracing these data flows is 100% a job for a machine. This can really help with domain primitives but also can be used standalone and can even key in on heuristics like variable names: for example, all variables containing ‚Äúpassword‚Äù. You can tie this into all of your critical tools, from code review to CI/CD.&lt;/p&gt;
    &lt;p&gt;This is especially potent against kitchen sinks and embedded secrets, because those data structures can be tainted by secret values and checked accordingly.&lt;/p&gt;
    &lt;head rend="h4"&gt;Not awesome&lt;/head&gt;
    &lt;p&gt;Some personal opinions on drawbacks: I do feel like taint checking rules tend to be a bit difficult to write. I really, really like Semgrep, but I‚Äôm also not the biggest fan of YAML.&lt;/p&gt;
    &lt;p&gt;It also turns out that data flow analysis is an NP-hard problem so for large codebases and monorepos, you likely can‚Äôt run full taint analysis on every pull request or commit. Because it runs in CI/CD and as part of change management, when it works, it can prevent the introduction of insecure logging into the codebase.&lt;/p&gt;
    &lt;p&gt;But, like all of the lead bullets we‚Äôve discussed and will discuss, they can miss. How can we handle that?&lt;/p&gt;
    &lt;head rend="h3"&gt;üóÉÔ∏è Log formatters&lt;/head&gt;
    &lt;p&gt;Let‚Äôs say we made the mistake of logging too much data with our email service:&lt;/p&gt;
    &lt;code&gt;{
  tenantId: "52902156-7fb6-4ab0-b659-6b07b80cf89a",
  email: {
    subject: "Log in to your account",
    html: '&amp;lt;a href="https://acme.com/login/98fPm..."&amp;gt;Click here&amp;lt;/a&amp;gt; to log in!',
    from: "AcmeCorp &amp;lt;[email¬†protected]&amp;gt;",
    to: "Darth Plagueis (The Wise) &amp;lt;[email¬†protected]&amp;gt;",
    ...
  },
  response: {
    status: 200,
    originalRequest: {
      headers: {
        Authorization: "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIi..."
      },
      body: '{"html": "&amp;lt;a href=\\"https://acme.com/login/98fP...\\"&amp;gt;Click..."}',
      ...
    }
    ....
  },
  ...
}
&lt;/code&gt;
    &lt;p&gt;We have a couple of our usual suspects here. Because we‚Äôre logging email contents, magic links show up in logs&amp;amp;mldr; twice! We‚Äôre also logging some kitchen sinks, like email metadata and the original request, so we have PII and authorization headers also in logs. But because this data is structured, if we can traverse these objects, it turns out that we can zero in on these leaks quite effectively.&lt;/p&gt;
    &lt;code&gt;{
  tenantId: "52902156-7fb6-4ab0-b659-6b07b80cf89a",
  email: {
    subject: "Log in to your account",
    html: '&amp;lt;a href="https://acme.com/login/REDACTED"&amp;gt;Click here&amp;lt;/a&amp;gt; to log in!',
    from: "AcmeCorp &amp;lt;[email¬†protected]&amp;gt;",
    to: "REDACTED",
    ...
  },
  response: {
    status: 200,
    originalRequest: {
      headers: "REDACTED",
      body: '{"html": "&amp;lt;a href=\\"https://acme.com/login/REDACTED\\"&amp;gt;..."}',
      ...
    }
    ....
  },
  ...
}
&lt;/code&gt;
    &lt;p&gt;If we can introspect these objects, we can scan for dangerous substrings like our login links, and then drop or redact them. Or we can drop whole values, if we know that certain paths like &lt;code&gt;email.to&lt;/code&gt; are particularly dangerous. Fields like
&lt;code&gt;request&lt;/code&gt; or &lt;code&gt;headers&lt;/code&gt; tend to be risky objects that we can also remove. We can
even drop the whole log object if it doesn‚Äôt meet some admission criteria,
or‚Äîwe can simply error out.&lt;/p&gt;
    &lt;p&gt;So, how and where do we deploy something like this? Most application loggers should have some type of middleware stack or pipeline, kinda like here on the right. These are typically configured for operations like converting objects into JSON, turning error objects into readable formats, or enriching logs by inserting useful context like network information. We can invert that, and instead of enriching with useful data, we can remove or redact sensitive data.&lt;/p&gt;
    &lt;code&gt;export const logger = createLogger({
  format: format.combine(
    transform(),
    handleErrors(),
    enrich(),

      redact(), // üëà insert here!

    truncate(),
    jsonify(),
    ...
  ),
  ...
});
&lt;/code&gt;
    &lt;p&gt;This is a type of guardrail that helps catch many of the common problems we described previously, like request headers or config objects. I‚Äôve used this with decent success and found that it works best as a rifle instead of a shotgun. Because it‚Äôs at the application tier, you can customize it for the type of data or context that each application handles. For example, we can make it so that any of our domain primitives that reach this layer are quickly detected and removed.&lt;/p&gt;
    &lt;p&gt;This is extremely cheap to introduce, but there are some trade-offs. It‚Äôs certainly more of a safety net than hard control, and a developer determined to bypass it, can and will. Steady state, I measured this at less than 1% of clock time, but there are some deeply unfortunate ways this can go wrong such as poorly written regexes and self-ReDoS.&lt;/p&gt;
    &lt;p&gt;More or less, these risks can be mitigated with solid unit-testing. Which leads us to&amp;amp;mldr;&lt;/p&gt;
    &lt;head rend="h3"&gt;üß™ Unit tests&lt;/head&gt;
    &lt;p&gt;Lead bullet #7: hooking into and using the existing test suite‚Äîthat‚Äôs already there‚Äîto our advantage. We can use several of the tools we discussed, but instead of simply detecting or redacting secrets, we can ramp up the sensitivity in our test environment to fail or error loudly.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Technique&lt;/cell&gt;
        &lt;cell role="head"&gt;Prod&lt;/cell&gt;
        &lt;cell role="head"&gt;Test&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;ü™® Domain primitives&lt;/cell&gt;
        &lt;cell&gt;Redact&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;üéÅ Read-once objects&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;üóÉÔ∏è Log formatters&lt;/cell&gt;
        &lt;cell&gt;Redact&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;üïµÔ∏è Sensitive data scanners&lt;/cell&gt;
        &lt;cell&gt;Detect&lt;/cell&gt;
        &lt;cell&gt;Error&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;I‚Äôll cover sensitive data scanners next, but many test suites are already set up to capture &lt;code&gt;stdout&lt;/code&gt; and &lt;code&gt;stderr&lt;/code&gt;, and so you can even point your scanners to
these capture buffers.&lt;/p&gt;
    &lt;p&gt;The takeaway here is that you can reap the same benefits of CI/CD and change management by catching unsafe code before it‚Äôs merged or deployed, but of course, you‚Äôre also dependent on coverage and if the right code and data paths are exercised.&lt;/p&gt;
    &lt;head rend="h3"&gt;üïµÔ∏è Sensitive data scanners&lt;/head&gt;
    &lt;p&gt;These are fairly blunt but effective tools that can discover and remove sensitive data. I‚Äôm actively going to avoid diving deep here, because it does seem like many teams and vendors focus on this as the solution. So instead, I‚Äôd like to pose a few questions that might help you reason about trade-offs:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Where and when in your logging pipeline is it most effective?&lt;/item&gt;
      &lt;item&gt;Is it a gate, in-line of the critical path, or does it scan asynchronously?&lt;/item&gt;
      &lt;item&gt;Do you simply want to detect or do you bias towards masking and redaction? How will your team handle and deal with false positives?&lt;/item&gt;
      &lt;item&gt;How far do the general, out-of-box rules take you? Can you tailor it specifically to your usage patterns?&lt;/item&gt;
      &lt;item&gt;Can you verify the credentials? Can that even keep up with log throughput?&lt;/item&gt;
      &lt;item&gt;And then perhaps what tends to be the long pole in the tent: what are the costs, and can you sample instead?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I think these tools tend to be better suited for defense-in-depth, because they presume that secrets made it into logs to begin with. They can help catch the more elusive causes we discussed like configuration changes or user input.&lt;/p&gt;
    &lt;head rend="h4"&gt;Sampling&lt;/head&gt;
    &lt;p&gt;A very brief segue into sampling. Logs tend to have a kind of power law distribution, where certain types of logs vastly outnumber others. And typically what you see is that log sources have static points in code, generally with the same type of data running through them. And so within each log type, scanning and finding a single true positive might be highly representative of that group.&lt;/p&gt;
    &lt;p&gt;And so you might run into a scenario where, given some global sample rate, you‚Äôre wasting a lot of work for high frequency logs and not even scanning lower frequency logs. I think a better alternative to a global sample rate is to aggregate logs by some heuristic like type or origin, and to ensure you hit some minimum threshold.&lt;/p&gt;
    &lt;p&gt;In practice, I‚Äôve found this difficult or impossible to configure with out-of-box solutions. I‚Äôve had to introduce additional infrastructure to help. And that‚Äôs our next lead bullet.&lt;/p&gt;
    &lt;head rend="h3"&gt;ü§ñ Log pre-processors&lt;/head&gt;
    &lt;p&gt;Second to last lead bullet, #9: log pre-processors. These sit between apps that emit logs, and the final data stores.&lt;/p&gt;
    &lt;p&gt;In the above example, something like Vector can receive and process logs from our microservices before dispatching them to DataDog or wherever logs end up. We can configure it to drop sensitive data in-place using many of the techniques we discussed before. And we can sample some subset of them and store them onto an S3 bucket, using a more powerful tool like Trufflehog or an LLM to catch and verify secrets.&lt;/p&gt;
    &lt;p&gt;The idea here is to process logs streams before they‚Äôre persisted. It doesn‚Äôt need to be Vector, chances are, you already have this existing infrastructure that‚Äôs used for deduping, aggregation, and dropping noisy debug logs. We can re-use it to prevent and detect secrets in logs. This pairs very well with sensitive data scanners that we discussed earlier, and might even unlock new ones you thought were out of reach.&lt;/p&gt;
    &lt;head rend="h3"&gt;ü¶∏ People&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúHuman practitioners are the adaptable element of complex systems. Practitioners and first line management actively adapt the system to maximize production and minimize accidents.‚Äù&lt;/p&gt;
      &lt;p&gt;-Richard Cook, https://how.complexsystems.fail/#12&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Our last stop is people. Modern software is a complex system. And while people will write the code that accidentally introduces sensitive data into logs, they‚Äôre also the ones that will report, respond, and fix them. They‚Äôll build out the systems and infrastructure that will keep these complex systems safe. And early on in your maturity story and before you‚Äôre able to build out secure-by-design frameworks, this is the lead bullet you‚Äôll most likely use the most.&lt;/p&gt;
    &lt;p&gt;The most important message I want to convey here is that your security team isn‚Äôt alone, especially if you:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;educate your teammates on secure logging design&lt;/item&gt;
      &lt;item&gt;empower them to report and address these issues&lt;/item&gt;
      &lt;item&gt;and equip them with tools that get out of their way and helps them succeed.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h3"&gt;Recap&lt;/head&gt;
    &lt;p&gt;Alright, so we‚Äôve covered lead bullets that protect code, protect data, and protect logs:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;üìê Data architecture&lt;/item&gt;
      &lt;item&gt;üçû Data transformations&lt;/item&gt;
      &lt;item&gt;ü™® Domain primitives&lt;/item&gt;
      &lt;item&gt;üéÅ Read-once objects&lt;/item&gt;
      &lt;item&gt;üóÉÔ∏è Log formatters&lt;/item&gt;
      &lt;item&gt;üß™ Unit tests&lt;/item&gt;
      &lt;item&gt;üïµÔ∏è Sensitive data scanners&lt;/item&gt;
      &lt;item&gt;ü§ñ Log pre-processors&lt;/item&gt;
      &lt;item&gt;üîé Taint checking&lt;/item&gt;
      &lt;item&gt;ü¶∏ People&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Some of these might work for you, some of these won‚Äôt, and some that we haven‚Äôt even mentioned could be a homerun for you. Maybe you have super tight control over your log schemas or maybe you‚Äôre using LLMs in a really neat and effective way. Or maybe you‚Äôre building or using a language that has first class support for controlling secrets.&lt;/p&gt;
    &lt;p&gt;These worked for me. I have some personal opinions on ones which are foundational, some that are powerful in the long-run, and some that are really easy to get started. But your story is different, so I‚Äôd like to zoom out and close out with a high-level, methodical strategy that you can apply for your security programs, and that we‚Äôll apply and walk through with an example.&lt;/p&gt;
    &lt;head rend="h2"&gt;Strategy&lt;/head&gt;
    &lt;p&gt;Here‚Äôs a general strategy:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Lay the foundation&lt;/item&gt;
      &lt;item&gt;Understand the data Ô¨Çow&lt;/item&gt;
      &lt;item&gt;Protect at chokepoints&lt;/item&gt;
      &lt;item&gt;Apply defense-in-depth&lt;/item&gt;
      &lt;item&gt;Plan for response and recovery&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I‚Äôm not shooting for a Nobel, here. You‚Äôre probably doing some of these already, and chances are, you have some type of playbook or process that looks just like this. The key idea here is to not miss the forest for the trees, and use these explicit steps to place our efforts where they‚Äôll matter most. I‚Äôll walk you through a hypothetical system and we‚Äôll apply these in order.&lt;/p&gt;
    &lt;head rend="h3"&gt;0. Lay the foundation&lt;/head&gt;
    &lt;p&gt;Step zero is the foundation. Table stakes. This is like the base tier of Maslow‚Äôs hierarchy, and we need these before we try anything else.&lt;/p&gt;
    &lt;p&gt;Developing expectations, culture, and support is a must-have. They‚Äôre easy to ignore or forget about, but can make or break success. If you work at place that hasn‚Äôt addressed these in the past, it can be quite jarring or difficult to shift that mentality.&lt;/p&gt;
    &lt;p&gt;I don‚Äôt have a ton of advice here other than making sure your org is aligned on this. It‚Äôll probably feel like it‚Äôs getting worse before it‚Äôs getting better, but that is a sign of progress. A great litmus test for a solid foundation is if developers will (or already have) come to you to report secrets they found in logs.&lt;/p&gt;
    &lt;p&gt;The second thing we‚Äôll need is to decide is what we consider a secret to begin with. I, admittedly, used secrets and sensitive data interchangeably. This may not be the case for you. It doesn‚Äôt need to be perfect or comprehensive, and maybe it‚Äôs just a framework. But employees, especially the security team, need common understanding.&lt;/p&gt;
    &lt;p&gt;The third item is technical. If our logs aren‚Äôt structured or aren‚Äôt JSON, it‚Äôll make this endeavor a lot more difficult. A lot of the techniques we discussed just won‚Äôt work. If we don‚Äôt have that central pipeline or there isn‚Äôt One and Only One Way to both dispatch and view logs, we‚Äôll have to do a lot more lifting. We‚Äôve seen a few ways that logs bypass this, but having a central pipeline should cover most of the bases.&lt;/p&gt;
    &lt;head rend="h3"&gt;1. Understand the data flow&lt;/head&gt;
    &lt;p&gt;With the foundation laid, the next best thing to do is to understand and chart out how secrets flow through your system. This is basically a Data Flow Diagram, and we‚Äôll go through a fairly modest example.&lt;/p&gt;
    &lt;p&gt;On the left, we have users that visit some type of single-page web app. Requests and data flow through an application load balancer to several web application services running in containers. This is our core compute and where all the application code runs. Let‚Äôs assume that these are disparate microservices processing all types of data, some of which are considered secret. For the most sensitive data, they use KMS to encrypt and then store the ciphertext blobs in their respective database.&lt;/p&gt;
    &lt;p&gt;And then, applications use a standard logging library to emit to stdout, which gets shipped to CloudWatch and then forwarded to Datadog. That‚Äôs the final stop, and that‚Äôs where employees, devs, support staff, etc. can view them.&lt;/p&gt;
    &lt;p&gt;I highly recommend going through an exercise like this, because not only does it force you to understand the flows and boundaries of the system, if you spend time at each node and threat model it, you end up finding a bunch of unexpected ways and places that secrets make it into logs. For example&amp;amp;mldr;&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Front-end analytics! It turns out that secrets from things like form contents to session replays could end up getting sent to your user analytics platform.&lt;/item&gt;
      &lt;item&gt;And then what about our application load balancers? These ship their HTTP logs directly to CloudWatch, so we could be logging embedded secrets in URLs, and it‚Äôs totally bypassing our application tiers.&lt;/item&gt;
      &lt;item&gt;Last surprise: error monitoring! Let‚Äôs just say that some team wired up Sentry instead of DataDog for error monitoring, because of course they did, and now you have another stream of secrets in logs.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We could go further, and we haven‚Äôt even drilled into application architecture, but I think this is a good time to move from discovery to action.&lt;/p&gt;
    &lt;head rend="h3"&gt;2. Protect at chokepoints&lt;/head&gt;
    &lt;p&gt;The next step we want to take is to protect the chokepoints. And if some flow isn‚Äôt going through that chokepoint, like our rogue team that yeeted Sentry into prod, we fix it! We can get rid of Sentry and get that team onto the paved path of our logging pipeline.&lt;/p&gt;
    &lt;p&gt;We have a very clear chokepoint; a narrow path that most logs eventually flow through. Here‚Äôs where most of our lead bullets should go.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs that chokepoint splayed out. I also added an upstream node to represent CI/CD, because that‚Äôs how code get into our apps. We can then put the bulk of our protective controls here on the critical path.&lt;/p&gt;
    &lt;p&gt;We can re-architect the app to use a single logging library and secure-by-default domain primitives. Then we could use those to build out and augment our static analysis, taint-checking, and unit tests. These give us a decent front-line defense for our logging pipeline.&lt;/p&gt;
    &lt;head rend="h3"&gt;3. Apply defense-in-depth&lt;/head&gt;
    &lt;quote&gt;
      &lt;p&gt;‚ÄúEvery preventative control should have a detective control at the same level and/or one level downstream in the architecture.‚Äù -Phil Venables, https://www.philvenables.com/post/defense-in-depth&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;The third step is about adding depth to that defense, a concept we‚Äôre all familiar with. I really like how Phil Venables crystallizes what defense-in-depth means and I think he generally gives great advice. The idea is that our controls are not simply overlapping, but mutually supportive. Something‚Äôs always got your back.&lt;/p&gt;
    &lt;p&gt;Along this chokepoint we add our downstream components, in depth. Some are preventative, while some are detective.&lt;/p&gt;
    &lt;p&gt;We can add additional protections like tokenization and read-once objects. We can add the downstream tools like our custom log formatters, and employ various sensitive data scanners at different points. And then finally, we can educate and equip our team.&lt;/p&gt;
    &lt;p&gt;This is what defense-in-depth looks like to me, and I think this maximizes chances of success.&lt;/p&gt;
    &lt;head rend="h3"&gt;4. Plan for response and recovery&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Determine the scope&lt;/item&gt;
      &lt;item&gt;Restrict access&lt;/item&gt;
      &lt;item&gt;Stop the bleeding / Ô¨Åx the source&lt;/item&gt;
      &lt;item&gt;Clean up all the places, e.g. indexes&lt;/item&gt;
      &lt;item&gt;Restore access&lt;/item&gt;
      &lt;item&gt;Do a post-mortem&lt;/item&gt;
      &lt;item&gt;Make it ~impossible to happen again&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;But, of course, if we do miss or if we manage to only detect vs. prevent, we should be prepared for response and recovery. You already know how to respond to incidents like this, so I won‚Äôt add much here, other than making sure you‚Äôre sticking to a playbook in the right order, pulling levers to restrict and restore access while you‚Äôre responding, as well as thinking about all the weird places secrets might persist in logs, like indexes.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;And that‚Äôs it. This is the culmination of our strategy, our work, and about 30 some minutes of blabber.&lt;/p&gt;
    &lt;p&gt;With a solid foundation and understanding of our data flows, we protected our chokepoints in-depth and kept secrets out of logs. We‚Äôve also introduced a lot of other strong primitives that materially improve our security program. So is that it? Is the job done?&lt;/p&gt;
    &lt;p&gt;Well, no, because the data team wired up some ETL jobs that are now spewing secrets into data lake logs, because of course they did.&lt;/p&gt;
    &lt;p&gt;Like most things in security, the job often isn‚Äôt ever done. But we have the understanding, the tools, and a strategy to fight the next fight. Keeping secrets out of logs is in your hands.&lt;/p&gt;
    &lt;p&gt;*me&lt;/p&gt;
    &lt;p&gt;If you liked what you heard, or if you hated it, I‚Äôd love to hear your story. Please, reach out! Thanks! ‚úåÔ∏è&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45160774</guid></item><item><title>How to make metals from Martian dirt</title><link>https://www.csiro.au/en/news/All/Articles/2025/August/Metals-out-of-martian-dirt</link><description>&lt;doc fingerprint="270d494f78a464bb"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Key points&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Swinburne and CSIRO researchers are investigating ways to produce native metals using materials found on Mars.&lt;/item&gt;
      &lt;item&gt;Martian settlements will require large amounts of metal that are difficult to ship from Earth.&lt;/item&gt;
      &lt;item&gt;The team have successfully produced iron using regolith simulant that mimics what is available on the Red Planet.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The idea of building settlements on Mars is a popular goal of billionaires, space agencies and interplanetary enthusiasts.&lt;/p&gt;
    &lt;p&gt;But construction demands materials, and we can't ship it all from Earth: it cost US$243 million just to send NASA's one tonne Perseverance Rover to the Red Planet.&lt;/p&gt;
    &lt;p&gt;Unless we're building a settlement for ants, we'll need much, much more stuff. So how do we get it there?&lt;/p&gt;
    &lt;p&gt;CSIRO Postdoctoral Fellow and Swinburne alum Dr Deddy Nababan has been pondering this question for years. His answer lies in the Martian dirt, known as regolith.&lt;/p&gt;
    &lt;p&gt;"Sending metals to Mars from Earth might be feasible, but it's not economical. Can you imagine bringing tonnes of metals to Mars? It's just not practical," Dr Nababan says.&lt;/p&gt;
    &lt;p&gt;"Instead, we can use what's available on Mars. It's called in-situ resource utilisation, or ISRU."&lt;lb/&gt; More specifically, Dr Nababan is looking at astrometallurgy ‚Äî making metals in space.&lt;/p&gt;
    &lt;head rend="h2"&gt;Building an off-world foundry&lt;/head&gt;
    &lt;p&gt;As it turns out, Mars has all the ingredients needed to make native metals. This includes iron-rich oxides in regolith and carbon from its thin atmosphere, which acts as a reducing agent.&lt;/p&gt;
    &lt;p&gt;Swinburne University of Technology astrometallurgist, Professor Akbar Rhamdhani, is working with Dr Nababan to test this process with regolith simulant - an artificial recreation of the stuff found of Mars.&lt;/p&gt;
    &lt;p&gt;"We picked a simulant with very similar properties to that found at Gale Crater on Mars and processed them on Earth with simulated Mars conditions. This gives us a good idea of how the process would perform off-world," he says.&lt;/p&gt;
    &lt;p&gt;The simulant is placed inside a chamber at Mars surface pressure and heated at increasing temperatures. The experiments showed pure iron metal formation around 1000¬∞C, with liquid silicon-iron alloys produced around 1400¬∞C.&lt;/p&gt;
    &lt;p&gt;"At high enough temperatures, all of the metals coalesced into one large droplet. This could then be separated from liquid slag the same way it is on Earth," Professor Rhamdhani says.&lt;/p&gt;
    &lt;p&gt;Along with Dr Nababan, Prof Rhamdhani is collaborating with CSIRO's Dr Mark Pownceby to further advance the process. They're particularly focused on making metals with zero waste, where the byproducts of the process are used to make useful items.&lt;/p&gt;
    &lt;head rend="h2"&gt;If you can't ship it, make it&lt;/head&gt;
    &lt;p&gt;ISRU is a growing area of space science because in rocket launches, every kilogram counts. While the cost of launches is going down, the demands of human exploration are immense.&lt;/p&gt;
    &lt;p&gt;But huge developments are already happening, including the first demonstration of ISRU off-world. The MOXIE experiment on board the Mars Perseverance rover produced breathable oxygen using only the carbon dioxide in the planet's atmosphere.&lt;/p&gt;
    &lt;p&gt;Metal production is the next giant leap. Prof Rhamdhani hopes Mars-made alloys could be used as shells for housing or research facilities, and in machinery for excavation.&lt;/p&gt;
    &lt;p&gt;"There are certainly challenges. We need to better understand how these alloys would perform over time, and of course whether this process can be recreated on the real Martian surface," Prof Rhamdhani says.&lt;/p&gt;
    &lt;p&gt;But in the meantime, Swinburne and its partners are doubling down. Prof Rhamdhani together with Dr Nababan and Dr Matt Shaw, another CSIRO researcher and Swinburne alum, recently delivered a 4-day bespoke workshop on astrometallurgy in South Korea. The feedback was promising.&lt;/p&gt;
    &lt;p&gt;"We're starting to see increased interest in this field globally as the world gets serious about Mars exploration," he says.&lt;/p&gt;
    &lt;p&gt;"To make it happen, we're going to need experts from many fields ‚Äî mining, engineering, geology, and much more."&lt;/p&gt;
    &lt;p&gt;For Dr Nababan, the benefits go beyond exploration. He hopes their research will also drive more efficient metallurgy here on Earth.&lt;/p&gt;
    &lt;p&gt;"By doing this, I wish that I can help the development of space exploration, and at the end it will bring good to human life here on Earth."&lt;/p&gt;
    &lt;p&gt;This was article is published from Swinburne University. Read the original article.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45161229</guid></item><item><title>No Silver Bullet: Essence and Accidents of Software Engineering (1986) [pdf]</title><link>https://www.cs.unc.edu/techreports/86-020.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45161556</guid></item><item><title>Everything from 1991 Radio Shack ad I now do with my phone (2014)</title><link>https://www.trendingbuffalo.com/life/uncle-steves-buffalo/everything-from-1991-radio-shack-ad-now/</link><description>&lt;doc fingerprint="7421b90d5be73a32"&gt;
  &lt;main&gt;
    &lt;p&gt;Some people like to spend $3 on a cup of coffee. While that sounds like a gamble I probably wouldn‚Äôt take, I‚Äôll always like to gamble‚Äì especially as little as three bucks‚Äì on what I might be able to dig up on Buffalo and Western New York, our collective past, and what it means for our future.&lt;/p&gt;
    &lt;p&gt;I recently came across a big pile of Buffalo News front sections from 1991, every day for the first three months of the year‚Ä¶ collected as the First Gulf War unfolded. $3. I probably could have chiseled the guy down a buck, but I happily paid to see what else was in those papers.&lt;/p&gt;
    &lt;p&gt;There‚Äôs plenty about a run up to the first Superbowl appearance ever for the Bills, and mixed in with the disappointment is an air of hope and expectation for what is to come. Harumph. There are also some great local ads commemorating and/or coat-tailing on the Bills success.&lt;/p&gt;
    &lt;p&gt;We‚Äôll get to those someday, but today, something much simpler. The back page of the front section on Saturday, February 16, 1991 was 4/5ths covered with a Radio Shack ad.&lt;/p&gt;
    &lt;p&gt;There are 15 electronic gimzo type items on this page, being sold from America‚Äôs Technology Store. 13 of the 15 you now always have in your pocket.&lt;/p&gt;
    &lt;p&gt;So here‚Äôs the list of what I‚Äôve replaced with my iPhone.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;All weather personal stereo, $11.88. I now use my iPhone with an Otter Box&lt;/item&gt;
      &lt;item&gt;AM/FM clock radio, $13.88. iPhone.&lt;/item&gt;
      &lt;item&gt;In-Ear Stereo Phones, $7.88. Came with iPhone.&lt;/item&gt;
      &lt;item&gt;Microthin calculator, $4.88. Swipe up on iPhone.&lt;/item&gt;
      &lt;item&gt;Tandy 1000 TL/3, $1599. I actually owned a Tandy 1000, and I used it for games and word processing. I now do most of both of those things on my phone.&lt;/item&gt;
      &lt;item&gt;VHS Camcorder, $799. iPhone.&lt;/item&gt;
      &lt;item&gt;Mobile Cellular Telephone, $199. Obvs.&lt;/item&gt;
      &lt;item&gt;Mobile CB, $49.95. Ad says ‚ÄúYou‚Äôll never drive ‚Äòalone‚Äô again!‚Äù iPhone.&lt;/item&gt;
      &lt;item&gt;20-Memory Speed-Dial phone, $29.95.&lt;/item&gt;
      &lt;item&gt;Deluxe Portable CD Player, $159.95. 80 minutes of music, or 80 hours of music? iPhone.&lt;/item&gt;
      &lt;item&gt;10-Channel Desktop Scanner, $99.55. I still have a scanner, but I have a scanner app, too. iPhone.&lt;/item&gt;
      &lt;item&gt;Easiest-to-Use Phone Answerer, $49.95. iPhone voicemail.&lt;/item&gt;
      &lt;item&gt;Handheld Cassette Tape Recorder, $29.95. I use the Voice Memo app almost daily.&lt;/item&gt;
      &lt;item&gt;BONUS REPLACEMENT: It‚Äôs not an item for sale, but at the bottom of the ad, you‚Äôre instructed to ‚Äòcheck your phone book for the Radio Shack Store nearest you.‚Äô Do you even know how to use a phone book?&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;You‚Äôd have spent $3054.82 in 1991 to buy all the stuff in this ad that you can now do with your phone. That amount is roughly equivalent to about $5100 in 2012 dollars.&lt;/p&gt;
    &lt;p&gt;The only two items on the page that my phone really can‚Äôt replace:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tiny Dual-Superhet Radar Detector, $79.95. But when is the last time you heard the term ‚Äúfuzzbuster‚Äù anyway?&lt;/item&gt;
      &lt;item&gt;3-Way speaker with massive 15‚Ä≥ Woofer, $149.95.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;It‚Äôs nothing new, but it‚Äôs a great example of the technology of only two decades ago now replaced by the 3.95 ounce bundle of plastic, glass, and processors in our pockets.&lt;/p&gt;
    &lt;p&gt;Buffalo story teller and Historian Steve Cichon brings us along as he explores the nooks and crannies of Buffalo‚Äôs past present and future, which can mean just about anything‚Äì twice a week on Trending Buffalo.&lt;/p&gt;
    &lt;p&gt;As he collects WNY‚Äôs pop culture history, Steve looks for Buffalo‚Äôs good stories and creative ways to tell them as the President and founder of Buffalo Stories LLC. He‚Äôd love to help your business tell its story. For a decade, he‚Äôs also collected and shared Buffalo‚Äôs pop culture history at staffannouncer.com. His latest book, Gimme Jimmy! The James D. Griffin Story, is available now at www.mayorgriffin.com.&lt;/p&gt;
    &lt;p&gt;steve@buffalostories.com | @SteveBuffalo | www.facebook.com/stevecichon&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45161816</guid></item><item><title>Pico CSS ‚Äì Minimal CSS Framework for Semantic HTML</title><link>https://picocss.com</link><description>&lt;doc fingerprint="be2be4ff293b3380"&gt;
  &lt;main&gt;
    &lt;head rend="h3"&gt;Class-light and Semantic&lt;/head&gt;
    &lt;p&gt;Thriving on simplicity, Pico directly styles your HTML tags, using fewer than 10 &lt;code&gt;.classes&lt;/code&gt; overall. It also comes with a class-less version for wild HTML√Ç¬†purists.&lt;/p&gt;
    &lt;p&gt;A minimalist and lightweight starter√Ç kit that prioritizes semantic√Ç syntax, making every HTML√Ç element responsive and elegant√Ç by√Ç default.&lt;/p&gt;
    &lt;p&gt;Write√Ç HTML, Add√Ç Pico√Ç CSS, and√Ç Voil√É !&lt;/p&gt;
    &lt;p&gt;14.8K&lt;/p&gt;
    &lt;p&gt;65.8K&lt;/p&gt;
    &lt;p&gt;(Last month)&lt;/p&gt;
    &lt;p&gt;12.6M&lt;/p&gt;
    &lt;p&gt;(Last month)&lt;/p&gt;
    &lt;p&gt;With just the right√Ç amount of√Ç everything, Pico is a great starting√Ç point for a√Ç clean and√Ç lightweight design√Ç system.&lt;/p&gt;
    &lt;p&gt;Thriving on simplicity, Pico directly styles your HTML tags, using fewer than 10 &lt;code&gt;.classes&lt;/code&gt; overall. It also comes with a class-less version for wild HTML√Ç¬†purists.&lt;/p&gt;
    &lt;p&gt;No extra baggage needed. Pico works seamlessly without dependencies, package√Ç managers, external files, or JavaScript, achieving elegant and straightforward styles with pure HTML√Ç markup.&lt;/p&gt;
    &lt;p&gt;Effortless elegance on every√Ç device. Pico natively scales font sizes and spacings with screen widths, resulting in a consistent and elegant look across devices. No extra classes or configuration needed.&lt;/p&gt;
    &lt;p&gt;Pico comes with two accessible, neutral color schemes out of the box: light and dark. The best part? It automatically adapts to users' &lt;code&gt;prefers-color-scheme&lt;/code&gt;, all without the use of JavaScript.&lt;/p&gt;
    &lt;p&gt;Customize Pico with over 130 CSS√Ç variables, or dive deeper by using SASS. Switch between 20 handcrafted color√Ç themes and compose with 30+ modular√Ç components to tailor the UI to your brand's look√Ç and√Ç feel.&lt;/p&gt;
    &lt;p&gt;Speed meets elegance. Unlike bulky and overcomplicated frameworks that demand extensive√Ç class overrides and JavaScript, Pico keeps your HTML lean, decreases memory usage by avoiding excessive CSS specificity, and reduces loaded files.&lt;/p&gt;
    &lt;p&gt;A strong design foundation thrives on simplicity and ease of maintenance.&lt;/p&gt;
    &lt;p&gt;√∞ Pico CSS&lt;/p&gt;
    &lt;code&gt;&amp;lt;form&amp;gt;
  &amp;lt;input type="text"&amp;gt;
  &amp;lt;button type="submit"&amp;gt;Action&amp;lt;/button&amp;gt;
&amp;lt;/form&amp;gt;&lt;/code&gt;
    &lt;p&gt;√∞¬•¬µ Utility CSS Framework&lt;/p&gt;
    &lt;code&gt;&amp;lt;div class="container display-flex my-md mx-sm"&amp;gt;
  &amp;lt;form class="form shadow-md my-md mx-sm align-center"&amp;gt;
    &amp;lt;div class="input-wrapper border-radius-sm"&amp;gt;
      &amp;lt;input type="text" class="input text-color-gray placeholder-color-light-gray focus-outline-blue"&amp;gt;
    &amp;lt;/div&amp;gt;
    &amp;lt;div class="button-wrapper border-radius-sm"&amp;gt;
      &amp;lt;button type="submit" class="button bg-color-blue text-color-white focus-light-blue hover-light-blue"&amp;gt;
        Action
      &amp;lt;/button&amp;gt;
    &amp;lt;/div&amp;gt;
  &amp;lt;/form&amp;gt;
&amp;lt;/div&amp;gt;&lt;/code&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45161855</guid></item><item><title>Taco Bell AI Drive-Thru</title><link>https://aidarwinawards.org/nominees/taco-bell-ai-drive-thru.html</link><description>&lt;doc fingerprint="be03308f8313374f"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;Taco Bell AI Drive-Thru - ‚ÄúHold the AI, Extra Chaos‚Äù&lt;/head&gt;
    &lt;p&gt;Nominee: Taco Bell Corporation for deploying voice AI ordering systems at 500+ drive-throughs and discovering that artificial intelligence meets its match at ‚Äúextra sauce, no cilantro, and make it weird.‚Äù&lt;/p&gt;
    &lt;p&gt;Reported by: Isabelle Bousquette, Technology Reporter for The Wall Street Journal - August 28, 2025.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Innovation&lt;/head&gt;
    &lt;p&gt;Taco Bell boldly deployed voice AI-powered ordering systems across more than 500 drive-through locations, convinced that artificial intelligence could finally solve humanity's greatest challenge: efficiently ordering tacos. The company's confidence was so spectacular that they rolled out the technology at massive scale, apparently believing that voice AI had conquered human speech patterns, regional accents, and the creative chaos that occurs when hungry humans interact with fast food menus.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Reality Check&lt;/head&gt;
    &lt;p&gt;The Wall Street Journal revealed that customers were not quite as enthusiastic about their robotic taco consultant as Taco Bell had hoped. The AI systems faced a perfect storm of customer complaints, system glitches, and what might charitably be described as ‚Äúcreative user interaction‚Äù‚Äîincluding customers deliberately trolling the AI with absurd orders that would make even experienced drive-thru workers question their life choices.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Strategic Reassessment&lt;/head&gt;
    &lt;p&gt;Faced with mounting evidence that artificial intelligence and natural stupidity don't mix well at the drive-thru window, Taco Bell began ‚Äúreassessing‚Äù their AI deployment. The company announced they were evaluating where AI is most effective and considering human intervention during peak periods‚Äîcorporate speak for ‚Äúour robots can't handle the breakfast rush and we're not sure why we thought they could.‚Äù&lt;/p&gt;
    &lt;head rend="h3"&gt;The Perfect Storm&lt;/head&gt;
    &lt;p&gt;This incident represents the collision of three unstoppable forces: corporate AI evangelism, the infinite creativity of hungry customers, and the fundamental reality that ordering food involves more chaos variables than training a large language model to play chess. Customers reported ‚Äúglitches and delays‚Äù, while others were ‚Äúintent on trolling the [AI] system‚Äù with absurd orders, proving that humans can out-weird artificial intelligence even when they're just trying to get a burrito.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why They're Nominated&lt;/head&gt;
    &lt;p&gt;Taco Bell achieved the perfect AI Darwin Award trifecta: spectacular overconfidence in AI capabilities, deployment at massive scale without adequate testing, and a public admission that their cutting-edge technology was defeated by the simple human desire to customise taco orders. When The Wall Street Journal reports that ‚Äúthe most transformative technology in over a century may have finally found its limit: ordering tacos‚Äù, you've achieved a special kind of technological hubris that deserves recognition. Even more remarkably, despite this spectacular AI fail, Taco Bell is reportedly still moving forward with voice AI, which they say remains a critical part of the product road map‚Äîproving that true AI confidence means never letting reality interfere with your technological roadmap.&lt;/p&gt;
    &lt;p&gt;Sources: The Wall Street Journal: Taco Bell Rethinks Future of Voice AI at the Drive-Through&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45162220</guid></item><item><title>Creative Technology: The Sound Blaster</title><link>https://www.abortretry.fail/p/the-story-of-creative-technology</link><description>&lt;doc fingerprint="a50e953b59ba8d0d"&gt;
  &lt;main&gt;
    &lt;p&gt;Sim Wong Hoo was born on the 28th of April in 1955, the tenth child in a family of twelve children (five brothers, seven sisters). His family were Singaporean Hoklo with ancestry in the southernmost area of Fujian, China, and they spoke Hokkien. He grew up in a kampung called End of Coconut Hill in Bukit Panjang, and his father, Sim Chye Thiam, was a factory worker while his mother, Tan Siok Kee, raised chickens, ducks, pigs, and rabbits, and grew fruits and herbs. The young Sim had chores around the house and around the farm as soon as he was physically able, and he often sold eggs at the local market before school classes started each day. This afforded him the ability to buy things for himself such as his harmonica when he was about 11. The harmonica was a hobby he greatly enjoyed throughout his life. He also enjoyed making his own games.&lt;/p&gt;
    &lt;p&gt;Sim graduated from Bukit Panjang Government High School and then went on to attend Ngee Ann Technical College for engineering. At the college, Sim was a member of both the harmonica troupe, consisting of thirty people, and the Practice Theatre School. In the theatre, Sim provided musical accompaniment for the school‚Äôs performances with the harmonica and the accordion, often performing his own arrangements. His two interests collided at this time in his life. When writing or arranging music, he‚Äôd only be able to hear his composition during weekly practice. Having seen a computer, he realized that a computer could allow him to hear the music precisely as written while still working on it. Sim envisioned a computer that could play music, talk, or even sing, and his earlier entrepreneurial spirit drove him to an ambitious goal: selling 100 million units of a single piece of equipment. Sim graduated in 1975 and then entered the uniformed services for his obligatory two years.&lt;/p&gt;
    &lt;p&gt;For three to four years following his service, Sim worked a brief stint on an offshore oil rig, designing computerized seismic data logging equipment. After that, he opened a computer education center at Coronation Plaza. As he was more interested in teaching and researching, he left the business work to his business partner. This wasn‚Äôt a great decision. His partner took off with all the money.&lt;/p&gt;
    &lt;p&gt;On the 1st of July in 1981, Sim founded Creative Technology with Ng Kai Wa, who had been his childhood friend and classmate in a 440 sqft shop at Pearls Center using his own savings of around $6000. The company initially did computer repair and sold parts and accessories for microcomputers. Business wasn‚Äôt great, so Sim also did some teaching. In whatever time he had left to him, he was busy developing his own products.&lt;/p&gt;
    &lt;p&gt;The first Creative product (at least, for which I can find any evidence at all) was a memory board for the Apple II. Having an understanding of the Apple II, Creative followed their memory board by producing the CUBIC 99 in 1984. This was an Apple II compatible machine with a 6502, but it also featured a Zilog Z80 for compatibility with CP/M. I am not certain how this was arranged, but I imagine that it wasn‚Äôt entirely dissimilar to the Microsoft Z80 SoftCard. Of course, this is Creative Technology, so the machine also featured a voice synthesizer allowing users to record and playback words in English or Chinese. The computer also had an optional Cubic Phone Sitter which could make and answer calls. This was the first computer to be designed and manufactured in Singapore.&lt;/p&gt;
    &lt;p&gt;The market was moving quickly, and the IBM PC had created a standard. The CUBIC CT was released in 1986 as a PC compatible, and it featured graphics and sound capabilities. This was, essentially, a multimedia PC (with a weaker CPU than that standard would later dictate) localized in the Chinese language. Unfortunately, it was too early. With nearly zero software support for anything approaching the capabilities of the CT and an even smaller local market, the product was a failure.&lt;/p&gt;
    &lt;p&gt;Realizing that the sound features of the CUBIC CT were likely more salable and supportable than the computer itself, Sim and his company chose to sell the sound card by itself as the Creative Music System (also C/MS or CT-1300). This board was built around two Philips SAA1099 chips providing 12 channels of square-wave stereo sound on a half-length 8bit ISA card, and it shipped with five 360K 5.25 inch floppy disks (Master Disk, Intelligent Organ, Sound Disk 1, Sound Disk 2, Utilities). To promote this card, Sim moved to California in 1988 and established Creative Labs. His goal was to sell at least 20,000 cards generating $1 million in revenue. The USA was the largest PC market, and he knew that sound cards were seeing good sales.&lt;/p&gt;
    &lt;p&gt;Being in the USA, Sim quickly realized that games were the software titles driving sound card sales, and this meant that he‚Äôd need new branding and software partners. The C/MS became the Game Blaster, and the included software was now just the Intelligent Organ, a test utility, a TSR, and drivers for Sierra Online games. The inclusion of those drivers was key to what would follow. Creative‚Äôs partnership with Sierra meant that some of the most popular games of the era would support the Game Blaster; ultimately, over 100 games would support the C/MS and Game Blaster. Naturally, selling a card required a store front, and Creative found a partner in Radio Shack. While the Game Blaster sold better than any Creative product before it, it didn‚Äôt overtake the Adlib.&lt;/p&gt;
    &lt;p&gt;To better compete, Creative needing something that was better than the Adlib but still compatible with it. This came in 1989 with the CT1310, better known as the Sound Blaster. The Sound Blaster offered 12-voice C/MS stereo sound, 11-voice FM synthesis with Adlib compatibility (via the Yamaha YM3812), a MIDI interface, a joystick port, microphone jack with a built-in amplifier, a stereo amplifier with volume dial, the ability to play back mono-sampled sound at up to 22kHz, and record at 12kHz. While a sample rate of 22k doesn‚Äôt seem great (because it isn‚Äôt) this did allow simultaneous output of sound effects and music in a game. Likewise, while a game port doesn‚Äôt seem like all too big a deal, it saved the buyer an extra $50 to buy one separately, and it saved an ISA slot too. The Sound Blaster was the first sound card to feature digital sample playback, and it took over the market, quickly becoming the top-selling expansion card of any kind in under a year, and Creative‚Äôs revenues hit $5.5 million. With the C/MS never having been too popular, Creative followed the CT1310 with the CT1320 which removed the C/MS chips but kept sockets for them on the card.&lt;/p&gt;
    &lt;p&gt;1989 also saw Creative release the PJS operating system and the PJ Views word processor and desktop publishing system which included support for 70,000 Chinese characters. As far as I know, these products were only released in Southeast Asia.&lt;/p&gt;
    &lt;p&gt;Announced in May of 1991, the Sound Blaster Pro, CT1330, was a major redesign of Creative‚Äôs sound card. This card used two Yamaha YM3812 chips to provide stereo sound while maintaining full backward compatibility with the original Sound Blaster and Adlib. Sample rates were increased to 22kHz for stereo, 44.1kHz for mono. A simple mixer, low-pass filter, high-pass filter, and CD-ROM interface were added. This CD-ROM interface could take multiple forms, but it was useful in pushing CD-ROMs into the mainstream. Many early CD-ROM drives were SCSI-only and that was expensive. Creative worked with MKE in Japan to produce low-cost IDE CD-ROM drives, and then included support on their cards. As for the card itself, while the card did have the AT connector, it wasn‚Äôt 16bit. The Pro was still an 8bit card. The presence of the 16bit AT connector was for additional interrupts and DMAs on the 16bit bus that supported the Multimedia PC standard from Microsoft. The Sound Blaster Pro 2 was released shortly after the original, and it replaced the YM3812s with a single YMF262. The Pro series was often sold in Multimedia Upgrade Kits where it was bundled with a CD-ROM drive and software titles. Given that CD-ROMs were quite new, these kits often represented a significant value to consumers.&lt;/p&gt;
    &lt;p&gt;This card can also be found in Tandy Multimedia PCs as the Tandy Multimedia Audio Adapter. Immediately noticeable changes were from the regular joystick port to two mini-DIN connectors compatible with the Tandy 1000 joysticks, and the addition of a mini-DIN MIDI port. For both the joystick connectors and MIDI connector, adapters were required. A less noticeable change, the Tandy card used a different bus interface chip, the CT1346, and the output amplifier could be disabled via a jumper. Finally, the card featured a high-DMA channel allocated for audio which allowed 16bit 44.1kHz mono output in Windows.&lt;/p&gt;
    &lt;p&gt;The Sound Blaster 2, or Sound Blaster Deluxe, model CT1350 was released in October of 1991. This model improved the board layout allowing for a more compact card, and it completely eliminated the C/MS chips. This model improved on its predecessor by adding auto-init to DMA allowing the card to play continuously without the crackling or pausing that was experienced on the original. The sample rate for digital audio on this card was increased to 44kHz for playback and to 15kHz for recording. With this card, a DSP upgrade was made available to owners of the original Sound Blaster, which was required for full compatibility with the Windows 3 multimedia extensions.&lt;/p&gt;
    &lt;p&gt;Creative was growing quickly, achieving an estimated 72% market share of the sound card market globally in 1992, but it was also facing significant competition. Media Vision‚Äôs Pro Audio Spectrum Plus, released in 1991, was capable of 8bit digital sampling and 16bit digital audio playback. It featured a CD-ROM interface, and it was Sound Blaster compatible. The Pro Audio Spectrum 16 of 1992 moved the company to 16bit ISA, added 16bit stereo digital audio, and featured stereo FM synthesis while maintaining full Sound Blaster compatibility. Then, there was Aztech in the more low-end market making some serious OEM deals with likes of Dell and Compaq. They entered the market in 1992 at a much lower price point and offered broad compatibility with sound cards like the Adlib, Sound Blaster 2, Sound Blaster Pro, Cover Speech Thing, Disney Sound Source, and Windows Sound System.&lt;/p&gt;
    &lt;p&gt;To answer the competition and maintain their lead, the company released the Sound Blaster 16, CT1740, in June of 1992. This was a fully 16bit sound card and featured support for 16bit 44.1kHz digital audio. Creative had partnered with E-mu Systems to offer the Wave Blaster daughter board that brought wavetable synthesis to card through the header on the top of the card. The empty socket seen on the SB16 was for the Creative Signal Processor, CT1748, which brought hardware-assisted speech synthesis, QSound audio spatialization for digital wave playback, and PCM audio compression/decompression. The SB16 was more popular than any card before it, and the wavetable daughter board was popular enough to push Creative to acquire E-mu in March of 1993 for $54 million.&lt;/p&gt;
    &lt;p&gt;Creative went public in August of 1992 and became the first Singaporean company to be listed on the NASDAQ. In September of 1992, Creative expanded into China establishing a joint venture in Beijing called Chuang Tong Multimedia Computer Ltd. Creative held 70%, NewStone 20%, and Da Heng 10%. In addition to selling the company‚Äôs multimedia hardware, the Chinese subsidiary developed and distributed CD-ROM software in the Chinese language, and sold PJS and PJ Views. The following year, Ed Esber, formerly of Ashton-Tate, joined Creative Labs as CEO, and he assembled a team that included Rich Buchanan, Gail Pomerantz, and Rich Sorkin. Of the new team in the USA, Sorkin had the most lasting impact. He began licensing programs, shortened product development cycles, and began legal endeavors to protect Creative‚Äôs intellectual property. Throughout 1993, Creative established itself Australia, Japan, the UK, and Ireland. Finally, that same year, Creative acquired ShareVision Technology who made videoconferencing technologies. Creative‚Äôs later attempts in that market didn‚Äôt make it far.&lt;/p&gt;
    &lt;p&gt;By 1994, the Sound Blaster 16 was the audio card. The company needed both a low-end product and high-end product, and so the ViBRA 16, CT2501, took the low, and the AWE32 took the high. The ViBRA was a cost-reduced, single-chip implementation of the SB16 and was frequently supplied to OEMs. Some ViBRA models included an on-board modem. The AWE32 featured the CT1748 CSP, CT1747A with OPL3 FM synth, CT1971 (EMU8000) and CT1972 (EM8011, 1MB sample ROM) wavetable synth, CT1745A mixer, CT1741 DSP, a CD-ROM interface, wavetable header, SPDIF header, and 512K of sample RAM upgradeable to 28MB via two 30-pin SIMM slots. The AWE32, CT3900, was a full-length, 16bit, ISA card. With the SB16, ViBRA, and AWE32 on the market, the company‚Äôs revenues exceeded $650 million, and the company was listed on the Singapore stock exchange.&lt;/p&gt;
    &lt;p&gt;On the 26th of October in 1994, in time for the Christmas shopping season, Creative released the 3DO Blaster. This brought 3DO games to the PC via a full-length, 16bit, ISA card. On the card was a 32bit RISC CPU, a DSP for CD audio, two graphics processors, 2MB of RAM, 1MB of ROM, 1MB of VRAM, and 32K SRAM with battery backup. The box contained two games (Shockwave, Gridders), some demos, drivers, Aldus Photostyler and Gallery Effects, a controller, manuals, the card itself, and a registration card. Of course, the 3DO blaster was not, itself, a standard VGA card. To use the 3DO Blaster, one‚Äôs PC would need to be at least a 25MHz Intel 386, have at least 4MB of RAM, a VGA card, Windows 3.1, a CD-ROM drive (either Matsushita or Creative CR-564), a Sound Blaster, and some speakers. The press release from 3DO read, in part:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;With the introduction of 3DO Blaster, Creative is targeting their extensive installed base of CD-ROM users. 3DO Blaster provides PC owners with the ultimate game platform ‚Äî exciting 3DO games recognized for unprecedented interactive realism, full-motion video, CD-quality audio and three-dimensional sound effects.&lt;/p&gt;
      &lt;p&gt;‚ÄúToday‚Äôs announcement reflects the efforts of two of the most advanced technology suppliers, Creative Technology and 3DO. The 3DO Blaster provides the advantage of Creative‚Äôs and 3DO‚Äôs innovation to the installed base of PC‚Äôs already using Creative multimedia products,‚Äù said Sim Wong Hoo, CEO and chairman of Creative Technology Ltd.&lt;/p&gt;
      &lt;p&gt;‚ÄúCreative‚Äôs and 3DO‚Äôs technologies create an advanced entertainment platform which will enhance the capabilities of PCs, and expand the imagination of users by providing them access to exciting, interactive products that fully exploit the potential of multimedia entertainment.‚Äù&lt;/p&gt;
      &lt;p&gt;Trip Hawkins, president and CEO of The 3DO Company, said today‚Äôs announcement enables his company to expand quickly and aggressively into the vast PC market. ‚ÄúCreative is the leading supplier of multimedia products for PCs, providing us with the opportunity to deliver 3DO‚Äôs advanced interactive technology to an even broader audience,‚Äù said Hawkins.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Given that the 3DO Blaster cost $399.95 and the 3DO console didn‚Äôt do too well, this product was moribund from the start.&lt;/p&gt;
    &lt;p&gt;Also in October of 1994, Creative released HansVision. This was a Chinese-language office suite for Windows, and while Windows replaced PJS, HansVision replaced PJ Views. Also in 1994, Creative acquired Digicom Systems, a modem company. This resulted in the Creative Phone Blaster in 1995. The Phone Blaster, CT3110, was largely just a ViBRA 16 with an integrated modem and a wavetable header, but it was a full-length, 16bit, ISA card. It faired better than the company‚Äôs attempts at video conferencing, but it wasn‚Äôt much of a success.&lt;/p&gt;
    &lt;p&gt;A cost reduced version of the AWE32 was released in 1995 as the Sound Blaster 32. It was roughly equivalent to the AWE32 but lacked the on-board RAM, Wave Blaster Support, and CSP. Additionally, it utilized the CQM chip from the ViBRA instead of the OPL3. The CQM (Creative Quadratic Modulation) commonly suffered audio clipping, hiss, and ringing when playing digital audio.&lt;/p&gt;
    &lt;p&gt;Esber, Buchanan, and Pomerantz left the company in 1995. They‚Äôd never really got on with the folks in Singapore, and the two groups had disagreements over the company‚Äôs strategy. Sorkin, however, was promoted to General Manager of the audio division, and then to executive VP of business development and corporate investments.&lt;/p&gt;
    &lt;p&gt;With the earlier success of the company‚Äôs CD-ROM and sound card bundles packing Matsushita, Mitsumi, and other vendors‚Äô drives, Creative had gone into the CD-ROM drive business. In 1995, the industry had a large oversupply and Creative dumped its inventory incurring a loss of $30 million, and causing the company‚Äôs share price to drop nearly 75%.&lt;/p&gt;
    &lt;p&gt;In 1995, Creative released the 3D Blaster, CT6200. This was a 3D accelerator card built around the 3DLabs GLINT 300SX processor. The GLINT 300SX was built of about a million transistors on IBM‚Äôs 3.3V, 0.5 micron process, and it was capable of about 2.5 billion operations per second. As with many cards that would follow, GLINT was designed to process Gouraud-shaded, Z-buffered, dithered triangles that were generated by an application or game and passed to GLINT via the OpenGL API (in this case CGL, and later DirectX). The chip was accompanied by 2MB (or 4MB with the 2MB daughter board) of DRAM, and this VESA Local Bus card achieved a pixel filtrate of 25MP/s. The card cost $349.95 at launch and it only handled 3D, requiring the user to have a 2D card installed and use VGA passthrough. This was roughly a year before the first Voodoo card, but shortly after the Diamond Edge 3D with an NV1 at $299 for 2MB. Given that this was a VLB card, the 3D Blaster was largely a card for 486 machines, and given the price, it didn‚Äôt sell well. As far as I am aware, there were roughly 13 game titles to support CGL. Of those, there was Rebel Moon which was exclusive to the CT6200, and even having been designed exclusively for this card, it wasn‚Äôt great. Frame rates would get quite sluggish at times, likely having been hampered by the 486 at the heart of VLB machines.&lt;/p&gt;
    &lt;p&gt;The Sound Blaster AWE64 was released in November of 1996, and it improved on the AWE32 in a few ways. First, it increased the signal to noise ratio (especially in the Gold version), and increased component integration resulting in traces that likewise avoided noise. Given increases in integration, the board also became smaller than its predecessor and decreased cost. It‚Äôs also notable that with general technological advancements made in the industry, the ICs were of a consistently higher quality than those used in earlier cards despite being less expensive. The card came in two versions; one was the standard version which later was re-branded as the Value version (CT4500) with 512K RAM, and the other was the Gold version (CT4390) with 4MB of RAM, a 20bit DAC, and separate SPDIF output. Functionally, there were two major differences between the AWE64 and AWE32. The AWE64 added WaveGuide which synthesizes instrument sounds. While the Wave Blaster is no longer supported, the AWE64 Gold does have line inputs on the rear for an external Sound Canvas or similar product. Effectively, the WaveGuide feature allowed for greater polyphony through the use of 32 extra software-emulated channels, but in practice this used more CPU time and wasn‚Äôt very popular. The other change was the removal of 30-pin SIMM slots in favor of proprietary memory daughter boards. In all other respects, the AWE64 was simply a better AWE32. For purists, the AWE64 lacks Sound Blaster Pro compatibility and genuine OPL3 FM Synthesis, but for those who want SB16 compatibility, mostly noise-free output, hassle-free plug-n-play, and General MIDI capabilities, the AWE64 is wonderful. For collectors today, however, owning a genuine AWE64 Gold will set a buyer back between $200 and $400. That price will increase for those desiring a SIMMConn (replacing the proprietary memory daughter board with a 30-pin SIMM adapter).&lt;/p&gt;
    &lt;p&gt;Creative closed 1996 with $1.6 billion in revenues, and Sorkin left the company for Elon Musk‚Äôs Zip2.&lt;/p&gt;
    &lt;p&gt;Media Vision will get its own article at some point, but the company collapsed in a scandal, and Aureal Semiconductor was formed on the 9th of November in 1995 out of the prior company‚Äôs remnants. On the 14th of July in 1997, Aureal announced the Vortex AU8820 with high quality positional audio via the company‚Äôs A3D technology. This allowed a human listener to perceive audio as coming from a rather precise location, and it had originally been developed by Crystal River Engineering for NASA‚Äôs Virtual Environment Workstation Project. Crystal River had been acquired by Aureal in May of 1996, and Aureal productized the technology. The Vortex proved to be extremely popular and its features were supported by many of the most popular gaming titles of the time: Half-Life, Unreal, Quake II, and so on.&lt;/p&gt;
    &lt;p&gt;For Creative, the arrival of the Vortex card was existential. Most of the company‚Äôs revenues came from sound cards, and the Vortex had gained the respect of gamers and audiophiles almost immediately following its release. What was worse was that its feature set was being incorporated into games where once the Sound Blaster had been the de facto standard. The fastest way to gain expertise is to buy it, and Creative bought Ensoniq in January of 1998 for $77 million. Within Creative, Ensoniq was merged with E-mu Systems. The acquisition brought the Ensoniq AudioPCI into Creative, and this was a card intended to be cheap, functional, and feature rich. It supported digital effects such as reverb, chorus, and spatial enhancement, as well as DirectSound3D, and sample-based synthesis. For the new owner, the card couldn‚Äôt have been better as it support Sound Blaster compatibility through the use of a TSR despite being a PCI card. This card was rebranded several times as the Sound Blaster PCI 64, PCI 128, Vibra PCI and so on. The Ensoniq ES1370 that powered the card became the Creative 5507, and then revised into further AC97 variants. A major downside of the card was that it ran with a 44kHz sample rate only, and thus, audio recorded at any other rate was resampled which lowered fidelity and increased CPU time. The later AC97 variants supported only 48kHz natively, and therefore likewise resampled audio. While the AudioPCI wouldn‚Äôt win over audiophiles, its low cost moved units and won the company some OEM deals.&lt;/p&gt;
    &lt;p&gt;On the 20th of January in 1998, Creative chose to remedy the mistake it had made with their first 3D accelerator, and they released the CT6670, or 3D Blaster Voodoo2. It used the PCI bus, had 8MB of 25ns EDO RAM, and like all Voodoos, supported Glide. In September the same year, the company released the 3D Blaster Voodoo Banshee AGP card (CT6750) as well as the CT6760 PCI card. Depending upon the SKU, these could come with 8MB, 12MB or 16MB of SDRAM. While using the same name, the AGP card was designed entirely by Creative, and it was the only Creative board using a 3dfx chip to be so.&lt;/p&gt;
    &lt;p&gt;In July of 1998, Creative proved to be a leader in a different market segment with the introduction of HansVision Future 2000 in schools around Singapore. HVF2K featured the HansWord word processor, the HansBrowser bidirectional English-Chinese dictionary, and the HanSight online translator of webpages. Creative had successfully implemented productivity tools on the web, and they‚Äôd done machine translation of the web. Truly outstanding for the time.&lt;/p&gt;
    &lt;p&gt;Beginning in 1997, Creative Labs optical drive bundles began featuring DVD drives and speaker sets (thanks to the acquisition of Cambridge SoundWorks), and on the 10th of March in 1998 these products dropped in price rather dramatically and were expanded in their contents. One example, the Creative Components 700 (the most expensive on offer) included Creative‚Äôs PC-DVDx2 drive, Sound Blaster AWE64, the new Graphics Blaster Exxtreme (PCI, 3DLabs Permedia 2 chip, 4MB SGRAM, 64bit data path, OpenGL, up to 1600 by 1200, 60Hz to 150Hz refresh), Creative MPEG-2/Dolby Digital decoder board, and Cambridge SoundWorks‚Äô PCWorks speaker system. This was priced at $479.99. The DVD-ROM drive was $149.99 stand-alone, and the decoder board was $169.99 stand-alone.&lt;/p&gt;
    &lt;p&gt;In August of 1998, Creative released the Sound Blaster Live! (CT4670) as a successor to the ViBRA range of sound cards. These were built around the EMU10K1 chip and supported DirectSound3D, EAX (Environmental Audio Extensions) versions 1 and 2, and featured an onboard, 64-voice, wavetable synthesizer though it did use main memory for sample storage. This was a PCI bus card, and it utilized Ensoniq‚Äôs TSR for the emulation of Adlib, Sound Blaster, and General MIDI (the adaptation of that TSR was a condition of the acquisition of Ensoniq).&lt;/p&gt;
    &lt;p&gt;1998 was a year of intense litigation for Creative. The first suit was filed by Creative against Aureal over MIDI caching patent infringements. This was followed by a counter claim of defamation and unfair competition by Aureal against Creative. Creative‚Äôs advertising of the Sound Blaster Live! then sparked more lawsuits by Aureal against Creative over claimed falsehoods. By the end of 1999, Aureal had won but had gone bankrupt as a result of legal costs. I am sure it cut quite deeply, but Creative acquired Aureal in September of 2000 for $32 million.&lt;/p&gt;
    &lt;p&gt;After 3dfx acquired STB, they began making their own cards. As a result, Creative began making, mostly, Nvidia-based cards for video and graphics. There were some exceptions. The Creative 3D Blaster Savage 4 obviously used the S3 Savage 4 chipset, and the Graphics Blaster Exxtreme used chips from 3DLabs. Possibly to prevent the sort of problem they‚Äôd had with 3dfx, Creative then acquired 3DLabs in June of 2002. From 1999 onward, Creative would release a handful of graphics cards, some did well and others didn‚Äôt, but they were no longer a substantial source of revenue for the company.&lt;/p&gt;
    &lt;p&gt;Creative had some great timing with one particular product. WinAmp brought MP3 support to the desktop in 1997, and Windows Media Player 5.2 gained MP3 support in 1998. Creative released the NOMAD MP3 player in April of 1999 for $429. In June of 1999, Napster was born, and MP3s exploded in popularity. The NOMAD connected to a user‚Äôs PC via a cradle, and that cradle attached to the PC via parallel port. The device had either 32MB or 64MB of battery backed RAM depending upon the model purchased, with more storage provided by flash media. The NOMAD also provided an FM tuner for those who wished to listen to radio, and a microphone for voice recordings. On the PC side of things, Creative provided both a CD ripper and the NOMAD Manager. The latter of which was for handling the transfer of content to the device. The box proudly claims that 64MB would provide an hour of CD-quality audio, and that‚Äôs‚Ä¶ well‚Ä¶ not true at all. MP3 encoding is quite lossy, and to compress 700MB of lossless CD audio into 64MB infers an incredibly low sample rate. An hour of audio in 64MB would absolutely not be ‚ÄúCD-quality.‚Äù Marketing aside, the NOMAD was a cool product.&lt;/p&gt;
    &lt;p&gt;The NOMAD II launched the following year, and it was well received by the press. This time, Creative used USB 1.1 instead of parallel, 32MB of internal memory, bundled 64MB Smart Media flash, and added EAX support, WMA support, a backlight for the LCD, a wired remote for controls, and slightly better microphone for voice recording. This was followed by the IIc which removed the FM tuner and offered either 64MB or 128MB of internal memory.&lt;/p&gt;
    &lt;p&gt;Creative released two further units in 2000, the NOMAD Jukebox and the NOMAD II MG. These also used USB. The II MG returned to the format of the original NOMAD, but it added equalizer presets, ID3-tag support, the wired remote, and the FM tuner returned and now featured. a sleep timer and recording. The NOMAD Jukebox was different. It was roughly the size and shape of a Discman, though slightly thicker, and had a 2.5 inch, 6GB, IDE hard disk in it. The Jukebox also had WAV support, line-in for recording, and two line-out jacks for four speaker systems like Creative‚Äôs own Cambridge SoundWorks four point surround. If NiMH batteries were being used, the Jukebox featured a DC jack, and it could charge those batteries. Given the use of spinning rust, battery life was just four hours. For adventurous folks today, the hard disk in this is upgradeable, but the disk didn‚Äôt have any identifiable partitions or formatting, and as a result the first 32MB need to be copied with something like &lt;code&gt;dd&lt;/code&gt; and then the drive inserted into the Jukebox and the format function used by holding the Play and Stop buttons (or EAX and Down on newer units) during the ‚Äúloading‚Äù sequence.&lt;/p&gt;
    &lt;p&gt;Following the 2001 crash, Creative became an increasingly audio-only company. Some Chinese/English, electronic, pocketable dictionaries would continue in Asia, but most of Creative‚Äôs other endeavors ceased. The company was focused on speakers, headphones, sound cards, and portable music players.&lt;/p&gt;
    &lt;p&gt;US patent 6928433 was awarded to Creative on the 9th of August in 2005 for the user interface of the Zen and NOMAD Jukebox MP3 Players. This patent had been applied for on the 5th of January in 2001. Creative filed suit against Apple in May of 2006, and the two companies reached a settlement in August with Apple agreeing to pay $100 million.&lt;/p&gt;
    &lt;p&gt;Time wasn‚Äôt kind to Creative. Motherboard audio had become good enough for most people, and fewer than a quarter of desktop users bought dedicated sound cards. Worse, the shift to laptops during the first decade of the new millennium meant that a majority of PC users couldn‚Äôt make use of a sound card. Creative voluntarily delisted from the NASDAQ with the last day of trading having been the 31st of August in 2007. The company continued to be listed on SGX-ST. Layoffs of some staff in Stillwater, Oklahoma followed in 2008.&lt;/p&gt;
    &lt;p&gt;In 2009, 3DLabs and Creative‚Äôs Personal Digital Entertainment divisions were combined and reformed as ZiiLABS. This division designed a series of semi-custom ARM chips with 24 to 96 processing units called StemCells. These StemCells were sort of DSPs, and video, audio, and 3D graphics tasks were handled by these coprocessors. ZiiLABS produced at least five SKUs: ZMS-05, ZMS-08, ZMS-20, ZMS-40, and ZMS-50. On the 19th of November in 2012, Creative announced that they‚Äôd licensed ZiiLABS technology and patents to Intel for $20 million, and they sold engineering resources and assets to Intel for $30 million. Creative stated in the announcement that they‚Äôd retained the patents themselves. The ZiiLABS website was online through 2023, but it later went dormant with a default tomcat page in 2024, and the domain is no longer active. From 2012 forward, the website hadn‚Äôt been updated.&lt;/p&gt;
    &lt;p&gt;Today, Creative is led by Freddy Sim (Sim Wong Hoo‚Äôs brother), Tan Jok Jin is the executive chairman, and Ng Kai Wa is vice chairman. The company‚Äôs 2024 net sales stood at $62.8 million (12% increase over 2023) with $59.4 million of that being due to audio, speakers, and headphones. The company reported a net loss of $11 million for 2024, down from $17 million in 2023. The company continues to sell Sound Blaster products including both internal and external sound cards, DACs, and amplifiers. Their speakers, headphones, and headsets sell well and have won the company some awards.&lt;/p&gt;
    &lt;p&gt;Creative rose to dominate the sound card market at a time when there weren‚Äôt many options. They made an excellent product, marketed well, and made solid relationships with software makers. The primary issue for the company was that their entire business was built around a single product category, and their attempts to break out of that category weren‚Äôt successful. With video cards, they were right on time with a decent product, but the Voodoo was superior. They pivoted and survived that transition only to have 3dfx abandon board partners. They then moved to MP3 players, saw some success, but were beaten by Apple. Today, the company continues in the same niche they once dominated, and they continue to make excellent sound cards. They are simply a much smaller company. Among retro-tech enthusiasts, however, the Sound Blaster 16, Pro, and AWE64 continue to have loyal fans.&lt;/p&gt;
    &lt;p&gt;My dear readers, many of you worked at, ran, or even founded the companies I cover here on ARF, and some of you were present at those companies for the time periods I cover. A few of you have been mentioned by name. All corrections to the record are sincerely welcome, and I would love any additional insights, corrections, or feedback. Please feel free to leave a comment.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45162501</guid></item><item><title>Taking Buildkite from a side project to a global company</title><link>https://www.valleyofdoubt.com/p/taking-buildkite-from-a-side-project</link><description>&lt;doc fingerprint="2f55911511a62ddc"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Taking Buildkite from a Side Project to a Global Company&lt;/head&gt;
    &lt;head rend="h3"&gt;How an Australian developer turned his frustration with CI tools into a 13-year journey that redefined developer tooling.&lt;/head&gt;
    &lt;p&gt;üëã Welcome to Valley of Doubt, a free weekly newsletter that goes deep into founder stories from the early days of startups. üöÄ&lt;/p&gt;
    &lt;p&gt;Keith Pitt is the co-founder and former CEO of Buildkite, a devtools company that started in Melbourne and grew to have some of Silicon Valley‚Äôs biggest companies as clients.&lt;/p&gt;
    &lt;p&gt;In this interview we dig into:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Starting Buildkite as a side project&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Running out of money and having to go back to investors&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;The challenges of growing into a venture capital valuation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Finding your product voice&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Scott Handsaker (SH): Tell me who Keith Pitt is in 30 seconds or less.&lt;/p&gt;
    &lt;p&gt;Keith Pitt (KP): I'm a dad with three kids. I'm someone who loves to explore with his hands and code, and I love exploring ideas.&lt;/p&gt;
    &lt;p&gt;It's no surprise that I ended up in developer tools. Before I was a programmer, I was a magician doing magic shows. That's all about pleasing people, bringing joy and excitement to someone's day, subverting expectations and having them walk away with a memorable experience of something you spent a lot of time working on.&lt;/p&gt;
    &lt;p&gt;Developer tools is the closest I can get to being a magician in programming, because I get to please my peers, help them be better, and bring a little magic to their day.&lt;/p&gt;
    &lt;p&gt;I don't consider myself an entrepreneur. I never have, even though I do entrepreneurial things. I consider myself more of an inventor. The way that I like to figure out if my inventions are any good is by selling them, because I feel like the only true way to determine if something is of value is if someone's bought it.&lt;/p&gt;
    &lt;p&gt;SH: Is the ‚Äúinventor‚Äù identity something that you carry with you throughout your life?&lt;/p&gt;
    &lt;p&gt;KP: Yeah, I think so. I like making stuff, whether it‚Äôs Lego with the kids or software for developers. I have a mad Lego collection, but I make something and then just pull it apart and put it in a box because I don't like displaying Lego. I like making it.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Accidental Beginning&lt;/head&gt;
    &lt;p&gt;SH: Tell me how you got started with Buildkite.&lt;/p&gt;
    &lt;p&gt;KP: It was a side project of a side project. I started building an email transaction API like Mailgun. I needed some CI for it, and at work we were transitioning off Travis CI. My personal requirements collided with my work requirements and I thought, I'll quickly just bust something out. Years earlier I‚Äôd written a CI tool just for fun, so I dusted that off.&lt;/p&gt;
    &lt;p&gt;At the same time, PIN Payments launched in Australia. It was sort of the first time in Australian development history where you could charge for something on the internet without messing around with PayPal or getting a merchant account and SOAP APIs. It was a REST API to charge credit cards.&lt;/p&gt;
    &lt;p&gt;Because I'm a commercially minded inventor, I packaged up Buildkite. I spent my evenings working on it, and I wasn‚Äôt really building it to make money. I was just building it for me and what I thought was good.&lt;/p&gt;
    &lt;p&gt;I'd go to work and show my peers, "Hey, what do you think of this?" Little did I know that I was testing product-market fit. I thought I was just working on something that I liked.&lt;/p&gt;
    &lt;p&gt;SH: When did you know you had something people wanted?&lt;/p&gt;
    &lt;p&gt;KP: I eventually strapped a credit card system on there and put it on the internet. People I knew bought it to start with, but then one company bought it that I didn't know from Europe. I think they were called Moneybird. I was like,&lt;/p&gt;
    &lt;p&gt;"Who the hell are these people?"&lt;/p&gt;
    &lt;p&gt;That's when I knew I had something that people wanted.&lt;/p&gt;
    &lt;p&gt;SH: How did pricing evolve?&lt;/p&gt;
    &lt;p&gt;KP: I started at $5, $30, and $100 a month. Just classic SaaS tiers.&lt;/p&gt;
    &lt;p&gt;I remember I was walking back from lunch and I got a ping on my phone that someone had upgraded to $500 a month. That was a moment of pure joy for me because this particular customer knew who I was, but they had believed in me. There's a lot of people telling you no throughout the whole journey, and so part of that whole experience was just ignoring people that didn't believe in you.&lt;/p&gt;
    &lt;p&gt;When that enterprise customer came through, it was the first moment that I felt like someone else believed in me. That's when I knew I would be okay.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Bootstrap Years&lt;/head&gt;
    &lt;p&gt;SH: Were you still working at Envato at that point?&lt;/p&gt;
    &lt;p&gt;KP: I was doing Buildkite after hours because my wife was studying to be a social worker at the time. I would do my day job and come home and work on Buildkite in the evenings. The work at Envato was super easy. I was on a green fields project, but my creative itches weren't being scratched at work.&lt;/p&gt;
    &lt;p&gt;I needed someone that could support my Buildkite work. I wanted to do one day a week on Buildkite, but Envato weren't too keen on a part-time position. So I ended up leaving Envato and going to work at PIN Payments. They were cool with me working four days a week.&lt;/p&gt;
    &lt;p&gt;SH: At what point did the co-founders come into it?&lt;/p&gt;
    &lt;p&gt;KP: I was two years in and still solo. I remember the moment it started. I was on Twitter and one of my competitors had announced they'd raised some money and I was really pissed off because I thought,&lt;/p&gt;
    &lt;p&gt;"Why can't I raise money? Why is no one giving me money?"&lt;/p&gt;
    &lt;p&gt;I tweeted something salty and Matt Allen from the Ruby community replied. He said, "You can raise money. I'll show you." So I started applying for accelerator programs. All of them knocked me back because they didn't want to support first-time founders. This was 13-14 years ago when the Australian startup ecosystem was pretty small and risk tolerance was basically non-existent.&lt;/p&gt;
    &lt;p&gt;They were like, "You have to find a co-founder." I went co-founder hunting but couldn't really find one. I had this weird romantic relationship with bootstrapping and going at it alone.&lt;/p&gt;
    &lt;p&gt;At that same moment, I asked Tim Lucas, who I was working with at PIN, "Hey man, can you make me a new logo for Buildkite?" Instead of making me a logo he said, "How about I be your co-founder instead?"&lt;/p&gt;
    &lt;p&gt;I said, "Okay." That was basically what it was.&lt;/p&gt;
    &lt;head rend="h2"&gt;The First Raise and Growing Pains&lt;/head&gt;
    &lt;p&gt;KP: Matt Allen helped us raise 200 grand from people we didn't know. Just a bunch of rich people, some of whom I still have not met to this day. We quit our jobs, paid ourselves 80 grand a year, and just worked on becoming profitable.&lt;/p&gt;
    &lt;p&gt;We built things and sold them, tried marketing, and made enough money to hire someone. We did this until we were 15 people, just rinse and repeat. Grow the bank balance, increase revenue, hire people.&lt;/p&gt;
    &lt;p&gt;SH: You mentioned it was a hard slog during those years. Do any particular moments stand out?&lt;/p&gt;
    &lt;p&gt;KP: I ran out of money. I stuffed up big time. In Australia you have to pay employees superannuation, and no one told me that superannuation payments were quarterly. I thought I was doing the right thing by using our cash to pre-buy EC2 instances, and then a couple weeks later our external accounting firm told me, "Super's due".&lt;/p&gt;
    &lt;p&gt;I thought, "Oh, we don't have enough money."&lt;/p&gt;
    &lt;p&gt;But I wasn't that scared because I had a plan. I thought, "Why not be open about it?" because it would probably help. I was completely open about it and said to the team, "All right, so here's the situation, but here's what we're going to do."&lt;/p&gt;
    &lt;p&gt;I converted a bunch of customers to annual payments, went back to investors for loans, and did some invoice advances. It was about 100 grand to see us through until we got R&amp;amp;D tax credits and bigger invoices.&lt;/p&gt;
    &lt;p&gt;SH: What did you learn from that experience?&lt;/p&gt;
    &lt;p&gt;KP: Cash flow is an art form in and of itself. It's not just the bank balance. It's the bank balance at any one moment in time. At that point the game changed for me to making sure that I don't run out of money. That's the number one thing.&lt;/p&gt;
    &lt;p&gt;Don't run out of money and do whatever it takes to not run out of money. Buildkite took a long time to bloom and blossom, but if we had run out of money earlier on, I might have just given up.&lt;/p&gt;
    &lt;p&gt;Brought to you by Murmar&lt;/p&gt;
    &lt;head rend="h2"&gt;Finding the Third Co-founder&lt;/head&gt;
    &lt;p&gt;SH: How did Lachlan join as the third co-founder?&lt;/p&gt;
    &lt;p&gt;KP: Lachlan joined a couple years in as a late-stage co-founder. I wasn't precious about the titles. When he joined, I thought if he does what we think he's going to do, the company that exists will be just as much of his creation as mine.&lt;/p&gt;
    &lt;p&gt;The reason he joined was that Tim and I didn't really know what we were doing. We had a good sense of how to navigate, but we didn't know big company enterprise stuff and were still thinking small fry. Lachlan really helped us charge lots of money.&lt;/p&gt;
    &lt;p&gt;I didn't have the confidence to walk into a customer and say, "This is $30,000," but Lachlan did.&lt;/p&gt;
    &lt;p&gt;That's probably the biggest gift he gave the company. He gave us the confidence to ask for more.&lt;/p&gt;
    &lt;p&gt;I didn't realise at the time that by having such a low price point, people weren't taking us seriously. For us to charge more, we had to really believe in ourselves. To walk into a company and say, "This is $100,000" with a straight face and truly believe it in your soul.&lt;/p&gt;
    &lt;p&gt;SH: How did you handle the dynamics between three founders?&lt;/p&gt;
    &lt;p&gt;KP: It's always hard when each of you want to be the CEO. I think all of us wanted a little bit of it, and that was super hard to navigate.&lt;/p&gt;
    &lt;p&gt;I gave the role away twice. I started as CEO, but I was the CEO of one person and I never really liked the title. I gave it to Tim because I was struggling with it. I always thought I wasn't doing what I thought that role was supposed to be. I was too invested in the product, too invested in customers, and wasn't interested in budgets or back office stuff.&lt;/p&gt;
    &lt;p&gt;Later on Lachlan took it over. I struggled with that because I always thought I could do a good job of it, but I wasn't ready to take the title and really own it.&lt;/p&gt;
    &lt;p&gt;It wasn't until a couple years later that I personally gained the confidence to say, "I'm the CEO." When I eventually could do it, I think I did a pretty good job because I learned I needed to be the sort of CEO that I was. I couldn't be trying to replicate something I thought in my head.&lt;/p&gt;
    &lt;p&gt;I decided I was a product-centric CEO, and when I owned that, that's when I started to flourish in the role. If we were selling brake pads, you'd want the person in charge to be an expert on brake pads. If we're selling developer tools, the person in charge should be an expert on developer tools.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Series A and Growth Challenges&lt;/head&gt;
    &lt;p&gt;SH: Tell me about the first major funding round.&lt;/p&gt;
    &lt;p&gt;KP: In 2019 we raised a Series A of $28 million. The reason we did that was because I wanted to buy a house. I had previously been to the bank for a home loan and every bank rejected me. I was too risky because Buildkite's finances were largely US revenue, and my finances were Buildkite's finances.&lt;/p&gt;
    &lt;p&gt;A lot of that Series A round was taken out as secondaries. Everyone who had invested the original 200 grand in the seed round got their money back plus some. I had enough money to buy a house, and only a few million went into the company as rainy day money.&lt;/p&gt;
    &lt;p&gt;SH: What did you learn about taking VC money?&lt;/p&gt;
    &lt;p&gt;KP: We effectively chose the one that gave us the highest valuation and a clean term sheet. At the time it was awesome because it meant we could sell less of our company for more money, but no one really tells you what happens if you go in at a high valuation early on.&lt;/p&gt;
    &lt;p&gt;If you're given a high valuation, you're not worth that at the moment. They're pricing you based on what they think you're going to be worth. You effectively need to grow into the valuation, and if you don't then in your next round, you end up with a down round.&lt;/p&gt;
    &lt;p&gt;It made the next round much harder. We hadn‚Äôt grown fast enough to justify the valuation, so the next round was effectively flat. No one warns you about that.&lt;/p&gt;
    &lt;p&gt;I think we were naive going into that transaction. We always painted the picture of being a long-term sustainable business, and our position on that never changed. We weren't building something to sell. We were building something that would last forever. The VCs gave us money even though we said those words, but I think in their mind they were hoping we would change.&lt;/p&gt;
    &lt;p&gt;Once you take big VC money, expectations change. They want big results, and we were still behaving like 37signals.&lt;/p&gt;
    &lt;p&gt;Eventually I realised that if we‚Äôre going to play the enterprise game, we have to play it properly. That meant raising again, and building the team to go after bigger customers.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Transformation: Embracing Sales&lt;/head&gt;
    &lt;p&gt;SH: What changed when you took over as CEO the second time?&lt;/p&gt;
    &lt;p&gt;KP: When I took over CEO, I really embraced having a sales team. Up to that point, Buildkite didn't have a sales team. I used to have this romantic relationship with the Atlassian approach, who famously had never had a salesperson. In reality they did have salespeople, they just called them something different.&lt;/p&gt;
    &lt;p&gt;The biggest learning from hiring a salesperson was that there is a certain dance you need to have with customers during a sale, and you want someone whose job it is to do that tango.&lt;/p&gt;
    &lt;p&gt;We hired a salesperson and built a sales team, and tripled revenue in a couple of years through doing that.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Product Philosophy That Almost Came Too Late&lt;/head&gt;
    &lt;p&gt;SH: Buildkite is well known for being a much-loved product in the developer community. Tell me about your product development process.&lt;/p&gt;
    &lt;p&gt;KP: It took me nine years to figure out my product voice. I read a book by Rory Sutherland called Alchemy. It was a book on marketing, but weirdly enough, this marketing book helped me find my product voice.&lt;/p&gt;
    &lt;p&gt;I had a lot of stuff that was just instinct and gut, but I never figured out how to turn that gut into something actionable or a set of product principles. Reading this book helped me find that voice.&lt;/p&gt;
    &lt;p&gt;I remember the day I showcased to the company,&lt;/p&gt;
    &lt;p&gt;"Hey, I finally figured out what I've struggled with all this time."&lt;/p&gt;
    &lt;p&gt;I did a three-hour presentation to the company, and I think everyone hated it. Everyone hated it because so many people disagreed with the principles.&lt;/p&gt;
    &lt;p&gt;SH: What were some of those principles?&lt;/p&gt;
    &lt;p&gt;KP: Here's a great example: don't listen to customers. Listen to them, but don't listen to them. What I learned is that when a customer comes to you with a problem, they come to you with a cry for help disguised as a feature request. You have to ignore the feature request and get at their problem.&lt;/p&gt;
    &lt;p&gt;Ninety-nine percent of the time, they've had some configuration issue somewhere else, 10 steps earlier, that's led them to this point where they've fucked it up so badly that they need something to solve that.&lt;/p&gt;
    &lt;p&gt;A feature request is not a feature request. It's a cry for help. That was difficult for a lot of people to unwind in their head, but that principle helped a lot.&lt;/p&gt;
    &lt;p&gt;Another principle: I always wanted developers to be treated as humans, not robots. Just look at any error message in any developer tool. It's probably obscure and shit. One of the magical parts of Buildkite was that I spent a lot of time taking what the developer would give me, and interpreting it to be what they actually meant.&lt;/p&gt;
    &lt;p&gt;You have to have pure empathy for the user. You're almost like a parent because you can't always give them what they want. You can easily just add all these checkboxes and make everyone happy, but you pay for it later with a million checkboxes.&lt;/p&gt;
    &lt;p&gt;SH: Why do you think the team reacted negatively?&lt;/p&gt;
    &lt;p&gt;KP: I think I should have packaged up my principles differently or had a different approach. They were pretty black and white, quite dogmatic. It was like, "This is how it is." Anyone receiving that would probably not like being told "this is the way it is from now on."&lt;/p&gt;
    &lt;p&gt;At that moment, I realised the company I had was somewhat incompatible with that. I should have spent more time thinking about that stuff before going on mass hiring.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hard Truth About Hiring and Culture&lt;/head&gt;
    &lt;p&gt;KP: There's something not a lot of founders talk about because maybe they're scared about it, but in industry there's this thing called culture fit that people get wrong all the time.&lt;/p&gt;
    &lt;p&gt;If you hire too many different people with different work styles, different wants, different needs, different approaches, different ideas, different values, different ethics, then everything becomes hard mode.&lt;/p&gt;
    &lt;p&gt;I wish I had gone back and been more involved in hiring. I would have opted more for culture fit. I would have been more bullish and precise: here is the type of person we want, let's go find that type of person.&lt;/p&gt;
    &lt;p&gt;At some point in hiring you get exhausted and you just hire the person who's at the top of the pile, even if they might not match who you wanted. You think, "We need somebody, so we just get them." That's a trap I see a lot of founders fall into, and it has major repercussions over time.&lt;/p&gt;
    &lt;p&gt;When I'm starting my new company, Unreasonable Magic, I'm being very strict about who I let in, way more exclusionary this time around. Teams that build products aren't a party. It's a set of inventors and makers, and you need everyone on the same wavelength.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Board Dynamics and Exit&lt;/head&gt;
    &lt;p&gt;SH: How did having a board change the way you operated?&lt;/p&gt;
    &lt;p&gt;KP: I personally struggled with board stuff. The board was heavily American, and they were all operators who were used to working at a particular pace. I wasn't at that pace, and I couldn't help but feel that I was always letting them down. Not going fast enough.&lt;/p&gt;
    &lt;p&gt;Board work at some point becomes theater, and I struggle with that. I struggle with having to put on a show for a board. I was always struggling to find the balance between how do I get the most out of these smart people versus how is this not just a press conference? I never figured it out.&lt;/p&gt;
    &lt;p&gt;The thing about boards is they have a lot of control, but these people think about your company for only a couple of hours a month. That was super tough because you care a thousand percent, and they care, but they only care for a couple of hours.&lt;/p&gt;
    &lt;p&gt;SH: You have since moved on from Buildkite. Can you talk about your exit?&lt;/p&gt;
    &lt;p&gt;KP: I worked on Buildkite for 13 years, and the hardest part about leaving was that I didn't get to see what happened next. It was almost like I left halfway through a season of a TV show, and there were all these characters and stories and threads I never got to see the ending to.&lt;/p&gt;
    &lt;p&gt;Whenever any founder exits a company, so much of that company is them, so you're leaving a part of yourself. The biggest struggle I've had since leaving is that no one's ever going to care as much about the thing as you did.&lt;/p&gt;
    &lt;p&gt;It's like when you have an electrician come to your house. Sometimes they don't vacuum up as much as you would. You're always going to find that extra wire or bit of gyprock because it's not their house. Your company is your house.&lt;/p&gt;
    &lt;p&gt;No exit is good enough for any founder after a 13-year thing. It's almost an impossible task to try to figure out how to exit someone from a company after 13 years gracefully.&lt;/p&gt;
    &lt;head rend="h2"&gt;What's Next: Unreasonable Magic&lt;/head&gt;
    &lt;p&gt;SH: What's next for you with Unreasonable Magic?&lt;/p&gt;
    &lt;p&gt;KP: I've got 13 years of ideas to explore. Where I'm focusing my energy is on programming. A lot of programmers like programming, as they like to code. The rise of coding tools fundamentally changes the relationship you have with the work, more than anything in the history of programming.&lt;/p&gt;
    &lt;p&gt;A lot of programmers who would program and get fulfilment out of it have become managers of a coding machine. Whenever I have a day of using Claude or similar tools, I walk away feeling dumb and empty. It's like popcorn. It's not a real meal, it's not feeding your soul.&lt;/p&gt;
    &lt;p&gt;Unreasonable Magic is about how can we take these tools and make you feel fulfilled? How can we let you be as productive as using a coding assistant but also feel that you are the boss, that you walk away smarter at the end of a coding session?&lt;/p&gt;
    &lt;p&gt;It's that doing-the-opposite thing again where everyone's trying to be more productive, but for me it's not about productivity. It's about relationships to the code.&lt;/p&gt;
    &lt;p&gt;Programming is thinking. When I'm writing code, I'm thinking about how each line will be executed, what happens if this line goes wrong. You just don't get that same level of intimacy with the program when you use coding assistants because they can whip out so much code so quickly.&lt;/p&gt;
    &lt;p&gt;The products I built with Claude are worse than without them because I use programming as a way to think and interact with the problem. When you're coding, you're deeply invested in the problem you're solving, getting intimate with the problem. With AI tools, it's surface level. It's a one-night stand with a problem versus a deep and meaningful relationship.&lt;/p&gt;
    &lt;p&gt;Claude certainly makes me more productive, but at what cost? You're paying with the knowledge you would have learned from interacting with the problem.&lt;/p&gt;
    &lt;head rend="h2"&gt;The 3 easy questions&lt;/head&gt;
    &lt;p&gt;SH: What‚Äôs a book we should all read?&lt;/p&gt;
    &lt;p&gt;KP: Alchemy by Rory Sutherland. It‚Äôs the book that helped me find who I am. That book changed more about Buildkite than I can list out.&lt;/p&gt;
    &lt;p&gt;SH: What is a band or artist that we should all listen to?&lt;/p&gt;
    &lt;p&gt;KP: Uhmm, I like beatboxing.&lt;/p&gt;
    &lt;p&gt;SH: Do you beatbox yourself?&lt;/p&gt;
    &lt;p&gt;KP: Oh gawd no. But there is this band I like, and the song is called Candy Thief. It‚Äôs K-pop. I probably listen to that song on repeat every day.&lt;/p&gt;
    &lt;p&gt;SH: What is a podcast we should all listen to?&lt;/p&gt;
    &lt;p&gt;KP: I listen to Conan O‚ÄôBrien Needs A Friend. If I‚Äôm working all day, I feel like it's important for me to make sure my brain goes somewhere else at the end of the day. And Conan O'Brien is just a very silly human.&lt;/p&gt;
    &lt;p&gt;SH: Amazing. Thanks for your time Keith, and good luck with Unreasonable Magic!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45162593</guid></item><item><title>The demo scene is dying, but that's alright</title><link>https://www.datagubbe.se/sceneherit/</link><description>&lt;doc fingerprint="f641d011dffaaaad"&gt;
  &lt;main&gt;
    &lt;head rend="h2"&gt;The Demo Scene is Dying, But That's Alright&lt;/head&gt;
    &lt;p&gt;Autumn 2025&lt;/p&gt;
    &lt;p&gt;The demo scene was recently proclaimed a UNESCO Living Cultural Heritage in Sweden, following several other European nations. As a scener, I'm not quite sure how I feel about that. It's amusing on some level, there's maybe even a bit of pride involved, but also fear that it might bring unwanted attention to a Good Thing, because Good Things are always at risk of spoiling when receiving too much attention. Then again, such worries are probably unfounded. Despite being a living cultural heritage, the demo scene is - all things considered - slowly approaching its demise.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Scene Isn't Dead&lt;/head&gt;
    &lt;p&gt;The scene - incorporating the cracking scene of the 1980s and early 1990s - has been declared dead several times over since the late 1980s. Some claimed the Amiga would kill it, others that the PC would destroy its soul or that the Internet would rob it of its essence. This is to be expected: In a subculture - which is what the scene is - there's typically going to be elders lamenting the changing of the old ways, and newcomers eager to bring their own ideas into the mix.&lt;/p&gt;
    &lt;p&gt;Except... That last part might no longer apply to the scene. Examine photos from a late 1980s rave party and they'll show a bunch of young people partying hard. Fast forward in time and look at photos from a 2025 rave party, and the concept remains basically the same, but there's now a different bunch of young people partying hard. When performing the same experiment on the scene, there's close to zero teenagers among 2025 demo party attendants. Look closer and it becomes evident that many of the 2025 attendants are in fact the very same persons as the teenagers and twenty-somethings appearing in party photos from 1989.&lt;/p&gt;
    &lt;p&gt;Some subcultures are regularly replenished or revived, whereas others are not. The scene seems to fall distinctly into the latter category: With few exceptions, most active sceners - even those who create demos for modern gaming PCs - belong to the home computer generation, meaning people who remember the heydays of Commodore, Atari, Amiga and MS-DOS.&lt;/p&gt;
    &lt;head rend="h3"&gt;High Effort, Low Reward&lt;/head&gt;
    &lt;p&gt;Like most subcultures, the scene grew out of a mix of unique circumstances at a very particular point in time; specifically, the advent of affordable but primitive home computers, lack of accessible digital mass communication, and limited cultural precedent.&lt;/p&gt;
    &lt;p&gt;I believe that all subcultures need a bit of gatekeeping in order to retain their original soul - the more accessible they are, the easier it is to turn them into exploitable markets. Punks, hippies and mods have all been removed from their original context, ground through a mainstream cultural filter, and repurposed for consumerism: A convenient way of selling brand apparel to identity-seeking middle class youth hungry for something, anything, ostensibly genuine.&lt;/p&gt;
    &lt;p&gt;The scene, on the other hand, is a perfect example of a subculture that's hard to repackage for sale: It lacks apparent external traits, such as a particular fashion, a specific style of music and - especially considering how common computers are today - any uniquely defining equipment.&lt;/p&gt;
    &lt;p&gt;Combined with its shadier activities - cracking games, software piracy, postage fraud and phreaking - the scene was always on the introverted side. Decidedly anti-commercial and without tangible and marketable artifacts, corporate interest was and is usually limited to a few sponsors at really large demo parties, and using the scene as a recruitment pool for game developers. Apart from the time and effort required to build scene skills, computers and other traditionally geeky hobbies were decidedly low status in the schoolyard pecking order during the formative years of scene culture. Hence, there was very little potential reward in pretending to be someone who hunkered down in front of a CRT all weekend, trying to move a sprite across the screen. And, let's be frank, it never did attract very many girls.&lt;/p&gt;
    &lt;p&gt;Thus, unlike other skill-based subcultures with a bigger mainstream appeal and a more pronounced aesthetic - such as skateboarding - the effort required to be accepted by the in-group and the low potential of reward from the out-group, means the scene has never interested posers in any significant numbers. It simply never became cool, and, consequently, still remains well out of view from establishment actors. I happen to like it that way.&lt;/p&gt;
    &lt;head rend="h3"&gt;Still Not Mainstream&lt;/head&gt;
    &lt;p&gt;There are regular discussions on the scene about how to attract new talent, because newcomers are nowadays few and far between. I believe the effort to get listed as a living cultural heritage is, in some way, part of a desire to rejuvenate the scene and keep the culture alive. Time will tell if it's successful - I have serious doubts.&lt;/p&gt;
    &lt;p&gt;It's not that it's hard to get accepted on the scene, especially not these days. Talk of lamers and elites is now just self-referential irony, and the cracking scene is much farther removed from demo making than ever before. The mystery of what a demo actually is has been well documented, not least on Wikipedia, and examples are prevalent on easy access platforms like Youtube. To partake, all that's required is (preferably) going to a demo party, talking to people, and bringing something to the table. Congratulations - you are now a scener!&lt;/p&gt;
    &lt;p&gt;If measured in number of active participants, the scene peaked somewhere during the early 1990s, when parties like Assembly, The Party and The Gathering attracted visitors in the thousands. Since then, attendance has dwindled, and has proven hard to increase again despite various outreach initiatives.&lt;/p&gt;
    &lt;head rend="h3"&gt;Kill All Audio and Lights&lt;/head&gt;
    &lt;p&gt;During the latter half of the 1990s, big demo parties started attracting people almost exclusively interested in playing networked games at scale, effectively visiting parties to use the digital infrastructure they provided. In theory, this could have been an opportunity for the scene to grow, gaining a natural venue for showcasing itself to a new audience apparently interested in computers. In reality, few gamers made the leap, and subcultural differences instead created friction. Sceners were annoyed by gamers disturbing the demo competitions with loud music and other disruptive behavior, and ticket prices went up due to increased demand. Some events were completely taken over by gamers: Dreamhack started as a small demo party and is now a global LAN party franchise. Eventually, sceners simply retreated to other venues, in a natural and uneventful split.&lt;/p&gt;
    &lt;p&gt;There are still hybrid events, but the biggest, most popular and influential parties are once again exclusive to the scene - not by actively banning a certain category of visitors, but by simply organizing and marketing scene events in a way that makes them inherently uninteresting for the average LAN party visitor.&lt;/p&gt;
    &lt;p&gt;Arguably successful hybrids, such as what Assembly has transformed into, has more or less compartmentalized the scene in order to protect it from the otherwise completely dominating and highly commercialized mix of e-sports, cosplay and live music acts. During the 1990s, many big parties offered cash prizes, and Assembly is as far as I know the last one that still does. In an outreach effort, they've also introduced a compo segment specifically for beginner sceners and this does seem to attract a number of first releases each year. A positive sign, though I personally feel a bit squeamish about keeping money in the mix: the vast majority of productions released, even during the peak party years, were never created for winning money, but because it was fun.&lt;/p&gt;
    &lt;head rend="h3"&gt;Why Grow?&lt;/head&gt;
    &lt;p&gt;For most sceners, the scene was a part of their formative teenage years. Lasting friendships, life-long skills and creative exploration is the stuff of fond memories, and our pursuits are still a source of much joy. It's only natural to want to share this positive experience with others. Some seem to think the scene could still be a potential talent pool, lamenting the lack of new scene recruits for low level programming jobs and game development. And, of course, it might be comforting to see a cultural legacy carried on: a validation of your own life choices. However, as heartwarming as it is to see new, younger talents appear on the scene, the slow trickle appears to be well below meaningful replacement levels: we're very far from the hundreds or even thousands of youngsters that once hiked across Europe to fill giant convention centers.&lt;/p&gt;
    &lt;p&gt;What makes rejuvenation hard is that many of the things that once gave the scene its special allure are simply gone. There are no longer home computers offering a fixed hardware platform for exploring, sharing and creating on equal footing. Spreading digital creations is cheap and easy on an Internet taken for granted. Affordable yet immensely powerful computers have opened up entirely new creative avenues, unhindered by the technical limitations that once upon a time forced the essence of demos to become what it is. And, of course, the scene is no longer new and exciting, but filled with middle-aged grown-ups.&lt;/p&gt;
    &lt;head rend="h3"&gt;Degrowth&lt;/head&gt;
    &lt;p&gt;The scene grew organically through devoted and creative people, hell-bent on doing their own thing without the involvement of clueless adults, corporate incentives or detailed career prospects. It was and remains, first and foremost, about challenging yourself and sharing a passion. It's not the scene's responsibility to supply the software industry with capable developers, and it's certainly not the responsibility of today's teenagers to fill the ranks of a peculiar hacker niche - one based on unwritten rules thought up four decades ago by a bunch of old fogeys who are, to a large extent, still farting around with MS-DOS, 6510 CPUs and blitter chips.&lt;/p&gt;
    &lt;p&gt;There are other venues today where kids can experiment, learn and build a social context around their activities - probably ones where clueless adults can be kept at arm's length. If they want to join the scene, let them - but above all, let them find their own thing, invent their own creative ways, and let them enjoy doing so.&lt;/p&gt;
    &lt;p&gt;As for me, I'm fine with the seemingly unavoidable, generational death of the scene. I crave no subcultural legacy, no future audience for my present creative pursuits. I'm just happy to be left alone in my narrow little niche, enjoying it with like-minded individuals, far from the scrutiny of commercial interests and other potential threats to our Good Thing.&lt;/p&gt;
    &lt;p&gt;And if a significant number of kids do, in fact, eventually pick up the scene torch? Well, I just hope they'll have fun, too.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45162803</guid></item><item><title>Formatting code should be unnecessary</title><link>https://maxleiter.com/blog/formatting</link><description>&lt;doc fingerprint="2658533aba372ff4"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;Formatting code should be unnecessary&lt;/head&gt;and we knew this back in the 80s&lt;p&gt;I had a (maybe slightly overqualified) computer science teacher back in highschool, Mr. Paige. He worked on the Ada compiler and has been programming since the early 80s.&lt;/p&gt;&lt;p&gt;One day I complained about linter tooling that was driving me nuts. I said something to the effect of, "it's 2016, how are we still dealing with this sort of thing?"&lt;/p&gt;&lt;p&gt;Turns out, that problem was solved four decades ago (well, three at that point). Back when he was working on Ada, they didn't store text sources at all ‚Äî they used an IR called DIANA. Everyone had their own pretty-printing settings for viewing it however they wanted.&lt;/p&gt;&lt;p&gt;We've been debating some linter settings at work recently and I keep thinking back to Mr. Paige. It's 2025, how are we still dealing with this sort of thing?&lt;/p&gt;&lt;p&gt;Well, to answer that it would help to know what we're missing.&lt;/p&gt;&lt;p&gt;I believe he was working with the Rational R1000, of which there isn't a ton of info (like all things Ada, it was used by the DoD):&lt;/p&gt;&lt;p&gt;The R1000 had a lot of bleeding-edge features: incremental compilation, semantic analysis, version control, and first-class debugging all built-in. It was a workstation similar to the Xerox Alto but using Ada instead of Smalltalk.&lt;/p&gt;&lt;p&gt;DIANA (Descriptive Intermediate Attributed Notation for Ada) was a key component of Ada that enabled a lot of the more advanced features.&lt;/p&gt;&lt;p&gt;Taken from Experiences with Code Generation (1984)&lt;/p&gt;&lt;p&gt;Instead of storing plain-text source code, the R1000 wrote DIANA. The compiler and the IDE built into the machine both understood DIANA too, so you could view the source however you wanted. Spaces vs. tabs didn't matter because neither affects the semantics and the editor on the system let you modify the program tree directly (known today as projectional editing).&lt;/p&gt;&lt;p&gt;Grady Booch summarizes it well:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;R1000 was effectively a DIANA machine. We didn't store source code: source code was simply a pretty-printing of the DIANA tree.&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Imagine that. No wasted time due to formatting discussions or fighting linters, without forcing everyone into the same editor setup (looking at you, eslint-config-airbnb).&lt;/p&gt;&lt;p&gt;And there were other benefits:&lt;/p&gt;&lt;quote&gt;&lt;p&gt;Using DIANA with hardware acceleration made it possible to do incremental compilation (unheard of at the time, for strongly typed languages), easy refactoring (though that word had not yet been invented), and incredibly fast integration (essential for the large systems that we being built with Ada).&lt;/p&gt;&lt;/quote&gt;&lt;p&gt;Today, we don't need to worry about hardware-accelerated compilation (hopefully), and we have better tools for refactoring (thanks, Claude). But with formatting, we regressed. I'm not advocating for everyone to use projectional editing and a live environment (although I think they're awesome and we should be exploring them more), but surely we can figure out something that fits into todays programming paradigms.&lt;/p&gt;&lt;head rend="h3"&gt;Further reading&lt;/head&gt;&lt;p&gt;This post was meant to be me saying "it'd be easier if we just pushed minified code", but I had too much fun researching the R1000 during it. Here are some of the documents I looked at:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Experiences with Code Generation (1985): https://www2.eecs.berkeley.edu/Pubs/TechRpts/1985/CSD-85-249.pdf&lt;/item&gt;&lt;item&gt;Ada Compiler Validation Summary Report: Rational Environment (1985): https://apps.dtic.mil/sti/tr/pdf/ADA157830.pdf&lt;/item&gt;&lt;item&gt;Grady Booch's blog post about the 5th anniversary on Rational's acquisition (2008): https://web.archive.org/web/20160304051102/https://www.ibm.com/developerworks/community/blogs/gradybooch/entry/rational_anniversary?lang=en#pagePlaceBar&lt;/item&gt;&lt;/list&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45163043</guid></item><item><title>Using Claude Code to modernize a 25-year-old kernel driver</title><link>https://dmitrybrant.com/2025/09/07/using-claude-code-to-modernize-a-25-year-old-kernel-driver</link><description>&lt;doc fingerprint="fe903b5f08a4bbc9"&gt;
  &lt;main&gt;
    &lt;p&gt;As a bit of background, one of my hobbies is helping people recover data from old tape cartridges, such as QIC-80 tapes, which were a rather popular backup medium in the 1990s among individuals, small businesses, BBS operators, and the like. I have a soft spot for tape media; there‚Äôs something about the tactile sensation of holding these tapes in my hands that makes the whole process very joyful, even though QIC tapes are notorious for their many design flaws. With some careful inspection and reconditioning, the data on these tapes is still totally recoverable, even after all these years.&lt;/p&gt;
    &lt;p&gt;Whenever I receive a QIC-80 tape for recovery, I power up one of my older PC workstations which has the appropriate tape drive attached to it, and boot into a very old version of Linux (namely CentOS 3.5), because this is the only way to use the &lt;code&gt;ftape&lt;/code&gt; driver, which is the kernel driver necessary for communicating with this tape drive, allowing the user to dump the binary contents of the tape.&lt;/p&gt;
    &lt;p&gt;You see, the drive that reads these tapes connects to the floppy controller on the motherboard. This clever hack was done as a cost-saving measure: instead of having to purchase a separate SCSI adapter (the standard interface for higher-tier tape media), you can just connect this tape drive to your floppy controller, which was already available on most PCs. It can even work alongside your existing floppy drive, on the same ribbon cable! The tradeoff, of course, is that the data rate is limited by the speed of the floppy controller, which was something like 500 Kbps (that‚Äôs kilobits, not bytes).&lt;/p&gt;
    &lt;p&gt;The other downside is that the protocol for communicating with these tape drives through the floppy controller was very messy, nonstandard, and not very well-supported. It was a ‚Äúhack‚Äù in every sense: your motherboard‚Äôs BIOS had no knowledge of the tape drive being connected, and it was entirely up to the end-user software to know exactly how to manipulate the hardware I/O ports, timings, interrupts, etc. to trick the floppy controller into sending the appropriate commands to the tape drive.&lt;/p&gt;
    &lt;p&gt;There were a small number of proprietary tools for MS-DOS and Windows 3.x/9x for dealing with these drives, and only one open-source implementation for Linux, namely &lt;code&gt;ftape&lt;/code&gt;. Of course it is possible to use those original DOS/Windows tools to read the tapes, but it‚Äôs actually only &lt;code&gt;ftape&lt;/code&gt; that allows us to read the ‚Äúraw‚Äù binary contents of the tape, regardless of which proprietary software originally wrote it, which is why I prefer it for dumping the contents and worrying afterwards about decoding the proprietary logical formatting, and then extracting the files from it.&lt;/p&gt;
    &lt;p&gt;The trouble is, the &lt;code&gt;ftape&lt;/code&gt; driver hasn‚Äôt been supported since roughly the year 2000, and was soon removed from the Linux kernel for this reason. This is why I‚Äôve needed to run a painfully old version of Linux anytime I have to work with one of these drives. It would be great if &lt;code&gt;ftape&lt;/code&gt; worked on a modern distro, with all the benefits and affordances that would provide.&lt;/p&gt;
    &lt;p&gt;So a couple of weeks ago, it occurred to me to make a simple request to Claude Code:&lt;/p&gt;
    &lt;code&gt;&amp;gt; This repository is a Linux kernel driver that communicates with legacy tape drives connected to the floppy
  controller (FDC) on the motherboard. Unfortunately, this driver hasn't been maintained for a long time, and
  can only compile under kernel version 2.4. I'd like to modernize this driver, allowing it to be built with
  the latest versions of the kernel.

‚óè I'll help you modernize this Linux kernel driver for legacy tape drives. This is a significant task that
  will require updating the code to work with modern kernel APIs and conventions.
&lt;/code&gt;
    &lt;p&gt;And after several iterations of ‚Äúcombobulating‚Äù and whatever else Claude claims to do, I suddenly had a kernel driver that was compiling without errors. This is because Claude is able to take the compiler output and feed it back into itself, until the compilation works correctly. There was a laundry list of kernel functions and structures that were understandably deprecated or replaced, in the eternity that elapsed between kernel version 2.4 and version 6.8. Amazingly, Claude found all the outdated bits and replaced them with the correct modern equivalents, requiring just a few manual touch-ups of the code (a bit more on this later).&lt;/p&gt;
    &lt;p&gt;At this point, however, the kernel driver was still expected to be compiled as part of a full kernel tree, whereas I just wanted it to become a standalone loadable kernel module. OK, no problem:&lt;/p&gt;
    &lt;code&gt;&amp;gt; Is there a way to compile just this module in-place, instead of copying it into a kernel source tree?

‚óè Yes! You can compile kernel modules out-of-tree without copying them into the kernel source. Let me create
  a proper standalone build system for the ftape driver.
&lt;/code&gt;
    &lt;p&gt;‚Ä¶And it did. Without any further prompting. At the end of this stage, I had a loadable kernel module (&lt;code&gt;.ko&lt;/code&gt;) that I could start trying to use with actual live hardware. Unsurprisingly, the module did not yet ‚Äúwork‚Äù ‚Äî it was loading, but something was preventing it from communicating with the tape drive correctly. But I pressed on.&lt;/p&gt;
    &lt;p&gt;From this point forward, since loading/unloading kernel modules requires sudo, I could no longer let Claude ‚Äúiterate‚Äù on such sensitive operations by itself. Luckily, the &lt;code&gt;ftape&lt;/code&gt; driver was already generously sprinkled with logging messages (&lt;code&gt;printk()&lt;/code&gt; calls that are printed to &lt;code&gt;dmesg&lt;/code&gt;), so I loaded the module myself, and iteratively pasted the output of &lt;code&gt;dmesg&lt;/code&gt; into Claude manually, and let Claude compare it to a ‚Äúknown-good‚Äù dmesg log that I‚Äôd previously saved from a successful tape read. After a few iterations of this, Claude identified the remaining issues preventing the module from communicating with the hardware.&lt;/p&gt;
    &lt;code&gt;‚óè Perfect! The module has parameters that can be set. The issue is that without configuration, all the base
  addresses default to -1, which gets converted to 0xffff in the config function, causing the detection to fail.
‚óè The "No such device or address" (ENXIO) error occurs because the ftape-internal module is not configured
  with the correct I/O port base address for your floppy controller.
  The issue is in fdc-internal.c:735-737: if fdc-&amp;gt;sra == 0xffff, it returns -ENXIO. Since no configuration was
  provided, all values default to -1, which becomes 0xffff....
&lt;/code&gt;
    &lt;p&gt;And at last, I had a kernel module that was able to a) load, b) detect the tape drive, and c) dump the contents of a test tape!&lt;/p&gt;
    &lt;p&gt;Using the &lt;code&gt;ftape&lt;/code&gt; driver on a modern kernel, a feat that I thought was hopelessly out of reach, was suddenly completed over the course of two evenings.&lt;/p&gt;
    &lt;p&gt;As a giant caveat, I should note that I have a small bit of prior experience working with kernel modules, and a good amount of experience with C in general, so I don‚Äôt want to overstate Claude‚Äôs success in this scenario. As in, it wasn‚Äôt literally three prompts to get Claude to poop out a working kernel module, but rather several back-and-forth conversations and, yes, several manual fixups of the code. It would absolutely not be possible to perform this modernization without a baseline knowledge of the internals of a kernel module.&lt;/p&gt;
    &lt;p&gt;This led me to crystallize some thoughts on working with such coding agents in our current moment:&lt;/p&gt;
    &lt;head rend="h4"&gt;Open yourself up to a genuine collaboration with these tools.&lt;/head&gt;
    &lt;p&gt;Interacting with Claude Code felt like an actual collaboration with a fellow engineer. People like to compare it to working with a ‚Äújunior‚Äù engineer, and I think that‚Äôs broadly accurate: it will do whatever you tell it to do, it‚Äôs eager to please, it‚Äôs overconfident, it‚Äôs quick to apologize and praise you for being ‚Äúabsolutely right‚Äù when you point out a mistake it made, and so on. Because of this, you (the human) are still the one who must provide the guardrails, make product decisions, enforce architectural guidelines, and spot potential problems as early as possible.&lt;/p&gt;
    &lt;head rend="h4"&gt;Be as specific as possible, making sure to use the domain-specific keywords for the task.&lt;/head&gt;
    &lt;p&gt;I‚Äôm not claiming to suddenly be an expert in prompt engineering, but the prompts that I‚Äôve found to be most successful are ones that clearly lay out the verbal scaffolding for a feature, and then describe the gaps in the scaffolding that the LLM should fill in. (For some reason the image that comes to mind is one of those biological stem-cell scaffolds where an artificial human ear will grow.)&lt;/p&gt;
    &lt;head rend="h4"&gt;Develop an intuition for the kinds of tasks that are ‚Äúwell-suited‚Äù for an agent to complete.&lt;/head&gt;
    &lt;p&gt;These agents are not magical, and can‚Äôt do literally everything you ask. If you ask it to do something for which it‚Äôs not well-suited, you will become frustrated and prematurely reject these tools before you allow them to shine. On this point, it‚Äôs useful to learn how LLMs actually work, so that you develop a sense of their strengths and weaknesses.&lt;/p&gt;
    &lt;head rend="h4"&gt;Use these tools as a massive force multiplier of your own skills.&lt;/head&gt;
    &lt;p&gt;I‚Äôm sure that if I really wanted to, I could have done this modernization effort on my own. But that would have required me to learn kernel development as it was done 25 years ago. This would have probably taken me several weeks of nonstop poring over documentation that would be completely useless knowledge today. Instead of all that, I spent a couple of days chatting with an agent and having it explain to me all the things it did.&lt;/p&gt;
    &lt;p&gt;Naturally, I verified and tested the changes it made, and in the process I did end up learning a huge amount of things that will be actually useful to me in the future, such as modern kernel conventions, some interesting details of x86 architecture, as well as several command line incantations that I‚Äôll be keeping in my arsenal.&lt;/p&gt;
    &lt;head rend="h4"&gt;Use these tools for rapid onboarding onto new frameworks.&lt;/head&gt;
    &lt;p&gt;I am not a kernel developer by any stretch, but this particular experience ignited a spark that might lead to more kernel-level work, and it turns out that kernel development isn‚Äôt nearly as difficult as it might sound. In another unrelated ‚Äúvibe-coding‚Äù session, I built a Flutter app without having used Flutter before. If you‚Äôre like me, and your learning style is to learn by doing, these tools can radically accelerate your pace of learning new frameworks, freeing you up to do more high-level architectural thinking.&lt;/p&gt;
    &lt;p&gt;In any case, circling all the way back, I am now happy to say that &lt;code&gt;ftape&lt;/code&gt; lives on! Twenty-five years after its last official release, it is once again buildable and usable on modern Linux. I‚Äôm still in the process of making some further tweaks and new feature additions, but I have already verified that it works with the floppy-based tape drives in my collection, as well as parallel-port-based drives which it also supports.&lt;/p&gt;
    &lt;p&gt;The physical setup looks very similar, but the OS is now Xubuntu 24.04, instead of CentOS 3.5! üéâ&lt;lb/&gt; Until next time!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45163362</guid></item><item><title>How the Slavic Migration Reshaped Central and Eastern Europe</title><link>https://www.mpg.de/25256341/0827-evan-slavic-migration-reshaped-central-and-eastern-europe-150495-x</link><description>&lt;doc fingerprint="b95131954dab128c"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;How the Slavic migration reshaped Central and Eastern Europe&lt;/head&gt;
    &lt;p&gt;Genetic analyses of medieval human remains reveal large-scale migrations, regional diversity, and new insights into early medieval communities&lt;/p&gt;
    &lt;head rend="h2"&gt;To the point&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dramatic population change: Analysis of genome-wide data from more than 550 ancient individuals demonstrates that, during the 6th-8th centuries CE, Eastern Germany, Poland/Ukraine, and the Northern Balkans experienced a major shift in ancestry, with over 80 percent originating from eastern European newcomers.&lt;/item&gt;
      &lt;item&gt;Support from other analysis: An independent study of 18 genomes from the South Moravian region linked to one of the first Slavic-speaking polities confirms this pattern.&lt;/item&gt;
      &lt;item&gt;Regional differences: While genetic turnover was nearly complete in the north, regions like the Balkans saw more mixing between Eastern European incomers and local communities. This diversity of ancestries persists until today in the modern populations of these areas.&lt;/item&gt;
      &lt;item&gt;Integration, not conquest: Genetic evidence shows no sex bias in the migration‚Äîentire families and communities seemed to have moved and integrated, rather than just male warriors.&lt;/item&gt;
      &lt;item&gt;Flexible social structure: In Eastern Germany, the migrants brought a new way of social organization, visible in the formation of large patrilinear pedigrees‚Äîa stark contrast to the much smaller family units typical of the preceding Migration Period. Meanwhile, in Croatia, early immigrant communities appear to have maintained more traditional or regionally continuous social structures, with less dramatic changes from the patterns seen before the demographic shift.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The spread of the Slavs stands as one of the most formative yet least understood events in European history. Starting in the 6th century CE, Slavic groups began to appear in the written records of Byzantine and Western sources, settling lands from the Baltic to the Balkans, and from the Elbe to the Volga. Yet, in stark contrast to the famous migrations of Germanic tribes like the Goths or Langobards or the legendary conquests of the Huns, the Slavic story has long been a difficult puzzle for historians of the European Middle Ages.&lt;/p&gt;
    &lt;p&gt;This is partly because early Slavic communities left behind rather little for archaeologists to find: they practiced cremation, built simple houses, and produced plain, undecorated pottery. Perhaps most significantly, they did not leave behind written records of their own for several centuries. As a result, the term ‚ÄúSlavs‚Äù itself has been ambiguous, sometimes imposed by outside chroniclers and often mis-used in later nationalist or ideological debates. Where did these people come from, and how did they so thoroughly change the cultural and linguistic map of Europe?&lt;/p&gt;
    &lt;p&gt;Historians have long debated whether the spread of Slavic material culture and language was driven by a mass migration of people, the gradual ‚ÄúSlavicisation‚Äù of local populations, or a combination of both. But the evidence was thin‚Äîespecially in the crucial early centuries, when cremation made DNA studies nearly impossible and archaeological traces were modest.&lt;/p&gt;
    &lt;head rend="h2"&gt;How the Slavs transformed Europe&lt;/head&gt;
    &lt;p&gt;Now, an international research team of researchers from Germany, Austria, Poland, Czechia and Croatia led by the HistoGenes consortium and working in close cooperation with the State Office for Heritage Management and Archaeology Saxony-Anhalt, has provided answers with the first comprehensive ancient DNA study of medieval Slavic populations. By sequencing over 550 ancient genomes, the team has revealed that the rise of the Slavs was, at its core, a story of people on the move. Their genetic signatures point to an origin in the region stretching from southern Belarus to central Ukraine‚Äîa geographic area that matches what many linguistic and archaeological reconstructions had long suggested. "While direct evidence from early Slavic core regions is still rare, our genetic results offer the first concrete clues to the formation of Slavic ancestry‚Äîpointing to a likely origin somewhere between the Dniester and Don rivers" says Joscha Gretzinger, a geneticist from the Max Planck Institute for Evolutionary Anthropology in Leipzig and lead author of the study.&lt;/p&gt;
    &lt;p&gt;The data show that, beginning in the 6th century CE, large-scale migrations carried this Eastern European ancestry across wide areas of Central and Eastern Europe, which caused the genetic makeup of regions like Eastern Germany and Poland to shift almost entirely. Yet the expansion did not follow the model of conquest and empire: Instead of sweeping armies and rigid hierarchies, the migrants built their new societies on flexible communities, often organized around extended families and patrilineal kinship ties. Also, this was not a single, uniform model across all regions. In Eastern Germany, the shift was profound: large, multi-generational pedigrees became the backbone of society, with kinship networks more extensive and structured than the small nuclear families seen in the preceding Migration Period. In contrast, in areas such as Croatia, the arrival of Eastern European groups brought much less disruption to existing social patterns. Here, social organization often retained many features of earlier periods, resulting in communities where new and old traditions blended or persisted side by side. This regional diversity in social structure highlights how the spread of Slavic groups was not a one-size-fits-all process, but rather a dynamic transformation that adapted to local contexts and histories.&lt;/p&gt;
    &lt;p&gt;‚ÄúRather than a single people moving as one, the Slavic expansion was not a monolithic event but a mosaic of different groups, each adapting and blending in its own way‚Äîsuggesting there was never just one ‚ÄòSlavic‚Äô identity, but many.‚Äù explains Zuzana Hofmanov√° from the MPI EVA and Masaryk University in Brno, Czechia, one of the senior lead authors of the study. Notably, the genetic record reveals no significant sex bias in these migrations: entire families moved together, and both men and women contributed equally to the emerging societies. More data will show in the upcoming years how each community adapted, integrated, or reinvented itself in response to both migration and its own local history.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spotlight on Eastern Germany&lt;/head&gt;
    &lt;p&gt;In Eastern Germany specifically, the genetic data show an especially striking story. Following the decline of the Thuringian kingdom, more than 85 percent of the ancestry in the region can be attributed to new arrivals from the East. This marks a shift from the earlier Migration Period, when the population was a cosmopolitan mix as best illustrated by the site of Br√ºcken, a richly furnished late antique cemetery from Sachsen Anhalt that displayed a mix of Northern, Central and Southern European ancestry. With the spread of the Slavs, this diversity gave way to a population profile almost identical to modern Slavic-speaking groups in Eastern Europe. Archaeological evidence from cemeteries confirms that these new communities organized themselves around large extended families and patrilineal descent‚Äîwhile women of marriageable age typically left their home villages to join new households elsewhere. Notably, the genetic legacy of these early Eastern European settlers endures today among the Sorbs, a Slavic-speaking minority in Eastern Germany. Despite centuries of surrounding cultural and linguistic change, the Sorbs have retained a genetic profile closely related to the early medieval Slavic populations that settled the region more than 1,000 years ago.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spotlight on Poland&lt;/head&gt;
    &lt;p&gt;In Poland specifically, the research overturns earlier ideas of long-term population continuity. Genetic results show that starting in the 6th and 7th centuries CE, the region‚Äôs earlier inhabitants‚Äîdescendants of populations with strong links to Northern Europe and Scandinavia in particular‚Äîalmost entirely disappeared and were successively replaced by newcomers from the East, who are closely related to modern Poles, Ukrainians, and Belarusians. This conclusion is reinforced by the analysis of some of the earliest known Slavic inhumation graves in Poland, excavated at the site of Gr√≥dek, which provide rare and direct evidence of these early migrants. While the population shift was overwhelming, the genetic evidence also reveals minor traces of mixing with local populations. These findings underscore both the scale of population change and the complex dynamics that shaped the roots of today‚Äôs Central and Eastern European linguistic landscape.&lt;/p&gt;
    &lt;head rend="h2"&gt;Spotlight on Croatia&lt;/head&gt;
    &lt;p&gt;The Northern Balkans specifically present a different pattern compared to the northern immigration area‚Äîa story of both change and continuity. Ancient DNA from Croatia and neighboring regions reveals a significant influx of Eastern European-related ancestry, but not a complete genetic replacement. Instead, Eastern European migrants mixed with the region‚Äôs diverse local populations, creating new, hybrid communities. Genetic analyses indicate that in present-day Balkan populations, the proportion of this incoming Eastern European ancestry varies considerably but often makes up roughly half or even less of the modern gene pool, highlighting the region‚Äôs complex demographic history. The formation of such a mixed community is clearly seen at the site of Velim, where some of the oldest Slavic burials in the region show evidence of both Eastern European migrants and up to 30% local ancestry. Here, the Slavic migration was not a wave of conquest but a long process of intermarriage and adaptation, resulting in the cultural, linguistic and genetic diversity that still characterizes the Balkan Peninsula today.&lt;/p&gt;
    &lt;head rend="h2"&gt;Independent confirmation in Moravia, Czechia&lt;/head&gt;
    &lt;p&gt;In an independent study at the same time today published in Genome Biology, supported among others by Czech projects FORMOR and RES-HUM, researchers from Czechia, Germany, Switzerland and UK with a senior leader of Dr. Zuzana Hofmanov√° found that there was also a population change in Southern Moravia (Czechia) and that also this demographic shift can be linked to the change to the Slavic-associated material culture which originated in modern-day Ukraine. While whole genomes of preceding, Migration-period individuals showed a large genetic diversity, individuals linked with the Slavic-related cultural horizons had affinities to Northeastern Europe, a feature that was not present before.This dataset included an individual, an infant, buried within a very early Slavic context usually only linked to cremations thus narrowing regionally the change in time and associating it to the Prague-Korchak culture. Importantly, the same genetic signal was present not only for individuals from 7th and 8th centuries but was regionally continuous to 9th and 10th century when this region is associated to one of the earliest Slavic polities, Moravian principality, known because of Saints Cyril and Methodius and the first literary Slavic language (Old Church Slavonic) and Glagolithic script they created for their mission among the Moravian Slavs.&lt;/p&gt;
    &lt;head rend="h2"&gt;A new chapter in European history&lt;/head&gt;
    &lt;p&gt;This study does not just resolve the historical puzzle how one of the world's largest linguistic and cultural groups came to be. It also offers new perspectives on why Slavic groups spread so successfully, and why they left so few traces of the kinds historians once sought:&lt;/p&gt;
    &lt;p&gt;As Walter Pohl, one of the senior lead authors of the study and medievalist at the Austrian Academy of Sciences, puts it, the Slavic migration represents a fundamentally different model of social organization: ‚Äúa demic diffusion or grass-root movement, often in small groups or temporary alliances, settling new territories without imposing a fixed identity or elite structures.‚Äù Their success may have been due not to conquest but to a pragmatic, egalitarian lifestyle‚Äîone that avoided the heavy burdens and hierarchies of the crumbling Roman world. In many places, the Slavs offered a credible alternative to the declining empires around them. Their social resilience, relatively simple subsistence economy, and willingness to adapt made them well-suited to periods of instability, whether caused by climate change or plague.&lt;/p&gt;
    &lt;p&gt;The new genetic findings support this interpretation. Mostly where early Slavic groups are found in the archaeological and historical record, their genetic traces match: a common ancestral origin, but regional differences shaped by the degree of mixing with local populations. In the north, earlier Germanic peoples had largely moved away, leaving room for Slavic settlement. In the south, the Eastern European newcomers merged with established communities. This patchwork process explains the remarkable diversity found in the cultures, languages, and even the genetics of today‚Äôs Central and Eastern Europe. "The spread of the Slavs was likely the last demographic event of continental scale to permanently and fundamentally reshape both the genetic and linguistic landscape of Europe." says Johannes Krause, director at the Max Planck Institute for Evolutionary Anthropology and one of the senior authors of the study.&lt;/p&gt;
    &lt;p&gt;With these new results, researchers can finally see beyond the gaps in the written and archaeological record to trace the true scope of the Slavic migrations‚Äîone of the most influential yet understated chapters in Europe‚Äôs past. The echoes of this history remain today, in the languages, cultures, and even the DNA of millions across the continent.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45163598</guid></item><item><title>'Make invalid states unrepresentable' considered harmful</title><link>https://www.seangoedecke.com/invalid-states/</link><description>&lt;doc fingerprint="6bfb795bae006bd6"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;'Make invalid states unrepresentable' considered harmful&lt;/head&gt;
    &lt;p&gt;One of the most controversial things I believe about good software design is that your code should be more flexible than your domain model. This is in direct opposition to a lot of popular design advice, which is all about binding your code to your domain model as tightly as possible.&lt;/p&gt;
    &lt;p&gt;For instance, a popular principle for good software design is to make invalid states unrepresentable. This usually means doing two things:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Enforcing a single source of truth in your database schema. If users and profiles are associated with a &lt;code&gt;user_id&lt;/code&gt;on the&lt;code&gt;profiles&lt;/code&gt;table, don‚Äôt also put a&lt;code&gt;profile_id&lt;/code&gt;on the&lt;code&gt;users&lt;/code&gt;table, because then you could have a mismatch.&lt;/item&gt;
      &lt;item&gt;Enforcing stricter types. If you use an ‚Äúpublished/pending‚Äù enum to track comment status instead of a string field, you don‚Äôt have to worry about weird strings you don‚Äôt expect.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;I can see why people like this principle. The more you can constrain your software to match your domain model, the easier it will be easier to reason about. However, it‚Äôs possible to take it too far. In my view, your software should include as few hard constraints as possible. Real-world software is already subject to the genuinely hard constraints of the real world. If you add further constraints to make your software neater, you risk making it difficult to change when you really, really have to. Because of this, good software design should allow the system to represent some invalid states.&lt;/p&gt;
    &lt;head rend="h3"&gt;State machines should allow arbitrary state transitions&lt;/head&gt;
    &lt;p&gt;For instance, it‚Äôs popular advice to represent many complex software processes as a ‚Äústate machine‚Äù. Instead of writing ad-hoc code, you can label the various states the system can be in and define a graph of which states can transition to which other states. The edges of that graph become your system‚Äôs actions.&lt;/p&gt;
    &lt;p&gt;Here‚Äôs an example. If you run an app marketplace, you might thus define a set of states like ‚Äúdraft‚Äù, ‚Äúpending review‚Äù, ‚Äúapproved‚Äù, and ‚Äúpublished‚Äù. The actions that connect those states might be ‚Äúsubmit‚Äù, ‚Äúapprove‚Äù, ‚Äúreject‚Äù, ‚Äúpublish‚Äù and ‚Äúhide‚Äù.&lt;/p&gt;
    &lt;p&gt;Note that you can only submit a draft app, you can only reject a pending app, you can only hide a published app, and so on. These constraints are the entire point of using a state machine. It‚Äôs the constraints that make the system much easier to reason about: instead of a ton of app state that could all be modified independently, you have four possible states and five possible actions.&lt;/p&gt;
    &lt;p&gt;The problem, of course, is in the edge cases. What happens when you need to account for ‚Äúofficial‚Äù apps, which are developed internally and shouldn‚Äôt go through the normal review process? What happens when a key partner‚Äôs app is mistakenly rejected, and the engineering team is asked to ‚Äúun-reject‚Äù it without forcing the partner to resubmit? What happens when a published app has to be hidden in a way that prevents it from being published again?&lt;/p&gt;
    &lt;p&gt;There are two ways to handle edge cases in a state machine. The first is to update the design. Maybe you can add an ‚Äúofficial‚Äù status that can directly move to ‚Äúpublished‚Äù without review, or a ‚Äúmanually-approved‚Äù action that can take an app straight from ‚Äúdraft‚Äù to ‚Äúapproved‚Äù, or a ‚Äúhide-and-reject‚Äù action that can take an app from ‚Äúpublished‚Äù back to ‚Äúdraft‚Äù. However, this can dramatically complicate the design:&lt;/p&gt;
    &lt;p&gt;The second way to handle edge cases is to allow arbitrary state transitions. In other words, to relax the constraint that forces state machines to transition only via predefined actions. This keeps the core design simple, at the cost of allowing exceptions.&lt;/p&gt;
    &lt;p&gt;In almost all cases, you should update the design (for instance, any app marketplace needs a ‚Äúhide-and-reject‚Äù action handy). But you need to remain flexible enough to allow some arbitrary transitions. Any engineering team that owns a customer-facing service will always be asked to do arbitrary one-off tasks. If you redesign your software each time to allow them, you will end up in a nasty tangle1. Thus you should ensure that your technical constraints are not absolute.&lt;/p&gt;
    &lt;head rend="h3"&gt;Foreign key constraints&lt;/head&gt;
    &lt;p&gt;Abnother classic example of this is foreign key constraints. In a relational database, tables are related by primary key (typically ID): a &lt;code&gt;posts&lt;/code&gt; table will have a &lt;code&gt;user_id&lt;/code&gt; column to show which user owns which post, corresponding to the value of the &lt;code&gt;id&lt;/code&gt; column in the &lt;code&gt;users&lt;/code&gt; table. When you want to fetch the posts belonging to user 3, you‚Äôll run SQL like &lt;code&gt;SELECT * FROM posts WHERE user_id = 3&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;A foreign key constraint forces &lt;code&gt;user_id&lt;/code&gt; to correspond to an actual row in the &lt;code&gt;users&lt;/code&gt; table. If you try to create or update a post with user_id 999, and there is no user with that id, the foreign key constraint will cause the SQL query to fail.&lt;/p&gt;
    &lt;p&gt;This sounds great, right? A record pointing at a non-existent user is in an invalid state. Shouldn‚Äôt we want it to be impossible to represent invalid states? However, many large tech companies - including the two I‚Äôve worked for, GitHub and Zendesk - deliberately choose not to use foreign key constraints. Why not?&lt;/p&gt;
    &lt;p&gt;The main reason is flexibility2. In practice, it‚Äôs much easier to deal with some illegal states in application logic (like posts with no user attached) than it is to deal with the constraint. With foreign key constraints, you have to delete all related records when a parent record is deleted. That might be okay for users and posts - though it could become a very expensive operation - but what about relationships that are less solid? If a post has a &lt;code&gt;reviewer_id&lt;/code&gt;, what happens when that reviewer‚Äôs account is deleted? It doesn‚Äôt seem right to delete the post, surely. And so on.&lt;/p&gt;
    &lt;p&gt;If you want to change the database schema, foreign key constraints can be a big problem. Maybe you want to move a table to a different database cluster or shard. If it has any foreign key relationships to other tables, watch out! If you‚Äôre not also moving those tables over, you‚Äôll have to remove the foreign key constraint then anyway. Even if you are moving those tables too, it‚Äôs a giant hassle to move the data in a way that‚Äôs compliant with the constraint, because you can‚Äôt just replicate a single table at a time - you have to move the data in chunks that keep the foreign key relationships intact.&lt;/p&gt;
    &lt;p&gt;The principle here is the same as with state machines: at some point you will be forced to do something that violates your tidy constraints, and if you‚Äôve made those constraints truly immovable you‚Äôre buying yourself a lot of trouble.&lt;/p&gt;
    &lt;head rend="h3"&gt;Protocol buffers and required fields&lt;/head&gt;
    &lt;p&gt;For a third example, consider Protocol Buffers. Protobufs are Google‚Äôs popular open-source serialization format. The first iteration of protobufs allowed you to tag fields as &lt;code&gt;required&lt;/code&gt;. If a client parsing a protobuf saw it was missing a required field, that client would reject the message. This sounds sensible enough, right? Many kinds of message don‚Äôt make any sense without certain values, so why not encode that constraint into the serialization layer? Isn‚Äôt it good to make invalid messages impossible to represent?&lt;/p&gt;
    &lt;p&gt;However, in the second iteration, Google dropped the ability to mark any field as required. This was a controversial decision. In fact, many believe that all proto fields should always be required, on the grounds that more constraints make the underlying types more elegant and easier to read about. For the other side of the argument, read this Hacker News comment from a protobuf designer.&lt;/p&gt;
    &lt;p&gt;In my view, this debate comes down to how seriously you take the problem of changing schemas in a system with multiple consumers. If you want to add a required field to a protobuf, you have to do it like so:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Add the required field to every service that creates the protobuf from-scratch&lt;/item&gt;
      &lt;item&gt;Add the required field to any middlemen that are taking the protobuf and passing it on to some other system&lt;/item&gt;
      &lt;item&gt;Add the required field to all other consumers&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you do this out-of-order, messages get dropped on the floor, likely causing some kind of production outage. Removing a required field requires a similar order-dependent process, except in reverse - consumers must drop the field first, followed by middlemen, followed by producers. If you forget to upgrade a consumer service schema (not as unlikely as it sounds, in large companies with thousands of half-forgotten services), the part of it that needs the protobuf will just stop working.&lt;/p&gt;
    &lt;p&gt;When you know all fields are optional, you can change protobuf schemas in a completely order-independent way. All services can upgrade to the new version of the schema more or less at their convenience. The tradeoff is that you won‚Äôt have the data until both you and the producer are upgraded to the new schema, so you‚Äôll need to handle that case in your application code.&lt;/p&gt;
    &lt;p&gt;In case you couldn‚Äôt tell, I am very much on the Prococol Buffers side of the debate. Having done a lot of schema changes of various kinds, I think it is safer to tolerate incomplete data at the application level during a schema upgrade than be forced to upgrade services in the right order or risk an outage. In other words, I think application code should be willing to tolerate data that violates the domain model.&lt;/p&gt;
    &lt;head rend="h3"&gt;Final thoughts&lt;/head&gt;
    &lt;p&gt;The harder the constraint, the more dangerous it is. When I say that a constraint is hard, I mean that it is very difficult to undo it if you need to. A line of code validating something is a soft constraint, because you can simply remove the line if needed. Something baked into a database schema is a harder constraint, because it requires a migration to change, which (depending on the amount of data and the read volume) can be operationally very difficult. Some constraints are built into the architecture of the entire system: consider the ‚Äúno data is ever truly deleted‚Äù constraint in blockchain or ledger-based systems3.&lt;/p&gt;
    &lt;p&gt;For most software, domain models are not real. A domain model is only a model of real-world processes. Because of that, the constraints inherent to the domain model (like ‚Äútickets must always be marked as completed before being archived‚Äù) cannot be truly hard constraints. This is trivially true about most line-of-business or SaaS software, and gets less true the more generic and library-like your software is. If you‚Äôre writing a library to do efficient matrix multiplications, you can get away with much harder constraints than if you‚Äôre writing directly user-facing code. For much more on this, see my post Pure and impure software engineering.&lt;/p&gt;
    &lt;p&gt;I am not arguing that all constraints are bad. Constraints make a system possible to reason about, and the harder the constraint, the better it does its job. A system with no constraints at all (or only very soft constraints) is more of a programming language than a program. I like many kinds of hard constraint: for instance, I prefer protobufs to JSON, I like type signatures, and I strongly prefer relational databases with a set schema to schemaless databases. However, user-facing software will eventaully be forced to break many of its constraints in the interest of better fulfilling the real-world goal of that software. Thus, some invalid states ought to be representable.&lt;/p&gt;
    &lt;p&gt;edit: apologies to my email subscribers, the version of this that went out over email had a typo in the title (it read ‚Äúrepresentable‚Äù instead of ‚Äúunrepresentable‚Äù).&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;The other solution some engineers seem to like - refusing to do the task, on the grounds that it‚Äôd compromise the software design - is a non-starter, in my opinion. As engineers, it‚Äôs our job to support the needs of the business.&lt;/p&gt;‚Ü©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Foreign key constraints also have performance issues at scale, make database migrations very difficult when you‚Äôre touching the foreign key column, and complicate common big-company patterns like soft-deletes.&lt;/p&gt;‚Ü©&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;What would it take to allow for true data deletion in a blockchain (for instance, to comply with GDPR)? You would need to change the protocol to something like how Kafka handles true deletion: allowing ‚Äútombstone‚Äù records to be written into the ledger and then safely compacted away by every node. I leave ‚Äúhow do you safely compact a portion of a Merkle tree in a zero-trust environment‚Äù as an exercise for the reader.&lt;/p&gt;‚Ü©&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you liked this post, consider subscribing to email updates about my new posts, or sharing it on Hacker News.&lt;/p&gt;
    &lt;p&gt;September 8, 2025 ‚îÇ Tags: software design&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45164444</guid></item><item><title>AI Adoption Rate Trending Down for Large Companies</title><link>https://www.apolloacademy.com/ai-adoption-rate-trending-down-for-large-companies/</link><description>&lt;doc fingerprint="b71ba2579973b144"&gt;
  &lt;main&gt;
    &lt;p&gt;The US Census Bureau conducts a biweekly survey of 1.2 million firms, and one question is whether a business has used AI tools such as machine learning, natural language processing, virtual agents or voice recognition to help produce goods or services in the past two weeks. Recent data by firm size shows that AI adoption has been declining among companies with more than 250 employees, see chart below.&lt;/p&gt;
    &lt;p&gt;The bottom line is that the biweekly Census data is starting to show a slowdown in AI adoption for large companies.&lt;/p&gt;
    &lt;p&gt;This presentation may not be distributed, transmitted or otherwise communicated to others in whole or in part without the express consent of Apollo Global Management, Inc. (together with its subsidiaries, ‚ÄúApollo‚Äù).&lt;/p&gt;
    &lt;p&gt;Apollo makes no representation or warranty, expressed or implied, with respect to the accuracy, reasonableness, or completeness of any of the statements made during this presentation, including, but not limited to, statements obtained from third parties. Opinions, estimates and projections constitute the current judgment of the speaker as of the date indicated. They do not necessarily reflect the views and opinions of Apollo and are subject to change at any time without notice. Apollo does not have any responsibility to update this presentation to account for such changes. There can be no assurance that any trends discussed during this presentation will continue.&lt;/p&gt;
    &lt;p&gt;Statements made throughout this presentation are not intended to provide, and should not be relied upon for, accounting, legal or tax advice and do not constitute an investment recommendation or investment advice. Investors should make an independent investigation of the information discussed during this presentation, including consulting their tax, legal, accounting or other advisors about such information. Apollo does not act for you and is not responsible for providing you with the protections afforded to its clients. This presentation does not constitute an offer to sell, or the solicitation of an offer to buy, any security, product or service, including interest in any investment product or fund or account managed or advised by Apollo.&lt;/p&gt;
    &lt;p&gt;Certain statements made throughout this presentation may be ‚Äúforward-looking‚Äù in nature. Due to various risks and uncertainties, actual events or results may differ materially from those reflected or contemplated in such forward-looking information. As such, undue reliance should not be placed on such statements. Forward-looking statements may be identified by the use of terminology including, but not limited to, ‚Äúmay‚Äù, ‚Äúwill‚Äù, ‚Äúshould‚Äù, ‚Äúexpect‚Äù, ‚Äúanticipate‚Äù, ‚Äútarget‚Äù, ‚Äúproject‚Äù, ‚Äúestimate‚Äù, ‚Äúintend‚Äù, ‚Äúcontinue‚Äù or ‚Äúbelieve‚Äù or the negatives thereof or other variations thereon or comparable terminology.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=45165019</guid></item></channel></rss>