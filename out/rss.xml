<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 24 Dec 2025 04:53:31 +0000</lastBuildDate><item><title>Fabrice Bellard Releases MicroQuickJS</title><link>https://github.com/bellard/mquickjs/blob/main/README.md</link><description>&lt;doc fingerprint="3b74d0de090e4684"&gt;
  &lt;main&gt;
    &lt;p&gt;We read every piece of feedback, and take your input very seriously.&lt;/p&gt;
    &lt;p&gt;To see all available qualifiers, see our documentation.&lt;/p&gt;
    &lt;p&gt;There was an error while loading. Please reload this page.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46367224</guid><pubDate>Tue, 23 Dec 2025 17:33:42 +0000</pubDate></item><item><title>Towards a secure peer-to-peer app platform for Clan</title><link>https://clan.lol/blog/towards-app-platform-vmtech/</link><description>&lt;doc fingerprint="97bdc952c9982edd"&gt;
  &lt;main&gt;
    &lt;p&gt;While most of the existing Clan framework is dedicated to machine and service management, there’s more on the horizon. Our mission is to make sure peer-to-peer, user-controlled, community software can beat Big Tech solutions. That’s why we’re working on platform fundamentals that would open the way for our FOSS stack to match the usability and convenience of proprietary platforms.&lt;/p&gt;
    &lt;p&gt;Unfortunately, the FOSS world is still lagging behind commercial platforms in some important aspects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Web and mobile apps are strongly sandboxed, so while they can get very aggressive in snooping on the data they are allowed to access, the enforcement of the isolation model is very robust — and there is a model for sharing data that makes the isolated applications actually useful.. &lt;list rend="ul"&gt;&lt;item&gt;Meanwhile in the FOSS world, it’s still extremely common to run software with full access to the user’s account. The only project that has built anything close to a similar platform for local software is Flatpak, which is still not perfect and its main repo has a very lax policy;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Centralized Web services can have “multiple instances” simply by switching accounts; self-hosting Web services is trivially multi-instance; even Android now provides a multi-instance facility.. &lt;list rend="ul"&gt;&lt;item&gt;Meanwhile local software often doesn’t have a global database, but when it does, it can be impossible to make it multi-instance without advanced knowledge;&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;Commercial apps come with their own always-online remote servers. Users don’t have to think about connecting the clients to the servers, it’s all pre-connected! &lt;list rend="ul"&gt;&lt;item&gt;Meanwhile decentralized community software is stuck between various bad options. Supporting multiple commercial backends is tedious and defeats the point anyway. Self-hosting traditional web servers can get complex and unreliable, and exposes attack surface to the public Web. Direct peer-to-peer connections can be hard to set up and unreliable too, and typically don’t provide asynchronous communication.&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;So… What do we need to make it possible for communities to share apps install and load quickly, already pre-connected to network services; are isolated to a worry-free level of security, and yet allow for enough sharing via explicit permissions to make them useful?&lt;/p&gt;
    &lt;p&gt;The first piece of the puzzle is, unsurprisingly, Nix. The entire Clan project is built on Nix, and the future app platform is no exception. Nix makes it possible to quickly fetch and run any software – thanks to caching, as long as we steer everyone towards using very few common versions of the nixpkgs tree, most downloads could be almost as fast as web app loads.&lt;/p&gt;
    &lt;p&gt;Then we have to add a microVM hypervisor with Wayland and GPU virtualization and a side of D-Bus portals… and we can finally get a glimpse of the future!&lt;/p&gt;
    &lt;head rend="h2"&gt;microVMs&lt;/head&gt;
    &lt;p&gt;Secure isolation is essential for any modern app platform. Hardware-based virtualization is a lot more confidence-inspiring than shared-kernel isolation mechanisms like Linux namespaces. But it’s not only a security measure. Running apps in VMs also improves environment consistency/reproducibility by ensuring everyone runs the same kernel — which can also give us portability, since it enables running on completely different host OSes as well.&lt;/p&gt;
    &lt;p&gt;If your experience with virtualization on desktop has only been with booting entire Linux distros under something like VirtualBox, you might be very skeptical of the same technology being involved in launching applications all the time. But that’s not at all inherent to the use of KVM!&lt;/p&gt;
    &lt;p&gt;Conventional VMs feel “heavy” —slow to launch, big RAM footprint, extra background CPU usage, fixed storage allocation, usually not very well integrated with the host desktop— only because their goal is to simulate a whole another computer within your existing computer. For app isolation, we don’t need that, so the whole stack can be vastly simplified and optimized for high performance and low overhead. The microVM idea was first popularized by AWS’s Firecracker on the server side, powering instantly-launching event/request handlers in Lambda. A microVM boots directly into the kernel (skipping firmware) and does not emulate any legacy PC hardware, which results in very fast boot times, on the order of a couple hundred milliseconds.&lt;/p&gt;
    &lt;p&gt;Now, has this been used on the client side already? Yes, most prominently by Asahi Linux, motivated by a technical restriction that was preventing Apple machines from playing legacy Windows games. That’s the muvm project, powered by libkrun – a Firecracker-like VMM provided as a dynamic library so that different frontends could be built. For our platform development, we have indeed adopted muvm (after submitting some changes that make it more useful for us), combining it with namespace-based Bubblewrap to make a script that runs NixOS system closures in microVMs.&lt;/p&gt;
    &lt;p&gt;…Wait, did someone mention playing games– like, highly GPU-demanding games? In a VM? Without a dedicated GPU?&lt;/p&gt;
    &lt;head rend="h2"&gt;Desktop and GPU support&lt;/head&gt;
    &lt;p&gt;In the Beginning (of virtio-gpu), there was the Framebuffer. An emulated computer monitor, a single rectangle representing the entire graphical output of the VM. Then there was VirGL, a way to forward the OpenGL API across the VM boundary to make the host render on its GPU on behalf of the VM, so that 3D graphics could be displayed on the emulated monitor. It wasn’t super fast, it wasn’t compatible with the latest GL extensions, it wasn’t very secure, but it was something. With the advent of Vulkan, Venus was started as the Vulkan version of the same thing.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the Chrome OS team was working on adding support for Linux apps. While it was initially based on namespaces, they quickly started working on switching to hardware virtualization. The virtio-gpu device was extended to support arbitrary “cross-domain” protocols, making it possible —with some wrapping-unwrapping— to forward Unix domain sockets that pass certain types of file descriptors (shared memory and DMA-BUF) to the guest. (Well, initially it was a whole separate virtual device but let’s skip over that.) Google’s crosvm supports connecting to the host Wayland socket to that facility, and the team wrote Sommelier as the guest-side proxy that exposes a normal Wayland socket to guest apps.&lt;/p&gt;
    &lt;p&gt;The part of crosvm responsible for handling the virtio-gpu device was written as a reusable library called Rutabaga (now living outside of the CrOS repos), and integrated into other VMMs such as good old Qemu. Sommelier was packaged by various Linux distros as well, and one enthusiast wrote an entire alternative to Sommelier.&lt;/p&gt;
    &lt;p&gt;Meanwhile, there was also a lot to improve in terms of accessing the GPU. As we’ve mentioned already, API forwarding solutions like VirGL/Venus leave a lot to be desired. PCIe passthrough requires a dedicated GPU, or SR-IOV support which GPU vendors have mostly restricted to enterprise models. However… of course a better way was possible! Rob Clark presented DRM native contexts at XDC 2022: this approach essentially paravirtualizes the kernel-space GPU driver, letting the guest submit hardware-specific commands that the host would run in separate contexts (relying on the same separation as between programs on the host). That’s the approach that was picked up by the Asahi Linux project for gaming because of the amazing performance it allows for, but it’s also intended to be a stronger security boundary due to providing way less attack surface on the host (it’s all I/O management rather than implementing complex APIs).&lt;/p&gt;
    &lt;p&gt;So, was it possible to take all of this technology and use it? Well… it required quite a bit of debugging and fixing everywhere – but that’s exactly why I joined! So far I’ve discovered that:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the rutabaga_gfx integration in QEMU (which was the thing we tried to use initially) and other C consumers was broken with the latest versions due to an &lt;code&gt;ifdef&lt;/code&gt;mistake (fixed)&lt;/item&gt;
      &lt;item&gt;it’s not documented everywhere that kernel &amp;gt;= 6.13 is required to be able to touch AMD GPU memory from KVM guests in any way&lt;/item&gt;
      &lt;item&gt;Sommelier was assuming Chromium OS kernel patches and misinterpreting &lt;code&gt;ioctl&lt;/code&gt;responses on regular mainline Linux&lt;/item&gt;
      &lt;item&gt;libkrun’s internal version of rutabaga_gfx contained a tiny strange API modification incompatible with Sommelier/proxy-virtwl and didn’t handle memfd seals (fixed)&lt;/item&gt;
      &lt;item&gt;RADV (Radeon Vulkan driver in Mesa) only recognized PCI devices including for virtgpu, ignoring the &lt;code&gt;virtio-mmio&lt;/code&gt;setup used by libkrun (fixed)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;And we’re continuing with more work in this area.&lt;/p&gt;
    &lt;head rend="h2"&gt;D-Bus / XDG Desktop Portals&lt;/head&gt;
    &lt;p&gt;Application isolation is great, but completely isolated applications tend to have limited usefulness. That’s why we’re also integrating desktop portals that Flatpak uses —at least the file-opening / document portal— into the microVM-based platform.&lt;/p&gt;
    &lt;p&gt;The sidebus project is inspired by Spectrum’s setup for using the document portal with virtiofs to dynamically expose chosen files to the guest, using vsock as the D-Bus transport. It is based on the busd broker library, and uses the portal frontend on the host for perfect integration with arbitrary desktop environments.&lt;/p&gt;
    &lt;p&gt;With the switch to libkrun however, we are looking at the possibility of making the Camera and Screencast portals working, with full hardware acceleration – by switching to virtgpu cross-domain as the transport instead of vsock. Currently libkrun already has added some PipeWire support to its copy of rutabaga_gfx, however that’s fixed to one system-wide socket. How these portals work is that for every request they pass a new restricted PipeWire remote socket over D-Bus. So we’re looking to make rutabaga’s cross-domain sockets more generic, to be able to just pass through that whole chain of file descriptor passing.&lt;/p&gt;
    &lt;p&gt;(And yes, lots of people are worried about PipeWire attack surface — it’s definitely possible to mitigate that with a proxy on the host that would only allow a small validated subset of the PipeWire protocol.)&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;We’re looking to finally make a peer-to-peer community software platform that’s competitive with commercial ones in terms of security, usability and convenience. If you want to try it out now, you can! Just follow the installation instructions on our munix project. Note that it’s still actively being developed, so if you find any issues, please open up a bug report!&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46367232</guid><pubDate>Tue, 23 Dec 2025 17:34:22 +0000</pubDate></item><item><title>We replaced H.264 streaming with JPEG screenshots (and it worked better)</title><link>https://blog.helix.ml/p/we-mass-deployed-15-year-old-screen</link><description>&lt;doc fingerprint="26854c146509ce1d"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;We Mass-Deployed 15-Year-Old Screen Sharing Technology and It's Actually Better&lt;/head&gt;
    &lt;head rend="h3"&gt;Or: How JPEG Screenshots Defeated Our Beautiful H.264 WebCodecs Pipeline&lt;/head&gt;
    &lt;p&gt;Part 2 of our video streaming saga. Read Part 1: How we replaced WebRTC with WebSockets →&lt;/p&gt;
    &lt;head rend="h2"&gt;The Year is 2025 and We’re Sending JPEGs&lt;/head&gt;
    &lt;p&gt;Let me tell you about the time we spent three months building a gorgeous, hardware-accelerated, WebCodecs-powered, 60fps H.264 streaming pipeline over WebSockets...&lt;/p&gt;
    &lt;p&gt;...and then replaced it with &lt;code&gt;grim | curl&lt;/code&gt; when the WiFi got a bit sketchy.&lt;/p&gt;
    &lt;p&gt;I wish I was joking.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act I: Hubris (Also Known As “Enterprise Networking Exists”)&lt;/head&gt;
    &lt;p&gt;We’re building Helix, an AI platform where autonomous coding agents work in cloud sandboxes. Users need to watch their AI assistants work. Think “screen share, but the thing being shared is a robot writing code.”&lt;/p&gt;
    &lt;p&gt;Last week, we explained how we replaced WebRTC with a custom WebSocket streaming pipeline. This week: why that wasn’t enough.&lt;/p&gt;
    &lt;p&gt;The constraint that ruined everything: It has to work on enterprise networks.&lt;/p&gt;
    &lt;p&gt;You know what enterprise networks love? HTTP. HTTPS. Port 443. That’s it. That’s the list.&lt;/p&gt;
    &lt;p&gt;You know what enterprise networks hate?&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;UDP — Blocked. Deprioritized. Dropped. “Security risk.”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebRTC — Requires TURN servers, which requires UDP, which is blocked&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Custom ports — Firewall says no&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;STUN/ICE — NAT traversal? In my corporate network? Absolutely not&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Literally anything fun — Denied by policy&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We tried WebRTC first. Worked great in dev. Worked great in our cloud. Deployed to an enterprise customer.&lt;/p&gt;
    &lt;p&gt;“The video doesn’t connect.”&lt;/p&gt;
    &lt;p&gt;checks network — Outbound UDP blocked. TURN server unreachable. ICE negotiation failing.&lt;/p&gt;
    &lt;p&gt;We could fight this. Set up TURN servers. Configure enterprise proxies. Work with IT departments.&lt;/p&gt;
    &lt;p&gt;Or we could accept reality: Everything must go through HTTPS on port 443.&lt;/p&gt;
    &lt;p&gt;So we built a pure WebSocket video pipeline:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;H.264 encoding via GStreamer + VA-API (hardware acceleration, baby)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Binary frames over WebSocket (L7 only, works through any proxy)&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebCodecs API for hardware decoding in the browser&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;60fps at 40Mbps with sub-100ms latency&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We were so proud. We wrote Rust. We wrote TypeScript. We implemented our own binary protocol. We measured things in microseconds.&lt;/p&gt;
    &lt;p&gt;Then someone tried to use it from a coffee shop.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act II: Denial&lt;/head&gt;
    &lt;p&gt;“The video is frozen.”&lt;/p&gt;
    &lt;p&gt;“Your WiFi is bad.”&lt;/p&gt;
    &lt;p&gt;“No, the video is definitely frozen. And now my keyboard isn’t working.”&lt;/p&gt;
    &lt;p&gt;checks the video&lt;/p&gt;
    &lt;p&gt;It’s showing what the AI was doing 30 seconds ago. And the delay is growing.&lt;/p&gt;
    &lt;p&gt;Turns out, 40Mbps video streams don’t appreciate 200ms+ network latency. Who knew.&lt;/p&gt;
    &lt;p&gt;When the network gets congested:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Frames buffer up in the TCP/WebSocket layer&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;They arrive in-order (thanks TCP!) but increasingly delayed&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Video falls further and further behind real-time&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;You’re watching the AI type code from 45 seconds ago&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;By the time you see a bug, the AI has already committed it to main&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Everything is terrible forever&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;“Just lower the bitrate,” you say. Great idea. Now it’s 10Mbps of blocky garbage that’s still 30 seconds behind.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act III: Bargaining&lt;/head&gt;
    &lt;p&gt;We tried everything:&lt;/p&gt;
    &lt;p&gt;“What if we only send keyframes?”&lt;/p&gt;
    &lt;p&gt;This was our big brain moment. H.264 keyframes (IDR frames) are self-contained. No dependencies on previous frames. Just drop all the P-frames on the server side, send only keyframes, get ~1fps of corruption-free video. Perfect for low-bandwidth fallback!&lt;/p&gt;
    &lt;p&gt;We added a &lt;code&gt;keyframes_only&lt;/code&gt; flag. We modified the video decoder to check &lt;code&gt;FrameType::Idr&lt;/code&gt;. We set GOP to 60 (one keyframe per second at 60fps). We tested.&lt;/p&gt;
    &lt;p&gt;We got exactly ONE frame.&lt;/p&gt;
    &lt;p&gt;One single, beautiful, 1080p IDR frame. Then silence. Forever.&lt;/p&gt;
    &lt;code&gt;[WebSocket] Keyframe received (frame 121), sending
[WebSocket] ...
[WebSocket] ...
[WebSocket] It's been 14 seconds why is nothing else coming
[WebSocket] Failed to send audio frame: Closed&lt;/code&gt;
    &lt;p&gt;checks Wolf logs — encoder still running&lt;/p&gt;
    &lt;p&gt;checks GStreamer pipeline — frames being produced&lt;/p&gt;
    &lt;p&gt;checks Moonlight protocol layer — nothing coming through&lt;/p&gt;
    &lt;p&gt;We’re using Wolf, an excellent open-source game streaming server (seriously, the documentation is great). But our WebSocket streaming layer sits on top of the Moonlight protocol, which is reverse-engineered from NVIDIA GameStream. Somewhere in that protocol stack, something decides that if you’re not consuming P-frames, you’re not ready for more frames. Period.&lt;/p&gt;
    &lt;p&gt;We poked around for an hour or two, but without diving deep into the Moonlight protocol internals, we weren’t going to fix this. The protocol wanted all its frames, or no frames at all.&lt;/p&gt;
    &lt;p&gt;“What if we implement proper congestion control?”&lt;/p&gt;
    &lt;p&gt;looks at TCP congestion control literature&lt;/p&gt;
    &lt;p&gt;closes tab&lt;/p&gt;
    &lt;p&gt;“What if we just... don’t have bad WiFi?”&lt;/p&gt;
    &lt;p&gt;stares at enterprise firewall that’s throttling everything&lt;/p&gt;
    &lt;head rend="h2"&gt;Act IV: Depression&lt;/head&gt;
    &lt;p&gt;One late night, while debugging why the stream was frozen again, I opened our screenshot debugging endpoint in a browser tab:&lt;/p&gt;
    &lt;code&gt;GET /api/v1/external-agents/abc123/screenshot?format=jpeg&amp;amp;quality=70&lt;/code&gt;
    &lt;p&gt;The image loaded instantly.&lt;/p&gt;
    &lt;p&gt;A pristine, 150KB JPEG of the remote desktop. Crystal clear. No artifacts. No waiting for keyframes. No decoder state. Just... pixels.&lt;/p&gt;
    &lt;p&gt;I refreshed. Another instant image.&lt;/p&gt;
    &lt;p&gt;I mashed F5 like a degenerate. 5 FPS of perfect screenshots.&lt;/p&gt;
    &lt;p&gt;I looked at my beautiful WebCodecs pipeline. I looked at the JPEGs. I looked at the WebCodecs pipeline again.&lt;/p&gt;
    &lt;p&gt;No.&lt;/p&gt;
    &lt;p&gt;No, we are not doing this.&lt;/p&gt;
    &lt;p&gt;We are professionals. We implement proper video codecs. We don’t spam HTTP requests for individual frames like it’s 2009.&lt;/p&gt;
    &lt;head rend="h2"&gt;Act V: Acceptance&lt;/head&gt;
    &lt;code&gt;// Poll screenshots as fast as possible (capped at 10 FPS max)
const fetchScreenshot = async () =&amp;gt; {
  const response = await fetch(`/api/v1/external-agents/${sessionId}/screenshot`)
  const blob = await response.blob()
  screenshotImg.src = URL.createObjectURL(blob)
  setTimeout(fetchScreenshot, 100) // yolo
}&lt;/code&gt;
    &lt;p&gt;We did it. We’re sending JPEGs.&lt;/p&gt;
    &lt;p&gt;And you know what? It works perfectly.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why JPEGs Actually Slap&lt;/head&gt;
    &lt;p&gt;Here’s the thing about our fancy H.264 pipeline:&lt;/p&gt;
    &lt;p&gt;A JPEG screenshot is self-contained. It either arrives complete, or it doesn’t. There’s no “partial decode.” There’s no “waiting for the next keyframe.” There’s no “decoder state corruption.”&lt;/p&gt;
    &lt;p&gt;When the network is bad, you get... fewer JPEGs. That’s it. The ones that arrive are perfect.&lt;/p&gt;
    &lt;p&gt;And the size! A 70% quality JPEG of a 1080p desktop is like 100-150KB. A single H.264 keyframe is 200-500KB. We’re sending LESS data per frame AND getting better reliability.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Hybrid: Have Your Cake and Eat It Too&lt;/head&gt;
    &lt;p&gt;We didn’t throw away the H.264 pipeline. We’re not complete animals.&lt;/p&gt;
    &lt;p&gt;Instead, we built adaptive switching:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Good connection (RTT &amp;lt; 150ms): Full 60fps H.264, hardware decoded, buttery smooth&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Bad connection detected: Pause video, switch to screenshot polling&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Connection recovers: User clicks to retry video&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The key insight: we still need the WebSocket for input.&lt;/p&gt;
    &lt;p&gt;Keyboard and mouse events are tiny. Like, 10 bytes each. The WebSocket handles those perfectly even on a garbage connection. We just needed to stop sending the massive video frames.&lt;/p&gt;
    &lt;p&gt;So we added one control message:&lt;/p&gt;
    &lt;p&gt;json&lt;/p&gt;
    &lt;code&gt;{"set_video_enabled": false}&lt;/code&gt;
    &lt;p&gt;Server receives this, stops sending video frames. Client polls screenshots instead. Input keeps flowing. Everyone’s happy.&lt;/p&gt;
    &lt;p&gt;15 lines of Rust. I am not joking.&lt;/p&gt;
    &lt;p&gt;rust&lt;/p&gt;
    &lt;code&gt;if !video_enabled.load(Ordering::Relaxed) {
    continue; // skip frame, it's screenshot time baby
}&lt;/code&gt;
    &lt;head rend="h2"&gt;The Oscillation Problem (Lol)&lt;/head&gt;
    &lt;p&gt;We almost shipped a hilarious bug.&lt;/p&gt;
    &lt;p&gt;When you stop sending video frames, the WebSocket becomes basically empty. Just tiny input events and occasional pings.&lt;/p&gt;
    &lt;p&gt;The latency drops dramatically.&lt;/p&gt;
    &lt;p&gt;Our adaptive mode sees low latency and thinks: “Oh nice! Connection recovered! Let’s switch back to video!”&lt;/p&gt;
    &lt;p&gt;Video resumes. 40Mbps floods the connection. Latency spikes. Mode switches to screenshots.&lt;/p&gt;
    &lt;p&gt;Latency drops. Mode switches to video.&lt;/p&gt;
    &lt;p&gt;Latency spikes. Mode switches to screenshots.&lt;/p&gt;
    &lt;p&gt;Forever. Every 2 seconds.&lt;/p&gt;
    &lt;p&gt;The fix was embarrassingly simple: once you fall back to screenshots, stay there until the user explicitly clicks to retry.&lt;/p&gt;
    &lt;code&gt;setAdaptiveLockedToScreenshots(true) // no oscillation for you&lt;/code&gt;
    &lt;p&gt;We show an amber icon and a message: “Video paused to save bandwidth. Click to retry.”&lt;/p&gt;
    &lt;p&gt;Problem solved. User is in control. No infinite loops.&lt;/p&gt;
    &lt;head rend="h2"&gt;Ubuntu Doesn’t Ship JPEG Support in grim Because Of Course It Doesn’t&lt;/head&gt;
    &lt;code&gt;$ grim -t jpeg screenshot.jpg
error: jpeg support disabled&lt;/code&gt;
    &lt;p&gt;Oh, you thought we were done? Cute.&lt;/p&gt;
    &lt;p&gt;&lt;code&gt;grim&lt;/code&gt; is a Wayland screenshot tool. Perfect for our needs. Supports JPEG output for smaller files.&lt;/p&gt;
    &lt;p&gt;Except Ubuntu compiles it without libjpeg.&lt;/p&gt;
    &lt;p&gt;incredible&lt;/p&gt;
    &lt;p&gt;So now our Dockerfile has a build stage that compiles grim from source:&lt;/p&gt;
    &lt;code&gt;FROM ubuntu:25.04 AS grim-build
RUN apt-get install -y meson ninja-build libjpeg-turbo8-dev ...
RUN git clone https://git.sr.ht/~emersion/grim &amp;amp;&amp;amp; \
    meson setup build -Djpeg=enabled &amp;amp;&amp;amp; \
    ninja -C build&lt;/code&gt;
    &lt;p&gt;We’re building a screenshot tool from source so we can send JPEGs in 2025. This is fine.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Final Architecture&lt;/head&gt;
    &lt;code&gt;┌─────────────────────────────────────────────────────────────┐
│                     User's Browser                          │
├─────────────────────────────────────────────────────────────┤
│  WebSocket (always connected)                               │
│  ├── Video frames (H.264) ──────────── when RTT &amp;lt; 150ms     │
│  ├── Input events (keyboard/mouse) ── always                │
│  └── Control messages ─────────────── {"set_video_enabled"} │
│                                                             │
│  HTTP (screenshot polling) ──────────── when RTT &amp;gt; 150ms    │
│  └── GET /screenshot?quality=70                             │
└─────────────────────────────────────────────────────────────┘&lt;/code&gt;
    &lt;p&gt;Good connection: 60fps H.264, hardware accelerated, beautiful&lt;/p&gt;
    &lt;p&gt;Bad connection: 2-10fps JPEGs, perfectly reliable, works everywhere&lt;/p&gt;
    &lt;p&gt;The screenshot quality adapts too:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Frame took &amp;gt;500ms? Drop quality by 10%&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Frame took &amp;lt;300ms? Increase quality by 5%&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Target: minimum 2 FPS, always&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Lessons Learned&lt;/head&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Simple solutions often beat complex ones. Three months of H.264 pipeline work. One 2am hacking session the night before production deployment: “what if we just... screenshots?”&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Graceful degradation is a feature. Users don’t care about your codec. They care about seeing their screen and typing.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;WebSockets are for input, not necessarily video. The input path staying responsive is more important than video frames.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Ubuntu packages are missing random features. Always check. Or just build from source like it’s 2005.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Measure before optimizing. We assumed video streaming was the only option. It wasn’t.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;Try It Yourself&lt;/head&gt;
    &lt;p&gt;Helix is source available: github.com/helixml/helix&lt;/p&gt;
    &lt;p&gt;The shameful-but-effective screenshot code:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;api/cmd/screenshot-server/main.go&lt;/code&gt;— 200 lines of Go that changed everything&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;MoonlightStreamViewer.tsx&lt;/code&gt;— React component with adaptive logic&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;websocket-stream.ts&lt;/code&gt;— WebSocket client with&lt;code&gt;setVideoEnabled()&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The beautiful H.264 pipeline we’re still proud of:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;moonlight-web-stream/&lt;/code&gt;— Rust WebSocket server&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Still used when your WiFi doesn’t suck&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;We’re building Helix, open-source AI infrastructure that works in the real world — even on terrible WiFi. We started by killing WebRTC, then we killed our replacement. Sometimes the 15-year-old solution is the right one.&lt;/p&gt;
    &lt;p&gt;Want to experience the joy of interacting with an agent desktop at 6 JPEGs a second yourself? Join us for the private beta on Discord:&lt;/p&gt;
    &lt;p&gt;Star us on GitHub: github.com/helixml/helix&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46367475</guid><pubDate>Tue, 23 Dec 2025 18:00:31 +0000</pubDate></item><item><title>Volvo Centum is Dalton Maag's new typeface for Volvo</title><link>https://www.wallpaper.com/design-interiors/corporate-design-branding/volvo-new-font-volvo-centum</link><description>&lt;doc fingerprint="d51e5267b557a443"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Volvo’s quest for safety has resulted in this new, ultra-legible in-car typeface, Volvo Centum&lt;/head&gt;
    &lt;p&gt;Dalton Maag designs a new sans serif typeface for the Swedish carmaker, Volvo Centum, building on the brand’s strong safety ethos&lt;/p&gt;
    &lt;p&gt;Volvo will celebrate its centennial in 2027 and the company is already starting to ramp up the celebrations. Last year saw the opening of the World of Volvo in Gothenburg, a vast circular timber structure designed by Scandinavian design specialists Henning Larsen.&lt;/p&gt;
    &lt;p&gt;2026 will see another significant design upheaval, albeit on a much smaller and less obvious scale. The arrival of a new brand typeface, Volvo Centum, manages to combine the company’s obvious love of modernist design with its obsessive pursuit of safety.&lt;/p&gt;
    &lt;p&gt;The work of London-based type design studio Dalton Maag, the new typeface is designed ‘to improve readability, sharpen attention, and promote a calmer, safety-focused driving experience.’&lt;/p&gt;
    &lt;p&gt;With so much information conveyed via screens – especially capacitive touch screens – legibility and clarity have become an essential component of a modern car’s HMI. Volvo Centum is a sans serif of exceptional clarity and simplicity, designed for what the company calls ‘glance-driven environments.’&lt;/p&gt;
    &lt;p&gt;The first car to be installed with Volvo Centum is the newly revised XC60 mid-size SUV and its upcoming all-new EX60 electric sibling. Over-the-air updates will then be used to roll out the typeface across millions of other cars.&lt;/p&gt;
    &lt;p&gt;If there’s been one criticism of Volvo in recent years, it’s that the company has off-loaded huge amounts of information and driver input onto a central touchscreen. Volvo was ahead of the curve in adopting Google’s Android in 2017 and were a key partner in the development of Android Auto. How this system dovetails with the new typeface remains to be seen.&lt;/p&gt;
    &lt;p&gt;Dalton Maag faced a formidable challenge in ensuring Volvo Centum worked across the brand’s many platforms, as well as being legible in all driving conditions and in 35 different languages, including Chinese, Arabic, Japanese, and Korean.&lt;/p&gt;
    &lt;p&gt;Receive our daily digest of inspiration, escapism and design stories from around the world direct to your inbox.&lt;/p&gt;
    &lt;p&gt;By creating a set of distinct character shapes, the studio has striven to avoid any unintentional misreading, with clear spacing and a scaling system that simplifies detailed elements to retain legibility.&lt;/p&gt;
    &lt;p&gt;The studio, founded in 1991 by Swiss typeface designer Bruno Maag, has worked across a number of industries, including media (BBC, Netflix, USA Today), transportation (Ducati and Korean Air) and technology (Vodafone and Wix), amongst others.&lt;/p&gt;
    &lt;p&gt;DaltonMaag.com, @Dalton.Maag, Volvo.com, @Volvocars&lt;/p&gt;
    &lt;p&gt;Jonathan Bell has written for Wallpaper* magazine since 1999, covering everything from architecture and transport design to books, tech and graphic design. He is now the magazine’s Transport and Technology Editor. Jonathan has written and edited 15 books, including Concept Car Design, 21st Century House, and The New Modern House. He is also the host of Wallpaper’s first podcast.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Terrified to get inked? This inviting Brooklyn tattoo parlour is for people who are 'a little bit nervous'&lt;p&gt;With minty-green walls and an option to 'call mom', Tiny Zaps' Williamsburg location was designed to tame jitters&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Let’s hear it for the Chopard L.U.C Grand Strike chiming watch&lt;p&gt;The Swiss watchmaker’s most complicated timepiece to date features an innovative approach to producing a crystal-clear sound&lt;/p&gt;&lt;/item&gt;
      &lt;item&gt; Form... and flavour? The best design-led restaurant debuts of 2025&lt;p&gt;A Wallpaper* edit of the restaurant interiors that shaped how we ate, gathered and lingered this year&lt;/p&gt;&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46367864</guid><pubDate>Tue, 23 Dec 2025 18:33:01 +0000</pubDate></item><item><title>Help My c64 caught on fire</title><link>https://c0de517e.com/026_c64fire.htm</link><description>&lt;doc fingerprint="f2f8d1d66eb7a2e8"&gt;
  &lt;main&gt;
    &lt;div&gt;&lt;p&gt; I flew back to Italy for the Christmas holidays, as I usually do. Here I have my childhood c64, on which I learend how to program, and which in the last few years I took to refurbishing.&lt;/p&gt;&lt;p&gt; In general, everytime I'm back to my parent's place I spend some time fixing and sorting out things, and this is one of them. &lt;/p&gt;&lt;p&gt; Now it works like a charm, and of course I also added some &lt;/p&gt;bells and whistles&lt;p&gt;, mostly stuff that allows me to easily tranfer programs from a PC (a kung-fu cart and a pi1541) - so this year I thought it was time to actually do something with it! &lt;/p&gt;&lt;p&gt; I decided to turn it into a cozy fireplace: &lt;/p&gt;(click on the image for the full-color version)&lt;p&gt; I was quite surprised that it worked, that I managed to complete this project in a few hours over two days, and that it was, most of all, fun! &lt;/p&gt;&lt;p&gt; I expected... more friction, having to work with arcane cross-platform toolchains and the like, but instead I completed almost of all it in a web-based IDE/emulator combo! &lt;/p&gt;&lt;p&gt; Moreover, I am far from being an expert when it comes to c64 coding. Yes, I used to program with one... when I was six or seven! And yes, I do follow its demoscene, and over the years I've read quite a bit about its chips and inner workings, but I've never written any demo effect for it. &lt;/p&gt;&lt;p&gt; In other words... if I managed, you can too! That's what made me want to write this post... &lt;/p&gt;All you need for Christmas...&lt;p&gt; ...is a 6502. &lt;/p&gt;&lt;p&gt; Here, I'll give you a crash course on the c64 - the key points of what I knew before starting this. &lt;/p&gt;&lt;p&gt; If you know how modern CPUs work, and how to optimize for them - try to forget all of that. The 64 comes from an era where RAM was faster than compute. Lookup tables are your friend, as it is fully unrolling loops / code generation. We have no caches! &lt;/p&gt;&lt;p&gt; Don't be surprised then to learn that the famous 6502 CPU has only one arithmetic register, the "accumulator". There are also two "index" registers used to offset memory locations, a status register, and a program counter - but you can't do math on any of these, at best, test or increment/decrement.&lt;/p&gt;&lt;p&gt; Moreover, the program counter is the only 16-bit register, all the others are 8-bit. &lt;/p&gt;&lt;p&gt; But who needs registers when you have fast RAM? Memory is your register file: pretty much all 6502 instructions operate between the accumulator and memory locations (or numeric constants, that are still memory, just part of the instruction itself). &lt;/p&gt;&lt;p&gt; To further facilitate this, the 6502 comes with a rich set of addressing modes, most instructions can fetch memory in few different ways: at absolute addresses, at addresses offset with one of the two index registers or even at indirect addresses (addresses contained in memory locations).&lt;/p&gt;&lt;p&gt; There is also a special memory area, called the "zero page", the first 256 bytes of memory (a page is, unsurprisingly, 256 bytes), which has an extra optimization: addressing there takes one cycle less, because the address can be encoded in the instruction in a single byte instead of two. &lt;/p&gt;&lt;p&gt; Have a look at the &lt;/p&gt;6502 instruction set&lt;p&gt;, it's very simple! Won't take more than 15 minutes to skim over them all. &lt;/p&gt;The plan. &lt;p&gt; Where things get less simple is to deal with all the c64 custom chips, the SID (sound) and the VIC-II (graphics). That's how demo-scene effects are done! Manipulating these chips in very precise ways to cause them to generate crazy stuff, most of which was never considered possible by the c64 designers back then! &lt;/p&gt;&lt;p&gt; The average c64 demo effect is all about this - generating lookup tables and unrolled assembly to then be able to exactly time internal chip status changes as the video signal is being generated line by line ("racing the beam"). Usually, what is shown on screen is not at all what it seems - i.e. it's not how you would create a similar effect on a PC. &lt;/p&gt;We won't be porting Second Reality...&lt;p&gt; I know almost nothing of any of this - so my plan was to avoid it all! I wanted to set the VIC-II in some graphic mode that gave me a decently simple, linear framebuffer, and from there on write the code like I would have done on any other computer, hoping that a fire effect is simple enough to compute that the 64 would just be able to cope with it. &lt;/p&gt;&lt;p&gt; Luckily, there is one such mode. The default, vanilla, character mode! Here, we have 40x25 characters on screen, chosen from a set of 256. So, one byte per "pixel", and 1000 pixels in total - great! &lt;/p&gt;The default "petscii" character set.&lt;p&gt; Now, the default character set does not really lean itself to creating a demo effect, but I knew I could create a custom one. My idea was to simply making a dither pattern, and as much as possible work like I had a 8-bit "grayscale" screen. &lt;/p&gt;&lt;p&gt; C64 characters are 8x8, so I could create a dither pattern that has a number of "on" pixels in the character corresponding to its position in the charset: 0 being fully off (background color), 64 being fully on (foreground color), and everything in between being a mix.&lt;/p&gt;&lt;p&gt; Of course though this would give us only 64 values, ideally, I wanted to utilize the whole 8-bit space... On obvious idea is that we could add colors to the mix. For example, having the first 64 values go from black (background) to brown (foreground), then the next 64 have brown as background and red as foreground, then red and yellow, and finally yellow and white. &lt;/p&gt;My custom charset. Made by hand in C64Studio.&lt;p&gt; This can't be achieved in the default character mode, unfortunately, as only the foreground color is controllable per-character (via another memory location, still using one byte per character - easy), while the background is shared. Luckily though, there is an &lt;/p&gt;"extended color mode"&lt;p&gt; that fits the bill exactly. In this mode the character set is limited to 64, but the two high bits of each character can be used to control the background color, while the foreground remains in the separate memory location as usual. &lt;/p&gt;Implementation. &lt;p&gt; All development was done in the &lt;/p&gt;retrogamecoders c64 IDE&lt;p&gt;, which handly couples the &lt;/p&gt;cc65 compiler&lt;p&gt; with an emulator. &lt;/p&gt;&lt;p&gt; Writing c64 code in C is generally a terrible idea, and cc65 is not even the "best" compiler out there (is quite old and barely does optimize the generated code - &lt;/p&gt;llvm-mos&lt;p&gt; might be better), but being able to test in C and then gradually "port" to assembly was crucial for a noob like me. If I were to keep working on this, I'd probably move to &lt;/p&gt;Kick Assembler&lt;p&gt;, which is particularly suited for the kind of code-generation that you want to do in demo-coding. &lt;/p&gt;&lt;p&gt; Anyhow, here are the .c files, step by step, in chronological order. You can just copy and paste them in the retrogamecoders IDE and see how things work, start tinkering if you like! Enjoy! &lt;/p&gt; First test of the ECM charset idea. Slowest fire-effect ever in pure C. Still in C. Starting to "unroll". Starting to port to assembly. Ported to assembly, with side-by-side C. Assembly only, some more ECM tricks. "Final" version. &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46368300</guid><pubDate>Tue, 23 Dec 2025 19:09:45 +0000</pubDate></item><item><title>Terrence Malick's Disciples</title><link>https://yalereview.org/article/bilge-ebiri-terrence-malick</link><description>&lt;doc fingerprint="37c09103add60b3d"&gt;
  &lt;main&gt;
    &lt;p&gt;In the winter of 2024, the photographer and filmmaker RaMell Ross released Nickel Boys, a masterful adaptation of a novel by Colson Whitehead. In a fragmentary, impressionistic style, the film portrays the friendship of two African American teens at a brutal Florida reform academy during the Jim Crow era. Acclaimed as a visionary movie, it ended up on many critics’ best-of-the-year lists and earned an Oscar nomination for Best Picture.&lt;/p&gt;
    &lt;p&gt;Ross is a fiercely independent artist. His first film, the lyrical 2018 documentary Hale County This Morning, This Evening, was also nominated for an Oscar. Afterward, he refused Hollywood’s overtures for years. So why did he take a meeting with the producers who reached out to him about making a studio-financed, big-budget adaptation of Nickel Boys? Ross’s explanation was simple: because one of them had produced Terrence Malick’s 2011 film, The Tree of Life.&lt;/p&gt;
    &lt;p&gt;Ross’s reverence for Malick is plain in his films, which, like Malick’s, rely on extended montages of the everyday and do away with the conventional rules of cinematic storytelling, hovering instead between distant, melancholy reverie and hyperfocused, lived-in specificity. And he is not the only recent filmmaker who has fallen under Malick’s spell. Indeed, Malick’s sensibility, visual style, and working methods have had a profound influence on some of today’s best and most interesting directors.&lt;/p&gt;
    &lt;p&gt;Take Chloé Zhao, the director of the Oscar-winning Nomadland (2020). Her early films, all set in the American heartland, were regularly compared to Malick’s, and she herself pointed to The Tree of Life and Malick’s 2005 film, The New World, as influences on her 2021 Marvel superhero movie, Eternals. Those overtones persist in her latest, Hamnet, a film about the death of William Shakespeare’s only son and his subsequent creation of Hamlet. The movie may take place in Elizabethan England, but it is replete with lyrical passages and visions of nature that recall Malick’s work.&lt;/p&gt;
    &lt;p&gt;The same is true of the director Clint Bentley’s newest film, Train Dreams, an adaptation of Denis Johnson’s 2011 novella about the unremarkable life of a logger and railroad worker in the early years of the twentieth century. Weaving episodes from its character’s life into an elegiac collage that incorporates domestic bliss, harrowing tragedy, and melancholic resignation, Train Dreams—which premiered at the Sundance Film Festival in January and was quickly acquired by Netflix—unfolds across 102 minutes, yet seems to contain a whole world. Its protagonist, played by a reserved Joel Edgerton, is a simple man who occasionally questions his place in the universe but never understands it, save for a brief moment near the end when he takes a ride in an airplane—something he’s never done before—and, in one shining (and recognizably Malickian) instant, sees the shape of his life and feels something like transcendence.&lt;/p&gt;
    &lt;p&gt;Malick’s influence is intriguing in part because he is not an obvious choice for filmmakers to emulate. He has had, to be sure, a fascinating career: a publicity-shy Harvard philosophy grad, Rhodes Scholar, former MIT lecturer, and New Yorker writer, he made two brilliant and highly acclaimed films in the 1970s—the lovers-on-the-run drama Badlands and the visually striking romantic tragedy Days of Heaven—before stepping away from filmmaking for twenty years. In 1998, he returned with The Thin Red Line, a dreamy, diffuse adaptation of James Jones’s World War II novel, and followed that with two more ruminative epics: The New World, about the settlement of Jamestown and the romance between John Smith and Pocahontas, and The Tree of Life, a massive autobiographical film that frames a mid-century Texas coming-of-age tale against the spectacular origins of the universe and of life on Earth. His films since then have been less ambitious in scope but, in some ways, more stylistically bold.&lt;/p&gt;
    &lt;p&gt;Many of Malick’s films have been critically acclaimed, and two have received Oscar nominations for Best Picture (albeit without much chance of winning). But none could be called box-office hits, and some have been savaged by critics. Indeed, thanks to his fondness for oblique storytelling, poetic voice-over, and overt spiritual themes, Malick’s oeuvre has become one of the more contentious in cinema. Each new release inspires debate over whether the film at hand is a deep, philosophical masterpiece or boring, pretentious drivel. Young directors looking for heroes tend not to gravitate toward divisive religious artists whose movies don’t make money or win awards. So what accounts for Malick’s impact on twenty-first-century American film?&lt;/p&gt;
    &lt;p&gt;particularly since his return to filmmaking, Malick has sought to reconnect American cinema to a lost spirituality, earnestly tackling questions about faith and the design of the world at a time when most mainstream cinema has avoided such topics. Malick is a devout Episcopalian. But the spirituality in his films is rarely illustrative or prescriptive. He doesn’t use religion as a cudgel or a doctrinaire superstructure with which to explain the world. Rather, he sees it as an inner light in people. In The Thin Red Line, for instance, soldiers, in voice-over, speak solemnly of inner longing. These otherwise inarticulate men’s voices read heartfelt love letters, or dabble in poetry, or edge their way into philosophical inquiries about the cruelty and redemptiveness of nature. A soldier remembers his mother reaching for an angel at the instant of her death; another recalls the serenity he experienced with his wife before he had to leave her behind. The effect is like eavesdropping on a kind of Emersonian oversoul. Malick endows even his most minor characters with humanity, which he views as a kind of holiness. Amid the gaunt and haunted faces of these soldiers, Malick finds grace.&lt;/p&gt;
    &lt;p&gt;This kind of earnestness stood out in an age of relentless irony and snark. It served as a corrective to the glossy productions of Hollywood in its imperial phase, that period of the late 1990s and early 2000s, when budgets ballooned and American cinema, armed with state-of-the-art CGI and desperate to service a growing international market, became increasingly driven by fantasy spectacle and special effects. Malick’s films were a rebuke to even the hip grittiness of independent films of the era. He had an eye for light and an ear for music, he immersed viewers in color and texture, and he used his classical scores to underscore the glory of what he saw. Handcrafted, personal, achingly sincere, and at times proudly “flawed,” his pictures stood out against both the mainstream and the underground.&lt;/p&gt;
    &lt;p&gt;By the end, we are overwhelmed with emotion for this unremarkable life lived in near anonymity.&lt;/p&gt;
    &lt;p&gt;This proved irresistible for a certain kind of filmmaker frustrated with the options available to them. In 2000, for instance, the director David Gordon Green released George Washington, a drifting, multicharacter drama featuring young African American kids in a dead-end North Carolina steel town. Despite his impoverished setting, Green avoids miserabilist clichés and gives his characters a romantic grandeur. He takes their hopes and desires at face value. The title comes from the fact that one of the kids, named George, dreams of being president of the United States, a fact that Green does not treat with bitter irony or fashionable cynicism.&lt;/p&gt;
    &lt;p&gt;Malick’s effect on George Washington is undeniable—rare was the review that didn’t mention the connection—and it is also clear in Green’s second feature, All the Real Girls (2003), an atmospheric and largely uneventful romance defined by the passions of the two shy lovers at its center. Noel (Zooey Deschanel) and Paul (Paul Schneider), like Malick’s characters in Badlands and Days of Heaven, are not extroverted or articulate. But Green’s film thrums with a visual splendor that reflects the characters’ longing, turning another depressed Southern town into a vibrant emotional landscape.&lt;/p&gt;
    &lt;p&gt;Zhao’s films also highlight the great beauty of the otherwise unremarkable. Her masterpiece, 2017’s The Rider, follows a wounded rodeo cowboy (played by Brady Jandreau, a real-life rodeo star who sustained a career-ending head injury) from a Lakota Sioux reservation in South Dakota as he struggles with his inability to ride again. The film is made up of small moments, highlighting brief interactions and quotidian actions, but Zhao’s shooting and cutting, much like Malick’s, elevate these scenes toward the transcendent, finding a sacredness in the existence of a character who has lost his sense of purpose.&lt;/p&gt;
    &lt;p&gt;The same could be said of Bentley’s Train Dreams, which follows a man with very little direction in the world: he’s an orphan, raised in poverty, who finds work as a logger and spends his years felling trees and building railroads. Though he sees racism and murder around him, he can do nothing about it. He finds happiness by starting a family but then loses that family to a raging wildfire. The film’s rhythms are not those of a typical drama; for all the squalls of guilt and grief, the movie moves with a steady cadence that suggests that the mysteries, tragedies, and glories of life are all part of the same thing. This seems like it would result in a cold, opaque film, yet by the end, we are overwhelmed with emotion for this unremarkable life lived in near anonymity, a life that is more like our own than we might want to admit.&lt;/p&gt;
    &lt;p&gt;You can also see Malick’s philosophical influence in three films directed by Laura Dunn (all of which he produced): The Unforeseen (2007), about the dire social and environmental consequences of a mining company’s development of a vast patch of Austin real estate,Look &amp;amp; See: A Portrait of Wendell Berry (2016), about the life of the titular Kentucky farmer, writer, and activist, and All Illusions Must Be Broken (2024), about the American cultural anthropologist Ernest Becker’s ideas around the human denial of mortality and self-knowledge. In each, Dunn portrays a society that is fraying at the seams owing to its increasing disconnection from the natural world and the organic patterns of life. Her films avoid the density of political and philosophical jargon. Instead, they create meaning through images of ordinary people: children playing, adults working in the fields, reconnecting viewers with a different state of being. The films’ form embodies her overall thesis that, despite our endless efforts to deny it, we humans are not separate from nature but inextricably part of it.&lt;/p&gt;
    &lt;p&gt;malick’s humanism is refracted through his visual style—the aspect of his films that’s most obviously influential. He loves to shoot with natural light whenever possible: “Vermeer yourself ” is a common direction he gives to actors, indicating that they should lean into the available light during a take. His fondness for shooting at the “magic hour,” that time when the sun is setting and the sky emits a distinctive dark glow, is legendary. He also talks about “quail hunting”: capturing unscripted moments when the light happens to be perfect and you find something unexpected and real. Then there are “rabbit holes”: quick scenes and exchanges shot when the light isn’t perfect. Natural metaphors, found moments, a dogged pursuit of real light—the way Malick approaches the act of shooting enacts his philosophical view of the world.&lt;/p&gt;
    &lt;p&gt;The lilting, fairy-tale surfaces of that film speak to a search for beauty that the characters cannot find.&lt;/p&gt;
    &lt;p&gt;Malick’s influence on the way movies look has become a cliché. (A short 2015 video titled “Not Directed by Terrence Malick,” compiled by Jacob T. Swinney, features a collection of clips of films apparently influenced by Malick; it includes movies like Up in the Air, Beasts of No Nation, and Ex Machina.) But anybody with some skill can shoot with natural light or cut away to a field of wheat. What distinguishes Malick’s work—what makes it truly revelatory to viewers—emerges from the harmony between a film’s images and its sensibility. In George Washington, Green frames his characters in gorgeous light and scores their interactions with symphonic drones that suggest something heroic. And in Nickel Boys, Ross tells a tale filled with injustice, racism, torture, and murder—a story that should be the very height of despair—yet finds an almost overwhelming humanity with his probing camera. Like Malick in The Thin Red Line, Ross sees evidence of grace in the basest of places.&lt;/p&gt;
    &lt;p&gt;By contrast, it’s jarring—if fascinating—when a film’s visual approach borrows from Malick but doesn’t match the sensibility at work. That’s the case with The Assassination of Jesse James by the Coward Robert Ford (2007), a remarkably beautiful Western directed by Andrew Dominik, who worked as an uncredited cameraman on The New World. The film has a twilight grandeur and a fascination with the natural world that suggests Dominik learned quite a bit working for Malick. But despite the unmistakable surface similarities, Dominik’s dark moral vision bears little resemblance to Malick’s. The outlaws of the James gang live in a universe of endless, savage scrutiny, fearful of both the law and their own viral, panopticist distrust, with each member set against the others. The lilting, fairy-tale surfaces of that film speak to a search for beauty that the characters cannot find; Dominik longs for Malick’s vision of grace but sees no evidence of it. Or maybe he just doesn’t really want to find it.&lt;/p&gt;
    &lt;p&gt;malick’s working style is also appealing to many filmmakers. He shoots incessantly, improvises constantly, pays more attention to capturing footage of flora and fauna than he does to scripted scenes with actors, and then spends months in postproduction with teams of editors assembling his movies in unorthodox ways. This approach is inviting not just because it is unusually creative and collaborative but because it is rooted in the nature of cinema itself.&lt;/p&gt;
    &lt;p&gt;Malick does not rely on the nineteenth-century theatrical conventions that most moviemakers remain bound to, with their focus on acts and protagonists and inciting incidents and A and B storylines. His films also avoid the novelistic, flowing instead like a series of thoughts, or memories, or maybe rivers. His is an intuitive and almost abstract filmmaking process that deprioritizes the presentational and the narrative. Malick focuses on collecting images, ideas, offhand moments, and sounds that can then be used during editing, applied almost like brushstrokes in a painting.&lt;/p&gt;
    &lt;p&gt;He also welcomes spontaneous suggestions on set and encourages experimentation. There are three editors credited on The Thin Red Line, four on The New World, and five on The Tree of Life; for the latter, the director reportedly invited students from the University of Southern California and the University of Texas at Austin to come in and try their hand at cutting footage. “Kids don’t censor themselves—their brains are in a different place,” the editor Billy Weber, one of Malick’s longtime collaborators, told me at the time. “So we had students give it a shot. And we’d have interns come in at night and cut scenes.” This is, to be clear, nothing like the way most other movies are put together. All too often, a film production is like a train that can’t be stopped or set on a different course once it leaves the station. But Malick has found ways to guide the train gently off the tracks—and in new, unexpected, undiscovered directions. It’s easy to see why other directors might be drawn to his less regimented approach.&lt;/p&gt;
    &lt;p&gt;This working process re-creates the filmmaking method that Malick chanced upon with Days of Heaven, which, for all the acclaim it garnered, was something of a salvage job. Coming off the critical success of Badlands, Malick had gone into Days of Heaven with a dense, detailed, ambitious script. But as described in John Bleasdale’s excellent 2024 biography, The Magic Hours: The Films and Hidden Life of Terrence Malick, the director found himself unhappy with the results he was getting over the course of production. He didn’t like how his dialogue sounded. His scenes felt phony. The shoot ran wildly over budget and behind schedule, as the Canada-based production team spent days trying to get the light perfect, and Malick’s original cinematographer, Néstor Almendros (who would go on to win an Oscar for the film), left halfway through and was replaced by Haskell Wexler. Meanwhile, Malick sent a photographer friend to capture nature footage that he could intersperse throughout the movie.&lt;/p&gt;
    &lt;p&gt;Along the way, the director found himself fascinated with the off-the-cuff observations made by another one of his leads, fifteen-year-old Linda Manz, and recorded her describing scenes from the movie in her own words; he eventually shaped that into one of the most indelible voice-over narrations in cinema history, an offbeat series of childlike reflections that provide a poetic counterpoint to the elemental storyline. Everything about Malick’s evolving approach speaks to a heightened sense of possibility, and to a desire to reinvigorate the frustrating rhythm of film production with openness, spontaneity, and discovery.&lt;/p&gt;
    &lt;p&gt;What’s remarkable about this approach is that despite his seemingly scattershot and impulsive methods, Malick’s films possess an aesthetic unity. Ross suggests something similar when talking about his own work. In an interview around the release of Nickel Boys, he described to me the collage-like quality of his film: “It’s jumping time, and jumping textures, and jumping images, and points of view, and focal lengths, and sounds, but also it’s coherent.”&lt;/p&gt;
    &lt;p&gt;influence can be a straitjacket. In 2014, A. J. Edwards, who had worked as an editor on two of Malick’s films, released The Better Angels, a black-and-white meditation on Abraham Lincoln’s years as a young man living in rural Indiana. It’s a bold movie in many ways, almost confrontationally nonnarrative and context-free; aside from a brief coda set immediately after his assassination, we see almost nothing of Lincoln as a grown man or president. And yet there’s a curious emptiness at its heart. Filled with handheld reveries bathed in heavenly light, it replicates the yearning style of Malick’s work without the instances of genuine humanity that undergird his cinematic tapestries; though the characters in The Better Angels are based on real historical personages, they never come across as real people.&lt;/p&gt;
    &lt;p&gt;A similar emptiness afflicts David Lowery’s crime melodrama Ain’t Them Bodies Saints (2013), which has ravishing cinematography, ethereal music, and an elliptical narrative, all of which clearly owe something to Malick’s work. It follows the return of a fugitive to the woman he once loved and the child he has never met, and the tortured romance that ensues. Lowery explained at the time that he was not interested in another story about a crime but instead wanted to explore its emotional aftermath. But for all its loveliness, the film’s glancing storytelling has the opposite effect of Malick’s openness to the world; it dulls the senses, makes the characters and their feelings seem smaller and less significant. (Lowery’s more recent films owe little to Malick and are the better for it.)&lt;/p&gt;
    &lt;p&gt;Think of it this way: What use is Malick’s liberated style of working if a filmmaker merely replicates it? The most successful Malickian films borrow from his work but find ways to transcend it and to convey new ideas. Take RaMell Ross. In his first film, he used fleeting, beautiful glimpses into mundane moments to convey, in just seventy-six short minutes, the arc of his subjects’ lives. In Nickel Boys, he expanded the fragmented lyricism of his earlier film by crossing it with a first-person camera: the story is told almost entirely through shots that appropriate the perspectives of the two characters. The result is a work that is immersive and experiential, otherworldly and mythic. It’s also entirely his.&lt;/p&gt;
    &lt;p&gt;malick has always been frustrated with the typical methods of making movies. In fact, he seems to become restless even with his own methods. If he’s helped liberate other filmmakers, he has also continuously sought to liberate himself. That may be why what has remained constant throughout his work has been change. Even if certain aspects of his films—his love of natural light, his attention to found moments, his use of voice-over—have recurred, his subject matter, and his style, have never been fixed. The films that made his reputation in the 1970s—Badlands and Days of Heaven—are very different from the epics he made after his return to filmmaking, which are even less conventional in terms of narrative. The earlier pictures, compact and diamond-sharp, dance around their ideas, and their young protagonists don’t always grasp the gravity of their stories. Holly, the narrator of Badlands, is just as likely to talk about a movie star or a photograph as she is to talk about the fact that her boyfriend is a serial killer; Linda, the narrator of Days of Heaven, generally talks about everything and anything aside from the fact that her brother and his lover are cruelly betraying a dying man. This is very different from the prayerlike directness we find in the voice-overs for The Thin Red Line, The New World, and The Tree of Life. Many-voiced and at times even rambling, those later period films are pointedly diffuse: each scene, each thought feels like it could expand into a whole other movie; all the characters seem so resolutely alive.&lt;/p&gt;
    &lt;p&gt;Malick then pivoted again, following his trio of epics with a trilogy of low-budget works—To the Wonder (2012), Knight of Cups (2015), and Song to Song (2017)—which seemed at times to be not-so-veiled dramatizations of events from the director’s own life. These works were Malick’s first films to be set in something like the present. They are messier, more frenzied. The characters in these later pictures are rootless, always searching. And the filmmaking in them is centered more on movement than meditations on nature.&lt;/p&gt;
    &lt;p&gt;In his vision, our endless seeking makes us human and therefore holy.&lt;/p&gt;
    &lt;p&gt;To the Wonder, for instance, tells the story of the breakdown of the marriage between an American man and a Ukrainian woman after they return from Paris (where they met) to his home in Oklahoma. The film eschews dialogue, relying instead on characters’ movements to express their emotions and changing relationships. A mother and daughter, newly arrived in the United States, twirl and leap through the aisles of an enormous supermarket, the likes of which they’ve never seen before; the stolid shoulders of a frustrated husband dominate the foreground of the frame, while his effervescent wife moves daintily before him; a flirtation is expressed with a quick curtsy, shame with a penitent bow. It can almost be seen as a dance film.&lt;/p&gt;
    &lt;p&gt;Instructing the film’s team of editors, Malick gave them copies of Margaret Anne Doody’s introduction to a Penguin Classics edition of Samuel Richardson’s 1740 epistolary novel, Pamela, and pointed them to a line about how the author loved “the formless, the radiant zigzag becoming.” The phrase “radiant zigzag becoming” became their own unofficial title for the film, the editors told me; it spoke to the project’s energetic sense of movement. It also reflected the fact that Malick’s characters were always in the process of self-actualizing without ever fully doing so.&lt;/p&gt;
    &lt;p&gt;Something similar could be said for Malick’s films themselves. To the Wonder, in fact, led directly to one of the most intriguing of Malick-influenced movies, a hybrid on multiple levels. In 2018, the veteran photographer and documentarian Eugene Richards premiered a mesmerizing forty-three-minute film called Thy Kingdom Come, which consists of footage Richards shot for To the Wonder, featuring Javier Bardem as a priest who has lost his faith ministering to the impoverished residents of an Oklahoma town.&lt;/p&gt;
    &lt;p&gt;Malick had Bardem go into real people’s lives—into trailer-park homes, a county jail, a homeless shelter—and had Richards document those people speaking to the actor’s clearly fictional priest. Only a small portion of the footage would be used in the finished feature, so Richards and Bardem developed a plan to make a separate film out of the material. In Thy Kingdom Come, Bardem says little; most of the picture consists of these people—drug addicts, inmates, a homeless couple, a former Ku Klux Klan member, a woman grieving a dead baby, and more—describing their experiences and their thoughts. There is no narrative, nor even much of an emotional through line. Aside from a couple of brief exchanges, there is barely any mention of God. And yet spirituality is ever present. These people know the priest isn’t real, but they open up to him as if he were; they do not, in any way, seem to be acting.&lt;/p&gt;
    &lt;p&gt;“Is this a true story?” Bardem asks in the opening narration. “Yes, I would say so. Is the priest a real priest? No. But it’s as if they were waiting for him.” The onrush of faces and lives that then ensues suggests the anticipation goes both ways: it’s as if the film were waiting for them. These people might not have found grace, but the camera eye—Richards’s but also Malick’s—finds grace in them. Thus, this riff on Malick reveals something essential about Malick’s work. In his vision, our endless seeking makes us human and therefore holy. The search for God is not a search for meaning; the meaning lies in the search itself. Through the films he’s made and the ways he’s made them, Malick has turned cinema into the vessel for that search.&lt;/p&gt;
    &lt;p&gt;Bilge Ebiri is a film critic for Vulture and New York magazine. His work has also appeared in The New York Times and the Criterion Collection.&lt;/p&gt;
    &lt;p&gt;A Literary Gift in Print&lt;/p&gt;
    &lt;p&gt;Give a year of The Yale Review—four beautifully printed issues featuring new literature and ideas.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46368557</guid><pubDate>Tue, 23 Dec 2025 19:35:20 +0000</pubDate></item><item><title>HTTP Caching, a Refresher</title><link>https://danburzo.ro/http-caching-refresher/</link><description>&lt;doc fingerprint="b55d69584f161f32"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;HTTP caching, a refresher&lt;/head&gt;
    &lt;p&gt;This is a reading of RFC 9111 (2022), the latest iteration of the HTTP Caching standard.&lt;/p&gt;
    &lt;p&gt;It defines the &lt;code&gt;Cache-Control&lt;/code&gt; HTTP header as a way to prescribe how caches should store and reuse HTTP responses, with regards to not just the browser cache, but to any other intermediary caches, such as proxies and content delivery networks, that may exist between the client and the origin server.&lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;Cache-Control&lt;/code&gt; header accepts a set of comma-separated directives, some of which are meant to be added to HTTP requests, and others to HTTP responses. A typical response header:&lt;/p&gt;
    &lt;code&gt;HTTP/2 200
Cache-Control: max-age=0, must-revalidate
&lt;/code&gt;
    &lt;p&gt;Some of these directives specifically target shared caches, that is intermediary caches that serve the same cached responses to many users, while others also apply to private caches such as the browser cache.&lt;/p&gt;
    &lt;head rend="h2"&gt;Whatâs fresh?&lt;/head&gt;
    &lt;p&gt;Whenever the cache receives a request, it must figure out if the cached response is still fresh and can therefore be reused without incurring the performance tax of an HTTP request, or whether it has gone stale and should be validated with the server.&lt;/p&gt;
    &lt;p&gt;To decide on freshness, the cache compares the age of the response to the responseâs so-called freshness timeline.&lt;/p&gt;
    &lt;p&gt;The age of a cached response is the time elapsed since it was last generated or revalidated by the origin server. To the time spent in its own cache, the browser will add any &lt;code&gt;Age: &amp;lt;seconds&amp;gt;&lt;/code&gt; header received from intermediary caches.&lt;/p&gt;
    &lt;p&gt;The freshness timeline is a duration beyond which the cached response is to be considered stale. Itâs usually signaled by the server via the appropriate response headers, but may also be guesstimated by the cache in the absence of explicit, valid cues. In order of precedence:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;the server establishes a freshness timeline, in seconds, with the &lt;code&gt;Cache-Control: max-age=&amp;lt;number&amp;gt;&lt;/code&gt;directive on the response; otherwise,&lt;/item&gt;
      &lt;item&gt;the cache falls back to computing the interval between the &lt;code&gt;Expires: &amp;lt;date&amp;gt;&lt;/code&gt;and&lt;code&gt;Date: &amp;lt;date&amp;gt;&lt;/code&gt;response headers, if available; otherwise,&lt;/item&gt;
      &lt;item&gt;if thereâs no &lt;code&gt;Expires&lt;/code&gt;header, the response lacks an explicit expiration, and a heuristic freshness based on the&lt;code&gt;Last-Modified&lt;/code&gt;response header might be applicable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For shared caches, the special &lt;code&gt;s-maxage=&amp;lt;number&amp;gt;&lt;/code&gt; directive takes precedence over all others.&lt;/p&gt;
    &lt;head rend="h2"&gt;Going past expiration&lt;/head&gt;
    &lt;p&gt;Just because a response has gone stale, it doesnât mean it needs to be thrown out.&lt;/p&gt;
    &lt;p&gt;When it receives a request for a stale cached response, the cache should validate it with its upstream server. Although validation always generates an HTTP request, it avoids a data transfer when thereâs no newer version of the cached response on the server, so it can still be faster than a regular request.&lt;/p&gt;
    &lt;p&gt;Validation uses a mechanism known as a conditional HTTP request, which includes one or more special headers called preconditions:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;if the precondition with the highest precedence is met, the server responds with HTTP 200 OK and an updated response body; otherwise,&lt;/item&gt;
      &lt;item&gt;it responds with HTTP 304 Not Modified and an empty body, confirming the existing response can be reused.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To generate the preconditions needed for these conditional requests, which the server uses to compare the cached response to the freshest version available, responses must be tagged in a way thatâs unique to each version:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;historically, this was done with the &lt;code&gt;Last-Modified: &amp;lt;date&amp;gt;&lt;/code&gt;header, corresponding to the latest update to the content;&lt;/item&gt;
      &lt;item&gt;a more flexible and robust alternative is the &lt;code&gt;ETag: "&amp;lt;value&amp;gt;"&lt;/code&gt;header, which stores an arbitrary ASCII string that uniquely identifies the response. This string is usually a hash incorporating one or more aspects: the modification time, the file size, and the file content.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When performing the validation, the cached response headers are mirrored as preconditions for the conditional request:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;&lt;code&gt;Last-Modified: &amp;lt;date&amp;gt;&lt;/code&gt;becomes&lt;code&gt;If-Modified-Since: &amp;lt;date&amp;gt;&lt;/code&gt;;&lt;/item&gt;
      &lt;item&gt;&lt;code&gt;ETag: "&amp;lt;value&amp;gt;"&lt;/code&gt;becomes&lt;code&gt;If-None-Match: "&amp;lt;value&amp;gt;"&lt;/code&gt;.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;When both preconditions are present, only &lt;code&gt;If-None-Match&lt;/code&gt; is evaluated.&lt;/p&gt;
    &lt;p&gt;Regardless of the result of the validation request, the cached response headers are updated with the new values received from the server, and the fresh-o-meter on the cached response is reset.&lt;/p&gt;
    &lt;p&gt;Certain caches may be set up to serve stale responses in some circumstances, such as when losing the connection to the server or in the event of an HTTP 5xx server error. There are also &lt;code&gt;Cache-Control&lt;/code&gt; directives that influence how stale responses are handled, covered in the next section.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cache-Control response directives&lt;/head&gt;
    &lt;p&gt;
      &lt;code&gt;max-age=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;max-age&lt;/code&gt; response directive defines the responseâs freshness timeline in seconds, after which the response should be considered stale. â RFC 9111 Â§ 5.2.2.1&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;must-revalidate&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;must-revalidate&lt;/code&gt; response directive indicates that the cache must not reuse a stale response until itâs been successfully validated by the origin server. â RFC 9111 Â§ 5.2.2.2&lt;/p&gt;
    &lt;p&gt;If the server throws an error, the cache must surface that instead of reusing a stale response. If the cache is disconnected, it must produce an error with the HTTP 504 Gateway Timeout status code, or another more applicable error code.&lt;/p&gt;
    &lt;p&gt;Side effects: &lt;code&gt;must-revalidate&lt;/code&gt; is one of the directives, along with &lt;code&gt;s-maxage&lt;/code&gt; and &lt;code&gt;public&lt;/code&gt;, that allow shared caches to store and reuse a response to a request containing an &lt;code&gt;Authorization&lt;/code&gt; header, which they are generally prohibited from doing.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;no-cache&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;no-cache&lt;/code&gt; response directive indicates that the cache must not reuse any response until itâs successfully validated by the origin server. â RFC 9111 Â§ 5.2.2.4&lt;/p&gt;
    &lt;p&gt;This is similar to &lt;code&gt;must-revalidate&lt;/code&gt; but refers to all cached responses, not just stale ones. In effect, &lt;code&gt;no-cache&lt;/code&gt; is a sort of &lt;code&gt;max-age=0, must-revalidate&lt;/code&gt;.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;no-store&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;no-store&lt;/code&gt; response directive indicates that private and shared caches must not store any part of the request or the response, and to never reuse the response. The standard is quick to warn that the effect is not guaranteed:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;âMUST NOT storeâ in this context means that the cache MUST NOT intentionally store the information in non-volatile storage and MUST make a best-effort attempt to remove the information from volatile storage as promptly as possible after forwarding it. This directive is not a reliable or sufficient mechanism for ensuring privacy. In particular, malicious or compromised caches might not recognize or obey this directive, and communications networks might be vulnerable to eavesdropping. â RFC 9111 Â§ 5.2.2.4&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;Side effects: The directive can also influence non-HTTP caches. Most browsers will exclude from the back/forward cache pages having the &lt;code&gt;no-store&lt;/code&gt; response directive. Chrome, however, has recently started to make some of these pages eligible for bfcache when the browser deems it safe.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;must-understand&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;must-understand&lt;/code&gt; response directive indicates that the cache shouldnât store or reuse responses with HTTP status codes whose semantics the cache doesnât understand and conform to. The directive is meant to future-proof existing implementations from status codes that might have special requirements in regards to caching. â RFC 9111 Â§ 5.2.2.3&lt;/p&gt;
    &lt;p&gt;Itâs recommended to use &lt;code&gt;must-understand, no-store&lt;/code&gt; together as a fallback, and caches are encouraged to ignore the &lt;code&gt;no-store&lt;/code&gt; directive if they do understand the semantics of the HTTP status code. This ensures older caches that donât recognize the &lt;code&gt;must-understand&lt;/code&gt; directive donât cache the response at all, although by 2025 this should be an exceedingly rare sight.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;private&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;private&lt;/code&gt; response directive indicates that the response is meant for a single user. â RFC 9111 Â§ 5.2.2.7.&lt;/p&gt;
    &lt;p&gt;Therefore:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a shared cache must not store the response; and&lt;/item&gt;
      &lt;item&gt;a private cache may store the response even if the response wouldnât otherwise be heuristically cacheable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The &lt;code&gt;private&lt;/code&gt; directive can be used to guard against other directives that might inadvertently make authenticated responses available to shared caches.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;public&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;public&lt;/code&gt; response directive indicates two things:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;a shared cache may store and reuse a response to a request containing an &lt;code&gt;Authorization&lt;/code&gt;header, which itâs generally prohibited from doing; and&lt;/item&gt;
      &lt;item&gt;a private cache may store the response even if the response wouldnât otherwise be heuristically cacheable.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;
      &lt;code&gt;s-maxage=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;s-maxage=&amp;lt;number&amp;gt;&lt;/code&gt; response directive is analogous to &lt;code&gt;max-age&lt;/code&gt;, but only affects shared caches. â RFC 9111 Â§ 5.2.2.10.&lt;/p&gt;
    &lt;p&gt;The directive also incorporates the semantics of the &lt;code&gt;proxyârevalidate&lt;/code&gt; response directive, in that a shared cache must not use a stale response until it has been successfully validated with the origin server.&lt;/p&gt;
    &lt;p&gt;Side effects: &lt;code&gt;s-maxage&lt;/code&gt; is one of the directives, along with &lt;code&gt;must-revalidate&lt;/code&gt; and &lt;code&gt;public&lt;/code&gt;, that allow shared caches to store and reuse a response to a request containing an &lt;code&gt;Authorization&lt;/code&gt; header, which they are generally prohibited from doing.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;proxy-revalidate&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;proxy-revalidate&lt;/code&gt; response directive is analogous to &lt;code&gt;must-revalidate&lt;/code&gt;, but only affects shared caches. â RFC 9111 Â§ 5.2.2.8.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;no-transform&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;no-transform&lt;/code&gt; response directive indicates that intermediaries, regardless of whether they implement a cache or not, must not transform the response content, such as optimizing images or compressing stylesheets and scripts. â RFC 9111 Â§ 5.2.2.6.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;stale-while-revalidate=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;The &lt;code&gt;stale-while-revalidate&lt;/code&gt; response directive was defined in RFC 5861: HTTP Cache-Control Extensions for Stale Content (2010). It indicates that the cache may use a cached response if it hasnât exceeded its freshness lifetime by more than the specified number of seconds.&lt;/p&gt;
    &lt;p&gt;Whenever the presence of this directive causes a stale response to be served, the cache should trigger a background revalidation of the response.&lt;/p&gt;
    &lt;p&gt;The author of the RFC, Mark Nottingham, has written a rationale for this directive.&lt;/p&gt;
    &lt;p&gt;
      &lt;code&gt;stale-if-error=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/p&gt;
    &lt;p&gt;Also defined in the RFC 5861 extension, the &lt;code&gt;stale-if-error&lt;/code&gt; response directive indicates that the cache may use a cached response if it hasnât exceeded its freshness lifetime by more than the specified number of seconds, if the attempt to validate the stale response results in an error.&lt;/p&gt;
    &lt;p&gt;HTTP Caching Tests suggests this directive is not well supported.&lt;/p&gt;
    &lt;head rend="h2"&gt;Cache-Control request directives&lt;/head&gt;
    &lt;p&gt;As web developers, we most often deal with &lt;code&gt;Cache-Control&lt;/code&gt; in HTTP responses, but this header can also be included on HTTP requests. Browsers, for example, use them when the user refreshes the page.&lt;/p&gt;
    &lt;p&gt;When used in HTTP requests, &lt;code&gt;Cache-Control&lt;/code&gt; directives express the clientâs preference in regards to the freshness or age of the response. Caches reconcile these requests with the &lt;code&gt;Cache-Control&lt;/code&gt; response directives of its cached responses.&lt;/p&gt;
    &lt;quote&gt;&lt;p&gt;Note: the&lt;/p&gt;&lt;code&gt;cache&lt;/code&gt;option for&lt;code&gt;fetch()&lt;/code&gt;has a separate set of values that map to&lt;code&gt;Cache-Control&lt;/code&gt;request directives, but the mappings are not always intuitive. For example,&lt;code&gt;cache: 'no-cache'&lt;/code&gt;maps to&lt;code&gt;Cache-Control: max-age=0&lt;/code&gt;. For the curious, the mappings are defined here. You can always set your&lt;code&gt;Cache-Control&lt;/code&gt;headers directly with the&lt;code&gt;headers&lt;/code&gt;option.&lt;/quote&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;max-age=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;max-age&lt;/code&gt; request directive indicates that the client wants a fresh response whose age is less than or equal to the specified number of seconds. When combined with &lt;code&gt;max-stale&lt;/code&gt;, the client will accept some stale responses. â RFC 9111 Â§ 5.2.1.1.&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;max-stale=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;max-stale&lt;/code&gt; request directive indicates that the client will accept a stale response that has exceeded its freshness lifetime by no more than the specified number of seconds. When used without an argument, &lt;code&gt;max-stale&lt;/code&gt; indicates that the client will accept any stale response, no matter how old. â RFC 9111 Â§ 5.2.1.2&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;min-fresh=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;min-fresh&lt;/code&gt; request directive indicates that the client prefers a response that still has at least the specified number of seconds of freshness left. â RFC 9111 Â§ 5.2.1.3&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;no-cache&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;no-cache&lt;/code&gt; request directive indicates that the client prefers caches not to use a stored response without successfully validating it with the origin server. â RFC 9111 Â§ 5.2.1.4&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;no-store&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;no-store&lt;/code&gt; request directive indicates that a cache must not store any part of either this request or any response to it. The same caveats as to its response counterpart apply. â RFC 9111 Â§ 5.2.1.5&lt;/p&gt;
    &lt;p&gt;If a cache serves this request with a response that was previously stored, the &lt;code&gt;no-store&lt;/code&gt; request directive doesnât cause the cache to remove the response after serving it.&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;no-transform&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;no-transform&lt;/code&gt; request directive indicates that the client is asking for intermediaries to avoid transforming the content. â RFC 9111 Â§ 5.2.1.6&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;only-if-cached&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;The &lt;code&gt;only-if-cached&lt;/code&gt; request directive indicates that the client only wants a stored response. Caches should respond with either a stored response that satisfies all the other constraints, or an HTTP 504 Gateway Timeout status code. â RFC 9111 Â§ 5.2.1.7&lt;/p&gt;
    &lt;head rend="h3"&gt;
      &lt;code&gt;stale-if-error=&amp;lt;number&amp;gt;&lt;/code&gt;
    &lt;/head&gt;
    &lt;p&gt;Similarly to its response counterpart, the &lt;code&gt;stale-if-error&lt;/code&gt; request directive indicates that the client will accept a stale response that has exceeded its freshness lifetime by no more than the specified number of seconds, if an attempt to validate it resulted in a server error.&lt;/p&gt;
    &lt;head rend="h2"&gt;Browser refresh mechanisms&lt;/head&gt;
    &lt;p&gt;Browsers typically offer two refresh mechanisms:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;soft reloads, triggered by the reload button, a corresponding menu item and keyboard shortcut, and the pull-to-refresh gesture in mobile browsers, are meant to get an updated representation of the page, for example getting the latest posts on a social media timeline.&lt;/item&gt;
      &lt;item&gt;hard reloads, enabled with a modifier key, skip the cache altogether and are meant to fix interrupted loads, outdated cached responses, and other broken states.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Hereâs how some browsers on macOS implement these behaviors.&lt;/p&gt;
    &lt;head rend="h3"&gt;Soft reloads&lt;/head&gt;
    &lt;p&gt;Triggered by Ctrl + R on Windows/Linux and Command + R on macOS.&lt;/p&gt;
    &lt;p&gt;Firefox triggers a conditional request to revalidate the cached response for the main resource (the HTML file). Sub-resources such as stylesheets, scripts, and images are reloaded as usual, according to their cache directives.&lt;/p&gt;
    &lt;p&gt;Chrome behaves similarly, with the difference that the validation request for the main resource also includes a &lt;code&gt;Cache-Control: max-age=0&lt;/code&gt; directive (which canât hurt).&lt;/p&gt;
    &lt;p&gt;Instead of revalidating its cached response, Safari performs a non-conditional request for the main resource, then loads sub-resources as usual.&lt;/p&gt;
    &lt;head rend="h3"&gt;Hard reloads&lt;/head&gt;
    &lt;p&gt;Triggered by Ctrl + Shift + R on Windows/Linux and Command + Shift + R on macOS except Safari, which uses Command + Option + R. (If youâve applied your muscle memory to Safari before, you know all too well that the common shortcut opens Reader Mode insteadâ¦)&lt;/p&gt;
    &lt;p&gt;On a hard reload, all three browsers trigger non-conditional requests with the &lt;code&gt;Cache-Control: no-cache&lt;/code&gt; directive on the HTML page and its sub-resources.&lt;/p&gt;
    &lt;p&gt;Curiously, once you perform a hard reload in Safari, subsequent soft reloads will still use the &lt;code&gt;Cache-Control: no-cache&lt;/code&gt; request directive to fetch the main resource, which is probably an unintended, but otherwise benign behavior.&lt;/p&gt;
    &lt;head rend="h3"&gt;The &lt;code&gt;immutable&lt;/code&gt; response directive&lt;/head&gt;
    &lt;p&gt;Reloading a web page didnât always work like this. Historically, when performing a soft reload, all the sub-resources would be revalidated along with the main resource, in effect freshening up the cache for the current page.&lt;/p&gt;
    &lt;p&gt;Circa 2015, Facebook was seeing several HTTP 304 Not Modified responses on long-lived resources like scripts and stylesheets whenever a user would refresh their feed page with the browserâs reload button.&lt;/p&gt;
    &lt;p&gt;To address this issue, Patrick McManus from Mozilla proposed the &lt;code&gt;immutable&lt;/code&gt; response directive, which later became RFC 8246: HTTP Immutable Responses.&lt;/p&gt;
    &lt;p&gt;The directive indicates that the origin server wonât update a resource during the freshness lifetime of the cached response, so a cache shouldnât issue conditional requests for responses that are still fresh when the user reloads the page, unless the user really, really wants an updated response (e.g. a hard reload).&lt;/p&gt;
    &lt;p&gt;Around the time that support for &lt;code&gt;immutable&lt;/code&gt; landed in Firefox 49 and Facebook began to use it to great effect, Chrome introduced a new way to perform reloads that solved the problem without introducing additional directives: instead of revalidating everything on a soft reload, just revalidate the main resource and load sub-resources as usual. Safari switched over to the new reload policy soon after [Webkit#169756], and Firefox eventually did with Firefox 100.&lt;/p&gt;
    &lt;p&gt;That leaves the &lt;code&gt;immutable&lt;/code&gt; directive in an awkward place. Safari added support [Webkit#167497] but Chrome representatives remain unconvinced that it offers a significant benefit on top of the current reload behavior [Chromium#41253661].&lt;/p&gt;
    &lt;head rend="h2"&gt;Caching responses to authenticated requests&lt;/head&gt;
    &lt;p&gt;One of the more confusing aspects of HTTP caching is how various &lt;code&gt;Cache-Control&lt;/code&gt; response directives affect the way shared caches treat responses to requests that contain an &lt;code&gt;Authorization&lt;/code&gt; header, which are understood as specific to a single user.&lt;/p&gt;
    &lt;p&gt;As per RFC 9111 Â§ 3.5, shared caches are not allowed to store these responses &lt;quote&gt;unless the response contains a Cache-Control field with a response directive that allows it to be stored by a shared cache, and the cache conforms to the requirements of that directive for that response.&lt;/quote&gt;&lt;/p&gt;
    &lt;p&gt;The three directives that enable shared caches to store authenticated responses, and which must therefore be carefully evaluated before deploying, are:&lt;/p&gt;
    &lt;p&gt;Conversely, a &lt;code&gt;private&lt;/code&gt; directive prevents any other directive from making authenticated responses eligible to shared caches.&lt;/p&gt;
    &lt;head rend="h2"&gt;Conclusion&lt;/head&gt;
    &lt;p&gt;I wrote this article to clarify for myself what the various cache directives stand for and how they overlap and interact. It only covers the main ideas, without delving into the more obscure corners of HTTP semantics. I approached the subject with a âclear cacheâ, and mainly used the normative references (RFC 9111 and its extensions), aided by various guides from different eras:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Caching Tutorial for Web Authors and Webmasters (1998â) by Mark Nottingham;&lt;/item&gt;
      &lt;item&gt;Caching best practices &amp;amp; max-age gotchas (2016) by Jake Archibald;&lt;/item&gt;
      &lt;item&gt;Prevent unnecessary network requests with the HTTP Cache (2018) by Ilya Grigorik and Jeff Posnik;&lt;/item&gt;
      &lt;item&gt;Cache control for civilians (2019â2025) and Why Do We Have a Cache-Control Request Header? (2025) by Harry Roberts;&lt;/item&gt;
      &lt;item&gt;Cache-Control Recommendations (2021) by April King;&lt;/item&gt;
      &lt;item&gt;Web Caching on MDN.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Whatâs interesting about these guides is that the recommendations donât just encode an interpretation of the specs, but also incorporate safeguards against non-conformant or outdated browser caches and intermediares.&lt;/p&gt;
    &lt;p&gt;In light of developments as recent as 2022, it would be cool to figure out to what extent things have improved, and which of these safeguards can be discarded. HTTP Caching Tests seems to be a good resource for assessing the situation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46368616</guid><pubDate>Tue, 23 Dec 2025 19:41:39 +0000</pubDate></item><item><title>Some Epstein file redactions are being undone with hacks</title><link>https://www.theguardian.com/us-news/2025/dec/23/epstein-unredacted-files-social-media</link><description>&lt;doc fingerprint="ff61601cb40a8325"&gt;
  &lt;main&gt;
    &lt;p&gt;People examining documents released by the Department of Justice in the Jeffrey Epstein case discovered that some of the file redaction can be undone with Photoshop techniques, or by simply highlighting text to paste into a word processing file.&lt;/p&gt;
    &lt;p&gt;Un-redacted text from these documents began circulating through social media on Monday evening. An exhibit in a civil case in the Virgin Islands against Darren K Indyke and Richard D Kahn, two executors of Epstein’s estate, contains redacted allegations explaining how Epstein and his associates had facilitated the sexual abuse of children. The exhibit was the second amended complaint in the state case against Indyke and Kahn.&lt;/p&gt;
    &lt;p&gt;In section 85, the redacted portion states: “Between September 2015 and June 2019, Indyke signed (FAC) for over $400,000 made payable to young female models and actresses, including a former Russian model who received over $380,000 through monthly payments of $8,333 made over a period of more than three and a half years until the middle of 2019.”&lt;/p&gt;
    &lt;p&gt;Prosecutors in the Virgin Islands settled its civil sex-trafficking case against Epstein’s estate, Indyke and Kahn in 2022 for $105m, plus one half of the proceeds from the sale of Little St James, the island on which Epstein resided and on which many of his crimes occurred. The justice department press release announcing the settlement did not include an admission of liability.&lt;/p&gt;
    &lt;p&gt;Indyke, an attorney who represented Epstein for decades, has not been criminally indicted by federal authorities. He was hired by the Parlatore Law Group in 2022, before the justice department settled the Epstein case. That firm represents the defense secretary, Pete Hegseth, and previously represented Donald Trump in his defense against charges stemming from the discovery of classified government documents stored at Trump’s Florida estate. Calls and email seeking comment from Indyke and the Parlatore Law Group have not yet been returned.&lt;/p&gt;
    &lt;p&gt;Trump has repeatedly denied any knowledge of or involvement in Epstein’s criminal activities and any wrongdoing.&lt;/p&gt;
    &lt;p&gt;Other sections further allege how Epstein’s enterprise concealed crimes.&lt;/p&gt;
    &lt;p&gt;“Defendants also attempted to conceal their criminal sex trafficking and abuse, conduct by paying large sums of money to participant-witnesses, including by paying for their attorneys’ fees and case costs in litigation related to this conduct,” reads one redacted passage.&lt;/p&gt;
    &lt;p&gt;“Epstein also threatened harm to victims and helped release damaging stories about them to damage their credibility when they tried to go public with their stories of being trafficked and sexually abused. Epstein also instructed one or more Epstein Enterprise participant-witnesses to destroy evidence relevant to ongoing court proceedings involving Defendants’ criminal sex trafficking and abuse conduct.”&lt;/p&gt;
    &lt;p&gt;Redactions of sections 184 through 192 of the document describe property taxes paid by companies incorporated by Epstein on properties that were not on the balance sheet for those firms.&lt;/p&gt;
    &lt;p&gt;“For instance, Cypress’s Balance Sheet as of December 31, 2018 did not reflect any assets other than cash of $18,824. Further, Cypress reported only $301 in expenses for the year ended December 31, 2018, despite it paying $106,394.60 in Santa Fe property taxes on November 6, 2018,” reads one redacted passage.&lt;/p&gt;
    &lt;p&gt;“Similarly, in 2017, Cypress reported as its only asset cash in the amount of $29,736 and expenses of $150, despite it paying $55,770.41 and $113,679.56 in Santa Fe property taxes during 2017.”&lt;/p&gt;
    &lt;p&gt;The Epstein Files Transparency Act signed into law last month permits the Department of Justice “to withhold certain information such as the personal information of victims and materials that would jeopardize an active federal investigation”.&lt;/p&gt;
    &lt;p&gt;It was unclear how property material complies with the redaction standard under the law. An inquiry to the Department of Justice has not yet been answered.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46368946</guid><pubDate>Tue, 23 Dec 2025 20:10:30 +0000</pubDate></item><item><title>I didn't realize my LG TV was spying on me until I turned off Live Plus</title><link>https://www.pocket-lint.com/lg-tv-turn-off-live-plus/</link><description>&lt;doc fingerprint="d0fcd0966b03abba"&gt;
  &lt;main&gt;
    &lt;p&gt;When I first set up my LG TV, my main focus was ensuring the picture quality was perfect. To LG's credit, the TV automatically detected all of my devices -- my PC, PS5, Switch 2, and Fire TV Stick 4K Max -- and applied the best settings for each. The image looked immaculate right out of the box, so I didn't spend much time digging through the settings menu and instead jumped straight into movies and gaming.&lt;/p&gt;
    &lt;p&gt;However, about a week later, while I was trying to disable ads on the home screen, I stumbled across a setting I didn't even know existed and instantly knew I wanted to be disabled. It's called Live Plus.&lt;/p&gt;
    &lt;p&gt;If you've never heard of Live Plus before, it's a feature on LG smart TVs that uses ACR (automatic content recognition) to analyze what's displayed on your screen (via The Markup). LG then uses that data to offer "personalized services," including content recommendations and advertisements.&lt;/p&gt;
    &lt;p&gt;Naturally, reading what the feature was on my TV was a bit of a shock. I wasn't thrilled to discover a setting enabled by default that essentially spies on my screen just for ads, but at the same time, I felt a wave of relief when I realized how easy it is to turn off. If you don't want your LG TV quietly snooping on what you watch and using it to serve you ads, here's how to turn Live Plus off.&lt;/p&gt;
    &lt;head rend="h2"&gt;How to turn off Live Plus on your LG TV&lt;/head&gt;
    &lt;head rend="h3"&gt;Other TV brands have a similar setting&lt;/head&gt;
    &lt;p&gt;Thankfully, LG doesn't make disabling Live Plus too hard, though you do have to click through a few menus. If you want to turn it off, here's how:&lt;/p&gt;
    &lt;p&gt;1. Press the Settings button on your remote (the gear icon).&lt;/p&gt;
    &lt;p&gt;2. When the side menu pops up, select Settings.&lt;/p&gt;
    &lt;p&gt;3. Chose the General option.&lt;/p&gt;
    &lt;p&gt;4. Scroll down and select System.&lt;/p&gt;
    &lt;p&gt;5, Select Additional Settings.&lt;/p&gt;
    &lt;p&gt;6. Toggle Live Plus off.&lt;/p&gt;
    &lt;p&gt;In the Settings menu on its TVs, LG says, "By turning Live Plus on, you understand that the content displayed on your TV can be recognized, and that the viewing information may be used to provide you with an enhanced viewing experience and personalized services including content recommendations and advertisements."&lt;/p&gt;
    &lt;p&gt;Fortunately, once you've toggled Live Plus off, you no longer have to worry about your TV screen constantly being read to see what you're watching and to give you targeted ads.&lt;/p&gt;
    &lt;p&gt;...to be clear, this isn't just an LG problem -- other smart TV brands do the same thing with similar ACR (automatic content recognition) settings.&lt;/p&gt;
    &lt;p&gt;While it's frustrating that a setting like this exists in the first place, I was at least relieved by how easy it was to turn off. And to be clear, this isn't just an LG problem -- other smart TV brands do the same thing with similar ACR (automatic content recognition) settings.&lt;/p&gt;
    &lt;p&gt;On Samsung smart TVs, for example, you can disable targeted ads by going to Privacy Choices, selecting Terms and Conditions, and toggling off Viewing Information Services and Internet-Based Advertisement Services. On Roku TVs, ACR can be turned off by disabling Use info from TV inputs, which is tucked away in the settings menu under Smart TV Experience.&lt;/p&gt;
    &lt;p&gt;In other LG-related news, DirecTV recently launched its live TV app on the LG webOS app store, and LG recently made the controversial move to automatically install Copilot on all LG Smart TVs with its latest webOS update.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46369860</guid><pubDate>Tue, 23 Dec 2025 21:47:33 +0000</pubDate></item><item><title>X-ray: a Python library for finding bad redactions in PDF documents</title><link>https://github.com/freelawproject/x-ray</link><description>&lt;doc fingerprint="4e9453d17ca78994"&gt;
  &lt;main&gt;
    &lt;p&gt;x-ray is a Python library for finding bad redactions in PDF documents.&lt;/p&gt;
    &lt;p&gt;At Free Law Project, we collect millions of PDFs. An ongoing problem is that people fail to properly redact things. Instead of doing it the right way, they just draw a black rectangle or a black highlight on top of black text and call it a day. Well, when that happens you just select the text under the rectangle, and you can read it again. Not great.&lt;/p&gt;
    &lt;p&gt;After witnessing this problem for years, we decided it would be good to figure out how common it is, so, with some help, we built this simple tool. You give the tool the path to a PDF. It tells you if it has worthless redactions in it.&lt;/p&gt;
    &lt;p&gt;Right now, &lt;code&gt;x-ray&lt;/code&gt; works pretty well and we are using it to analyze documents
in our collections. It could be better though. Bad redactions take many forms.
See the issues tab for other examples we don't yet support. We'd love your
help solving some of tougher cases.&lt;/p&gt;
    &lt;p&gt;With uv, do:&lt;/p&gt;
    &lt;code&gt;uv add x-ray
&lt;/code&gt;
    &lt;p&gt;With pip, that'd be:&lt;/p&gt;
    &lt;code&gt;pip install x-ray
&lt;/code&gt;
    &lt;p&gt;&lt;code&gt;uvx&lt;/code&gt; lets you run this without even installing it. For example, here's an amicus brief we filed that doesn't have any bad redactions:&lt;/p&gt;
    &lt;code&gt;uvx --from x-ray xray https://storage.courtlistener.com/recap/gov.uscourts.ca3.125346/gov.uscourts.ca3.125346.45.0.pdf
{}
&lt;/code&gt;
    &lt;p&gt;Once you do install x-ray, you can easily use it on the command line. Once installed, just:&lt;/p&gt;
    &lt;code&gt;% xray path/to/your/file.pdf
{
  "1": [
    {
      "bbox": [
        58.550079345703125,
        72.19873046875,
        75.65007781982422,
        739.3987426757812
      ],
      "text": "The Ring travels by way of Cirith Ungol"
    }
  ]
}&lt;/code&gt;
    &lt;p&gt;Or if you have the file on a server somewhere, give it a URL. If it starts with &lt;code&gt;https://&lt;/code&gt;, it will be interpreted as a PDF to download. Here's congressional testimony our director made (it doesn't have any bad redactions):&lt;/p&gt;
    &lt;code&gt;% xray https://free.law/pdf/congressional-testimony-michael-lissner-free-law-project-hearing-on-ethics-and-transparency-2021-10-26.pdf
{}&lt;/code&gt;
    &lt;p&gt;A fun trick you can do is to make a file with one URL per line, call it &lt;code&gt;urls.txt&lt;/code&gt;. Then you can run this to check each URL:&lt;/p&gt;
    &lt;code&gt;xargs -n 1 xray  &amp;lt; urls.txt&lt;/code&gt;
    &lt;p&gt;However you run &lt;code&gt;xray&lt;/code&gt; on the command line, you'll get JSON as output. When you have that, you can use it with tools like &lt;code&gt;jq&lt;/code&gt;. The format is as follows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;It's a dict.&lt;/item&gt;
      &lt;item&gt;The keys are page numbers.&lt;/item&gt;
      &lt;item&gt;Each page number maps to a list of dicts.&lt;/item&gt;
      &lt;item&gt;Each of those dicts maps to two keys.&lt;/item&gt;
      &lt;item&gt;The first key is &lt;code&gt;bbox&lt;/code&gt;. This is a four-tuple that indicates the x,y positions of the upper left corner and then lower right corners of the bad redaction.&lt;/item&gt;
      &lt;item&gt;The second key is &lt;code&gt;text&lt;/code&gt;. This is the text under the bad rectangle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Simple enough.&lt;/p&gt;
    &lt;p&gt;You can also use it as a Python module, if you prefer the long-form:&lt;/p&gt;
    &lt;code&gt;% python -m xray some-file.pdf
&lt;/code&gt;
    &lt;p&gt;But that's not as easy to remember.&lt;/p&gt;
    &lt;p&gt;If you want a bit more, you can, of course, use &lt;code&gt;xray&lt;/code&gt; in Python:&lt;/p&gt;
    &lt;code&gt;from pprint import pprint
import xray
bad_redactions = xray.inspect("some/path/to/your/file.pdf")  # Pathlib works too
pprint(bad_redactions)
{1: [{'bbox': (58.550079345703125,
               72.19873046875,
               75.65007781982422,
               739.3987426757812),
      'text': 'Aragorn is the one true king.'}]}&lt;/code&gt;
    &lt;p&gt;The output is the same as above, except it's a Python object, not a JSON object.&lt;/p&gt;
    &lt;p&gt;If you already have the file contents as a &lt;code&gt;bytes&lt;/code&gt; object, that'll work too:&lt;/p&gt;
    &lt;code&gt;some_bytes = requests.get("https://lotr-secrets.com/some-doc.pdf").content
bad_redactions = xray.inspect(some_bytes)&lt;/code&gt;
    &lt;p&gt;Note that because the &lt;code&gt;inspect&lt;/code&gt; method uses the same signature no matter what,
the type of the object you give it is essential:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Input&lt;/cell&gt;
        &lt;cell role="head"&gt;&lt;code&gt;xray&lt;/code&gt;'s Assumption&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;str&lt;/code&gt; or Pathlib &lt;code&gt;Path&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;local file&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;&lt;code&gt;str&lt;/code&gt; that starts with &lt;code&gt;https://&lt;/code&gt;&lt;/cell&gt;
        &lt;cell&gt;URL to download&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;bytes&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;PDF in memory&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;This means that if you provide the filename on disk as a bytes object instead of a &lt;code&gt;str&lt;/code&gt;, it's not going to work. This will fail:&lt;/p&gt;
    &lt;code&gt;xray.inspect(b"some-file-path.pdf")&lt;/code&gt;
    &lt;p&gt;That's pretty much it. There are no configuration files or other variables to learn. You give it a file name. If there is a bad redaction in it, you'll soon find out.&lt;/p&gt;
    &lt;p&gt;Under the covers, &lt;code&gt;xray&lt;/code&gt; uses the high-performant PyMuPDF project to parse PDFs. It has been a wonderful project to work with.&lt;/p&gt;
    &lt;p&gt;You can read the source to see how it works, but the general idea is to:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Find rectangles in a PDF.&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Find letters in the same location&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Render the rectangle as an image&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Inspect the rectangle to see if it's all one color. If it is, then that's a bad redaction. If not, then we assume you can see a mix of text and drawings, indicating a redaction that's OK.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The PDF format is a big and complicated one, so it's difficult to do all this perfectly. We do our best, but there's always more to do to make it better. Donations and sponsored work help.&lt;/p&gt;
    &lt;p&gt;Please see the issues list on Github for things we need, or start a conversation if you have questions. Before you do your first contribution, we'll need a signed contributor license agreement. See the template in the repo.&lt;/p&gt;
    &lt;p&gt;Releases happen automatically via Github Actions. To trigger an automated build:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Update CHANGES.md&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Update the version in pyproject.toml&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Tag the commit with something like "v0.0.0".&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;If you wish to create a new version manually, the process is:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Update version info in&lt;/p&gt;
        &lt;code&gt;pyproject.toml&lt;/code&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Configure your Pypi credentials with Poetry&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Build and publish the version:&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;poetry publish --build&lt;/code&gt;
    &lt;p&gt;This repository is available under the permissive BSD license, making it easy and safe to incorporate in your own libraries.&lt;/p&gt;
    &lt;p&gt;Pull and feature requests welcome. Online editing in GitHub is possible (and easy!).&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46369923</guid><pubDate>Tue, 23 Dec 2025 21:54:30 +0000</pubDate></item><item><title>Texas app store age verification law blocked by federal judge</title><link>https://www.macrumors.com/2025/12/23/texas-app-store-law-blocked/</link><description>&lt;doc fingerprint="b6c6f8ad132711dc"&gt;
  &lt;main&gt;
    &lt;div&gt;
      &lt;head rend="h1"&gt;Texas App Store Age Verification Law Blocked by Federal Judge&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;p&gt;A Texas federal judge today blocked an App Store age verification law that was set to go into effect on January 1, 2026, which means Apple may not have to support the changes after all.&lt;/p&gt;
          &lt;p&gt;&lt;lb/&gt;The Texas App Store Accountability Act (SB2420) requires Apple and other app marketplaces to confirm user age when a person creates an Apple Account. Apple Accounts for users under 18 would need to join a Family Sharing group, with new controls available for parents and restrictions for minors.&lt;/p&gt;
          &lt;p&gt;In a preliminary injunction that delays the implementation of the act, Judge Robert Pitman said that it violates the First Amendment and is "more likely than not unconstitutional."&lt;/p&gt;
          &lt;quote&gt;
            &lt;p&gt;The Act is akin to a law that would require every bookstore to verify the age of every customer at the door and, for minors, require parental consent before the child or teen could enter and again when they try to purchase a book. As set out below, the Court finds a likelihood that, when considered on the merits, SB 2420 violates the First Amendment.&lt;/p&gt;
          &lt;/quote&gt;
          &lt;p&gt;The injunction was in response to a motion filed by the Computer and Communications Industry Association (CCIA), a group that includes Apple and Google. Today's decision is a win for Apple, as Apple has been fighting against age assurance requirements in Texas and other states. Apple says that the Texas law impacts user privacy.&lt;/p&gt;
          &lt;quote&gt;
            &lt;p&gt;While we share the goal of strengthening kids' online safety, we are concerned that SB2420 impacts the privacy of users by requiring the collection of sensitive, personally identifiable information to download any app, even if a user simply wants to check the weather or sports scores.&lt;/p&gt;
          &lt;/quote&gt;
          &lt;p&gt;The court will move on to determining whether the law is facially invalid, which would mean that it is unconstitutional and will be entirely thrown out.&lt;/p&gt;
          &lt;p&gt;Note: Due to the political or social nature of the discussion regarding this topic, the discussion thread is located in our Political News forum. All forum members and site visitors are welcome to read and follow the thread, but posting is limited to forum members with at least 100 posts.&lt;/p&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;head rend="h2"&gt;Popular Stories&lt;/head&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple hasn't updated the Apple TV 4K since 2022, and 2025 was supposed to be the year that we got a refresh. There were rumors suggesting Apple would release the new Apple TV before the end of 2025, but it looks like that's not going to happen now. Subscribe to the MacRumors YouTube channel for more videos. Bloomberg's Mark Gurman said several times across 2024 and 2025 that Apple would...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;The European Commission today praised the interoperability changes that Apple is introducing in iOS 26.3, once again crediting the Digital Markets Act (DMA) with bringing "new opportunities" to European users and developers. The Digital Markets Act requires Apple to provide third-party accessories with the same capabilities and access to device features that Apple's own products get. In iOS...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;You'd think things would be slowing down heading into the holidays, but this week saw a whirlwind of Apple leaks and rumors while Apple started its next cycle of betas following last week's release of iOS 26.2 and related updates. This week also saw the release of a new Apple Music integration with ChatGPT, so read on below for all the details on this week's biggest stories! Top Stories i...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;While the iPhone 18 Pro and iPhone 18 Pro Max are not expected to launch for another nine months, there are already plenty of rumors about the devices. Below, we have recapped 12 features rumored for the iPhone 18 Pro models. The same overall design is expected, with 6.3-inch and 6.9-inch display sizes, and a "plateau" housing three rear cameras Under-screen Face ID Front camera in...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Earlier this month, Apple released iOS 26.2, following more than a month of beta testing. It is a big update, with many new features and changes for iPhones. iOS 26.2 adds a Liquid Glass slider for the Lock Screen's clock, offline lyrics in Apple Music, and more. Below, we have highlighted a total of eight new features. Liquid Glass Slider on Lock Screen A new slider in the Lock...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Next year's iPhone 18 Pro and iPhone 18 Pro Max will be equipped with under-screen Face ID, and the front camera will be moved to the top-left corner of the screen, according to a new report from The Information's Wayne Ma and Qianer Liu. As a result of these changes, the report said the iPhone 18 Pro models will not have a pill-shaped Dynamic Island cutout at the top of the screen....&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple is significantly increasing its reliance on Samsung for iPhone memory as component prices surge, according to The Korea Economic Daily. Apple is said to be expanding the share of iPhone memory it sources from Samsung due to rapidly rising memory prices. The shift is expected to result in Samsung supplying roughly 60% to 70% of the low-power DRAM used in the iPhone 17, compared with a...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
      &lt;div&gt;
        &lt;div&gt;
          &lt;div&gt;
            &lt;p&gt;Apple's first foldable iPhone, rumored for release next year, may turn out to be smaller than most people imagine, if a recent report is anything to go by. According to The Information, the outer display on the book-style device will measure just 5.3 inches – that's smaller than the 5.4-inch screen on the iPhone mini, a line Apple discontinued in 2022 due to poor sales. The report has led ...&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;
    &lt;/div&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46370012</guid><pubDate>Tue, 23 Dec 2025 22:03:46 +0000</pubDate></item><item><title>Microspeak: North Star – The Old New Thing (2015)</title><link>https://devblogs.microsoft.com/oldnewthing/20151103-00/?p=91861</link><description>&lt;doc fingerprint="740e9f0f4f6e4c55"&gt;
  &lt;main&gt;
    &lt;p&gt;I noted it in the interview with the Defrag Tools show, but I’ll make a proper Microspeak for it. Today’s term is North star.&lt;/p&gt;
    &lt;p&gt;This term rose quickly to prominence in October 2015. My research suggests that it had been simmering below the surface for about a year. For example, here’s an isolated citation from May 2015:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;The best you can do is paint a compelling picture of an improved world (your north star), and plan the long journey to it.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This citation is interesting because it seems to give a definition for “north star”: It means “a compelling picture of an improved world”.&lt;/p&gt;
    &lt;p&gt;The term has become wildly popular of late at Microsoft. I guess a major executive used the term recently, so now it’s suddenly the cool thing to say.&lt;/p&gt;
    &lt;p&gt;We had a team meeting a little while ago. One of the agenda items was “Longer term North star topics”, which was itself rather intriguing. During the meeting, I noted¹ the following uses of the term:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;There may be changes along the way, but your north star of the feature is intact.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;quote&gt;
      &lt;p&gt;We have to decide where we want to go as a north star.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;I raised my hand. “What do you mean by north star? Because if you follow the north star, you end up at the north pole, and not where you actually want to go.”&lt;/p&gt;
    &lt;p&gt;The speaker seemed a bit frustrated by this question. “Who is this idiot who doesn’t know what a north star is? Certainly this person hasn’t been in all the meetings I’ve been in, where people are saying ‘north star’ all over the place.”&lt;/p&gt;
    &lt;p&gt;The speaker noted that I might want to look it up in the dictionary, because it would have told me that the north star is the goal you have beyond your immediate goal. It’s a guiding principle that keeps you on the right path for your journey. (Curiously, this definition doesn’t appear anywhere in any online dictionary I could find. It also doesn’t match the citation at the top of this article.)&lt;/p&gt;
    &lt;p&gt;So there you go. An explicit definition, as provided by somebody who used the term. I embarrassed myself in front of my whole team for you.&lt;/p&gt;
    &lt;p&gt;Bonus chatter: Later that same day, a top executive sent mail to the entire company. It too used the term “north star”:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;With Microsoft’s mission as our north star—to empower every person and every organization on the planet to achieve more—we have a…&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;¹ Yes, when I attend meetings, one of the things I pay particular attention to is new jargon, so I can add it to my collection of citations. If you see me pull out my phone and jot something down, it’s either because I’m writing down a question to ask later, or I’m preserving something you said so I can add it to my Microspeak citations.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46370180</guid><pubDate>Tue, 23 Dec 2025 22:23:49 +0000</pubDate></item><item><title>Is Northern Virginia still the least reliable AWS region?</title><link>https://statusgator.com/blog/aws-least-reliable-region-in-2025/</link><description>&lt;doc fingerprint="3630a8658c210b4f"&gt;
  &lt;main&gt;
    &lt;p&gt;This updated analysis is based on StatusGator outage data collected from January 1 to December 9, 2025. We decided to review our AWS analysis of outages in 2022 due to several new AWS incidents, especially another widely discussed AWS outage in us-east-1 (N. Virginia) that occurred on October 20, 2025.&lt;/p&gt;
    &lt;p&gt;We’ve expanded the report with fresh 2025 regional data as well as a new breakdown of affected AWS services.&lt;/p&gt;
    &lt;head rend="h2"&gt;The Data Behind the Study&lt;/head&gt;
    &lt;p&gt;StatusGator continuously monitors the official AWS status pages and aggregates incidents across every public AWS Region. This analysis reflects:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Major, publicly acknowledged AWS outages&lt;/item&gt;
      &lt;item&gt;All commercial AWS regions (GovCloud excluded)&lt;/item&gt;
      &lt;item&gt;Data timeframe: January 1, 2025 – December 9, 2025&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;AWS Outage Ranking by Region&lt;/head&gt;
    &lt;p&gt;So let’s take a look at the number of outages, duration, and components affected.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Region&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of outages&lt;/cell&gt;
        &lt;cell role="head"&gt;Duration&lt;/cell&gt;
        &lt;cell role="head"&gt;Components Affected&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Regionless&lt;/cell&gt;
        &lt;cell&gt;12&lt;/cell&gt;
        &lt;cell&gt;31:55:19&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Canada-Central&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;3:49:57&lt;/cell&gt;
        &lt;cell&gt;19&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Hyderabad&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;0:44:59&lt;/cell&gt;
        &lt;cell&gt;46&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Ireland&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;0:44:51&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;N. Virginia&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;33:49:33&lt;/cell&gt;
        &lt;cell&gt;126&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Ohio&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;1:20:45&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Oregon&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;2:59:41&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Osaka&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;2:15:01&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sao Paulo&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;0:44:51&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Singapore&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;0:54:59&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Stockholm&lt;/cell&gt;
        &lt;cell&gt;2&lt;/cell&gt;
        &lt;cell&gt;11:54:49&lt;/cell&gt;
        &lt;cell&gt;81&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Sydney&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;0:50:00&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Tokyo&lt;/cell&gt;
        &lt;cell&gt;3&lt;/cell&gt;
        &lt;cell&gt;1:24:51&lt;/cell&gt;
        &lt;cell&gt;18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Zurich&lt;/cell&gt;
        &lt;cell&gt;1&lt;/cell&gt;
        &lt;cell&gt;4:54:55&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Key Findings&lt;/head&gt;
    &lt;p&gt;N. Virginia (us-east-1) is once again the least reliable AWS Region.&lt;/p&gt;
    &lt;p&gt;It leads the dataset in:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Total number of outages (10)&lt;/item&gt;
      &lt;item&gt;Total downtime (33 hours, 49 minutes)&lt;/item&gt;
      &lt;item&gt;Total components affected (126)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;No other region even comes close. Stockholm ranks second in downtime (11+ hours). Despite only 2 outages, each incident had a massive regional impact.&lt;/p&gt;
    &lt;p&gt;Regionless outages were unusually high. This category recorded 12 outages and 32 hours of downtime, indicating:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;More widespread AWS service disruptions in 2025&lt;/item&gt;
      &lt;item&gt;More failures affecting multiple regions simultaneously&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;AWS Services with the Most Outages&lt;/head&gt;
    &lt;p&gt;AWS doesn’t just experience regional outages. Service-level incidents are just as impactful.&lt;lb/&gt;We analyzed the most frequently disrupted AWS services in 2025, ranked by the number of outages.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Service&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of Outages&lt;/cell&gt;
        &lt;cell role="head"&gt;Duration&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon EC2&lt;/cell&gt;
        &lt;cell&gt;14&lt;/cell&gt;
        &lt;cell&gt;19:14:01&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon SageMaker&lt;/cell&gt;
        &lt;cell&gt;11&lt;/cell&gt;
        &lt;cell&gt;20:40:21&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AWS Glue&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;15:51:40&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon EMR&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;21:39:31&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Amazon ECS&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;19:54:32&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Key Findings&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Compute services dominated the outage list, especially:&lt;list rend="ul"&gt;&lt;item&gt;Amazon EC2 (core compute)&lt;/item&gt;&lt;item&gt;Amazon ECS (containers)&lt;/item&gt;&lt;item&gt;Amazon EMR (big data)&lt;/item&gt;&lt;/list&gt;&lt;/item&gt;
      &lt;item&gt;EMR had the longest duration among the top five (21 hours and 39 minutes).&lt;/item&gt;
      &lt;item&gt;SageMaker experienced more outages than expected for an ML service, an emerging reliability trend.&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt;AWS Services With the Longest Outage Duration or Broadest Impact&lt;/head&gt;
    &lt;p&gt;These services didn’t always have the highest count, but had the longest or most severe incidents.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Service&lt;/cell&gt;
        &lt;cell role="head"&gt;Number of Outages&lt;/cell&gt;
        &lt;cell role="head"&gt;Duration&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon OpenSearch Service&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;25:36:36&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon EMR Serverless&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;25:30:08&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon CloudWatch&lt;/cell&gt;
        &lt;cell&gt;6&lt;/cell&gt;
        &lt;cell&gt;24:58:49&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon Connect&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;22:52:42&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AWS STS&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;22:48:39&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon VPC Lattice&lt;/cell&gt;
        &lt;cell&gt;7&lt;/cell&gt;
        &lt;cell&gt;22:35:47&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon EMR&lt;/cell&gt;
        &lt;cell&gt;10&lt;/cell&gt;
        &lt;cell&gt;21:39:31&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon EventBridge&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;21:24:32&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon Kinesis Data Streams&lt;/cell&gt;
        &lt;cell&gt;5&lt;/cell&gt;
        &lt;cell&gt;21:15:00&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AWS DataSync&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;20:36:52&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon Elastic Load Balancing&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;12:34:20&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Amazon DynamoDB&lt;/cell&gt;
        &lt;cell&gt;9&lt;/cell&gt;
        &lt;cell&gt;13:19:18&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;AWS Transit Gateway&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;17:14:51&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;AWS Lambda&lt;/cell&gt;
        &lt;cell&gt;8&lt;/cell&gt;
        &lt;cell&gt;13:50:15&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;head rend="h3"&gt;Key Findings&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;OpenSearch, EMR Serverless, and CloudWatch each exceeded 24+ hours of cumulative downtime.&lt;/item&gt;
      &lt;item&gt;Mission-critical systems like STS, DynamoDB, Lambda, and ELB saw prolonged disruptions.&lt;/item&gt;
      &lt;item&gt;EMR appears in both spreadsheets, indicating it experienced frequent and long-lasting ones.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Across Tables 1–3 above, we see a consistent pattern emerge:&lt;/p&gt;
    &lt;p&gt;1. Many of the affected components were concentrated in N. Virginia&lt;/p&gt;
    &lt;p&gt;With 126 components affected, us-east-1 experienced the widest service disruption footprint.&lt;/p&gt;
    &lt;p&gt;2. Region-level outages and service-level outages are correlated&lt;/p&gt;
    &lt;p&gt;Major incidents involving:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;EC2&lt;/item&gt;
      &lt;item&gt;SageMaker&lt;/item&gt;
      &lt;item&gt;EMR&lt;/item&gt;
      &lt;item&gt;CloudWatch&lt;/item&gt;
      &lt;item&gt;OpenSearch&lt;/item&gt;
      &lt;item&gt;STS&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;…almost always touch N. Virginia due to:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Higher customer density&lt;/item&gt;
      &lt;item&gt;More service deployment fronts&lt;/item&gt;
      &lt;item&gt;More inter-service dependency points&lt;/item&gt;
      &lt;item&gt;Heavier API traffic&lt;/item&gt;
      &lt;item&gt;Higher multi-AZ coordination complexity&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;3. The longest-running outages disproportionately affected us-east-1&lt;/p&gt;
    &lt;p&gt;Duration-heavy outages (CloudWatch, OpenSearch, EMR Serverless) frequently included N. Virginia, driving up the region’s total downtime.&lt;/p&gt;
    &lt;p&gt;Conclusion:&lt;/p&gt;
    &lt;p&gt;N. Virginia is not only the region with the most outages, but it is also the region where service outages cascade the widest and run the longest.&lt;/p&gt;
    &lt;head rend="h2"&gt;AWS Outage on October 20, 2025&lt;/head&gt;
    &lt;p&gt;On October 20, 2025, AWS experienced one of the most significant cloud outages in its history. 76 individual AWS components in the N. Virginia region alone showed disruption, by far the most heavily affected region.&lt;/p&gt;
    &lt;p&gt;Portions of Amazon Web Services were down for nearly 15 hours, causing cascading failures across thousands of SaaS platforms.&lt;/p&gt;
    &lt;p&gt;StatusGator’s Early Warning Signals detected the incident approximately ten minutes before AWS officially acknowledged it, ultimately identifying outages across more than 2,000 of the 6,000 services in our monitoring network.&lt;/p&gt;
    &lt;p&gt;However, the magnitude of the event meant StatusGator was also impacted, experiencing two periods of dashboard and status page downtime due to a surge in global traffic and failures in upstream infrastructure.&lt;/p&gt;
    &lt;p&gt;Despite these disruptions, StatusGator delivered over 100,000 outage notifications throughout the incident and has since implemented architectural improvements to strengthen reliability during future large-scale cloud failures.&lt;/p&gt;
    &lt;head rend="h2"&gt;Why Is N. Virginia Still the Least Reliable Region in 2025?&lt;/head&gt;
    &lt;p&gt;We revisited the three common theories from our 2023 AWS outage analysis and compared them against this year’s dataset.&lt;/p&gt;
    &lt;head rend="h3"&gt;Assumption 1: “N. Virginia Has More Services, So More Things Can Break”&lt;/head&gt;
    &lt;p&gt;In 2023, we found this explanation to be weak. But the 2025 “Components Affected” numbers tell a new story:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;N. Virginia affected 126 components&lt;/item&gt;
      &lt;item&gt;Next highest: Stockholm with 81&lt;/item&gt;
      &lt;item&gt;Most regions affected ≤ 20 components&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;This indicates:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Broader blast when outages occur in N. Virginia&lt;/item&gt;
      &lt;item&gt;More interconnected or high-density service dependency&lt;/item&gt;
      &lt;item&gt;More potential points of failure&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Still, high service count alone doesn’t explain the full scale:&lt;lb/&gt;Regions like Oregon and Ireland offer nearly as many services but have far fewer issues.&lt;/p&gt;
    &lt;p&gt;So the number of components contributes to complexity, but not the root cause.&lt;/p&gt;
    &lt;head rend="h3"&gt;Assumption 2: “N. Virginia Is the Most Used and Most Heavily Loaded Region”&lt;/head&gt;
    &lt;p&gt;This remains the strongest and most likely explanation. StatusGator monitoring AWS status data historically shows:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;N. Virginia is monitored by almost 2× as many users as Oregon&lt;/item&gt;
      &lt;item&gt;And over 3× as many as many other U.S. and global regions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;More customers → heavier load → more real-world stress → more outages that reach public visibility.&lt;/p&gt;
    &lt;p&gt;So this assumption is very likely true, and reinforced by 2025 data.&lt;/p&gt;
    &lt;head rend="h3"&gt;Assumption 3: “N. Virginia Is Older and Built Differently”&lt;/head&gt;
    &lt;p&gt;AWS provides no evidence that us-east-1 uses a fundamentally different architecture. And our 2025 numbers don’t suggest “old region issues”:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Tokyo and Sydney (both older) had minimal downtime&lt;/item&gt;
      &lt;item&gt;Newer regions, like Zurich and Hyderabad, had multi-hour outages&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Like in 2023, we still see no evidence supporting this theory.&lt;/p&gt;
    &lt;head rend="h2"&gt;Summary: AWS Reliability in 2025&lt;/head&gt;
    &lt;p&gt;With only weeks left in 2025, the data is clear:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Us-east-1 (N. Virginia) remains the least reliable AWS Region&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Most outages&lt;/item&gt;
      &lt;item&gt;Most downtime&lt;/item&gt;
      &lt;item&gt;Most components affected&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Compute, analytics, and AI/ML services were the most outage-prone&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;EC2, SageMaker, Glue, EMR, and ECS led the list.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Several AWS services experienced extremely long-running disruptions&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;OpenSearch, CloudWatch, EMR Serverless, and STS had over 24 hours of cumulative downtime.&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Multi-region outages increased&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Regionless category shows a notable rise in cross-region or global incidents in 2025.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get Notified of AWS Outages Before AWS Reports Them&lt;/head&gt;
    &lt;p&gt;StatusGator aggregates every AWS service and region into a single unified dashboard.&lt;lb/&gt;We alert you instantly, often before AWS posts the incident publicly.&lt;/p&gt;
    &lt;p&gt;Get instant, account-specific AWS outage alerts through StatusGator’s unified dashboard, now enhanced with AWS Health integration for Enterprise customers. It delivers trusted, direct notifications about incidents, outages, and maintenance affecting your services, with built-in filtering to reduce noise, and seamless delivery to Slack, Microsoft Teams, Discord, Google Chat, and more.&lt;/p&gt;
    &lt;p&gt;Monitor AWS outages in real time with StatusGator — free to try.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46370592</guid><pubDate>Tue, 23 Dec 2025 23:12:03 +0000</pubDate></item><item><title>Unifi Travel Router</title><link>https://blog.ui.com/article/travel-in-style-unifi-style-unifi-travel-router</link><description>&lt;doc fingerprint="fdc5dc11f27532a9"&gt;
  &lt;main&gt;
    &lt;p&gt;Slip it into your pocket, power it on, and your UniFi network follows you wherever you land. No reconfiguring, no rethinking, just the same environment you trust, now mobile.&lt;/p&gt;
    &lt;p&gt;22 December 2025&lt;/p&gt;
    &lt;p&gt;View Count: 65.3K&lt;/p&gt;
    &lt;p&gt;Slip it into your pocket, power it on, and your UniFi network follows you wherever you land. No reconfiguring, no rethinking, just the same environment you trust, now mobile.&lt;/p&gt;
    &lt;p&gt;Wherever you arrive, your network identity comes with you. Location aware policies, routing rules, and WiFi broadcasts light up the moment the router connects, giving you continuity instead of compromise.&lt;/p&gt;
    &lt;p&gt;Bring your UniFi gear and the tools you already rely on. The moment they power up, everything links together as if it never left home.&lt;/p&gt;
    &lt;p&gt;The Travel Router may be compact, but it is engineered for flexibility. Ethernet, WiFi, or 5G connections via USB can all serve as uplinks, while captive portal logins on hotel networks are handled quietly in the background.&lt;/p&gt;
    &lt;p&gt;Travel light, stay connected, and keep your UniFi experience no matter where you are.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46371135</guid><pubDate>Wed, 24 Dec 2025 00:30:18 +0000</pubDate></item><item><title>Nabokov's guide to foreigners learning Russian</title><link>https://twitter.com/haravayin_hogh/status/2003299405907247502</link><description>&lt;doc fingerprint="d635f48b34542867"&gt;
  &lt;main&gt;
    &lt;p&gt;We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.&lt;/p&gt;
    &lt;p&gt;Help Center&lt;/p&gt;
    &lt;p&gt;Terms of Service Privacy Policy Cookie Policy Imprint Ads info © 2025 X Corp.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46371423</guid><pubDate>Wed, 24 Dec 2025 01:20:39 +0000</pubDate></item><item><title>Open source USB to GPIB converter (for Test and Measurement instruments)</title><link>https://github.com/xyphro/UsbGpib</link><description>&lt;doc fingerprint="37f899d5396b59b0"&gt;
  &lt;main&gt;&lt;p&gt;📰 Latest updates &lt;lb/&gt; 🔨 Buy it or build it &lt;lb/&gt; 👋 UsbGpib Introduction &lt;lb/&gt; 🏠 Housing / Enclosure &lt;lb/&gt; 💻 Software &lt;lb/&gt; 🔌 Using the device &lt;lb/&gt; ⚙️ Setting and getting parameters &lt;lb/&gt; ✔️ Testing status &lt;lb/&gt; 📓 Tutorials &amp;lt;- New! &lt;lb/&gt; ❤️ Support the Project&lt;/p&gt;&lt;p&gt;For detailed visibility look under the Latest update link above!&lt;/p&gt;&lt;p&gt;[30th Nov 2025]: Released new firmware version V2.2. See Latest updates section.&lt;/p&gt;&lt;p&gt;[23rd Nov 2025]: Added a User manual for UsbGPIB V2 under the Tutorial section&lt;/p&gt;&lt;p&gt;Tired of tinkering? Want to skip the hassle and get straight to using your GPIB setup?&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;I’ve teamed up with Elecrow to bring you ready-to-roll GPIB-USB converters – perfect for those who want results, not a weekend project.&lt;/item&gt;&lt;item&gt;These adapters come fully assembled, programmed, housed, tested, and ready to go. No fuss. No build. Just plug it in and start working.&lt;/item&gt;&lt;item&gt;The Adapters match 100% the V2 version described in this repository.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Grab yours now: https://www.elecrow.com/xyphrolabs-gpibusb.html&lt;/p&gt;&lt;p&gt;Let the data flow begin!&lt;/p&gt;&lt;p&gt;This doesn’t mean I’m going fully commercial - far from it. Offering pre-built adapters is simply a convenience for those who’d rather skip the build and get straight to using their gear.&lt;/p&gt;&lt;p&gt;All sharing, support, and collaboration will remain fully open, just as before. This is about choice, not change!&lt;/p&gt;&lt;p&gt;The full design files are located in this repository in case you want to build the devices yourself.&lt;/p&gt;&lt;p&gt;Versatile, cheap, portable and robust USB to GPIB converter (USBTMC class based).&lt;/p&gt;&lt;p&gt;You'll find many projects like this, but this one is special (ok, everybody will claim this) :-)&lt;/p&gt;&lt;p&gt;If you have a lot of test equipment at home, you might know the issues: Lots of devices only have GPIB as interface and the GPIB adapters and GPIB cables on the market are very expensive and some of them even have many issues, when run under Windows 10 (device driver does not work). Or they e.g. are not able to be operated with VISA, because they are UART based, need special command sequences, ...&lt;/p&gt;The adapters are also typically very long, such that they extend the overall length of your test equipment by at least 10cm (~4 inches).&lt;p&gt;Apart of the 2 very big manufacturers, other GPIB adapters, e.g. with Ethernet or also USB interface are not recognized by normal VISA providers or PyVisa, making the measurement control implementation specific for your GPIB adapter.&lt;/p&gt;&lt;p&gt;I've got frustrated and tried to turn it into something positive. Here a video showing the final device in action - click to view:&lt;/p&gt;&lt;p&gt;Some goals of the project were:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;Work based on the standard USBTMC protocol. This allows the GPIB test equipment to look like a normal USB based measurement device and work flawless with e.g. NI VISA, Labview, Matlab or PyVisa.&lt;/item&gt;&lt;item&gt;Have a small length - otherwise my equipment has the risk of falling from the shelf :-) Also the USB cable should connect 90 degree angled, to make it very short. With V2 HW, the length got further reduced to the size of a GPIB connector.&lt;/item&gt;&lt;item&gt;It should be cheap but still versatile (you can build a single one of these for only 14 USD!)&lt;/item&gt;&lt;item&gt;It should support ALL my test equipment, from many different GPIB implementation generations and different GPIB flavors&lt;/item&gt;&lt;item&gt;The Firmware should be upgradeable over USB&lt;/item&gt;&lt;item&gt;It should be rock-solid (!) I don't want to end up in a very long measurement being interrupted because of a software issue of my USB GPIB converter.&lt;/item&gt;&lt;item&gt;It should support additional features like serial poll, remote enabling/disabling&lt;/item&gt;&lt;item&gt;If there is no GPIB device connected to the USBGpib converter, or the GPIB device is powered down, there should be no USB device visible on the PC.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;All those goals are met.&lt;/p&gt;&lt;p&gt;Although I typically would prefer nowadays an ARM Cortex M0/3/4/7 controller, there is an issue with it. Available devices support only max. 3.3V supply voltages, such that there would be a requirement for a level-shifter towards the GPIB Bus. GPIB is based on 5V (not exactly true, but a first iteration).&lt;/p&gt;&lt;p&gt;This limited the microcontroller choice to e.g. AVR or PIC controllers. Because of very good availability I ended up in ATMEGA32U4 controllers. Apart of the device supporting 5V I/O voltages, it also does not require a regulator to be part of the application - it has an internal 3.3V regulator. This minimizes the full application schematic and BOM.&lt;/p&gt;&lt;p&gt;Apart from that, there is an excellent USB stack available http://www.fourwalledcubicle.com/LUFA.php.&lt;/p&gt;&lt;p&gt;The GPIB side of the schematic can be directly connected to the ATMega32U4 IO pins. The IO pins from the microcontroller side are only set to 2 different states: Tri-state (input) or output LOW, to talk over GPIB.&lt;/p&gt;&lt;p&gt;All components are easy to source, so I only specify the potential critical ones:&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;16 MHz Crystal: Farnell 2853867 - MCSJK-7E-16.00-8-30-100-B-30&lt;/item&gt;&lt;item&gt;REV 1 GPIB connector: Farnell 2421095 - NORCOMP 112-024-113R001. For REV 2 use a straight 24P male solder type connector e.g. from AliExpress.&lt;/item&gt;&lt;item&gt;USB connector for V1 HW: Farnell 2668483 - Amphenol ICC 61729-1011BLF&lt;/item&gt;&lt;item&gt;USB connector for V2 HW: Best is to look on AliExpress for 57 series 24P connector as a starting point.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The PCB can be ordered at nearly any PCB pool production service (e.g. 10 PCBs for 2 USD + shipping). The gerber files are included in the "HW/Gerber files" subdirectory.&lt;/p&gt;&lt;p&gt;The PCB is available in 2 revisions.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;REV 1 is the most popularly used right now due to age. It has a USB Type-B connector and an L-shaped housing visible on a few photos of this page.&lt;/item&gt;&lt;item&gt;REV 2 has some improvements like being smaller, better fit and USB Type-C connector.&lt;/item&gt;&lt;item&gt;A REV 3 is upcoming. It will be a major redesign with more speed, Ethernet and power over Ethernet support, but the same DNA: Standard protocols being used and ensure compatibility and stability!&lt;/item&gt;&lt;/list&gt;&lt;p&gt;Choose whatever you prefer. The software images, but also the external behavior is the same.&lt;/p&gt;&lt;p&gt;I created a sophisticated 3D printable housing for this adapter. The design was made with Fusion 360. The project file + the STL files are included in the "Housing" sub directory.&lt;/p&gt;&lt;p&gt;The PCB fits perfectly into it. Optionally it can be fixed with 2 mounting screws (the GPIB connector has 2 threads, use 2 times 4-40 UNC x 3/8) and the TOP cover snaps onto the housing base.&lt;/p&gt;&lt;p&gt;I printed this using an Ender 5 3D printer with black PLA, 0.15mm layer height, 1mm wall thickness, no support. Take care, that you rotate the TOP part of the housing by 180 degrees, so that the flat side is located on the printer bed. Printing works fine, several iterations of the design were made to ensure good printability. I printed so far 15 housings, without a single fail.&lt;/p&gt;&lt;p&gt;More information on this REV 1 can be found here: REV 1. Note, that also the programming/build instructions moved to this location.&lt;/p&gt;&lt;p&gt;The REV 2 housing is a lot smaller, but requires 2 screws. The housing is quite important to be able to connect and disconnect the board without breaking anything. It is key for mechanical stability of the adapter. When operating the device without housing, take very well care when plugging in and out the board in case the GPIB connector has a very tight fit.&lt;/p&gt;&lt;p&gt;Information on how to build it can be found in the HW/REV 2 folder: REV 2. Note, that also the programming/build instructions moved to this location.&lt;/p&gt;&lt;p&gt;The source code of the Boot loader (slightly modified LUFA MassStorage Boot loader) and the main USBGPIB converter are located in the "SW" subdirectory. At the time of publication LUFA 170418 release was used, with GCC as compiler.&lt;/p&gt;&lt;p&gt;Note: The Software is compatible with any HW revision in this repository. For REV 1 and REV 2 hardware you don't need different SW images.&lt;/p&gt;&lt;p&gt;For those, that just want to create their own device, I've included the binary output in the "SW/binaries" subdirectory.&lt;/p&gt;&lt;p&gt;You might be surprised initially, that the device does not show up in your device manager (or lsusb), when you connect only the USB side. This is a feature, not a bug (really!). Only, if a GPIB device is connected, you can see the device on your PC too.&lt;/p&gt;&lt;p&gt;The reason behind the feature is simple: Instead of having a standard GPIB wiring, where you have a single GPIB controller and lots of GPIB devices interconnected, USBGPIB supports only a direct connection of the USBGPIB device to your measurement device. If you have like me e.g. 14 Instruments you don't want all to show up in the device manager, if the measurement device itself is powered down - you won't anyway be able to communicate with a powered down device.&lt;/p&gt;&lt;p&gt;When USB and the GPIB side is connected, the device enumerates. The USBGPIB device reads out the ID of the instrument and constructs a unique USB Serial number out of it. It is thus easily possible to assiate multiple connected USBGPIB devices with the measurement instrument.&lt;/p&gt;&lt;p&gt;The VISA ressource name is constructed from this USB Serial number. You can identify easily e.g. in NiMax, which device is connected:&lt;/p&gt;&lt;p&gt;If you connect your USBGPIB device afterwards to another GPIB measurement device, it will disconnect and connect with a new serial number string, matching the other GPIB device *IDN? response again.&lt;/p&gt;&lt;p&gt;GPIBUSB does probe all GPIB primary addresses (and secondary address 0) for presence of a GPIB Talker/listener. It is thus not required to set a specific GPIB address - GPIBUSB will find it itself.&lt;/p&gt;&lt;p&gt;The only importance setting on the measurement device is, that the GPIB interface is enabled, which is typically the case.&lt;/p&gt;&lt;p&gt;The LED indicates different states:&lt;/p&gt;&lt;p&gt;LED blinking: The USBGPIB converter is connected to a measurement instrument, it is powered off or its GPIB port is disabled. In this state, the device is also not connected to USB and will not show up in the device manager or lsusb. LED on: The device is connected to a measurement device and GPIB communication possible. It is also accessible over USB LED off: The device is not connected over USB, or the PC powered off :-)&lt;/p&gt;&lt;p&gt;As this converter implements the standard USBTMC Test and measurement class, you can control your instrument from ANY of the normal VISA tools. I tried so far R&amp;amp;S VISA and NI Visa, using Python and Matlab to talk to the devices.&lt;/p&gt;&lt;p&gt;This project is actively maintained since more than 6 years and I personally consider it as very stable. It is proven by multiple users to operate on MacOSX, Linux and Windows and measurement equipment from several decades.&lt;/p&gt;&lt;p&gt;In general I take bug reports very serious and want to "fix them all". In case you don't get an answer on-time on items you report in the issues section, please write me: Xyphro@gmail.com. I often miss seeing issue reports very quickly, as I don't get an email once they are filed.&lt;/p&gt;&lt;p&gt;Below list is equipment I mainly myself tested. Many other users have other measurement equipment proven in use already.&lt;/p&gt;&lt;list rend="ul"&gt;&lt;item&gt;R&amp;amp;S FSEM30&lt;/item&gt;&lt;item&gt;R&amp;amp;S SMIQ03&lt;/item&gt;&lt;item&gt;R&amp;amp;S CMU200&lt;/item&gt;&lt;item&gt;R&amp;amp;S SMW200A&lt;/item&gt;&lt;item&gt;R&amp;amp;S FSW&lt;/item&gt;&lt;item&gt;R&amp;amp;S FSV&lt;/item&gt;&lt;item&gt;Keithley 199 multimeter&lt;/item&gt;&lt;item&gt;HP 34401 DMM from different generations / with different FW versions&lt;/item&gt;&lt;item&gt;HP 3325A synthesizer/frequency generator&lt;/item&gt;&lt;item&gt;HP 3457A multimeter&lt;/item&gt;&lt;item&gt;Agilent E4406A VSA transmitter tester&lt;/item&gt;&lt;item&gt;Tektronix TDS7104 Digital Phosphor Oscilloscope&lt;/item&gt;&lt;item&gt;HP 8596A spectrum analyzer&lt;/item&gt;&lt;item&gt;Agilent E3648A dual power supply&lt;/item&gt;&lt;/list&gt;&lt;list rend="ul"&gt;&lt;item&gt;It works under all popular operating systems like Windows 7 and 10, 11, Linux and MacOSX&lt;/item&gt;&lt;item&gt;USB1.1, USB2.0 and USB3.x ports tested, with and without USB HUB in between.&lt;/item&gt;&lt;item&gt;The connection stays responsive, when power cycling the PC, or hibernating/sleeping it&lt;/item&gt;&lt;item&gt;Different connection cycles (GPIB side connected first, USB side connected first, swapping GPIB side equipment, ...)&lt;/item&gt;&lt;item&gt;Extensive testing of timeout scenarious. E.g. making an illegal query and testing, if the USBTMC handles the timeouts properly. This was a very tricky part to get right.&lt;/item&gt;&lt;item&gt;Tested special transfer modes. E.g. capturing screenshots from different equipment is usually something, which will drive other GPIB adapters to the limits, because binary data of unknown length needs to be transported successfully.&lt;/item&gt;&lt;/list&gt;&lt;p&gt;The firmware version from 13th January 2024 onwards has the ability for human readable text base configuration of several parameters. The previous methods are still supported, but won't be further documented. You can look them back in the history of this file.&lt;/p&gt;&lt;p&gt;The command parser is quite simple. For that reason follow the exact syntax as shown below. Don't add extra spaces or make other modifications or concatenate commands.&lt;/p&gt;&lt;p&gt;While most GPIB interfaces use the hardware signal EOI to signal the end of a message, not all old equipment supports it. Some older instruments even don't have the EOI pin hardware wise wired and use \r or \n termination.&lt;/p&gt;&lt;p&gt;The USB-TMC standard allows to set the read termination. While in firmware versions before &amp;lt; 2.0 I did not enable that method, it is now finally supported with standard compliance.&lt;/p&gt;&lt;p&gt;I document here the older method (using pulse indicator request), but I add also the newer methods which are 100% UsbTmc compliant and portable across different equipments. You can choose which ones to use, but in some cases pulse indicator requests can be difficult to issue, for which reason it is likely better to use the standard compliant method.&lt;/p&gt;&lt;p&gt;(old method)&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!term cr')
&lt;/code&gt;&lt;p&gt;Above setting is volatile. To make this a permanent setting call the below mentioned "!term store" command.&lt;/p&gt;&lt;p&gt;(prefered method) Alternatively you can set the termination directly with visa means, e.g. using PyVisa:&lt;/p&gt;&lt;code&gt;dev.set_visa_attribute(visa.constants.VI_ATTR_TERMCHAR_EN, True)
dev.set_visa_attribute(visa.constants.VI_ATTR_TERMCHAR, ord('\r') )
&lt;/code&gt;&lt;p&gt;(old method)&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!term lf')
&lt;/code&gt;&lt;p&gt;Above setting is volatile. To make this a permanent setting call the below mentioned "!term store" command.&lt;/p&gt;&lt;p&gt;(prefered method) Alternatively you can set the termination directly with visa means, e.g. using PyVisa:&lt;/p&gt;&lt;code&gt;dev.set_visa_attribute(visa.constants.VI_ATTR_TERMCHAR_EN, True)
dev.set_visa_attribute(visa.constants.VI_ATTR_TERMCHAR, ord('\n') )
&lt;/code&gt;&lt;p&gt;(old method)&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!term eoi')
&lt;/code&gt;&lt;p&gt;Above setting is volatile. To make this a permanent setting call the below mentioned "!term store" command.&lt;/p&gt;&lt;p&gt;(prefered method) Alternatively you can set the termination directly with visa means, e.g. using PyVisa:&lt;/p&gt;&lt;code&gt;dev.set_visa_attribute(visa.constants.VI_ATTR_TERMCHAR_EN, False)
dev.set_visa_attribute(visa.constants.VI_ATTR_TERMCHAR, ord('\0') )
&lt;/code&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!term store')
&lt;/code&gt;&lt;p&gt;Above relates to the "old non prefered method". In general it is a good idea to set the read termination volatile and use the above suggested methods highlighted with (prefered method).&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
print(dev.query('!term?'))
&lt;/code&gt;&lt;p&gt;This returns a text string containing "lf", "cr" or "eoi"&lt;/p&gt;&lt;p&gt;Default wise the GPIB adapter tries during power on of the instrument to query using *IDN? or ID? commands the instrument name automatically. This is used to build the USB serial number, which finally gets part of the VISA ressource string.&lt;/p&gt;&lt;p&gt;Not all instruments support this *IDN / ID? query. For this reason this feature can be turned off. The serial number will then be built based on the GPIB address of the instrument.&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!autoid off')
&lt;/code&gt;&lt;p&gt;This setting is stored in eeprom = non volatile memory, so will survive a power cycle&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!autoid on')
&lt;/code&gt;&lt;p&gt;This setting is stored in eeprom = non volatile memory, so will survive a power cycle&lt;/p&gt;&lt;p&gt;Some instruments need after turn on some seconds before GPIB is responsive.&lt;/p&gt;&lt;p&gt;With below 3 settings you can set a delay which is applied before the instrument ID is queried after power on. Note that it will take then also more time, before the USB device is recognized by the PC.&lt;/p&gt;&lt;p&gt;Also this setting is non-volatile.&lt;/p&gt;&lt;p&gt;Delay 5 seconds:&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!autoid slow')
&lt;/code&gt;&lt;p&gt;Delay 15 seconds:&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!autoid slower')
&lt;/code&gt;&lt;p&gt;Delay 30 seconds:&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!autoid slowest')
&lt;/code&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # 
print(dev.query('!autoid?'))
&lt;/code&gt;&lt;p&gt;Returns as text string either: "off", "on", "slow", "slower" or "slowest".&lt;/p&gt;&lt;p&gt;Finally I implemented a command to query the USB adapters firmware version :-)&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
print(dev.query('!ver?'))
&lt;/code&gt;&lt;p&gt;A user discovered that Matlab has a limitation in the VISA ressource string length and shared a pull request to reduce the length. I expose this now first time in the baseline firmware with the following options.&lt;/p&gt;&lt;p&gt;This setting is stored in eeprom = non volatile.&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!string short')
&lt;/code&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
dev.write('!string normal')
&lt;/code&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); # USBTMC pulse indicator request (enables internal command processing)
print(dev.query('!string?'))
&lt;/code&gt;&lt;p&gt;This returns as text string either "normal" or "short".&lt;/p&gt;&lt;code&gt;dev.control_in(0xa1, 0x40, 0, 0, 1); 
dev.write('!reset')
&lt;/code&gt;&lt;p&gt;Do a reset of the adapter. Note that due to the reset you have to close the visa session and start a new one as the device re-enumerates on the USB.&lt;/p&gt;&lt;p&gt;Or support by buying a fully prebuilt adapter at: https://www.elecrow.com/xyphrolabs-gpibusb.html&lt;/p&gt;&lt;p&gt;In general any email, post or feedback is also very valuable, and don't feel bad if you don't use the support options :-) Feel free to contact me at Xyphro@gmail.com&lt;/p&gt;&lt;p&gt;This page is from XyphroLabs UsbGpib project: https://github.com/xyphro/UsbGpib&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46371429</guid><pubDate>Wed, 24 Dec 2025 01:21:58 +0000</pubDate></item><item><title>'Dracula's Chivito': Hubble reveals largest birthplace of planets ever observed</title><link>https://phys.org/news/2025-12-chaotic-dracula-chivito-hubble-reveals.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46371673</guid><pubDate>Wed, 24 Dec 2025 02:01:49 +0000</pubDate></item><item><title>Could lockfiles just be SBOMs?</title><link>https://nesbitt.io/2025/12/23/could-lockfiles-just-be-sboms.html</link><description>&lt;doc fingerprint="3f31283b8d60a490"&gt;
  &lt;main&gt;
    &lt;p&gt;Every package manager has its own lockfile format. Gemfile.lock, package-lock.json, yarn.lock, Cargo.lock, poetry.lock, composer.lock, go.sum. They all record roughly the same information: which packages were installed, at what versions, with what checksums, from where.&lt;/p&gt;
    &lt;p&gt;Lockfiles are SBOMs.&lt;/p&gt;
    &lt;p&gt;Meanwhile, the security world has been pushing CycloneDX and SPDX as standardized formats for describing software components. Lockfiles do the same job, just in bespoke formats. Adoption in open source projects remains low, but that’s changing: the EU’s Cyber Resilience Act will push vendors toward providing SBOMs, and that pressure will flow upstream. The typical workflow involves generating an SBOM from a lockfile, which means running a tool like Syft or Trivy to convert one format to another. This conversion is sometimes lossy.&lt;/p&gt;
    &lt;p&gt;What if we cut out the middle step? What if package managers wrote SBOMs directly as their lockfile format? The short answer is: yes, mostly, with some sharp edges. I wanted to map out exactly where the gaps are.&lt;/p&gt;
    &lt;head rend="h2"&gt;What lockfiles record&lt;/head&gt;
    &lt;p&gt;Looking across the major package managers, lockfiles generally contain:&lt;/p&gt;
    &lt;p&gt;Package identity: Name, version, and where it came from. npm records a resolved URL. Bundler records the registry and gem source. Cargo uses a source field. Go uses module paths.&lt;/p&gt;
    &lt;p&gt;Integrity: Some form of checksum. npm uses SHA-512 integrity hashes. Cargo stores checksums. Go puts SHA-256 hashes in go.sum. Bundler historically didn’t include checksums in Gemfile.lock, though newer versions do.&lt;/p&gt;
    &lt;p&gt;Dependencies: The relationship between packages. Most lockfiles record which packages depend on which, either inline (Bundler lists dependencies under each gem) or as a separate structure (npm’s packages object, Cargo’s dependencies array).&lt;/p&gt;
    &lt;p&gt;Scope: Whether something is a dev dependency or a production one. npm marks this with dev/optional flags. Bundler separates groups in the Gemfile but flattens them in the lockfile. Poetry distinguishes packages from packages-dev.&lt;/p&gt;
    &lt;p&gt;Metadata: Tool versions, platform constraints, runtime versions. Bundler records BUNDLED WITH and RUBY VERSION. npm stores lockfileVersion. Cargo has a format version. These ensure the right tool interprets the file correctly.&lt;/p&gt;
    &lt;p&gt;Here’s how the major lockfile formats compare (you can find examples of each format if you want to dig deeper):&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="8"&gt;
        &lt;cell role="head"&gt;Field&lt;/cell&gt;
        &lt;cell role="head"&gt;Gemfile.lock&lt;/cell&gt;
        &lt;cell role="head"&gt;package-lock.json&lt;/cell&gt;
        &lt;cell role="head"&gt;yarn.lock&lt;/cell&gt;
        &lt;cell role="head"&gt;Cargo.lock&lt;/cell&gt;
        &lt;cell role="head"&gt;poetry.lock&lt;/cell&gt;
        &lt;cell role="head"&gt;composer.lock&lt;/cell&gt;
        &lt;cell role="head"&gt;go.sum&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Package name&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Version&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Checksum&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Source URL&lt;/cell&gt;
        &lt;cell&gt;registry1&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Dependencies&lt;/cell&gt;
        &lt;cell&gt;inline2&lt;/cell&gt;
        &lt;cell&gt;nested3&lt;/cell&gt;
        &lt;cell&gt;inline2&lt;/cell&gt;
        &lt;cell&gt;list4&lt;/cell&gt;
        &lt;cell&gt;table5&lt;/cell&gt;
        &lt;cell&gt;nested3&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Dev/prod scope&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Platform variants&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="8"&gt;
        &lt;cell&gt;Tool version&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Runtime version&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;yes&lt;/cell&gt;
        &lt;cell&gt;no&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The formats differ in structure but the core data is similar. The interesting variations are in the metadata: Bundler cares about the Ruby runtime and platforms because gems can have native extensions. npm tracks dev dependencies because it matters for production installs. Go’s go.sum is a bit of an outlier: it’s purely an integrity file (checksums only), not a resolution record. The actual version selection lives in go.mod. This weakens the “lockfiles are SBOMs” claim, but an integrity-only SBOM is still an SBOM, just an incomplete one. The pattern holds for most ecosystems.&lt;/p&gt;
    &lt;head rend="h2"&gt;What CycloneDX provides&lt;/head&gt;
    &lt;p&gt;CycloneDX is designed for software bills of materials, but its data model maps reasonably well to lockfile concepts. It’s now an ECMA standard (ECMA-424), and package URL (purl) is also standardized as ECMA-427.&lt;/p&gt;
    &lt;p&gt;For each component, you can record:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;name, version, and group&lt;/item&gt;
      &lt;item&gt;purl (package URL), which encodes type, namespace, name, version, and optionally a &lt;code&gt;repository_url&lt;/code&gt;qualifier for internal or third-party registries&lt;/item&gt;
      &lt;item&gt;hashes (MD5, SHA-1, SHA-256, SHA-512, and others)&lt;/item&gt;
      &lt;item&gt;externalReferences for source URLs and documentation&lt;/item&gt;
      &lt;item&gt;scope (required, optional, excluded)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For relationships:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;A dependencies array links components by their bom-ref&lt;/item&gt;
      &lt;item&gt;Each entry lists what a component depends on&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;For metadata:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;tools records what generated the BOM&lt;/item&gt;
      &lt;item&gt;properties allow arbitrary key-value pairs&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That properties mechanism is both the strength and the weakness. CycloneDX explicitly supports extension through namespaced properties. A package manager could store its platform constraints, runtime version requirements, and other metadata there. But once everything important lives in properties, you’ve effectively reinvented a bespoke format inside CycloneDX. Generic tooling won’t understand it. This is already happening: different SBOM generators use different property conventions, and consumers have to know which tool produced the file to interpret it correctly.&lt;/p&gt;
    &lt;head rend="h2"&gt;A compatibility table&lt;/head&gt;
    &lt;p&gt;Here’s how lockfile fields could map to CycloneDX’s component model:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Lockfile field&lt;/cell&gt;
        &lt;cell role="head"&gt;CycloneDX equivalent&lt;/cell&gt;
        &lt;cell role="head"&gt;Notes&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Package name&lt;/cell&gt;
        &lt;cell&gt;component.name&lt;/cell&gt;
        &lt;cell&gt;Direct mapping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Version&lt;/cell&gt;
        &lt;cell&gt;component.version&lt;/cell&gt;
        &lt;cell&gt;Direct mapping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Checksum&lt;/cell&gt;
        &lt;cell&gt;component.hashes&lt;/cell&gt;
        &lt;cell&gt;Multiple algorithms supported&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Source URL&lt;/cell&gt;
        &lt;cell&gt;purl + &lt;code&gt;repository_url&lt;/code&gt; qualifier&lt;/cell&gt;
        &lt;cell&gt;Handles internal/third-party registries&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dependencies&lt;/cell&gt;
        &lt;cell&gt;dependencies array&lt;/cell&gt;
        &lt;cell&gt;Uses bom-ref&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Dev scope&lt;/cell&gt;
        &lt;cell&gt;component.scope = “optional”&lt;/cell&gt;
        &lt;cell&gt;Not a perfect fit&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Platform constraints&lt;/cell&gt;
        &lt;cell&gt;component.properties&lt;/cell&gt;
        &lt;cell&gt;Custom namespace needed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Tool version&lt;/cell&gt;
        &lt;cell&gt;metadata.tools&lt;/cell&gt;
        &lt;cell&gt;Direct mapping&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Runtime version&lt;/cell&gt;
        &lt;cell&gt;metadata.properties&lt;/cell&gt;
        &lt;cell&gt;Custom namespace needed&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Platform-specific variants&lt;/cell&gt;
        &lt;cell&gt;purl qualifiers (&lt;code&gt;arch&lt;/code&gt;, &lt;code&gt;os&lt;/code&gt;)&lt;/cell&gt;
        &lt;cell&gt;Each variant is a separate component&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Most fields have reasonable mappings. The gaps:&lt;/p&gt;
    &lt;p&gt;Dev vs production: CycloneDX’s scope field has three values: required, optional, and excluded. This doesn’t cleanly map to npm’s dev/devOptional/optional/peer distinctions. The mismatch isn’t accidental: SBOM scope is consumer-centric (what does the end user need?), while lockfile scope encodes resolver semantics (how should I install this?). You could use properties, but then tooling needs to understand your custom namespace.&lt;/p&gt;
    &lt;p&gt;Platform-specific packages: Bundler handles gems like ffi that have different builds for different platforms (ffi-1.17.2-arm64-darwin vs ffi-1.17.2-x86_64-linux-gnu). purl qualifiers can encode this (&lt;code&gt;pkg:gem/[email protected]?arch=arm64&amp;amp;os=darwin&lt;/code&gt;), though each variant becomes a separate component rather than a single entry with multiple platforms.&lt;/p&gt;
    &lt;p&gt;Peer dependencies: npm’s peer dependency concept has no direct equivalent. A package declaring a peer dependency expects the parent to provide it. CycloneDX’s dependency graph is simpler.&lt;/p&gt;
    &lt;p&gt;Direct vs transitive: Some lockfiles distinguish what you asked for (Gemfile) from what got pulled in transitively. CycloneDX can represent this through the dependency graph but doesn’t have an explicit flag. This matters more than it sounds: policy engines often treat direct and transitive dependencies differently for licensing or vulnerability remediation. It’s a philosophical gap, not just a missing field.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we’d gain&lt;/head&gt;
    &lt;p&gt;If package managers adopted a standard lockfile format:&lt;/p&gt;
    &lt;p&gt;No conversion step. Security scanners could read lockfiles directly without ecosystem-specific parsers. Vulnerability databases already index by purl; a purl-native lockfile would be immediately queryable.&lt;/p&gt;
    &lt;p&gt;Cross-ecosystem tooling. Dependency graph analysis, license compliance, and supply chain tools could work the same way across languages. Today each tool needs to understand Gemfile.lock, package-lock.json, Cargo.lock, and a dozen others.&lt;/p&gt;
    &lt;p&gt;Better interoperability. Multi-language projects wouldn’t need multiple tools to get a complete picture. A monorepo with Ruby, JavaScript, and Rust could have lockfiles in the same format.&lt;/p&gt;
    &lt;p&gt;First-class SBOMs. Projects would ship SBOMs by default because the lockfile is the SBOM. No extra generation step, no drift between what’s installed and what’s documented.&lt;/p&gt;
    &lt;head rend="h2"&gt;What we’d lose&lt;/head&gt;
    &lt;p&gt;Human readability. Gemfile.lock and Cargo.lock are reasonably readable. CycloneDX supports JSON, XML, and YAML, but even the YAML format is more verbose than purpose-built lockfiles. You could tune the output, but it would never be as scannable as a format designed for the task.&lt;/p&gt;
    &lt;p&gt;Machine diffability. This is distinct from human readability. Many lockfile formats are deliberately designed to minimize merge conflicts. Cargo.lock and yarn.lock sort entries deterministically. Line-based formats diff cleanly. Some package managers even structure their lockfiles so that adding a dependency only touches one section. CycloneDX in any format would produce noisier diffs. Adding one dependency in package-lock.json might touch a handful of lines; in CycloneDX it could expand into dozens of lines, guaranteeing a messy diff. CycloneDX YAML would be friendlier than JSON for this, but it’s still more verbose than purpose-built formats. This might be the biggest practical blocker. Developers hit lockfile conflicts constantly, and the pain of resolving them could kill adoption before any other benefits materialize.&lt;/p&gt;
    &lt;p&gt;Ecosystem-specific semantics. Each package manager has evolved its lockfile format to handle specific needs: Bundler’s platform handling, npm’s peer dependencies, Poetry’s extras. CycloneDX properties could store all of this, but generic SBOM tooling wouldn’t understand the semantics. A vulnerability scanner could read the components, but wouldn’t know how to interpret npm’s peer dependency rules or Bundler’s platform resolution.&lt;/p&gt;
    &lt;p&gt;Intentional incompleteness. Some lockfile splits are deliberate. Go separates go.mod (requirements) from go.sum (checksums) because they serve different purposes and change at different times. A unified format might force awkward decisions about what belongs together.&lt;/p&gt;
    &lt;p&gt;Migration cost. Every package manager would need to support reading and writing a new format. Every CI pipeline, every deployment script, every lockfile parser would need updates. The ecosystem has a lot of inertia.&lt;/p&gt;
    &lt;head rend="h2"&gt;We’re already halfway there&lt;/head&gt;
    &lt;p&gt;Many package managers already generate SBOMs. npm has &lt;code&gt;npm sbom&lt;/code&gt; built in. Cargo has cargo-sbom. Python has cyclonedx-bom. Ruby has cyclonedx-ruby and bundler-sbom. Go, PHP, .NET all have tools. The machinery exists.&lt;/p&gt;
    &lt;p&gt;These tools read lockfiles and output CycloneDX or SPDX. The reverse operation (reading an SBOM and using it for installation) is the missing piece. But if a package manager can generate a complete SBOM from a lockfile, in theory it contains enough information to reverse the process.&lt;/p&gt;
    &lt;p&gt;A gradual path forward:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;&lt;p&gt;Package managers that already have&lt;/p&gt;&lt;code&gt;sbom&lt;/code&gt;commands could add an experimental flag:&lt;code&gt;--lockfile-format=cyclonedx&lt;/code&gt;. Write the lockfile as an SBOM. Read it back the same way.&lt;/item&gt;
      &lt;item&gt;&lt;p&gt;Standardize a “lockfile profile” within CycloneDX. This is the most important step. Without it, CycloneDX-as-lockfile is a dead end. Define exactly how package managers should use properties for runtime versions, platforms, and scope distinctions. CycloneDX has a property taxonomy for registering namespaces. Something like&lt;/p&gt;&lt;code&gt;cdx:lockfile:direct&lt;/code&gt;or&lt;code&gt;cdx:lockfile:runtime-version&lt;/code&gt;would need to land there. Otherwise every package manager invents its own conventions and we get the same fragmentation problem inside CycloneDX that we have outside it.&lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Let projects opt in. If your tooling works with CycloneDX and you don’t need platform-specific edge cases, use it. Keep the native format as fallback.&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Python ecosystem is trying something related with PEP 751, which proposes a standardized pylock.toml format. It’s not CycloneDX, but it addresses the same fragmentation problem (Poetry, PDM, pip-tools, and uv all have different lockfile formats).&lt;/p&gt;
    &lt;p&gt;This is where the question shifts from formats to trust. SBOMit takes a different approach entirely. Rather than scanning lockfiles after the fact, it uses Witness to capture cryptographically signed attestations during each step of the build process: version control, dependency resolution, testing, packaging. The SBOM becomes a verified record of what actually happened, not a best-effort reconstruction from whatever files are lying around.&lt;/p&gt;
    &lt;p&gt;Package managers could do the same thing. During &lt;code&gt;bundle install&lt;/code&gt; or &lt;code&gt;npm install&lt;/code&gt;, the resolver already knows exactly which packages it fetched, from where, with what checksums. It could emit attestations as it goes. Combined with Sigstore for artifact signing and trusted publishing for verifying upload provenance, the lockfile becomes not just a list of versions, but a cryptographically verifiable record of the entire dependency graph.&lt;/p&gt;
    &lt;head rend="h2"&gt;The actual point&lt;/head&gt;
    &lt;p&gt;The exercise of mapping lockfiles to CycloneDX reveals something interesting: these formats are more similar than they look. Strip away the syntax differences and you have packages, versions, checksums, sources, and dependencies. The variations are mostly in metadata and edge cases. The conversion tools exist because we built two systems for the same purpose.&lt;/p&gt;
    &lt;p&gt;Whether unification happens doesn’t really matter. What matters is recognizing that lockfiles are software supply chain artifacts. They deserve the same attention we give to SBOMs. The security properties we want from SBOMs (integrity, provenance, completeness) are the same properties we want from lockfiles.&lt;/p&gt;
    &lt;p&gt;If you maintain a package manager, consider what it would take to output CycloneDX. If you work on SBOM tooling, consider what lockfile features you’re not capturing. The gap between these worlds is smaller than it appears.&lt;/p&gt;
    &lt;p&gt;There’s also a bigger problem neither lockfiles nor SBOMs currently solve well: system dependencies. Python wheels bundle compiled C libraries. Ruby gems link against libxml2 or openssl. These phantom dependencies are invisible to both lockfiles and most SBOM generators. PEP 770 proposes embedding SBOM documents inside Python packages to capture what’s actually bundled. That’s a step toward complete software composition, but it highlights how much is still missing from the picture.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46371705</guid><pubDate>Wed, 24 Dec 2025 02:10:34 +0000</pubDate></item><item><title>Show HN: Turn raw HTML into production-ready images for free</title><link>https://html2png.dev</link><description>&lt;doc fingerprint="8f284f53b0f2fff2"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Convert HTML to PNG.&lt;/head&gt;
    &lt;p&gt;The image generation API your LLM will love to use. Turn raw HTML into production-ready images for free. No sign up required.&lt;/p&gt;
    &lt;p&gt;HTML EDITOR0 characters&lt;/p&gt;
    &lt;p&gt; SETTINGS &lt;/p&gt;
    &lt;p&gt;EXAMPLES&lt;/p&gt;
    &lt;p&gt;SIZE &amp;amp; FORMAT&lt;/p&gt;
    &lt;p&gt;W&lt;/p&gt;
    &lt;p&gt;H&lt;/p&gt;
    &lt;p&gt;S&lt;/p&gt;
    &lt;p&gt;OPTIONS&lt;/p&gt;
    &lt;p&gt;D&lt;/p&gt;
    &lt;p&gt;Z&lt;/p&gt;
    &lt;p&gt;DEVELOPER API&lt;/p&gt;
    &lt;p&gt; HTML PREVIEW1200×630 PNG&lt;/p&gt;
    &lt;head rend="h2"&gt;Vibe-Ready Endpoint.&lt;/head&gt;
    &lt;p&gt;Post https://html2png.dev&lt;/p&gt;
    &lt;p&gt; The Request&lt;/p&gt;
    &lt;p&gt; Parameters&lt;/p&gt;
    &lt;p&gt;str Raw HTML string.&lt;/p&gt;
    &lt;p&gt;int Output width.&lt;/p&gt;
    &lt;p&gt;int Output height.&lt;/p&gt;
    &lt;p&gt;str png | jpeg | webp | pdf&lt;/p&gt;
    &lt;p&gt;int Retina scaling (1-4).&lt;/p&gt;
    &lt;p&gt;int Wait time in ms.&lt;/p&gt;
    &lt;p&gt;num Viewport zoom (0.1-3.0).&lt;/p&gt;
    &lt;p&gt;bool Transparent background.&lt;/p&gt;
    &lt;p&gt; The Response&lt;/p&gt;
    &lt;p&gt; Fields&lt;/p&gt;
    &lt;code&gt;url&lt;/code&gt;
    &lt;p&gt;Public path to your generated asset. Immutable.&lt;/p&gt;
    &lt;code&gt;filename&lt;/code&gt;
    &lt;p&gt;Unique identifier generated for the file.&lt;/p&gt;
    &lt;code&gt;success&lt;/code&gt;
    &lt;p&gt;Boolean status of the render operation.&lt;/p&gt;
    &lt;p&gt;Beyond MCP&lt;/p&gt;
    &lt;head rend="h2"&gt; Not everything &lt;lb/&gt;needs an MCP.&lt;/head&gt;
    &lt;p&gt;Stop waiting for MCP server updates or proxy configurations. Your LLM agents are already capable of making HTTP requests. Give them the instructions, and let them render directly to the edge.&lt;/p&gt;
    &lt;p&gt;01&lt;/p&gt;
    &lt;head rend="h4"&gt;Zero Setup&lt;/head&gt;
    &lt;p&gt;No plugins, no servers, no local tunnels.&lt;/p&gt;
    &lt;p&gt;02&lt;/p&gt;
    &lt;head rend="h4"&gt;Agent Native&lt;/head&gt;
    &lt;p&gt;Works with Claude, GPT-5, and any tool-capable AI.&lt;/p&gt;
    &lt;p&gt;A Single Prompt&lt;/p&gt;
    &lt;p&gt; Paste into your Agent: &lt;/p&gt;
    &lt;p&gt; Prompt &amp;amp; Vibe &lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46371743</guid><pubDate>Wed, 24 Dec 2025 02:18:38 +0000</pubDate></item><item><title>Correspondence Between Don Knuth and Peter van Emde Boas on Priority Deques 1977 [pdf]</title><link>https://staff.fnwi.uva.nl/p.vanemdeboas/knuthnote.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46371759</guid><pubDate>Wed, 24 Dec 2025 02:21:42 +0000</pubDate></item></channel></rss>