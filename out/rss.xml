<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>Hacker News: Front Page</title><link>https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml</link><description>Hacker News RSS</description><atom:link href="https://raw.githubusercontent.com/Prabesh01/hnrss-content-extract/refs/heads/main/out/rss.xml" rel="self"/><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><language>en</language><lastBuildDate>Wed, 07 Jan 2026 15:45:22 +0000</lastBuildDate><item><title>A 30B Qwen model walks into a Raspberry Pi and runs in real time</title><link>https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/</link><description>&lt;doc fingerprint="3edb28de9d388188"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt; A 30B Qwen Model Walks Into a Raspberry Pi√¢¬¶&lt;lb/&gt; and Runs in Real Time &lt;/head&gt;
    &lt;p&gt;For this release, we optimize for what people actually experience when they run a model: fast, high-quality responses on a specific target device.&lt;/p&gt;
    &lt;p&gt;We use Shapelearn, our bitlength learning method to choose weight datatypes for Qwen3-30B-A3B-Instruct-2507 that maximize performance in terms of tokens per second (TPS) and output quality, with one practical constraint: the model must fit comfortably in the available memory. Once it fits, making the file smaller isn't a goal by itself. We only shrink further when it also improves the real tradeoff people care about: speed vs. quality.&lt;/p&gt;
    &lt;p&gt;Approaching bitlength learning this way matters because in llama.cpp, "fewer bits" doesn't automatically mean "more speed." Different quantization formats can trigger different kernels and overheads, and on some GPUs, going lower-bit can even get slower, despite using less memory.&lt;/p&gt;
    &lt;p&gt;Bottom line: treat memory as a budget to meet, then optimize what matters most: TPS and quality.&lt;/p&gt;
    &lt;head rend="h2"&gt;TL;DR&lt;/head&gt;
    &lt;p&gt; Yes, this 30B Qwen3 runs on a Raspberry Pi. On a Pi 5 (16GB), &lt;code&gt;
              
                Q3_K_S-2.70bpw [KQ-2]
              
            &lt;/code&gt;
            hits 8.03 TPS at 2.70 BPW and maintains 94.18% of BF16 quality. It genuinely feels
            real-time. More broadly, the same pattern shows up everywhere else: ByteShape models
            give you a better TPS/quality tradeoff than the alternatives (here we look at Unsloth
            and MagicQuant).
          &lt;/p&gt;
    &lt;head rend="h2"&gt;CPUs&lt;/head&gt;
    &lt;p&gt;On CPUs, the reducing footprint via shorter bitlengths affects the TPS and accuracy tradeoff as one would expect: once the model fits, reducing footprint tends to increase TPS in a fairly monotonic way. If datatypes are selected correctly, you can trade a bit of quality for speed predictably, which makes it much easier to pick a point on the curve that matches your constraints.&lt;/p&gt;
    &lt;p&gt;We'll start with the most memory-constrained CPU case (Raspberry Pi 5 16GB), where "fits in RAM" is the limiting factor, then move to an Intel i7 with 64GB, where everything fits.&lt;/p&gt;
    &lt;head rend="h3"&gt;Raspberry Pi 5&lt;/head&gt;
    &lt;p&gt;The figure below shows TPS vs. normalized accuracy for the models that fit in RAM on the Raspberry Pi 5 16GB.&lt;/p&gt;
    &lt;p&gt;Notably, sustaining 8.5 TPS at 92%+ baseline accuracy with a 30B model on a Raspberry Pi reshapes expectations for Pi-class systems. Overall, the trend shows that ShapeLearn consistently produces better models, with ByteShape trending up and to the right of Unsloth, achieving higher tokens per second at the same quality, or higher quality at the same throughput.&lt;/p&gt;
    &lt;p&gt;We highlight choices for two primary objectives: accuracy or response time.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Optimizing for response time while maintaining accuracy: For interactive, on-device use, perceived responsiveness is driven by how quickly text appears, not peak throughput. In practice, generation feels real-time once it reaches roughly 8 TPS, comfortably above typical reading speed. In this Raspberry Pi real-time regime, &lt;code&gt;Q3_K_S-2.70bpw [KQ-2]&lt;/code&gt;(2.70 BPW, 8.03 TPS, 94.18% accuracy) is our go-to recommendation: it crosses the real-time threshold while maintaining high accuracy. Compared to Unsloth models at similar quality, ByteShape achieves real-time performance at lower BPW and higher TPS, making it the more efficient choice for interactive edge deployment.&lt;/item&gt;
      &lt;item&gt; Accuracy above all: The table below lists the models that achieve the highest accuracy while still being able to run on a Raspberry Pi. Within this set, ByteShape models make the best use of the available resources to maximize accuracy, occupying the lowest-error rows (~1.1√¢1.3% relative error, ~98.8% accuracy), while the strongest Unsloth entries remain around 2.1√¢2.2% error (~97.9% accuracy). Compared to Unsloth's &lt;code&gt;UD-Q3_K_XL [8]&lt;/code&gt;, ByteShape achieves up to a 1.87√É lower error rate while still operating at ~5√¢6 TPS, comfortably within TPS-norms on Raspberry PI making it the better choice when accuracy is the priority.&lt;lb/&gt;Even when prioritizing maximum speed with some reduction in accuracy,&lt;code&gt;Q3_K_S-3.25bpw [KQ-5]&lt;/code&gt;offers a better tradeoff: more accurate, smaller, and faster than the fastest Unsloth model.&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Model&lt;/cell&gt;
        &lt;cell role="head"&gt;Relative Error&lt;/cell&gt;
        &lt;cell role="head"&gt;BPW&lt;/cell&gt;
        &lt;cell role="head"&gt;TPS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;
                    
                      Q4_K_S-3.92bpw [KQ-7]
                    
                  &lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;1.14%&lt;/cell&gt;
        &lt;cell&gt;3.92&lt;/cell&gt;
        &lt;cell&gt;5.30&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;
                    
                      Q4_K_S-3.61bpw [KQ-6]
                    
                  &lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;1.25%&lt;/cell&gt;
        &lt;cell&gt;3.61&lt;/cell&gt;
        &lt;cell&gt;5.94&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;
                    
                      Q3_K_S-3.25bpw [KQ-5]
                    
                  &lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;2.03%&lt;/cell&gt;
        &lt;cell&gt;3.25&lt;/cell&gt;
        &lt;cell&gt;6.68&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;
          &lt;code&gt;UD-IQ3_XXS [6]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;2.22%&lt;/cell&gt;
        &lt;cell&gt;3.38&lt;/cell&gt;
        &lt;cell&gt;5.03&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;UD-Q3_K_XL [8]&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;2.13%&lt;/cell&gt;
        &lt;cell&gt;3.62&lt;/cell&gt;
        &lt;cell&gt;6.28&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Many other Unsloth and MagicQuant models (some of ours too!) are not in this chart. We compare them in other sections, but they're not applicable in the Raspberry Pi case. They simply don't fit!&lt;/p&gt;
    &lt;head rend="h3"&gt;Intel i7&lt;/head&gt;
    &lt;p&gt;Next, we move to the Intel i7 with 64GB RAM. The figure below shows TPS vs normalized accuracy for all models.&lt;/p&gt;
    &lt;p&gt;Overall, ByteShape models outperform both Unsloth and MagicQuant, delivering higher quality at comparable throughput using fewer bits per parameter. Only ByteShape offers models that run in the 26+ TPS range, extending performance well beyond the other methods.&lt;/p&gt;
    &lt;p&gt;Highlights:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; Quality-first: At the high-accuracy end of the table, &lt;code&gt;IQ4_XS-4.67bpw [KQ-9]&lt;/code&gt;achieves the lowest relative error (0.25%), outperforming the best-running Unsloth models (&lt;code&gt;Q6_K [20]&lt;/code&gt;and&lt;code&gt;Q5_K_M [18]&lt;/code&gt;whose relative errors are 0.36% and 0.44%). Compared directly, ByteShape delivers up to a 1.44√É lower error rate with higher throughput than&lt;code&gt;Q6_K [20]&lt;/code&gt;, and a 1.76√É lower error rate at essentially the same speed as&lt;code&gt;Q5_K_M [18]&lt;/code&gt;. MagicQuant&lt;code&gt;mxfp4 [3]&lt;/code&gt;trails in this regime, with both higher error and lower TPS.&lt;/item&gt;
      &lt;item&gt; Balanced point: In the mid-accuracy, high-throughput region, &lt;code&gt;Q3_K_S-3.25bpw [KQ-5]&lt;/code&gt;combines ~98% accuracy with 23.1 TPS at just 3.25 BPW, offering the best overall balance in the table. Matching or exceeding this accuracy with Unsloth (&lt;code&gt;IQ4_XS [10]&lt;/code&gt;) requires higher BPW and lower TPS, while choosing an Unsloth model closer in speed (&lt;code&gt;Q3_K_S [7]&lt;/code&gt;) incurs a 1.73√É higher error rate. MagicQuant does not offer a competitive model in this range; its fastest entry (&lt;code&gt;IQ4_NL [2]&lt;/code&gt;) is behind both ByteShape and Unsloth in accuracy and throughput.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Takeaway: Across both quality-first and balanced settings, ByteShape consistently converts the available bit budget into either higher accuracy or higher TPS, and is the only approach that simultaneously covers the high-quality and 26+ TPS balanced-performance regions in this comparison.&lt;/p&gt;
    &lt;head rend="h2"&gt;GPUs: RTX5090/32GB and RTX4080/16GB&lt;/head&gt;
    &lt;p&gt;On GPUs, performance depends as much on kernel choice as on raw memory footprint. For matmul/matvec, llama.cpp's quantization-specific GPU decode paths incur very different overheads, so fewer bits per weight do not reliably translate to higher TPS. Instead, TPS often peaks at quantization-specific sweet spots. Pushing BPW lower can even increase VRAM traffic and instruction count, hurting performance rather than improving it. We dig into this behavior in more detail right after the GPU results section, where the kernel-level tradeoffs become more apparent.&lt;/p&gt;
    &lt;p&gt;We evaluate on two GPUs: an RTX 5090 (32 GB), which can run models above 4 BPW and typically reach the fastest sweet spots, and an RTX 4080 (16 GB), where &amp;gt;4 BPW models do not fit, forcing different trade-offs and making the device-optimized curve easier to see.&lt;/p&gt;
    &lt;head rend="h3"&gt;RTX 5090 (32GB of VRAM)&lt;/head&gt;
    &lt;p&gt;Let's start with the 5090, which has enough VRAM to support all of the quantized models. The figure below shows TPS vs normalized accuracy.&lt;/p&gt;
    &lt;p&gt; Two things stand out immediately:&lt;lb/&gt; First, this GPU shows a clear ~4-bit sweet spot: several ~4b models cluster at very high TPS with nearly identical quality. Examples include &lt;code&gt;Unsloth Q4_0 [12]&lt;/code&gt;, &lt;code&gt;Unsloth IQ4_XS [10]&lt;/code&gt;,
            &lt;code&gt;
              
                IQ4_XS-3.87bpw [IQ-6]
              
            &lt;/code&gt;
            , and MagicQuant &lt;code&gt;iq4_nl-EHQKOUD-IQ4NL [1]&lt;/code&gt;, all running around ~302√¢303 TPS
            at ~98.4√¢98.9% accuracy. Within 
            this tight cluster, Unsloth edges out slightly in throughput and quality.
          &lt;/p&gt;
    &lt;p&gt;Second, outside of that sweet spot, the tradeoff becomes much more uneven:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Many other Unsloth and Magic Quant models show significantly lower TPS, regardless of whether they are quantized more or less aggressively.&lt;/item&gt;
      &lt;item&gt;Past the ~4b region, only ByteShape continues to increase TPS with a more predictable reduction in quality.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt; Accuracy-critical workloads: when output quality is paramount, ByteShape delivers the most accurate model on the 5090: &lt;code&gt;
              
                IQ4_XS-4.67bpw [IQ-8]
              
            &lt;/code&gt;
            (4.67 BPW, 272.98 TPS, 99.75% accuracy). It surpasses &lt;code&gt;Unsloth Q6_K [20]&lt;/code&gt; (6.57 BPW, 264.88 TPS, 99.64% accuracy) 
            while using fewer bits and achieving slightly higher throughput, and it clearly outperforms MagicQuant 
            &lt;code&gt;mxfp4_moe-H-B16-EUR-IQ4NL-KO-Q5K-QD-Q6K [3]&lt;/code&gt; (5.46 BPW, 240.42 TPS, 99.32% accuracy) in both 
            accuracy and speed, making it the strongest choice when accuracy is a task-critical deployment requirement.
          &lt;/p&gt;
    &lt;p&gt;&lt;lb/&gt;Practical takeaway. If your GPU has enough VRAM to run a strong ~4b model that already meets your speed and accuracy requirements, that cluster is an excellent default. The curve becomes more interesting when task-critical deployment constraints demand higher accuracy or smaller models as for example, under tighter memory budgets or constrained environments (as we'll see on the 4080). &lt;/p&gt;
    &lt;head rend="h3"&gt;RTX 4080 (16GB of VRAM)&lt;/head&gt;
    &lt;p&gt;Next, let's move to a more accessible GPU, especially in these memory-challenged times. The biggest stumbling block for the 4080 is its 16GB of VRAM, which is not sufficient to support the "magical" ~4b quantizations for a 30B model. How convenient! This "avoids" the 5090's ~4b sweet spot and forces a more "real-world" comparison under a hard VRAM budget. The figure below shows TPS versus normalized accuracy for all models that fit on the 4080.&lt;/p&gt;
    &lt;p&gt;On the RTX 4080, ByteShape consistently outperforms Unsloth under the same 16 GB VRAM constraint, delivering a better TPS√¢quality tradeoff.&lt;/p&gt;
    &lt;p&gt; In particular, ByteShape's highest-quality model that fits, &lt;code&gt;
              
                IQ4_XS-3.87bpw [IQ-6]
              
            &lt;/code&gt;
            (3.87 BPW, 214.81 TPS, 98.66% accuracy) delivers:
          &lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt; a 1.59√É lower error rate and 9.4% higher TPS vs. &lt;code&gt;Unsloth Q3_K_XL [8]&lt;/code&gt;(3.62 BPW, 196.42 TPS, 97.87% accuracy).&lt;/item&gt;
      &lt;item&gt; a 2.54√É lower error rate at the same TPS vs. &lt;code&gt;Unsloth IQ2_M [2]&lt;/code&gt;(2.84 BPW, 214.79 TPS, 96.59% accuracy).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;As we move to higher throughput, ByteShape's maintains accuracy, while Unsloth's error rate experiences a cliff.&lt;/p&gt;
    &lt;head rend="h3"&gt;The Elephant in the Room: When 3-bits is not just 3-bits&lt;/head&gt;
    &lt;p&gt;There is an inconvenient truth hiding in these results. On several setups, around 4 bpw is already flying, and pushing quantization harder does not make things faster. It just manages to be smaller and slower at the same time.&lt;/p&gt;
    &lt;p&gt;Reducing the size of data doesn't automatically speed things up. While using fewer bits to store each number seems like it should reduce memory traffic and speed up computation, GPUs don't work that way. NVIDIA GPUs process work in fixed groups of 32 threads called "warps," which move through instructions together in near lock-step. The GPU hardware is optimized for specific data formats, memory access patterns, and operations that the chip's circuits are physically designed to handle efficiently. When your workload matches these "golden paths", you get peak performance. Step outside them, and you hit slowdowns. This isn't a design flaw, it's a deliberate tradeoff. Supporting more flexibility would require additional circuitry: more wires, more transistors, more complexity. That extra hardware consumes more power and adds latency to every operation, whether a program needs that flexibility or not.&lt;/p&gt;
    &lt;p&gt;Here a few examples of relevant hardware "quirks": VRAM is read in aligned 32-byte blocks, so reading one or 32 bytes consumes the same memory bandwidth. Both on-chip and off-chip memories can also suffer contention depending on how data is laid out, meaning that a warp's accesses may complete in a single step or, in the worst case, be serialized into 32 steps. And of course, decoding quantized values before computation can require extra instructions, with the cost depending on the quantization scheme.&lt;/p&gt;
    &lt;p&gt;This explains the behaviour we observe: 4-bit kernels use VRAM bandwidth more efficiently than 3- or 2-bit kernels and require fewer decode steps before computation. At the same time, 4-bit kernels exploit subword parallelism just as effectively as lower-bit kernels, and all rely primarily on dynamic caches rather than shared memory to take advantage of data reuse when possible.&lt;/p&gt;
    &lt;p&gt;So why llama.cpp hasn't been optimized to deliver peak speed for every bit-length? Our understanding is that llama.cpp prioritizes portable, space-efficient quantization that can run across a wide range of hardware. That design goal limits how aggressively backends can reshape data layouts or reorder computation in ways that might help one GPU or one bit-width.&lt;/p&gt;
    &lt;p&gt;A key example is its choice to store quantized weights in fixed blocks of 256 values. Each block is self-contained (it carries everything needed to decode it) and sits at a simple, predictable offset in the tensor, which makes the format easy to implement and fast to locate.&lt;/p&gt;
    &lt;p&gt;The tradeoff is that GPUs often need to decode many blocks in parallel to keep their wide compute units busy. With many independent 256-value blocks, those parallel decodes can translate into more scattered or fragmented VRAM reads and extra decode overhead, reducing bandwidth efficiency, especially for some lower-bit formats.&lt;/p&gt;
    &lt;p&gt; Point for example on RTX 5090: a matrix multiply [256, 768] √É [768, 2048] takes ~54√Ç¬µs with &lt;code&gt;iq4_xs&lt;/code&gt; datatype, but ~62√Ç¬µs with 
            &lt;code&gt;iq3_xxs&lt;/code&gt; (mul_mat_q()+mul_mat_q_stream_k_fixup()). In other words, 
            cutting nearly 1.2 bits per weight (a reduction of more than 25% in weight footprint) leads 
            to a ~13% slowdown, directly hurting user experience.
          &lt;/p&gt;
    &lt;p&gt;An excellent reminder that bitlength learning matters: Heuristics can get us part of the way, but not all the way. ShapeLearn makes deliberate, per-tensor datatype choices that improve speed without sacrificing accuracy.&lt;/p&gt;
    &lt;head rend="h2"&gt;Methodology (brief recap)&lt;/head&gt;
    &lt;p&gt;If you're wondering how we are scoring these points, the full methodology is discussed in our previous blog post. This post is intentionally focused on the curves and device tradeoffs, so here is the quick version.&lt;/p&gt;
    &lt;p&gt;For each quantized variant, we measure throughput (TPS) on the target device and compute a single normalized quality score relative to the BF16 baseline, using the same evaluation harness and prompts as the methodology post. The quality score aggregates standard benchmarks (MMLU, GSM8K, IFEval, LiveCodeBench V4) into one number so you can compare points directly. In other words, every dot in the plots answers two questions: how fast does it run on this device, and how much quality does it retain compared to BF16, with memory fit as the first constraint.&lt;/p&gt;
    &lt;p&gt;We also want to thank all for the many, excellent suggestions on our recent Reddit post for improving and extending this evaluation strategy, and we√¢re actively working through them. Right now, evaluation is the main bottleneck and not bitlength learning/quantization. Careful evaluation is essential to clearly communicate the strengths of each model.&lt;/p&gt;
    &lt;head rend="h2"&gt;Wrapping up&lt;/head&gt;
    &lt;p&gt;First, thank you for your tenacity. You made it through all of this without giving up. We are sincerely flattered!&lt;/p&gt;
    &lt;p&gt;The takeaway is simple: treat memory as a constraint, not a goal. Once a model fits on your device, what matters is the tradeoff curve, TPS versus quality. Across CPUs and GPUs, ByteShape consistently lands on the better side of that curve, delivering either more speed at the same quality or higher quality at the same speed.&lt;/p&gt;
    &lt;p&gt; If you're deploying on a Raspberry Pi 5 (16 GB) and want a genuinely interactive experience, start with &lt;code&gt;
              
                Q3_K_S-2.70bpw [KQ-2]
              
            &lt;/code&gt;
            . On larger CPUs or GPUs, you can move up the curve toward higher-quality points with
            little loss in throughput, the same rule applies:
            fit first, then optimize the tradeoff.
          &lt;/p&gt;
    &lt;p&gt;We'll keep releasing more device-targeted variants (and more plots). If your system can't run a 30B model smoothly, don't blame the model or the silicon. Blame the datatypes.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46518573</guid><pubDate>Tue, 06 Jan 2026 20:55:01 +0000</pubDate></item><item><title>Oral microbiome sequencing after taking probiotics</title><link>https://blog.booleanbiotech.com/oral-microbiome-biogaia</link><description>&lt;doc fingerprint="8a7c03af02b6652a"&gt;
  &lt;main&gt;
    &lt;p&gt;Recently, a friend recommended BioGaia Prodentis to me. It is a DTC oral probiotic you can buy online that is supposedly good for oral health. I thought it would be interesting to do some sequencing to see what, if anything, it did to my oral microbiome.&lt;/p&gt;
    &lt;p&gt;BioGaia Prodentis is available online for $20 or less for a month's supply&lt;/p&gt;
    &lt;head rend="h1"&gt;BioGaia&lt;/head&gt;
    &lt;p&gt;BioGaia has a fascinating story. They are a Swedish company, founded over 30 years ago, that exclusively sells probiotics DTC. They have developed multiple strains of Limosilactobacillus reuteri, mainly for gut and oral health. They apparently sell well! Their market cap is around $1B‚Äîimpressive for a consumer biotech.&lt;/p&gt;
    &lt;p&gt;Going in, I expected scant evidence for any real benefits to their probiotics, but the data (over 250 clinical studies) is much more complete than I expected.&lt;/p&gt;
    &lt;p&gt;Most notably, their gut probiotic, Protectis, seems to have a significant effect on preventing Necrotizing Enterocolitis (NEC) in premature babies. According to their website:&lt;/p&gt;
    &lt;quote&gt;
      &lt;p&gt;5-10% of the smallest premature infants develop NEC, a potentially lethal disorder in which portions of the bowel undergo tissue death.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;In March 2025, the FDA granted Breakthrough Therapy Designation to IBP-9414, an L. reuteri probiotic developed by BioGaia spinout IBT.&lt;/p&gt;
    &lt;p&gt;This is not specifically for the oral health product, but it's for sure more science than I expected to see going in.&lt;/p&gt;
    &lt;head rend="h3"&gt;Prodentis&lt;/head&gt;
    &lt;p&gt;BioGaia Prodentis contains two strains of L. reuteri: DSM 17938 and ATCC PTA 5289. The claimed benefits include fresher breath, healthier gums, and outcompeting harmful bacteria.&lt;/p&gt;
    &lt;head rend="h1"&gt;Sequencing with Plasmidsaurus&lt;/head&gt;
    &lt;p&gt;Many readers will be familiar with Plasmidsaurus. Founded in 2021, the team took a relatively simple idea: use massively multiplexed Oxford Nanopore (ONT) to offer complete plasmid sequencing with one day turnaround for $15, and scaled it. Plasmidsaurus quickly became part of biotech's core infrastructure, and spread like wildfire. It also inspired multiple copycats.&lt;/p&gt;
    &lt;p&gt;Compared to Illumina, ONT is faster and has much longer reads, but lower throughput and lower accuracy. This profile is a great fit to many sequencing problems like plasmid QC, where you only need megabases of sequences, but want an answer within 24 hours.&lt;/p&gt;
    &lt;p&gt;Over time, Plasmidsaurus has been adding services, including genome sequencing, RNA-Seq, and microbiome sequencing, all based on ONT sequencing.&lt;/p&gt;
    &lt;p&gt;Plasmidsaurus accepts many kinds of sample for microbiome sequencing&lt;/p&gt;
    &lt;p&gt;I used their 16S sequencing product, which costs $45 for ~5000 reads, plus $15 for DNA extraction. 16S sequencing is an efficient way to amplify and sequence a small amount of DNA (the ubiquitous 16S region) and be able to assign reads to specific species or even strains.&lt;/p&gt;
    &lt;p&gt;This experiment cost me $240 for four samples, and I got data back in around a week. It's very convenient that I no longer have to do my own sequencing. As a side note, you can pay more for more than 5000 reads, but unless you want information on very rare strains (&amp;lt;&amp;lt;1% frequency), this is a waste of money.&lt;/p&gt;
    &lt;p&gt;Sample collection is simple: take 100-250 ¬µL of saliva and mix with 500 ¬µL of Zymo DNA/RNA Shield (which I also had to buy for around $70.) You also need 2 mL screwtop tubes to ship in.&lt;/p&gt;
    &lt;p&gt;The reads are high quality for nanopore sequencing, with a median Q score of 23 (99%+ accuracy). This is more than sufficient accuracy for this experiment. The read length is very tightly distributed around 1500 nt (the length of a typical 16S region).&lt;/p&gt;
    &lt;p&gt;The results provided by Plasmidsaurus include taxonomy tables, per-read assignments, and some basic plots. I include a download of the results at the end of this article, as well as the FASTQ files.&lt;/p&gt;
    &lt;head rend="h1"&gt;The experiment&lt;/head&gt;
    &lt;p&gt;The main idea of the experiment was to see if any L. reuteri would colonize by the end of 30 days of probiotic use, and if so, whether it would persist beyond that. I collected four saliva samples:&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;Timing&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline A&lt;/cell&gt;
        &lt;cell&gt;Day -4&lt;/cell&gt;
        &lt;cell&gt;A few days before starting BioGaia&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline B&lt;/cell&gt;
        &lt;cell&gt;Day -1&lt;/cell&gt;
        &lt;cell&gt;The day before I started BioGaia&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Day 30&lt;/cell&gt;
        &lt;cell&gt;Day 30&lt;/cell&gt;
        &lt;cell&gt;The last day of the 30 day course&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Week after&lt;/cell&gt;
        &lt;cell&gt;Day 37&lt;/cell&gt;
        &lt;cell&gt;One week after completing the course&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Heatmap of the top 20 species. All species assignments were done by Plasmidsaurus&lt;/p&gt;
    &lt;head rend="h2"&gt;Did L. reuteri colonize?&lt;/head&gt;
    &lt;p&gt;There was no L. reuteri found in any of the samples. I did a manual analysis to check for any possible misassignments, but the closest read was only 91% identical to either L. reuteri strain.&lt;/p&gt;
    &lt;p&gt;The probiotic either (a) didn't colonize the oral cavity; (b) was present only transiently while actively taking the lozenges; (c) was below the detection threshold.&lt;/p&gt;
    &lt;p&gt;Probiotics are generally bad at colonizing, which is why you have to keep taking them. Still, I was surprised not to see a single L. reuteri read in there.&lt;/p&gt;
    &lt;head rend="h2"&gt;What actually changed?&lt;/head&gt;
    &lt;p&gt;Even though the probiotic itself didn't show up, the oral microbiome did change quite a lot.&lt;/p&gt;
    &lt;p&gt;The most striking change was a massive increase in S. salivarius. S. salivarius went from essentially absent to ~20% of my oral microbiome on the last day. However, this happened one week after I stopped taking the probiotic, so it's very unclear if it is related.&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Sample&lt;/cell&gt;
        &lt;cell role="head"&gt;S. mitis&lt;/cell&gt;
        &lt;cell role="head"&gt;S. salivarius&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline A&lt;/cell&gt;
        &lt;cell&gt;2.0%&lt;/cell&gt;
        &lt;cell&gt;0.4%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Baseline B&lt;/cell&gt;
        &lt;cell&gt;15.9%&lt;/cell&gt;
        &lt;cell&gt;0.0%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;Day 30&lt;/cell&gt;
        &lt;cell&gt;10.2%&lt;/cell&gt;
        &lt;cell&gt;0.8%&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Week after&lt;/cell&gt;
        &lt;cell&gt;1.0%&lt;/cell&gt;
        &lt;cell&gt;19.3%&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;We see S. mitis decreasing as S. salivarius increases, while the total Streptococcus fraction stayed roughly stable. It's possible one species replaced the other within the same ecological niche.&lt;/p&gt;
    &lt;p&gt;S. salivarius is itself a probiotic species. The strain BLIS K12 was isolated from a healthy New Zealand child and is sold commercially for oral health. It produces bacteriocins that kill Streptococcus pyogenes (strep throat bacteria).&lt;/p&gt;
    &lt;p&gt;At the same time, V. tobetsuensis increased in abundance from 2.1% to 5.7%. Veillonella bacteria can't eat sugar directly‚Äîthey survive by consuming lactate that Streptococcus produces. The S. salivarius bloom is plausibly feeding them.&lt;/p&gt;
    &lt;head rend="h2"&gt;Are these changes real or intra-day variation?&lt;/head&gt;
    &lt;p&gt;There was a lot more variation in species than I expected, especially comparing the two baseline samples. In retrospect, I should have taken multiple samples on the same day, and mixed them to smooth it out.&lt;/p&gt;
    &lt;p&gt;However, there is some light evidence that the variation I see is not just intra-day variation. Specifically, there are several species that stay consistent in frequency across all samples: e.g., Neisseria subflava, Streptococcus viridans, Streptococcus oralis.&lt;/p&gt;
    &lt;head rend="h1"&gt;Conclusions&lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;L. reuteri didn't detectably colonize my mouth. Oral probiotics are surprisingly difficult to detect, even if you sample the same day as dosing.&lt;/item&gt;
      &lt;item&gt;S. salivarius increased massively in abundance, but this increase happened after I stopped taking BioGaia&lt;/item&gt;
      &lt;item&gt;Microbiome sequencing can be used to assess oral health. None of the "red complex" bacteria (P. gingivalis, T. forsythia, T. denticola) associated with gum disease were found in any sample.&lt;/item&gt;
      &lt;item&gt;The oral microbiome is dynamic, with huge swings in species abundance over a timeframe of just weeks&lt;/item&gt;
      &lt;item&gt;Microbiome sequencing is very easy and convenient these days thanks to Plasmidsaurus&lt;/item&gt;
      &lt;item&gt;Prodentis tastes good, may help with oral health, and I'd consider taking it again&lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46518804</guid><pubDate>Tue, 06 Jan 2026 21:10:47 +0000</pubDate></item><item><title>Laylo (YC S20) ‚Äì Head of Growth (Organic and Partners and Loops and AI) ‚Äì Remote US</title><link>https://www.ycombinator.com/companies/laylo/jobs/ZtLHRXe-head-of-growth</link><description>&lt;doc fingerprint="b1dc8874f8cd840c"&gt;
  &lt;main&gt;
    &lt;p&gt;The CRM powering iconic musicians and events&lt;/p&gt;
    &lt;p&gt;Laylo is the Drop CRM powering iconic artists and live events. The platform combines landing pages, messaging, link tracking, and more to drive more tickets, merch and streams through Instagram DM, SMS and Email. Today, we power CRM across hundreds of millions of fans for some of the biggest names in entertainment like Sabrina Carpenter, Outside Lands and Skrillex.&lt;/p&gt;
    &lt;p&gt;Role Overview&lt;/p&gt;
    &lt;p&gt;We‚Äôre looking for a Head of Growth (player/coach) to build and run Laylo‚Äôs growth engine. A 0‚Üí1 builder who doesn‚Äôt just ideate, but ships. You‚Äôll create great content, run scrappy experiments, and build product growth loops that compound. This is a hands-on role: you‚Äôll set strategy, execute a rapid experiment cadence, measure results, and turn wins into repeatable playbooks.&lt;/p&gt;
    &lt;p&gt;You‚Äôll focus primarily on non-advertising channels: organic social, influencer/creator collaborations, channel partners, and product growth loops. We value strong taste, speed, and a rigorous learning cadence.&lt;/p&gt;
    &lt;p&gt;You‚Äôll report directly to the CEO and work closely with Product, Engineering, Design, Partnerships, and Sales. You‚Äôre likely a good fit if you‚Äôre excited to open Adobe/Figma/Notion/PostHog and ship something today.&lt;/p&gt;
    &lt;p&gt;Requirements:&lt;/p&gt;
    &lt;p&gt;You‚Äôll thrive here if you can:&lt;/p&gt;
    &lt;p&gt;Key Responsibilities:&lt;/p&gt;
    &lt;p&gt;What Success Looks Like:&lt;/p&gt;
    &lt;p&gt;What You Bring:&lt;/p&gt;
    &lt;p&gt;How To Apply:&lt;/p&gt;
    &lt;p&gt;Send us your first out-of-the-box idea for your first campaign in this role. Bonus points for mentioning other companies and campaigns you think are relevant.&lt;/p&gt;
    &lt;p&gt;Our founders met while building competing consumer startups. We launched multiple products across consumer and SaaS and talked to thousands of fans and creators in the process. In 2020, we realized one of the biggest pain points artists and events face is actually driving their audience from socials into their own CRM.&lt;/p&gt;
    &lt;p&gt;In 2020, we joined Y Combinator‚Äôs summer batch and began building a product that quickly gained strong early traction. We raised from top-tier investors like Eldridge and Sony and have since grown into a team of 24 exceptional individuals spanning product, sales, and operations.&lt;/p&gt;
    &lt;p&gt;We have a strong written documentation culture. We try to do as much as possible asynchronously to move quickly and efficiently. We have a daily 30 minute standup and team-specific meetings throughout the week.&lt;/p&gt;
    &lt;p&gt;Creators and Brands have a few key moments that drive the majority of their sales and fan engagement, we call them drops. At Laylo, we're building the Drop CRM to make these moments perfect.&lt;/p&gt;
    &lt;p&gt;With Laylo, creators and brands can notify fans the second they drop new content, merch and events. From there, they get a full featured CRM, a dashboard to connect with fans forever in the future, high conversion landing pages and deep analytics to conversions, click throughs and sales.&lt;/p&gt;
    &lt;p&gt;We work with some of biggest creators, brands, records labels and managers in the world to create incredible drop experiences.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46519303</guid><pubDate>Tue, 06 Jan 2026 21:44:37 +0000</pubDate></item><item><title>Show HN: SMTP Tunnel ‚Äì A SOCKS5 proxy disguised as email traffic to bypass DPI</title><link>https://github.com/x011/smtp-tunnel-proxy</link><description>&lt;doc fingerprint="8cf1e0524c113892"&gt;
  &lt;main&gt;
    &lt;quote&gt;
      &lt;p&gt;A high-speed covert tunnel that disguises TCP traffic as SMTP email communication to bypass Deep Packet Inspection (DPI) firewalls.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;code&gt;‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Application ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Client    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Server    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Internet    ‚îÇ
‚îÇ  (Browser)  ‚îÇ TCP  ‚îÇ SOCKS5:1080 ‚îÇ SMTP ‚îÇ  Port 587   ‚îÇ TCP  ‚îÇ              ‚îÇ
‚îÇ             ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ             ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ             ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ                    ‚îÇ
                            ‚îÇ   Looks like       ‚îÇ
                            ‚îÇ   Email Traffic    ‚îÇ
                            ‚ñº                    ‚ñº
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ     DPI Firewall               ‚îÇ
                     ‚îÇ  ‚úÖ Sees: Normal SMTP Session  ‚îÇ
                     ‚îÇ  ‚ùå Cannot see: Tunnel Data    ‚îÇ
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Feature&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üîí TLS Encryption&lt;/cell&gt;
        &lt;cell&gt;All traffic encrypted with TLS 1.2+ after STARTTLS&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üé≠ DPI Evasion&lt;/cell&gt;
        &lt;cell&gt;Initial handshake mimics real SMTP servers (Postfix)&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;‚ö° High Speed&lt;/cell&gt;
        &lt;cell&gt;Binary streaming protocol after handshake - minimal overhead&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üë• Multi-User&lt;/cell&gt;
        &lt;cell&gt;Per-user secrets, IP whitelists, and logging settings&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üîë Authentication&lt;/cell&gt;
        &lt;cell&gt;Per-user pre-shared keys with HMAC-SHA256&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üåê SOCKS5 Proxy&lt;/cell&gt;
        &lt;cell&gt;Standard proxy interface - works with any application&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üì° Multiplexing&lt;/cell&gt;
        &lt;cell&gt;Multiple connections over single tunnel&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üõ°Ô∏è IP Whitelist&lt;/cell&gt;
        &lt;cell&gt;Per-user access control by IP address/CIDR&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üì¶ Easy Install&lt;/cell&gt;
        &lt;cell&gt;One-liner server installation with systemd service&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üéÅ Client Packages&lt;/cell&gt;
        &lt;cell&gt;Auto-generated ZIP files for each user&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;üîÑ Auto-Reconnect&lt;/cell&gt;
        &lt;cell&gt;Client automatically reconnects on connection loss&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;quote&gt;
      &lt;p&gt;üìö For in-depth technical details, protocol specifications, and security analysis, see TECHNICAL.md.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Server: Linux VPS with Python 3.8+, port 587 open&lt;/item&gt;
      &lt;item&gt;Client: Windows/macOS/Linux with Python 3.8+&lt;/item&gt;
      &lt;item&gt;Domain name: Required for TLS certificate verification (free options: DuckDNS, No-IP, FreeDNS)&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Get a free domain pointing to your VPS:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;ü¶Ü DuckDNS - Recommended, simple and free&lt;/item&gt;
      &lt;item&gt;üåê No-IP - Free tier available&lt;/item&gt;
      &lt;item&gt;üÜì FreeDNS - Many domain options&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Example: &lt;code&gt;myserver.duckdns.org&lt;/code&gt; ‚Üí &lt;code&gt;203.0.113.50&lt;/code&gt; (your VPS IP)&lt;/p&gt;
    &lt;code&gt;curl -sSL https://raw.githubusercontent.com/x011/smtp-tunnel-proxy/main/install.sh | sudo bash&lt;/code&gt;
    &lt;p&gt;The installer will:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;üì• Download and install everything&lt;/item&gt;
      &lt;item&gt;‚ùì Ask for your domain name&lt;/item&gt;
      &lt;item&gt;üîê Generate TLS certificates automatically&lt;/item&gt;
      &lt;item&gt;üë§ Offer to create your first user&lt;/item&gt;
      &lt;item&gt;üî• Configure firewall&lt;/item&gt;
      &lt;item&gt;üöÄ Start the service&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;That's it! Your server is ready.&lt;/p&gt;
    &lt;code&gt;smtp-tunnel-adduser bob      # Add user + generate client ZIP
smtp-tunnel-listusers        # List all users
smtp-tunnel-deluser bob      # Remove a user&lt;/code&gt;
    &lt;code&gt;smtp-tunnel-update           # Updates code, preserves config/certs/users&lt;/code&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Get your &lt;code&gt;username.zip&lt;/code&gt;file from the server admin&lt;/item&gt;
      &lt;item&gt;Extract the ZIP file&lt;/item&gt;
      &lt;item&gt;Run the launcher:&lt;/item&gt;
    &lt;/list&gt;
    &lt;table&gt;
      &lt;row span="2"&gt;
        &lt;cell role="head"&gt;Platform&lt;/cell&gt;
        &lt;cell role="head"&gt;How to Run&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;ü™ü Windows&lt;/cell&gt;
        &lt;cell&gt;Double-click &lt;code&gt;start.bat&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="2"&gt;
        &lt;cell&gt;üêß Linux&lt;/cell&gt;
        &lt;cell&gt;Run &lt;code&gt;./start.sh&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;üçé macOS&lt;/cell&gt;
        &lt;cell&gt;Run &lt;code&gt;./start.sh&lt;/code&gt;&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;The launcher will automatically install dependencies and start the client.&lt;/p&gt;
    &lt;p&gt;‚úÖ You should see:&lt;/p&gt;
    &lt;code&gt;SMTP Tunnel Proxy Client
User: alice

[INFO] Starting SMTP Tunnel...
[INFO] SOCKS5 proxy will be available at 127.0.0.1:1080

Connecting to myserver.duckdns.org:587
Connected - binary mode active
SOCKS5 proxy on 127.0.0.1:1080
&lt;/code&gt;
    &lt;code&gt;cd alice
pip install -r requirements.txt
python client.py&lt;/code&gt;
    &lt;code&gt;# Download files
scp root@myserver.duckdns.org:/etc/smtp-tunnel/ca.crt .

# Create config.yaml:
cat &amp;gt; config.yaml &amp;lt;&amp;lt; EOF
client:
  server_host: "myserver.duckdns.org"
  server_port: 587
  socks_port: 1080
  username: "alice"
  secret: "your-secret-from-admin"
  ca_cert: "ca.crt"
EOF

# Run client
python client.py -c config.yaml&lt;/code&gt;
    &lt;p&gt;Set SOCKS5 proxy to: &lt;code&gt;127.0.0.1:1080&lt;/code&gt;&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Settings ‚Üí Network Settings ‚Üí Settings&lt;/item&gt;
      &lt;item&gt;Manual proxy configuration&lt;/item&gt;
      &lt;item&gt;SOCKS Host: &lt;code&gt;127.0.0.1&lt;/code&gt;, Port:&lt;code&gt;1080&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Select SOCKS v5&lt;/item&gt;
      &lt;item&gt;‚úÖ Check "Proxy DNS when using SOCKS v5"&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Install "Proxy SwitchyOmega" extension&lt;/item&gt;
      &lt;item&gt;Create profile with SOCKS5: &lt;code&gt;127.0.0.1:1080&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Settings ‚Üí Network &amp;amp; Internet ‚Üí Proxy ‚Üí Manual setup ‚Üí &lt;code&gt;socks=127.0.0.1:1080&lt;/code&gt;&lt;/p&gt;
    &lt;p&gt;System Preferences ‚Üí Network ‚Üí Advanced ‚Üí Proxies ‚Üí SOCKS Proxy ‚Üí &lt;code&gt;127.0.0.1:1080&lt;/code&gt;&lt;/p&gt;
    &lt;code&gt;export ALL_PROXY=socks5://127.0.0.1:1080&lt;/code&gt;
    &lt;code&gt;# curl
curl -x socks5h://127.0.0.1:1080 https://ifconfig.me

# git
git config --global http.proxy socks5://127.0.0.1:1080

# Environment variable
export ALL_PROXY=socks5://127.0.0.1:1080&lt;/code&gt;
    &lt;code&gt;# Should show your VPS IP
curl -x socks5://127.0.0.1:1080 https://ifconfig.me&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;host&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Listen interface&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;0.0.0.0&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;port&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Listen port&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;587&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;hostname&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;SMTP hostname (must match certificate)&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;mail.example.com&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;cert_file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;TLS certificate path&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;server.crt&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;key_file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;TLS private key path&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;server.key&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;users_file&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Path to users configuration&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;users.yaml&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;log_users&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Global logging setting&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;true&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;Each user can have individual settings:&lt;/p&gt;
    &lt;code&gt;users:
  alice:
    secret: "auto-generated-secret"
    # whitelist:              # Optional: restrict to specific IPs
    #   - "192.168.1.100"
    #   - "10.0.0.0/8"        # CIDR notation supported
    # logging: true           # Optional: disable to stop logging this user

  bob:
    secret: "another-secret"
    whitelist:
      - "203.0.113.50"        # Bob can only connect from this IP
    logging: false            # Don't log Bob's activity&lt;/code&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;secret&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;User's authentication secret&lt;/cell&gt;
        &lt;cell&gt;Required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;whitelist&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Allowed IPs for this user (CIDR supported)&lt;/cell&gt;
        &lt;cell&gt;All IPs&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;logging&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Enable activity logging for this user&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;true&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell role="head"&gt;Option&lt;/cell&gt;
        &lt;cell role="head"&gt;Description&lt;/cell&gt;
        &lt;cell role="head"&gt;Default&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;server_host&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Server domain name&lt;/cell&gt;
        &lt;cell&gt;Required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;server_port&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Server port&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;587&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;socks_port&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Local SOCKS5 port&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;1080&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;socks_host&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Local SOCKS5 interface&lt;/cell&gt;
        &lt;cell&gt;
          &lt;code&gt;127.0.0.1&lt;/code&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;username&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Your username&lt;/cell&gt;
        &lt;cell&gt;Required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;code&gt;secret&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;Your authentication secret&lt;/cell&gt;
        &lt;cell&gt;Required&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;
          &lt;code&gt;ca_cert&lt;/code&gt;
        &lt;/cell&gt;
        &lt;cell&gt;CA certificate for verification&lt;/cell&gt;
        &lt;cell&gt;Recommended&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;code&gt;# Check status
sudo systemctl status smtp-tunnel

# Restart after config changes
sudo systemctl restart smtp-tunnel

# View logs
sudo journalctl -u smtp-tunnel -n 100

# Uninstall
sudo /opt/smtp-tunnel/uninstall.sh&lt;/code&gt;
    &lt;code&gt;python server.py [-c CONFIG] [-d]

  -c, --config    Config file (default: config.yaml)
  -d, --debug     Enable debug logging&lt;/code&gt;
    &lt;code&gt;python client.py [-c CONFIG] [--server HOST] [--server-port PORT]
                 [-p SOCKS_PORT] [-u USERNAME] [-s SECRET] [--ca-cert FILE] [-d]

  -c, --config      Config file (default: config.yaml)
  --server          Override server domain
  --server-port     Override server port
  -p, --socks-port  Override local SOCKS port
  -u, --username    Your username
  -s, --secret      Override secret
  --ca-cert         CA certificate path
  -d, --debug       Enable debug logging&lt;/code&gt;
    &lt;code&gt;smtp-tunnel-adduser &amp;lt;username&amp;gt; [-u USERS_FILE] [-c CONFIG] [--no-zip]
    Add a new user and generate client package

smtp-tunnel-deluser &amp;lt;username&amp;gt; [-u USERS_FILE] [-f]
    Remove a user (use -f to skip confirmation)

smtp-tunnel-listusers [-u USERS_FILE] [-v]
    List all users (use -v for detailed info)

smtp-tunnel-update
    Update server to latest version (preserves config/certs/users)&lt;/code&gt;
    &lt;code&gt;smtp_proxy/
‚îú‚îÄ‚îÄ üìÑ server.py               # Server (runs on VPS)
‚îú‚îÄ‚îÄ üìÑ client.py               # Client (runs locally)
‚îú‚îÄ‚îÄ üìÑ common.py               # Shared utilities
‚îú‚îÄ‚îÄ üìÑ generate_certs.py       # Certificate generator
‚îú‚îÄ‚îÄ üìÑ config.yaml             # Server/client configuration
‚îú‚îÄ‚îÄ üìÑ users.yaml              # User database
‚îú‚îÄ‚îÄ üìÑ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ üìÑ install.sh              # One-liner server installer
‚îú‚îÄ‚îÄ üìÑ smtp-tunnel.service     # Systemd unit file
‚îú‚îÄ‚îÄ üîß smtp-tunnel-adduser     # Add user script
‚îú‚îÄ‚îÄ üîß smtp-tunnel-deluser     # Remove user script
‚îú‚îÄ‚îÄ üîß smtp-tunnel-listusers   # List users script
‚îú‚îÄ‚îÄ üîß smtp-tunnel-update      # Update server script
‚îú‚îÄ‚îÄ üìÑ README.md               # This file
‚îî‚îÄ‚îÄ üìÑ TECHNICAL.md            # Technical documentation
&lt;/code&gt;
    &lt;code&gt;/opt/smtp-tunnel/              # Application files
/etc/smtp-tunnel/              # Configuration files
  ‚îú‚îÄ‚îÄ config.yaml
  ‚îú‚îÄ‚îÄ users.yaml
  ‚îú‚îÄ‚îÄ server.crt
  ‚îú‚îÄ‚îÄ server.key
  ‚îî‚îÄ‚îÄ ca.crt
/usr/local/bin/                # Management commands
  ‚îú‚îÄ‚îÄ smtp-tunnel-adduser
  ‚îú‚îÄ‚îÄ smtp-tunnel-deluser
  ‚îú‚îÄ‚îÄ smtp-tunnel-listusers
  ‚îî‚îÄ‚îÄ smtp-tunnel-update
&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check server is running: &lt;code&gt;systemctl status smtp-tunnel&lt;/code&gt;or&lt;code&gt;ps aux | grep server.py&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check port is open: &lt;code&gt;netstat -tlnp | grep 587&lt;/code&gt;&lt;/item&gt;
      &lt;item&gt;Check firewall: &lt;code&gt;ufw status&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Verify &lt;code&gt;username&lt;/code&gt;and&lt;code&gt;secret&lt;/code&gt;match in users.yaml&lt;/item&gt;
      &lt;item&gt;Check server time is accurate (within 5 minutes)&lt;/item&gt;
      &lt;item&gt;Run &lt;code&gt;smtp-tunnel-listusers -v&lt;/code&gt;to verify user exists&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Check user's whitelist in users.yaml&lt;/item&gt;
      &lt;item&gt;Your current IP must match a whitelist entry&lt;/item&gt;
      &lt;item&gt;CIDR notation is supported (e.g., &lt;code&gt;10.0.0.0/8&lt;/code&gt;)&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Ensure you're using a domain name, not IP address&lt;/item&gt;
      &lt;item&gt;Verify &lt;code&gt;server_host&lt;/code&gt;matches the certificate hostname&lt;/item&gt;
      &lt;item&gt;Ensure you have the correct &lt;code&gt;ca.crt&lt;/code&gt;from the server&lt;/item&gt;
    &lt;/list&gt;
    &lt;code&gt;# Enable detailed logging
python server.py -d
python client.py -d

# View systemd logs
journalctl -u smtp-tunnel -f&lt;/code&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;‚úÖ Always use a domain name for proper TLS verification&lt;/item&gt;
      &lt;item&gt;‚úÖ Always use &lt;code&gt;ca_cert&lt;/code&gt;to prevent man-in-the-middle attacks&lt;/item&gt;
      &lt;item&gt;‚úÖ Use &lt;code&gt;smtp-tunnel-adduser&lt;/code&gt;to generate strong secrets automatically&lt;/item&gt;
      &lt;item&gt;‚úÖ Use per-user IP whitelists if you know client IPs&lt;/item&gt;
      &lt;item&gt;‚úÖ Protect &lt;code&gt;users.yaml&lt;/code&gt;- contains all user secrets (chmod 600)&lt;/item&gt;
      &lt;item&gt;‚úÖ Disable logging for sensitive users with &lt;code&gt;logging: false&lt;/code&gt;&lt;/item&gt;
    &lt;/list&gt;
    &lt;quote&gt;
      &lt;p&gt;üìö For detailed security analysis and threat model, see TECHNICAL.md.&lt;/p&gt;
    &lt;/quote&gt;
    &lt;p&gt;This project is provided for educational and authorized use only. Use responsibly and in accordance with applicable laws.&lt;/p&gt;
    &lt;p&gt;This tool is designed for legitimate privacy and censorship circumvention purposes. Users are responsible for ensuring their use complies with applicable laws and regulations.&lt;/p&gt;
    &lt;p&gt;Made with ‚ù§Ô∏è for internet freedom&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46520926</guid><pubDate>Wed, 07 Jan 2026 00:30:09 +0000</pubDate></item><item><title>Electronic nose for indoor mold detection and identification</title><link>https://advanced.onlinelibrary.wiley.com/doi/10.1002/adsr.202500124</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46520935</guid><pubDate>Wed, 07 Jan 2026 00:31:01 +0000</pubDate></item><item><title>We recreated Steve Jobs's 1975 Atari horoscope program</title><link>https://blog.adafruit.com/2026/01/06/we-recreated-steve-jobss-1975-atari-horoscope-program-and-you-can-run-it/</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46521029</guid><pubDate>Wed, 07 Jan 2026 00:44:43 +0000</pubDate></item><item><title>On the slow death of scaling</title><link>https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5877662</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46522308</guid><pubDate>Wed, 07 Jan 2026 03:48:05 +0000</pubDate></item><item><title>Optery (YC W22) Hiring a CISO and Web Scraping Engineers (Node) (US and Latam)</title><link>https://www.optery.com/careers/</link><description>&lt;doc fingerprint="ca6bb85f43b74372"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;Careers&lt;/head&gt;
    &lt;p&gt;üí°Page not loading? Optery‚Äôs Career page uses Cookies to display the full page content. If you‚Äôre not seeing anything, try opening the cookie banner (cookie icon in the bottom left corner) and Accept Personalization cookies.&lt;/p&gt;
    &lt;p&gt;üí°Page not loading? Optery‚Äôs Career page uses Cookies to display the full page content. If you‚Äôre not seeing anything, try opening the cookie banner (cookie icon in the bottom left corner) and Accept Personalization cookies.&lt;/p&gt;
    &lt;p&gt;Ready to safeguard your personal data?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46525394</guid><pubDate>Wed, 07 Jan 2026 12:00:20 +0000</pubDate></item><item><title>The Eric and Wendy Schmidt Observatory System</title><link>https://www.schmidtsciences.org/schmidt-observatory-system/</link><description>&lt;doc fingerprint="f611b05667864f3b"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;The Eric and Wendy Schmidt Observatory System&lt;/head&gt;
    &lt;p&gt;Four novel observatories expanding access and enabling new ways to explore the cosmos&lt;/p&gt;
    &lt;p&gt;The Eric and Wendy Schmidt Observatory System is designed to pioneer a new paradigm for astronomical observatories, fundamentally rethinking how they are conceived, developed, and utilized. This initiative compresses development timelines from decades to years, dramatically lowering barriers to global participation and accelerating the pace of discovery. By uniting rapid development cycles with open data and shared scientific tools, the system empowers researchers everywhere to engage in frontier astrophysics.&lt;/p&gt;
    &lt;head rend="h2"&gt; Strategic Pillars &lt;/head&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;p&gt;Rapid observatory development leveraging risk-tolerant technical innovation&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Modular designs that leverage economies of scale&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Open data and software for global access&lt;/p&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;p&gt;Global, cross-disciplinary scientific collaboration&lt;/p&gt;
      &lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h2"&gt; Major Projects &lt;/head&gt;
    &lt;p&gt;Through more accessible and responsive scientific infrastructure, the Eric and Wendy Schmidt Observatory System seeks to support discovery for the benefit of all. Explore our projects by clicking the tiles below.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Argus Array&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Deep Synoptic Array (DSA)&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Large Fiber Array Spectroscopic Telescope (LFAST)&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;Lazuli Space Observatory&lt;/head&gt;
      &lt;/item&gt;
      &lt;item&gt;
        &lt;head rend="h3"&gt;FirstLight Awards&lt;/head&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46525542</guid><pubDate>Wed, 07 Jan 2026 12:19:01 +0000</pubDate></item><item><title>‚ÄúStop Designing Languages. Write Libraries Instead‚Äù (2016)</title><link>https://lbstanza.org/purpose_of_programming_languages.html</link><description>&lt;doc fingerprint="2fc219af0a3a92d0"&gt;
  &lt;main&gt;
    &lt;table&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;HomePhilosophyDownloadsDocumentationPeopleCommunityNewsReference&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="3"&gt;
        &lt;cell&gt;
          &lt;head&gt;NAVIGATION&lt;/head&gt;
          &lt;head&gt;"Stop Designing Languages. Write Libraries Instead."&lt;/head&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;head&gt;"Stop Designing Languages. Write Libraries Instead."&lt;/head&gt;
          &lt;p&gt;Patrick S. Li - May 29, 2016&lt;/p&gt;
          &lt;p&gt;I had a friend tell me recently that all programming languages seem very similar to each other. They all have variables, and arrays, a few loop constructs, functions, and some arithmetic constructs. Sure, some languages have fancier features like first-class functions or coroutines, but he doesn't consider himself an expert programmer anyway and doesn't use those features.&lt;/p&gt;
          &lt;p&gt;What really makes a programming language productive for him, he says, are the libraries it comes with. For example, he got into programming by using the popular Ruby on Rails web framework. There is no way that he could have written a full database-driven web stack by himself, nor is he interested in doing so. But thanks to Ruby on Rails, he doesn't have to! So he said that he has no particular opinion about the Ruby programming language, but he absolutely loves Rails. The vast majority of programmers are non-experts, like himself, and the largest gains in productivity for non-experts come from having a wide spectrum of easy-to-use libraries. Subtle language features like first-class functions, and object systems, are lost on them because they don't really use them anyway. Computer scientists should really be spending their time developing new libraries rather than inventing new programming languages.&lt;/p&gt;
          &lt;p&gt;My friend's opinion about programming languages is a common one, and I have heard it repeatedly from experts and non-experts alike. Being a language designer myself, I, of course, don't share this opinion. Here is what I consider to be the purpose of a general-purpose programming language.&lt;/p&gt;
          &lt;p&gt;To start off, I would say that my friend's opinion is completely correct, just incomplete. The greatest productivity gains are indeed the result of having a wide spectrum of libraries. Ruby on Rails is a fantastic framework, and it has enabled thousands (if not millions) of non-experts to build sophisticated websites quickly. So the natural question then is, why isn't there now a Rails framework for every programming language?&lt;/p&gt;
          &lt;p&gt;Some languages that are semantically similar to Ruby do have their own web frameworks. Python, for example, has Django. But as of now, there is still no decent web framework for Java that is as easy to use as Ruby on Rails. Why is that? Are Java developers just not as competent as Ruby programmers? If David Hansson could design and develop Rails by himself, why can't a group of programmers just copy the design to Java? What makes this even more embarrassing is the fact that Java initially marketed itself as the web programming language, because of its applet technology. To emphasize this point, let me add that there is no good web framework for C either, and it is unlikely that there ever will be. Let me assure you that it's not because C programmers are worse than Ruby programmers.&lt;/p&gt;
          &lt;p&gt;Economics is not the reason either. The Tiobe index lists Java and C as the most widely used programming languages today, with Ruby coming in eighth place. There are many times more Java and C programmers than there are Ruby programmers. If someone would just write Java on Rails their framework would have many times more users than Ruby on Rails, and it would instantly propel him to internet fame and fortune.&lt;/p&gt;
          &lt;p&gt;So it's not because of incompetency. Nor is it because of economics. So why else wouldn't someone port Ruby on Rails to Java? Well, simply, because they can't.&lt;/p&gt;
          &lt;p&gt;If you're a knowledgeable Ruby programmer and you take a deep look through an introductory Rails tutorial, you'll notice that pretty much all of the Ruby language features come into play in some way. Rail's ActiveRecords library makes pervasive use of Ruby's meta-programming features. Rail's template system heavily relies upon Ruby's runtime evaluation features. To make your website respond to a user click, you subclass &lt;/p&gt;
          &lt;p&gt;So, completely unbeknownst to my friend, he is actually making heavy use of all those subtle language features that he claimed he never cared about. And this is intentional! Ruby on Rails was designed to make it possible to build websites without understanding type theory, or memory management, or object-oriented design patterns. Rails allow website designers to focus on designing websites, not managing their software infrastructure. My friend is enjoying all the benefits of Ruby without even knowing it, and that's the whole point.&lt;/p&gt;
          &lt;p&gt;Taking a step back, the concept of packaging code into easy-to-use libraries is not new. It's been around even in the days when programs were stored on punched paper tape. There are still vast libraries of assembly code containing useful subroutines. And every programming language ever designed provided some way for common functionality to be reused. To me, this is the primary purpose of a general-purpose programming language, to enable the creation of a wide spectrum of easy-to-use libraries.&lt;/p&gt;
          &lt;p&gt;The design of the programming language directly determines what sort of libraries you can write and how easy they are to use in the end. In the C language, the only major feature provided for enabling reuse is the ability to declare and call functions. So guess what? The majority of C libraries are basically large collections of functions. Ruby on Rails provides a concise way for expressing: do this when the button is clicked. The "do this" part is implemented in Ruby as a first-class function. How would it be implemented in languages like Java which don't support them? Well, the behaviour of first-class functions can be mocked by defining a new event handler class with a single &lt;/p&gt;
          &lt;p&gt;In the early days of software, collections of functions were sufficient in allowing us to code reusable components. A lot of early software was numerical in nature, and there was a library function for every numerical algorithm you would want to run. Numbers go in. Numbers come out. Functions were perfectly adequate for this. Unix and C were also designed in a time when the majority of computing happens in batch mode. You prepare some input data, call a function or run a program, and you get some output data back. But computing has changed radically since the 70's. Nowadays, most interesting programs are interactive. When a user clicks a button, it should do something. It was rare to want to extend the functionality of a library of the 70's. The library provides a collection of useful functions. If one of them does what you want, then use it. If not, then write your own. But with the advent of interactive software, the need for extensible libraries became apparent. Programmers wanted GUI libraries that allowed them to say: when a user clicks a button, please run my code. Java (and C++) provides a limited method for extending an existing library's functionality through its subclassing mechanism. So using a Java library often consists of subclassing a number of magical classes and then overriding a number of magical methods. This style of library became so pervasive at one point that we even gave them a new name. They're called frameworks.&lt;/p&gt;
          &lt;p&gt;I surmise that probably many general purpose programming languages were originally designed because of the author's inability to write a good library for the language that he was using at the time. The initial impetus that got me thinking about designing Stanza, for example, came out of my frustrations with trying to write an easy-to-use game programming library in Java. To handle concurrency, traditional game programming frameworks required sprite behaviours to be programmed using a state machine model. But that's not how we intuitively think about sprites in our heads. Intuitively, we think about a character's behaviour as consisting of a sequence of steps. For example, first the character jumps, and then after he lands he looks to his left and then his right for the nearest enemy. If he sees one then he goes to attack it, otherwise he jumps again. He does this three times, and if he doesn't see an enemy after three jumps, then he takes a short nap. Transforming this sequence of steps into a state machine is an incredibly tedious and error-prone process, and most importantly, feels repetitive. It felt like I was doing the same thing again and again. So the natural question is, can I just make this state machine transformation a library and re-use it? It turns out I couldn't, not in Java at least. The language feature that I needed was some sort of coroutine or continuation mechanism. After some research I found that the Scheme language supports continuations, so the Scheme version of my game programming library was much easier to use than the Java version.&lt;/p&gt;
          &lt;p&gt;Because of its support for continuations, the Scheme version of my game library does not require users to write their sprite behaviour as state machines. But it wasn't better than the Java version in every way. Most importantly, the Java version was statically typed and so the compiler automatically caught many of your mistakes for you. The Scheme version didn't have this ability and thus debugging my games took a bit longer. At this point, the right question to ask would be, well can you write a static-typing library for Scheme that then automatically checks your code for type errors? And the current answer, for now and for the foreseeable future, is no. No mainstream language today allows you to write a library to extend its type system. Stanza doesn't either. It just attempts to provide one that is useful for a wider audience.&lt;/p&gt;
          &lt;p&gt;Since the purpose of general-purpose programming languages are to enable the creation of powerful libraries, this means that different languages can also be characterized by what features they provide that cannot be written as libraries. Stanza provides an optional type system, garbage collection, and a multimethod based object system. But if you don't like Stanza's object system, there is no way to write your own. This is one of the main directions of programming language research. Can we design a language so expressive that library writers can easily write the most appropriate object system, or most appropriate type system, to fit their application? Perhaps one day we'll have such a language. Racket and Shen provide mechanisms for extending their type systems and research on meta-object protocols were attempts at designing extensible object systems. So languages are differentiated by what types of libraries you can write in them and what types of libraries you can't.&lt;/p&gt;
          &lt;p&gt;In summary, the purpose of a general-purpose programming language is to enable the creation of powerful and easy-to-use libraries. The more powerful the language, the easier the libraries are to use. Code that makes use of a perfectly tuned library should read almost like a set of instructions for a coworker. So the next time you come across a particularly elegant library, know that many decades of language research has gone into making that possible. If you're curious about specifically which language features a library makes use of, then you can dig deeper, explore, and appreciate the thought that went into its implementation. If you're not curious about all this subtle language stuff, you can safely ignore it all and get on with your work. That's the whole point.&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Site design by Luca Li. Copyright 2015.&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46525640</guid><pubDate>Wed, 07 Jan 2026 12:29:11 +0000</pubDate></item><item><title>A4 Paper Stories</title><link>https://susam.net/a4-paper-stories.html</link><description>&lt;doc fingerprint="7299db7cc73604b0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;A4 Paper Stories&lt;/head&gt;
    &lt;p&gt;I sometimes resort to a rather common measuring technique that is neither fast, nor accurate, nor recommended by any standards body and yet it hasn't failed me whenever I have had to use it. I will describe it here, though calling it a technique might be overselling it. Please do not use it for installing kitchen cabinets or anything that will stare back at you every day for the next ten years. It involves one tool: a sheet of A4 paper.&lt;/p&gt;
    &lt;p&gt;Like most sensible people with a reasonable sense of priorities, I do not carry a ruler with me wherever I go. Nevertheless, I often find myself needing to measure something at short notice, usually in situations where a certain amount of inaccuracy is entirely forgivable. When I cannot easily fetch a ruler, I end up doing what many people do and reach for the next best thing, which for me is a sheet of A4 paper, available in abundant supply where I live.&lt;/p&gt;
    &lt;p&gt;From photocopying night-sky charts to serving as a scratch pad for working through mathematical proofs, A4 paper has been a trusted companion since my childhood days. I use it often. If I am carrying a bag, there is almost always some A4 paper inside: perhaps a printed research paper or a mathematical problem I have worked on recently and need to chew on a bit more during my next train ride.&lt;/p&gt;
    &lt;head rend="h2"&gt;Dimensions&lt;/head&gt;
    &lt;p&gt;The dimensions of A4 paper are the solution to a simple, elegant problem. Imagine designing a sheet of paper such that, when you cut it in half parallel to its shorter side, both halves have exactly the same aspect ratio as the original. In other words, if the shorter side has length \( x \) and the longer side has length \( y , \) then \[ \frac{y}{x} = \frac{x}{y / 2} \] which gives us \[ \frac{y}{x} = \sqrt{2}. \] Test it out. Suppose we have \( y/x = \sqrt{2}. \) We cut the paper in half parallel to the shorter side to get two halves, each with shorter side \( x' = y / 2 = x \sqrt{2} / 2 = x / \sqrt{2} \) and longer side \( y' = x. \) Then indeed \[ \frac{y'}{x'} = \frac{x}{x / \sqrt{2}} = \sqrt{2}. \] In fact, we can keep cutting the halves like this and we'll keep getting even smaller sheets with the aspect ratio \( \sqrt{2} \) intact. To summarise, when a sheet of paper has the aspect ratio \( \sqrt{2}, \) bisecting it parallel to the shorter side leaves us with two halves that preserve the aspect ratio. A4 paper has this property.&lt;/p&gt;
    &lt;p&gt;But what are the exact dimensions of A4 and why is it called A4? What does 4 mean here? Like most good answers, this one too begins by considering the numbers zero and one. Let me elaborate.&lt;/p&gt;
    &lt;p&gt;Let us say we want to make a sheet of paper that is \( 1 \, \mathrm{m}^2 \) in area and has the aspect-ratio-preserving property that we just discussed. What should its dimensions be? We want \[ xy = 1 \, \mathrm{m}^2 \] subject to the condition \[ \frac{y}{x} = \sqrt{2}. \] Solving these two equations gives us \[ x^2 = \frac{1}{\sqrt{2}} \, \mathrm{m}^2 \] from which we obtain \[ x = \frac{1}{\sqrt[4]{2}} \, \mathrm{m}, \quad y = \sqrt[4]{2} \, \mathrm{m}. \] Up to three decimal places, this amounts to \[ x = 0.841 \, \mathrm{m}, \quad y = 1.189 \, \mathrm{m}. \] These are the dimensions of A0 paper. They are precisely the dimensions specified by the ISO standard for it. It is quite large to scribble mathematical solutions on, unless your goal is to make a spectacle of yourself and cause your friends and family to reassess your sanity. So we need something smaller that allows us to work in peace, without inviting commentary or concerns from passersby. We take the A0 paper of size \[ 84.1 \, \mathrm{cm} \times 118.9 \, \mathrm{cm} \] and bisect it to get A1 paper of size \[ 59.4 \, \mathrm{cm} \times 84.1 \, \mathrm{cm}. \] Then we bisect it again to get A2 paper with dimensions \[ 42.0 \, \mathrm{cm} \times 59.4 \, \mathrm{cm}. \] And once again to get A3 paper with dimensions \[ 29.7 \, \mathrm{cm} \times 42.0 \, \mathrm{cm}. \] And then once again to get A4 paper with dimensions \[ 21.0 \, \mathrm{cm} \times 29.7 \, \mathrm{cm}. \] There we have it. The dimensions of A4 paper. These numbers are etched in my memory like the multiplication table of \( 1. \) We can keep going further to get A5, A6, etc. We could, in theory, go all the way up to A\( \infty. \) Hold on, I think I hear someone heckle. What's that? Oh, we can't go all the way to A\( \infty? \) Something about atoms, was it? Hmm. Security! Where's security? Ah yes, thank you, sir. Please show this gentleman out, would you?&lt;/p&gt;
    &lt;p&gt;Sorry for the interruption, ladies and gentlemen. Phew! That fellow! Atoms? Honestly. We, the mathematically inclined, are not particularly concerned with such trivial limitations. We drink our tea from doughnuts. We are not going to let the size of atoms dictate matters, now are we?&lt;/p&gt;
    &lt;p&gt;So I was saying that we can bisect our paper like this and go all the way to A\( \infty. \) That reminds me. Last night I was at a bar in Hoxton and I saw an infinite number of mathematicians walk in. The first one asked, "Sorry to bother you, but would it be possible to have a sheet of A0 paper? I just need something to scribble a few equations on." The second one asked, "If you happen to have one spare, could I please have an A1 sheet?" The third one said, "An A2 would be perfectly fine for me, thank you." Before the fourth one could ask, the bartender disappeared into the back for a moment and emerged with two sheets of A0 paper and said, "Right. That should do it. Do know your limits and split these between yourselves."&lt;/p&gt;
    &lt;p&gt;In general, a sheet of A\( n \) paper has the dimensions \[ 2^{-(2n + 1)/4} \, \mathrm{m} \times 2^{-(2n - 1)/4} \, \mathrm{m}. \] If we plug in \( n = 4, \) we indeed get the dimensions of A4 paper: \[ 0.210 \, \mathrm{m} \times 0.297 \, \mathrm{m}. \]&lt;/p&gt;
    &lt;head rend="h2"&gt;Measuring Stuff&lt;/head&gt;
    &lt;p&gt;Let us now return to the business of measuring things. As I mentioned earlier, the dimensions of A4 are lodged firmly into my memory. Getting hold of a sheet of A4 paper is rarely a challenge where I live. I have accumulated a number of A4 paper stories over the years. Let me share a recent one. I was hanging out with a few folks of the nerd variety one afternoon when the conversation drifted, as it sometimes does, to a nearby computer monitor that happened to be turned off. At some point, someone confidently declared that the screen in front of us was 27 inches. That sounded plausible but we wanted to confirm it. So I reached for my trusted measuring instrument: an A4 sheet of paper. What followed was neither fast, nor especially precise, but it was more than adequate for settling the matter at hand.&lt;/p&gt;
    &lt;p&gt;I lined up the longer edge of the A4 sheet with the width of the monitor. One length. Then I repositioned it and measured a second length. The screen was still sticking out slightly at the end. By eye, drawing on an entirely unjustified confidence built from years of measuring things that never needed measuring, I estimated the remaining bit at about \( 1 \, \mathrm{cm}. \) That gives us a width of \[ 29.7 \, \mathrm{cm} + 29.7 \, \mathrm{cm} + 1.0 \, \mathrm{cm} = 60.4 \, \mathrm{cm}. \] Let us round that down to \( 60 \, \mathrm{cm}. \) For the height, I switched to the shorter edge. One full \( 21 \, \mathrm{cm} \) fit easily. For the remainder, I folded the paper parallel to the shorter side, producing an A5-sized rectangle with dimensions \( 14.8 \, \mathrm{cm} \times 21.0 \, \mathrm{cm}. \) Using the \( 14.8 \, \mathrm{cm} \) edge, I discovered that it overshot the top of the screen slightly. Again, by eye, I estimated the excess at around \( 2 \, \mathrm{cm}. \) That gives us \[ 21.0 \, \mathrm{cm} + 14.8 \, \mathrm{cm} -2.0 \, \mathrm{cm} = 33.8 \, \mathrm{cm}. \] Let us round this up to \( 34 \, \mathrm{cm}. \) The ratio \( 60 / 34 \approx 1.76 \) is quite close to \( 16/9, \) a popular aspect ratio of modern displays. At this point the measurements were looking good. So far, the paper had not embarrassed itself. Invoking the wisdom of the Pythagoreans, we can now estimate the diagonal as \[ \sqrt{(60 \, \mathrm{cm})^2 + (34 \, \mathrm{cm})^2} \approx 68.9 \,\mathrm{cm}. \] Finally, there is the small matter of units. One inch is \( 2.54 \, \mathrm{cm}, \) another figure that has embedded itself in my head. Dividing \( 68.9 \) by \( 2.54 \) gives us roughly \( 27.2 \, \mathrm{in}. \) So yes. It was indeed a \( 27 \)-inch display. My elaborate exercise in showing off my A4 paper skills was now complete. Nobody said anything. A few people looked away in silence. I assumed they were reflecting. I am sure they were impressed deep down. Or perhaps... no, no. They were definitely impressed. I am sure.&lt;/p&gt;
    &lt;p&gt;Hold on. I think I hear another heckle. What is that? There are mobile phone apps that can measure things now? Really? Right. Security. Where's security?&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46525888</guid><pubDate>Wed, 07 Jan 2026 12:54:43 +0000</pubDate></item><item><title>Show HN: KeelTest ‚Äì AI-driven VS Code unit test generator with bug discovery</title><link>https://keelcode.dev/keeltest</link><description>&lt;doc fingerprint="10373b5026f706a0"&gt;
  &lt;main&gt;
    &lt;head rend="h1"&gt;AI Tests That Find &lt;lb/&gt;Bugs Before Production&lt;/head&gt;
    &lt;p&gt;Generate pytest suites that actually run - and expose issues in your code. 90% average pass rate. Source bugs flagged with fix suggestions.&lt;/p&gt;
    &lt;p&gt;Free forever ¬∑ 7 credits/month ¬∑ No credit card required&lt;/p&gt;
    &lt;p&gt;* Stats updated weekly. Based on active Alpha usage.&lt;/p&gt;
    &lt;head rend="h2"&gt;Get Started in 3 Simple Steps&lt;/head&gt;
    &lt;p&gt;KeelTest is a VS Code extension that installs in seconds. No complex setup, no external services.&lt;/p&gt;
    &lt;head rend="h3"&gt;Open VS Code Extensions&lt;/head&gt;
    &lt;p&gt;Press Ctrl+Shift+X (Windows/Linux) or Cmd+Shift+X (Mac) to open the Extensions view in VS Code.&lt;/p&gt;
    &lt;head rend="h3"&gt;Search for KeelTest&lt;/head&gt;
    &lt;p&gt;Type "KeelTest" in the search bar and click Install on the official KeelTest extension.&lt;/p&gt;
    &lt;head rend="h3"&gt;Right-Click and Generate&lt;/head&gt;
    &lt;p&gt;Right-click any Python file in your workspace and select "KeelTest: Generate Tests" to start.&lt;/p&gt;
    &lt;p&gt;Free to install ‚Ä¢ Available on VS Code Marketplace ‚Ä¢ No credit card required&lt;/p&gt;
    &lt;head rend="h2"&gt;Why developers switch &lt;lb/&gt;to KeelTest&lt;/head&gt;
    &lt;p&gt;Moving beyond simple prompts. We combined static analysis with a multi-step verification pipeline to deliver production-grade tests.&lt;/p&gt;
    &lt;head rend="h3"&gt;Deep Static Analysis&lt;/head&gt;
    &lt;p&gt;Our engine builds a full AST (Abstract Syntax Tree) representation of your code, identifying exactly which branches need coverage and which edge cases are most likely to cause regressions.&lt;/p&gt;
    &lt;head rend="h2"&gt;From Code to Tests in 3 Clicks&lt;/head&gt;
    &lt;head rend="h2"&gt;Start Free, Scale When Ready&lt;/head&gt;
    &lt;p&gt;Join ... developers already on the waitlist for our upcoming premium tiers.&lt;/p&gt;
    &lt;head rend="h3"&gt;Individual Plans&lt;/head&gt;
    &lt;head rend="h4"&gt;Starter&lt;/head&gt;
    &lt;p&gt;For regular development&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Priority queue&lt;/item&gt;
      &lt;item&gt;Usage analytics&lt;/item&gt;
      &lt;item&gt;Bug detection&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Pro&lt;/head&gt;
    &lt;p&gt;For power users&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Priority queue&lt;/item&gt;
      &lt;item&gt;Usage analytics&lt;/item&gt;
      &lt;item&gt;Bug detection&lt;/item&gt;
      &lt;item&gt;Early access to features&lt;/item&gt;
    &lt;/list&gt;
    &lt;head rend="h4"&gt;Free&lt;/head&gt;
    &lt;p&gt;Perfect for trying it out&lt;/p&gt;
    &lt;head rend="h4"&gt;Detailed Comparison&lt;/head&gt;
    &lt;p&gt;Everything you get with each tier&lt;/p&gt;
    &lt;head rend="h2"&gt;Real Pass Rates, Not marketing&lt;/head&gt;
    &lt;p&gt;Every test is executed in a sandbox before it reaches your editor. We don't just generate code; we deliver verified functionality.&lt;/p&gt;
    &lt;p&gt;Pass Rate&lt;/p&gt;
    &lt;p&gt;Self-Healing GenerationFailures are automatically fixed by our AI validator before delivery.&lt;/p&gt;
    &lt;p&gt;Source Bug DetectionReal issues in your source code are triaged and clearly flagged.&lt;/p&gt;
    &lt;head rend="h3"&gt;How far your credits go&lt;/head&gt;
    &lt;p&gt;Estimated file generation per month based on complexity&lt;/p&gt;
    &lt;table&gt;
      &lt;row span="4"&gt;
        &lt;cell role="head"&gt;Complexity&lt;/cell&gt;
        &lt;cell role="head"&gt;Free&lt;/cell&gt;
        &lt;cell role="head"&gt;Starter&lt;/cell&gt;
        &lt;cell role="head"&gt;Pro&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Small files (‚â§15 fn)Approx. 15 functions&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~7&lt;/p&gt;
          &lt;p&gt;files&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~30&lt;/p&gt;
          &lt;p&gt;files&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~70&lt;/p&gt;
          &lt;p&gt;files&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="4"&gt;
        &lt;cell&gt;Medium files (~30 fn)Approx. 30 functions&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~3&lt;/p&gt;
          &lt;p&gt;files&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~15&lt;/p&gt;
          &lt;p&gt;files&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~35&lt;/p&gt;
          &lt;p&gt;files&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;Large files (~50 fn)Approx. 50 functions&lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~1&lt;/p&gt;
          &lt;p&gt;files&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~7&lt;/p&gt;
          &lt;p&gt;files&lt;/p&gt;
        &lt;/cell&gt;
        &lt;cell&gt;
          &lt;p&gt;~17&lt;/p&gt;
          &lt;p&gt;files&lt;/p&gt;
        &lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;1 credit = up to 15 functions. Larger files use proportionally more credits.&lt;/p&gt;
    &lt;head rend="h2"&gt;Tests That Actually Test&lt;/head&gt;
    &lt;p&gt;We tested leading AI models on generating unit tests for complex e-commerce logic. KeelTest's agentic approach-combining AI with static code analysis, test validation, and actual execution-achieved a staff-level score of 8.5/10, outperforming pure zero-shot prompts by 54%.&lt;/p&gt;
    &lt;head rend="h3"&gt;Overall Quality ScoreStaff Engineer = 10&lt;/head&gt;
    &lt;head rend="h3"&gt;Detailed Evaluation Criteria&lt;/head&gt;
    &lt;table&gt;
      &lt;row span="5"&gt;
        &lt;cell role="head"&gt;Criteria&lt;/cell&gt;
        &lt;cell role="head"&gt;KeelTest&lt;/cell&gt;
        &lt;cell role="head"&gt;Model B&lt;/cell&gt;
        &lt;cell role="head"&gt;Model C&lt;/cell&gt;
        &lt;cell role="head"&gt;Model A&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Unit Test Isolation&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Mocking &amp;amp; Dependency Injection&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Edge Case Coverage&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Following Instructions&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row span="5"&gt;
        &lt;cell&gt;Technical Correctness&lt;/cell&gt;
      &lt;/row&gt;
      &lt;row&gt;
        &lt;cell&gt;DateTime/Float Precision&lt;/cell&gt;
      &lt;/row&gt;
    &lt;/table&gt;
    &lt;p&gt;"KeelTest demonstrates the deepest understanding of unit testing principles with proper isolation, comprehensive mocking, and dependency injection. It's what a staff engineer would produce."&lt;/p&gt;
    &lt;p&gt;* KeelTest leverages advanced AI models enhanced with static code analysis, automated test validation, and real-time execution feedback-not just raw prompts.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46526088</guid><pubDate>Wed, 07 Jan 2026 13:22:35 +0000</pubDate></item><item><title>Show HN: RepoReaper ‚Äì AST-aware, JIT-loading code audit agent (Python/AsyncIO)</title><link>https://github.com/tzzp1224/RepoReaper</link><description>&lt;doc fingerprint="a952fc1567f8ad52"&gt;
  &lt;main&gt;
    &lt;p&gt;üëá Live Demo Access / Âú®Á∫ø‰ΩìÈ™å üëá&lt;/p&gt;
    &lt;p/&gt;
    &lt;p&gt;An intelligent, agentic system for automated architectural analysis and semantic code search.&lt;/p&gt;
    &lt;p&gt;This project transcends traditional "Chat with Code" paradigms by implementing an autonomous Agent that mimics the cognitive process of a Senior Tech Lead. Instead of statically indexing a repository, the system treats the Large Language Model (LLM) as the CPU and the Vector Store as a high-speed Context Cache. The agent dynamically traverses the repository structure, pre-fetching critical contexts into the "cache" (RAG) and performing Just-In-Time (JIT) reads when semantic gaps are detected.&lt;/p&gt;
    &lt;p&gt;In traditional code assistants, RAG (Retrieval-Augmented Generation) is often a static lookup table. In this architecture, we redefine RAG as a Dynamic L2 Cache for the LLM:&lt;/p&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;Cold Start (Repo Map): The agent first parses the Abstract Syntax Tree (AST) of the entire repository to build a lightweight symbol map (Classes/Functions). This serves as the "index" to the file system.&lt;/item&gt;
      &lt;item&gt;Prefetching (Analysis Phase): During the initial analysis, the agent autonomously selects the most critical 10-20 files based on architectural relevance, parses them, and "warms up" the vector store (the cache).&lt;/item&gt;
      &lt;item&gt;Cache Miss Handling (ReAct Loop): During user Q&amp;amp;A, if the retrieval mechanism (BM25 + Vector) returns insufficient context, the Agent triggers a Just-In-Time (JIT) file read. It autonomously tools the GitHub API to fetch missing files, updates the cache in real-time, and re-generates the answer.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Standard text chunking destroys code logic. We utilize Python's &lt;code&gt;ast&lt;/code&gt; module to implement Structure-Aware Chunking.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Logical Boundaries: Code is split by Class and Method definitions, ensuring that a function is never severed in the middle.&lt;/item&gt;
      &lt;item&gt;Context Injection: Large classes are decomposed into methods, but the parent class's signature and docstrings are injected into every child chunk. This ensures the LLM understands the "why" (class purpose) even when looking at the "how" (method implementation).&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;Built on top of &lt;code&gt;asyncio&lt;/code&gt; and &lt;code&gt;httpx&lt;/code&gt;, the system is designed for high-throughput I/O operations.&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Non-Blocking Ingestion: Repository parsing, AST extraction, and vector embedding occur concurrently.&lt;/item&gt;
      &lt;item&gt;Worker Scalability: The application runs behind Gunicorn with Uvicorn workers, utilizing a stateless design pattern where the Vector Store Manager synchronizes context via persistent disk storage and shared ChromaDB instances. This allows multiple workers to serve requests without race conditions.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The Chat Service implements a sophisticated Reasoning + Acting (ReAct) loop:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Query Rewrite: User queries (often vague or in different languages) are first rewritten by an LLM into precise, English-language technical keywords for optimal BM25/Vector retrieval.&lt;/item&gt;
      &lt;item&gt;Self-Correction: If the retrieved context is insufficient, the model does not hallucinate. Instead, it issues a &lt;code&gt;&amp;lt;tool_code&amp;gt;&lt;/code&gt;command to fetch specific file paths from the repository. The system intercepts this command, pulls the fresh data, indexes it, and feeds it back to the model in a single inference cycle.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;To balance semantic understanding with exact keyword matching, the retrieval engine employs a weighted hybrid approach:&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dense Retrieval (Vector): Uses &lt;code&gt;BAAI/bge-m3&lt;/code&gt;embeddings to find conceptually similar code (e.g., matching "authentication" to "login logic").&lt;/item&gt;
      &lt;item&gt;Sparse Retrieval (BM25): Captures exact variable names, error codes, and specific function signatures that vector embeddings might miss.&lt;/item&gt;
      &lt;item&gt;Reciprocal Rank Fusion (RRF): Results are fused and re-ranked to ensure the highest fidelity context is provided to the LLM.&lt;/item&gt;
    &lt;/list&gt;
    &lt;p&gt;The architecture is completely language-agnostic but optimized for dual-language environments (English/Chinese).&lt;/p&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Dynamic Prompt Engineering: The system detects the user's input language and hot-swaps the System Prompts to ensure the output format, tone, and technical terminology align with the user's locale.&lt;/item&gt;
      &lt;item&gt;UI Integration: The frontend includes a dedicated language toggle that influences the entire generation pipeline, from the initial architectural report to the final Q&amp;amp;A.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Core: Python 3.10+, FastAPI, AsyncIO&lt;/item&gt;
      &lt;item&gt;LLM Integration: OpenAI SDK (compatible with DeepSeek/SiliconFlow)&lt;/item&gt;
      &lt;item&gt;Vector Database: ChromaDB (Persistent Storage)&lt;/item&gt;
      &lt;item&gt;Search Algorithms: BM25Okapi, Rank-BM25&lt;/item&gt;
      &lt;item&gt;Parsing: Python &lt;code&gt;ast&lt;/code&gt;(Abstract Syntax Trees)&lt;/item&gt;
      &lt;item&gt;Frontend: HTML5, Server-Sent Events (SSE) for real-time streaming, Mermaid.js for architecture diagrams.&lt;/item&gt;
      &lt;item&gt;Deployment: Docker, Gunicorn, Uvicorn.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ul"&gt;
      &lt;item&gt;Session Management: Uses browser &lt;code&gt;sessionStorage&lt;/code&gt;coupled with server-side persistent contexts, allowing users to refresh pages without losing the "warm" cache state.&lt;/item&gt;
      &lt;item&gt;Network Resilience: Implements robust error handling for GitHub API rate limits (403/429) and network timeouts during long-context generation.&lt;/item&gt;
      &lt;item&gt;Memory Efficiency: The &lt;code&gt;VectorStoreManager&lt;/code&gt;is designed to be stateless in memory but stateful on disk, preventing memory leaks in long-running container environments.&lt;/item&gt;
    &lt;/list&gt;
    &lt;list rend="ol"&gt;
      &lt;item&gt;
        &lt;p&gt;Prerequisites:&lt;/p&gt;
        &lt;list rend="ul"&gt;
          &lt;item&gt;Python 3.9+&lt;/item&gt;
          &lt;item&gt;Valid GitHub Token&lt;/item&gt;
          &lt;item&gt;LLM API Keys (DeepSeek-V3 &amp;amp; SiliconFlow bge-m3 recommended).&lt;/item&gt;
        &lt;/list&gt;
        &lt;list rend="ol"&gt;
          &lt;item&gt;
            &lt;p&gt;Clone the Repository&lt;/p&gt;
            &lt;code&gt;git clone [https://github.com/tzzp1224/RepoReaper.git](https://github.com/tzzp1224/RepoReaper.git) cd RepoReaper&lt;/code&gt;
          &lt;/item&gt;
          &lt;item&gt;
            &lt;p&gt;Install Dependencies Using a virtual environment is recommended:&lt;/p&gt;
            &lt;quote&gt;# Create and activate venv python -m venv venv source venv/bin/activate # Windows: venv\Scripts\activate # Install requirements pip install -r requirements.txt&lt;/quote&gt;
          &lt;/item&gt;
          &lt;item&gt;&lt;p&gt;Configure Environment Create a&lt;/p&gt;&lt;code&gt;.env&lt;/code&gt;file in the root directory:&lt;quote&gt;# GitHub Personal Access Token GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxx # LLM API Key (e.g., DeepSeek) DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxx # Embedding API Key (SiliconFlow) SILICON_API_KEY=sk-xxxxxxxxxxxxxxx&lt;/quote&gt;&lt;/item&gt;
          &lt;item&gt;&lt;p&gt;Start the Service&lt;/p&gt;&lt;p&gt;Option A: Local Run (Universal) Compatible with Windows, macOS, and Linux. Recommended for development:&lt;/p&gt;&lt;quote&gt;python -m app.main&lt;/quote&gt;&lt;p&gt;(Note: Linux users can still use&lt;/p&gt;&lt;code&gt;gunicorn -c gunicorn_conf.py app.main:app&lt;/code&gt;for production deployment)&lt;p&gt;Option B: Docker Run üê≥ Run in an isolated container:&lt;/p&gt;&lt;quote&gt;# 1. Build Image docker build -t reporeaper . # 2. Run Container (loading env vars) docker run -d -p 8000:8000 --env-file .env --name reporeaper reporeaper&lt;/quote&gt;&lt;/item&gt;
          &lt;item&gt;&lt;p&gt;Access Dashboard Navigate to&lt;/p&gt;&lt;code&gt;http://localhost:8000&lt;/code&gt;. Enter a GitHub repository URL to trigger the autonomous analysis agent.&lt;/item&gt;
        &lt;/list&gt;
      &lt;/item&gt;
    &lt;/list&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46526584</guid><pubDate>Wed, 07 Jan 2026 14:15:32 +0000</pubDate></item><item><title>Sugar industry influenced researchers and blamed fat for CVD</title><link>https://www.ucsf.edu/news/2016/09/404081/sugar-papers-reveal-industry-role-shifting-national-heart-disease-focus</link><description>&lt;doc fingerprint="e398a9b742a7c320"&gt;
  &lt;main&gt;&lt;p&gt;This article is archived and only made available for historical reference. If you‚Äôd like to discover UCSF‚Äôs most recent advances in research, education and patient care, please visit the UCSF News Center.&lt;/p&gt;&lt;head rend="h1"&gt;Archive: Sugar Papers Reveal Industry Role in Shifting National Heart Disease Focus to Saturated Fat&lt;/head&gt;&lt;p&gt;A newly discovered cache of industry documents revealed that the sugar industry began working closely with nutrition scientists in the mid-1960s to single out fat and cholesterol as the dietary causes of coronary heart disease and to downplay evidence that sucrose consumption was also a risk factor.&lt;/p&gt;&lt;p&gt;An analysis of those papers by researchers at UC San Francisco appears Sept. 12, 2016, in JAMA Internal Medicine.&lt;/p&gt;&lt;p&gt;The internal industry documents, which were found in public archives, showed that a sugar industry trade organization recognized as early as 1954 that if Americans adopted low-fat diets, then per-capita consumption of sucrose would increase by more than one-third. The trade organization represented 30 international members.&lt;/p&gt;&lt;p&gt;Meanwhile, evidence linking sugar consumption to high blood cholesterol and triglyceride levels ‚Äì both thought to be risk factors for coronary heart disease ‚Äì began to emerge in the scientific literature and popular press.&lt;/p&gt;&lt;head rend="h2"&gt;Literature Shaped Public Opinion&lt;/head&gt;&lt;p&gt;After a 1965 spike in media attention to the heart disease risks of sucrose, the sugar industry commissioned Project 226, a literature review written by researchers at the Harvard University School of Public Health Nutrition Department, which was published in the highly respected New England Journal of Medicine (NEJM) in 1967. It concluded there was ‚Äúno doubt‚Äù that the only dietary intervention required to prevent coronary heart disease was to reduce dietary cholesterol and substitute polyunsaturated fat for saturated fat in the American diet.&lt;/p&gt;Cristin Kearns, DDS, MBA&lt;p&gt;‚ÄúThe literature review helped shape not only public opinion on what causes heart problems but also the scientific community‚Äôs view of how to evaluate dietary risk factors for heart disease,‚Äù said lead author Cristin Kearns, DDS, MBA, who discovered the industry documents.&lt;/p&gt;&lt;p&gt;The UCSF researchers analyzed more than 340 documents, totaling 1,582 pages of text, between the sugar industry and two individuals: Roger Adams, then a professor of organic chemistry who served on scientific advisory boards for the sugar industry; and D. Mark Hegsted, one of the Harvard researchers who produced the literature review.&lt;/p&gt;&lt;p&gt;To conduct the literature review, the sugar industry paid the Harvard scientists the equivalent of $50,000 in 2016 dollars, then set the review‚Äôs objective, contributed articles to be included, and received drafts. Yet the industry‚Äôs funding and role were not disclosed in the final NEJM publication.&lt;/p&gt;&lt;p&gt;The literature review heavily criticized studies linking sucrose to heart disease, while ignoring limitations of studies investigating dietary fats. The review argued that blood cholesterol levels were the only significant risk factor for coronary heart disease, which made the high sucrose content of the American diet seem less hazardous than if blood triglycerides were also considered to be a risk factor.&lt;/p&gt;&lt;head rend="h2"&gt;Need for More Transparent Scientific Reviews&lt;/head&gt;Stanton A. Glantz, PhD&lt;p&gt;The authors emphasized that this analysis demonstrates the importance of having scientific reviews written by people without conflicts of interest, as well as the need for financial disclosure in nutrition science.&lt;/p&gt;&lt;p&gt;‚ÄúAs the saying goes, he who pays the piper calls the tune,‚Äù said senior author Stanton A. Glantz, PhD, UCSF professor of medicine and director of the UCSF Center for Tobacco Control Research and Education. ‚ÄúThere are all kinds of ways that you can subtly manipulate the outcome of a study, which industry is very well practiced at.‚Äù&lt;/p&gt;&lt;p&gt;Co-author Laura Schmidt, PhD, who is also principal investigator on the UCSF-led SugarScience initiative, noted that after decades of focusing on saturated fat as the dietary culprit in heart disease, the science is building around sugar‚Äôs role, but health policy has only just begun to catch up.&lt;/p&gt;Laura Schmidt, PhD&lt;p&gt;‚ÄúThere is now a considerable body of evidence linking added sugars to hypertension and cardiovascular disease, which is the No. 1 cause of premature death in the developed world,‚Äù Schmidt said. ‚ÄúYet, health policy documents are still inconsistent in citing heart disease risk as a health consequence of added sugars consumption.‚Äù&lt;/p&gt;&lt;p&gt;The study was funded by the UCSF Philip R. Lee Institute for Health Policy Studies; a donation by the Hellmann Family Fund to the UCSF Center for Tobacco Control Research and Education; the UCSF School of Dentistry Department of Orofacial Sciences and Global Oral Health Program; and grants from the National Institute of Dental and Craniofacial Research and the National Cancer Institute.&lt;/p&gt;&lt;p&gt;UCSF is a leading university dedicated to promoting health worldwide through advanced biomedical research, graduate-level education in the life sciences and health professions, and excellence in patient care. It includes top-ranked graduate schools of dentistry, medicine, nursing and pharmacy; a graduate division with nationally renowned programs in basic, biomedical, translational and population sciences; and a preeminent biomedical research enterprise. It also includes UCSF Health, which comprises two top-ranked hospitals, UCSF Medical Center and UCSF Benioff Children‚Äôs Hospital San Francisco, and other partner and affiliated hospitals and healthcare providers throughout the Bay Area.&lt;/p&gt;&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46526740</guid><pubDate>Wed, 07 Jan 2026 14:29:25 +0000</pubDate></item><item><title>LaTeX Coffee Stains [pdf] (2021)</title><link>https://ctan.math.illinois.edu/graphics/pgf/contrib/coffeestains/coffeestains-en.pdf</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46526933</guid><pubDate>Wed, 07 Jan 2026 14:46:31 +0000</pubDate></item><item><title>Meditation as Wakeful Relaxation: Unclenching Smooth Muscle</title><link>https://psychotechnology.substack.com/p/meditation-as-wakeful-relaxation</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527157</guid><pubDate>Wed, 07 Jan 2026 15:03:34 +0000</pubDate></item><item><title>Shipmap.org</title><link>https://www.shipmap.org/</link><description>&lt;doc fingerprint="14b4e15227c5a82f"&gt;
  &lt;main&gt;
    &lt;p&gt;Data: exactEarth &amp;amp; Clarksons&lt;/p&gt;
    &lt;p&gt;Due to popular demand the designers of this map, Kiln, are now selling stunning high-resolution versions of the world √¢routes√¢ view. There are two versions available: coloured by ship type over the inky-blue base map; or just the ship in a single colour a transparent background so you can overlay or print onto whatever background colour you like. Contact [email protected] for pricing and further information.&lt;/p&gt;
    &lt;p&gt;Yes. You are welcome to embed this map. Please include a link back to Kiln somewhere in the text of your article. Use the following embed code for a fully responsive embed that will adjust to the width of your website. Feel free to change the height and/or give it a fixed width if you prefer.&lt;/p&gt;
    &lt;p&gt;You can see movements of the global merchant fleet over the course of 2012, overlaid on a bathymetric map. You can also see a few statistics such as a counter for emitted CO2 (in thousand tonnes) and maximum freight carried by represented vessels (varying units).&lt;/p&gt;
    &lt;p&gt;You can pan and zoom in the usual ways, and skip back and forward in time using the timeline at the bottom of the screen. The controls at the top right let you show and hide different map layers: port names, the background map, routes (a plot of all recorded vessel positions), and the animated ships view. There are also controls for filtering and colouring by vessel type.&lt;/p&gt;
    &lt;p&gt;The merchant fleet is divided into five categories, each of which has a filter and a CO2 and freight counter for the hour shown on the clock. The ship types and units are as follows:&lt;/p&gt;
    &lt;p&gt;In some cases this is because there are ships navigating via canals or rivers that aren√¢t visible on the map. Generally, though, this effect is an artefact of animating a ship between two recorded positions with missing data between, especially when the positions are separated by a narrow strip of land. We may develop the map to remove this effect in the future.&lt;/p&gt;
    &lt;p&gt;Unfortunately the data we are using for the map is incomplete for the first few months of the year: roughly January to April.&lt;/p&gt;
    &lt;p&gt;The map was created by Kiln based on data from the UCL Energy Institute (UCL EI)&lt;/p&gt;
    &lt;p&gt;Website: Duncan Clark &amp;amp; Robin Houston from Kiln&lt;/p&gt;
    &lt;p&gt;Data: Julia Schaumeier &amp;amp; Tristan Smith from the UCL EI&lt;/p&gt;
    &lt;p&gt;Music: Bach Goldberg Variations played by Kimiko Ishizaka&lt;/p&gt;
    &lt;p&gt;UCL EI took data showing location and speed of ships and cross-checked it with another database to get the vessel characteristics, such as engine type and hull measurements. With this information they were able to compute the CO2 emissions for each observed hour, following the approach laid out in the Third IMO Greenhouse Gas Study 2014. Kiln took the resulting dataset and visualized it with WebGL on top of a specially created base map, which shows bathymetry (ocean depth), based on the GEBCO_2014 Grid (version 20150318), as well as continents and major rivers from Natural Earth.&lt;/p&gt;
    &lt;p&gt;Our data sources for shipping positions are exactEarth for AIS data (location/speed) and Clarksons Research UK World Fleet Register (static vessel information). We are very grateful to our funders, the European Climate Foundation.&lt;/p&gt;
  &lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527161</guid><pubDate>Wed, 07 Jan 2026 15:03:41 +0000</pubDate></item><item><title>Supreme Court Increasingly Favors the Rich, Economists Say</title><link>https://www.nytimes.com/2026/01/05/us/politics/supreme-court-study-rich-poor.html</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527443</guid><pubDate>Wed, 07 Jan 2026 15:24:42 +0000</pubDate></item><item><title>We might have been slower to abandon StackOverflow if it wasn't a toxic hellhole</title><link>https://www.pcloadletter.dev/blog/abandoning-stackoverflow/</link><description>&lt;doc fingerprint="f4f0ed37f679dd8"&gt;
  &lt;main&gt;&lt;head rend="h1"&gt;We might have been slower to abandon Stack Overflow if it wasn't a toxic hellhole&lt;/head&gt;&lt;p&gt;If you were a software developer prior to 2024, you probably used Stack Overflow. It was a reliable place to find good answers to many technical questions. If you asked any questions there, you probably also know that it was a toxic hellhole‚Äîyou often got criticized for not having basic knowledge or not understanding error messages.&lt;/p&gt;&lt;p&gt;This isn't exactly a secret. In 2018, Stack Overflow published a blog post entitled Stack Overflow Isn't Very Welcoming. It's Time for That to Change.&lt;/p&gt;&lt;p&gt;I don't know about you, but I never felt like behavior on the site got any better. Any time I asked a question I braced for impact. Was I about to get smacked down?&lt;/p&gt;&lt;p&gt;Recently, a graph of the total number of Stack Overflow questions over time started making rounds on the internet:&lt;/p&gt;&lt;p&gt;This graph shows a steady decline in usage from about 2017 to 2023 and then usage falls off a cliff. In a Reddit posts asking why developers are moving away from Stack Overflow, one user put it pretty succinctly:&lt;/p&gt;&lt;p&gt;"Because I can get an answer from an LLM (which does need to be verified) in less than a minute versus the hours or days I would have to wait to get a toxic and potentially useless reply on stackoverflow. They should really downsize or just kill the company it‚Äôs a relic of the past and most developers won‚Äôt miss it."&lt;/p&gt;&lt;p&gt;This is how a lot of us feel. Developer sentiment on generative AI can be mixed, but we know that at least it won't be an asshole to us, unlike many "helpers" on Stack Overflow.&lt;/p&gt;&lt;p&gt;One question I have is if we would have been slower to abandon Stack Overflow if it was a welcoming community. I don't know. Getting answers from generative AI would still have been faster.&lt;/p&gt;&lt;p&gt;I suspect we may have fought a little harder to preserve Stack Overflow in some capacity if it was a positive place. And I think this should be a lesson to other communities out there: instead of relying solely on being necessary, you should also be a positive place. Because when the next thing comes along and makes you less necessary, people won't hesitate to abandon you.&lt;/p&gt;Enjoy this post? Please subscribe to my RSS feed on Feedly or add my RSS XML file to another reader!&lt;/main&gt;
  &lt;comments/&gt;
&lt;/doc&gt;</description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527471</guid><pubDate>Wed, 07 Jan 2026 15:27:22 +0000</pubDate></item><item><title>US Job Openings Decline to Lowest Level in More Than a Year</title><link>https://www.bloomberg.com/news/articles/2026-01-07/us-job-openings-decline-to-lowest-level-in-more-than-a-year</link><description></description><guid isPermaLink="false">https://news.ycombinator.com/item?id=46527533</guid><pubDate>Wed, 07 Jan 2026 15:32:44 +0000</pubDate></item></channel></rss>